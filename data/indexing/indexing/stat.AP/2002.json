[{"id": "2002.00066", "submitter": "Ville Rimpil\\\"ainen", "authors": "Alexandra Koulouri and Ville Rimpilainen", "title": "Simultaneous Skull Conductivity and Focal Source Imaging from EEG\n  Recordings with the help of Bayesian Uncertainty Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The electroencephalography (EEG) source imaging problem is very sensitive to\nthe electrical modelling of the skull of the patient under examination.\nUnfortunately, the currently available EEG devices and their embedded software\ndo not take this into account; instead, it is common to use a literature-based\nskull conductivity parameter. In this paper, we propose a statistical method\nbased on the Bayesian approximation error approach to compensate for source\nimaging errors due to the unknown skull conductivity and, simultaneously, to\ncompute a low-order estimate for the actual skull conductivity value. By using\nsimulated EEG data that corresponds to focal source activity, we demonstrate\nthe potential of the method to reconstruct the underlying focal sources and\nlow-order errors induced by the unknown skull conductivity. Subsequently, the\nestimated errors are used to approximate the skull conductivity. The results\nindicate clear improvements in the source localization accuracy and feasible\nskull conductivity estimates.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 21:33:56 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Koulouri", "Alexandra", ""], ["Rimpilainen", "Ville", ""]]}, {"id": "2002.00079", "submitter": "Duzhe Wang", "authors": "Duzhe Wang, Haoda Fu, Po-Ling Loh", "title": "Boosting Algorithms for Estimating Optimal Individualized Treatment\n  Rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present nonparametric algorithms for estimating optimal individualized\ntreatment rules. The proposed algorithms are based on the XGBoost algorithm,\nwhich is known as one of the most powerful algorithms in the machine learning\nliterature. Our main idea is to model the conditional mean of clinical outcome\nor the decision rule via additive regression trees, and use the boosting\ntechnique to estimate each single tree iteratively. Our approaches overcome the\nchallenge of correct model specification, which is required in current\nparametric methods. The major contribution of our proposed algorithms is\nproviding efficient and accurate estimation of the highly nonlinear and complex\noptimal individualized treatment rules that often arise in practice. Finally,\nwe illustrate the superior performance of our algorithms by extensive\nsimulation studies and conclude with an application to the real data from a\ndiabetes Phase III trial.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 22:26:38 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Wang", "Duzhe", ""], ["Fu", "Haoda", ""], ["Loh", "Po-Ling", ""]]}, {"id": "2002.00100", "submitter": "Dean Eckles", "authors": "Madhav Kumar, Dean Eckles, Sinan Aral", "title": "Scalable bundling via dense product embeddings", "comments": "47 pages, 14 figures, 22 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bundling, the practice of jointly selling two or more products at a discount,\nis a widely used strategy in industry and a well examined concept in academia.\nHistorically, the focus has been on theoretical studies in the context of\nmonopolistic firms and assumed product relationships, e.g., complementarity in\nusage. We develop a new machine-learning-driven methodology for designing\nbundles in a large-scale, cross-category retail setting. We leverage historical\npurchases and consideration sets created from clickstream data to generate\ndense continuous representations of products called embeddings. We then put\nminimal structure on these embeddings and develop heuristics for\ncomplementarity and substitutability among products. Subsequently, we use the\nheuristics to create multiple bundles for each product and test their\nperformance using a field experiment with a large retailer. We combine the\nresults from the experiment with product embeddings using a hierarchical model\nthat maps bundle features to their purchase likelihood, as measured by the\nadd-to-cart rate. We find that our embeddings-based heuristics are strong\npredictors of bundle success, robust across product categories, and generalize\nwell to the retailer's entire assortment.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 23:34:56 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Kumar", "Madhav", ""], ["Eckles", "Dean", ""], ["Aral", "Sinan", ""]]}, {"id": "2002.00228", "submitter": "Thanuja Ambegoda", "authors": "Thanuja D. Ambegoda, Julien N. P. Martel, Jozef Adamcik, Matthew Cook,\n  Richard H. R. Hahnloser", "title": "Estimation of Z-Thickness and XY-Anisotropy of Electron Microscopy\n  Images using Gaussian Processes", "comments": null, "journal-ref": "Journal of Neuroinformatics and Neuroimaging. 2018;2(2):15-22", "doi": null, "report-no": null, "categories": "eess.IV cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Serial section electron microscopy (ssEM) is a widely used technique for\nobtaining volumetric information of biological tissues at nanometer scale.\nHowever, accurate 3D reconstructions of identified cellular structures and\nvolumetric quantifications require precise estimates of section thickness and\nanisotropy (or stretching) along the XY imaging plane. In fact, many image\nprocessing algorithms simply assume isotropy within the imaging plane. To\nameliorate this problem, we present a method for estimating thickness and\nstretching of electron microscopy sections using non-parametric Bayesian\nregression of image statistics. We verify our thickness and stretching\nestimates using direct measurements obtained by atomic force microscopy (AFM)\nand show that our method has a lower estimation error compared to a recent\nindirect thickness estimation method as well as a relative Z coordinate\nestimation method. Furthermore, we have made the first dataset of ssSEM images\nwith directly measured section thickness values publicly available for the\nevaluation of indirect thickness estimation methods.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2020 15:18:55 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2020 21:32:52 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Ambegoda", "Thanuja D.", ""], ["Martel", "Julien N. P.", ""], ["Adamcik", "Jozef", ""], ["Cook", "Matthew", ""], ["Hahnloser", "Richard H. R.", ""]]}, {"id": "2002.00351", "submitter": "Freeh Alenezi", "authors": "Freeh Alenezi and Chris. Tsokos", "title": "Bayesian Reliability Analysis of the Power Law Process with Respect to\n  the Higgins-Tsokos Loss Function for Modeling Software Failure Times", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Power Law Process, also known as Non-Homogeneous Poisson Process, has\nbeen used in various aspects, one of which is the software reliability\nassessment. Specifically, by using its intensity function to compute the rate\nof change of a software reliability as time-varying function. Justification of\nBayesian analysis applicability to the Power Law Process was shown using real\ndata. The probability distribution that best characterizes the behavior of the\nkey parameter of the intensity function was first identified, then the\nlikelihood-based Bayesian reliability estimate of the Power Law Process under\nthe Higgins-Tsokos loss function was obtained. As a result of a simulation\nstudy and using real data, the Bayesian estimate shows an outstanding\nperformance compared to the maximum likelihood estimate using different sample\nsizes. In addition, a sensitivity analysis was performed, resulting in the\nBayesian estimate being sensitive to the prior selection; whether parametric or\nnon-parametric.\n", "versions": [{"version": "v1", "created": "Sun, 2 Feb 2020 08:34:32 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Alenezi", "Freeh", ""], ["Tsokos", "Chris.", ""]]}, {"id": "2002.00386", "submitter": "Andr\\'as B\\'alint", "authors": "Andr\\'as B\\'alint, Carol A.C. Flannagan, Andrew Leslie, Sheila Klauer,\n  Feng Guo, Marco Dozza", "title": "Multitasking additional-to-driving: Prevalence, structure, and\n  associated risk in SHRP2 naturalistic driving data", "comments": "Accepted manuscript, to appear in Accident Analysis and Prevention.\n  21 pages, 11 figures", "journal-ref": "Accident Analysis & Prevention 137, March 2020, 105455", "doi": "10.1016/j.aap.2020.105455", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper 1) analyzes the extent to which drivers engage in multitasking\nadditional-to-driving (MAD) under various conditions, 2) specifies odds ratios\n(ORs) of crashing associated with MAD compared to no task engagement, and 3)\nexplores the structure of MAD, based on data from the Second Strategic Highway\nResearch Program Naturalistic Driving Study (SHRP2 NDS). Sensitivity analysis\nin which secondary tasks were re-defined by grouping similar tasks was\nperformed to investigate the extent to which ORs are affected by the specific\ntask definitions in SHRP2. A novel visual representation of multitasking was\ndeveloped to show which secondary tasks co-occur frequently and which ones do\nnot. MAD occurs in 11% of control driving segments, 22% of crashes and\nnear-crashes (CNC), 26% of Level 1-3 crashes and 39% of rear-end striking\ncrashes, and 9%, 16%, 17% and 28% respectively for the same event types if MAD\nis defined in terms of general task groups. The most common co-occurrences of\nsecondary tasks vary substantially among event types; for example, 'Passenger\nin adjacent seat - interaction' and 'Other non-specific internal eye glance'\ntend to co-occur in CNC but tend not to co-occur in control driving segments.\nThe odds ratios of MAD compared to driving without any secondary task and the\ncorresponding 95% confidence intervals are 2.38 (2.17-2.61) for CNC, 3.72\n(3.11-4.45) for Level 1-3 crashes and 8.48 (5.11-14.07) for rear-end striking\ncrashes. The corresponding ORs using general task groups to define MAD are\nslightly lower at 2.00 (1.80-2.21) for CNC, 3.03 (2.48-3.69) for Level 1-3\ncrashes and 6.94 (4.04-11.94) for rear-end striking crashes. The results\nconfirm that independently of whether secondary tasks are defined according to\nSHRP2 or general task groups, the reduction of driving performance from MAD\nobserved in simulator studies is manifested in real-world crashes as well.\n", "versions": [{"version": "v1", "created": "Sun, 2 Feb 2020 13:00:31 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["B\u00e1lint", "Andr\u00e1s", ""], ["Flannagan", "Carol A. C.", ""], ["Leslie", "Andrew", ""], ["Klauer", "Sheila", ""], ["Guo", "Feng", ""], ["Dozza", "Marco", ""]]}, {"id": "2002.00426", "submitter": "Ye Liang", "authors": "Ye Liang, Dan Xu, Shang Fu, Kewa Gao, Jingjing Huan, Linyong Xu,\n  Jia-da Li", "title": "A Simple Prediction Model for the Development Trend of 2019-nCov\n  Epidemics Based on Medical Observations", "comments": "Written on February 1, 2020 at 15:00 (GMT+08:00) 12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to predict the development trend of the 2019 coronavirus\n(2019-nCov), we established an prediction model to predict the number of\ndiagnoses case in China except Hubei Province. From January 25 to January 29,\n2020, we optimized 6 prediction models, 5 of them based on the number of\nmedical observations to predicts the peak time of confirmed diagnosis will\nappear on the period of morning of January 29 from 24:00 to February 2 before 5\no'clock 24:00. Then we tracked the data from 24 o'clock on January 29 to 24\no'clock on January 31, and found that the predicted value of the data on the\n3rd has a small deviation from the actual value, and the actual value has\nalways remained within the range predicted by the comprehensive prediction\nmodel 6. Therefore we discloses this finding and will continue to track whether\nthis pattern can be maintained for longer. We believe that the changes medical\nobservation case number may help to judge the trend of the epidemic situation\nin advance.\n", "versions": [{"version": "v1", "created": "Sun, 2 Feb 2020 16:20:11 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Liang", "Ye", ""], ["Xu", "Dan", ""], ["Fu", "Shang", ""], ["Gao", "Kewa", ""], ["Huan", "Jingjing", ""], ["Xu", "Linyong", ""], ["Li", "Jia-da", ""]]}, {"id": "2002.00427", "submitter": "Nooshin Yousefi", "authors": "Nooshin Yousefi, David W. Coit, Zhu Xiaoyan", "title": "Dynamic maintenance policy for systems with repairable components\n  subject to mutually dependent competing failure processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a repairable multi-component system is studied where all the\ncomponents can be repaired individually within the system. The whole system is\ninspected at inspection intervals and the failed components are detected and\nreplaced with a new one, while the other components continue functioning.\nReplacing components individually within the system makes their initial age to\nbe different at each inspection time. Different initial age of all the\ncomponents have affect on the system reliability and probability of failure and\nconsequently the optimal inspection time, which is for the whole system not\nindividual components. A dynamic maintenance policy is proposed to find the\nnext inspection time based on the initial age of all the components. Two\ncompeting failure processes of degradation and a shock process are considered\nfor each component. In our paper, there is a mutual dependency between the\ndegradation process and shock process. Each incoming shock adds additional\nabrupt damages on the cumulative degradation path of all the components, and\nthe shock arrival process is affected by the system degradation process. A\nrealistic numerical example is presented to illustrate the proposed reliability\nand maintenance model.\n", "versions": [{"version": "v1", "created": "Sun, 2 Feb 2020 16:29:31 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Yousefi", "Nooshin", ""], ["Coit", "David W.", ""], ["Xiaoyan", "Zhu", ""]]}, {"id": "2002.00542", "submitter": "Jae Youn Ahn", "authors": "Rosy Oh, Youngju Lee, Dan Zhu, and Jae Youn Ahn", "title": "Predictive Risk Analysis in Collective Risk Model: Choices between\n  Historical Frequency and Aggregate Severity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical risk classification procedure in insurance is consists of a priori\nrisk classification determined by observable risk characteristics, and a\nposteriori risk classification where the premium is adjusted to reflect the\npolicyholder's claim history. While using the full claim history data is\noptimal in a posteriori risk classification procedure, i.e. giving premium\nestimators with the minimal variances, some insurance sectors, however, only\nuse partial information of the claim history for determining the appropriate\npremium to charge. Classical examples include that auto insurances premium are\ndetermined by the claim frequency data and workers' compensation insurances are\nbased on the aggregate severity. The motivation for such practice is to have a\nsimplified and efficient posteriori risk classification procedure which is\ncustomized to the involved insurance policy. This paper compares the relative\nefficiency of the two simplified posteriori risk classifications, i.e. based on\nfrequency versus severity, and provides the mathematical framework to assist\npractitioners in choosing the most appropriate practice.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 02:47:21 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Oh", "Rosy", ""], ["Lee", "Youngju", ""], ["Zhu", "Dan", ""], ["Ahn", "Jae Youn", ""]]}, {"id": "2002.00920", "submitter": "Vincent Adam", "authors": "Vincent Adam, Alexandre Hyafil", "title": "Non-linear regression models for behavioral and neural data analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression models are popular tools in empirical sciences to infer the\ninfluence of a set of variables onto a dependent variable given an experimental\ndataset. In neuroscience and cognitive psychology, Generalized Linear Models\n(GLMs) -including linear regression, logistic regression, and Poisson GLM- is\nthe regression model of choice to study the factors that drive participant's\nchoices, reaction times and neural activations. These methods are however\nlimited as they only capture linear contributions of each regressors. Here, we\nintroduce an extension of GLMs called Generalized Unrestricted Models (GUMs),\nwhich allows to infer a much richer set of contributions of the regressors to\nthe dependent variable, including possible interactions between the regressors.\nIn a GUM, each regressor is passed through a linear or nonlinear function, and\nthe contribution of the different resulting transformed regressors can be\nsummed or multiplied to generate a predictor for the dependent variable. We\npropose a Bayesian treatment of these models in which we endow functions with\nGaussian Process priors, and we present two methods to compute a posterior over\nthe functions given a dataset: the Laplace method and a sparse variational\napproach, which scales better for large dataset. For each method, we assess the\nquality of the model estimation and we detail how the hyperparameters (defining\nfor example the expected smoothness of the function) can be fitted. Finally, we\nillustrate the power of the method on a behavioral dataset where subjects\nreported the average perceived orientation of a series of gratings. The method\nallows to recover the mapping of the grating angle onto perceptual evidence for\neach subject, as well as the impact of the grating based on its position.\nOverall, GUMs provides a very rich and flexible framework to run nonlinear\nregression analysis in neuroscience, psychology, and beyond.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 18:00:18 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Adam", "Vincent", ""], ["Hyafil", "Alexandre", ""]]}, {"id": "2002.00938", "submitter": "Haoyu Wang", "authors": "Haoyu Wang, Srikanth Patala and Brian J. Reich", "title": "Constrained Bayesian Nonparametric Regression for Grain Boundary Energy\n  Predictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grain boundary (GB) energy is a fundamental property that affects the form of\ngrain boundary and plays an important role to unveil the behavior of\npolycrystalline materials. With a better understanding of grain boundary energy\ndistribution (GBED), we can produce more durable and efficient materials that\nwill further improve productivity and reduce loss. The lack of robust GB\nstructure-property relationships still remains one of the biggest obstacles\ntowards developing true bottom-up models for the behavior of polycrystalline\nmaterials. Progress has been slow because of the inherent complexity associated\nwith the structure of interfaces and the vast five-dimensional configurational\nspace in which they reside. Estimating the GBED is challenging from a\nstatistical perspective because there are not direct measurements on the grain\nboundary energy. We only have indirect information in the form of an\nunidentifiable homogeneous set of linear equations. In this paper, we propose a\nnew statistical model to determine the GBED from the microstructures of\npolycrystalline materials. We apply spline-based regression with constraints to\nsuccessfully recover the GB energy surface. Hamiltonian Monte Carlo and Gibbs\nsampling are used for computation and model fitting. Compared with conventional\nmethods, our method not only gives more accurate predictions but also provides\nprediction uncertainties.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 18:47:08 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Wang", "Haoyu", ""], ["Patala", "Srikanth", ""], ["Reich", "Brian J.", ""]]}, {"id": "2002.00951", "submitter": "Nathalie Peyrard", "authors": "Marie-Jos\\'ee Cros and Jean-No\\\"el Aubertot and Sabrina Gaba and\n  Xavier Reboud and R\\'egis Sabbadin and Nathalie Peyrard", "title": "Improving Pest Monitoring Networks in order to reduce pesticide use in\n  agriculture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disease and pest control largely rely on pesticides use and progress still\nremains to be made towards more sustainable practices. Pest Monitoring Networks\n(PMNs) can provide useful information for improving crop protection by\nrestricting pesticide use to the situations that best require it. However, the\nefficacy of a PMN to control pests may depend on its spatial density and\nspace/time sampling balance. Furthermore the best trade-off between the\nmonitoring effort and the impact of the PMN information may be pest dependent.\nWe developed a generic simulation model that links PMN information to treatment\ndecisions and pest dynamics. We derived the number of treatments, the epidemic\nextension and the global gross margin for different families of pests. For\nsoil-borne pathogens and weeds, we found that increasing the spatial density of\na PMN significantly decreased the number of treatments (up to 67\\%), with an\nonly marginal increase in infection. Considering past observations had a\nsecond-order effect (up to a 13\\% decrease). For the spatial scale of our\nstudy, the PMN information had practically no influence in the case of insects.\nThe next step is to go beyond PMN analysis to design and chose among\nsustainable management strategies at the landscape scale.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 16:01:25 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Cros", "Marie-Jos\u00e9e", ""], ["Aubertot", "Jean-No\u00ebl", ""], ["Gaba", "Sabrina", ""], ["Reboud", "Xavier", ""], ["Sabbadin", "R\u00e9gis", ""], ["Peyrard", "Nathalie", ""]]}, {"id": "2002.00993", "submitter": "Martina Vittorietti", "authors": "Martina Vittorietti, Javier Hidalgo, Jilt Sietsma, Wei Li, Geurt\n  Jongbloed", "title": "Isotonic regression for metallic microstructure data: estimation and\n  testing under order restrictions", "comments": "18 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Investigating the main determinants of the mechanical performance of metals\nis not a simple task. Already known physical inspired qualitative relations\nbetween 2D microstructure characteristics and 3D mechanical properties can act\nas the starting point of the investigation. Isotonic regression allows to take\ninto account ordering relations and leads to more efficient and accurate\nresults when the underlying assumptions actually hold. The main goal in this\npaper is to test order relations in a model inspired by a materials science\napplication. The statistical estimation procedure is described considering\nthree different scenarios according to the knowledge of the variances: known\nvariance ratio, completely unknown variances, variances under order\nrestrictions. New likelihood ratio tests are developed in the last two cases.\nBoth parametric and non-parametric bootstrap approaches are developed for\nfinding the distribution of the test statistics under the null hypothesis.\nFinally an application on the relation between Geometrically Necessary\nDislocations and number of observed microstructure precipitations is shown.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 19:42:04 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Vittorietti", "Martina", ""], ["Hidalgo", "Javier", ""], ["Sietsma", "Jilt", ""], ["Li", "Wei", ""], ["Jongbloed", "Geurt", ""]]}, {"id": "2002.01019", "submitter": "Yu Wang", "authors": "Yu Wang, Nhu D. Le, James V. Zidek", "title": "Approximately Optimal Spatial Design: How Good is it?", "comments": "Accepted in Spatial Statistics, Special Issues in Spatial Data\n  Science", "journal-ref": null, "doi": "10.1016/j.spasta.2020.100409", "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing recognition of the association between adverse human health\nconditions and many environmental substances as well as processes has led to\nthe need to monitor them. An important problem that arises in environmental\nstatistics is the design of the locations of the monitoring stations for those\nenvironmental processes of interest. One particular design criterion for\nmonitoring networks that tries to reduce the uncertainty about predictions of\nunseen processes is called the maximum-entropy design. However, this design\ncriterion involves a hard optimization problem that is computationally\nintractable for large data sets. Previous work of Wang et al. (2017) examined a\nprobabilistic model that can be implemented efficiently to approximate the\nunderlying optimization problem. In this paper, we attempt to establish\nstatistically sound tools for assessing the quality of the approximations.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 21:17:36 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Wang", "Yu", ""], ["Le", "Nhu D.", ""], ["Zidek", "James V.", ""]]}, {"id": "2002.01106", "submitter": "Karol Rydzewski", "authors": "Karol Rydzewski, Jerzy Konorski", "title": "A reactive algorithm for deducing nodal forwarding behavior in a\n  multihop ad-hoc wireless network in the presence of errors", "comments": "7 pages, 4 figures, computer networks, detection, agent systems, ad\n  hoc networks, reputation, errors", "journal-ref": "A reactive algorithm for deducing nodal forwarding behavior in a\n  multihop ad-hoc wireless networks in the presence of errors, Intl. Journal of\n  Electronics and Telecommunications, vol. 66, no. 1, pp. 193-199, Polish\n  Academy of Sciences 2020", "doi": "10.24425/ijet.2020.131863", "report-no": null, "categories": "cs.NI stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  novel algorithm is presented to deduce individual nodal forwarding behavior\nfrom standard end-to-end acknowledgments. The algorithm is based on a\nwell-established mathematical method and is robust to network related errors\nand nodal behavior changes. The proposed solution was verified in a network\nsimulation, during which it achieved sound results in a challenging multihop\nad-hoc network environment.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 03:33:45 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Rydzewski", "Karol", ""], ["Konorski", "Jerzy", ""]]}, {"id": "2002.01110", "submitter": "Abdollah Shafieezadeh", "authors": "Zeyu Wang and Abdollah Shafieezadeh", "title": "REAK: Reliability analysis through Error rate-based Adaptive Kriging", "comments": null, "journal-ref": null, "doi": "10.1016/j.ress.2018.10.004", "report-no": null, "categories": "stat.AP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As models in various fields are becoming more complex, associated\ncomputational demands have been increasing significantly. Reliability analysis\nfor these systems when failure probabilities are small is significantly\nchallenging, requiring a large number of costly simulations. To address this\nchallenge, this paper introduces Reliability analysis through Error rate-based\nAdaptive Kriging (REAK). An extension of the Central Limit Theorem based on\nLindeberg condition is adopted here to derive the distribution of the number of\ndesign samples with wrong sign estimate and subsequently determine the maximum\nerror rate for failure probability estimates. This error rate enables optimal\nestablishment of effective sampling regions at each stage of an adaptive scheme\nfor strategic generation of design samples. Moreover, it facilitates setting a\ntarget accuracy for failure probability estimation, which is used as stopping\ncriterion for reliability analysis. These capabilities together can\nsignificantly reduce the number of calls to sophisticated, computationally\ndemanding models. The application of REAK for four examples with varying extent\nof nonlinearity and dimension is presented. Results indicate that REAK is able\nto reduce the computational demand by as high as 50% compared to\nstate-of-the-art methods of Adaptive Kriging with Monte Carlo Simulation\n(AK-MCS) and Improved Sequential Kriging Reliability Analysis (ISKRA).\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 03:47:20 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Wang", "Zeyu", ""], ["Shafieezadeh", "Abdollah", ""]]}, {"id": "2002.01193", "submitter": "Marius \\\"Otting", "authors": "Marius \\\"Otting, Roland Langrock, Antonello Maruotti", "title": "A copula-based multivariate hidden Markov model for modelling momentum\n  in football", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the potential occurrence of change points - commonly referred\nto as \"momentum shifts\" - in the dynamics of football matches. For that\npurpose, we model minute-by-minute in-game statistics of Bundesliga matches\nusing hidden Markov models (HMMs). To allow for within-state correlation of the\nvariables considered, we formulate multivariate state-dependent distributions\nusing copulas. For the Bundesliga data considered, we find that the fitted HMMs\ncomprise states which can be interpreted as a team showing different levels of\ncontrol over a match. Our modelling framework enables inference related to\ncauses of momentum shifts and team tactics, which is of much interest to\nmanagers, bookmakers, and sports fans.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 09:44:13 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 09:50:19 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["\u00d6tting", "Marius", ""], ["Langrock", "Roland", ""], ["Maruotti", "Antonello", ""]]}, {"id": "2002.01328", "submitter": "Riccardo Fogliato", "authors": "Riccardo Fogliato, Natalia L. Oliveira, Ronald Yurko", "title": "TRAP: A Predictive Framework for Trail Running Assessment of Performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trail running is an endurance sport in which athletes face severe physical\nchallenges. Due to the growing number of participants, the organization of\nlimited staff, equipment, and medical support in these races now plays a key\nrole. Monitoring runner's performance is a difficult task that requires\nknowledge of the terrain and of the runner's ability. In the past, choices were\nsolely based on the organizers' experience without reliance on data. However,\nthis approach is neither scalable nor transferable. Instead, we propose a firm\nstatistical methodology to perform this task, both before and during the race.\nOur proposed framework, Trail Running Assessment of Performance (TRAP), studies\n(1) the the assessment of the runner's ability to reach the next checkpoint,\n(2) the prediction of the runner's expected passage time at the next\ncheckpoint, and (3) corresponding prediction intervals for the passage time. To\nobtain data on the ability of runners, we introduce a Python package,\nScrapITRA, to access the race history of runners from the International Trail\nRunning Association (ITRA). We apply our methodology, using the ITRA data along\nwith checkpoint and terrain-level information, to the \"holy grail\" of\nultra-trail running, the Ultra-Trail du Mont-Blanc (UTMB) race, demonstrating\nthe predictive power of our methodology.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 14:47:06 GMT"}, {"version": "v2", "created": "Sun, 12 Jul 2020 10:44:13 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Fogliato", "Riccardo", ""], ["Oliveira", "Natalia L.", ""], ["Yurko", "Ronald", ""]]}, {"id": "2002.01369", "submitter": "Marta Bofill Roig", "authors": "Marta Bofill Roig and Guadalupe G\\'omez Melis", "title": "A class of two-sample nonparametric statistics for binary and\n  time-to-event outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a class of two-sample statistics for testing the equality of\nproportions and the equality of survival functions. We build our proposal on a\nweighted combination of a score test for the difference in proportions and a\nWeighted Kaplan-Meier statistic-based test for the difference of survival\nfunctions. The proposed statistics are fully non-parametric and do not rely on\nthe proportional hazards assumption for the survival outcome. We present the\nasymptotic distribution of these statistics, propose a variance estimator and\nshow their asymptotic properties under fixed and local alternatives. We discuss\ndifferent choices of weights including those that control the relative\nrelevance of each outcome and emphasize the type of difference to be detected\nin the survival outcome. We evaluate the performance of these statistics with a\nsimulation study, and illustrate their use with a randomized phase III cancer\nvaccine trial. We have implemented the proposed statistics in the R package\nSurvBin, available on GitHub (https://github.com/MartaBofillRoig/SurvBin).\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 15:35:51 GMT"}, {"version": "v2", "created": "Sat, 4 Apr 2020 15:57:41 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Roig", "Marta Bofill", ""], ["Melis", "Guadalupe G\u00f3mez", ""]]}, {"id": "2002.01467", "submitter": "Lilianne Mujica-Parodi", "authors": "LR Mujica-Parodi and HH Strey", "title": "Making Sense of Computational Psychiatry", "comments": "16 pages, 5 figures. Int J Neuropsychopharmacol. 2020 Mar 27", "journal-ref": null, "doi": "10.1093/ijnp/pyaa013", "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In psychiatry, we often speak of constructing \"models.\" Here we try to make\nsense of what such a claim might mean, starting with the most fundamental\nquestion: \"What is (and isn't) a model?\". We then discuss, in a concrete\nmeasurable sense, what it means for a model to be useful. In so doing, we first\nidentify the added value that a computational model can provide, in the context\nof accuracy and power. We then present the limitations of standard statistical\nmethods and provide suggestions for how we can expand the explanatory power of\nour analyses by reconceptualizing statistical models as dynamical systems.\nFinally, we address the problem of model building, suggesting ways in which\ncomputational psychiatry can escape the potential for cognitive biases imposed\nby classical hypothesis-driven research, exploiting deep systems-level\ninformation contained within neuroimaging data to advance our understanding of\npsychiatric neuroscience.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 18:46:58 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 15:30:17 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Mujica-Parodi", "LR", ""], ["Strey", "HH", ""]]}, {"id": "2002.01578", "submitter": "Geoff Boeing", "authors": "Geoff Boeing, Jake Wegmann, Junfeng Jiao", "title": "Rental Housing Spot Markets: How Online Information Exchanges Can\n  Supplement Transacted-Rents Data", "comments": null, "journal-ref": "Journal of Planning Education and Research, 2020", "doi": "10.1177/0739456X20904435", "report-no": null, "categories": "econ.GN cs.CY q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional US rental housing data sources such as the American Community\nSurvey and the American Housing Survey report on the transacted market - what\nexisting renters pay each month. They do not explicitly tell us about the spot\nmarket - i.e., the asking rents that current homeseekers must pay to acquire\nhousing - though they are routinely used as a proxy. This study compares\ngovernmental data to millions of contemporaneous rental listings and finds that\nasking rents diverge substantially from these most recent estimates.\nConventional housing data understate current market conditions and\naffordability challenges, especially in cities with tight and expensive rental\nmarkets.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 23:23:35 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Boeing", "Geoff", ""], ["Wegmann", "Jake", ""], ["Jiao", "Junfeng", ""]]}, {"id": "2002.01706", "submitter": "Aleksandar Kolev", "authors": "Aleksandar A. Kolev, Gordon J. Ross", "title": "Semiparametric Bayesian Forecasting of Spatial Earthquake Occurrences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-exciting Hawkes processes are used to model events which cluster in time\nand space, and have been widely studied in seismology under the name of the\nEpidemic Type Aftershock Sequence (ETAS) model. In the ETAS framework, the\noccurrence of the mainshock earthquakes in a geographical region is assumed to\nfollow an inhomogeneous spatial point process, and aftershock events are then\nmodelled via a separate triggering kernel. Most previous studies of the ETAS\nmodel have relied on point estimates of the model parameters due to the\ncomplexity of the likelihood function, and the difficulty in estimating an\nappropriate mainshock distribution. In order to take estimation uncertainty\ninto account, we instead propose a fully Bayesian formulation of the ETAS model\nwhich uses a nonparametric Dirichlet process mixture prior to capture the\nspatial mainshock process. Direct inference for the resulting model is\nproblematic due to the strong correlation of the parameters for the mainshock\nand triggering processes, so we instead use an auxiliary latent variable\nroutine to perform efficient inference.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 10:11:26 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Kolev", "Aleksandar A.", ""], ["Ross", "Gordon J.", ""]]}, {"id": "2002.01713", "submitter": "Hanyi Luo", "authors": "Hanyi Luo, Mengzhan Liufu, Dongrong Li", "title": "Intelligent Online Food Delivery System: A Dynamic Model to Generate\n  Delivery Strategy and Tip Advice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the rapid development of online food ordering platforms and rocketing\ngrowth of demand, the market is about to saturate soon, and the future trend is\nto seek efficient utilization of resources. Specifically speaking, food company\nmust have a reliable algorithm to help them produce efficient delivery\nstrategies; individual customers need planning for their decision making in\nthis field. For example, when customers add tip to their order with the sake of\ncontrolling or reducing latency. However, few customers know how much tip is\nenough to reach their desired latency. Therefore, in our paper, we establish a\ndynamic model to generate delivery strategy for companies and tip advice for\ncustomers. We believe that the system we design is more efficient than the\ncurrently primitive system. We simulate the delivery process and generate\ndelivery strategies using genetic annealing because it can approach a near\noptimal solution. High-quality delivery queue ensures that those orders can be\ndelivered within an acceptable amount of time. Next, we construct regressions\nto find out relationships between multiple factors and latency and then\ngenerate the advisory amount of tip. Finally, we plug in those values and\ndesired waiting time, getting the advisory tip price. Multiple indexes suggest\nthat our regression results are accurate and reliable.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 10:28:40 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Luo", "Hanyi", ""], ["Liufu", "Mengzhan", ""], ["Li", "Dongrong", ""]]}, {"id": "2002.01798", "submitter": "Zhengxiao Li", "authors": "Liang Yang, Zhengxiao Li, Shengwang Meng", "title": "Risk Loadings in Classification Ratemaking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The risk premium of a policy is the sum of the pure premium and the risk\nloading. In the classification ratemaking process, generalized linear models\nare usually used to calculate pure premiums, and various premium principles are\napplied to derive the risk loadings. No matter which premium principle is used,\nsome risk loading parameters should be given in advance subjectively. To\novercome this subjective problem and calculate the risk premium more reasonably\nand objectively, we propose a top-down method to calculate these risk loading\nparameters. First, we implement the bootstrap method to calculate the total\nrisk premium of the portfolio. Then, under the constraint that the portfolio's\ntotal risk premium should equal the sum of the risk premiums of each policy,\nthe risk loading parameters are determined. During this process, besides using\ngeneralized linear models, three kinds of quantile regression models are also\napplied, namely, traditional quantile regression model, fully parametric\nquantile regression model, and quantile regression model with coefficient\nfunctions. The empirical result shows that the risk premiums calculated by the\nmethod proposed in this study can reasonably differentiate the heterogeneity of\ndifferent risk classes.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 14:08:32 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Yang", "Liang", ""], ["Li", "Zhengxiao", ""], ["Meng", "Shengwang", ""]]}, {"id": "2002.01890", "submitter": "Thais Fonseca Dr", "authors": "Victhor S. Sart\\'orio and Tha\\'is C. O. Fonseca", "title": "Dynamic clustering of time series data", "comments": "27 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for clustering multivariate time-series data based on\nDynamic Linear Models. Whereas usual time-series clustering methods obtain\nstatic membership parameters, our proposal allows each time-series to\ndynamically change their cluster memberships over time. In this context, a\nmixture model is assumed for the time series and a flexible Dirichlet evolution\nfor mixture weights allows for smooth membership changes over time. Posterior\nestimates and predictions can be obtained through Gibbs sampling, but a more\nefficient method for obtaining point estimates is presented, based on\nStochastic Expectation-Maximization and Gradient Descent. Finally, two\napplications illustrate the usefulness of our proposed model to model both\nunivariate and multivariate time-series: World Bank indicators for the\nrenewable energy consumption of EU nations and the famous Gapminder dataset\ncontaining life-expectancy and GDP per capita for various countries.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 12:01:28 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Sart\u00f3rio", "Victhor S.", ""], ["Fonseca", "Tha\u00eds C. O.", ""]]}, {"id": "2002.02354", "submitter": "Abdollah Shafieezadeh", "authors": "Chi Zhang, Zeyu Wang, and Abdollah Shafieezadeh", "title": "Value of Information Analysis via Active Learning and Knowledge Sharing\n  in Error-Controlled Adaptive Kriging", "comments": null, "journal-ref": null, "doi": "10.1109/ACCESS.2020.2980228", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large uncertainties in many phenomena have challenged decision making.\nCollecting additional information to better characterize reducible\nuncertainties is among decision alternatives. Value of information (VoI)\nanalysis is a mathematical decision framework that quantifies expected\npotential benefits of new data and assists with optimal allocation of resources\nfor information collection. However, analysis of VoI is computational very\ncostly because of the underlying Bayesian inference especially for\nequality-type information. This paper proposes the first surrogate-based\nframework for VoI analysis. Instead of modeling the limit state functions\ndescribing events of interest for decision making, which is commonly pursued in\nsurrogate model-based reliability methods, the proposed framework models system\nresponses. This approach affords sharing equality-type information from\nobservations among surrogate models to update likelihoods of multiple events of\ninterest. Moreover, two knowledge sharing schemes called model and training\npoints sharing are proposed to most effectively take advantage of the knowledge\noffered by costly model evaluations. Both schemes are integrated with an error\nrate-based adaptive training approach to efficiently generate accurate Kriging\nsurrogate models. The proposed VoI analysis framework is applied for an optimal\ndecision-making problem involving load testing of a truss bridge. While\nstate-of-the-art methods based on importance sampling and adaptive Kriging\nMonte Carlo simulation are unable to solve this problem, the proposed method is\nshown to offer accurate and robust estimates of VoI with a limited number of\nmodel evaluations. Therefore, the proposed method facilitates the application\nof VoI for complex decision problems.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 16:58:27 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 02:30:25 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Zhang", "Chi", ""], ["Wang", "Zeyu", ""], ["Shafieezadeh", "Abdollah", ""]]}, {"id": "2002.02434", "submitter": "John Bob Gali", "authors": "John Bob Gali, Priyadip Ray and Goutam Das", "title": "GLRT based Adaptive-Thresholding for CFAR-Detection of Pareto-Target in\n  Pareto-Distributed Clutter", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After Pareto distribution has been validated for sea clutter returns in\nvaried scenarios, some heuristics of adaptive-thresholding appeared in the\nliterature for constant false alarm rate (CFAR) criteria. These schemes used\nthe same adaptive-thresholding form that was originally derived for detecting\nSwerling-I (exponential) target in exponentially distributed clutter.\nStatistical procedures obtained under such idealistic assumptions would affect\nthe detection performance when applied to newer target and clutter models, esp.\nheavy tail distributions like Pareto. Further, in addition to the sea clutter\nreturns, it has also been reported that Generalized Pareto distribution fits\nbest for the measured Radar-cross-section (RCS) data of a SAAB aircraft.\nTherefore, in Radar application scenarios like Airborne Warning and Control\nSystem (AWACS), when both the target and clutter are Pareto distributed, we\npose the detection problem as a two-sample, Pareto vs. Pareto composite\nhypothesis testing problem. We address this problem by corroborating the binary\nhypothesis framework instead of the conventional way of tweaking the existing\nadaptive-thresholding CFAR detector. Whereby, for the composite case,\nconsidering no knowledge of both scale and shape parameters of Pareto\ndistributed clutter, we derive the new adaptive-thresholding detector based on\nthe generalized likelihood ratio test (GLRT) statistic. We further show that\nour proposed adaptive-thresholding detector has a CFAR property. We provide\nextensive simulation results to demonstrate the performance of the proposed\ndetector.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 18:36:15 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 06:35:16 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Gali", "John Bob", ""], ["Ray", "Priyadip", ""], ["Das", "Goutam", ""]]}, {"id": "2002.02493", "submitter": "Simon Berrebi", "authors": "Simon Berrebi and Sanskruti Joshi and Kari E Watkins", "title": "On Ridership and Frequency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.SI econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even before the start of the COVID-19 pandemic, bus ridership in the United\nStates had attained its lowest level since 1973. If transit agencies hope to\nreverse this trend, they must understand how their service allocation policies\naffect ridership. This paper is among the first to model ridership trends on a\nhyper-local level over time. A Poisson fixed-effects model is developed to\nevaluate the ridership elasticity to frequency on weekdays using passenger\ncount data from Portland, Miami, Minneapolis/St-Paul, and Atlanta between 2012\nand 2018. In every agency, ridership is found to be elastic to frequency when\nobserving the variation between individual route-segments at one point in time.\nIn other words, the most frequent routes are already the most productive in\nterms of passengers per vehicle-trip. When observing the variation within each\nroute-segment over time, however, ridership is inelastic; each additional\nvehicle-trip is expected to generate less ridership than the average bus\nalready on the route. In three of the four agencies, the elasticity is a\ndecreasing function of prior frequency, meaning that low-frequency routes are\nthe most sensitive to changes in frequency. This paper can help transit\nagencies anticipate the marginal effect of shifting service throughout the\nnetwork. As the quality and availability of passenger count data improve, this\npaper can serve as the methodological basis to explore the dynamics of bus\nridership.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 19:57:04 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 19:26:12 GMT"}, {"version": "v3", "created": "Fri, 9 Apr 2021 22:23:30 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Berrebi", "Simon", ""], ["Joshi", "Sanskruti", ""], ["Watkins", "Kari E", ""]]}, {"id": "2002.02592", "submitter": "Nicholas James", "authors": "Nick James, Max Menzies", "title": "Equivalence relations and $L^p$ distances between time series", "comments": "Equal contribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a general framework for defining equivalence and measuring\ndistances between time series, and a first concrete method for doing so. We\nprove the existence of equivalence relations on the space of time series, such\nthat the quotient spaces can be equipped with a metrizable topology. We\nillustrate algorithmically how to calculate such distances among a collection\nof time series, and perform clustering analysis based on these distances. We\napply these insights to analyse the recent bushfires in NSW, Australia. There,\nwe introduce a new method to analyse time series in a cross-contextual setting.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 02:32:33 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["James", "Nick", ""], ["Menzies", "Max", ""]]}, {"id": "2002.02601", "submitter": "Eric Lock", "authors": "Eric F. Lock, Jun Young Park, and Katherine A. Hoadley", "title": "Bidimensional linked matrix factorization for pan-omics pan-cancer\n  analysis", "comments": "46 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several modern applications require the integration of multiple large data\nmatrices that have shared rows and/or columns. For example, cancer studies that\nintegrate multiple omics platforms across multiple types of cancer, pan-omics\npan-cancer analysis, have extended our knowledge of molecular heterogenity\nbeyond what was observed in single tumor and single platform studies. However,\nthese studies have been limited by available statistical methodology. We\npropose a flexible approach to the simultaneous factorization and decomposition\nof variation across such bidimensionally linked matrices, BIDIFAC+. This\ndecomposes variation into a series of low-rank components that may be shared\nacross any number of row sets (e.g., omics platforms) or column sets (e.g.,\ncancer types). This builds on a growing literature for the factorization and\ndecomposition of linked matrices, which has primarily focused on multiple\nmatrices that are linked in one dimension (rows or columns) only. Our objective\nfunction extends nuclear norm penalization, is motivated by random matrix\ntheory, gives an identifiable decomposition under relatively mild conditions,\nand can be shown to give the mode of a Bayesian posterior distribution. We\napply BIDIFAC+ to pan-omics pan-cancer data from TCGA, identifying shared and\nspecific modes of variability across 4 different omics platforms and 29\ndifferent cancer types.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 03:11:44 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Lock", "Eric F.", ""], ["Park", "Jun Young", ""], ["Hoadley", "Katherine A.", ""]]}, {"id": "2002.02627", "submitter": "{\\O}ystein S{\\o}rensen", "authors": "{\\O}ystein S{\\o}rensen, Andreas M Brandmaier, D\\'idac Maci\\`a, Klaus\n  Ebmeier, Paolo Ghisletta, Rogier A Kievit, Athanasia M Mowinckel, Kristine B\n  Walhovd, Rene Westerhausen, Anders Fjell", "title": "Meta-Analysis of Generalized Additive Models in Neuroimaging Studies", "comments": null, "journal-ref": null, "doi": "10.1016/j.neuroimage.2020.117416", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing data from multiple neuroimaging studies has great potential in\nterms of increasing statistical power, enabling detection of effects of smaller\nmagnitude than would be possible when analyzing each study separately and also\nallowing to systematically investigate between-study differences. Restrictions\ndue to privacy or proprietary data as well as more practical concerns can make\nit hard to share neuroimaging datasets, such that analyzing all data in a\ncommon location might be impractical or impossible. Meta-analytic methods\nprovide a way to overcome this issue, by combining aggregated quantities like\nmodel parameters or risk ratios. Most meta-analytic tools focus on parametric\nstatistical models, and methods for meta-analyzing semi-parametric models like\ngeneralized additive models have not been well developed. Parametric models are\noften not appropriate in neuroimaging, where for instance age-brain\nrelationships may take forms that are difficult to accurately describe using\nsuch models. In this paper we introduce meta-GAM, a method for meta-analysis of\ngeneralized additive models which does not require individual participant data,\nand hence is suitable for increasing statistical power while upholding privacy\nand other regulatory concerns. We extend previous works by enabling the\nanalysis of multiple model terms as well as multivariate smooth functions. In\naddition, we show how meta-analytic $p$-values can be computed for smooth\nterms. The proposed methods are shown to perform well in simulation\nexperiments, and are demonstrated in a real data analysis on hippocampal volume\nand self-reported sleep quality data from the Lifebrain consortium. We argue\nthat application of meta-GAM is especially beneficial in lifespan neuroscience\nand imaging genetics. The methods are implemented in an accompanying R package\n\\verb!metagam!, which is also demonstrated.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 05:25:27 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2020 10:31:05 GMT"}, {"version": "v3", "created": "Wed, 23 Sep 2020 11:03:38 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["S\u00f8rensen", "\u00d8ystein", ""], ["Brandmaier", "Andreas M", ""], ["Maci\u00e0", "D\u00eddac", ""], ["Ebmeier", "Klaus", ""], ["Ghisletta", "Paolo", ""], ["Kievit", "Rogier A", ""], ["Mowinckel", "Athanasia M", ""], ["Walhovd", "Kristine B", ""], ["Westerhausen", "Rene", ""], ["Fjell", "Anders", ""]]}, {"id": "2002.02763", "submitter": "Andr\\'e Paul Neto-Bradley", "authors": "Andr\\'e Paul Neto-Bradley (1), Ruchi Choudhary (1 and 2), Amir Bazaz\n  (3) ((1) University of Cambridge, (2) Alan Turing Institute, (3) Indian\n  Institute for Human Settlements)", "title": "Slipping through the net: can data science approaches help target clean\n  cooking policy interventions?", "comments": "42 pages, 7 figures", "journal-ref": "Energy Policy 144C (2020) 111650", "doi": "10.1016/j.enpol.2020.111650", "report-no": null, "categories": "physics.soc-ph cs.CY stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Reliance on solid biomass cooking fuels in India has negative health and\nsocio-economic consequences for households, yet policies aimed at promoting\nuptake of LPG for cooking have not always been effective at promoting sustained\ntransition to cleaner cooking amongst intended beneficiaries. This paper uses a\ntwo step approach combining predictive and descriptive analyses of the IHDS\npanel dataset to identify different groups of households that switched stove\nbetween 2004/5 and 2011/12. A tree-based ensemble machine learning predictive\nanalysis identifies key determinants of a switch from biomass to non-biomass\nstoves. A descriptive clustering analysis is used to identify groups of\nstove-switching households that follow different transition pathways. There are\nthree key findings of this study: Firstly non-income determinants of stove\nswitching do not have a linear effect on stove switching, in particular\nvariables on time of use and appliance ownership which offer a proxy for\nhousehold energy practices; secondly location specific factors including\nregion, infrastructure availability, and dwelling quality are found to be key\ndeterminants and as a result policies must be tailored to take into account\nlocal variations; thirdly clean cooking interventions must enact a range of\nmeasures to address the barriers faced by households on different energy\ntransition pathways.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 18:09:30 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 11:51:52 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Neto-Bradley", "Andr\u00e9 Paul", "", "1 and 2"], ["Choudhary", "Ruchi", "", "1 and 2"], ["Bazaz", "Amir", ""]]}, {"id": "2002.03007", "submitter": "Jin Jin", "authors": "Jin Jin, Marie-Karelle Riviere, Xiaodong Luo, Yingwen Dong", "title": "Bayesian Methods for the Analysis of Early-Phase Oncology Basket Trials\n  with Information Borrowing across Cancer Types", "comments": "22 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in oncology has changed the focus from histological properties of\ntumors in a specific organ to a specific genomic aberration potentially shared\nby multiple cancer types. This motivates the basket trial, which assesses the\nefficacy of treatment simultaneously on multiple cancer types that have a\ncommon aberration. Although the assumption of homogeneous treatment effects\nseems reasonable given the shared aberration, in reality, the treatment effect\nmay vary by cancer type, and potentially only a subgroup of the cancer types\nrespond to the treatment. Various approaches have been proposed to increase the\ntrial power by borrowing information across cancer types, which, however, tend\nto inflate the type I error rate. In this paper, we review some representative\nBayesian information borrowing methods for the analysis of early-phase basket\ntrials. We then propose a novel method called the Bayesian hierarchical model\nwith a correlated prior (CBHM), which conducts more flexible borrowing across\ncancer types according to sample similarity. We did simulation studies to\ncompare CBHM with independent analysis and three information borrowing\napproaches: the conventional Bayesian hierarchical model, the EXNEX approach\nand Liu's two-stage approach. Simulation results show that all information\nborrowing approaches substantially improve the power of independent analysis if\na large proportion of the cancer types truly respond to the treatment. Our\nproposed CBHM approach shows an advantage over the existing information\nborrowing approaches, with a power similar to that of EXNEX or Liu's approach,\nbut the potential to provide substantially better control of type I error rate.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 20:28:28 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Jin", "Jin", ""], ["Riviere", "Marie-Karelle", ""], ["Luo", "Xiaodong", ""], ["Dong", "Yingwen", ""]]}, {"id": "2002.03224", "submitter": "Michael Porter", "authors": "Michael D. Porter and Alphonse Akakpo", "title": "Detecting, identifying, and localizing radiological material in urban\n  environments using scan statistics", "comments": "(6 pages, 6 figures) 2019 IEEE International Symposium on\n  Technologies for Homeland Security (HST)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method is proposed, based on scan statistics, to detect, identify, and\nlocalize illicit radiological material using mobile sensors in an urban\nenvironment. Our method handles varying levels of background radiation that\nchange according to an (unknown) environment. Our method can accurately\ndetermine if a source is present along a street segment as well as identify\nwhich of six possible sources generated the radiation. Our method can also\nlocalize the source, when detected, to within a few seconds. We have presented\nour results across a range of decision thresholds allowing stakeholders to\nevaluate the performance at different false alarm rates. Due to the simplicity\nof our approach, our models can be trained in a few minutes with very little\ntraining data and holds the potential to score a run in real-time. Our method\nwas one of the top performing submissions in the 'Detecting Radiological\nThreats in Urban Areas' competition.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 19:44:28 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Porter", "Michael D.", ""], ["Akakpo", "Alphonse", ""]]}, {"id": "2002.03251", "submitter": "Yishu Xue", "authors": "Guanyu Hu, Yishu Xue, Fred Huffer", "title": "A comparison of Bayesian accelerated failure time models with spatially\n  varying coefficients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The accelerated failure time (AFT) model is a commonly used tool in analyzing\nsurvival data. In public health studies, data is often collected from medical\nservice providers in different locations. Survival rates from different\nlocations often present geographically varying patterns. In this paper, we\nfocus on the accelerated failure time model with spatially varying\ncoefficients. We compare three types of the priors for spatially varying\ncoefficients. A model selection criterion, logarithm of the pseudo-marginal\nlikelihood (LPML), is developed to assess the fit of AFT model with different\npriors. Extensive simulation studies are carried out to examine the empirical\nperformance of the proposed methods. Finally, we apply our model to SEER data\non prostate cancer in Louisiana and demonstrate the existence of spatially\nvarying effects on survival rates from prostate cancer data.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 23:40:09 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Hu", "Guanyu", ""], ["Xue", "Yishu", ""], ["Huffer", "Fred", ""]]}, {"id": "2002.03277", "submitter": "Miao Yu", "authors": "Miao Yu, Wenbin Lu, Rui Song", "title": "A New Framework for Online Testing of Heterogeneous Treatment Effect", "comments": "8 pages, no figures. To be published on AAAI 2020 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework for online testing of heterogeneous treatment\neffects. The proposed test, named sequential score test (SST), is able to\ncontrol type I error under continuous monitoring and detect multi-dimensional\nheterogeneous treatment effects. We provide an online p-value calculation for\nSST, making it convenient for continuous monitoring, and extend our tests to\nonline multiple testing settings by controlling the false discovery rate. We\nexamine the empirical performance of the proposed tests and compare them with a\nstate-of-art online test, named mSPRT using simulations and a real data. The\nresults show that our proposed test controls type I error at any time, has\nhigher detection power and allows quick inference on online A/B testing.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 04:02:11 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Yu", "Miao", ""], ["Lu", "Wenbin", ""], ["Song", "Rui", ""]]}, {"id": "2002.03419", "submitter": "Razvan Marinescu", "authors": "Razvan V. Marinescu, Neil P. Oxtoby, Alexandra L. Young, Esther E.\n  Bron, Arthur W. Toga, Michael W. Weiner, Frederik Barkhof, Nick C. Fox, Arman\n  Eshaghi, Tina Toni, Marcin Salaterski, Veronika Lunina, Manon Ansart, Stanley\n  Durrleman, Pascal Lu, Samuel Iddi, Dan Li, Wesley K. Thompson, Michael C.\n  Donohue, Aviv Nahon, Yarden Levy, Dan Halbersberg, Mariya Cohen, Huiling\n  Liao, Tengfei Li, Kaixian Yu, Hongtu Zhu, Jose G. Tamez-Pena, Aya Ismail,\n  Timothy Wood, Hector Corrada Bravo, Minh Nguyen, Nanbo Sun, Jiashi Feng, B.T.\n  Thomas Yeo, Gang Chen, Ke Qi, Shiyang Chen, Deqiang Qiu, Ionut Buciuman, Alex\n  Kelner, Raluca Pop, Denisa Rimocea, Mostafa M. Ghazi, Mads Nielsen, Sebastien\n  Ourselin, Lauge Sorensen, Vikram Venkatraghavan, Keli Liu, Christina Rabe,\n  Paul Manser, Steven M. Hill, James Howlett, Zhiyue Huang, Steven Kiddle, Sach\n  Mukherjee, Anais Rouanet, Bernd Taschler, Brian D. M. Tom, Simon R. White,\n  Noel Faux, Suman Sedai, Javier de Velasco Oriol, Edgar E. V. Clemente, Karol\n  Estrada, Leon Aksman, Andre Altmann, Cynthia M. Stonnington, Yalin Wang,\n  Jianfeng Wu, Vivek Devadas, Clementine Fourrier, Lars Lau Raket, Aristeidis\n  Sotiras, Guray Erus, Jimit Doshi, Christos Davatzikos, Jacob Vogel, Andrew\n  Doyle, Angela Tam, Alex Diaz-Papkovich, Emmanuel Jammeh, Igor Koval, Paul\n  Moore, Terry J. Lyons, John Gallacher, Jussi Tohka, Robert Ciszek, Bruno\n  Jedynak, Kruti Pandya, Murat Bilgel, William Engels, Joseph Cole, Polina\n  Golland, Stefan Klein, Daniel C. Alexander", "title": "The Alzheimer's Disease Prediction Of Longitudinal Evolution (TADPOLE)\n  Challenge: Results after 1 Year Follow-up", "comments": "Presents final results of the TADPOLE competition. 35 pages, 5\n  tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present the findings of \"The Alzheimer's Disease Prediction Of\nLongitudinal Evolution\" (TADPOLE) Challenge, which compared the performance of\n92 algorithms from 33 international teams at predicting the future trajectory\nof 219 individuals at risk of Alzheimer's disease. Challenge participants were\nrequired to make a prediction, for each month of a 5-year future time period,\nof three key outcomes: clinical diagnosis, Alzheimer's Disease Assessment Scale\nCognitive Subdomain (ADAS-Cog13), and total volume of the ventricles. No single\nsubmission was best at predicting all three outcomes. For clinical diagnosis\nand ventricle volume prediction, the best algorithms strongly outperform simple\nbaselines in predictive ability. However, for ADAS-Cog13 no single submitted\nprediction method was significantly better than random guessing. Two ensemble\nmethods based on taking the mean and median over all predictions, obtained top\nscores on almost all tasks. Better than average performance at diagnosis\nprediction was generally associated with the additional inclusion of features\nfrom cerebrospinal fluid (CSF) samples and diffusion tensor imaging (DTI). On\nthe other hand, better performance at ventricle volume prediction was\nassociated with inclusion of summary statistics, such as patient-specific\nbiomarker trends. The submission system remains open via the website\nhttps://tadpole.grand-challenge.org, while code for submissions is being\ncollated by TADPOLE SHARE: https://tadpole-share.github.io/. Our work suggests\nthat current prediction algorithms are accurate for biomarkers related to\nclinical diagnosis and ventricle volume, opening up the possibility of cohort\nrefinement in clinical trials for Alzheimer's disease.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 18:32:02 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Marinescu", "Razvan V.", ""], ["Oxtoby", "Neil P.", ""], ["Young", "Alexandra L.", ""], ["Bron", "Esther E.", ""], ["Toga", "Arthur W.", ""], ["Weiner", "Michael W.", ""], ["Barkhof", "Frederik", ""], ["Fox", "Nick C.", ""], ["Eshaghi", "Arman", ""], ["Toni", "Tina", ""], ["Salaterski", "Marcin", ""], ["Lunina", "Veronika", ""], ["Ansart", "Manon", ""], ["Durrleman", "Stanley", ""], ["Lu", "Pascal", ""], ["Iddi", "Samuel", ""], ["Li", "Dan", ""], ["Thompson", "Wesley K.", ""], ["Donohue", "Michael C.", ""], ["Nahon", "Aviv", ""], ["Levy", "Yarden", ""], ["Halbersberg", "Dan", ""], ["Cohen", "Mariya", ""], ["Liao", "Huiling", ""], ["Li", "Tengfei", ""], ["Yu", "Kaixian", ""], ["Zhu", "Hongtu", ""], ["Tamez-Pena", "Jose G.", ""], ["Ismail", "Aya", ""], ["Wood", "Timothy", ""], ["Bravo", "Hector Corrada", ""], ["Nguyen", "Minh", ""], ["Sun", "Nanbo", ""], ["Feng", "Jiashi", ""], ["Yeo", "B. T. Thomas", ""], ["Chen", "Gang", ""], ["Qi", "Ke", ""], ["Chen", "Shiyang", ""], ["Qiu", "Deqiang", ""], ["Buciuman", "Ionut", ""], ["Kelner", "Alex", ""], ["Pop", "Raluca", ""], ["Rimocea", "Denisa", ""], ["Ghazi", "Mostafa M.", ""], ["Nielsen", "Mads", ""], ["Ourselin", "Sebastien", ""], ["Sorensen", "Lauge", ""], ["Venkatraghavan", "Vikram", ""], ["Liu", "Keli", ""], ["Rabe", "Christina", ""], ["Manser", "Paul", ""], ["Hill", "Steven M.", ""], ["Howlett", "James", ""], ["Huang", "Zhiyue", ""], ["Kiddle", "Steven", ""], ["Mukherjee", "Sach", ""], ["Rouanet", "Anais", ""], ["Taschler", "Bernd", ""], ["Tom", "Brian D. M.", ""], ["White", "Simon R.", ""], ["Faux", "Noel", ""], ["Sedai", "Suman", ""], ["Oriol", "Javier de Velasco", ""], ["Clemente", "Edgar E. V.", ""], ["Estrada", "Karol", ""], ["Aksman", "Leon", ""], ["Altmann", "Andre", ""], ["Stonnington", "Cynthia M.", ""], ["Wang", "Yalin", ""], ["Wu", "Jianfeng", ""], ["Devadas", "Vivek", ""], ["Fourrier", "Clementine", ""], ["Raket", "Lars Lau", ""], ["Sotiras", "Aristeidis", ""], ["Erus", "Guray", ""], ["Doshi", "Jimit", ""], ["Davatzikos", "Christos", ""], ["Vogel", "Jacob", ""], ["Doyle", "Andrew", ""], ["Tam", "Angela", ""], ["Diaz-Papkovich", "Alex", ""], ["Jammeh", "Emmanuel", ""], ["Koval", "Igor", ""], ["Moore", "Paul", ""], ["Lyons", "Terry J.", ""], ["Gallacher", "John", ""], ["Tohka", "Jussi", ""], ["Ciszek", "Robert", ""], ["Jedynak", "Bruno", ""], ["Pandya", "Kruti", ""], ["Bilgel", "Murat", ""], ["Engels", "William", ""], ["Cole", "Joseph", ""], ["Golland", "Polina", ""], ["Klein", "Stefan", ""], ["Alexander", "Daniel C.", ""]]}, {"id": "2002.03537", "submitter": "Samuel W.K. Wong", "authors": "Samuel W.K. Wong", "title": "Calibrating wood products for load duration and rate: A statistical look\n  at three damage models", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lumber and wood-based products are versatile construction materials that are\nsusceptible to weakening as a result of applied stresses. To assess the effects\nof load duration and rate, experiments have been carried out by applying preset\nload profiles to sample specimens. This paper studies these effects via a\ndamage modeling approach, by considering three models in the literature: the\nGerhards and Foschi accumulated damage models, and a degradation model based on\nthe gamma process. We present a statistical framework for fitting these models\nto failure time data generated by a combination of ramp and constant load\nsettings, and show how estimation uncertainty can be quantified. The models and\nmethods are illustrated and compared via a novel analysis of a Hemlock lumber\ndataset. Practical usage of the fitted damage models is demonstrated with an\napplication to long-term reliability prediction under stochastic future\nloadings.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 04:29:52 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Wong", "Samuel W. K.", ""]]}, {"id": "2002.03792", "submitter": "Onel Luis Alcaraz L\\'opez", "authors": "Onel L. A. L\\'opez, Samuel Montejo-S\\'anchez, Richard D. Souza, Hirley\n  Alves, Constantinos B. Papadias", "title": "On CSI-free Multi-Antenna Schemes for Massive RF Wireless Energy\n  Transfer", "comments": "16 pags, 10 figs, 1 table, submitted to IEEE JSAC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.NI cs.PF stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wireless Energy Transfer (WET) is emerging as a potential green enabler for\nmassive Internet of Things (IoT). Herein, we analyze Channel State Information\n(CSI)-free multi-antenna strategies for powering wirelessly a large set of\nsingle-antenna IoT devices. The CSI-free schemes are AA-SS (AA-IS), where all\nantennas transmit the same (independent) signal(s), and SA, where just one\nantenna transmits at a time such that all antennas are utilized during the\ncoherence block. We characterize the distribution of the provided energy under\ncorrelated Rician fading for each scheme and find out that while AA-IS and SA\ncannot take advantage of the multiple antennas to improve the average provided\nenergy, its dispersion can be significantly reduced. Meanwhile, AA-SS provides\nthe greatest average energy, but also the greatest energy dispersion, and the\ngains depend critically on the mean phase shifts between the antenna elements.\nWe find that consecutive antennas must be $\\pi$ phase-shifted for optimum\naverage energy performance under AA-SS. Our numerical results evidenced that\ncorrelation is beneficial under AA-SS, while a greater line of sight (LOS)\nand/or number of antennas is not always beneficial under such scheme.\nMeanwhile, both AA-IS and SA schemes benefit from small correlation, large LOS\nand/or large number of antennas.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 07:38:53 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 15:05:43 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["L\u00f3pez", "Onel L. A.", ""], ["Montejo-S\u00e1nchez", "Samuel", ""], ["Souza", "Richard D.", ""], ["Alves", "Hirley", ""], ["Papadias", "Constantinos B.", ""]]}, {"id": "2002.03828", "submitter": "Jun Li", "authors": "Jun Li", "title": "A Robust Stochastic Method of Estimating the Transmission Potential of\n  2019-nCoV", "comments": "Short paper, 4 page text, total 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE physics.soc-ph stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent outbreak of a novel coronavirus (2019-nCoV) has quickly evolved\ninto a global health crisis. The transmission potential of 2019-nCoV has been\nmodelled and studied in several recent research works. The key factors such as\nthe basic reproductive number, $R_{0}$, of the virus have been identified by\nfitting contagious disease spreading models to aggregated data. The data\ninclude the reported cases both within China and in closely connected cities\nover the world. In this paper, we study the transmission potential of 2019-nCoV\nfrom the perspective of the robustness of the statistical estimation, in light\nof varying data quality and timeliness in the initial stage of the outbreak.\nSample consensus algorithm has been adopted to improve model fitting when\noutliers are present. The robust estimation enables us to identify two clusters\nof transmission models, both are of substantial concern, one with\n$R_0:8\\sim14$, comparable to that of measles and the other dictates a large\ninitial infected group.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 16:01:28 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Li", "Jun", ""]]}, {"id": "2002.03853", "submitter": "Samuel Thiriot", "authors": "Samuel Thiriot and Marie Sevenet", "title": "Pairing for Generation of Synthetic Populations: the Direct\n  Probabilistic Pairing method", "comments": "Author draft prior to submission elsewhere. 49 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for the Generation of Synthetic Populations do generate the entities\nrequired for micro models or multi-agent models, such as they match field\nobservations or hypothesis on the population under study. We tackle here the\nspecific question of creating synthetic populations made of two types of\nentities linked together by 0, 1 or more links. Potential applications include\nthe creation of dwellings inhabited by households, households owning cars,\ndwellings equipped with appliances, worker employed by firms, etc. We propose a\ntheoretical framework to tackle this problem. We then highlight how this\nproblem is over-constrained and requires relaxation of some constraints to be\nsolved. We propose a method to solve the problem analytically which lets the\nuser select which input data should be preserved and adapts the others in order\nto make the data consistent. We illustrate this method by synthesizing a\npopulation made of dwellings containing 0, 1 or 2 households in the city of\nLille (France). In this population, the distributions of the dwellings' and\nhouseholds' characteristics are preserved, and both are linked according to\nstatistical pairing statistics.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 10:35:22 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Thiriot", "Samuel", ""], ["Sevenet", "Marie", ""]]}, {"id": "2002.04094", "submitter": "Subhro Das", "authors": "Subhro Das, Prasanth Lade, Soundar Srinivasan", "title": "Model adaptation and unsupervised learning with non-stationary batch\n  data under smooth concept drift", "comments": "11 pages, 4 figures, 3 tables, 2016 NIPS Time Series Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most predictive models assume that training and test data are generated from\na stationary process. However, this assumption does not hold true in practice.\nIn this paper, we consider the scenario of a gradual concept drift due to the\nunderlying non-stationarity of the data source. While previous work has\ninvestigated this scenario under a supervised-learning and adaption conditions,\nfew have addressed the common, real-world scenario when labels are only\navailable during training. We propose a novel, iterative algorithm for\nunsupervised adaptation of predictive models. We show that the performance of\nour batch adapted prediction algorithm is better than that of its corresponding\nunadapted version. The proposed algorithm provides similar (or better, in most\ncases) performance within significantly less run time compared to other state\nof the art methods. We validate our claims though extensive numerical\nevaluations on both synthetic and real data.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 21:29:09 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Das", "Subhro", ""], ["Lade", "Prasanth", ""], ["Srinivasan", "Soundar", ""]]}, {"id": "2002.04101", "submitter": "Shanglin Lu", "authors": "Lajos Horv\\'ath (1), Zhenya Liu (2 and 3) and Shanglin Lu (2) ((1)\n  Department of Mathematics, University of Utah, (2) School of Finance, Renmin\n  University of China, (3) CERGAM, Aix--Marseille University)", "title": "Sequential Monitoring of Changes in Housing Prices", "comments": "47 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a sequential monitoring scheme to find structural breaks in real\nestate markets. The changes in the real estate prices are modeled by a\ncombination of linear and autoregressive terms. The monitoring scheme is based\non a detector and a suitably chosen boundary function. If the detector crosses\nthe boundary function, a structural break is detected. We provide the\nasymptotics for the procedure under the stability null hypothesis and the\nstopping time under the change point alternative. Monte Carlo simulation is\nused to show the size and the power of our method under several conditions. We\nstudy the real estate markets in Boston, Los Angeles and at the national U.S.\nlevel. We find structural breaks in the markets, and we segment the data into\nstationary segments. It is observed that the autoregressive parameter is\nincreasing but stays below 1.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 21:49:19 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Horv\u00e1th", "Lajos", "", "2 and 3"], ["Liu", "Zhenya", "", "2 and 3"], ["Lu", "Shanglin", ""]]}, {"id": "2002.04127", "submitter": "Maria Ines Silva", "authors": "Maria In\\^es Silva and Roberto Henriques", "title": "Finding manoeuvre motifs in vehicle telematics", "comments": "11 pages, 3 figures, submitted to Accident Analysis & Prevention", "journal-ref": null, "doi": "10.1016/j.aap.2020.105467", "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driving behaviour has a great impact on road safety. A popular way of\nanalysing driving behaviour is to move the focus to the manoeuvres as they give\nuseful information about the driver who is performing them. In this paper, we\ninvestigate a new way of identifying manoeuvres from vehicle telematics data,\nthrough motif detection in time-series. We implement a modified version of the\nExtended Motif Discovery (EMD) algorithm, a classical variable-length motif\ndetection algorithm for time-series and we applied it to the UAH-DriveSet, a\npublicly available naturalistic driving dataset. After a systematic exploration\nof the extracted motifs, we were able to conclude that the EMD algorithm was\nnot only capable of extracting simple manoeuvres such as accelerations, brakes\nand curves, but also more complex manoeuvres, such as lane changes and\novertaking manoeuvres, which validates motif discovery as a worthwhile line for\nfuture research.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 23:07:53 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Silva", "Maria In\u00eas", ""], ["Henriques", "Roberto", ""]]}, {"id": "2002.04148", "submitter": "Edgar Santos-Fernandez", "authors": "Edgar Santos-Fernandez, Francesco Denti, Kerrie Mengersen, Antonietta\n  Mira", "title": "The role of intrinsic dimension in high-resolution player tracking data\n  -- Insights in basketball", "comments": "21 pages, 16 figures, Codes + data + results can be found in\n  https://github.com/EdgarSantos-Fernandez/id_basketball, Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new range of statistical analysis has emerged in sports after the\nintroduction of the high-resolution player tracking technology, specifically in\nbasketball. However, this high dimensional data is often challenging for\nstatistical inference and decision making. In this article, we employ Hidalgo,\na state-of-the-art Bayesian mixture model that allows the estimation of\nheterogeneous intrinsic dimensions (ID) within a dataset and propose some\ntheoretical enhancements. ID results can be interpreted as indicators of\nvariability and complexity of basketball plays and games. This technique allows\nclassification and clustering of NBA basketball player's movement and shot\ncharts data. Analyzing movement data, Hidalgo identifies key stages of\noffensive actions such as creating space for passing, preparation/shooting and\nfollowing through. We found that the ID value spikes reaching a peak between 4\nand 8 seconds in the offensive part of the court after which it declines. In\nshot charts, we obtained groups of shots that produce substantially higher and\nlower successes. Overall, game-winners tend to have a larger intrinsic\ndimension which is an indication of more unpredictability and unique shot\nplacements. Similarly, we found higher ID values in plays when the score margin\nis small compared to large margin ones. These outcomes could be exploited by\ncoaches to obtain better offensive/defensive results.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 00:34:32 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Santos-Fernandez", "Edgar", ""], ["Denti", "Francesco", ""], ["Mengersen", "Kerrie", ""], ["Mira", "Antonietta", ""]]}, {"id": "2002.04151", "submitter": "Vojtech Kejzlar", "authors": "Vojtech Kejzlar, L\\'eo Neufcourt, Witold Nazarewicz, Paul-Gerhard\n  Reinhard", "title": "Statistical aspects of nuclear mass models", "comments": "Accepted for publication in J. Phys. G Focus Issue on \"Focus on\n  further enhancing the interaction between nuclear experiment and theory\n  through information and statistics (ISNET 2.0),\"", "journal-ref": null, "doi": "10.1088/1361-6471/ab907c", "report-no": null, "categories": "nucl-th stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the information content of nuclear masses from the perspective of\nglobal models of nuclear binding energies. To this end, we employ a number of\nstatistical methods and diagnostic tools, including Bayesian calibration,\nBayesian model averaging, chi-square correlation analysis, principal component\nanalysis, and empirical coverage probability. Using a Bayesian framework, we\ninvestigate the structure of the 4-parameter Liquid Drop Model by considering\ndiscrepant mass domains for calibration. We then use the chi-square correlation\nframework to analyze the 14-parameter Skyrme energy density functional\ncalibrated using homogeneous and heterogeneous datasets. We show that a quite\ndramatic parameter reduction can be achieved in both cases. The advantage of\nBayesian model averaging for improving uncertainty quantification is\ndemonstrated. The statistical approaches used are pedagogically described; in\nthis context this work can serve as a guide for future applications.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 00:47:22 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 14:45:36 GMT"}, {"version": "v3", "created": "Thu, 7 May 2020 00:39:02 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Kejzlar", "Vojtech", ""], ["Neufcourt", "L\u00e9o", ""], ["Nazarewicz", "Witold", ""], ["Reinhard", "Paul-Gerhard", ""]]}, {"id": "2002.04176", "submitter": "Callum Stewart", "authors": "Callum L. Stewart, Amos Folarin, Richard Dobson", "title": "Personalized acute stress classification from physiological signals with\n  neural processes", "comments": "16 pages (inc. references), 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: A person's affective state has known relationships to\nphysiological processes which can be measured by wearable sensors. However,\nwhile there are general trends those relationships can be person-specific. This\nwork proposes using neural processes as a way to address individual\ndifferences.\n  Methods: Stress classifiers built from classic machine learning models and\nfrom neural processes are compared on two datasets using\nleave-one-participant-out cross-validation. The neural processes models are\ncontextualized on data from a brief period of a particular person's recording.\n  Results: The neural processes models outperformed the standard machine\nlearning models, and had the best performance when using periods of stress and\nbaseline as context. Contextual points chosen from other participants led to\nlower performance.\n  Conclusion: Neural processes can learn to adapt to person-specific\nphysiological sensor data. There are a wide range of affective and medical\napplications for which this model could prove useful.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 02:38:39 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Stewart", "Callum L.", ""], ["Folarin", "Amos", ""], ["Dobson", "Richard", ""]]}, {"id": "2002.04312", "submitter": "Saulo Martiello Mastelini", "authors": "Everton Jose Santana and Felipe Rodrigues dos Santos and Saulo\n  Martiello Mastelini and Fabio Luiz Melquiades and Sylvio Barbon Jr", "title": "Improved prediction of soil properties with Multi-target Stacked\n  Generalisation on EDXRF spectra", "comments": "20 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning (ML) algorithms have been used for assessing soil quality\nparameters along with non-destructive methodologies. Among spectroscopic\nanalytical methodologies, energy dispersive X-ray fluorescence (EDXRF) is one\nof the more quick, environmentally friendly and less expensive when compared to\nconventional methods. However, some challenges in EDXRF spectral data analysis\nstill demand more efficient methods capable of providing accurate outcomes.\nUsing Multi-target Regression (MTR) methods, multiple parameters can be\npredicted, and also taking advantage of inter-correlated parameters the overall\npredictive performance can be improved. In this study, we proposed the\nMulti-target Stacked Generalisation (MTSG), a novel MTR method relying on\nlearning from different regressors arranged in stacking structure for a boosted\noutcome. We compared MTSG and 5 MTR methods for predicting 10 parameters of\nsoil fertility. Random Forest and Support Vector Machine (with linear and\nradial kernels) were used as learning algorithms embedded into each MTR method.\nResults showed the superiority of MTR methods over the Single-target Regression\n(the traditional ML method), reducing the predictive error for 5 parameters.\nParticularly, MTSG obtained the lowest error for phosphorus, total organic\ncarbon and cation exchange capacity. When observing the relative performance of\nSupport Vector Machine with a radial kernel, the prediction of base saturation\npercentage was improved in 19%. Finally, the proposed method was able to reduce\nthe average error from 0.67 (single-target) to 0.64 analysing all targets,\nrepresenting a global improvement of 4.48%.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 11:05:03 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Santana", "Everton Jose", ""], ["Santos", "Felipe Rodrigues dos", ""], ["Mastelini", "Saulo Martiello", ""], ["Melquiades", "Fabio Luiz", ""], ["Barbon", "Sylvio", "Jr"]]}, {"id": "2002.04328", "submitter": "Giuseppe Brandi", "authors": "Giuseppe Brandi and T. Di Matteo", "title": "Predicting Multidimensional Data via Tensor Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of multidimensional data is becoming a more and more relevant\ntopic in statistical and machine learning research. Given their complexity,\nsuch data objects are usually reshaped into matrices or vectors and then\nanalysed. However, this methodology presents several drawbacks. First of all,\nit destroys the intrinsic interconnections among datapoints in the\nmultidimensional space and, secondly, the number of parameters to be estimated\nin a model increases exponentially. We develop a model that overcomes such\ndrawbacks. In particular, in this paper, we propose a parsimonious tensor\nregression model that retains the intrinsic multidimensional structure of the\ndataset. Tucker structure is employed to achieve parsimony and a shrinkage\npenalization is introduced to deal with over-fitting and collinearity. To\nestimate the model parameters, an Alternating Least Squares algorithm is\ndeveloped. In order to validate the model performance and robustness, a\nsimulation exercise is produced. Moreover, we perform an empirical analysis\nthat highlight the forecasting power of the model with respect to benchmark\nmodels. This is achieved by implementing an autoregressive specification on the\nFoursquares spatio-temporal dataset together with a macroeconomic panel\ndataset. Overall, the proposed model is able to outperform benchmark models\npresent in the forecasting literature.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 11:57:07 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 15:37:28 GMT"}, {"version": "v3", "created": "Wed, 7 Apr 2021 18:13:53 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Brandi", "Giuseppe", ""], ["Di Matteo", "T.", ""]]}, {"id": "2002.04362", "submitter": "Emma Simpson", "authors": "Emma S. Simpson and Jennifer L. Wadsworth", "title": "Conditional Modelling of Spatio-Temporal Extremes for Red Sea Surface\n  Temperatures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent extreme value theory literature has seen significant emphasis on the\nmodelling of spatial extremes, with comparatively little consideration of\nspatio-temporal extensions. This neglects an important feature of extreme\nevents: their evolution over time. Many existing models for the spatial case\nare limited by the number of locations they can handle; this impedes extension\nto space-time settings, where models for higher dimensions are required.\nMoreover, the spatio-temporal models that do exist are restrictive in terms of\nthe range of extremal dependence types they can capture. Recently, conditional\napproaches for studying multivariate and spatial extremes have been proposed,\nwhich enjoy benefits in terms of computational efficiency and an ability to\ncapture both asymptotic dependence and asymptotic independence. We extend this\nclass of models to a spatio-temporal setting, conditioning on the occurrence of\nan extreme value at a single space-time location. We adopt a composite\nlikelihood approach for inference, which combines information from full\nlikelihoods across multiple space-time conditioning locations. We apply our\nmodel to Red Sea surface temperatures, show that it fits well using a range of\ndiagnostic plots, and demonstrate how it can be used to assess the risk of\ncoral bleaching attributed to high water temperatures over consecutive days.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 13:10:28 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 10:15:30 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Simpson", "Emma S.", ""], ["Wadsworth", "Jennifer L.", ""]]}, {"id": "2002.04401", "submitter": "Yuren Zhou", "authors": "Yuren Zhou, Billy Pik Lik Lau, Zann Koh, Chau Yuen, Benny Kai Kiat Ng", "title": "Understanding Crowd Behaviors in a Social Event by Passive WiFi Sensing\n  and Data Mining", "comments": "This manuscript has been accepted by IEEE Internet of Things journal.\n  Copyright (c) 2020 IEEE. Personal use of this material is permitted. However,\n  permission to use this material for any other purposes must be obtained from\n  the IEEE by sending a request to pubs-permissions@ieee.org", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding crowd behaviors in a large social event is crucial for event\nmanagement. Passive WiFi sensing, by collecting WiFi probe requests sent from\nmobile devices, provides a better way to monitor crowds compared with people\ncounters and cameras in terms of free interference, larger coverage, lower\ncost, and more information on people's movement. In existing studies, however,\nnot enough attention has been paid to the thorough analysis and mining of\ncollected data. Especially, the power of machine learning has not been fully\nexploited. In this paper, therefore, we propose a comprehensive data analysis\nframework to fully analyze the collected probe requests to extract three types\nof patterns related to crowd behaviors in a large social event, with the help\nof statistics, visualization, and unsupervised machine learning. First,\ntrajectories of the mobile devices are extracted from probe requests and\nanalyzed to reveal the spatial patterns of the crowds' movement. Hierarchical\nagglomerative clustering is adopted to find the interconnections between\ndifferent locations. Next, k-means and k-shape clustering algorithms are\napplied to extract temporal visiting patterns of the crowds by days and\nlocations, respectively. Finally, by combining with time, trajectories are\ntransformed into spatiotemporal patterns, which reveal how trajectory duration\nchanges over the length and how the overall trends of crowd movement change\nover time. The proposed data analysis framework is fully demonstrated using\nreal-world data collected in a large social event. Results show that one can\nextract comprehensive patterns from data collected by a network of passive WiFi\nsensors.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 03:36:00 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Zhou", "Yuren", ""], ["Lau", "Billy Pik Lik", ""], ["Koh", "Zann", ""], ["Yuen", "Chau", ""], ["Ng", "Benny Kai Kiat", ""]]}, {"id": "2002.04592", "submitter": "Min Zhou", "authors": "Yang Feng, Min Zhou, Xin Tong", "title": "Imbalanced classification: a paradigm-based review", "comments": "34 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common issue for classification in scientific research and industry is the\nexistence of imbalanced classes. When sample sizes of different classes are\nimbalanced in training data, naively implementing a classification method often\nleads to unsatisfactory prediction results on test data. Multiple resampling\ntechniques have been proposed to address the class imbalance issues. Yet, there\nis no general guidance on when to use each technique. In this article, we\nprovide a paradigm-based review of the common resampling techniques for binary\nclassification under imbalanced class sizes. The paradigms we consider include\nthe classical paradigm that minimizes the overall classification error, the\ncost-sensitive learning paradigm that minimizes a cost-adjusted weighted type I\nand type II errors, and the Neyman-Pearson paradigm that minimizes the type II\nerror subject to a type I error constraint. Under each paradigm, we investigate\nthe combination of the resampling techniques and a few state-of-the-art\nclassification methods. For each pair of resampling techniques and\nclassification methods, we use simulation studies and a real data set on credit\ncard fraud to study the performance under different evaluation metrics. From\nthese extensive numerical experiments, we demonstrate under each classification\nparadigm, the complex dynamics among resampling techniques, base classification\nmethods, evaluation metrics, and imbalance ratios. We also summarize a few\ntakeaway messages regarding the choices of resampling techniques and base\nclassification methods, which could be helpful for practitioners.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 18:34:48 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 02:08:35 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Feng", "Yang", ""], ["Zhou", "Min", ""], ["Tong", "Xin", ""]]}, {"id": "2002.04739", "submitter": "Juan Aurelio Tamayo", "authors": "Javier Gamero, Juan A. Tamayo and Juan A. Martinez-Roman", "title": "Forecast of the evolution of the contagious disease caused by novel\n  coronavirus (2019-nCoV) in China", "comments": "8 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The outbreak of novel coronavirus (2019-nCoV) in China has caused a viral\nepidemic affecting tens of thousands of persons. Though the danger of this\nepidemic is evident, the statistical analysis of data offered in this paper\nindicates that the increase in new cases in China will stabilize in the coming\ndays or weeks. Our forecast could serve to evaluate risks and control the\nevolution of this disease.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 00:15:20 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Gamero", "Javier", ""], ["Tamayo", "Juan A.", ""], ["Martinez-Roman", "Juan A.", ""]]}, {"id": "2002.04872", "submitter": "Leonardo Egidi PhD", "authors": "Teresa Fazia, Leonardo Egidi, Burcu Ayoglu, Ashley Beecham, Pier Paolo\n  Bitti, Anna Ticca, Hui Guo, Jacob L. McCauley, Peter Nilsson, Rosanna\n  Asselta, Carlo Berzuini, Luisa Bernardinelli", "title": "Mendelian Randomization with Incomplete Exposure Data: a Bayesian\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We expand Mendelian Randomization (MR) methodology to deal with randomly\nmissing data on either the exposure or the outcome variable, and furthermore\nwith data from nonindependent individuals (eg components of a family). Our\nmethod rests on the Bayesian MR framework proposed by Berzuini et al (2018),\nwhich we apply in a study of multiplex Multiple Sclerosis (MS) Sardinian\nfamilies to characterise the role of certain plasma proteins in MS causation.\nThe method is robust to presence of pleiotropic effects in an unknown number of\ninstruments, and is able to incorporate inter-individual kinship information.\nIntroduction of missing data allows us to overcome the bias introduced by the\n(reverse) effect of treatment (in MS cases) on level of protein. From a\nsubstantive point of view, our study results confirm recent suspicion that an\nincrease in circulating IL12A and STAT4 protein levels does not cause an\nincrease in MS risk, as originally believed, suggesting that these two proteins\nmay not be suitable drug targets for MS.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 09:48:26 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 11:24:07 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Fazia", "Teresa", ""], ["Egidi", "Leonardo", ""], ["Ayoglu", "Burcu", ""], ["Beecham", "Ashley", ""], ["Bitti", "Pier Paolo", ""], ["Ticca", "Anna", ""], ["Guo", "Hui", ""], ["McCauley", "Jacob L.", ""], ["Nilsson", "Peter", ""], ["Asselta", "Rosanna", ""], ["Berzuini", "Carlo", ""], ["Bernardinelli", "Luisa", ""]]}, {"id": "2002.05048", "submitter": "Jakub Pecanka", "authors": "Marianne A. Jonker and Jakub Pecanka", "title": "A powerful MAF-neutral allele-based test for case-control association\n  studies", "comments": "24 pages, 5 figures, includes Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.GN q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a case-control study aimed at locating autosomal disease variants for a\ndisease of interest, association between markers and the disease status is\noften tested by comparing the marker minor allele frequencies (MAFs) between\ncases and controls. For most common allele-based tests the statistical power is\nhighly dependent on the actual values of these MAFs, where associated markers\nwith low MAFs have less power to be detected compared to associated markers\nwith high MAFs. Therefore, the popular strategy of selecting markers for\nfollow-up studies based primarily on their p-values is likely to preferentially\nselect markers with high MAFs. We propose a new test which does not favor\nmarkers with high MAFs and improves the power for markers with low to moderate\nMAFs without sacrificing performance for markers with high MAFs and is\ntherefore superior to most existing tests in this regard. An explicit formula\nfor the asymptotic power function of the proposed test is derived\ntheoretically, which allows for fast and easy computation of the corresponding\np-values. The performance of the proposed test is compared with several\nexisting tests both in the asymptotic and the finite sample size settings.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 15:31:32 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Jonker", "Marianne A.", ""], ["Pecanka", "Jakub", ""]]}, {"id": "2002.05249", "submitter": "Yun-Hee Choi", "authors": "Yun-Hee Choi, Hae Jung, Saundra Buys, Mary Daly, Esther John, John\n  Hopper, Irene Andrulis, Mary-Beth Terry, Laurent Briollais", "title": "A Competing Risks Model with Binary Time Varying Covariates for\n  Estimation of Breast Cancer Risks in BRCA1 Families", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mammographic screening and prophylactic surgery such as risk-reducing\nsalpingo oophorectomy (RRSO) can potentially reduce breast cancer risks among\nmutation carriers of BRCA families. The evaluation of these interventions is\nusually complicated by the fact that their effects on breast cancer may change\nover time and by the presence of competing risks. We introduce a correlated\ncompeting risks model to model breast and ovarian cancer risks within BRCA1\nfamilies that accounts for time-varying covariates (TVCs). Different parametric\nforms for these TVCs are proposed for more flexibility and a correlated gamma\nfrailty model is specified to account for the correlated competing events. We\nalso introduced a new ascertainment correction approach that accounts for the\nselection of families through probands affected with either breast or ovarian\ncancer, or unaffected. Our simulation studies demonstrate the good performances\nof our proposed approach in terms of bias and precision of the estimators of\nmodel parameters and cause-specific penetrances over different levels of\nfamilial correlations. We apply our new approach to 498 BRCA1 mutation carrier\nfamilies recruited through the Breast Cancer Family Registry. Our results\ndemonstrate the importance of the functional form of the TVC when assessing the\nrole of RRSO on breast cancer. In particular, under the best fitting TVC model,\nthe overall effect of RRSO on breast cancer risk was statistically significant\nin women with BRCA1.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 21:45:27 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 11:02:18 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Choi", "Yun-Hee", ""], ["Jung", "Hae", ""], ["Buys", "Saundra", ""], ["Daly", "Mary", ""], ["John", "Esther", ""], ["Hopper", "John", ""], ["Andrulis", "Irene", ""], ["Terry", "Mary-Beth", ""], ["Briollais", "Laurent", ""]]}, {"id": "2002.05272", "submitter": "Ying Liao", "authors": "Ying Liao, Yisha Xiang, and Min Wang", "title": "Health Assessment and Prognostics Based on Higher Order Hidden\n  Semi-Markov Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new and flexible prognostics framework based on a\nhigher order hidden semi-Markov model (HOHSMM) for systems or components with\nunobservable health states and complex transition dynamics. The HOHSMM extends\nthe basic hidden Markov model (HMM) by allowing the hidden state to depend on\nits more distant history and assuming generally distributed state duration. An\neffective Gibbs sampling algorithm is designed for statistical inference of an\nHOHSMM. The performance of the proposed HOHSMM sampler is evaluated by\nconducting a simulation experiment. We further design a decoding algorithm to\nestimate the hidden health states using the learned model. Remaining useful\nlife (RUL) is predicted using a simulation approach given the decoded hidden\nstates. The practical utility of the proposed prognostics framework is\ndemonstrated by a case study on NASA turbofan engines. The results show that\nthe HOHSMM-based prognostics framework provides good hidden health state\nassessment and RUL estimation for complex systems.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 23:06:21 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Liao", "Ying", ""], ["Xiang", "Yisha", ""], ["Wang", "Min", ""]]}, {"id": "2002.05335", "submitter": "Jay Bartroff", "authors": "Maria Allayioti, Jay Bartroff, Larry Goldstein, Susan Luczak, Gary\n  Rosen", "title": "$M$-estimation in a diffusion model with application to biosensor\n  transdermal blood alcohol monitoring", "comments": "29 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the goal of well-founded statistical inference on an individual's blood\nalcohol level based on noisy measurements of their skin alcohol content, we\ndevelop $M$-estimation methodology in a general setting. We then apply it to a\ndiffusion equation-based model for the blood/skin alcohol relationship thereby\nestablishing existence, consistency, and asymptotic normality of the nonlinear\nleast squares estimator of the diffusion model's parameter. Simulation studies\nshow agreement between the estimator's performance and its asymptotic\ndistribution, and it is applied to a real skin alcohol data set collected via\nbiosensor.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 04:20:23 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Allayioti", "Maria", ""], ["Bartroff", "Jay", ""], ["Goldstein", "Larry", ""], ["Luczak", "Susan", ""], ["Rosen", "Gary", ""]]}, {"id": "2002.05438", "submitter": "Sylvain Le Corff", "authors": "Alice Martin (TIPIC-SAMOVAR), Marie-Pierre Etienne (IRMAR), Pierre\n  Gloaguen (MIA-Paris), Sylvain Le Corff (TIPIC-SAMOVAR), Jimmy Olsson (KTH)", "title": "Backward importance sampling for online estimation of state space models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new Sequential Monte Carlo algorithm to perform online\nestimation in the context of state space models when either the transition\ndensity of the latent state or the conditional likelihood of an observation\ngiven a state is intractable. In this setting, obtaining low variance\nestimators of expectations under the posterior distributions of the unobserved\nstates given the observations is a challenging task. Following recent\ntheoretical results for pseudo-marginal sequential Monte Carlo smoothers, a\npseudo-marginal backward importance sampling step is introduced to estimate\nsuch expectations. This new step allows to reduce very significantly the\ncomputational time of the existing numerical solutions based on an\nacceptance-rejection procedure for similar performance, and to broaden the\nclass of eligible models for such methods. For instance, in the context of\nmultivariate stochastic differential equations, the proposed algorithm makes\nuse of unbiased estimates of the unknown transition densities under much weaker\nassumptions than standard alternatives. The performance of this estimator is\nassessed for high-dimensional discrete-time latent data models, for recursive\nmaximum likelihood estimation in the context of partially observed diffusion\nprocess, and in the case of a bidimensional partially observed stochastic\nLotka-Volterra model.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 10:46:02 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 12:20:20 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Martin", "Alice", "", "TIPIC-SAMOVAR"], ["Etienne", "Marie-Pierre", "", "IRMAR"], ["Gloaguen", "Pierre", "", "MIA-Paris"], ["Corff", "Sylvain Le", "", "TIPIC-SAMOVAR"], ["Olsson", "Jimmy", "", "KTH"]]}, {"id": "2002.05592", "submitter": "Manuel Lladser", "authors": "Antony Pearson and Manuel E. Lladser", "title": "On Contamination of Symbolic Datasets", "comments": "18 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR math.ST q-bio.GN stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data taking values on discrete sample spaces are the embodiment of modern\nbiological research. \"Omics\" experiments produce millions of symbolic outcomes\nin the form of reads (i.e., DNA sequences of a few dozens to a few hundred\nnucleotides). Unfortunately, these intrinsically non-numerical datasets are\noften highly contaminated, and the possible sources of contamination are\nusually poorly characterized. This contrasts with numerical datasets where\nGaussian-type noise is often well-justified. To overcome this hurdle, we\nintroduce the notion of latent weight, which measures the largest expected\nfraction of samples from a contaminated probabilistic source that conform to a\nmodel in a well-structured class of desired models. We examine various\nproperties of latent weights, which we specialize to the class of exchangeable\nprobability distributions. As proof of concept, we analyze DNA methylation data\nfrom the 22 human autosome pairs. Contrary to what it is usually assumed, we\nprovide strong evidence that highly specific methylation patterns are\noverrepresented at some genomic locations when contamination is taken into\naccount.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 16:13:09 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Pearson", "Antony", ""], ["Lladser", "Manuel E.", ""]]}, {"id": "2002.05655", "submitter": "Subhro Das", "authors": "Subhro Das, Sebastian Steffen, Wyatt Clarke, Prabhat Reddy, Erik\n  Brynjolfsson, Martin Fleming", "title": "Learning Occupational Task-Shares Dynamics for the Future of Work", "comments": "9 pages, 5 figures, 6 tables, Proceedings of the AAAI/ACM Conference\n  on AI, Ethics, and Society (AIES), 2020", "journal-ref": null, "doi": "10.1145/3375627.3375826", "report-no": null, "categories": "cs.CY cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent wave of AI and automation has been argued to differ from previous\nGeneral Purpose Technologies (GPTs), in that it may lead to rapid change in\noccupations' underlying task requirements and persistent technological\nunemployment. In this paper, we apply a novel methodology of dynamic task\nshares to a large dataset of online job postings to explore how exactly\noccupational task demands have changed over the past decade of AI innovation,\nespecially across high, mid and low wage occupations. Notably, big data and AI\nhave risen significantly among high wage occupations since 2012 and 2016,\nrespectively. We built an ARIMA model to predict future occupational task\ndemands and showcase several relevant examples in Healthcare, Administration,\nand IT. Such task demands predictions across occupations will play a pivotal\nrole in retraining the workforce of the future.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 21:20:33 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Das", "Subhro", ""], ["Steffen", "Sebastian", ""], ["Clarke", "Wyatt", ""], ["Reddy", "Prabhat", ""], ["Brynjolfsson", "Erik", ""], ["Fleming", "Martin", ""]]}, {"id": "2002.05746", "submitter": "Luke Miratrix", "authors": "Luke Miratrix", "title": "Using Simulation to Analyze Interrupted Time Series Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are sometimes forced to use the Interrupted Time Series (ITS) design as an\nidentification strategy for potential policy change, such as when we only have\na single treated unit and no comparable controls. For example, with recent\ncounty- and state-wide criminal justice reform efforts, where judicial bodies\nhave changed bail setting practices for everyone in their jurisdiction in order\nto reduce rates of pre-trial detention while maintaining court order and public\nsafety, we have no natural comparison group other than the past. In these\ncontexts, it is imperative to model pre-policy trends with a light touch,\nallowing for structures such as autoregressive departures from any pre-existing\ntrend, in order to accurately and realistically assess the statistical\nuncertainty of our projections (beyond the stringent assumptions necessary for\nthe subsequent causal inferences). To tackle this problem we provide a\nmethodological approach rooted in commonly understood and used modeling\napproaches that better captures uncertainty. We quantify uncertainty with\nsimulation, generating a distribution of plausible counterfactual trajectories\nto compare to the observed; this approach naturally allows for incorporating\nseasonality and other time varying covariates, and provides confidence\nintervals along with point estimates for the potential impacts of policy\nchange. We find simulation provides a natural framework to capture and show\nuncertainty in the ITS designs. It also allows for easy extensions such as\nnonparametric smoothing in order to handle multiple post-policy time points or\nmore structural models to account for seasonality.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 19:10:23 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Miratrix", "Luke", ""]]}, {"id": "2002.05747", "submitter": "Nicolo Colombo", "authors": "Nicolo Colombo", "title": "Multiple Metric Learning for Structured Data", "comments": "24 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.MG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of merging graph and feature-space information while\nlearning a metric from structured data. Existing algorithms tackle the problem\nin an asymmetric way, by either extracting vectorized summaries of the graph\nstructure or adding hard constraints to feature-space algorithms. Following a\ndifferent path, we define a metric regression scheme where we train\nmetric-constrained linear combinations of dissimilarity matrices. The idea is\nthat the input matrices can be pre-computed dissimilarity measures obtained\nfrom any kind of available data (e.g. node attributes or edge structure). As\nthe model inputs are distance measures, we do not need to assume the existence\nof any underlying feature space. Main challenge is that metric constraints\n(especially positive-definiteness and sub-additivity), are not automatically\nrespected if, for example, the coefficients of the linear combination are\nallowed to be negative. Both positive and sub-additive constraints are linear\ninequalities, but the computational complexity of imposing them scales as\nO(D3), where D is the size of the input matrices (i.e. the size of the data\nset). This becomes quickly prohibitive, even when D is relatively small. We\npropose a new graph-based technique for optimizing under such constraints and\nshow that, in some cases, our approach may reduce the original computational\ncomplexity of the optimization process by one order of magnitude. Contrarily to\nexisting methods, our scheme applies to any (possibly non-convex)\nmetric-constrained objective function.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 19:11:32 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Colombo", "Nicolo", ""]]}, {"id": "2002.05793", "submitter": "Mamadou Yauck", "authors": "Mamadou Yauck, Erica E. M. Moodie, Herak Apelian, Marc-Messier Peet,\n  Gilles Lambert, Daniel Grace, Nathan J. Lachowsky, Trevor Hart, Joseph Cox", "title": "Sampling from Networks: Respondent-Driven Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Respondent-Driven Sampling (RDS) is a variant of link-tracing, a sampling\ntechnique for surveying hard-to-reach communities that takes advantage of\ncommunity members' social networks to reach potential participants. As a\nnetwork-based sampling method, RDS is faced with the fundamental problem of\nsampling from population networks where features such as homophily (the\ntendency for individuals with similar traits to share social ties) and\ndifferential activity (the ratio of the average number of connections by\nattribute) are sensitive to the choice of a sampling method. Though not clearly\ndescribed in the RDS literature, many simple methods exist to generate\nsimulated RDS data, with specific levels of network features, where the focus\nis on estimating simple estimands. However, the accuracy of these methods in\ntheir abilities to consistently recover those targeted network features remains\nunclear. This is also motivated by recent findings that some population network\nparameters (e.g.~homophily) cannot be consistently estimated from the RDS data\nalone \\citep{Crawford17}.\n  In this paper, we conduct a simulation study to assess the accuracy of\nexisting RDS simulation methods, in terms of their abilities to generate RDS\nsamples with the desired levels of two network parameters: homophily and\ndifferential activity. The results show that (1) homophily cannot be\nconsistently estimated from simulated RDS samples and (2) differential activity\nestimates are more precise when groups, defined by traits, are equally active\nand equally represented in the population. We use this approach to mimic\nfeatures of the Engage Study, an RDS sample of gay, bisexual and other men who\nhave sex with men in Montreal.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 21:55:36 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 22:16:14 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Yauck", "Mamadou", ""], ["Moodie", "Erica E. M.", ""], ["Apelian", "Herak", ""], ["Peet", "Marc-Messier", ""], ["Lambert", "Gilles", ""], ["Grace", "Daniel", ""], ["Lachowsky", "Nathan J.", ""], ["Hart", "Trevor", ""], ["Cox", "Joseph", ""]]}, {"id": "2002.05850", "submitter": "Justin Angevaare", "authors": "Justin Angevaare and Zeny Feng and Rob Deardon", "title": "Pathogen.jl: Infectious Disease Transmission Network Modelling with\n  Julia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Pathogen.jl for simulation and inference of transmission network\nindividual level models (TN-ILMs) of infectious disease spread. TN-ILMs can be\nused to jointly infer transmission networks, event times, and model parameters\nwithin a Bayesian framework via Markov chain Monte Carlow (MCMC). We detail our\nspecific strategies for conducting MCMC for TN-ILMs, and our implementation of\nthese strategies in the Julia package, Pathogen.jl, which leverages key\nfeatures of the Julia language. We provide an example using Pathogen.jl to\nsimulate an epidemic following a susceptible-infectious-removed (SIR) TN-ILM,\nand then perform inference using observations that were generated from that\nepidemic. We also demonstrate the functionality of Pathogen.jl with an\napplication of TN-ILMs to data from a measles outbreak that occurred in\nHagelloch, Germany in 1861 (Pfeilsticker 1863; Oesterle 1992).\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 02:32:14 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 01:59:49 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Angevaare", "Justin", ""], ["Feng", "Zeny", ""], ["Deardon", "Rob", ""]]}, {"id": "2002.05953", "submitter": "Stephen Johnson", "authors": "Stephen R. Johnson, Daniel A. Henderson and Richard J. Boys", "title": "On Bayesian inference for the Extended Plackett-Luce model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of rank ordered data has a long history in the statistical\nliterature across a diverse range of applications. In this paper we consider\nthe Extended Plackett-Luce model that induces a flexible (discrete)\ndistribution over permutations. The parameter space of this distribution is a\ncombination of potentially high-dimensional discrete and continuous components\nand this presents challenges for parameter interpretability and also posterior\ncomputation. Particular emphasis is placed on the interpretation of the\nparameters in terms of observable quantities and we propose a general framework\nfor preserving the mode of the prior predictive distribution. Posterior\nsampling is achieved using an effective simulation based approach that does not\nrequire imposing restrictions on the parameter space. Working in the Bayesian\nframework permits a natural representation of the posterior predictive\ndistribution and we draw on this distribution to address the rank aggregation\nproblem and also to identify potential lack of model fit. The flexibility of\nthe Extended Plackett-Luce model along with the effectiveness of the proposed\nsampling scheme are demonstrated using several simulation studies and real data\nexamples.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 10:19:20 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Johnson", "Stephen R.", ""], ["Henderson", "Daniel A.", ""], ["Boys", "Richard J.", ""]]}, {"id": "2002.06032", "submitter": "Irene Kyomuhangi", "authors": "Irene Kyomuhangi, Tarekegn A. Abeku, Matthew J. Kirby, Gezahegn\n  Tesfaye and Emanuele Giorgi", "title": "Understanding the effects of dichotomization of continuous outcomes on\n  geostatistical inference", "comments": "18 pages, 5 figures, to be published in the journal of Spatial\n  Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diagnosis is often based on the exceedance or not of continuous health\nindicators of a predefined cut-off value, so as to classify patients into\npositives and negatives for the disease under investigation. In this paper, we\ninvestigate the effects of dichotomization of spatially-referenced continuous\noutcome variables on geostatistical inference. Although this issue has been\nextensively studied in other fields, dichotomization is still a common practice\nin epidemiological studies. Furthermore, the effects of this practice in the\ncontext of prevalence mapping have not been fully understood. Here, we\ndemonstrate how spatial correlation affects the loss of information due to\ndichotomization, how linear geostatistical models can be used to map disease\nprevalence and thus avoid dichotomization, and finally, how dichotomization\naffects our predictive inference on prevalence. To pursue these objectives, we\ndevelop a metric, based on the composite likelihood, which can be used to\nquantify the potential loss of information after dichotomization without\nrequiring the fitting of Binomial geostatistical models. Through a simulation\nstudy and two applications on disease mapping in Africa, we show that, as\nthresholds used for dichotomization move further away from the mean of the\nunderlying process, the performance of binomial geostatistical models\ndeteriorates substantially. We also find that dichotomization can lead to the\nloss of fine scale features of disease prevalence and increased uncertainty in\nthe parameter estimates, especially in the presence of a large noise to signal\nratio. These findings strongly support the conclusions from previous studies\nthat dichotomization should be always avoided whenever feasible.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 13:32:14 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Kyomuhangi", "Irene", ""], ["Abeku", "Tarekegn A.", ""], ["Kirby", "Matthew J.", ""], ["Tesfaye", "Gezahegn", ""], ["Giorgi", "Emanuele", ""]]}, {"id": "2002.06060", "submitter": "Sebastian Weichwald", "authors": "Sebastian Weichwald and Jonas Peters", "title": "Causality in cognitive neuroscience: concepts, challenges, and\n  distributional robustness", "comments": null, "journal-ref": "Journal of Cognitive Neuroscience, 33(2):226-247, 2021", "doi": "10.1162/jocn_a_01623", "report-no": null, "categories": "q-bio.NC stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While probabilistic models describe the dependence structure between observed\nvariables, causal models go one step further: they predict, for example, how\ncognitive functions are affected by external interventions that perturb\nneuronal activity. In this review and perspective article, we introduce the\nconcept of causality in the context of cognitive neuroscience and review\nexisting methods for inferring causal relationships from data. Causal inference\nis an ambitious task that is particularly challenging in cognitive\nneuroscience. We discuss two difficulties in more detail: the scarcity of\ninterventional data and the challenge of finding the right variables. We argue\nfor distributional robustness as a guiding principle to tackle these problems.\nRobustness (or invariance) is a fundamental principle underlying causal\nmethodology. A causal model of a target variable generalises across\nenvironments or subjects as long as these environments leave the causal\nmechanisms intact. Consequently, if a candidate model does not generalise, then\neither it does not consist of the target variable's causes or the underlying\nvariables do not represent the correct granularity of the problem. In this\nsense, assessing generalisability may be useful when defining relevant\nvariables and can be used to partially compensate for the lack of\ninterventional data.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 14:49:34 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 07:39:52 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Weichwald", "Sebastian", ""], ["Peters", "Jonas", ""]]}, {"id": "2002.06102", "submitter": "Minjing Tao", "authors": "Shuguang Zhang, Minjing Tao, Xu-Feng Niu, and Fred Huffer", "title": "Time-Varying Gaussian-Cauchy Mixture Models for Financial Risk\n  Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are various metrics for financial risk, such as value at risk (VaR),\nexpected shortfall, expected/unexpected loss, etc. When estimating these\nmetrics, it was very common to assume Gaussian distribution for the asset\nreturns, which may underestimate the real risk of the market, especially during\nthe financial crisis. In this paper, we propose a series of time-varying\nmixture models for risk analysis and management. These mixture models contain\ntwo components: one component with Gaussian distribution, and the other one\nwith a fat-tailed Cauchy distribution. We allow the distribution parameters and\ncomponent weights to change over time to increase the flexibility of the\nmodels. Monte Carlo Expectation-Maximization algorithm is utilized to estimate\nthe parameters. To verify the good performance of our models, we conduct some\nsimulation studies, and implement our models to the real stock market. Based on\nthese studies, our models are appropriate under different economic conditions,\nand the component weights can capture the correct pattern of the market\nvolatility.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 16:13:09 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Zhang", "Shuguang", ""], ["Tao", "Minjing", ""], ["Niu", "Xu-Feng", ""], ["Huffer", "Fred", ""]]}, {"id": "2002.06105", "submitter": "Joseph Guinness", "authors": "Joseph Guinness, Debasmita Bhattacharya, Jenny Chen, Max Chen, Angela\n  Loh", "title": "An Observational Study of the Effect of Nike Vaporfly Shoes on Marathon\n  Performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We collected marathon performance data from a systematic sample of elite and\nsub-elite athletes over the period 2015 to 2019, then searched the internet for\npublicly-available photographs of these performances, identifying whether the\nNike Vaporfly shoes were worn or not in each performance. Controlling for\nathlete ability and race difficulty, we estimated the effect on marathon times\nof wearing the Vaporfly shoes. Assuming that the effect of Vaporfly shoes is\nadditive, we estimate that the Vaporfly shoes improve men's times between 2.0\nand 3.9 minutes, while they improve women's times between 0.8 and 3.5 minutes.\nAssuming that the effect of Vaporfly shoes is multiplicative, we estimate that\nthey improve men's times between 1.4 and 2.8 percent and women's performances\nbetween 0.6 and 2.2 percent. The improvements are in comparison to the shoe the\nathlete was wearing before switching to Vaporfly shoes, and represents an\nexpected improvement rather than a guaranteed improvement.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 16:22:39 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 18:36:28 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Guinness", "Joseph", ""], ["Bhattacharya", "Debasmita", ""], ["Chen", "Jenny", ""], ["Chen", "Max", ""], ["Loh", "Angela", ""]]}, {"id": "2002.06159", "submitter": "Mohsen Ebadi", "authors": "Mohsen Ebadi, Shoja'eddin Chenouri, Dennis K. J. Lin, Stefan H.\n  Steiner", "title": "Statistical Monitoring of the Covariance Matrix in Multivariate\n  Processes: A Literature Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monitoring several correlated quality characteristics of a process is common\nin modern manufacturing and service industries. Although a lot of attention has\nbeen paid to monitoring the multivariate process mean, not many control charts\nare available for monitoring the covariance matrix. This paper presents a\ncomprehensive overview of the literature on control charts for monitoring the\ncovariance matrix in a multivariate statistical process monitoring (MSPM)\nframework. It classifies the research that has previously appeared in the\nliterature. We highlight the challenging areas for research and provide some\ndirections for future research.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 18:18:42 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 03:15:40 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Ebadi", "Mohsen", ""], ["Chenouri", "Shoja'eddin", ""], ["Lin", "Dennis K. J.", ""], ["Steiner", "Stefan H.", ""]]}, {"id": "2002.06322", "submitter": "Mohammad Islam", "authors": "Mohammad M. Islam, Ph.D. and Erik L. Heiny, Ph.D", "title": "A Robust Statistical method to Estimate the Intervention Effect with\n  Longitudinal Data", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmented regression is a standard statistical procedure used to estimate the\neffect of a policy intervention on time series outcomes. This statistical\nmethod assumes the normality of the outcome variable, a large sample size, no\nautocorrelation in the observations, and a linear trend over time. Also,\nsegmented regression is very sensitive to outliers. In a small sample study, if\nthe outcome variable does not follow a Gaussian distribution, then using\nsegmented regression to estimate the intervention effect leads to incorrect\ninferences. To address the small sample problem and non-normality in the\noutcome variable, including outliers, we describe and develop a robust\nstatistical method to estimate the policy intervention effect in a series of\nlongitudinal data. A simulation study is conducted to demonstrate the effect of\noutliers and non-normality in the outcomes by calculating the power of the test\nstatistics with the segmented regression and the proposed robust statistical\nmethods. Moreover, since finding the sampling distribution of the proposed\nrobust statistic is analytically difficult, we use a nonparametric bootstrap\ntechnique to study the properties of the sampling distribution and make\nstatistical inferences. Simulation studies show that the proposed method has\nmore power than the standard t-test used in segmented regression analysis under\nthe non-normality error distribution. Finally, we use the developed technique\nto estimate the intervention effect of the Istanbul Declaration on illegal\norgan activities. The robust method detected more significant effects compared\nto the standard method and provided shorter confidence intervals.\n", "versions": [{"version": "v1", "created": "Sat, 15 Feb 2020 04:46:51 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Islam", "Mohammad M.", ""], ["D.", "Ph.", ""], ["Heiny", "Erik L.", ""], ["D", "Ph.", ""]]}, {"id": "2002.06503", "submitter": "Xun Shen", "authors": "Xun Shen, Tinghui Ouyang, Jiancang Zhuang, Chanyut Khajorntraidet", "title": "Statistical Simulator for the Engine Knock", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a statistical simulator for the engine knock based on the\nMixture Density Network (MDN) and the accept-reject method. The proposed\nsimulator can generate the random knock intensity signal corresponding to the\ninput signal. The generated knock intensity has a consistent probability\ndistribution with the real engine. Firstly, the statistical analysis is\nconducted with the experimental data. From the analysis results, some important\nassumptions on the statistical properties of the knock intensity are made.\nRegarding the knock intensity as a random variable on the discrete-time index,\nit is independent and identically distributed if the input of the engine is\nidentical. The probability distribution of the knock intensity under identical\ninput can be approximated by the Gaussian Mixture Model(GMM). The parameter of\nthe GMM is a function of the input. Based on these assumptions, two\nsub-problems for establishing the statistical simulator are formulated: One is\nto approximate the function from input to the parameters of the knock intensity\ndistribution with an absolutely continuous function; The other one is to design\na random number generator that outputs the random data consistent with the\ngiven distribution. The MDN is applied to approximate the probability density\nof the knock intensity and the accept-reject algorithm is used for the random\nnumber generator design. The proposed method is evaluated in experimental\ndata-based validation.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 04:12:35 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Shen", "Xun", ""], ["Ouyang", "Tinghui", ""], ["Zhuang", "Jiancang", ""], ["Khajorntraidet", "Chanyut", ""]]}, {"id": "2002.06663", "submitter": "Yishu Xue", "authors": "Guanyu Hu, Junxian Geng, Yishu Xue, Huiyan Sang", "title": "Bayesian Spatial Homogeneity Pursuit of Functional Data: an Application\n  to the U.S. Income Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  An income distribution describes how an entity's total wealth is distributed\namongst its population. A problem of interest to regional economics researchers\nis to understand the spatial homogeneity of income distributions among\ndifferent regions. In economics, the Lorenz curve is a well-known functional\nrepresentation of income distribution. In this article, we propose a mixture of\nfinite mixtures (MFM) model as well as a Markov random field constrained\nmixture of finite mixtures (MRFC-MFM) model in the context of spatial\nfunctional data analysis to capture spatial homogeneity of Lorenz curves. We\ndesign efficient Markov chain Monte Carlo (MCMC) algorithms to simultaneously\ninfer the posterior distributions of the number of clusters and the clustering\nconfiguration of spatial functional data. Extensive simulation studies are\ncarried out to show the effectiveness of the proposed methods compared with\nexisting methods. We apply the proposed spatial functional clustering method to\nstate level income Lorenz curves from the American Community Survey Public Use\nMicrodata Sample (PUMS) data. The results reveal a number of important\nclustering patterns of state-level income distributions across US.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 20:02:15 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2020 14:54:36 GMT"}, {"version": "v3", "created": "Mon, 14 Jun 2021 01:23:52 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Hu", "Guanyu", ""], ["Geng", "Junxian", ""], ["Xue", "Yishu", ""], ["Sang", "Huiyan", ""]]}, {"id": "2002.06678", "submitter": "Peng Zhao", "authors": "Peng Zhao, Hou-Cheng Yang, Dipak K. Dey, Guanyu Hu", "title": "Bayesian Spatial Homogeneity Pursuit Regression for Count Value Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial regression models are ubiquitous in many different areas such as\nenvironmental science, geoscience, and public health. Exploring relationships\nbetween response variables and covariates with complex spatial patterns is a\nvery important work. In this paper, we propose a novel spatially clustered\ncoefficients regression model for count value data based on nonparametric\nBayesian methods. Our proposed method detects the spatial homogeneity of the\nPoisson regression coefficients. A Markov random field constraint mixture of\nfinite mixtures prior provides a consistent estimator of the number of the\nclusters of regression coefficients with the geographically neighborhood\ninformation. The theoretical properties of our proposed method are established.\nAn efficient Markov chain Monte Carlo algorithm is developed by using\nmultivariate log gamma distribution as a base distribution. Extensive\nsimulation studies are carried out to examine empirical performance of the\nproposed method. Additionally, we analyze Georgia premature deaths data as an\nillustration of the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 21:05:45 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Zhao", "Peng", ""], ["Yang", "Hou-Cheng", ""], ["Dey", "Dipak K.", ""], ["Hu", "Guanyu", ""]]}, {"id": "2002.06710", "submitter": "Rina Friedberg", "authors": "Rina Friedberg, Clea Sarnquist, Gavin Nyairo, Mary Amuyunzu-Nyamongo,\n  Michael Baiocchi", "title": "Understanding the spatial burden of gender-based violence: Modelling\n  patterns of violence in Nairobi, Kenya through geospatial information", "comments": "18 pages, 2 figures, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present statistical techniques for analyzing global positioning system\n(GPS) data in order to understand, communicate about, and prevent patterns of\nviolence. In this pilot study, participants in Nairobi, Kenya were asked to\nrate their safety at several locations, with the goal of predicting safety and\nlearning important patterns. These approaches are meant to help articulate\ndifferences in experiences, fostering a discussion that will help communities\nidentify issues and policymakers develop safer communities. A generalized\nlinear mixed model incorporating spatial information taken from existing maps\nof Kibera showed significant predictors of perceived lack of safety included\nbeing alone and time of day; in debrief interviews, participants described\nfeeling unsafe in spaces with hiding places, disease carrying animals, and\ndangerous individuals. This pilot study demonstrates promise for detecting\nspatial patterns of violence, which appear to be confirmed by actual rates of\nmeasured violence at schools. Several factors relevant to community building\nconsistently predict perceived safety and emerge in participants' qualitative\ndescriptions, telling a cohesive story about perceived safety and empowering\ncommunication to community stakeholders.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 23:36:12 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Friedberg", "Rina", ""], ["Sarnquist", "Clea", ""], ["Nyairo", "Gavin", ""], ["Amuyunzu-Nyamongo", "Mary", ""], ["Baiocchi", "Michael", ""]]}, {"id": "2002.06987", "submitter": "Wei Deng", "authors": "Wei Deng and Junwei Pan and Tian Zhou and Deguang Kong and Aaron\n  Flores and Guang Lin", "title": "DeepLight: Deep Lightweight Feature Interactions for Accelerating CTR\n  Predictions in Ad Serving", "comments": "Accepted by WSDM 2021; Source code:\n  https://github.com/WayneDW/DeepLight_Deep-Lightweight-Feature-Interactions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Click-through rate (CTR) prediction is a crucial task in online display\nadvertising. The embedding-based neural networks have been proposed to learn\nboth explicit feature interactions through a shallow component and deep feature\ninteractions using a deep neural network (DNN) component. These sophisticated\nmodels, however, slow down the prediction inference by at least hundreds of\ntimes. To address the issue of significantly increased serving delay and high\nmemory usage for ad serving in production, this paper presents\n\\emph{DeepLight}: a framework to accelerate the CTR predictions in three\naspects: 1) accelerate the model inference via explicitly searching informative\nfeature interactions in the shallow component; 2) prune redundant layers and\nparameters at intra-layer and inter-layer level in the DNN component; 3)\npromote the sparsity of the embedding layer to preserve the most discriminant\nsignals. By combining the above efforts, the proposed approach accelerates the\nmodel inference by 46X on Criteo dataset and 27X on Avazu dataset without any\nloss on the prediction accuracy. This paves the way for successfully deploying\ncomplicated embedding-based neural networks in production for ad serving.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 14:51:31 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 01:46:08 GMT"}, {"version": "v3", "created": "Wed, 6 Jan 2021 22:13:51 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Deng", "Wei", ""], ["Pan", "Junwei", ""], ["Zhou", "Tian", ""], ["Kong", "Deguang", ""], ["Flores", "Aaron", ""], ["Lin", "Guang", ""]]}, {"id": "2002.07039", "submitter": "Giuliano Vitali", "authors": "Giuliano Vitali, Sergei Rogosin and Guido Baldoni", "title": "Climate Change and Grain Production Fluctuations", "comments": "10 pages + 7 composite figures + 2 appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  50 year-long time series from a Long Term Agronomic Experiment have been used\nto to investigate the effects of climate change on yields of Wheat and Maize.\nTrends and fluctuations, useful to estimate production forecasts and related\nrisks are compared to national ones, a classical regional climatic index as\nWestern Mediterranean Oscillation Index, and a global one given by Sun Spot\nNumber. Data, denoised by EMD and SSA, show how SSN oscillations slowing down\nin the last decades, affects regional scale dynamics, where in the last two\ndecades a range of fluctuations (7-16 years) are also evident. Both signals\nreflects on yield fluctuations of Wheat and Maize both a national and local\nlevel.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 10:44:39 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Vitali", "Giuliano", ""], ["Rogosin", "Sergei", ""], ["Baldoni", "Guido", ""]]}, {"id": "2002.07069", "submitter": "Daniel Griffin", "authors": "Daniel K. Griffin", "title": "The Big Three: A Methodology to Increase Data Science ROI by Answering\n  the Questions Companies Care About", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Companies may be achieving only a third of the value they could be getting\nfrom data science in industry applications. In this paper, we propose a\nmethodology for categorizing and answering 'The Big Three' questions (what is\ngoing on, what is causing it, and what actions can I take that will optimize\nwhat I care about) using data science. The applications of data science seem to\nbe nearly endless in today's modern landscape, with each company jockeying for\nposition in the new data and insights economy. Yet, data scientists seem to be\nsolely focused on using classification, regression, and clustering methods to\nanswer the question 'what is going on'. Answering questions about why things\nare happening or how to take optimal actions to improve metrics are relegated\nto niche fields of research and generally neglected in industry data science\nanalysis. We survey technical methods to answer these other important\nquestions, describe areas in which some of these methods are being applied, and\nprovide a practical example of how to apply our methodology and selected\nmethods to a real business use case.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 21:25:56 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Griffin", "Daniel K.", ""]]}, {"id": "2002.07774", "submitter": "Michael O'Malley", "authors": "Michael O'Malley, Adam M. Sykulski, Romuald Laso-Jadart, Mohammed-Amin\n  Madoui", "title": "Estimating the travel time and the most likely path from Lagrangian\n  drifters", "comments": "27 pages, 10 figures in the main text. 13 pages, 8 figures in the\n  supplemental material", "journal-ref": null, "doi": "10.1175/JTECH-D-20-0134.1", "report-no": null, "categories": "stat.AP physics.ao-ph physics.data-an physics.flu-dyn stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a novel methodology for computing the most likely path taken by\ndrifters between arbitrary fixed locations in the ocean. We also provide an\nestimate of the travel time associated with this path. Lagrangian pathways and\ntravel times are of practical value not just in understanding surface\nvelocities, but also in modelling the transport of ocean-borne species such as\nplanktonic organisms, and floating debris such as plastics. In particular, the\nestimated travel time can be used to compute an estimated Lagrangian distance,\nwhich is often more informative than Euclidean distance in understanding\nconnectivity between locations. Our methodology is purely data-driven, and\nrequires no simulations of drifter trajectories, in contrast to existing\napproaches. Our method scales globally and can simultaneously handle multiple\nlocations in the ocean. Furthermore, we provide estimates of the error and\nuncertainty associated with both the most likely path and the associated travel\ntime.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 18:06:51 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 17:49:13 GMT"}, {"version": "v3", "created": "Tue, 1 Dec 2020 18:08:33 GMT"}, {"version": "v4", "created": "Thu, 18 Mar 2021 18:35:21 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["O'Malley", "Michael", ""], ["Sykulski", "Adam M.", ""], ["Laso-Jadart", "Romuald", ""], ["Madoui", "Mohammed-Amin", ""]]}, {"id": "2002.07873", "submitter": "Emily T Winn", "authors": "Emily T. Winn, Marilyn Vazquez, Prachi Loliencar, Kaisa Taipale, Xu\n  Wang and Giseon Heo", "title": "A survey of statistical learning techniques as applied to inexpensive\n  pediatric Obstructive Sleep Apnea data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pediatric obstructive sleep apnea affects an estimated 1-5% of\nelementary-school aged children and can lead to other detrimental health\nproblems. Swift diagnosis and treatment are critical to a child's growth and\ndevelopment, but the variability of symptoms and the complexity of the\navailable data make this a challenge. We take a first step in streamlining the\nprocess by focusing on inexpensive data from questionnaires and craniofacial\nmeasurements. We apply correlation networks, the Mapper algorithm from\ntopological data analysis, and singular value decomposition in a process of\nexploratory data analysis. We then apply a variety of supervised and\nunsupervised learning techniques from statistics, machine learning, and\ntopology, ranging from support vector machines to Bayesian classifiers and\nmanifold learning. Finally, we analyze the results of each of these methods and\ndiscuss the implications for a multi-data-sourced algorithm moving forward.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 18:15:32 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2020 14:35:46 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Winn", "Emily T.", ""], ["Vazquez", "Marilyn", ""], ["Loliencar", "Prachi", ""], ["Taipale", "Kaisa", ""], ["Wang", "Xu", ""], ["Heo", "Giseon", ""]]}, {"id": "2002.07964", "submitter": "Yanzhao Li", "authors": "Shaolong Sun, Yanzhao Li, Ju-e Guo, Shouyang Wang", "title": "Tourism Demand Forecasting: An Ensemble Deep Learning Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of tourism-related big data increases the potential to\nimprove the accuracy of tourism demand forecasting, but presents significant\nchallenges for forecasting, including curse of dimensionality and high model\ncomplexity. A novel bagging-based multivariate ensemble deep learning approach\nintegrating stacked autoencoders and kernel-based extreme learning machines\n(B-SAKE) is proposed to address these challenges in this study. By using\nhistorical tourist arrival data, economic variable data and search intensity\nindex (SII) data, we forecast tourist arrivals in Beijing from four countries.\nThe consistent results of multiple schemes suggest that our proposed B-SAKE\napproach outperforms benchmark models in terms of level accuracy, directional\naccuracy and even statistical significance. Both bagging and stacked\nautoencoder can effectively alleviate the challenges brought by tourism big\ndata and improve the forecasting performance of the models. The ensemble deep\nlearning model we propose contributes to tourism forecasting literature and\nbenefits relevant government officials and tourism practitioners.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 02:23:38 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 13:30:18 GMT"}, {"version": "v3", "created": "Sat, 16 Jan 2021 08:02:31 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Sun", "Shaolong", ""], ["Li", "Yanzhao", ""], ["Guo", "Ju-e", ""], ["Wang", "Shouyang", ""]]}, {"id": "2002.08021", "submitter": "Dan Bi", "authors": "Shaolong Suna, Dan Bi, Ju-e Guo, Shouyang Wang", "title": "Seasonal and Trend Forecasting of Tourist Arrivals: An Adaptive\n  Multiscale Ensemble Learning Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accurate seasonal and trend forecasting of tourist arrivals is a very\nchallenging task. In the view of the importance of seasonal and trend\nforecasting of tourist arrivals, and limited research work paid attention to\nthese previously. In this study, a new adaptive multiscale ensemble (AME)\nlearning approach incorporating variational mode decomposition (VMD) and least\nsquare support vector regression (LSSVR) is developed for short-, medium-, and\nlong-term seasonal and trend forecasting of tourist arrivals. In the\nformulation of our developed AME learning approach, the original tourist\narrivals series are first decomposed into the trend, seasonal and remainders\nvolatility components. Then, the ARIMA is used to forecast the trend component,\nthe SARIMA is used to forecast seasonal component with a 12-month cycle, while\nthe LSSVR is used to forecast remainder volatility components. Finally, the\nforecasting results of the three components are aggregated to generate an\nensemble forecasting of tourist arrivals by the LSSVR based nonlinear ensemble\napproach. Furthermore, a direct strategy is used to implement multi-step-ahead\nforecasting. Taking two accuracy measures and the Diebold-Mariano test, the\nempirical results demonstrate that our proposed AME learning approach can\nachieve higher level and directional forecasting accuracy compared with other\nbenchmarks used in this study, indicating that our proposed approach is a\npromising model for forecasting tourist arrivals with high seasonality and\nvolatility.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 06:32:01 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 13:37:10 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Suna", "Shaolong", ""], ["Bi", "Dan", ""], ["Guo", "Ju-e", ""], ["Wang", "Shouyang", ""]]}, {"id": "2002.08113", "submitter": "Vladimir Panov", "authors": "Anatoly N. Varaksin and Vladimir G. Panov", "title": "Linear Regression Models in Epidemiology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper proposes to analyze epidemiological data using regression models\nwhich enable subject-matter (epidemiological) interpretation of such data\nwhether with uncorrelated or correlated predictors. To this end, response\nfunctions should include not only terms linear in predictors but also higher\norder ones (e.g. quadratic and cross terms). For epidemiological interpretation\nof a regression model, the suggestion is to construct conditional functions\nderived from the general regression function with the values of all predictor\nvariables held fixed excepting one predictor. Unlike the conventional\ntechniques based on linear-predictor models in which the coefficient at any\nvariable is interpreted, our approach proposes to interpret this conditional\nfunction, which is multivariate for any predictor being dependent on the values\nof all the other predictors. It is such functions that can describe\nrelationships between Y and a predictor that have different forms in different\npredictor domains. The paper discusses differences in the interpretation of the\nproposed conditional functions between cases involving correlated and\nuncorrelated predictor variables. The construction and analysis of regression\nmodels for epidemiological and environmental data are illustrated with\nexamples.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 11:34:09 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Varaksin", "Anatoly N.", ""], ["Panov", "Vladimir G.", ""]]}, {"id": "2002.08146", "submitter": "Nienke Dijkstra", "authors": "Nienke F. S. Dijkstra, Henning Tiemeier, Bernd C. Figner, Patrick J.\n  F. Groenen", "title": "A censored mixture model for modeling risk taking", "comments": "29 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Risk behavior can have substantial consequences for health, well-being, and\nfunctioning. Previous studies have shown an association between real-world risk\nbehavior and risk behavior on experimental tasks, such as the Columbia Card\nTask, but their modeling is challenging for several reasons. First, many of the\nexperimental risk tasks may end prematurely leading to censored observations.\nSecond, certain outcome values can be more attractive than others. Third, a\npriori unknown groups of participants can react differently to certain\nrisk-levels. Here, we propose the Censored Mixture Model (CMM), which models\nrisk taking while handling censoring, experimental conditions, and\nattractiveness to certain outcomes.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 12:56:36 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Dijkstra", "Nienke F. S.", ""], ["Tiemeier", "Henning", ""], ["Figner", "Bernd C.", ""], ["Groenen", "Patrick J. F.", ""]]}, {"id": "2002.08262", "submitter": "Vincent G. A. B\\\"oning", "authors": "Vincent G. A. B\\\"oning, Aaron C. Birch, Laurent Gizon, Thomas L.\n  Duvall Jr., Jesper Schou", "title": "Characterizing the spatial pattern of solar supergranulation using the\n  bispectrum", "comments": "16 pages, 12 figures, accepted for publication by A&A", "journal-ref": "A&A 635, A181 (2020)", "doi": "10.1051/0004-6361/201937331", "report-no": null, "categories": "astro-ph.SR nlin.PS physics.flu-dyn stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context. The spatial power spectrum of supergranulation does not fully\ncharacterize the underlying physics of turbulent convection. For example, it\ndoes not describe the non-Gaussianity in the horizontal flow divergence.\n  Aims. Our aim is to statistically characterize the spatial pattern of solar\nsupergranulation beyond the power spectrum. The next-order statistic is the\nbispectrum. It measures correlations of three Fourier components and is related\nto the nonlinearities in the underlying physics.\n  Methods. We estimated the bispectrum of supergranular horizontal surface\ndivergence maps that were obtained using local correlation tracking (LCT) and\ntime-distance helioseismology (TD) from one year of data from the Helioseismic\nand Magnetic Imager on-board the Solar Dynamics Observatory starting in May\n2010.\n  Results. We find significantly nonzero and consistent estimates for the\nbispectrum. The strongest nonlinearity is present when the three coupling wave\nvectors are at the supergranular scale. These are the same wave vectors that\nare present in regular hexagons, which were used in analytical studies of solar\nconvection. At these Fourier components, the bispectrum is positive, consistent\nwith the positive skewness in the data and with supergranules preferentially\nconsisting of outflows surrounded by a network of inflows. We use the\nbispectrum to generate synthetic divergence maps that are very similar to the\ndata by a model that consists of a Gaussian term and a weaker quadratic\nnonlinear component. Thereby, we estimate the fraction of the variance in the\ndivergence maps from the nonlinear component to be of the order of 4-6%.\n  Conclusions. We propose that bispectral analysis is useful for understanding\nsolar turbulent convection, for example for comparing observations and\nnumerical models of supergranular flows. This analysis may also be useful to\ngenerate synthetic flow fields.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 16:07:55 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["B\u00f6ning", "Vincent G. A.", ""], ["Birch", "Aaron C.", ""], ["Gizon", "Laurent", ""], ["Duvall", "Thomas L.", "Jr."], ["Schou", "Jesper", ""]]}, {"id": "2002.08457", "submitter": "Hyunseung Kang", "authors": "Hyunseung Kang, Yang Jiang, Qingyuan Zhao, and Dylan S. Small", "title": "ivmodel: An R Package for Inference and Sensitivity Analysis of\n  Instrumental Variables Models with One Endogenous Variable", "comments": "24 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a comprehensive R software ivmodel for analyzing instrumental\nvariables with one endogenous variable. The package implements a general class\nof estimators called k- class estimators and two confidence intervals that are\nfully robust to weak instruments. The package also provides power formulas for\nvarious test statistics in instrumental variables. Finally, the package\ncontains methods for sensitivity analysis to examine the sensitivity of the\ninference to instrumental variables assumptions. We demonstrate the software on\nthe data set from Card (1995), looking at the causal effect of levels of\neducation on log earnings where the instrument is proximity to a four-year\ncollege.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 21:41:27 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 03:10:08 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Kang", "Hyunseung", ""], ["Jiang", "Yang", ""], ["Zhao", "Qingyuan", ""], ["Small", "Dylan S.", ""]]}, {"id": "2002.08465", "submitter": "Georgios Giasemidis Dr", "authors": "Georgios Giasemidis", "title": "Descriptive and Predictive Analysis of Euroleague Basketball Games and\n  the Wisdom of Basketball Crowds", "comments": "24 pages, several figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study we focus on the prediction of basketball games in the\nEuroleague competition using machine learning modelling. The prediction is a\nbinary classification problem, predicting whether a match finishes 1 (home win)\nor 2 (away win). Data is collected from the Euroleague's official website for\nthe seasons 2016-2017, 2017-2018 and 2018-2019, i.e. in the new format era.\nFeatures are extracted from matches' data and off-the-shelf supervised machine\nlearning techniques are applied. We calibrate and validate our models. We find\nthat simple machine learning models give accuracy not greater than 67% on the\ntest set, worse than some sophisticated benchmark models. Additionally, the\nimportance of this study lies in the \"wisdom of the basketball crowd\" and we\ndemonstrate how the predicting power of a collective group of basketball\nenthusiasts can outperform machine learning models discussed in this study. We\nargue why the accuracy level of this group of \"experts\" should be set as the\nbenchmark for future studies in the prediction of (European) basketball games\nusing machine learning.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 22:04:29 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Giasemidis", "Georgios", ""]]}, {"id": "2002.08505", "submitter": "Jingxiong Xu", "authors": "Jingxiong Xu, Wei Xu, Laurent Briollais", "title": "A Bayes Factor Approach with Informative Prior for Rare Genetic Variant\n  Analysis from Next Generation Sequencing Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discovery of rare genetic variants through Next Generation Sequencing is\na very challenging issue in the field of human genetics. We propose a novel\nregion-based statistical approach based on a Bayes Factor (BF) to assess\nevidence of association between a set of rare variants (RVs) located on the\nsame genomic region and a disease outcome in the context of case-control\ndesign. Marginal likelihoods are computed under the null and alternative\nhypotheses assuming a binomial distribution for the RV count in the region and\na beta or mixture of Dirac and beta prior distribution for the probability of\nRV. We derive the theoretical null distribution of the BF under our prior\nsetting and show that a Bayesian control of the False Discovery Rate (BFDR) can\nbe obtained for genome-wide inference. Informative priors are introduced using\nprior evidence of association from a Kolmogorov-Smirnov test statistic. We use\nour simulation program, sim1000G, to generate RV data similar to the 1,000\ngenomes sequencing project. Our simulation studies showed that the new BF\nstatistic outperforms standard methods (SKAT, SKAT-O, Burden test) in\ncase-control studies with moderate sample sizes and is equivalent to them under\nlarge sample size scenarios. Our real data application to a lung cancer\ncase-control study found enrichment for RVs in known and novel cancer genes. It\nalso suggests that using the BF with informative prior improves the overall\ngene discovery compared to the BF with non-informative prior.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 00:34:46 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Xu", "Jingxiong", ""], ["Xu", "Wei", ""], ["Briollais", "Laurent", ""]]}, {"id": "2002.08514", "submitter": "Richard La", "authors": "Michael Lin, Nuno C. Martins, Richard J. La", "title": "Queueing Subject To Action-Dependent Server Performance: Utilization\n  Rate Reduction", "comments": "16 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a discrete-time system comprising a first-come-first-served\nqueue, a non-preemptive server, and a stationary non-work-conserving scheduler.\nNew tasks enter the queue according to a Bernoulli process with a pre-specified\narrival rate. At each instant, the server is either busy working on a task or\nis available. When the server is available, the scheduler either assigns a new\ntask to the server or allows it to remain available (to rest). In addition to\nthe aforementioned availability state, we assume that the server has an\ninteger-valued activity state. The activity state is non-decreasing during work\nperiods, and is non-increasing otherwise. In a typical application of our\nframework, the server performance (understood as task completion probability)\nworsens as the activity state increases. In this article, we build on and\ntranscend recent stabilizability results obtained for the same framework.\nSpecifically, we establish methods to design scheduling policies that not only\nstabilize the queue but also reduce the utilization rate - understood as the\ninfinite-horizon time-averaged portion of time the server is working. This\narticle has a main theorem leading to two key results: (i) We put forth a\ntractable method to determine, using a finite-dimensional linear program (LP),\nthe infimum of all utilization rates that can be achieved by scheduling\npolicies that are stabilizing, for a given arrival rate. (ii) We propose a\ndesign method, also based on finite-dimensional LPs, to obtain stabilizing\nscheduling policies that can attain a utilization rate arbitrarily close to the\naforementioned infimum. We also establish structural and distributional\nconvergence properties, which are used throughout the article, and are\nsignificant in their own right.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 01:13:03 GMT"}, {"version": "v2", "created": "Sat, 4 Jul 2020 13:23:32 GMT"}, {"version": "v3", "created": "Tue, 4 Aug 2020 02:06:59 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Lin", "Michael", ""], ["Martins", "Nuno C.", ""], ["La", "Richard J.", ""]]}, {"id": "2002.08538", "submitter": "Yahya Sattar", "authors": "Yahya Sattar and Samet Oymak", "title": "Non-asymptotic and Accurate Learning of Nonlinear Dynamical Systems", "comments": null, "journal-ref": "arXiv preprint:2002.08538, 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning stabilizable systems governed by\nnonlinear state equation $h_{t+1}=\\phi(h_t,u_t;\\theta)+w_t$. Here $\\theta$ is\nthe unknown system dynamics, $h_t $ is the state, $u_t$ is the input and $w_t$\nis the additive noise vector. We study gradient based algorithms to learn the\nsystem dynamics $\\theta$ from samples obtained from a single finite trajectory.\nIf the system is run by a stabilizing input policy, we show that\ntemporally-dependent samples can be approximated by i.i.d. samples via a\ntruncation argument by using mixing-time arguments. We then develop new\nguarantees for the uniform convergence of the gradients of empirical loss.\nUnlike existing work, our bounds are noise sensitive which allows for learning\nground-truth dynamics with high accuracy and small sample complexity. Together,\nour results facilitate efficient learning of the general nonlinear system under\nstabilizing policy. We specialize our guarantees to entry-wise nonlinear\nactivations and verify our theory in various numerical experiments\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 02:36:44 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Sattar", "Yahya", ""], ["Oymak", "Samet", ""]]}, {"id": "2002.08609", "submitter": "Arthur Lui", "authors": "Arthur Lui, Juhee Lee, Peter F. Thall, May Daher, Katy Rezvani, Rafet\n  Barar", "title": "A Bayesian Feature Allocation Model for Identification of Cell\n  Subpopulations Using Cytometry Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Bayesian feature allocation model (FAM) is presented for identifying cell\nsubpopulations based on multiple samples of cell surface or intracellular\nmarker expression level data obtained by cytometry by time of flight (CyTOF).\nCell subpopulations are characterized by differences in expression patterns of\nmakers, and individual cells are clustered into the subpopulations based on the\npatterns of their observed expression levels. A finite Indian buffet process is\nused to model subpopulations as latent features, and a model-based method based\non these latent feature subpopulations is used to construct cell clusters\nwithin each sample. Non-ignorable missing data due to technical artifacts in\nmass cytometry instruments are accounted for by defining a static missing data\nmechanism. In contrast to conventional cell clustering methods based on\nobserved marker expression levels that are applied separately to different\nsamples, the FAM based method can be applied simultaneously to multiple\nsamples, and can identify important cell subpopulations likely to be missed by\nconventional clustering. The proposed FAM based method is applied to jointly\nanalyze three datasets, generated by CyTOF, to study natural killer (NK) cells.\nBecause the subpopulations identified by the FAM may define novel NK cell\nsubsets, this statistical analysis may provide useful information about the\nbiology of NK cells and their potential role in cancer immunotherapy which may\nlead, in turn, to development of improved cellular therapies. Simulation\nstudies of the proposed method's behavior under two cases of known\nsubpopulations also are presented, followed by analysis of the CyTOF NK cell\nsurface marker data.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 08:07:03 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Lui", "Arthur", ""], ["Lee", "Juhee", ""], ["Thall", "Peter F.", ""], ["Daher", "May", ""], ["Rezvani", "Katy", ""], ["Barar", "Rafet", ""]]}, {"id": "2002.08849", "submitter": "Minjing Tao", "authors": "Wenjing Wang and Minjing Tao", "title": "Forecasting Realized Volatility Matrix With Copula-Based Models", "comments": "26 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate volatility modeling and forecasting are crucial in financial\neconomics. This paper develops a copula-based approach to model and forecast\nrealized volatility matrices. The proposed copula-based time series models can\ncapture the hidden dependence structure of realized volatility matrices. Also,\nthis approach can automatically guarantee the positive definiteness of the\nforecasts through either Cholesky decomposition or matrix logarithm\ntransformation. In this paper we consider both multivariate and bivariate\ncopulas; the types of copulas include Student's t, Clayton and Gumbel copulas.\nIn an empirical application, we find that for one-day ahead volatility matrix\nforecasting, these copula-based models can achieve significant performance both\nin terms of statistical precision as well as creating economically\nmean-variance efficient portfolio. Among the copulas we considered, the\nmultivariate-t copula performs better in statistical precision, while\nbivariate-t copula has better economical performance.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 16:36:17 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Wang", "Wenjing", ""], ["Tao", "Minjing", ""]]}, {"id": "2002.09014", "submitter": "Eric Bax", "authors": "Eric Bax", "title": "Heavy Tails Make Happy Buyers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT econ.TH stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a second-price auction with i.i.d. (independent identically distributed)\nbidder valuations, adding bidders increases expected buyer surplus if the\ndistribution of valuations has a sufficiently heavy right tail. While this does\nnot imply that a bidder in an auction should prefer for more bidders to join\nthe auction, it does imply that a bidder should prefer it in exchange for the\nbidder being allowed to participate in more auctions. Also, for a heavy-tailed\nvaluation distribution, marginal expected seller revenue per added bidder\nremains strong even when there are already many bidders.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 20:47:42 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Bax", "Eric", ""]]}, {"id": "2002.09119", "submitter": "Sharmistha Guha", "authors": "Sharmistha Guha, Jerome P. Reiter and Andrea Mercatanti", "title": "Bayesian Causal Inference with Bipartite Record Linkage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many scenarios, the observational data needed for causal inferences are\nspread over two data files. In particular, we consider scenarios where one file\nincludes covariates and the treatment measured on one set of individuals, and a\nsecond file includes responses measured on another, partially overlapping set\nof individuals. In the absence of error free direct identifiers like social\nsecurity numbers, straightforward merging of separate files is not feasible, so\nthat records must be linked using error-prone variables such as names, birth\ndates, and demographic characteristics. Typical practice in such situations\ngenerally follows a two-stage procedure: first link the two files using a\nprobabilistic linkage technique, then make causal inferences with the linked\ndataset. This does not propagate uncertainty due to imperfect linkages to the\ncausal inference, nor does it leverage relationships among the study variables\nto improve the quality of the linkages. We propose a hierarchical model for\nsimultaneous Bayesian inference on probabilistic linkage and causal effects\nthat addresses these deficiencies. Using simulation studies and theoretical\narguments, we show the hierarchical model can improve the accuracy of estimated\ntreatment effects, as well as the record linkages, compared to the two-stage\nmodeling option. We illustrate the hierarchical model using a causal study of\nthe effects of debit card possession on household spending.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 04:04:29 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 20:58:02 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Guha", "Sharmistha", ""], ["Reiter", "Jerome P.", ""], ["Mercatanti", "Andrea", ""]]}, {"id": "2002.09162", "submitter": "Daniel Andrade", "authors": "Daniel Andrade and Yuzuru Okajima", "title": "Adaptive Covariate Acquisition for Minimizing Total Cost of\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In some applications, acquiring covariates comes at a cost which is not\nnegligible. For example in the medical domain, in order to classify whether a\npatient has diabetes or not, measuring glucose tolerance can be expensive.\nAssuming that the cost of each covariate, and the cost of misclassification can\nbe specified by the user, our goal is to minimize the (expected) total cost of\nclassification, i.e. the cost of misclassification plus the cost of the\nacquired covariates. We formalize this optimization goal using the\n(conditional) Bayes risk and describe the optimal solution using a recursive\nprocedure. Since the procedure is computationally infeasible, we consequently\nintroduce two assumptions: (1) the optimal classifier can be represented by a\ngeneralized additive model, (2) the optimal sets of covariates are limited to a\nsequence of sets of increasing size. We show that under these two assumptions,\na computationally efficient solution exists. Furthermore, on several medical\ndatasets, we show that the proposed method achieves in most situations the\nlowest total costs when compared to various previous methods. Finally, we\nweaken the requirement on the user to specify all misclassification costs by\nallowing the user to specify the minimally acceptable recall (target recall).\nOur experiments confirm that the proposed method achieves the target recall\nwhile minimizing the false discovery rate and the covariate acquisition costs\nbetter than previous methods.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 07:30:52 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Andrade", "Daniel", ""], ["Okajima", "Yuzuru", ""]]}, {"id": "2002.09267", "submitter": "Matthias Reuber", "authors": "Alfred M\\\"uller, Matthias Reuber", "title": "A copula-based time series model for global horizontal irradiation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing importance of solar power for electricity generation leads to\nan increasing demand for probabilistic forecasting of local and aggregated PV\nyields. In this paper we use an indirect modeling approach for hourly medium to\nlong term local PV yields based on publicly available irradiation data. We\nsuggest a time series model for global horizontal irradiation for which it is\neasy to generate an arbitrary number of scenarios and thus allows for\nmultivariate probabilistic forecasts for arbitrary time horizons. In contrast\nto many simplified models that have been considered in the literature so far it\nfeatures several important stylized facts. Sharp time dependent lower and upper\nbounds of global horizontal irradiations are estimated that improve the often\nused physical bounds. The parameters of the beta distributed marginals of the\ntransformed data are allowed to be time dependent. A copula-based time series\nmodel is introduced for the hourly and daily dependence structure based on a\nsimple graphical structure known from the theory of vine copulas. Non-Gaussian\ncopulas like Gumbel and BB1 copulas are used that allow for the important\nfeature of so-called tail dependence. Evaluation methods like the continuous\nranked probability score (CRPS), the energy score (ES) and the variogram score\n(VS) are used to compare the power of the model for multivariate probabilistic\nforecasting with other models used in the literature showing that our model\noutperforms other models in many respects.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 13:26:01 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["M\u00fcller", "Alfred", ""], ["Reuber", "Matthias", ""]]}, {"id": "2002.09269", "submitter": "Binh T. Nguyen", "authors": "Tuan-Binh Nguyen, J\\'er\\^ome-Alexis Chevalier, Bertrand Thirion,\n  Sylvain Arlot", "title": "Aggregation of Multiple Knockoffs", "comments": "Accepted to ICML 2020 (Thirty-seventh International Conference on\n  Machine Learning). This version includes both the main text of the conference\n  paper and supplementary materials (as appendices). 35 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an extension of the Knockoff Inference procedure, introduced by\nBarber and Candes (2015). This new method, called Aggregation of Multiple\nKnockoffs (AKO), addresses the instability inherent to the random nature of\nKnockoff-based inference. Specifically, AKO improves both the stability and\npower compared with the original Knockoff algorithm while still maintaining\nguarantees for False Discovery Rate control. We provide a new inference\nprocedure, prove its core properties, and demonstrate its benefits in a set of\nexperiments on synthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 13:28:40 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 14:26:21 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Nguyen", "Tuan-Binh", ""], ["Chevalier", "J\u00e9r\u00f4me-Alexis", ""], ["Thirion", "Bertrand", ""], ["Arlot", "Sylvain", ""]]}, {"id": "2002.09287", "submitter": "Kai Zhou", "authors": "Kai Zhou and Jiong Tang", "title": "Uncertainty Quantification of Mode Shape Variation Utilizing Multi-Level\n  Multi-Response Gaussian Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mode shape information play the essential role in deciding the spatial\npattern of vibratory response of a structure. The uncertainty quantification of\nmode shape, i.e., predicting mode shape variation when the structure is\nsubjected to uncertainty, can provide guidance for robust design and control.\nNevertheless, computational efficiency is a challenging issue. Direct Monte\nCarlo simulation is unlikely to be feasible especially for a complex structure\nwith large number of degrees of freedom. In this research, we develop a new\nprobabilistic framework built upon Gaussian process meta-modeling architecture\nto analyze mode shape variation. To expedite the generation of input dataset\nfor meta-model establishment, a multi-level strategy is adopted which can blend\na large amount of low-fidelity data acquired from order-reduced analysis with a\nsmall amount of high-fidelity data produced by high-dimensional full finite\nelement analysis. To take advantage of the intrinsic relation of spatial\ndistribution of mode shape, a multi-response strategy is incorporated to\npredict mode shape variation at different locations simultaneously. These yield\na multi-level, multi-response Gaussian process that can efficiently and\naccurately quantify the effect of structural uncertainty to mode shape\nvariation. Comprehensive case studies are carried out for demonstration and\nvalidation.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 15:08:53 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Zhou", "Kai", ""], ["Tang", "Jiong", ""]]}, {"id": "2002.09470", "submitter": "Nathaniel Garton", "authors": "Nathaniel Garton, Danica Ommen, Jarad Niemi, Alicia Carriquiry", "title": "Score-based likelihood ratios to evaluate forensic pattern evidence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2016, the European Network of Forensic Science Institutes (ENFSI)\npublished guidelines for the evaluation, interpretation and reporting of\nscientific evidence. In the guidelines, ENFSI endorsed the use of the\nlikelihood ratio (LR) as a means to represent the probative value of most types\nof evidence. While computing the value of a LR is practical in several forensic\ndisciplines, calculating an LR for pattern evidence such as fingerprints,\nfirearm and other toolmarks is particularly challenging because standard\nstatistical approaches are not applicable. Recent research suggests that\nmachine learning algorithms can summarize a potentially large set of features\ninto a single score which can then be used to quantify the similarity between\npattern samples. It is then possible to compute a score-based likelihood ratio\n(SLR) and obtain an approximation to the value of the evidence, but research\nhas shown that the SLR can be quite different from the LR not only in size but\nalso in direction. We provide theoretical and empirical arguments that under\nreasonable assumptions, the SLR can be a practical tool for forensic\nevaluations.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 18:47:16 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 14:59:23 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Garton", "Nathaniel", ""], ["Ommen", "Danica", ""], ["Niemi", "Jarad", ""], ["Carriquiry", "Alicia", ""]]}, {"id": "2002.09515", "submitter": "Thibaut Astic", "authors": "Thibaut Astic, Lindsey J. Heagy, and Douglas W. Oldenburg", "title": "Petrophysically and geologically guided multi-physics inversion using a\n  dynamic Gaussian mixture model", "comments": "47 pages in one column format; 14 figures; to be published in\n  Geophysical Journal International", "journal-ref": null, "doi": "10.1093/gji/ggaa378", "report-no": null, "categories": "physics.geo-ph cs.CE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a previous paper, we introduced a framework for carrying out\npetrophysically and geologically guided geophysical inversions. In that\nframework, petrophysical and geological information is modelled with a Gaussian\nMixture Model (GMM). In the inversion, the GMM serves as a prior for the\ngeophysical model. The formulation was confined to problems in which a single\nphysical property model was sought, with a single geophysical dataset. In this\npaper, we extend that framework to jointly invert multiple geophysical datasets\nthat depend on multiple physical properties. The petrophysical and geological\ninformation is used to couple geophysical surveys that, otherwise, rely on\nindependent physics. This requires advancements in two areas. First, an\nextension from a univariate to a multivariate analysis of the petrophysical\ndata, and their inclusion within the inverse problem, is necessary. Second, we\naddress the practical issues of simultaneously inverting data from multiple\nsurveys and finding a solution that acceptably reproduces each one, along with\nthe petrophysical and geological information. To illustrate the efficacy of our\napproach and the advantages of carrying out multi-physics inversions, we invert\nsynthetic gravity and magnetic data associated with a kimberlite deposit. The\nkimberlite pipe contains two distinct facies embedded in a host rock. Inverting\nthe datasets individually leads to a binary geological model: background or\nkimberlite. A multi-physics inversion, with petrophysical information,\ndifferentiates between the two main kimberlite facies of the pipe. Through this\nexample, we also highlight the capabilities of our framework to work with\ninterpretive geologic assumptions when minimal quantitative information is\navailable. In those cases, the dynamic updates of the Gaussian Mixture Model\nallow us to perform multi-physics inversions by learning a petrophysical model.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 19:20:17 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 19:54:14 GMT"}, {"version": "v3", "created": "Mon, 19 Oct 2020 03:50:04 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Astic", "Thibaut", ""], ["Heagy", "Lindsey J.", ""], ["Oldenburg", "Douglas W.", ""]]}, {"id": "2002.09535", "submitter": "Qingsong Wen", "authors": "Qingsong Wen, Kai He, Liang Sun, Yingying Zhang, Min Ke, Huan Xu", "title": "RobustPeriod: Time-Frequency Mining for Robust Multiple Periodicity\n  Detection", "comments": "Accepted by SIGMOD 2021; 10 pages, 6 figures, 8 tables, and 70\n  referred papers", "journal-ref": null, "doi": "10.1145/3448016.3452779", "report-no": null, "categories": "cs.LG eess.SP stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Periodicity detection is a crucial step in time series tasks, including\nmonitoring and forecasting of metrics in many areas, such as IoT applications\nand self-driving database management system. In many of these applications,\nmultiple periodic components exist and are often interlaced with each other.\nSuch dynamic and complicated periodic patterns make the accurate periodicity\ndetection difficult. In addition, other components in the time series, such as\ntrend, outliers and noises, also pose additional challenges for accurate\nperiodicity detection. In this paper, we propose a robust and general framework\nfor multiple periodicity detection. Our algorithm applies maximal overlap\ndiscrete wavelet transform to transform the time series into multiple\ntemporal-frequency scales such that different periodic components can be\nisolated. We rank them by wavelet variance, and then at each scale detect\nsingle periodicity by our proposed Huber-periodogram and Huber-ACF robustly. We\nrigorously prove the theoretical properties of Huber-periodogram and justify\nthe use of Fisher's test on Huber-periodogram for periodicity detection. To\nfurther refine the detected periods, we compute unbiased autocorrelation\nfunction based on Wiener-Khinchin theorem from Huber-periodogram for improved\nrobustness and efficiency. Experiments on synthetic and real-world datasets\nshow that our algorithm outperforms other popular ones for both single and\nmultiple periodicity detection.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 20:10:36 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 00:59:11 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Wen", "Qingsong", ""], ["He", "Kai", ""], ["Sun", "Liang", ""], ["Zhang", "Yingying", ""], ["Ke", "Min", ""], ["Xu", "Huan", ""]]}, {"id": "2002.09545", "submitter": "Qingsong Wen", "authors": "Jingkun Gao, Xiaomin Song, Qingsong Wen, Pichao Wang, Liang Sun, Huan\n  Xu", "title": "RobustTAD: Robust Time Series Anomaly Detection via Decomposition and\n  Convolutional Neural Networks", "comments": "9 pages, 5 figures, and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The monitoring and management of numerous and diverse time series data at\nAlibaba Group calls for an effective and scalable time series anomaly detection\nservice. In this paper, we propose RobustTAD, a Robust Time series Anomaly\nDetection framework by integrating robust seasonal-trend decomposition and\nconvolutional neural network for time series data. The seasonal-trend\ndecomposition can effectively handle complicated patterns in time series, and\nmeanwhile significantly simplifies the architecture of the neural network,\nwhich is an encoder-decoder architecture with skip connections. This\narchitecture can effectively capture the multi-scale information from time\nseries, which is very useful in anomaly detection. Due to the limited labeled\ndata in time series anomaly detection, we systematically investigate data\naugmentation methods in both time and frequency domains. We also introduce\nlabel-based weight and value-based weight in the loss function by utilizing the\nunbalanced nature of the time series anomaly detection problem. Compared with\nthe widely used forecasting-based anomaly detection algorithms,\ndecomposition-based algorithms, traditional statistical algorithms, as well as\nrecent neural network based algorithms, RobustTAD performs significantly better\non public benchmark datasets. It is deployed as a public online service and\nwidely adopted in different business scenarios at Alibaba Group.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 20:43:45 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Gao", "Jingkun", ""], ["Song", "Xiaomin", ""], ["Wen", "Qingsong", ""], ["Wang", "Pichao", ""], ["Sun", "Liang", ""], ["Xu", "Huan", ""]]}, {"id": "2002.09573", "submitter": "Sebastian Weichwald", "authors": "Sebastian Weichwald, Martin E Jakobsen, Phillip B Mogensen, Lasse\n  Petersen, Nikolaj Thams, Gherardo Varando", "title": "Causal structure learning from time series: Large regression\n  coefficients may predict causal links better in practice than small p-values", "comments": null, "journal-ref": "Proceedings of the NeurIPS 2019 Competition and Demonstration\n  Track, Proceedings of Machine Learning Research, 123:27-36, 2020 (\n  http://proceedings.mlr.press/v123/weichwald20a.html )", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we describe the algorithms for causal structure learning\nfrom time series data that won the Causality 4 Climate competition at the\nConference on Neural Information Processing Systems 2019 (NeurIPS). We examine\nhow our combination of established ideas achieves competitive performance on\nsemi-realistic and realistic time series data exhibiting common challenges in\nreal-world Earth sciences data. In particular, we discuss a) a rationale for\nleveraging linear methods to identify causal links in non-linear systems, b) a\nsimulation-backed explanation as to why large regression coefficients may\npredict causal links better in practice than small p-values and thus why\nnormalising the data may sometimes hinder causal structure learning.\n  For benchmark usage, we detail the algorithms here and provide\nimplementations at https://github.com/sweichwald/tidybench . We propose the\npresented competition-proven methods for baseline benchmark comparisons to\nguide the development of novel algorithms for structure learning from time\nseries.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 23:02:00 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 09:24:53 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Weichwald", "Sebastian", ""], ["Jakobsen", "Martin E", ""], ["Mogensen", "Phillip B", ""], ["Petersen", "Lasse", ""], ["Thams", "Nikolaj", ""], ["Varando", "Gherardo", ""]]}, {"id": "2002.09644", "submitter": "Stephen Bates", "authors": "Stephen Bates, Matteo Sesia, Chiara Sabatti, Emmanuel Candes", "title": "Causal Inference in Genetic Trio Studies", "comments": null, "journal-ref": "Proc. Natl. Acad. Sci. U.S.A. 177 (2020) 24117-24126", "doi": "10.1073/pnas.2007743117", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method to rigorously draw causal inferences---inferences\nimmune to all possible confounding---from genetic data that include parents and\noffspring. Causal conclusions are possible with these data because the natural\nrandomness in meiosis can be viewed as a high-dimensional randomized\nexperiment. We make this observation actionable by developing a novel\nconditional independence test that identifies regions of the genome containing\ndistinct causal variants. The proposed Digital Twin Test compares an observed\noffspring to carefully constructed synthetic offspring from the same parents in\norder to determine statistical significance, and it can leverage any black-box\nmultivariate model and additional non-trio genetic data in order to increase\npower. Crucially, our inferences are based only on a well-established\nmathematical description of the rearrangement of genetic material during\nmeiosis and make no assumptions about the relationship between the genotypes\nand phenotypes.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 06:40:05 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Bates", "Stephen", ""], ["Sesia", "Matteo", ""], ["Sabatti", "Chiara", ""], ["Candes", "Emmanuel", ""]]}, {"id": "2002.09695", "submitter": "Yifan Yang", "authors": "Guowei Zhang, Tao Ren, and Yifan Yang", "title": "A New Unified Deep Learning Approach with\n  Decomposition-Reconstruction-Ensemble Framework for Time Series Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new variational mode decomposition (VMD) based deep learning approach is\nproposed in this paper for time series forecasting problem. Firstly, VMD is\nadopted to decompose the original time series into several sub-signals. Then, a\nconvolutional neural network (CNN) is applied to learn the reconstruction\npatterns on the decomposed sub-signals to obtain several reconstructed\nsub-signals. Finally, a long short term memory (LSTM) network is employed to\nforecast the time series with the decomposed sub-signals and the reconstructed\nsub-signals as inputs. The proposed VMD-CNN-LSTM approach is originated from\nthe decomposition-reconstruction-ensemble framework, and innovated by embedding\nthe reconstruction, single forecasting, and ensemble steps in a unified deep\nlearning approach. To verify the forecasting performance of the proposed\napproach, four typical time series datasets are introduced for empirical\nanalysis. The empirical results demonstrate that the proposed approach\noutperforms consistently the benchmark approaches in terms of forecasting\naccuracy, and also indicate that the reconstructed sub-signals obtained by CNN\nis of importance for further improving the forecasting performance.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 12:57:50 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Zhang", "Guowei", ""], ["Ren", "Tao", ""], ["Yang", "Yifan", ""]]}, {"id": "2002.09734", "submitter": "Abraham Nunes", "authors": "Abraham Nunes, Martin Alda, and Thomas Trappenberg", "title": "Multiplicative Decomposition of Heterogeneity in Mixtures of Continuous\n  Distributions", "comments": null, "journal-ref": null, "doi": "10.3390/e22080858", "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A system's heterogeneity (\\textit{diversity}) is the effective size of its\nevent space, and can be quantified using the R\\'enyi family of indices (also\nknown as Hill numbers in ecology or Hannah-Kay indices in economics), which are\nindexed by an elasticity parameter $q \\geq 0$. Under these indices, the\nheterogeneity of a composite system (the $\\gamma$-heterogeneity) is\ndecomposable into heterogeneity arising from variation \\textit{within} and\n\\textit{between} component subsystems (the $\\alpha$- and $\\beta$-heterogeneity,\nrespectively). Since the average heterogeneity of a component subsystem should\nnot be greater than that of the pooled system, we require that $\\gamma \\geq\n\\alpha$. There exists a multiplicative decomposition for R\\'enyi heterogeneity\nof composite systems with discrete event spaces, but less attention has been\npaid to decomposition in the continuous setting. We therefore describe\nmultiplicative decomposition of the R\\'enyi heterogeneity for continuous\nmixture distributions under parametric and non-parametric pooling assumptions.\nUnder non-parametric pooling, the $\\gamma$-heterogeneity must often be\nestimated numerically, but the multiplicative decomposition holds such that\n$\\gamma \\geq \\alpha$ for $q > 0$. Conversely, under parametric pooling,\n$\\gamma$-heterogeneity can be computed efficiently in closed-form, but the\n$\\gamma \\geq \\alpha$ condition holds reliably only at $q=1$. Our findings will\nfurther contribute to heterogeneity measurement in continuous systems.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 17:12:07 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 18:11:57 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Nunes", "Abraham", ""], ["Alda", "Martin", ""], ["Trappenberg", "Thomas", ""]]}, {"id": "2002.09897", "submitter": "Harvey Goldstein", "authors": "Harvey Goldstein, George Leckie, Lucy Prior", "title": "Providing educational accountability for Local Authorities based upon\n  sampling pupils within schools: moving away from simplistic school league\n  tables", "comments": "5700 words", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper proposes an alternative educational accountability system for\nEngland that moves away from simplistic comparisons, or league tables, among\nschools towards a more nuanced reporting at a level of education authorities or\nother school groupings. Based upon the sampling of pupils within schools, it\nproposes the use of quantitative pupil assessment data as screening devices\nthat provide evidence within an integrated accountability system that includes\ninspection. At the same time, it aims to provide a richer set of data than\ncurrently available for research as well as accountability purposes. We argue\nthat if carefully implemented within a context of school improvement, such a\nsystem has the potential to largely eliminate the deleterious side effects and\ncurriculum distortions of the present system. While being proposed within the\ncontext of the current English secondary school system, the proposals will have\nrelevance for other phases of schooling and similar systems in other countries.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 13:21:37 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Goldstein", "Harvey", ""], ["Leckie", "George", ""], ["Prior", "Lucy", ""]]}, {"id": "2002.09910", "submitter": "Shirin Golchi", "authors": "Shirin Golchi", "title": "Use of Historical Individual Patient Data in Analysis of Clinical Trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Historical data from previous clinical trials, observational studies and\nhealth records may be utilized in analysis of clinical trials data to\nstrengthen inference. Under the Bayesian framework incorporation of information\nobtained from any source other than the current data is facilitated through\nconstruction of an informative prior. The existing methodology for defining an\ninformative prior based on historical data relies on measuring similarity to\nthe current data at the study level and does not take advantage of individual\npatient data (IPD). This paper proposes a family of priors that utilize IPD to\nstrengthen statistical inference. It is demonstrated that the proposed prior\nconstruction approach outperforms the existing methods where the historical\ndata are partially exchangeable with the present data. The proposed method is\napplied to IPD from a set of trials in non-small cell lung cancer.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 14:18:36 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 13:30:48 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Golchi", "Shirin", ""]]}, {"id": "2002.10038", "submitter": "Han Lin Shang", "authors": "Han Lin Shang", "title": "Bayesian bandwidth estimation and semi-metric selection for a functional\n  partial linear model with unknown error density", "comments": "27 pages, 10 figures, to appear in Journal of Applied Statistics", "journal-ref": "Journal of Applied Statistics (2020)", "doi": "10.1080/02664763.2020.1736527", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study examines the optimal selections of bandwidth and semi-metric for a\nfunctional partial linear model. Our proposed method begins by estimating the\nunknown error density using a kernel density estimator of residuals, where the\nregression function, consisting of parametric and nonparametric components, can\nbe estimated by functional principal component and functional Nadayara-Watson\nestimators. The estimation accuracy of the regression function and error\ndensity crucially depends on the optimal estimations of bandwidth and\nsemi-metric. A Bayesian method is utilized to simultaneously estimate the\nbandwidths in the regression function and kernel error density by minimizing\nthe Kullback-Leibler divergence. For estimating the regression function and\nerror density, a series of simulation studies demonstrate that the functional\npartial linear model gives improved estimation and forecast accuracies compared\nwith the functional principal component regression and functional nonparametric\nregression. Using a spectroscopy dataset, the functional partial linear model\nyields better forecast accuracy than some commonly used functional regression\nmodels. As a by-product of the Bayesian method, a pointwise prediction interval\ncan be obtained, and marginal likelihood can be used to select the optimal\nsemi-metric.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 02:15:34 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Shang", "Han Lin", ""]]}, {"id": "2002.10046", "submitter": "Anderson Winkler", "authors": "Anderson M. Winkler, Olivier Renaud, Stephen M. Smith, Thomas E.\n  Nichols", "title": "Permutation Inference for Canonical Correlation Analysis", "comments": "49 pages, 2 figures, 10 tables, 3 algorithms, 119 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canonical correlation analysis (CCA) has become a key tool for population\nneuroimaging, allowing investigation of associations between many imaging and\nnon-imaging measurements. As other variables are often a source of variability\nnot of direct interest, previous work has used CCA on residuals from a model\nthat removes these effects, then proceeded directly to permutation inference.\nWe show that such a simple permutation test leads to inflated error rates. The\nreason is that residualisation introduces dependencies among the observations\nthat violate the exchangeability assumption. Even in the absence of nuisance\nvariables, however, a simple permutation test for CCA also leads to excess\nerror rates for all canonical correlations other than the first. The reason is\nthat a simple permutation scheme does not ignore the variability already\nexplained by previous canonical variables. Here we propose solutions for both\nproblems: in the case of nuisance variables, we show that transforming the\nresiduals to a lower dimensional basis where exchangeability holds results in a\nvalid permutation test; for more general cases, with or without nuisance\nvariables, we propose estimating the canonical correlations in a stepwise\nmanner, removing at each iteration the variance already explained, while\ndealing with different number of variables in both sides. We also discuss how\nto address the multiplicity of tests, proposing an admissible test that is not\nconservative, and provide a complete algorithm for permutation inference for\nCCA.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 02:47:01 GMT"}, {"version": "v2", "created": "Sat, 14 Mar 2020 22:45:59 GMT"}, {"version": "v3", "created": "Tue, 26 May 2020 18:23:36 GMT"}, {"version": "v4", "created": "Thu, 18 Jun 2020 01:15:58 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Winkler", "Anderson M.", ""], ["Renaud", "Olivier", ""], ["Smith", "Stephen M.", ""], ["Nichols", "Thomas E.", ""]]}, {"id": "2002.10497", "submitter": "Roland Langrock", "authors": "Brett T. McClintock, Roland Langrock, Olivier Gimenez, Emmanuelle Cam,\n  David L. Borchers, Richard Glennie, Toby A. Patterson", "title": "Uncovering ecological state dynamics with hidden Markov models", "comments": null, "journal-ref": null, "doi": "10.1111/ele.13610", "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ecological systems can often be characterised by changes among a finite set\nof underlying states pertaining to individuals, populations, communities, or\nentire ecosystems through time. Owing to the inherent difficulty of empirical\nfield studies, ecological state dynamics operating at any level of this\nhierarchy can often be unobservable or \"hidden\". Ecologists must therefore\noften contend with incomplete or indirect observations that are somehow related\nto these underlying processes. By formally disentangling state and observation\nprocesses based on simple yet powerful mathematical properties that can be used\nto describe many ecological phenomena, hidden Markov models (HMMs) can\nfacilitate inferences about complex system state dynamics that might otherwise\nbe intractable. However, while HMMs are routinely applied in other disciplines,\nthey have only recently begun to gain traction within the broader ecological\ncommunity. We provide a gentle introduction to HMMs, establish some common\nterminology, and review the immense scope of HMMs for applied ecological\nresearch. We also provide a supplemental tutorial on some of the more technical\naspects of HMM implementation and interpretation. By illustrating how\npractitioners can use a simple conceptual template to customise HMMs for their\nspecific systems of interest, revealing methodological links between existing\napplications, and highlighting some practical considerations and limitations of\nthese approaches, our goal is to help establish HMMs as a fundamental\ninferential tool for ecologists.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 19:26:48 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 20:58:01 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["McClintock", "Brett T.", ""], ["Langrock", "Roland", ""], ["Gimenez", "Olivier", ""], ["Cam", "Emmanuelle", ""], ["Borchers", "David L.", ""], ["Glennie", "Richard", ""], ["Patterson", "Toby A.", ""]]}, {"id": "2002.10545", "submitter": "Michael LuValle", "authors": "Michael LuValle", "title": "Statistical inference for Axiom A attractors", "comments": "16 pages 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From the climate system to the effect of the internet on society, chaotic\nsystems appear to have a significant role in our future. Here a method of\nstatistical learning for a class of chaotic systems is described along with\nunderlying theory that can be used not only for predicting those systems a\nshort time ahead, but also as a basis for statistical inference about their\ndynamics. The method is applied to prediction of 3 different systems. The\nstatistical inference aspect can be applied to explore and enhance computer\nmodels of such systems which in turn can provide feedback for even better\nprediction and more precise inference.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 21:03:54 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["LuValle", "Michael", ""]]}, {"id": "2002.10566", "submitter": "Ekaterina Abramova", "authors": "Ekaterina Abramova, Derek Bunn", "title": "Forecasting the Intra-Day Spread Densities of Electricity Prices", "comments": "31 pages, 25 figures. arXiv admin note: substantial text overlap with\n  arXiv:1903.06668", "journal-ref": "Energies 2020, 13(3), 687", "doi": "10.3390/en13030687", "report-no": null, "categories": "stat.AP cs.LG econ.EM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intra-day price spreads are of interest to electricity traders, storage and\nelectric vehicle operators. This paper formulates dynamic density functions,\nbased upon skewed-t and similar representations, to model and forecast the\nGerman electricity price spreads between different hours of the day, as\nrevealed in the day-ahead auctions. The four specifications of the density\nfunctions are dynamic and conditional upon exogenous drivers, thereby\npermitting the location, scale and shape parameters of the densities to respond\nhourly to such factors as weather and demand forecasts. The best fitting and\nforecasting specifications for each spread are selected based on the Pinball\nLoss function, following the closed-form analytical solutions of the cumulative\ndistribution functions.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 19:57:07 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Abramova", "Ekaterina", ""], ["Bunn", "Derek", ""]]}, {"id": "2002.10573", "submitter": "Federico Rodas", "authors": "Federico Rodas Baja\\~na, Luis Hernan Montoya Lara, Manolo Paredes,\n  Elena Gimenez de Ory", "title": "Multi Linear Regression applied to Communications systems Analysis", "comments": "22 pages,11 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a propagation model of electromagnetic signals emitted at\nfrequencies of 20 and 40 MHz for the Ecuadorian jungle. It is expected that the\nresults obtained at the end of this research will be applied to produce a\ncomplete coverage map for wireless communications technologies, which will\noptimize the radio spectrum in operations carried out by the Armed Forces in\nthe Ecuadorian border jungle. The final expression found is an adjustment\nfunction that relates the Receiving Power (PRX) to factors that determine the\ngeometry of the Fresnell Zone (Connectivity). The resulting model of the\nresearch improves the discrepancy between the simulated power (PRX) in\ncommercial software and a sample of measured wireless transmissions in situ.\nThe analysis was based on the results and methodology presented by\nLongley-Rice. It was determined the non-normality of the discrepancy between\nthe losses (LLR) calculated by Longley-Rice Model (LMR) and the data obtained\nin the field, It was added correction coefficients on the expression of LMR.\nSubsequently, the mathematical expression was linearized to implement\nmultivariate linear adjustment techniques. Alternative formulations to the\nLinear Regression model were sought and their goodness of fit was compared; all\nthese techniques are introduced theoretically. To conclude, an analysis of the\nerror of the found model is made. Mathematical modeling software such as MATLAB\nand SPSS were used for the formulation and numerical analysis, whose algorithms\nare also introduced. Finally, we propose future lines of research that allow a\nglobal understanding of the behavior of telecommunications technologies under\nhostile environments.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 22:19:53 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Baja\u00f1a", "Federico Rodas", ""], ["Lara", "Luis Hernan Montoya", ""], ["Paredes", "Manolo", ""], ["de Ory", "Elena Gimenez", ""]]}, {"id": "2002.10633", "submitter": "Hau-tieng Wu", "authors": "John Malik, Elsayed Z Soliman, Hau-Tieng Wu", "title": "An Adaptive QRS Detection Algorithm for Ultra-Long-Term ECG Recordings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Accurate detection of QRS complexes during mobile,\nultra-long-term ECG monitoring is challenged by instances of high heart rate,\ndramatic and persistent changes in signal amplitude, and intermittent\ndeformations in signal quality that arise due to subject motion, background\nnoise, and misplacement of the ECG electrodes. Purpose: We propose a revised\nQRS detection algorithm which addresses the above-mentioned challenges. Methods\nand Results: Our proposed algorithm is based on a state-of-the-art algorithm\nafter applying two key modifications. The first modification is implementing\nlocal estimates for the amplitude of the signal. The second modification is a\nmechanism by which the algorithm becomes adaptive to changes in heart rate. We\nvalidated our proposed algorithm against the state-of-the-art algorithm using\nshort-term ECG recordings from eleven annotated databases available at\nPhysionet, as well as four ultra-long-term (14-day) ECG recordings which were\nvisually annotated at a central ECG core laboratory. On the database of\nultra-long-term ECG recordings, our proposed algorithm showed a sensitivity of\n99.90% and a positive predictive value of 99.73%. Meanwhile, the\nstate-of-the-art QRS detection algorithm achieved a sensitivity of 99.30% and a\npositive predictive value of 99.68% on the same database. The numerical\nefficiency of our new algorithm was evident, as a 14-day recording sampled at\n200 Hz was analyzed in approximately 157 seconds. Conclusions: We developed a\nnew QRS detection algorithm. The efficiency and accuracy of our algorithm makes\nit a good fit for mobile health applications, ultra-long-term and pathological\nECG recordings, and the batch processing of large ECG databases.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 02:44:56 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Malik", "John", ""], ["Soliman", "Elsayed Z", ""], ["Wu", "Hau-Tieng", ""]]}, {"id": "2002.10709", "submitter": "Arkopal Choudhury", "authors": "Arkopal Choudhury and Michael R. Kosorok", "title": "Missing Data Imputation for Classification Problems", "comments": "27 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imputation of missing data is a common application in various classification\nproblems where the feature training matrix has missingness. A widely used\nsolution to this imputation problem is based on the lazy learning technique,\n$k$-nearest neighbor (kNN) approach. However, most of the previous work on\nmissing data does not take into account the presence of the class label in the\nclassification problem. Also, existing kNN imputation methods use variants of\nMinkowski distance as a measure of distance, which does not work well with\nheterogeneous data. In this paper, we propose a novel iterative kNN imputation\ntechnique based on class weighted grey distance between the missing datum and\nall the training data. Grey distance works well in heterogeneous data with\nmissing instances. The distance is weighted by Mutual Information (MI) which is\na measure of feature relevance between the features and the class label. This\nensures that the imputation of the training data is directed towards improving\nclassification performance. This class weighted grey kNN imputation algorithm\ndemonstrates improved performance when compared to other kNN imputation\nalgorithms, as well as standard imputation algorithms such as MICE and\nmissForest, in imputation and classification problems. These problems are based\non simulated scenarios and UCI datasets with various rates of missingness.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 07:48:45 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Choudhury", "Arkopal", ""], ["Kosorok", "Michael R.", ""]]}, {"id": "2002.10878", "submitter": "Fatemeh Najibi", "authors": "Fatemeh Najibi, Dimitra Apostolopoulou, and Eduardo Alonso", "title": "Gaussian Process Regression for Probabilistic Short-term Solar Output\n  Forecast", "comments": null, "journal-ref": null, "doi": "10.1016/j.ijepes.2021.106916", "report-no": null, "categories": "stat.AP cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With increasing concerns of climate change, renewable resources such as\nphotovoltaic (PV) have gained popularity as a means of energy generation. The\nsmooth integration of such resources in power system operations is enabled by\naccurate forecasting mechanisms that address their inherent intermittency and\nvariability. This paper proposes a probabilistic framework to predict\nshort-term PV output taking into account the uncertainty of weather. To this\nend, we make use of datasets that comprise of power output and meteorological\ndata such as irradiance, temperature, zenith, and azimuth. First, we categorise\nthe data into four groups based on solar output and time by using k-means\nclustering. Next, a correlation study is performed to choose the weather\nfeatures which affect solar output to a greater extent. Finally, we determine a\nfunction that relates the aforementioned selected features with solar output by\nusing Gaussian Process Regression and Matern 5/2 as a kernel function. We\nvalidate our method with five solar generation plants in different locations\nand compare the results with existing methodologies. More specifically, in\norder to test the proposed model, two different methods are used: (i) 5-fold\ncross-validation; and (ii) holding out 30 random days as test data. To confirm\nthe model accuracy, we apply our framework 30 independent times on each of the\nfour clusters. The average error follows a normal distribution, and with 95%\nconfidence level, it takes values between -1.6% to 1.4%.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 15:54:37 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Najibi", "Fatemeh", ""], ["Apostolopoulou", "Dimitra", ""], ["Alonso", "Eduardo", ""]]}, {"id": "2002.10882", "submitter": "James Tee", "authors": "James Tee and Desmond P. Taylor", "title": "A Quantized Representation of Intertemporal Choice in the Brain", "comments": "9 pages, 19 figures. arXiv admin note: substantial text overlap with\n  arXiv:1805.01631", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Value [4][5] is typically modeled using a continuous representation (i.e., a\nReal number). A discrete representation of value has recently been postulated\n[6]. A quantized representation of probability in the brain was also posited\nand supported by experimental data [7]. Value and probability are inter-related\nvia Prospect Theory [4][5]. In this paper, we hypothesize that intertemporal\nchoices may also be quantized. For example, people may treat (or discount) 16\ndays indifferently to 17 days. To test this, we analyzed an intertemporal task\nby using 2 novel models: quantized hyperbolic discounting, and quantized\nexponential discounting. Our work here is a re-examination of the behavioral\ndata previously collected for an fMRI study [8]. Both quantized hyperbolic and\nquantized exponential models were compared using AIC and BIC tests. We found\nthat 13/20 participants were best fit to the quantized exponential model, while\nthe remaining 7/20 were best fit to the quantized hyperbolic model. Overall,\n15/20 participants were best fit to models with a 5-bit precision (i.e., 2^5 =\n32 steps). In conclusion, regardless of hyperbolic or exponential, quantized\nversions of these models are better fit to the experimental data than their\ncontinuous forms. We finally outline some potential applications of our\nfindings.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 03:24:08 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 00:40:49 GMT"}, {"version": "v3", "created": "Tue, 15 Sep 2020 22:54:03 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Tee", "James", ""], ["Taylor", "Desmond P.", ""]]}, {"id": "2002.10902", "submitter": "Owen Thomas", "authors": "Owen Thomas, Henri Pesonen, Jukka Corander", "title": "Probabilistic elicitation of expert knowledge through assessment of\n  computer simulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for probabilistic elicitation of expert knowledge\nusing binary responses of human experts assessing simulated data from a\nstatistical model, where the parameters are subject to uncertainty. The binary\nresponses describe either the absolute realism of individual simulations or the\nrelative realism of a pair of simulations in the two alternative versions of\nout approach. Each version provides a nonparametric representation of the\nexpert belief distribution over the values of a model parameter, without\ndemanding the assertion of any opinion on the parameter values themselves. Our\nframework also integrates the use of active learning to efficiently query the\nexperts, with the possibility to additionally provide a useful misspecification\ndiagnostic. We validate both methods on an automatic expert judging a binomial\ndistribution, and on human experts judging the distribution of voters across\npolitical parties in the United States and Norway. Both methods provide\nflexible and meaningful representations of the human experts' beliefs,\ncorrectly identifying the higher dispersion of voters between parties in\nNorway.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 14:42:48 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 13:28:23 GMT"}, {"version": "v3", "created": "Mon, 9 Mar 2020 14:18:20 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Thomas", "Owen", ""], ["Pesonen", "Henri", ""], ["Corander", "Jukka", ""]]}, {"id": "2002.10997", "submitter": "Sina Mews", "authors": "Sina Mews, Roland Langrock, Ruth King, Nicola Quick", "title": "Continuous-time multi-state capture-recapture models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-state capture-recapture data comprise individual-specific sighting\nhistories together with information on individuals' states related, for\nexample, to breeding status, infection level, or geographical location. Such\ndata are often analysed using the Arnason-Schwarz model, where transitions\nbetween states are modelled using a discrete-time Markov chain, making the\nmodel most easily applicable to regular time series. When time intervals\nbetween capture occasions are not of equal length, more complex time-dependent\nconstructions may be required, increasing the number of parameters to estimate,\ndecreasing interpretability, and potentially leading to reduced precision. Here\nwe develop a novel continuous-time multi-state model that can be regarded as an\nanalogue of the Arnason-Schwarz model for irregularly sampled data. Statistical\ninference is carried out by regarding the capture-recapture data as\nrealisations from a continuous-time hidden Markov model, which allows the\nassociated efficient algorithms to be used for maximum likelihood estimation\nand state decoding. To illustrate the feasibility of the modelling framework,\nwe use a long-term survey of bottlenose dolphins where capture occasion are not\nregularly spaced through time. Here we are particularly interested in seasonal\neffects on the movement rates of the dolphins along the Scottish east coast.\nThe results reveal seasonal movement patterns between two core areas of their\nrange, providing information that will inform conservation management.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 16:55:26 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Mews", "Sina", ""], ["Langrock", "Roland", ""], ["King", "Ruth", ""], ["Quick", "Nicola", ""]]}, {"id": "2002.11184", "submitter": "Aaron King", "authors": "Aaron A. King, Qianying Lin, Edward L. Ionides", "title": "The Sampled Moran Genealogy Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE math.PR stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We define the Sampled Moran Genealogy Process, a continuous-time Markov\nprocess on the space of genealogies with the demography of the classical Moran\nprocess, sampled through time. To do so, we begin by defining the Moran\nGenealogy Process using a novel representation. We then extend this process to\ninclude sampling through time. We derive exact conditional and marginal\nprobability distributions for the sampled process under a stationarity\nassumption, and an exact expression for the likelihood of any sequence of\ngenealogies it generates. This leads to some interesting observations pertinent\nto existing phylodynamic methods in the literature. Throughout, our proofs are\noriginal and make use of strictly forward-in-time calculations and are exact\nfor all population sizes and sampling processes.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 21:38:12 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2020 16:40:08 GMT"}, {"version": "v3", "created": "Mon, 19 Oct 2020 19:42:47 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["King", "Aaron A.", ""], ["Lin", "Qianying", ""], ["Ionides", "Edward L.", ""]]}, {"id": "2002.11215", "submitter": "Sarthak .", "authors": "Sarthak, Shikhar Shukla, Surya Prakash Tripathi", "title": "EmbPred30: Assessing 30-days Readmission for Diabetic Patients using\n  Categorical Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hospital readmission is a crucial healthcare quality measure that helps in\ndetermining the level of quality of care that a hospital offers to a patient\nand has proven to be immensely expensive. It is estimated that more than $25\nbillion are spent yearly due to readmission of diabetic patients in the USA.\nThis paper benchmarks existing models and proposes a new embedding based\nstate-of-the-art deep neural network(DNN). The model can identify whether a\nhospitalized diabetic patient will be readmitted within 30 days or not with an\naccuracy of 95.2% and Area Under the Receiver Operating Characteristics(AUROC)\nof 97.4% on data collected from 130 US hospitals between 1999-2008. The results\nare encouraging with patients having changes in medication while admitted\nhaving a high chance of getting readmitted. Identifying prospective patients\nfor readmission could help the hospital systems in improving their inpatient\ncare, thereby saving them from unnecessary expenditures.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 22:59:47 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Sarthak", "", ""], ["Shukla", "Shikhar", ""], ["Tripathi", "Surya Prakash", ""]]}, {"id": "2002.11228", "submitter": "Joel Dabrowski Dr", "authors": "Joel Janek Dabrowski and Ashfaqur Rahman and Daniel Edward Pagendam\n  and Andrew George", "title": "Enforcing Mean Reversion in State Space Models for Prawn Pond Water\n  Quality Forecasting", "comments": null, "journal-ref": "Computers and Electronics in Agriculture, Volume 168, 2020,\n  105120, ISSN 0168-1699", "doi": "10.1016/j.compag.2019.105120", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The contribution of this study is a novel approach to introduce mean\nreversion in multi-step-ahead forecasts of state-space models. This approach is\ndemonstrated in a prawn pond water quality forecasting application. The mean\nreversion constrains forecasts by gradually drawing them to an average of\npreviously observed dynamics. This corrects deviations in forecasts caused by\nirregularities such as chaotic, non-linear, and stochastic trends. The key\nfeatures of the approach include (1) it enforces mean reversion, (2) it\nprovides a means to model both short and long-term dynamics, (3) it is able to\napply mean reversion to select structural state-space components, and (4) it is\nsimple to implement. Our mean reversion approach is demonstrated on various\nstate-space models and compared with several time-series models on a prawn pond\nwater quality dataset. Results show that mean reversion reduces long-term\nforecast errors by over 60% to produce the most accurate models in the\ncomparison.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 00:13:53 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Dabrowski", "Joel Janek", ""], ["Rahman", "Ashfaqur", ""], ["Pagendam", "Daniel Edward", ""], ["George", "Andrew", ""]]}, {"id": "2002.11239", "submitter": "Sidney Resnick", "authors": "Ross A. Maller and Sidney I. Resnick", "title": "Extremes of Censored and Uncensored Lifetimes in Survival Data", "comments": "1 figure, 23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The i.i.d. censoring model for survival analysis assumes two independent\nsequences of i.i.d. positive random variables, $(T_i^*)_{1\\le i\\le n}$ and\n$(U_i)_{1\\le i\\le n}$. The data consists of observations on the random sequence\n$\\big(T_i=\\min(T_i^*,U_i)$ together with accompanying censor indicators. Values\nof $T_i$ with $T_i^*\\le U_i$ are said to be uncensored, those with $T_i^*> U_i$\nare censored. We assume that the distributions of the $T_i^*$ and $U_i$ are in\nthe domain of attraction of the Gumbel distribution and obtain the asymptotic\ndistributions, as sample size $n\\to\\infty$, of the maximum values of the\ncensored and uncensored lifetimes in the data, and of statistics related to\nthem. These enable us to examine questions concerning the possible existence of\ncured individuals in the population.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 00:51:35 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Maller", "Ross A.", ""], ["Resnick", "Sidney I.", ""]]}, {"id": "2002.11243", "submitter": "Roel Ceballos", "authors": "Rena Sandy H. Baculinao and Roel F. Ceballos", "title": "Correspondence Analysis between the Location and the Leading Causes of\n  Death in the United States", "comments": null, "journal-ref": "International Journal of Ecological Economics and Statistics,\n  41(1), 47-54, 2020", "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correspondence Analysis analyzes two-way or multi-way tables withe each row\nand column becoming a point ion a multidimensional graphical map called biplot.\nIt can be used to extract essential dimensions allowing simplification of the\ndata matrix. This study aims to measure the association between the location\nand the leading causes of death in the United States of America and to\ndetermine the location where a particular disease is highly associated. The\nresearch data consists of two variables with 510 data points. Results show that\nthere is a significant association between the location ad leading cause of\ndeath in the United States, and 61% of the variance in the model are explained\nby the first two dimensions.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 01:08:41 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Baculinao", "Rena Sandy H.", ""], ["Ceballos", "Roel F.", ""]]}, {"id": "2002.11431", "submitter": "Robert Free", "authors": "Robert C. Free", "title": "Simpler handling of clinical concepts in R with clinconcept", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Routinely collected data in electronic healthcare records are often\nunderpinned by clinical concept dictionaries. Increasingly data sets from these\nsources are being made available and used for research purposes, but without\nadditional tooling it can be difficult to work effectively with these\ndictionaries due to their design, size and complex nature. In an effort to\nimprove this situation the clinconcept package was created to provide a\nstraightforward way for researchers to build, manage and interrogate databases\ncontaining commmonly used clinical concept dictionaries. This article describes\nthe rationale behind the package, how to install it and use it and how it can\nbe extended to support other data sources.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 12:20:55 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Free", "Robert C.", ""]]}, {"id": "2002.11570", "submitter": "Matthew Deakin", "authors": "Matthew Deakin, Sarah Sheehy, David M. Greenwood, Sara Walker, Phil C.\n  Taylor", "title": "Calculations of System Adequacy Considering Heat Transition Pathways", "comments": "Accepted for presentation at PMAPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.SY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The decarbonisation of heat in developed economies represents a significant\nchallenge, with increased penetration of electrical heating technologies\npotentially leading to unprecedented increases in peak electricity demand. This\nwork considers a method to evaluate the impact of rapid electrification of heat\nby utilising historic gas demand data. The work is intended to provide a\ndata-driven complement to popular generative heat demand models, with a\nparticular aim of informing regulators and actors in capacity markets as to how\npolicy changes could impact on medium-term system adequacy metrics (up to five\nyears ahead). Results from a GB case study show that the representation of heat\ndemand using scaled gas demand profiles increases the rate at which 1-in-20\nsystem peaks grow by 60\\%, when compared to the use of scaled electricity\ndemand profiles. Low end-use system efficiency, in terms of aggregate\ncoefficient of performance and demand side response capabilities, are shown to\npotentially lead to a doubling of electrical demand-temperature sensitivity\nfollowing five years of heat demand growth.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 15:41:37 GMT"}, {"version": "v2", "created": "Sun, 24 May 2020 07:44:17 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Deakin", "Matthew", ""], ["Sheehy", "Sarah", ""], ["Greenwood", "David M.", ""], ["Walker", "Sara", ""], ["Taylor", "Phil C.", ""]]}, {"id": "2002.11765", "submitter": "Lucas B\\\"ottcher", "authors": "Lucas B\\\"ottcher, Nino Antulov-Fantulin", "title": "Unifying continuous, discrete, and hybrid susceptible-infected-recovered\n  processes on networks", "comments": "14 pages, 10 figures", "journal-ref": "Phys. Rev. Research 2, 033121 (2020)", "doi": "10.1103/PhysRevResearch.2.033121", "report-no": null, "categories": "physics.soc-ph cond-mat.stat-mech cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Waiting times between two consecutive infection and recovery events in\nspreading processes are often assumed to be exponentially distributed, which\nresults in Markovian (i.e., memoryless) continuous spreading dynamics. However,\nthis is not taking into account memory (correlation) effects and discrete\ninteractions that have been identified as relevant in social, transportation,\nand disease dynamics. We introduce a framework to model continuous, discrete,\nand hybrid forms of (non-)Markovian susceptible-infected-recovered (SIR)\nstochastic processes on networks. The hybrid SIR processes that we study in\nthis paper describe infections as discrete-time Markovian and recovery events\nas continuous-time non-Markovian processes, which mimic the distribution of\ncell cycles. Our results suggest that the effective-infection-rate description\nof epidemic processes fails to uniquely capture the behavior of such hybrid and\nalso general non-Markovian disease dynamics. Providing a unifying description\nof general Markovian and non-Markovian disease outbreaks, we instead show that\nthe mean transmissibility produces the same phase diagrams independent of the\nunderlying inter-event-time distributions.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 19:50:58 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 20:41:58 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["B\u00f6ttcher", "Lucas", ""], ["Antulov-Fantulin", "Nino", ""]]}, {"id": "2002.11767", "submitter": "Johanna Hardin", "authors": "Albert Y. Kim and Johanna Hardin", "title": "\"Playing the whole game\": A data collection and analysis exercise with\n  Google Calendar", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We provide a computational exercise suitable for early introduction in an\nundergraduate statistics or data science course that allows students to 'play\nthe whole game' of data science: performing both data collection and data\nanalysis. While many teaching resources exist for data analysis, such resources\nare not as abundant for data collection given the inherent difficulty of the\ntask. Our proposed exercise centers around student use of Google Calendar to\ncollect data with the goal of answering the question 'How do I spend my time?'\nOn the one hand, the exercise involves answering a question with near universal\nappeal, but on the other hand, the data collection mechanism is not beyond the\nreach of a typical undergraduate student. A further benefit of the exercise is\nthat it provides an opportunity for discussions on ethical questions and\nconsiderations that data providers and data analysts face in today's age of\nlarge-scale internet-based data collection.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 05:27:26 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 19:14:16 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Kim", "Albert Y.", ""], ["Hardin", "Johanna", ""]]}, {"id": "2002.11916", "submitter": "Shih-Ting Huang", "authors": "Shih-Ting Huang, Fang Xie, and Johannes Lederer", "title": "Tuning-free ridge estimators for high-dimensional generalized linear\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ridge estimators regularize the squared Euclidean lengths of parameters. Such\nestimators are mathematically and computationally attractive but involve tuning\nparameters that can be difficult to calibrate. In this paper, we show that\nridge estimators can be modified such that tuning parameters can be avoided\naltogether. We also show that these modified versions can improve on the\nempirical prediction accuracies of standard ridge estimators combined with\ncross-validation, and we provide first theoretical guarantees.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 05:01:42 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Huang", "Shih-Ting", ""], ["Xie", "Fang", ""], ["Lederer", "Johannes", ""]]}, {"id": "2002.11940", "submitter": "Chaochao Chen", "authors": "Chaochao Chen, Ziqi Liu, Jun Zhou, Xiaolong Li, Yuan Qi, Yujing Jiao,\n  and Xingyu Zhong", "title": "How Much Can A Retailer Sell? Sales Forecasting on Tmall", "comments": "Accepted by PAKDD'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-series forecasting is an important task in both academic and industry,\nwhich can be applied to solve many real forecasting problems like stock,\nwater-supply, and sales predictions. In this paper, we study the case of\nretailers' sales forecasting on Tmall|the world's leading online B2C platform.\nBy analyzing the data, we have two main observations, i.e., sales seasonality\nafter we group different groups of retails and a Tweedie distribution after we\ntransform the sales (target to forecast). Based on our observations, we design\ntwo mechanisms for sales forecasting, i.e., seasonality extraction and\ndistribution transformation. First, we adopt Fourier decomposition to\nautomatically extract the seasonalities for different categories of retailers,\nwhich can further be used as additional features for any established regression\nalgorithms. Second, we propose to optimize the Tweedie loss of sales after\nlogarithmic transformations. We apply these two mechanisms to classic\nregression models, i.e., neural network and Gradient Boosting Decision Tree,\nand the experimental results on Tmall dataset show that both mechanisms can\nsignificantly improve the forecasting results.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 06:41:00 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Chen", "Chaochao", ""], ["Liu", "Ziqi", ""], ["Zhou", "Jun", ""], ["Li", "Xiaolong", ""], ["Qi", "Yuan", ""], ["Jiao", "Yujing", ""], ["Zhong", "Xingyu", ""]]}, {"id": "2002.11989", "submitter": "Alessandra Mattei", "authors": "Alessandra Mattei and Fabrizia Mealli and Peng Ding", "title": "Assessing causal effects in the presence of treatment switching through\n  principal stratification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical trials focusing on survival outcomes often allow patients in the\ncontrol arm to switch to the treatment arm if their physical conditions are\nworse than certain tolerance levels. The Intention-To-Treat analysis provides\nvalid causal estimates of the effect of assignment, but it does not measure the\neffect of the actual receipt of the treatment and ignores the information of\ntreatment switching. Other existing methods propose to reconstruct the outcome\na unit would have had if s/he had not switched under strong assumptions. We\npropose to re-define the problem of treatment switching using principal\nstratification focusing on principal causal effects for patients belonging to\nsubpopulations defined by the switching behavior under control. We use a\nBayesian approach to inference taking into account that (i) switching happens\nin continuous time generating infinitely many principal strata; (ii) switching\ntime is not defined for units who never switch in a particular experiment; and\n(iii) survival time and switching time are subject to censoring. We illustrate\nour framework using a synthetic dataset based on the Concorde study, a\nrandomized controlled trial aimed to assess causal effects on time-to-disease\nprogression or death of immediate versus deferred treatment with zidovudine\namong patients with asymptomatic HIV infection.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 09:25:29 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Mattei", "Alessandra", ""], ["Mealli", "Fabrizia", ""], ["Ding", "Peng", ""]]}, {"id": "2002.12318", "submitter": "Thomas Opitz", "authors": "Thomas Opitz, Florent Bonneu, Edith Gabriel", "title": "Point-process based Bayesian modeling of space-time structures of forest\n  fire occurrences in Mediterranean France", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to climate change and human activity, wildfires are expected to become\nmore frequent and extreme worldwide, causing economic and ecological disasters.\nThe deployment of preventive measures and operational forecasts can be aided by\nstochastic modeling that helps to understand and quantify the mechanisms\ngoverning the occurrence intensity. We here develop a point process framework\nfor wildfire ignition points observed in the French Mediterranean basin since\n1995, and we fit a spatio-temporal log-Gaussian Cox process with monthly\ntemporal resolution in a Bayesian framework using the integrated nested Laplace\napproximation (INLA). Human activity is the main direct cause of wildfires and\nis indirectly measured through a number of appropriately defined proxies\nrelated to land-use covariates (urbanization, road network) in our approach,\nand we further integrate covariates of climatic and environmental conditions to\nexplain wildfire occurrences. We include spatial random effects with Mat\\'ern\ncovariance and temporal autoregression at yearly resolution. Two major\nmethodological challenges are tackled: first, handling and unifying multi-scale\nstructures in data is achieved through computer-intensive preprocessing steps\nwith GIS software and kriging techniques; second, INLA-based estimation with\nhigh-dimensional response vectors and latent models is facilitated through\nintra-year subsampling, taking into account the occurrence structure of\nwildfires.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 10:41:40 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Opitz", "Thomas", ""], ["Bonneu", "Florent", ""], ["Gabriel", "Edith", ""]]}, {"id": "2002.12632", "submitter": "Ilia Derevitskii", "authors": "Ilya V. Derevitskii, Daria A. Savitskaya, Alina Y. Babenko, Sergey V.\n  Kovalchuk", "title": "The Atrial Fibrillation Risk Score for Hyperthyroidism Patients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thyrotoxicosis (TT) is associated with an increase in both total and\ncardiovascu-lar mortality. One of the main thyrotoxicosis risks is Atrial\nFibrillation (AF). Right AF predicts help medical personal prescribe the\ncorrect medicaments and correct surgical or radioiodine therapy. The main goal\nof this study is creating a method for practical treatment and diagnostic AF.\nThis study proposes a new method for assessing the risk of occurrence atrial\nfibrillation for patients with TT. This method considers both the features of\nthe complication and the specifics of the chronic disease. A model is created\nbased on case histories of patients with thyrotoxicosis. We used Machine\nLearning methods for creating several models. Each model has advantages and\ndisadvantages depending on the diagnostic and medical purposes. The resulting\nmodels show high results in the different metrics of the prediction of AF.\nThese models interpreted and simple for use. Therefore, models can be used as\npart of the support and decision-making system (DSS) by medical specialists in\nthe treatment and diagnostic of AF.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 10:23:46 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Derevitskii", "Ilya V.", ""], ["Savitskaya", "Daria A.", ""], ["Babenko", "Alina Y.", ""], ["Kovalchuk", "Sergey V.", ""]]}, {"id": "2002.12721", "submitter": "Rupam Acharyya", "authors": "Ankani Chattoraj, Rupam Acharyya, Sabyasachi Shivkumar, Md Iftekar\n  Tanveer, Mohammad Rafayet Ali", "title": "To be or not to be? A spatial predictive crime model for Rochester", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This project uses a spatial model (Geographically Weighted Regression) to\nrelate various physical and social features to crime rates. Besides making\ninteresting predictions from basic data statistics, the trained model can be\nused to predict on the test data. The high accuracy of this prediction on test\ndata then allows us to make predictions of crime probabilities in different\nareas based on the location, the population, the property rate, the time of the\nday/year and so on. This then further gives us the idea that an application can\nbe built to help people traveling around Rochester be aware when and if they\nenter crime prone area.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 06:34:42 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Chattoraj", "Ankani", ""], ["Acharyya", "Rupam", ""], ["Shivkumar", "Sabyasachi", ""], ["Tanveer", "Md Iftekar", ""], ["Ali", "Mohammad Rafayet", ""]]}]