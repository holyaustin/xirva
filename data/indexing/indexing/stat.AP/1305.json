[{"id": "1305.0060", "submitter": "Gregory Ely", "authors": "Gregory Ely, Shuchin Aeron", "title": "Complexity penalized hydraulic fracture localization and moment tensor\n  estimation under limited model information", "comments": null, "journal-ref": null, "doi": "10.1121/1.4806001", "report-no": null, "categories": "physics.geo-ph cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a novel technique for micro-seismic localization\nusing a group sparse penalization that is robust to the focal mechanism of the\nsource and requires only a velocity model of the stratigraphy rather than a\nfull Green's function model of the earth's response. In this technique we\nconstruct a set of perfect delta detector responses, one for each detector in\nthe array, to a seismic event at a given location and impose a group sparsity\nacross the array. This scheme is independent of the moment tensor and exploits\nthe time compactness of the incident seismic signal. Furthermore we present a\nmethod for improving the inversion of the moment tensor and Green's function\nwhen the geometry of seismic array is limited. In particular we demonstrate\nthat both Tikhonov regularization and truncated SVD can improve the recovery of\nthe moment tensor and be robust to noise. We evaluate our algorithm on\nsynthetic data and present error bounds for both estimation of the moment\ntensor as well as localization. Furthermore we discuss the estimated moment\ntensor accuracy as a function of both array geometry and fault orientation.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2013 00:11:14 GMT"}, {"version": "v2", "created": "Thu, 9 May 2013 18:24:33 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Ely", "Gregory", ""], ["Aeron", "Shuchin", ""]]}, {"id": "1305.0247", "submitter": "Jelena Fiosina", "authors": "Helen Afanasyeva", "title": "The Estimation of Transport Logistic Processes Models on the Base of\n  Intensive Computer Methods of Statistics", "comments": "Summary of the promotion work, Riga: Transport and communication\n  Institute, 2006. -48 p. arXiv admin note: text overlap with arXiv:1304.6670\n  by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The promotion work 'The Estimation of Transport Logistic Processes Models on\nthe Base of Intensive Computer Methods of Statistics' has been worked out by\nHelen Afanasyeva to obtain the scientific degree of 'Doctor of Science\nEngineering in Telematics and Logistics'. Scientific supervisor of the work is\nDr.habil.sc.ing., professor Alexander Andronov. The work is devoted to the\nimplementation of the modern statistical methods for the transport and logistic\nmodels analysis. The intensive computer method resampling is especially\nattended. This method is non-parametrical and gives the most efficient\nestimators of the systems' characteristics in the case of small initial sample\nsizes. The investigation was held in three main directions: the forecasting and\nestimation of logistic models, the estimation of the reliability and efficiency\nof carries, and the analysis of inventory control problems in logistic systems.\nResampling method usage algorithms and inferences for formulas of the\nefficiency comparison of traditional and resampling approaches were suggested\nfor each task implementing corresponding mathematical model. The efficiency\ncriteria were: bias, variance or mean squared error of the estimator. The\nnumerical results proving the efficiency of the suggested approach were\nobtained. The conclusions and recommendations, concerning the conditions in\nwhich the suggested approach is the most effective were made. The obtained\nresults are general, because can be used in other subject areas.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2013 19:15:15 GMT"}, {"version": "v2", "created": "Sat, 18 May 2013 15:17:26 GMT"}], "update_date": "2013-05-21", "authors_parsed": [["Afanasyeva", "Helen", ""]]}, {"id": "1305.0709", "submitter": "Andrea Rau", "authors": "Gr\\'egory Nuel and Andrea Rau and Florence Jaffr\\'ezic", "title": "Joint likelihood calculation for intervention and observational data\n  from a Gaussian Bayesian network", "comments": "technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methodological development for the inference of gene regulatory networks from\ntranscriptomic data is an active and important research area. Several\napproaches have been proposed to infer relationships among genes from\nobservational steady-state expression data alone, mainly based on the use of\ngraphical Gaussian models. However, these methods rely on the estimation of\npartial correlations and are only able to provide undirected graphs that cannot\nhighlight causal relationships among genes. A major upcoming challenge is to\njointly analyze observational transcriptomic data and intervention data\nobtained by performing knock-out or knock-down experiments in order to uncover\ncausal gene regulatory relationships. To this end, in this technical note we\npresent an explicit formula for the likelihood function for any complex\nintervention design in the context of Gaussian Bayesian networks, as well as\nits analytical maximization. This allows a direct calculation of the causal\neffects for known graph structure. We also show how to obtain the Fisher\ninformation in this context, which will be extremely useful for the choice of\noptimal intervention designs in the future.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2013 14:03:14 GMT"}, {"version": "v2", "created": "Tue, 7 May 2013 07:13:47 GMT"}, {"version": "v3", "created": "Wed, 26 Jun 2013 17:19:55 GMT"}, {"version": "v4", "created": "Thu, 27 Jun 2013 08:49:59 GMT"}], "update_date": "2013-06-28", "authors_parsed": [["Nuel", "Gr\u00e9gory", ""], ["Rau", "Andrea", ""], ["Jaffr\u00e9zic", "Florence", ""]]}, {"id": "1305.0979", "submitter": "Raymond K. W. Wong", "authors": "Raymond K. W. Wong, Paul Baines, Alexander Aue, Thomas C. M. Lee,\n  Vinay L. Kashyap", "title": "Automatic estimation of flux distributions of astrophysical source\n  populations", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS750 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 3, 1690-1712", "doi": "10.1214/14-AOAS750", "report-no": "IMS-AOAS-AOAS750", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In astrophysics a common goal is to infer the flux distribution of\npopulations of scientifically interesting objects such as pulsars or\nsupernovae. In practice, inference for the flux distribution is often conducted\nusing the cumulative distribution of the number of sources detected at a given\nsensitivity. The resulting \"$\\log(N>S)$-$\\log (S)$\" relationship can be used to\ncompare and evaluate theoretical models for source populations and their\nevolution. Under restrictive assumptions the relationship should be linear. In\npractice, however, when simple theoretical models fail, it is common for\nastrophysicists to use prespecified piecewise linear models. This paper\nproposes a methodology for estimating both the number and locations of\n\"breakpoints\" in astrophysical source populations that extends beyond existing\nwork in this field. An important component of the proposed methodology is a new\ninterwoven EM algorithm that computes parameter estimates. It is shown that in\nsimple settings such estimates are asymptotically consistent despite the\ncomplex nature of the parameter space. Through simulation studies it is\ndemonstrated that the proposed methodology is capable of accurately detecting\nstructural breaks in a variety of parameter configurations. This paper\nconcludes with an application of our methodology to the Chandra Deep Field\nNorth (CDFN) data set.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2013 02:13:45 GMT"}, {"version": "v2", "created": "Mon, 24 Nov 2014 10:47:15 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Wong", "Raymond K. W.", ""], ["Baines", "Paul", ""], ["Aue", "Alexander", ""], ["Lee", "Thomas C. M.", ""], ["Kashyap", "Vinay L.", ""]]}, {"id": "1305.1385", "submitter": "Yang Feng", "authors": "Weiping Ma, Yang Feng, Kani Chen, Zhiliang Ying", "title": "Functional and Parametric Estimation in a Semi- and Nonparametric Model\n  with Application to Mass-Spectrometry Data", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by modeling and analysis of mass-spectrometry data, a semi- and\nnonparametric model is proposed that consists of a linear parametric component\nfor individual location and scale and a nonparametric regression function for\nthe common shape. A multi-step approach is developed that simultaneously\nestimates the parametric components and the nonparametric function. Under\ncertain regularity conditions, it is shown that the resulting estimators is\nconsistent and asymptotic normal for the parametric part and achieve the\noptimal rate of convergence for the nonparametric part when the bandwidth is\nsuitably chosen. Simulation results are presented to demonstrate the\neffectiveness and finite-sample performance of the method. The method is also\napplied to a SELDI-TOF mass spectrometry data set from a study of liver cancer\npatients.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2013 02:30:58 GMT"}], "update_date": "2013-05-08", "authors_parsed": [["Ma", "Weiping", ""], ["Feng", "Yang", ""], ["Chen", "Kani", ""], ["Ying", "Zhiliang", ""]]}, {"id": "1305.1980", "submitter": "Vasanthan Raghavan", "authors": "Vasanthan Raghavan, Greg Ver Steeg, Aram Galstyan, Alexander G.\n  Tartakovsky", "title": "Modeling Temporal Activity Patterns in Dynamic Social Networks", "comments": "23 pages, 7 figures, 13 tables, submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.SI physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The focus of this work is on developing probabilistic models for user\nactivity in social networks by incorporating the social network influence as\nperceived by the user. For this, we propose a coupled Hidden Markov Model,\nwhere each user's activity evolves according to a Markov chain with a hidden\nstate that is influenced by the collective activity of the friends of the user.\nWe develop generalized Baum-Welch and Viterbi algorithms for model parameter\nlearning and state estimation for the proposed framework. We then validate the\nproposed model using a significant corpus of user activity on Twitter. Our\nnumerical studies show that with sufficient observations to ensure accurate\nmodel learning, the proposed framework explains the observed data better than\neither a renewal process-based model or a conventional uncoupled Hidden Markov\nModel. We also demonstrate the utility of the proposed approach in predicting\nthe time to the next tweet. Finally, clustering in the model parameter space is\nshown to result in distinct natural clusters of users characterized by the\ninteraction dynamic between a user and his network.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2013 00:30:49 GMT"}], "update_date": "2013-05-10", "authors_parsed": [["Raghavan", "Vasanthan", ""], ["Steeg", "Greg Ver", ""], ["Galstyan", "Aram", ""], ["Tartakovsky", "Alexander G.", ""]]}, {"id": "1305.1985", "submitter": "Jarrett Byrnes", "authors": "Jarrett E. K. Byrnes, Lars Gamfeldt, Forest Isbell, Jonathan S.\n  Lefcheck, John N. Griffin, Andrew Hector, Bradley J. Cardinale, David U.\n  Hooper, Laura E. Dee, J. Emmett Duffy", "title": "Investigating the relationship between biodiversity and ecosystem\n  multifunctionality: Challenges and solutions", "comments": "This article has been submitted to Methods in Ecology & Evolution for\n  review", "journal-ref": null, "doi": "10.1111/2041-210X.12143", "report-no": null, "categories": "q-bio.QM q-bio.PE stat.AP", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Extensive research shows that more species-rich assemblages are generally\nmore productive and efficient in resource use than comparable assemblages with\nfewer species. But the question of how diversity simultaneously affects the\nwide variety of ecological functions that ecosystems perform remains relatively\nunderstudied, and it presents several analytical and empirical challenges that\nremain unresolved. In particular, researchers have developed several disparate\nmetrics to quantify multifunctionality, each characterizing different aspects\nof the concept, and each with pros and cons. We compare four approaches to\ncharacterizing multifunctionality and its dependence on biodiversity,\nquantifying 1) magnitudes of multiple individual functions separately, 2) the\nextent to which different species promote different functions, 3) the average\nlevel of a suite of functions, and 4) the number of functions that\nsimultaneously exceed a critical threshold. We illustrate each approach using\ndata from the pan-European BIODEPTH experiment and the R multifunc package\ndeveloped for this purpose, evaluate the strengths and weaknesses of each\napproach, and implement several methodological improvements. We conclude that a\nextension of the fourth approach that systematically explores all possible\nthreshold values provides the most comprehensive description of\nmultifunctionality to date. We outline this method and recommend its use in\nfuture research.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2013 01:08:37 GMT"}], "update_date": "2014-01-28", "authors_parsed": [["Byrnes", "Jarrett E. K.", ""], ["Gamfeldt", "Lars", ""], ["Isbell", "Forest", ""], ["Lefcheck", "Jonathan S.", ""], ["Griffin", "John N.", ""], ["Hector", "Andrew", ""], ["Cardinale", "Bradley J.", ""], ["Hooper", "David U.", ""], ["Dee", "Laura E.", ""], ["Duffy", "J. Emmett", ""]]}, {"id": "1305.2026", "submitter": "Thordis Thorarinsdottir", "authors": "Sebastian Lerch and Thordis L. Thorarinsdottir", "title": "Comparison of nonhomogeneous regression models for probabilistic wind\n  speed forecasting", "comments": null, "journal-ref": "Tellus A 2013, 65, 21206", "doi": "10.3402/tellusa.v65i0.21206", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In weather forecasting, nonhomogeneous regression is used to statistically\npostprocess forecast ensembles in order to obtain calibrated predictive\ndistributions. For wind speed forecasts, the regression model is given by a\ntruncated normal distribution where location and spread are derived from the\nensemble. This paper proposes two alternative approaches which utilize the\ngeneralized extreme value (GEV) distribution. A direct alternative to the\ntruncated normal regression is to apply a predictive distribution from the GEV\nfamily, while a regime switching approach based on the median of the forecast\nensemble incorporates both distributions. In a case study on daily maximum wind\nspeed over Germany with the forecast ensemble from the European Centre for\nMedium-Range Weather Forecasts, all three approaches provide calibrated and\nsharp predictive distributions with the regime switching approach showing the\nhighest skill in the upper tail.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2013 08:04:33 GMT"}], "update_date": "2013-11-19", "authors_parsed": [["Lerch", "Sebastian", ""], ["Thorarinsdottir", "Thordis L.", ""]]}, {"id": "1305.2169", "submitter": "Gregory Ely", "authors": "Gregory Ely, Shuchin Aeron", "title": "Robust Hydraulic Fracture Monitoring (HFM) of Multiple Time Overlapping\n  Events Using a Generalized Discrete Radon Transform", "comments": null, "journal-ref": null, "doi": "10.1109/IGARSS.2012.6351517", "report-no": null, "categories": "physics.geo-ph cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a novel algorithm for multiple-event localization for\nHydraulic Fracture Monitoring (HFM) through the exploitation of the sparsity of\nthe observed seismic signal when represented in a basis consisting of space\ntime propagators. We provide explicit construction of these propagators using a\nforward model for wave propagation which depends non-linearly on the problem\nparameters - the unknown source location and mechanism of fracture, time and\nextent of event, and the locations of the receivers. Under fairly general\nassumptions and an appropriate discretization of these parameters we first\nbuild an over-complete dictionary of generalized Radon propagators and assume\nthat the data is well represented as a linear superposition of these\npropagators. Exploiting this structure we propose sparsity penalized algorithms\nand workflow for super-resolution extraction of time overlapping multiple\nseismic events from single well data.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2013 18:23:29 GMT"}], "update_date": "2013-05-13", "authors_parsed": [["Ely", "Gregory", ""], ["Aeron", "Shuchin", ""]]}, {"id": "1305.2170", "submitter": "Gregory Ely", "authors": "Gregory Ely, Shuchin Aeron, Eric L. Miller", "title": "Exploiting Structural Complexity for Robust and Rapid Hyperspectral\n  Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents several strategies for spectral de-noising of\nhyperspectral images and hypercube reconstruction from a limited number of\ntomographic measurements. In particular we show that the non-noisy spectral\ndata, when stacked across the spectral dimension, exhibits low-rank. On the\nother hand, under the same representation, the spectral noise exhibits a banded\nstructure. Motivated by this we show that the de-noised spectral data and the\nunknown spectral noise and the respective bands can be simultaneously estimated\nthrough the use of a low-rank and simultaneous sparse minimization operation\nwithout prior knowledge of the noisy bands. This result is novel for for\nhyperspectral imaging applications. In addition, we show that imaging for the\nComputed Tomography Imaging Systems (CTIS) can be improved under limited angle\ntomography by using low-rank penalization. For both of these cases we exploit\nthe recent results in the theory of low-rank matrix completion using nuclear\nnorm minimization.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2013 18:24:20 GMT"}], "update_date": "2013-05-10", "authors_parsed": [["Ely", "Gregory", ""], ["Aeron", "Shuchin", ""], ["Miller", "Eric L.", ""]]}, {"id": "1305.2378", "submitter": "Krzysztof Bartoszek", "authors": "Krzysztof Bartoszek", "title": "Quantifying the effects of anagenetic and cladogenetic evolution", "comments": null, "journal-ref": "Mathematical Biosciences 2014, 254, 42-57", "doi": null, "report-no": null, "categories": "q-bio.PE math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An ongoing debate in evolutionary biology is whether phenotypic change occurs\npredominantly around the time of speciation or whether it instead accumulates\ngradually over time. In this work I propose a general framework incorporating\nboth types of change, quantify the effects of speciational change via the\ncorrelation between species and attribute the proportion of change to each\ntype. I discuss results of parameter estimation of Hominoid body size in this\nlight. I derive mathematical formulae related to this problem, the probability\ngenerating functions of the number of speciation events along a randomly drawn\nlineage and from the most recent common ancestor of two randomly chosen tip\nspecies for a conditioned Yule tree. Additionally I obtain in closed form the\nvariance of the distance from the root to the most recent common ancestor of\ntwo randomly chosen tip species.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2013 15:58:46 GMT"}, {"version": "v2", "created": "Mon, 13 May 2013 14:53:49 GMT"}, {"version": "v3", "created": "Mon, 26 May 2014 17:23:06 GMT"}], "update_date": "2014-07-02", "authors_parsed": [["Bartoszek", "Krzysztof", ""]]}, {"id": "1305.2565", "submitter": "Piyush Tagade", "authors": "Piyush M. Tagade, Byeong-Min Jeong, Han-Lim Choi", "title": "A Gaussian Process Emulator Approach for Rapid Contaminant\n  Characterization with an Integrated Multizone-CFD Model", "comments": "The paper is submitted to the journal \"Building and Environment\" for\n  possible publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores a Gaussian process emulator based approach for rapid\nBayesian inference of contaminant source location and characteristics in an\nindoor environment. In the pre-event detection stage, the proposed approach\nrepresents transient contaminant fate and transport as a random function with\nmultivariate Gaussian process prior. Hyper-parameters of the Gaussian process\nprior are inferred using a set of contaminant fate and transport simulation\nruns obtained at predefined source locations and characteristics. This paper\nuses an integrated multizone-CFD model to simulate contaminant fate and\ntransport. Mean of the Gaussian process, conditional on the inferred\nhyper-parameters, is used as an computationally efficient statistical emulator\nof the multizone-CFD simulator. In the post event-detection stage, the Bayesian\nframework is used to infer the source location and characteristics using the\ncontaminant concentration data obtained through a sensor network. The Gaussian\nprocess emulator of the contaminant fate and transport is used for Markov Chain\nMonte Carlo sampling to efficiently explore the posterior distribution of\nsource location and characteristics. Efficacy of the proposed method is\ndemonstrated for a hypothetical contaminant release through multiple sources in\na single storey seven room building. The method is found to infer location and\ncharacteristics of the multiple sources accurately. The posterior distribution\nobtained using the proposed method is found to agree closely with the posterior\ndistribution obtained by directly coupling the multizone-CFD simulator with the\nMarkov Chain Monte Carlo sampling.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2013 07:17:02 GMT"}, {"version": "v2", "created": "Tue, 14 May 2013 09:32:15 GMT"}], "update_date": "2013-05-15", "authors_parsed": [["Tagade", "Piyush M.", ""], ["Jeong", "Byeong-Min", ""], ["Choi", "Han-Lim", ""]]}, {"id": "1305.2788", "submitter": "Fabian Pedregosa", "authors": "Fabian Pedregosa (INRIA Paris - Rocquencourt, INRIA Saclay - Ile de\n  France), Michael Eickenberg (INRIA Saclay - Ile de France, LNAO), Bertrand\n  Thirion (INRIA Saclay - Ile de France, LNAO), Alexandre Gramfort (LTCI)", "title": "HRF estimation improves sensitivity of fMRI encoding and decoding models", "comments": "3nd International Workshop on Pattern Recognition in NeuroImaging\n  (2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting activation patterns from functional Magnetic Resonance Images\n(fMRI) datasets remains challenging in rapid-event designs due to the inherent\ndelay of blood oxygen level-dependent (BOLD) signal. The general linear model\n(GLM) allows to estimate the activation from a design matrix and a fixed\nhemodynamic response function (HRF). However, the HRF is known to vary\nsubstantially between subjects and brain regions. In this paper, we propose a\nmodel for jointly estimating the hemodynamic response function (HRF) and the\nactivation patterns via a low-rank representation of task effects.This model is\nbased on the linearity assumption behind the GLM and can be computed using\nstandard gradient-based solvers. We use the activation patterns computed by our\nmodel as input data for encoding and decoding studies and report performance\nimprovement in both settings.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2013 14:19:24 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Pedregosa", "Fabian", "", "INRIA Paris - Rocquencourt, INRIA Saclay - Ile de\n  France"], ["Eickenberg", "Michael", "", "INRIA Saclay - Ile de France, LNAO"], ["Thirion", "Bertrand", "", "INRIA Saclay - Ile de France, LNAO"], ["Gramfort", "Alexandre", "", "LTCI"]]}, {"id": "1305.2815", "submitter": "Jon Forster", "authors": "Jonathan J. Forster, Agus Sudjianto", "title": "Modelling time and vintage variability in retail credit portfolios: the\n  decomposition approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of modelling historical data on retail\ncredit portfolio performance, with a view to forecasting future performance,\nand facilitating strategic decision making. We consider a situation, common in\npractice, where accounts with common origination date (typically month) are\naggregated into a single vintage for analysis, and the data for analysis\nconsists of a time series of a univariate portfolio performance variable (for\nexample, the proportion of defaulting accounts) for each vintage over\nsuccessive time periods since origination. An invaluable management tool for\nunderstanding portfolio behaviour can be obtained by decomposing the data\nseries nonparametrically into components of exogenous variability (E), maturity\n(time since origination; M) and vintage (V), referred to as an EMV model. For\nexample, identification of a good macroeconomic model is the key to effective\nforecasting, particularly in applications such as stress testing, and\nidentification of this can be facilitated by investigation of the macroeconomic\ncomponent of an EMV decomposition. We show that care needs to be taken with\nsuch a decomposition, drawing parallels with the Age-Period-Cohort approach,\ncommon in demography, epidemiology and sociology. We develop a practical\ndecomposition strategy, and illustrate our approach using data extracted from a\ncredit card portfolio.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2013 15:46:59 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Forster", "Jonathan J.", ""], ["Sudjianto", "Agus", ""]]}, {"id": "1305.2824", "submitter": "Gerard Keogh Dr.", "authors": "Gerard Keogh", "title": "The Statistical and Econometric Analysis of Asylum Application Trends\n  and their relationship to GDP in the EEA", "comments": "23 pages, 3 figures, 2 tables, 5 appendix tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sharp decline in Ireland's economic performance in recent years has\ncoincided with a recent fall in asylum applications. Simultaneously countries\nsuch as Switzerland are seeing increases in asylum numbers with evidence for\ngreater numbers of Nigerian applicants, a group that have for some time been\nthe largest nationality group applying in Ireland. A possible reason for this\nshift in asylum seeker preference is the general economic conditions here\nversus those in other European countries. In this paper we investigate whether\nthis belief holds water. We model asylum applications as a function of GDP\nusing a time varying parameter multiplicative growth model. Our results show\nthere is an economic basis for asylum seeker preferences. We further show there\nis no regional basis for asylum seekers' expectation of a more favourable claim\nin the 'developed box' in central Europe as compared to countries on the\nso-called 'periphery'.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2013 16:11:30 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Keogh", "Gerard", ""]]}, {"id": "1305.3244", "submitter": "Edward Aboufadel", "authors": "Edward Aboufadel", "title": "A Scoring System for Continuous Glucose Monitor Data", "comments": "12 pages, 10 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As continuous glucose monitors (CGMs) are used increasingly by diabetic\npatients, new and intuitive tools are needed to help patients and their\nphysicians use these streams of data to improve blood glucose management. In\nthis paper, we propose a daily CGM Score which can be calculated from CGM data.\nThe calculation involves assigning grades and scores to 80-minute periods of\nCGM data, and then aggregating the results. Scores for an individual patient,\nor among a set of patients, can then be compared and contrasted, and\nlongitudinal studies of CGM data can also be accomplished.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2013 13:04:18 GMT"}], "update_date": "2013-05-15", "authors_parsed": [["Aboufadel", "Edward", ""]]}, {"id": "1305.3544", "submitter": "Florian Hartig", "authors": "Florian Hartig and Carsten F. Dormann", "title": "Does \"model-free\" forecasting really outperform the \"true\" model? A\n  reply to Perretti et al", "comments": "Letter submitted to PNAS, with additional supplementary information.\n  R code included in the latex source", "journal-ref": "Proceedings of the National Academy of Sciences, 110, E3975, 2013", "doi": "10.1073/pnas.1308603110", "report-no": null, "categories": "q-bio.PE nlin.CD stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating population models from uncertain observations is an important\nproblem in ecology. Perretti et al. observed that standard Bayesian state-space\nsolutions to this problem may provide biased parameter estimates when the\nunderlying dynamics are chaotic. Consequently, forecasts based on these\nestimates showed poor predictive accuracy compared to simple \"model-free\"\nmethods, which lead Perretti et al. to conclude that \"Model-free forecasting\noutperforms the correct mechanistic model for simulated and experimental data\".\nHowever, a simple modification of the statistical methods also suffices to\nremove the bias and reverse their results.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2013 17:01:13 GMT"}], "update_date": "2013-10-28", "authors_parsed": [["Hartig", "Florian", ""], ["Dormann", "Carsten F.", ""]]}, {"id": "1305.3692", "submitter": "Miroslav Bacak", "authors": "Philipp Benner, Miroslav Bacak, Pierre-Yves Bourguignon", "title": "Point estimates in phylogenetic reconstructions", "comments": "The original paper \"Computing the posterior expectation of\n  phylogenetic trees\" was substantially rewritten, renamed to \"Point estimates\n  in phylogenetic reconstructions\" and published in Bioinformatics", "journal-ref": "Bioinformatics (2014) 30 (17): i534-i540", "doi": "10.1093/bioinformatics/btu461", "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: The construction of statistics for summarizing posterior samples\nreturned by a Bayesian phylogenetic study has so far been hindered by the poor\ngeometric insights available into the space of phylogenetic trees, and ad hoc\nmethods such as the derivation of a consensus tree makeup for the\nill-definition of the usual concepts of posterior mean, while bootstrap methods\nmitigate the absence of a sound concept of variance. Yielding satisfactory\nresults with sufficiently concentrated posterior distributions, such methods\nfall short of providing a faithful summary of posterior distributions if the\ndata do not offer compelling evidence for a single topology.\n  Results: Building upon previous work of Billera et al., summary statistics\nsuch as sample mean, median and variance are defined as the geometric median,\nFr\\'echet mean and variance, respectively. Their computation is enabled by\nrecently published works, and embeds an algorithm for computing shortest paths\nin the space of trees. Studying the phylogeny of a set of plants, where several\ntree topologies occur in the posterior sample, the posterior mean balances\ncorrectly the contributions from the different topologies, where a consensus\ntree would be biased. Comparisons of the posterior mean, median and consensus\ntrees with the ground truth using simulated data also reveals the benefits of a\nsound averaging method when reconstructing phylogenetic trees.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2013 07:07:15 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2013 19:59:04 GMT"}, {"version": "v3", "created": "Sun, 28 Jul 2013 17:24:48 GMT"}, {"version": "v4", "created": "Mon, 16 Dec 2013 15:48:24 GMT"}, {"version": "v5", "created": "Fri, 10 Oct 2014 11:34:18 GMT"}], "update_date": "2014-10-13", "authors_parsed": [["Benner", "Philipp", ""], ["Bacak", "Miroslav", ""], ["Bourguignon", "Pierre-Yves", ""]]}, {"id": "1305.4366", "submitter": "Xiang Zhou", "authors": "Xiang Zhou and Matthew Stephens", "title": "Efficient Algorithms for Multivariate Linear Mixed Models in Genome-wide\n  Association Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate linear mixed models (mvLMMs) have been widely used in many areas\nof genetics, and have attracted considerable recent interest in genome-wide\nassociation studies (GWASs). However, fitting mvLMMs is computationally\nnon-trivial, and no existing method is computationally practical for performing\nthe likelihood ratio test (LRT) for mvLMMs in GWAS settings with moderate\nsample size n. The existing software MTMM perform an approximate LRT for two\nphenotypes, and as we find, its p values can substantially understate the\nsignificance of associations. Here, we present novel computationally-efficient\nalgorithms for fitting mvLMMs, and computing the LRT in GWAS settings. After a\nsingle initial eigen-decomposition (with complexity O(n^3)) the algorithms i)\nreduce computational complexity (per iteration of the optimizer) from cubic to\nlinear in n; and ii) in GWAS analyses, reduces per-marker complexity from cubic\nto quadratic in n. These innovations make it practical to compute the LRT for\nmvLMMs in GWASs for tens of thousands of samples and a moderate number of\nphenotypes (~2-10). With simulations, we show that the LRT provides correct\ncontrol for type I error. With both simulations and real data we find that the\nLRT is more powerful than the approximate LRT from MTMM, and illustrate the\nbenefits of analyzing more than two phenotypes. The method is implemented in\nthe GEMMA software package, freely available at\nhttp://stephenslab.uchicago.edu/software.html\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2013 14:53:48 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2013 21:10:44 GMT"}], "update_date": "2013-09-13", "authors_parsed": [["Zhou", "Xiang", ""], ["Stephens", "Matthew", ""]]}, {"id": "1305.4390", "submitter": "Libo Sun", "authors": "Libo Sun, Chihoon Lee, and Jennifer A. Hoeting", "title": "A penalized simulated maximum likelihood approach in parameter\n  estimation for stochastic differential equations", "comments": "23 pages, 4 figures, 3 tables", "journal-ref": "Computational Statistics & Data Analysis 84 (2015): 54-67", "doi": "10.1016/j.csda.2014.11.007", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating parameters of stochastic differential\nequations (SDEs) with discrete-time observations that are either completely or\npartially observed. The transition density between two observations is\ngenerally unknown. We propose an importance sampling approach with an auxiliary\nparameter when the transition density is unknown. We embed the auxiliary\nimportance sampler in a penalized maximum likelihood framework which produces\nmore accurate and computationally efficient parameter estimates. Simulation\nstudies in three different models illustrate promising improvements of the new\npenalized simulated maximum likelihood method. The new procedure is designed\nfor the challenging case when some state variables are unobserved and moreover,\nobserved states are sparse over time, which commonly arises in ecological\nstudies. We apply this new approach to two epidemics of chronic wasting disease\nin mule deer.\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2013 18:42:46 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2013 17:03:19 GMT"}, {"version": "v3", "created": "Wed, 21 May 2014 02:41:56 GMT"}, {"version": "v4", "created": "Tue, 8 Sep 2015 18:24:06 GMT"}], "update_date": "2015-09-09", "authors_parsed": [["Sun", "Libo", ""], ["Lee", "Chihoon", ""], ["Hoeting", "Jennifer A.", ""]]}, {"id": "1305.4511", "submitter": "Alberto Sorrentino", "authors": "Alberto Sorrentino, Gianvittorio Luria and Riccardo Aramini", "title": "Bayesian Multi--Dipole Modeling of a Single Topography in MEG by\n  Adaptive Sequential Monte Carlo Samplers", "comments": "20 pages, 4 figures", "journal-ref": "Inverse Problems 30 (2014) 045010", "doi": "10.1088/0266-5611/30/4/045010", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present paper, we develop a novel Bayesian approach to the problem of\nestimating neural currents in the brain from a fixed distribution of magnetic\nfield (called \\emph{topography}), measured by magnetoencephalography.\nDifferently from recent studies that describe inversion techniques, such as\nspatio-temporal regularization/filtering, in which neural dynamics always plays\na role, we face here a purely static inverse problem. Neural currents are\nmodelled as an unknown number of current dipoles, whose state space is\ndescribed in terms of a variable--dimension model. Within the resulting\nBayesian framework, we set up a sequential Monte Carlo sampler to explore the\nposterior distribution. An adaptation technique is employed in order to\neffectively balance the computational cost and the quality of the sample\napproximation. Then, both the number and the parameters of the unknown current\ndipoles are simultaneously estimated. The performance of the method is assessed\nby means of synthetic data, generated by source configurations containing up to\nfour dipoles. Eventually, we describe the results obtained by analyzing data\nfrom a real experiment, involving somatosensory evoked fields, and compare them\nto those provided by three other methods.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2013 12:38:34 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2014 18:28:12 GMT"}], "update_date": "2014-03-20", "authors_parsed": [["Sorrentino", "Alberto", ""], ["Luria", "Gianvittorio", ""], ["Aramini", "Riccardo", ""]]}, {"id": "1305.4942", "submitter": "Alexander Gorban", "authors": "E. M. Mirkes, I. Alexandrakis, K. Slater, R. Tuli, A. N. Gorban", "title": "Computational diagnosis and risk evaluation for canine lymphoma", "comments": "24 pages, 86 references in the bibliography, Significantly extended\n  version with review of lymphoma biomarkers and data mining methods (Three new\n  sections are added: 1.1. Biomarkers for canine lymphoma, 1.2. Acute phase\n  proteins as lymphoma biomarkers and 3.1. Data mining methods for biomarker\n  cancer diagnosis. Flowcharts of data analysis are included as supplementary\n  material (20 pages)", "journal-ref": "Computers in Biology and Medicine, Volume 53, 1 October 2014,\n  279-290", "doi": "10.1016/j.compbiomed.2014.08.006", "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The canine lymphoma blood test detects the levels of two biomarkers, the\nacute phase proteins (C-Reactive Protein and Haptoglobin). This test can be\nused for diagnostics, for screening, and for remission monitoring as well. We\nanalyze clinical data, test various machine learning methods and select the\nbest approach to these problems. Three family of methods, decision trees, kNN\n(including advanced and adaptive kNN) and probability density evaluation with\nradial basis functions, are used for classification and risk estimation.\nSeveral pre-processing approaches were implemented and compared. The best of\nthem are used to create the diagnostic system. For the differential diagnosis\nthe best solution gives the sensitivity and specificity of 83.5% and 77%,\nrespectively (using three input features, CRP, Haptoglobin and standard\nclinical symptom). For the screening task, the decision tree method provides\nthe best result, with sensitivity and specificity of 81.4% and >99%,\nrespectively (using the same input features). If the clinical symptoms\n(Lymphadenopathy) are considered as unknown then a decision tree with CRP and\nHapt only provides sensitivity 69% and specificity 83.5%. The lymphoma risk\nevaluation problem is formulated and solved. The best models are selected as\nthe system for computational lymphoma diagnosis and evaluation the risk of\nlymphoma as well. These methods are implemented into a special web-accessed\nsoftware and are applied to problem of monitoring dogs with lymphoma after\ntreatment. It detects recurrence of lymphoma up to two months prior to the\nappearance of clinical signs. The risk map visualisation provides a friendly\ntool for explanatory data analysis.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2013 20:03:30 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2014 17:22:47 GMT"}, {"version": "v3", "created": "Thu, 3 Jul 2014 19:06:21 GMT"}], "update_date": "2014-10-13", "authors_parsed": [["Mirkes", "E. M.", ""], ["Alexandrakis", "I.", ""], ["Slater", "K.", ""], ["Tuli", "R.", ""], ["Gorban", "A. N.", ""]]}, {"id": "1305.4982", "submitter": "Brandy Ringham", "authors": "Brandy M. Ringham, Todd A. Alonzo, John T. Brinton, Sarah M. Kreidler,\n  Aarti Munjal, Keith E. Muller, Deborah H. Glueck", "title": "Reducing decision errors in the paired comparison of the diagnostic\n  accuracy of screening tests with Gaussian outcomes", "comments": "32 pages, including 5 figures and 2 tables, submitted to BMC Medical\n  Research Methodology March, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientists often use a paired comparison of the areas under the receiver\noperating characteristic curves to decide which continuous cancer screening\ntest has the best diagnostic accuracy. In the paired design, all participants\nare screened with both tests. Participants with unremarkable screening results\nenter a follow-up period. Participants with suspicious screening results and\nthose who show evidence of disease during follow-up receive the gold standard\ntest. The remaining participants are classified as non-cases, even though some\nmay have occult disease. The standard analysis includes all study participants\nin the analysis, which can create bias in the estimates of diagnostic accuracy.\nIf the bias affects the area under the curve for one screening test more than\nthe other screening test, scientists may make the wrong decision as to which\nscreening test has better diagnostic accuracy. We describe a weighted maximum\nlikelihood bias correction method to reduce decision errors. We assessed the\nability of the bias correction method to reduce decision errors via simulation\nstudies. The simulations compared the Type I error rate and power of the\nstandard analysis with that of the bias-corrected analysis. The performance of\nthe bias correction method depends on characteristics of the screening tests\nand the disease, and on the percentage of study participants who receive the\ngold standard test. In studies with a large amount of bias in the difference in\nthe full area under the curve, the bias correction method reduces the Type I\nerror rate and improves power for the correct decision. In order to determine\nif bias correction is needed for a specific screening trial, we recommend the\ninvestigator conduct a simulation study using our free software.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2013 23:11:57 GMT"}], "update_date": "2013-05-23", "authors_parsed": [["Ringham", "Brandy M.", ""], ["Alonzo", "Todd A.", ""], ["Brinton", "John T.", ""], ["Kreidler", "Sarah M.", ""], ["Munjal", "Aarti", ""], ["Muller", "Keith E.", ""], ["Glueck", "Deborah H.", ""]]}, {"id": "1305.5238", "submitter": "Taiane Prass", "authors": "Taiane S. Prass and S\\'ilvia R.C. Lopes", "title": "Risk Measure Estimation On Fiegarch Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Fractionally Integrated Exponential Generalized\nAutoregressive Conditional Heteroskedasticity process, denoted by\nFIEGARCH(p,d,q), introduced by Bollerslev and Mikkelsen (1996). We present a\nsimulated study regarding the estimation of the risk measure $VaR_p$ on\nFIEGARCH processes. We consider the distribution function of the portfolio\nlog-returns (univariate case) and the multivariate distribution function of the\nrisk-factor changes (multivariate case). We also compare the performance of the\nrisk measures $VaR_p$, $ES_p$ and MaxLoss for a portfolio composed by stocks of\nfour Brazilian companies.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2013 19:15:11 GMT"}], "update_date": "2013-05-23", "authors_parsed": [["Prass", "Taiane S.", ""], ["Lopes", "S\u00edlvia R. C.", ""]]}, {"id": "1305.5355", "submitter": "Jun Chen", "authors": "Jun Chen, Hongzhe Li", "title": "Variable selection for sparse Dirichlet-multinomial regression with an\n  application to microbiome data analysis", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS592 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 1, 418-442", "doi": "10.1214/12-AOAS592", "report-no": "IMS-AOAS-AOAS592", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of next generation sequencing technology, researchers\nhave now been able to study the microbiome composition using direct sequencing,\nwhose output are bacterial taxa counts for each microbiome sample. One goal of\nmicrobiome study is to associate the microbiome composition with environmental\ncovariates. We propose to model the taxa counts using a Dirichlet-multinomial\n(DM) regression model in order to account for overdispersion of observed\ncounts. The DM regression model can be used for testing the association between\ntaxa composition and covariates using the likelihood ratio test. However, when\nthe number of covariates is large, multiple testing can lead to loss of power.\nTo address the high dimensionality of the problem, we develop a penalized\nlikelihood approach to estimate the regression parameters and to select the\nvariables by imposing a sparse group $\\ell_1$ penalty to encourage both\ngroup-level and within-group sparsity. Such a variable selection procedure can\nlead to selection of the relevant covariates and their associated bacterial\ntaxa. An efficient block-coordinate descent algorithm is developed to solve the\noptimization problem. We present extensive simulations to demonstrate that the\nsparse DM regression can result in better identification of the\nmicrobiome-associated covariates than models that ignore overdispersion or only\nconsider the proportions. We demonstrate the power of our method in an analysis\nof a data set evaluating the effects of nutrient intake on human gut microbiome\ncomposition. Our results have clearly shown that the nutrient intake is\nstrongly associated with the human gut microbiome.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2013 09:20:53 GMT"}], "update_date": "2013-05-24", "authors_parsed": [["Chen", "Jun", ""], ["Li", "Hongzhe", ""]]}, {"id": "1305.5601", "submitter": "Sijia Liu", "authors": "Sijia Liu, Makan Fardad, Engin Masazade, Pramod K. Varshney", "title": "Optimal Periodic Sensor Scheduling in Networks of Dynamical Systems", "comments": "Accepted in IEEE Transactions on Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2014.2320455", "report-no": null, "categories": "stat.AP cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of finding optimal time-periodic sensor schedules for\nestimating the state of discrete-time dynamical systems. We assume that\n{multiple} sensors have been deployed and that the sensors are subject to\nresource constraints, which limits the number of times each can be activated\nover one period of the periodic schedule. We seek an algorithm that strikes a\nbalance between estimation accuracy and total sensor activations over one\nperiod. We make a correspondence between active sensors and the nonzero columns\nof estimator gain. We formulate an optimization problem in which we minimize\nthe trace of the error covariance with respect to the estimator gain while\nsimultaneously penalizing the number of nonzero columns of the estimator gain.\nThis optimization problem is combinatorial in nature, and we employ the\nalternating direction method of multipliers (ADMM) to find its locally optimal\nsolutions. Numerical results and comparisons with other sensor scheduling\nalgorithms in the literature are provided to illustrate the effectiveness of\nour proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2013 01:53:53 GMT"}, {"version": "v2", "created": "Thu, 17 Apr 2014 18:06:25 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Liu", "Sijia", ""], ["Fardad", "Makan", ""], ["Masazade", "Engin", ""], ["Varshney", "Pramod K.", ""]]}, {"id": "1305.5682", "submitter": "Kosuke Imai", "authors": "Kosuke Imai, Marc Ratkovic", "title": "Estimating treatment effect heterogeneity in randomized program\n  evaluation", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS593 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 1, 443-470", "doi": "10.1214/12-AOAS593", "report-no": "IMS-AOAS-AOAS593", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When evaluating the efficacy of social programs and medical treatments using\nrandomized experiments, the estimated overall average causal effect alone is\noften of limited value and the researchers must investigate when the treatments\ndo and do not work. Indeed, the estimation of treatment effect heterogeneity\nplays an essential role in (1) selecting the most effective treatment from a\nlarge number of available treatments, (2) ascertaining subpopulations for which\na treatment is effective or harmful, (3) designing individualized optimal\ntreatment regimes, (4) testing for the existence or lack of heterogeneous\ntreatment effects, and (5) generalizing causal effect estimates obtained from\nan experimental sample to a target population. In this paper, we formulate the\nestimation of heterogeneous treatment effects as a variable selection problem.\nWe propose a method that adapts the Support Vector Machine classifier by\nplacing separate sparsity constraints over the pre-treatment parameters and\ncausal heterogeneity parameters of interest. The proposed method is motivated\nby and applied to two well-known randomized evaluation studies in the social\nsciences. Our method selects the most effective voter mobilization strategies\nfrom a large number of alternative strategies, and it also identifies the\ncharacteristics of workers who greatly benefit from (or are negatively affected\nby) a job training program. In our simulation studies, we find that the\nproposed method often outperforms some commonly used alternatives.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2013 10:49:13 GMT"}], "update_date": "2013-05-27", "authors_parsed": [["Imai", "Kosuke", ""], ["Ratkovic", "Marc", ""]]}, {"id": "1305.5695", "submitter": "Jaeil Ahn", "authors": "Jaeil Ahn, Bhramar Mukherjee, Stephen B. Gruber, Malay Ghosh", "title": "Bayesian semiparametric analysis for two-phase studies of\n  gene-environment interaction", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS599 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 1, 543-569", "doi": "10.1214/12-AOAS599", "report-no": "IMS-AOAS-AOAS599", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The two-phase sampling design is a cost-efficient way of collecting expensive\ncovariate information on a judiciously selected subsample. It is natural to\napply such a strategy for collecting genetic data in a subsample enriched for\nexposure to environmental factors for gene-environment interaction (G x E)\nanalysis. In this paper, we consider two-phase studies of G x E interaction\nwhere phase I data are available on exposure, covariates and disease status.\nStratified sampling is done to prioritize individuals for genotyping at phase\nII conditional on disease and exposure. We consider a Bayesian analysis based\non the joint retrospective likelihood of phases I and II data. We address\nseveral important statistical issues: (i) we consider a model with multiple\ngenes, environmental factors and their pairwise interactions. We employ a\nBayesian variable selection algorithm to reduce the dimensionality of this\npotentially high-dimensional model; (ii) we use the assumption of gene-gene and\ngene-environment independence to trade off between bias and efficiency for\nestimating the interaction parameters through use of hierarchical priors\nreflecting this assumption; (iii) we posit a flexible model for the joint\ndistribution of the phase I categorical variables using the nonparametric Bayes\nconstruction of Dunson and Xing [J. Amer. Statist. Assoc. 104 (2009)\n1042-1051].\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2013 11:45:43 GMT"}], "update_date": "2013-05-27", "authors_parsed": [["Ahn", "Jaeil", ""], ["Mukherjee", "Bhramar", ""], ["Gruber", "Stephen B.", ""], ["Ghosh", "Malay", ""]]}, {"id": "1305.5962", "submitter": "Pasquale Erto", "authors": "Pasquale Erto", "title": "A note on monitoring ratios of two Weibull percentiles", "comments": "13 pages; 4 figures; 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note introduces a new Bayesian control chart to compare two processes by\nmonitoring the ratio of their percentiles under Weibull assumption. Both\nin-control and out-of-control parameters are supposed unknown. The chart\nanalyses the sampling data directly, instead of transforming them in order to\ncomply with the usual normality assumption, as most charts do. The chart uses\nthe whole accumulated knowledge, resulting from the current and all the past\nsamples, to monitor the current value of the ratio. Two real applications in\nthe wood industry and in the concrete industry give a first picture of the\nfeatures of the chart.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2013 19:43:56 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2015 07:36:48 GMT"}, {"version": "v3", "created": "Tue, 30 Jun 2015 18:46:47 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Erto", "Pasquale", ""]]}, {"id": "1305.6372", "submitter": "Armin Schwartzman", "authors": "Armin Schwartzman, Andrew Jaffe, Yulia Gavrilov, Clifford A. Meyer", "title": "Multiple testing of local maxima for detection of peaks in ChIP-Seq data", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS594 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 1, 471-494", "doi": "10.1214/12-AOAS594", "report-no": "IMS-AOAS-AOAS594", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A topological multiple testing approach to peak detection is proposed for the\nproblem of detecting transcription factor binding sites in ChIP-Seq data. After\nkernel smoothing of the tag counts over the genome, the presence of a peak is\ntested at each observed local maximum, followed by multiple testing correction\nat the desired false discovery rate level. Valid p-values for candidate peaks\nare computed via Monte Carlo simulations of smoothed Poisson sequences, whose\nbackground Poisson rates are obtained via linear regression from a Control\nsample at two different scales. The proposed method identifies nearby binding\nsites that other methods do not.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2013 05:33:15 GMT"}], "update_date": "2013-05-29", "authors_parsed": [["Schwartzman", "Armin", ""], ["Jaffe", "Andrew", ""], ["Gavrilov", "Yulia", ""], ["Meyer", "Clifford A.", ""]]}, {"id": "1305.6479", "submitter": "Stan Zachary", "authors": "Chris Dent and Stan Zachary", "title": "Capacity Value of Additional Generation: Probability Theory and Sampling\n  Uncertainty", "comments": "Probabilistic Methods Applied to Power Systems conference,\n  Istanbul.(2012). 6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of capacity value is widely used to quantify the contribution of\nadditional generation (most notably renewables) within generation adequacy\nassessments. This paper surveys the existing probability theory of assessment\nof the capacity value of additional generation, and discusses the available\nstatistical estimation methods for risk measures which depend on the joint\ndistribution of demand and available additional capacity (with particular\nreference to wind).\n  Preliminary results are presented on assessment of sampling uncertainty in\nhindcast LOLE and capacity value calculations, using bootstrap resampling.\nThese results indicate strongly that, if the hindcast calculation is dominated\nby extremes of demand minus wind, there is very large sampling uncertainty in\nthe results due to very limited historic experience of high demands coincident\nwith poor wind resource. For meaningful calculations, some form of statistical\nsmoothing will therefore be required in distribution estimation.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2013 13:09:08 GMT"}], "update_date": "2013-05-29", "authors_parsed": [["Dent", "Chris", ""], ["Zachary", "Stan", ""]]}, {"id": "1305.6738", "submitter": "Efstratios Rappos", "authors": "Efstratios Rappos and Stephan Robert", "title": "Using GPU Simulation to Accurately Fit to the Power-Law Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DC physics.comp-ph physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes a methodology for fitting experimental data to the\ndiscrete power-law distribution and provides the results of a detailed\nsimulation exercise used to calculate accurate cutoff values used to assess the\nfit to a power-law distribution when using the maximum likelihood estimation\nfor the exponent of the distribution. Using massively parallel programming\ncomputing, we were able to accelerate by a factor of 60 the computational time\nrequired for these calculations across a range of parameters and construct a\nseries of detailed tables containing the test values to be used in a\nKolmogorov-Smirnov goodness-of-fit test, allowing for an accurate assessment of\nthe power-law fit from empirical data.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2013 09:26:41 GMT"}], "update_date": "2013-05-30", "authors_parsed": [["Rappos", "Efstratios", ""], ["Robert", "Stephan", ""]]}, {"id": "1305.7010", "submitter": "Adrien Ickowicz", "authors": "Adrien Ickowicz and Ross Sparks", "title": "Estimation of an Origin/Destination matrix: Application to a ferry\n  transport data", "comments": "21 pages, 12 figures. Submitted to Transportation Research Part B:\n  Methodology", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of the number of passengers with the identical journey is a\ncommon problem for public transport authorities. This problem is also known as\nthe Origin- Destination estimation (OD) problem and it has been widely studied\nfor the past thirty years. However, the theory is missing when the observations\nare not limited to the passenger counts but also includes station surveys. Our\naim is to provide a solid framework for the estimation of an OD matrix when\nonly a portion of the journey counts are observable. Our method consists of a\nstatistical estimation technique for OD matrix when we have the sum-of-row\ncounts and survey-based observations. Our technique differs from the previous\nstudies in that it does not need a prior OD matrix which can be hard to obtain.\nInstead, we model the passengers behavior through the survey data, and use the\ndiagonalization of the partial OD matrix to reduce the space parameter and\nderive a consistent global OD matrix estimator. We demonstrate the robustness\nof our estimator and apply it to several examples showcasing the proposed\nmodels and approach. We highlight how other sources of data can be incorporated\nin the model such as explanatory variables, e.g. rainfall, indicator variables\nfor major events, etc, and inference made in a principled, non-heuristic way.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2013 06:27:19 GMT"}], "update_date": "2013-05-31", "authors_parsed": [["Ickowicz", "Adrien", ""], ["Sparks", "Ross", ""]]}, {"id": "1305.7284", "submitter": "Subhabrata Majumdar", "authors": "Saurabh Ghosh, Subhabrata Majumdar", "title": "Adjusting for Treatment Effects in Studies of Quantitative Traits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  A population-based study of a quantitative trait, e.g. Blood Pressure(BP) may\nbe seriously compromised when the trait is subject to the effects of a\ntreatment. Without appropriate corrections this can lead to considerable\nreduction of statistical power. Here we demonestrate this in the scenario of\nQTL mapping through Single-Marker Analysis. The data are simulated from a\nnormal mixtrure for different values of allele frequencies, separation between\nnormal populations and Linkage Disequilibrium, and several methods of\ncorrection are compared to check which can best compensate for the loss of\npower if treatment effects are ignored. In one of these methods, underlying BPs\nare approximated by subtracting an estimate of mean value of medicine effect\nfrom obsereved BPs in treated subjects. We domonestrate the efficacy of this\nmethod throughout different choices of parameters. Finally to account for\nquantitative traits that follow non-normal distributions, data are simulated\nfrom lognormal mixtures similarly and Kruskal-Wallis test is used to obtain\nestimates of powers for different methods of analysis.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2013 02:30:56 GMT"}], "update_date": "2013-06-03", "authors_parsed": [["Ghosh", "Saurabh", ""], ["Majumdar", "Subhabrata", ""]]}, {"id": "1305.7331", "submitter": "Naresh Kumar Mallenahalli Prof. Dr.", "authors": "M. Naresh Kumar", "title": "Alternating Decision trees for early diagnosis of dengue fever", "comments": "13 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dengue fever is a flu-like illness spread by the bite of an infected mosquito\nwhich is fast emerging as a major health problem. Timely and cost effective\ndiagnosis using clinical and laboratory features would reduce the mortality\nrates besides providing better grounds for clinical management and disease\nsurveillance. We wish to develop a robust and effective decision tree based\napproach for predicting dengue disease. Our analysis is based on the clinical\ncharacteristics and laboratory measurements of the diseased individuals. We\nhave developed and trained an alternating decision tree with boosting and\ncompared its performance with C4.5 algorithm for dengue disease diagnosis. Of\nthe 65 patient records a diagnosis establishes that 53 individuals have been\nconfirmed to have dengue fever. An alternating decision tree based algorithm\nwas able to differentiate the dengue fever using the clinical and laboratory\ndata with number of correctly classified instances as 89%, F-measure of 0.86\nand receiver operator characteristics (ROC) of 0.826 as compared to C4.5 having\ncorrectly classified instances as 78%,h F-measure of 0.738 and ROC of 0.617\nrespectively. Alternating decision tree based approach with boosting has been\nable to predict dengue fever with a higher degree of accuracy than C4.5 based\ndecision tree using simple clinical and laboratory features. Further analysis\non larger data sets is required to improve the sensitivity and specificity of\nthe alternating decision trees.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2013 09:15:47 GMT"}, {"version": "v2", "created": "Wed, 5 Jun 2013 04:56:15 GMT"}], "update_date": "2013-06-06", "authors_parsed": [["Kumar", "M. Naresh", ""]]}]