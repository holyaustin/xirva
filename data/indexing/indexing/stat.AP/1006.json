[{"id": "1006.0054", "submitter": "Yipeng Liu Dr.", "authors": "Yipeng Liu, Qun Wan, Fei Wen, Jia Xu, Yingning Peng", "title": "Anti-measurement Matrix Uncertainty Sparse Signal Recovery for\n  Compressive Sensing", "comments": "13 pages, 3 figures; Accepted by International Journal of the\n  Physical Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.NA stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressive sensing (CS) is a technique for estimating a sparse signal from\nthe random measurements and the measurement matrix. Traditional sparse signal\nrecovery methods have seriously degeneration with the measurement matrix\nuncertainty (MMU). Here the MMU is modeled as a bounded additive error. An\nanti-uncertainty constraint in the form of a mixed L2 and L1 norm is deduced\nfrom the sparse signal model with MMU. Then we combine the sparse constraint\nwith the anti-uncertainty constraint to get an anti-uncertainty sparse signal\nrecovery operator. Numerical simulations demonstrate that the proposed operator\nhas a better reconstructing performance with the MMU than traditional methods.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2010 04:54:20 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2010 08:33:08 GMT"}, {"version": "v3", "created": "Sat, 18 Jun 2011 09:02:41 GMT"}], "update_date": "2011-06-21", "authors_parsed": [["Liu", "Yipeng", ""], ["Wan", "Qun", ""], ["Wen", "Fei", ""], ["Xu", "Jia", ""], ["Peng", "Yingning", ""]]}, {"id": "1006.0158", "submitter": "Konstantin S. Turitsyn", "authors": "Konstantin S. Turitsyn", "title": "Statistics of voltage drop in radial distribution circuits: a dynamic\n  programming approach", "comments": "5 pages, 3 figures, accepted to IEEE SIBIRCON 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze a power distribution line with high penetration of distributed\ngeneration and strong variations of power consumption and generation levels. In\nthe presence of uncertainty the statistical description of the system is\nrequired to assess the risks of power outages. In order to find the probability\nof exceeding the constraints for voltage levels we introduce the probability\ndistribution of maximal voltage drop and propose an algorithm for finding this\ndistribution. The algorithm is based on the assumption of random but\nstatistically independent distribution of loads on buses. Linear complexity in\nthe number of buses is achieved through the dynamic programming technique. We\nillustrate the performance of the algorithm by analyzing a simple 4-bus system\nwith high variations of load levels.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2010 15:30:09 GMT"}], "update_date": "2010-06-02", "authors_parsed": [["Turitsyn", "Konstantin S.", ""]]}, {"id": "1006.0320", "submitter": "Boris Zyryanov A.", "authors": "Boris A. Zyryanov", "title": "Development of the signals complex elaboration system (application for\n  analysis of occupational injuries process)", "comments": null, "journal-ref": null, "doi": null, "report-no": "Report n. 89-029, Politecnico di Milano, Boris A. Zyrianov\n  \"Occupational Injuries: comparative analysis (Italy and URSS), spectral\n  analysis and synergetics\"", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theoretical base of the research of occupational injuries is the idea of\nthe process as Markov chain of random variables. However the exact proof of\nthis position was not carried out whereas the experimental passing of the\nhypothesis is connected always with the determined confidence limits and\nconsequently it gives the space for alternative assumptions. In this research\nsome databases of occupational injuries had been studied using spectral\nanalysis techniques and the presentation of the occupational injuries as the\ntemporal sequence of the cases (\"telegraph wave\" process type). Databases had\ncorresponding chapters such as \"enterprise\" with number of employees about\n7000, \"big enterprise\" (the number of employees about 35000), \"whole branch of\nindustry\", \"whole enterprises of industrial region\" receiving during 10 years\nfrom different countries having distinguish system of the work organization\n(Russia and Italy). The behaviour of spectra on principal is not changed when\nvary the length of realization, resolution, smoothing, upper boundary\nfrequency, country and year of datas. All spectra showed that the occupational\ninjuries process has a not Markov, but deterministic polyharmonic behaviour.\nThis phenomenon is characterized by the spectral lines with two frequencies\ncorresponding 24 hours (circadian rythm have main amplitude) and 7 days\n(amplitude is two times smaller) exactly. Harmonics corresponding one month\n(biorythms) were not founded. The task of the futher investigations is to find\nthe psychophysiological and biomedical processes determined and having high\nlevel of correlation with the occupational injuries process.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2010 08:20:53 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Zyryanov", "Boris A.", ""]]}, {"id": "1006.1015", "submitter": "Susan Holmes", "authors": "John Chakerian and Susan Holmes", "title": "Computational Tools for Evaluating Phylogenetic and Hierarchical\n  Clustering Trees", "comments": "25 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferential summaries of tree estimates are useful in the setting of\nevolutionary biology, where phylogenetic trees have been built from DNA data\nsince the 1960's. In bioinformatics, psychometrics and data mining,\nhierarchical clustering techniques output the same mathematical objects, and\npractitioners have similar questions about the stability and `generalizability'\nof these summaries. This paper provides an implementation of the geometric\ndistance between trees developed by Billera, Holmes and Vogtmann (2001) [BHV]\nequally applicable to phylogenetic trees and hieirarchical clustering trees,\nand shows some of the applications in statistical inference for which this\ndistance can be useful. In particular, since BHV have shown that the space of\ntrees is negatively curved (a CAT(0) space), a natural representation of a\ncollection of trees is a tree. We compare this representation to the Euclidean\napproximations of treespace made available through Multidimensional Scaling of\nthe matrix of distances between trees. We also provide applications of the\ndistances between trees to hierarchical clustering trees constructed from\nmicroarrays. Our method gives a new way of evaluating the influence both of\ncertain columns (positions, variables or genes) and of certain rows (whether\nspecies, observations or arrays).\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2010 01:26:59 GMT"}], "update_date": "2010-06-08", "authors_parsed": [["Chakerian", "John", ""], ["Holmes", "Susan", ""]]}, {"id": "1006.1029", "submitter": "Andrej Kastrin", "authors": "Andrej Kastrin, Borut Peterlin, Dimitar Hristovski", "title": "Chi-square-based scoring function for categorization of MEDLINE\n  citations", "comments": "34 pages, 2 figures", "journal-ref": "Methods of Information in Medicine, 2010;49(4):371-380", "doi": "10.3414/ME09-01-0009", "report-no": null, "categories": "cs.IR stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objectives: Text categorization has been used in biomedical informatics for\nidentifying documents containing relevant topics of interest. We developed a\nsimple method that uses a chi-square-based scoring function to determine the\nlikelihood of MEDLINE citations containing genetic relevant topic. Methods: Our\nprocedure requires construction of a genetic and a nongenetic domain document\ncorpus. We used MeSH descriptors assigned to MEDLINE citations for this\ncategorization task. We compared frequencies of MeSH descriptors between two\ncorpora applying chi-square test. A MeSH descriptor was considered to be a\npositive indicator if its relative observed frequency in the genetic domain\ncorpus was greater than its relative observed frequency in the nongenetic\ndomain corpus. The output of the proposed method is a list of scores for all\nthe citations, with the highest score given to those citations containing MeSH\ndescriptors typical for the genetic domain. Results: Validation was done on a\nset of 734 manually annotated MEDLINE citations. It achieved predictive\naccuracy of 0.87 with 0.69 recall and 0.64 precision. We evaluated the method\nby comparing it to three machine learning algorithms (support vector machines,\ndecision trees, na\\\"ive Bayes). Although the differences were not statistically\nsignificantly different, results showed that our chi-square scoring performs as\ngood as compared machine learning algorithms. Conclusions: We suggest that the\nchi-square scoring is an effective solution to help categorize MEDLINE\ncitations. The algorithm is implemented in the BITOLA literature-based\ndiscovery support system as a preprocessor for gene symbol disambiguation\nprocess.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2010 08:14:27 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Kastrin", "Andrej", ""], ["Peterlin", "Borut", ""], ["Hristovski", "Dimitar", ""]]}, {"id": "1006.1030", "submitter": "Andrej Kastrin", "authors": "Andrej Kastrin, Borut Peterlin", "title": "Rasch-based high-dimensionality data reduction and class prediction with\n  applications to microarray gene expression data", "comments": null, "journal-ref": "Expert Systems with Applications, 2010;37(7):5178-5185", "doi": "10.1016/j.eswa.2009.12.074", "report-no": null, "categories": "cs.AI stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Class prediction is an important application of microarray gene expression\ndata analysis. The high-dimensionality of microarray data, where number of\ngenes (variables) is very large compared to the number of samples (obser-\nvations), makes the application of many prediction techniques (e.g., logistic\nregression, discriminant analysis) difficult. An efficient way to solve this\nprob- lem is by using dimension reduction statistical techniques. Increasingly\nused in psychology-related applications, Rasch model (RM) provides an appealing\nframework for handling high-dimensional microarray data. In this paper, we\nstudy the potential of RM-based modeling in dimensionality reduction with\nbinarized microarray gene expression data and investigate its prediction ac-\ncuracy in the context of class prediction using linear discriminant analysis.\nTwo different publicly available microarray data sets are used to illustrate a\ngeneral framework of the approach. Performance of the proposed method is\nassessed by re-randomization scheme using principal component analysis (PCA) as\na benchmark method. Our results show that RM-based dimension reduction is as\neffective as PCA-based dimension reduction. The method is general and can be\napplied to the other high-dimensional data problems.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2010 08:27:29 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Kastrin", "Andrej", ""], ["Peterlin", "Borut", ""]]}, {"id": "1006.1328", "submitter": "Jonathan Huang", "authors": "Jonathan Huang and Carlos Guestrin", "title": "Uncovering the Riffled Independence Structure of Rankings", "comments": "65 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing distributions over permutations can be a daunting task due to\nthe fact that the number of permutations of $n$ objects scales factorially in\n$n$. One recent way that has been used to reduce storage complexity has been to\nexploit probabilistic independence, but as we argue, full independence\nassumptions impose strong sparsity constraints on distributions and are\nunsuitable for modeling rankings. We identify a novel class of independence\nstructures, called \\emph{riffled independence}, encompassing a more expressive\nfamily of distributions while retaining many of the properties necessary for\nperforming efficient inference and reducing sample complexity. In riffled\nindependence, one draws two permutations independently, then performs the\n\\emph{riffle shuffle}, common in card games, to combine the two permutations to\nform a single permutation. Within the context of ranking, riffled independence\ncorresponds to ranking disjoint sets of objects independently, then\ninterleaving those rankings. In this paper, we provide a formal introduction to\nriffled independence and present algorithms for using riffled independence\nwithin Fourier-theoretic frameworks which have been explored by a number of\nrecent papers. Additionally, we propose an automated method for discovering\nsets of items which are riffle independent from a training set of rankings. We\nshow that our clustering-like algorithms can be used to discover meaningful\nlatent coalitions from real preference ranking datasets and to learn the\nstructure of hierarchically decomposable models based on riffled independence.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2010 18:45:46 GMT"}], "update_date": "2010-06-08", "authors_parsed": [["Huang", "Jonathan", ""], ["Guestrin", "Carlos", ""]]}, {"id": "1006.1860", "submitter": "Jan C. Neddermeyer", "authors": "Rainer Dahlhaus and Jan C. Neddermeyer", "title": "On-line Spot Volatility-Estimation and Decomposition with Nonlinear\n  Market Microstructure Noise Models", "comments": "10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A technique for on-line estimation of spot volatility for high-frequency data\nis developed. The algorithm works directly on the transaction data and updates\nthe volatility estimate immediately after the occurrence of a new transaction.\nFurthermore, a nonlinear market microstructure noise model is proposed that\nreproduces several stylized facts of high-frequency data. A computationally\nefficient particle filter is used that allows for the approximation of the\nunknown efficient prices and, in combination with a recursive EM algorithm, for\nthe estimation of the volatility curve. We neither assume that the transaction\ntimes are equidistant nor do we use interpolated prices. We also make a\ndistinction between volatility per time unit and volatility per transaction and\nprovide estimators for both. More precisely we use a model with random time\nchange where spot volatility is decomposed into spot volatility per transaction\ntimes the trading intensity - thus highlighting the influence of trading\nintensity on volatility.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2010 17:33:56 GMT"}, {"version": "v2", "created": "Mon, 18 Apr 2011 19:27:37 GMT"}, {"version": "v3", "created": "Mon, 13 Aug 2012 18:45:28 GMT"}, {"version": "v4", "created": "Sun, 13 Jan 2013 21:14:28 GMT"}], "update_date": "2013-01-15", "authors_parsed": [["Dahlhaus", "Rainer", ""], ["Neddermeyer", "Jan C.", ""]]}, {"id": "1006.2300", "submitter": "Gael Varoquaux", "authors": "G. Varoquaux (INRIA Saclay - Ile de France, LNAO), S. Sadaghiani\n  (LCogn), P. Pinel (LCogn), A. Kleinschmidt (LCogn), J. B. Poline (LNAO), B.\n  Thirion (INRIA Saclay - Ile de France, LNAO)", "title": "A group model for stable multi-subject ICA on fMRI datasets", "comments": null, "journal-ref": "NeuroImage 2010;51(1):288-99", "doi": "10.1016/j.neuroimage.2010.02.010", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial Independent Component Analysis (ICA) is an increasingly used\ndata-driven method to analyze functional Magnetic Resonance Imaging (fMRI)\ndata. To date, it has been used to extract sets of mutually correlated brain\nregions without prior information on the time course of these regions. Some of\nthese sets of regions, interpreted as functional networks, have recently been\nused to provide markers of brain diseases and open the road to paradigm-free\npopulation comparisons. Such group studies raise the question of modeling\nsubject variability within ICA: how can the patterns representative of a group\nbe modeled and estimated via ICA for reliable inter-group comparisons? In this\npaper, we propose a hierarchical model for patterns in multi-subject fMRI\ndatasets, akin to mixed-effect group models used in linear-model-based\nanalysis. We introduce an estimation procedure, CanICA (Canonical ICA), based\non i) probabilistic dimension reduction of the individual data, ii) canonical\ncorrelation analysis to identify a data subspace common to the group iii)\nICA-based pattern extraction. In addition, we introduce a procedure based on\ncross-validation to quantify the stability of ICA patterns at the level of the\ngroup. We compare our method with state-of-the-art multi-subject fMRI ICA\nmethods and show that the features extracted using our procedure are more\nreproducible at the group level on two datasets of 12 healthy controls: a\nresting-state and a functional localizer study.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2010 13:29:05 GMT"}], "update_date": "2011-02-08", "authors_parsed": [["Varoquaux", "G.", "", "INRIA Saclay - Ile de France, LNAO"], ["Sadaghiani", "S.", "", "LCogn"], ["Pinel", "P.", "", "LCogn"], ["Kleinschmidt", "A.", "", "LCogn"], ["Poline", "J. B.", "", "LNAO"], ["Thirion", "B.", "", "INRIA Saclay - Ile de France, LNAO"]]}, {"id": "1006.2302", "submitter": "Gael Varoquaux", "authors": "Ga\\\"el Varoquaux (INRIA Saclay - Ile de France, LNAO), Merlin Keller\n  (LNAO), Jean Baptiste Poline (LNAO), Philippe Ciuciu (LNAO), Bertrand Thirion\n  (INRIA Saclay - Ile de France, LNAO)", "title": "ICA-based sparse feature recovery from fMRI datasets", "comments": null, "journal-ref": "Biomedical Imaging, IEEE International Symposium on, Rotterdam :\n  Netherlands (2010)", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial Independent Components Analysis (ICA) is increasingly used in the\ncontext of functional Magnetic Resonance Imaging (fMRI) to study cognition and\nbrain pathologies. Salient features present in some of the extracted\nIndependent Components (ICs) can be interpreted as brain networks, but the\nsegmentation of the corresponding regions from ICs is still ill-controlled.\nHere we propose a new ICA-based procedure for extraction of sparse features\nfrom fMRI datasets. Specifically, we introduce a new thresholding procedure\nthat controls the deviation from isotropy in the ICA mixing model. Unlike\ncurrent heuristics, our procedure guarantees an exact, possibly conservative,\nlevel of specificity in feature detection. We evaluate the sensitivity and\nspecificity of the method on synthetic and fMRI data and show that it\noutperforms state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2010 13:32:44 GMT"}], "update_date": "2010-06-16", "authors_parsed": [["Varoquaux", "Ga\u00ebl", "", "INRIA Saclay - Ile de France, LNAO"], ["Keller", "Merlin", "", "LNAO"], ["Poline", "Jean Baptiste", "", "LNAO"], ["Ciuciu", "Philippe", "", "LNAO"], ["Thirion", "Bertrand", "", "INRIA Saclay - Ile de France, LNAO"]]}, {"id": "1006.2555", "submitter": "Yaroslav Ivanenko", "authors": "Yaroslav Ivanenko", "title": "Price as a matter of choice and nonstochastic randomness", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PR math.PR q-fin.PM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A version of indifference valuation of a European call option is proposed\nthat includes statistical regularities of nonstochastic randomness. Classical\nrelations (forward contract value and Black-Scholes formula) are obtained as\nparticular cases. We show that in the general case of nonstochastic randomness\nthe minimal expected profit of uncovered European option position is always\nnegative. A version of delta hedge is proposed.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2010 19:15:50 GMT"}, {"version": "v2", "created": "Sat, 11 Sep 2010 08:07:19 GMT"}, {"version": "v3", "created": "Wed, 15 Sep 2010 11:05:39 GMT"}, {"version": "v4", "created": "Fri, 17 Sep 2010 08:32:35 GMT"}, {"version": "v5", "created": "Mon, 21 Mar 2011 13:45:39 GMT"}], "update_date": "2011-03-22", "authors_parsed": [["Ivanenko", "Yaroslav", ""]]}, {"id": "1006.2792", "submitter": "Mykola Pryimak", "authors": "M. V Pryjmak", "title": "Periodic functions with variable period", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.GM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The examples of rhythmical signals with variable period are considered. The\ndefinition of periodic function with the variable period is given as a model of\nsuch signals. The examples of such functions are given and their variable\nperiods are written in the explicit form. The system of trigonometric functions\nwith the variable period is considered and its orthogonality is proved. The\ngeneralized system of trigonometric functions with the variable period is also\nsuggested; some conditions of its existence are considered.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2010 17:48:14 GMT"}], "update_date": "2010-06-15", "authors_parsed": [["Pryjmak", "M. V", ""]]}, {"id": "1006.3764", "submitter": "Erik Sauleau A", "authors": "Erik A. Sauleau, Valentina Mameli and Monica Musio", "title": "Using Integrated Nested Laplace Approximation for Modeling Spatial\n  Healthcare Utilization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, spatial and spatio-temporal modeling have become an\nimportant area of research in many fields (epidemiology, environmental studies,\ndisease mapping). In this work we propose different spatial models to study\nhospital recruitment, including some potentially explicative variables.\nInterest is on the distribution per geographical unit of the ratio between the\nnumber of patients living in this geographical unit and the population in the\nsame unit. Models considered are within the framework of Bayesian Latent\nGaussian models. Our response variable is assumed to follow a binomial\ndistribution, with logit link, whose parameters are the population in the\ngeographical unit and the corresponding relative risk. The structured additive\npredictor accounts for effects of various covariates in an additive way,\nincluding smoothing functions of the covariates (for example spatial effect),\nlinear effect of covariates. To approximate posterior marginals, which not\navailable in closed form, we use integrated nested Laplace approximations\n(INLA), recently proposed for approximate Bayesian inference in latent Gaussian\nmodels. INLA has the advantage of giving very accurate approximations and being\nfaster than McMC methods when the number of parameters does not exceed 6 (as it\nis in our case). Model comparisons are assessed using DIC criterion.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2010 18:08:42 GMT"}], "update_date": "2010-06-21", "authors_parsed": [["Sauleau", "Erik A.", ""], ["Mameli", "Valentina", ""], ["Musio", "Monica", ""]]}, {"id": "1006.3901", "submitter": "Fabian Wauthier", "authors": "Fabian L. Wauthier and Michael I. Jordan", "title": "Heavy-Tailed Processes for Selective Shrinkage", "comments": "10 pages, 4 figures, submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heavy-tailed distributions are frequently used to enhance the robustness of\nregression and classification methods to outliers in output space. Often,\nhowever, we are confronted with \"outliers\" in input space, which are isolated\nobservations in sparsely populated regions. We show that heavy-tailed\nstochastic processes (which we construct from Gaussian processes via a copula),\ncan be used to improve robustness of regression and classification estimators\nto such outliers by selectively shrinking them more strongly in sparse regions\nthan in dense regions. We carry out a theoretical analysis to show that\nselective shrinkage occurs, provided the marginals of the heavy-tailed process\nhave sufficiently heavy tails. The analysis is complemented by experiments on\nbiological data which indicate significant improvements of estimates in sparse\nregions while producing competitive results in dense regions.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2010 23:34:53 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2010 09:09:01 GMT"}], "update_date": "2010-06-24", "authors_parsed": [["Wauthier", "Fabian L.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1006.4310", "submitter": "Brian Macdonald", "authors": "Brian Macdonald", "title": "A Regression-based Adjusted Plus-Minus Statistic for NHL Players", "comments": "39 pages, 4 figures, 25 tables. Version 3: Typos fixed. Table of\n  contents, list of tables, and list of figures added. Two paragraphs in\n  discussion of goalies at the end of Section 3.3 were added", "journal-ref": "Macdonald, Brian (2011) \"A Regression-Based Adjusted Plus-Minus\n  Statistic for NHL Players,\" Journal of Quantitative Analysis in Sports: Vol.\n  7: Iss. 3, Article 4", "doi": "10.2202/1559-0410.1284", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to develop an adjusted plus-minus statistic for NHL\nplayers that is independent of both teammates and opponents. We use data from\nthe shift reports on NHL.com in a weighted least squares regression to estimate\nan NHL player's effect on his team's success in scoring and preventing goals at\neven strength. Both offensive and defensive components of adjusted plus-minus\nare given, estimates in terms of goals per 60 minutes and goals per season are\ngiven, and estimates for forwards, defensemen, and goalies are given.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2010 15:13:17 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2010 17:31:27 GMT"}, {"version": "v3", "created": "Mon, 1 Nov 2010 22:54:19 GMT"}], "update_date": "2011-11-28", "authors_parsed": [["Macdonald", "Brian", ""]]}, {"id": "1006.4330", "submitter": "Ana Georgina Flesia MS", "authors": "Valeria Rulloni, Oscar Bustos and Ana Georgina Flesia", "title": "Large gaps imputation in remote sensed imagery of the environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imputation of missing data in large regions of satellite imagery is necessary\nwhen the acquired image has been damaged by shadows due to clouds, or\ninformation gaps produced by sensor failure.\n  The general approach for imputation of missing data, that could not be\nconsidered missed at random, suggests the use of other available data. Previous\nwork, like local linear histogram matching, take advantage of a co-registered\nolder image obtained by the same sensor, yielding good results in filling\nhomogeneous regions, but poor results if the scenes being combined have radical\ndifferences in target radiance due, for example, to the presence of sun glint\nor snow.\n  This study proposes three different alternatives for filling the data gaps.\nThe first two involves merging radiometric information from a lower resolution\nimage acquired at the same time, in the Fourier domain (Method A), and using\nlinear regression (Method B). The third method consider segmentation as the\nmain target of processing, and propose a method to fill the gaps in the map of\nclasses, avoiding direct imputation (Method C).\n  All the methods were compared by means of a large simulation study,\nevaluating performance with a multivariate response vector with four measures:\nQ, RMSE, Kappa and Overall Accuracy coefficients. Difference in performance\nwere tested with a MANOVA mixed model design with two main effects, imputation\nmethod and type of lower resolution extra data, and a blocking third factor\nwith a nested sub-factor, introduced by the real Landsat image and the\nsub-images that were used. Method B proved to be the best for all criteria.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2010 16:54:56 GMT"}], "update_date": "2010-06-23", "authors_parsed": [["Rulloni", "Valeria", ""], ["Bustos", "Oscar", ""], ["Flesia", "Ana Georgina", ""]]}, {"id": "1006.4334", "submitter": "Vinay Kashyap", "authors": "Vinay L. Kashyap, David A. van Dyk, Alanna Connors, Peter Freeman,\n  Aneta Siemiginowska, Jin Xu, Andreas Zezas", "title": "On Computing Upper Limits to Source Intensities", "comments": "30 pages, 12 figures, accepted in ApJ", "journal-ref": "The Astrophysical Journal (2010), 719, 900", "doi": "10.1088/0004-637X/719/1/900", "report-no": null, "categories": "astro-ph.IM astro-ph.HE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common problem in astrophysics is determining how bright a source could be\nand still not be detected. Despite the simplicity with which the problem can be\nstated, the solution involves complex statistical issues that require careful\nanalysis. In contrast to the confidence bound, this concept has never been\nformally analyzed, leading to a great variety of often ad hoc solutions. Here\nwe formulate and describe the problem in a self-consistent manner. Detection\nsignificance is usually defined by the acceptable proportion of false positives\n(the TypeI error), and we invoke the complementary concept of false negatives\n(the TypeII error), based on the statistical power of a test, to compute an\nupper limit to the detectable source intensity. To determine the minimum\nintensity that a source must have for it to be detected, we first define a\ndetection threshold, and then compute the probabilities of detecting sources of\nvarious intensities at the given threshold. The intensity that corresponds to\nthe specified TypeII error probability defines that minimum intensity, and is\nidentified as the upper limit. Thus, an upper limit is a characteristic of the\ndetection procedure rather than the strength of any particular source and\nshould not be confused with confidence intervals or other estimates of source\nintensity. This is particularly important given the large number of catalogs\nthat are being generated from increasingly sensitive surveys. We discuss the\ndifferences between these upper limits and confidence bounds. Both measures are\nuseful quantities that should be reported in order to extract the most science\nfrom catalogs, though they answer different statistical questions: an upper\nbound describes an inference range on the source intensity, while an upper\nlimit calibrates the detection process. We provide a recipe for computing upper\nlimits that applies to all detection algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2010 17:16:15 GMT"}], "update_date": "2016-02-19", "authors_parsed": [["Kashyap", "Vinay L.", ""], ["van Dyk", "David A.", ""], ["Connors", "Alanna", ""], ["Freeman", "Peter", ""], ["Siemiginowska", "Aneta", ""], ["Xu", "Jin", ""], ["Zezas", "Andreas", ""]]}, {"id": "1006.4432", "submitter": "Grace Chiu", "authors": "Grace S. Chiu and Anton H. Westveld", "title": "A Statistical Social Network Model for Consumption Data in Food Webs", "comments": "On 2013-09-05, a revised version entitled \"A Statistical Social\n  Network Model for Consumption Data in Trophic Food Webs\" was accepted for\n  publication in the upcoming Special Issue \"Statistical Methods for Ecology\"\n  in the journal Statistical Methodology", "journal-ref": null, "doi": "10.1016/j.stamet.2013.09.001", "report-no": null, "categories": "stat.ME q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We adapt existing statistical modeling techniques for social networks to\nstudy consumption data observed in trophic food webs. These data describe the\nfeeding volume (non-negative) among organisms grouped into nodes, called\ntrophic species, that form the food web. Model complexity arises due to the\nextensive amount of zeros in the data, as each node in the web is predator/prey\nto only a small number of other trophic species. Many of the zeros are regarded\nas structural (non-random) in the context of feeding behavior. The presence of\nbasal prey and top predator nodes (those who never consume and those who are\nnever consumed, with probability 1) creates additional complexity to the\nstatistical modeling. We develop a special statistical social network model to\naccount for such network features. The model is applied to two empirical food\nwebs; focus is on the web for which the population size of seals is of concern\nto various commercial fisheries.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2010 07:15:44 GMT"}, {"version": "v2", "created": "Tue, 24 Aug 2010 06:58:56 GMT"}, {"version": "v3", "created": "Fri, 12 Jul 2013 15:49:59 GMT"}, {"version": "v4", "created": "Fri, 6 Sep 2013 04:49:23 GMT"}], "update_date": "2013-09-26", "authors_parsed": [["Chiu", "Grace S.", ""], ["Westveld", "Anton H.", ""]]}, {"id": "1006.4642", "submitter": "Badri Padhukasahasram", "authors": "Badri Padhukasahasram, Eran Halperin, Jennifer Wessel, Daryl Thomas,\n  Elana Silver, Heather Trumbower, Michele Cargill, Dietrich Stephan", "title": "Presymptomatic risk assessment for chronic non-communicable diseases", "comments": "Plos ONE paper. Previous version was withdrawn to be updated by the\n  journal's pdf version", "journal-ref": "2010 PLoS ONE 5(12): e14338. doi:10.1371/journal.pone.0014338", "doi": "10.1371/journal.pone.0014338", "report-no": null, "categories": "q-bio.GN q-bio.QM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prevalence of common chronic non-communicable diseases (CNCDs) far\novershadows the prevalence of both monogenic and infectious diseases combined.\nAll CNCDs, also called complex genetic diseases, have a heritable genetic\ncomponent that can be used for pre-symptomatic risk assessment. Common single\nnucleotide polymorphisms (SNPs) that tag risk haplotypes across the genome\ncurrently account for a non-trivial portion of the germ-line genetic risk and\nwe will likely continue to identify the remaining missing heritability in the\nform of rare variants, copy number variants and epigenetic modifications. Here,\nwe describe a novel measure for calculating the lifetime risk of a disease,\ncalled the genetic composite index (GCI), and demonstrate its predictive value\nas a clinical classifier. The GCI only considers summary statistics of the\neffects of genetic variation and hence does not require the results of\nlarge-scale studies simultaneously assessing multiple risk factors. Combining\nGCI scores with environmental risk information provides an additional tool for\nclinical decision-making. The GCI can be populated with heritable risk\ninformation of any type, and thus represents a framework for CNCD\npre-symptomatic risk assessment that can be populated as additional risk\ninformation is identified through next-generation technologies.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2010 20:48:49 GMT"}, {"version": "v2", "created": "Tue, 26 Oct 2010 06:46:47 GMT"}, {"version": "v3", "created": "Tue, 4 Jan 2011 21:56:29 GMT"}, {"version": "v4", "created": "Fri, 24 Jun 2011 19:43:36 GMT"}, {"version": "v5", "created": "Tue, 28 Jun 2011 06:37:41 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Padhukasahasram", "Badri", ""], ["Halperin", "Eran", ""], ["Wessel", "Jennifer", ""], ["Thomas", "Daryl", ""], ["Silver", "Elana", ""], ["Trumbower", "Heather", ""], ["Cargill", "Michele", ""], ["Stephan", "Dietrich", ""]]}, {"id": "1006.4645", "submitter": "Thomas Bartz-Beielstein", "authors": "Thomas Bartz-Beielstein", "title": "SPOT: An R Package For Automatic and Interactive Tuning of Optimization\n  Algorithms by Sequential Parameter Optimization", "comments": "Related software can be downloaded from\n  http://cran.r-project.org/web/packages/SPOT/index.html", "journal-ref": null, "doi": null, "report-no": "CIOP Technical Report 05/10. Cologne University of Applied Sciences", "categories": "cs.NE cs.AI math.OC stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The sequential parameter optimization (SPOT) package for R is a toolbox for\ntuning and understanding simulation and optimization algorithms. Model-based\ninvestigations are common approaches in simulation and optimization. Sequential\nparameter optimization has been developed, because there is a strong need for\nsound statistical analysis of simulation and optimization algorithms. SPOT\nincludes methods for tuning based on classical regression and analysis of\nvariance techniques; tree-based models such as CART and random forest; Gaussian\nprocess models (Kriging), and combinations of different meta-modeling\napproaches. This article exemplifies how SPOT can be used for automatic and\ninteractive tuning.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2010 21:18:36 GMT"}], "update_date": "2010-06-25", "authors_parsed": [["Bartz-Beielstein", "Thomas", ""]]}, {"id": "1006.4929", "submitter": "Caroline Uhler", "authors": "Anna-Sapfo Malaspinas and Caroline Uhler", "title": "Detecting epistasis via Markov bases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid research progress in genotyping techniques have allowed large\ngenome-wide association studies. Existing methods often focus on determining\nassociations between single loci and a specific phenotype. However, a\nparticular phenotype is usually the result of complex relationships between\nmultiple loci and the environment. In this paper, we describe a two-stage\nmethod for detecting epistasis by combining the traditionally used single-locus\nsearch with a search for multiway interactions. Our method is based on an\nextended version of Fisher's exact test. To perform this test, a Markov chain\nis constructed on the space of multidimensional contingency tables using the\nelements of a Markov basis as moves. We test our method on simulated data and\ncompare it to a two-stage logistic regression method and to a fully Bayesian\nmethod, showing that we are able to detect the interacting loci when other\nmethods fail to do so. Finally, we apply our method to a genome-wide data set\nconsisting of 685 dogs and identify epistasis associated with canine hair\nlength for four pairs of SNPs.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2010 08:08:17 GMT"}], "update_date": "2010-06-28", "authors_parsed": [["Malaspinas", "Anna-Sapfo", ""], ["Uhler", "Caroline", ""]]}, {"id": "1006.4968", "submitter": "Sebastian D\\\"ohler", "authors": "Sebastian D\\\"ohler", "title": "Validation of credit default probabilities via multiple testing\n  procedures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply multiple testing procedures to the validation of estimated default\nprobabilities in credit rating systems. The goal is to identify rating classes\nfor which the probability of default is estimated inaccurately, while still\nmaintaining a predefined level of committing type I errors as measured by the\nfamilywise error rate (FWER) and the false discovery rate (FDR). For FWER, we\nalso consider procedures that take possible discreteness of the data resp. test\nstatistics into account. The performance of these methods is illustrated in a\nsimulation setting and for empirical default data.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2010 12:00:44 GMT"}], "update_date": "2010-06-28", "authors_parsed": [["D\u00f6hler", "Sebastian", ""]]}, {"id": "1006.5117", "submitter": "Grace Chiu", "authors": "Penny S Reynolds and Grace S Chiu", "title": "Understanding thermoregulatory transitions during haemorrhage by\n  piecewise regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transition points are common in physiological processes. However the\ntransition between normothermia and hypothermia during haemorrhagic shock has\nrarely been systematically quantified from intensive time series data. We\nestimated the critical transition point (CTP) and provided confidence intervals\nfor core body temperature response to acute severe haemorrhage in a conscious\nrat model. Estimates were obtained by traditional piecewise linear regression\n(broken stick model) and compared to those from the more novel bent cable\nregression. Bent cable regression relaxes the assumption of an abrupt point\ntransition, and thus allows the capture of a potentially gradual transition\nphase; the broken stick is a special case of the bent cable model. We\ncalculated two types of confidence intervals, assuming either independent or\nautoregressive structure for the residuals. In spite of the severity of the\nhaemorrhage, median temperature change was minor (0.8 C; IQR 0.57-1.31 C) and\nonly four of 38 rats were clinically hypothermic (core temperature < 35 C).\nHowever, a transition could be estimated for 23 rats. Bent cable fits were\nsuperior when the transition appeared to be gradual rather than abrupt. In all\ncases, assuming independence gave incorrect uncertainty estimates of CTP. For\n15 animals, neither model could be fitted because of irregular temperature\nprofiles that did not conform to the assumption of a single transition.\nArbitrary imposition of broken stick fits on a gradual transition profile and\nassuming independent rather than autocorrelated error may result in misleading\nestimates of CTP. Identification of the onset of irreversible shock will\nrequire further quantification of appropriate time-dependent physiological\nvariables and their behaviour during haemorrhage.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2010 07:03:05 GMT"}], "update_date": "2010-07-01", "authors_parsed": [["Reynolds", "Penny S", ""], ["Chiu", "Grace S", ""]]}, {"id": "1006.5170", "submitter": "Babak Shahbaba", "authors": "Babak Shahbaba, Robert Tibshirani, Catherine M. Shachaf, and Sylvia K.\n  Plevritis", "title": "Bayesian Gene Set Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gene expression microarray technologies provide the simultaneous measurements\nof a large number of genes. Typical analyses of such data focus on the\nindividual genes, but recent work has demonstrated that evaluating changes in\nexpression across predefined sets of genes often increases statistical power\nand produces more robust results. We introduce a new methodology for\nidentifying gene sets that are differentially expressed under varying\nexperimental conditions. Our approach uses a hierarchical Bayesian framework\nwhere a hyperparameter measures the significance of each gene set. Using\nsimulated data, we compare our proposed method to alternative approaches, such\nas Gene Set Enrichment Analysis (GSEA) and Gene Set Analysis (GSA). Our\napproach provides the best overall performance. We also discuss the application\nof our method to experimental data based on p53 mutation status.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2010 22:11:47 GMT"}], "update_date": "2010-06-29", "authors_parsed": [["Shahbaba", "Babak", ""], ["Tibshirani", "Robert", ""], ["Shachaf", "Catherine M.", ""], ["Plevritis", "Sylvia K.", ""]]}, {"id": "1006.5575", "submitter": "Geoff Nicholls", "authors": "Geoff K. Nicholls and Patrick D. Nunn", "title": "On building and fitting a spatio-temporal change-point model for\n  settlement and growth at Bourewa, Fiji Islands", "comments": "25 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bourewa beach site on the Rove Peninsula of Viti Levu is the earliest\nknown human settlement in the Fiji Islands. How did the settlement at Bourewa\ndevelop in space and time? We have radiocarbon dates on sixty specimens, found\nin association with evidence for human presence, taken from pits across the\nsite. Owing to the lack of diagnostic stratigraphy, there is no direct\narchaeological evidence for distinct phases of occupation through the period of\ninterest. We give a spatio-temporal analysis of settlement at Bourewa in which\nthe deposition rate for dated specimens plays an important role.\nSpatio-temporal mapping of radiocarbon date intensity is confounded by uneven\npost-depositional thinning. We assume that the confounding processes act in\nsuch a way that the absence of dates remains informative of zero rate for the\noriginal deposition process. We model and fit the onset-field, that is, we\nestimate for each location across the site the time at which deposition of\ndatable specimens began. The temporal process generating our spatial\nonset-field is a model of the original settlement dynamics.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2010 11:50:58 GMT"}], "update_date": "2010-06-30", "authors_parsed": [["Nicholls", "Geoff K.", ""], ["Nunn", "Patrick D.", ""]]}]