[{"id": "0907.0199", "submitter": "Susan Buchman", "authors": "Susan M. Buchman, Ann B. Lee, Chad M. Schafer", "title": "High-Dimensional Density Estimation via SCA: An Example in the Modelling\n  of Hurricane Tracks", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present nonparametric techniques for constructing and verifying density\nestimates from high-dimensional data whose irregular dependence structure\ncannot be modelled by parametric multivariate distributions. A low-dimensional\nrepresentation of the data is critical in such situations because of the curse\nof dimensionality. Our proposed methodology consists of three main parts: (1)\ndata reparameterization via dimensionality reduction, wherein the data are\nmapped into a space where standard techniques can be used for density\nestimation and simulation; (2) inverse mapping, in which simulated points are\nmapped back to the high-dimensional input space; and (3) verification, in which\nthe quality of the estimate is assessed by comparing simulated samples with the\nobserved data. These approaches are illustrated via an exploration of the\nspatial variability of tropical cyclones in the North Atlantic; each datum in\nthis case is an entire hurricane trajectory. We conclude the paper with a\ndiscussion of extending the methods to model the relationship between TC\nvariability and climatic variables.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2009 16:55:07 GMT"}], "update_date": "2009-07-02", "authors_parsed": [["Buchman", "Susan M.", ""], ["Lee", "Ann B.", ""], ["Schafer", "Chad M.", ""]]}, {"id": "0907.1254", "submitter": "Jean-Michel Marin", "authors": "Jean-Marie Cornuet (CBGP, INRA, Montpellier), Jean-Michel Marin (I3M,\n  Montpellier), Antonietta Mira (University of Lugano) and Christian P. Robert\n  (Universite Paris Dauphine)", "title": "Adaptive Multiple Importance Sampling", "comments": "20 pages, 3 figures, revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Adaptive Multiple Importance Sampling (AMIS) algorithm is aimed at an\noptimal recycling of past simulations in an iterated importance sampling\nscheme. The difference with earlier adaptive importance sampling\nimplementations like Population Monte Carlo is that the importance weights of\nall simulated values, past as well as present, are recomputed at each\niteration, following the technique of the deterministic multiple mixture\nestimator of Owen and Zhou (2000). Although the convergence properties of the\nalgorithm cannot be fully investigated, we demonstrate through a challenging\nbanana shape target distribution and a population genetics example that the\nimprovement brought by this technique is substantial.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2009 16:29:47 GMT"}, {"version": "v2", "created": "Thu, 16 Dec 2010 13:17:24 GMT"}, {"version": "v3", "created": "Mon, 20 Dec 2010 11:02:21 GMT"}, {"version": "v4", "created": "Wed, 4 May 2011 15:30:58 GMT"}, {"version": "v5", "created": "Mon, 3 Oct 2011 09:48:09 GMT"}], "update_date": "2011-10-04", "authors_parsed": [["Cornuet", "Jean-Marie", "", "CBGP, INRA, Montpellier"], ["Marin", "Jean-Michel", "", "I3M,\n  Montpellier"], ["Mira", "Antonietta", "", "University of Lugano"], ["Robert", "Christian P.", "", "Universite Paris Dauphine"]]}, {"id": "0907.1368", "submitter": "Marie Cottrell", "authors": "Marie Cottrell (CES, SAMOS), Patrice Gaubert (CES, SAMOS), C\\'edric\n  Eloy (DICE), Damien Fran\\c{c}ois (DICE), Geoffroy Hallaux (DICE), J\\'er\\^ome\n  Lacaille (SNECMA), Michel Verleysen (DICE)", "title": "Fault prediction in aircraft engines using Self-Organizing Maps", "comments": "Communication pr\\'esent\\'ee au 7th International Workshop WSOM 09, St\n  Augustine, Floride, USA, June 2009", "journal-ref": "Advances in Self-Organizing Maps, Jos\\'e Principe, Risto\n  Miikkulainen (Ed.) (2009) 37-44", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aircraft engines are designed to be used during several tens of years. Their\nmaintenance is a challenging and costly task, for obvious security reasons. The\ngoal is to ensure a proper operation of the engines, in all conditions, with a\nzero probability of failure, while taking into account aging. The fact that the\nsame engine is sometimes used on several aircrafts has to be taken into account\ntoo. The maintenance can be improved if an efficient procedure for the\nprediction of failures is implemented. The primary source of information on the\nhealth of the engines comes from measurement during flights. Several variables\nsuch as the core speed, the oil pressure and quantity, the fan speed, etc. are\nmeasured, together with environmental variables such as the outside\ntemperature, altitude, aircraft speed, etc. In this paper, we describe the\ndesign of a procedure aiming at visualizing successive data measured on\naircraft engines. The data are multi-dimensional measurements on the engines,\nwhich are projected on a self-organizing map in order to allow us to follow the\ntrajectories of these data over time. The trajectories consist in a succession\nof points on the map, each of them corresponding to the two-dimensional\nprojection of the multi-dimensional vector of engine measurements. Analyzing\nthe trajectories aims at visualizing any deviation from a normal behavior,\nmaking it possible to anticipate an operation failure.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2009 09:31:58 GMT"}], "update_date": "2009-07-10", "authors_parsed": [["Cottrell", "Marie", "", "CES, SAMOS"], ["Gaubert", "Patrice", "", "CES, SAMOS"], ["Eloy", "C\u00e9dric", "", "DICE"], ["Fran\u00e7ois", "Damien", "", "DICE"], ["Hallaux", "Geoffroy", "", "DICE"], ["Lacaille", "J\u00e9r\u00f4me", "", "SNECMA"], ["Verleysen", "Michel", "", "DICE"]]}, {"id": "0907.2079", "submitter": "Yong Zhang", "authors": "Zhaosong Lu and Yong Zhang", "title": "An Augmented Lagrangian Approach for Sparse Principal Component Analysis", "comments": "42 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG math.ST stat.AP stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is a widely used technique for data\nanalysis and dimension reduction with numerous applications in science and\nengineering. However, the standard PCA suffers from the fact that the principal\ncomponents (PCs) are usually linear combinations of all the original variables,\nand it is thus often difficult to interpret the PCs. To alleviate this\ndrawback, various sparse PCA approaches were proposed in literature [15, 6, 17,\n28, 8, 25, 18, 7, 16]. Despite success in achieving sparsity, some important\nproperties enjoyed by the standard PCA are lost in these methods such as\nuncorrelation of PCs and orthogonality of loading vectors. Also, the total\nexplained variance that they attempt to maximize can be too optimistic. In this\npaper we propose a new formulation for sparse PCA, aiming at finding sparse and\nnearly uncorrelated PCs with orthogonal loading vectors while explaining as\nmuch of the total variance as possible. We also develop a novel augmented\nLagrangian method for solving a class of nonsmooth constrained optimization\nproblems, which is well suited for our formulation of sparse PCA. We show that\nit converges to a feasible point, and moreover under some regularity\nassumptions, it converges to a stationary point. Additionally, we propose two\nnonmonotone gradient methods for solving the augmented Lagrangian subproblems,\nand establish their global and local convergence. Finally, we compare our\nsparse PCA approach with several existing methods on synthetic, random, and\nreal data, respectively. The computational results demonstrate that the sparse\nPCs produced by our approach substantially outperform those by other methods in\nterms of total explained variance, correlation of PCs, and orthogonality of\nloading vectors.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2009 00:45:51 GMT"}], "update_date": "2009-07-14", "authors_parsed": [["Lu", "Zhaosong", ""], ["Zhang", "Yong", ""]]}, {"id": "0907.2135", "submitter": "Robert B. Gramacy", "authors": "Robert B. Gramacy and Ester Pantaleo", "title": "Shrinkage regression for multivariate inference with missing data, and\n  an application to portfolio balancing", "comments": "25 pages, 4 figures, 2 tables, to appear in BA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Portfolio balancing requires estimates of covariance between asset returns.\nReturns data have histories which greatly vary in length, since assets begin\npublic trading at different times. This can lead to a huge amount of missing\ndata--too much for the conventional imputation-based approach. Fortunately, a\nwell-known factorization of the MVN likelihood under the prevailing historical\nmissingness pattern leads to a simple algorithm of OLS regressions that is much\nmore reliable. When there are more assets than returns, however, OLS becomes\nunstable. Gramacy, et al. (2008), showed how classical shrinkage regression may\nbe used instead, thus extending the state of the art to much bigger asset\ncollections, with further accuracy and interpretation advantages. In this\npaper, we detail a fully Bayesian hierarchical formulation that extends the\nframework further by allowing for heavy-tailed errors, relaxing the historical\nmissingness assumption, and accounting for estimation risk. We illustrate how\nthis approach compares favorably to the classical one using synthetic data and\nan investment exercise with real returns. An accompanying R package is on CRAN.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2009 11:04:34 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2010 19:05:41 GMT"}, {"version": "v3", "created": "Sat, 27 Feb 2010 10:32:03 GMT"}], "update_date": "2010-02-27", "authors_parsed": [["Gramacy", "Robert B.", ""], ["Pantaleo", "Ester", ""]]}, {"id": "0907.2393", "submitter": "Paul Slater", "authors": "Paul B. Slater", "title": "Multiscale Network Reduction Methodologies: Bistochastic and Disparity\n  Filtering of Human Migration Flows between 3,000+ U. S. Counties", "comments": "35 pages, 12 figures, some rewriting. Dendrogram included as EPAPS\n  file", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.SI physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To control for multiscale effects in networks, one can transform the matrix\nof (in general) weighted, directed internodal flows to bistochastic\n(doubly-stochastic) form, using the iterative proportional fitting\n(Sinkhorn-Knopp) procedure, which alternatively scales row and column sums to\nall equal 1. The dominant entries in the bistochasticized table can then be\nemployed for network reduction, using strong component hierarchical clustering.\nWe illustrate various facets of this well-established, widely-applied two-stage\nalgorithm with the 3, 107 x 3, 107 (asymmetric) 1995-2000 intercounty migration\nflow table for the United States. We compare the results obtained with ones\nusing the disparity filter, for \"extracting the \"multiscale backbone of complex\nweighted networks\", recently put forth by Serrano, Boguna and Vespignani (SBV)\n(Proc. Natl. Acad. Sci. 106 [2009], 6483), upon which we have briefly commented\n(Proc. Natl. Acad. Sci. 106 [2009], E66). The performance of the bistochastic\nfilter appears to be superior-at least in this specific case-in two respects:\n(1) it requires far fewer links to complete a stongly-connected network\nbackbone; and (2) it \"belittles\" small flows and nodes less-a principal\ndesideratum of SBV-in the sense that the correlations of the nonzero raw flows\nare considerably weaker with the corresponding bistochastized links than with\nthe significance levels yielded by the disparity filter. Additional comparative\nstudies--as called for by SBV-of these two filtering procedures, in particular\nas regards their topological properties, should be of considerable interest.\nRelatedly, in its many geographic applications, the two-stage procedure\nhas--with rare exceptions-clustered contiguous areas, often reconstructing\ntraditional regions (islands, for example), even though no contiguity\nconstraints, at all, are imposed beforehand.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2009 16:00:00 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2009 18:22:34 GMT"}, {"version": "v3", "created": "Tue, 19 Jan 2010 20:04:58 GMT"}, {"version": "v4", "created": "Tue, 23 Mar 2010 18:13:13 GMT"}, {"version": "v5", "created": "Thu, 20 May 2010 17:52:39 GMT"}, {"version": "v6", "created": "Fri, 24 Sep 2010 21:41:35 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Slater", "Paul B.", ""]]}, {"id": "0907.2478", "submitter": "Andrew Gelman", "authors": "Andrew Gelman, Jennifer Hill, Masanao Yajima", "title": "Why we (usually) don't have to worry about multiple comparisons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applied researchers often find themselves making statistical inferences in\nsettings that would seem to require multiple comparisons adjustments. We\nchallenge the Type I error paradigm that underlies these corrections. Moreover\nwe posit that the problem of multiple comparisons can disappear entirely when\nviewed from a hierarchical Bayesian perspective. We propose building multilevel\nmodels in the settings where multiple comparisons arise.\n  Multilevel models perform partial pooling (shifting estimates toward each\nother), whereas classical procedures typically keep the centers of intervals\nstationary, adjusting for multiple comparisons by making the intervals wider\n(or, equivalently, adjusting the $p$-values corresponding to intervals of fixed\nwidth). Thus, multilevel models address the multiple comparisons problem and\nalso yield more efficient estimates, especially in settings with low\ngroup-level variation, which is where multiple comparisons are a particular\nconcern.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2009 01:34:16 GMT"}], "update_date": "2009-07-16", "authors_parsed": [["Gelman", "Andrew", ""], ["Hill", "Jennifer", ""], ["Yajima", "Masanao", ""]]}, {"id": "0907.2480", "submitter": "Andrew Gelman", "authors": "Andrew Gelman", "title": "Thoughts on new statistical procedures for age-period-cohort analyses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Age-period-cohort analysis is mathematically intractable because of\nfundamental nonidentifiability of linear trends. However, some understanding\ncan be gained in the context of individual problems.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2009 01:41:24 GMT"}], "update_date": "2009-07-16", "authors_parsed": [["Gelman", "Andrew", ""]]}, {"id": "0907.2770", "submitter": "Lizhen Xu", "authors": "Lizhen Xu, Radu V. Craiu, Lei Sun", "title": "Bayesian methods to overcome the winner's curse in genetic studies", "comments": "Published in at http://dx.doi.org/10.1214/10-AOAS373 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 1, 201-231", "doi": "10.1214/10-AOAS373", "report-no": "IMS-AOAS-AOAS373", "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameter estimates for associated genetic variants, report ed in the initial\ndiscovery samples, are often grossly inflated compared to the values observed\nin the follow-up replication samples. This type of bias is a consequence of the\nsequential procedure in which the estimated effect of an associated genetic\nmarker must first pass a stringent significance threshold. We propose a\nhierarchical Bayes method in which a spike-and-slab prior is used to account\nfor the possibility that the significant test result may be due to chance. We\nexamine the robustness of the method using different priors corresponding to\ndifferent degrees of confidence in the testing results and propose a Bayesian\nmodel averaging procedure to combine estimates produced by different models.\nThe Bayesian estimators yield smaller variance compared to the conditional\nlikelihood estimator and outperform the latter in studies with low power. We\ninvestigate the performance of the method with simulations and applications to\nfour real data examples.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2009 08:01:08 GMT"}, {"version": "v2", "created": "Thu, 14 Apr 2011 11:55:44 GMT"}], "update_date": "2011-04-15", "authors_parsed": [["Xu", "Lizhen", ""], ["Craiu", "Radu V.", ""], ["Sun", "Lei", ""]]}, {"id": "0907.3166", "submitter": "Kathy Dopp", "authors": "Kathy Dopp", "title": "Checking election outcome accuracy Post-election audit sampling", "comments": "37 pages + 19 pages of appendices & references, 3 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article\n  * provides an overview of post-election audit sampling research and compares\nvarious approaches to calculating post-election audit sample sizes, focusing on\nrisklimiting audits,\n  * discusses fundamental concepts common to all risk-limiting post-election\naudits, presenting new margin error bounds, sampling weights and sampling\nprobabilities that improve upon existing approaches and work for any size audit\nunit and for single or multi-winner election contests,\n  * provides two new simple formulas for estimating post-election audit sample\nsizes in cases when detailed data, expertise, or tools are not available,\n  * summarizes four improved methods for calculating risk-limiting election\naudit sample sizes, showing how to apply precise margin error bounds to improve\nthe accuracy and efficacy of existing methods, and\n  * discusses sampling mistakes that reduce post-election audit effectiveness.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2009 21:55:49 GMT"}], "update_date": "2009-09-30", "authors_parsed": [["Dopp", "Kathy", ""]]}, {"id": "0907.3373", "submitter": "John Panaretos", "authors": "John Panaretos and Chrisovaladis Malesios", "title": "Influential Mathematicians: Birth, Education and Affiliation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research output and impact is currently the focus of serious debate\nworldwide. Quantitative analyses based on a wide spectrum of indices indicate a\nclear advantage of US institutions as compared to institutions in Europe and\nthe rest of the world. However the measures used to quantify research\nperformance are mostly static: Even though research output is the result of a\nprocess that extends in time as well as in space, indices often only take into\naccount the current affiliation when assigning influential research to\ninstitutions. In this paper, we focus on the field of mathematics and\ninvestigate whether the image that emerges from static indices persists when\nbringing in more dynamic information, through the study of the \"trajectories\"\nof highly cited mathematicians: birthplace, country of first degree, country of\nPhD and current affiliation. While the dominance of the US remains apparent,\nsome interesting patterns -that perhaps explain this dominance- emerge.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2009 11:00:46 GMT"}], "update_date": "2009-07-21", "authors_parsed": [["Panaretos", "John", ""], ["Malesios", "Chrisovaladis", ""]]}, {"id": "0907.3426", "submitter": "Theodore Alexandrov", "authors": "Theodore Alexandrov, Klaus Steinhorst, Oliver Keszoecze, Stefan\n  Schiffler", "title": "SparseCodePicking: feature extraction in mass spectrometry using sparse\n  coding algorithms", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML physics.med-ph stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mass spectrometry (MS) is an important technique for chemical profiling which\ncalculates for a sample a high dimensional histogram-like spectrum. A crucial\nstep of MS data processing is the peak picking which selects peaks containing\ninformation about molecules with high concentrations which are of interest in\nan MS investigation. We present a new procedure of the peak picking based on a\nsparse coding algorithm. Given a set of spectra of different classes, i.e. with\ndifferent positions and heights of the peaks, this procedure can extract peaks\nby means of unsupervised learning. Instead of an $l_1$-regularization penalty\nterm used in the original sparse coding algorithm we propose using an\nelastic-net penalty term for better regularization. The evaluation is done by\nmeans of simulation. We show that for a large region of parameters the proposed\npeak picking method based on the sparse coding features outperforms a mean\nspectrum-based method. Moreover, we demonstrate the procedure applying it to\ntwo real-life datasets.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2009 15:50:22 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2009 08:58:10 GMT"}], "update_date": "2009-10-05", "authors_parsed": [["Alexandrov", "Theodore", ""], ["Steinhorst", "Klaus", ""], ["Keszoecze", "Oliver", ""], ["Schiffler", "Stefan", ""]]}, {"id": "0907.3708", "submitter": "Filippo Radicchi", "authors": "Andrea Lancichinetti, Filippo Radicchi, Jose J. Ramasco", "title": "Statistical significance of communities in networks", "comments": "9 pages, 8 figures, 2 tables. The software to calculate the C-score\n  can be found at http://filrad.homelinux.org/cscore", "journal-ref": "Phys. Rev. E 81, 046110 (2010)", "doi": "10.1103/PhysRevE.81.046110", "report-no": null, "categories": "physics.soc-ph cond-mat.stat-mech stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nodes in real-world networks are usually organized in local modules. These\ngroups, called communities, are intuitively defined as sub-graphs with a larger\ndensity of internal connections than of external links. In this work, we\nintroduce a new measure aimed at quantifying the statistical significance of\nsingle communities. Extreme and Order Statistics are used to predict the\nstatistics associated with individual clusters in random graphs. These\ndistributions allows us to define one community significance as the probability\nthat a generic clustering algorithm finds such a group in a random graph. The\nmethod is successfully applied in the case of real-world networks for the\nevaluation of the significance of their communities.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2009 18:19:03 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2010 16:47:27 GMT"}], "update_date": "2010-04-21", "authors_parsed": [["Lancichinetti", "Andrea", ""], ["Radicchi", "Filippo", ""], ["Ramasco", "Jose J.", ""]]}, {"id": "0907.4000", "submitter": "Nele Goeyvaerts", "authors": "Nele Goeyvaerts, Niel Hens, Benson Ogunjimi, Marc Aerts, Ziv Shkedy,\n  Pierre Van Damme, Philippe Beutels", "title": "Estimating infectious disease parameters from data on social contacts\n  and serological status", "comments": "25 pages, 6 figures", "journal-ref": "Journal of the Royal Statistical Society, Series C, 2010, Vol. 59,\n  Part 2, p. 255-277, www3.interscience.wiley.com/journal/117997424/home", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In dynamic models of infectious disease transmission, typically various\nmixing patterns are imposed on the so-called Who-Acquires-Infection-From-Whom\nmatrix (WAIFW). These imposed mixing patterns are based on prior knowledge of\nage-related social mixing behavior rather than observations. Alternatively, one\ncan assume that transmission rates for infections transmitted predominantly\nthrough non-sexual social contacts, are proportional to rates of conversational\ncontact which can be estimated from a contact survey. In general, however,\ncontacts reported in social contact surveys are proxies of those events by\nwhich transmission may occur and there may exist age-specific characteristics\nrelated to susceptibility and infectiousness which are not captured by the\ncontact rates. Therefore, in this paper, transmission is modeled as the product\nof two age-specific variables: the age-specific contact rate and an\nage-specific proportionality factor, which entails an improvement of fit for\nthe seroprevalence of the varicella-zoster virus (VZV) in Belgium. Furthermore,\nwe address the impact on the estimation of the basic reproduction number, using\nnon-parametric bootstrapping to account for different sources of variability\nand using multi-model inference to deal with model selection uncertainty. The\nproposed method makes it possible to obtain important information on\ntransmission dynamics that cannot be inferred from approaches traditionally\napplied hitherto.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2009 08:56:30 GMT"}, {"version": "v2", "created": "Tue, 5 Apr 2011 07:48:57 GMT"}], "update_date": "2011-04-06", "authors_parsed": [["Goeyvaerts", "Nele", ""], ["Hens", "Niel", ""], ["Ogunjimi", "Benson", ""], ["Aerts", "Marc", ""], ["Shkedy", "Ziv", ""], ["Van Damme", "Pierre", ""], ["Beutels", "Philippe", ""]]}, {"id": "0907.4728", "submitter": "Alain Celisse", "authors": "Sylvain Arlot (LIENS), Alain Celisse (MIA)", "title": "A survey of cross-validation procedures for model selection", "comments": null, "journal-ref": "Statistics Surveys 4 (2010) 40--79", "doi": "10.1214/09-SS054", "report-no": null, "categories": "math.ST stat.AP stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Used to estimate the risk of an estimator or to perform model selection,\ncross-validation is a widespread strategy because of its simplicity and its\napparent universality. Many results exist on the model selection performances\nof cross-validation procedures. This survey intends to relate these results to\nthe most recent advances of model selection theory, with a particular emphasis\non distinguishing empirical statements from rigorous theoretical results. As a\nconclusion, guidelines are provided for choosing the best cross-validation\nprocedure according to the particular features of the problem in hand.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2009 18:37:23 GMT"}], "update_date": "2011-02-01", "authors_parsed": [["Arlot", "Sylvain", "", "LIENS"], ["Celisse", "Alain", "", "MIA"]]}, {"id": "0907.4849", "submitter": "Giovanni Mana", "authors": "D Calonico, F Levi, L Lorini and G Mana", "title": "Bayesian estimate of the zero-density frequency of a Cs fountain", "comments": "13 pages, 5 figures, submitted to Metrologia, application of the\n  Bayes theorem to metrology", "journal-ref": null, "doi": "10.1088/0026-1394/46/6/004", "report-no": null, "categories": "stat.AP math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Caesium fountain frequency-standards realize the second in the International\nSystem of Units with a relative uncertainty approaching 10^-16. Among the main\ncontributions to the accuracy budget, cold collisions play an important role\nbecause of the atomic density shift of the reference atomic transition. This\npaper describes an application of the Bayesian analysis of the clock frequency\nto estimate the density shift and describes how the Bayes theorem allows the a\npriori knowledge of the sign of the collisional coefficient to be rigourously\nembedded into the analysis. As an application, data from the INRIM caesium\nfountain are used and the Bayesian and orthodox analyses are compared. The\nBayes theorem allows the orthodox uncertainty to be reduced by 28% and\ndemonstrates to be an important tool in primary frequency-metrology.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2009 07:09:05 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Calonico", "D", ""], ["Levi", "F", ""], ["Lorini", "L", ""], ["Mana", "G", ""]]}, {"id": "0907.4865", "submitter": "Denis Belomestny", "authors": "Denis Belomestny", "title": "Spectral estimation of the L\\'evy density in partially observed affine\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The problem of estimating the L\\'evy density of a partially observed\nmultidimensional affine process from low-frequency and mixed-frequency data is\nconsidered. The estimation methodology is based on the log-affine\nrepresentation of the conditional characteristic function of an affine process\nand local linear smoothing in time. We derive almost sure uniform rates of\nconvergence for the estimated L\\'evy density both in mixed-frequency and\nlow-frequency setups and prove that these rates are optimal in the minimax\nsense. Finally, the performance of the estimation algorithms is illustrated in\nthe case of the Bates stochastic volatility model.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2009 08:57:08 GMT"}, {"version": "v2", "created": "Sun, 28 Mar 2010 08:48:26 GMT"}, {"version": "v3", "created": "Tue, 15 Feb 2011 14:07:18 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Belomestny", "Denis", ""]]}, {"id": "0907.4903", "submitter": "Marie-Pierre Etienne", "authors": "Marie-Pierre Etienne, Eric Parent, Benoit Hugues, Bernier Jacques", "title": "Random effects compound Poisson model to represent data with extra zeros", "comments": "48", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a compound Poisson-based random effects structure for\nmodeling zero-inflated data. Data with large proportion of zeros are found in\nmany fields of applied statistics, for example in ecology when trying to model\nand predict species counts (discrete data) or abundance distributions\n(continuous data). Standard methods for modeling such data include mixture and\ntwo-part conditional models. Conversely to these methods, the stochastic models\nproposed here behave coherently with regards to a change of scale, since they\nmimic the harvesting of a marked Poisson process in the modeling steps. Random\neffects are used to account for inhomogeneity. In this paper, model design and\ninference both rely on conditional thinking to understand the links between\nvarious layers of quantities : parameters, latent variables including random\neffects and zero-inflated observations. The potential of these parsimonious\nhierarchical models for zero-inflated data is exemplified using two marine\nmacroinvertebrate abundance datasets from a large scale scientific bottom-trawl\nsurvey. The EM algorithm with a Monte Carlo step based on importance sampling\nis checked for this model structure on a simulated dataset : it proves to work\nwell for parameter estimation but parameter values matter when re-assessing the\nactual coverage level of the confidence regions far from the asymptotic\nconditions.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2009 12:45:15 GMT"}], "update_date": "2009-07-29", "authors_parsed": [["Etienne", "Marie-Pierre", ""], ["Parent", "Eric", ""], ["Hugues", "Benoit", ""], ["Jacques", "Bernier", ""]]}, {"id": "0907.4970", "submitter": "Luk Arnaut", "authors": "L. R. Arnaut", "title": "Sampling Distributions of Random Electromagnetic Fields in Mesoscopic or\n  Dynamical Systems", "comments": "12 pages, 15 figures, accepted for publication in Phys. Rev. E, minor\n  typos corrected", "journal-ref": null, "doi": "10.1103/PhysRevE.80.036601", "report-no": null, "categories": "cond-mat.mes-hall cond-mat.stat-mech math.DS math.ST nlin.CD physics.optics stat.AP stat.TH", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  We derive the sampling probability density function (pdf) of an ideal\nlocalized random electromagnetic field, its amplitude and intensity in an\nelectromagnetic environment that is quasi-statically time-varying statistically\nhomogeneous or static statistically inhomogeneous. The results allow for the\nestimation of field statistics and confidence intervals when a single spatial\nor temporal stochastic process produces randomization of the field. Results for\nboth coherent and incoherent detection techniques are derived, for Cartesian,\nplanar and full-vectorial fields. We show that the functional form of the\nsampling pdf depends on whether the random variable is dimensioned (e.g., the\nsampled electric field proper) or is expressed in dimensionless standardized or\nnormalized form (e.g., the sampled electric field divided by its sampled\nstandard deviation). For dimensioned quantities, the electric field, its\namplitude and intensity exhibit different types of\n  Bessel $K$ sampling pdfs, which differ significantly from the asymptotic\nGauss normal and $\\chi^{(2)}_{2p}$ ensemble pdfs when $\\nu$ is relatively\nsmall. By contrast, for the corresponding standardized quantities, Student $t$,\nFisher-Snedecor $F$ and root-$F$ sampling pdfs are obtained that exhibit\nheavier tails than comparable Bessel $K$ pdfs. Statistical uncertainties\nobtained from classical small-sample theory for dimensionless quantities are\nshown to be overestimated compared to dimensioned quantities. Differences in\nthe sampling pdfs arising from de-normalization versus de-standardization are\nobtained.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2009 17:40:16 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2009 11:04:03 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Arnaut", "L. R.", ""]]}, {"id": "0907.5024", "submitter": "Aris L. Moustakas", "authors": "P. Kazakopoulos, P. Mertikopoulos, A. L. Moustakas and G. Caire", "title": "Living at the Edge: A Large Deviations Approach to the Outage MIMO\n  Capacity", "comments": "Accepted for publication, IEEE Transactions on Information Theory\n  (2010). Part of this work appears in the Proc. IEEE Information Theory\n  Workshop, June 2009, Volos, Greece", "journal-ref": "IEEE Transactions on Information Theory, vol. 57, no 4, p. 1984,\n  April 2011", "doi": "10.1109/TIT.2011.2112050", "report-no": null, "categories": "cs.IT cond-mat.stat-mech math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using a large deviations approach we calculate the probability distribution\nof the mutual information of MIMO channels in the limit of large antenna\nnumbers. In contrast to previous methods that only focused at the distribution\nclose to its mean (thus obtaining an asymptotically Gaussian distribution), we\ncalculate the full distribution, including its tails which strongly deviate\nfrom the Gaussian behavior near the mean. The resulting distribution\ninterpolates seamlessly between the Gaussian approximation for rates $R$ close\nto the ergodic value of the mutual information and the approach of Zheng and\nTse for large signal to noise ratios $\\rho$. This calculation provides us with\na tool to obtain outage probabilities analytically at any point in the $(R,\n\\rho, N)$ parameter space, as long as the number of antennas $N$ is not too\nsmall. In addition, this method also yields the probability distribution of\neigenvalues constrained in the subspace where the mutual information per\nantenna is fixed to $R$ for a given $\\rho$. Quite remarkably, this eigenvalue\ndensity is of the form of the Marcenko-Pastur distribution with square-root\nsingularities, and it depends on the values of $R$ and $\\rho$.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2009 23:41:51 GMT"}, {"version": "v2", "created": "Thu, 14 Oct 2010 12:34:53 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Kazakopoulos", "P.", ""], ["Mertikopoulos", "P.", ""], ["Moustakas", "A. L.", ""], ["Caire", "G.", ""]]}, {"id": "0907.5126", "submitter": "John Panaretos", "authors": "John Panaretos, Chrisovaladis Malesios", "title": "A population-modulated bibliometric measure with an application in the\n  field of statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.DL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use confirmatory factor analysis to derive a unifying measure of\ncomparison of scientists based on bibliometric measurements, by utilizing the\nh-index, some similar h-type indices as well as other common measures of\nscientific performance. We use a real data example from nine well-known\ndepartments of statistics to demonstrate our approach and argue that our\ncombined measure results in a better overall evaluation of a researchers'\nscientific work.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2009 13:38:55 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2009 08:08:54 GMT"}], "update_date": "2009-08-05", "authors_parsed": [["Panaretos", "John", ""], ["Malesios", "Chrisovaladis", ""]]}]