[{"id": "1405.0131", "submitter": "Daniel Kosiorowski", "authors": "Daniel Kosiorowski (Department of Statistics, Cracow University of\n  Economics)", "title": "Two Procedures for Robust Monitoring of Probability Distributions of\n  Economic Data Streams induced by Depth Functions", "comments": "Operations Research and Decisions, vol. 25, No. 1, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data streams (streaming data) consist of transiently observed, evolving in\ntime, multidimensional data sequences that challenge our computational and/or\ninferential capabilities. In this paper we propose user friendly approaches for\nrobust monitoring of selected properties of unconditional and conditional\ndistribution of the stream basing on depth functions. Our proposals are robust\nto a small fraction of outliers and/or inliers but sensitive to a regime change\nof the stream at the same time. Their implementations are available in our free\nR package DepthProc.\n", "versions": [{"version": "v1", "created": "Thu, 1 May 2014 10:51:29 GMT"}, {"version": "v2", "created": "Sun, 28 Dec 2014 11:35:41 GMT"}, {"version": "v3", "created": "Wed, 14 Jan 2015 18:59:42 GMT"}, {"version": "v4", "created": "Sat, 17 Jan 2015 11:26:15 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["Kosiorowski", "Daniel", "", "Department of Statistics, Cracow University of\n  Economics"]]}, {"id": "1405.0212", "submitter": "Siamak Yousefi mr", "authors": "Siamak Yousefi, Xiao-Wen Chang, Benoit Champagne", "title": "Mobile Localization in Non-Line-of-Sight Using Constrained Square-Root\n  Unscented Kalman Filter", "comments": "Under review by IEEE Trans. on Vehicular Technology", "journal-ref": null, "doi": "10.1109/TVT.2014.2339734", "report-no": null, "categories": "stat.AP cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Localization and tracking of a mobile node (MN) in non-line-of-sight (NLOS)\nscenarios, based on time of arrival (TOA) measurements, is considered in this\nwork. To this end, we develop a constrained form of square root unscented\nKalman filter (SRUKF), where the sigma points of the unscented transformation\nare projected onto the feasible region by solving constrained optimization\nproblems. The feasible region is the intersection of several discs formed by\nthe NLOS measurements. We show how we can reduce the size of the optimization\nproblem and formulate it as a convex quadratically constrained quadratic\nprogram (QCQP), which depends on the Cholesky factor of the \\textit{a\nposteriori} error covariance matrix of SRUKF. As a result of these\nmodifications, the proposed constrained SRUKF (CSRUKF) is more efficient and\nhas better numerical stability compared to the constrained UKF. Through\nsimulations, we also show that the CSRUKF achieves a smaller localization error\ncompared to other techniques and that its performance is robust under different\nNLOS conditions.\n", "versions": [{"version": "v1", "created": "Thu, 1 May 2014 16:34:33 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Yousefi", "Siamak", ""], ["Chang", "Xiao-Wen", ""], ["Champagne", "Benoit", ""]]}, {"id": "1405.0231", "submitter": "Alexander Franks", "authors": "Alexander Franks, Andrew Miller, Luke Bornn, Kirk Goldsberry", "title": "Characterizing the spatial structure of defensive skill in professional\n  basketball", "comments": "Published at http://dx.doi.org/10.1214/14-AOAS799 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 1, 94-121", "doi": "10.1214/14-AOAS799", "report-no": "IMS-AOAS-AOAS799", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although basketball is a dualistic sport, with all players competing on both\noffense and defense, almost all of the sport's conventional metrics are\ndesigned to summarize offensive play. As a result, player valuations are\nlargely based on offensive performances and to a much lesser degree on\ndefensive ones. Steals, blocks and defensive rebounds provide only a limited\nsummary of defensive effectiveness, yet they persist because they summarize\nsalient events that are easy to observe. Due to the inefficacy of traditional\ndefensive statistics, the state of the art in defensive analytics remains\nqualitative, based on expert intuition and analysis that can be prone to human\nbiases and imprecision. Fortunately, emerging optical player tracking systems\nhave the potential to enable a richer quantitative characterization of\nbasketball performance, particularly defensive performance. Unfortunately, due\nto computational and methodological complexities, that potential remains unmet.\nThis paper attempts to fill this void, combining spatial and spatio-temporal\nprocesses, matrix factorization techniques and hierarchical regression models\nwith player tracking data to advance the state of defensive analytics in the\nNBA. Our approach detects, characterizes and quantifies multiple aspects of\ndefensive play in basketball, supporting some common understandings of\ndefensive effectiveness, challenging others and opening up many new insights\ninto the defensive elements of basketball.\n", "versions": [{"version": "v1", "created": "Thu, 1 May 2014 17:39:21 GMT"}, {"version": "v2", "created": "Mon, 15 Dec 2014 02:37:42 GMT"}, {"version": "v3", "created": "Thu, 28 May 2015 10:57:57 GMT"}], "update_date": "2015-05-29", "authors_parsed": [["Franks", "Alexander", ""], ["Miller", "Andrew", ""], ["Bornn", "Luke", ""], ["Goldsberry", "Kirk", ""]]}, {"id": "1405.0294", "submitter": "Julio C\\'esar Hern\\'andez S\\'anchez", "authors": "Jos\\'e Luis Vicente-Villard\\'on, Julio C\\'esar Hern\\'andez S\\'anchez", "title": "Logistic Biplots for Ordinal Data with an Application to Job\n  Satisfaction of Doctorate Degree Holders in Spain", "comments": "26 pages, 11 figures,2 tables. arXiv admin note: text overlap with\n  arXiv:1309.5486", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biplot Methods allow for the simultaneous representation of individuals and\nvariables of a data matrix. For Binary or Nominal data, Logistic biplots have\nbeen recently developed to extend the classical linear representations for\ncontinuous data. When data are ordinal, linear, binary or nominal logistic\nbiplots are not adequate and techniques as Categorical Principal Component\nAnalysis (CATPCA) or Item Response Theory (IRT) for ordinal items should be\nused instead.\n  In this paper we extend the Biplot to ordinal data. The resulting method is\ntermed Ordinal Logistic Biplot (OLB). Row scores are computed to have ordinal\nlogistic responses along the dimensions and column parameters produce logistic\nresponse surfaces that, projected onto the space spanned by the row scores,\ndefine a linear biplot. A proportional odds model is used, obtaining a\nmultidimensional model known as graded response model in the Item Response\nTheory literature. We study the geometry of such a representation and construct\ncomputational algorithms for the estimation of parameters and the calculation\nof prediction directions. Ordinal Logistic Biplots extend both CATPCA and IRT\nin the sense that gives a graphical representation for IRT similar to the\nbiplot for CATPCA.\n  The main theoretical results are applied to the study of job satisfaction of\ndoctorate (PhD) holders in Spain. Holders of doctorate degrees or other\nresearch qualifications are crucial to the creation, commercialization and\ndissemination of knowledge and to innovation. The proposed methods are used to\nextract useful information from the Spanish data from the international 'Survey\non the careers of doctorate holders (CDH)', jointly carried out Eurostat, the\nOrganisation for Economic Co-operation and Development (OECD) and UNESCO's\nInstitute for Statistics (UIS).\n", "versions": [{"version": "v1", "created": "Thu, 1 May 2014 20:06:17 GMT"}], "update_date": "2014-05-05", "authors_parsed": [["Vicente-Villard\u00f3n", "Jos\u00e9 Luis", ""], ["S\u00e1nchez", "Julio C\u00e9sar Hern\u00e1ndez", ""]]}, {"id": "1405.0595", "submitter": "Enkelejd Hashorva", "authors": "Julia Farkas and Enkelejd Hashorva", "title": "Tail approximation for reinsurance portfolios of Gaussian-like risks", "comments": "In press, Scandinavian Actuarial Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two different portfolios of proportional reinsurance of the same\npool of risks. This contribution is concerned with Gaussian-like risks, which\nmeans that for large values the survival function of such risks is, up to a\nmultiplier, the same as that of a standard Gaussian risk. We establish the tail\nasymptotic behavior of the total loss of each of the reinsurance portfolios and\ndetermine also the relation between randomly scaled Gaussian-like portfolios\nand unscaled ones. Further we show that jointly two portfolios of Gaussian-like\nrisks exhibit asymptotic independence and their weak tail dependence\ncoefficient is non-negative.\n", "versions": [{"version": "v1", "created": "Sat, 3 May 2014 14:54:20 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Farkas", "Julia", ""], ["Hashorva", "Enkelejd", ""]]}, {"id": "1405.0607", "submitter": "Enkelejd Hashorva", "authors": "D. Kortschak and E. Hashorva", "title": "Efficient simulation of tail probabilities for sums of log-elliptical\n  risks", "comments": null, "journal-ref": "2013 Journal of Computational and Applied Mathematics, 247, 53-67", "doi": "10.1016/j.cam.2012.11.025", "report-no": null, "categories": "math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the framework of dependent risks it is a crucial task for risk management\npurposes to quantify the probability that the aggregated risk exceeds some\nlarge value u. Motivated by Asmussen et al. (2011) in this paper we introduce a\nmodified Asmussen-Kroese estimator for simulation of the rare event that the\naggregated risk exceeds u. We show that in the framework of log-Gaussian risks\nour novel estimator has the best possible performance. For the more general\nclass of log-elliptical risks with marginal distributions in the Gumbel\nmax-domain of attraction we propose a modified Rojas-Nandayapa estimator of the\nrare events of interest. Numerical results demonstrate the excellent\nperformance of our novel Asmussen-Kroese algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 3 May 2014 17:01:15 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Kortschak", "D.", ""], ["Hashorva", "E.", ""]]}, {"id": "1405.0673", "submitter": "Rodrigo Labouriau", "authors": "Rodrigo Labouriau", "title": "A Note on the Identifiability of Generalized Linear Mixed Models", "comments": "9 pages, no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I present here a simple proof that, under general regularity conditions, the\nstandard parametrization of generalized linear mixed model is identifiable. The\nproof is based on the assumptions of generalized linear mixed models on the\nfirst and second order moments and some general mild regularity conditions,\nand, therefore, is extensible to quasi-likelihood based generalized linear\nmodels. In particular, binomial and Poisson mixed models with dispersion\nparameter are identifiable when equipped with the standard parametrization.\n", "versions": [{"version": "v1", "created": "Sun, 4 May 2014 09:36:03 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Labouriau", "Rodrigo", ""]]}, {"id": "1405.0723", "submitter": "Dimitris Vavoulis", "authors": "Dimitrios V Vavoulis and Margherita Francescatto and Peter Heutink and\n  Julian Gough", "title": "DGEclust: differential expression analysis of clustered count data", "comments": "26 pages, 7 figures", "journal-ref": "Genome Biology 2015, 16:39", "doi": "10.1186/s13059-015-0604-6", "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most published studies on the statistical analysis of count data generated by\nnext-generation sequencing technologies have paid surprisingly little attention\non cluster analysis. We present a statistical methodology (DGEclust) for\nclustering digital expression data, which (contrary to alternative methods)\nsimultaneously addresses the problem of model selection (i.e. how many clusters\nare supported by the data) and uncertainty in parameter estimation. We show how\nthis methodology can be utilised in differential expression analysis and we\ndemonstrate its applicability on a more general class of problems and higher\naccuracy, when compared to popular alternatives. DGEclust is freely available\nat https://bitbucket.org/DimitrisVavoulis/dgeclust\n", "versions": [{"version": "v1", "created": "Sun, 4 May 2014 17:36:45 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Vavoulis", "Dimitrios V", ""], ["Francescatto", "Margherita", ""], ["Heutink", "Peter", ""], ["Gough", "Julian", ""]]}, {"id": "1405.0785", "submitter": "Nicolle Clements", "authors": "Nicolle Clements, Sanat K. Sarkar, Zhigen Zhao, Dong-Yun Kim", "title": "Applying multiple testing procedures to detect change in East African\n  vegetation", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS686 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 1, 286-308", "doi": "10.1214/13-AOAS686", "report-no": "IMS-AOAS-AOAS686", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of vegetation fluctuations gives valuable information toward\neffective land use and development. We consider this problem for the East\nAfrican region based on the Normalized Difference Vegetation Index (NDVI)\nseries from satellite remote sensing data collected between 1982 and 2006 over\n8-kilometer grid points. We detect areas with significant increasing or\ndecreasing monotonic vegetation changes using a multiple testing procedure\ncontrolling the mixed directional false discovery rate (mdFDR). Specifically,\nwe use a three-stage directional Benjamini--Hochberg (BH) procedure with proven\nmdFDR control under independence and a suitable adaptive version of it. The\nperformance of these procedures is studied through simulations before applying\nthem to the vegetation data. Our analysis shows increasing vegetation in the\nNorthern hemisphere as well as coastal Tanzania and generally decreasing\nSouthern hemisphere vegetation trends, which are consistent with historical\nevidence.\n", "versions": [{"version": "v1", "created": "Mon, 5 May 2014 06:10:33 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Clements", "Nicolle", ""], ["Sarkar", "Sanat K.", ""], ["Zhao", "Zhigen", ""], ["Kim", "Dong-Yun", ""]]}, {"id": "1405.0788", "submitter": "David Rossell", "authors": "David Rossell, Camille Stephan-Otto Attolini, Manuel Kroiss, Almond\n  St\\\"ocker", "title": "Quantifying alternative splicing from paired-end RNA-sequencing data", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS687 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org). With corrections", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 1, 309-330", "doi": "10.1214/13-AOAS687", "report-no": "IMS-AOAS-AOAS687", "categories": "stat.AP q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RNA-sequencing has revolutionized biomedical research and, in particular, our\nability to study gene alternative splicing. The problem has important\nimplications for human health, as alternative splicing may be involved in\nmalfunctions at the cellular level and multiple diseases. However, the\nhigh-dimensional nature of the data and the existence of experimental biases\npose serious data analysis challenges. We find that the standard data summaries\nused to study alternative splicing are severely limited, as they ignore a\nsubstantial amount of valuable information. Current data analysis methods are\nbased on such summaries and are hence suboptimal. Further, they have limited\nflexibility in accounting for technical biases. We propose novel data summaries\nand a Bayesian modeling framework that overcome these limitations and determine\nbiases in a nonparametric, highly flexible manner. These summaries adapt\nnaturally to the rapid improvements in sequencing technology. We provide\nefficient point estimates and uncertainty assessments. The approach allows to\nstudy alternative splicing patterns for individual samples and can also be the\nbasis for downstream analyses. We found a severalfold improvement in estimation\nmean square error compared popular approaches in simulations, and substantially\nhigher consistency between replicates in experimental data. Our findings\nindicate the need for adjusting the routine summarization and analysis of\nalternative splicing RNA-seq studies. We provide a software implementation in\nthe R package casper.\n", "versions": [{"version": "v1", "created": "Mon, 5 May 2014 06:24:09 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2015 07:50:57 GMT"}], "update_date": "2015-12-11", "authors_parsed": [["Rossell", "David", ""], ["Attolini", "Camille Stephan-Otto", ""], ["Kroiss", "Manuel", ""], ["St\u00f6cker", "Almond", ""]]}, {"id": "1405.0803", "submitter": "Jingyong Su", "authors": "Jingyong Su, Sebastian Kurtek, Eric Klassen, Anuj Srivastava", "title": "Statistical analysis of trajectories on Riemannian manifolds: Bird\n  migration, hurricane tracking and video surveillance", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS701 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 1, 530-552", "doi": "10.1214/13-AOAS701", "report-no": "IMS-AOAS-AOAS701", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the statistical analysis of trajectories on Riemannian manifolds\nthat are observed under arbitrary temporal evolutions. Past methods rely on\ncross-sectional analysis, with the given temporal registration, and\nconsequently may lose the mean structure and artificially inflate observed\nvariances. We introduce a quantity that provides both a cost function for\ntemporal registration and a proper distance for comparison of trajectories.\nThis distance is used to define statistical summaries, such as sample means and\ncovariances, of synchronized trajectories and \"Gaussian-type\" models to capture\ntheir variability at discrete times. It is invariant to identical time-warpings\n(or temporal reparameterizations) of trajectories. This is based on a novel\nmathematical representation of trajectories, termed transported square-root\nvector field (TSRVF), and the $\\mathbb{L}^2$ norm on the space of TSRVFs. We\nillustrate this framework using three representative\nmanifolds---$\\mathbb{S}^2$, $\\mathrm {SE}(2)$ and shape space of planar\ncontours---involving both simulated and real data. In particular, we\ndemonstrate: (1) improvements in mean structures and significant reductions in\ncross-sectional variances using real data sets, (2) statistical modeling for\ncapturing variability in aligned trajectories, and (3) evaluating random\ntrajectories under these models. Experimental results concern bird migration,\nhurricane tracking and video surveillance.\n", "versions": [{"version": "v1", "created": "Mon, 5 May 2014 07:34:41 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Su", "Jingyong", ""], ["Kurtek", "Sebastian", ""], ["Klassen", "Eric", ""], ["Srivastava", "Anuj", ""]]}, {"id": "1405.0807", "submitter": "Nicolas Raillard", "authors": "Nicolas Raillard, Pierre Ailliot, Jianfeng Yao", "title": "Modeling extreme values of processes observed at irregular time steps:\n  Application to significant wave height", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS711 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 1, 622-647", "doi": "10.1214/13-AOAS711", "report-no": "IMS-AOAS-AOAS711", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is motivated by the analysis of the extremal behavior of buoy and\nsatellite data describing wave conditions in the North Atlantic Ocean. The\navailable data sets consist of time series of significant wave height (Hs) with\nirregular time sampling. In such a situation, the usual statistical methods for\nanalyzing extreme values cannot be used directly. The method proposed in this\npaper is an extension of the peaks over threshold (POT) method, where the\ndistribution of a process above a high threshold is approximated by a\nmax-stable process whose parameters are estimated by maximizing a composite\nlikelihood function. The efficiency of the proposed method is assessed on an\nextensive set of simulated data. It is shown, in particular, that the method is\nable to describe the extremal behavior of several common time series models\nwith regular or irregular time sampling. The method is then used to analyze Hs\ndata in the North Atlantic Ocean. The results indicate that it is possible to\nderive realistic estimates of the extremal properties of Hs from satellite\ndata, despite its complex space--time sampling.\n", "versions": [{"version": "v1", "created": "Mon, 5 May 2014 07:44:26 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Raillard", "Nicolas", ""], ["Ailliot", "Pierre", ""], ["Yao", "Jianfeng", ""]]}, {"id": "1405.0808", "submitter": "Abhik Ghosh", "authors": "Abhik Ghosh", "title": "Divergence based Robust Estimation of the Tail Index through An\n  Exponential Regression Model", "comments": "Pre-Print, 35 pages, To appear in \"Statistical Methods and\n  Applications\"", "journal-ref": "Statistical Methods & Applications (2017), Volume 26, Issue 2, pp\n  181--213", "doi": "10.1007/s10260-016-0364-9", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extreme value theory is very popular in applied sciences including\nFinance, economics, hydrology and many other disciplines. In univariate extreme\nvalue theory, we model the data by a suitable distribution from the general\nmax-domain of attraction (MAD) characterized by its tail index; there are three\nbroad classes of tails -- the Pareto type, the Weibull type and the Gumbel\ntype. The simplest and most common estimator of the tail index is the Hill\nestimator that works only for Pareto type tails and has a high bias; it is also\nhighly non-robust in presence of outliers with respect to the assumed model.\nThere have been some recent attempts to produce asymptotically unbiased or\nrobust alternative to the Hill estimator; however all the robust alternatives\nwork for any one type of tail. This paper proposes a new general estimator of\nthe tail index that is both robust and has smaller bias under all the three\ntail types compared to the existing robust estimators. This essentially\nproduces a robust generalization of the estimator proposed by Matthys and\nBeirlant (2003) under the same model approximation through a suitable\nexponential regression framework using the density power divergence. The\nrobustness properties of the estimator are derived in the paper along with an\nextensive simulation study. A method for bias correction is also proposed with\napplication to some real data examples.\n", "versions": [{"version": "v1", "created": "Mon, 5 May 2014 07:44:57 GMT"}, {"version": "v2", "created": "Fri, 1 Jul 2016 19:44:58 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Ghosh", "Abhik", ""]]}, {"id": "1405.0849", "submitter": "Xin Wang", "authors": "Xin Wang, Ke Yuan, Christoph Hellmayr, Wei Liu, Florian Markowetz", "title": "Reconstructing evolving signalling networks by hidden Markov nested\n  effects models", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS696 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 1, 448-480", "doi": "10.1214/13-AOAS696", "report-no": "IMS-AOAS-AOAS696", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring time-varying networks is important to understand the development\nand evolution of interactions over time. However, the vast majority of\ncurrently used models assume direct measurements of node states, which are\noften difficult to obtain, especially in fields like cell biology, where\nperturbation experiments often only provide indirect information of network\nstructure. Here we propose hidden Markov nested effects models (HM-NEMs) to\nmodel the evolving network by a Markov chain on a state space of signalling\nnetworks, which are derived from nested effects models (NEMs) of indirect\nperturbation data. To infer the hidden network evolution and unknown parameter,\na Gibbs sampler is developed, in which sampling network structure is\nfacilitated by a novel structural Metropolis--Hastings algorithm. We\ndemonstrate the potential of HM-NEMs by simulations on synthetic time-series\nperturbation data. We also show the applicability of HM-NEMs in two real\nbiological case studies, in one capturing dynamic crosstalk during the\nprogression of neutrophil polarisation, and in the other inferring an evolving\nnetwork underlying early differentiation of mouse embryonic stem cells.\n", "versions": [{"version": "v1", "created": "Mon, 5 May 2014 10:31:17 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Wang", "Xin", ""], ["Yuan", "Ke", ""], ["Hellmayr", "Christoph", ""], ["Liu", "Wei", ""], ["Markowetz", "Florian", ""]]}, {"id": "1405.0922", "submitter": "Aaron Fisher", "authors": "Aaron Fisher, Brian Caffo, Brian Schwartz and Vadim Zipunnikov", "title": "Fast, Exact Bootstrap Principal Component Analysis for p>1 million", "comments": "25 pages, including 9 figures and link to R package. 2014-05-14\n  update: final formatting edits for journal submission, condensed figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many have suggested a bootstrap procedure for estimating the sampling\nvariability of principal component analysis (PCA) results. However, when the\nnumber of measurements per subject ($p$) is much larger than the number of\nsubjects ($n$), the challenge of calculating and storing the leading principal\ncomponents from each bootstrap sample can be computationally infeasible. To\naddress this, we outline methods for fast, exact calculation of bootstrap\nprincipal components, eigenvalues, and scores. Our methods leverage the fact\nthat all bootstrap samples occupy the same $n$-dimensional subspace as the\noriginal sample. As a result, all bootstrap principal components are limited to\nthe same $n$-dimensional subspace and can be efficiently represented by their\nlow dimensional coordinates in that subspace. Several uncertainty metrics can\nbe computed solely based on the bootstrap distribution of these low dimensional\ncoordinates, without calculating or storing the $p$-dimensional bootstrap\ncomponents. Fast bootstrap PCA is applied to a dataset of sleep\nelectroencephalogram (EEG) recordings ($p=900$, $n=392$), and to a dataset of\nbrain magnetic resonance images (MRIs) ($p\\approx$ 3 million, $n=352$). For the\nbrain MRI dataset, our method allows for standard errors for the first 3\nprincipal components based on 1000 bootstrap samples to be calculated on a\nstandard laptop in 47 minutes, as opposed to approximately 4 days with standard\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 5 May 2014 15:19:42 GMT"}, {"version": "v2", "created": "Tue, 6 May 2014 22:04:00 GMT"}, {"version": "v3", "created": "Wed, 14 May 2014 14:12:12 GMT"}], "update_date": "2014-05-15", "authors_parsed": [["Fisher", "Aaron", ""], ["Caffo", "Brian", ""], ["Schwartz", "Brian", ""], ["Zipunnikov", "Vadim", ""]]}, {"id": "1405.1164", "submitter": "Charles-Alban Deledalle", "authors": "Charles-Alban Deledalle (IMB), Samuel Vaiter (CEREMADE), Jalal M.\n  Fadili (GREYC), Gabriel Peyr\\'e (CEREMADE)", "title": "Stein Unbiased GrAdient estimator of the Risk (SUGAR) for multiple\n  parameter selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms to solve variational regularization of ill-posed inverse problems\nusually involve operators that depend on a collection of continuous parameters.\nWhen these operators enjoy some (local) regularity, these parameters can be\nselected using the so-called Stein Unbiased Risk Estimate (SURE). While this\nselection is usually performed by exhaustive search, we address in this work\nthe problem of using the SURE to efficiently optimize for a collection of\ncontinuous parameters of the model. When considering non-smooth regularizers,\nsuch as the popular l1-norm corresponding to soft-thresholding mapping, the\nSURE is a discontinuous function of the parameters preventing the use of\ngradient descent optimization techniques. Instead, we focus on an approximation\nof the SURE based on finite differences as proposed in (Ramani et al., 2008).\nUnder mild assumptions on the estimation mapping, we show that this\napproximation is a weakly differentiable function of the parameters and its\nweak gradient, coined the Stein Unbiased GrAdient estimator of the Risk\n(SUGAR), provides an asymptotically (with respect to the data dimension)\nunbiased estimate of the gradient of the risk. Moreover, in the particular case\nof soft-thresholding, it is proved to be also a consistent estimator. This\ngradient estimate can then be used as a basis to perform a quasi-Newton\noptimization. The computation of the SUGAR relies on the closed-form (weak)\ndifferentiation of the non-smooth function. We provide its expression for a\nlarge class of iterative methods including proximal splitting ones and apply\nour strategy to regularizations involving non-smooth convex structured\npenalties. Illustrations on various image restoration and matrix completion\nproblems are given.\n", "versions": [{"version": "v1", "created": "Tue, 6 May 2014 06:34:57 GMT"}, {"version": "v2", "created": "Sat, 9 Aug 2014 14:28:21 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Deledalle", "Charles-Alban", "", "IMB"], ["Vaiter", "Samuel", "", "CEREMADE"], ["Fadili", "Jalal M.", "", "GREYC"], ["Peyr\u00e9", "Gabriel", "", "CEREMADE"]]}, {"id": "1405.1444", "submitter": "Robert McGibbon", "authors": "Robert T. McGibbon, Bharath Ramsundar, Mohammad M. Sultan, Gert Kiss,\n  and Vijay S. Pande", "title": "Understanding Protein Dynamics with L1-Regularized Reversible Hidden\n  Markov Models", "comments": null, "journal-ref": "Proceedings of the 31st International Conference on Machine\n  Learning, Beijing, China, 2014", "doi": null, "report-no": null, "categories": "q-bio.BM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a machine learning framework for modeling protein dynamics. Our\napproach uses L1-regularized, reversible hidden Markov models to understand\nlarge protein datasets generated via molecular dynamics simulations. Our model\nis motivated by three design principles: (1) the requirement of massive\nscalability; (2) the need to adhere to relevant physical law; and (3) the\nnecessity of providing accessible interpretations, critical for both cellular\nbiology and rational drug design. We present an EM algorithm for learning and\nintroduce a model selection criteria based on the physical notion of\nconvergence in relaxation timescales. We contrast our model with standard\nmethods in biophysics and demonstrate improved robustness. We implement our\nalgorithm on GPUs and apply the method to two large protein simulation datasets\ngenerated respectively on the NCSA Bluewaters supercomputer and the\nFolding@Home distributed computing network. Our analysis identifies the\nconformational dynamics of the ubiquitin protein critical to cellular\nsignaling, and elucidates the stepwise activation mechanism of the c-Src kinase\nprotein.\n", "versions": [{"version": "v1", "created": "Tue, 6 May 2014 20:16:41 GMT"}], "update_date": "2014-05-08", "authors_parsed": [["McGibbon", "Robert T.", ""], ["Ramsundar", "Bharath", ""], ["Sultan", "Mohammad M.", ""], ["Kiss", "Gert", ""], ["Pande", "Vijay S.", ""]]}, {"id": "1405.1502", "submitter": "Esa Ollila", "authors": "Esa Ollila, Hyon-Jung Kim and Visa Koivunen", "title": "Robust iterative hard thresholding for compressed sensing", "comments": "To appear in Proc. of ISCCSP 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed sensing (CS) or sparse signal reconstruction (SSR) is a signal\nprocessing technique that exploits the fact that acquired data can have a\nsparse representation in some basis. One popular technique to reconstruct or\napproximate the unknown sparse signal is the iterative hard thresholding (IHT)\nwhich however performs very poorly under non-Gaussian noise conditions or in\nthe face of outliers (gross errors). In this paper, we propose a robust IHT\nmethod based on ideas from $M$-estimation that estimates the sparse signal and\nthe scale of the error distribution simultaneously. The method has a negligible\nperformance loss compared to IHT under Gaussian noise, but superior performance\nunder heavy-tailed non-Gaussian noise conditions.\n", "versions": [{"version": "v1", "created": "Wed, 7 May 2014 04:38:45 GMT"}], "update_date": "2014-05-08", "authors_parsed": [["Ollila", "Esa", ""], ["Kim", "Hyon-Jung", ""], ["Koivunen", "Visa", ""]]}, {"id": "1405.1534", "submitter": "Andrea Thum", "authors": "Andrea Thum, Susann M\\\"onchgesang, Lore Westphal, Tilo L\\\"ubken,\n  Sabine Rosahl, Steffen Neumann and Stefan Posch", "title": "Supervised Penalized Canonical Correlation Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The canonical correlation analysis (CCA) is commonly used to analyze data\nsets with paired data, e.g. measurements of gene expression and metabolomic\nintensities of the same experiments. This allows to find interesting\nrelationships between the data sets, e.g. they can be assigned to biological\nprocesses. However, it can be difficult to interpret the processes and often\nthe relationships observed are not related to the experimental design but to\nsome unknown parameters.\n  Here we present an extension of the penalized CCA, the supervised penalized\napproach (spCCA), where the experimental design is used as a third data set and\nthe correlation of the biological data sets with the design data set is\nmaximized to find interpretable and meaningful canonical variables. The spCCA\nwas successfully tested on a data set of Arabidopsis thaliana with gene\nexpression and metabolite intensity measurements and resulted in eight\nsignificant canonical variables and their interpretation. We provide an\nR-package under the GPL license.\n", "versions": [{"version": "v1", "created": "Wed, 7 May 2014 08:58:14 GMT"}], "update_date": "2014-05-08", "authors_parsed": [["Thum", "Andrea", ""], ["M\u00f6nchgesang", "Susann", ""], ["Westphal", "Lore", ""], ["L\u00fcbken", "Tilo", ""], ["Rosahl", "Sabine", ""], ["Neumann", "Steffen", ""], ["Posch", "Stefan", ""]]}, {"id": "1405.1569", "submitter": "Dominic Magirr Dr", "authors": "Dominic Magirr, Thomas Jaki, Franz Koenig, Martin Posch", "title": "Adaptive Survival Trials", "comments": "22 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mid-study design modifications are becoming increasingly accepted in\nconfirmatory clinical trials, so long as appropriate methods are applied such\nthat error rates are controlled. It is therefore unfortunate that the important\ncase of time-to-event endpoints is not easily handled by the standard theory.\nWe analyze current methods that allow design modifications to be based on the\nfull interim data, i.e., not only the observed event times but also secondary\nendpoint and safety data from patients who are yet to have an event. We show\nthat the final test statistic may ignore a substantial subset of the observed\nevent times. Since it is the data corresponding to the earliest recruited\npatients that is ignored, this neglect becomes egregious when there is specific\ninterest in learning about long-term survival. An alternative test\nincorporating all event times is proposed, where a conservative assumption is\nmade in order to guarantee type I error control. We examine the properties of\nour proposed approach using the example of a clinical trial comparing two\ncancer therapies.\n", "versions": [{"version": "v1", "created": "Wed, 7 May 2014 11:01:07 GMT"}], "update_date": "2014-05-08", "authors_parsed": [["Magirr", "Dominic", ""], ["Jaki", "Thomas", ""], ["Koenig", "Franz", ""], ["Posch", "Martin", ""]]}, {"id": "1405.1603", "submitter": "Wei Sun", "authors": "Min Jin Ha, Wei Sun, Jichun Xie", "title": "PenPC: A Two-step Approach to Estimate the Skeletons of High Dimensional\n  Directed Acyclic Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of the skeleton of a directed acyclic graph (DAG) is of great\nimportance for understanding the underlying DAG and causaleffects can be\nassessed from the skeleton when the DAG is notidentifiable. We propose a novel\nmethod named PenPC toestimate the skeleton of a high-dimensional DAG by a\ntwo-stepapproach. We first estimate the non-zero entries of a\nconcentrationmatrix using penalized regression, and then fix the\ndifferencebetween the concentration matrix and the skeleton by evaluating aset\nof conditional independence hypotheses. For high dimensionalproblems where the\nnumber of vertices $p$ is in polynomial orexponential scale of sample size $n$,\nwe study the asymptoticproperty of PenPC on two types of graphs:\ntraditionalrandom graphs where all the vertices have the same expected numberof\nneighbors, and scale-free graphs where a few vertices may have alarge number of\nneighbors. As illustrated by extensive simulationsand applications on gene\nexpression data of cancer patients, PenPChas higher sensitivity and specificity\nthan the standard-of-the-artmethod, the PC-stable algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 7 May 2014 13:37:00 GMT"}], "update_date": "2014-05-08", "authors_parsed": [["Ha", "Min Jin", ""], ["Sun", "Wei", ""], ["Xie", "Jichun", ""]]}, {"id": "1405.1791", "submitter": "Nassim Nicholas Taleb", "authors": "Nassim N Taleb, Raphael Douady", "title": "On the Super-Additivity and Estimation Biases of Quantile Contributions", "comments": null, "journal-ref": "Physica A: Statistical Mechanics and its Applications 429,\n  252-260, 2015", "doi": "10.1016/j.physa.2015.02.038", "report-no": null, "categories": "stat.AP q-fin.RM q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sample measures of top centile contributions to the total (concentration) are\ndownward biased, unstable estimators, extremely sensitive to sample size and\nconcave in accounting for large deviations. It makes them particularly unfit in\ndomains with power law tails, especially for low values of the exponent. These\nestimators can vary over time and increase with the population size, as shown\nin this article, thus providing the illusion of structural changes in\nconcentration. They are also inconsistent under aggregation and mixing\ndistributions, as the weighted average of concentration measures for A and B\nwill tend to be lower than that from A U B. In addition, it can be shown that\nunder such fat tails, increases in the total sum need to be accompanied by\nincreased sample size of the concentration measurement. We examine the\nestimation superadditivity and bias under homogeneous and mixed distributions.\n", "versions": [{"version": "v1", "created": "Thu, 8 May 2014 01:53:03 GMT"}, {"version": "v2", "created": "Sun, 11 May 2014 22:44:14 GMT"}, {"version": "v3", "created": "Wed, 12 Nov 2014 18:46:14 GMT"}], "update_date": "2016-09-05", "authors_parsed": [["Taleb", "Nassim N", ""], ["Douady", "Raphael", ""]]}, {"id": "1405.1976", "submitter": "Brian Reich", "authors": "Brian J. Reich and Beth Gardner", "title": "A spatial capture-recapture model for territorial species", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in field techniques have lead to an increase in spatially-referenced\ncapture-recapture data to estimate a species' population size as well as other\ndemographic parameters and patterns of space usage. Statistical models for\nthese data have assumed that the number of individuals in the population and\ntheir spatial locations follow a homogeneous Poisson point process model, which\nimplies that the individuals are uniformly and independently distributed over\nthe spatial domain of interest. In many applications there is reason to\nquestion independence, for example when species display territorial behavior.\nIn this paper, we propose a new statistical model which allows for dependence\nbetween locations to account for avoidance or territorial behavior. We show via\na simulation study that accounting for this can improve population size\nestimates. The method is illustrated using a case study of small mammal\ntrapping data to estimate avoidance and population density of adult female\nfield voles (Microtus agrestis) in northern England.\n", "versions": [{"version": "v1", "created": "Thu, 8 May 2014 15:40:40 GMT"}], "update_date": "2014-05-09", "authors_parsed": [["Reich", "Brian J.", ""], ["Gardner", "Beth", ""]]}, {"id": "1405.2322", "submitter": "Line Chloe Le Goff", "authors": "Line Chlo\\'e Le Goff, Philippe Soulier (MODAL'X)", "title": "Parameter estimation of a two-colored urn model class", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though widely used in applications, reinforced random walk on graphs have\nnever been the subject of a valid statistical inference. We develop in this\npaper a statistical framework for a general two-colored urn model. The\nprobability to draw a ball at each step depends on the number of balls of each\ncolor and on a multidimensional parameter $\\theta$ through a function $f$,\ncalled a choice function. We introduce two estimators of $\\theta$: the maximum\nlikelihood estimator and a weighted least squares estimator which is less\nefficient, but is closer to the calibration techniques used in the applied\nliterature. In general, the model is an inhomogeneous Markov chain and this\nproperty makes the estimation of the parameter impossible on a single path,\neven if it were infinite. Therefore we assume that we observe i.i.d.\nexperiments, each of a predetermined finite length. This is coherent with the\nusual experimental set-ups. We apply the statistical framework to a real life\nexperiment: the selection of a path among pre-existing channels by an ant\ncolony. We performed experiments, which consisted of letting ants pass through\nthe branches of a fork. We consider the particular urn model proposed by J.-L.\nDeneubourg in 1990 to describe this phenomenon. We simulate this model for\nseveral parameter values in order to assess the accuracy of the MLE and the\nWLSE. Then we estimate the parameter from the experimental data and evaluate\nconfident regions with Bootstrap algorithms. The findings of this paper do not\ncontradict the biological literature, but give statistical significance to the\nvalues of the parameter found therein.\n", "versions": [{"version": "v1", "created": "Fri, 9 May 2014 19:15:21 GMT"}, {"version": "v2", "created": "Tue, 4 Oct 2016 12:46:45 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Goff", "Line Chlo\u00e9 Le", "", "MODAL'X"], ["Soulier", "Philippe", "", "MODAL'X"]]}, {"id": "1405.2528", "submitter": "Esa Ollila", "authors": "Esa Ollila and David E. Tyler", "title": "Regularized $M$-estimators of scatter matrix", "comments": "Submitted to IEEE Transactions on Signal Processing (contains a\n  corrected proof of convergence of the proposed iterative algorithm)", "journal-ref": null, "doi": "10.1109/TSP.2014.2360826", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a general class of regularized $M$-estimators of scatter\nmatrix are proposed which are suitable also for low or insufficient sample\nsupport (small $n$ and large $p$) problems. The considered class constitutes a\nnatural generalization of $M$-estimators of scatter matrix (Maronna, 1976) and\nare defined as a solution to a penalized $M$-estimation cost function that\ndepend on a pair $(\\alpha,\\beta)$ of regularization parameters. We derive\ngeneral conditions for uniqueness of the solution using concept of geodesic\nconvexity. Since these conditions do not include Tyler's $M$-estimator,\nnecessary and sufficient conditions for uniqueness of the penalized Tyler's\ncost function are established separately. For the regularized Tyler's\n$M$-estimator, we also derive a simple, closed form and data dependent solution\nfor choosing the regularization parameter based on shape matrix matching in the\nmean squared sense. An iterative algorithm that converges to the solution of\nthe regularized $M$-estimating equation is also provided. Finally, some\nsimulations studies illustrate the improved accuracy of the proposed\nregularized $M$-estimators of scatter compared to their non-regularized\ncounterparts in low sample support problems. An example of radar detection\nusing normalized matched filter (NMF) illustrate that an adaptive NMF detector\nbased on regularized $M$-estimators are able to maintain accurately the preset\nCFAR level and at at the same time provide similar probability of detection as\nthe (theoretical) NMF detector.\n", "versions": [{"version": "v1", "created": "Sun, 11 May 2014 12:34:43 GMT"}, {"version": "v2", "created": "Thu, 15 May 2014 08:59:21 GMT"}, {"version": "v3", "created": "Mon, 9 Jun 2014 08:31:00 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Ollila", "Esa", ""], ["Tyler", "David E.", ""]]}, {"id": "1405.2566", "submitter": "Edoardo Airoldi", "authors": "Elham Azizi, James E. Galagan, Edoardo M. Airoldi", "title": "Learning modular structures from network data and node variables", "comments": "22 pages, 6 figures, 3 tables, 3 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SI physics.soc-ph q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A standard technique for understanding underlying dependency structures among\na set of variables posits a shared conditional probability distribution for the\nvariables measured on individuals within a group. This approach is often\nreferred to as module networks, where individuals are represented by nodes in a\nnetwork, groups are termed modules, and the focus is on estimating the network\nstructure among modules. However, estimation solely from node-specific\nvariables can lead to spurious dependencies, and unverifiable structural\nassumptions are often used for regularization. Here, we propose an extended\nmodel that leverages direct observations about the network in addition to\nnode-specific variables. By integrating complementary data types, we avoid the\nneed for structural assumptions. We illustrate theoretical and practical\nsignificance of the model and develop a reversible-jump MCMC learning procedure\nfor learning modules and model parameters. We demonstrate the method accuracy\nin predicting modular structures from synthetic data and capability to learn\ninfluence structures in twitter data and regulatory modules in the\nMycobacterium tuberculosis gene regulatory network.\n", "versions": [{"version": "v1", "created": "Sun, 11 May 2014 18:48:26 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Azizi", "Elham", ""], ["Galagan", "James E.", ""], ["Airoldi", "Edoardo M.", ""]]}, {"id": "1405.2656", "submitter": "Yanxun Xu", "authors": "Yanxun Xu, Peter Mueller, Abdus S. Wahed, Peter F. Thall", "title": "Bayesian Nonparametric Estimation for Dynamic Treatment Regimes with\n  Sequential Transition Times", "comments": null, "journal-ref": null, "doi": "10.1080/01621459.2015.1086353", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic treatment regimes in oncology and other disease areas often can be\ncharacterized by an alternating sequence of treatments or other actions and\ntransition times between disease states. The sequence of transition states may\nvary substantially from patient to patient, depending on how the regime plays\nout, and in practice there often are many possible counterfactual outcome\nsequences. For evaluating the regimes, the mean final overall time may be\nexpressed as a weighted average of the means of all possible sums of successive\ntransitions times. A common example arises in cancer therapies where the\ntransition times between various sequences of treatments, disease remission,\ndisease progression, and death characterize overall survival time. For the\ngeneral setting, we propose estimating mean overall outcome time by assuming a\nBayesian nonparametric regression model for the logarithm of each transition\ntime. A dependent Dirichlet process prior with Gaussian process base measure\n(DDP-GP) is assumed, and a joint posterior is obtained by Markov chain Monte\nCarlo (MCMC) sampling. We provide general guidelines for constructing a prior\nusing empirical Bayes methods. We compare the proposed approach with inverse\nprobability of treatment weighting. These comparisons are done by simulation\nstudies of both single-stage and multi-stage regimes, with treatment assignment\ndepending on baseline covariates. The method is applied to analyze a dataset\narising from a clinical trial involving multi-stage chemotherapy regimes for\nacute leukemia. An R program for implementing the DDP-GP-based Bayesian\nnonparametric analysis is freely available at\nhttps://www.ma.utexas.edu/users/yxu/.\n", "versions": [{"version": "v1", "created": "Mon, 12 May 2014 07:52:38 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Xu", "Yanxun", ""], ["Mueller", "Peter", ""], ["Wahed", "Abdus S.", ""], ["Thall", "Peter F.", ""]]}, {"id": "1405.2673", "submitter": "Pierre Minvielle", "authors": "P. Minvielle, A. Todeschini, F. Caron, P. Del Moral", "title": "Particle MCMC for Bayesian Microwave Control", "comments": null, "journal-ref": null, "doi": "10.1088/1742-6596/542/1/012007", "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of local radioelectric property estimation from\nglobal electromagnetic scattering measurements. This challenging ill-posed high\ndimensional inverse problem can be explored by intensive computations of a\nparallel Maxwell solver on a petaflopic supercomputer. Then, it is shown how\nBayesian inference can be perfomed with a Particle Marginal Metropolis-Hastings\n(PMMH) approach, which includes a Rao-Blackwellised Sequential Monte Carlo\nalgorithm with interacting Kalman filters. Material properties, including a\nmultiple components \"Debye relaxation\"/\"Lorenzian resonant\" material model, are\nestimated; it is illustrated on synthetic data. Eventually, we propose\ndifferent ways to deal with higher dimensional problems, from parallelization\nto the original introduction of efficient sequential data assimilation\ntechniques, widely used in weather forecasting, oceanography, geophysics, etc.\n", "versions": [{"version": "v1", "created": "Mon, 12 May 2014 08:43:56 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Minvielle", "P.", ""], ["Todeschini", "A.", ""], ["Caron", "F.", ""], ["Del Moral", "P.", ""]]}, {"id": "1405.3241", "submitter": "H Frost", "authors": "H. Robert Frost, Zhigang Li and Jason H. Moore", "title": "Spectral gene set enrichment (SGSE)", "comments": null, "journal-ref": "BMC Bioinformatics 2015, 16:70 (3 March 2015)", "doi": "10.1186/s12859-015-0490-7", "report-no": null, "categories": "q-bio.QM q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Gene set testing is typically performed in a supervised context\nto quantify the association between groups of genes and a clinical phenotype.\nIn many cases, however, a gene set-based interpretation of genomic data is\ndesired in the absence of a phenotype variable. Although methods exist for\nunsupervised gene set testing, they predominantly compute enrichment relative\nto clusters of the genomic variables with performance strongly dependent on the\nclustering algorithm and number of clusters. Results: We propose a novel\nmethod, spectral gene set enrichment (SGSE), for unsupervised competitive\ntesting of the association between gene sets and empirical data sources. SGSE\nfirst computes the statistical association between gene sets and principal\ncomponents (PCs) using our principal component gene set enrichment (PCGSE)\nmethod. The overall statistical association between each gene set and the\nspectral structure of the data is then computed by combining the PC-level\np-values using the weighted Z-method with weights set to the PC variance scaled\nby Tracey-Widom test p-values. Using simulated data, we show that the SGSE\nalgorithm can accurately recover spectral features from noisy data. To\nillustrate the utility of our method on real data, we demonstrate the superior\nperformance of the SGSE method relative to standard cluster-based techniques\nfor testing the association between MSigDB gene sets and the variance structure\nof microarray gene expression data. Availability:\nhttp://cran.r-project.org/web/packages/PCGSE/index.html Contact:\nrob.frost@dartmouth.edu or jason.h.moore@dartmouth.edu\n", "versions": [{"version": "v1", "created": "Tue, 13 May 2014 17:29:32 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Frost", "H. Robert", ""], ["Li", "Zhigang", ""], ["Moore", "Jason H.", ""]]}, {"id": "1405.3295", "submitter": "Ronald Hochreiter", "authors": "Ronald Hochreiter and Christoph Waldhauser", "title": "Effects of Sampling Methods on Prediction Quality. The Case of\n  Classifying Land Cover Using Decision Trees", "comments": null, "journal-ref": "Proceedings of COMPSTAT 2014: 585-592. 2014", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clever sampling methods can be used to improve the handling of big data and\nincrease its usefulness. The subject of this study is remote sensing,\nspecifically airborne laser scanning point clouds representing different\nclasses of ground cover. The aim is to derive a supervised learning model for\nthe classification using CARTs. In order to measure the effect of different\nsampling methods on the classification accuracy, various experiments with\nvarying types of sampling methods, sample sizes, and accuracy metrics have been\ndesigned. Numerical results for a subset of a large surveying project covering\nthe lower Rhine area in Germany are shown. General conclusions regarding\nsampling design are drawn and presented.\n", "versions": [{"version": "v1", "created": "Tue, 13 May 2014 20:07:09 GMT"}], "update_date": "2014-09-17", "authors_parsed": [["Hochreiter", "Ronald", ""], ["Waldhauser", "Christoph", ""]]}, {"id": "1405.3429", "submitter": "Antony Schutz", "authors": "Antony Schutz, Martin Vannier, David Mary, Andre Ferrari, Florentin\n  Millour, Romain Petrov", "title": "Statistical characterization of polychromatic absolute and differential\n  squared visibilities obtained from AMBER/VLTI instrument", "comments": "A&A, 13 pages and 9 figures", "journal-ref": null, "doi": "10.1051/0004-6361/201322227", "report-no": null, "categories": "astro-ph.IM physics.optics stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In optical interferometry, the visibility squared modulus are generally\nassumed to follow a Gaussian distribution and to be independent of each other.\nA quantitative analysis of the relevance of such assumptions is important to\nhelp improving the exploitation of existing and upcoming multi-wavelength\ninterferometric instruments. Analyze the statistical behaviour of both the\nabsolute and the colour-differential squared visibilities: distribution laws,\ncorrelations and cross-correlations between different baselines. We use\nobservations of stellar calibrators obtained with AMBER instrument on VLTI in\ndifferent instrumental and observing configurations, from which we extract the\nframe-by-frame transfer function. Statistical hypotheses tests and diagnostics\nare then systematically applied. For both absolute and differential squared\nvisibilities and under all instrumental and observing conditions, we find a\nbetter fit for the Student distribution than for the Gaussian, log-normal and\nCauchy distributions. We find and analyze clear correlation effects caused by\natmospheric perturbations. The differential squared visibilities allow to keep\na larger fraction of data with respect to selected absolute squared\nvisibilities and thus benefit from reduced temporal dispersion, while their\ndistribution is more clearly characterized. The frame selection based on the\ncriterion of a fixed SNR value might result in either a biased sample of frames\nor in a too severe selection.\n", "versions": [{"version": "v1", "created": "Wed, 14 May 2014 09:51:49 GMT"}], "update_date": "2014-05-15", "authors_parsed": [["Schutz", "Antony", ""], ["Vannier", "Martin", ""], ["Mary", "David", ""], ["Ferrari", "Andre", ""], ["Millour", "Florentin", ""], ["Petrov", "Romain", ""]]}, {"id": "1405.3689", "submitter": "Elvan Ceyhan", "authors": "Elvan Ceyhan", "title": "Nearest Neighbor Methods for Testing Reflexivity and\n  Species-Correspondence", "comments": "23 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": "Technical Report # KU-EC-14-1", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nearest neighbor (NN) methods are employed for drawing inferences about\nspatial patterns of points from two or more classes. We consider Pielou's test\nof niche specificity which is defined using a contingency table based on the NN\nrelationships between the data points. We demonstrate that Pielou's contingency\ntable for niche specificity is actually more appropriate for testing\nreflexivity in NN structure, hence we call this table as NN reflexivity\ncontingency table (NN-RCT) henceforth. We also derive an asymptotic\napproximation for the distribution of the entries of the NN-RCT and consider\nvariants of Fisher's exact test on it. Moreover, we introduce a new test of\nclass- or species-correspondence inspired by spatial niche/habitat specificity\nand the associated contingency table called species-correspondence contingency\ntable (SCCT). We also determine the appropriate null hypotheses and the\nunderlying conditions appropriate for these tests. We investigate the finite\nsample performance of the tests in terms of empirical size and power by\nextensive Monte Carlo simulations and the methods are illustrated on a\nreal-life ecological data set.\n", "versions": [{"version": "v1", "created": "Wed, 14 May 2014 21:09:28 GMT"}], "update_date": "2014-05-16", "authors_parsed": [["Ceyhan", "Elvan", ""]]}, {"id": "1405.3738", "submitter": "Nicolas Chapados", "authors": "Nicolas Chapados", "title": "Effective Bayesian Modeling of Groups of Related Count Time Series", "comments": "10 pages, 7 figures. Appears in Proceedings of the 31st International\n  Conference on Machine Learning, Beijing, China, 2014. JMLR: W&CP volume 32", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series of counts arise in a variety of forecasting applications, for\nwhich traditional models are generally inappropriate. This paper introduces a\nhierarchical Bayesian formulation applicable to count time series that can\neasily account for explanatory variables and share statistical strength across\ngroups of related time series. We derive an efficient approximate inference\ntechnique, and illustrate its performance on a number of datasets from supply\nchain planning.\n", "versions": [{"version": "v1", "created": "Thu, 15 May 2014 04:32:29 GMT"}], "update_date": "2014-05-16", "authors_parsed": [["Chapados", "Nicolas", ""]]}, {"id": "1405.4001", "submitter": "Alex Lang", "authors": "Alex H. Lang, Charles K. Fisher, Thierry Mora, and Pankaj Mehta", "title": "Thermodynamics of statistical inference by cells", "comments": "15 pages, 5 figures. V2 includes edits and 2 new figures. V3 is\n  updated to version published in Phys. Rev. Lett", "journal-ref": "Phys. Rev. Lett. 113, 148103 (2014)", "doi": "10.1103/PhysRevLett.113.148103", "report-no": null, "categories": "physics.bio-ph cond-mat.stat-mech q-bio.CB q-bio.MN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deep connection between thermodynamics, computation, and information is\nnow well established both theoretically and experimentally. Here, we extend\nthese ideas to show that thermodynamics also places fundamental constraints on\nstatistical estimation and learning. To do so, we investigate the constraints\nplaced by (nonequilibrium) thermodynamics on the ability of biochemical\nsignaling networks within cells to estimate the concentration of an external\nsignal. We show that accuracy is limited by energy consumption, suggesting that\nthere are fundamental thermodynamic constraints on statistical inference.\n", "versions": [{"version": "v1", "created": "Thu, 15 May 2014 20:25:57 GMT"}, {"version": "v2", "created": "Mon, 9 Jun 2014 19:35:23 GMT"}, {"version": "v3", "created": "Mon, 6 Oct 2014 18:36:32 GMT"}], "update_date": "2014-10-08", "authors_parsed": [["Lang", "Alex H.", ""], ["Fisher", "Charles K.", ""], ["Mora", "Thierry", ""], ["Mehta", "Pankaj", ""]]}, {"id": "1405.4081", "submitter": "Lawrence Murray", "authors": "Pierre Del Moral and Lawrence M. Murray", "title": "Sequential Monte Carlo with Highly Informative Observations", "comments": "25 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose sequential Monte Carlo (SMC) methods for sampling the posterior\ndistribution of state-space models under highly informative observation\nregimes, a situation in which standard SMC methods can perform poorly. A\nspecial case is simulating bridges between given initial and final values. The\nbasic idea is to introduce a schedule of intermediate weighting and resampling\ntimes between observation times, which guide particles towards the final state.\nThis can always be done for continuous-time models, and may be done for\ndiscrete-time models under sparse observation regimes; our main focus is on\ncontinuous-time diffusion processes. The methods are broadly applicable in that\nthey support multivariate models with partial observation, do not require\nsimulation of the backward transition (which is often unavailable), and, where\npossible, avoid pointwise evaluation of the forward transition. When simulating\nbridges, the last cannot be avoided entirely without concessions, and we\nsuggest an epsilon-ball approach (reminiscent of Approximate Bayesian\nComputation) as a workaround. Compared to the bootstrap particle filter, the\nnew methods deliver substantially reduced mean squared error in normalising\nconstant estimates, even after accounting for execution time. The methods are\ndemonstrated for state estimation with two toy examples, and for parameter\nestimation (within a particle marginal Metropolis--Hastings sampler) with three\napplied examples in econometrics, epidemiology and marine biogeochemistry.\n", "versions": [{"version": "v1", "created": "Fri, 16 May 2014 07:53:07 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2015 15:44:16 GMT"}], "update_date": "2015-07-10", "authors_parsed": [["Del Moral", "Pierre", ""], ["Murray", "Lawrence M.", ""]]}, {"id": "1405.4182", "submitter": "Sachin Malik", "authors": "Rajesh Singh, S.B. Gupta and Sachin Malik", "title": "Almost unbiased estimator using Known Value of Population Parameter (s)\n  in Sample Surveys", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we have proposed an almost unbiased estimator using known value\nof some population parameter(s). A class of estimators is defined which\nincludes Singh and Solanki [1] and Sahai and Ray [2], Sisodia and Dwivedi [3],\nSingh et. al. [4], Upadhyaya and Singh [5], Singh and Tailor [6] estimators.\nUnder simple random sampling without replacement (SRSWOR) scheme the\nexpressions for bias and mean square error (MSE) are derived. Numerical\nillustrations are given in support of the present study. Key words: Auxiliary\ninformation, bias, mean square error, unbiased estimator.\n", "versions": [{"version": "v1", "created": "Fri, 16 May 2014 14:32:03 GMT"}], "update_date": "2014-05-19", "authors_parsed": [["Singh", "Rajesh", ""], ["Gupta", "S. B.", ""], ["Malik", "Sachin", ""]]}, {"id": "1405.4237", "submitter": "Sachin Malik", "authors": "Sachin Malik and Rajesh Singh", "title": "An alternative estimator for estimating the finite population mean in\n  presence of measurement errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents the problem of estimating the population mean using\nauxiliary information in the presence of measurement errors. A numerical study\nis made among the proposed estimator, the exponential ratio estimator, Singh\nand Solanki (2012) estimator and the mean per unit estimator in the presence of\nmeasurement errors. Key words: Population mean, Study variate, Auxiliary\nvariates, Mean squared error, Measurement errors, Efficiency\n", "versions": [{"version": "v1", "created": "Fri, 16 May 2014 16:41:26 GMT"}], "update_date": "2014-05-19", "authors_parsed": [["Malik", "Sachin", ""], ["Singh", "Rajesh", ""]]}, {"id": "1405.4251", "submitter": "Kean Ming Tan", "authors": "Kean Ming Tan, Noah Simon, and Daniela Witten", "title": "Selection Bias Correction and Effect Size Estimation under Dependence", "comments": "21 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider large-scale studies in which it is of interest to test a very\nlarge number of hypotheses, and then to estimate the effect sizes corresponding\nto the rejected hypotheses. For instance, this setting arises in the analysis\nof gene expression or DNA sequencing data. However, naive estimates of the\neffect sizes suffer from selection bias, i.e., some of the largest naive\nestimates are large due to chance alone. Many authors have proposed methods to\nreduce the effects of selection bias under the assumption that the naive\nestimates of the effect sizes are independent. Unfortunately, when the effect\nsize estimates are dependent, these existing techniques can have very poor\nperformance, and in practice there will often be dependence. We propose an\nestimator that adjusts for selection bias under a recently-proposed frequentist\nframework, without the independence assumption. We study some properties of the\nproposed estimator, and illustrate that it outperforms past proposals in a\nsimulation study and on two gene expression data sets.\n", "versions": [{"version": "v1", "created": "Fri, 16 May 2014 17:32:34 GMT"}, {"version": "v2", "created": "Sat, 28 Mar 2015 19:31:52 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Tan", "Kean Ming", ""], ["Simon", "Noah", ""], ["Witten", "Daniela", ""]]}, {"id": "1405.4265", "submitter": "Forrest W. Crawford", "authors": "Forrest W. Crawford, Robert E. Weiss, Marc A. Suchard", "title": "Sex, lies and self-reported counts: Bayesian mixture models for heaping\n  in longitudinal count data via birth-death processes", "comments": "Published at http://dx.doi.org/10.1214/15-AOAS809 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 2, 572-596", "doi": "10.1214/15-AOAS809", "report-no": "IMS-AOAS-AOAS809", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surveys often ask respondents to report nonnegative counts, but respondents\nmay misremember or round to a nearby multiple of 5 or 10. This phenomenon is\ncalled heaping, and the error inherent in heaped self-reported numbers can bias\nestimation. Heaped data may be collected cross-sectionally or longitudinally\nand there may be covariates that complicate the inferential task. Heaping is a\nwell-known issue in many survey settings, and inference for heaped data is an\nimportant statistical problem. We propose a novel reporting distribution whose\nunderlying parameters are readily interpretable as rates of misremembering and\nrounding. The process accommodates a variety of heaping grids and allows for\nquasi-heaping to values nearly but not equal to heaping multiples. We present a\nBayesian hierarchical model for longitudinal samples with covariates to infer\nboth the unobserved true distribution of counts and the parameters that control\nthe heaping process. Finally, we apply our methods to longitudinal\nself-reported counts of sex partners in a study of high-risk behavior in\nHIV-positive youth.\n", "versions": [{"version": "v1", "created": "Fri, 16 May 2014 18:41:56 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2015 07:17:09 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Crawford", "Forrest W.", ""], ["Weiss", "Robert E.", ""], ["Suchard", "Marc A.", ""]]}, {"id": "1405.4637", "submitter": "Cedric Taverne", "authors": "Cedric Taverne and Philippe Lambert", "title": "Inflated Discrete Beta Regression Models for Likert and Discrete Rating\n  Scale Outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete ordinal responses such as Likert scales are regularly proposed in\nquestionnaires and used as dependent variable in modeling. The response\ndistribution for such scales is always discrete, with bounded support and often\nskewed. In addition, one particular level of the scale is frequently inflated\nas it cumulates respondents who invariably choose that particular level\n(typically the middle or one extreme of the scale) without hesitation with\nthose who chose that alternative but might have selected a neighboring one. The\ninflated discrete beta regression (IDBR) model addresses those four critical\ncharacteristics that have never been taken into account simultaneously by\nexisting models. The mean and the dispersion of rates are jointly regressed on\ncovariates using an underlying beta distribution. The probability that choosers\nof the inflated level invariably make that choice is also regressed on\ncovariates. Simulation studies used to evaluate the statistical properties of\nthe IDBR model suggest that it produces more precise predictions than competing\nmodels. The ability to jointly model the location and dispersion of (the\ndistribution of) an ordinal response, as well as to characterize the profile of\nsubject selecting an \"inflated\" alternative are the most relevant features of\nthe IDBR model. It is illustrated with the analysis of the political\npositioning on a \"left-right\" scale of the Belgian respondents in the 2012\nEuropean Social Survey.\n", "versions": [{"version": "v1", "created": "Mon, 19 May 2014 08:26:26 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Taverne", "Cedric", ""], ["Lambert", "Philippe", ""]]}, {"id": "1405.4645", "submitter": "Achraf Mallat", "authors": "Achraf Mallat, Sinan Gezici, Davide Dardari, Christophe Craeye, and\n  Luc Vandendorpe", "title": "Statistics of the MLE and Approximate Upper and Lower Bounds - Part 1:\n  Application to TOA Estimation", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2014.2355771", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In nonlinear deterministic parameter estimation, the maximum likelihood\nestimator (MLE) is unable to attain the Cramer-Rao lower bound at low and\nmedium signal-to-noise ratios (SNR) due the threshold and ambiguity phenomena.\nIn order to evaluate the achieved mean-squared-error (MSE) at those SNR levels,\nwe propose new MSE approximations (MSEA) and an approximate upper bound by\nusing the method of interval estimation (MIE). The mean and the distribution of\nthe MLE are approximated as well. The MIE consists in splitting the a priori\ndomain of the unknown parameter into intervals and computing the statistics of\nthe estimator in each interval. Also, we derive an approximate lower bound\n(ALB) based on the Taylor series expansion of noise and an ALB family by\nemploying the binary detection principle. The accurateness of the proposed\nMSEAs and the tightness of the derived approximate bounds are validated by\nconsidering the example of time-of-arrival estimation.\n", "versions": [{"version": "v1", "created": "Mon, 19 May 2014 09:04:21 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Mallat", "Achraf", ""], ["Gezici", "Sinan", ""], ["Dardari", "Davide", ""], ["Craeye", "Christophe", ""], ["Vandendorpe", "Luc", ""]]}, {"id": "1405.4647", "submitter": "Achraf Mallat", "authors": "Achraf Mallat, Sinan Gezici, Davide Dardari, and Luc Vandendorpe", "title": "Statistics of the MLE and Approximate Upper and Lower Bounds - Part 2:\n  Threshold Computation and Optimal Signal Design", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2014.2355776", "report-no": null, "categories": "stat.AP cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Threshold and ambiguity phenomena are studied in Part 1 of this work where\napproximations for the mean-squared-error (MSE) of the maximum likelihood\nestimator are proposed using the method of interval estimation (MIE), and where\napproximate upper and lower bounds are derived. In this part we consider\ntime-of-arrival estimation and we employ the MIE to derive closed-form\nexpressions of the begin-ambiguity, end-ambiguity and asymptotic\nsignal-to-noise ratio (SNR) thresholds with respect to some features of the\ntransmitted signal. Both baseband and passband pulses are considered. We prove\nthat the begin-ambiguity threshold depends only on the shape of the envelope of\nthe ACR, whereas the end-ambiguity and asymptotic thresholds only on the shape\nof the ACR. We exploit the results on the begin-ambiguity and asymptotic\nthresholds to optimize, with respect to the available SNR, the pulse that\nachieves the minimum attainable MSE. The results of this paper are valid for\nvarious estimation problems.\n", "versions": [{"version": "v1", "created": "Mon, 19 May 2014 09:19:47 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Mallat", "Achraf", ""], ["Gezici", "Sinan", ""], ["Dardari", "Davide", ""], ["Vandendorpe", "Luc", ""]]}, {"id": "1405.4696", "submitter": "Sakari Kuikka", "authors": "Sakari Kuikka, Jarno Vanhatalo, Henni Pulkkinen, Samu M\\\"antyniemi,\n  Jukka Corander", "title": "Experiences in Bayesian Inference in Baltic Salmon Management", "comments": "Published in at http://dx.doi.org/10.1214/13-STS431 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 1, 42-49", "doi": "10.1214/13-STS431", "report-no": "IMS-STS-STS431", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review a success story regarding Bayesian inference in fisheries\nmanagement in the Baltic Sea. The management of salmon fisheries is currently\nbased on the results of a complex Bayesian population dynamic model, and\nmanagers and stakeholders use the probabilities in their discussions. We also\ndiscuss the technical and human challenges in using Bayesian modeling to give\npractical advice to the public and to government officials and suggest future\nareas in which it can be applied. In particular, large databases in fisheries\nscience offer flexible ways to use hierarchical models to learn the population\ndynamics parameters for those by-catch species that do not have similar large\nstock-specific data sets like those that exist for many target species. This\ninformation is required if we are to understand the future ecosystem risks of\nfisheries.\n", "versions": [{"version": "v1", "created": "Mon, 19 May 2014 12:36:46 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Kuikka", "Sakari", ""], ["Vanhatalo", "Jarno", ""], ["Pulkkinen", "Henni", ""], ["M\u00e4ntyniemi", "Samu", ""], ["Corander", "Jukka", ""]]}, {"id": "1405.5111", "submitter": "Steven Ambadjes", "authors": "Steven Ambadjes", "title": "Ranking Swing Voters in Congressional Elections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a model for quantitatively identifying swing voters in\ncongressional elections. This is achieved by predicting an individual voter's\nlikelihood to vote and an individual voter's likelihood to vote for a given\nparty, if he votes. We make a rough prediction of these values. We then update\nthese predictions by incorporating information on a municipality wide basis via\naggregate data to enhance our estimate under the assumption that nearby voters\nhave similar behavior, which could be due to social interaction or common\nexternal factors. Finally, we use a ranking scheme on these predictions to\nidentify two key types of voter: 1) Voters who are likely to vote that we can\nconvince to vote for a given party; and, 2) Voters who are likely to vote for a\ngiven party, if they vote, that we can convince to actually turn out to vote.\nOnce these voters have been identified, a political campaign can use this\ninformation to micro-target voters and win more votes.\n", "versions": [{"version": "v1", "created": "Mon, 19 May 2014 06:30:42 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Ambadjes", "Steven", ""]]}, {"id": "1405.5251", "submitter": "Korbinian Strimmer", "authors": "Steve Hoffmann, Peter F. Stadler and Korbinian Strimmer", "title": "A Simple Data-Adaptive Probabilistic Variant Calling Model", "comments": "19 pages, 6 figures", "journal-ref": "Algorithms for Molecular Biology 2015, Vol. 10, Article 10", "doi": "10.1186/s13015-015-0037-5", "report-no": null, "categories": "q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Several sources of noise obfuscate the identification of single\nnucleotide variation (SNV) in next generation sequencing data. For instance,\nerrors may be introduced during library construction and sequencing steps. In\naddition, the reference genome and the algorithms used for the alignment of the\nreads are further critical factors determining the efficacy of variant calling\nmethods. It is crucial to account for these factors in individual sequencing\nexperiments.\n  Results: We introduce a simple data-adaptive model for variant calling. This\nmodel automatically adjusts to specific factors such as alignment errors. To\nachieve this, several characteristics are sampled from sites with low mismatch\nrates, and these are used to estimate empirical log-likelihoods. These\nlikelihoods are then combined to a score that typically gives rise to a mixture\ndistribution. From these we determine a decision threshold to separate\npotentially variant sites from the noisy background.\n  Conclusions: In simulations we show that our simple proposed model is\ncompetitive with frequently used much more complex SNV calling algorithms in\nterms of sensitivity and specificity. It performs specifically well in cases\nwith low allele frequencies. The application to next-generation sequencing data\nreveals stark differences of the score distributions indicating a strong\ninfluence of data specific sources of noise. The proposed model is specifically\ndesigned to adjust to these differences.\n", "versions": [{"version": "v1", "created": "Tue, 20 May 2014 22:09:17 GMT"}, {"version": "v2", "created": "Thu, 20 Nov 2014 18:20:54 GMT"}, {"version": "v3", "created": "Tue, 13 Jan 2015 15:44:05 GMT"}], "update_date": "2015-03-05", "authors_parsed": [["Hoffmann", "Steve", ""], ["Stadler", "Peter F.", ""], ["Strimmer", "Korbinian", ""]]}, {"id": "1405.5467", "submitter": "Guillaume Filion", "authors": "Guillaume Filion and Pol Cusc\\'o", "title": "jahmm: A tool for discretizing multiple ChIP seq profiles", "comments": "25 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chromatin immunoprecipitation and high throughput sequencing (ChIP-seq) is\nthe de facto standard method to map chromatin features on genomes. The output\nof ChIP-seq is quantitative within a single genome-wide profile, but there is\nno natural way to compare experiments, which is why the data is often\ndiscretized as present/absent calls. Many tools perform this task efficiently,\nhowever they process a single input at a time, which produces discretization\nconflicts among replicates. Here we present the implementation of a Hidden\nMarkov Model (HMM) using mixture negative multinomial emissions to discretize\nChIP-seq profiles. The method gives meaningful discretization for a wide range\nof features and allows to merge datasets from different origins into a single\ndiscretized profile, which resolves discretization conflicts. A quality control\nstep performed after the discretization accepts or rejects the discretization\nas a whole. The implementation of the model is called jahmm, and it is\navailable as an R package. The source can be downloaded from\nhttp://github.com/gui11aume/jahmm\n", "versions": [{"version": "v1", "created": "Wed, 21 May 2014 16:28:17 GMT"}], "update_date": "2014-05-22", "authors_parsed": [["Filion", "Guillaume", ""], ["Cusc\u00f3", "Pol", ""]]}, {"id": "1405.5559", "submitter": "Stanley Lazic", "authors": "Stanley E. Lazic, Johannes Fuss, Peter Gass", "title": "Quantifying the behavioural relevance of hippocampal neurogenesis", "comments": "To be published in PLoS ONE", "journal-ref": null, "doi": "10.1371/journal.pone.0113855", "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few studies that examine the neurogenesis--behaviour relationship formally\nestablish covariation between neurogenesis and behaviour or rule out competing\nexplanations. The behavioural relevance of neurogenesis might therefore be\noverestimated if other mechanisms account for some, or even all, of the\nexperimental effects. A systematic review of the literature was conducted and\nthe data reanalysed using causal mediation analysis, which can estimate the\nbehavioural contribution of new hippocampal neurons separately from other\nmechanisms that might be operating. Results from eleven eligible individual\nstudies were then combined in a meta-analysis to increase precision\n(representing data from 215 animals) and showed that neurogenesis made a\nnegligible contribution to behaviour (standarised effect = 0.15; 95% CI = -0.04\nto 0.34; p = 0.128); other mechanisms accounted for the majority of\nexperimental effects (standardised effect = 1.06; 95% CI = 0.74 to 1.38; p =\n1.7 $\\times 10^{-11}$).\n", "versions": [{"version": "v1", "created": "Wed, 21 May 2014 21:21:02 GMT"}, {"version": "v2", "created": "Sun, 9 Nov 2014 18:35:29 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Lazic", "Stanley E.", ""], ["Fuss", "Johannes", ""], ["Gass", "Peter", ""]]}, {"id": "1405.5623", "submitter": "Linda S. L. Tan", "authors": "Linda S. L. Tan", "title": "Stochastic variational inference for large-scale discrete choice models\n  using adaptive batch sizes", "comments": null, "journal-ref": "Statistics and Computing (2017) 27 pp 237-257", "doi": "10.1007/s11222-015-9618-x", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete choice models describe the choices made by decision makers among\nalternatives and play an important role in transportation planning, marketing\nresearch and other applications. The mixed multinomial logit (MMNL) model is a\npopular discrete choice model that captures heterogeneity in the preferences of\ndecision makers through random coefficients. While Markov chain Monte Carlo\nmethods provide the Bayesian analogue to classical procedures for estimating\nMMNL models, computations can be prohibitively expensive for large datasets.\nApproximate inference can be obtained using variational methods at a lower\ncomputational cost with competitive accuracy. In this paper, we develop\nvariational methods for estimating MMNL models that allow random coefficients\nto be correlated in the posterior and can be extended easily to large-scale\ndatasets. We explore three alternatives: (1) Laplace variational inference, (2)\nnonconjugate variational message passing and (3) stochastic linear regression.\nTheir performances are compared using real and simulated data. To accelerate\nconvergence for large datasets, we develop stochastic variational inference for\nMMNL models using each of the above alternatives. Stochastic variational\ninference allows data to be processed in minibatches by optimizing global\nvariational parameters using stochastic gradient approximation. A novel\nstrategy for increasing minibatch sizes adaptively within stochastic\nvariational inference is proposed.\n", "versions": [{"version": "v1", "created": "Thu, 22 May 2014 03:24:14 GMT"}, {"version": "v2", "created": "Mon, 26 May 2014 01:32:44 GMT"}, {"version": "v3", "created": "Mon, 4 Aug 2014 20:05:09 GMT"}, {"version": "v4", "created": "Thu, 8 Oct 2015 15:55:37 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Tan", "Linda S. L.", ""]]}, {"id": "1405.6268", "submitter": "Vikas Kumar Sharma", "authors": "Vikas Kumar Sharma, Sanjay Kumar Singh, Umesh Singh, Varun Agiwal", "title": "The inverse Lindley distribution: A stress-strength reliability model", "comments": "17 pages, 4 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we proposed an inverse Lindley distribution and studied its\nfundamental properties such as quantiles, mode, stochastic ordering and entropy\nmeasure. The proposed distribution is observed to be a heavy-tailed\ndistribution and has a upside-down bathtub shape for its failure rate. Further,\nwe proposed its applicability as a stress-strength reliability model for\nsurvival data analysis. The estimation of stress-strength parameters and\n$R=P[X>Y]$, the stress-strength reliability has been approached by both\nclassical and Bayesian paradigms. Under Bayesian set-up, non-informative\n(Jeffrey) and informative (gamma) priors are considered under a symmetric\n(squared error) and a asymmetric (entropy) loss functions, and a\nLindley-approximation technique is used for Bayesian computation. The proposed\nestimators are compared in terms of their mean squared errors through a\nsimulation study. Two real data sets representing survival of Head and Neck\ncancer patients are fitted using the inverse Lindley distribution and used to\nestimate the stress-strength parameters and reliability.\n", "versions": [{"version": "v1", "created": "Sat, 24 May 2014 05:08:09 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Sharma", "Vikas Kumar", ""], ["Singh", "Sanjay Kumar", ""], ["Singh", "Umesh", ""], ["Agiwal", "Varun", ""]]}, {"id": "1405.6392", "submitter": "Sonam Maheshwari", "authors": "B. P. Singh, P. S. Pudir and Sonam Maheshwari", "title": "Parameter estimation of beta-geometric model with application to human\n  fecundability data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present study deals with the estimation of the mean value of\nfecundability by fitting a theoretical distribution from the observed month of\nfirst conception of the married women who did not use any contraceptive method\nbefore their first conception. It is assumed that fecundability is fixed for a\ngiven couple, but across couples it varies according to a specified\ndistribution. Under the classical approach, methods of moment and maximum\nlikelihood are used while for Bayesian approach we use the above two estimates\nas prior for fecundability parameter. A real data analysis from the third\nNational Family Health Survey (NFHS-III) is analyzed as an application of\nmodel. Finally, a simulation study is performed to access the performance of\nthe several of methods used in this paper\n", "versions": [{"version": "v1", "created": "Sun, 25 May 2014 14:29:51 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Singh", "B. P.", ""], ["Pudir", "P. S.", ""], ["Maheshwari", "Sonam", ""]]}, {"id": "1405.6447", "submitter": "Xiaotong Suo", "authors": "Xiaotong Suo, Robert Tibshirani", "title": "An Ordered Lasso and Sparse Time-Lagged Regression", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": "10.1080/00401706.2015.1079245", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider regression scenarios where it is natural to impose an order\nconstraint on the coefficients. We propose an order-constrained version of\nL1-regularized regression for this problem, and show how to solve it\nefficiently using the well-known Pool Adjacent Violators Algorithm as its\nproximal operator. The main application of this idea is time-lagged regression,\nwhere we predict an outcome at time t from features at the previous K time\npoints. In this setting it is natural to assume that the coefficients decay as\nwe move farther away from t, and hence the order constraint is reasonable.\nPotential applications include financial time series and prediction of dynamic\npatient out- comes based on clinical measurements. We illustrate this idea on\nreal and simulated data.\n", "versions": [{"version": "v1", "created": "Mon, 26 May 2014 01:37:21 GMT"}, {"version": "v2", "created": "Tue, 3 Jun 2014 06:26:45 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Suo", "Xiaotong", ""], ["Tibshirani", "Robert", ""]]}, {"id": "1405.6900", "submitter": "C\\'ecile Chauvel", "authors": "C\\'ecile Chauvel and John O'Quigley", "title": "Survival model construction guided by fit and predictive strength", "comments": "30 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We describe a unified framework within which we can build survival models.\nThe motivation for this work comes from a study on the prediction of relapse\namong breast cancer patients treated at the Curie Institute in Paris, France.\nOur focus is on how to best code, or characterize, the effects of the\nvariables, either alone or in combination with others. We consider simple\ngraphical techniques that not only provide an immediate indication as to the\ngoodness of fit but, in cases of departure from model assumptions, point in the\ndirection of a more involved alternative model. These techniques help support\nour intuition. This intuition is backed up by formal theorems that underlie the\nprocess of building richer models from simpler ones. Goodness-of-fit techniques\nare used alongside measures of predictive strength and, again, formal theorems\nshow that these measures can be used to help identify models closest to the\nunknown non-proportional hazards mechanism that we can suppose generates the\nobservations. We consider many examples and show how these tools can be of help\nin guiding the practical problem of efficient model construction for survival\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 27 May 2014 13:16:30 GMT"}], "update_date": "2014-05-28", "authors_parsed": [["Chauvel", "C\u00e9cile", ""], ["O'Quigley", "John", ""]]}, {"id": "1405.6947", "submitter": "\\'Oli Geirsson", "authors": "\\'Oli P\\'all Geirsson, Birgir Hrafnkelsson and Daniel Simpson", "title": "Computationally efficient spatial modeling of annual maximum 24 hour\n  precipitation. An application to data from Iceland", "comments": "32 pages, 16 figures, submitted to Environmetrics", "journal-ref": null, "doi": "10.1002/env.2343", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a computationally efficient statistical method to obtain\ndistributional properties of annual maximum 24 hour precipitation on a 1 km by\n1 km regular grid over Iceland. A latent Gaussian model is built which takes\ninto account observations, spatial variations and outputs from a local\nmeteorological model. A covariate based on the meteorological model is\nconstructed at each observational site and each grid point in order to\nassimilate available scientific knowledge about precipitation into the\nstatistical model. The model is applied to two data sets on extreme\nprecipitation, one uncorrected data set and one data set that is corrected for\nphase and wind. The observations are assumed to follow the generalized extreme\nvalue distribution. At the latent level, we implement SPDE spatial models for\nboth the location and scale parameters of the likelihood. An efficient MCMC\nsampler which exploits the model structure is constructed, which yields fast\ncontinuous spatial predictions for spatially varying model parameters and\nquantiles.\n", "versions": [{"version": "v1", "created": "Tue, 27 May 2014 15:18:01 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Geirsson", "\u00d3li P\u00e1ll", ""], ["Hrafnkelsson", "Birgir", ""], ["Simpson", "Daniel", ""]]}, {"id": "1405.7091", "submitter": "Jonathan Heydari", "authors": "Jonathan Heydari", "title": "Bayesian hierarchical modelling for inferring genetic interactions in\n  yeast", "comments": "All images rasterized. Contact author for non-rasterized and\n  searchable Figures. 158 pages, PhD thesis, Newcastle University (2014),\n  Institute for Cell & Molecular Biosciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.CO stat.ME stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying genetic interactions for a given microorganism such as yeast is\ndifficult. Quantitative Fitness Analysis (QFA) is a high-throughput\nexperimental and computational methodology for quantifying the fitness of\nmicrobial cultures. QFA can be used to compare between fitness observations for\ndifferent genotypes and thereby infer genetic interaction strengths. Current\n\"naive\" frequentist statistical approaches used in QFA do not model\nbetween-genotype variation or difference in genotype variation under different\nconditions. In this thesis, a Bayesian approach is introduced to evaluate\nhierarchical models that better reflect the structure or design of QFA\nexperiments. First, a two-stage approach is presented: a hierarchical logistic\nmodel is fitted to microbial culture growth curves and then a hierarchical\ninteraction model is fitted to fitness summaries inferred for each genotype.\nNext, a one-stage Bayesian approach is presented: a joint hierarchical model\nwhich does not require a univariate summary of fitness, used to pass\ninformation between models. The new hierarchical approaches are then compared\nusing a dataset examining the effect of telomere defects on yeast. By better\ndescribing the experimental structure, new evidence is found for genes and\ncomplexes which interact with the telomere cap. Various extensions of these\nmodels, including models for data transformation, batch effects, and\nintrinsically stochastic growth models are also considered.\n", "versions": [{"version": "v1", "created": "Tue, 27 May 2014 23:49:48 GMT"}], "update_date": "2014-05-29", "authors_parsed": [["Heydari", "Jonathan", ""]]}, {"id": "1405.7206", "submitter": "Arnab Hazra", "authors": "Arnab Hazra, Sourabh Bhattacharya, Sabyasachi Bhattacharya, Pabitra\n  Banik", "title": "A Note on the Misuse of the Variance Test in Meteorological Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The erroneous assumption \"for all distributions for which the theoretical\nvariance can be computed independently from parameters estimated by any method\ndifferent from the method of moments\" has been used in the case of fitting the\ngamma distribution to a rainfall data by Mooley (1973) which was followed by\nseveral researchers. We show that the asymptotic distribution of the test\nstatistic is generally not even comparable to any central chi-square\ndistribution. We also describe a method for checking the validity of the\nasymptotic distribution for a class of distributions.\n", "versions": [{"version": "v1", "created": "Wed, 28 May 2014 11:46:50 GMT"}], "update_date": "2014-05-29", "authors_parsed": [["Hazra", "Arnab", ""], ["Bhattacharya", "Sourabh", ""], ["Bhattacharya", "Sabyasachi", ""], ["Banik", "Pabitra", ""]]}, {"id": "1405.7227", "submitter": "Jonathan Bradley", "authors": "Jonathan R. Bradley, Christopher K. Wikle, Scott H. Holan", "title": "Bayesian Spatial Change of Support for Count-Valued Survey Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Bayesian spatial change of support methodology for count-valued\nsurvey data with known survey variances. Our proposed methodology is motivated\nby the American Community Survey (ACS), an ongoing survey administered by the\nU.S. Census Bureau that provides timely information on several key demographic\nvariables. Specifically, the ACS produces 1-year, 3-year, and 5-year\n\"period-estimates,\" and corresponding margins of errors, for published\ndemographic and socio-economic variables recorded over predefined geographies\nwithin the United States. Despite the availability of these predefined\ngeographies it is often of interest to data users to specify customized\nuser-defined spatial supports. In particular, it is useful to estimate\ndemographic variables defined on \"new\" spatial supports in \"real-time.\" This\nproblem is known as spatial change of support (COS), which is typically\nperformed under the assumption that the data follows a Gaussian distribution.\nHowever, count-valued survey data is naturally non-Gaussian and, hence, we\nconsider modeling these data using a Poisson distribution. Additionally,\nsurvey-data are often accompanied by estimates of error, which we incorporate\ninto our analysis. We interpret Poisson count-valued data in small areas as an\naggregation of events from a spatial point process. This approach provides us\nwith the flexibility necessary to allow ACS users to consider a variety of\nspatial supports in \"real-time.\" We demonstrate the effectiveness of our\napproach through a simulated example as well as through an analysis using\npublic-use ACS data.\n", "versions": [{"version": "v1", "created": "Wed, 28 May 2014 13:33:36 GMT"}, {"version": "v2", "created": "Tue, 28 Oct 2014 17:58:03 GMT"}], "update_date": "2014-10-29", "authors_parsed": [["Bradley", "Jonathan R.", ""], ["Wikle", "Christopher K.", ""], ["Holan", "Scott H.", ""]]}, {"id": "1405.7436", "submitter": "Branko Ristic", "authors": "Branko Ristic, Ajith Gunatilaka, Ralph Gailis", "title": "Cramer-Rao bound for source estimation using a network of binary sensors", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper derives the theoretical Cramer-Rao lower bound for parameter\nestimation of a source (of emitting energy, gas, aerosol), monitored by a\nnetwork of sensors providing binary measurements. The theoretical bound is\nstudied in the context of a source of a continuous release in the atmosphere of\nhazardous gas or aerosol. Numerical results show a good agreement with the\nempirical errors, obtained using an MCMC parameter estimation technique.\n", "versions": [{"version": "v1", "created": "Thu, 29 May 2014 01:22:10 GMT"}], "update_date": "2014-05-30", "authors_parsed": [["Ristic", "Branko", ""], ["Gunatilaka", "Ajith", ""], ["Gailis", "Ralph", ""]]}, {"id": "1405.7555", "submitter": "Daniele Durante", "authors": "Tommaso Rigon, Daniele Durante and Nicola Torelli", "title": "Bayesian semiparametric modelling of contraceptive behavior in India via\n  sequential logistic regressions", "comments": null, "journal-ref": "Journal of the Royal Statistical Society: Series A (2019). 182,\n  225-247", "doi": "10.1111/rssa.12361", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Family planning has been characterized by highly different strategic programs\nin India, including method-specific contraceptive targets, coercive\nsterilization, and more recent target-free approaches. These major changes in\nfamily planning policies over time have motivated a considerable interest\ntowards assessing the effectiveness of the different programs, while\nunderstanding which subsets of the population have not been properly addressed.\nCurrent studies consider specific aspects of the above policies, including, for\nexample, the factors associated with the choice of alternative contraceptive\nmethods other than sterilization, for women using contraceptives. Although\nthese analyses produce relevant insights, they fail to provide a global\noverview of the different family planning policies, and the determinants\nunderlying the contraceptive choices. Motivated by this consideration, we\npropose a Bayesian semiparametric model relying on a reparameterization of the\nmultinomial probability mass function via a set of conditional Bernoulli\nchoices. The sequential binary structure is defined to be consistent with the\ncurrent family planning policies in India, and coherent with a reasonable\nprocess characterizing the contraceptive choices. This combination of flexible\nrepresentations and careful reparameterizations allows a broader and\ninterpretable overview of the different policies and contraceptive preferences\nin India, within a single model.\n", "versions": [{"version": "v1", "created": "Thu, 29 May 2014 13:40:14 GMT"}, {"version": "v2", "created": "Mon, 26 Sep 2016 15:38:31 GMT"}, {"version": "v3", "created": "Mon, 31 Jul 2017 15:48:25 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Rigon", "Tommaso", ""], ["Durante", "Daniele", ""], ["Torelli", "Nicola", ""]]}, {"id": "1405.7701", "submitter": "Torsten Ensslin", "authors": "Torsten En{\\ss}lin", "title": "Astrophysical data analysis with information field theory", "comments": "4 pages, 2 figures, accepted chapter to the conference proceedings\n  for MaxEnt 2013, to be published by AIP", "journal-ref": null, "doi": "10.1063/1.4903709", "report-no": null, "categories": "astro-ph.IM physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-parametric imaging and data analysis in astrophysics and cosmology can be\naddressed by information field theory (IFT), a means of Bayesian, data based\ninference on spatially distributed signal fields. IFT is a statistical field\ntheory, which permits the construction of optimal signal recovery algorithms.\nIt exploits spatial correlations of the signal fields even for nonlinear and\nnon-Gaussian signal inference problems. The alleviation of a perception\nthreshold for recovering signals of unknown correlation structure by using IFT\nwill be discussed in particular as well as a novel improvement on instrumental\nself-calibration schemes. IFT can be applied to many areas. Here, applications\nin in cosmology (cosmic microwave background, large-scale structure) and\nastrophysics (galactic magnetism, radio interferometry) are presented.\n", "versions": [{"version": "v1", "created": "Thu, 29 May 2014 20:00:14 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["En\u00dflin", "Torsten", ""]]}]