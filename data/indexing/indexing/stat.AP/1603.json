[{"id": "1603.00050", "submitter": "Glenn Healey", "authors": "Glenn Healey", "title": "Learning, Visualizing, and Exploiting a Model for the Intrinsic Value of\n  a Batted Ball", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm for learning the intrinsic value of a batted ball in\nbaseball. This work addresses the fundamental problem of separating the value\nof a batted ball at contact from factors such as the defense, weather, and\nballpark that can affect its observed outcome. The algorithm uses a Bayesian\nmodel to construct a continuous mapping from a vector of batted ball parameters\nto an intrinsic measure defined as the expected value of a linear weights\nrepresentation for run value. A kernel method is used to build nonparametric\nestimates for the component probability density functions in Bayes theorem from\na set of over one hundred thousand batted ball measurements recorded by the\nHITf/x system during the 2014 major league baseball (MLB) season.\nCross-validation is used to determine the optimal vector of smoothing\nparameters for the density estimates. Properties of the mapping are visualized\nby considering reduced-dimension subsets of the batted ball parameter space. We\nuse the mapping to derive statistics for intrinsic quality of contact for\nbatters and pitchers which have the potential to improve the accuracy of player\nmodels and forecasting systems. We also show that the new approach leads to a\nsimple automated measure of contact-adjusted defense and provides insight into\nthe impact of environmental variables on batted balls.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2016 06:27:40 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Healey", "Glenn", ""]]}, {"id": "1603.00074", "submitter": "Jonathan Skaza", "authors": "Jonathan Skaza and Brian Blais", "title": "Modeling the Infectiousness of Twitter Hashtags", "comments": null, "journal-ref": null, "doi": "10.1016/j.physa.2016.08.038", "report-no": null, "categories": "cs.SI q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study applies dynamical and statistical modeling techniques to quantify\nthe proliferation and popularity of trending hashtags on Twitter. Using\ntime-series data reflecting actual tweets in New York City and San Francisco,\nwe present estimates for the dynamics (i.e., rates of infection and recovery)\nof several hundred trending hashtags using an epidemic modeling framework\ncoupled with Bayesian Markov Chain Monte Carlo (MCMC) methods. This\nmethodological strategy is an extension of techniques traditionally used to\nmodel the spread of infectious disease. We demonstrate that in some models,\nhashtags can be grouped by infectiousness, possibly providing a method for\nquantifying the trendiness of a topic.\n", "versions": [{"version": "v1", "created": "Mon, 29 Feb 2016 22:21:42 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Skaza", "Jonathan", ""], ["Blais", "Brian", ""]]}, {"id": "1603.00568", "submitter": "Andr\\'es Riquelme", "authors": "Andr\\'es Riquelme and Marcela Parada", "title": "The Value of A Statistical Life in Absence of Panel Data: What can we\n  do?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper I show how reliable estimates of the Value of a Statistical\nLife (VSL) can be obtained using cross sectional data using Garen's\ninstrumental variable (IV) approach. The increase in the range confidence\nintervals due to the IV setup can be reduced by a factor of 3 by using a proxy\nto risk attitude. In order state the \"precision\" of the cross sectional VSL\nestimates I estimate the VSL using Chilean panel data and use them as benchmark\nfor different cross sectional specifications. The use of the proxy eliminates\nneed for using hard-to-find instruments for the job risk level and narrows the\nconfidence intervals for the workers in the Chilean labor market for the year\n2009.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 03:46:19 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Riquelme", "Andr\u00e9s", ""], ["Parada", "Marcela", ""]]}, {"id": "1603.00896", "submitter": "Yuchen Zheng", "authors": "Ross P. Hilton, Nicoleta Serban, Richard Y. Zheng", "title": "Uncovering Longitudinal Healthcare Utilization from Patient-Level\n  Medical Claims Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this study is to introduce methodology for studying\nlongitudinal claims data observed at the patient level, with inference on the\nheterogeneity of healthcare utilization behaviors within large healthcare\nsystems such as Medicaid. The proposed approach is model-based, allowing for\nvisualization of longitudinal utilization behaviors using simple stochastic\ngraphical networks. The approach is general, providing a framework for the\nstudy of other chronic conditions wherever longitudinal healthcare utilization\ndata are available. Our methods are inspired by and applied to patient-level\nMedicaid claims for asthma-diagnosed children diagnosed observed over a period\nof five years, with a comparison of two neighboring states, Georgia and North\nCarolina.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 21:29:21 GMT"}], "update_date": "2016-03-04", "authors_parsed": [["Hilton", "Ross P.", ""], ["Serban", "Nicoleta", ""], ["Zheng", "Richard Y.", ""]]}, {"id": "1603.00974", "submitter": "Pixu Shi", "authors": "Pixu Shi, Anru Zhang, Hongzhe Li", "title": "Regression Analysis for Microbiome Compositional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One important problem in microbiome analysis is to identify the bacterial\ntaxa that are associated with a response, where the microbiome data are\nsummarized as the composition of the bacterial taxa at different taxonomic\nlevels. This paper considers regression analysis with such compositional data\nas covariates. In order to satisfy the subcompositional coherence of the\nresults, linear models with a set of linear constraints on the regression\ncoefficients are introduced. Such models allow regression analysis for\nsubcompositions and include the log-contrast model for compositional covariates\nas a special case. A penalized estimation procedure for estimating the\nregression coefficients and for selecting variables under the linear\nconstraints is developed. A method is also proposed to obtain de-biased\nestimates of the regression coefficients that are asymptotically unbiased and\nhave a joint asymptotic multivariate normal distribution. This provides valid\nconfidence intervals of the regression coefficients and can be used to obtain\nthe $p$-values. Simulation results show the validity of the confidence\nintervals and smaller variances of the de-biased estimates when the linear\nconstraints are imposed. The proposed methods are applied to a gut microbiome\ndata set and identify four bacterial genera that are associated with the body\nmass index after adjusting for the total fat and caloric intakes.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 05:29:30 GMT"}], "update_date": "2016-03-04", "authors_parsed": [["Shi", "Pixu", ""], ["Zhang", "Anru", ""], ["Li", "Hongzhe", ""]]}, {"id": "1603.01041", "submitter": "Gareth Peters Dr", "authors": "Gareth W. Peters, Wilson Y. Chen, Richard H. Gerlach", "title": "Estimating Quantile Families of Loss Distributions for Non-Life\n  Insurance Modelling via L-moments", "comments": "42 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses different classes of loss models in non-life insurance\nsettings. It then overviews the class Tukey transform loss models that have not\nyet been widely considered in non-life insurance modelling, but offer\nopportunities to produce flexible skewness and kurtosis features often required\nin loss modelling. In addition, these loss models admit explicit quantile\nspecifications which make them directly relevant for quantile based risk\nmeasure calculations. We detail various parameterizations and sub-families of\nthe Tukey transform based models, such as the g-and-h, g-and-k and g-and-j\nmodels, including their properties of relevance to loss modelling.\n  One of the challenges with such models is to perform robust estimation for\nthe loss model parameters that will be amenable to practitioners when fitting\nsuch models. In this paper we develop a novel, efficient and robust estimation\nprocedure for estimation of model parameters in this family Tukey transform\nmodels, based on L-moments. It is shown to be more robust and efficient than\ncurrent state of the art methods of estimation for such families of loss models\nand is simple to implement for practical purposes.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 10:11:29 GMT"}], "update_date": "2016-03-04", "authors_parsed": [["Peters", "Gareth W.", ""], ["Chen", "Wilson Y.", ""], ["Gerlach", "Richard H.", ""]]}, {"id": "1603.01066", "submitter": "Anna-Kaisa Ylitalo", "authors": "Anna-Kaisa Ylitalo, Aila S\\\"arkk\\\"a and Peter Guttorp", "title": "What we look at in paintings: A comparison between experienced and\n  inexperienced art viewers", "comments": "29 pages, 15 figures", "journal-ref": "Annals of Applied Statistics 2016, Vol.10, No. 2, 549-574", "doi": "10.1214/16-AOAS921", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How do people look at art? Are there any differences between how experienced\nand inexperienced art viewers look at a painting? We approach these questions\nby analyzing and modeling eye movement data from a cognitive art research\nexperiment, where the eye movements of twenty test subjects, ten experienced\nand ten inexperienced art viewers, were recorded while they were looking at\npaintings.\n  Eye movements consist of stops of the gaze as well as jumps between the\nstops. Hence, the observed gaze stop locations can be thought as a spatial\npoint pattern, which can be modeled by a spatio-temporal point process. We\nintroduce some statistical tools to analyze the spatio-temporal eye movement\ndata, and compare the eye movements of experienced and inexperienced art\nviewers. In addition, we develop a stochastic model, which is rather simple but\nfits quite well to the eye movement data, to further investigate the\ndifferences between the two groups through functional summary statistics.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 11:55:25 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Ylitalo", "Anna-Kaisa", ""], ["S\u00e4rkk\u00e4", "Aila", ""], ["Guttorp", "Peter", ""]]}, {"id": "1603.01134", "submitter": "Mauricio Santillana", "authors": "Mauricio Santillana and Ashleigh Tuite and Tahmina Nasserie and Paul\n  Fine and David Champredon and Leonid Chindelevitch and Jonathan Dushoff and\n  David Fisman", "title": "Relatedness of the Incidence Decay with Exponential Adjustment (IDEA)\n  Model, \"Farr's Law\" and Compartmental Difference Equation SIR Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mathematical models are often regarded as recent innovations in the\ndescription and analysis of infectious disease outbreaks and epidemics, but\nsimple models have been in use for projection of epidemic trajectories for more\nthan a century. We recently described a single equation model (the incidence\ndecay with exponential adjustment, or IDEA, model) that can be used for short\nterm forecasting. In the mid-19th century, Dr. William Farr developed a single\nequation approach (Farr's law) for epidemic forecasting. We show here that the\ntwo models are in fact identical, and can be expressed in terms of one another,\nand also in terms of a susceptible-infectious-removed (SIR) compartmental model\nwith improving control. This demonstrates that the concept of the reproduction\nnumber, R0, is implicit to Farr's (pre-microbial era) work, and also suggests\nthat control of epidemics, whether via behavior change or intervention, is as\nintegral to the natural history of epidemics as is the dynamics of disease\ntransmission.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 15:27:26 GMT"}], "update_date": "2016-03-04", "authors_parsed": [["Santillana", "Mauricio", ""], ["Tuite", "Ashleigh", ""], ["Nasserie", "Tahmina", ""], ["Fine", "Paul", ""], ["Champredon", "David", ""], ["Chindelevitch", "Leonid", ""], ["Dushoff", "Jonathan", ""], ["Fisman", "David", ""]]}, {"id": "1603.01308", "submitter": "Leopoldo Catania", "authors": "Leopoldo Catania", "title": "Dynamic Adaptive Mixture Models", "comments": "47 pages, 4 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new class of Dynamic Mixture Models (DAMMs) being\nable to sequentially adapt the mixture components as well as the mixture\ncomposition using information coming from the data. The information driven\nnature of the proposed class of models allows to exactly compute the full\nlikelihood and to avoid computer intensive simulation schemes. An extensive\nMonte Carlo experiment reveals that the new proposed model can accurately\napproximate the more complicated Stochastic Dynamic Mixture Model previously\nintroduced in the literature as well as other kind of models. The properties of\nthe new proposed class of models are discussed through the paper and an\napplication in financial econometrics is reported.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 22:50:57 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Catania", "Leopoldo", ""]]}, {"id": "1603.01376", "submitter": "Florian Ziel", "authors": "Florian Ziel and Bidong Liu", "title": "Lasso estimation for GEFCom2014 probabilistic electric load forecasting", "comments": "appears in the special issue Probabilistic Energy Forecasting of the\n  International Journal of Forecasting", "journal-ref": "International Journal of Forecasting, 32.3 (2016) 1029-1037", "doi": "10.1016/j.ijforecast.2016.01.001", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a methodology for probabilistic load forecasting that is based on\nlasso (least absolute shrinkage and selection operator) estimation. The model\nconsidered can be regarded as a bivariate time-varying threshold\nautoregressive(AR) process for the hourly electric load and temperature. The\njoint modeling approach incorporates the temperature effects directly, and\nreflects daily, weekly, and annual seasonal patterns and public holiday\neffects. We provide two empirical studies, one based on the probabilistic load\nforecasting track of the Global Energy Forecasting Competition 2014\n(GEFCom2014-L), and the other based on another recent probabilistic load\nforecasting competition that follows a setup similar to that of GEFCom2014-L.\nIn both empirical case studies, the proposed methodology outperforms two\nmultiple linear regression based benchmarks from among the top eight entries to\nGEFCom2014-L.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 08:39:22 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Ziel", "Florian", ""], ["Liu", "Bidong", ""]]}, {"id": "1603.01397", "submitter": "Sunil Kumar Dr", "authors": "Sunil Kumar", "title": "Latent class analyisis for reliable measure of inflation expectation in\n  the indian public", "comments": "16 pages, 04 Tables and 03 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.GN q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main aim of this paper is to inspect the properties of survey based on\nhouseholds inflation expectations, conducted by Reserve Bank of India. It is\ntheorized that the respondents answers are exaggerated by extreme response\nbias. Latent class analysis has been hailed as a promising technique for\nstudying measurement errors in surveys, because the model produces estimates of\nthe error rates associated with a given question of the questionnaire. I have\nidentified a model with optimum performance and hence categorize the objective\nas well as reliable classifiers or otherwise.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 09:44:21 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Kumar", "Sunil", ""]]}, {"id": "1603.01411", "submitter": "L\\'aszl\\'o Csat\\'o", "authors": "L\\'aszlo Csat\\'o", "title": "A pairwise comparison approach to ranking in chess team championships", "comments": "8 pages, 2 figures, Tavaszi Sz\\'el 2012 Konferenciak\\\"otet.\n  Doktoranduszok Orsz\\'agos Sz\\\"ovets\\'ege. Budapest. ISBN 978-963-89560-0-2.\n  pages 514-519", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chess championships are often organised as a Swiss-system tournament, causing\ngreat challenges in ranking the participants due to the different strength of\nschedules and possible circular triads. The paper suggests that pairwise\ncomparison matrices perform well in similar ranking problems. Some features of\nthe proposed method are illustrated by the results of the 18th European Team\nChess Championship. The analysis is able to take into account the influence of\ndifferent opponents and robust with respect to the scaling technique chosen.\nThe method is simple to compute as a solution of a linear equation system.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 10:19:38 GMT"}, {"version": "v2", "created": "Wed, 2 Nov 2016 10:20:30 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Csat\u00f3", "L\u00e1szlo", ""]]}, {"id": "1603.01476", "submitter": "Nicole Barthel", "authors": "Nicole Barthel, Candida Geerdens, Matthias Killiches, Paul Janssen and\n  Claudia Czado", "title": "Vine copula based likelihood estimation of dependence patterns in\n  multivariate event time data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many studies multivariate event time data are generated from clusters\nhaving a possibly complex association pattern. Flexible models are needed to\ncapture this dependence. Vine copulas serve this purpose. Inference methods for\nvine copulas are available for complete data. Event time data, however, are\noften subject to right-censoring. As a consequence, the existing inferential\ntools, e.g. likelihood estimation, need to be adapted. A two-stage estimation\napproach is proposed. First, the marginal distributions are modeled. Second,\nthe dependence structure modeled by a vine copula is estimated via likelihood\nmaximization. Due to the right-censoring single and double integrals show up in\nthe copula likelihood expression such that numerical integration is needed for\nits evaluation. For the dependence modeling a sequential estimation approach\nthat facilitates the computational challenges of the likelihood optimization is\nprovided. A three-dimensional simulation study provides evidence for the good\nfinite sample performance of the proposed method. Using four-dimensional\nmastitis data, it is shown how an appropriate vine copula model can be selected\nfor data at hand.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 14:29:18 GMT"}, {"version": "v2", "created": "Sat, 22 Jul 2017 08:22:19 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Barthel", "Nicole", ""], ["Geerdens", "Candida", ""], ["Killiches", "Matthias", ""], ["Janssen", "Paul", ""], ["Czado", "Claudia", ""]]}, {"id": "1603.01640", "submitter": "Benjamin Trendelkamp-Schroer", "authors": "Benjamin Trendelkamp-Schroer and Hao Wu and Frank Noe", "title": "Reversible Markov chain estimation using convex-concave programming", "comments": "17pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a convex-concave reformulation of the reversible Markov chain\nestimation problem and outline an efficient numerical scheme for the solution\nof the resulting problem based on a primal-dual interior point method for\nmonotone variational inequalities. Extensions to situations in which\ninformation about the stationary vector is available can also be solved via the\nconvex- concave reformulation. The method can be generalized and applied to the\ndiscrete transition matrix reweighting analysis method to perform inference\nfrom independent chains with specified couplings between the stationary\nprobabilities. The proposed approach offers a significant speed-up compared to\na fixed-point iteration for a number of relevant applications.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 21:55:40 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Trendelkamp-Schroer", "Benjamin", ""], ["Wu", "Hao", ""], ["Noe", "Frank", ""]]}, {"id": "1603.01840", "submitter": "Gal Dalal", "authors": "Gal Dalal, Elad Gilboa, Shie Mannor", "title": "Hierarchical Decision Making In Electricity Grid Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The power grid is a complex and vital system that necessitates careful\nreliability management. Managing the grid is a difficult problem with multiple\ntime scales of decision making and stochastic behavior due to renewable energy\ngenerations, variable demand and unplanned outages. Solving this problem in the\nface of uncertainty requires a new methodology with tractable algorithms. In\nthis work, we introduce a new model for hierarchical decision making in complex\nsystems. We apply reinforcement learning (RL) methods to learn a proxy, i.e., a\nlevel of abstraction, for real-time power grid reliability. We devise an\nalgorithm that alternates between slow time-scale policy improvement, and fast\ntime-scale value function approximation. We compare our results to prevailing\nheuristics, and show the strength of our method.\n", "versions": [{"version": "v1", "created": "Sun, 6 Mar 2016 16:30:34 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Dalal", "Gal", ""], ["Gilboa", "Elad", ""], ["Mannor", "Shie", ""]]}, {"id": "1603.01851", "submitter": "Zhigang Li", "authors": "Zhigang Li, H. R. Frost, Tor D. Tosteson, Lihui Zhao, Lei Liu,\n  Kathleen Lyons, Huaihou Chen, Bernard Cole, David Currow and Marie Bakitas", "title": "A Semiparametric Joint Model for Terminal Trend of Quality of Life and\n  Survival in Palliative Care Research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Palliative medicine is an interdisciplinary specialty focusing on improving\nquality of life (QOL) for patients with serious illness and their families.\nPalliative care programs are available or under development at over 80% of\nlarge US hospitals (300+ beds). Palliative care clinical trials present unique\nanalytic challenges relative to evaluating the palliative care treatment\nefficacy which is to improve patients diminishing QOL as disease progresses\ntowards end of life (EOL). A unique feature of palliative care clinical trials\nis that patients will experience decreasing QOL during the trial despite\npotentially beneficial treatment. Often longitudinal QOL and survival data are\nhighly correlated which, in the face of censoring, makes it challenging to\nproperly analyze and interpret longitudinal QOL trajectory. To address these\nissues, we propose a novel semiparametric statistical approach to jointly model\nlongitudinal QOL and survival data. There are two sub-models in our approach: a\nsemiparametric mixed effects model for longitudinal QOL and a Cox model for\nsurvival. We use regression splines method to estimate the nonparametric curves\nand AIC to select knots. We assess the model through simulation and application\nto establish a novel modeling approach that could be applied in future\npalliative care treatment research trials.\n", "versions": [{"version": "v1", "created": "Sun, 6 Mar 2016 18:03:01 GMT"}, {"version": "v2", "created": "Mon, 14 Mar 2016 16:40:41 GMT"}, {"version": "v3", "created": "Sat, 16 Apr 2016 00:35:05 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Li", "Zhigang", ""], ["Frost", "H. R.", ""], ["Tosteson", "Tor D.", ""], ["Zhao", "Lihui", ""], ["Liu", "Lei", ""], ["Lyons", "Kathleen", ""], ["Chen", "Huaihou", ""], ["Cole", "Bernard", ""], ["Currow", "David", ""], ["Bakitas", "Marie", ""]]}, {"id": "1603.01871", "submitter": "Gildas  Ratovomirija", "authors": "Enkelejd Hashorva, Gildas Ratovomirija, Maissa Tamraz", "title": "Insurance Applications of Some New Dependence Models derived from\n  Multivariate Collective Models", "comments": "21 pages, 13 tables, 1 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider two different portfolios which have claims triggered by the same\nevents. Their corresponding collective model over a fixed time period is given\nin terms of individual claim sizes $(X_i,Y_i), i\\ge 1$ and a claim counting\nrandom variable $N$. In this paper we are concerned with the joint distribution\nfunction $F$ of the \\ece{largest claim sizes} $(X_{N:N}, Y_{N:N})$. By allowing\n$N$ to depend on some parameter, say $\\theta$, then $F=F(\\theta)$ is for\nvarious choices of $N$ a tractable parametric family of bivariate distribution\nfunctions. We present three applications of the implied parametric models to\nsome data from the literature and a new data set from a Swiss insurance\ncompany. Furthermore, we investigate both distributional and asymptotic\nproperties of $(X_{N:N}, Y_{N:N})$.\n", "versions": [{"version": "v1", "created": "Sun, 6 Mar 2016 20:36:27 GMT"}, {"version": "v2", "created": "Thu, 6 Oct 2016 19:44:11 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Hashorva", "Enkelejd", ""], ["Ratovomirija", "Gildas", ""], ["Tamraz", "Maissa", ""]]}, {"id": "1603.01985", "submitter": "Lucia Modugno mrs", "authors": "Silvia Cagnone, Simone Giannerini, Lucia Modugno", "title": "Multilevel Models with Stochastic Volatility for Repeated\n  Cross-Sections: an Application to tribal Art Prices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a multilevel specification with stochastic\nvolatility for repeated cross-sectional data. Modelling the time dynamics in\nrepeated cross sections requires a suitable adaptation of the multilevel\nframework where the individuals/items are modelled at the first level whereas\nthe time component appears at the second level. We perform maximum likelihood\nestimation by means of a nonlinear state space approach combined with\nGauss-Legendre quadrature methods to approximate the likelihood function. We\napply the model to the first database of tribal art items sold in the most\nimportant auction houses worldwide. The model allows to account properly for\nthe heteroscedastic and autocorrelated volatility observed and has superior\nforecasting performance. Also, it provides valuable information on market\ntrends and on predictability of prices that can be used by art markets\nstakeholders.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 09:47:12 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Cagnone", "Silvia", ""], ["Giannerini", "Simone", ""], ["Modugno", "Lucia", ""]]}, {"id": "1603.02494", "submitter": "Tapesh Santra", "authors": "Tapesh Santra", "title": "A Bayesian non-parametric method for clustering high-dimensional binary\n  data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real life problems, objects are described by large number of binary\nfeatures. For instance, documents are characterized by presence or absence of\ncertain keywords; cancer patients are characterized by presence or absence of\ncertain mutations etc. In such cases, grouping together similar\nobjects/profiles based on such high dimensional binary features is desirable,\nbut challenging. Here, I present a Bayesian non parametric algorithm for\nclustering high dimensional binary data. It uses a Dirichlet Process (DP)\nmixture model and simulated annealing to not only cluster binary data, but also\nfind optimal number of clusters in the data. The performance of the algorithm\nwas evaluated and compared with other algorithms using simulated datasets. It\noutperformed all other clustering methods that were tested in the simulation\nstudies. It was also used to cluster real datasets arising from document\nanalysis, handwritten image analysis and cancer research. It successfully\ndivided a set of documents based on their topics, hand written images based on\ndifferent styles of writing digits and identified tissue and mutation\nspecificity of chemotherapy treatments.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 12:02:59 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Santra", "Tapesh", ""]]}, {"id": "1603.02665", "submitter": "Maryam Sohrabi", "authors": "Maryam Sohrabi and Mahmoud Zarepour", "title": "A Note on Bootstrapping M-estimates from Unstable AR(2) Process with\n  Infinite Variance Innovations", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The limiting distribution for M-estimates in a non-stationary autoregressive\nmodel with heavy-tailed error is computationally intractable. To make\ninferences based on the M-estimates, the bootstrap procedure can be used to\napproximate the sampling distribution. In this paper, we show that the\nbootstrap scheme with $m=o(n)$ resampling sample size when $m/n \\to 0$ is\napproximately valid in a multiple unit roots time series with innovations in\nthe domain of attraction of a stable law with index $0<\\alpha\\leq2$.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 20:40:59 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Sohrabi", "Maryam", ""], ["Zarepour", "Mahmoud", ""]]}, {"id": "1603.02977", "submitter": "Sayed Pouria Talebi", "authors": "Sayed Pouria Talebi and Danilo P. Mandic", "title": "Frequency estimation in three-phase power systems with harmonic\n  contamination: A multistage quaternion Kalman filtering approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the need for accurate frequency information, a novel algorithm\nfor estimating the fundamental frequency and its rate of change in three-phase\npower systems is developed. This is achieved through two stages of Kalman\nfiltering. In the first stage a quaternion extended Kalman filter, which\nprovides a unified framework for joint modeling of voltage measurements from\nall the phases, is used to estimate the instantaneous phase increment of the\nthree-phase voltages. The phase increment estimates are then used as\nobservations of the extended Kalman filter in the second stage that accounts\nfor the dynamic behavior of the system frequency and simultaneously estimates\nthe fundamental frequency and its rate of change. The framework is then\nextended to account for the presence of harmonics. Finally, the concept is\nvalidated through simulation on both synthetic and real-world data.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 08:07:26 GMT"}], "update_date": "2016-03-10", "authors_parsed": [["Talebi", "Sayed Pouria", ""], ["Mandic", "Danilo P.", ""]]}, {"id": "1603.03316", "submitter": "Marius Thomas", "authors": "Marius Thomas and Bj\\\"orn Bornkamp", "title": "Comparing Approaches to Treatment Effect Estimation for Subgroups in\n  Clinical Trials", "comments": "Version 2 is a minor revision of the original manuscript based on\n  reviewers' comments. The title of the manuscript has been changed and several\n  small remarks have been added throughout the document, that should improve\n  the presentation of the methods and the discussion of results. Moreover\n  supplementary material has been added", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying subgroups, which respond differently to a treatment, both in\nterms of efficacy and safety, is an important part of drug development. A\nwell-known challenge in exploratory subgroup analyses is the small sample size\nin the considered subgroups, which is usually too low to allow for definite\ncomparisons. In early phase trials this problem is further exaggerated, because\nlimited or no clinical prior information on the drug and plausible subgroups is\navailable. We evaluate novel strategies for treatment effect estimation in\nthese settings in a simulation study motivated by real clinical trial\nsituations. We compare several approaches to estimate treatment effects for\nselected subgroups, employing model averaging, resampling and Lasso regression\nmethods. Two subgroup identification approaches are employed, one based on\ncategorization of covariates and the other based on splines. Our results show\nthat naive estimation of the treatment effect, which ignores that a selection\nhas taken place, leads to bias and overoptimistic conclusions. For the\nconsidered simulation scenarios virtually all evaluated novel methods provide\nmore adequate estimates of the treatment effect for selected subgroups, in\nterms of bias, MSE and confidence interval coverage.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 16:19:24 GMT"}, {"version": "v2", "created": "Mon, 27 Jun 2016 09:47:11 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Thomas", "Marius", ""], ["Bornkamp", "Bj\u00f6rn", ""]]}, {"id": "1603.03448", "submitter": "Sijia Liu", "authors": "Sijia Liu, Swarnendu Kar, Makan Fardad, Pramod K. Varshney", "title": "Optimized Sensor Collaboration for Estimation of Temporally Correlated\n  Parameters", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2016.2612173", "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim to design the optimal sensor collaboration strategy for\nthe estimation of time-varying parameters, where collaboration refers to the\nact of sharing measurements with neighboring sensors prior to transmission to a\nfusion center. We begin by addressing the sensor collaboration problem for the\nestimation of uncorrelated parameters. We show that the resulting collaboration\nproblem can be transformed into a special nonconvex optimization problem, where\na difference of convex functions carries all the nonconvexity. This specific\nproblem structure enables the use of a convex-concave procedure to obtain a\nnear-optimal solution. When the parameters of interest are temporally\ncorrelated, a penalized version of the convex-concave procedure becomes well\nsuited for designing the optimal collaboration scheme. In order to improve\ncomputational efficiency, we further propose a fast algorithm that scales\ngracefully with problem size via the alternating direction method of\nmultipliers. Numerical results are provided to demonstrate the effectiveness of\nour approach and the impact of parameter correlation and temporal dynamics of\nsensor networks on estimation performance.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 21:18:36 GMT"}, {"version": "v2", "created": "Fri, 26 Aug 2016 02:03:23 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Liu", "Sijia", ""], ["Kar", "Swarnendu", ""], ["Fardad", "Makan", ""], ["Varshney", "Pramod K.", ""]]}, {"id": "1603.03473", "submitter": "Rodrigo Labouriau", "authors": "Rodrigo Labouriau", "title": "The Laplace transform and polynomial approximation in L2", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CA math.FA stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short note gives a sufficient condition for having the class of\npolynomials dense in the space of square integrable functions with respect to a\nfinite measure dominated by the Lebesgue measure in the real line, here denoted\nby $L^2$. It is shown that if the Laplace transform of the measure in play is\nbounded in a neighbourhood of the origin, then the moments of all order are\nfinite and the class of polynomials is dense in $L^2$. The existence of the\nmoments of all orders is well known for the case where the measure is\nconcentrated in the positive real line (see Feller, 1966), but the result\nconcerning the polynomial approximation is original, even thought the proof is\nrelatively simple. Additionally, an alternative stronger condition easier to be\nverified not involving the calculation of the Laplace transform is given. The\ncondition essentially says that the density of the measure should have\nexponential decaying tails. The tools presented are of interest for\nconstructing semiparametric extensions of classic parametric models.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2016 17:33:14 GMT"}], "update_date": "2016-03-14", "authors_parsed": [["Labouriau", "Rodrigo", ""]]}, {"id": "1603.03484", "submitter": "Luca Rossini", "authors": "Luciana Dalla Valle, Fabrizio Leisen, Luca Rossini", "title": "Bayesian Nonparametric Conditional Copula Estimation of Twin Data", "comments": "Forthcoming in Journal of the Royal Statistical Society (Series C)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several studies on heritability in twins aim at understanding the different\ncontribution of environmental and genetic factors to specific traits.\nConsidering the National Merit Twin Study, our purpose is to correctly analyse\nthe influence of the socioeconomic status on the relationship between twins'\ncognitive abilities. Our methodology is based on conditional copulas, which\nallow us to model the effect of a covariate driving the strength of dependence\nbetween the main variables. We propose a flexible Bayesian nonparametric\napproach for the estimation of conditional copulas, which can model any\nconditional copula density. Our methodology extends the work of Wu et al (2015)\nby introducing dependence from a covariate in an infinite mixture model. Our\nresults suggest that environmental factors are more influential in families\nwith lower socio-economic position.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 23:11:37 GMT"}, {"version": "v2", "created": "Mon, 14 Mar 2016 09:28:20 GMT"}, {"version": "v3", "created": "Wed, 22 Feb 2017 08:10:05 GMT"}, {"version": "v4", "created": "Mon, 3 Jul 2017 06:28:35 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Valle", "Luciana Dalla", ""], ["Leisen", "Fabrizio", ""], ["Rossini", "Luca", ""]]}, {"id": "1603.03593", "submitter": "Vincent Brault", "authors": "Vincent Brault and Julien Chiquet and C\\'eline L\\'evy-Leduc", "title": "Fast Detection of Block Boundaries in Block Wise Constant Matrices: An\n  Application to HiC data", "comments": "35 pages, 19 figures, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for estimating the location of block boundaries\n(change-points) in a random matrix consisting of a block wise constant matrix\nobserved in white noise. Our method consists in rephrasing this task as a\nvariable selection issue. We use a penalized least-squares criterion with an\n$\\ell_1$-type penalty for dealing with this issue. We first provide some\ntheoretical results ensuring the consistency of our change-point estimators.\nThen, we explain how to implement our method in a very efficient way. Finally,\nwe provide some empirical evidence to support our claims and apply our approach\nto HiC data which are used in molecular biology for better understanding the\ninfluence of the chromosomal conformation on the cells functioning.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 11:01:50 GMT"}], "update_date": "2016-03-14", "authors_parsed": [["Brault", "Vincent", ""], ["Chiquet", "Julien", ""], ["L\u00e9vy-Leduc", "C\u00e9line", ""]]}, {"id": "1603.03799", "submitter": "Gustavo C. Amaral Dr.", "authors": "Mario Souto, Joaquim D. Garcia and Gustavo C. Amaral", "title": "$\\ell_1$ Adaptive Trend Filter via Fast Coordinate Descent", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying the unknown underlying trend of a given noisy signal is extremely\nuseful for a wide range of applications. The number of potential trends might\nbe exponential, which can be computationally exhaustive even for short signals.\nAnother challenge, is the presence of abrupt changes and outliers at unknown\ntimes which impart resourceful information regarding the signal's\ncharacteristics. In this paper, we present the $\\ell_1$ Adaptive Trend Filter,\nwhich can consistently identify the components in the underlying trend and\nmultiple level-shifts, even in the presence of outliers. Additionally, an\nenhanced coordinate descent algorithm which exploit the filter design is\npresented. Some implementation details are discussed and a version in the Julia\nlanguage is presented along with two distinct applications to illustrate the\nfilter's potential.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 21:58:02 GMT"}, {"version": "v2", "created": "Mon, 16 May 2016 19:03:24 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Souto", "Mario", ""], ["Garcia", "Joaquim D.", ""], ["Amaral", "Gustavo C.", ""]]}, {"id": "1603.03950", "submitter": "Pavel Krupskii", "authors": "Pavel Krupskii and Marc G. Genton", "title": "A Copula Model for Non-Gaussian Multivariate Spatial Data", "comments": "33 pages, 4 tables and 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new copula model for replicated multivariate spatial data.\nUnlike classical models that assume multivariate normality of the data, the\nproposed copula is based on the assumption that some factors exist that affect\nthe joint spatial dependence of all measurements of each variable as well as\nthe joint dependence among these variables. The model is parameterized in terms\nof a cross-covariance function that may be chosen from the many models proposed\nin the literature. In addition, there are additive factors in the model that\nallow tail dependence and reflection asymmetry of each variable measured at\ndifferent locations and of different variables to be modeled. The proposed\napproach can therefore be seen as an extension of the linear model of\ncoregionalization widely used for modeling multivariate spatial data. The\nlikelihood of the model can be obtained in a simple form and therefore the\nlikelihood estimation is quite fast. The model is not restricted to the set of\ndata locations, and using the estimated copula, spatial data can be\ninterpolated at locations where values of variables are unknown. We apply the\nproposed model to temperature and pressure data and compare its performance\nwith the performance of a popular model from multivariate geostatistics.\n", "versions": [{"version": "v1", "created": "Sat, 12 Mar 2016 17:59:59 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 22:11:09 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Krupskii", "Pavel", ""], ["Genton", "Marc G.", ""]]}, {"id": "1603.04096", "submitter": "Weston Faber", "authors": "W. Faber, S. Chakravorty, and Islam I. Hussein", "title": "Multi-Target Tracking Using A Randomized Hypothesis Generation Technique", "comments": "27 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a randomized version of the finite set statistics\n(FISST) Bayesian recursions for multi-object tracking problems. We propose a\nhypothesis level derivation of the FISST equations that shows that the\nmulti-object tracking problem may be considered as a finite state space\nBayesian filtering problem, albeit with a growing state space. We further show\nthat the FISST and Multi-Hypothesis Tracking (MHT) methods for multi-target\ntracking are essentially the same. We propose a randomized scheme, termed\nrandomized FISST (R-FISST), where we sample the highly likely hypotheses using\nMarkov Chain Monte Carlo (MCMC) methods which allows us to keep the problem\ncomputationally tractable. We apply the R-FISST technique to a fifty-object\nbirth and death Space Situational Awareness (SSA) tracking and detection\nproblem. We also compare the R-FISST technique to the Hypothesis Oriented\nMultiple Hypothesis Tracking (HOMHT) method using an SSA example.\n", "versions": [{"version": "v1", "created": "Sun, 13 Mar 2016 23:45:27 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Faber", "W.", ""], ["Chakravorty", "S.", ""], ["Hussein", "Islam I.", ""]]}, {"id": "1603.04189", "submitter": "Olivier Bouaziz", "authors": "Olivier Bouaziz (MAP5), Gr\\'egory Nuel (LPMA)", "title": "A Change-Point Model for Detecting Heterogeneity in Ordered Survival\n  Responses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we suggest a new statistical approach considering survival\nheterogeneity as a breakpoint model in an ordered sequence of time to event\nvariables. The survival responses need to be ordered according to a numerical\ncovariate. Our esti- mation method will aim at detecting heterogeneity that\ncould arise through the or- dering covariate. We formally introduce our model\nas a constrained Hidden Markov Model (HMM) where the hidden states are the\nunknown segmentation (breakpoint locations) and the observed states are the\nsurvival responses. We derive an efficient Expectation-Maximization (EM)\nframework for maximizing the likelihood of this model for a wide range of\nbaseline hazard forms (parametrics or nonparametric). The posterior\ndistribution of the breakpoints is also derived and the selection of the number\nof segments using penalized likelihood criterion is discussed. The performance\nof our survival breakpoint model is finally illustrated on a diabetes dataset\nwhere the observed survival times are ordered according to the calendar time of\ndisease onset.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 10:25:59 GMT"}, {"version": "v2", "created": "Fri, 23 Sep 2016 11:14:47 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Bouaziz", "Olivier", "", "MAP5"], ["Nuel", "Gr\u00e9gory", "", "LPMA"]]}, {"id": "1603.04834", "submitter": "Dionysios Kalogerias", "authors": "Dionysios S. Kalogerias and Athina P. Petropulu", "title": "Mobile Beamforming & Spatially Controlled Relay Communications", "comments": "41st International Conference on Acoustics, Speech & Signal\n  Processing (ICASSP 2016) Presentation Available: http://sigport.org/831", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.IT math.IT math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider stochastic motion planning in single-source single-destination\nrobotic relay networks, under a cooperative beamforming framework. Assuming\nthat the communication medium constitutes a spatiotemporal stochastic field, we\npropose a 2-stage stochastic programming formulation of the problem of\nspecifying the positions of the relays, such that the expected reciprocal of\ntheir total beamforming power is maximized. Stochastic decision making is made\non the basis of random causal CSI. Recognizing the intractability of the\noriginal problem, we propose a lower bound relaxation, resulting to a\nnontrivial optimization problem with respect to the relay locations, which is\nequivalent to a small set of simple, tractable subproblems. Our formulation\nresults in spatial controllers with a predictive character; at each time slot,\nthe new relay positions should be such that the expected power reciprocal at\nthe next time slot is maximized. Quite interestingly, the optimal control\npolicy to the relaxed problem is purely selective; under a certain sense, only\nthe best relay should move.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 19:42:05 GMT"}, {"version": "v2", "created": "Sat, 19 Mar 2016 19:48:04 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Kalogerias", "Dionysios S.", ""], ["Petropulu", "Athina P.", ""]]}, {"id": "1603.04835", "submitter": "William Chad Young", "authors": "William Chad Young, Ka Yee Yeung, Adrian E. Raftery", "title": "A Posterior Probability Approach for Gene Regulatory Network Inference\n  in Genetic Perturbation Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring gene regulatory networks is an important problem in systems\nbiology. However, these networks can be hard to infer from experimental data\nbecause of the inherent variability in biological data as well as the large\nnumber of genes involved. We propose a fast, simple method for inferring\nregulatory relationships between genes from knockdown experiments in the NIH\nLINCS dataset by calculating posterior probabilities, incorporating prior\ninformation. We show that the method is able to find previously identified\nedges from TRANSFAC and JASPAR and discuss the merits and limitations of this\napproach.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 19:43:20 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Young", "William Chad", ""], ["Yeung", "Ka Yee", ""], ["Raftery", "Adrian E.", ""]]}, {"id": "1603.04929", "submitter": "Konstantin Zuev M", "authors": "Konstantin Zuev", "title": "Statistical Inference", "comments": "ACM Lecture Notes, 145 pages, 79 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.HO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What is Statistics? Opinions vary. In fact, there is a continuous spectrum of\nattitudes toward statistics ranging from pure theoreticians, proving asymptotic\nefficiency and searching for most powerful tests, to wild practitioners,\nblindly reporting p-values and claiming statistical significance for\nscientifically insignificant results. In these notes statistics is viewed as a\nbranch of mathematical engineering, that studies ways of extracting reliable\ninformation from limited data for learning, prediction, and decision making in\nthe presence of uncertainty. These ACM lecture notes are based on the courses\nthe author taught at the University of Southern California in 2012 and 2013,\nand at the California Institute of Technology in 2016.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 01:02:07 GMT"}], "update_date": "2016-03-17", "authors_parsed": [["Zuev", "Konstantin", ""]]}, {"id": "1603.05038", "submitter": "Reik Donner", "authors": "Jonathan F. Siegmund, Nicole Siegmund, Reik V. Donner", "title": "CoinCalc -- A new R package for quantifying simultaneities of event\n  series", "comments": null, "journal-ref": null, "doi": "10.1016/j.cageo.2016.10.004", "report-no": null, "categories": "stat.CO physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the new R package CoinCalc for performing event coincidence\nanalysis (ECA), a novel statistical method to quantify the simultaneity of\nevents contained in two series of observations, either as simultaneous or\nlagged coincidences within a user-specific temporal tolerance window. The\npackage also provides different analytical as well as surrogate-based\nsignificance tests (valid under different assumptions about the nature of the\nobserved event series) as well as an intuitive visualization of the identified\ncoincidences. We demonstrate the usage of CoinCalc based on two typical\ngeoscientific example problems addressing the relationship between\nmeteorological extremes and plant phenology as well as that between soil\nproperties and land cover.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 11:23:22 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Siegmund", "Jonathan F.", ""], ["Siegmund", "Nicole", ""], ["Donner", "Reik V.", ""]]}, {"id": "1603.05215", "submitter": "Kejun Huang", "authors": "Kejun Huang, Yonina C. Eldar, Nicholas D. Sidiropoulos", "title": "Phase Retrieval from 1D Fourier Measurements: Convexity, Uniqueness, and\n  Algorithms", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2016.2601291", "report-no": null, "categories": "math.OC cs.IR cs.IT math.IT math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers phase retrieval from the magnitude of 1D over-sampled\nFourier measurements, a classical problem that has challenged researchers in\nvarious fields of science and engineering. We show that an optimal vector in a\nleast-squares sense can be found by solving a convex problem, thus establishing\na hidden convexity in Fourier phase retrieval. We also show that the standard\nsemidefinite relaxation approach yields the optimal cost function value (albeit\nnot necessarily an optimal solution) in this case. A method is then derived to\nretrieve an optimal minimum phase solution in polynomial time. Using these\nresults, a new measuring technique is proposed which guarantees uniqueness of\nthe solution, along with an efficient algorithm that can solve large-scale\nFourier phase retrieval problems with uniqueness and optimality guarantees.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 18:47:21 GMT"}, {"version": "v2", "created": "Thu, 7 Jul 2016 17:03:54 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Huang", "Kejun", ""], ["Eldar", "Yonina C.", ""], ["Sidiropoulos", "Nicholas D.", ""]]}, {"id": "1603.05265", "submitter": "Yuan Wang", "authors": "Yuan Wang, Kamran Paynabar, Yajun Mei", "title": "Thresholded Multivariate Principal Component Analysis for Multi-channel\n  Profile Monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monitoring multichannel profiles has important applications in manufacturing\nsystems improvement, but it is non-trivial to develop efficient statistical\nmethods due to two main challenges. First, profiles are high-dimensional\nfunctional data with intrinsic inner- and inter-channel correlations, and one\nneeds to develop a dimension reduction method that can deal with such intricate\ncorrelations for the purpose of effective monitoring. The second, and probably\nmore fundamental, challenge is that the functional structure of multi-channel\nprofiles might change over time, and thus the dimension reduction method should\nbe able to automatically take into account the potential unknown change. To\ntackle these two challenges, we propose a novel thresholded multivariate\nprincipal component analysis (PCA) method for multi-channel profile monitoring.\nOur proposed method consists of two steps of dimension reduction: It first\napplies the functional PCA to extract a reasonable large number of features\nunder the normal operational (in-control) state, and then use the\nsoft-thresholding techniques to further select significant features capturing\nprofile information in the out-of-control state. The choice of tuning parameter\nfor soft-thresholding is provided based on asymptotic analysis, and extensive\nsimulation studies are conducted to illustrate the efficacy of our proposed\nthresholded PCA methodology.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 20:30:30 GMT"}], "update_date": "2016-03-18", "authors_parsed": [["Wang", "Yuan", ""], ["Paynabar", "Kamran", ""], ["Mei", "Yajun", ""]]}, {"id": "1603.05294", "submitter": "Ekaterina Sorokina", "authors": "Ekaterina Sorokina", "title": "Modeling and Estimation of the Risk When Choosing a Provider", "comments": "4 pages, 1 figures, 1 table, 14 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper provides an algorithm for the risk estimation when a company\nselects an outsourcing service provider for innovation product. Calculations\nare based on expert surveys conducted among customers and among providers of\noutsourcing. The surveys assessed the degree of materiality of species at risk.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 21:57:56 GMT"}], "update_date": "2016-03-18", "authors_parsed": [["Sorokina", "Ekaterina", ""]]}, {"id": "1603.05297", "submitter": "James Balamuta", "authors": "James Balamuta, Stephane Guerrier, Roberto Molinari and Wenchao Yang", "title": "A Computationally Efficient Framework for Automatic Inertial Sensor\n  Calibration", "comments": "20 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The calibration of (low-cost) inertial sensors has become increasingly\nimportant over the past years since their use has grown exponentially in many\napplications going from unmanned aerial vehicle navigation to 3D-animation.\nHowever, this calibration procedure is often quite problematic since the\nsignals issued from these sensors have a complex spectral structure and the\nmethods available to estimate the parameters of these models are either\nunstable, computationally intensive and/or statistically inconsistent. This\npaper presents a new software platform for inertial sensor calibration based on\nthe Generalized Method of Wavelet Moments which provides a computationally\nefficient, flexible, user-friendly and statistically sound tool to estimate and\nselect from a wide range of complex models. The software is developed within\nthe open-source statistical software R and is based on C++ language allowing it\nto achieve high computational performance.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 22:09:54 GMT"}, {"version": "v2", "created": "Thu, 21 Jul 2016 02:02:18 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Balamuta", "James", ""], ["Guerrier", "Stephane", ""], ["Molinari", "Roberto", ""], ["Yang", "Wenchao", ""]]}, {"id": "1603.05324", "submitter": "Shiwen Zhao", "authors": "Shiwen Zhao and Barbara E. Engelhardt and Sayan Mukherjee and David B.\n  Dunson", "title": "Fast moment estimation for generalized latent Dirichlet models", "comments": "corrected a typo in figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a generalized method of moments (GMM) approach for fast parameter\nestimation in a new class of Dirichlet latent variable models with mixed data\ntypes. Parameter estimation via GMM has been demonstrated to have computational\nand statistical advantages over alternative methods, such as expectation\nmaximization, variational inference, and Markov chain Monte Carlo. The key\ncomputational advan- tage of our method (MELD) is that parameter estimation\ndoes not require instantiation of the latent variables. Moreover, a\nrepresentational advantage of the GMM approach is that the behavior of the\nmodel is agnostic to distributional assumptions of the observations. We derive\npopulation moment conditions after marginalizing out the sample-specific\nDirichlet latent variables. The moment conditions only depend on component mean\nparameters. We illustrate the utility of our approach on simulated data,\ncomparing results from MELD to alternative methods, and we show the promise of\nour approach through the application of MELD to several data sets.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 00:36:39 GMT"}, {"version": "v2", "created": "Wed, 23 Mar 2016 18:12:35 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Zhao", "Shiwen", ""], ["Engelhardt", "Barbara E.", ""], ["Mukherjee", "Sayan", ""], ["Dunson", "David B.", ""]]}, {"id": "1603.05334", "submitter": "Edgar Dobriban", "authors": "Edgar Dobriban", "title": "Weighted mining of massive collections of $p$-values by convex\n  optimization", "comments": "This is an entirely rewritten version of the paper. The title of the\n  paper, the name of the method, and the introduction have been changed, with\n  the goal of making the paper more accessible and appealing to practitioners.\n  New sections on monotone likelihood ratio families and two-sided tests have\n  been added, which expand the scope of the method", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers in data-rich disciplines---think of computational genomics and\nobservational cosmology---often wish to mine large bodies of $p$-values looking\nfor significant effects, while controlling the false discovery rate or\nfamily-wise error rate. Increasingly, researchers also wish to prioritize\ncertain hypotheses, for example those thought to have larger effect sizes, by\nupweighting, and to impose constraints on the underlying mining, such as\nmonotonicity along a certain sequence.\n  We introduce Princessp, a principled method for performing weighted multiple\ntesting by constrained convex optimization. Our method elegantly allows one to\nprioritize certain hypotheses through upweighting and to discount others\nthrough downweighting, while constraining the underlying weights involved in\nthe mining process. When the $p$-values derive from monotone likelihood ratio\nfamilies like the Gaussian means model, the new method allows exact solution of\nan important optimal weighting problem previously thought to be nonconvex and\ncomputationally infeasible. Our method scales to massive dataset sizes.\n  We illustrate the applications of Princessp on a series of standard genomics\ndatasets and offer comparisons with several previous `standard' methods.\nPrincessp offers both ease of operation and the ability to scale to extremely\nlarge problem sizes. The method is available as open-source software from\nhttp://github.com/dobriban/pvalue_weighting_matlab .\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 02:06:02 GMT"}, {"version": "v2", "created": "Fri, 16 Jun 2017 20:27:56 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Dobriban", "Edgar", ""]]}, {"id": "1603.05341", "submitter": "Paramita Saha-Chauchuri", "authors": "Paramita Saha-Chaudhuri", "title": "Covariate Microaggregation for Logistic Regression: An Application for\n  Analysis of Confidential Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent past, electronic health records and distributed data networks\nemerged as a viable resource for medical and scientific research. As the use of\nconfidential patient information from such sources become more common,\nmaintaining privacy of patients is of utmost importance. For a binary disease\noutcome of interest, we show that the techniques of microaggregation\n(equivalent to specimen pooling) and \\underline{Po}oled \\underline{Lo}gistic\n\\underline{R}egression (PoLoR) could be applied for analysis of large and/or\ndistributed data while respecting patient privacy. PoLoR is exactly the same as\nstandard logistic regression, but instead of using individual covariate level,\nthe analysis uses microaggregated covariate level when microaggregation is\nconditional on the outcome status. Aggregate levels of covariates can be passed\nfrom the nodes of the network to the analysis center without revealing\nindividual-level microdata and can be used very easily with standard softwares\nfor estimation of disease odds ratio associated with a set of categorical or\ncontinuous covariates. Microaggregation of covariates allows for consistent\nestimation of the parameters of logistic regression model that can include\nconfounders and transformation of exposure. Additionally, since the microdata\ncan be accessed within nodes, effect modifiers can be accommodated and\nconsistently estimated. For analysis of confidential health data, covariate\nmicroaggregation for logistic regression will provide a practical and\nstraightforward alternative to more complicated existing options.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 02:48:08 GMT"}], "update_date": "2016-03-18", "authors_parsed": [["Saha-Chaudhuri", "Paramita", ""]]}, {"id": "1603.05522", "submitter": "Sumeetpal S Singh", "authors": "Lan Jiang, Sumeetpal S. Singh", "title": "Tracking multiple moving objects in images using Markov Chain Monte\n  Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new Bayesian state and parameter learning algorithm for multiple target\ntracking (MTT) models with image observations is proposed. Specifically, a\nMarkov chain Monte Carlo algorithm is designed to sample from the posterior\ndistribution of the unknown number of targets, their birth and death times,\nstates and model parameters, which constitutes the complete solution to the\ntracking problem. The conventional approach is to pre-process the images to\nextract point observations and then perform tracking. We model the image\ngeneration process directly to avoid potential loss of information when\nextracting point observations. Numerical examples show that our algorithm has\nimproved tracking performance over commonly used techniques, for both synthetic\nexamples and real florescent microscopy data, especially in the case of dim\ntargets with overlapping illuminated regions.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 15:03:10 GMT"}], "update_date": "2016-03-18", "authors_parsed": [["Jiang", "Lan", ""], ["Singh", "Sumeetpal S.", ""]]}, {"id": "1603.05583", "submitter": "L\\'aszl\\'o Gyarmati", "authors": "Laszlo Gyarmati, Mohamed Hefeeda", "title": "Analyzing In-Game Movements of Soccer Players at Scale", "comments": "MIT Sloan Sports Analytics Conference 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is challenging to get access to datasets related to the physical\nperformance of soccer players. The teams consider such information highly\nconfidential, especially if it covers in-game performance.Hence, most of the\nanalysis and evaluation of the players' performance do not contain much\ninformation on the physical aspect of the game, creating a blindspot in\nperformance analysis. We propose a novel method to solve this issue by deriving\nmovement characteristics of soccer players. We use event-based datasets from\ndata provider companies covering 50+ soccer leagues allowing us to analyze the\nmovement profiles of potentially tens of thousands of players without any major\ninvestment. Our methodology does not require expensive, dedicated player\ntracking system deployed in the stadium. We also compute the similarity of the\nplayers based on their movement characteristics and as such identify potential\ncandidates who may be able to replace a given player. Finally, we quantify the\nuniqueness and consistency of players in terms of their in-game movements. Our\nstudy is the first of its kind that focuses on the movements of soccer players\nat scale, while it derives novel, actionable insights for the soccer industry\nfrom event-based datasets.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 23:54:55 GMT"}], "update_date": "2016-03-18", "authors_parsed": [["Gyarmati", "Laszlo", ""], ["Hefeeda", "Mohamed", ""]]}, {"id": "1603.05766", "submitter": "Gordon Smyth", "authors": "Belinda Phipson and Gordon K. Smyth", "title": "Permutation p-values should never be zero: calculating exact p-values\n  when permutations are randomly drawn", "comments": "12 pages, 2 figures", "journal-ref": "Stat. Appl. Genet. Molec. Biol., Volume 9 (2010), Issue 1, Article\n  39", "doi": "10.2202/1544-6115.1585", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Permutation tests are amongst the most commonly used statistical tools in\nmodern genomic research, a process by which p-values are attached to a test\nstatistic by randomly permuting the sample or gene labels. Yet permutation\np-values published in the genomic literature are often computed incorrectly,\nunderstated by about 1/m, where m is the number of permutations. The same is\noften true in the more general situation when Monte Carlo simulation is used to\nassign p-values. Although the p-value understatement is usually small in\nabsolute terms, the implications can be serious in a multiple testing context.\nThe understatement arises from the intuitive but mistaken idea of using\npermutation to estimate the tail probability of the test statistic. We argue\ninstead that permutation should be viewed as generating an exact discrete null\ndistribution. The relevant literature, some of which is likely to have been\nrelatively inaccessible to the genomic community, is reviewed and summarized. A\ncomputation strategy is developed for exact p-values when permutations are\nrandomly drawn. The strategy is valid for any number of permutations and\nsamples. Some simple recommendations are made for the implementation of\npermutation tests in practice.\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2016 04:55:55 GMT"}], "update_date": "2016-03-21", "authors_parsed": [["Phipson", "Belinda", ""], ["Smyth", "Gordon K.", ""]]}, {"id": "1603.05770", "submitter": "Wei Xiao", "authors": "Wei Xiao", "title": "A Probabilistic Machine Learning Approach to Detect Industrial Plant\n  Faults", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fault detection in industrial plants is a hot research area as more and more\nsensor data are being collected throughout the industrial process. Automatic\ndata-driven approaches are widely needed and seen as a promising area of\ninvestment. This paper proposes an effective machine learning algorithm to\npredict industrial plant faults based on classification methods such as\npenalized logistic regression, random forest and gradient boosted tree. A\nfault's start time and end time are predicted sequentially in two steps by\nformulating the original prediction problems as classification problems. The\nalgorithms described in this paper won first place in the Prognostics and\nHealth Management Society 2015 Data Challenge.\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2016 05:31:12 GMT"}], "update_date": "2016-03-21", "authors_parsed": [["Xiao", "Wei", ""]]}, {"id": "1603.05875", "submitter": "Salehe Erfanian Ebadi", "authors": "Salehe Erfanian Ebadi, Valia Guerra Ones, Ebroul Izquierdo", "title": "Approximated Robust Principal Component Analysis for Improved General\n  Scene Background Subtraction", "comments": "arXiv admin note: text overlap with arXiv:1511.01245 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The research reported in this paper addresses the fundamental task of\nseparation of locally moving or deforming image areas from a static or globally\nmoving background. It builds on the latest developments in the field of robust\nprincipal component analysis, specifically, the recently reported practical\nsolutions for the long-standing problem of recovering the low-rank and sparse\nparts of a large matrix made up of the sum of these two components. This\narticle addresses a few critical issues including: embedding global motion\nparameters in the matrix decomposition model, i.e., estimation of global motion\nparameters simultaneously with the foreground/background separation task,\nconsidering matrix block-sparsity rather than generic matrix sparsity as\nnatural feature in video processing applications, attenuating background\nghosting effects when foreground is subtracted, and more critically providing\nan extremely efficient algorithm to solve the low-rank/sparse matrix\ndecomposition task. The first aspect is important for background/foreground\nseparation in generic video sequences where the background usually obeys global\ndisplacements originated by the camera motion in the capturing process. The\nsecond aspect exploits the fact that in video processing applications the\nsparse matrix has a very particular structure, where the non-zero matrix\nentries are not randomly distributed but they build small blocks within the\nsparse matrix. The next feature of the proposed approach addresses removal of\nghosting effects originated from foreground silhouettes and the lack of\ninformation in the occluded background regions of the image. Finally, the\nproposed model also tackles algorithmic complexity by introducing an extremely\nefficient \"SVD-free\" technique that can be applied in most\nbackground/foreground separation tasks for conventional video processing.\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2016 13:53:26 GMT"}], "update_date": "2016-03-21", "authors_parsed": [["Ebadi", "Salehe Erfanian", ""], ["Ones", "Valia Guerra", ""], ["Izquierdo", "Ebroul", ""]]}, {"id": "1603.05882", "submitter": "Carel F.W. Peeters", "authors": "Carel F.W. Peeters", "title": "Bayesian Constrained-Model Selection for Factor Analytic Modeling", "comments": "8 pages, 3 figures; Preprint based on the first chapter of my\n  unpublished PhD dissertation. Published version can be retrieved from URL:\n  http://www.apadivisions.org/division-5/publications/score/2016/04/index.aspx,\n  The Score, April 2016 Issue", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  My dissertation revolves around Bayesian approaches towards constrained\nstatistical inference in the factor analysis (FA) model. Two interconnected\ntypes of restricted-model selection are considered. These types have a natural\nconnection to selection problems in the exploratory FA (EFA) and confirmatory\nFA (CFA) model and are termed Type I and Type II model selection. Type I\nconstrained-model selection is taken to mean the determination of the\nappropriate dimensionality of a model. This type of constrained-model selection\nconnects with EFA in the sense of selecting the optimal dimensionality of the\nlatent vector. Type II model selection is taken to mean the determination of\nappropriate inequality, order or shape restrictions on the parameter space. The\ndissertation connects Type II constrained-model selection to CFA by focusing on\nthe determination of linear inequality constraints as expressions of the\ndirection and (relative) strength of factor loadings. The figures accompanying\nthis article are taken from the slides of my Division 5 Awards Symposium\nInvited address at the APA 2015 Annual Convention in Toronto. These slides can\nbe retrieved from \\url{https://github.com/CFWP/ConventionTalk}.\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2016 14:12:19 GMT"}, {"version": "v2", "created": "Tue, 12 Apr 2016 09:05:28 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Peeters", "Carel F. W.", ""]]}, {"id": "1603.05914", "submitter": "Giulio Cimini", "authors": "Stanislao Gualdi, Giulio Cimini, Kevin Primicerio, Riccardo Di\n  Clemente, Damien Challet", "title": "Statistically validated network of portfolio overlaps and systemic risk", "comments": null, "journal-ref": "Scientific Reports 6, 39467 (2016)", "doi": "10.1038/srep39467", "report-no": null, "categories": "q-fin.RM physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common asset holding by financial institutions, namely portfolio overlap, is\nnowadays regarded as an important channel for financial contagion with the\npotential to trigger fire sales and thus severe losses at the systemic level.\nIn this paper we propose a method to assess the statistical significance of the\noverlap between pairs of heterogeneously diversified portfolios, which then\nallows us to build a validated network of financial institutions where links\nindicate potential contagion channels due to realized portfolio overlaps. The\nmethod is implemented on a historical database of institutional holdings\nranging from 1999 to the end of 2013, but can be in general applied to any\nbipartite network where the presence of similar sets of neighbors is of\ninterest. We find that the proportion of validated network links (i.e., of\nstatistically significant overlaps) increased steadily before the 2007-2008\nglobal financial crisis and reached a maximum when the crisis occurred. We\nargue that the nature of this measure implies that systemic risk from fire\nsales liquidation was maximal at that time. After a sharp drop in 2008,\nsystemic risk resumed its growth in 2009, with a notable acceleration in 2013,\nreaching levels not seen since 2007. We finally show that market trends tend to\nbe amplified in the portfolios identified by the algorithm, such that it is\npossible to have an informative signal about financial institutions that are\nabout to suffer (enjoy) the most significant losses (gains).\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2016 16:47:21 GMT"}, {"version": "v2", "created": "Tue, 27 Sep 2016 13:16:39 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Gualdi", "Stanislao", ""], ["Cimini", "Giulio", ""], ["Primicerio", "Kevin", ""], ["Di Clemente", "Riccardo", ""], ["Challet", "Damien", ""]]}, {"id": "1603.05915", "submitter": "Jingyi Jessica Li", "authors": "Wei Vivian Li, Anqi Zhao, Shihua Zhang, Jingyi Jessica Li", "title": "MSIQ: Joint Modeling of Multiple RNA-seq Samples for Accurate Isoform\n  Quantification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.GN q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Next-generation RNA sequencing (RNA-seq) technology has been widely used to\nassess full-length RNA isoform abundance in a high-throughput manner. RNA-seq\ndata offer insight into gene expression levels and transcriptome structures,\nenabling us to better understand the regulation of gene expression and\nfundamental biological processes. Accurate isoform quantification from RNA-seq\ndata is challenging due to the information loss in sequencing experiments. A\nrecent accumulation of multiple RNA-seq data sets from the same tissue or cell\ntype provides new opportunities to improve the accuracy of isoform\nquantification. However, existing statistical or computational methods for\nmultiple RNA-seq samples either pool the samples into one sample or assign\nequal weights to the samples when estimating isoform abundance. These methods\nignore the possible heterogeneity in the quality of different samples and could\nresult in biased and unrobust estimates. In this article, we develop a method,\nwhich we call \"joint modeling of multiple RNA-seq samples for accurate isoform\nquantification\" (MSIQ), for more accurate and robust isoform quantification by\nintegrating multiple RNA-seq samples under a Bayesian framework. Our method\naims to (1) identify a consistent group of samples with homogeneous quality and\n(2) improve isoform quantification accuracy by jointly modeling multiple\nRNA-seq samples by allowing for higher weights on the consistent group. We show\nthat MSIQ provides a consistent estimator of isoform abundance, and we\ndemonstrate the accuracy and effectiveness of MSIQ compared with alternative\nmethods through simulation studies on D. melanogaster genes. We justify MSIQ's\nadvantages over existing approaches via application studies on real RNA-seq\ndata from human embryonic stem cells, brain tissues, and the HepG2 immortalized\ncell line.\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2016 16:48:43 GMT"}, {"version": "v2", "created": "Mon, 21 Aug 2017 22:49:59 GMT"}, {"version": "v3", "created": "Sat, 2 Dec 2017 23:25:02 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Li", "Wei Vivian", ""], ["Zhao", "Anqi", ""], ["Zhang", "Shihua", ""], ["Li", "Jingyi Jessica", ""]]}, {"id": "1603.05938", "submitter": "Mette Langaas", "authors": "K. K. Halle and {\\O}. Bakke and S. Djurovic and A. Bye and E. Ryeng\n  and U. Wisl{\\o}ff and O. A. Andreassen and M. Langaas", "title": "Efficient and powerful familywise error control in genome-wide\n  association studies using generalized linear models", "comments": null, "journal-ref": null, "doi": "10.1111/sjos.12451", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In genetic association studies, detecting phenotype-genotype association is a\nprimary goal. We assume that the relationship between the data -phenotype,\ngenetic markers and environmental covariates - can be modelled by a generalized\nlinear model (GLM). The inclusion of environmental covariates makes it possible\nto account for important confounding factors, such as sex and population\nsubstructure. A multivariate score statistic, which under the complete null\nhypothesis of no phenotype-genotype association asymptotically has a\nmultivariate normal distribution with a covariance matrix that can be estimated\nfrom the data, is used to test a large number of genetic markers for\nassociation with the phenotype. We stress the importance of controlling the\nfamilywise error rate (FWER), and use the asymptotic distribution of the\nmultivariate score test statistic to find a local significance level for the\nindividual test. Using real data (from one study on schizophrenia and bipolar\ndisorder and one on maximal oxygen uptake) and constructed correlated\nstructures, we show that our method is a powerful alternative to the popular\nBonferroni and Sidak methods. For GLMs without environmental covariates, we\nshow that our method is an efficient alternative to permutation methods for\nmultiple testing. Further, we show that if environmental covariates and genetic\nmarkers are uncorrelated, the estimated covariance matrix of the score test\nstatistic can be approximated by the estimated correlation matrix for just the\ngenetic markers. As byproducts of our method, an effective number of\nindependent tests can be defined, and FWER-adjusted $p$-values can be\ncalculated as an alternative to using a local significance level.\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2016 17:51:37 GMT"}, {"version": "v2", "created": "Thu, 22 Dec 2016 12:09:23 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Halle", "K. K.", ""], ["Bakke", "\u00d8.", ""], ["Djurovic", "S.", ""], ["Bye", "A.", ""], ["Ryeng", "E.", ""], ["Wisl\u00f8ff", "U.", ""], ["Andreassen", "O. A.", ""], ["Langaas", "M.", ""]]}, {"id": "1603.06230", "submitter": "Josh Merel", "authors": "Josh Merel, Ben Shababo, Alex Naka, Hillel Adesnik, Liam Paninski", "title": "Bayesian methods for event analysis of intracellular currents", "comments": null, "journal-ref": null, "doi": "10.1016/j.jneumeth.2016.05.015", "report-no": null, "categories": "q-bio.QM q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Investigation of neural circuit functioning often requires statistical\ninterpretation of events in subthreshold electrophysiological recordings. This\nproblem is non-trivial because recordings may have moderate levels of\nstructured noise and events may have distinct kinetics. In addition, novel\nexperimental designs that combine optical and electrophysiological methods will\ndepend upon statistical tools that combine multimodal data. We present a\nBayesian approach for inferring the timing, strength, and kinetics of\npostsynaptic currents (PSCs) from voltage-clamp recordings on a per event\nbasis. The simple generative model for a single voltage-clamp recording\nflexibly extends to include network-level structure to enable experiments\ndesigned to probe synaptic connectivity. We validate the approach on simulated\nand real data. We also demonstrate that extensions of the basic PSC detection\nalgorithm can handle recordings contaminated with optically evoked currents,\nand we simulate a scenario in which calcium imaging observations, available for\na subset of neurons, can be fused with electrophysiological data to achieve\nhigher temporal resolution. We apply this approach to simulated and real ground\ntruth data to demonstrate its higher sensitivity in detecting small\nsignal-to-noise events and its increased robustness to noise compared to\nstandard methods for detecting PSCs. The new Bayesian event analysis approach\nfor electrophysiological recordings should allow for better estimation of\nphysiological parameters under more variable conditions and help support new\nexperimental designs for circuit mapping.\n", "versions": [{"version": "v1", "created": "Sun, 20 Mar 2016 15:44:40 GMT"}, {"version": "v2", "created": "Wed, 18 May 2016 16:35:52 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["Merel", "Josh", ""], ["Shababo", "Ben", ""], ["Naka", "Alex", ""], ["Adesnik", "Hillel", ""], ["Paninski", "Liam", ""]]}, {"id": "1603.06455", "submitter": "Roza Maghsood", "authors": "Roza Maghsood and Jonas Wallin", "title": "Online estimation of driving events and fatigue damage on vehicles", "comments": "23 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driving events, such as maneuvers at slow speed and turns, are important for\ndurability assessments of vehicle components. By counting the number of driving\nevents, one can estimate the fatigue damage caused by the same kind of events.\nThrough knowledge of the distribution of driving events for a group of\ncustomers, the vehicles producers can tailor the design, of vehicles, for the\ngroup. In this article, we propose an algorithm that can be applied on-board a\nvehicle to online estimate the expected number of driving events occurring, and\nthus be used to estimate the distribution of driving events for a certain group\nof customers. Since the driving events are not observed directly, the algorithm\nuses a hidden Markov model to extract the events. The parameters of the HMM are\nestimated using an online EM algorithm. The introduction of the online EM is\ncrucial for practical usage, on-board vehicles, due to that its complexity of\nan iteration is fixed. Typically, the EM algorithm is used to find the, fixed,\nparameters that maximizes the likelihood. By introducing a fixed forgetting\nfactor in the online EM, an adaptive algorithm is acquired. This is important\nin practice since the driving conditions changes over time and a single trip\ncan contain different road types such as city and highway, making the\nassumption of fixed parameters unrealistic. Finally, we also derive a method to\nonline compute the expected damage.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 15:28:54 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Maghsood", "Roza", ""], ["Wallin", "Jonas", ""]]}, {"id": "1603.06476", "submitter": "Jue Wang", "authors": "Jue Wang, Sheng Luo, Liang Li", "title": "Dynamic Prediction for Multiple Repeated Measures and Event Time Data:\n  An Application to Parkinson's Disease", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many clinical trials studying neurodegenerative diseases such as\nParkinson's disease (PD), multiple longitudinal outcomes are collected to fully\nexplore the multidimensional impairment caused by this disease. If the outcomes\ndeteriorate rapidly, patients may reach a level of functional disability\nsufficient to initiate levodopa therapy for ameliorating disease symptoms. An\naccurate prediction of the time to functional disability is helpful for\nclinicians to monitor patients' disease progression and make informative\nmedical decisions. In this article, we first propose a joint model that\nconsists of a semiparametric multilevel latent trait model (MLLTM) for the\nmultiple longitudinal outcomes, and a survival model for event time. The two\nsubmodels are linked together by an underlying latent variable. We develop a\nBayesian approach for parameter estimation and a dynamic prediction framework\nfor predicting target patients' future outcome trajectories and risk of a\nsurvival event, based on their multivariate longitudinal measurements. Our\nproposed model is evaluated by simulation studies and is applied to the DATATOP\nstudy, a motivating clinical trial assessing the effect of deprenyl among\npatients with early PD.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 16:00:59 GMT"}, {"version": "v2", "created": "Wed, 17 May 2017 04:43:43 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Wang", "Jue", ""], ["Luo", "Sheng", ""], ["Li", "Liang", ""]]}, {"id": "1603.06649", "submitter": "Faranak Golestaneh", "authors": "Faranak Golestaneh, Pierre Pinson, Hoay Beng Gooi", "title": "Generation and Evaluation of Space-Time Trajectories of Photovoltaic\n  Power", "comments": "33 pages, 11 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the probabilistic energy forecasting literature, emphasis is mainly placed\non deriving marginal predictive densities for which each random variable is\ndealt with individually. Such marginals description is sufficient for power\nsystems related operational problems if and only if optimal decisions are to be\nmade for each lead-time and each location independently of each other. However,\nmany of these operational processes are temporally and spatially coupled, while\nuncertainty in photovoltaic (PV) generation is strongly dependent in time and\nin space. This issue is addressed here by analysing and capturing\nspatio-temporal dependencies in PV generation. Multivariate predictive\ndistributions are modelled and space-time trajectories describing the potential\nevolution of forecast errors through successive lead-times and locations are\ngenerated. Discrimination ability of the relevant scoring rules on performance\nassessment of space-time trajectories of PV generation is also studied.\nFinally, the advantage of taking into account space-time correlations over\nprobabilistic and point forecasts is investigated. The empirical investigation\nis based on the solar PV dataset of the Global Energy Forecasting Competition\n(GEFCom) 2014.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 00:11:32 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Golestaneh", "Faranak", ""], ["Pinson", "Pierre", ""], ["Gooi", "Hoay Beng", ""]]}, {"id": "1603.06790", "submitter": "Giovanni Montana", "authors": "A.W. Chung and M.D. Schirmer and M.L. Krishna and G. Ball and P.\n  Aljabar and A.D. Edwards and G. Montana", "title": "Characterising brain network topologies: a dynamic analysis approach\n  using heat kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network theory provides a principled abstraction of the human brain: reducing\na complex system into a simpler representation from which to investigate brain\norganisation. Recent advancement in the neuroimaging field are towards\nrepresenting brain connectivity as a dynamic process in order to gain a deeper\nunderstanding of how the brain is organised for information transport. In this\npaper we propose a network modelling approach based on the heat kernel to\ncapture the process of heat diffusion in complex networks. By applying the heat\nkernel to structural brain networks, we define new features which quantify\nchange in energy flow. Identifying suitable features which can classify\nnetworks between cohorts is useful towards understanding the effect of disease\non brain architecture. We demonstrate the discriminative power of heat kernel\nfeatures in both synthetic and clinical preterm data. By generating an\nextensive range of synthetic networks with varying density and randomisation,\nwe investigate how heat flows in the networks in relation to changes in network\ntopology. We demonstrate that our proposed features provide a metric of network\nefficiency and may be indicative of organisational principles commonly\nassociated with, for example, small-world architecture. In addition, we show\nthe potential of these features to characterise and classify between network\ntopologies. We further demonstrate our methodology in a clinical setting by\napplying it to a large cohort of preterm babies scanned at term equivalent age\nfrom which diffusion networks were computed. We show that our heat kernel\nfeatures are able to successfully predict motor function measured at two years\nof age (sensitivity, specificity, F-score, accuracy = 75.0, 82.5, 78.6, 82.3%,\nrespectively.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 13:53:00 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Chung", "A. W.", ""], ["Schirmer", "M. D.", ""], ["Krishna", "M. L.", ""], ["Ball", "G.", ""], ["Aljabar", "P.", ""], ["Edwards", "A. D.", ""], ["Montana", "G.", ""]]}, {"id": "1603.07211", "submitter": "Shuo Chen", "authors": "Shuo Chen, F. DuBois Bowman, and Yishi Xing", "title": "Differentially Expressed Functional Connectivity Networks with K-partite\n  Graph Topology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emerging brain network studies suggest that interactions between various\ndistributed neuronal populations may be characterized by an organized complex\ntopological structure. Many brain diseases are associated with altered\ntopological patterns of brain connectivity. Therefore, a key inquiry of\nconnectivity analysis is to identify network-level differentially expressed\nconnections that have low false positive rates, sufficient statistical power,\nand high reproducibility. In this paper, we propose a novel statistical\napproach to fulfill this goal by leveraging the topological structure of\ndifferentially expressed functional connections or edges in a graphical\nrepresentation. We propose a new algorithm to automatically detect the latent\ntopology of a k-partite graph structure, and we also provide statistical\ninferential techniques to test the detected topology. We evaluate our new\nmethods via extensive numerical studies. We also apply our new approach to\nresting state fMRI data (24 cases and 18 controls) for Parkinson's disease\nresearch. The detected connectivity network biomaker with the k-partite graph\ntopological structure reveals underlying neural features distinguishing\nParkinson's disease patients from healthy control subjects.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 14:54:56 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Chen", "Shuo", ""], ["Bowman", "F. DuBois", ""], ["Xing", "Yishi", ""]]}, {"id": "1603.07237", "submitter": "Coralie Merle", "authors": "Coralie Merle (IMAG, CBGP, IBC), Rapha\\\"el Leblois (CBGP, IBC),\n  Fran\\c{c}ois Rousset (ISEM, IBC), Pierre Pudlo (I2M, IBC)", "title": "Resampling: an improvement of Importance Sampling in varying population\n  size models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.AP stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential importance sampling algorithms have been defined to estimate\nlikelihoods in models of ancestral population processes. However, these\nalgorithms are based on features of the models with constant population size,\nand become inefficient when the population size varies in time, making\nlikelihood-based inferences difficult in many demographic situations. In this\nwork, we modify a previous sequential importance sampling algorithm to improve\nthe efficiency of the likelihood estimation. Our procedure is still based on\nfeatures of the model with constant size, but uses a resampling technique with\na new resampling probability distribution depending on the pairwise composite\nlikelihood. We tested our algorithm, called sequential importance sampling with\nresampling (SISR) on simulated data sets under different demographic cases. In\nmost cases, we divided the computational cost by two for the same accuracy of\ninference, in some cases even by one hundred. This study provides the first\nassessment of the impact of such resampling techniques on parameter inference\nusing sequential importance sampling, and extends the range of situations where\nlikelihood inferences can be easily performed.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 15:31:05 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Merle", "Coralie", "", "IMAG, CBGP, IBC"], ["Leblois", "Rapha\u00ebl", "", "CBGP, IBC"], ["Rousset", "Fran\u00e7ois", "", "ISEM, IBC"], ["Pudlo", "Pierre", "", "I2M, IBC"]]}, {"id": "1603.07279", "submitter": "Olivier Delestre", "authors": "M Abily (I-CiTy), N. Bertrand (IRSN), O Delestre (MAPMO,IJLRA,JAD), P\n  Gourbesville (I-CiTy), C.-M. Duluc (IRSN)", "title": "Spatial Global Sensitivity Analysis of High Resolution classified\n  topographic data use in 2D urban flood modelling", "comments": null, "journal-ref": "Environmental Modelling and Software, Elsevier, 2016, 77,\n  pp.183-195", "doi": "10.1016/j.envsoft.2015.12.002", "report-no": null, "categories": "stat.AP math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a spatial Global Sensitivity Analysis (GSA) approach in a\n2D shallow water equations based High Resolution (HR) flood model. The aim of a\nspatial GSA is to produce sensitivity maps which are based on Sobol index\nestimations. Such an approach allows to rank the effects of uncertain HR\ntopographic data input parameters on flood model output. The influence of the\nthree following parameters has been studied: the measurement error, the level\nof details of above-ground elements representation and the spatial\ndiscretization resolution. To introduce uncertainty, a Probability Density\nFunction and discrete spatial approach have been applied to generate 2, 000\nDEMs. Based on a 2D urban flood river event modelling, the produced sensitivity\nmaps highlight the major influence of modeller choices compared to HR\nmeasurement errors when HR topographic data are used, and the spatial\nvariability of the ranking. Highlights $\\bullet$ Spatial GSA allowed the\nproduction of Sobol index maps, enhancing the relative weight of each uncertain\nparameter on the variability of calculated output parameter of interest. 1\n$\\bullet$ The Sobol index maps illustrate the major influence of the modeller\nchoices, when using the HR topographic data in 2D hydraulic models with respect\nto the influence of HR dataset accuracy. $\\bullet$ Added value is for modeller\nto better understand limits of his model. $\\bullet$ Requirements and limits for\nthis approach are related to subjectivity of choices and to computational cost.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 17:27:08 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Abily", "M", "", "I-CiTy"], ["Bertrand", "N.", "", "IRSN"], ["Delestre", "O", "", "MAPMO,IJLRA,JAD"], ["Gourbesville", "P", "", "I-CiTy"], ["Duluc", "C. -M.", "", "IRSN"]]}, {"id": "1603.07409", "submitter": "Andrew Finley Dr.", "authors": "Andrew O. Finley, Sudipto Banerjee, Yuzhen Zhou, Bruce D. Cook, Chad\n  Babcock", "title": "Joint hierarchical models for sparsely sampled high-dimensional LiDAR\n  and forest variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancements in remote sensing technology, specifically Light\nDetection and Ranging (LiDAR) sensors, provide the data needed to quantify\nforest characteristics at a fine spatial resolution over large geographic\ndomains. From an inferential standpoint, there is interest in prediction and\ninterpolation of the often sparsely sampled and spatially misaligned LiDAR\nsignals and forest variables. We propose a fully process-based Bayesian\nhierarchical model for above ground biomass (AGB) and LiDAR signals. The\nprocess-based framework offers richness in inferential capabilities, e.g.,\ninference on the entire underlying processes instead of estimates only at\npre-specified points. Key challenges we obviate include misalignment between\nthe AGB observations and LiDAR signals and the high-dimensionality in the model\nemerging from LiDAR signals in conjunction with the large number of spatial\nlocations. We offer simulation experiments to evaluate our proposed models and\nalso apply them to a challenging dataset comprising LiDAR and spatially\ncoinciding forest inventory variables collected on the Penobscot Experimental\nForest (PEF), Maine. Our key substantive contributions include AGB data\nproducts with associated measures of uncertainty for the PEF and, more broadly,\na methodology that should find use in a variety of current and upcoming forest\nvariable mapping efforts using sparsely sampled remotely sensed\nhigh-dimensional data.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 01:42:19 GMT"}, {"version": "v2", "created": "Mon, 5 Dec 2016 21:40:47 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Finley", "Andrew O.", ""], ["Banerjee", "Sudipto", ""], ["Zhou", "Yuzhen", ""], ["Cook", "Bruce D.", ""], ["Babcock", "Chad", ""]]}, {"id": "1603.07432", "submitter": "Shouhuai Xu", "authors": "Zhenxin Zhan and Maochao Xu and Shouhuai Xu", "title": "Predicting Cyber Attack Rates with Extreme Values", "comments": null, "journal-ref": "IEEE Transactions on Information Forensics & Security, 10(8):\n  1666-1677 (2015)", "doi": null, "report-no": null, "categories": "cs.CR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is important to understand to what extent, and in what perspectives, cyber\nattacks can be predicted. Despite its evident importance, this problem was not\ninvestigated until very recently, when we proposed using the innovative\nmethodology of {\\em gray-box prediction}. This methodology advocates the use of\ngray-box models, which accommodate the statistical properties/phenomena\nexhibited by the data. Specifically, we showed that gray-box models that\naccommodate the Long-Range Dependence (LRD) phenomenon can predict the attack\nrate (i.e., the number of attacks per unit time) 1-hour ahead-of-time with an\naccuracy of 70.2-82.1\\%. To the best of our knowledge, this is the first result\nshowing the feasibility of prediction in this domain. We observe that the\nprediction errors are partly caused by the models' incapability in predicting\nthe large attack rates, which are called {\\em extreme values} in statistics.\nThis motivates us to analyze the {\\em extreme-value phenomenon}, by using two\ncomplementary approaches: the Extreme Value Theory (EVT) and the Time Series\nTheory (TST). In this paper, we show that EVT can offer long-term predictions\n(e.g., 24-hour ahead-of-time), while gray-box TST models can predict attack\nrates 1-hour ahead-of-time with an accuracy of 86.0-87.9\\%. We explore\nconnections between the two approaches, and point out future research\ndirections. Although our prediction study is based on specific cyber attack\ndata, our methodology can be equally applied to analyze any cyber attack data\nof its kind.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 04:25:12 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["Zhan", "Zhenxin", ""], ["Xu", "Maochao", ""], ["Xu", "Shouhuai", ""]]}, {"id": "1603.07433", "submitter": "Shouhuai Xu", "authors": "Zhenxin Zhan and Maochao Xu and Shouhuai Xu", "title": "Characterizing Honeypot-Captured Cyber Attacks: Statistical Framework\n  and Case Study", "comments": null, "journal-ref": "IEEE Transactions on Information Forensics & Security (IEEE TIFS),\n  8(11): 1775-1789, (2013)", "doi": null, "report-no": null, "categories": "cs.CR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rigorously characterizing the statistical properties of cyber attacks is an\nimportant problem. In this paper, we propose the {\\em first} statistical\nframework for rigorously analyzing honeypot-captured cyber attack data. The\nframework is built on the novel concept of {\\em stochastic cyber attack\nprocess}, a new kind of mathematical objects for describing cyber attacks. To\ndemonstrate use of the framework, we apply it to analyze a low-interaction\nhoneypot dataset, while noting that the framework can be equally applied to\nanalyze high-interaction honeypot data that contains richer information about\nthe attacks. The case study finds, for the first time, that Long-Range\nDependence (LRD) is exhibited by honeypot-captured cyber attacks. The case\nstudy confirms that by exploiting the statistical properties (LRD in this\ncase), it is feasible to predict cyber attacks (at least in terms of attack\nrate) with good accuracy. This kind of prediction capability would provide\nsufficient early-warning time for defenders to adjust their defense\nconfigurations or resource allocations. The idea of \"gray-box\" (rather than\n\"black-box\") prediction is central to the utility of the statistical framework,\nand represents a significant step towards ultimately understanding (the degree\nof) the {\\em predictability} of cyber attacks.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 04:27:09 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["Zhan", "Zhenxin", ""], ["Xu", "Maochao", ""], ["Xu", "Shouhuai", ""]]}, {"id": "1603.07438", "submitter": "Shouhuai Xu", "authors": "Zhenxin Zhan and Maochao Xu and Shouhuai Xu", "title": "A Characterization of Cybersecurity Posture from Network Telescope Data", "comments": null, "journal-ref": "Proceedings of the 6th International Conference Trusted Systems\n  (INTRUST'2014), pp 105-126", "doi": null, "report-no": null, "categories": "cs.CR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven understanding of cybersecurity posture is an important problem\nthat has not been adequately explored. In this paper, we analyze some real data\ncollected by CAIDA's network telescope during the month of March 2013. We\npropose to formalize the concept of cybersecurity posture from the perspectives\nof three kinds of time series: the number of victims (i.e., telescope IP\naddresses that are attacked), the number of attackers that are observed by the\ntelescope, and the number of attacks that are observed by the telescope.\nCharacterizing cybersecurity posture therefore becomes investigating the\nphenomena and statistical properties exhibited by these time series, and\nexplaining their cybersecurity meanings. For example, we propose the concept of\n{\\em sweep-time}, and show that sweep-time should be modeled by stochastic\nprocess, rather than random variable. We report that the number of attackers\n(and attacks) from a certain country dominates the total number of attackers\n(and attacks) that are observed by the telescope. We also show that\nsubstantially smaller network telescopes might not be as useful as a large\ntelescope.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 04:44:18 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["Zhan", "Zhenxin", ""], ["Xu", "Maochao", ""], ["Xu", "Shouhuai", ""]]}, {"id": "1603.07439", "submitter": "Shouhuai Xu", "authors": "Yu-Zhong Chen and Zi-Gang Huang and Shouhuai Xu and Ying-Cheng Lai", "title": "Spatiotemporal patterns and predictability of cyberattacks", "comments": null, "journal-ref": "PLoS One 10(5): e0124472 (2015)", "doi": "10.1371/journal.pone.0124472", "report-no": null, "categories": "cs.CR physics.data-an physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A relatively unexplored issue in cybersecurity science and engineering is\nwhether there exist intrinsic patterns of cyberattacks. Conventional wisdom\nfavors absence of such patterns due to the overwhelming complexity of the\nmodern cyberspace. Surprisingly, through a detailed analysis of an extensive\ndata set that records the time-dependent frequencies of attacks over a\nrelatively wide range of consecutive IP addresses, we successfully uncover\nintrinsic spatiotemporal patterns underlying cyberattacks, where the term\n\"spatio\" refers to the IP address space. In particular, we focus on analyzing\n{\\em macroscopic} properties of the attack traffic flows and identify two main\npatterns with distinct spatiotemporal characteristics: deterministic and\nstochastic. Strikingly, there are very few sets of major attackers committing\nalmost all the attacks, since their attack \"fingerprints\" and target selection\nscheme can be unequivocally identified according to the very limited number of\nunique spatiotemporal characteristics, each of which only exists on a\nconsecutive IP region and differs significantly from the others. We utilize a\nnumber of quantitative measures, including the flux-fluctuation law, the Markov\nstate transition probability matrix, and predictability measures, to\ncharacterize the attack patterns in a comprehensive manner. A general finding\nis that the attack patterns possess high degrees of predictability, potentially\npaving the way to anticipating and, consequently, mitigating or even preventing\nlarge-scale cyberattacks using macroscopic approaches.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 04:57:11 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["Chen", "Yu-Zhong", ""], ["Huang", "Zi-Gang", ""], ["Xu", "Shouhuai", ""], ["Lai", "Ying-Cheng", ""]]}, {"id": "1603.07511", "submitter": "Roland Langrock", "authors": "Toby A Patterson and Alison Parton and Roland Langrock and Paul G\n  Blackwell and Len Thomas and Ruth King", "title": "Statistical modelling of individual animal movement: an overview of key\n  methods and a discussion of practical challenges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the influx of complex and detailed tracking data gathered from\nelectronic tracking devices, the analysis of animal movement data has recently\nemerged as a cottage industry amongst biostatisticians. New approaches of ever\ngreater complexity are continue to be added to the literature. In this paper,\nwe review what we believe to be some of the most popular and most useful\nclasses of statistical models used to analyze individual animal movement data.\nSpecifically we consider discrete-time hidden Markov models, more general\nstate-space models and diffusion processes. We argue that these models should\nbe core components in the toolbox for quantitative researchers working on\nstochastic modelling of individual animal movement. The paper concludes by\noffering some general observations on the direction of statistical analysis of\nanimal movement. There is a trend in movement ecology toward what are arguably\noverly-complex modelling approaches which are inaccessible to ecologists,\nunwieldy with large data sets or not based in mainstream statistical practice.\nAdditionally, some analysis methods developed within the ecological community\nignore fundamental properties of movement data, potentially leading to\nmisleading conclusions about animal movement. Corresponding approaches, e.g.\nbased on L\\'evy walk-type models, continue to be popular despite having been\nlargely discredited. We contend that there is a need for an appropriate balance\nbetween the extremes of either being overly complex or being overly simplistic,\nwhereby the discipline relies on models of intermediate complexity that are\nusable by general ecologists, but grounded in well-developed statistical\npractice and efficient to fit to large data sets.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 10:30:20 GMT"}, {"version": "v2", "created": "Sat, 3 Sep 2016 18:51:40 GMT"}, {"version": "v3", "created": "Mon, 30 Jan 2017 08:15:06 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Patterson", "Toby A", ""], ["Parton", "Alison", ""], ["Langrock", "Roland", ""], ["Blackwell", "Paul G", ""], ["Thomas", "Len", ""], ["King", "Ruth", ""]]}, {"id": "1603.07532", "submitter": "Nassim Nicholas Taleb", "authors": "Nassim Nicholas Taleb", "title": "A Short Note on P-Value Hacking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the expected values from p-value hacking as a choice of the\nminimum p-value among $m$ independents tests, which can be considerably lower\nthan the \"true\" p-value, even with a single trial, owing to the extreme\nskewness of the meta-distribution.\n  We first present an exact probability distribution (meta-distribution) for\np-values across ensembles of statistically identical phenomena. We derive the\ndistribution for small samples $2<n \\leq n^*\\approx 30$ as well as the limiting\none as the sample size $n$ becomes large. We also look at the properties of the\n\"power\" of a test through the distribution of its inverse for a given p-value\nand parametrization.\n  The formulas allow the investigation of the stability of the reproduction of\nresults and \"p-hacking\" and other aspects of meta-analysis.\n  P-values are shown to be extremely skewed and volatile, regardless of the\nsample size $n$, and vary greatly across repetitions of exactly same protocols\nunder identical stochastic copies of the phenomenon; such volatility makes the\nminimum $p$ value diverge significantly from the \"true\" one. Setting the power\nis shown to offer little remedy unless sample size is increased markedly or the\np-value is lowered by at least one order of magnitude.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 11:37:23 GMT"}, {"version": "v2", "created": "Mon, 28 Mar 2016 11:32:12 GMT"}, {"version": "v3", "created": "Fri, 8 Apr 2016 01:29:21 GMT"}, {"version": "v4", "created": "Thu, 25 Jan 2018 20:15:34 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Taleb", "Nassim Nicholas", ""]]}, {"id": "1603.07630", "submitter": "James Russell", "authors": "James C. Russell, Ephraim M. Hanks, Murali Haran, David P. Hughes", "title": "A Spatially-Varying Stochastic Differential Equation Model for Animal\n  Movement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Animal movement exhibits complex behavior which can be influenced by\nunobserved environmental conditions. We propose a model which allows for a\nspatially-varying movement rate and spatially-varying drift through a\nsemiparametric potential surface and a separate motility surface. These\nsurfaces are embedded in a stochastic differential equation framework which\nallows for complex animal movement patterns in space. The resulting model is\nused to analyze the spatially-varying behavior of ants to provide insight into\nthe spatial structure of ant movement in the nest.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 15:35:32 GMT"}, {"version": "v2", "created": "Sun, 26 Feb 2017 20:44:20 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Russell", "James C.", ""], ["Hanks", "Ephraim M.", ""], ["Haran", "Murali", ""], ["Hughes", "David P.", ""]]}, {"id": "1603.07653", "submitter": "Sayed Pouria Talebi", "authors": "Sayed Pouria Talebi and Professor Danilo P. Mandic", "title": "A Quaternion Frequency and Phasor Estimator for Three-Phase Power\n  Distribution Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the first time quaternions have been used for real-time frequency\nestimation, where the multi-dimensional nature of quaternions allows for the\nfull characterization of three-phase power systems. This is achieved through\nthe use of quaternions to provide a unified framework for incorporating voltage\nmeasurements from all the phases of a three-phase system and then employing the\nrecently introduced HR-calculus to derive a state space estimator based on the\nquaternion extended Kalman filter (QEKF). The components of the state space\nvector are designed such that they can be deployed for adaptive estimation of\nthe system phasors. Finally, the proposed algorithm is validated through\nsimulations using both synthetic and real-world data, which indicate that the\ndeveloped quaternion frequency estimator can outperform its complex-valued\ncounterparts.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 20:18:32 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["Talebi", "Sayed Pouria", ""], ["Mandic", "Professor Danilo P.", ""]]}, {"id": "1603.07668", "submitter": "Longhai Li", "authors": "Shi Qiu and Cindy X. Feng and Longhai Li", "title": "Approximating Cross-validatory Predictive P-values with Integrated IS\n  for Disease Mapping Models", "comments": "21 pages", "journal-ref": null, "doi": "10.1002/sim.7278", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important statistical task in disease mapping problems is to identify out-\nlier/divergent regions with unusually high or low residual risk of disease.\nLeave-one-out cross-validatory (LOOCV) model assessment is a gold standard for\ncomputing predictive p-value that can flag such outliers. However, actual LOOCV\nis time-consuming because one needs to re-simulate a Markov chain for each\nposterior distribution in which an observation is held out as a test case. This\npaper introduces a new method, called iIS, for approximating LOOCV with only\nMarkov chain samples simulated from a posterior based on a full data set. iIS\nis based on importance sampling (IS). iIS integrates the p-value and the\nlikelihood of the test observation with respect to the distribution of the\nlatent variable without reference to the actual observation. The predictive\np-values computed with iIS can be proved to be equivalent to the LOOCV\npredictive p-values, following the general theory for IS. We com- pare iIS and\nother three existing methods in the literature with a lip cancer dataset\ncollected in Scotland. Our empirical results show that iIS provides predictive\np-values that are al- most identical to the actual LOOCV predictive p-values\nand outperforms the existing three methods, including the recently proposed\nghosting method by Marshall and Spiegelhalter (2007).\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 17:16:08 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Qiu", "Shi", ""], ["Feng", "Cindy X.", ""], ["Li", "Longhai", ""]]}, {"id": "1603.07749", "submitter": "Xi Luo", "authors": "Yi Zhao, Xi Luo", "title": "Pathway Lasso: Estimate and Select Sparse Mediation Pathways with High\n  Dimensional Mediators", "comments": "26 pages and 7 figures. Presented at the 2016 ENAR meeting, March 8,\n  2016, see slides at\n  https://rluo.github.io/slides/MultipleMediator_ENAR_2016.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many scientific studies, it becomes increasingly important to delineate\nthe causal pathways through a large number of mediators, such as genetic and\nbrain mediators. Structural equation modeling (SEM) is a popular technique to\nestimate the pathway effects, commonly expressed as products of coefficients.\nHowever, it becomes unstable to fit such models with high dimensional\nmediators, especially for a general setting where all the mediators are\ncausally dependent but the exact causal relationships between them are unknown.\nThis paper proposes a sparse mediation model using a regularized SEM approach,\nwhere sparsity here means that a small number of mediators have nonzero\nmediation effects between a treatment and an outcome. To address the model\nselection challenge, we innovate by introducing a new penalty called Pathway\nLasso. This penalty function is a convex relaxation of the non-convex product\nfunction, and it enables a computationally tractable optimization criterion to\nestimate and select many pathway effects simultaneously. We develop a fast\nADMM-type algorithm to compute the model parameters, and we show that the\niterative updates can be expressed in closed form. On both simulated data and a\nreal fMRI dataset, the proposed approach yields higher pathway selection\naccuracy and lower estimation bias than other competing methods.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 20:46:24 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Zhao", "Yi", ""], ["Luo", "Xi", ""]]}, {"id": "1603.07775", "submitter": "Michel Bessani", "authors": "Michel Bessani, Rodrigo Z. Fanucchi, Alexandre C.C. Delbem, Carlos D.\n  Maciel", "title": "The Impact of Operators' Performance in the Reliability of\n  Cyber-Physical Power Distribution Systems", "comments": null, "journal-ref": null, "doi": "10.1049/iet-gtd.2015.1062", "report-no": null, "categories": "cs.SY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyber-Physical Systems are the result of integrating information and\ncommunication technologies into physical systems. One particular case are\nCyber-Physical Power Systems (CPPS), which use communication technologies to\nperform real-time monitoring and operation. These kinds of systems have become\nmore complex, impacting on the systems' characteristics, such as their\nreliability. In addition, it is already known that in terms of the reliability\nof Cyber-Physical Power Distribution Systems (CPPDS), the failures of the\ncommunication network are just as relevant as the electrical network failures.\nHowever, some of the operators' performances, such as response time and\ndecision quality, during CPPDS contingencies have not been investigated yet. In\nthis paper, we introduce a model to the operator response time, present a\nSequential Monte Carlo Simulation methodology that incorporates the response\ntime in CPPDS reliability indices estimation, and evaluate the impact of the\noperator response time in reliability indices. Our method is tested on a CPPDS\nusing different values for the average response time of operators. The results\nshow that the response time of the operators affects the reliability indices\nthat are related to the durations of the failure, indicating that a fast\ndecision directly contributes to the system performance. We conclude that the\nimprovement of CPPDS reliability is not only dependent on the electric and\ncommunication components, but also dependent on operators' performance.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 22:54:24 GMT"}, {"version": "v2", "created": "Wed, 27 Apr 2016 19:32:08 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Bessani", "Michel", ""], ["Fanucchi", "Rodrigo Z.", ""], ["Delbem", "Alexandre C. C.", ""], ["Maciel", "Carlos D.", ""]]}, {"id": "1603.07888", "submitter": "Xiaoyu Liu", "authors": "Xiaoyu Liu and Serge Guillas", "title": "Dimension reduction for Gaussian process emulation: an application to\n  the influence of bathymetry on tsunami heights", "comments": "26 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High accuracy complex computer models, or simulators, require large resources\nin time and memory to produce realistic results. Statistical emulators are\ncomputationally cheap approximations of such simulators. They can be built to\nreplace simulators for various purposes, such as the propagation of\nuncertainties from inputs to outputs or the calibration of some internal\nparameters against observations. However, when the input space is of high\ndimension, the construction of an emulator can become prohibitively expensive.\nIn this paper, we introduce a joint framework merging emulation with dimension\nreduction in order to overcome this hurdle. The gradient-based kernel dimension\nreduction technique is chosen due to its ability to drastically decrease\ndimensionality with little loss in information. The Gaussian process emulation\ntechnique is combined with this dimension reduction approach. Our proposed\napproach provides an answer to the dimension reduction issue in emulation for a\nwide range of simulation problems that cannot be tackled using existing\nmethods. The efficiency and accuracy of the proposed framework is demonstrated\ntheoretically, and compared with other methods on an elliptic partial\ndifferential equation (PDE) problem. We finally present a realistic application\nto tsunami modeling. The uncertainties in the bathymetry (seafloor elevation)\nare modeled as high-dimensional realizations of a spatial process using a\ngeostatistical approach. Our dimension-reduced emulation enables us to compute\nthe impact of these uncertainties on resulting possible tsunami wave heights\nnear-shore and on-shore. We observe a significant increase in the spread of\nuncertainties in the tsunami heights due to the contribution of the bathymetry\nuncertainties. These results highlight the need to include the effect of\nuncertainties in the bathymetry in tsunami early warnings and risk assessments.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 11:59:53 GMT"}, {"version": "v2", "created": "Wed, 24 Aug 2016 12:17:55 GMT"}], "update_date": "2016-08-25", "authors_parsed": [["Liu", "Xiaoyu", ""], ["Guillas", "Serge", ""]]}, {"id": "1603.07948", "submitter": "Rebecca Wooten Dr.", "authors": "Rebecca D. Wooten and J. D'Andrea", "title": "Modeling Hurricanes using Principle Component Analysis in conjunction\n  with Non-Response Analysis", "comments": "17 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper demonstrates how principle component analysis can be used to\ndetermine the distinct factors that house the terms that explain the variance\namong the co-dependent variables and how non-response analysis can be applied\nto model the non-functional relationship that exist in a dynamic system.\nMoreover, the analysis indicates that there are pumping actions or ebb and flow\nbetween the pressure and the water temperature readings near the surface of the\nwater days before a tropical storm forms in the Atlantic Basin and that there\nis a high correlation between storm conditions and buoy conditions three-four\ndays before a storm forms. Further analysis shows that that the relationship\namong the variables is conical.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 16:15:17 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Wooten", "Rebecca D.", ""], ["D'Andrea", "J.", ""]]}, {"id": "1603.08163", "submitter": "Farouk Nathoo", "authors": "Farouk S. Nathoo, Keelin Greenlaw, Mary Lesperance", "title": "Regularization Parameter Selection for a Bayesian Multi-Level Group\n  Lasso Regression Model with Application to Imaging Genomics", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the choice of tuning parameters for a Bayesian multi-level\ngroup lasso model developed for the joint analysis of neuroimaging and genetic\ndata. The regression model we consider relates multivariate phenotypes\nconsisting of brain summary measures (volumetric and cortical thickness values)\nto single nucleotide polymorphism (SNPs) data and imposes penalization at two\nnested levels, the first corresponding to genes and the second corresponding to\nSNPs. Associated with each level in the penalty is a tuning parameter which\ncorresponds to a hyperparameter in the hierarchical Bayesian formulation.\nFollowing previous work on Bayesian lassos we consider the estimation of tuning\nparameters through either hierarchical Bayes based on hyperpriors and Gibbs\nsampling or through empirical Bayes based on maximizing the marginal likelihood\nusing a Monte Carlo EM algorithm. For the specific model under consideration we\nfind that these approaches can lead to severe overshrinkage of the regression\nparameter estimates in the high-dimensional setting or when the genetic effects\nare weak. We demonstrate these problems through simulation examples and study\nan approximation to the marginal likelihood which sheds light on the cause of\nthis problem. We then suggest an alternative approach based on the widely\napplicable information criterion (WAIC), an asymptotic approximation to\nleave-one-out cross-validation that can be computed conveniently within an MCMC\nframework.\n", "versions": [{"version": "v1", "created": "Sun, 27 Mar 2016 02:34:02 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Nathoo", "Farouk S.", ""], ["Greenlaw", "Keelin", ""], ["Lesperance", "Mary", ""]]}, {"id": "1603.08188", "submitter": "Yimin Liu", "authors": "Yimin Liu, Hang Ruan, Lei Wang, and Arye Nehorai", "title": "The Random Frequency Diverse Array: A New Antenna Structure for\n  Uncoupled Direction-Range Indication in Active Sensing", "comments": "13 pages, 10 figures", "journal-ref": null, "doi": "10.1109/JSTSP.2016.2627183", "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new type of array antenna, termed the Random\nFrequency Diverse Array (RFDA), for an uncoupled indication of target direction\nand range with low system complexity. In RFDA, each array element has a narrow\nbandwidth and a randomly assigned carrier frequency. The beampattern of the\narray is shown to be stochastic but thumbtack-like, and its stochastic\ncharacteristics, such as the mean, variance, and asymptotic distribution are\nderived analytically. Based on these two features, we propose two kinds of\nalgorithms for signal processing. One is matched filtering, due to the\nbeampattern's good characteristics. The other is compressive sensing, because\nthe new approach can be regarded as a sparse and random sampling of target\ninformation in the spatial-frequency domain. Fundamental limits, such as the\nCram\\'er-Rao bound and the observing matrix's mutual coherence, are provided as\nperformance guarantees of the new array structure. The features and\nperformances of RFDA are verified with numerical results.\n", "versions": [{"version": "v1", "created": "Sun, 27 Mar 2016 08:50:45 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Liu", "Yimin", ""], ["Ruan", "Hang", ""], ["Wang", "Lei", ""], ["Nehorai", "Arye", ""]]}, {"id": "1603.08189", "submitter": "Yimin Liu", "authors": "Yimin Liu, Le Xiao, Xiqin Wang, and Arye Nehorai", "title": "On Clutter Ranks of Frequency Diverse Radar Waveforms", "comments": "13 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frequency diverse (FD) radar waveforms are attractive in radar research and\npractice. By combining two typical FD waveforms, the frequency diverse array\n(FDA) and the stepped-frequency (SF) pulse train, we propose a general FD\nwaveform model, termed the random frequency diverse multi-input-multi-output\n(RFD-MIMO) in this paper. The new model can be applied to specific FD waveforms\nby adapting parameters. Furthermore, by exploring the characteristics of the\nclutter covariance matrix, we provide an approach to evaluate the clutter rank\nof the RFD-MIMO radar, which can be adopted as a quantitive metric for the\nclutter suppression potentials of FD waveforms. Numerical simulations show the\neffectiveness of the clutter rank estimation method, and reveal helpful results\nfor comparing the clutter suppression performance of different FD waveforms.\n", "versions": [{"version": "v1", "created": "Sun, 27 Mar 2016 08:51:58 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Liu", "Yimin", ""], ["Xiao", "Le", ""], ["Wang", "Xiqin", ""], ["Nehorai", "Arye", ""]]}, {"id": "1603.08326", "submitter": "Luke Lloyd-Jones", "authors": "Luke R. Lloyd-Jones, Hien D. Nguyen and Geoffrey J. McLachlan", "title": "A globally convergent algorithm for lasso-penalized mixture of linear\n  regression models", "comments": "38 pages, 4 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection is an old and pervasive problem in regression analysis.\nOne solution is to impose a lasso penalty to shrink parameter estimates toward\nzero and perform continuous model selection. The lasso-penalized mixture of\nlinear regressions model (L-MLR) is a class of regularization methods for the\nmodel selection problem in the fixed number of variables setting. In this\narticle, we propose a new algorithm for the maximum penalized-likelihood\nestimation of the L-MLR model. This algorithm is constructed via the\nminorization--maximization algorithm paradigm. Such a construction allows for\ncoordinate-wise updates of the parameter components, and produces globally\nconvergent sequences of estimates that generate monotonic sequences of\npenalized log-likelihood values. These three features are missing in the\npreviously presented approximate expectation-maximization algorithms. The\nprevious difficulty in producing a globally convergent algorithm for the\nmaximum penalized-likelihood estimation of the L-MLR model is due to the\nintractability of finding exact updates for the mixture model mixing\nproportions in the maximization-step. In our algorithm, we solve this issue by\nshowing that it can be converted into a polynomial root finding problem. Our\nsolution to this problem involves a polynomial basis conversion that is\ninteresting in its own right. The method is tested in simulation and with an\napplication to Major League Baseball salary data from the 1990s and the present\nday. We explore the concept of whether player salaries are associated with\nbatting performance.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 07:13:43 GMT"}, {"version": "v2", "created": "Mon, 2 May 2016 23:00:53 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Lloyd-Jones", "Luke R.", ""], ["Nguyen", "Hien D.", ""], ["McLachlan", "Geoffrey J.", ""]]}, {"id": "1603.08588", "submitter": "Dennis Feehan", "authors": "Dennis M. Feehan, Mary Mahy, and Matthew J. Salganik", "title": "The network survival method for estimating adult mortality: Evidence\n  from a survey experiment in Rwanda", "comments": null, "journal-ref": null, "doi": "10.1007/s13524-017-0594-y", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adult death rates are a critical indicator of population health and\nwellbeing. Wealthy countries have high-quality vital registration systems, but\npoor countries lack this infrastructure and must rely on estimates that are\noften problematic. In this paper, we introduce the network survival method, a\nnew approach for estimating adult death rates. We derive the precise conditions\nunder which it produces estimates that are consistent and unbiased. Further, we\ndevelop an analytical framework for sensitivity analysis. To assess the\nperformance of the network survival method in a realistic setting, we conducted\na nationally-representative survey experiment in Rwanda (n=4,669). Network\nsurvival estimates were similar to estimates from other methods, even though\nthe network survival estimates were made with substantially smaller samples and\nare based entirely on data from Rwanda, with no need for model life tables or\npooling of data from other countries. Our analytic results demonstrate that the\nnetwork survival method has attractive properties, and our empirical results\nshow that it can be used in countries where reliable estimates of adult death\nrates are sorely needed.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 23:07:41 GMT"}, {"version": "v2", "created": "Fri, 11 Nov 2016 07:15:05 GMT"}, {"version": "v3", "created": "Sun, 19 Feb 2017 20:39:06 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Feehan", "Dennis M.", ""], ["Mahy", "Mary", ""], ["Salganik", "Matthew J.", ""]]}, {"id": "1603.08813", "submitter": "Deniz Akdemir", "authors": "Deniz Akdemir, Jean-Luc Jannink", "title": "Locally Epistatic Models for Genome-wide Prediction and Association by\n  Importance Sampling", "comments": "*Corresponding Author: Deniz Akdemir (denjz.akdemir.work@gmail.com)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In statistical genetics an important task involves building predictive models\nfor the genotype-phenotype relationships and thus attribute a proportion of the\ntotal phenotypic variance to the variation in genotypes. Numerous models have\nbeen proposed to incorporate additive genetic effects into models for\nprediction or association. However, there is a scarcity of models that can\nadequately account for gene by gene or other forms of genetical interactions.\nIn addition, there is an increased interest in using marker annotations in\ngenome-wide prediction and association. In this paper, we discuss an hybrid\nmodeling methodology which combines the parametric mixed modeling approach and\nthe non-parametric rule ensembles. This approach gives us a flexible class of\nmodels that can be used to capture additive, locally epistatic genetic effects,\ngene x background interactions and allows us to incorporate one or more\nannotations into the genomic selection or association models. We use benchmark\ndata sets covering a range of organisms and traits in addition to simulated\ndata sets to illustrate the strengths of this approach. The improvement of\nmodel accuracies and association results suggest that a part of the \"missing\nheritability\" in complex traits can be captured by modeling local epistasis.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 15:30:46 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Akdemir", "Deniz", ""], ["Jannink", "Jean-Luc", ""]]}, {"id": "1603.08969", "submitter": "Xin Zhang", "authors": "Xin Zhang, Mohammed Nabil El Korso, Marius Pesavento", "title": "MIMO Radar Target Localization and Performance Evaluation under SIRP\n  Clutter", "comments": "34 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple-input multiple-output (MIMO) radar has become a thriving subject of\nresearch during the past decades. In the MIMO radar context, it is sometimes\nmore accurate to model the radar clutter as a non-Gaussian process, more\nspecifically, by using the spherically invariant random process (SIRP) model.\nIn this paper, we focus on the estimation and performance analysis of the\nangular spacing between two targets for the MIMO radar under the SIRP clutter.\nFirst, we propose an iterative maximum likelihood as well as an iterative\nmaximum a posteriori estimator, for the target's spacing parameter estimation\nin the SIRP clutter context. Then we derive and compare various\nCram\\'er-Rao-like bounds (CRLBs) for performance assessment. Finally, we\naddress the problem of target resolvability by using the concept of angular\nresolution limit (ARL), and derive an analytical, closed-form expression of the\nARL based on Smith's criterion, between two closely spaced targets in a MIMO\nradar context under SIRP clutter. For this aim we also obtain the non-matrix,\nclosed-form expressions for each of the CRLBs. Finally, we provide numerical\nsimulations to assess the performance of the proposed algorithms, the validity\nof the derived ARL expression, and to reveal the ARL's insightful properties.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 21:09:43 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["Zhang", "Xin", ""], ["Korso", "Mohammed Nabil El", ""], ["Pesavento", "Marius", ""]]}, {"id": "1603.08982", "submitter": "Xin Zhang", "authors": "Xin Zhang, Mohammed Nabil El Korso, Marius Pesavento", "title": "Maximum Likelihood and Maximum A Posteriori Direction-of-Arrival\n  Estimation in the Presence of SIRP Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The maximum likelihood (ML) and maximum a posteriori (MAP) estimation\ntechniques are widely used to address the direction-of-arrival (DOA) estimation\nproblems, an important topic in sensor array processing. Conventionally the ML\nestimators in the DOA estimation context assume the sensor noise to follow a\nGaussian distribution. In real-life application, however, this assumption is\nsometimes not valid, and it is often more accurate to model the noise as a\nnon-Gaussian process. In this paper we derive an iterative ML as well as an\niterative MAP estimation algorithm for the DOA estimation problem under the\nspherically invariant random process noise assumption, one of the most popular\nnon-Gaussian models, especially in the radar context. Numerical simulation\nresults are provided to assess our proposed algorithms and to show their\nadvantage in terms of performance over the conventional ML algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 21:59:28 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["Zhang", "Xin", ""], ["Korso", "Mohammed Nabil El", ""], ["Pesavento", "Marius", ""]]}, {"id": "1603.09000", "submitter": "Adel Javanmard", "authors": "Adel Javanmard and Andrea Montanari", "title": "Online Rules for Control of False Discovery Rate and False Discovery\n  Exceedance", "comments": "44 pages, 9 figures, to appear in Annals of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.AP stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple hypothesis testing is a core problem in statistical inference and\narises in almost every scientific field. Given a set of null hypotheses\n$\\mathcal{H}(n) = (H_1,\\dotsc, H_n)$, Benjamini and Hochberg introduced the\nfalse discovery rate (FDR), which is the expected proportion of false positives\namong rejected null hypotheses, and proposed a testing procedure that controls\nFDR below a pre-assigned significance level. Nowadays FDR is the criterion of\nchoice for large scale multiple hypothesis testing. In this paper we consider\nthe problem of controlling FDR in an \"online manner\". Concretely, we consider\nan ordered --possibly infinite-- sequence of null hypotheses $\\mathcal{H} =\n(H_1,H_2,H_3,\\dots )$ where, at each step $i$, the statistician must decide\nwhether to reject hypothesis $H_i$ having access only to the previous\ndecisions. This model was introduced by Foster and Stine. We study a class of\n\"generalized alpha-investing\" procedures and prove that any rule in this class\ncontrols online FDR, provided $p$-values corresponding to true nulls are\nindependent from the other $p$-values. (Earlier work only established mFDR\ncontrol.) Next, we obtain conditions under which generalized alpha-investing\ncontrols FDR in the presence of general $p$-values dependencies. Finally, we\ndevelop a modified set of procedures that also allow to control the false\ndiscovery exceedance (the tail of the proportion of false discoveries).\nNumerical simulations and analytical results indicate that online procedures do\nnot incur a large loss in statistical power with respect to offline approaches,\nsuch as Benjamini-Hochberg.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 23:41:51 GMT"}, {"version": "v2", "created": "Thu, 25 Aug 2016 23:49:37 GMT"}, {"version": "v3", "created": "Thu, 6 Jul 2017 23:37:43 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Javanmard", "Adel", ""], ["Montanari", "Andrea", ""]]}, {"id": "1603.09208", "submitter": "Willem Eerland", "authors": "Willem J. Eerland and Simon Box", "title": "Trajectory Clustering, Modelling, and Selection with the Focus on\n  Airspace Protection", "comments": "15 pages, work presented at AIAA Scitech 2016", "journal-ref": null, "doi": "10.2514/6.2016-1411", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Take-off and landing are the periods of a flight where aircraft are most\nvulnerable to a ground based rocket attack by terrorists. While aircraft\napproach and depart from airports on pre-defined flight paths, there is a\ndegree of uncertainty in the trajectory of each individual aircraft. Capturing\nand characterizing these deviations is important for accurate strategic\nplanning for the defence of airports against terrorist attack. A methodology is\ndemonstrated whereby approach and departure trajectories to a given airport are\ncharacterized statistically from historical data. It uses a two-step process of\nfirst clustering to extract the common trend, and then modelling uncertainty\nusing Gaussian Processes (GPs). Furthermore it is shown that this approach can\nbe used to either select probabilistic regions of airspace where trajectories\nare likely and - if required - can automatically generate a set of\nrepresentative trajectories, or select key trajectories that are both likely\nand critically vulnerable. An evaluation of the methodology is demonstrated on\nan example data-set collected by the ground radar at an airport. The evaluation\nindicates that 99.8% of the calculated footprint underestimates less than 5%\nwhen replacing the original trajectory data with a set of representative\ntrajectories.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 14:13:58 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["Eerland", "Willem J.", ""], ["Box", "Simon", ""]]}, {"id": "1603.09297", "submitter": "Sayed Pouria Talebi", "authors": "Sayed Pouria Talebi", "title": "On Distributed Frequency Estimation in Three-Phase Power Distribution\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The earlier work of the author on Frequency estimation in three-phase power\nsystems ( that is included as the reference number 1) is expanded to the\ndistributed setting in order present a framework for the implementation of such\na frequency estimator in real-world power distribution networks. For rigor, the\nmean and mean square performance of the distributed frequency estimator is\nanalysed. The performance of the developed algorithm is validated through\nsimulations on both synthetic data and real-world data recordings, where it is\nshown to outperform standard linear and the recently introduced widely liner\nfrequency estimators.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 18:00:40 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["Talebi", "Sayed Pouria", ""]]}, {"id": "1603.09436", "submitter": "Amit Sharma", "authors": "Benjamin Shulman, Amit Sharma, Dan Cosley", "title": "Predictability of Popularity: Gaps between Prediction and Understanding", "comments": "10 pages, ICWSM 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can we predict the future popularity of a song, movie or tweet? Recent work\nsuggests that although it may be hard to predict an item's popularity when it\nis first introduced, peeking into its early adopters and properties of their\nsocial network makes the problem easier. We test the robustness of such claims\nby using data from social networks spanning music, books, photos, and URLs. We\nfind a stronger result: not only do predictive models with peeking achieve high\naccuracy on all datasets, they also generalize well, so much so that models\ntrained on any one dataset perform with comparable accuracy on items from other\ndatasets.\n  Though practically useful, our models (and those in other work) are\nintellectually unsatisfying because common formulations of the problem, which\ninvolve peeking at the first small-k adopters and predicting whether items end\nup in the top half of popular items, are both too sensitive to the speed of\nearly adoption and too easy. Most of the predictive power comes from looking at\nhow quickly items reach their first few adopters, while for other features of\nearly adopters and their networks, even the direction of correlation with\npopularity is not consistent across domains. Problem formulations that examine\nitems that reach k adopters in about the same amount of time reduce the\nimportance of temporal features, but also overall accuracy, highlighting that\nwe understand little about why items become popular while providing a context\nin which we might build that understanding.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 01:52:34 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Shulman", "Benjamin", ""], ["Sharma", "Amit", ""], ["Cosley", "Dan", ""]]}, {"id": "1603.09716", "submitter": "Yisa Yakubu", "authors": "Yisa Yakubu, Angela Unna Chukwu, Bamiduro Timothy Adebayo, Amahia\n  Godwin Nwanzo", "title": "Effects of missing observations on predictive capability of central\n  composite designs", "comments": "18 PAGES, 12 FIGURES", "journal-ref": "International Journal on Computational Sciences & Applications\n  (IJCSA) Vol.4, No.6, December 2014", "doi": "10.5121/ijcsa.2014.4601", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quite often in experimental work, many situations arise where some\nobservations are lost or become unavailable due to some accidents or cost\nconstraints. When there are missing observations, some desirable design\nproperties like orthogonality, rotatability and optimality can be adversely\naffected. Some attention has been given, in literature, to investigating the\nprediction capability of response surface designs; however, little or no effort\nhas been devoted to investigating same for such designs when some observations\nare missing. This work therefore investigates the impact of a single missing\nobservation of the various design points: factorial, axial and center points,\non the estimation and predictive capability of Central Composite Designs\n(CCDs). It was observed that for each of the designs considered, precision of\nmodel parameter estimates and the design prediction properties were adversely\naffected by the missing observations and that the largest loss in precision of\nparameters corresponds to a missing factorial point.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jan 2015 14:52:03 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Yakubu", "Yisa", ""], ["Chukwu", "Angela Unna", ""], ["Adebayo", "Bamiduro Timothy", ""], ["Nwanzo", "Amahia Godwin", ""]]}]