[{"id": "0905.0116", "submitter": "Paul Seed", "authors": "Paul T Seed", "title": "Correspondence: The use of cost information when defining critical\n  values for prediction of rare events using logistic regression and similar\n  methods", "comments": "Response to Berk (2009) Forecasting murder within a population of\n  probationers and parolees: a high stakes application of statistical learning.\n  J.R. Statist. Soc. A, 172, 191-211", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Balancing a rare and serious possibility against a more common and less\nserious one is a familiar problem in many situations, such as the prediction of\nrare diseases. The relative costs of forecasting errors can be used for any\nprediction method that gives an estimated probability of a future event. The\nprobability at which the likely cost (defined as cost x probability) of a\npossible false negative is exactly equal to that of a possible false positive\ngives the relevant cutpoint and all subjects with probability of disease\ngreater than this have a positive test result. All standard methods of logistic\nregression will give the log-odds and hence the predicted probability of a\npositive outcome for every subject:\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2009 16:41:33 GMT"}], "update_date": "2009-05-04", "authors_parsed": [["Seed", "Paul T", ""]]}, {"id": "0905.0210", "submitter": "Ramses Mena H", "authors": "Ruth Fuentes-Garcia, Ramses H Mena and Stephen G Walker", "title": "A probability for classification based on the mixture of Dirichlet\n  process model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide an explicit probability distribution for\nclassification purposes. It is derived from the Bayesian nonparametric mixture\nof Dirichlet process model, but with suitable modifications which remove\nunsuitable aspects of the classification based on this model. The resulting\napproach then more closely resembles a classical hierarchical grouping rule in\nthat it depends on sums of squares of neighboring values. The proposed\nprobability model for classification relies on a simulation algorithm which\nwill be based on a reversible MCMC algorithm for determining the probabilities,\nand we provide numerical illustrations comparing with alternative ideas for\nclassification.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2009 15:08:44 GMT"}], "update_date": "2009-05-05", "authors_parsed": [["Fuentes-Garcia", "Ruth", ""], ["Mena", "Ramses H", ""], ["Walker", "Stephen G", ""]]}, {"id": "0905.0445", "submitter": "Stephen McIntyre", "authors": "Stephen McIntyre and Ross McKitrick", "title": "An updated comparison of model ensemble and observed temperature trends\n  in the tropical troposphere", "comments": "21 pages, 2 figures. Includes source code as Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A debate exists over whether tropical troposphere temperature trends in\nclimate models are inconsistent with observations (Karl et al. 2006, IPCC\n(2007), Douglass et al 2007, Santer et al 2008). Most recently, Santer et al\n(2008, herein S08) asserted that the Douglass et al statistical methodology was\nflawed and that a correct methodology showed there is no statistically\nsignificant difference between the model ensemble mean trend and either RSS or\nUAH satellite observations. However this result was based on data ending in\n1999. Using data up to the end of 2007 (as available to S08) or to the end of\n2008 and applying exactly the same methodology as S08 results in a\nstatistically significant difference between the ensemble mean trend and UAH\nobservations and approaching statistical significance for the RSS T2 data. The\nclaim by S08 to have achieved a \"partial resolution\" of the discrepancy between\nobservations and the model ensemble mean trend is unwarranted.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2009 18:09:16 GMT"}], "update_date": "2009-05-05", "authors_parsed": [["McIntyre", "Stephen", ""], ["McKitrick", "Ross", ""]]}, {"id": "0905.0454", "submitter": "Pierre Comon", "authors": "Pierre Comon", "title": "Tensor Decompositions, State of the Art and Applications", "comments": null, "journal-ref": "Mathematics in Signal Processing V, J. G. McWhirter and I. K.\n  Proudler (Ed.) (2002) 1-24", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a partial survey of the tools borrowed from tensor\nalgebra, which have been utilized recently in Statistics and Signal Processing.\nIt is shown why the decompositions well known in linear algebra can hardly be\nextended to tensors. The concept of rank is itself difficult to define, and its\ncalculation raises difficulties. Numerical algorithms have nevertheless been\ndeveloped, and some are reported here, but their limitations are emphasized.\nThese reports hopefully open research perspectives for enterprising readers.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2009 18:53:45 GMT"}], "update_date": "2009-05-05", "authors_parsed": [["Comon", "Pierre", ""]]}, {"id": "0905.0603", "submitter": "Nicole Kraemer", "authors": "Nicole Kraemer, Juliane Schaefer, Anne-Laure Boulesteix", "title": "Regularized estimation of large-scale gene association networks using\n  graphical Gaussian models", "comments": "added additional experiments", "journal-ref": "BMC Bioinformatics, 10:384, 2010", "doi": "10.1186/1471-2105-10-384", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical Gaussian models are popular tools for the estimation of\n(undirected) gene association networks from microarray data. A key issue when\nthe number of variables greatly exceeds the number of samples is the estimation\nof the matrix of partial correlations. Since the (Moore-Penrose) inverse of the\nsample covariance matrix leads to poor estimates in this scenario, standard\nmethods are inappropriate and adequate regularization techniques are needed. In\nthis article, we investigate a general framework for combining regularized\nregression methods with the estimation of Graphical Gaussian models. This\nframework includes various existing methods as well as two new approaches based\non ridge regression and adaptive lasso, respectively. These methods are\nextensively compared both qualitatively and quantitatively within a simulation\nstudy and through an application to six diverse real data sets. In addition,\nall proposed algorithms are implemented in the R package \"parcor\", available\nfrom the R repository CRAN.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2009 13:30:23 GMT"}, {"version": "v2", "created": "Sun, 30 Aug 2009 21:54:02 GMT"}], "update_date": "2010-08-13", "authors_parsed": [["Kraemer", "Nicole", ""], ["Schaefer", "Juliane", ""], ["Boulesteix", "Anne-Laure", ""]]}, {"id": "0905.0727", "submitter": "Steven Anderson", "authors": "Steven Matthew Anderson, Shahar Boneh, Nels Grevstad", "title": "Statistical Methods for Determining Optimal Rifle Cartridge Dimensions", "comments": "8 pages, 5 figures, 7 tables", "journal-ref": "Anderson, S., Boneh, S., Grevstad, N. 2008. Statistical Methods\n  for Determining Optimal Rifle Cartridge Dimensions. In JSM Proceedings", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have designed and carried out a statistical study to determine the optimal\ncartridge dimensions for a Savage 10FLP law enforcement grade rifle. Optimal\nperformance is defined as minimal group diameter. A full factorial block design\nwith two main factors and one blocking factor was used. The two main factors\nwere bullet seating depth and powder charge. The experimental units were\nindividual shots taken from a bench-rest position and fired into separate\ntargets. Additionally, thirteen covariates describing various cartridge\ndimensions were recorded. The data analysis includes ANOVA and ANCOVA. We will\ndescribe the experiment, the analysis, and some results.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2009 01:26:52 GMT"}], "update_date": "2009-05-07", "authors_parsed": [["Anderson", "Steven Matthew", ""], ["Boneh", "Shahar", ""], ["Grevstad", "Nels", ""]]}, {"id": "0905.1422", "submitter": "Philip Stark", "authors": "Philip B. Stark", "title": "Auditing a collection of races simultaneously", "comments": "9pp", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A collection of races in a single election can be audited as a group by\nauditing a random sample of batches of ballots and combining observed\ndiscrepancies in the races represented in those batches in a particular way:\nthe maximum across-race relative overstatement of pairwise margins (MARROP). A\nrisk-limiting audit for the entire collection of races can be built on this\nballot-based auditing using a variety of probability sampling schemes. The\naudit controls the familywise error rate (the chance that one or more incorrect\noutcomes fails to be corrected by a full hand count) at a cost that can be\nlower than that of controlling the per-comparison error rate with independent\naudits. The approach is particularly efficient if batches are drawn with\nprobability proportional to a bound on the MARROP (PPEB sampling).\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2009 18:11:00 GMT"}], "update_date": "2009-05-12", "authors_parsed": [["Stark", "Philip B.", ""]]}, {"id": "0905.1983", "submitter": "Luis L\\'opez-Oliveros", "authors": "Luis Lopez-Oliveros and Sidney I. Resnick", "title": "Extremal dependence analysis of network sessions", "comments": "24 pages, 10 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We refine a stimulating study by Sarvotham et al. [2005] which highlighted\nthe influence of peak transmission rate on network burstiness. From TCP packet\nheaders, we amalgamate packets into sessions where each session is\ncharacterized by a 5-tuple (S, D, R, Peak R, Initiation T)=(total payload,\nduration, average transmission rate, peak transmission rate, initiation time).\nAfter careful consideration, a new definition of peak rate is required. Unlike\nSarvotham et al. [2005] who segmented sessions into two groups labelled alpha\nand beta, we segment into 10 sessions according to the empirical quantiles of\nthe peak rate variable as a demonstration that the beta group is far from\nhomogeneous. Our more refined segmentation reveals additional structure that is\nmissed by segmentation into two groups. In each segment, we study the\ndependence structure of (S, D, R) and find that it varies across the groups.\nFurthermore, within each segment, session initiation times are well\napproximated by a Poisson process whereas this property does not hold for the\ndata set taken as a whole. Therefore, we conclude that the peak rate level is\nimportant for understanding structure and for constructing accurate simulations\nof data in the wild. We outline a simple method of simulating network traffic\nbased on our findings.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2009 22:58:13 GMT"}], "update_date": "2009-05-14", "authors_parsed": [["Lopez-Oliveros", "Luis", ""], ["Resnick", "Sidney I.", ""]]}, {"id": "0905.2236", "submitter": "Patrick Harrington Jr.", "authors": "Patrick L. Harrington Jr., Alfred O. Hero III", "title": "Percolation Thresholds of Updated Posteriors for Tracking Causal Markov\n  Processes in Complex Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Percolation on complex networks has been used to study computer viruses,\nepidemics, and other casual processes. Here, we present conditions for the\nexistence of a network specific, observation dependent, phase transition in the\nupdated posterior of node states resulting from actively monitoring the\nnetwork. Since traditional percolation thresholds are derived using observation\nindependent Markov chains, the threshold of the posterior should more\naccurately model the true phase transition of a network, as the updated\nposterior more accurately tracks the process. These conditions should provide\ninsight into modeling the dynamic response of the updated posterior to active\nintervention and control policies while monitoring large complex networks.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2009 02:49:16 GMT"}], "update_date": "2009-05-15", "authors_parsed": [["Harrington", "Patrick L.", "Jr."], ["Hero", "Alfred O.", "III"]]}, {"id": "0905.2315", "submitter": "John Rice", "authors": "Thomas J. Loredo, John Rice, Michael L. Stein", "title": "Introduction to papers on astrostatistics", "comments": "Published in at http://dx.doi.org/10.1214/09-AOAS234 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 1, 1-5", "doi": "10.1214/09-AOAS234", "report-no": "IMS-AOAS-AOAS234", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are pleased to present a Special Section on Statistics and Astronomy in\nthis issue of the The Annals of Applied Statistics. Astronomy is an\nobservational rather than experimental science; as a result, astronomical data\nsets both small and large present particularly challenging problems to analysts\nwho must make the best of whatever the sky offers their instruments. The\nresulting statistical problems have enormous diversity. In one problem, one may\nhave to carefully quantify uncertainty in a hard-won, sparse data set; in\nanother, the sheer volume of data may forbid a formally optimal analysis,\nrequiring judicious balancing of model sophistication, approximations, and\nclever algorithms. Often the data bear a complex relationship to the underlying\nphenomenon producing them, much in the manner of inverse problems.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2009 13:23:23 GMT"}], "update_date": "2009-05-15", "authors_parsed": [["Loredo", "Thomas J.", ""], ["Rice", "John", ""], ["Stein", "Michael L.", ""]]}, {"id": "0905.2544", "submitter": "Bodhisattva Sen", "authors": "Bodhisattva Sen, Moulinath Banerjee, Michael Woodroofe, Mario Mateo,\n  Matthew Walker", "title": "Streaming motion in Leo I", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS211 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 1, 96-116", "doi": "10.1214/08-AOAS211", "report-no": "IMS-AOAS-AOAS211", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whether a dwarf spheroidal galaxy is in equilibrium or being tidally\ndisrupted by the Milky Way is an important question for the study of its dark\nmatter content and distribution. This question is investigated using 328 recent\nobservations from the dwarf spheroidal Leo I. For Leo I, tidal disruption is\ndetected, at least for stars sufficiently far from the center, but the effect\nappears to be quite modest. Statistical tools include isotonic and split point\nestimators, asymptotic theory, and resampling methods.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2009 13:08:11 GMT"}], "update_date": "2009-05-18", "authors_parsed": [["Sen", "Bodhisattva", ""], ["Banerjee", "Moulinath", ""], ["Woodroofe", "Michael", ""], ["Mateo", "Mario", ""], ["Walker", "Matthew", ""]]}, {"id": "0905.2547", "submitter": "David A. van Dyk", "authors": "David A. van Dyk, Steven DeGennaro, Nathan Stein, William H. Jefferys,\n  Ted von Hippel", "title": "Statistical analysis of stellar evolution", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS219 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 1, 117-143", "doi": "10.1214/08-AOAS219", "report-no": "IMS-AOAS-AOAS219", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Color-Magnitude Diagrams (CMDs) are plots that compare the magnitudes\n(luminosities) of stars in different wavelengths of light (colors). High\nnonlinear correlations among the mass, color, and surface temperature of newly\nformed stars induce a long narrow curved point cloud in a CMD known as the main\nsequence. Aging stars form new CMD groups of red giants and white dwarfs. The\nphysical processes that govern this evolution can be described with\nmathematical models and explored using complex computer models. These\ncalculations are designed to predict the plotted magnitudes as a function of\nparameters of scientific interest, such as stellar age, mass, and metallicity.\nHere, we describe how we use the computer models as a component of a complex\nlikelihood function in a Bayesian analysis that requires sophisticated\ncomputing, corrects for contamination of the data by field stars, accounts for\ncomplications caused by unresolved binary-star systems, and aims to compare\ncompeting physics-based computer models of stellar evolution.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2009 13:53:02 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["van Dyk", "David A.", ""], ["DeGennaro", "Steven", ""], ["Stein", "Nathan", ""], ["Jefferys", "William H.", ""], ["von Hippel", "Ted", ""]]}, {"id": "0905.2592", "submitter": "Emily B. Fox", "authors": "Emily B. Fox, Erik B. Sudderth, Michael I. Jordan, Alan S. Willsky", "title": "A sticky HDP-HMM with application to speaker diarization", "comments": "Published in at http://dx.doi.org/10.1214/10-AOAS395 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 2A, 1020-1056", "doi": "10.1214/10-AOAS395", "report-no": "IMS-AOAS-AOAS395", "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of speaker diarization, the problem of segmenting an\naudio recording of a meeting into temporal segments corresponding to individual\nspeakers. The problem is rendered particularly difficult by the fact that we\nare not allowed to assume knowledge of the number of people participating in\nthe meeting. To address this problem, we take a Bayesian nonparametric approach\nto speaker diarization that builds on the hierarchical Dirichlet process hidden\nMarkov model (HDP-HMM) of Teh et al. [J. Amer. Statist. Assoc. 101 (2006)\n1566--1581]. Although the basic HDP-HMM tends to over-segment the audio\ndata---creating redundant states and rapidly switching among them---we describe\nan augmented HDP-HMM that provides effective control over the switching rate.\nWe also show that this augmentation makes it possible to treat emission\ndistributions nonparametrically. To scale the resulting architecture to\nrealistic diarization problems, we develop a sampling algorithm that employs a\ntruncated approximation of the Dirichlet process to jointly resample the full\nstate sequence, greatly improving mixing rates. Working with a benchmark NIST\ndata set, we show that our Bayesian nonparametric architecture yields\nstate-of-the-art speaker diarization results.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2009 18:06:13 GMT"}, {"version": "v2", "created": "Tue, 19 May 2009 13:26:48 GMT"}, {"version": "v3", "created": "Wed, 11 Aug 2010 22:50:10 GMT"}, {"version": "v4", "created": "Tue, 16 Aug 2011 09:16:24 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Fox", "Emily B.", ""], ["Sudderth", "Erik B.", ""], ["Jordan", "Michael I.", ""], ["Willsky", "Alan S.", ""]]}, {"id": "0905.2819", "submitter": "Yoav Benjamini", "authors": "Yoav Benjamini, Yulia Gavrilov", "title": "A simple forward selection procedure based on false discovery rate\n  control", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS194 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 1, 179-198", "doi": "10.1214/08-AOAS194", "report-no": "IMS-AOAS-AOAS194", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the use of a new false discovery rate (FDR) controlling procedure\nas a model selection penalized method, and compare its performance to that of\nother penalized methods over a wide range of realistic settings: nonorthogonal\ndesign matrices, moderate and large pool of explanatory variables, and both\nsparse and nonsparse models, in the sense that they may include a small and\nlarge fraction of the potential variables (and even all). The comparison is\ndone by a comprehensive simulation study, using a quantitative framework for\nperformance comparisons in the form of empirical minimaxity relative to a\n\"random oracle\": the oracle model selection performance on data dependent\nforward selected family of potential models. We show that FDR based procedures\nhave good performance, and in particular the newly proposed method, emerges as\nhaving empirical minimax performance. Interestingly, using FDR level of 0.05 is\na global best.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2009 07:44:00 GMT"}], "update_date": "2009-05-19", "authors_parsed": [["Benjamini", "Yoav", ""], ["Gavrilov", "Yulia", ""]]}, {"id": "0905.2979", "submitter": "Jo Bovy", "authors": "Jo Bovy, David W. Hogg, Sam T. Roweis", "title": "Extreme deconvolution: Inferring complete distribution functions from\n  noisy, heterogeneous and incomplete observations", "comments": "Published in at http://dx.doi.org/10.1214/10-AOAS439 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 2B, 1657-1677", "doi": "10.1214/10-AOAS439", "report-no": "IMS-AOAS-AOAS439", "categories": "stat.ME astro-ph.GA physics.data-an stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalize the well-known mixtures of Gaussians approach to density\nestimation and the accompanying Expectation--Maximization technique for finding\nthe maximum likelihood parameters of the mixture to the case where each data\npoint carries an individual $d$-dimensional uncertainty covariance and has\nunique missing data properties. This algorithm reconstructs the\nerror-deconvolved or \"underlying\" distribution function common to all samples,\neven when the individual data points are samples from different distributions,\nobtained by convolving the underlying distribution with the heteroskedastic\nuncertainty distribution of the data point and projecting out the missing data\ndirections. We show how this basic algorithm can be extended with conjugate\npriors on all of the model parameters and a \"split-and-merge\" procedure\ndesigned to avoid local maxima of the likelihood. We demonstrate the full\nmethod by applying it to the problem of inferring the three-dimensional\nvelocity distribution of stars near the Sun from noisy two-dimensional,\ntransverse velocity measurements from the Hipparcos satellite.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2009 16:26:26 GMT"}, {"version": "v2", "created": "Fri, 29 Jul 2011 10:31:54 GMT"}], "update_date": "2011-08-01", "authors_parsed": [["Bovy", "Jo", ""], ["Hogg", "David W.", ""], ["Roweis", "Sam T.", ""]]}, {"id": "0905.3217", "submitter": "Patrick J. Wolfe", "authors": "Keigo Hirakawa and Patrick J. Wolfe", "title": "Skellam shrinkage: Wavelet-based intensity estimation for inhomogeneous\n  Poisson data", "comments": "27 pages, 8 figures, slight formatting changes; submitted for\n  publication", "journal-ref": "IEEE Transactions on Information Theory, vol. 58, pp. 1080-1093,\n  2012", "doi": "10.1109/TIT.2011.2165933", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ubiquity of integrating detectors in imaging and other applications\nimplies that a variety of real-world data are well modeled as Poisson random\nvariables whose means are in turn proportional to an underlying vector-valued\nsignal of interest. In this article, we first show how the so-called Skellam\ndistribution arises from the fact that Haar wavelet and filterbank transform\ncoefficients corresponding to measurements of this type are distributed as sums\nand differences of Poisson counts. We then provide two main theorems on Skellam\nshrinkage, one showing the near-optimality of shrinkage in the Bayesian setting\nand the other providing for unbiased risk estimation in a frequentist context.\nThese results serve to yield new estimators in the Haar transform domain,\nincluding an unbiased risk estimate for shrinkage of Haar-Fisz\nvariance-stabilized data, along with accompanying low-complexity algorithms for\ninference. We conclude with a simulation study demonstrating the efficacy of\nour Skellam shrinkage estimators both for the standard univariate wavelet test\nfunctions as well as a variety of test images taken from the image processing\nliterature, confirming that they offer substantial performance improvements\nover existing alternatives.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2009 05:05:14 GMT"}, {"version": "v2", "created": "Fri, 29 May 2009 17:33:23 GMT"}], "update_date": "2012-10-15", "authors_parsed": [["Hirakawa", "Keigo", ""], ["Wolfe", "Patrick J.", ""]]}, {"id": "0905.3463", "submitter": "Carrie A. Hosman", "authors": "Carrie A. Hosman, Ben B. Hansen, Paul W. Holland", "title": "The sensitivity of linear regression coefficients' confidence limits to\n  the omission of a confounder", "comments": "Published in at http://dx.doi.org/10.1214/09-AOAS315 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2010, Vol. 4, No. 2, 849-870", "doi": "10.1214/09-AOAS315", "report-no": "IMS-AOAS-AOAS315", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Omitted variable bias can affect treatment effect estimates obtained from\nobservational data due to the lack of random assignment to treatment groups.\nSensitivity analyses adjust these estimates to quantify the impact of potential\nomitted variables. This paper presents methods of sensitivity analysis to\nadjust interval estimates of treatment effect---both the point estimate and\nstandard error---obtained using multiple linear regression. Central to our\napproach is what we term benchmarking, the use of data to establish reference\npoints for speculation about omitted confounders. The method adapts to\ntreatment effects that may differ by subgroup, to scenarios involving omission\nof multiple variables, and to combinations of covariance adjustment with\npropensity score stratification. We illustrate it using data from an\ninfluential study of health outcomes of patients admitted to critical care.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2009 09:36:56 GMT"}, {"version": "v2", "created": "Tue, 9 Nov 2010 09:36:17 GMT"}], "update_date": "2010-11-10", "authors_parsed": [["Hosman", "Carrie A.", ""], ["Hansen", "Ben B.", ""], ["Holland", "Paul W.", ""]]}, {"id": "0905.3620", "submitter": "Murray Aitkin", "authors": "Murray Aitkin, Charles C. Liu, Tom Chadwick", "title": "Bayesian model comparison and model averaging for small-area estimation", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS205 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 1, 199-221", "doi": "10.1214/08-AOAS205", "report-no": "IMS-AOAS-AOAS205", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers small-area estimation with lung cancer mortality data,\nand discusses the choice of upper-level model for the variation over areas.\nInference about the random effects for the areas may depend strongly on the\nchoice of this model, but this choice is not a straightforward matter. We give\na general methodology for both evaluating the data evidence for different\nmodels and averaging over plausible models to give robust area effect\ndistributions. We reanalyze the data of Tsutakawa [Biometrics 41 (1985) 69--79]\non lung cancer mortality rates in Missouri cities, and show the differences in\nconclusions about the city rates from this methodology.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2009 07:30:29 GMT"}], "update_date": "2009-05-25", "authors_parsed": [["Aitkin", "Murray", ""], ["Liu", "Charles C.", ""], ["Chadwick", "Tom", ""]]}, {"id": "0905.4691", "submitter": "Joseph Hall", "authors": "Joseph Lorenzo Hall (1 and 2), Luke W. Miratrix (3), Philip B. Stark\n  (3), Melvin Briones (4), Elaine Ginnold (4), Freddie Oakley (5), Martin\n  Peaden (6), Gail Pellerin (6), Tom Stanionis (5), and Tricia Webber (6) ((1)\n  University of California, Berkeley; School of Information, (2) Princeton\n  University; Center for Information Technology Policy, (3) University of\n  California, Berkeley; Department of Statistics, (4) Marin County, California;\n  Registrar of Voters, (5) Yolo County, California; County Clerk/Recorder, (6)\n  Santa Cruz County, California; County Clerk)", "title": "Implementing Risk-Limiting Post-Election Audits in California", "comments": "Accepted to the Electronic Voting Technology Workshop/Workshop on\n  Trustworthy Elections 2009 (EVT/WOTE '09),\n  http://www.usenix.org/events/evtwote09/", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Risk-limiting post-election audits limit the chance of certifying an\nelectoral outcome if the outcome is not what a full hand count would show.\nBuilding on previous work, we report on pilot risk-limiting audits in four\nelections during 2008 in three California counties: one during the February\n2008 Primary Election in Marin County and three during the November 2008\nGeneral Elections in Marin, Santa Cruz and Yolo Counties. We explain what makes\nan audit risk-limiting and how existing and proposed laws fall short. We\ndiscuss the differences among our four pilot audits. We identify challenges to\npractical, efficient risk-limiting audits and conclude that current approaches\nare too complex to be used routinely on a large scale. One important logistical\nbottleneck is the difficulty of exporting data from commercial election\nmanagement systems in a format amenable to audit calculations. Finally, we\npropose a bare-bones risk-limiting audit that is less efficient than these\npilot audits, but avoids many practical problems.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2009 16:15:36 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2009 00:56:05 GMT"}, {"version": "v3", "created": "Wed, 1 Jul 2009 20:19:30 GMT"}, {"version": "v4", "created": "Fri, 10 Jul 2009 22:50:04 GMT"}], "update_date": "2010-01-15", "authors_parsed": [["Hall", "Joseph Lorenzo", "", "1 and 2"], ["Miratrix", "Luke W.", ""], ["Stark", "Philip B.", ""], ["Briones", "Melvin", ""], ["Ginnold", "Elaine", ""], ["Oakley", "Freddie", ""], ["Peaden", "Martin", ""], ["Pellerin", "Gail", ""], ["Stanionis", "Tom", ""], ["Webber", "Tricia", ""]]}]