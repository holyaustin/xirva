[{"id": "1507.00123", "submitter": "Ilya Soloveychik", "authors": "Ilya Soloveychik and Ami Wiesel", "title": "Joint Covariance Estimation with Mutual Linear Structure", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2015.2502556", "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of joint estimation of structured covariance\nmatrices. Assuming the structure is unknown, estimation is achieved using\nheterogeneous training sets. Namely, given groups of measurements coming from\ncentered populations with different covariances, our aim is to determine the\nmutual structure of these covariance matrices and estimate them. Supposing that\nthe covariances span a low dimensional affine subspace in the space of\nsymmetric matrices, we develop a new efficient algorithm discovering the\nstructure and using it to improve the estimation. Our technique is based on the\napplication of principal component analysis in the matrix space. We also derive\nan upper performance bound of the proposed algorithm in the Gaussian scenario\nand compare it with the Cramer-Rao lower bound. Numerical simulations are\npresented to illustrate the performance benefits of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2015 06:54:11 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Soloveychik", "Ilya", ""], ["Wiesel", "Ami", ""]]}, {"id": "1507.00167", "submitter": "Emilie Devijver", "authors": "Emilie Devijver (LM-Orsay, SELECT), Yannig Goude (EDF R\\&D),\n  Jean-Michel Poggi (LM-Orsay)", "title": "Clustering electricity consumers using high-dimensional regression\n  mixture models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive informations about individual (household, small and medium\nenterprise) consumption are now provided with new metering technologies and the\nsmart grid. Two major exploitations of these data are load profiling and\nforecasting at different scales on the grid. Customer segmentation based on\nload classification is a natural approach for these purposes. We propose here a\nnew methodology based on mixture of high-dimensional regression models. The\nnovelty of our approach is that we focus on uncovering classes or clusters\ncorresponding to different regression models. As a consequence, these classes\ncould then be exploited for profiling as well as forecasting in each class or\nfor bottom-up forecasts in a unified view. We consider a real dataset of Irish\nindividual consumers of 4,225 meters, each with 48 half-hourly meter reads per\nday over 1 year: from 1st January 2010 up to 31st December 2010, to demonstrate\nthe feasibility of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2015 09:51:40 GMT"}], "update_date": "2015-07-02", "authors_parsed": [["Devijver", "Emilie", "", "LM-Orsay, SELECT"], ["Goude", "Yannig", "", "EDF R\\&D"], ["Poggi", "Jean-Michel", "", "LM-Orsay"]]}, {"id": "1507.00171", "submitter": "Kevin Bleakley", "authors": "G\\'erard Biau (LSTA), Kevin Bleakley (LMO, SELECT), Benoit Cadre (ENS\n  Rennes, UEB, IRMAR)", "title": "The Statistical Performance of Collaborative Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical analysis of massive and complex data sets will require the\ndevelopment of algorithms that depend on distributed computing and\ncollaborative inference. Inspired by this, we propose a collaborative framework\nthat aims to estimate the unknown mean $\\theta$ of a random variable $X$. In\nthe model we present, a certain number of calculation units, distributed across\na communication network represented by a graph, participate in the estimation\nof $\\theta$ by sequentially receiving independent data from $X$ while\nexchanging messages via a stochastic matrix $A$ defined over the graph. We give\nprecise conditions on the matrix $A$ under which the statistical precision of\nthe individual units is comparable to that of a (gold standard) virtual\ncentralized estimate, even though each unit does not have access to all of the\ndata. We show in particular the fundamental role played by both the non-trivial\neigenvalues of $A$ and the Ramanujan class of expander graphs, which provide\nremarkable performance for moderate algorithmic cost.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2015 10:05:29 GMT"}], "update_date": "2015-07-02", "authors_parsed": [["Biau", "G\u00e9rard", "", "LSTA"], ["Bleakley", "Kevin", "", "LMO, SELECT"], ["Cadre", "Benoit", "", "ENS\n  Rennes, UEB, IRMAR"]]}, {"id": "1507.00225", "submitter": "Ricardo Ehlers", "authors": "Taciana K. O. Shimizu, Francisco Louzada, Adriano K. Suzuki, Ricardo\n  S. Ehlers", "title": "Modeling Compositional Regression with uncorrelated and correlated\n  errors: a Bayesian approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compositional data consist of known compositions vectors whose components are\npositive and defined in the interval (0,1) representing proportions or\nfractions of a \"whole\". The sum of these components must be equal to one.\nCompositional data is present in different knowledge areas, as in geology,\neconomy, medicine among many others. In this paper, we introduce a Bayesian\nanalysis for compositional regression applying additive log-ratio (ALR)\ntransformation and assuming uncorrelated and correlated errors. The Bayesian\ninference procedure based on Markov Chain Monte Carlo Methods (MCMC). The\nmethodology is illustrated on an artificial and a real data set of volleyball.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2015 13:24:27 GMT"}], "update_date": "2015-07-02", "authors_parsed": [["Shimizu", "Taciana K. O.", ""], ["Louzada", "Francisco", ""], ["Suzuki", "Adriano K.", ""], ["Ehlers", "Ricardo S.", ""]]}, {"id": "1507.00280", "submitter": "David Hallac", "authors": "David Hallac, Jure Leskovec, Stephen Boyd", "title": "Network Lasso: Clustering and Optimization in Large Graphs", "comments": null, "journal-ref": null, "doi": "10.1145/2783258.2783313", "report-no": null, "categories": "cs.SI math.OC stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convex optimization is an essential tool for modern data analysis, as it\nprovides a framework to formulate and solve many problems in machine learning\nand data mining. However, general convex optimization solvers do not scale\nwell, and scalable solvers are often specialized to only work on a narrow class\nof problems. Therefore, there is a need for simple, scalable algorithms that\ncan solve many common optimization problems. In this paper, we introduce the\n\\emph{network lasso}, a generalization of the group lasso to a network setting\nthat allows for simultaneous clustering and optimization on graphs. We develop\nan algorithm based on the Alternating Direction Method of Multipliers (ADMM) to\nsolve this problem in a distributed and scalable manner, which allows for\nguaranteed global convergence even on large graphs. We also examine a\nnon-convex extension of this approach. We then demonstrate that many types of\nproblems can be expressed in our framework. We focus on three in particular -\nbinary classification, predicting housing prices, and event detection in time\nseries data - comparing the network lasso to baseline approaches and showing\nthat it is both a fast and accurate method of solving large optimization\nproblems.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2015 16:25:38 GMT"}], "update_date": "2015-07-02", "authors_parsed": [["Hallac", "David", ""], ["Leskovec", "Jure", ""], ["Boyd", "Stephen", ""]]}, {"id": "1507.00363", "submitter": "Zhengyi Zhou", "authors": "Zhengyi Zhou, David S. Matteson", "title": "Predicting Melbourne Ambulance Demand using Kernel Warping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting ambulance demand accurately in fine resolutions in space and time\nis critical for ambulance fleet management and dynamic deployment. Typical\nchallenges include data sparsity at high resolutions and the need to respect\ncomplex urban spatial domains. To provide spatial density predictions for\nambulance demand in Melbourne, Australia as it varies over hourly intervals, we\npropose a predictive spatio-temporal kernel warping method. To predict for each\nhour, we build a kernel density estimator on a sparse set of the most similar\ndata from relevant past time periods (labeled data), but warp these kernels to\na larger set of past data irregardless of time periods (point cloud). The point\ncloud represents the spatial structure and geographical characteristics of\nMelbourne, including complex boundaries, road networks, and neighborhoods.\nBorrowing from manifold learning, kernel warping is performed through a graph\nLaplacian of the point cloud and can be interpreted as a regularization\ntowards, and a prior imposed, for spatial features. Kernel bandwidth and degree\nof warping are efficiently estimated via cross-validation, and can be made\ntime- and/or location-specific. Our proposed model gives significantly more\naccurate predictions compared to a current industry practice, an unwarped\nkernel density estimation, and a time-varying Gaussian mixture model.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2015 20:19:26 GMT"}], "update_date": "2015-07-03", "authors_parsed": [["Zhou", "Zhengyi", ""], ["Matteson", "David S.", ""]]}, {"id": "1507.00364", "submitter": "Zhengyi Zhou", "authors": "Zhengyi Zhou, David S. Matteson", "title": "Predicting Ambulance Demand: a Spatio-Temporal Kernel Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting ambulance demand accurately at fine time and location scales is\ncritical for ambulance fleet management and dynamic deployment. Large-scale\ndatasets in this setting typically exhibit complex spatio-temporal dynamics and\nsparsity at high resolutions. We propose a predictive method using\nspatio-temporal kernel density estimation (stKDE) to address these challenges,\nand provide spatial density predictions for ambulance demand in Toronto, Canada\nas it varies over hourly intervals. Specifically, we weight the spatial kernel\nof each historical observation by its informativeness to the current predictive\ntask. We construct spatio-temporal weight functions to incorporate various\ntemporal and spatial patterns in ambulance demand, including location-specific\nseasonalities and short-term serial dependence. This allows us to draw out the\nmost helpful historical data, and exploit spatio-temporal patterns in the data\nfor accurate and fast predictions. We further provide efficient estimation and\ncustomizable prediction procedures. stKDE is easy to use and interpret by\nnon-specialized personnel from the emergency medical service industry. It also\nhas significantly higher statistical accuracy than the current industry\npractice, with a comparable amount of computational expense.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2015 20:27:29 GMT"}], "update_date": "2015-07-03", "authors_parsed": [["Zhou", "Zhengyi", ""], ["Matteson", "David S.", ""]]}, {"id": "1507.00565", "submitter": "Alexandra Schmidt", "authors": "Alexandra M. Schmidt, Caroline P. de Moraes and Helio S. Migon", "title": "A Hierarchical Dynamic Beta Regression Model of School Performance in\n  the Brazilian Mathematical Olympiads for Public Schools", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Brazilian Mathematical Olympiads for Public Schools (OBMEP) is held every\nyear since 2005. In the 2013 edition there were over 47,000 schools registered\ninvolving nearly 19.2 million students. The Brazilian public educational system\nis structured into three administrative levels: federal, state and municipal.\nStudents participating in the OBMEP come from three educational levels, two in\nprimary and one in secondary school. We aim at studying the performance of\nBrazilian public schools which have been taking part of the OBMEP from 2006\nuntil 2013. We propose a standardization of the mean scores of schools per year\nand educational level which is modeled through a hierarchical dynamic beta\nregression model. Both the mean and precision of the beta distribution are\nmodeled as a function of covariates whose effects evolve smoothly with time.\nResults show that, regardless of the educational level, federal schools have\nbetter performance than municipal or state schools. The mean performance of\nschools increases with the human development index (HDI) of the municipality\nthe school is located in. Moreover, the difference in mean performance between\nfederal and state or municipal schools tends to increase with the HDI. Schools\nwith higher proportion of boys tend to have better mean performance in the\nsecond and third educational levels of OBMEP.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 13:10:09 GMT"}], "update_date": "2015-07-03", "authors_parsed": [["Schmidt", "Alexandra M.", ""], ["de Moraes", "Caroline P.", ""], ["Migon", "Helio S.", ""]]}, {"id": "1507.00634", "submitter": "Ioannis Ntzoufras", "authors": "Vasileios Manasis, Ioannis Ntzoufras, James Reade", "title": "Competitive balance measures and the Uncertainty of Outcome Hypothesis\n  in European football", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Competitive balance is an important concept for professional sports and one\nof the key issues that European football has to address in order to ensure its\nlong-term prosperity. However, the quantification of competitive balance is not\nan easy task. The difficulties are mainly associated with its\nmulti-dimensionality character as well as the structure of each particular\nsport. This article uses data from eight domestic leagues over 60 years to\nidentify the best index for a holistic view of competitive balance in European\nfootball. The findings support the longstanding Uncertainty of Outcome\nHypothesis using indices designed for the important three identified levels of\ncompetition and offering a weighting pattern for ranking places. Important\nconclusions may be derived concerning the relative importance of different\naspects of competitive balance depending on the specific features of the best\nindex.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 15:44:34 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 10:09:03 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Manasis", "Vasileios", ""], ["Ntzoufras", "Ioannis", ""], ["Reade", "James", ""]]}, {"id": "1507.00683", "submitter": "Andrew Poppick", "authors": "Andrew Poppick, David J. McInerney, Elisabeth J. Moyer, and Michael L.\n  Stein", "title": "Temperatures in transient climates: improved methods for simulations\n  with evolving temporal covariances", "comments": "35 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future climate change impacts depend on temperatures not only through changes\nin their means but also through changes in their variability. General\ncirculation models (GCMs) predict changes in both means and variability;\nhowever, GCM output should not be used directly as simulations for impacts\nassessments because GCMs do not fully reproduce present-day temperature\ndistributions. This paper addresses an ensuing need for simulations of future\ntemperatures that combine both the observational record and GCM projections of\nchanges in means and temporal covariances. Our perspective is that such\nsimulations should be based on transforming observations to account for GCM\nprojected changes, in contrast to methods that transform GCM output to account\nfor discrepancies with observations. Our methodology is designed for simulating\ntransient (non-stationary) climates, which are evolving in response to changes\nin CO$_2$ concentrations (as is the Earth at present). This work builds on\npreviously described methods for simulating equilibrium (stationary) climates.\nSince the proposed simulation relies on GCM projected changes in covariance, we\ndescribe a statistical model for the evolution of temporal covariances in a GCM\nunder future forcing scenarios, and apply this model to an ensemble of runs\nfrom one GCM, CCSM3. We find that, at least in CCSM3, changes in the local\ncovariance structure can be explained as a function of the regional mean change\nin temperature and the rate of change of warming. This feature means that the\nstatistical model can be used to emulate the evolving covariance structure of\nGCM temperatures under scenarios for which the GCM has not been run. When\ncombined with an emulator for mean temperature, our methodology can simulate\nevolving temperatures under such scenarios, in a way that accounts for\nprojections of changes while still retaining fidelity with the observational\nrecord.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 18:38:17 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2015 17:53:07 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Poppick", "Andrew", ""], ["McInerney", "David J.", ""], ["Moyer", "Elisabeth J.", ""], ["Stein", "Michael L.", ""]]}, {"id": "1507.00749", "submitter": "Robert Wolpert", "authors": "Robert L. Wolpert", "title": "ACME: A Partially Periodic Estimator of Avian & Chiropteran Mortality at\n  Wind Turbines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the mortality of birds and bats at wind turbines based on periodic\ncarcass counts is challenging because carcasses may be removed by scavengers or\nmay be missed in investigators' searches, leading to undercounting. Existing\nmortality estimators intended to correct for this offer wildly different\nestimates when search intervals are short.\n  We introduce a new estimator that includes many existing ones as special\ncases but extends and improves them in three ways to reflect phenomena\ndiscovered in the field:\n  * Decreasing removal rate by scavengers as carcasses age;\n  * Diminishing proficiency of Field Technicians in discovering carcasses\n  as they age;\n  * Possibility that some (but not all) carcasses arriving in earlier search\n  periods may be discovered in the current period. It is this feature\n  that makes the new estimator \"partially periodic\".\n  Both point estimates and 50% and 90% Objective Bayes interval estimates are\nprovided for mortality. The new ACME mortality estimator will significantly\nimprove the accuracy and credibility of mortality estimates for birds and bats\nnear wind turbines.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 20:26:17 GMT"}], "update_date": "2015-07-06", "authors_parsed": [["Wolpert", "Robert L.", ""]]}, {"id": "1507.01135", "submitter": "Yubin Park", "authors": "Yubin Park, Rajiv Khanna, Joydeep Ghosh, Daniel Mihalko", "title": "DPM: A State Space Model for Large-Scale Direct Marketing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel statistical model to answer three challenges in direct\nmarketing: which channel to use, which offer to make, and when to offer. There\nare several potential applications for the proposed model, for example,\ndeveloping personalized marketing strategies and monitoring members' needs.\nFurthermore, the results from the model can complement and can be integrated\nwith other existing models.\n  The proposed model, named Dynamic Propensity Model, is a latent variable time\nseries model that utilizes both marketing and purchase histories of a customer.\nThe latent variable in the model represents the customer's propensity to buy a\nproduct. The propensity derives from purchases and other observable responses.\nMarketing touches increase a member's propensity, and propensity score\nattenuates and propagates over time as governed by data-driven parameters. To\nestimate the parameters of the model, a new statistical methodology has been\ndeveloped. This methodology makes use of particle methods with a stochastic\ngradient descent approach, resulting in fast estimation of the model\ncoefficients even from big datasets. The model is validated using six months'\nmarketing records from one of the largest insurance companies in the U.S.\nExperimental results indicate that the effects of marketing touches vary\ndepending on both channels and products. We compare the predictive performance\nof the proposed model with lagged variable logistic regression. Limitations and\nextensions of the proposed algorithm are also discussed.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2015 19:10:02 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Park", "Yubin", ""], ["Khanna", "Rajiv", ""], ["Ghosh", "Joydeep", ""], ["Mihalko", "Daniel", ""]]}, {"id": "1507.01242", "submitter": "Maria DeYoreo", "authors": "Maria DeYoreo and Athanasios Kottas", "title": "Modeling for Dynamic Ordinal Regression Relationships: An Application to\n  Estimating Maturity of Rockfish in California", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a Bayesian nonparametric framework for modeling ordinal regression\nrelationships which evolve in discrete time. The motivating application\ninvolves a key problem in fisheries research on estimating dynamically evolving\nrelationships between age, length and maturity, the latter recorded on an\nordinal scale. The methodology builds from nonparametric mixture modeling for\nthe joint stochastic mechanism of covariates and latent continuous responses.\nThis approach yields highly flexible inference for ordinal regression functions\nwhile at the same time avoiding the computational challenges of parametric\nmodels. A novel dependent Dirichlet process prior for time-dependent mixing\ndistributions extends the model to the dynamic setting. The methodology is used\nfor a detailed study of relationships between maturity, age, and length for\nChilipepper rockfish, using data collected over 15 years along the coast of\nCalifornia.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2015 16:44:23 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["DeYoreo", "Maria", ""], ["Kottas", "Athanasios", ""]]}, {"id": "1507.01397", "submitter": "Sarah Lemler", "authors": "Agathe Guilloux (LSTA), Sarah Lemler (LaMME), Marie-Luce Taupin\n  (LaMME, Unit\\'e MIAJ)", "title": "Adaptive kernel estimation of the baseline function in the Cox model,\n  with high-dimensional covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this article is to propose a novel kernel estimator of the\nbaseline function in a general high-dimensional Cox model, for which we derive\nnon-asymptotic rates of convergence. To construct our estimator, we first\nestimate the regression parameter in the Cox model via a Lasso procedure. We\nthen plug this estimator into the classical kernel estimator of the baseline\nfunction, obtained by smoothing the so-called Breslow estimator of the\ncumulative baseline function. We propose and study an adaptive procedure for\nselecting the bandwidth, in the spirit of Gold-enshluger and Lepski (2011). We\nstate non-asymptotic oracle inequalities for the final estimator, which reveal\nthe reduction of the rates of convergence when the dimension of the covariates\ngrows.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2015 11:30:27 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Guilloux", "Agathe", "", "LSTA"], ["Lemler", "Sarah", "", "LaMME"], ["Taupin", "Marie-Luce", "", "LaMME, Unit\u00e9 MIAJ"]]}, {"id": "1507.01452", "submitter": "Alexander Holiday", "authors": "Assimakis A. Kattis, Alexander Holiday, Ana-Andreea Stoica, Ioannis G.\n  Kevrekidis", "title": "Modeling epidemics on adaptively evolving networks: a data-mining\n  perspective", "comments": "24 pages, 8 figures, submitted to Virulence", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exploration of epidemic dynamics on dynamically evolving (\"adaptive\")\nnetworks poses nontrivial challenges to the modeler, such as the determination\nof a small number of informative statistics of the detailed network state (that\nis, a few \"good observables\") that usefully summarize the overall (macroscopic,\nsystems level) behavior. Trying to obtain reduced, small size, accurate models\nin terms of these few statistical observables - that is, coarse-graining the\nfull network epidemic model to a small but useful macroscopic one - is even\nmore daunting. Here we describe a data-based approach to solving the first\nchallenge: the detection of a few informative collective observables of the\ndetailed epidemic dynamics. This will be accomplished through Diffusion Maps, a\nrecently developed data-mining technique. We illustrate the approach through\nsimulations of a simple mathematical model of epidemics on a network: a model\nknown to exhibit complex temporal dynamics. We will discuss potential\nextensions of the approach, as well as possible shortcomings.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2015 21:35:00 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Kattis", "Assimakis A.", ""], ["Holiday", "Alexander", ""], ["Stoica", "Ana-Andreea", ""], ["Kevrekidis", "Ioannis G.", ""]]}, {"id": "1507.01454", "submitter": "Katharine Turner", "authors": "Vanessa Robins, Katharine Turner", "title": "Principal Component Analysis of Persistent Homology Rank Functions with\n  case studies of Spatial Point Patterns, Sphere Packing and Colloids", "comments": "25 pages, 15 figures", "journal-ref": null, "doi": "10.1016/j.physd.2016.03.007", "report-no": null, "categories": "math.ST math.AT stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Persistent homology, while ostensibly measuring changes in topology, captures\nmultiscale geometrical information. It is a natural tool for the analysis of\npoint patterns. In this paper we explore the statistical power of the\n(persistent homology) rank functions. For a point pattern $X$ we construct a\nfiltration of spaces by taking the union of balls of radius $a$ centered on\npoints in $X$, $X_a = \\cup_{x\\in X}B(x,a)$. The rank function\n${\\beta}_k(X):{\\{(a,b)\\in \\mathbb{R}^2: a\\leq b\\}} \\to \\mathbb{R}$ is then\ndefined by ${\\beta}_k(X)(a,b) = rank ( \\iota_*:H_k(X_a) \\to H_k(X_b))$ where\n$\\iota_*$ is the induced map on homology from the inclusion map on spaces. We\nconsider the rank functions as lying in a Hilbert space and show that under\nreasonable conditions the rank functions from multiple simulations or\nexperiments will lie in an affine subspace. This enables us to perform\nfunctional principal component analysis which we apply to experimental data\nfrom colloids at different effective temperatures and of sphere packings with\ndifferent volume fractions. We also investigate the potential of rank functions\nin providing a test of complete spatial randomness of 2D point patterns using\nthe distances to an empirically computed mean rank function of binomial point\npatterns in the unit square.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2015 13:41:57 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Robins", "Vanessa", ""], ["Turner", "Katharine", ""]]}, {"id": "1507.01816", "submitter": "Hugh Chipman", "authors": "Lu Xin and Mu Zhu and Hugh Chipman", "title": "A Continuous-time Stochastic Block Model for Basketball Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For professional basketball, finding valuable and suitable players is the key\nto building a winning team. To deal with such challenges, basketball managers,\nscouts and coaches are increasingly turning to analytics. Objective evaluation\nof players and teams has always been the top goal of basketball analytics.\nTypical statistical analytics mainly focuses on the box score and has developed\nvarious metrics. In spite of the more and more advanced methods, metrics built\nupon box score statistics provide limited information about how players\ninteract with each other. Two players with similar box scores may deliver\ndistinct team plays. Thus professional basketball scouts have to watch real\ngames to evaluate players. Live scouting is effective, but suffers from\ninefficiency and subjectivity. In this paper, we go beyond the static box score\nand model basketball games as dynamic networks. The proposed Continuous-time\nStochastic Block Model clusters the players according to their playing style\nand performance. The model provides cluster-specific estimates of the\neffectiveness of players at scoring, rebounding, stealing, etc, and also\ncaptures player interaction patterns within and between clusters. By clustering\nsimilar players together, the model can help basketball scouts to narrow down\nthe search space. Moreover, the model is able to reveal the subtle differences\nin the offensive strategies of different teams. An application to NBA\nbasketball games illustrates the performance of the model.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2015 20:14:45 GMT"}, {"version": "v2", "created": "Sat, 23 Jul 2016 10:03:14 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Xin", "Lu", ""], ["Zhu", "Mu", ""], ["Chipman", "Hugh", ""]]}, {"id": "1507.01853", "submitter": "Isabella Gollini", "authors": "Isabella Gollini and Jonathan Rougier", "title": "Rapidly bounding the exceedance probabilities of high aggregate losses", "comments": "33 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of assessing the righthand tail of an insurer's loss\ndistribution for some specified period, such as a year. We present and analyse\nsix different approaches: four upper bounds, and two approximations. We examine\nthese approaches under a variety of conditions, using a large event loss table\nfor US hurricanes. For its combination of tightness and computational speed, we\nfavour the Moment bound. We also consider the appropriate size of Monte Carlo\nsimulations, and the imposition of a cap on single event losses. We strongly\nfavour the Gamma distribution as a flexible model for single event losses, for\nits tractable form in all of the methods we analyse, its generalisability, and\nbecause of the ease with which a cap on losses can be incorporated.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2015 15:53:34 GMT"}], "update_date": "2015-07-08", "authors_parsed": [["Gollini", "Isabella", ""], ["Rougier", "Jonathan", ""]]}, {"id": "1507.01933", "submitter": "Zhixiang Lin", "authors": "Zhixiang Lin, Tao Wang, Can Yang and Hongyu Zhao", "title": "On Joint Estimation of Gaussian Graphical Models for Spatial and\n  Temporal Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we first propose a Bayesian neighborhood selection method to\nestimate Gaussian Graphical Models (GGMs). We show the graph selection\nconsistency of this method in the sense that the posterior probability of the\ntrue model converges to one. When there are multiple groups of data available,\ninstead of estimating the networks independently for each group, joint\nestimation of the networks may utilize the shared information among groups and\nlead to improved estimation for each individual network. Our method is extended\nto jointly estimate GGMs in multiple groups of data with complex structures,\nincluding spatial data, temporal data and data with both spatial and temporal\nstructures. Markov random field (MRF) models are used to efficiently\nincorporate the complex data structures. We develop and implement an efficient\nalgorithm for statistical inference that enables parallel computing. Simulation\nstudies suggest that our approach achieves better accuracy in network\nestimation compared with methods not incorporating spatial and temporal\ndependencies when there are shared structures among the networks, and that it\nperforms comparably well otherwise. Finally, we illustrate our method using the\nhuman brain gene expression microarray dataset, where the expression levels of\ngenes are measured in different brain regions across multiple time periods.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2015 19:55:33 GMT"}], "update_date": "2015-07-08", "authors_parsed": [["Lin", "Zhixiang", ""], ["Wang", "Tao", ""], ["Yang", "Can", ""], ["Zhao", "Hongyu", ""]]}, {"id": "1507.01982", "submitter": "Bo Li", "authors": "Bo Li, Athina P. Petropulu, and Wade Trappe", "title": "Optimum Design for Coexistence Between Matrix Completion Based MIMO\n  Radars and a MIMO Communication System", "comments": "31 pages, 15 figures", "journal-ref": null, "doi": "10.1109/TSP.2016.2569479", "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently proposed multiple input multiple output radars based on matrix\ncompletion (MIMO-MC) employ sparse sampling to reduce the amount of data that\nneed to be forwarded to the radar fusion center, and as such enable savings in\ncommunication power and bandwidth. This paper proposes designs that optimize\nthe sharing of spectrum between a MIMO-MC radar and a communication system, so\nthat the latter interferes minimally with the former. First, the communication\nsystem transmit covariance matrix is designed to minimize the effective\ninterference power (EIP) to the radar receiver, while maintaining certain\naverage capacity and transmit power for the communication system. Two\napproaches are proposed, namely a noncooperative and a cooperative approach,\nwith the latter being applicable when the radar sampling scheme is known at the\ncommunication system. Second, a joint design of the communication transmit\ncovariance matrix and the MIMO-MC radar sampling scheme is proposed, which\nachieves even further EIP reduction.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2015 22:36:51 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Li", "Bo", ""], ["Petropulu", "Athina P.", ""], ["Trappe", "Wade", ""]]}, {"id": "1507.02216", "submitter": "Cecile Chenot", "authors": "Cecile Chenot, Jerome Bobin and Jeremy Rapin", "title": "Robust Sparse Blind Source Separation", "comments": "Submitted to IEEE Signal Processing Letters", "journal-ref": null, "doi": "10.1109/LSP.2015.2463232", "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind Source Separation is a widely used technique to analyze multichannel\ndata. In many real-world applications, its results can be significantly\nhampered by the presence of unknown outliers. In this paper, a novel algorithm\ncoined rGMCA (robust Generalized Morphological Component Analysis) is\nintroduced to retrieve sparse sources in the presence of outliers. It\nexplicitly estimates the sources, the mixing matrix, and the outliers. It also\ntakes advantage of the estimation of the outliers to further implement a\nweighting scheme, which provides a highly robust separation procedure.\nNumerical experiments demonstrate the efficiency of rGMCA to estimate the\nmixing matrix in comparison with standard BSS techniques.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 16:50:26 GMT"}, {"version": "v2", "created": "Mon, 25 Apr 2016 12:06:29 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Chenot", "Cecile", ""], ["Bobin", "Jerome", ""], ["Rapin", "Jeremy", ""]]}, {"id": "1507.02219", "submitter": "Fotini Pallikari", "authors": "Fotini Pallikari", "title": "Investigating the nature of intangible brain-machine interaction", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The hypothesis that direct human intention can modulate the concurrent\noutcomes of a stochastic process has been under test for over 35 years,\nsurrounded by inconclusive evidence and a great amount of ambiguity. An\nincreased interest has recently and prematurely risen concerning possible\nscientific applications of such effects. The amassed information around the\npurported phenomena, as presented in a meta-analysis, is thoroughly re-examined\nhere through the application of several recognised mathematical tools. It is\nshown that there exists no validation of the hypothesis of brain-machine\ninteraction precisely in absence of interface devices. It is concluded that any\nevidence in favour of the alleged intangible brain-machine effects, must have\nresulted from unintended errors during data collection and treatment, known as\nthe experimenter expectancy effect.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 16:56:41 GMT"}], "update_date": "2015-07-10", "authors_parsed": [["Pallikari", "Fotini", ""]]}, {"id": "1507.02511", "submitter": "Yoann Altmann", "authors": "Yoann Altmann, Ximing Ren, Aongus McCarthy, Gerald S. Buller, Steve\n  McLaughlin", "title": "Lidar waveform based analysis of depth images constructed using sparse\n  single-photon data", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2016.2526784", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new Bayesian model and algorithm used for depth and\nintensity profiling using full waveforms from the time-correlated single photon\ncounting (TCSPC) measurement in the limit of very low photon counts. The model\nproposed represents each Lidar waveform as a combination of a known impulse\nresponse, weighted by the target intensity, and an unknown constant background,\ncorrupted by Poisson noise. Prior knowledge about the problem is embedded in a\nhierarchical model that describes the dependence structure between the model\nparameters and their constraints. In particular, a gamma Markov random field\n(MRF) is used to model the joint distribution of the target intensity, and a\nsecond MRF is used to model the distribution of the target depth, which are\nboth expected to exhibit significant spatial correlations. An adaptive Markov\nchain Monte Carlo algorithm is then proposed to compute the Bayesian estimates\nof interest and perform Bayesian inference. This algorithm is equipped with a\nstochastic optimization adaptation mechanism that automatically adjusts the\nparameters of the MRFs by maximum marginal likelihood estimation. Finally, the\nbenefits of the proposed methodology are demonstrated through a serie of\nexperiments using real data.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 14:08:45 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Altmann", "Yoann", ""], ["Ren", "Ximing", ""], ["McCarthy", "Aongus", ""], ["Buller", "Gerald S.", ""], ["McLaughlin", "Steve", ""]]}, {"id": "1507.02513", "submitter": "Anna Heath", "authors": "Anna Heath and Ioanna Manolopoulou and Gianluca Baio", "title": "A Review of Methods for the Analysis of the Expected Value of\n  Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over recent years Value of Information analysis has become more widespread in\nhealth-economic evaluations, specifically as a tool to perform Probabilistic\nSensitivity Analysis. This is largely due to methodological advancements\nallowing for the fast computation of a typical summary known as the Expected\nValue of Partial Perfect Information (EVPPI). A recent review discussed some\nestimations method for calculating the EVPPI but as the research has been\nactive over the intervening years this review does not discuss some key\nestimation methods. Therefore, this paper presents a comprehensive review of\nthese new methods. We begin by providing the technical details of these\ncomputation methods. We then present a case study in order to compare the\nestimation performance of these new methods. We conclude that the most recent\ndevelopment based on non-parametric regression offers the best method for\ncalculating the EVPPI efficiently. This means that the EVPPI can now be used\npractically in health economic evaluations, especially as all the methods are\ndeveloped in parallel with R\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 14:10:41 GMT"}], "update_date": "2015-07-10", "authors_parsed": [["Heath", "Anna", ""], ["Manolopoulou", "Ioanna", ""], ["Baio", "Gianluca", ""]]}, {"id": "1507.02739", "submitter": "Shira Mitchell", "authors": "Shira Mitchell, Rebecca Ross, Susanna Makela, Elizabeth A. Stuart, Avi\n  Feller, Alan M. Zaslavsky, Andrew Gelman", "title": "Design of the Millennium Villages Project Sampling Plan: a simulation\n  study for a multi-module survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Millennium Villages Project (MVP) is a ten-year integrated rural\ndevelopment project implemented in ten sub-Saharan African sites. At its\nconclusion we will conduct an evaluation of its causal effect on a variety of\ndevelopment outcomes, measured via household surveys in treatment and\ncomparison areas. Outcomes are measured by six survey modules, with sample\nsizes for each demographic group determined by budget, logistics, and the\ngroup's vulnerability. We design a sampling plan that aims to reduce effort for\nsurvey enumerators and maximize precision for all outcomes. We propose\ntwo-stage sampling designs, sampling households at the first stage, followed by\na second stage sample that differs across demographic groups. Two-stage designs\nare usually constructed by simple random sampling (SRS) of households and\nproportional within-household sampling, or probability proportional to size\nsampling (PPS) of households with fixed sampling within each. No measure of\nhousehold size is proportional for all demographic groups, putting PPS schemes\nat a disadvantage. The SRS schemes have the disadvantage that multiple\nindividuals sampled per household decreases efficiency due to intra-household\ncorrelation. We conduct a simulation study (using both design- and model-based\nsurvey inference) to understand these tradeoffs and recommend a sampling plan\nfor the Millennium Villages Project. Similar design issues arise in other\nstudies with surveys that target different demographic groups.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 23:20:51 GMT"}], "update_date": "2015-07-13", "authors_parsed": [["Mitchell", "Shira", ""], ["Ross", "Rebecca", ""], ["Makela", "Susanna", ""], ["Stuart", "Elizabeth A.", ""], ["Feller", "Avi", ""], ["Zaslavsky", "Alan M.", ""], ["Gelman", "Andrew", ""]]}, {"id": "1507.02741", "submitter": "Harrison Quick", "authors": "Harrison Quick and Lance A. Waller and Michele Casper", "title": "A Nonseparable Multivariate Space-Time Model for Analyzing County-Level\n  Heart Disease Death Rates by Race and Gender", "comments": null, "journal-ref": "Journal of the Royal Statistical Society; Series C, 67 (2018)\n  291-304", "doi": "10.1111/rssc.12215", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While death rates due to diseases of the heart have experienced a sharp\ndecline over the past 50 years, these diseases continue to be the leading cause\nof death in the United States, and the rate of decline varies by geographic\nlocation, race, and gender. We look to harness the power of hierarchical\nBayesian methods to obtain a clearer picture of the declines from county-level,\ntemporally varying heart disease death rates for men and women of different\nraces in the US. Specifically, we propose a nonseparable multivariate\nspatio-temporal Bayesian model which allows for group-specific temporal\ncorrelations and temporally-evolving covariance structures in the multivariate\nspatio-temporal component of the model. After verifying the effectiveness of\nour model via simulation, we apply our model to a dataset of over 200,000\ncounty-level heart disease death rates. In addition to yielding a superior fit\nthan other common approaches for handling such data, the richness of our model\nprovides insight into racial, gender, and geographic disparities underlying\nheart disease death rates in the US which are not permitted by more restrictive\nmodels.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 23:23:43 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Quick", "Harrison", ""], ["Waller", "Lance A.", ""], ["Casper", "Michele", ""]]}, {"id": "1507.02822", "submitter": "Patrick Laub", "authors": "Patrick J. Laub, Thomas Taimre, and Philip K. Pollett", "title": "Hawkes Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hawkes processes are a particularly interesting class of stochastic process\nthat have been applied in diverse areas, from earthquake modelling to financial\nanalysis. They are point processes whose defining characteristic is that they\n'self-excite', meaning that each arrival increases the rate of future arrivals\nfor some period of time. Hawkes processes are well established, particularly\nwithin the financial literature, yet many of the treatments are inaccessible to\none not acquainted with the topic. This survey provides background, introduces\nthe field and historical developments, and touches upon all major aspects of\nHawkes processes.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2015 09:31:52 GMT"}], "update_date": "2015-07-13", "authors_parsed": [["Laub", "Patrick J.", ""], ["Taimre", "Thomas", ""], ["Pollett", "Philip K.", ""]]}, {"id": "1507.02954", "submitter": "Thomas Lundgaard Hansen", "authors": "Thomas L. Hansen, Peter B. J{\\o}rgensen, Mihai-Alin Badiu and Bernard\n  H. Fleury", "title": "An Iterative Receiver for OFDM With Sparsity-Based Parametric Channel\n  Estimation", "comments": "Major revision, accepted for IEEE Transactions on Signal Processing", "journal-ref": "IEEE Transactions on Signal Processing, Vol. 66, No. 20, Oct. 15,\n  2018", "doi": "10.1109/TSP.2018.2868314", "report-no": null, "categories": "cs.IT eess.SP math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we design a receiver that iteratively passes soft information\nbetween the channel estimation and data decoding stages. The receiver\nincorporates sparsity-based parametric channel estimation. State-of-the-art\nsparsity-based iterative receivers simplify the channel estimation problem by\nrestricting the multipath delays to a grid. Our receiver does not impose such a\nrestriction. As a result it does not suffer from the leakage effect, which\ndestroys sparsity. Communication at near capacity rates in high SNR requires a\nlarge modulation order. Due to the close proximity of modulation symbols in\nsuch systems, the grid-based approximation is of insufficient accuracy. We show\nnumerically that a state-of-the-art iterative receiver with grid-based sparse\nchannel estimation exhibits a bit-error-rate floor in the high SNR regime. On\nthe contrary, our receiver performs very close to the perfect channel state\ninformation bound for all SNR values. We also demonstrate both theoretically\nand numerically that parametric channel estimation works well in dense\nchannels, i.e., when the number of multipath components is large and each\nindividual component cannot be resolved.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2015 16:19:21 GMT"}, {"version": "v2", "created": "Fri, 15 Jul 2016 14:29:02 GMT"}, {"version": "v3", "created": "Sun, 16 Sep 2018 16:11:39 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Hansen", "Thomas L.", ""], ["J\u00f8rgensen", "Peter B.", ""], ["Badiu", "Mihai-Alin", ""], ["Fleury", "Bernard H.", ""]]}, {"id": "1507.03047", "submitter": "Chris Oates", "authors": "Chris. J. Oates", "title": "Accelerated Nonparametrics for Cascades of Poisson Processes", "comments": "To appear in Stat", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cascades of Poisson processes are probabilistic models for spatio-temporal\nphenomena in which (i) previous events may trigger subsequent events, and (ii)\nboth the background and triggering processes are conditionally Poisson. Such\nphenomena are typically \"data rich but knowledge poor\", in the sense that large\ndatasets are available yet a mechanistic understanding of the background and\ntriggering processes which generate the data are unavailable. In these settings\nnonparametric estimation plays a central role. However existing nonparametric\nestimators have computational and storage complexity $\\mathcal{O}(N^2)$,\nprecluding their application on large datasets. Here, by assuming the\ntriggering process acts only locally, we derive nonparametric estimators with\ncomputational complexity $\\mathcal{O}(N\\log N)$ and storage complexity\n$\\mathcal{O}(N)$. Our approach automatically learns the domain of the\ntriggering process from data and is essentially free from hyperparameters. The\nmethodology is applied to a large seismic dataset where estimation under\nexisting algorithms would be infeasible.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2015 23:34:44 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Oates", "Chris. J.", ""]]}, {"id": "1507.03283", "submitter": "Dmytro Zubov", "authors": "Dmytro Zubov, Humberto A. Barbosa, Gregory S. Duane", "title": "A Nonanticipative Analog Method for Long-Term Forecasting of Air\n  Temperature Extremes", "comments": "14 pages, 1 figure, 2 tables, 35 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A nonanticipative analog method is used for the long-term forecast of air\ntemperature extremes. The data to be used for prediction include average daily\nair temperature, mean visibility, mean wind speed, mean dew point, maximum and\nminimum temperatures reported during the day from 66 places around the world,\nas well as sea level, average monthly Darwin and Tahiti sea level pressures,\nSOI, equatorial SOI, sea surface temperature, and multivariate ENSO index.\nEvery dataset is split into two samples - learning (1973-2010) and validation\n(2011-2013). Initially, the sum of variables in datasets for two locations,\nminus corresponding climatological values, is calculated over a summation\ninterval of length from 1 to 365 days. A \"quality criterion\" selects datasets\nfor two locations with appropriate lead-time and summation interval, which have\nmaximum (or minimum) sum compared with the rest of data four times at least,\nwhen extreme events occur later within the learning sample. Up to 18.2% of all\nextremes are specifically predicted. The methodology has 100% accuracy with\nrespect to the sign of predicted and actual values. It is more useful than\ncurrent methods for predicting extreme values because it does not require the\nestimation of a probability distribution from scarce observations.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2015 22:41:39 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Zubov", "Dmytro", ""], ["Barbosa", "Humberto A.", ""], ["Duane", "Gregory S.", ""]]}, {"id": "1507.03401", "submitter": "Stefano Castruccio", "authors": "Stefano Castruccio and Joseph Guinness", "title": "An Evolutionary Spectrum Approach to Incorporate Large-scale\n  Geographical Descriptors on Global Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a nonstationary spatio-temporal statistical model for gridded\ndata on the sphere. The model specifies a computationally convenient covariance\nstructure that depends on heterogeneous geography. Widely used statistical\nmodels on a spherical domain are nonstationary for different latitudes, but\nstationary at the same latitude (axial symmetry). This assumption has been\nacknowledged to be too restrictive for quantities such as surface temperature,\nwhose statistical behavior is influenced by large scale geographical\ndescriptors such as land and ocean. We propose an evolutionary spectrum\napproach that is able to account for different regimes across the Earth's\ngeography, and results in a more general and flexible class of models that\nvastly outperforms axially symmetric models and captures longitudinal patterns\nthat would otherwise be assumed constant. The model can be estimated with in a\nmulti-step conditional likelihood approximation that preserves the\nnonstationary features while allowing for easily distributed computations: we\nshow how the fit of a data sets larger than 20 million data can be performed in\nless than one day on a state-of-the-art workstation. Once the parameters are\nestimated, it is possible to instantaneously generate surrogate runs from a\ncommon laptop. Further, the resulting estimates from the statistical model can\nbe regarded as a synthetic description (i.e. a compression) of the space-time\ncharacteristics of an entire initial condition ensemble. Compared to\ntraditional algorithms aiming at compressing the bit-by-bit information on each\nclimate model run, the proposed approach achieves vastly superior compression\nrates.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2015 11:19:26 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2016 16:40:50 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Castruccio", "Stefano", ""], ["Guinness", "Joseph", ""]]}, {"id": "1507.03429", "submitter": "Lukas Martig", "authors": "Lukas Martig, J\\\"urg H\\\"usler", "title": "On consistency of the likelihood moment estimators for a linear process\n  with regularly varying innovations", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 1975 James Pickands III showed that the excesses over a high threshold are\napproximatly Generalized Pareto distributed. Since then, a variety of\nestimators for the parameters of this cdf have been studied, but always\nassuming the underlying data to be independent. In this paper we consider the\nspecial case where the underlying data arises from a linear process with\nregularly varying (i.e. heavy-tailed) innovations. Using this setup, we then\nshow that the likelihood moment estimators introduced by Zhang (2007) are\nconsistent estimators for the parameters of the Generalized Pareto\ndistribution.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2015 12:45:33 GMT"}, {"version": "v2", "created": "Wed, 25 May 2016 12:19:55 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Martig", "Lukas", ""], ["H\u00fcsler", "J\u00fcrg", ""]]}, {"id": "1507.03479", "submitter": "S\\'andor Baran", "authors": "S\\'andor Baran and Annette M\\\"oller", "title": "Bivariate ensemble model output statistics approach for joint\n  forecasting of wind speed and temperature", "comments": "21 pages; 5 figures; 3 tables. arXiv admin note: text overlap with\n  arXiv:1404.3681", "journal-ref": "Meteorology and Atmospheric Physics 129 (2017), no. 1, 99-112", "doi": "10.1007/s00703-016-0467-8", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecast ensembles are typically employed to account for prediction\nuncertainties in numerical weather prediction models. However, ensembles often\nexhibit biases and dispersion errors, thus they require statistical\npost-processing to improve their predictive performance. Two popular univariate\npost-processing models are the Bayesian model averaging (BMA) and the ensemble\nmodel output statistics (EMOS).\n  In the last few years increased interest has emerged in developing\nmultivariate post-processing models, incorporating dependencies between weather\nquantities, such as for example a bivariate distribution for wind vectors or\neven a more general setting allowing to combine any types of weather variables.\n  In line with a recently proposed approach to model temperature and wind speed\njointly by a bivariate BMA model, this paper introduces a bivariate EMOS model\nfor these weather quantities based on a truncated normal distribution.\n  The bivariate EMOS model is applied to temperature and wind speed forecasts\nof the eight-member University of Washington mesoscale ensemble and of the\neleven-member ALADIN-HUNEPS ensemble of the Hungarian Meteorological Service\nand its predictive performance is compared to the performance of the bivariate\nBMA model and a multivariate Gaussian copula approach, post-processing the\nmargins with univariate EMOS. While the predictive skills of the compared\nmethods are similar, the bivariate EMOS model requires considerably lower\ncomputation times than the bivariate BMA method.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2015 14:45:42 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2015 14:04:29 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Baran", "S\u00e1ndor", ""], ["M\u00f6ller", "Annette", ""]]}, {"id": "1507.03538", "submitter": "Giri Gopalan", "authors": "Giri Gopalan, Saeqa Dil Vrtilek, and Luke Bornn", "title": "Classifying X-ray Binaries: A Probabilistic Approach", "comments": "Editing of figure captions and correction of y-axis labels for bar\n  charts in figures 6 and 7", "journal-ref": "2015 ApJ 809 40", "doi": "10.1088/0004-637X/809/1/40", "report-no": null, "categories": "astro-ph.HE stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In X-ray binary star systems consisting of a compact object that accretes\nmaterial from an orbiting secondary star, there is no straightforward means to\ndecide if the compact object is a black hole or a neutron star. To assist this\nclassification, we develop a Bayesian statistical model that makes use of the\nfact that X-ray binary systems appear to cluster based on their compact object\ntype when viewed from a 3-dimensional coordinate system derived from X-ray\nspectral data. The first coordinate of this data is the ratio of counts in mid\nto low energy band (color 1), the second coordinate is the ratio of counts in\nhigh to low energy band (color 2), and the third coordinate is the sum of\ncounts in all three bands. We use this model to estimate the probabilities that\nan X-ray binary system contains a black hole, non-pulsing neutron star, or\npulsing neutron star. In particular, we utilize a latent variable model in\nwhich the latent variables follow a Gaussian process prior distribution, and\nhence we are able to induce the spatial correlation we believe exists between\nsystems of the same type. The utility of this approach is evidenced by the\naccurate prediction of system types using Rossi X-ray Timing Explorer All Sky\nMonitor data, but it is not flawless. In particular, non-pulsing neutron\nsystems containing \"bursters\" that are close to the boundary demarcating\nsystems containing black holes tend to be classified as black hole systems. As\na byproduct of our analyses, we provide the astronomer with public R code that\ncan be used to predict the compact object type of X-ray binaries given training\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2015 18:24:16 GMT"}, {"version": "v2", "created": "Thu, 28 Dec 2017 07:01:28 GMT"}, {"version": "v3", "created": "Fri, 17 Aug 2018 13:12:02 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Gopalan", "Giri", ""], ["Vrtilek", "Saeqa Dil", ""], ["Bornn", "Luke", ""]]}, {"id": "1507.03955", "submitter": "Behtash Babadi", "authors": "Abbas Kazemipour, Min Wu, and Behtash Babadi", "title": "Robust Estimation of Self-Exciting Generalized Linear Models with\n  Application to Neuronal Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.IT cs.SY math.IT math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating self-exciting generalized linear models\nfrom limited binary observations, where the history of the process serves as\nthe covariate. We analyze the performance of two classes of estimators, namely\nthe $\\ell_1$-regularized maximum likelihood and greedy estimators, for a\ncanonical self-exciting process and characterize the sampling tradeoffs\nrequired for stable recovery in the non-asymptotic regime. Our results extend\nthose of compressed sensing for linear and generalized linear models with\ni.i.d. covariates to those with highly inter-dependent covariates. We further\nprovide simulation studies as well as application to real spiking data from the\nmouse's lateral geniculate nucleus and the ferret's retinal ganglion cells\nwhich agree with our theoretical predictions.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2015 18:07:31 GMT"}, {"version": "v2", "created": "Mon, 2 May 2016 21:12:12 GMT"}, {"version": "v3", "created": "Wed, 22 Mar 2017 23:13:49 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["Kazemipour", "Abbas", ""], ["Wu", "Min", ""], ["Babadi", "Behtash", ""]]}, {"id": "1507.03984", "submitter": "Peng Ding", "authors": "Peng Ding and Tyler VanderWeele", "title": "Sensitivity Analysis Without Assumptions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unmeasured confounding may undermine the validity of causal inference with\nobservational studies. Sensitivity analysis provides an attractive way to\npartially circumvent this issue by assessing the potential influence of\nunmeasured confounding on the causal conclusions. However, previous sensitivity\nanalysis approaches often make strong and untestable assumptions such as having\na confounder that is binary, or having no interaction between the effects of\nthe exposure and the confounder on the outcome, or having only one confounder.\nWithout imposing any assumptions on the confounder or confounders, we derive a\nbounding factor and a sharp inequality such that the sensitivity analysis\nparameters must satisfy the inequality if an unmeasured confounder is to\nexplain away the observed effect estimate or reduce it to a particular level.\nOur approach is easy to implement and involves only two sensitivity parameters.\nSurprisingly, our bounding factor, which makes no simplifying assumptions, is\nno more conservative than a number of previous sensitivity analysis techniques\nthat do make assumptions. Our new bounding factor implies not only the\ntraditional Cornfield conditions that both the relative risk of the exposure on\nthe confounder and that of the confounder on the outcome must satisfy, but also\na high threshold that the maximum of these relative risks must satisfy.\nFurthermore, this new bounding factor can be viewed as a measure of the\nstrength of confounding between the exposure and the outcome induced by a\nconfounder.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2015 19:57:33 GMT"}], "update_date": "2015-07-15", "authors_parsed": [["Ding", "Peng", ""], ["VanderWeele", "Tyler", ""]]}, {"id": "1507.04143", "submitter": "Somayeh Ashrafi", "authors": "S. Zarezadeh and S. Ashrafi and M. Asadi", "title": "A Shock Model Based Approach to Network Reliability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We consider a network consisting of $n$ components (links or nodes) and\nassume that the network has two states, up and down. We further suppose that\nthe network is subject to shocks that appear according to a counting process\nand that each shock may lead to the component failures. Under some assumptions\non the shock occurrences, we present a new variant of the notion of signature\nwhich we call it t-signature. Then t-signature based mixture representations\nfor the reliability function of the network are obtained. Several stochastic\nproperties of the network lifetime are investigated. In particular, under the\nassumption that the number of failures at each shock follows a binomial\ndistribution and the process of shocks is non-homogeneous Poisson process,\nexplicit form of the network reliability is derived and its aging properties\nare explored. Several examples are also provided\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 09:52:00 GMT"}], "update_date": "2015-07-16", "authors_parsed": [["Zarezadeh", "S.", ""], ["Ashrafi", "S.", ""], ["Asadi", "M.", ""]]}, {"id": "1507.04178", "submitter": "Eben Kenah", "authors": "Eben Kenah, Tom Britton, M. Elizabeth Halloran, Ira M. Longini Jr", "title": "Molecular Infectious Disease Epidemiology: Survival Analysis and\n  Algorithms Linking Phylogenies to Transmission Trees", "comments": "28 pages, 11 figures, 3 tables", "journal-ref": "PLoS Computational Biology 12(4): e1004869 (2016)", "doi": "10.1371/journal.pcbi.1004869", "report-no": null, "categories": "q-bio.QM q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has attempted to use whole-genome sequence data from pathogens to\nreconstruct the transmission trees linking infectors and infectees in\noutbreaks. However, transmission trees from one outbreak do not generalize to\nfuture outbreaks. Reconstruction of transmission trees is most useful to public\nhealth if it leads to generalizable scientific insights about disease\ntransmission. In a survival analysis framework, estimation of transmission\nparameters is based on sums or averages over the possible transmission trees. A\nphylogeny can increase the precision of these estimates by providing partial\ninformation about who infected whom. The leaves of the phylogeny represent\nsampled pathogens, which have known hosts. The interior nodes represent common\nancestors of sampled pathogens, which have unknown hosts. Starting from\nassumptions about disease biology and epidemiologic study design, we prove that\nthere is a one-to-one correspondence between the possible assignments of\ninterior node hosts and the transmission trees simultaneously consistent with\nthe phylogeny and the epidemiologic data on person, place, and time. We develop\nalgorithms to enumerate these transmission trees and show these can be used to\ncalculate likelihoods that incorporate both epidemiologic data and a phylogeny.\nA simulation study confirms that this leads to more efficient estimates of\nhazard ratios for infectiousness and baseline hazards of infectious contact,\nand we use these methods to analyze data from a foot-and-mouth disease virus\noutbreak in the United Kingdom in 2001. These results demonstrate the\nimportance of data on individuals who escape infection, which is often\noverlooked. The combination of survival analysis and algorithms linking\nphylogenies to transmission trees is a rigorous but flexible statistical\nfoundation for molecular infectious disease epidemiology.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 11:57:28 GMT"}, {"version": "v2", "created": "Tue, 5 Apr 2016 01:10:48 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Kenah", "Eben", ""], ["Britton", "Tom", ""], ["Halloran", "M. Elizabeth", ""], ["Longini", "Ira M.", "Jr"]]}, {"id": "1507.04199", "submitter": "Fan Li", "authors": "Fan Li, Alessandra Mattei, Fabrizia Mealli", "title": "Evaluating the Causal Effect of University Grants on Student Dropout:\n  Evidence from a Regression Discontinuity Design Using Principal\n  Stratification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression discontinuity (RD) designs are often interpreted as local\nrandomized experiments: a RD design can be considered as a randomized\nexperiment for units with a realized value of a so-called forcing variable\nfalling around a pre-fixed threshold. Motivated by the evaluation of Italian\nuniversity grants, we consider a fuzzy RD design where the receipt of the\ntreatment is based on both eligibility criteria and a voluntary application\nstatus. Resting on the fact that grant application and grant receipt statuses\nare post-assignment (post-eligibility) intermediate variables, we use the\nprincipal stratification framework to define causal estimands within the Rubin\nCausal Model. We propose a probabilistic formulation of the assignment\nmechanism underlying RD designs, by re-formulating the Stable Unit Treatment\nValue Assumption (SUTVA) and making an explicit local overlap assumption for a\nsubpopulation around the threshold. A local randomization assumption is invoked\ninstead of more standard continuity assumptions. We also develop a model-based\nBayesian approach to select the target subpopulation(s) with adjustment for\nmultiple comparisons, and to draw inference for the target causal estimands in\nthis framework. Applying the method to the data from two Italian universities,\nwe find evidence that university grants are effective in preventing students\nfrom low-income families from dropping out of higher education.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 13:06:49 GMT"}], "update_date": "2015-07-16", "authors_parsed": [["Li", "Fan", ""], ["Mattei", "Alessandra", ""], ["Mealli", "Fabrizia", ""]]}, {"id": "1507.04727", "submitter": "Behtash Babadi", "authors": "Alireza Sheikhattar, Jonathan B. Fritz, Shihab A. Shamma, and Behtash\n  Babadi", "title": "Recursive Sparse Point Process Regression with Application to\n  Spectrotemporal Receptive Field Plasticity Analysis", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2015.2512560", "report-no": null, "categories": "cs.NE cs.SY math.OC stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the sparse time-varying parameter\nvectors of a point process model in an online fashion, where the observations\nand inputs respectively consist of binary and continuous time series. We\nconstruct a novel objective function by incorporating a forgetting factor\nmechanism into the point process log-likelihood to enforce adaptivity and\nemploy $\\ell_1$-regularization to capture the sparsity. We provide a rigorous\nanalysis of the maximizers of the objective function, which extends the\nguarantees of compressed sensing to our setting. We construct two recursive\nfilters for online estimation of the parameter vectors based on proximal\noptimization techniques, as well as a novel filter for recursive computation of\nstatistical confidence regions. Simulation studies reveal that our algorithms\noutperform several existing point process filters in terms of trackability,\ngoodness-of-fit and mean square error. We finally apply our filtering\nalgorithms to experimentally recorded spiking data from the ferret primary\nauditory cortex during attentive behavior in a click rate discrimination task.\nOur analysis provides new insights into the time-course of the spectrotemporal\nreceptive field plasticity of the auditory neurons.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 19:43:54 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Sheikhattar", "Alireza", ""], ["Fritz", "Jonathan B.", ""], ["Shamma", "Shihab A.", ""], ["Babadi", "Behtash", ""]]}, {"id": "1507.05018", "submitter": "Carolina Eu\\'an Campos", "authors": "Carolina Euan, Hernando Ombao and Joaquin Ortega", "title": "Spectral Synchronicity in Brain Signals", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain activity following stimulus presentation and during resting state are\noften the result of highly coordinated responses of large numbers of neurons\nboth locally and globally. Coordinated activity of neurons can give rise to\noscillations which are captured by electroencephalograms (EEG). In this paper,\nwe examine EEGs as this is the primary data being used by our collaborators who\nare studying coordination of neuronal response during the execution of tasks\nsuch as learning, and memory formation, retention and retrieval. In this paper,\nwe develop the spectral merger clustering (SMC) method that identifies\nsynchronized brain regions during resting state in a sense that these regions\nshare similar oscillations or waveforms. The SMC method, produces clusters of\nEEGs which serve as a proxy for segmenting the brain cortical surface since the\nEEGs capture neuronal activity over a locally distributed region on the\ncortical surface. The extent of desynchronicity between a pair of EEGs is\nmeasured using the total variation distance (TVD) which gives the largest\npossible difference between the spectral densities of the pair of EEGs. We\nconsidered the spectral merger algorithm for clustering EEGs, which updates the\nspectral estimate of the cluster from a weighted average of the spectral\nestimate obtained from each EEG in the cluster. Numerical experiments suggest\nthat the SMC method performs very well in producing the correct clusters. When\napplied to resting state EEG data, the method showed how some regions, though\nnot contiguously connected on the cortical surface, are spectrally synchronized\nduring resting state. Moreover, the method demonstrates that brain\norganization, as expressed in cluster formation, evolves over resting state.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 16:17:48 GMT"}], "update_date": "2015-07-20", "authors_parsed": [["Euan", "Carolina", ""], ["Ombao", "Hernando", ""], ["Ortega", "Joaquin", ""]]}, {"id": "1507.05019", "submitter": "Paudel Subodh", "authors": "Subodh Paudel (Mines Nantes), Phuong H. Nguyen, Wil L. Kling, Mohamed\n  Elmitri, Bruno Lacarri\\`ere (Mines Nantes), Olivier Le Corre (Mines Nantes)", "title": "Support Vector Machine in Prediction of Building Energy Demand Using\n  Pseudo Dynamic Approach", "comments": "Proceedings of ECOS 2015-The 28th International Conference on\n  Efficiency, Cost, Optimization, Simulation and Environmental Impact of Energy\n  Systems , Jun 2015, Pau, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building's energy consumption prediction is a major concern in the recent\nyears and many efforts have been achieved in order to improve the energy\nmanagement of buildings. In particular, the prediction of energy consumption in\nbuilding is essential for the energy operator to build an optimal operating\nstrategy, which could be integrated to building's energy management system\n(BEMS). This paper proposes a prediction model for building energy consumption\nusing support vector machine (SVM). Data-driven model, for instance, SVM is\nvery sensitive to the selection of training data. Thus the relevant days data\nselection method based on Dynamic Time Warping is used to train SVM model. In\naddition, to encompass thermal inertia of building, pseudo dynamic model is\napplied since it takes into account information of transition of energy\nconsumption effects and occupancy profile. Relevant days data selection and\nwhole training data model is applied to the case studies of Ecole des Mines de\nNantes, France Office building. The results showed that support vector machine\nbased on relevant data selection method is able to predict the energy\nconsumption of building with a high accuracy in compare to whole data training.\nIn addition, relevant data selection method is computationally cheaper (around\n8 minute training time) in contrast to whole data training (around 31 hour for\nweekend and 116 hour for working days) and reveals realistic control\nimplementation for online system as well.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 16:19:11 GMT"}], "update_date": "2015-07-20", "authors_parsed": [["Paudel", "Subodh", "", "Mines Nantes"], ["Nguyen", "Phuong H.", "", "Mines Nantes"], ["Kling", "Wil L.", "", "Mines Nantes"], ["Elmitri", "Mohamed", "", "Mines Nantes"], ["Lacarri\u00e8re", "Bruno", "", "Mines Nantes"], ["Corre", "Olivier Le", "", "Mines Nantes"]]}, {"id": "1507.05045", "submitter": "L\\'aszl\\'o Csat\\'o", "authors": "L\\'aszl\\'o Csat\\'o", "title": "On the ranking of a Swiss system chess team tournament", "comments": "21 pages, 3 figures", "journal-ref": "Annals of Operations Research, 254(1-2): 17-36, 2017", "doi": "10.1007/s10479-017-2440-4", "report-no": null, "categories": "stat.AP cs.DM cs.GT math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper suggests a family of paired comparison-based scoring procedures for\nranking the participants of a Swiss system chess team tournament. We present\nthe challenges of ranking in Swiss system, the features of individual and team\ncompetitions as well as the failures of the official rankings based on\nlexicographical order. The tournament is represented as a ranking problem such\nthat the linearly-solvable row sum (score), generalized row sum, and least\nsquares methods have favourable axiomatic properties.\n  Two chess team European championships are analysed as case studies. Final\nrankings are compared by their distances and visualized with multidimensional\nscaling (MDS). Differences to the official ranking are revealed by the\ndecomposition of the least squares method. Rankings are evaluated by prediction\npower, retrodictive performance, and stability. The paper argues for the use of\nleast squares method with a results matrix favouring match points on the basis\nof its relative insensitivity to the choice between match and board points,\nretrodictive accuracy, and robustness.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 11:44:06 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2016 06:28:50 GMT"}, {"version": "v3", "created": "Fri, 23 Sep 2016 13:57:42 GMT"}, {"version": "v4", "created": "Mon, 26 Sep 2016 14:13:31 GMT"}, {"version": "v5", "created": "Thu, 26 Oct 2017 11:27:25 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Csat\u00f3", "L\u00e1szl\u00f3", ""]]}, {"id": "1507.05066", "submitter": "Thordis Thorarinsdottir", "authors": "Annette M\\\"oller, Thordis L. Thorarinsdottir, Alex Lenkoski and\n  Tilmann Gneiting", "title": "Spatially adaptive, Bayesian estimation for probabilistic temperature\n  forecasts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty in the prediction of future weather is commonly assessed through\nthe use of forecast ensembles that employ a numerical weather prediction model\nin distinct variants. Statistical postprocessing can correct for biases in the\nnumerical model and improves calibration. We propose a Bayesian version of the\nstandard ensemble model output statistics (EMOS) postprocessing method, in\nwhich spatially varying bias coefficients are interpreted as realizations of\nGaussian Markov random fields. Our Markovian EMOS (MEMOS) technique utilizes\nthe recently developed stochastic partial differential equation (SPDE) and\nintegrated nested Laplace approximation (INLA) methods for computationally\nefficient inference. The MEMOS approach shows good predictive performance in a\ncomparative study of 24-hour ahead temperature forecasts over Germany based on\nthe 50-member ensemble of the European Centre for Medium-Range Weather\nForecasting (ECMWF).\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 18:35:49 GMT"}, {"version": "v2", "created": "Fri, 10 Jun 2016 18:01:55 GMT"}, {"version": "v3", "created": "Wed, 15 Jun 2016 07:02:04 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["M\u00f6ller", "Annette", ""], ["Thorarinsdottir", "Thordis L.", ""], ["Lenkoski", "Alex", ""], ["Gneiting", "Tilmann", ""]]}, {"id": "1507.05144", "submitter": "Reza Abdolee", "authors": "Reza Abdolee, Benoit Champagne", "title": "Centralized Adaptation for Parameter Estimation over Wireless Sensor\n  Networks", "comments": "IEEE Communication Letter, 2015", "journal-ref": null, "doi": "10.1109/LCOMM.2015.2454502", "report-no": null, "categories": "cs.SY math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the performance of centralized least mean-squares (CLMS) algorithms\nin wireless sensor networks where nodes transmit their data over fading\nchannels to a central processing unit (e.g., fusion center or cluster head),\nfor parameter estimation. Wireless channel impairments, including fading and\npath loss, distort the transmitted data, cause link failure and degrade the\nperformance of the adaptive solutions. To address this problem, we propose a\nnovel CLMS algorithm that uses a refined version of the transmitted data and\nbenefits from a link failure alarm strategy to discard severely distorted data.\nFurthermore, to remove the bias due to communication noise from the estimate,\nwe introduce a bias-elimination scheme that also leads to a lower steady-state\nmean-square error. Our theoretical findings are supported by numerical\nsimulation results.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2015 04:30:57 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Abdolee", "Reza", ""], ["Champagne", "Benoit", ""]]}, {"id": "1507.05270", "submitter": "Denis Chetverikov", "authors": "Denis Chetverikov and Daniel Wilhelm", "title": "Nonparametric instrumental variable estimation under monotonicity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ill-posedness of the inverse problem of recovering a regression function\nin a nonparametric instrumental variable model leads to estimators that may\nsuffer from a very slow, logarithmic rate of convergence. In this paper, we\nshow that restricting the problem to models with monotone regression functions\nand monotone instruments significantly weakens the ill-posedness of the\nproblem. In stark contrast to the existing literature, the presence of a\nmonotone instrument implies boundedness of our measure of ill-posedness when\nrestricted to the space of monotone functions. Based on this result we derive a\nnovel non-asymptotic error bound for the constrained estimator that imposes\nmonotonicity of the regression function. For a given sample size, the bound is\nindependent of the degree of ill-posedness as long as the regression function\nis not too steep. As an implication, the bound allows us to show that the\nconstrained estimator converges at a fast, polynomial rate, independently of\nthe degree of ill-posedness, in a large, but slowly shrinking neighborhood of\nconstant functions. Our simulation study demonstrates significant finite-sample\nperformance gains from imposing monotonicity even when the regression function\nis rather far from being a constant. We apply the constrained estimator to the\nproblem of estimating gasoline demand functions from U.S. data.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2015 10:04:41 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Chetverikov", "Denis", ""], ["Wilhelm", "Daniel", ""]]}, {"id": "1507.05366", "submitter": "Hau-tieng Wu", "authors": "Ingrid Daubechies, Yi Wang, Hau-tieng Wu", "title": "ConceFT: Concentration of Frequency and Time via a multitapered\n  synchrosqueezed transform", "comments": null, "journal-ref": null, "doi": "10.1098/rsta.2015.0193", "report-no": null, "categories": "math.ST cs.NA stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new method is proposed to determine the time-frequency content of\ntime-dependent signals consisting of multiple oscillatory components, with\ntime-varying amplitudes and instantaneous frequencies. Numerical experiments as\nwell as a theoretical analysis are presented to assess its effectiveness.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2015 02:05:14 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Daubechies", "Ingrid", ""], ["Wang", "Yi", ""], ["Wu", "Hau-tieng", ""]]}, {"id": "1507.05372", "submitter": "Hau-tieng Wu", "authors": "Yu-Ting Lin, Patrick Flandrin, Hau-tieng Wu", "title": "When interpolation-induced reflection artifact meets time-frequency\n  analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While extracting the temporal dynamical features based on the time-frequency\nanalyses, like the reassignment and synchrosqueezing transform, attracts more\nand more interest in bio-medical data analysis, we should be careful about\nartifacts generated by interpolation schemes, in particular when the sampling\nrate is not significantly higher than the frequency of the oscillatory\ncomponent we are interested in. In this study, we formulate the problem called\nthe reflection effect and provide a theoretical justification of the statement.\nWe also show examples in the anesthetic depth analysis with clear but\nundesirable artifacts. The results show that the artifact associated with the\nreflection effect exists not only theoretically but practically. Its influence\nis pronounced when we apply the time-frequency analyses to extract the\ntime-varying dynamics hidden inside the signal. In conclusion, we have to\ncarefully deal with the artifact associated with the reflection effect by\nchoosing a proper interpolation scheme.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2015 02:47:36 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2015 03:26:46 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Lin", "Yu-Ting", ""], ["Flandrin", "Patrick", ""], ["Wu", "Hau-tieng", ""]]}, {"id": "1507.05617", "submitter": "Madhura Killedar", "authors": "Madhura Killedar, Stefano Borgani, Dunja Fabjan, Klaus Dolag, Gian\n  Luigi Granato, Massimo Meneghetti, Susana Planelles, Cinthia Ragone-Figueroa", "title": "Simulation-based marginal likelihood for cluster strong lensing\n  cosmology", "comments": "15 pages, 6 figures, 1 table, accepted to MNRAS", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.CO astro-ph.IM physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparisons between observed and predicted strong lensing properties of\ngalaxy clusters have been routinely used to claim either tension or consistency\nwith $\\Lambda$CDM cosmology. However, standard approaches to such cosmological\ntests are unable to quantify the preference for one cosmology over another. We\nadvocate approximating the relevant Bayes factor using a marginal likelihood\nthat is based on the following summary statistic: the posterior probability\ndistribution function for the parameters of the scaling relation between\nEinstein radii and cluster mass, $\\alpha$ and $\\beta$. We demonstrate, for the\nfirst time, a method of estimating the marginal likelihood using the X-ray\nselected $z>0.5$ MACS clusters as a case in point and employing both N-body and\nhydrodynamic simulations of clusters. We investigate the uncertainty in this\nestimate and consequential ability to compare competing cosmologies, that\narises from incomplete descriptions of baryonic processes, discrepancies in\ncluster selection criteria, redshift distribution, and dynamical state. The\nrelation between triaxial cluster masses at various overdensities provide a\npromising alternative to the strong lensing test.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2015 20:00:28 GMT"}, {"version": "v2", "created": "Thu, 31 Mar 2016 07:52:20 GMT"}, {"version": "v3", "created": "Fri, 1 Sep 2017 05:09:24 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Killedar", "Madhura", ""], ["Borgani", "Stefano", ""], ["Fabjan", "Dunja", ""], ["Dolag", "Klaus", ""], ["Granato", "Gian Luigi", ""], ["Meneghetti", "Massimo", ""], ["Planelles", "Susana", ""], ["Ragone-Figueroa", "Cinthia", ""]]}, {"id": "1507.05834", "submitter": "Manuel Bastuck", "authors": "Manuel Bastuck, Martin Leidinger, Tilman Sauerwald, Andreas Sch\\\"utze", "title": "Improved quantification of naphthalene using non-linear Partial Least\n  Squares Regression", "comments": "2 pages, 1 figure, ISOEN 2015, 16th International Symposium on\n  Olfaction and Electronic Noses, Dijon, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A test dataset is generated using temperature cycled operation with a WO3\nmetal oxide semiconductor (MOS) gas sensor. Six concentrations of naphthalene\nfrom 0 to 40 ppb are measured and, subsequently, used to evaluate the\nperformance of three variants of Partial Least Squares Regression (PLSR).\nOrdinary PLSR produces highly non-linear models due to the non-linear response\nof the sensor. Double-logarithmic data results in a model with much better\nlinearity which has a resolution of 4 ppb in the range from 0 to 20 ppb. The\nmore complex Locally Weighted PLSR (LW-PLSR) produces an even better model,\nespecially for higher concentrations, without making any assumptions for\nrelationships in the underlying data.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 13:49:32 GMT"}], "update_date": "2015-07-22", "authors_parsed": [["Bastuck", "Manuel", ""], ["Leidinger", "Martin", ""], ["Sauerwald", "Tilman", ""], ["Sch\u00fctze", "Andreas", ""]]}, {"id": "1507.05935", "submitter": "Peng Ding", "authors": "Zhichao Jiang, Peng Ding, Zhi Geng", "title": "Principal causal effect identification and surrogate endpoint evaluation\n  by multiple trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal stratification is a causal framework to analyze randomized\nexperiments with a post-treatment variable between the treatment and endpoint\nvariables. Because the principal strata defined by the potential outcomes of\nthe post-treatment variable are not observable, we generally cannot identify\nthe causal effects within principal strata. Motivated by a real data set of\nphase III adjuvant colon clinical trials, we propose approaches to identifying\nand estimating the principal causal effects via multiple trials. For the\nidentifiability, we remove the commonly-used exclusion restriction assumption\nby stipulating that the principal causal effects are homogeneous across these\ntrials. To remove another commonly-used monotonicity assumption, we give a\nnecessary condition for the local identifiability, which requires at least\nthree trials. Applying our approaches to the data from adjuvant colon clinical\ntrials, we find that the commonly-used monotonicity assumption is untenable,\nand disease-free survival with three-year follow-up is a valid surrogate\nendpoint for overall survival with five-year follow-up, which satisfies both\nthe causal necessity and the causal sufficiency. We also propose a sensitivity\nanalysis approach based on Bayesian hierarchical models to investigate the\nimpact of the deviation from the homogeneity assumption.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 18:19:01 GMT"}], "update_date": "2015-07-22", "authors_parsed": [["Jiang", "Zhichao", ""], ["Ding", "Peng", ""], ["Geng", "Zhi", ""]]}, {"id": "1507.05943", "submitter": "Hau-tieng Wu", "authors": "Hau-tieng Wu, Han-Kuei Wu, Chun-Li Wang, Yueh-Lung Yang, Wen-Hsiang\n  Wu, Tung-Hu Tsai, Hen-Hong Chang", "title": "Modeling the pulse signal by wave-shape function and analyzing by\n  synchrosqueezing transform", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0157135", "report-no": null, "categories": "stat.AP physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply the recently developed adaptive non-harmonic model based on the\nwave-shape function, as well as the time-frequency analysis tool called\nsynchrosqueezing transform (SST) to model and analyze oscillatory physiological\nsignals. To demonstrate how the model and algorithm work, we apply them to\nstudy the pulse wave signal. By extracting features called the spectral pulse\nsignature, {and} based on functional regression, we characterize the\nhemodynamics from the radial pulse wave signals recorded by the\nsphygmomanometer. Analysis results suggest the potential of the proposed signal\nprocessing approach to extract health-related hemodynamics features.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 18:56:33 GMT"}, {"version": "v2", "created": "Thu, 26 May 2016 18:27:47 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Wu", "Hau-tieng", ""], ["Wu", "Han-Kuei", ""], ["Wang", "Chun-Li", ""], ["Yang", "Yueh-Lung", ""], ["Wu", "Wen-Hsiang", ""], ["Tsai", "Tung-Hu", ""], ["Chang", "Hen-Hong", ""]]}, {"id": "1507.06015", "submitter": "Helin Zhu", "authors": "Helin Zhu, Tianyi Liu and Enlu Zhou", "title": "Risk Quantification in Stochastic Simulation under Input Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When simulating a complex stochastic system, the behavior of output response\ndepends on input parameters estimated from finite real-world data, and the\nfiniteness of data brings input uncertainty into the system. The quantification\nof the impact of input uncertainty on output response has been extensively\nstudied. Most of the existing literature focuses on providing inferences on the\nmean response at the true but unknown input parameter, including point\nestimation and confidence interval construction. Risk quantification of mean\nresponse under input uncertainty often plays an important role in system\nevaluation and control, because it provides inferences on extreme scenarios of\nmean response in all possible input models. To the best of our knowledge, it\nhas rarely been systematically studied in the literature. In this paper, first\nwe introduce risk measures of mean response under input uncertainty, and\npropose a nested Monte Carlo simulation approach to estimate them. Then we\ndevelop asymptotical properties such as consistency and asymptotic normality\nfor the proposed nested risk estimators. We further study the associated budget\nallocation problem for efficient nested risk simulation, and finally use a\nsharing economy example to illustrate the importance of accessing and\ncontrolling risk due to input uncertainty.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 23:59:56 GMT"}, {"version": "v2", "created": "Sun, 31 Jul 2016 19:47:07 GMT"}, {"version": "v3", "created": "Mon, 18 Dec 2017 22:30:03 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Zhu", "Helin", ""], ["Liu", "Tianyi", ""], ["Zhou", "Enlu", ""]]}, {"id": "1507.06064", "submitter": "Ivo Siekmann", "authors": "Ivo Siekmann, Pengxing Cao, James Sneyd and Edmund J. Crampin", "title": "Data-driven modelling of the inositol trisphosphate receptor (IPR) and\n  its role in calcium induced calcium release (CICR)", "comments": "23 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.BM q-bio.SC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a review of the current state of the art of data-driven modelling of\nthe inositol trisphosphate receptor (IPR). After explaining that the IPR plays\na crucial role as a central regulator in calcium dynamics, several sources of\nrelevant experimental data are introduced. Single ion channels are best studied\nby recording single-channel currents under different ligand concentrations via\nthe patch-clamp technique. The particular relevance of modal gating, the\nspontaneous switching between different levels of channel activity that occur\neven at constant ligand concentrations, is highlighted. In order to investigate\nthe interactions of IPRs, calcium release from small clusters of channels,\nso-called calcium puffs, can be used. We then present the mathematical\nframework common to all models based on single-channel data, aggregated\ncontinuous-time Markov models, and give a short review of statistical\napproaches for parameterising these models with experimental data. The process\nof building a Markov model that integrates various sources of experimental data\nis illustrated using two recent examples, the model by Ullah et al. and the\n\"Park-Drive\" model by Siekmann et al., the only models that account for all\nsources of data currently available. Finally, it is demonstrated that the\nessential features of the Park-Drive model in different models of calcium\ndynamics are preserved after reducing it to a two-state model that only\naccounts for the switching between the inactive \"park\" and the active \"drive\"\nmode. This highlights the fact that modal gating is the most important\nmechanism of ligand regulation in the IPR. It also emphasises that data-driven\nmodels of ion channels do not necessarily have to lead to detailed models but\ncan be constructed so that relevant data is selected to represent ion channels\nat the appropriate level of complexity for a given application.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 05:13:57 GMT"}], "update_date": "2015-07-23", "authors_parsed": [["Siekmann", "Ivo", ""], ["Cao", "Pengxing", ""], ["Sneyd", "James", ""], ["Crampin", "Edmund J.", ""]]}, {"id": "1507.06119", "submitter": "Pradip Kundu", "authors": "Pradip Kundu, Nil Kamal Hazra, Asok K. Nanda", "title": "Reliability study of a coherent system with single general standby\n  component", "comments": null, "journal-ref": "Statistics & Probability Letters 110 (2016) 25-33", "doi": "10.1016/j.spl.2015.11.023", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The properties of a coherent system with a single general standby component\nis investigated. Here three different switch over viz. perfect switching,\nimperfect switching and random worm up period of the standby component are\nconsidered with some numerical examples.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 10:17:17 GMT"}, {"version": "v2", "created": "Sun, 17 Jan 2016 05:56:48 GMT"}, {"version": "v3", "created": "Thu, 12 Nov 2020 05:37:04 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Kundu", "Pradip", ""], ["Hazra", "Nil Kamal", ""], ["Nanda", "Asok K.", ""]]}, {"id": "1507.06517", "submitter": "S\\'andor Baran", "authors": "S\\'andor Baran and Sebastian Lerch", "title": "Mixture EMOS model for calibrating ensemble forecasts of wind speed", "comments": "24 pages, 7 tables, 10 figures", "journal-ref": "Environmetrics 2016, 27, 116-130", "doi": "10.1002/env.2380", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensemble model output statistics (EMOS) is a statistical tool for\npost-processing forecast ensembles of weather variables obtained from multiple\nruns of numerical weather prediction models in order to produce calibrated\npredictive probability density functions (PDFs). The EMOS predictive PDF is\ngiven by a parametric distribution with parameters depending on the ensemble\nforecasts. We propose an EMOS model for calibrating wind speed forecasts based\non weighted mixtures of truncated normal (TN) and log-normal (LN) distributions\nwhere model parameters and component weights are estimated by optimizing the\nvalues of proper scoring rules over a rolling training period. The new model is\ntested on wind speed forecasts of the 50 member European Centre for\nMedium-Range Weather Forecasts ensemble, the 11 member Aire Limit\\'ee\nAdaptation dynamique D\\'eveloppement International-Hungary Ensemble Prediction\nSystem ensemble of the Hungarian Meteorological Service and the eight-member\nUniversity of Washington mesoscale ensemble, and its predictive performance is\ncompared to that of various benchmark EMOS models based on single parametric\nfamilies and combinations thereof. The results indicate improved calibration of\nprobabilistic and accuracy of point forecasts in comparison with the raw\nensemble and climatological forecasts. The mixture EMOS model significantly\noutperforms the TN and LN EMOS methods, moreover, it provides better calibrated\nforecasts than the TN-LN combination model and offers an increased flexibility\nwhile avoiding covariate selection problems.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2015 14:48:17 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2015 13:28:26 GMT"}, {"version": "v3", "created": "Fri, 11 Dec 2015 20:44:58 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["Baran", "S\u00e1ndor", ""], ["Lerch", "Sebastian", ""]]}, {"id": "1507.06710", "submitter": "Gilad Lerman Dr", "authors": "Xu Wang and Gilad Lerman", "title": "Nonparametric Bayesian Regression on Manifolds via Brownian Motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel framework for manifold-valued regression and\nestablishes its consistency as well as its contraction rate. It assumes a\npredictor with values in the interval $[0,1]$ and response with values in a\ncompact Riemannian manifold $M$. This setting is useful for applications such\nas modeling dynamic scenes or shape deformations, where the visual scene or the\ndeformed objects can be modeled by a manifold. The proposed framework is\nnonparametric and uses the heat kernel (and its associated Brownian motion) on\nmanifolds as an averaging procedure. It directly generalizes the use of the\nGaussian kernel (as a natural model of additive noise) in vector-valued\nregression problems. In order to avoid explicit dependence on estimates of the\nheat kernel, we follow a Bayesian setting, where Brownian motion on $M$ induces\na prior distribution on the space of continuous functions $C([0,1], M)$. For\nthe case of discretized Brownian motion, we establish the consistency of the\nposterior distribution in terms of the $L_{q}$ distances for any $1 \\leq q <\n\\infty$. Most importantly, we establish contraction rate of order\n$O(n^{-1/4+\\epsilon})$ for any fixed $\\epsilon>0$, where $n$ is the number of\nobservations. For the continuous Brownian motion we establish weak consistency.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 00:33:04 GMT"}], "update_date": "2015-07-27", "authors_parsed": [["Wang", "Xu", ""], ["Lerman", "Gilad", ""]]}, {"id": "1507.06780", "submitter": "Jia Liu", "authors": "Jia Liu", "title": "An improved EM algorithm for solving MLE in constrained diffusion\n  kurtosis imaging of human brain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The displacement distribution of a water molecular is characterized\nmathematically as Gaussianity without considering potential diffusion barriers\nand compartments. However, this is not true in real scenario: most biological\ntissues are comprised of cell membranes, various intracellular and\nextracellular spaces, and of other compartments, where the water diffusion is\nreferred to have a non-Gaussian distribution. Diffusion kurtosis imaging (DKI),\nrecently considered to be one sensitive biomarker, is an extension of diffusion\ntensor imaging, which quantifies the degree of non-Gaussianity of the\ndiffusion. This work proposes an efficient scheme of maximum likelihood\nestimation (MLE) in DKI: we start from the Rician noise model of the signal\nintensities. By augmenting a Von-Mises distributed latent phase variable, the\nRician likelihood is transformed to a tractable joint density without loss of\ngenerality. A fast computational method, an expectation-maximization (EM)\nalgorithm for MLE is proposed in DKI. To guarantee the physical relevance of\nthe diffusion kurtosis we apply the ternary quartic (TQ) parametrization to\nutilize its positivity, which imposes the upper bound to the kurtosis. A\nFisher-scoring method is used for achieving fast convergence of the individual\ndiffusion compartments. In addition, we use the barrier method to constrain the\nlower bound to the kurtosis. The proposed estimation scheme is conducted on\nboth synthetic and real data with an objective of healthy human brain. We\ncompared the method with the other popular ones with promising performance\nshown in the results.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 08:43:23 GMT"}], "update_date": "2015-07-27", "authors_parsed": [["Liu", "Jia", ""]]}, {"id": "1507.06807", "submitter": "Andrew Golightly", "authors": "Gavin A. Whitaker, Andrew Golightly, Richard J. Boys and Chris\n  Sherlock", "title": "Bayesian inference for diffusion driven mixed-effects models", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic differential equations (SDEs) provide a natural framework for\nmodelling intrinsic stochasticity inherent in many continuous-time physical\nprocesses. When such processes are observed in multiple individuals or\nexperimental units, SDE driven mixed-effects models allow the quantification of\nbetween (as well as within) individual variation. Performing Bayesian inference\nfor such models, using discrete time data that may be incomplete and subject to\nmeasurement error is a challenging problem and is the focus of this paper. We\nextend a recently proposed MCMC scheme to include the SDE driven mixed-effects\nframework. Fundamental to our approach is the development of a novel construct\nthat allows for efficient sampling of conditioned SDEs that may exhibit\nnonlinear dynamics between observation times. We apply the resulting scheme to\nsynthetic data generated from a simple SDE model of orange tree growth, and\nreal data consisting of observations on aphid numbers recorded under a variety\nof different treatment regimes. In addition, we provide a systematic comparison\nof our approach with an inference scheme based on a tractable approximation of\nthe SDE, that is, the linear noise approximation.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 10:52:44 GMT"}, {"version": "v2", "created": "Wed, 18 May 2016 13:52:01 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["Whitaker", "Gavin A.", ""], ["Golightly", "Andrew", ""], ["Boys", "Richard J.", ""], ["Sherlock", "Chris", ""]]}, {"id": "1507.06907", "submitter": "Jonathon OBrien", "authors": "Jonathon O'Brien, Harsha Gunawardena, Xian Chen, Joseph Ibrahim,\n  Bahjat Qaqish", "title": "The Midpoint Mixed Model with a Missingness Mechanism (M5): A\n  Likelihood-Based Framework for Quantification of Mass Spectrometry Proteomics\n  Data (Preprint)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical models for proteomics data often estimate protein fold changes\nbetween two samples, A and B, as the average peptide intensity from sample A\ndivided by the average peptide intensity from sample B. Such average intensity\nratios fail to take full advantage of the experimental design which eliminates\nunseen confounding variables by processing peptides from both samples under\nidentical conditions. Typically this structure is exploited through the\nestimation of a protein ratio as the median ratio of matched peptide\nintensities. This simple solution fails to account for a substantial missing\ndata bias which has led to the development of more sophisticated average\nintensity models. Here we develop the first statistical model that accounts for\nnonignorable missingness while utilizing peptide level matched pairs across\nsamples. Our simulation analysis shows that models which fail to utilize\npeptide level ratios, su.er astonishing losses to accuracy with basic ANOVA\nestimates having an average MSE 371% higher than median ratio estimates. In\nturn, median ratio estimates have an average MSE 35% higher than our model\nestimates. An analysis of breast cancer data reinforces these relationships and\nshows that our model is capable of increasing the number of proteins estimated\nby 22%.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 16:21:48 GMT"}], "update_date": "2015-07-27", "authors_parsed": [["O'Brien", "Jonathon", ""], ["Gunawardena", "Harsha", ""], ["Chen", "Xian", ""], ["Ibrahim", "Joseph", ""], ["Qaqish", "Bahjat", ""]]}, {"id": "1507.06954", "submitter": "Alex Reinhart", "authors": "Alex Reinhart, Val\\'erie Ventura and Alex Athey", "title": "Detecting changes in maps of gamma spectra with Kolmogorov-Smirnov tests", "comments": "8 pages, 7 figures; revised in response to reviewer comments", "journal-ref": "Nuclear Instruments and Methods A, vol. 802 (2015), pp. 31-37", "doi": "10.1016/j.nima.2015.09.002", "report-no": null, "categories": "physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various security, regulatory, and consequence management agencies are\ninterested in continuously monitoring wide areas for unexpected changes in\nradioactivity. Existing detection systems are designed to search for\nradioactive sources but are not suited to repeat mapping and change detection.\nUsing a set of daily spectral observations collected at the Pickle Research\nCampus, we improved on the prior Spectral Comparison Ratio Anomaly Mapping\n(SCRAM) algorithm and developed a new method based on two-sample\nKolmogorov-Smirnov tests to detect sudden spectral changes. We also designed\nsimulations and visualizations of statistical power to compare methods and\nguide deployment scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 18:45:28 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2015 14:31:55 GMT"}], "update_date": "2015-09-28", "authors_parsed": [["Reinhart", "Alex", ""], ["Ventura", "Val\u00e9rie", ""], ["Athey", "Alex", ""]]}, {"id": "1507.06985", "submitter": "Vivek Goyal", "authors": "Dongeek Shin, Jeffrey H. Shapiro, and Vivek K Goyal", "title": "Single-Photon Depth Imaging Using a Union-of-Subspaces Model", "comments": "5 pages", "journal-ref": null, "doi": "10.1109/LSP.2015.2475274", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light detection and ranging systems reconstruct scene depth from\ntime-of-flight measurements. For low light-level depth imaging applications,\nsuch as remote sensing and robot vision, these systems use single-photon\ndetectors that resolve individual photon arrivals. Even so, they must detect a\nlarge number of photons to mitigate Poisson shot noise and reject anomalous\nphoton detections from background light. We introduce a novel framework for\naccurate depth imaging using a small number of detected photons in the presence\nof an unknown amount of background light that may vary spatially. It employs a\nPoisson observation model for the photon detections plus a union-of-subspaces\nconstraint on the discrete-time flux from the scene at any single pixel.\nTogether, they enable a greedy signal-pursuit algorithm to rapidly and\nsimultaneously converge on accurate estimates of scene depth and background\nflux, without any assumptions on spatial correlations of the depth or\nbackground flux. Using experimental single-photon data, we demonstrate that our\nproposed framework recovers depth features with 1.7 cm absolute error, using 15\nphotons per image pixel and an illumination pulse with 6.7-cm scaled\nroot-mean-square length. We also show that our framework outperforms the\nconventional pixelwise log-matched filtering, which is a\ncomputationally-efficient approximation to the maximum-likelihood solution, by\na factor of 6.1 in absolute depth error.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 05:15:56 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Shin", "Dongeek", ""], ["Shapiro", "Jeffrey H.", ""], ["Goyal", "Vivek K", ""]]}, {"id": "1507.07024", "submitter": "Matthew Parno", "authors": "Matthew Parno, Tarek Moselhy, and Youssef Marzouk", "title": "A multiscale strategy for Bayesian inference using transport maps", "comments": null, "journal-ref": null, "doi": "10.1137/15M1032478", "report-no": null, "categories": "stat.CO math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many inverse problems, model parameters cannot be precisely determined\nfrom observational data. Bayesian inference provides a mechanism for capturing\nthe resulting parameter uncertainty, but typically at a high computational\ncost. This work introduces a multiscale decomposition that exploits conditional\nindependence across scales, when present in certain classes of inverse\nproblems, to decouple Bayesian inference into two stages: (1) a computationally\ntractable coarse-scale inference problem; and (2) a mapping of the\nlow-dimensional coarse-scale posterior distribution into the original\nhigh-dimensional parameter space. This decomposition relies on a\ncharacterization of the non-Gaussian joint distribution of coarse- and\nfine-scale quantities via optimal transport maps. We demonstrate our approach\non a sequence of inverse problems arising in subsurface flow, using the\nmultiscale finite element method to discretize the steady state pressure\nequation. We compare the multiscale strategy with full-dimensional Markov chain\nMonte Carlo on a problem of moderate dimension (100 parameters) and then use it\nto infer a conductivity field described by over 10,000 parameters.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 21:40:44 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2016 00:04:24 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Parno", "Matthew", ""], ["Moselhy", "Tarek", ""], ["Marzouk", "Youssef", ""]]}, {"id": "1507.07046", "submitter": "Alexander Wong", "authors": "Dorothy Lui, Amen Modhafar, Masoom Haider, and Alexander Wong", "title": "Monte Carlo-based Noise Compensation in Coil Intensity Corrected\n  Endorectal MRI", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Prostate cancer is one of the most common forms of cancer found\nin males making early diagnosis important. Magnetic resonance imaging (MRI) has\nbeen useful in visualizing and localizing tumor candidates and with the use of\nendorectal coils (ERC), the signal-to-noise ratio (SNR) can be improved. The\ncoils introduce intensity inhomogeneities and the surface coil intensity\ncorrection built into MRI scanners is used to reduce these inhomogeneities.\nHowever, the correction typically performed at the MRI scanner level leads to\nnoise amplification and noise level variations. Methods: In this study, we\nintroduce a new Monte Carlo-based noise compensation approach for coil\nintensity corrected endorectal MRI which allows for effective noise\ncompensation and preservation of details within the prostate. The approach\naccounts for the ERC SNR profile via a spatially-adaptive noise model for\ncorrecting non-stationary noise variations. Such a method is useful\nparticularly for improving the image quality of coil intensity corrected\nendorectal MRI data performed at the MRI scanner level and when the original\nraw data is not available. Results: SNR and contrast-to-noise ratio (CNR)\nanalysis in patient experiments demonstrate an average improvement of 11.7 dB\nand 11.2 dB respectively over uncorrected endorectal MRI, and provides strong\nperformance when compared to existing approaches. Conclusions: A new noise\ncompensation method was developed for the purpose of improving the quality of\ncoil intensity corrected endorectal MRI data performed at the MRI scanner\nlevel. We illustrate that promising noise compensation performance can be\nachieved for the proposed approach, which is particularly important for\nprocessing coil intensity corrected endorectal MRI data performed at the MRI\nscanner level and when the original raw data is not available.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2015 00:51:56 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Lui", "Dorothy", ""], ["Modhafar", "Amen", ""], ["Haider", "Masoom", ""], ["Wong", "Alexander", ""]]}, {"id": "1507.07070", "submitter": "Baidurya Bhattacharya", "authors": "Baidurya Bhattacharya", "title": "The Extremal Index and the Maximum of a Dependent Stationary Pulse Load\n  Process Observed above a High Threshold", "comments": null, "journal-ref": "Structural Safety, Elsevier, vol. 30, no. 1, pp. 34 - 48, 2008", "doi": "10.1016/j.strusafe.2006.05.001", "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Observing a load process above high thresholds, modeling it as a pulse\nprocess with random occurrence times and magnitudes, and extrapolating\nlife-time maximum or design loads from the data is a common task in structural\nreliability analyses. In this paper, we consider a stationary live load\nsequence that arrive according to a dependent point process and allow for a\nweakened mixing-type dependence in the load pulse magnitudes that\nasymptotically decreases to zero with increasing separation in the sequence.\nInclusion of dependence in the model eliminates the unnecessary conservatism\nintroduced by the i.i.d. (independent and identically distributed) assumption\noften made in determining maximum live load distribution. The scale of\nfluctuation of the loading process is used to identify clusters of exceedances\nabove high thresholds which in turn is used to estimate the extremal index of\nthe process. A Bayesian updating of the empirical distribution function,\nderived from the distribution of order statistics in a dependent stationary\nseries, is performed. The pulse arrival instants are modeled as a Cox process\ngoverened by a stationary lognormal intensity. An illustrative example utilizes\nin-service peak strain data from ambient traffic collected on a high volume\nhighway bridge, and analyzes the asymptotic behavior of the maximum load.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2015 05:42:01 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Bhattacharya", "Baidurya", ""]]}, {"id": "1507.07113", "submitter": "Krzysztof Bartoszek", "authors": "Krzysztof Bartoszek", "title": "Phylogenetic effective sample size", "comments": null, "journal-ref": "Journal of Theoretical Biology 407:371-386, 2016", "doi": "10.1016/j.jtbi.2016.06.026", "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper I address the question - how large is a phylogenetic sample I\npropose a definition of a phylogenetic effective sample size for Brownian\nmotion and Ornstein-Uhlenbeck processes - the regression effective sample size.\nI discuss how mutual information can be used to define an effective sample size\nin the non-normal process case and compare these two definitions to an already\npresent concept of effective sample size (the mean effective sample size).\nThrough a simulation study I find that the AICc is robust if one corrects for\nthe number of species or effective number of species. Lastly I discuss how the\nconcept of the phylogenetic effective sample size can be useful for\nbiodiversity quantification, identification of interesting clades and deciding\non the importance of phylogenetic correlations.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2015 15:50:38 GMT"}, {"version": "v2", "created": "Wed, 11 May 2016 13:37:31 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Bartoszek", "Krzysztof", ""]]}, {"id": "1507.07139", "submitter": "Jakub Chorowski", "authors": "Jakub Chorowski", "title": "Nonparametric volatility estimation in scalar diffusions: Optimality\n  across observation frequencies", "comments": "41 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The nonparametric volatility estimation problem of a scalar diffusion process\nobserved at equidistant time points is addressed. Using the spectral\nrepresentation of the volatility in terms of the invariant density and an\neigenpair of the infinitesimal generator the first known estimator that attains\nthe minimax optimal convergence rates for both high and low-frequency\nobservations is constructed. The proofs are based on a posteriori error bounds\nfor generalized eigenvalue problems as well as the path properties of scalar\ndiffusions and stochastic analysis. The finite sample performance is\nillustrated by a numerical example.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2015 21:44:52 GMT"}, {"version": "v2", "created": "Thu, 31 Mar 2016 07:49:14 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Chorowski", "Jakub", ""]]}, {"id": "1507.07271", "submitter": "Alex Reinhart", "authors": "Wesley Tansey, Alex Athey, Alex Reinhart, and James G. Scott", "title": "Multiscale spatial density smoothing: an application to large-scale\n  radiological survey and anomaly detection", "comments": "36 pages, 10 figures", "journal-ref": "Journal of the American Statistical Association, vol. 112 no. 519\n  (2017), pp. 1047-1063", "doi": "10.1080/01621459.2016.1276461", "report-no": null, "categories": "stat.ME physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating a spatially varying density function,\nmotivated by problems that arise in large-scale radiological survey and anomaly\ndetection. In this context, the density functions to be estimated are the\nbackground gamma-ray energy spectra at sites spread across a large geographical\narea, such as nuclear production and waste-storage sites, military bases,\nmedical facilities, university campuses, or the downtown of a city. Several\nchallenges combine to make this a difficult problem. First, the spectral\ndensity at any given spatial location may have both smooth and non-smooth\nfeatures. Second, the spatial correlation in these density functions is neither\nstationary nor locally isotropic. Finally, at some spatial locations, there is\nvery little data. We present a method called multiscale spatial density\nsmoothing that successfully addresses these challenges. The method is based on\nrecursive dyadic partition of the sample space, and therefore shares much in\ncommon with other multiscale methods, such as wavelets and P\\'olya-tree priors.\nWe describe an efficient algorithm for finding a maximum a posteriori (MAP)\nestimate that leverages recent advances in convex optimization for non-smooth\nfunctions.\n  We apply multiscale spatial density smoothing to real data collected on the\nbackground gamma-ray spectra at locations across a large university campus. The\nmethod exhibits state-of-the-art performance for spatial smoothing in density\nestimation, and it leads to substantial improvements in power when used in\nconjunction with existing methods for detecting the kinds of radiological\nanomalies that may have important consequences for public health and safety.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2015 00:41:16 GMT"}, {"version": "v2", "created": "Fri, 16 Sep 2016 16:02:04 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Tansey", "Wesley", ""], ["Athey", "Alex", ""], ["Reinhart", "Alex", ""], ["Scott", "James G.", ""]]}, {"id": "1507.07295", "submitter": "Kirill Dyagilev", "authors": "Kirill Dyagilev, Suchi Saria", "title": "Learning (Predictive) Risk Scores in the Presence of Censoring due to\n  Interventions", "comments": null, "journal-ref": "Machine Learning Journal, Special Issue on on Machine Learning for\n  Health and Medicine, pp. 1-26, 2015", "doi": null, "report-no": null, "categories": "cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large and diverse set of measurements are regularly collected during a\npatient's hospital stay to monitor their health status. Tools for integrating\nthese measurements into severity scores, that accurately track changes in\nillness severity, can improve clinicians ability to provide timely\ninterventions. Existing approaches for creating such scores either 1) rely on\nexperts to fully specify the severity score, or 2) train a predictive score,\nusing supervised learning, by regressing against a surrogate marker of severity\nsuch as the presence of downstream adverse events. The first approach does not\nextend to diseases where an accurate score cannot be elicited from experts. The\nsecond approach often produces scores that suffer from bias due to\ntreatment-related censoring (Paxton, 2013). We propose a novel ranking based\nframework for disease severity score learning (DSSL). DSSL exploits the\nfollowing key observation: while it is challenging for experts to quantify the\ndisease severity at any given time, it is often easy to compare the disease\nseverity at two different times. Extending existing ranking algorithms, DSSL\nlearns a function that maps a vector of patient's measurements to a scalar\nseverity score such that the resulting score is temporally smooth and\nconsistent with the expert's ranking of pairs of disease states. We apply DSSL\nto the problem of learning a sepsis severity score using a large, real-world\ndataset. The learned scores significantly outperform state-of-the-art clinical\nscores in ranking patient states by severity and in early detection of future\nadverse events. We also show that the learned disease severity trajectories are\nconsistent with clinical expectations of disease evolution. Further, using\nsimulated datasets, we show that DSSL exhibits better generalization\nperformance to changes in treatment patterns compared to the above approaches.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2015 03:56:37 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Dyagilev", "Kirill", ""], ["Saria", "Suchi", ""]]}, {"id": "1507.07536", "submitter": "Vassilis Kekatos", "authors": "Dimitris Berberidis, Vassilis Kekatos, Georgios B. Giannakis", "title": "Online Censoring for Large-Scale Regressions with Application to\n  Streaming Big Data", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2016.2546225", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear regression is arguably the most prominent among statistical inference\nmethods, popular both for its simplicity as well as its broad applicability. On\npar with data-intensive applications, the sheer size of linear regression\nproblems creates an ever growing demand for quick and cost efficient solvers.\nFortunately, a significant percentage of the data accrued can be omitted while\nmaintaining a certain quality of statistical inference with an affordable\ncomputational budget. The present paper introduces means of identifying and\nomitting \"less informative\" observations in an online and data-adaptive\nfashion, built on principles of stochastic approximation and data censoring.\nFirst- and second-order stochastic approximation maximum likelihood-based\nalgorithms for censored observations are developed for estimating the\nregression coefficients. Online algorithms are also put forth to reduce the\noverall complexity by adaptively performing censoring along with estimation.\nThe novel algorithms entail simple closed-form updates, and have provable\n(non)asymptotic convergence guarantees. Furthermore, specific rules are\ninvestigated for tuning to desired censoring patterns and levels of\ndimensionality reduction. Simulated tests on real and synthetic datasets\ncorroborate the efficacy of the proposed data-adaptive methods compared to\ndata-agnostic random projection-based alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2015 19:25:29 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Berberidis", "Dimitris", ""], ["Kekatos", "Vassilis", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1507.07587", "submitter": "Davide Pigoli", "authors": "Davide Pigoli, Pantelis Z. Hadjipantelis, John S. Coleman and John\n  A.D. Aston", "title": "The statistical analysis of acoustic phonetic data: exploring\n  differences between spoken Romance languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The historical and geographical spread from older to more modern languages\nhas long been studied by examining textual changes and in terms of changes in\nphonetic transcriptions. However, it is more difficult to analyze language\nchange from an acoustic point of view, although this is usually the dominant\nmode of transmission. We propose a novel analysis approach for acoustic\nphonetic data, where the aim will be to statistically model the acoustic\nproperties of spoken words. We explore phonetic variation and change using a\ntime-frequency representation, namely the log-spectrograms of speech\nrecordings. We identify time and frequency covariance functions as a feature of\nthe language; in contrast, mean spectrograms depend mostly on the particular\nword that has been uttered. We build models for the mean and covariances\n(taking into account the restrictions placed on the statistical analysis of\nsuch objects) and use these to define a phonetic transformation that models how\nan individual speaker would sound in a different language, allowing the\nexploration of phonetic differences between languages. Finally, we map back\nthese transformations to the domain of sound recordings, allowing us to listen\nto the output of the statistical analysis. The proposed approach is\ndemonstrated using recordings of the words corresponding to the numbers from\n\"one\" to \"ten\" as pronounced by speakers from five different Romance languages.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2015 21:10:42 GMT"}, {"version": "v2", "created": "Thu, 18 May 2017 11:07:13 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Pigoli", "Davide", ""], ["Hadjipantelis", "Pantelis Z.", ""], ["Coleman", "John S.", ""], ["Aston", "John A. D.", ""]]}, {"id": "1507.07869", "submitter": "Roman Ryutin Dr", "authors": "V.A. Petrov and R.A. Ryutin", "title": "Single and double diffractive dissociation and the problem of extraction\n  of the proton-Pomeron cross-section", "comments": "9 pages, 10 figures", "journal-ref": "International Journal of Modern Physics A, Vol. 31 (2016) 1650049", "doi": "10.1142/S0217751X16500494", "report-no": null, "categories": "hep-ph hep-th stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffractive dissociation processes are analysed in the framework of covariant\nreggeization. We have considered the general form of hadronic tensor and its\nasymptotic behaviour for $t\\rightarrow 0$ in the case of conserved tensor\ncurrents before reggeization. Resulting expressions for differential\ncross-section of single dissociation (SD) process ($pp \\rightarrow p M$),\ndouble dissociation (DD) ($pp\\rightarrow M_1 M_2$) and for the proton-Pomeron\ncross-section are given in detail, and corresponding problems of the approach\nare discussed.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2015 17:47:30 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Petrov", "V. A.", ""], ["Ryutin", "R. A.", ""]]}, {"id": "1507.08376", "submitter": "Li Chen", "authors": "Li Chen, Joshua T. Vogelstein, Vince Lyzinski, Carey E. Priebe", "title": "A Joint Graph Inference Case Study: the C.elegans Chemical and\n  Electrical Connectomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate joint graph inference for the chemical and electrical\nconnectomes of the \\textit{Caenorhabditis elegans} roundworm. The\n\\textit{C.elegans} connectomes consist of $253$ non-isolated neurons with known\nfunctional attributes, and there are two types of synaptic connectomes,\nresulting in a pair of graphs. We formulate our joint graph inference from the\nperspectives of seeded graph matching and joint vertex classification. Our\nresults suggest that connectomic inference should proceed in the joint space of\nthe two connectomes, which has significant neuroscientific implications.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 04:57:43 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2015 16:10:31 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Chen", "Li", ""], ["Vogelstein", "Joshua T.", ""], ["Lyzinski", "Vince", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1507.08484", "submitter": "Hideyasu Shimadzu", "authors": "Hideyasu Shimadzu, Maria Dornelas and Anne E. Magurran", "title": "Measuring temporal turnover in ecological communities", "comments": null, "journal-ref": "Methods in Ecology and Evolution 6(12) (2015) 1384-1394", "doi": "10.1111/2041-210X.12438", "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Range migrations in response to climate change, invasive species and the\nemergence of novel ecosystems highlight the importance of temporal turnover in\ncommunity composition as a fundamental part of global change in the\nAnthropocene. Temporal turnover is usually quantified using a variety of\nmetrics initially developed to capture spatial change. However, temporal\nturnover is the consequence of unidirectional community dynamics resulting from\nprocesses such as population growth, colonisation and local extinction. Here,\nwe develop a framework based on community dynamics, and propose a new temporal\nturnover measure. A simulation study and an analysis of an estuarine fish\ncommunity both clearly demonstrate that our proposed turnover measure offers\nadditional insights relative to spatial-context-based metrics. Our approach\nreveals whether community turnover is due to shifts in community composition or\nin community abundance, and identifies the species and/or environmental factors\nthat are responsible for any change.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 13:03:06 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2015 11:11:34 GMT"}, {"version": "v3", "created": "Thu, 21 Apr 2016 23:28:24 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Shimadzu", "Hideyasu", ""], ["Dornelas", "Maria", ""], ["Magurran", "Anne E.", ""]]}, {"id": "1507.08526", "submitter": "Allan De Freitas", "authors": "Allan De Freitas and Fran\\c{c}ois Septier and Lyudmila Mihaylova and\n  Simon Godsill", "title": "How Can Subsampling Reduce Complexity in Sequential MCMC Methods and\n  Deal with Big Data in Target Tracking?", "comments": "International Conference on Information Fusion, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Target tracking faces the challenge in coping with large volumes of data\nwhich requires efficient methods for real time applications. The complexity\nconsidered in this paper is when there is a large number of measurements which\nare required to be processed at each time step. Sequential Markov chain Monte\nCarlo (MCMC) has been shown to be a promising approach to target tracking in\ncomplex environments, especially when dealing with clutter. However, a large\nnumber of measurements usually results in large processing requirements. This\npaper goes beyond the current state-of-the-art and presents a novel Sequential\nMCMC approach that can overcome this challenge through adaptively subsampling\nthe set of measurements. Instead of using the whole large volume of available\ndata, the proposed algorithm performs a trade off between the number of\nmeasurements to be used and the desired accuracy of the estimates to be\nobtained in the presence of clutter. We show results with large improvements in\nprocessing time, more than 40% with a negligible loss in tracking performance,\ncompared with the solution without subsampling.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 14:54:41 GMT"}], "update_date": "2015-08-03", "authors_parsed": [["De Freitas", "Allan", ""], ["Septier", "Fran\u00e7ois", ""], ["Mihaylova", "Lyudmila", ""], ["Godsill", "Simon", ""]]}, {"id": "1507.08563", "submitter": "Dean Oliver", "authors": "Dean S. Oliver", "title": "Metropolized Randomized Maximum Likelihood for sampling from multimodal\n  distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes a method for using optimization to derive efficient\nindependent transition functions for Markov chain Monte Carlo simulations. Our\ninterest is in sampling from a posterior density $\\pi(x)$ for problems in which\nthe dimension of the model space is large, $\\pi(x)$ is multimodal with regions\nof low probability separating the modes, and evaluation of the likelihood is\nexpensive. We restrict our attention to the special case for which the target\ndensity is the product of a multivariate Gaussian prior and a likelihood\nfunction for which the errors in observations are additive and Gaussian.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 16:24:56 GMT"}, {"version": "v2", "created": "Tue, 12 Jul 2016 10:36:18 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Oliver", "Dean S.", ""]]}, {"id": "1507.08645", "submitter": "Reza Solgi", "authors": "Luke Bornn, Neil Shephard and Reza Solgi", "title": "Moment conditions and Bayesian nonparametrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR math.ST stat.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models phrased though moment conditions are central to much of modern\ninference. Here these moment conditions are embedded within a nonparametric\nBayesian setup. Handling such a model is not probabilistically straightforward\nas the posterior has support on a manifold. We solve the relevant issues,\nbuilding new probability and computational tools using Hausdorff measures to\nanalyze them on real and simulated data. These new methods which involve\nsimulating on a manifold can be applied widely, including providing Bayesian\nanalysis of quasi-likelihoods, linear and nonlinear regression, missing data\nand hierarchical models.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 19:45:52 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2016 18:58:42 GMT"}], "update_date": "2016-01-14", "authors_parsed": [["Bornn", "Luke", ""], ["Shephard", "Neil", ""], ["Solgi", "Reza", ""]]}]