[{"id": "1911.00151", "submitter": "Joe Watson", "authors": "Joe Watson, Ruth Joy, Dominic Tollit, Sheila J Thornton, Marie\n  Auger-M\\'eth\\'e", "title": "Estimating animal utilization distributions from multiple data types: a\n  joint spatio-temporal point process framework", "comments": "23 pages of main text + 13 page supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models of the spatial distribution of animals provide useful tools to help\necologists quantify species-environment relationships, and they are\nincreasingly being used to help determine the impacts of climate and habitat\nchanges on species. While high-quality survey-style data with known effort are\nsometimes available, often researchers have multiple datasets of varying\nquality and type. In particular, collections of sightings made by citizen\nscientists are becoming increasingly common, with no information typically\nprovided on their observer effort. Many standard modelling approaches ignore\nobserver effort completely, which can severely bias estimates of an animal's\ndistribution. Combining sightings data from observers who followed different\nprotocols is challenging. Any differences in observer skill, spatial effort,\nand the detectability of the animals across space all need to be accounted for.\nTo achieve this, we build upon the recent advancements made in integrative\nspecies distribution models and present a novel marked spatio-temporal point\nprocess framework for estimating the utilization distribution (UD) of the\nindividuals of a highly mobile species. We show that in certain settings, we\ncan also use the framework to combine the UDs from the sampled individuals to\nestimate the species' distribution. We combine the empirical results from a\nsimulation study with the implications outlined in a causal directed acyclic\ngraph to identify the necessary assumptions required for our framework to\ncontrol for observer effort when it is unknown. We then apply our framework to\ncombine multiple datasets collected on the endangered Southern Resident Killer\nWhales, to estimate their monthly effort-corrected space-use.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 23:43:16 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 18:43:09 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Watson", "Joe", ""], ["Joy", "Ruth", ""], ["Tollit", "Dominic", ""], ["Thornton", "Sheila J", ""], ["Auger-M\u00e9th\u00e9", "Marie", ""]]}, {"id": "1911.00356", "submitter": "Chen Jia", "authors": "Chen Jia", "title": "Kinetic foundation of the zero-inflated negative binomial model for\n  single-cell RNA sequencing data", "comments": "19 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.MN physics.bio-ph q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-cell RNA sequencing data have complex features such as dropout events,\nover-dispersion, and high-magnitude outliers, resulting in complicated\nprobability distributions of mRNA abundances that are statistically\ncharacterized in terms of a zero-inflated negative binomial (ZINB) model. Here\nwe provide a mesoscopic kinetic foundation of the widely used ZINB model based\non the biochemical reaction kinetics underlying transcription. Using multiscale\nmodeling and simplification techniques, we show that the ZINB distribution of\nmRNA abundance and the phenomenon of transcriptional bursting naturally emerge\nfrom a three-state stochastic transcription model. We further reveal a\nnontrivial quantitative relation between dropout events and transcriptional\nbursting, which provides novel insights into how and to what extent the burst\nsize and burst frequency could reduce the dropout rate. Three different\nbiophysical origins of over-dispersion are also clarified at the single-cell\nlevel.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 13:06:55 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Jia", "Chen", ""]]}, {"id": "1911.00420", "submitter": "Johan Wahlstr\\\"om", "authors": "Johan Wahlstrom, Andrew Markham, and Niki Trigoni", "title": "FootSLAM meets Adaptive Thresholding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calibration of the zero-velocity detection threshold is an essential\nprerequisite for zero-velocity-aided inertial navigation. However, the\nliterature is lacking a self-contained calibration method, suitable for\nlarge-scale use in unprepared environments without map information or\npre-deployed infrastructure. In this paper, the calibration of the\nzero-velocity detection threshold is formulated as a maximum likelihood\nproblem. The likelihood function is approximated using estimation quantities\nreadily available from the FootSLAM algorithm. Thus, we obtain a method for\nadaptive thresholding that does not require map information, measurements from\nsupplementary sensors, or user input. Experimental evaluations are conducted\nusing data with different gait speeds, sensor placements, and walking\ntrajectories. The proposed calibration method is shown to outperform\nfixed-threshold zero-velocity detectors and a benchmark using a speed-based\nthreshold classifier.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 15:20:00 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2020 11:11:14 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Wahlstrom", "Johan", ""], ["Markham", "Andrew", ""], ["Trigoni", "Niki", ""]]}, {"id": "1911.00469", "submitter": "Stefan B\\\"ohringer", "authors": "Stefan B\\\"ohringer and Dietmar Lohmann", "title": "Exact model comparisons in the plausibility framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plausibility is a formalization of exact tests for parametric models and\ngeneralizes procedures such as Fisher's exact test. The resulting tests are\nbased on cumulative probabilities of the probability density function and\nevaluate consistency with a parametric family while providing exact control of\nthe $\\alpha$ level for finite sample size. Model comparisons are inefficient in\nthis approach. We generalize plausibility by incorporating weighing which\nallows to perform model comparisons. We show that one weighing scheme is\nasymptotically equivalent to the likelihood ratio test (LRT) and has finite\nsample guarantees for the test size under the null hypothesis unlike the LRT.\nWe confirm theoretical properties in simulations that mimic the data set of our\ndata application. We apply the method to a retinoblastoma data set and\ndemonstrate a parent-of-origin effect. Weighted plausibility also has\napplications in high-dimensional data analysis and P-values for penalized\nregression models can be derived. We demonstrate superior performance as\ncompared to a data-splitting procedure in a simulation study. We apply weighted\nplausibility to a high-dimensional gene expression, case-control prostate\ncancer data set. We discuss the flexibility of the approach by relating\nweighted plausibility to targeted learning, the bootstrap, and sparsity\nselection.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 17:19:33 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 17:03:07 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["B\u00f6hringer", "Stefan", ""], ["Lohmann", "Dietmar", ""]]}, {"id": "1911.00511", "submitter": "Tu\\u{g}ba \\\"Onal-S\\\"uzek", "authors": "Talip Zengin, Tu\\u{g}ba \\\"Onal-S\\\"uzek", "title": "Analysis of Genomic and Transcriptomic Variations as Prognostic\n  Signature for Lung Adenocarcinoma", "comments": "46 pages", "journal-ref": null, "doi": "10.1186/s12859-020-03691-3", "report-no": null, "categories": "q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lung cancer is the leading cause of the largest number of deaths worldwide\nand lung adenocarcinoma (LUAD) is the most common form of lung cancer. In this\nstudy, we carried out an integrated meta-analysis of the mutations including\nsingle-nucleotide variations (SNVs), the copy number variations (CNVs), RNA-seq\nand clinical data of patients with LUAD downloaded from The Cancer Genome Atlas\n(TCGA). We integrated significant SNV and CNV genes, differentially expressed\ngenes (DEGs) and the DEGs in active subnetworks to construct a prognosis\nsignature. Cox proportional hazards model (LOOCV) with Lasso penalty was used\nto identify the best gene signature among different gene categories. The\npatients in both training and test data were clustered into high-risk and\nlow-risk groups by using risk scores of the patients calculated based on\nselected gene signature. We generated a 12-gene signature (DEPTOR, ZBTB16,\nBCHE, MGLL, MASP2, TNNI2, RAPGEF3, SGK2, MYO1A, CYP24A1, PODXL2, CCNA1) for\noverall survival prediction. The survival time of high-risk and low-risk groups\nwas significantly different. This 12-gene signature could predict prognosis and\nthey are potential predictors for the survival of the patients with LUAD.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 14:31:34 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 13:56:54 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Zengin", "Talip", ""], ["\u00d6nal-S\u00fczek", "Tu\u011fba", ""]]}, {"id": "1911.00512", "submitter": "Fui Swen Kuh", "authors": "F. Swen Kuh and Grace S. Chiu and Anton H. Westveld", "title": "Modeling National Latent Socioeconomic Health and Examination of Policy\n  Effects via Causal Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.GN q-fin.EC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research develops a socioeconomic health index for nations through a\nmodel-based approach which incorporates spatial dependence and examines the\nimpact of a policy through a causal modeling framework. As the gross domestic\nproduct (GDP) has been regarded as a dated measure and tool for benchmarking a\nnation's economic performance, there has been a growing consensus for an\nalternative measure---such as a composite `wellbeing' index---to holistically\ncapture a country's socioeconomic health performance. Many conventional ways of\nconstructing wellbeing/health indices involve combining different observable\nmetrics, such as life expectancy and education level, to form an index.\nHowever, health is inherently latent with metrics actually being observable\nindicators of health. In contrast to the GDP or other conventional health\nindices, our approach provides a holistic quantification of the overall\n`health' of a nation. We build upon the latent health factor index (LHFI)\napproach that has been used to assess the unobservable ecological/ecosystem\nhealth. This framework integratively models the relationship between metrics,\nthe latent health, and the covariates that drive the notion of health. In this\npaper, the LHFI structure is integrated with spatial modeling and statistical\ncausal modeling, so as to evaluate the impact of a policy variable (mandatory\nmaternity leave days) on a nation's socioeconomic health, while formally\naccounting for spatial dependency among the nations. We apply our model to\ncountries around the world using data on various metrics and potential\ncovariates pertaining to different aspects of societal health. The approach is\nstructured in a Bayesian hierarchical framework and results are obtained by\nMarkov chain Monte Carlo techniques.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 18:13:30 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Kuh", "F. Swen", ""], ["Chiu", "Grace S.", ""], ["Westveld", "Anton H.", ""]]}, {"id": "1911.00572", "submitter": "Tomi Peltola", "authors": "Tomi Peltola, Jussi Jokinen, Samuel Kaski", "title": "Probabilistic Formulation of the Take The Best Heuristic", "comments": "Annual Meeting of the Cognitive Science Society, CogSci 2018\n  Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The framework of cognitively bounded rationality treats problem solving as\nfundamentally rational, but emphasises that it is constrained by cognitive\narchitecture and the task environment. This paper investigates a simple\ndecision making heuristic, Take The Best (TTB), within that framework. We\nformulate TTB as a likelihood-based probabilistic model, where the decision\nstrategy arises by probabilistic inference based on the training data and the\nmodel constraints. The strengths of the probabilistic formulation, in addition\nto providing a bounded rational account of the learning of the heuristic,\ninclude natural extensibility with additional cognitively plausible constraints\nand prior information, and the possibility to embed the heuristic as a subpart\nof a larger probabilistic model. We extend the model to learn cue\ndiscrimination thresholds for continuous-valued cues and experiment with using\nthe model to account for biased preference feedback from a boundedly rational\nagent in a simulated interactive machine learning task.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 20:08:37 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Peltola", "Tomi", ""], ["Jokinen", "Jussi", ""], ["Kaski", "Samuel", ""]]}, {"id": "1911.00708", "submitter": "Johnatan Cardona Jim\\'enez", "authors": "Johnatan Cardona Jim\\'enez", "title": "fMRI group analysis based on outputs from Matrix-Variate Dynamic Linear\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we describe in more detail how to perform fMRI group analysis\nusing inputs from modeling fMRI signal using Matrix-Variate Dynamic Linear\nModels (MDLM) at the individual level. After computing a posterior distribution\nfor the average group activation, the three algorithms (FEST, FSTS, and FFBS)\nproposed from the previous work by Jim\\'enez et al. [2019] can be easily\nimplemented. We also propose an additional algorithm, which we call\nAG-algorithm, to draw on-line trajectories of the state parameter and therefore\nassess voxel activation at the group level. The performance of our method is\nillustrated through one practical example using real fMRI data from a\n\"voice-localizer\" experiment.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 13:04:32 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Jim\u00e9nez", "Johnatan Cardona", ""]]}, {"id": "1911.00817", "submitter": "Ali Eshragh", "authors": "Ali Eshragh, Benjamin Ganim, Terry Perkins, Kasun Bandara", "title": "The Importance of Environmental Factors in Forecasting Australian Power\n  Demand", "comments": "Keywords: Electricity power peak demand forecasting, Environmental\n  factors, SARIMA-regression Model", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a time series model to forecast weekly peak power demand for three\nmain states of Australia for a yearly time-scale, and show the crucial role of\nenvironmental factors in improving the forecasts. More precisely, we construct\na seasonal autoregressive integrated moving average (SARIMA) model and\nreinforce it by employing the exogenous environmental variables including,\nmaximum temperature, minimum temperature, and solar exposure. The estimated\nhybrid SARIMA-regression model exhibits an excellent mean absolute percentage\nerror (MAPE) of 3.45%. More importantly, our analysis demonstrates the\nimportance of the environmental variables by showing an average MAPE\nimprovement of 45% for the hybrid model over the crude one which does not\ninclude the environmental variables. In order to illustrate the efficacy of our\nmodel, we compare our outcome with the state-of-the-art machine learning\nmethods in forecasting. The results reveal that our model outperforms the\nlatter approach.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2019 03:17:49 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 11:34:31 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Eshragh", "Ali", ""], ["Ganim", "Benjamin", ""], ["Perkins", "Terry", ""], ["Bandara", "Kasun", ""]]}, {"id": "1911.00827", "submitter": "Roberto da Silva", "authors": "Roberto da Silva, Sandra D. Prado", "title": "A simple study of the correlation effects in the superposition of waves\n  of electric fields: the emergence of extreme events", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": "10.1016/j.physleta.2019.126231", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the effects of correlated random phases in the\nintensity of a superposition of $N$ wave-fields. Our results suggest that\nregardless of whether the phase distribution is continuous or discrete if the\nphases are random correlated variables, we must observe a heavier tail\ndistribution and the emergence of extreme events as the correlation between\nphases increases. We believe that such a simple method can be easily applied in\nother situations to show the existence of extreme statistical events in the\ncontext of nonlinear complex systems.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2019 04:24:33 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["da Silva", "Roberto", ""], ["Prado", "Sandra D.", ""]]}, {"id": "1911.01081", "submitter": "Alvaro Mendez Civieta", "authors": "\\'Alvaro M\\'endez Civieta and M. Carmen Aguilera-Morillo and Rosa E.\n  Lillo", "title": "Quantile regression: a penalization approach", "comments": "9 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse group LASSO (SGL) is a penalization technique used in regression\nproblems where the covariates have a natural grouped structure and provides\nsolutions that are both between and within group sparse. In this paper the SGL\nis introduced to the quantile regression (QR) framework, and a more flexible\nversion, the adaptive sparse group LASSO (ASGL), is proposed. This proposal\nadds weights to the penalization improving prediction accuracy. Usually,\nadaptive weights are taken as a function of the original nonpenalized solution\nmodel. This approach is only feasible in the n > p framework. In this work, a\nsolution that allows using adaptive weights in high-dimensional scenarios is\nproposed. The benefits of this proposal are studied both in synthetic and real\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 09:17:34 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Civieta", "\u00c1lvaro M\u00e9ndez", ""], ["Aguilera-Morillo", "M. Carmen", ""], ["Lillo", "Rosa E.", ""]]}, {"id": "1911.01257", "submitter": "Xiaoting Du", "authors": "Yuan Zhou, Xiaoting Du, Yeda Zhang, and Sun-Yuan Kung", "title": "Cross-Scale Residual Network for Multiple Tasks:Image Super-resolution,\n  Denoising, and Deblocking", "comments": "11 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV eess.SP stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In general, image restoration involves mapping from low quality images to\ntheir high-quality counterparts. Such optimal mapping is usually non-linear and\nlearnable by machine learning. Recently, deep convolutional neural networks\nhave proven promising for such learning processing. It is desirable for an\nimage processing network to support well with three vital tasks, namely,\nsuper-resolution, denoising, and deblocking. It is commonly recognized that\nthese tasks have strong correlations. Therefore, it is imperative to harness\nthe inter-task correlations. To this end, we propose the cross-scale residual\nnetwork to exploit scale-related features and the inter-task correlations among\nthe three tasks. The proposed network can extract multiple spatial scale\nfeatures and establish multiple temporal feature reusage. Our experiments show\nthat the proposed approach outperforms state-of-the-art methods in both\nquantitative and qualitative evaluations for multiple image restoration tasks.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 14:51:25 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Zhou", "Yuan", ""], ["Du", "Xiaoting", ""], ["Zhang", "Yeda", ""], ["Kung", "Sun-Yuan", ""]]}, {"id": "1911.01304", "submitter": "Nathan Gold", "authors": "Nathan Gold, Christophe L. Herry, Xiaogang Wang, Martin G. Frasch", "title": "Fetal cardiovascular decompensation during labor predicted from the\n  individual heart rate: a prospective study in fetal sheep near term and the\n  impact of low sampling rate", "comments": null, "journal-ref": "Front. Pediatr. 2021", "doi": "10.3389/fped.2021.593889", "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a novel computerized fetal heart rate intrapartum algorithm for\nearly and individualized prediction of fetal cardiovascular decompensation, a\nkey event in the causal chain leading to brain injury. This real-time machine\nlearning algorithm performs well on noisy fetal heart rate data and requires ~2\nhours to train on the individual fetal heart rate tracings in the first stage\nof labor; once trained, the algorithm predicts the event of fetal\ncardiovascular decompensation with 92% sensitivity. We show that the\nalgorithm's performance suffers reducing sensitivity to 67% when the fetal\nheart rate is acquired at the sampling rate of 4 Hz used in ultrasound\ncardiotocographic monitors compared to the electrocardiogram(ECG)-derived\nsignals as can be acquired from maternal abdominal ECG.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 16:12:57 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 21:28:33 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Gold", "Nathan", ""], ["Herry", "Christophe L.", ""], ["Wang", "Xiaogang", ""], ["Frasch", "Martin G.", ""]]}, {"id": "1911.01516", "submitter": "Guanhua Fang", "authors": "Guanhua Fang and Zhiliang Ying", "title": "Latent Theme Dictionary Model for Finding Co-occurrent Patterns in\n  Process Data", "comments": "65 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Process data, temporally ordered categorical observations, are of recent\ninterest due to its increasing abundance and the desire to extract useful\ninformation. A process is a collection of time-stamped events of different\ntypes, recording how an individual behaves in a given time period. The process\ndata are too complex in terms of size and irregularity for the classical\npsychometric models to be applicable, at least directly, and, consequently, it\nis desirable to develop new ways for modeling and analysis. We introduce herein\na latent theme dictionary model (LTDM) for processes that identifies\nco-occurrent event patterns and individuals with similar behavioral patterns.\nTheoretical properties are established under certain regularity conditions for\nthe likelihood based estimation and inference. A non-parametric Bayes LTDM\nalgorithm using the Markov Chain Monte Carlo method is proposed for\ncomputation. Simulation studies show that the proposed approach performs well\nin a range of situations. The proposed method is applied to an item in the 2012\nProgramme for International Student Assessment with interpretable findings.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 22:22:08 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 03:03:37 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Fang", "Guanhua", ""], ["Ying", "Zhiliang", ""]]}, {"id": "1911.01525", "submitter": "Wei Han", "authors": "Wei Han, Yun Yang", "title": "Statistical Inference in Mean-Field Variational Bayes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We conduct non-asymptotic analysis on the mean-field variational inference\nfor approximating posterior distributions in complex Bayesian models that may\ninvolve latent variables. We show that the mean-field approximation to the\nposterior can be well-approximated relative to the Kullback-Leibler divergence\ndiscrepancy measure by a normal distribution whose center is the maximum\nlikelihood estimator (MLE). In particular, our results imply that the center of\nthe mean-field approximation matches the MLE up to higher-order terms and there\nis essentially no loss of efficiency in using it as a point estimator for the\nparameter in any regular parametric model with latent variables. We also\npropose a new class of variational weighted likelihood bootstrap (VWLB) methods\nfor quantifying the uncertainty in the mean-field variational inference. The\nproposed VWLB can be viewed as a new sampling scheme that produces independent\nsamples for approximating the posterior. Comparing with traditional sampling\nalgorithms such Markov Chain Monte Carlo, VWLB can be implemented in parallel\nand is free of tuning.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 23:08:49 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Han", "Wei", ""], ["Yang", "Yun", ""]]}, {"id": "1911.01583", "submitter": "Guanhua Fang", "authors": "Haochen Xu and Guanhua Fang and Zhiliang Ying", "title": "A Latent Topic Model with Markovian Transition for Process Data", "comments": "42 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a latent topic model with a Markovian transition for process data,\nwhich consist of time-stamped events recorded in a log file. Such data are\nbecoming more widely available in computer-based educational assessment with\ncomplex problem solving items. The proposed model can be viewed as an extension\nof the hierarchical Bayesian topic model with a hidden Markov structure to\naccommodate the underlying evolution of an examinee's latent state. Using topic\ntransition probabilities along with response times enables us to capture\nexaminees' learning trajectories, making clustering/classification more\nefficient. A forward-backward variational expectation-maximization (FB-VEM)\nalgorithm is developed to tackle the challenging computational problem. Useful\ntheoretical properties are established under certain asymptotic regimes. The\nproposed method is applied to a complex problem solving item in 2012 Programme\nfor International Student Assessment (PISA 2012).\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 02:59:17 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Xu", "Haochen", ""], ["Fang", "Guanhua", ""], ["Ying", "Zhiliang", ""]]}, {"id": "1911.01607", "submitter": "Inez Maria Zwetsloot", "authors": "Inez Maria Zwetsloot, Tahir Mahmood and William H. Woodall", "title": "Multivariate Time-Between-Events Monitoring -- An overview and some\n  (overlooked) underlying complexities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review methods for monitoring multivariate time-between-events (TBE) data.\nWe present some underlying complexities that have been overlooked in the\nliterature. It is helpful to classify multivariate TBE monitoring applications\ninto two fundamentally different scenarios. One scenario involves monitoring\nindividual vectors of TBE data. The other involves the monitoring of several,\npossibly correlated, temporal point processes in which events could occur at\ndifferent rates. We discuss performance measures and advise the use of\ntime-between-signal based metrics for the design and comparison of methods. We\nre-evaluate an existing multivariate TBE monitoring method, offer some advice\nand some directions for future research.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 04:01:55 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Zwetsloot", "Inez Maria", ""], ["Mahmood", "Tahir", ""], ["Woodall", "William H.", ""]]}, {"id": "1911.01733", "submitter": "Xiaozhou Yang", "authors": "Xiaozhou Yang, Nan Chen and Chao Zhai", "title": "A Control Chart Approach to Power System Line Outage Detection Under\n  Transient Dynamics", "comments": "9 pages, 8 figures, under review for IEEE Transactions on Power\n  Systems", "journal-ref": null, "doi": "10.1109/TPWRS.2020.3006465", "report-no": null, "categories": "eess.SY cs.SY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online transmission line outage detection over the entire network enables\ntimely corrective action to be taken, which prevents a local event from\ncascading into a large scale blackout. Line outage detection aims to detect an\noutage as soon as possible after it happened. Traditional methods either do not\nconsider the transient dynamics following an outage or require a full Phasor\nMeasurement Unit (PMU) deployment. Using voltage phase angle data collected\nfrom a limited number of PMUs, we propose a real-time dynamic outage detection\nscheme based on alternating current (AC) power flow model and statistical\nchange detection theory. The proposed method can capture system dynamics since\nit retains the time-variant and nonlinear nature of the power system. The\nmethod is computationally efficient and scales to large and realistic networks.\nExtensive simulation studies on IEEE 39-bus and 2383-bus systems demonstrated\nthe effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 11:50:55 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 07:38:04 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Yang", "Xiaozhou", ""], ["Chen", "Nan", ""], ["Zhai", "Chao", ""]]}, {"id": "1911.01808", "submitter": "George Streftaris", "authors": "David Thong, George Streftaris and Gavin J. Gibson", "title": "Latent likelihood ratio tests for assessing spatial kernels in epidemic\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most important issues in the critical assessment of\nspatio-temporal stochastic models for epidemics is the selection of the\ntransmission kernel used to represent the relationship between infectious\nchallenge and spatial separation of infected and susceptible hosts. As the\ndesign of control strategies is often based on an assessment of the distance\nover which transmission can realistically occur and estimation of this distance\nis very sensitive to the choice of kernel function, it is important that models\nused to inform control strategies can be scrutinised in the light of\nobservation in order to elicit possible evidence against the selected kernel\nfunction. While a range of approaches to model criticism are in existence, the\nfield remains one in which the need for further research is recognised. In this\npaper, building on earlier contributions by the authors, we introduce a new\napproach to assessing the validity of spatial kernels - the latent likelihood\nratio tests - and compare its capacity to detect model misspecification with\nthat of tests based on the use of infection-link residuals. We demonstrate that\nthe new approach, which combines Bayesian and frequentist ideas by treating the\nstatistical decision maker as a complex entity, can be used to formulate tests\nwith greater power than infection-link residuals to detect kernel\nmisspecification particularly when the degree of misspecification is modest.\nThis new approach avoids the use of a fully Bayesian approach which may\nintroduce undesirable complications related to computational complexity and\nprior sensitivity.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 14:36:56 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Thong", "David", ""], ["Streftaris", "George", ""], ["Gibson", "Gavin J.", ""]]}, {"id": "1911.01815", "submitter": "Leonardo Egidi PhD", "authors": "Leonardo Egidi and Ioannis Ntzoufras", "title": "A Bayesian Quest for Finding a Unified Model for Predicting Volleyball\n  Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Volleyball is a team sport with unique and specific characteristics. We\nintroduce a new two level-hierarchical Bayesian model which accounts for theses\nvolleyball specific characteristics. In the first level, we model the set\noutcome with a simple logistic regression model. Conditionally on the winner of\nthe set, in the second level, we use a truncated negative binomial distribution\nfor the points earned by the loosing team. An additional Poisson distributed\ninflation component is introduced to model the extra points played in the case\nthat the two teams have point difference less than two points. The number of\npoints of the winner within each set is deterministically specified by the\nwinner of the set and the points of the inflation component. The team specific\nabilities and the home effect are used as covariates on all layers of the model\n(set, point, and extra inflated points). The implementation of the proposed\nmodel on the Italian Superlega 2017/2018 data shows an exceptional\nreproducibility of the final league table and a satisfactory predictive\nability.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 14:39:54 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 09:20:20 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Egidi", "Leonardo", ""], ["Ntzoufras", "Ioannis", ""]]}, {"id": "1911.01850", "submitter": "Niklas Pfister", "authors": "Niklas Pfister, Evan G. Williams, Jonas Peters, Ruedi Aebersold and\n  Peter B\\\"uhlmann", "title": "Stabilizing Variable Selection and Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider regression in which one predicts a response $Y$ with a set of\npredictors $X$ across different experiments or environments. This is a common\nsetup in many data-driven scientific fields and we argue that statistical\ninference can benefit from an analysis that takes into account the\ndistributional changes across environments. In particular, it is useful to\ndistinguish between stable and unstable predictors, i.e., predictors which have\na fixed or a changing functional dependence on the response, respectively. We\nintroduce stabilized regression which explicitly enforces stability and thus\nimproves generalization performance to previously unseen environments. Our work\nis motivated by an application in systems biology. Using multiomic data, we\ndemonstrate how hypothesis generation about gene function can benefit from\nstabilized regression. We believe that a similar line of arguments for\nexploiting heterogeneity in data can be powerful for many other applications as\nwell. We draw a theoretical connection between multi-environment regression and\ncausal models, which allows to graphically characterize stable versus unstable\nfunctional dependence on the response. Formally, we introduce the notion of a\nstable blanket which is a subset of the predictors that lies between the direct\ncausal predictors and the Markov blanket. We prove that this set is optimal in\nthe sense that a regression based on these predictors minimizes the mean\nsquared prediction error given that the resulting regression generalizes to\nunseen new environments.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 15:04:33 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 05:58:56 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Pfister", "Niklas", ""], ["Williams", "Evan G.", ""], ["Peters", "Jonas", ""], ["Aebersold", "Ruedi", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "1911.01874", "submitter": "Abel Dasylva Dr.", "authors": "Abel Dasylva, Arthur Goussanou, David Ajavon and Hanan Abousaleh", "title": "Revisiting the probabilistic method of record linkage", "comments": "Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DB cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In theory, the probabilistic linkage method provides two distinct advantages\nover non-probabilistic methods, including minimal rates of linkage error and\naccurate measures of these rates for data users. However, implementations can\nfall short of these expectations either because the conditional independence\nassumption is made, or because a model with interactions is used but lacks the\nidentification property. In official statistics, this is currently the main\nchallenge to the automated production and use of linked data. To address this\nchallenge, a new methodology is described for proper linkage problems, where\nmatched records may be identified with a probability that is bounded away from\nzero, regardless of the population size. It models the number of neighbours of\na given record, i.e. the number of resembling records. To be specific, the\nproposed model is a finite mixture where each component is the sum of a\nBernoulli variable with an independent Poisson variable. It has the\nidentification property and yields solutions for many longstanding problems,\nincluding the evaluation of blocking criteria and the estimation of linkage\nerrors for probabilistic or non-probabilistic linkages, all without clerical\nreviews or conditional independence assumptions. Thus it also enables\nunsupervised machine learning solutions for record linkage problems.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 15:28:46 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Dasylva", "Abel", ""], ["Goussanou", "Arthur", ""], ["Ajavon", "David", ""], ["Abousaleh", "Hanan", ""]]}, {"id": "1911.01910", "submitter": "Federico Ferrari", "authors": "Federico Ferrari and David B. Dunson", "title": "Identifying main effects and interactions among exposures using Gaussian\n  processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is motivated by the problem of studying the joint effect of\ndifferent chemical exposures on human health outcomes. This is essentially a\nnonparametric regression problem, with interest being focused not on a black\nbox for prediction but instead on selection of main effects and interactions.\nFor interpretability, we decompose the expected health outcome into a linear\nmain effect, pairwise interactions, and a non-linear deviation. Our interest is\nin model selection for these different components, accounting for uncertainty\nand addressing non-identifability between the linear and nonparametric\ncomponents of the semiparametric model. We propose a Bayesian approach to\ninference, placing variable selection priors on the different components, and\ndeveloping a Markov chain Monte Carlo (MCMC) algorithm. A key component of our\napproach is the incorporation of a heredity constraint to only include\ninteractions in the presence of main effects, effectively reducing\ndimensionality of the model search. We adapt a projection approach developed in\nthe spatial statistics literature to enforce identifiability in modeling the\nnonparametric component using a Gaussian process. We also employ a dimension\nreduction strategy to sample the non-linear random effects that aids the mixing\nof the MCMC algorithm. The proposed MixSelect framework is evaluated using a\nsimulation study, and is illustrated using data from the National Health and\nNutrition Examination Survey (NHANES). Code is available on GitHub.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 16:15:07 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 17:31:10 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Ferrari", "Federico", ""], ["Dunson", "David B.", ""]]}, {"id": "1911.01919", "submitter": "Shaoming Xie", "authors": "Shao-Ming Xie", "title": "Neural Network Based Parameter Estimation Method for the Pareto/NBD\n  Model", "comments": "35 pages, 6 figures, 14 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whether stochastic or parametric, the Pareto/NBD model can only be utilized\nfor an in-sample prediction rather than an out-of-sample prediction. This\nresearch thus provides a neural network based extension of the Pareto/NBD model\nto estimate the out-of-sample parameters, which overrides the estimation burden\nand the application dilemma of the Pareto/NBD approach. The empirical results\nindicate that the Pareto/NBD model and neural network algorithms have similar\npredictability for identifying inactive customers. Even with a strong trend\nfitting on the customer count of each repeat purchase point, the Pareto/NBD\nmodel underestimates repeat purchases at both the individual and aggregate\nlevels. Nonetheless, when embedding the likelihood function of the Pareto/NBD\nmodel into the loss function, the proposed parameter estimation method shows\nextraordinary predictability on repeat purchases at these two levels.\nFurthermore, the proposed neural network based method is highly efficient and\nresource-friendly and can be deployed in cloud computing to handle with big\ndata analysis.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 16:27:41 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Xie", "Shao-Ming", ""]]}, {"id": "1911.01961", "submitter": "Mayrim Vega-Hern\\'andez", "authors": "Mayrim Vega-Hern\\'andez, Jos\\'e M. S\\'anchez-Bornot, Agust\\'in\n  Lage-Castellanos, Jhoanna P\\'erez-Hidalgo-Gato, Jos\\'e E. Alvarez-Iglesias,\n  Daysi Garc\\'ia-Agustin, Eduardo Mart\\'inez-Montes, Pedro A. Vald\\'es-Sosa", "title": "Multiple penalized least squares and sign constraints with modified\n  Newton-Raphson algorithms: application to EEG source imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple penalized least squares (MPLS) models are a more flexible approach\nto find adaptive least squares solutions required to be simultaneously sparse\nand smooth. This is particularly important when addressing real-life inverse\nproblems where there is no ground truth available, such as electrophysiological\nsource imaging. In this work we formalize a modified Newton-Raphson (MNR)\nalgorithm to estimate general MPLS models, and propose its extension to perform\nefficient optimization over the active set of selected features (AMNR). This\nalgorithm can be used to minimize continuously differentiable objective\nfunctions with multiple restrictions, including sign constraints. We show that\nthese algorithms provide solutions with acceptable reconstruction in simulated\nscenarios that do not cope with model assumptions, and for low n/p ratios. We\nthen use both algorithms for estimating different electroencephalography (EEG)\ninverse models with multiple penalties. We also show how the AMNR allows us to\nestimate new models in the EEG inverse problem context, such as nonnegative\nversions of Smooth Garrote and Smooth LASSO. Due to its similarity to the least\nangle regression (LARS) algorithm, synthetic data were used for a preliminary\ncomparison between solutions obtained using both AMNR and LARS for the same\nmodels, according to well-known quality measures. A visual event-related EEG\nfrom healthy young subjects and a resting-state EEG study associated to walking\nspeed decline in active elders were used to illustrate its usefulness in the\nanalysis of real experimental data.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 17:40:50 GMT"}, {"version": "v2", "created": "Mon, 18 Nov 2019 17:41:59 GMT"}, {"version": "v3", "created": "Tue, 29 Jun 2021 20:19:10 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Vega-Hern\u00e1ndez", "Mayrim", ""], ["S\u00e1nchez-Bornot", "Jos\u00e9 M.", ""], ["Lage-Castellanos", "Agust\u00edn", ""], ["P\u00e9rez-Hidalgo-Gato", "Jhoanna", ""], ["Alvarez-Iglesias", "Jos\u00e9 E.", ""], ["Garc\u00eda-Agustin", "Daysi", ""], ["Mart\u00ednez-Montes", "Eduardo", ""], ["Vald\u00e9s-Sosa", "Pedro A.", ""]]}, {"id": "1911.02171", "submitter": "Xin Xing", "authors": "Xin Xing, Zuofeng Shang, Pang Du, Ping Ma, Wenxuan Zhong and Jun S.\n  Liu", "title": "Minimax Nonparametric Two-sample Test under Smoothing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of comparing probability densities between two\ngroups. A new probabilistic tensor product smoothing spline framework is\ndeveloped to model the joint density of two variables. Under such a framework,\nthe probability density comparison is equivalent to testing the\npresence/absence of interactions. We propose a penalized likelihood ratio test\nfor such interaction testing and show that the test statistic is asymptotically\nchi-square distributed under the null hypothesis. Furthermore, we derive a\nsharp minimax testing rate based on the Bernstein width for nonparametric\ntwo-sample tests and show that our proposed test statistics is minimax optimal.\nIn addition, a data-adaptive tuning criterion is developed to choose the\npenalty parameter. Simulations and real applications demonstrate that the\nproposed test outperforms the conventional approaches under various scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 02:40:35 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 23:05:21 GMT"}, {"version": "v3", "created": "Sun, 5 Jan 2020 19:57:41 GMT"}, {"version": "v4", "created": "Mon, 11 Jan 2021 19:34:53 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Xing", "Xin", ""], ["Shang", "Zuofeng", ""], ["Du", "Pang", ""], ["Ma", "Ping", ""], ["Zhong", "Wenxuan", ""], ["Liu", "Jun S.", ""]]}, {"id": "1911.02175", "submitter": "Robert Osazuwa Ness", "authors": "Robert Osazuwa Ness, Kaushal Paneri, and Olga Vitek", "title": "Integrating Markov processes with structural causal modeling enables\n  counterfactual inference in complex systems", "comments": "Accepted to Thirty-third Conference on Neural Information Processing\n  Systems (2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.MN stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This manuscript contributes a general and practical framework for casting a\nMarkov process model of a system at equilibrium as a structural causal model,\nand carrying out counterfactual inference. Markov processes mathematically\ndescribe the mechanisms in the system, and predict the system's equilibrium\nbehavior upon intervention, but do not support counterfactual inference. In\ncontrast, structural causal models support counterfactual inference, but do not\nidentify the mechanisms. This manuscript leverages the benefits of both\napproaches. We define the structural causal models in terms of the parameters\nand the equilibrium dynamics of the Markov process models, and counterfactual\ninference flows from these settings. The proposed approach alleviates the\nidentifiability drawback of the structural causal models, in that the\ncounterfactual inference is consistent with the counterfactual trajectories\nsimulated from the Markov process model. We showcase the benefits of this\nframework in case studies of complex biomolecular systems with nonlinear\ndynamics. We illustrate that, in presence of Markov process model\nmisspecification, counterfactual inference leverages prior data, and therefore\nestimates the outcome of an intervention more accurately than a direct\nsimulation.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 03:01:45 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Ness", "Robert Osazuwa", ""], ["Paneri", "Kaushal", ""], ["Vitek", "Olga", ""]]}, {"id": "1911.02249", "submitter": "Ghulam Abdul Qadir", "authors": "Ghulam A. Qadir, Ying Sun, Sebastian Kurtek", "title": "Estimation of Spatial Deformation for Nonstationary Processes via\n  Variogram Alignment", "comments": null, "journal-ref": "Technometrics 2021", "doi": "10.1080/00401706.2021.1883481", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modeling spatial processes, a second-order stationarity assumption is\noften made. However, for spatial data observed on a vast domain, the covariance\nfunction often varies over space, leading to a heterogeneous spatial dependence\nstructure, therefore requiring nonstationary modeling. Spatial deformation is\none of the main methods for modeling nonstationary processes, assuming the\nnonstationary process has a stationary counterpart in the deformed space. The\nestimation of the deformation function poses severe challenges. Here, we\nintroduce a novel approach for nonstationary geostatistical modeling, using\nspace deformation, when a single realization of the spatial process is\nobserved. Our method is based, at a fundamental level, on aligning regional\nvariograms, where warping variability of the distance from each subregion\nexplains the spatial nonstationarity. We propose to use multi-dimensional\nscaling to map the warped distances to spatial locations. We asses the\nperformance of our new method using multiple simulation studies. Additionally,\nwe illustrate our methodology on precipitation data to estimate the\nheterogeneous spatial dependence and to perform spatial predictions.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 08:33:28 GMT"}, {"version": "v2", "created": "Thu, 7 Nov 2019 09:01:11 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Qadir", "Ghulam A.", ""], ["Sun", "Ying", ""], ["Kurtek", "Sebastian", ""]]}, {"id": "1911.02258", "submitter": "Ghulam Qadir", "authors": "Ghulam A. Qadir, Ying Sun", "title": "Semiparametric Estimation of Cross-covariance Functions for Multivariate\n  Random Fields", "comments": null, "journal-ref": "Biometric Methodology 2020", "doi": "10.1111/biom.13323", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prevalence of spatially referenced multivariate data has impelled\nresearchers to develop a procedure for the joint modeling of multiple spatial\nprocesses. This ordinarily involves modeling marginal and cross-process\ndependence for any arbitrary pair of locations using a multivariate spatial\ncovariance function. However, building a flexible multivariate spatial\ncovariance function that is nonnegative definite is challenging. Here, we\npropose a semiparametric approach for multivariate spatial covariance function\nestimation with approximate Mat\\'ern marginals and highly flexible\ncross-covariance functions via their spectral representations. The flexibility\nin our cross-covariance function arises due to B-spline based specification of\nthe underlying coherence functions, which in turn allows us to capture\nnon-trivial cross-spectral features. We then develop a likelihood-based\nestimation procedure and perform multiple simulation studies to demonstrate the\nperformance of our method, especially on the coherence function estimation.\nFinally, we analyze particulate matter concentrations ($\\text{PM}_{2.5}$) and\nwind speed data over the North-Eastern region of the United States, where we\nillustrate that our proposed method outperforms the commonly used full\nbivariate Mat\\'ern model and the linear model of coregionalization for spatial\nprediction.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 08:53:31 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Qadir", "Ghulam A.", ""], ["Sun", "Ying", ""]]}, {"id": "1911.02272", "submitter": "Leanne McCabe", "authors": "L. McCabe (1), I. R. White (1), N. V. Vinh Chau (2), E. Barnes (3), S.\n  L. Pett (1), G. S. Cooke (4), A. S. Walker (1) (for the SEARCH investigators,\n  (1) MRC Clinical Trials Unit at UCL, (2) Hospital for Tropical Diseases Ho\n  Chi Minh City, (3) Oxford University, (4) Imperial College London)", "title": "The design and statistical aspects of VIETNARMS: a strategic\n  post-licensing trial of multiple oral direct acting antiviral Hepatitis C\n  treatment strategies in Vietnam", "comments": "26 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background Achieving hepatitis C elimination is hampered by the costs of\ntreatment and the need to treat hard-to-reach populations. Treatment access\ncould be widened by shortening treatment, but limited research means it is\nunclear which strategies could achieve sufficiently high cure rates to be\nacceptable. We present the statistical aspects of a multi-arm trial designed to\ntest multiple strategies simultaneously with a monitoring mechanism to detect\nand stop those with unacceptably low cure rates quickly. Methods The VIETNARMS\ntrial will factorially randomise patients to three randomisations. We will use\nBayesian monitoring at interim analyses to detect and stop recruitment into\nunsuccessful strategies, defined as a >0.95 posterior probability of the true\ncure rate being <90%. Here, we tested the operating characteristics of the\nstopping guideline, planned the timing of the interim analyses and explored\npower at the final analysis. Results A beta(4.5, 0.5) prior for the true cure\nrate produces <0.05 probability of incorrectly stopping a group with true cure\nrate >90%. Groups with very low cure rates (<60%) are very likely (>0.9\nprobability) to stop after ~25% patients are recruited. Groups with moderately\nlow cure rates (80%) are likely to stop (0.7 probability) before the end of\nrecruitment. Interim analyses 7, 10, 13 and 18 months after recruitment\ncommences provide good probabilities of stopping inferior groups. For an\noverall true cure rate of 95%, power is >90% to detect non-inferiority in the\nregimen and strategy comparisons using 5% and 10% margins respectively,\nregardless of the control cure rate, and to detect a 5% absolute difference in\nthe ribavirin comparison. Conclusions The operating characteristics of the\nstopping guideline are appropriate and interim analyses can be timed to detect\nfailing groups at various stages.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 09:42:36 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["McCabe", "L.", ""], ["White", "I. R.", ""], ["Chau", "N. V. Vinh", ""], ["Barnes", "E.", ""], ["Pett", "S. L.", ""], ["Cooke", "G. S.", ""], ["Walker", "A. S.", ""]]}, {"id": "1911.02418", "submitter": "Yinzhi Wang", "authors": "Yinzhi Wang, Ingrid Hob{\\ae}k Haff and Arne Huseby", "title": "Modelling extreme claims via composite models and threshold selection\n  methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existence of large and extreme claims of a non-life insurance portfolio\ninfluences the ability of (re)insurers to estimate the reserve. The excess\nover-threshold method provides a way to capture and model the typical behaviour\nof insurance claim data. This paper discusses several composite models with\ncommonly used bulk distributions, combined with a 2-parameter Pareto\ndistribution above the threshold. We have explored how several threshold\nselection methods perform when estimating the reserve as well as the effect of\nthe choice of bulk distribution, with varying sample size and tail properties.\nTo investigate this, a simulation study has been performed. Our study shows\nthat when data are sufficient, the square root rule has the overall best\nperformance in terms of the quality of the reserve estimate. The second best is\nthe exponentiality test, especially when the right tail of the data is extreme.\nAs the sample size becomes small, the simultaneous estimation has the best\nperformance. Further, the influence of the choice of bulk distribution seems to\nbe rather large, especially when the distribution is heavy-tailed. Moreover, it\nshows that the empirical estimate of $p_{\\leq b}$, the probability that a claim\nis below the threshold, is more robust than the theoretical one.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 14:56:06 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Wang", "Yinzhi", ""], ["Haff", "Ingrid Hob\u00e6k", ""], ["Huseby", "Arne", ""]]}, {"id": "1911.02548", "submitter": "Azra Bihorac", "authors": "Matthew M Ruppert (BS) Jessica Lipori (BA), Sandip Patel (MD),\n  Elizabeth Ingersent, Tezcan Ozrazgat-Baslanti (PhD), Tyler Loftus (MD),\n  Parisa Rashidi (PhD), Azra Bihorac (MD, MS)", "title": "ICU Delirium Prediction Models: A Systematic Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Summarize ICU delirium prediction models published within the past\nfive years. Methods: Electronic searches were conducted in April 2019 using\nPubMed, Embase, Cochrane Central, Web of Science, and CINAHL to identify peer\nreviewed studies published in English during the past five years that\nspecifically addressed the development, validation, or recalibration of\ndelirium prediction models in adult ICU populations. Data were extracted using\nCHARMS checklist elements for systematic reviews of prediction studies,\nincluding the following characteristics: study design, participant descriptions\nand recruitment methods, predicted outcomes, a priori candidate predictors,\nsample size, model development, model performance, study results,\ninterpretation of those results, and whether the study included missing data.\nResults: Twenty studies featuring 26 distinct prediction models were included.\nModel performance varied greatly, as assessed by AUROC (0.68-0.94), specificity\n(56.5%-92.5%), and sensitivity (59%-90.9%). Most models used data collected\nfrom a single time point or window to predict the occurrence of delirium at any\npoint during hospital or ICU admission, and lacked mechanisms for providing\npragmatic, actionable predictions to clinicians. Conclusions: Although most ICU\ndelirium prediction models have relatively good performance, they have limited\napplicability to clinical practice. Most models were static, making predictions\nbased on data collected at a single time-point, failing to account for\nfluctuating conditions during ICU admission. Further research is needed to\ncreate clinically relevant dynamic delirium prediction models that can adapt to\nchanges in individual patient physiology over time and deliver actionable\npredictions to clinicians.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 13:03:28 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Ruppert", "Matthew M", "", "BS"], ["Lipori", "Jessica", "", "BA"], ["Patel", "Sandip", "", "MD"], ["Ingersent", "Elizabeth", "", "PhD"], ["Ozrazgat-Baslanti", "Tezcan", "", "PhD"], ["Loftus", "Tyler", "", "MD"], ["Rashidi", "Parisa", "", "PhD"], ["Bihorac", "Azra", "", "MD, MS"]]}, {"id": "1911.02629", "submitter": "Peter Marcy", "authors": "Peter W. Marcy, Scott A. Vander Wiel, Curtis B. Storlie, Veronica\n  Livescu, Curt A. Bronkhorst", "title": "Modeling Material Stress Using Integrated Gaussian Markov Random Fields", "comments": null, "journal-ref": null, "doi": "10.1080/02664763.2019.1686131", "report-no": "LA-UR-16-28920", "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The equations of a physical constitutive model for material stress within\ntantalum grains were solved numerically using a tetrahedrally meshed volume.\nThe resulting output included a scalar vonMises stress for each of the more\nthan 94,000 tetrahedra within the finite element discretization. In this paper,\nwe define an intricate statistical model for the spatial field of vonMises\nstress which uses the given grain geometry in a fundamental way. Our model\nrelates the three-dimensional field to integrals of latent stochastic processes\ndefined on the vertices of the one- and two-dimensional grain boundaries. An\nintuitive neighborhood structure of said boundary nodes suggested the use of a\nlatent Gaussian Markov random field (GMRF). However, despite the potential for\ncomputational gains afforded by GMRFs, the integral nature of our model and the\nsheer number of data points pose substantial challenges for a full Bayesian\nanalysis. To overcome these problems and encourage efficient exploration of the\nposterior distribution, a number of techniques are now combined: parallel\ncomputing, sparse matrix methods, and a modification of a block update strategy\nwithin the sampling routine. In addition, we use an auxiliary variables\napproach to accommodate the presence of outliers in the data.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 21:07:30 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Marcy", "Peter W.", ""], ["Wiel", "Scott A. Vander", ""], ["Storlie", "Curtis B.", ""], ["Livescu", "Veronica", ""], ["Bronkhorst", "Curt A.", ""]]}, {"id": "1911.02647", "submitter": "Justin Swaney", "authors": "Aaron Zhang, Evan W. Patton, Justin M. Swaney, and Tingying Helen Zeng", "title": "A Statistical Analysis of Recent Traffic Crashes in Massachusetts", "comments": "4 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A statistical analysis implemented in the Python programming language was\nperformed on the available MassDOT car accident data to identify whether a\ncertain set of traffic circumstances would increase the likelihood of injuries.\nIn the analysis, we created a binary classifier as a model to separate crashes\nthat resulted in injury from those that did not. To accomplish this, we first\ncleaned up the initial data, then proceeded to represent categorical variables\nnumerically through one hot encoding before finally producing models with\nRecursive Feature Elimination (RFE) and without RFE, in conjunction with\nlogistic regression. This statistical analysis plays a significant role in our\nmodern road network that has presented us with a heap of obstacles, one of the\nmost critical being the issue of how we can ensure the safety of all drivers\nand passengers. Findings from our analysis identify that tough weather and road\nconditions, senior/teen drivers and dangerous intersections play prominent\nroles in accidents that resulted in injuries in Massachusetts. These new\nfindings can provide valuable references and scientific data support to\nrelevant authorities and policy makers for upgrading road infrastructure,\npassing regulations, etc.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 17:56:15 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Zhang", "Aaron", ""], ["Patton", "Evan W.", ""], ["Swaney", "Justin M.", ""], ["Zeng", "Tingying Helen", ""]]}, {"id": "1911.02673", "submitter": "Emily Aiken", "authors": "Emily L. Aiken, Andre T. Nguyen, Mauricio Santillana", "title": "Towards the Use of Neural Networks for Influenza Prediction at Multiple\n  Spatial Resolutions", "comments": "Machine Learning for Health (ML4H) at NeurIPS 2019 - Extended\n  Abstract; Added Footer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the use of a Gated Recurrent Unit (GRU) for influenza prediction\nat the state- and city-level in the US, and experiment with the inclusion of\nreal-time flu-related Internet search data. We find that a GRU has lower\nprediction error than current state-of-the-art methods for data-driven\ninfluenza prediction at time horizons of over two weeks. In contrast with other\nmachine learning approaches, the inclusion of real-time Internet search data\ndoes not improve GRU predictions.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 23:14:53 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2019 05:10:27 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Aiken", "Emily L.", ""], ["Nguyen", "Andre T.", ""], ["Santillana", "Mauricio", ""]]}, {"id": "1911.02731", "submitter": "Moo K. Chung", "authors": "Moo K. Chung, Shih-Gu Huang, Tananun Songdechakraiwut, Ian C. Carroll,\n  and H. Hill Goldsmith", "title": "Statistical Analysis of Dynamic Functional Brain Networks in Twins", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown that functional brain brainwork is dynamic even\nduring rest. A common approach to modeling the brain network in whole brain\nresting-state fMRI is to compute the correlation between anatomical regions via\nsliding windows. However, the direct use of the sample correlation matrices is\nnot reliable due to the image acquisition, processing noises and the use of\ndiscrete windows that often introduce spurious high-frequency fluctuations and\nthe zig-zag pattern in the estimated time-varying correlation measures. To\naddress the problem and obtain more robust correlation estimates, we propose\nthe heat kernel based dynamic correlations. We demonstrate that the proposed\nheat kernel method can smooth out the unwanted high-frequency fluctuations in\ncorrelation estimations and achieve higher accuracy in identifying dynamically\nchanging distinct states. The method is further used in determining if such\ndynamic state change is genetically heritable using a large-scale twin study.\nVarious methodological challenges for analyzing paired twin dynamic networks\nare addressed.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 02:57:11 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 06:50:30 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Chung", "Moo K.", ""], ["Huang", "Shih-Gu", ""], ["Songdechakraiwut", "Tananun", ""], ["Carroll", "Ian C.", ""], ["Goldsmith", "H. Hill", ""]]}, {"id": "1911.02809", "submitter": "Luca Zilberti", "authors": "Alessandro Arduino, Oriano Bottauscio, Mario Chiampi, Luca Zilberti", "title": "Uncertainty propagation in phaseless electric properties tomography", "comments": "4 pages, 6 figures. 2019 International Conference on Electromagnetics\n  in Advanced Applications (ICEAA)", "journal-ref": null, "doi": "10.1109/ICEAA.2019.8879147", "report-no": null, "categories": "physics.med-ph eess.SP physics.app-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty propagation in a phaseless magnetic resonance-based electric\nproperties tomography technique is investigated using the Monte Carlo method.\nThe studied inverse method, which recovers the electric properties distribution\nat radiofrequency inside a scatterer irradiated by the coils of a magnetic\nresonance imaging scanner, is based on the contrast source inversion technique\nadapted to process phaseless input data.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 08:58:32 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Arduino", "Alessandro", ""], ["Bottauscio", "Oriano", ""], ["Chiampi", "Mario", ""], ["Zilberti", "Luca", ""]]}, {"id": "1911.02878", "submitter": "Andr\\'as B\\'alint", "authors": "Jordanka Kovaceva, Andr\\'as B\\'alint, Ron Schindler, Anja Schneider", "title": "Safety benefit assessment of autonomous emergency braking and steering\n  systems for the protection of cyclists and pedestrians based on a combination\n  of computer simulation and real-world test results", "comments": "Accepted manuscript, to appear in Accident Analysis and Prevention.\n  26 pages, 6 figures", "journal-ref": "Accident Analysis & Prevention 136, 2020, 105352", "doi": "10.1016/j.aap.2019.105352", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyclists and pedestrians account for a significant share of fatalities and\nserious injuries in the road transport system. In order to protect them,\nadvanced driver assistance systems are being developed and introduced to the\nmarket, including autonomous emergency braking and steering systems (AEBSS)\nthat autonomously perform braking or an evasive manoeuvre by steering in case\nof a pending collision, in order to avoid the collision or mitigate its\nseverity. This study proposes a new prospective framework for quantifying\nsafety benefit of AEBSS for the protection of cyclists and pedestrians in terms\nof saved lives and reduction in the number of people suffering serious\ninjuries. The core of the framework is a novel application of Bayesian\ninference in such a way that prior information from counterfactual simulation\nis updated with new observations from real-world testing of a prototype AEBSS.\nAs an illustration of the method, the framework is applied for safety benefit\nassessment of the AEBSS developed in the European Union (EU) project PROSPECT.\nIn this application of the framework, counterfactual simulation results based\non the German In-Depth Accident Study Pre-Crash Matrix (GIDAS-PCM) data were\ncombined with results from real-world tests on proving grounds. The proposed\nframework gives a systematic way for the combination of results from different\nsources and can be considered for understanding the real-world benefit of new\nAEBSS. Additionally, the Bayesian modelling approach used in this paper has a\ngreat potential to be used in a wide range of other research studies.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 12:41:20 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Kovaceva", "Jordanka", ""], ["B\u00e1lint", "Andr\u00e1s", ""], ["Schindler", "Ron", ""], ["Schneider", "Anja", ""]]}, {"id": "1911.02926", "submitter": "Marie Roald", "authors": "Marie Roald, Suchita Bhinge, Chunying Jia, Vince Calhoun, T\\\"ulay\n  Adal{\\i}, Evrim Acar", "title": "Tracing Network Evolution Using the PARAFAC2 Model", "comments": "5 pages, 5 figures, conference", "journal-ref": null, "doi": "10.1109/ICASSP40776.2020.9053902", "report-no": null, "categories": "stat.AP cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Characterizing time-evolving networks is a challenging task, but it is\ncrucial for understanding the dynamic behavior of complex systems such as the\nbrain. For instance, how spatial networks of functional connectivity in the\nbrain evolve during a task is not well-understood. A traditional approach in\nneuroimaging data analysis is to make simplifications through the assumption of\nstatic spatial networks. In this paper, without assuming static networks in\ntime and/or space, we arrange the temporal data as a higher-order tensor and\nuse a tensor factorization model called PARAFAC2 to capture underlying patterns\n(spatial networks) in time-evolving data and their evolution. Numerical\nexperiments on simulated data demonstrate that PARAFAC2 can successfully reveal\nthe underlying networks and their dynamics. We also show the promising\nperformance of the model in terms of tracing the evolution of task-related\nfunctional connectivity in the brain through the analysis of functional\nmagnetic resonance imaging data.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 08:45:29 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Roald", "Marie", ""], ["Bhinge", "Suchita", ""], ["Jia", "Chunying", ""], ["Calhoun", "Vince", ""], ["Adal\u0131", "T\u00fclay", ""], ["Acar", "Evrim", ""]]}, {"id": "1911.03130", "submitter": "David Edward Williams", "authors": "Lena Weissert, Kyle Alberti, Elaine Miles, Georgia Miskell, Brandon\n  Feenstra, Geoff S Henshaw, Vasileios Papapostolou, Hamesh Patel, Andrea\n  Polidori, Jennifer A Salmond, David E Williams", "title": "Low-cost sensor networks and land-use regression: interpolating nitrogen\n  dioxide concentration at high temporal and spatial resolution in Southern\n  California", "comments": "26 pages, 11 figures", "journal-ref": null, "doi": "10.1016/j.atmosenv.2020.117287", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The development of low-cost sensors and novel calibration algorithms offer\nnew opportunities to supplement existing regulatory networks to measure air\npollutants at a high spatial resolution and at hourly and sub-hourly\ntimescales. We use a random forest model on data from a network of low-cost\nsensors to describe the effect of land use features on local-scale air quality,\nextend this model to describe the hourly-scale variation of air quality at high\nspatial resolution, and show that deviations from the model can be used to\nidentify particular conditions and locations where air quality differs from the\nexpected land-use effect. The conditions and locations under which deviations\nwere detected conform to expectations based on general experience.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 08:48:04 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Weissert", "Lena", ""], ["Alberti", "Kyle", ""], ["Miles", "Elaine", ""], ["Miskell", "Georgia", ""], ["Feenstra", "Brandon", ""], ["Henshaw", "Geoff S", ""], ["Papapostolou", "Vasileios", ""], ["Patel", "Hamesh", ""], ["Polidori", "Andrea", ""], ["Salmond", "Jennifer A", ""], ["Williams", "David E", ""]]}, {"id": "1911.03136", "submitter": "David Edward Williams", "authors": "Lena Weissert, Elaine Miles, Georgia Miskell, Kyle Alberti, Brandon\n  Feenstra, Geoff S Henshaw, Vasileios Papapostolou, Hamesh Patel, Andrea\n  Polidori, Jennifer A Salmond, David E Williams", "title": "Hierarchical network design for nitrogen dioxide measurement in urban\n  environments, part 2: network-based sensor calibration", "comments": "40 pages inclusive of supporting information. 13 figure in main text;\n  15 figure in SI", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a management and data correction framework for low-cost\nelectrochemical sensors for nitrogen dioxide (NO2) deployed within a\nhierarchical network of low-cost and regulatory-grade instruments. The\nframework is founded on the idea that it is possible in a suitably configured\nnetwork to identify a source of reliable proxy data for each sensor site that\nhas a similar probability distribution of measurement values over a suitable\ntime period. Previous work successfully applied these ideas to a sensor system\nwith a simple linear 2-parameter (slope and offset) response. Applying these\nideas to electrochemical sensors for NO2 presents significant additional\ndifficulties for which we demonstrate solutions. The three NO2 sensor response\nparameters (offset, ozone (O3) response slope, and NO2 response slope) are\nknown to vary significantly as a consequence of ambient humidity and\ntemperature variations. Here we demonstrate that these response parameters can\nbe estimated by minimising the Kullback-Leibler divergence between\nsensor-estimated and proxy NO2 distributions over a 3-day window. We then\nestimate an additional offset term by using co-location data. This offset term\nis dependent on climate and spatially correlated and can thus be projected\nacross the network. Co-location data also estimates the time-, space- and\nconcentration-dependent error distribution between sensors and regulatory-grade\ninstruments. We show how the parameter variations can be used to indicate both\nsensor failure and failure of the proxy assumption. We apply the procedures to\na network of 56 sensors distributed across the Inland Empire and Los Angeles\nCounty regions, demonstrating the need for reliable data from dense networks of\nmonitors to supplement the existing regulatory networks.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 09:08:22 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Weissert", "Lena", ""], ["Miles", "Elaine", ""], ["Miskell", "Georgia", ""], ["Alberti", "Kyle", ""], ["Feenstra", "Brandon", ""], ["Henshaw", "Geoff S", ""], ["Papapostolou", "Vasileios", ""], ["Patel", "Hamesh", ""], ["Polidori", "Andrea", ""], ["Salmond", "Jennifer A", ""], ["Williams", "David E", ""]]}, {"id": "1911.03137", "submitter": "David Edward Williams", "authors": "Lena Weissert, Georgia Miskell, Elaine Miles, Kyle Alberti, Brandon\n  Feenstra, Hamesh Patel, Vasileios Papapostolou, Andrea Polidori, Geoff S\n  Henshaw, Jennifer A Salmond, David E Williams", "title": "Hierarchical network design for nitrogen dioxide measurement in urban\n  environments, part 1: proxy selection", "comments": "24 pages, 7 figures, supplementary information appended to main text", "journal-ref": null, "doi": "10.1016/j.atmosenv.2020.117428", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Previous studies have shown that a hierarchical network comprising a number\nof compliant reference stations and a much larger number of low-cost sensors\ncan deliver reliable air quality data at high temporal and spatial resolution\nfor ozone at neighbourhood scales. Key to this framework is the concept of a\nproxy: a reliable (regulatory) data source whose results have sufficient\nstatistical similarity over some period of time to those from any given\nlow-cost measurement site. This enables the low-cost instruments to be\ncalibrated remotely, avoiding the need for costly on-site calibration of dense\nnetworks. This paper assesses the suitability of this method for local air\npollutants such as nitrogen dioxide which show large temporal and spatial\nvariability in concentration. The proxy technique is evaluated using the data\nfrom the network of regulatory air monitoring stations measuring nitrogen\ndioxide in Southern California to avoid errors introduced by low-cost\ninstrument performance. Proxies chosen based on land use similarity signalled\ntypically less than 0.1 percent false alarms. Although poor proxy performance\nwas observed when the local geography was unusual (a semi-enclosed valley) in\nthis instance the closest neighbour station proved to be an appropriate\nalternative. The method also struggled when wind speeds were low and very local\nsources presumably dominated the concentration patterns. Overall, we\ndemonstrate that the technique can be applied to nitrogen dioxide, and that\nappropriate proxies can be found even within a spatially sparse network of\nstations in a region with large spatio-temporal variation in concentration.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 09:15:08 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Weissert", "Lena", ""], ["Miskell", "Georgia", ""], ["Miles", "Elaine", ""], ["Alberti", "Kyle", ""], ["Feenstra", "Brandon", ""], ["Patel", "Hamesh", ""], ["Papapostolou", "Vasileios", ""], ["Polidori", "Andrea", ""], ["Henshaw", "Geoff S", ""], ["Salmond", "Jennifer A", ""], ["Williams", "David E", ""]]}, {"id": "1911.03278", "submitter": "Jeffrey Doser", "authors": "Jeffrey W. Doser, Andrew O. Finley, Eric P. Kasten, Stuart H. Gage", "title": "Assessing soundscape disturbance through hierarchical models and\n  acoustic indices: a case study on a shelterwood logged northern Michigan\n  forest", "comments": "25 pages, 7 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing the effects of anthropogenic disturbances on wildlife is a\nnecessary conservation task. The soundscape is a critical habitat component for\nacoustically communicating organisms, but the use of the soundscape as a tool\nfor assessing disturbance impacts has been relatively unexplored until\nrecently. Here we present a broad modeling framework for assessing disturbance\nimpacts on soundscapes, which we apply to quantify the influence of a\nshelterwood logging on soundscapes in northern Michigan. Our modeling approach\ncan be broadly applied to assess anthropogenic disturbance impacts on\nsoundscapes. The approach accommodates inherent differences in control and\ntreatment sites to improve inference about treatment effects, while also\naccounting for extraneous variables (e.g., rain) that influence acoustic\nindices.\n  Recordings were obtained at 13 sites before and after a shelterwood logging.\nFour sites were in the logging region and nine sites served as control\nrecordings outside the logging region. We quantify the soundscapes using common\nacoustic indices (Normalized Difference Soundscape Index (NDSI), Acoustic\nEntropy (H), Acoustic Complexity Index (ACI), Acoustic Evenness Index (AEI),\nWelch Power Spectral Density (PSD)) and build two hierarchical Bayesian models\nto quantify the changes in the soundscape over the study period.\n  Our analysis reveals no long-lasting effects of the shelterwood logging on\nthe soundscape diversity as measured by the NDSI, but analysis of H, AEI, and\nPSD suggest changes in the evenness of sounds across the frequency spectrum,\nindicating a potential shift in the avian species communicating in the\nsoundscapes as a result of the logging. Acoustic recordings, in conjunction\nwith this modeling framework, can deliver cost efficient assessment of\ndisturbance impacts on the landscape and underlying biodiversity as represented\nthrough the soundscape.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 14:21:20 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Doser", "Jeffrey W.", ""], ["Finley", "Andrew O.", ""], ["Kasten", "Eric P.", ""], ["Gage", "Stuart H.", ""]]}, {"id": "1911.03336", "submitter": "Carlos Ruiz", "authors": "Andr\\'es M. Alonso, F. Javier Nogales and Carlos Ruiz", "title": "Hierarchical Clustering for Smart Meter Electricity Loads based on\n  Quantile Autocovariances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to improve the efficiency and sustainability of electricity systems,\nmost countries worldwide are deploying advanced metering infrastructures, and\nin particular household smart meters, in the residential sector. This\ntechnology is able to record electricity load time series at a very high\nfrequency rates, information that can be exploited to develop new clustering\nmodels to group individual households by similar consumptions patterns. To this\nend, in this work we propose three hierarchical clustering methodologies that\nallow capturing different characteristics of the time series. These are based\non a set of \"dissimilarity\" measures computed over different features: quantile\nauto-covariances, and simple and partial autocorrelations. The main advantage\nis that they allow summarizing each time series in a few representative\nfeatures so that they are computationally efficient, robust against outliers,\neasy to automatize, and scalable to hundreds of thousands of smart meters\nseries. We evaluate the performance of each clustering model in a real-world\nsmart meter dataset with thousands of half-hourly time series. The results show\nhow the obtained clusters identify relevant consumption behaviors of households\nand capture part of their geo-demographic segmentation. Moreover, we apply a\nsupervised classification procedure to explore which features are more relevant\nto define each cluster.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 15:47:34 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Alonso", "Andr\u00e9s M.", ""], ["Nogales", "F. Javier", ""], ["Ruiz", "Carlos", ""]]}, {"id": "1911.03417", "submitter": "Claire Donnat", "authors": "Claire Donnat, Susan Holmes", "title": "Convex Hierarchical Clustering for Graph-Structured Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convex clustering is a recent stable alternative to hierarchical clustering.\nIt formulates the recovery of progressively coalescing clusters as a\nregularized convex problem. While convex clustering was originally designed for\nhandling Euclidean distances between data points, in a growing number of\napplications, the data is directly characterized by a similarity matrix or\nweighted graph. In this paper, we extend the robust hierarchical clustering\napproach to these broader classes of similarities. Having defined an\nappropriate convex objective, the crux of this adaptation lies in our ability\nto provide: (a) an efficient recovery of the regularization path and (b) an\nempirical demonstration of the use of our method. We address the first\nchallenge through a proximal dual algorithm, for which we characterize both the\ntheoretical efficiency as well as the empirical performance on a set of\nexperiments. Finally, we highlight the potential of our method by showing its\napplication to several real-life datasets, thus providing a natural extension\nto the current scope of applications of convex clustering.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 18:08:21 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 00:00:38 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Donnat", "Claire", ""], ["Holmes", "Susan", ""]]}, {"id": "1911.03454", "submitter": "Gabriel Riutort Mayol", "authors": "Gabriel Riutort-Mayol, Michael Riis Andersen, Aki Vehtari, Jos\\'e Luis\n  Lerma", "title": "Gaussian process with derivative information for the analysis of the\n  sunlight adverse effects on color of rock art paintings", "comments": "arXiv admin note: substantial text overlap with arXiv:1910.12575", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microfading Spectrometry (MFS) is a method for assessing light sensitivity\ncolor (spectral) variations of cultural heritage objects. The MFS technique\nprovides measurements of the surface under study, where each point of the\nsurface gives rise to a time-series that represents potential spectral (color)\nchanges due to sunlight exposition over time. Color fading is expected to be\nnon-decreasing as a function of time and stabilize eventually. These properties\ncan be expressed in terms of the partial derivatives of the functions. We\npropose a spatio-temporal model that takes this information into account by\njointly modeling the spatio-temporal process and its derivative process using\nGaussian processes (GPs). We fitted the proposed model to MFS data collected\nfrom the surface of prehistoric rock art paintings. A multivariate covariance\nfunction in a GP allows modeling trichromatic image color variables jointly\nwith spatial distances and time points variables as inputs to evaluate the\ncovariance structure of the data. We demonstrated that the colorimetric\nvariables are useful for predicting the color fading time-series for new\nunobserved spatial locations. Furthermore, constraining the model using\nderivative sign observations for monotonicity was shown to be beneficial in\nterms of both predictive performance and application-specific interpretability.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 11:55:01 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Riutort-Mayol", "Gabriel", ""], ["Andersen", "Michael Riis", ""], ["Vehtari", "Aki", ""], ["Lerma", "Jos\u00e9 Luis", ""]]}, {"id": "1911.03553", "submitter": "Keyu Nie", "authors": "Keyu Nie, Yinfei Kong, Ted Tao Yuan, Pauline Berry Burke", "title": "Dealing With Ratio Metrics in A/B Testing at the Presence of Intra-User\n  Correlation and Segments", "comments": "A/B Testing, repeated measures, uniformly minimum-variance unbiased\n  estimator, stratification, sensitivity, variance reduction", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study ratio metrics in A/B testing at the presence of correlation among\nobservations coming from the same user and provides practical guidance\nespecially when two metrics contradict each other. We propose new estimating\nmethods to quantitatively measure the intra-user correlation (within segments).\nWith the accurately estimated correlation, a uniformly minimum-variance\nunbiased estimator of the population mean, called correlation-adjusted mean, is\nproposed to account for such correlation structure. It is proved theoretically\nand numerically better than the other two unbiased estimators, naive mean and\nnormalized mean (averaging within users first and then across users). The\ncorrelation-adjusted mean method is unbiased and has reduced variance so it\ngains additional power. Several simulation studies are designed to show the\nestimation accuracy of the correlation structure, effectiveness in reducing\nvariance, and capability of obtaining more power. An application to the eBay\ndata is conducted to conclude this paper.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 21:40:42 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 00:59:02 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Nie", "Keyu", ""], ["Kong", "Yinfei", ""], ["Yuan", "Ted Tao", ""], ["Burke", "Pauline Berry", ""]]}, {"id": "1911.03592", "submitter": "Shanshan Cao", "authors": "Juan Du, Shanshan Cao, Jeffrey H. Hunt, Xiaoming Huo", "title": "Optimal Shape Control via $L_\\infty$ Loss for Composite Fuselage\n  Assembly", "comments": "31 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shape control is critical to ensure the quality of composite fuselage\nassembly. In current practice, the structures are adjusted to the design shape\nin terms of the $\\ell_2$ loss for further assembly without considering the\nexisting dimensional gap between two structures. Such practice has two\nlimitations: (1) the design shape may not be the optimal shape in terms of a\npair of incoming fuselages with different incoming dimensions; (2) the maximum\ngap is the key concern during the fuselage assembly process. This paper\nproposes an optimal shape control methodology via the $\\ell_\\infty$ loss for\ncomposite fuselage assembly process by considering the existing dimensional gap\nbetween the incoming pair of fuselages. Besides, due to the limitation on the\nnumber of available actuators in practice, we face an important problem of\nfinding the best locations for the actuators among many potential locations,\nwhich makes the problem a sparse estimation problem. We are the first to solve\nthe optimal shape control in fuselage assembly process using the $\\ell_\\infty$\nmodel under the framework of sparse estimation, where we use the $\\ell_1$\npenalty to control the sparsity of the resulting estimator. From statistical\npoint of view, this can be formulated as the $\\ell_\\infty$ loss based linear\nregression, and under some standard assumptions, such as the restricted\neigenvalue (RE) conditions, and the light tailed noise, the non-asymptotic\nestimation error of the $\\ell_1$ regularized $\\ell_\\infty$ linear model is\nderived to be the order of $O(\\sigma\\sqrt{\\frac{S\\log p}{n}})$, which meets the\nupper-bound in the existing literature. Compared to the current practice, the\ncase study shows that our proposed method significantly reduces the maximum gap\nbetween two fuselages after shape adjustments.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 00:33:03 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Du", "Juan", ""], ["Cao", "Shanshan", ""], ["Hunt", "Jeffrey H.", ""], ["Huo", "Xiaoming", ""]]}, {"id": "1911.03719", "submitter": "Sara Masoud", "authors": "Sara Masoud, Bijoy Chowdhury, Young-Jun Son, Russell Tronstad", "title": "Markov-chain Monte-Carlo Sampling for Optimal Fidelity Determination in\n  Dynamic Decision-Making", "comments": "6 pages, 8 figures, conference Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision making for dynamic systems is challenging due to the scale and\ndynamicity of such systems, and it is comprised of decisions at strategic,\ntactical, and operational levels. One of the most important aspects of decision\nmaking is incorporating real time information that reflects immediate status of\nthe system. This type of decision making, which may apply to any dynamic\nsystem, needs to comply with the system's current capabilities and calls for a\ndynamic data driven planning framework. Performance of dynamic data driven\nplanning frameworks relies on the decision making process which in return is\nrelevant to the quality of the available data. This means that the planning\nframework should be able to set the level of decision making based on the\ncurrent status of the system, which is learned through the continuous readings\nof sensory data. In this work, a Markov chain Monte Carlo sampling method is\nproposed to determine the optimal fidelity of decision making in a dynamic data\ndriven framework. To evaluate the performance of the proposed method, an\nexperiment is conducted, where the impact of workers performance on the\nproduction capacity and the fidelity level of decision making are studied.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 15:53:25 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Masoud", "Sara", ""], ["Chowdhury", "Bijoy", ""], ["Son", "Young-Jun", ""], ["Tronstad", "Russell", ""]]}, {"id": "1911.03730", "submitter": "Vikas Ramachandra", "authors": "Vikas Ramachandra", "title": "Forecasting the effect of heat stress index and climate change on cloud\n  data center energy consumption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we estimate the effect of heat stress index (a measure which\ntakes into account rising temperatures as well as humidity) on data center\nenergy consumption. We use forecasting models to predict future energy use by\ndata centers, taking into account rising temperature scenarios. We compare\nthose estimates with baseline forecasted energy consumption (without heat\nstress index or rising temperature correction) and present the result that\nthere is a sizeable and significant difference in the two forecasts. We show\nthat rising temperatures will cause a negative impact on data center energy\nconsumption, increasing it by about 8 percent, and conclude that data center\nenergy consumption analyses and forecasts must include the effects of heat\nstress index and rising temperatures and other climate change related effects.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 16:17:15 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Ramachandra", "Vikas", ""]]}, {"id": "1911.03795", "submitter": "Francisco Rowe Dr", "authors": "Francisco Rowe, Martin Bell, Aude Bernard, Elin Charles-Edwards and\n  Philipp Ueffing", "title": "Impact of internal migration on population redistribution in Europe:\n  Urbanisation, counterurbanisation or spatial equilibrium?", "comments": null, "journal-ref": "ROWE, Francisco et al. Impact of Internal Migration on Population\n  Redistribution in Europe: Urbanisation, Counterurbanisation or Spatial\n  Equilibrium?. Comparative Population Studies, [S.l.], v. 44, nov. 2019", "doi": "10.12765/CPoS-2019-18en", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The classical foundations of migration research date from the 1880s with\nRavenstein's Laws of migration, which represent the first comparative analyses\nof internal migration. While his observations remain largely valid, the ensuing\ncentury has seen considerable progress in data collection practices and methods\nof analysis, which in turn has permitted theoretical advances in understanding\nthe role of migration in population redistribution. Coupling the extensive\nrange of migration data now available with these recent theoretical and\nmethodological advances, we endeavour to advance beyond Ravenstein's\nunderstanding by examining the direction of population redistribution and\ncomparing the impact of internal migration on patterns of human settlement in\n27 European countries. Results show that the overall redistributive impact of\ninternal migration is low in most European countries but the mechanisms differ\nacross the continent. In Southern and Eastern Europe migration effectiveness is\nabove average but is offset by low migration intensities, whereas in Northern\nand Western Europe high intensities are absorbed in reciprocal flows resulting\nin low migration effectiveness. About half the European countries are\nexperiencing a process of concentration toward urbanised regions, particularly\nin Northern, Central and Eastern Europe, whereas countries in the West and\nSouth are undergoing a process of population deconcentration. These results\nsuggest that population deconcentration is now more common than it was in the\n1990s when counterurbanisation was limited to Western Europe. The results show\nthat 130 years on, Ravenstein's law of migration streams and counter-streams\nremains a central facet of migration dynamics, while underlining the importance\nof simple yet robust indices for the spatial analysis of migration.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 22:58:17 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Rowe", "Francisco", ""], ["Bell", "Martin", ""], ["Bernard", "Aude", ""], ["Charles-Edwards", "Elin", ""], ["Ueffing", "Philipp", ""]]}, {"id": "1911.03943", "submitter": "Amirreza Nickkar", "authors": "Amirreza Nickkar, Hyeon-Shic Shin, Andrew Farkas", "title": "Analysis of Ownership and Travel Behavior of Women Who Drive Electric\n  Vehicles: The case of Maryland", "comments": "6th International Conference on Womens Issues in Transportation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study aims to investigate the contributing socio-demographic\ncharacteristics and factors among female EV owners as well as their travel\nbehavior, commuting trip patterns, and purchasing/leasing ownerships. The\nobjective of the study is to recommend public policies to decision makers to\nprompt gender equity for EV purchase and use by identifying socio-demographic\nattributes that influence EV travel patterns and behavior.An online survey of\nEV owners of both sexes was conducted from May 28, 2015, to February 19, 2016.\nIn total, 1,257 EV owners in Maryland completed usable surveys. A set of\nstatistical analysis methods was employed to analyze the data. A set of\ncorrelation test, analysis of variance (ANOVA) and multinomial logit model\n(MNL) were constructed to examine the associations between EV owner\ncharacteristics and their EV purchasing/leasing and spatial commuting trip\nbehavior, and the results have been compared to the spatial travel patterns of\ndrivers of internal combustion engine vehicles (ICEVs) in Maryland. The results\nof this study showed that first, socio-demographic factors, including education\nand income, played a significant role in preferences attributes of participants\nfor purchasing/leasing an EV and in the commuting travel behavior and pattern\nand of EV drivers. Second, environmental issues are the main reason for\npurchasing/leasing EVs, but the EV owners who had longer commutes were more\nconcerned about the price and status of the EV owner and efficiency and\nperformance than were those with shorter commutes. The results of this study\nimprove equity in transportation into the future and could be utilized by\ntransportation authorities, transport investment agencies, and collaborators in\ndeveloping and improving emerging transportation systems.\n", "versions": [{"version": "v1", "created": "Sun, 10 Nov 2019 15:13:19 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Nickkar", "Amirreza", ""], ["Shin", "Hyeon-Shic", ""], ["Farkas", "Andrew", ""]]}, {"id": "1911.03961", "submitter": "Seyed Nasser Moosavi", "authors": "Seyed Nasser Moosavi, Ashkan Khalifeh, Ali Shojaee and Masoud Abessi", "title": "Analyzing the impact of two major factors on medical expenses paid by\n  health insurance organization in Iran", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In healthcare scope, the profound role of insurance companies is undeniable.\nHealth insurance establishments main responsibility is to support public health\nfinancially and promote the quality of health services. Governments subsidies\nto healthcare insurance, insured payments and insurance companies costs must be\nspecified in such a way that both people and insurers mutually benefit. In this\nresearch, we propose a model for determining healthcare costs paid by health\ninsurance organization with regard to two major factors, the geographical\nregions where the patients live and the seasons when they receive service,\nusing two-way ANOVA method. Since both effects are found to be significant,\nallocating different insurance costs to the people residing in different\nregions, and also changing the patterns of insurance extension in different\nseasons with regard to the results derived from the research, can detract\nhealthcare costs and give more satisfaction to the lower income insured\npatients.\n", "versions": [{"version": "v1", "created": "Sun, 10 Nov 2019 16:51:29 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Moosavi", "Seyed Nasser", ""], ["Khalifeh", "Ashkan", ""], ["Shojaee", "Ali", ""], ["Abessi", "Masoud", ""]]}, {"id": "1911.03963", "submitter": "Seyed Nasser Moosavi", "authors": "Seyed Nasser Moosavi, Ashkan Khalifeh, Ali Shojaee and Masoud Abessi", "title": "Estimating length of hospital stay, with regard to associated factors; A\n  model to enhance healthcare service efficiency and reduce healthcare resource\n  consumption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To assess the efficiency and the resource consumption level in healthcare\nscope, many economic and social factors have to be considered. An index which\nhas recently been studied by the researchers, is length of hospital stay (LOS)\ndefined as how long each patient is hospitalized. Detecting and controlling the\nfactors affecting this index can help to reduce healthcare costs and also to\nimprove healthcare service efficiency. The effects of three major factors, say,\nthe season during which the patient is hospitalized, the patients age and\nhis/her gender on LOS pertaining to 82718 patients receiving healthcare service\nfrom the hospitals under contract to insurance organization in Tehran, have\nbeen analyzed, using unbalanced factorial design. The test results imply that\nthe whole model is significant (P-value=0<0.01), the separate effects of all\nthree factors on LOS are significant (P-value=0<0.01) and the only significant\ninteraction at alpha=0.01 is the one between gender and age group\n(P-value=0<0.01). moreover, no two age groups have the significant equal means\nand age groups 1-10 and 26-40 years possess the minimum and maximum means of\nLOS, respectively. LOS means in winter and autumn are equal, being the maximum.\nDue to the significance of these effects, allocating specified budget and\nresources with regard to these factors will help hospitals and healthcare\ncenters enhance their efficiency and remain within their budget.\n", "versions": [{"version": "v1", "created": "Sun, 10 Nov 2019 17:08:41 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Moosavi", "Seyed Nasser", ""], ["Khalifeh", "Ashkan", ""], ["Shojaee", "Ali", ""], ["Abessi", "Masoud", ""]]}, {"id": "1911.03971", "submitter": "Seyed Nasser Moosavi", "authors": "Seyed Nasser Moosavi, Mohammad Saleh Owlia and Ashkan Khalifeh", "title": "A new method for phase II monitoring of multivariate simple linear\n  profiles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A scope in quality control, which has recently received a great deal of\nattention is profile that characterizes the quality of a product or process by\na relationship between two or more variables. In this paper, we propose an EWMA\nchart for phase II monitoring of multivariate simple linear profile in which\nseveral correlated response variables have linear relationships with one\nexplanatory variable. The statistical performance of this scheme is evaluated\nin terms of out-of-control average run length. Although it seldom signals for\nsmall shifts, it is superior to previous works in detecting moderate and big\nshifts.\n", "versions": [{"version": "v1", "created": "Sun, 10 Nov 2019 17:45:57 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Moosavi", "Seyed Nasser", ""], ["Owlia", "Mohammad Saleh", ""], ["Khalifeh", "Ashkan", ""]]}, {"id": "1911.03985", "submitter": "Hyunseung Kang", "authors": "Nan Bi, Hyunseung Kang, Jonathan Taylor", "title": "Inference After Selecting Plausibly Valid Instruments with Application\n  to Mendelian Randomization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mendelian randomization (MR) is a popular method in genetic epidemiology to\nestimate the effect of an exposure on an outcome by using genetic instruments.\nThese instruments are often selected from a combination of prior knowledge from\ngenome wide association studies (GWAS) and data-driven instrument selection\nprocedures or tests. Unfortunately, when testing for the exposure effect, the\ninstrument selection process done a priori is not accounted for. This paper\nstudies and highlights the bias resulting from not accounting for the\ninstrument selection process by focusing on a recent data-driven instrument\nselection procedure, sisVIVE, as an example. We introduce a conditional\ninference approach that conditions on the instrument selection done a priori\nand leverage recent advances in selective inference to derive conditional null\ndistributions of popular test statistics for the exposure effect in MR. The\nnull distributions can be characterized with individual-level or summary-level\ndata in MR. We show that our conditional confidence intervals derived from\nconditional null distributions attain the desired nominal level while typical\nconfidence intervals computed in MR do not. We conclude by reanalyzing the\neffect of BMI on diastolic blood pressure using summary-level data from the\nUKBiobank that accounts for instrument selection.\n", "versions": [{"version": "v1", "created": "Sun, 10 Nov 2019 19:28:23 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Bi", "Nan", ""], ["Kang", "Hyunseung", ""], ["Taylor", "Jonathan", ""]]}, {"id": "1911.04048", "submitter": "Rogers Silva", "authors": "Rogers F. Silva (1 and 2), Sergey M. Plis (1 and 2), Tulay Adali (3),\n  Marios S. Pattichis (4), Vince D. Calhoun (1 and 2) ((1) Tri-Institutional\n  Center for Translational Research in Neuroimaging and Data Science (TReNDS),\n  Georgia State University, Georgia Institute of Technology, and Emory\n  University, Atlanta, GA, USA, (2) The Mind Research Network, Albuquerque, NM,\n  USA, (3) Dept. of CSEE, University of Maryland Baltimore County, Baltimore,\n  Maryland, USA, (4) Dept. of ECE at The University of New Mexico, Albuquerque,\n  NM, USA)", "title": "Multidataset Independent Subspace Analysis with Application to\n  Multimodal Fusion", "comments": "For associated code, see https://github.com/rsilva8/MISA For\n  associated data, see https://github.com/rsilva8/MISA-data Submitted to IEEE\n  Transactions on Image Processing on Nov/7/2019: 13 pages, 8 figures\n  Supplement: 16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.IV eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last two decades, unsupervised latent variable models---blind source\nseparation (BSS) especially---have enjoyed a strong reputation for the\ninterpretable features they produce. Seldom do these models combine the rich\ndiversity of information available in multiple datasets. Multidatasets, on the\nother hand, yield joint solutions otherwise unavailable in isolation, with a\npotential for pivotal insights into complex systems.\n  To take advantage of the complex multidimensional subspace structures that\ncapture underlying modes of shared and unique variability across and within\ndatasets, we present a direct, principled approach to multidataset combination.\nWe design a new method called multidataset independent subspace analysis (MISA)\nthat leverages joint information from multiple heterogeneous datasets in a\nflexible and synergistic fashion.\n  Methodological innovations exploiting the Kotz distribution for subspace\nmodeling in conjunction with a novel combinatorial optimization for evasion of\nlocal minima enable MISA to produce a robust generalization of independent\ncomponent analysis (ICA), independent vector analysis (IVA), and independent\nsubspace analysis (ISA) in a single unified model.\n  We highlight the utility of MISA for multimodal information fusion, including\nsample-poor regimes and low signal-to-noise ratio scenarios, promoting novel\napplications in both unimodal and multimodal brain imaging data.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 02:52:55 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Silva", "Rogers F.", "", "1 and 2"], ["Plis", "Sergey M.", "", "1 and 2"], ["Adali", "Tulay", "", "1 and 2"], ["Pattichis", "Marios S.", "", "1 and 2"], ["Calhoun", "Vince D.", "", "1 and 2"]]}, {"id": "1911.04062", "submitter": "Junjie Liang", "authors": "Junjie Liang, Dongkuan Xu, Yiwei Sun and Vasant Honavar", "title": "LMLFM: Longitudinal Multi-Level Factorization Machine", "comments": "Thirty-Fourth AAAI Conference on Artificial Intelligence, accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning predictive models from longitudinal data,\nconsisting of irregularly repeated, sparse observations from a set of\nindividuals over time. Such data often exhibit {\\em longitudinal correlation}\n(LC) (correlations among observations for each individual over time), {\\em\ncluster correlation} (CC) (correlations among individuals that have similar\ncharacteristics), or both. These correlations are often accounted for using\n{\\em mixed effects models} that include {\\em fixed effects} and {\\em random\neffects}, where the fixed effects capture the regression parameters that are\nshared by all individuals, whereas random effects capture those parameters that\nvary across individuals. However, the current state-of-the-art methods are\nunable to select the most predictive fixed effects and random effects from a\nlarge number of variables, while accounting for complex correlation structure\nin the data and non-linear interactions among the variables. We propose\nLongitudinal Multi-Level Factorization Machine (LMLFM), to the best of our\nknowledge, the first model to address these challenges in learning predictive\nmodels from longitudinal data. We establish the convergence properties, and\nanalyze the computational complexity, of LMLFM. We present results of\nexperiments with both simulated and real-world longitudinal data which show\nthat LMLFM outperforms the state-of-the-art methods in terms of predictive\naccuracy, variable selection ability, and scalability to data with large number\nof variables. The code and supplemental material is available at\n\\url{https://github.com/junjieliang672/LMLFM}.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 03:45:39 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 19:06:26 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Liang", "Junjie", ""], ["Xu", "Dongkuan", ""], ["Sun", "Yiwei", ""], ["Honavar", "Vasant", ""]]}, {"id": "1911.04168", "submitter": "Veronica Vinciotti Dr", "authors": "Paolo Berta, Veronica Vinciotti, Francesco Moscone", "title": "Does hospital cooperation increase the quality of healthcare?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by reasons such as altruism, managers from different hospitals may\nengage in cooperative behaviours, which shape the networked healthcare economy.\nIn this paper we study the determinants of hospital cooperation and its\nassociation with the quality delivered by hospitals, using Italian\nadministrative data. We explore the impact on patient transfers between\nhospitals (cooperation/network) of a set of demand-supply factors, as well as\ndistance-based centrality measures. We then use this framework to assess how\nsuch cooperation is related to the overall quality for the hospital of origin\nand of destination of the patient transfer. The over-dispersed Poisson mixed\nmodel that we propose, inspired by the literature on social relations models,\nis suitably defined to handle network data, which are rarely used in health\neconomics. The results show that distance plays an important role in hospital\ncooperation, though there are other factors that matter such as geographical\ncentrality. Another empirical finding is the existence of a positive\nrelationship between hospital cooperation and the overall quality of the\nconnected hospitals. The absence of a source of information on the quality of\nhospitals accessible to all providers, such as in the form of star ratings, may\nprevent some hospitals to engage and cooperate with other hospitals of\npotentially higher quality. This may result in a lower degree of cooperation\namong hospitals and a reduction in quality overall.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 10:42:06 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Berta", "Paolo", ""], ["Vinciotti", "Veronica", ""], ["Moscone", "Francesco", ""]]}, {"id": "1911.04379", "submitter": "Sharaj Panwar", "authors": "Sharaj Panwar, Paul Rad, Tzyy-Ping Jung, Yufei Huang", "title": "Modeling EEG data distribution with a Wasserstein Generative Adversarial\n  Network to predict RSVP Events", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electroencephalography (EEG) data are difficult to obtain due to complex\nexperimental setups and reduced comfort with prolonged wearing. This poses\nchallenges to train powerful deep learning model with the limited EEG data.\nBeing able to generate EEG data computationally could address this limitation.\nWe propose a novel Wasserstein Generative Adversarial Network with gradient\npenalty (WGAN-GP) to synthesize EEG data. This network addresses several\nmodeling challenges of simulating time-series EEG data including frequency\nartifacts and training instability. We further extended this network to a\nclass-conditioned variant that also includes a classification branch to perform\nevent-related classification. We trained the proposed networks to generate one\nand 64-channel data resembling EEG signals routinely seen in a rapid serial\nvisual presentation (RSVP) experiment and demonstrated the validity of the\ngenerated samples. We also tested intra-subject cross-session classification\nperformance for classifying the RSVP target events and showed that\nclass-conditioned WGAN-GP can achieve improved event-classification performance\nover EEGNet.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 16:43:06 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2020 16:17:27 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Panwar", "Sharaj", ""], ["Rad", "Paul", ""], ["Jung", "Tzyy-Ping", ""], ["Huang", "Yufei", ""]]}, {"id": "1911.04387", "submitter": "Surya Tokdar", "authors": "Chris Glynn, Surya T Tokdar, Azeem Zaman, Valeria C Caruso, Jeffrey T\n  Mohl, Shawn M Willett and Jennifer M Groh", "title": "Analyzing second order stochasticity of neural spiking under\n  stimuli-bundle exposure", "comments": "26 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional analysis of neuroscience data involves computing average neural\nactivity over a group of trials and/or a period of time. This approach may be\nparticularly problematic when assessing the response patterns of neurons to\nmore than one simultaneously presented stimulus. In such cases, the brain must\nrepresent each individual component of the stimuli bundle, but\ntrial-and-time-pooled averaging methods are fundamentally unequipped to address\nthe means by which multi-item representation occurs. We introduce and\ninvestigate a novel statistical analysis framework that relates the firing\npattern of a single cell, exposed to a stimuli bundle, to the ensemble of its\nfiring patterns under each constituent stimulus. Existing statistical tools\nfocus on what may be called \"first order stochasticity\" in trial-to-trial\nvariation in the form of unstructured noise around a fixed firing rate curve\nassociated with a given stimulus. Our analysis is based upon the theoretical\npremise that exposure to a stimuli bundle induces additional stochasticity in\nthe cell's response pattern, in the form of a stochastically varying\nrecombination of its single stimulus firing rate curves. We discuss challenges\nto statistical estimation of such \"second order stochasticity\" and address them\nwith a novel dynamic admixture Poisson process (DAPP) model. DAPP is a\nhierarchical point process model that decomposes second order stochasticity\ninto a Gaussian stochastic process and a random vector of interpretable\nfeatures, and, facilitates borrowing of information on the latter across\nrepeated trials through latent clustering. We present empirical evidence of the\nutility of the DAPP analysis with synthetic and real neural recordings.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 16:56:41 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Glynn", "Chris", ""], ["Tokdar", "Surya T", ""], ["Zaman", "Azeem", ""], ["Caruso", "Valeria C", ""], ["Mohl", "Jeffrey T", ""], ["Willett", "Shawn M", ""], ["Groh", "Jennifer M", ""]]}, {"id": "1911.04541", "submitter": "Vasilis Palaskas", "authors": "Ioannis Ntzoufras, Vasilis Palaskas and Sotiris Drikos", "title": "Bayesian models for prediction of the set-difference in volleyball", "comments": "IMA Journal of Management Mathematics, 2021", "journal-ref": null, "doi": "10.1093/imaman/dpab007", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to study and develop Bayesian models for the\nanalysis of volleyball match outcomes as recorded by the set-difference. Due to\nthe peculiarity of the outcome variable (set-difference) which takes discrete\nvalues from $-3$ to $3$, we cannot consider standard models based on the usual\nPoisson or binomial assumptions used for other sports such as football/soccer.\nHence, the first and foremost challenge was to build models appropriate for the\nset-differences of each volleyball match. Here we consider two major\napproaches:\n  a) an ordered multinomial logistic regression model and\n  b) a model based on a truncated version of the Skellam distribution.\n  For the first model, we consider the set-difference as an ordinal response\nvariable within the framework of multinomial logistic regression models.\nConcerning the second model, we adjust the Skellam distribution in order to\naccount for the volleyball rules. We fit and compare both models with the same\ncovariate structure as in Karlis & Ntzoufras (2003). Both models are fitted,\nillustrated and compared within Bayesian framework using data from both the\nregular season and the play-offs of the season 2016/17 of the Greek national\nmen's volleyball league A1.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 19:48:22 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 17:18:58 GMT"}, {"version": "v3", "created": "Fri, 26 Feb 2021 20:29:37 GMT"}, {"version": "v4", "created": "Tue, 2 Mar 2021 10:03:50 GMT"}, {"version": "v5", "created": "Tue, 22 Jun 2021 07:34:16 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Ntzoufras", "Ioannis", ""], ["Palaskas", "Vasilis", ""], ["Drikos", "Sotiris", ""]]}, {"id": "1911.04561", "submitter": "Elizabeth Eisenhauer", "authors": "Elizabeth Eisenhauer and Ephraim Hanks", "title": "A Lattice and Random Intermediate Point Sampling Design for Animal\n  Movement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Animal movement studies have become ubiquitous in animal ecology for\nestimation of space use and analysis of movement behavior. In these studies,\nanimal movement data are primarily collected at regular time intervals. We\npropose an irregular sampling design which could lead to greater efficiency and\ninformation gain in animal movement studies. Our novel sampling design, called\nlattice and random intermediate point (LARI), combines samples at regular and\nrandom time intervals. We compare the LARI sampling design to regular sampling\ndesigns in an example with common black carpenter ant location data, an example\nwith guppy location data, and a simulation study of movement with a point of\nattraction. We modify a general stochastic differential equation model to allow\nfor irregular time intervals and use this framework to compare sampling\ndesigns. When parameters are estimated reasonably well, regular sampling\nresults in greater precision and accuracy in prediction of missing data.\nHowever, in each of the data and simulation examples explored in this paper,\nLARI sampling results in more accurate and precise parameter estimation, and\nthus better prediction of missing data as well. This result suggests that\nresearchers might gain greater insight into underlying animal movement\nprocesses by choosing LARI sampling over regular sampling.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 20:56:19 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Eisenhauer", "Elizabeth", ""], ["Hanks", "Ephraim", ""]]}, {"id": "1911.04616", "submitter": "Ziheng Chen", "authors": "Ziheng Chen and Hongshik Ahn", "title": "Item Response Theory based Ensemble in Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose a novel probabilistic framework to improve the\naccuracy of a weighted majority voting algorithm. In order to assign higher\nweights to the classifiers which can correctly classify hard-to-classify\ninstances, we introduce the Item Response Theory (IRT) framework to evaluate\nthe samples' difficulty and classifiers' ability simultaneously. Three models\nare created with different assumptions suitable for different cases. When\nmaking an inference, we keep a balance between the accuracy and complexity. In\nour experiment, all the base models are constructed by single trees via\nbootstrap. To explain the models, we illustrate how the IRT ensemble model\nconstructs the classifying boundary. We also compare their performance with\nother widely used methods and show that our model performs well on 19 datasets.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 23:48:18 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Chen", "Ziheng", ""], ["Ahn", "Hongshik", ""]]}, {"id": "1911.04927", "submitter": "Rebecka J\\\"ornsten", "authors": "Jonatan Kallus, Patrik Johansson, Sven Nelander, Rebecka J\\\"ornsten", "title": "MM-PCA: Integrative Analysis of Multi-group and Multi-view Data", "comments": "Manuscript+Supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data integration is the problem of combining multiple data groups (studies,\ncohorts) and/or multiple data views (variables, features). This task is\nbecoming increasingly important in many disciplines due to the prevalence of\nlarge and heterogeneous data sets. Data integration commonly aims to identify\nstructure that is consistent across multiple cohorts and feature sets. While\nsuch joint analyses can boost information from single data sets, it is also\npossible that a globally restrictive integration of heterogeneous data may\nobscure signal of interest.\n  Here, we therefore propose a data adaptive integration method, allowing for\nstructure in data to be shared across an a priori unknown \\emph{subset of\ncohorts and views}. The method, Multi-group Multi-view Principal Component\nAnalysis (MM-PCA), identifies partially shared, sparse low-rank components.\nThis also results in an integrative bi-clustering across cohorts and views. The\nstrengths of MM-PCA are illustrated on simulated data and on 'omics data from\nThe Cancer Genome Atlas. MM-PCA is available as an R-package.\n  Key words: Data integration, Multi-view, Multi-group, Bi-clustering\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 15:18:35 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Kallus", "Jonatan", ""], ["Johansson", "Patrik", ""], ["Nelander", "Sven", ""], ["J\u00f6rnsten", "Rebecka", ""]]}, {"id": "1911.04932", "submitter": "Jesus Lago", "authors": "Jesus Lago and Karel De Brabandere and Fjo De Ridder and Bart De\n  Schutter", "title": "Short-term forecasting of solar irradiance without local telemetry: a\n  generalized model using satellite data", "comments": null, "journal-ref": "Solar Energy 173 (2018), pages 566-577", "doi": "10.1016/j.solener.2018.07.050", "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the increasing integration of solar power into the electrical grid,\nforecasting short-term solar irradiance has become key for many applications,\ne.g.~operational planning, power purchases, reserve activation, etc. In this\ncontext, as solar generators are geographically dispersed and ground\nmeasurements are not always easy to obtain, it is very important to have\ngeneral models that can predict solar irradiance without the need of local\ndata. In this paper, a model that can perform short-term forecasting of solar\nirradiance in any general location without the need of ground measurements is\nproposed. To do so, the model considers satellite-based measurements and\nweather-based forecasts, and employs a deep neural network structure that is\nable to generalize across locations; particularly, the network is trained only\nusing a small subset of sites where ground data is available, and the model is\nable to generalize to a much larger number of locations where ground data does\nnot exist. As a case study, 25 locations in The Netherlands are considered and\nthe proposed model is compared against four local models that are individually\ntrained for each location using ground measurements. Despite the general nature\nof the model, it is shown show that the proposed model is equal or better than\nthe local models: when comparing the average performance across all the\nlocations and prediction horizons, the proposed model obtains a 31.31% rRMSE\n(relative root mean square error) while the best local model achieves a 32.01%\nrRMSE.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 15:30:55 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Lago", "Jesus", ""], ["De Brabandere", "Karel", ""], ["De Ridder", "Fjo", ""], ["De Schutter", "Bart", ""]]}, {"id": "1911.05036", "submitter": "Jayant Mukhopadhaya", "authors": "Jayant Mukhopadhaya, Brian T. Whitehead, John F. Quindlen, Juan J.\n  Alonso", "title": "Multi-Fidelity modeling of Probabilistic Aerodynamic Databases for Use\n  in Aerospace Engineering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.flu-dyn stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explicit quantification of uncertainty in engineering simulations is being\nincreasingly used to inform robust and reliable design practices. In the\naerospace industry, computationally-feasible analyses for design optimization\npurposes often introduce significant uncertainties due to deficiencies in the\nmathematical models employed. In this paper, we discuss two recent improvements\nin the quantification and combination of uncertainties from multiple sources\nthat can help generate probabilistic aerodynamic databases for use in aerospace\nengineering problems. We first discuss the eigenspace perturbation methodology\nto estimate model-form uncertainties stemming from inadequacies in the\nturbulence models used in Reynolds-Averaged Navier-Stokes Computational Fluid\nDynamics (RANS CFD) simulations. We then present a multi-fidelity Gaussian\nProcess framework that can incorporate noisy observations to generate\nintegrated surrogate models that provide mean as well as variance information\nfor Quantities of Interest (QoIs). The process noise is varied spatially across\nthe domain and across fidelity levels. Both these methodologies are\ndemonstrated through their application to a full configuration aircraft\nexample, the NASA Common Research Model (CRM) in transonic conditions. First,\nmodel-form uncertainties associated with RANS CFD simulations are estimated.\nThen, data from different sources is used to generate multi-fidelity\nprobabilistic aerodynamic databases for the NASA CRM. We discuss the\ntransformative effect that affordable and early treatment of uncertainties can\nhave in traditional aerospace engineering practices. The results are presented\nand compared to those from a Gaussian Process regression performed on a single\ndata source.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 17:52:32 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Mukhopadhaya", "Jayant", ""], ["Whitehead", "Brian T.", ""], ["Quindlen", "John F.", ""], ["Alonso", "Juan J.", ""]]}, {"id": "1911.05044", "submitter": "Kurt Riedel", "authors": "Kurt S. Riedel", "title": "Combinatorial Models of Cross-Country Dual Meets: What is a Big Victory?", "comments": "19 pages, 5 figures", "journal-ref": "J. Quant. Analysis in Sports, 2019", "doi": null, "report-no": null, "categories": "stat.AP econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combinatorial/probabilistic models for cross-country dual-meets are proposed.\nThe first model assumes that all runners are equally likely to finish in any\npossible order. The second model assumes that each team is selected from a\nlarge identically distributed population of potential runners and with each\npotential runner's ranking determined by the initial draw from the combined\npopulation.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 18:06:45 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Riedel", "Kurt S.", ""]]}, {"id": "1911.05103", "submitter": "Mark Risser", "authors": "Mark D. Risser and Michael F. Wehner", "title": "The effect of geographic sampling on evaluation of extreme precipitation\n  in high resolution climate models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traditional approaches for comparing global climate models and observational\ndata products typically fail to account for the geographic location of the\nunderlying weather station data. For modern high-resolution models, this is an\noversight since there are likely grid cells where the physical output of a\nclimate model is compared with a statistically interpolated quantity instead of\nactual measurements of the climate system. In this paper, we quantify the\nimpact of geographic sampling on the relative performance of high resolution\nclimate models' representation of precipitation extremes in Boreal winter (DJF)\nover the contiguous United States (CONUS), comparing model output from five\nearly submissions to the HighResMIP subproject of the CMIP6 experiment. We find\nthat properly accounting for the geographic sampling of weather stations can\nsignificantly change the assessment of model performance. Across the models\nconsidered, failing to account for sampling impacts the different metrics\n(extreme bias, spatial pattern correlation, and spatial variability) in\ndifferent ways (both increasing and decreasing). We argue that the geographic\nsampling of weather stations should be accounted for in order to yield a more\nstraightforward and appropriate comparison between models and observational\ndata sets, particularly for high resolution models. While we focus on the CONUS\nin this paper, our results have important implications for other global land\nregions where the sampling problem is more severe.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 19:11:29 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 17:34:29 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Risser", "Mark D.", ""], ["Wehner", "Michael F.", ""]]}, {"id": "1911.05139", "submitter": "Elias Chaibub Neto", "authors": "Elias Chaibub Neto, Meghasyam Tummalacherla, Lara Mangravite, Larsson\n  Omberg", "title": "Causality-based tests to detect the influence of confounders on mobile\n  health diagnostic applications: a comparison with restricted permutations", "comments": "Machine Learning for Health (ML4H) at NeurIPS 2019 - Extended\n  Abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning practice is often impacted by confounders. Confounding can\nbe particularly severe in remote digital health studies where the participants\nself-select to enter the study. While many different confounding adjustment\napproaches have been proposed in the literature, most of these methods rely on\nmodeling assumptions, and it is unclear how robust they are to violations of\nthese assumptions. This realization has recently motivated the development of\nrestricted permutation methods to quantify the influence of observed\nconfounders on the predictive performance of a machine learning models and\nevaluate if confounding adjustment methods are working as expected. In this\npaper we show, nonetheless, that restricted permutations can generate biased\nestimates of the contribution of the confounders to the predictive performance\nof a learner, and we propose an alternative approach to tackle this problem. By\nviewing a classification task from a causality perspective, we are able to\nleverage conditional independence tests between predictions and test set labels\nand confounders in order to detect confounding on the predictive performance of\na classifier. We illustrate the application of our causality-based approach to\ndata collected from mHealth study in Parkinson's disease.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 20:56:30 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Neto", "Elias Chaibub", ""], ["Tummalacherla", "Meghasyam", ""], ["Mangravite", "Lara", ""], ["Omberg", "Larsson", ""]]}, {"id": "1911.05148", "submitter": "Lizet Sanchez", "authors": "Patricia Luaces, Lizet Sanchez, Danay Saavedra, Tania Crombet, Wim Van\n  der Elst, Ariel Alonso, Geert Molenberghs, Agustin Lage", "title": "Identifying predictive biomarkers of CIMAvaxEGF success in advanced Lung\n  Cancer Patients", "comments": "5 pages, 1 table, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objectives: To identify predictive biomarkers of CIMAvaxEGF success in the\ntreatment of Non-Small Cell Lung Cancer Patients. Methods: Data from a clinical\ntrial evaluating the effect on survival time of CIMAvax-EGF versus best\nsupportive care were analyzed retrospectively following the causal inference\napproach. Pre-treatment potential predictive biomarkers included basal serum\nEGF concentration, peripheral blood parameters and immunosenescence biomarkers\n(The proportion of CD8 + CD28- T cells, CD4+ and CD8+ T cells, CD4 CD8 ratio\nand CD19+ B cells. The 33 patients with complete information were included. The\npredictive causal information (PCI) was calculated for all possible models. The\nmodel with a minimum number of predictors, but with high prediction accuracy\n(PCI>0.7) was selected. Good, rare and poor responder patients were identified\nusing the predictive probability of treatment success. Results: The mean of PCI\nincreased from 0.486, when only one predictor is considered, to 0.98 using the\nmultivariate approach with all predictors. The model considering the proportion\nof CD4+ T cell, basal EGF concentration, NLR, Monocytes, and Neutrophils as\npredictors were selected (PCI>0.74). Patients predicted as good responders\naccording to the pre-treatment biomarkers values treated with CIMAvax-EGF had a\nsignificant higher observed survival compared with the control group (p=0.03).\nNo difference was observed for bad responders. Conclusions: Peripheral blood\nparameters and immunosenescence biomarkers together with basal EGF\nconcentration in serum resulted in good predictors of the CIMAvax-EGF success\nin advanced NSCLC. The study illustrates the application of a new methodology,\nbased on causal inference, to evaluate multivariate pre-treatment predictors\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 21:10:26 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Luaces", "Patricia", ""], ["Sanchez", "Lizet", ""], ["Saavedra", "Danay", ""], ["Crombet", "Tania", ""], ["Van der Elst", "Wim", ""], ["Alonso", "Ariel", ""], ["Molenberghs", "Geert", ""], ["Lage", "Agustin", ""]]}, {"id": "1911.05423", "submitter": "Roel Ceballos", "authors": "Analaine May A. Tatoy, Roel F. Ceballos", "title": "Human Immunodeficiency Virus(HIV) Cases in the Philippines: Analysis and\n  Forecasting", "comments": null, "journal-ref": "JP Journal of Biostatistics, 16(2), 67-77, 2019", "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reports from the Health Department in the Philippines show that cases of\nHuman Immunodeficiency Virus (HIV) are increasing despite management and\ncontrol efforts by the government. Worldwide, the Philippines has one of the\nfastest growing number of HIV cases. The aim of the study is to analyze HIV\ncases by determining the best model in forecasting its future number of cases.\nThe data set was retrieved from National HIV/AIDS and STI Surveillance and\nStrategic Information Unit (NHSSS) of the Department of Health containing 132\nobservations. This data set was divided into two parts, one for model building\nand another for forecast evaluation. The original series has an increasing\ntrend and is nonstationary with indication of non-constant variance. Box-Cox\ntransformation and ordinary differencing were performed on the series. The\ndifferenced series is stationary and tentative models were identified through\nACF and PACF plots. SARIMA has the smallest chosen AIC value. The chosen model\nundergoes the diagnostic checking. The residuals of the model behave like a\nwhite noise while the forecast errors behave like a Gaussian white noise.\nConsidering all diagnostics, the model may be used for forecasting the monthly\ncases of HIV in the Philippines. Forecasted values show that HIV cases will\nmaintain their current trend.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 12:29:51 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Tatoy", "Analaine May A.", ""], ["Ceballos", "Roel F.", ""]]}, {"id": "1911.05497", "submitter": "Hongtao Liu", "authors": "Hongtao Liu", "title": "Going Negative Online? -- A Study of Negative Advertising on Social\n  Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A growing number of empirical studies suggest that negative advertising is\neffective in campaigning, while the mechanisms are rarely mentioned. With the\nscandal of Cambridge Analytica and Russian intervention behind the Brexit and\nthe 2016 presidential election, people have become aware of the political ads\non social media and have pressured congress to restrict political advertising\non social media. Following the related legislation, social media companies\nbegan disclosing their political ads archive for transparency during the summer\nof 2018 when the midterm election campaign was just beginning. This research\ncollects the data of the related political ads in the context of the U.S.\nmidterm elections since August to study the overall pattern of political ads on\nsocial media and uses sets of machine learning methods to conduct sentiment\nanalysis on these ads to classify the negative ads. A novel approach is applied\nthat uses AI image recognition to study the image data. Through data\nvisualization, this research shows that negative advertising is still the\nminority, Republican advertisers and third party organizations are more likely\nto engage in negative advertising than their counterparts. Based on ordinal\nregressions, this study finds that anger evoked information-seeking is one of\nthe main mechanisms causing negative ads to be more engaging and effective\nrather than the negative bias theory. Overall, this study provides a unique\nunderstanding of political advertising on social media by applying innovative\ndata science methods. Further studies can extend the findings, methods, and\ndatasets in this study, and several suggestions are given for future research.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 15:43:23 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Liu", "Hongtao", ""]]}, {"id": "1911.05522", "submitter": "Tyler McCormick", "authors": "Wesley Lee and Tyler H. McCormick and Joshua Neil and Cole Sodja and\n  Yanran Cui", "title": "Anomaly Detection in Large Scale Networks with Latent Space Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CR cs.SI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a real-time anomaly detection algorithm for directed activity on\nlarge, sparse networks. We model the propensity for future activity using a\ndynamic logistic model with interaction terms for sender- and receiver-specific\nlatent factors in addition to sender- and receiver-specific popularity scores;\ndeviations from this underlying model constitute potential anomalies. Latent\nnodal attributes are estimated via a variational Bayesian approach and may\nchange over time, representing natural shifts in network activity. Estimation\nis augmented with a case-control approximation to take advantage of the\nsparsity of the network and reduces computational complexity from $O(N^2)$ to\n$O(E)$, where $N$ is the number of nodes and $E$ is the number of observed\nedges. We run our algorithm on network event records collected from an\nenterprise network of over 25,000 computers and are able to identify a red team\nattack with half the detection rate required of the model without latent\ninteraction terms.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 14:57:20 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 18:20:46 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Lee", "Wesley", ""], ["McCormick", "Tyler H.", ""], ["Neil", "Joshua", ""], ["Sodja", "Cole", ""], ["Cui", "Yanran", ""]]}, {"id": "1911.05592", "submitter": "Haiyan Zheng", "authors": "Haiyan Zheng, Lisa V. Hampson, Thomas Jaki", "title": "A Bayesian hierarchical model for bridging across patient subgroups in\n  phase I clinical trials with animal data", "comments": "The main paper has 20 pages, 4 figures and 1 table", "journal-ref": "Statistical Methods in Medical Research 2021", "doi": "10.1177/0962280220986580", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorporating preclinical animal data, which can be regarded as a special\nkind of historical data, into phase I clinical trials can improve decision\nmaking when very little about human toxicity is known. In this paper, we\ndevelop a robust hierarchical modelling approach to leverage animal data into\nnew phase I clinical trials, where we bridge across non-overlapping,\npotentially heterogeneous patient subgroups. Translation parameters are used to\nbring both historical and contemporary data onto a common dosing scale. This\nleads to feasible exchangeability assumptions that the parameter vectors, which\nunderpin the dose-toxicity relationship per study, are assumed to be drawn from\na common distribution. Moreover, human dose-toxicity parameter vectors are\nassumed to be exchangeable either with the standardised, animal study-specific\nparameter vectors, or between themselves. Possibility of non-exchangeability\nfor each parameter vector is considered to avoid inferences for extreme\nsubgroups being overly influenced by the other. We illustrate the proposed\napproach with several trial data examples, and evaluate the operating\ncharacteristics of our model compared with several alternatives in a simulation\nstudy. Numerical results show that our approach yields robust inferences in\ncircumstances, where data from multiple sources are inconsistent and/or the\nbridging assumptions are incorrect.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 16:37:01 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Zheng", "Haiyan", ""], ["Hampson", "Lisa V.", ""], ["Jaki", "Thomas", ""]]}, {"id": "1911.05647", "submitter": "Ishanu Chattopadhyay", "authors": "Timmy Li, Yi Huang, James Evans and Ishanu Chattopadhyay", "title": "Long-range Event-level Prediction and Response Simulation for Urban\n  Crime and Global Terrorism with Granger Networks", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale trends in urban crime and global terrorism are well-predicted by\nsocio-economic drivers, but focused, event-level predictions have had limited\nsuccess. Standard machine learning approaches are promising, but lack\ninterpretability, are generally interpolative, and ineffective for precise\nfuture interventions with costly and wasteful false positives. Here, we are\nintroducing Granger Network inference as a new forecasting approach for\nindividual infractions with demonstrated performance far surpassing past\nresults, yet transparent enough to validate and extend social theory.\nConsidering the problem of predicting crime in the City of Chicago, we achieve\nan average AUC of ~90\\% for events predicted a week in advance within spatial\ntiles approximately $1000$ ft across. Instead of pre-supposing that crimes\nunfold across contiguous spaces akin to diffusive systems, we learn the local\ntransport rules from data. As our key insights, we uncover indications of\nsuburban bias -- how law-enforcement response is modulated by socio-economic\ncontexts with disproportionately negative impacts in the inner city -- and how\nthe dynamics of violent and property crimes co-evolve and constrain each other\n-- lending quantitative support to controversial pro-active policing policies.\nTo demonstrate broad applicability to spatio-temporal phenomena, we analyze\nterror attacks in the middle-east in the recent past, and achieve an AUC of\n~80% for predictions made a week in advance, and within spatial tiles measuring\napproximately 120 miles across. We conclude that while crime operates near an\nequilibrium quickly dissipating perturbations, terrorism does not. Indeed\nterrorism aims to destabilize social order, as shown by its dynamics being\nsusceptible to run-away increases in event rates under small perturbations.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 15:41:50 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Li", "Timmy", ""], ["Huang", "Yi", ""], ["Evans", "James", ""], ["Chattopadhyay", "Ishanu", ""]]}, {"id": "1911.05652", "submitter": "Petr Plechac", "authors": "Petr Plech\\'a\\v{c}", "title": "Relative contributions of Shakespeare and Fletcher in Henry VIII: An\n  Analysis Based on Most Frequent Words and Most Frequent Rhythmic Patterns", "comments": null, "journal-ref": null, "doi": "10.1093/llc/fqaa032", "report-no": null, "categories": "cs.CL cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The versified play Henry VIII is nowadays widely recognized to be a\ncollaborative work not written solely by William Shakespeare. We employ\ncombined analysis of vocabulary and versification together with machine\nlearning techniques to determine which authors also took part in the writing of\nthe play and what were their relative contributions. Unlike most previous\nstudies, we go beyond the attribution of particular scenes and use the rolling\nattribution approach to determine the probabilities of authorship of pieces of\ntexts, without respecting the scene boundaries. Our results highly support the\ncanonical division of the play between William Shakespeare and John Fletcher\nproposed by James Spedding, but also bring new evidence supporting the\nmodifications proposed later by Thomas Merriam.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 22:40:05 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Plech\u00e1\u010d", "Petr", ""]]}, {"id": "1911.05691", "submitter": "Reynaldo Martina", "authors": "Reynaldo Martina, Keith Abrams, Sylwia Bujkiewicz, David Jenkins,\n  Pascale Dequen, Michael Lees, Frank A. Corvino, and Jessica Davies", "title": "The use of registry data to extrapolate overall survival results from\n  randomised controlled trials", "comments": "21 Pages, 7 named figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Pre-marketing authorisation estimates of survival are generally\nrestricted to those observed directly in randomised controlled trials (RCTs).\nHowever, for regulatory and Health Technology Assessment (HTA) decision-making\na longer time horizon is often required than is studied in RCTs. Therefore,\nextrapolation is required to estimate long-term treatment effect. Registry data\ncan provide evidence to support extrapolation of treatment effects from RCTs,\nwhich are considered the main sources of evidence of effect for new drug\napplications. A number of methods are available to extrapolate survival data,\nsuch as Exponential, Weibull, Gompertz, log-logistic or log-normal parametric\nmodels. The different methods have varying functional forms and can result in\ndifferent survival estimates.\n  Methods: The aim of this paper was to use registry data to supplement the\nrelatively short term RCT data to obtain long term estimates of effect. No\nformal hypotheses were tested. We explore the above parametric regression\nmodels as well as a nonparametric regression model based on local linear\n(parametric) regression. We also explore a Bayesian model constrained to the\nlong term estimate of survival reported in literature, a Bayesian power prior\napproach on the variability observed from published literature, and a Bayesian\nModel Averaging (BMA) approach. The methods were applied to extrapolate overall\nsurvival of a RCT in metastatic melanoma.\n  Results: The results showed that the BMA approach was able to fit the RCT\ndata well, with the lowest variability of the area under the curve up to 72\nmonths with or without the SEER Medicare registry.\n  Conclusion: the BMA approach is a viable approach to extrapolate overall\nsurvival in the absence of long term data.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 11:11:33 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Martina", "Reynaldo", ""], ["Abrams", "Keith", ""], ["Bujkiewicz", "Sylwia", ""], ["Jenkins", "David", ""], ["Dequen", "Pascale", ""], ["Lees", "Michael", ""], ["Corvino", "Frank A.", ""], ["Davies", "Jessica", ""]]}, {"id": "1911.05770", "submitter": "Claire Donnat", "authors": "Claire Donnat, Leonardo Tozzi, Susan Holmes", "title": "Constrained Bayesian ICA for Brain Connectome Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain connectomics is a developing field in neurosciences which strives to\nunderstand cognitive processes and psychiatric diseases through the analysis of\ninteractions between brain regions. However, in the high-dimensional,\nlow-sample, and noisy regimes that typically characterize fMRI data, the\nrecovery of such interactions remains an ongoing challenge: how can we discover\npatterns of co-activity between brain regions that could then be associated to\ncognitive processes or psychiatric disorders? In this paper, we investigate a\nconstrained Bayesian ICA approach which, in comparison to current methods,\nsimultaneously allows (a) the flexible integration of multiple sources of\ninformation (fMRI, DWI, anatomical, etc.), (b) an automatic and parameter-free\nselection of the appropriate sparsity level and number of connected submodules\nand (c) the provision of estimates on the uncertainty of the recovered\ninteractions. Our experiments, both on synthetic and real-life data, validate\nthe flexibility of our method and highlight the benefits of integrating\nanatomical information for connectome inference.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 19:25:47 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Donnat", "Claire", ""], ["Tozzi", "Leonardo", ""], ["Holmes", "Susan", ""]]}, {"id": "1911.05881", "submitter": "Gregory Bopp", "authors": "Gregory P. Bopp, Benjamin A. Shaby, Chris E. Forest, Alfonso Mej\\'ia", "title": "Projecting Flood-Inducing Precipitation with a Bayesian Analogue Model", "comments": null, "journal-ref": null, "doi": "10.1007/s13253-020-00391-6", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The hazard of pluvial flooding is largely influenced by the spatial and\ntemporal dependence characteristics of precipitation. When extreme\nprecipitation possesses strong spatial dependence, the risk of flooding is\namplified due to catchment factors that cause runoff accumulation such as\ntopography. Temporal dependence can also increase flood risk as storm water\ndrainage systems operating at capacity can be overwhelmed by heavy\nprecipitation occurring over multiple days. While transformed Gaussian\nprocesses are common choices for modeling precipitation, their weak tail\ndependence may lead to underestimation of flood risk. Extreme value models such\nas the generalized Pareto processes for threshold exceedances and max-stable\nmodels are attractive alternatives, but are difficult to fit when the number of\nobservation sites is large, and are of little use for modeling the bulk of the\ndistribution, which may also be of interest to water management planners. While\nthe atmospheric dynamics governing precipitation are complex and difficult to\nfully incorporate into a parsimonious statistical model, non-mechanistic\nanalogue methods that approximate those dynamics have proven to be promising\napproaches to capturing the temporal dependence of precipitation. In this\npaper, we present a Bayesian analogue method that leverages large,\nsynoptic-scale atmospheric patterns to make precipitation forecasts. Changing\nspatial dependence across varying intensities is modeled as a mixture of\nspatial Student-t processes that can accommodate both strong and weak tail\ndependence. The proposed model demonstrates improved performance at capturing\nthe distribution of extreme precipitation over Community Atmosphere Model (CAM)\n5.2 forecasts.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 01:03:24 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Bopp", "Gregory P.", ""], ["Shaby", "Benjamin A.", ""], ["Forest", "Chris E.", ""], ["Mej\u00eda", "Alfonso", ""]]}, {"id": "1911.05952", "submitter": "Sayantan Banerjee", "authors": "Sayantan Banerjee and Kousik Guhathakurta", "title": "Change-point Analysis in Financial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major impact of globalization has been the information flow across the\nfinancial markets rendering them vulnerable to financial contagion. Research\nhas focused on network analysis techniques to understand the extent and nature\nof such information flow. It is now an established fact that a stock market\ncrash in one country can have a serious impact on other markets across the\nglobe. It follows that such crashes or critical regimes will affect the network\ndynamics of the global financial markets. In this paper, we use sequential\nchange point detection in dynamic networks to detect changes in the network\ncharacteristics of thirteen stock markets across the globe. Our method helps us\nto detect changes in network behavior across all known stock market crashes\nduring the period of study. In most of the cases, we can detect a change in the\nnetwork characteristics prior to crash. Our work thus opens the possibility of\nusing this technique to create a warning bell for critical regimes in financial\nmarkets.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 05:54:33 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Banerjee", "Sayantan", ""], ["Guhathakurta", "Kousik", ""]]}, {"id": "1911.06239", "submitter": "Aditya Narayan Ravi", "authors": "Aditya Narayan Ravi, Pranav Poduval and Dr. Sharayu Moharir", "title": "Unreliable Multi-Armed Bandits: A Novel Approach to Recommendation\n  Systems", "comments": "4 pages, 4 figures, Aditya Narayan Ravi and Pranav Poduval have equal\n  contribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use a novel modification of Multi-Armed Bandits to create a new model for\nrecommendation systems. We model the recommendation system as a bandit seeking\nto maximize reward by pulling on arms with unknown rewards. The catch however\nis that this bandit can only access these arms through an unreliable\nintermediate that has some level of autonomy while choosing its arms. For\nexample, in a streaming website the user has a lot of autonomy while choosing\ncontent they want to watch. The streaming sites can use targeted advertising as\na means to bias opinions of these users. Here the streaming site is the bandit\naiming to maximize reward and the user is the unreliable intermediate. We model\nthe intermediate as accessing states via a Markov chain. The bandit is allowed\nto perturb this Markov chain. We prove fundamental theorems for this setting\nafter which we show a close-to-optimal Explore-Commit algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 16:55:29 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Ravi", "Aditya Narayan", ""], ["Poduval", "Pranav", ""], ["Moharir", "Dr. Sharayu", ""]]}, {"id": "1911.06302", "submitter": "Hunter Stanke", "authors": "Hunter Stanke, Andrew O. Finley, Aaron S. Weed, Brian F. Walters,\n  Grant M. Domke", "title": "rFIA: An R package for estimation of forest attributes with the Forest\n  Inventory and Analysis Database", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forest Inventory and Analysis (FIA) is a US Department of Agriculture Forest\nService program that aims to monitor changes in forests across the US. FIA\nhosts one of the largest ecological datasets in the world, though its\ncomplexity limits access for many potential users. rFIA is an R package\ndesigned to simplify the estimation of forest attributes using data collected\nby the FIA Program. Specifically, rFIA improves access to the spatio-temporal\nestimation capacity of the FIA Database via space-time indexed summaries of\nforest variables within user-defined population boundaries (e.g., geographic,\ntemporal, biophysical). The package implements multiple design-based\nestimators, and has been validated against official estimates and sampling\nerrors produced by the FIA Program. We demonstrate the utility of rFIA by\nassessing changes in abundance and mortality rates of ash (Fraxinus spp.)\npopulations in the Lower Peninsula of Michigan following the establishment of\nemerald ash borer (Agrilus planipennis).\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 18:42:58 GMT"}, {"version": "v2", "created": "Mon, 18 Nov 2019 19:39:24 GMT"}, {"version": "v3", "created": "Tue, 4 Feb 2020 12:12:56 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Stanke", "Hunter", ""], ["Finley", "Andrew O.", ""], ["Weed", "Aaron S.", ""], ["Walters", "Brian F.", ""], ["Domke", "Grant M.", ""]]}, {"id": "1911.06380", "submitter": "Sayan Dasgupta", "authors": "Cheng Zheng, Sayan Dasgupta, Yuxiang Xie, Asad Haris, and Ying Qing\n  Chen", "title": "On Data Enriched Logistic Regression", "comments": "23 pages, 4 Figures, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Biomedical researchers usually study the effects of certain exposures on\ndisease risks among a well-defined population. To achieve this goal, the gold\nstandard is to design a trial with an appropriate sample from that population.\nDue to the high cost of such trials, usually the sample size collected is\nlimited and is not enough to accurately estimate some exposures' effect. In\nthis paper, we discuss how to leverage the information from external `big data'\n(data with much larger sample size) to improve the estimation accuracy at the\nrisk of introducing small bias. We proposed a family of weighted estimators to\nbalance the bias increase and variance reduction when including the big data.\nWe connect our proposed estimator to the established penalized regression\nestimators. We derive the optimal weights using both second order and higher\norder asymptotic expansions. Using extensive simulation studies, we showed that\nthe improvement in terms of mean square error (MSE) for the regression\ncoefficient can be substantial even with finite sample sizes and our weighted\nmethod outperformed the existing methods such as penalized regression and James\nStein's approach. Also we provide theoretical guarantee that the proposed\nestimators will never lead to asymptotic MSE larger than the maximum likelihood\nestimator using small data only in general. We applied our proposed methods to\nthe Asia Cohort Consortium China cohort data to estimate the relationships\nbetween age, BMI, smoking, alcohol use and mortality.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 20:53:30 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Zheng", "Cheng", ""], ["Dasgupta", "Sayan", ""], ["Xie", "Yuxiang", ""], ["Haris", "Asad", ""], ["Chen", "Ying Qing", ""]]}, {"id": "1911.06451", "submitter": "Martin Lysy", "authors": "Yun Ling, Martin Lysy, Ian Seim, Jay M. Newby, David B. Hill, Jeremy\n  Cribb, M. Gregory Forest", "title": "Measurement Error Correction in Particle Tracking Microrheology", "comments": "31 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In diverse biological applications, particle tracking of passive microscopic\nspecies has become the experimental measurement of choice -- when either the\nmaterials are of limited volume, or so soft as to deform uncontrollably when\nmanipulated by traditional instruments. In a wide range of particle tracking\nexperiments, a ubiquitous finding is that the mean squared displacement (MSD)\nof particle positions exhibits a power-law signature, the parameters of which\nreveal valuable information about the viscous and elastic properties of various\nbiomaterials. However, MSD measurements are typically contaminated by complex\nand interacting sources of instrumental noise. As these often affect the\nhigh-frequency bandwidth to which MSD estimates are particularly sensitive,\ninadequate error correction can lead to severe bias in power law estimation and\nthereby, the inferred viscoelastic properties. In this article, we propose a\nnovel strategy to filter high-frequency noise from particle tracking\nmeasurements. Our filters are shown theoretically to cover a broad spectrum of\nhigh-frequency noises, and lead to a parametric estimator of MSD power-law\ncoefficients for which an efficient computational implementation is presented.\nBased on numerous analyses of experimental and simulated data, results suggest\nour methods perform very well compared to other denoising procedures.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 02:28:38 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Ling", "Yun", ""], ["Lysy", "Martin", ""], ["Seim", "Ian", ""], ["Newby", "Jay M.", ""], ["Hill", "David B.", ""], ["Cribb", "Jeremy", ""], ["Forest", "M. Gregory", ""]]}, {"id": "1911.06454", "submitter": "Yanbing Wang", "authors": "Yanbing Wang, George Gunter, Matthew Nice, Daniel B. Work", "title": "Estimating adaptive cruise control model parameters from on-board radar\n  units", "comments": "Accepted for poster presentation at the Transportation Research Board\n  2020 Annual Meeting, Washington D.C", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two new methods are presented for estimating car-following model parameters\nusing data collected from the Adaptive Cruise Control (ACC) enabled vehicles.\nThe vehicle is assumed to follow a constant time headway relative velocity\nmodel in which the parameters are unknown and to be determined. The first\ntechnique is a batch method that uses a least-squares approach to estimate the\nparameters from time series data of the vehicle speed, space gap, and relative\nvelocity of a lead vehicle. The second method is an online approach that uses a\nparticle filter to simultaneously estimate both the state of the system and the\nmodel parameters. Numerical experiments demonstrate the accuracy and\ncomputational performance of the methods relative to a commonly used\nsimulation-based optimization approach. The methods are also assessed on\nempirical data collected from a 2019 model year ACC vehicle driven in a highway\nenvironment. Speed, space gap, and relative velocity data are recorded directly\nfrom the factory-installed radar unit via the vehicle's CAN bus. All three\nmethods return similar mean absolute error values in speed and spacing compared\nto the recorded data. The least-squares method has the fastest run-time\nperformance, and is up to 3 orders of magnitude faster than other methods. The\nparticle filter is faster than real-time, and therefore is suitable in\nstreaming applications in which the datasets can grow arbitrarily large.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 02:40:37 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Wang", "Yanbing", ""], ["Gunter", "George", ""], ["Nice", "Matthew", ""], ["Work", "Daniel B.", ""]]}, {"id": "1911.06708", "submitter": "Emanuele Aliverti", "authors": "Emanuele Aliverti, Jeff Tilson, Dayne Filer, Benjamin Babcock,\n  Alejandro Colaneri, Jennifer Ocasio, Timothy R. Gershon, Kirk C. Wilhelmsen\n  and David B. Dunson", "title": "Projected $t$-SNE for batch correction", "comments": "16 pages, 3 figures", "journal-ref": "Bioinformatics, 2020, 1-6", "doi": "10.1093/bioinformatics/btaa189", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biomedical research often produces high-dimensional data confounded by batch\neffects such as systematic experimental variations, different protocols and\nsubject identifiers. Without proper correction, low-dimensional representation\nof high-dimensional data might encode and reproduce the same systematic\nvariations observed in the original data, and compromise the interpretation of\nthe results. In this article, we propose a novel procedure to remove batch\neffects from low-dimensional embeddings obtained with t-SNE dimensionality\nreduction. The proposed methods are based on linear algebra and constrained\noptimization, leading to efficient algorithms and fast computation in many\nhigh-dimensional settings. Results on artificial single-cell transcription\nprofiling data show that the proposed procedure successfully removes multiple\nbatch effects from t-SNE embeddings, while retaining fundamental information on\ncell types. When applied to single-cell gene expression data to investigate\nmouse medulloblastoma, the proposed method successfully removes batches related\nwith mice identifiers and the date of the experiment, while preserving clusters\nof oligodendrocytes, astrocytes, and endothelial cells and microglia, which are\nexpected to lie in the stroma within or adjacent to the tumors.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 15:36:58 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2020 16:01:09 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Aliverti", "Emanuele", ""], ["Tilson", "Jeff", ""], ["Filer", "Dayne", ""], ["Babcock", "Benjamin", ""], ["Colaneri", "Alejandro", ""], ["Ocasio", "Jennifer", ""], ["Gershon", "Timothy R.", ""], ["Wilhelmsen", "Kirk C.", ""], ["Dunson", "David B.", ""]]}, {"id": "1911.06857", "submitter": "Samuele Centorrino", "authors": "Samuele Centorrino, Aman Ullah, Jing Xue", "title": "Semiparametric Estimation of Correlated Random Coefficient Models\n  without Instrumental Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a linear random coefficient model where slope parameters may be\ncorrelated with some continuous covariates. Such a model specification may\noccur in empirical research, for instance, when quantifying the effect of a\ncontinuous treatment observed at two time periods. We show one can carry\nidentification and estimation without instruments. We propose a semiparametric\nestimator of average partial effects and of average treatment effects on the\ntreated. We showcase the small sample properties of our estimator in an\nextensive simulation study. Among other things, we reveal that it compares\nfavorably with a control function estimator. We conclude with an application to\nthe effect of malaria eradication on economic development in Colombia.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 20:05:51 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Centorrino", "Samuele", ""], ["Ullah", "Aman", ""], ["Xue", "Jing", ""]]}, {"id": "1911.06884", "submitter": "George Leckie", "authors": "George Leckie, Lucy Prior and Harvey Goldstein", "title": "The implications of Labour's plan to scrap Key Stage 2 tests for\n  Progress 8 and secondary school accountability in England", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In England, Progress 8 is the Conservative government's headline secondary\nschool performance and accountability measure. Progress 8 attempts to measure\nthe average academic progress pupils make in each school between their KS2\ntests and their GCSE Attainment 8 examinations. The Labour opposition recently\nannounced they would scrap the KS2 tests were they to be elected. Such a move,\nhowever, would preclude the publication of Progress 8 and would leave schools\nto be compared in terms of their average Attainment 8 scores or, at best, their\nAttainment 8 scores only adjusted for school differences in pupil demographic\nand socioeconomic characteristics. In this paper, we argue and illustrate\nempirically that this best-case scenario of an 'Adjusted Attainment 8' measure\nwould prove less fair and meaningful than Progress 8 and therefore a backwards\nstep, especially when Progress 8 itself has been criticised as biased against\nschools teaching educationally disadvantaged intakes.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 21:28:17 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Leckie", "George", ""], ["Prior", "Lucy", ""], ["Goldstein", "Harvey", ""]]}, {"id": "1911.06888", "submitter": "George Leckie", "authors": "George Leckie, William Browne, Harvey Goldstein, Juan Merlo, Peter\n  Austin", "title": "Variance partitioning in multilevel models for count data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A first step when fitting multilevel models to continuous responses is to\nexplore the degree of clustering in the data. Researchers fit\nvariance-component models and then report the proportion of variation in the\nresponse that is due to systematic differences between clusters. Equally they\nreport the response correlation between units within a cluster. These\nstatistics are popularly referred to as variance partition coefficients (VPCs)\nand intraclass correlation coefficients (ICCs). When fitting multilevel models\nto categorical (binary, ordinal, or nominal) and count responses, these\nstatistics prove more challenging to calculate. For categorical response\nmodels, researchers appeal to their latent response formulations and report\nVPCs/ICCs in terms of latent continuous responses envisaged to underly the\nobserved categorical responses. For standard count response models, however,\nthere are no corresponding latent response formulations. More generally, there\nis a paucity of guidance on how to partition the variation. As a result,\napplied researchers are likely to avoid or inadequately report and discuss the\nsubstantive importance of clustering and cluster effects in their studies. A\nrecent article drew attention to a little-known exact algebraic expression for\nthe VPC/ICC for the special case of the two-level random-intercept Poisson\nmodel. In this article, we make a substantial new contribution. First, we\nderive exact VPC/ICC expressions for more flexible negative binomial models\nthat allows for overdispersion, a phenomenon which often occurs in practice.\nThen we derive exact VPC/ICC expressions for three-level and random-coefficient\nextensions to these models. We illustrate our work with an application to\nstudent absenteeism.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 21:41:59 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 21:32:15 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Leckie", "George", ""], ["Browne", "William", ""], ["Goldstein", "Harvey", ""], ["Merlo", "Juan", ""], ["Austin", "Peter", ""]]}, {"id": "1911.06913", "submitter": "Jann Goschenhofer", "authors": "Kamer A. Yuksel, Jann Goschenhofer, Hridya V. Varma, Urban Fietzek,\n  Franz M.J. Pfister", "title": "Granular Motor State Monitoring of Free Living Parkinson's Disease\n  Patients via Deep Learning", "comments": "Machine Learning for Health (ML4H) at NeurIPS 2019 -- Extended\n  Abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Parkinson's disease (PD) is the second most common neurodegenerative disease\nworldwide and affects around 1% of the (60+ years old) elderly population in\nindustrial nations. More than 80% of PD patients suffer from motor symptoms,\nwhich could be well addressed if a personalized medication schedule and dosage\ncould be administered to them. However, such personalized medication schedule\nrequires a continuous, objective and precise measurement of motor symptoms\nexperienced by the patients during their regular daily activities. In this\nwork, we propose the use of a wrist-worn smart-watch, which is equipped with 3D\nmotion sensors, for estimating the motor fluctuation severity of PD patients in\na free-living environment. We introduce a novel network architecture, a\npost-training scheme and a custom loss function that accounts for label noise\nto improve the results of our previous work in this domain and to establish a\nnovel benchmark for nine-level PD motor state estimation.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 23:30:02 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 23:55:59 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Yuksel", "Kamer A.", ""], ["Goschenhofer", "Jann", ""], ["Varma", "Hridya V.", ""], ["Fietzek", "Urban", ""], ["Pfister", "Franz M. J.", ""]]}, {"id": "1911.06999", "submitter": "Morteza Raeisi", "authors": "Morteza Raeisi, Florent Bonneu and Edith Gabriel", "title": "A spatio-temporal multi-scale model for Geyer saturation point process:\n  application to forest fire occurrences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because most natural phenomena exhibit dependence at multiple scales like\nlocations of earthquakes or forest fire occurrences, spatio-temporal\nsingle-scale point process models are unrealistic in many applications. This\nmotivates us to construct generalizations of classical Gibbs models. In this\npaper, we extend the Geyer saturation point process model to the\nspatio-temporal multi-scale framework. The simulation process is carried out\nthrough a birth-death Metropolis-Hastings algorithm. In a simulation study, we\ncompare two common methods for statistical inference in Gibbs models: the\npseudo-likelihood and logistic likelihood approaches that we tailor to this\nmodel. Finally, we illustrate this new model on forest fire occurrences\nmodeling in Southern France.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 09:18:19 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 13:18:19 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Raeisi", "Morteza", ""], ["Bonneu", "Florent", ""], ["Gabriel", "Edith", ""]]}, {"id": "1911.07007", "submitter": "Maria Choufany", "authors": "Maria Choufany, Davide Martinetti, Rachid Senoussi, Cindy E. Morris,\n  Samuel Soubeyrand", "title": "Spatiotemporal large-scale networks shaped by air mass movements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The movement of atmospheric air masses can be seen as a continuous and\ncomplex flow of particles hovering over our planet. It can however be locally\nsimplified by considering three-dimensional trajectories of air masses\nconnecting distant areas of the globe during a given period of time.\n  In this paper, we present a mathematical framework to construct spatial and\nspatiotemporal networks where the nodes are the subsets of a partition of a\ngeographical area and the links between these nodes are inferred from sampled\ntrajectories of air masses passing over and across the nodes. We propose\ndifferent estimators of link intensities relying on different bio-physical\nhypotheses and covering adjustable time periods. This approach leads to a new\nclass of spatiotemporal networks characterized by adjacency matrices. We\napplied the approach in two real geographical contexts: the watersheds of the\nFrench region Provence-Alpes-C\\^ote d'Azur and the coastline of the\nMediterranean Sea. The analysis of the constructed networks allowed identifying\na marked seasonal pattern in air mass movements in the study areas.\n  These constructed networks can be used to investigate issues, e.g., in\naerobiology and epidemiology of airborne plant pathogens. Similar networks\ncould be estimated from other types of trajectories, such as animal\ntrajectories.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 10:23:19 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 14:52:30 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Choufany", "Maria", ""], ["Martinetti", "Davide", ""], ["Senoussi", "Rachid", ""], ["Morris", "Cindy E.", ""], ["Soubeyrand", "Samuel", ""]]}, {"id": "1911.07049", "submitter": "Haotian Xu", "authors": "St\\'ephane Guerrier, Juan Jurado, Mehran Khaghani, Gaetan Bakalli,\n  Mucyo Karemera, Roberto Molinari, Samuel Orso, John Raquet, Christine M.\n  Schubert Kabban, Jan Skaloud, Haotian Xu, Yuming Zhang", "title": "Wavelet-Based Moment-Matching Techniques for Inertial Sensor Calibration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of inertial sensor calibration has required the development of\nvarious techniques to take into account the sources of measurement error coming\nfrom such devices. The calibration of the stochastic errors of these sensors\nhas been the focus of increasing amount of research in which the method of\nreference has been the so-called \"Allan variance slope method\" which, in\naddition to not having appropriate statistical properties, requires a\nsubjective input which makes it prone to mistakes. To overcome this, recent\nresearch has started proposing \"automatic\" approaches where the parameters of\nthe probabilistic models underlying the error signals are estimated by matching\nfunctions of the Allan variance or Wavelet Variance with their model-implied\ncounterparts. However, given the increased use of such techniques, there has\nbeen no study or clear direction for practitioners on which approach is optimal\nfor the purpose of sensor calibration. This paper formally defines the class of\nestimators based on this technique and puts forward theoretical and applied\nresults that, comparing with estimators in this class, suggest the use of the\nGeneralized Method of Wavelet Moments as an optimal choice.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 15:49:09 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Guerrier", "St\u00e9phane", ""], ["Jurado", "Juan", ""], ["Khaghani", "Mehran", ""], ["Bakalli", "Gaetan", ""], ["Karemera", "Mucyo", ""], ["Molinari", "Roberto", ""], ["Orso", "Samuel", ""], ["Raquet", "John", ""], ["Kabban", "Christine M. Schubert", ""], ["Skaloud", "Jan", ""], ["Xu", "Haotian", ""], ["Zhang", "Yuming", ""]]}, {"id": "1911.07142", "submitter": "Ick Hoon Jin", "authors": "Jaewoo Park and Ick Hoon Jin and Michael Schweinberger", "title": "Bayesian Model Selection for High-Dimensional Ising Models, With\n  Applications to Educational Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Doubly-intractable posterior distributions arise in many applications of\nstatistics concerned with discrete and dependent data, including physics,\nspatial statistics, machine learning, the social sciences, and other fields. A\nspecific example is psychometrics, which has adapted high-dimensional Ising\nmodels from machine learning, with a view to studying the interactions among\nbinary item responses in educational assessments. To estimate high-dimensional\nIsing models from educational assessment data, $\\ell_1$-penalized nodewise\nlogistic regressions have been used. Theoretical results in high-dimensional\nstatistics show that $\\ell_1$-penalized nodewise logistic regressions can\nrecover the true interaction structure with high probability, provided that\ncertain assumptions are satisfied. Those assumptions are hard to verify in\npractice and may be violated, and quantifying the uncertainty about the\nestimated interaction structure and parameter estimators is challenging. We\npropose a Bayesian approach that helps quantify the uncertainty about the\ninteraction structure and parameters without requiring strong assumptions, and\ncan be applied to Ising models with thousands of parameters. We demonstrate the\nadvantages of the proposed Bayesian approach compared with $\\ell_1$-penalized\nnodewise logistic regressions by simulation studies and applications to small\nand large educational data sets with up to 2,485 parameters. Among other\nthings, the simulation studies suggest that the Bayesian approach is more\nrobust against model misspecification due to omitted covariates than\n$\\ell_1$-penalized nodewise logistic regressions.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 03:19:10 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 06:49:00 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Park", "Jaewoo", ""], ["Jin", "Ick Hoon", ""], ["Schweinberger", "Michael", ""]]}, {"id": "1911.07227", "submitter": "Leen Alawieh", "authors": "Leen Alawieh, Jonathan Goodman, John B. Bell", "title": "Iterative Construction of Gaussian Process Surrogate Models for Bayesian\n  Inference", "comments": null, "journal-ref": null, "doi": "10.1016/j.jspi.2019.11.002", "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new algorithm is developed to tackle the issue of sampling non-Gaussian\nmodel parameter posterior probability distributions that arise from solutions\nto Bayesian inverse problems. The algorithm aims to mitigate some of the\nhurdles faced by traditional Markov Chain Monte Carlo (MCMC) samplers, through\nconstructing proposal probability densities that are both, easy to sample and\nthat provide a better approximation to the target density than a simple\nGaussian proposal distribution would. To achieve that, a Gaussian proposal\ndistribution is augmented with a Gaussian Process (GP) surface that helps\ncapture non-linearities in the log-likelihood function. In order to train the\nGP surface, an iterative approach is adopted for the optimal selection of\npoints in parameter space. Optimality is sought by maximizing the information\ngain of the GP surface using a minimum number of forward model simulation runs.\nThe accuracy of the GP-augmented surface approximation is assessed in two ways.\nThe first consists of comparing predictions obtained from the approximate\nsurface with those obtained through running the actual simulation model at\nhold-out points in parameter space. The second consists of a measure based on\nthe relative variance of sample weights obtained from sampling the approximate\nposterior probability distribution of the model parameters. The efficacy of\nthis new algorithm is tested on inferring reaction rate parameters in a 3-node\nand 6-node network toy problems, which imitate idealized reaction networks in\ncombustion applications.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 12:57:32 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Alawieh", "Leen", ""], ["Goodman", "Jonathan", ""], ["Bell", "John B.", ""]]}, {"id": "1911.07368", "submitter": "Jason Wei", "authors": "Lia X. Harrington, Jason W. Wei, Arief A. Suriawinata, Todd A.\n  Mackenzie, Saeed Hassanpour", "title": "Predicting colorectal polyp recurrence using time-to-event analysis of\n  medical records", "comments": "Accepted in AMIA 2020 Informatics Summit", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying patient characteristics that influence the rate of colorectal\npolyp recurrence can provide important insights into which patients are at\nhigher risk for recurrence. We used natural language processing to extract\npolyp morphological characteristics from 953 polyp-presenting patients'\nelectronic medical records. We used subsequent colonoscopy reports to examine\nhow the time to polyp recurrence (731 patients experienced recurrence) is\ninfluenced by these characteristics as well as anthropometric features using\nKaplan-Meier curves, Cox proportional hazards modeling, and random survival\nforest models. We found that the rate of recurrence differed significantly by\npolyp size, number, and location and patient smoking status. Additionally,\nright-sided colon polyps increased recurrence risk by 30% compared to\nleft-sided polyps. History of tobacco use increased polyp recurrence risk by\n20% compared to never-users. A random survival forest model showed an AUC of\n0.65 and identified several other predictive variables, which can inform\ndevelopment of personalized polyp surveillance plans.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 00:01:23 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Harrington", "Lia X.", ""], ["Wei", "Jason W.", ""], ["Suriawinata", "Arief A.", ""], ["Mackenzie", "Todd A.", ""], ["Hassanpour", "Saeed", ""]]}, {"id": "1911.07679", "submitter": "Josie Williams", "authors": "Josie Williams and Narges Razavian", "title": "Towards Quantification of Bias in Machine Learning for Healthcare: A\n  Case Study of Renal Failure Prediction", "comments": "Accepted at Fairness in Machine Learning in Health workshop at\n  NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As machine learning (ML) models, trained on real-world datasets, become\ncommon practice, it is critical to measure and quantify their potential biases.\nIn this paper, we focus on renal failure and compare a commonly used\ntraditional risk score, Tangri, with a more powerful machine learning model,\nwhich has access to a larger variable set and trained on 1.6 million patients'\nEHR data. We will compare and discuss the generalization and applicability of\nthese two models, in an attempt to quantify biases of status quo clinical\npractice, compared to ML-driven models.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 15:04:31 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Williams", "Josie", ""], ["Razavian", "Narges", ""]]}, {"id": "1911.07694", "submitter": "Cl\\'emence Karmann Mrs", "authors": "G\\'egout-Petit Anne, Gueudin-Muller Aur\\'elie, Karmann Cl\\'emence", "title": "Graph estimation for Gaussian data zero-inflated by double truncation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of graph estimation in a zero-inflated Gaussian\nmodel. In this model, zero-inflation is obtained by double truncation (right\nand left) of a Gaussian vector. The goal is to recover the latent graph\nstructure of the Gaussian vector with observations of the zero-inflated\ntruncated vector. We propose a two step estimation procedure. The first step\nconsists in estimating each term of the covariance matrix by maximising the\ncorresponding bivariate marginal log-likelihood of the truncated vector. The\nsecond one uses the graphical lasso procedure to estimate the precision matrix\nsparsity, which encodes the graph structure. We then state some theoretical\nconvergence results about the convergence rate of the covariance matrix and\nprecision matrix estimators. These results allow us to establish consistency of\nour procedure with respect to graph structure recovery. We also present some\nsimulation studies to corroborate the efficiency of our procedure.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 15:19:28 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Anne", "G\u00e9gout-Petit", ""], ["Aur\u00e9lie", "Gueudin-Muller", ""], ["Cl\u00e9mence", "Karmann", ""]]}, {"id": "1911.07827", "submitter": "\\'Alvaro Briz-Red\\'on", "authors": "\\'Alvaro Briz-Red\\'on and Francisco Mart\\'inez-Ruiz and Francisco\n  Montes", "title": "DRHotNet: An R package for detecting differential risk hotspots on a\n  linear network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most common applications of spatial data analysis is detecting\nzones, at a certain investigation level, where a point-referenced event under\nstudy is especially concentrated. The detection of this kind of zones, which\nare usually referred to as hotspots, is essential in certain fields such as\ncriminology, epidemiology or traffic safety. Traditionally, hotspot detection\nprocedures have been developed over areal units of analysis. Although working\nat this spatial scale can be suitable enough for many research or practical\npurposes, detecting hotspots at a more accurate level (for instance, at the\nroad segment level) may be more convenient sometimes. Furthermore, it is\ntypical that hotspot detection procedures are entirely focused on the\ndetermination of zones where an event is (overall) highly concentrated. It is\nless common, by far, that such procedures prioritize the location of zones\nwhere a specific type of event is overrepresented in relation to the other\ntypes observed, which have been denoted as differential risk hotspots. The R\npackage DRHotNet provides several functionalities to facilitate the detection\nof differential risk hotspots along a linear network. In this paper, DRHotNet\nis depicted and its usage in the R console is shown through a detailed analysis\nof a crime dataset.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 18:43:04 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Briz-Red\u00f3n", "\u00c1lvaro", ""], ["Mart\u00ednez-Ruiz", "Francisco", ""], ["Montes", "Francisco", ""]]}, {"id": "1911.07902", "submitter": "Tiandong Wang", "authors": "Tiandong Wang and Sidney I. Resnick", "title": "Common Growth Patterns for Regional Social Networks: a Point Process\n  Approach", "comments": "28 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although recent research on social networks emphasizes microscopic dynamics\nsuch as retweets and social connectivity of an individual user, we focus on\nmacroscopic growth dynamics of social network link formation. Rather than\nfocusing on one particular dataset, we find invariant behavior in regional\nsocial networks that are geographically concentrated. Empirical findings\nsuggest that the startup phase of a regional network can be modeled by a\nself-exciting point process. After the startup phase ends, the growth of the\nlinks can be modeled by a non-homogeneous Poisson process with constant rate\nacross the day but varying rates from day to day, plus a nightly inactive\nperiod when local users are expected to be asleep. Conclusions are drawn based\non analyzing four different datasets, three of which are regional and a\nnon-regional one is included for contrast.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 19:58:50 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Wang", "Tiandong", ""], ["Resnick", "Sidney I.", ""]]}, {"id": "1911.08022", "submitter": "Timothy Pollington MSc", "authors": "Timothy M. Pollington (1 and 3), Michael J. Tildesley (2), T.\n  D\\'eirdre Hollingsworth (3) and Lloyd A. C. Chapman (4) ((1) MathSys CDT,\n  University of Warwick, UK, (2) Zeeman Institute (SBIDER), School of Life\n  Sciences and Mathematics Institute, University of Warwick, UK, (3) Big Data\n  Institute, Li Ka Shing Centre for Health Information and Discovery,\n  University of Oxford, UK, (4) London School of Hygiene & Tropical Medicine,\n  UK)", "title": "Developments in statistical inference when assessing spatiotemporal\n  disease clustering with the tau statistic", "comments": "Corresponding author is Timothy M. Pollington. Equal contributions by\n  T. D\\'eirdre Hollingsworth and Lloyd A. C. Chapman. Accepted by Spatial\n  Statistics (7 Mch 2020). 43 pp, 4180 words, 11 figs, 1 graphical abstract.\n  Changes: This is our post-print after refereeing prior to proof; Title;\n  Clarity in Methods esp. spatial bootstrapping; Non-essentials moved to\n  appendices; Public GitHub repo; Figs", "journal-ref": null, "doi": "10.1016/j.spasta.2020.100438", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The tau statistic $\\tau$ uses geolocation and, usually, symptom onset time to\nassess global spatiotemporal clustering from epidemiological data. We test\ndifferent factors that could affect graphical hypothesis tests of clustering or\nbias clustering range estimates based on the statistic, by comparison with a\nbaseline analysis of an open access measles dataset.\n  From re-analysing this data we find that the spatial bootstrap sampling\nmethod used to construct the confidence interval for the tau estimate and\nconfidence interval (CI) type can bias clustering range estimates. We suggest\nthat the bias-corrected and accelerated (BCa) CI is essential for asymmetric\nsample bootstrap distributions of tau estimates.\n  We also find evidence against no spatiotemporal clustering, $p$-value $\\in$\n[0,0.014] (global envelope test). We develop a tau-specific modification of the\nLoh & Stein spatial bootstrap sampling method, which gives more precise\nbootstrapped tau estimates and a 20% higher estimated clustering endpoint than\npreviously published (36.0m; 95% BCa CI (14.9, 46.6), vs 30m) and an equivalent\nincrease in the clustering area of elevated disease odds by 44%. What appears a\nmodest radial bias in the range estimate is more than doubled on the areal\nscale, which public health resources are proportional to. This difference could\nhave important consequences for control.\n  Correct practice of hypothesis testing of no clustering and clustering range\nestimation of the tau statistic are illustrated in the Graphical abstract. We\nadvocate proper implementation of this useful statistic, ultimately to reduce\ninaccuracies in control policy decisions made during disease clustering\nanalysis.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 00:54:13 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 09:17:38 GMT"}, {"version": "v3", "created": "Thu, 28 Nov 2019 13:50:17 GMT"}, {"version": "v4", "created": "Mon, 9 Mar 2020 16:40:34 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Pollington", "Timothy M.", "", "1 and 3"], ["Tildesley", "Michael J.", ""], ["Hollingsworth", "T. D\u00e9irdre", ""], ["Chapman", "Lloyd A. C.", ""]]}, {"id": "1911.08103", "submitter": "Chenhe Zhang", "authors": "Yutong Nie and Chenhe Zhang", "title": "A Normal Approximation Method for Statistics in Knockouts", "comments": "18 pages, 12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The authors give an approximation method for Bayesian inference in arena\nmodel, which is focused on paired comparisons with eliminations and\nbifurcations. The approximation method simplifies the inference by reducing\nparameters and introducing normal distribution functions into the computation\nof posterior distribution, which is largely based on an important property of\nnormal random variables. Maximum a posteriori probability (MAP) and Bayesian\nprediction are then used to mine the information from the past pairwise\ncomparison data, such as an individual's strength or volatility and his\npossible future results. We conduct a simulation to show the accuracy and\nstability of the approximation method and demonstrate the algorithm on\nnonlinear parameter inference as well as prediction problem arising in the FIFA\nWorld Cup.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 05:35:36 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Nie", "Yutong", ""], ["Zhang", "Chenhe", ""]]}, {"id": "1911.08106", "submitter": "Mauricio Tec", "authors": "Mauricio Tec, Natalia Zuniga-Garcia, Randy B. Machemehl, James G.\n  Scott", "title": "How Likely are Ride-share Drivers to Earn a Living Wage? Large-scale\n  Spatio-temporal Density Smoothing with the Graph-fused Elastic Net", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ride-sourcing or transportation network companies (TNCs) provide on-demand\ntransportation service for compensation, connecting drivers of personal\nvehicles with passengers through smartphone applications. In this study, we\nconsider the problem of estimating a spatiotemporally varying probability\ndistribution for the productivity of a TNC driver, using data on more than 1.2\nmillion TNC trips in Austin, Texas. We propose a graph-based smoothing approach\nthat allows for distinct spatial and temporal dynamics, including different\ndegrees of smoothness, spatio-temporal interactions, and interpolation in\nregions with little or no data. For such a goal, we introduce the Graph-fused\nElastic Net (GFEN) and use it in combination with a dyadic tree decomposition\nfor density estimation. In addition, we present an optimization-driven approach\nfor fast point estimates scalable to massive graphs. Bayesian inference and\nuncertainty quantification with MCMC are also illustrated. The main results\ndemonstrate that the optimization strategy is an effective exploration tool for\nselecting adequate regularization schemes using Bayesian optimization of the\ncross-validation loss. Two key empirical findings made possible by our method\ninclude: 1) the probability that a TNC driver can expect to earn a living wage\nin Austin exhibits high variability in space and time, from as low as 25% to as\nhigh as 85%; and 2) some drivers suffer considerable \"tail risk\", with the\nbottom 10% of the earnings distribution falling below $10 per hour -- grossly\nbelow a living wage in Austin for a single adult -- for specific times and\nlocations. All code and data for the paper are publicly available, as a Shiny\napp for visualizing the results and a software package in Julia for\nimplementing the GFEN.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 06:08:02 GMT"}, {"version": "v2", "created": "Sat, 5 Dec 2020 23:26:47 GMT"}, {"version": "v3", "created": "Fri, 9 Jul 2021 16:50:57 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Tec", "Mauricio", ""], ["Zuniga-Garcia", "Natalia", ""], ["Machemehl", "Randy B.", ""], ["Scott", "James G.", ""]]}, {"id": "1911.08138", "submitter": "Marius \\\"Otting", "authors": "Marius \\\"Otting and Andreas Groll", "title": "A regularized hidden Markov model for analyzing the 'hot shoe' in\n  football", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although academic research on the 'hot hand' effect (in particular, in\nsports, especially in basketball) has been going on for more than 30 years, it\nstill remains a central question in different areas of research whether such an\neffect exists. In this contribution, we investigate the potential occurrence of\na 'hot shoe' effect for the performance of penalty takers in football based on\ndata from the German Bundesliga. For this purpose, we consider hidden Markov\nmodels (HMMs) to model the (latent) forms of players. To further account for\nindividual heterogeneity of the penalty taker as well as the opponent's\ngoalkeeper, player-specific abilities are incorporated in the model formulation\ntogether with a LASSO penalty. Our results suggest states which can be tied to\ndifferent forms of players, thus providing evidence for the hot shoe effect,\nand shed some light on exceptionally well-performing goalkeepers, which are of\npotential interest to managers and sports fans.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 08:02:24 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["\u00d6tting", "Marius", ""], ["Groll", "Andreas", ""]]}, {"id": "1911.08361", "submitter": "Ilkka Rautiainen", "authors": "Ilkka Rautiainen, Sami \\\"Ayr\\\"am\\\"o", "title": "Predicting overweight and obesity in later life from childhood data: A\n  review of predictive modeling approaches", "comments": "8 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Overweight and obesity are an increasing phenomenon worldwide.\nPredicting future overweight or obesity early in the childhood reliably could\nenable a successful intervention by experts. While a lot of research has been\ndone using explanatory modeling methods, capability of machine learning, and\npredictive modeling, in particular, remain mainly unexplored. In predictive\nmodeling models are validated with previously unseen examples, giving a more\naccurate estimate of their performance and generalization ability in real-life\nscenarios.\n  Objective: To find and review existing overweight or obesity research from\nthe perspective of employing childhood data and predictive modeling methods.\n  Methods: The initial phase included bibliographic searches using relevant\nsearch terms in PubMed, IEEE database and Google Scholar. The second phase\nconsisted of iteratively searching references of potential studies and recent\nresearch that cite the potential studies.\n  Results: Eight research articles and three review articles were identified as\nrelevant for this review.\n  Conclusions: Prediction models with high performance either have a relatively\nshort time period to predict or/and are based on late childhood data. Logistic\nregression is currently the most often used method in forming the prediction\nmodels. In addition to child's own weight and height information, maternal\nweight status or body mass index was often used as predictors in the models.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 15:44:09 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Rautiainen", "Ilkka", ""], ["\u00c4yr\u00e4m\u00f6", "Sami", ""]]}, {"id": "1911.08381", "submitter": "Andrea Cappozzo", "authors": "Andrea Cappozzo, Francesca Greselin, Thomas Brendan Murphy", "title": "Anomaly and Novelty detection for robust semi-supervised learning", "comments": null, "journal-ref": null, "doi": "10.1007/s11222-020-09959-1", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three important issues are often encountered in Supervised and\nSemi-Supervised Classification: class-memberships are unreliable for some\ntraining units (label noise), a proportion of observations might depart from\nthe main structure of the data (outliers) and new groups in the test set may\nhave not been encountered earlier in the learning phase (unobserved classes).\nThe present work introduces a robust and adaptive Discriminant Analysis rule,\ncapable of handling situations in which one or more of the afore-mentioned\nproblems occur. Two EM-based classifiers are proposed: the first one that\njointly exploits the training and test sets (transductive approach), and the\nsecond one that expands the parameter estimate using the test set, to complete\nthe group structure learned from the training set (inductive approach).\nExperiments on synthetic and real data, artificially adulterated, are provided\nto underline the benefits of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 16:28:58 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 09:51:34 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Cappozzo", "Andrea", ""], ["Greselin", "Francesca", ""], ["Murphy", "Thomas Brendan", ""]]}, {"id": "1911.08412", "submitter": "Indranil SenGupta", "authors": "Michael Roberts and Indranil SenGupta", "title": "Infinitesimal generators for two-dimensional L\\'evy process-driven\n  hypothesis testing", "comments": null, "journal-ref": null, "doi": "10.1007/s10436-019-00355-y", "report-no": null, "categories": "math.ST q-fin.MF stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the testing of four hypotheses on two streams of\nobservations that are driven by L\\'evy processes. This is applicable for\nsequential decision making on the state of two-sensor systems. In one case,\neach sensor receives or does not receive a signal obstructed by noise. In\nanother, each sensor receives data-driven by L\\'evy processes with large or\nsmall jumps. In either case, these give rise to four possibilities.\nInfinitesimal generators are presented and analyzed. Bounds for infinitesimal\ngenerators in terms of \\emph{super-solutions} and \\emph{sub-solutions} are\ncomputed. An application of this procedure for the stochastic model is also\npresented in relation to the financial market.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 16:26:44 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Roberts", "Michael", ""], ["SenGupta", "Indranil", ""]]}, {"id": "1911.08414", "submitter": "Hongqian Qin", "authors": "Hongqian Qin", "title": "Comparison of Deep learning models on time series forecasting : a case\n  study of Dissolved Oxygen Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has achieved impressive prediction performance in the field of\nsequence learning recently. Dissolved oxygen prediction, as a kind of\ntime-series forecasting, is suitable for this technique. Although many\nresearchers have developed hybrid models or variant models based on deep\nlearning techniques, there is no comprehensive and sound comparison among the\ndeep learning models in this field currently. Plus, most previous studies\nfocused on one-step forecasting by using a small data set. As the convenient\naccess to high-frequency data, this paper compares multi-step deep learning\nforecasting by using walk-forward validation. Specifically, we test\nConvolutional Neural Network (CNN), Temporal Convolutional Network (TCN), Long\nShort-Term Memory (LSTM), Gated Recurrent Unit (GRU), Bidirectional Recurrent\nNeural Network (BiRNN) based on the real-time data recorded automatically at a\nfixed observation point in the Yangtze River from 2012 to 2016. By comparing\nthe average accumulated statistical metrics of root mean square error (RMSE),\nmean absolute error (MAE), and coefficient of determination in each time step,\nWe find for multi-step time series forecasting, the average performance of each\ntime step does not decrease linearly. GRU outperforms other models with\nsignificant advantages.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 01:44:06 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 13:22:47 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Qin", "Hongqian", ""]]}, {"id": "1911.08438", "submitter": "Elea McDonnell Feit", "authors": "Ron Berman and Elea McDonnell Feit", "title": "Principal Stratification for Advertising Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advertising experiments often suffer from noisy responses making precise\nestimation of the average treatment effect (ATE) and evaluating ROI difficult.\nWe develop a principal stratification model that improves the precision of the\nATE by dividing the customers into three strata - those who buy regardless of\nad exposure, those who buy only if exposed to ads and those who do not buy\nregardless. The method decreases the variance of the ATE by separating out the\ntypically large share of customers who never buy and therefore have individual\ntreatment effects that are exactly zero. Applying the procedure to 5 catalog\nmailing experiments with sample sizes around 140,000 shows a reduction of\n36-57% in the variance of the estimate. When we include pre-randomization\ncovariates that predict stratum membership, we find that estimates of\ncustomers' past response to similar advertising are a good predictor of stratum\nmembership, even if such estimates are biased because past advertising was\ntargeted. Customers who have not purchased recently are also more likely to be\nin the \"never purchase\" stratum. We provide simple summary statistics that\nfirms can compute from their own experiment data to determine if the procedure\nis expected to be beneficial before applying it.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 18:02:11 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Berman", "Ron", ""], ["Feit", "Elea McDonnell", ""]]}, {"id": "1911.08504", "submitter": "Yan Li", "authors": "Yan Li, Matthew Sperrin, Glen P. Martin, Darren M Ashcroft, Tjeerd\n  Pieter van Staa", "title": "Examining the impact of data quality and completeness of electronic\n  health records on predictions of patients risks of cardiovascular disease", "comments": "2 tables 4 figures in the main manuscript, 1 table 3 figure in\n  appendix. Online published in IJMI, license CC-BY-NC-ND", "journal-ref": "https://doi.org/10.1016/j.ijmedinf.2019.104033", "doi": "10.1016/j.ijmedinf.2019.104033", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective is to assess the extent of variation of data quality and\ncompleteness of electronic health records and impact on the robustness of risk\npredictions of incident cardiovascular disease (CVD) using a risk prediction\ntool that is based on routinely collected data (QRISK3). The study design is a\nlongitudinal cohort study with a setting of 392 general practices (including\n3.6 million patients) linked to hospital admission data. Variation in data\nquality was assessed using Saez stability metrics quantifying outlyingness of\neach practice. Statistical frailty models evaluated whether accuracy of QRISK3\npredictions on individual predictions and effects of overall risk factors\n(linear predictor) varied between practices. There was substantial\nheterogeneity between practices in CVD incidence unaccounted for by QRISK3. In\nthe lowest quintile of statistical frailty, a QRISK3 predicted risk of 10% for\nfemale was in a range between 7.1% and 9.0% when incorporating practice\nvariability into the statistical frailty models; for the highest quintile, this\nwas 10.9%-16.4%. Data quality (using Saez metrics) and completeness were\ncomparable across different levels of statistical frailty. For example,\nrecording of missing information on ethnicity was 55.7%, 62.7%, 57.8%, 64.8%\nand 62.1% for practices from lowest to highest quintiles of statistical frailty\nrespectively. The effects of risk factors did not vary between practices with\nlittle statistical variation of beta coefficients. In conclusion, the\nconsiderable unmeasured heterogeneity in CVD incidence between practices was\nnot explained by variations in data quality or effects of risk factors. QRISK3\nrisk prediction should be supplemented with clinical judgement and evidence of\nadditional risk factors.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 19:02:44 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Li", "Yan", ""], ["Sperrin", "Matthew", ""], ["Martin", "Glen P.", ""], ["Ashcroft", "Darren M", ""], ["van Staa", "Tjeerd Pieter", ""]]}, {"id": "1911.08531", "submitter": "Adrienne Mendrik", "authors": "Adri\\\"enne M. Mendrik and Stephen R. Aylward", "title": "A Framework for Challenge Design: Insight and Deployment Challenges to\n  Address Medical Image Analysis Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP eess.IV stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we aim to refine the concept of grand challenges in medical\nimage analysis, based on statistical principles from quantitative and\nqualitative experimental research. We identify two types of challenges based on\ntheir generalization objective: 1) a deployment challenge and 2) an insight\nchallenge. A deployment challenge's generalization objective is to find\nalgorithms that solve a medical image analysis problem, which thereby requires\nthe use of a quantitative experimental design. An insight challenge's\ngeneralization objective is to gain a broad understanding of what class of\nalgorithms might be effective for a class of medical image analysis problems,\nin which case a qualitative experimental design is sufficient. Both challenge\ntypes are valuable, but problems arise when a challenge's design and objective\nare inconsistent, as is often the case when a challenge does not carefully\nconsider these concepts. Therefore, in this paper, we propose a theoretical\nframework, based on statistical principles, to guide researchers in challenge\ndesign, documentation, and assessment. Experimental results are given that\nexplore the factors that effect the practical implementation of this\ntheoretical framework.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 19:49:39 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Mendrik", "Adri\u00ebnne M.", ""], ["Aylward", "Stephen R.", ""]]}, {"id": "1911.08682", "submitter": "Haema Nilakanta", "authors": "Haema Nilakanta, Zack W. Almquist, and Galin L. Jones", "title": "Ensuring Reliable Monte Carlo Estimates of Network Properties", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The literature in social network analysis has largely focused on methods and\nmodels which require complete network data; however there exist many networks\nwhich can only be studied via sampling methods due to the scale or complexity\nof the network, access limitations, or the population of interest is hard to\nreach. In such cases, the application of random walk-based Markov chain Monte\nCarlo (MCMC) methods to estimate multiple network features is common. However,\nthe reliability of these estimates has been largely ignored. We consider and\nfurther develop multivariate MCMC output analysis methods in the context of\nnetwork sampling to directly address the reliability of the multivariate\nestimation. This approach yields principled, computationally efficient, and\nbroadly applicable methods for assessing the Monte Carlo estimation procedure.\nIn particular, with respect to two random-walk algorithms, a simple random walk\nand a Metropolis-Hastings random walk, we construct and compare network\nparameter estimates, effective sample sizes, coverage probabilities, and\nstopping rules, all of which speaks to the estimation reliability.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 03:29:05 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2019 01:55:59 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Nilakanta", "Haema", ""], ["Almquist", "Zack W.", ""], ["Jones", "Galin L.", ""]]}, {"id": "1911.08791", "submitter": "Andrea Gabrio", "authors": "Andrea Gabrio", "title": "Bayesian Hierarchical Models for the Prediction of Volleyball Results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical modelling of sports data has become more and more popular in the\nrecent years and different types of models have been proposed to achieve a\nvariety of objectives: from identifying the key characteristics which lead a\nteam to win or lose to predicting the outcome of a game or the team rankings in\nnational leagues. Although not as popular as football or basketball, volleyball\nis a team sport with both national and international level competitions in\nalmost every country. However, there is almost no study investigating the\nprediction of volleyball game outcomes and team rankings in national leagues.\nWe propose a Bayesian hierarchical model for the prediction of the rankings of\nvolleyball national teams, which also allows to estimate the results of each\nmatch in the league. We consider two alternative model specifications of\ndifferent complexity which are validated using data from the women's volleyball\nItalian Serie A1 2017-2018 season.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 09:46:10 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Gabrio", "Andrea", ""]]}, {"id": "1911.08822", "submitter": "Shisheng Chen", "authors": "Zhongqi Yu, Shisheng Chen, Nyuk Hien Wong, Marcel Ignatius, Jiyu Deng,\n  Yueer He, Daniel Jun Chung Hii", "title": "Empirical model of campus air temperature and urban morphology\n  parameters based on field measurement and machine learning in Singapore", "comments": "1st International Chinese Conference on Energy and Built Environment", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rising air temperature caused by Urban Heat Island (UHI) effect has\nbecome a problem for Singapore, it not only affects the thermal comfort of\noutdoor microclimate environment, but also increases the cooling energy\nconsumption of buildings. As part of a multiscale and multi-physics urban\nmicroclimate model, weather stations were installed at 15 points within kent\nridge campus of National University of Singapore (NUS) and continuously\nrecorded the microclimate data from February 2019 to May 2019. A Geographical\nInformation System (GIS) map and 3D model were constructed for extracting urban\nmorphology parameters such as BDG, PAVE, WALL and HBDG. Through a site survey,\nSVF and GnPR were calculated. By using multi-criteria linear regression and\nmachine learning, this research investigated five regression models for\nprediction of outdoor air temperature including linear regression (LR),\nk-nearest neighbours (KNN), support vector regression (SVR), decision tree (DT)\nand random forests (RF). The analysis of variables by best subsets regression\nshowed greenery played crucial role in the mitigation of both daytime and\nnight-time UHI. Pedestrian level wind flow was helpful in heat release in the\ndaytime. High-rise buildings provided self-shadowing to reduce ambient air\ntemperature but higher SVF was harmful to heat release in the night-time. For\nregression models, RF had the best predictive performance. Average RMSE of RF\nwas reduced by 4% to 29% compared to linear regression. The learning curve\nindicated that the predictive power of LR could not be improved by additional\ndata provision. In contrast, the downward trend in bias and variance suggested\nthat RF can benefit from the training of big data. During the deployment of\nlearning algorithms, RF continued to outperform other learning algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 11:07:58 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Yu", "Zhongqi", ""], ["Chen", "Shisheng", ""], ["Wong", "Nyuk Hien", ""], ["Ignatius", "Marcel", ""], ["Deng", "Jiyu", ""], ["He", "Yueer", ""], ["Hii", "Daniel Jun Chung", ""]]}, {"id": "1911.08979", "submitter": "Oladimeji Mudele", "authors": "Oladimeji Mudele, Fabio M. Bayer, Lucas Zanandrez, Alvaro E. Eiras,\n  Paolo Gamba", "title": "Modeling the Temporal Population Distribution of Ae. aegypti Mosquito\n  using Big Earth Observation Data", "comments": "Submitted to IEEE Access Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over 50% of the world population is at risk of mosquito-borne diseases.\nFemale Ae. aegypti mosquito species transmit Zika, Dengue, and Chikungunya. The\nspread of these diseases correlate positively with the vector population, and\nthis population depends on biotic and abiotic environmental factors including\ntemperature, vegetation condition, humidity and precipitation. To combat virus\noutbreaks, information about vector population is required. To this aim, Earth\nobservation (EO) data provide fast, efficient and economically viable means to\nestimate environmental features of interest. In this work, we present a\ntemporal distribution model for adult female Ae. aegypti mosquitoes based on\nthe joint use of the Normalized Difference Vegetation Index, the Normalized\nDifference Water Index, the Land Surface Temperature (both at day and night\ntime), along with the precipitation information, extracted from EO data. The\nmodel was applied separately to data obtained during three different vector\ncontrol and field data collection condition regimes, and used to explain the\ndifferences in environmental variable contributions across these regimes. To\nthis aim, a random forest (RF) regression technique and its nonlinear features\nimportance ranking based on mean decrease impurity (MDI) were implemented. To\nprove the robustness of the proposed model, we compared it to other machine\nlearning techniques and statistical models. Our results show that machine\nlearning techniques perform better than linear statistical models for the task\nat hand, and RF performs best. By ranking the importance of all features based\non MDI in RF and selecting the subset comprising the most informative ones, a\nmore parsimonious but equally effective and explainable model can be\nobtained...\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 15:46:10 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 22:13:42 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Mudele", "Oladimeji", ""], ["Bayer", "Fabio M.", ""], ["Zanandrez", "Lucas", ""], ["Eiras", "Alvaro E.", ""], ["Gamba", "Paolo", ""]]}, {"id": "1911.09001", "submitter": "Vikas Ramachandra", "authors": "Vikas Ramachandra", "title": "Weather event severity prediction using buoy data and machine learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we predict severity of extreme weather events (tropical\nstorms, hurricanes, etc.) using buoy data time series variables such as wind\nspeed and air temperature. The prediction/forecasting method is based on\nvarious forecasting and machine learning models. The following steps are used.\nData sources for the buoys and weather events are identified, aggregated and\nmerged. For missing data imputation, we use Kalman filters as well as splines\nfor multivariate time series. Then, statistical tests are run to ascertain\nincreasing trends in weather event severity. Next, we use machine learning to\npredict/forecast event severity using buoy variables, and report good\naccuracies for the models built.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 00:41:15 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Ramachandra", "Vikas", ""]]}, {"id": "1911.09171", "submitter": "Siyu Heng", "authors": "Siyu Heng, Bo Zhang, Xu Han, Scott A. Lorch, Dylan S. Small", "title": "Instrumental Variables: To Strengthen or Not to Strengthen?", "comments": "86 pages, 4 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instrumental variables (IVs) are extensively used to estimate treatment\neffects when the treatment and outcome are confounded by unmeasured\nconfounders; however, weak IVs are often encountered in empirical studies and\nmay cause problems. Many studies have considered building a stronger IV from\nthe original, possibly weak, IV in the design stage of a matched study at the\ncost of not using some of the samples in the analysis. It is widely accepted\nthat strengthening an IV tends to render nonparametric tests more powerful and\nwill increase the power of sensitivity analyses in large samples. In this\narticle, we re-evaluate this conventional wisdom to bring new insights into\nthis topic. We consider matched observational studies from three perspectives.\nFirst, we evaluate the trade-off between IV strength and sample size on\nnonparametric tests assuming the IV is valid and exhibit conditions under which\nstrengthening an IV increases power and conversely conditions under which it\ndecreases power. Second, we derive a necessary condition for a valid\nsensitivity analysis model with continuous doses. We show that the $\\Gamma$\nsensitivity analysis model, which has been previously used to come to the\nconclusion that strengthening an IV increases the power of sensitivity analyses\nin large samples, does not apply to the continuous IV setting and thus this\npreviously reached conclusion may be invalid. Third, we quantify the bias of\nthe Wald estimator with a possibly invalid IV under an oracle and leverage it\nto develop a valid sensitivity analysis framework; under this framework, we\nshow that strengthening an IV may amplify or mitigate the bias of the\nestimator, and may or may not increase the power of sensitivity analyses. We\nalso discuss how to better adjust for the observed covariates when building an\nIV in matched studies.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 21:07:16 GMT"}, {"version": "v2", "created": "Sat, 8 Aug 2020 02:44:46 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Heng", "Siyu", ""], ["Zhang", "Bo", ""], ["Han", "Xu", ""], ["Lorch", "Scott A.", ""], ["Small", "Dylan S.", ""]]}, {"id": "1911.09182", "submitter": "Andrey Sarantsev Mr", "authors": "Olga Rumyantseva, Andrey Sarantsev, Nikolay Strigul", "title": "Autoregressive Modeling of Forest Dynamics", "comments": "17 pages, 14 figures. Keywords: Forest biomass dynamics; random walk\n  model; AR(1) process; Bayesian analysis; patch dynamics. Published in MDPI\n  Forests, open access", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we employ autoregressive models developed in financial\nengineering for modeling of forest dynamics. Autoregressive models have some\ntheoretical advantage over currently employed forest modeling approaches such\nas Markov chains and individual-based models, as autoregressive models are both\nanalytically tractable and operate with continuous state space. We perform time\nseries statistical analysis of forest biomass and basal area recorded in Quebec\nprovincial forest inventories in 1970-2007. The geometric random walk model\nadequately describes the yearly average dynamics. For individual patches, we\nfit an AR(1) process capable to model negative feedback (mean-reversion).\nOverall, the best fit also turns out to be geometric random walk, however, the\nnormality tests for residuals fail. In contrast, yearly means are adequately\ndescribed by normal fluctuations, with annual growth, on average, 2.3%, but\nwith standard deviation of order 40%. We use Bayesian analysis to account for\nuneven number of observations per year. This work demonstrates that\nautoregressive models represent a valuable tool for modeling of forest\ndynamics. In particular, they quantify stochastic effects of environmental\ndisturbances and develop predictive empirical models on short and intermediate\ntemporal scales.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 21:32:40 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Rumyantseva", "Olga", ""], ["Sarantsev", "Andrey", ""], ["Strigul", "Nikolay", ""]]}, {"id": "1911.09254", "submitter": "Yujie Wu", "authors": "Yujie Wu, Mitchell H. Gail, Stephanie A. Smith-Warner, Regina G.\n  Ziegler, Molin Wang", "title": "Spline Analysis of Biomarker Data Pooled From Multiple Matched/Nested\n  Case-Control Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pooling biomarker data across multiple studies enables researchers to get\nmore precise estimates of the association between biomarker exposure\nmeasurements and disease risks due to increased sample sizes. However,\nbiomarker measurements vary significantly across different assays and\nlaboratories, and therefore calibration of the local laboratory measurements to\na reference laboratory is necessary before pooling data. We propose two methods\nthat can estimate a nonlinear relationship between biomarker exposure\nmeasurements and disease risks using spline functions with a nested\ncase-control study design: full calibration and internalized calibration. The\nfull calibration method calibrates all observations using a study-specific\ncalibration model while the internalized calibration method only calibrates\nobservations that do not have reference laboratory measurements available. We\ncompare the two methods with a naive method whereby data are pooled without\ncalibration. We find that: (1) Internalized and full calibration methods have\nsubstantially better performance than the naive method in terms of average\nrelative bias and coverage rate. (2) Full calibration is more robust than\ninternalized calibration when the size of calibration subsets varies. We apply\nour methods to a pooling project with nested case-control study design to\nestimate the association of circulating Vitamin D levels with the risk of\ncolorectal cancer.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 02:50:10 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Wu", "Yujie", ""], ["Gail", "Mitchell H.", ""], ["Smith-Warner", "Stephanie A.", ""], ["Ziegler", "Regina G.", ""], ["Wang", "Molin", ""]]}, {"id": "1911.09274", "submitter": "Pulong Ma", "authors": "Pulong Ma and Anirban Mondal and Bledar Konomi and Jonathan Hobbs and\n  Joon Song and Emily Kang", "title": "Computer Model Emulation with High-Dimensional Functional Output in\n  Large-Scale Observing System Uncertainty Experiments", "comments": "45 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Observing system uncertainty experiments (OSUEs) have been recently proposed\nas a cost-effective way to perform probabilistic assessment of retrievals for\nNASA's Orbiting Carbon Observatory-2 (OCO-2) mission. One important component\nin the OCO-2 retrieval algorithm is a full-physics forward model that describes\nthe mathematical relationship between atmospheric variables such as carbon\ndioxide and radiances measured by the remote sensing instrument. This forward\nmodel is complicated and computationally expensive but large-scale OSUEs\nrequire evaluation of this model numerous times, which makes it infeasible for\ncomprehensive experiments. To tackle this issue, we develop a statistical\nemulator to facilitate large-scale OSUEs in the OCO-2 mission with independent\nemulation. Within each distinct spectral band, the emulator represents\nradiances output at irregular wavelengths via a linear combination of basis\nfunctions and random coefficients. These random coefficients are then modeled\nwith nearest-neighbor Gaussian processes with built-in input dimension\nreduction via active subspace. The proposed emulator reduces dimensionality in\nboth input space and output space, so that fast computation is achieved within\na fully Bayesian inference framework. Validation experiments demonstrate that\nthis emulator outperforms other competing statistical methods and a reduced\norder model that approximates the full-physics forward model.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 03:58:27 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 17:33:13 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Ma", "Pulong", ""], ["Mondal", "Anirban", ""], ["Konomi", "Bledar", ""], ["Hobbs", "Jonathan", ""], ["Song", "Joon", ""], ["Kang", "Emily", ""]]}, {"id": "1911.09408", "submitter": "Yunxiao Chen", "authors": "Yunxiao Chen, Yan Lu, and Irini Moustaki", "title": "Detection of Two-Way Outliers in Multivariate Data and Application to\n  Cheating Detection in Educational Tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper proposes a new latent variable model for the simultaneous (two-way)\ndetection of outlying individuals and items for item-response-type data. The\nproposed model is a synergy between a factor model for binary responses and\ncontinuous response times that captures normal item response behaviour and a\nlatent class model that captures the outlying individuals and items. A\nstatistical decision framework is developed under the proposed model that\nprovides compound decision rules for controlling local false\ndiscovery/nondiscovery rates of outlier detection. Statistical inference is\ncarried out under a Bayesian framework, for which a Markov chain Monte Carlo\nalgorithm is developed. The proposed method is applied to the detection of\ncheating in educational tests due to item leakage using a case study of a\ncomputer-based nonadaptive licensure assessment. The performance of the\nproposed method is evaluated by simulation studies.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 11:08:16 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 21:54:15 GMT"}, {"version": "v3", "created": "Tue, 29 Jun 2021 10:40:38 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Chen", "Yunxiao", ""], ["Lu", "Yan", ""], ["Moustaki", "Irini", ""]]}, {"id": "1911.09471", "submitter": "Sahan Bulathwela", "authors": "Sahan Bulathwela, Maria Perez-Ortiz, Emine Yilmaz and John\n  Shawe-Taylor", "title": "TrueLearn: A Family of Bayesian Algorithms to Match Lifelong Learners to\n  Open Educational Resources", "comments": "In Proceedings of AAAI Conference on Artificial Intelligence 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent advances in computer-assisted learning systems and the\navailability of open educational resources today promise a pathway to providing\ncost-efficient, high-quality education to large masses of learners. One of the\nmost ambitious use cases of computer-assisted learning is to build a lifelong\nlearning recommendation system. Unlike short-term courses, lifelong learning\npresents unique challenges, requiring sophisticated recommendation models that\naccount for a wide range of factors such as background knowledge of learners or\nnovelty of the material while effectively maintaining knowledge states of\nmasses of learners for significantly longer periods of time (ideally, a\nlifetime). This work presents the foundations towards building a dynamic,\nscalable and transparent recommendation system for education, modelling\nlearner's knowledge from implicit data in the form of engagement with open\neducational resources. We i) use a text ontology based on Wikipedia to\nautomatically extract knowledge components of educational resources and, ii)\npropose a set of online Bayesian strategies inspired by the well-known areas of\nitem response theory and knowledge tracing. Our proposal, TrueLearn, focuses on\nrecommendations for which the learner has enough background knowledge (so they\nare able to understand and learn from the material), and the material has\nenough novelty that would help the learner improve their knowledge about the\nsubject and keep them engaged. We further construct a large open educational\nvideo lectures dataset and test the performance of the proposed algorithms,\nwhich show clear promise towards building an effective educational\nrecommendation system.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 13:56:40 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Bulathwela", "Sahan", ""], ["Perez-Ortiz", "Maria", ""], ["Yilmaz", "Emine", ""], ["Shawe-Taylor", "John", ""]]}, {"id": "1911.09511", "submitter": "Matias Cattaneo", "authors": "Matias D. Cattaneo, Nicolas Idrobo, Rocio Titiunik", "title": "A Practical Introduction to Regression Discontinuity Designs:\n  Foundations", "comments": null, "journal-ref": null, "doi": "10.1017/9781108684606", "report-no": null, "categories": "stat.ME econ.EM stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this Element and its accompanying Element, Matias D. Cattaneo, Nicolas\nIdrobo, and Rocio Titiunik provide an accessible and practical guide for the\nanalysis and interpretation of Regression Discontinuity (RD) designs that\nencourages the use of a common set of practices and facilitates the\naccumulation of RD-based empirical evidence. In this Element, the authors\ndiscuss the foundations of the canonical Sharp RD design, which has the\nfollowing features: (i) the score is continuously distributed and has only one\ndimension, (ii) there is only one cutoff, and (iii) compliance with the\ntreatment assignment is perfect. In the accompanying Element, the authors\ndiscuss practical and conceptual extensions to the basic RD setup.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 14:58:18 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Cattaneo", "Matias D.", ""], ["Idrobo", "Nicolas", ""], ["Titiunik", "Rocio", ""]]}, {"id": "1911.09656", "submitter": "Mike West", "authors": "Mike West", "title": "Bayesian forecasting of multivariate time series: Scalability, structure\n  uncertainty and decisions", "comments": "2018 Akaike Memorial Lecture Award paper", "journal-ref": null, "doi": "10.1007/s10463-019-00741-3", "report-no": null, "categories": "stat.ME eess.SP math.DS stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I overview recent research advances in Bayesian state-space modeling of\nmultivariate time series. A main focus is on the decouple/recouple concept that\nenables application of state-space models to increasingly large-scale data,\napplying to continuous or discrete time series outcomes. The scope includes\nlarge-scale dynamic graphical models for forecasting and multivariate\nvolatility analysis in areas such as economics and finance, multi-scale\napproaches for forecasting discrete/count time series in areas such as\ncommercial sales and demand forecasting, and dynamic network flow models for\nareas including internet traffic monitoring. In applications, explicit\nforecasting, monitoring and decision goals are paramount and should factor into\nmodel assessment and comparison, a perspective that is highlighted.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 18:35:12 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 15:57:39 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["West", "Mike", ""]]}, {"id": "1911.09765", "submitter": "Lizet Sanchez", "authors": "Lizet Sanchez, Patricia Lorenzo-Luaces, Claudia Fonte, Agustin Lage", "title": "Mixture survival models methodology: an application to cancer\n  immunotherapy assessment in clinical trials", "comments": "24 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Progress in immunotherapy revolutionized the treatment landscape for advanced\nlung cancer, raising survival expectations beyond those that were historically\nanticipated with this disease. In the present study, we describe the methods\nfor the adjustment of mixture parametric models of two populations for survival\nanalysis in the presence of long survivors. A methodology is proposed in\nseveral five steps: first, it is proposed to use the multimodality test to\ndecide the number of subpopulations to be considered in the model, second to\nadjust simple parametric survival models and mixture distribution models, to\nestimate the parameters and to select the best model fitted the data, finally,\nto test the hypotheses to compare the effectiveness of immunotherapies in the\ncontext of randomized clinical trials. The methodology is illustrated with data\nfrom a clinical trial that evaluates the effectiveness of the therapeutic\nvaccine CIMAvaxEGF vs the best supportive care for the treatment of advanced\nlung cancer. The mixture survival model allows estimating the presence of a\nsubpopulation of long survivors that is 44% for vaccinated patients. The\ndifferences between the treated and control group were significant in both\nsubpopulations (population of short-term survival: p = 0.001, the population of\nlong-term survival: p = 0.0002). For cancer therapies, where a proportion of\npatients achieves long-term control of the disease, the heterogeneity of the\npopulation must be taken into account. Mixture parametric models may be more\nsuitable to detect the effectiveness of immunotherapies compared to standard\nmodels.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 21:49:16 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Sanchez", "Lizet", ""], ["Lorenzo-Luaces", "Patricia", ""], ["Fonte", "Claudia", ""], ["Lage", "Agustin", ""]]}, {"id": "1911.09816", "submitter": "Szu-Chi Chung", "authors": "Szu-Chi Chung, Shao-Hsuan Wang, Po-Yao Niu, Su-Yun Huang, Wei-Hau\n  Chang and I-Ping Tu", "title": "Two-stage dimension reduction for noisy high-dimensional images and\n  application to Cryogenic Electron Microscopy", "comments": "29 pages, 8 figures and 3 tables", "journal-ref": "Annals of Mathematical Sciences and Applications. Volume 5, Number\n  2, 283-316, 2020", "doi": "10.4310/AMSA.2020.v5.n2.a4", "report-no": null, "categories": "eess.IV cs.CV stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Principal component analysis (PCA) is arguably the most widely used\ndimension-reduction method for vector-type data. When applied to a sample of\nimages, PCA requires vectorization of the image data, which in turn entails\nsolving an eigenvalue problem for the sample covariance matrix. We propose\nherein a two-stage dimension reduction (2SDR) method for image reconstruction\nfrom high-dimensional noisy image data. The first stage treats the image as a\nmatrix, which is a tensor of order 2, and uses multilinear principal component\nanalysis (MPCA) for matrix rank reduction and image denoising. The second stage\nvectorizes the reduced-rank matrix and achieves further dimension and noise\nreduction. Simulation studies demonstrate excellent performance of 2SDR, for\nwhich we also develop an asymptotic theory that establishes consistency of its\nrank selection. Applications to cryo-EM (cryogenic electronic microscopy),\nwhich has revolutionized structural biology, organic and medical chemistry,\ncellular and molecular physiology in the past decade, are also provided and\nillustrated with benchmark cryo-EM datasets. Connections to other\ncontemporaneous developments in image reconstruction and high-dimensional\nstatistical inference are also discussed.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 02:30:37 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 04:02:44 GMT"}, {"version": "v3", "created": "Wed, 10 Jun 2020 08:09:29 GMT"}, {"version": "v4", "created": "Sat, 27 Feb 2021 11:27:44 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Chung", "Szu-Chi", ""], ["Wang", "Shao-Hsuan", ""], ["Niu", "Po-Yao", ""], ["Huang", "Su-Yun", ""], ["Chang", "Wei-Hau", ""], ["Tu", "I-Ping", ""]]}, {"id": "1911.09851", "submitter": "Muhammad Ejaz", "authors": "Muhammad Ejaz, Chaitanya Joshi and Stephen Joe", "title": "Adversarial Risk Analysis for First-Price Sealed-Bid Auctions", "comments": "32 pages, 4 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial Risk Analysis (ARA) is an upcoming methodology that is considered\nto have advantages over the traditional decision theoretic and game theoretic\napproaches. ARA solutions for first-price sealed-bid (FPSB) auctions have been\nfound but only under strong assumptions which make the model somewhat\nunrealistic. In this paper, we use ARA methodology to model FPSB auctions using\nmore realistic assumptions. We define a new utility function that considers\nbidders' wealth, we assume a reserve price and find solutions not only for\nrisk-neutral but also for risk-averse as well as risk-seeking bidders. We model\nthe problem using ARA for non-strategic play and level-k thinking solution\nconcepts.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 04:33:27 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 22:50:59 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Ejaz", "Muhammad", ""], ["Joshi", "Chaitanya", ""], ["Joe", "Stephen", ""]]}, {"id": "1911.09856", "submitter": "Elliot Mitchell", "authors": "Elliot G Mitchell, Esteban G Tabak, Matthew E Levine, Lena Mamykina,\n  David J Albers", "title": "Enabling Personalized Decision Support with Patient-Generated Data and\n  Attributable Components", "comments": null, "journal-ref": "Journal of Biomedical Informatics. 113 (2021) 103639", "doi": "10.1016/j.jbi.2020.103639", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision-making related to health is complex. Machine learning (ML) and\npatient generated data can identify patterns and insights at the individual\nlevel, where human cognition falls short, but not all ML-generated information\nis of equal utility for making health-related decisions. We develop and apply\nattributable components analysis (ACA), a method inspired by optimal transport\ntheory, to type 2 diabetes self-monitoring data to identify patterns of\nassociation between nutrition and blood glucose control. In comparison with\nlinear regression, we found that ACA offers a number of characteristics that\nmake it promising for use in decision support applications. For example, ACA\nwas able to identify non-linear relationships, was more robust to outliers, and\noffered broader and more expressive uncertainty estimates. In addition, our\nresults highlight a tradeoff between model accuracy and interpretability, and\nwe discuss implications for ML-driven decision support systems.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 04:50:46 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 23:14:15 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Mitchell", "Elliot G", ""], ["Tabak", "Esteban G", ""], ["Levine", "Matthew E", ""], ["Mamykina", "Lena", ""], ["Albers", "David J", ""]]}, {"id": "1911.09879", "submitter": "Saurabh Khanna", "authors": "Saurabh Khanna and Vincent Y. F. Tan", "title": "Economy Statistical Recurrent Units For Inferring Nonlinear Granger\n  Causality", "comments": "A new RNN architecture for inferring nonlinear Granger causality from\n  time series data with emphasis on learning time-localized predictive features", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Granger causality is a widely-used criterion for analyzing interactions in\nlarge-scale networks. As most physical interactions are inherently nonlinear,\nwe consider the problem of inferring the existence of pairwise Granger\ncausality between nonlinearly interacting stochastic processes from their time\nseries measurements. Our proposed approach relies on modeling the embedded\nnonlinearities in the measurements using a component-wise time series\nprediction model based on Statistical Recurrent Units (SRUs). We make a case\nthat the network topology of Granger causal relations is directly inferrable\nfrom a structured sparse estimate of the internal parameters of the SRU\nnetworks trained to predict the processes$'$ time series measurements. We\npropose a variant of SRU, called economy-SRU, which, by design has considerably\nfewer trainable parameters, and therefore less prone to overfitting. The\neconomy-SRU computes a low-dimensional sketch of its high-dimensional hidden\nstate in the form of random projections to generate the feedback for its\nrecurrent processing. Additionally, the internal weight parameters of the\neconomy-SRU are strategically regularized in a group-wise manner to facilitate\nthe proposed network in extracting meaningful predictive features that are\nhighly time-localized to mimic real-world causal events. Extensive experiments\nare carried out to demonstrate that the proposed economy-SRU based time series\nprediction model outperforms the MLP, LSTM and attention-gated CNN-based time\nseries models considered previously for inferring Granger causality.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 06:40:07 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 04:48:30 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Khanna", "Saurabh", ""], ["Tan", "Vincent Y. F.", ""]]}, {"id": "1911.10035", "submitter": "Philip Stark", "authors": "Philip B. Stark", "title": "Sets of Half-Average Nulls Generate Risk-Limiting Audits: SHANGRLA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Risk-limiting audits (RLAs) for many social choice functions can be reduced\nto testing sets of null hypotheses of the form \"the average of this list is not\ngreater than 1/2\" for a collection of finite lists of nonnegative numbers. Such\nsocial choice functions include majority, super-majority, plurality,\nmulti-winner plurality, Instant Runoff Voting (IRV), Borda count, approval\nvoting, and STAR-Voting, among others. The audit stops without a full hand\ncount iff all the null hypotheses are rejected. The nulls can be tested in many\nways. Ballot-polling is particularly simple; two new ballot-polling\nrisk-measuring functions for sampling without replacement are given.\nBallot-level comparison audits transform each null into an equivalent assertion\nthat the mean of re-scaled tabulation errors is not greater than 1/2. In turn,\nthat null can then be tested using the same statistical methods used for ballot\npolling---but applied to different finite lists of nonnegative numbers.\nSHANGRLA comparison audits are more efficient than previous comparison audits\nfor two reasons: (i) for most social choice functions, the conditions tested\nare both necessary and sufficient for the reported outcome to be correct, while\nprevious methods tested conditions that were sufficient but not necessary, and\n(ii) the tests avoid a conservative approximation. The SHANGRLA abstraction\nsimplifies stratified audits, including audits that combine ballot polling with\nballot-level comparisons, producing sharper audits than the \"SUITE\" approach.\nSHANGRLA works with the \"phantoms to evil zombies\" strategy to treat missing\nballot cards and missing or redacted cast vote records. That also facilitates\nsampling from \"ballot-style manifests,\" which can dramatically improve\nefficiency when the audited contests do not appear on every ballot card.\nOpen-source software implementing SHANGRLA ballot-level comparison audits is\navailable.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 13:32:46 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2019 06:38:43 GMT"}, {"version": "v3", "created": "Sat, 1 Feb 2020 21:51:34 GMT"}, {"version": "v4", "created": "Tue, 24 Mar 2020 20:48:56 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Stark", "Philip B.", ""]]}, {"id": "1911.10191", "submitter": "Sen Tian", "authors": "Sen Tian, Clifford M. Hurvich, Jeffrey S. Simonoff", "title": "On the Use of Information Criteria for Subset Selection in Least Squares\n  Regression", "comments": "Code to reproduce the results in this paper, the complete set of\n  simulation results, and an R package 'BOSSreg' (also available on CRAN), are\n  publicly available at https://github.com/sentian/BOSSreg", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Least squares (LS)-based subset selection methods are popular in linear\nregression modeling. Best subset selection (BS) is known to be NP-hard and has\na computational cost that grows exponentially with the number of predictors.\nRecently, Bertsimas (2016) formulated BS as a mixed integer optimization (MIO)\nproblem and largely reduced the computation overhead by using a well-developed\noptimization solver, but the current methodology is not scalable to very large\ndatasets. In this paper, we propose a novel LS-based method, the best\northogonalized subset selection (BOSS) method, which performs BS upon an\northogonalized basis of ordered predictors and scales easily to large problem\nsizes. Another challenge in applying LS-based methods in practice is the\nselection rule to choose the optimal subset size k. Cross-validation (CV)\nrequires fitting a procedure multiple times, and results in a selected k that\nis random across repeated application to the same dataset. Compared to CV,\ninformation criteria only require fitting a procedure once, but they require\nknowledge of the effective degrees of freedom for the fitting procedure, which\nis generally not available analytically for complex methods. Since BOSS uses\northogonalized predictors, we first explore a connection for orthogonal\nnon-random predictors between BS and its Lagrangian formulation (i.e.,\nminimization of the residual sum of squares plus the product of a\nregularization parameter and k), and based on this connection propose a\nheuristic degrees of freedom (hdf) for BOSS that can be estimated via an\nanalytically-based expression. We show in both simulations and real data\nanalysis that BOSS using a proposed Kullback-Leibler based information\ncriterion AICc-hdf has the strongest performance of all of the LS-based methods\nconsidered and is competitive with regularization methods, with the\ncomputational effort of a single ordinary LS fit.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 18:54:43 GMT"}, {"version": "v2", "created": "Sun, 22 Dec 2019 05:00:42 GMT"}, {"version": "v3", "created": "Sat, 6 Mar 2021 03:46:41 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Tian", "Sen", ""], ["Hurvich", "Clifford M.", ""], ["Simonoff", "Jeffrey S.", ""]]}, {"id": "1911.10237", "submitter": "Knut Dundas Mor{\\aa}", "authors": "Sara Algeri, Jelle Aalbers, Knut Dundas Mor{\\aa}, Jan Conrad", "title": "Searching for new physics with profile likelihoods: Wilks and beyond", "comments": "Submitted to Nature Expert Recommendations", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an astro-ph.HE hep-ex hep-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle physics experiments use likelihood ratio tests extensively to\ncompare hypotheses and to construct confidence intervals. Often, the null\ndistribution of the likelihood ratio test statistic is approximated by a\n$\\chi^2$ distribution, following a theorem due to Wilks. However, many\ncircumstances relevant to modern experiments can cause this theorem to fail. In\nthis paper, we review how to identify these situations and construct valid\ninference.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 20:20:35 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Algeri", "Sara", ""], ["Aalbers", "Jelle", ""], ["Mor\u00e5", "Knut Dundas", ""], ["Conrad", "Jan", ""]]}, {"id": "1911.10339", "submitter": "Pedram Rowhani", "authors": "Adam B. Barrett, Steven Duivenvoorden, Edward E. Salakpi, James M.\n  Muthoka, John Mwangi, Seb Oliver and Pedram Rowhani", "title": "Forecasting vegetation condition for drought early warning systems in\n  pastoral communities in Kenya", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Droughts are a recurring hazard in sub-Saharan Africa, that can wreak huge\nsocioeconomic costs.Acting early based on alerts provided by early warning\nsystems (EWS) can potentially provide substantial mitigation, reducing the\nfinancial and human cost. However, existing EWS tend only to monitor current,\nrather than forecast future, environmental and socioeconomic indicators of\ndrought, and hence are not always sufficiently timely to be effective in\npractice. Here we present a novel method for forecasting satellite-based\nindicators of vegetation condition. Specifically, we focused on the 3-month\nVegetation Condition Index (VCI3M) over pastoral livelihood zones in Kenya,\nwhich is the indicator used by the Kenyan National Drought Management\nAuthority(NDMA). Using data from MODIS and Landsat, we apply linear\nautoregression and Gaussian process modeling methods and demonstrate high\nforecasting skill several weeks ahead. As a benchmark we predicted the drought\nalert marker used by NDMA (VCI3M<35). Both of our models were able to predict\nthis alert marker four weeks ahead with a hit rate of around 89% and a false\nalarm rate of around 4%, or 81% and 6% respectively six weeks ahead. The\nmethods developed here can thus identify a deteriorating vegetation condition\nwell and sufficiently in advance to help disaster risk managers act early to\nsupport vulnerable communities and limit the impact of a drought hazard.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 09:44:58 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 17:07:30 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Barrett", "Adam B.", ""], ["Duivenvoorden", "Steven", ""], ["Salakpi", "Edward E.", ""], ["Muthoka", "James M.", ""], ["Mwangi", "John", ""], ["Oliver", "Seb", ""], ["Rowhani", "Pedram", ""]]}, {"id": "1911.10540", "submitter": "Haipeng Xing", "authors": "Haipeng Xing, Yingru Wu, Yong Chen, Michael Zhang", "title": "Deciphering hierarchical organization of topologically associated\n  domains through change-point testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: The nucleus of eukaryotic cells spatially packages chromosomes\ninto a hierarchical and distinct segregation that plays critical roles in\nmaintaining transcription regulation. High-throughput methods of chromosome\nconformation capture, such as Hi-C, have revealed topologically associating\ndomains (TADs) that are defined by biased chromatin interactions within them.\n  Results: Here, we introduce a novel method, HiCKey, to decipher hierarchical\nTAD structures in Hi-C data and compare them across samples. We first derive a\ngeneralized likelihood-ratio (GLR) test for detecting change-points in an\ninteraction matrix that follows a negative binomial distribution or general\nmixture distribution. We then employ several optimal search strategies to\ndecipher hierarchical TADs with p-values calculated by the GLR test.\nLarge-scale validations of simulation data show that HiCKey has good precision\nin recalling known TADs and is robust against random collision noise of\nchromatin interactions. By applying HiCKey to Hi-C data of seven human cell\nlines, we identified multiple layers of TAD organization among them, but the\nvast majority had no more than four layers. In particular, we found that TAD\nboundaries are significantly enriched in active chromosomal regions compared to\nrepressed regions, indicating finer hierarchical architectures in active\nregions for precise gene transcription regulation.\n  Conclusions: HiCKey is optimized for processing large matrices constructed\nfrom high-resolution Hi-C experiments. The method and theoretical result of the\nGLR test provide a general framework for significance testing of similar\nexperimental chromatin interaction data that may not fully follow negative\nbinomial distributions but rather more general mixture distributions.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 14:48:26 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 11:51:36 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Xing", "Haipeng", ""], ["Wu", "Yingru", ""], ["Chen", "Yong", ""], ["Zhang", "Michael", ""]]}, {"id": "1911.11002", "submitter": "Jeffrey Doser", "authors": "Mahdi Teimouri, Jeffrey W. Doser and Andrew O. Finley", "title": "ForestFit : An R package for modeling tree diameter distributions", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling the diameter distribution of trees in forest stands is a common\nforestry task that supports key biologically and economically relevant\nmanagement decisions. The choice of model used to represent the diameter\ndistribution and how to estimate its parameters has received much attention in\nthe forestry literature; however, accessible software that facilitates\ncomprehensive comparison of the myriad modeling approaches is not available. To\nthis end, we developed an R package called ForestFit that simplifies estimation\nof common probability distributions used to model tree diameter distributions,\nincluding the two- and three-parameter Weibull distributions, Johnson's SB\ndistribution, Birnbaum-Saunders distribution, and finite mixture distributions.\nFrequentist and Bayesian techniques are provided for individual tree diameter\ndata, as well as grouped data. Additional functionality facilitates fitting\ngrowth curves to height-diameter data. The package also provides a set of\nfunctions for computing probability distributions and simulating random\nrealizations from common finite mixture models.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 15:49:57 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Teimouri", "Mahdi", ""], ["Doser", "Jeffrey W.", ""], ["Finley", "Andrew O.", ""]]}, {"id": "1911.11063", "submitter": "Kezia Irene", "authors": "Hanqing Huang, Kezia Irene, Nahyun Ryu", "title": "Voice Search and Typed Search Performance Comparison on Baidu Search\n  System", "comments": "13 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the voice search system is getting more and more developed, some\npeople still have difficulties when searching for information with the voice\nsearch system. This paper is a pilot study to compare the search performance of\npeople using voice search and typed search using Baidu search system. We\nsurveyed and interviewed 40 Chinese students who have been using the Baidu\nsearch system. Afterward, we analyzed 8 people who had a middle to advanced\nsearching ability by their behaviors, search results, and average query length.\nWe found that there are a lot of variations among the participants' time when\nsearching for different queries, and there were some interesting behaviors that\nwere displayed by a number of participants. We conclude that more participants\nare needed to make a firm conclusion on the performance comparison between the\nvoice search and typed search.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 10:40:26 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Huang", "Hanqing", ""], ["Irene", "Kezia", ""], ["Ryu", "Nahyun", ""]]}, {"id": "1911.11089", "submitter": "Irwin McNeely", "authors": "Trey McNeely, Ann B. Lee, Kimberly M. Wood, Dorit Hammerling", "title": "Unlocking GOES: A Statistical Framework for Quantifying the Evolution of\n  Convective Structure in Tropical Cyclones", "comments": "19 pages, 14 figures, Submitted to the Journal of Applied Meteorology\n  and Climatology", "journal-ref": "Journal of Applied Meteorology and Climatology 59.10 (2020):\n  1671-1689", "doi": "10.1175/JAMC-D-19-0286.1", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tropical cyclones (TCs) rank among the most costly natural disasters in the\nUnited States, and accurate forecasts of track and intensity are critical for\nemergency response. Intensity guidance has improved steadily but slowly, as\nprocesses which drive intensity change are not fully understood. Because most\nTCs develop far from land-based observing networks, geostationary satellite\nimagery is critical to monitor these storms. However, these complex data can be\nchallenging to analyze in real time, and off-the-shelf machine learning\nalgorithms have limited applicability on this front due to their ``black box''\nstructure. This study presents analytic tools that quantify convective\nstructure patterns in infrared satellite imagery for over-ocean TCs, yielding\nlower-dimensional but rich representations that support analysis and\nvisualization of how these patterns evolve during rapid intensity change. The\nproposed ORB feature suite targets the global Organization, Radial structure,\nand Bulk morphology of TCs. By combining ORB and empirical orthogonal\nfunctions, we arrive at an interpretable and rich representation of convective\nstructure patterns that serve as inputs to machine learning methods. This study\nuses the logistic lasso, a penalized generalized linear model, to relate\npredictors to rapid intensity change. Using ORB alone, binary classifiers\nidentifying the presence (versus absence) of such intensity change events can\nachieve accuracy comparable to classifiers using environmental predictors\nalone, with a combined predictor set improving classification accuracy in some\nsettings. More complex nonlinear machine learning methods did not perform\nbetter than the linear logistic lasso model for current data.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 17:45:25 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 00:10:23 GMT"}, {"version": "v3", "created": "Mon, 3 Aug 2020 15:02:34 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["McNeely", "Trey", ""], ["Lee", "Ann B.", ""], ["Wood", "Kimberly M.", ""], ["Hammerling", "Dorit", ""]]}, {"id": "1911.11221", "submitter": "Justin Williams", "authors": "Justin R. Williams, Hyung-Woo Kim, Catherine M. Crespi", "title": "Modeling Variables with a Detection Limit using a Truncated Normal\n  Distribution with Censoring", "comments": null, "journal-ref": "BMC Medical Research Methodology 20 (2020)", "doi": "10.1186/s12874-020-01032-9", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When data are collected subject to a detection limit, observations below the\ndetection limit may be considered censored. In addition, the domain of such\nobservations may be restricted; for example, values may be required to be\nnon-negative. We propose a regression method for censored observations that\nalso accounts for domain restriction. The method finds maximum likelihood\nestimates assuming an underlying truncated normal distribution. We show that\nour method, tcensReg, outperforms other methods commonly used for data with\ndetection limits such as Tobit regression and single imputation of the\ndetection limit or half detection limit with respect to bias and mean squared\nerror under a range of simulation settings. We apply our method to analyze\nvision quality data collected from ophthalmology clinical trials comparing\ndifferent types of intraocular lenses implanted during cataract surgery.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 20:36:04 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Williams", "Justin R.", ""], ["Kim", "Hyung-Woo", ""], ["Crespi", "Catherine M.", ""]]}, {"id": "1911.11342", "submitter": "Leiwen Gao", "authors": "Leiwen Gao, Sudipto Banerjee, and Abhirup Datta", "title": "Spatial Modeling for Correlated Cancers Using Bivariate Directed Graphs", "comments": "15 pages, 4 figures, ACE paper", "journal-ref": null, "doi": "10.21037/ace-19-41", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Disease maps are an important tool in cancer epidemiology used for the\nanalysis of geographical variations in disease rates and the investigation of\nenvironmental risk factors underlying spatial patterns. Cancer maps help\nepidemiologists highlight geographic areas with high and low prevalence,\nincidence, or mortality rates of cancers, and the variability of such rates\nover a spatial domain. When more than one cancer is of interest, the models\nmust also capture the inherent or endemic association between the diseases in\naddition to the spatial association. This article develops interpretable and\neasily implementable spatial autocorrelation models for two or more cancers.\nThe article builds upon recent developments in univariate disease mapping that\nhave shown the use of mathematical structures such as directed acyclic graphs\nto capture spatial association for a single cancer, estimating inherent or\nendemic association for two cancers in addition to the association over space\n(clustering) for each of the cancers. The method builds a Bayesian hierarchical\nmodel where the spatial effects are introduced as latent random effects for\neach cancer. We analyze the relationship between incidence rates of esophagus\nand lung cancer extracted from the Surveillance, Epidemiology, and End Results\n(SEER) Program. Our analysis shows statistically significant association\nbetween the county-wise incidence rates of lung and esophagus cancer across\nCalifornia. The bivariate directed acyclic graphical model performs better than\ncompeting bivariate spatial models in the existing literature.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 05:03:34 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Gao", "Leiwen", ""], ["Banerjee", "Sudipto", ""], ["Datta", "Abhirup", ""]]}, {"id": "1911.11460", "submitter": "Maxime Lenormand", "authors": "Olivier Billaud, Maxence Soubeyrand, Sandra Luque and Maxime Lenormand", "title": "Comprehensive decision-strategy space exploration for efficient\n  territorial planning strategies", "comments": "12 pages, 7 figures + Appendix", "journal-ref": "Computers, Environment and Urban Systems 83, 101516 (2020)", "doi": "10.1016/j.compenvurbsys.2020.101516", "report-no": null, "categories": "stat.AP cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GIS-based Multi-Criteria Decision Analysis is a well-known decision support\ntool that can be used in a wide variety of contexts. It is particularly useful\nfor territorial planning in situations where several actors with different, and\nsometimes contradictory, point of views have to take a decision regarding land\nuse development. While the impact of the weights used to represent the relative\nimportance of criteria has been widely studied in the recent literature, the\nimpact of the order weights used to combine the criteria have rarely been\ninvestigated. This paper presents a spatial sensitivity analysis to assess the\nimpact of order weights determination in GIS-based Multi-Criteria Analysis by\nOrdered Weighted Averaging. We propose a methodology based on an efficient\nexploration of the decision-strategy space defined by the level of risk and\ntrade-off in the decision process. We illustrate our approach with a land use\nplanning process in the South of France. The objective is to find suitable\nareas for urban development while preserving green areas and their associated\necosystem services. The ecosystem service approach has indeed the potential to\nwiden the scope of traditional landscape-ecological planning by including\necosystem-based benefits, including social and economic benefits, green\ninfrastructures and biophysical parameters in urban and territorial planning.\nWe show that in this particular case the decision-strategy space can be divided\ninto four clusters. Each of them is associated with a map summarizing the\naverage spatial suitability distribution used to identify potential areas for\nurban development. We also demonstrate the pertinence of a spatial variance\nwithin-cluster analysis to disentangle the relationship between risk and\ntrade-off values. At the end, we perform a site suitability ranking analysis to\nassess the relationship between the four detected clusters.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 11:21:37 GMT"}, {"version": "v2", "created": "Sat, 4 Jul 2020 09:41:48 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Billaud", "Olivier", ""], ["Soubeyrand", "Maxence", ""], ["Luque", "Sandra", ""], ["Lenormand", "Maxime", ""]]}, {"id": "1911.11467", "submitter": "Jorge Sicacha-Parada", "authors": "J. Sicacha-Parada, I. Steinsland, B. Cretois, J. Borgelt", "title": "Accounting for spatial varying sampling effort due to accessibility in\n  Citizen Science data: A case study of moose in Norway", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Citizen Scientists together with an increasing access to technology provide\nlarge datasets that can be used to study e.g. ecology and biodiversity. Unknown\nand varying sampling effort is a major issue when making inference based on\ncitizen science data. In this paper we propose a modeling approach for\naccounting for variation in sampling effort due to accessibility. The paper is\nbased on a illustrative case study using citizen science data of moose\noccurrence in Hedmark, Norway. The aim is to make inference about the\nimportance of two geographical properties known to influence moose occurrence;\nterrain ruggedness index and solar radiation. Explanatory analysis show that\nmoose occurrences are overrepresented close to roads, and we use distance to\nroads as a proxy for accessibility. We propose a model based on a Bayesian\nLog-Gaussian Cox Process specification for occurrence. The model accounts for\naccessibility through a distance sampling approach. This approach can be seen\nas a thinning process where probability of thinning, i.e. not observing,\nincreases with increasing distances. For the moose case study distance to roads\nare used. Computationally efficient full Bayesian inference is performed using\nthe Integrated Nested Laplace Approximation and the Stochastic Partial\nDifferential Equation approach for spatial modeling. The proposed model as well\nas the consequences of not accounting for varying sampling effort due to\naccessibility are studied through a simulation study based on the case study.\nConsiderable biases are found in estimates for the effect of radiation on moose\noccurrence when accessibility is not considered in the model.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 11:38:21 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Sicacha-Parada", "J.", ""], ["Steinsland", "I.", ""], ["Cretois", "B.", ""], ["Borgelt", "J.", ""]]}, {"id": "1911.11476", "submitter": "Timothy Pollington MSc", "authors": "Timothy M. Pollington (1 and 3), Michael J. Tildesley (2), T.\n  D\\'eirdre Hollingsworth (3), Lloyd A. C. Chapman (4) ((1) MathSys CDT,\n  University of Warwick, UK, (2) Zeeman Institute (SBIDER), School of Life\n  Sciences and Mathematics Institute, University of Warwick, UK, (3) Big Data\n  Institute, Li Ka Shing Centre for Health Information and Discovery,\n  University of Oxford, UK, (4) London School of Hygiene & Tropical Medicine,\n  UK)", "title": "The spatiotemporal tau statistic: a review", "comments": "Corresponding author is Timothy M. Pollington. Equal contributions by\n  T. D\\'eirdre Hollingsworth and Lloyd A. C. Chapman. 42 pgs, 7866 words, 1\n  fig., 1 table. Right-to-reply date added, refs' DoI links shortened", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Introduction\n  The tau statistic is a recent second-order correlation function that can\nassess the magnitude and range of global spatiotemporal clustering from\nepidemiological data containing geolocations of individual cases and, usually,\ndisease onset times. This is the first review of its use, and the aspects of\nits computation and presentation that could affect inferences drawn and bias\nestimates of the statistic.\n  Methods\n  Using Google Scholar we searched papers or preprints that cited the papers\nthat first defined/reformed the statistic. We tabulated their key\ncharacteristics to understand the statistic's development since 2012.\n  Results\n  Only half of the 16 studies found were considered to be using true tau\nstatistics, but their inclusion in the review still provided important insights\ninto their analysis motivations. All papers that used graphical hypothesis\ntesting and parameter estimation used incorrect methods. There is a lack of\nclarity over how to choose the time-relatedness interval to relate cases and\nthe distance band set, that are both required to calculate the statistic. Some\nstudies demonstrated nuanced applications of the tau statistic in settings with\nunusual data or time relation variables, which enriched understanding of its\npossibilities. A gap was noticed in the estimators available to account for\nvariable person-time at risk.\n  Discussion\n  Our review comprehensively covers current uses of the tau statistic for\ndescriptive analysis, graphical hypothesis testing, and parameter estimation of\nspatiotemporal clustering. We also define a new estimator of the tau statistic\nfor disease rates. For the tau statistic there are still open questions on its\nimplementation which we hope this review inspires others to research.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 11:53:20 GMT"}, {"version": "v2", "created": "Sun, 1 Dec 2019 13:21:31 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Pollington", "Timothy M.", "", "1 and 3"], ["Tildesley", "Michael J.", ""], ["Hollingsworth", "T. D\u00e9irdre", ""], ["Chapman", "Lloyd A. C.", ""]]}, {"id": "1911.11488", "submitter": "Riccardo Rastelli", "authors": "Laleh Tafakori, Armin Pourkhanali, Riccardo Rastelli", "title": "Measuring systemic risk and contagion in the European financial network", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel framework to study default dependence and\nsystemic risk in a financial network that evolves over time. We analyse several\nindicators of risk, and develop a new latent space model to assess the health\nof key European banks before, during, and after the recent financial crises.\nFirst, we adopt the measure of CoRisk to determine the impact of such crises on\nthe financial network. Then, we use minimum spanning trees to analyse the\ncorrelation structure and the centrality of the various banks. Finally, we\npropose a new statistical model that permits a latent space visualisation of\nthe financial system. This provides a clear and interpretable model-based\nsummary of the interaction data, and it gives a new perspective on the topology\nstructure of the network. Crucially, the methodology provides a new approach to\nassess and understand the systemic risk associated with a financial system, and\nto study how debt may spread between institutions. Our dynamic framework\nprovides an interpretable map that illustrates the default dependencies between\ninstitutions, highlighting the possible patterns of contagion and the\ninstitutions that may pose systemic threats.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 12:15:45 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 14:38:21 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Tafakori", "Laleh", ""], ["Pourkhanali", "Armin", ""], ["Rastelli", "Riccardo", ""]]}, {"id": "1911.11508", "submitter": "Weisi Guo", "authors": "Parya Broomandi, Xueyu Geng, Weisi Guo, Jong Kim, Alessio Pagani,\n  David Topping", "title": "Dynamic Complex Network Analysis of PM2.5 Concentrations in the UK using\n  Hierarchical Directed Graphs", "comments": "under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ao-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Worldwide exposure to fine atmospheric particles can exasperate the risk of a\nwide range of heart and respiratory diseases, due to their ability to penetrate\ndeep into the lungs and blood streams. Epidemiological studies in Europe and\nelsewhere have established the evidence base pointing to the important role of\nPM2.5 in causing over 4 million deaths per year. Traditional approaches to\nmodel atmospheric transportation of particles suffer from high dimensionality\nfrom both transport and chemical reaction processes, making multi-sale causal\ninference challenging. We apply alternative model reduction methods: a\ndata-driven directed graph representation to infer spatial embeddedness and\ncausal directionality. Using PM2.5 concentrations in 14 UK cities over a 12\nmonth period, we construct an undirected correlation and a directed Granger\ncausality network. We show for both reduced-order cases, the UK is divided into\ntwo a northern and southern connected city communities, with greater spatial\nembedding in spring and summer. We go on to infer stability to disturbances via\nthe network trophic coherence parameter, whereby we found that winter had the\ngreatest vulnerability. As a result of our novel graph-based reduced modeling,\nwe are able to represent high-dimensional knowledge into a causal inference and\nstability framework.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 13:14:17 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Broomandi", "Parya", ""], ["Geng", "Xueyu", ""], ["Guo", "Weisi", ""], ["Kim", "Jong", ""], ["Pagani", "Alessio", ""], ["Topping", "David", ""]]}, {"id": "1911.11795", "submitter": "Luca M. Giordano", "authors": "Luca M. Giordano and Daniela Morale", "title": "A fractional Brownian -- Hawkes model for the Italian electricity spot\n  market: estimation and forecasting", "comments": "40 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a model for the description and the forecast of the gross prices\nof electricity in the liberalized Italian energy market via an additive\ntwo-factor model driven by both a Hawkes and a fractional Brownian processes.\nWe discuss the seasonality, the identification of spikes and the estimates of\nthe Hurst coefficient. After the calibration and the validation of the model,\nwe discuss its forecasting performance via a class of adequate evaluation\nmetrics.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 19:11:46 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Giordano", "Luca M.", ""], ["Morale", "Daniela", ""]]}, {"id": "1911.11890", "submitter": "Soosan Beheshti", "authors": "Soosan Beheshti, Edward Nidoy, and Faizan Rahman", "title": "K-MACE and Kernel K-MACE Clustering", "comments": "13 pages, 4 Tables, 7 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining the correct number of clusters (CNC) is an important task in data\nclustering and has a critical effect on finalizing the partitioning results.\nK-means is one of the popular methods of clustering that requires CNC. Validity\nindex methods use an additional optimization procedure to estimate the CNC for\nK-means. We propose an alternative validity index approach denoted by\nk-minimizing Average Central Error (KMACE). K-means is one of the popular\nmethods of clustering that requires CNC. Validity ACE is the average error\nbetween the true unavailable cluster center and the estimated cluster center\nfor each sample data. Kernel K-MACE is kernel K-means that is equipped with an\nefficient CNC estimator. In addition, kernel K_MACE includes the first\nautomatically tuned procedure for choosing the Gaussian kernel parameters.\nSimulation results for both synthetic and read data show superiority of K_MACE\nand kernel K-MACE over the conventional clustering methods not only in CNC\nestimation but also in the partitioning procedure.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 00:14:05 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Beheshti", "Soosan", ""], ["Nidoy", "Edward", ""], ["Rahman", "Faizan", ""]]}, {"id": "1911.12185", "submitter": "Bret Zeldow", "authors": "Bret Zeldow and Laura A. Hatfield", "title": "Confounding and Regression Adjustment in Difference-in-Differences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Difference-in-differences (diff-in-diff) is a study design that compares\noutcomes of two groups (treated and comparison) at two time points (pre- and\npost-treatment) and is widely used in evaluating new policy implementations.\nFor instance, diff-in-diff has been used to estimate the effect that increasing\nminimum wage has on employment rates and to assess the Affordable Care Act's\neffect on health outcomes. Although diff-in-diff appears simple, potential\npitfalls lurk. In this paper, we discuss one such complication: time-varying\nconfounding. We provide rigorous definitions for confounders in diff-in-diff\nstudies and explore regression strategies to adjust for confounding. In\nsimulations, we show how and when regression adjustment can ameliorate\nconfounding for both time-invariant and time-varying covariates. We compare our\nregression approach to those models commonly fit in applied literature, which\noften fail to address the time-varying nature of confounding in diff-in-diff.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 14:39:26 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Zeldow", "Bret", ""], ["Hatfield", "Laura A.", ""]]}, {"id": "1911.12204", "submitter": "Olayid\\'e Boussari", "authors": "Olayid\\'e Boussari, Laurent Bordes, Ga\\\"elle Romain, Marc Colonna,\n  Nadine Bossard, Laurent Remontet, and Val\\'erie Jooste", "title": "Modeling excess hazard with time--to--cure as a parameter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cure models have been widely developed to estimate the cure fraction when\nsome subjects never experience the event of interest. However these models were\nrarely focused on the estimation of the time-to-cure i.e. the delay elapsed\nbetween the diagnosis and \"the time from which cure is reached\", an important\nindicator, for instance to address the question of access to insurance or loans\nfor subjects with personal history of cancer. We propose a new excess hazard\nregression model that includes the time-to-cure as a covariate dependent\nparameter to be estimated. The model is written similarly to a Beta probability\ndistribution function and is shown to be a particular case of the non-mixture\ncure models. Parameters are estimated through a maximum likelihood approach and\nsimulation studies demonstrate good performance of the model. Illustrative\napplications to two cancer data sets are provided and some limitations as well\nas possible extensions of the model are discussed. The proposed model offers a\nsimple and comprehensive way to estimate more accurately the time-to-cure. Key\nwords: Cancer; Cure model; Cure time; Net survival; Right to be forgotten.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 15:01:17 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Boussari", "Olayid\u00e9", ""], ["Bordes", "Laurent", ""], ["Romain", "Ga\u00eblle", ""], ["Colonna", "Marc", ""], ["Bossard", "Nadine", ""], ["Remontet", "Laurent", ""], ["Jooste", "Val\u00e9rie", ""]]}, {"id": "1911.12284", "submitter": "Aristidis K. Nikoloulopoulos", "authors": "Aristidis K. Nikoloulopoulos", "title": "The bivariate $K$-finite normal mixture \"blanket\" copula", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There exist many bivariate parametric copulas to model bivariate data with\ndifferent dependence features. We propose a new bivariate parametric copula\nfamily that cannot only handle various dependence patterns that appear in the\nexisting parametric bivariate copula families, but also provides a more\nenriched dependence structure. The proposed copula construction exploits finite\nmixtures of bivariate normal distributions. The mixing operation, the distinct\ncorrelation and mean parameters at each mixture component introduce quite a\nflexible dependence. The new parametric copula is theoretically investigated,\ncompared with a set of classical bivariate parametric copulas and illustrated\non two empirical examples from astrophysics and agriculture where some of the\nvariables have peculiar and asymmetric dependence, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 16:54:18 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 06:14:50 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Nikoloulopoulos", "Aristidis K.", ""]]}, {"id": "1911.12337", "submitter": "Thomas Loredo", "authors": "Thomas J. Loredo, Martin A. Hendry", "title": "Multilevel and hierarchical Bayesian modeling of cosmic populations", "comments": "33 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Demographic studies of cosmic populations must contend with measurement\nerrors and selection effects. We survey some of the key ideas astronomers have\ndeveloped to deal with these complications, in the context of galaxy surveys\nand the literature on corrections for Malmquist and Eddington bias. From the\nperspective of modern statistics, such corrections arise naturally in the\ncontext of multilevel models, particularly in Bayesian treatments of such\nmodels: hierarchical Bayesian models. We survey some key lessons from\nhierarchical Bayesian modeling, including shrinkage estimation, which is\nclosely related to traditional corrections devised by astronomers. We describe\na framework for hierarchical Bayesian modeling of cosmic populations, tailored\nto features of astronomical surveys that are not typical of surveys in other\ndisciplines. This thinned latent marked point process framework accounts for\nthe tie between selection (detection) and measurement in astronomical surveys,\ntreating selection and measurement error effects in a self-consistent manner.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 18:35:02 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Loredo", "Thomas J.", ""], ["Hendry", "Martin A.", ""]]}, {"id": "1911.12405", "submitter": "Luis Nieto-Barajas Dr.", "authors": "Luis E. Nieto-Barajas and Rodrigo S. Targino", "title": "Modelling dependence within and across run-off triangles for claims\n  reserving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a stochastic model for claims reserving that captures dependence\nalong development years within a single triangle. This dependence is of\nautoregressive form of order $p$ and is achieved through the use of latent\nvariables. We carry out bayesian inference on model parameters and borrow\nstrength across several triangles, coming from different lines of businesses or\ncompanies, through the use of hierarchical priors.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 20:06:08 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Nieto-Barajas", "Luis E.", ""], ["Targino", "Rodrigo S.", ""]]}, {"id": "1911.12445", "submitter": "Jonas Moss", "authors": "Jonas Moss, Riccardo De Bin", "title": "Modelling publication bias and p-hacking", "comments": "21 pager, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Publication bias and p-hacking are two well-known phenomena that strongly\naffect the scientific literature and cause severe problems in meta-analyses.\nDue to these phenomena, the assumptions of meta-analyses are seriously violated\nand the results of the studies cannot be trusted. While publication bias is\nalmost perfectly captured by the weighting function selection model, p-hacking\nis much harder to model and no definitive solution has been found yet. In this\npaper we propose to model both publication bias and p-hacking with selection\nmodels. We derive some properties for these models, and we compare them\nformally and through simulations. Finally, two real data examples are used to\nshow how the models work in practice.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 22:09:35 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 20:10:11 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Moss", "Jonas", ""], ["De Bin", "Riccardo", ""]]}, {"id": "1911.12474", "submitter": "Mouloud Belbahri", "authors": "Mouloud Belbahri, Alejandro Murua, Olivier Gandouet, Vahid Partovi Nia", "title": "Qini-based Uplift Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uplift models provide a solution to the problem of isolating the marketing\neffect of a campaign. For customer churn reduction, uplift models are used to\nidentify the customers who are likely to respond positively to a retention\nactivity only if targeted, and to avoid wasting resources on customers that are\nvery likely to switch to another company. We introduce a Qini-based uplift\nregression model to analyze a large insurance company's retention marketing\ncampaign. Our approach is based on logistic regression models. We show that a\nQini-optimized uplift model acts as a regularizing factor for uplift, much as a\npenalized likelihood model does for regression. This results in interpretable\nparsimonious models with few relevant xplanatory variables. Our results show\nthat performing Qini-based parameters estimation significantly improves the\nuplift models performance.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 01:09:42 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 18:29:18 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Belbahri", "Mouloud", ""], ["Murua", "Alejandro", ""], ["Gandouet", "Olivier", ""], ["Nia", "Vahid Partovi", ""]]}, {"id": "1911.12478", "submitter": "Benjamin Nagy", "authors": "Benjamin Nagy", "title": "Metre as a stylometric feature in Latin hexameter poetry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper demonstrates that metre is a privileged indicator of authorial\nstyle in classical Latin hexameter poetry. Using only metrical features,\npairwise classification experiments are performed between 5 first-century\nauthors (10 comparisons) using four different machine-learning models. The\nresults showed a two-label classification accuracy of at least 95% with samples\nas small as ten lines and no greater than eighty lines (up to around 500\nwords). These sample sizes are an order of magnitude smaller than those\ntypically recommended for BOW ('bag of words') or n-gram approaches, and the\nreported accuracy is outstanding. Additionally, this paper explores the\npotential for novelty (forgery) detection, or 'one-class classification'. An\nanalysis of the disputed Aldine Additamentum (Sil. Ital. Puni. 8:144-225)\nconcludes (p=0.0013) that the metrical style differs significantly from that of\nthe rest of the poem.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 01:35:51 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 02:20:28 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Nagy", "Benjamin", ""]]}, {"id": "1911.12624", "submitter": "Clemence Leyrat", "authors": "Clemence Leyrat, James R Carpenter, Sebastien Bailly, Elizabeth J\n  Willamson", "title": "A review and evaluation of standard methods to handle missing data on\n  time-varying confounders in marginal structural models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Marginal structural models (MSMs) are commonly used to estimate causal\nintervention effects in longitudinal non-randomised studies. A common issue\nwhen analysing data from observational studies is the presence of incomplete\nconfounder data, which might lead to bias in the intervention effect estimates\nif they are not handled properly in the statistical analysis. However, there is\ncurrently no recommendation on how to address missing data on covariates in\nMSMs under a variety of missingness mechanisms encountered in practice. We\nreviewed existing methods to handling missing data in MSMs and performed a\nsimulation study to compare the performance of complete case (CC) analysis, the\nlast observation carried forward (LOCF), the missingness pattern approach\n(MPA), multiple imputation (MI) and inverse-probability-of-missingness\nweighting (IPMW). We considered three mechanisms for non-monotone missing data\nwhich are common in observational studies using electronic health record data.\nWhereas CC analysis lead to biased estimates of the intervention effect in\nalmost all scenarios, the performance of the other approaches varied across\nscenarios. The LOCF approach led to unbiased estimates only under a specific\nnon-random mechanism in which confounder values were missing when their values\nremained unchanged since the previous measurement. In this scenario, MI, the\nMPA and IPMW were biased. MI and IPMW led to the estimation of unbiased effects\nwhen data were missing at random, given the covariates or the treatment but\nonly MI was unbiased when the outcome was a predictor of missingness.\nFurthermore, IPMW generally lead to very large standard errors. Lastly,\nregardless of the missingness mechanism, the MPA led to unbiased estimates only\nwhen the failure to record a confounder at a given time-point modified the\nsubsequent relationships between the partially observed covariate and the\noutcome.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 10:21:52 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Leyrat", "Clemence", ""], ["Carpenter", "James R", ""], ["Bailly", "Sebastien", ""], ["Willamson", "Elizabeth J", ""]]}, {"id": "1911.12704", "submitter": "Claire Bowen", "authors": "Claire McKay Bowen and Joshua Snoke", "title": "Comparative Study of Differentially Private Synthetic Data Algorithms\n  from the NIST PSCR Differential Privacy Synthetic Data Challenge", "comments": "32 pages (27 main, 5 references), 3 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differentially private synthetic data generation offers a recent solution to\nrelease analytically useful data while preserving the privacy of individuals in\nthe data. In order to utilize these algorithms for public policy decisions,\npolicymakers need an accurate understanding of these algorithms' comparative\nperformance. Correspondingly, data practitioners require standard metrics for\nevaluating the analytic qualities of the synthetic data. In this paper, we\npresent an in-depth evaluation of several differentially private synthetic data\nalgorithms using actual differentially private synthetic data sets created by\ncontestants in the 2018-2019 National Institute of Standards and Technology\nPublic Safety Communications Research (NIST PSCR) Division's ``Differential\nPrivacy Synthetic Data Challenge.'' We offer analyses of these algorithms based\non both the accuracy of the data they created and their usability by potential\ndata providers. We frame the methods used in the NIST PSCR data challenge\nwithin the broader differentially private synthetic data literature. We\nimplement additional utility metrics, including two of our own, on the\ndifferentially private synthetic data and compare mechanism utility on three\ncategories. Our comparative assessment of the differentially private data\nsynthesis methods and the quality metrics shows the relative usefulness, the\ngeneral strengths and weaknesses, and offers preferred choices of algorithms\nand metrics. Finally we describe the implications of our evaluation for\npolicymakers seeking to implement differentially private synthetic data\nalgorithms on future data products.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 13:38:17 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 14:28:10 GMT"}, {"version": "v3", "created": "Mon, 12 Oct 2020 13:38:52 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Bowen", "Claire McKay", ""], ["Snoke", "Joshua", ""]]}, {"id": "1911.12813", "submitter": "Qinghao Ye", "authors": "Qinghao Ye, Kaiyuan Hu, Yizhe Wang", "title": "Application of Time Series Analysis to Traffic Accidents in Los Angeles", "comments": "19 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the improvements of Los Angeles in many aspects, people in mounting\nnumbers tend to live or travel to the city. The primary objective of this paper\nis to apply a set of methods for the time series analysis of traffic accidents\nin Los Angeles in the past few years. The number of traffic accidents,\ncollected from 2010 to 2019 monthly reveals that the traffic accident happens\nseasonally and increasing with fluctuation. This paper utilizes the ensemble\nmethods to combine several different methods to model the data from various\nperspectives, which can lead to better forecasting accuracy. The IMA(1, 1),\nETS(A, N, A), and two models with Fourier items are failed in independence\nassumption checking. However, the Online Gradient Descent (OGD) model generated\nby the ensemble method shows the perfect fit in the data modeling, which is the\nstate-of-the-art model among our candidate models. Therefore, it can be easier\nto accurately forecast future traffic accidents based on previous data through\nour model, which can help designers to make better plans.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 18:01:54 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Ye", "Qinghao", ""], ["Hu", "Kaiyuan", ""], ["Wang", "Yizhe", ""]]}, {"id": "1911.12835", "submitter": "Andreas F. Haselsteiner", "authors": "Andreas F. Haselsteiner, Klaus-Dieter Thoben", "title": "Predicting wave heights for marine design by prioritizing extreme events\n  in a global model", "comments": "18 pages, 14 figures", "journal-ref": "Renewable Energy (2020), volume 156, pages 1146-1157", "doi": "10.1016/j.renene.2020.04.112", "report-no": null, "categories": "stat.AP physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the design process of marine structures like offshore wind turbines the\nlong-term distribution of significant wave height needs to be modelled to\nestimate loads. This is typically done by fitting a translated Weibull\ndistribution to wave data. However, the translated Weibull distribution often\nfits well at typical values, but poorly at high wave heights such that extreme\nloads are underestimated. Here, we analyzed wave datasets from six locations\nsuitable for offshore wind turbines. We found that the exponentiated Weibull\ndistribution provides better overall fit to these wave data than the translated\nWeibull distribution. However, when the exponentiated Weibull distribution was\nfitted using maximum likelihood estimation, model fit at the upper tail was\nsometimes still poor. Thus, to ensure good model fit at the tail, we estimated\nthe distribution's parameters by prioritizing observations of high wave height\nusing weighted least squares estimation. Then, the distribution fitted well at\nthe bulks of the six datasets (mean absolute error in the order of 0.1 m) and\nat the tails (mean absolute error in the order of 0.5 m). The proposed method\nalso estimated the wave height's 1-year return value accurately and could be\nused to calculate design loads for offshore wind turbines.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 19:15:51 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 19:33:20 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Haselsteiner", "Andreas F.", ""], ["Thoben", "Klaus-Dieter", ""]]}, {"id": "1911.12868", "submitter": "Michael Smith", "authors": "Michael T. Smith, Joel Ssematimba, Mauricio A. Alvarez, Engineer\n  Bainomugisha", "title": "Machine Learning for a Low-cost Air Pollution Network", "comments": "Presented at NeurIPS 2019 Workshop on Machine Learning for the\n  Developing World", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data collection in economically constrained countries often necessitates\nusing approximate and biased measurements due to the low-cost of the sensors\nused. This leads to potentially invalid predictions and poor policies or\ndecision making. This is especially an issue if methods from resource-rich\nregions are applied without handling these additional constraints. In this\npaper we show, through the use of an air pollution network example, how using\nprobabilistic machine learning can mitigate some of the technical constraints.\nSpecifically we experiment with modelling the calibration for individual\nsensors as either distributions or Gaussian processes over time, and discuss\nthe wider issues around the decision process.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 21:32:59 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Smith", "Michael T.", ""], ["Ssematimba", "Joel", ""], ["Alvarez", "Mauricio A.", ""], ["Bainomugisha", "Engineer", ""]]}, {"id": "1911.13018", "submitter": "Antonio Quintero-Rincon", "authors": "Antonio Quintero-Rinc\\'on, Catalina Carenzo, Joaqu\\'in Ems, Lourdes\n  Hirschson, Valeria Muro, Carlos D'Giano", "title": "Spike-and-wave epileptiform discharge pattern detection based on\n  Kendall's Tau-b coefficient", "comments": "8 pages, 3 figures. RESEARCH LETTERS/SHORT REPORTS", "journal-ref": "Applied Medical Informatics 41 (1), 1-8, 2019", "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Epilepsy is an important public health issue. An appropriate epileptiform\ndischarge pattern detection of this neurological disease is a typical problem\nin biomedical engineering. In this paper, a new method is proposed for\nspike-and-wave discharge pattern detection based on Kendall's Tau-b\ncoefficient. The proposed approach is demonstrated on a real dataset containing\nspike-and-wave discharge signals, where our performance is evaluated in terms\nof high Specificity, rule in (SpPIn) with 94% for patient-specific\nspike-and-wave discharge detection and 83% for a general spike-and-wave\ndischarge detection.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 09:41:45 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Quintero-Rinc\u00f3n", "Antonio", ""], ["Carenzo", "Catalina", ""], ["Ems", "Joaqu\u00edn", ""], ["Hirschson", "Lourdes", ""], ["Muro", "Valeria", ""], ["D'Giano", "Carlos", ""]]}, {"id": "1911.13136", "submitter": "Hector Rodriguez-Deniz", "authors": "Hector Rodriguez-Deniz, Mattias Villani and Augusto Voltes-Dorta", "title": "A Multilayered Block Network Model to Forecast Large Dynamic\n  Transportation Graphs: an Application to US Air Transport", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic transportation networks have been analyzed for years by means of\nstatic graph-based indicators in order to study the temporal evolution of\nrelevant network components, and to reveal complex dependencies that would not\nbe easily detected by a direct inspection of the data. This paper presents a\nstate-of-the-art latent network model to forecast multilayer dynamic graphs\nthat are increasingly common in transportation and proposes a community-based\nextension to reduce the computational burden. Flexible time series analysis is\nobtained by modeling the probability of edges between vertices through latent\nGaussian processes. The models and Bayesian inference are illustrated on a\nsample of 10-year data from four major airlines within the US air\ntransportation system. Results show how the estimated latent parameters from\nthe models are related to the airline's connectivity dynamics, and their\nability to project the multilayer graph into the future for out-of-sample full\nnetwork forecasts, while stochastic blockmodeling allows for the identification\nof relevant communities. Reliable network predictions would allow policy-makers\nto better understand the dynamics of the transport system, and help in their\nplanning on e.g. route development, or the deployment of new regulations.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 15:03:05 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 07:07:09 GMT"}, {"version": "v3", "created": "Sat, 19 Jun 2021 12:44:47 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Rodriguez-Deniz", "Hector", ""], ["Villani", "Mattias", ""], ["Voltes-Dorta", "Augusto", ""]]}]