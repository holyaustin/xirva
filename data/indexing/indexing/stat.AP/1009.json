[{"id": "1009.0405", "submitter": "Nathan Green Dr", "authors": "Nathan Green, Duncan Smith, Matthew Sperrin, Iain Buchan", "title": "A Novel Chronic Disease Policy Model", "comments": "24 pages, 13 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a simulation tool to support policy-decisions about healthcare for\nchronic diseases in defined populations. Incident disease-cases are generated\nin-silico from an age-sex characterised general population using standard\nepidemiological approaches. A novel disease-treatment model then simulates\ncontinuous life courses for each patient using discrete event simulation.\nIdeally, the discrete event simulation model would be inferred from complete\nlongitudinal healthcare data via a likelihood or Bayesian approach. Such data\nis seldom available for relevant populations, therefore an innovative approach\nto evidence synthesis is required. We propose a novel entropy-based approach to\nfit survival densities. This method provides a fully flexible way to\nincorporate the available information, which can be derived from arbitrary\nsources. Discrete event simulation then takes place on the fitted model using a\ncompeting hazards framework. The output is then used to help evaluate the\npotential impacts of policy options for a given population.\n", "versions": [{"version": "v1", "created": "Thu, 2 Sep 2010 12:17:02 GMT"}], "update_date": "2010-09-03", "authors_parsed": [["Green", "Nathan", ""], ["Smith", "Duncan", ""], ["Sperrin", "Matthew", ""], ["Buchan", "Iain", ""]]}, {"id": "1009.0779", "submitter": "Thomas Kitching", "authors": "Thomas Kitching, Sreekumar Balan, Gary Bernstein, Matthias Bethge,\n  Sarah Bridle, Frederic Courbin, Marc Gentile, Alan Heavens, Michael Hirsch,\n  Reshad Hosseini, Alina Kiessling, Adam Amara, Donnacha Kirk, Konrad Kuijken,\n  Rachel Mandelbaum, Baback Moghaddam, Guldariya Nurbaeva, Stephane\n  Paulin-Henriksson, Anais Rassat, Jason Rhodes, Bernhard Sch\\\"olkopf, John\n  Shawe-Taylor, Mandeep Gill, Marina Shmakova, Andy Taylor, Malin Velander,\n  Ludovic van Waerbeke, Dugan Witherick, David Wittman, Stefan Harmeling,\n  Catherine Heymans, Richard Massey, Barnaby Rowe, Tim Schrabback, Lisa Voigt", "title": "Gravitational Lensing Accuracy Testing 2010 (GREAT10) Challenge Handbook", "comments": "Published in at http://dx.doi.org/10.1214/11-AOAS484 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 3, 2231-2263", "doi": "10.1214/11-AOAS484", "report-no": "IMS-AOAS-AOAS484", "categories": "astro-ph.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GRavitational lEnsing Accuracy Testing 2010 (GREAT10) is a public image\nanalysis challenge aimed at the development of algorithms to analyze\nastronomical images. Specifically, the challenge is to measure varying image\ndistortions in the presence of a variable convolution kernel, pixelization and\nnoise. This is the second in a series of challenges set to the astronomy,\ncomputer science and statistics communities, providing a structured environment\nin which methods can be improved and tested in preparation for planned\nastronomical surveys. GREAT10 extends upon previous work by introducing\nvariable fields into the challenge. The \"Galaxy Challenge\" involves the precise\nmeasurement of galaxy shape distortions, quantified locally by two parameters\ncalled shear, in the presence of a known convolution kernel. Crucially, the\nconvolution kernel and the simulated gravitational lensing shape distortion\nboth now vary as a function of position within the images, as is the case for\nreal data. In addition, we introduce the \"Star Challenge\" that concerns the\nreconstruction of a variable convolution kernel, similar to that in a typical\nastronomical observation. This document details the GREAT10 Challenge for\npotential participants. Continually updated information is also available from\nhttp://www.greatchallenges.info.\n", "versions": [{"version": "v1", "created": "Fri, 3 Sep 2010 22:00:25 GMT"}, {"version": "v2", "created": "Wed, 30 Nov 2011 11:30:58 GMT"}], "update_date": "2011-12-01", "authors_parsed": [["Kitching", "Thomas", ""], ["Balan", "Sreekumar", ""], ["Bernstein", "Gary", ""], ["Bethge", "Matthias", ""], ["Bridle", "Sarah", ""], ["Courbin", "Frederic", ""], ["Gentile", "Marc", ""], ["Heavens", "Alan", ""], ["Hirsch", "Michael", ""], ["Hosseini", "Reshad", ""], ["Kiessling", "Alina", ""], ["Amara", "Adam", ""], ["Kirk", "Donnacha", ""], ["Kuijken", "Konrad", ""], ["Mandelbaum", "Rachel", ""], ["Moghaddam", "Baback", ""], ["Nurbaeva", "Guldariya", ""], ["Paulin-Henriksson", "Stephane", ""], ["Rassat", "Anais", ""], ["Rhodes", "Jason", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Shawe-Taylor", "John", ""], ["Gill", "Mandeep", ""], ["Shmakova", "Marina", ""], ["Taylor", "Andy", ""], ["Velander", "Malin", ""], ["van Waerbeke", "Ludovic", ""], ["Witherick", "Dugan", ""], ["Wittman", "David", ""], ["Harmeling", "Stefan", ""], ["Heymans", "Catherine", ""], ["Massey", "Richard", ""], ["Rowe", "Barnaby", ""], ["Schrabback", "Tim", ""], ["Voigt", "Lisa", ""]]}, {"id": "1009.0802", "submitter": "Richard D. Gill", "authors": "Richard D. Gill, Piet Groeneboom, Peter de Jong", "title": "Elementary Statistics on Trial (the case of Lucia de Berk)", "comments": "10 pages, 3 figures and 2 tables", "journal-ref": "CHANCE, 31:4, 9-15 (2018)", "doi": "10.1080/09332480.2018.1549809", "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the conviction of Lucia de Berk an important role was played by a simple\nhypergeometric model, used by the expert consulted by the court, which produced\nvery small probabilities of occurrences of certain numbers of incidents. We\nwant to draw attention to the fact that, if we take into account the variation\namong nurses in incidents they experience during their shifts, these\nprobabilities can become considerably larger. This points to the danger of\nusing an oversimplified discrete probability model in these circumstances.\n", "versions": [{"version": "v1", "created": "Sat, 4 Sep 2010 03:41:12 GMT"}, {"version": "v2", "created": "Thu, 6 Dec 2018 11:52:25 GMT"}, {"version": "v3", "created": "Tue, 22 Oct 2019 15:43:12 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Gill", "Richard D.", ""], ["Groeneboom", "Piet", ""], ["de Jong", "Peter", ""]]}, {"id": "1009.0893", "submitter": "Charles R. Doss", "authors": "Charles R. Doss, Marc A. Suchard, Ian Holmes, Midori Kato-Maeda,\n  Vladimir N. Minin", "title": "Fitting birth-death processes to panel data with applications to\n  bacterial DNA fingerprinting", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS673 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 4, 2315-2335", "doi": "10.1214/13-AOAS673", "report-no": "IMS-AOAS-AOAS673", "categories": "stat.CO q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous-time linear birth-death-immigration (BDI) processes are frequently\nused in ecology and epidemiology to model stochastic dynamics of the population\nof interest. In clinical settings, multiple birth-death processes can describe\ndisease trajectories of individual patients, allowing for estimation of the\neffects of individual covariates on the birth and death rates of the process.\nSuch estimation is usually accomplished by analyzing patient data collected at\nunevenly spaced time points, referred to as panel data in the biostatistics\nliterature. Fitting linear BDI processes to panel data is a nontrivial\noptimization problem because birth and death rates can be functions of many\nparameters related to the covariates of interest. We propose a novel\nexpectation--maximization (EM) algorithm for fitting linear BDI models with\ncovariates to panel data. We derive a closed-form expression for the joint\ngenerating function of some of the BDI process statistics and use this\ngenerating function to reduce the E-step of the EM algorithm, as well as\ncalculation of the Fisher information, to one-dimensional integration. This\nanalytical technique yields a computationally efficient and robust optimization\nalgorithm that we implemented in an open-source R package. We apply our method\nto DNA fingerprinting of Mycobacterium tuberculosis, the causative agent of\ntuberculosis, to study intrapatient time evolution of IS6110 copy number, a\ngenetic marker frequently used during estimation of epidemiological clusters of\nMycobacterium tuberculosis infections. Our analysis reveals previously\nundocumented differences in IS6110 birth-death rates among three major lineages\nof Mycobacterium tuberculosis, which has important implications for\nepidemiologists that use IS6110 for DNA fingerprinting of Mycobacterium\ntuberculosis.\n", "versions": [{"version": "v1", "created": "Sun, 5 Sep 2010 05:23:50 GMT"}, {"version": "v2", "created": "Fri, 15 Apr 2011 23:13:44 GMT"}, {"version": "v3", "created": "Sun, 16 Dec 2012 23:30:46 GMT"}, {"version": "v4", "created": "Tue, 13 Aug 2013 21:47:17 GMT"}, {"version": "v5", "created": "Fri, 10 Jan 2014 14:07:26 GMT"}], "update_date": "2014-01-13", "authors_parsed": [["Doss", "Charles R.", ""], ["Suchard", "Marc A.", ""], ["Holmes", "Ian", ""], ["Kato-Maeda", "Midori", ""], ["Minin", "Vladimir N.", ""]]}, {"id": "1009.1318", "submitter": "Fabrice Rossi", "authors": "Fabrice Rossi and Nathalie Villa-Vialaneix", "title": "Optimizing an Organized Modularity Measure for Topographic Graph\n  Clustering: a Deterministic Annealing Approach", "comments": null, "journal-ref": "Neurocomputing, 73(7--9):1142--1163, March 2010", "doi": "10.1016/j.neucom.2009.11.023", "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an organized generalization of Newman and Girvan's\nmodularity measure for graph clustering. Optimized via a deterministic\nannealing scheme, this measure produces topologically ordered graph clusterings\nthat lead to faithful and readable graph representations based on clustering\ninduced graphs. Topographic graph clustering provides an alternative to more\nclassical solutions in which a standard graph clustering method is applied to\nbuild a simpler graph that is then represented with a graph layout algorithm. A\ncomparative study on four real world graphs ranging from 34 to 1 133 vertices\nshows the interest of the proposed approach with respect to classical solutions\nand to self-organizing maps for graphs.\n", "versions": [{"version": "v1", "created": "Tue, 7 Sep 2010 14:57:15 GMT"}], "update_date": "2010-09-08", "authors_parsed": [["Rossi", "Fabrice", ""], ["Villa-Vialaneix", "Nathalie", ""]]}, {"id": "1009.1436", "submitter": "Anton H. Westveld", "authors": "Anton H. Westveld, Peter D. Hoff", "title": "A mixed effects model for longitudinal relational and network data, with\n  applications to international trade and conflict", "comments": "Published in at http://dx.doi.org/10.1214/10-AOAS403 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 2A, 843-872", "doi": "10.1214/10-AOAS403", "report-no": "IMS-AOAS-AOAS403", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The focus of this paper is an approach to the modeling of longitudinal social\nnetwork or relational data. Such data arise from measurements on pairs of\nobjects or actors made at regular temporal intervals, resulting in a social\nnetwork for each point in time. In this article we represent the network and\ntemporal dependencies with a random effects model, resulting in a stochastic\nprocess defined by a set of stationary covariance matrices. Our approach builds\nupon the social relations models of Warner, Kenny and Stoto [Journal of\nPersonality and Social Psychology 37 (1979) 1742--1757] and Gill and Swartz\n[Canad. J. Statist. 29 (2001) 321--331] and allows for an intra- and\ninter-temporal representation of network structures. We apply the methodology\nto two longitudinal data sets: international trade (continuous response) and\nmilitarized interstate disputes (binary response).\n", "versions": [{"version": "v1", "created": "Wed, 8 Sep 2010 01:47:34 GMT"}, {"version": "v2", "created": "Sun, 26 Sep 2010 23:43:36 GMT"}, {"version": "v3", "created": "Wed, 17 Aug 2011 05:24:21 GMT"}], "update_date": "2011-08-18", "authors_parsed": [["Westveld", "Anton H.", ""], ["Hoff", "Peter D.", ""]]}, {"id": "1009.1507", "submitter": "Tucker McElroy", "authors": "Tucker McElroy", "title": "Incompatibility of trends in multi-year estimates from the American\n  Community Survey", "comments": "Published in at http://dx.doi.org/10.1214/09-AOAS259 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 4, 1493-1504", "doi": "10.1214/09-AOAS259", "report-no": "IMS-AOAS-AOAS259", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The American Community Survey (ACS) provides one-year (1y), three-year (3y)\nand five-year (5y) multi-year estimates (MYEs) of various demographic and\neconomic variables for each \"community\", although the 1y and 3y may not be\navailable for communities with a small population. These survey estimates are\nnot truly measuring the same quantities, since they each cover different time\nspans. Using some simplistic models, we demonstrate that comparing different\nperiod-length MYEs results in spurious conclusions about trend movements. A\nsimple method utilizing weighted averages is presented that reduces the bias\ninherent in comparing trends of different MYEs. These weighted averages are\nnonparametric, require only a short span of data, and are designed to preserve\npolynomial characteristics of the time series that are relevant for trends. The\nbasic method, which only requires polynomial algebra, is outlined and applied\nto ACS data. In some cases there is an improvement to comparability, although a\nfinal verdict must await additional ACS data. We draw the conclusion that MYE\ndata is not comparable across different periods.\n", "versions": [{"version": "v1", "created": "Wed, 8 Sep 2010 11:34:08 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["McElroy", "Tucker", ""]]}, {"id": "1009.1555", "submitter": "Daniel Percival", "authors": "Di Liu and Daniel Percival and Stephen E. Fienberg", "title": "User Interest and Interaction Structure in Online Forums", "comments": "8 Pages, 7 Figures, Short form appears in Proc. of ICWSM 2010", "journal-ref": "Di Liu, Daniel Percival and Stephen E. Fienberg. User Interest and\n  Interaction Structure in Online Forums. Proc of ICWSM 2010", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new similarity measure tailored to posts in an online forum. Our\nmeasure takes into account all the available information about user interest\nand interaction --- the content of posts, the threads in the forum, and the\nauthor of the posts. We use this post similarity to build a similarity between\nusers, based on principal coordinate analysis. This allows easy visualization\nof the user activity as well. Similarity between users has numerous\napplications, such as clustering or classification. We show that including the\nauthor of a post in the post similarity has a smoothing effect on principal\ncoordinate projections. We demonstrate our method on real data drawn from an\ninternal corporate forum, and compare our results to those given by a standard\ndocument classification method. We conclude our method gives a more detailed\npicture of both the local and global network structure.\n", "versions": [{"version": "v1", "created": "Wed, 8 Sep 2010 14:53:42 GMT"}], "update_date": "2010-09-09", "authors_parsed": [["Liu", "Di", ""], ["Percival", "Daniel", ""], ["Fienberg", "Stephen E.", ""]]}, {"id": "1009.2766", "submitter": "Coryn Bailer-Jones", "authors": "C.A.L. Bailer-Jones (MPIA, Heidelberg)", "title": "Bayesian inference of stellar parameters and interstellar extinction\n  using parallaxes and multiband photometry", "comments": "MNRAS, in press. The catalogue is available from\n  http://tinyurl.com/qmethod. Minor changes/corrections to text made in v2", "journal-ref": null, "doi": "10.1111/j.1365-2966.2010.17699.x", "report-no": null, "categories": "astro-ph.IM astro-ph.GA stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Astrometric surveys provide the opportunity to measure the absolute\nmagnitudes of large numbers of stars, but only if the individual line-of-sight\nextinctions are known. Unfortunately, extinction is highly degenerate with\nstellar effective temperature when estimated from broad band optical/infrared\nphotometry. To address this problem, I introduce a Bayesian method for\nestimating the intrinsic parameters of a star and its line-of-sight extinction.\nIt uses both photometry and parallaxes in a self-consistent manner in order to\nprovide a non-parametric posterior probability distribution over the\nparameters. The method makes explicit use of domain knowledge by employing the\nHertzsprung--Russell Diagram (HRD) to constrain solutions and to ensure that\nthey respect stellar physics. I first demonstrate this method by using it to\nestimate effective temperature and extinction from BVJHK data for a set of\nartificially reddened Hipparcos stars, for which accurate effective\ntemperatures have been estimated from high resolution spectroscopy. Using just\nthe four colours, we see the expected strong degeneracy (positive correlation)\nbetween the temperature and extinction. Introducing the parallax, apparent\nmagnitude and the HRD reduces this degeneracy and improves both the precision\n(reduces the error bars) and the accuracy of the parameter estimates, the\nlatter by about 35%. The resulting accuracy is about 200K in temperature and\n0.2mag in extinction. I then apply the method to estimate these parameters and\nabsolute magnitudes for some 47000 F,G,K Hipparcos stars which have been\ncross-matched with 2MASS. The method can easily be extended to incorporate the\nestimation of other parameters, in particular metallicity and surface gravity,\nmaking it particularly suitable for the analysis of the 10^9 stars from Gaia.\n", "versions": [{"version": "v1", "created": "Tue, 14 Sep 2010 20:18:26 GMT"}, {"version": "v2", "created": "Fri, 5 Nov 2010 08:51:05 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Bailer-Jones", "C. A. L.", "", "MPIA, Heidelberg"]]}, {"id": "1009.3045", "submitter": "Ian Dryden", "authors": "Ian L. Dryden, Xavier Pennec and Jean-Marc Peyrat", "title": "Power Euclidean metrics for covariance matrices with application to\n  diffusion tensor imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various metrics for comparing diffusion tensors have been recently proposed\nin the literature. We consider a broad family of metrics which is indexed by a\nsingle power parameter. A likelihood-based procedure is developed for choosing\nthe most appropriate metric from the family for a given dataset at hand. The\napproach is analogous to using the Box-Cox transformation that is frequently\ninvestigated in regression analysis. The methodology is illustrated with a\nsimulation study and an application to a real dataset of diffusion tensor\nimages of canine hearts.\n", "versions": [{"version": "v1", "created": "Wed, 15 Sep 2010 21:32:36 GMT"}], "update_date": "2010-09-17", "authors_parsed": [["Dryden", "Ian L.", ""], ["Pennec", "Xavier", ""], ["Peyrat", "Jean-Marc", ""]]}, {"id": "1009.3072", "submitter": "Ian Dryden", "authors": "Kim Kenobi and Ian L. Dryden", "title": "Bayesian matching of unlabelled point sets using Procrustes and\n  configuration models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of matching unlabelled point sets using Bayesian inference is\nconsidered. Two recently proposed models for the likelihood are compared, based\non the Procrustes size-and-shape and the full configuration. Bayesian inference\nis carried out for matching point sets using Markov chain Monte Carlo\nsimulation. An improvement to the existing Procrustes algorithm is proposed\nwhich improves convergence rates, using occasional large jumps in the burn-in\nperiod. The Procrustes and configuration methods are compared in a simulation\nstudy and using real data, where it is of interest to estimate the strengths of\nmatches between protein binding sites. The performance of both methods is\ngenerally quite similar, and a connection between the two models is made using\na Laplace approximation.\n", "versions": [{"version": "v1", "created": "Thu, 16 Sep 2010 01:44:17 GMT"}], "update_date": "2010-09-17", "authors_parsed": [["Kenobi", "Kim", ""], ["Dryden", "Ian L.", ""]]}, {"id": "1009.3123", "submitter": "Ryszarda Getko", "authors": "Ryszarda Getko", "title": "The 155-day periodicity of the sunspot area fluctuations in the solar\n  cycle 16 is an alias", "comments": "11 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP astro-ph.SR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The short-term periodicities of the daily sunspot area fluctuations from\nAugust 1923 to October 1933 are discussed. For these data the correlative\nanalysis indicates negative correlation for the periodicity of about 155 days,\nbut the power spectrum analysis indicates a statistically significant peak in\nthis time interval. A new method of the diagnosis of an echo-effect in spectrum\nis proposed and it is stated that the 155-day periodicity is a harmonic of the\nperiodicities from the interval of [400,500] days.\n  The autocorrelation functions for the daily sunspot area fluctuations and for\nthe fluctuations of the one rotation time interval in the northern hemisphere,\nseparately for the whole solar cycle 16 and for the maximum activity period of\nthis cycle do not show differences, especially in the interval of [57, 173]\ndays. It proves against the thesis of the existence of strong positive\nfluctuations of the about 155-day interval in the maximum activity period of\nthe solar cycle 16 in the northern hemisphere. However, a similar analysis for\ndata from the southern hemisphere indicates that there is the periodicity of\nabout 155 days in sunspot area data in the maximum activity period of the cycle\n16 only.\n", "versions": [{"version": "v1", "created": "Thu, 16 Sep 2010 09:23:09 GMT"}], "update_date": "2010-09-17", "authors_parsed": [["Getko", "Ryszarda", ""]]}, {"id": "1009.3243", "submitter": "Brendan Nyhan", "authors": "Hans Noel and Brendan Nyhan", "title": "The \"Unfriending\" Problem: The Consequences of Homophily in Friendship\n  Retention for Causal Estimates of Social Influence", "comments": "26 pages, 4 figures", "journal-ref": null, "doi": "10.1016/j.socnet.2011.05.003", "report-no": null, "categories": "stat.AP cs.SI physics.data-an physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An increasing number of scholars are using longitudinal social network data\nto try to obtain estimates of peer or social influence effects. These data may\nprovide additional statistical leverage, but they can introduce new inferential\nproblems. In particular, while the confounding effects of homophily in\nfriendship formation are widely appreciated, homophily in friendship retention\nmay also confound causal estimates of social influence in longitudinal network\ndata. We provide evidence for this claim in a Monte Carlo analysis of the\nstatistical model used by Christakis, Fowler, and their colleagues in numerous\narticles estimating \"contagion\" effects in social networks. Our results\nindicate that homophily in friendship retention induces significant upward bias\nand decreased coverage levels in the Christakis and Fowler model if there is\nnon-negligible friendship attrition over time.\n", "versions": [{"version": "v1", "created": "Thu, 16 Sep 2010 18:50:51 GMT"}, {"version": "v2", "created": "Fri, 17 Jun 2011 19:17:20 GMT"}], "update_date": "2011-06-20", "authors_parsed": [["Noel", "Hans", ""], ["Nyhan", "Brendan", ""]]}, {"id": "1009.3507", "submitter": "Matthew Schofield", "authors": "Matthew R. Schofield, Richard J. Barker", "title": "Hierarchical Modeling of Abundance in Closed Population\n  Capture-Recapture Models Under Heterogeneity", "comments": null, "journal-ref": "Environmental and Ecological Statistics, September 2014, Volume\n  21, Issue 3, pp 435-451", "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical modeling of abundance in space or time using closed-population\nmark-recapture under heterogeneity (model M$_{h}$) presents two challenges: (i)\nfinding a flexible likelihood in which abundance appears as an explicit\nparameter and (ii) fitting the hierarchical model for abundance. The first\nchallenge arises because abundance not only indexes the population size, it\nalso determines the dimension of the capture probabilities in heterogeneity\nmodels. A common approach is to use data augmentation to include these capture\nprobabilities directly into the likelihood and fit the model using Bayesian\ninference via Markov chain Monte Carlo (MCMC). Two such examples of this\napproach are (i) explicit trans-dimensional MCMC, and (ii) superpopulation data\naugmentation. The superpopulation approach has the advantage of simple\nspecification that is easily implemented in BUGS and related software. However,\nit reparameterizes the model so that abundance is no longer included, except as\na derived quantity. This is a drawback when hierarchical models for abundance,\nor related parameters, are desired. Here, we analytically compare the two\napproaches and show that they are more closely related than might appear\nsuperficially. We exploit this relationship to specify the model in a way that\nallows us to include abundance as a parameter and that facilitates hierarchical\nmodeling using readily available software such as BUGS. We use this approach to\nmodel trends in grizzly bear abundance in Yellowstone National Park from\n1986-1998.\n", "versions": [{"version": "v1", "created": "Fri, 17 Sep 2010 21:14:45 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2015 00:04:14 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["Schofield", "Matthew R.", ""], ["Barker", "Richard J.", ""]]}, {"id": "1009.3516", "submitter": "Matthew Schofield", "authors": "Matthew R. Schofield, Richard J. Barker", "title": "Full Open Population Capture-Recapture Models with Individual Covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional analyses of capture-recapture data are based on likelihood\nfunctions that explicitly integrate out all missing data. We use a complete\ndata likelihood (CDL) to show how a wide range of capture-recapture models can\nbe easily fitted using readily available software JAGS/BUGS even when there are\nindividual-specific time-varying covariates. The models we describe extend\nthose that condition on first capture to include abundance parameters, or\nparameters related to abundance, such as population size, birth rates or\nlifetime. The use of a CDL means that any missing data, including uncertain\nindividual covariates, can be included in models without the need for\ncustomized likelihood functions. This approach also facilitates modeling\nprocesses of demographic interest rather than the complexities caused by\nnon-ignorable missing data. We illustrate using two examples, (i) open\npopulation modeling in the presence of a censored time-varying individual\ncovariate in a full robust-design, and (ii) full open population multi-state\nmodeling in the presence of a partially observed categorical variable.\n", "versions": [{"version": "v1", "created": "Fri, 17 Sep 2010 21:42:38 GMT"}], "update_date": "2010-09-21", "authors_parsed": [["Schofield", "Matthew R.", ""], ["Barker", "Richard J.", ""]]}, {"id": "1009.3601", "submitter": "David Hardoon", "authors": "David R. Hardoon and Kristiaan Pelcksman", "title": "Pair-Wise Cluster Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of learning clusters which are consistently\npresent in different (continuously valued) representations of observed data.\nOur setup differs slightly from the standard approach of (co-) clustering as we\nuse the fact that some form of `labeling' becomes available in this setup: a\ncluster is only interesting if it has a counterpart in the alternative\nrepresentation. The contribution of this paper is twofold: (i) the problem\nsetting is explored and an analysis in terms of the PAC-Bayesian theorem is\npresented, (ii) a practical kernel-based algorithm is derived exploiting the\ninherent relation to Canonical Correlation Analysis (CCA), as well as its\nextension to multiple views. A content based information retrieval (CBIR) case\nstudy is presented on the multi-lingual aligned Europal document dataset which\nsupports the above findings.\n", "versions": [{"version": "v1", "created": "Sun, 19 Sep 2010 02:28:35 GMT"}], "update_date": "2010-09-21", "authors_parsed": [["Hardoon", "David R.", ""], ["Pelcksman", "Kristiaan", ""]]}, {"id": "1009.3669", "submitter": "Michael Finegold", "authors": "Michael Finegold, Mathias Drton", "title": "Robust graphical modeling of gene networks using classical and\n  alternative T-distributions", "comments": "Published in at http://dx.doi.org/10.1214/10-AOAS410 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 2A, 1057-1080", "doi": "10.1214/10-AOAS410", "report-no": "IMS-AOAS-AOAS410", "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical Gaussian models have proven to be useful tools for exploring\nnetwork structures based on multivariate data. Applications to studies of gene\nexpression have generated substantial interest in these models, and resulting\nrecent progress includes the development of fitting methodology involving\npenalization of the likelihood function. In this paper we advocate the use of\nmultivariate $t$-distributions for more robust inference of graphs. In\nparticular, we demonstrate that penalized likelihood inference combined with an\napplication of the EM algorithm provides a computationally efficient approach\nto model selection in the $t$-distribution case. We consider two versions of\nmultivariate $t$-distributions, one of which requires the use of approximation\ntechniques. For this distribution, we describe a Markov chain Monte Carlo EM\nalgorithm based on a Gibbs sampler as well as a simple variational\napproximation that makes the resulting method feasible in large problems.\n", "versions": [{"version": "v1", "created": "Sun, 19 Sep 2010 23:36:42 GMT"}, {"version": "v2", "created": "Tue, 21 Sep 2010 12:46:28 GMT"}, {"version": "v3", "created": "Tue, 9 Aug 2011 08:02:29 GMT"}], "update_date": "2011-08-10", "authors_parsed": [["Finegold", "Michael", ""], ["Drton", "Mathias", ""]]}, {"id": "1009.3970", "submitter": "Song Cai", "authors": "Song Cai, James V. Zidek, and Nathaniel Newlands", "title": "Predicting phenological events using event-history analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an approach to phenology, one based on the use of a\nmethod developed by the authors for event history data. Of specific interest is\nthe prediction of the so-called \"bloom--date\" of fruit trees in the agriculture\nindustry and it is this application which we consider, although the method is\nmuch more broadly applicable. Our approach provides sensible estimate for a\nparameter that interests phenologists -- Tbase, the thresholding parameter in\nthe definition of the growing degree days (GDD). Our analysis supports\nscientists' empirical finding: the timing of a phenological event of a prenniel\ncrop is related the cumulative sum of GDDs. Our prediction of future\nbloom--dates are quite accurate, but the predictive uncertainty is high,\npossibly due to our crude climate model for predicting future temperature, the\ntime-dependent covariate in our regression model for phenological events. We\nfound that if we can manage to get accurate prediction of future temperature,\nour prediction of bloom--date is more accurate and the predictive uncertainty\nis much lower.\n", "versions": [{"version": "v1", "created": "Mon, 20 Sep 2010 23:58:59 GMT"}], "update_date": "2010-09-22", "authors_parsed": [["Cai", "Song", ""], ["Zidek", "James V.", ""], ["Newlands", "Nathaniel", ""]]}, {"id": "1009.3977", "submitter": "Marlene Marchena", "authors": "Marlene Silva Marchena", "title": "Measuring and implementing the bullwhip effect under a generalized\n  demand process", "comments": "24 pages and 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The measure of the bullwhip effect, a phenomenon in which demand variability\nincreases as one moves up the supply chain, is a major issue in Supply Chain\nManagement. Although it is simply defined (it is the ratio of the unconditional\nvariance of the order process to that of the demand process), explicit formulas\nare difficult to obtain. In this paper we investigate the theoretical and\npractical issues of Zhang [Manufacturing and Services Operations Management 6-2\n(2004b) 195] with the purpose of quantifying the bullwhip effect. Considering a\ntwo-stage supply chain, the bullwhip effect is measured for an ARMA(p,q) demand\nprocess admitting an infinite moving average representation. As particular\ncases of this time series model, the AR(p), MA(q), ARMA(1,1), AR(1) and AR(2)\nare discussed. For some of them, explicit formulas are obtained. We show that\nfor certain types of demand processes, the use of the optimal forecasting\nprocedure that minimizes the mean squared forecasting error leads to\nsignificant reduction in the safety stock level. This highlights the potential\neconomic benefits resulting from the use of this time series analysis. Finally,\nan R function called SCperf is programmed to calculate the bullwhip effect and\nother supply chain performance variables. It leads to a simple but powerful\ntool which could benefit both managers and researchers.\n", "versions": [{"version": "v1", "created": "Tue, 21 Sep 2010 01:05:33 GMT"}, {"version": "v2", "created": "Tue, 28 Sep 2010 12:28:03 GMT"}, {"version": "v3", "created": "Tue, 19 Oct 2010 17:29:52 GMT"}], "update_date": "2010-10-20", "authors_parsed": [["Marchena", "Marlene Silva", ""]]}, {"id": "1009.4342", "submitter": "Merlin Keller", "authors": "Merlin Keller, Eric Parent, Alberto Pasanisi", "title": "On the Role of Decision Theory in Uncertainty Analysis", "comments": "28 pages 5 figures, submitted to \"Reliability Engineering & System\n  Safety\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum likelihood estimation (MLE) and heuristic predictive estimation (HPE)\nare two widely used approaches in industrial uncertainty analysis. We review\nthem from the point of view of decision theory, using Bayesian inference as a\ngold standard for comparison. The main drawback of MLE is that it may fail to\nproperly account for the uncertainty on the physical process generating the\ndata, especially when only a small amount of data are available. HPE offers an\nimprovement in that it takes this uncertainty into account. However, we show\nthat this approach is actually equivalent to Bayes estimation for a particular\ncost function that is not explicitly chosen by the decision maker. This may\nproduce results that are suboptimal from a decisional perspective. These\nresults plead for a systematic use of Bayes estimators based on carefully\ndefined cost functions.\n", "versions": [{"version": "v1", "created": "Wed, 22 Sep 2010 12:32:22 GMT"}], "update_date": "2010-09-23", "authors_parsed": [["Keller", "Merlin", ""], ["Parent", "Eric", ""], ["Pasanisi", "Alberto", ""]]}, {"id": "1009.4362", "submitter": "Jan van den Broek", "authors": "Jan van den Broek, Hiroshi Nishiura", "title": "Using epidemic prevalence data to jointly estimate reproduction and\n  removal", "comments": "Published in at http://dx.doi.org/10.1214/09-AOAS270 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 4, 1505-1520", "doi": "10.1214/09-AOAS270", "report-no": "IMS-AOAS-AOAS270", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study proposes a nonhomogeneous birth--death model which captures the\ndynamics of a directly transmitted infectious disease. Our model accounts for\nan important aspect of observed epidemic data in which only symptomatic\ninfecteds are observed. The nonhomogeneous birth--death process depends on\nsurvival distributions of reproduction and removal, which jointly yield an\nestimate of the effective reproduction number $R(t)$ as a function of epidemic\ntime. We employ the Burr distribution family for the survival functions and, as\nspecial cases, proportional rate and accelerated event-time models are also\nemployed for the parameter estimation procedure. As an example, our model is\napplied to an outbreak of avian influenza (H7N7) in the Netherlands, 2003,\nconfirming that the conditional estimate of $R(t)$ declined below unity for the\nfirst time on day 23 since the detection of the index case.\n", "versions": [{"version": "v1", "created": "Wed, 22 Sep 2010 13:51:07 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Broek", "Jan van den", ""], ["Nishiura", "Hiroshi", ""]]}, {"id": "1009.4409", "submitter": "Boris Oreshkin N.", "authors": "Boris N. Oreshkin, Xuan Liu and Mark J. Coates", "title": "Efficient delay-tolerant particle filtering", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2011.2140110", "report-no": null, "categories": "stat.AP cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel framework for delay-tolerant particle filtering\nthat is computationally efficient and has limited memory requirements. Within\nthis framework the informativeness of a delayed (out-of-sequence) measurement\n(OOSM) is estimated using a lightweight procedure and uninformative\nmeasurements are immediately discarded. The framework requires the\nidentification of a threshold that separates informative from uninformative;\nthis threshold selection task is formulated as a constrained optimization\nproblem, where the goal is to minimize tracking error whilst controlling the\ncomputational requirements. We develop an algorithm that provides an\napproximate solution for the optimization problem. Simulation experiments\nprovide an example where the proposed framework processes less than 40% of all\nOOSMs with only a small reduction in tracking accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 22 Sep 2010 16:22:41 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Oreshkin", "Boris N.", ""], ["Liu", "Xuan", ""], ["Coates", "Mark J.", ""]]}, {"id": "1009.5173", "submitter": "Laurent Jacob", "authors": "Laurent Jacob, Pierre Neuvial, Sandrine Dudoit", "title": "Gains in Power from Structured Two-Sample Tests of Means on Graphs", "comments": null, "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 2, 561-600", "doi": "10.1214/11-AOAS528", "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider multivariate two-sample tests of means, where the location shift\nbetween the two populations is expected to be related to a known graph\nstructure. An important application of such tests is the detection of\ndifferentially expressed genes between two patient populations, as shifts in\nexpression levels are expected to be coherent with the structure of graphs\nreflecting gene properties such as biological process, molecular function,\nregulation, or metabolism. For a fixed graph of interest, we demonstrate that\naccounting for graph structure can yield more powerful tests under the\nassumption of smooth distribution shift on the graph. We also investigate the\nidentification of non-homogeneous subgraphs of a given large graph, which poses\nboth computational and multiple testing problems. The relevance and benefits of\nthe proposed approach are illustrated on synthetic data and on breast cancer\ngene expression data analyzed in context of KEGG pathways.\n", "versions": [{"version": "v1", "created": "Mon, 27 Sep 2010 07:21:22 GMT"}], "update_date": "2014-05-16", "authors_parsed": [["Jacob", "Laurent", ""], ["Neuvial", "Pierre", ""], ["Dudoit", "Sandrine", ""]]}, {"id": "1009.5177", "submitter": "Julien Bect", "authors": "Julien Bect and David Ginsbourger and Ling Li and Victor Picheny and\n  Emmanuel Vazquez", "title": "Sequential design of computer experiments for the estimation of a\n  probability of failure", "comments": "This is an author-generated postprint version. The published version\n  is available at http://www.springerlink.com", "journal-ref": "Statistics and Computing, 22(3):773-793, 2012", "doi": "10.1007/s11222-011-9241-4", "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the problem of estimating the volume of the excursion\nset of a function $f:\\mathbb{R}^d \\to \\mathbb{R}$ above a given threshold,\nunder a probability measure on $\\mathbb{R}^d$ that is assumed to be known. In\nthe industrial world, this corresponds to the problem of estimating a\nprobability of failure of a system. When only an expensive-to-simulate model of\nthe system is available, the budget for simulations is usually severely limited\nand therefore classical Monte Carlo methods ought to be avoided. One of the\nmain contributions of this article is to derive SUR (stepwise uncertainty\nreduction) strategies from a Bayesian-theoretic formulation of the problem of\nestimating a probability of failure. These sequential strategies use a Gaussian\nprocess model of $f$ and aim at performing evaluations of $f$ as efficiently as\npossible to infer the value of the probability of failure. We compare these\nstrategies to other strategies also based on a Gaussian process model for\nestimating a probability of failure.\n", "versions": [{"version": "v1", "created": "Mon, 27 Sep 2010 07:41:59 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2012 20:02:53 GMT"}], "update_date": "2012-04-26", "authors_parsed": [["Bect", "Julien", ""], ["Ginsbourger", "David", ""], ["Li", "Ling", ""], ["Picheny", "Victor", ""], ["Vazquez", "Emmanuel", ""]]}, {"id": "1009.5520", "submitter": "Sandor Soos", "authors": "Sandor Soos (1), George Kampis (2) ((1) Institute for Research Policy\n  Studies, Hungarian Academy of Sciences, Hungary, (2) History and Philosophy\n  of Science, Lorand Eotvos University, Hungary)", "title": "Diversity and Polarization of Research Performance: Evidence from\n  Hungary", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring the intellectual diversity encoded in publication records as a\nproxy to the degree of interdisciplinarity has recently received considerable\nattention in the science mapping community. The present paper draws upon the\nuse of the Stirling index as a diversity measure applied to a network model\n(customized science map) of research profiles, proposed by several authors. A\nmodified version of the index is used and compared with the previous versions\non a sample data set in order to rank top Hungarian research organizations\n(HROs) according to their research performance diversity. Results, unexpected\nin several respects, show that the modified index is a candidate for measuring\nthe degree of polarization of a research profile. The study also points towards\na possible typology of publication portfolios that instantiate different types\nof diversity.\n", "versions": [{"version": "v1", "created": "Tue, 28 Sep 2010 10:20:21 GMT"}], "update_date": "2010-09-29", "authors_parsed": [["Soos", "Sandor", ""], ["Kampis", "George", ""]]}, {"id": "1009.5664", "submitter": "Lutz Mattner", "authors": "Lutz Mattner and Frauke Mattner", "title": "Confidence bounds for the sensitivity lack of a less specific diagnostic\n  test, without gold standard", "comments": "27 pages. Final version to appear in Metrika. Some typos corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of comparing two diagnostic tests based on a sample\nof paired test results without true state determinations, in cases where the\nsecond test can reasonably be assumed to be at least as specific as the first.\nFor such cases, we provide two informative confidence bounds: A lower one for\nthe prevalence times the sensitivity gain of the second test with respect to\nthe first, and an upper one for the sensitivity of the first test. Neither\nconditional independence of the two tests nor perfectness of any of them needs\nto be assumd.\n  An application of the proposed confidence bounds to a sample of 256 pairs of\nlaboratory test results for toxigenic Clostridium difficile provides evidence\nfor a dramatic sensitivity gain through first appropriately culturing\nClostridium difficile from stool samples before applying an\nenzyme-immuno-assay.\n", "versions": [{"version": "v1", "created": "Tue, 28 Sep 2010 19:44:38 GMT"}, {"version": "v2", "created": "Mon, 28 Feb 2011 15:19:16 GMT"}, {"version": "v3", "created": "Fri, 8 Apr 2011 07:53:11 GMT"}, {"version": "v4", "created": "Sat, 7 Jan 2012 15:46:43 GMT"}, {"version": "v5", "created": "Wed, 18 Jan 2012 10:16:40 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Mattner", "Lutz", ""], ["Mattner", "Frauke", ""]]}, {"id": "1009.5741", "submitter": "Sivan Aldor-Noiman", "authors": "Sivan Aldor-Noiman, Paul D. Feigin, Avishai Mandelbaum", "title": "Workload forecasting for a call center: Methodology and a case study", "comments": "Published in at http://dx.doi.org/10.1214/09-AOAS255 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 4, 1403-1447", "doi": "10.1214/09-AOAS255", "report-no": "IMS-AOAS-AOAS255", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's call center managers face multiple operational decision-making tasks.\nOne of the most common is determining the weekly staffing levels to ensure\ncustomer satisfaction and meeting their needs while minimizing service costs.\nAn initial step for producing the weekly schedule is forecasting the future\nsystem loads which involves predicting both arrival counts and average service\ntimes. We introduce an arrival count model which is based on a mixed Poisson\nprocess approach. The model is applied to data from an Israeli Telecom company\ncall center. In our model, we also consider the effect of events such as\nbilling on the arrival process and we demonstrate how to incorporate them as\nexogenous variables in the model. After obtaining the forecasted system load,\nin large call centers, a manager can choose to apply the QED\n(Quality-Efficiency Driven) regime's \"square-root staffing\" rule in order to\nbalance the offered-load per server with the quality of service. Implementing\nthis staffing rule requires that the forecasted values of the arrival counts\nand average service times maintain certain levels of precision. We develop\ndifferent goodness of fit criteria that help determine our model's practical\nperformance under the QED regime. These show that during most hours of the day\nthe model can reach desired precision levels.\n", "versions": [{"version": "v1", "created": "Tue, 28 Sep 2010 13:59:55 GMT"}], "update_date": "2010-09-30", "authors_parsed": [["Aldor-Noiman", "Sivan", ""], ["Feigin", "Paul D.", ""], ["Mandelbaum", "Avishai", ""]]}, {"id": "1009.5742", "submitter": "Yingchun Zhou", "authors": "Yingchun Zhou, Nell Sedransk", "title": "Functional data analytic approach of modeling ECG T-wave shape to\n  measure cardiovascular behavior", "comments": "Published in at http://dx.doi.org/10.1214/09-AOAS273 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 4, 1382-1402", "doi": "10.1214/09-AOAS273", "report-no": "IMS-AOAS-AOAS273", "categories": "stat.AP physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The T-wave of an electrocardiogram (ECG) represents the ventricular\nrepolarization that is critical in restoration of the heart muscle to a\npre-contractile state prior to the next beat. Alterations in the T-wave reflect\nvarious cardiac conditions; and links between abnormal (prolonged) ventricular\nrepolarization and malignant arrhythmias have been documented. Cardiac safety\ntesting prior to approval of any new drug currently relies on two points of the\nECG waveform: onset of the Q-wave and termination of the T-wave; and only a few\nbeats are measured. Using functional data analysis, a statistical approach\nextracts a common shape for each subject (reference curve) from a sequence of\nbeats, and then models the deviation of each curve in the sequence from that\nreference curve as a four-dimensional vector. The representation can be used to\ndistinguish differences between beats or to model shape changes in a subject's\nT-wave over time. This model provides physically interpretable parameters\ncharacterizing T-wave shape, and is robust to the determination of the endpoint\nof the T-wave. Thus, this dimension reduction methodology offers the strong\npotential for definition of more robust and more informative biomarkers of\ncardiac abnormalities than the QT (or QT corrected) interval in current use.\n", "versions": [{"version": "v1", "created": "Tue, 28 Sep 2010 12:46:05 GMT"}], "update_date": "2010-09-30", "authors_parsed": [["Zhou", "Yingchun", ""], ["Sedransk", "Nell", ""]]}, {"id": "1009.5743", "submitter": "Edward Mulrow", "authors": "Edward Mulrow, Hee-Choon Shin, Fritz Scheuren", "title": "Assessing uncertainty in the American Indian Trust Fund", "comments": "Published in at http://dx.doi.org/10.1214/09-AOAS274 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 4, 1370-1381", "doi": "10.1214/09-AOAS274", "report-no": "IMS-AOAS-AOAS274", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fiscal year-end balances of the Individual Indian Money System (a part of the\nIndian Trust) were constructed from data related to money collected in the\nsystem and disbursed by the system from 1887 to 2007. The data set of fiscal\nyear accounting information had a high proportion of missing values, and much\nof the available data did not satisfy basic accounting relationships. Instead\nof just calculating a single estimate and arguing to the Court that the\nassumptions needed for the computation were reasonable, a distribution of\ncalculated balances was developed using multiple imputation and time series\nmodels. These provided information to assess the uncertainty of the estimate\ndue to missing and questionable data.\n", "versions": [{"version": "v1", "created": "Tue, 28 Sep 2010 11:50:47 GMT"}], "update_date": "2010-09-30", "authors_parsed": [["Mulrow", "Edward", ""], ["Shin", "Hee-Choon", ""], ["Scheuren", "Fritz", ""]]}, {"id": "1009.5744", "submitter": "Herman Chernoff", "authors": "Herman Chernoff, Shaw-Hwa Lo, Tian Zheng", "title": "Discovering influential variables: A method of partitions", "comments": "Published in at http://dx.doi.org/10.1214/09-AOAS265 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 4, 1335-1369", "doi": "10.1214/09-AOAS265", "report-no": "IMS-AOAS-AOAS265", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A trend in all scientific disciplines, based on advances in technology, is\nthe increasing availability of high dimensional data in which are buried\nimportant information. A current urgent challenge to statisticians is to\ndevelop effective methods of finding the useful information from the vast\namounts of messy and noisy data available, most of which are noninformative.\nThis paper presents a general computer intensive approach, based on a method\npioneered by Lo and Zheng for detecting which, of many potential explanatory\nvariables, have an influence on a dependent variable $Y$. This approach is\nsuited to detect influential variables, where causal effects depend on the\nconfluence of values of several variables. It has the advantage of avoiding a\ndifficult direct analysis, involving possibly thousands of variables, by\ndealing with many randomly selected small subsets from which smaller subsets\nare selected, guided by a measure of influence $I$. The main objective is to\ndiscover the influential variables, rather than to measure their effects. Once\nthey are detected, the problem of dealing with a much smaller group of\ninfluential variables should be vulnerable to appropriate analysis. In a sense,\nwe are confining our attention to locating a few needles in a haystack.\n", "versions": [{"version": "v1", "created": "Tue, 28 Sep 2010 11:11:08 GMT"}], "update_date": "2010-09-30", "authors_parsed": [["Chernoff", "Herman", ""], ["Lo", "Shaw-Hwa", ""], ["Zheng", "Tian", ""]]}, {"id": "1009.5745", "submitter": "Edwin S. Iversen Jr.", "authors": "David A. Orlando, Edwin S. Iversen Jr., Alexander J. Hartemink, Steven\n  B. Haase", "title": "A branching process model for flow cytometry and budding index\n  measurements in cell synchrony experiments", "comments": "Published in at http://dx.doi.org/10.1214/09-AOAS264 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 4, 1521-1541", "doi": "10.1214/09-AOAS264", "report-no": "IMS-AOAS-AOAS264", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a flexible branching process model for cell population dynamics in\nsynchrony/time-series experiments used to study important cellular processes.\nIts formulation is constructive, based on an accounting of the unique cohorts\nin the population as they arise and evolve over time, allowing it to be written\nin closed form. The model can attribute effects to subsets of the population,\nproviding flexibility not available using the models historically applied to\nthese populations. It provides a tool for in silico synchronization of the\npopulation and can be used to deconvolve population-level experimental\nmeasurements, such as temporal expression profiles. It also allows for the\ndirect comparison of assay measurements made from multiple experiments. The\nmodel can be fit either to budding index or DNA content measurements, or both,\nand is easily adaptable to new forms of data. The ability to use DNA content\ndata makes the model applicable to almost any organism. We describe the model\nand illustrate its utility and flexibility in a study of cell cycle progression\nin the yeast Saccharomyces cerevisiae.\n", "versions": [{"version": "v1", "created": "Tue, 28 Sep 2010 09:22:58 GMT"}], "update_date": "2010-09-30", "authors_parsed": [["Orlando", "David A.", ""], ["Iversen", "Edwin S.", "Jr."], ["Hartemink", "Alexander J.", ""], ["Haase", "Steven B.", ""]]}, {"id": "1009.5750", "submitter": "Raymond J. Carroll", "authors": "Josue G. Martinez, Jianhua Z. Huang, Robert C. Burghardt, Rola\n  Barhoumi, Raymond J. Carroll", "title": "Use of multiple singular value decompositions to analyze complex\n  intracellular calcium ion signals", "comments": "Published in at http://dx.doi.org/10.1214/09-AOAS253 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 4, 1467-1492", "doi": "10.1214/09-AOAS253", "report-no": "IMS-AOAS-AOAS253", "categories": "stat.AP cs.CV physics.bio-ph q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare calcium ion signaling ($\\mathrm {Ca}^{2+}$) between two exposures;\nthe data are present as movies, or, more prosaically, time series of images.\nThis paper describes novel uses of singular value decompositions (SVD) and\nweighted versions of them (WSVD) to extract the signals from such movies, in a\nway that is semi-automatic and tuned closely to the actual data and their many\ncomplexities. These complexities include the following. First, the images\nthemselves are of no interest: all interest focuses on the behavior of\nindividual cells across time, and thus, the cells need to be segmented in an\nautomated manner. Second, the cells themselves have 100$+$ pixels, so that they\nform 100$+$ curves measured over time, so that data compression is required to\nextract the features of these curves. Third, some of the pixels in some of the\ncells are subject to image saturation due to bit depth limits, and this\nsaturation needs to be accounted for if one is to normalize the images in a\nreasonably unbiased manner. Finally, the $\\mathrm {Ca}^{2+}$ signals have\noscillations or waves that vary with time and these signals need to be\nextracted. Thus, our aim is to show how to use multiple weighted and standard\nsingular value decompositions to detect, extract and clarify the $\\mathrm\n{Ca}^{2+}$ signals. Our signal extraction methods then lead to simple although\nfinely focused statistical methods to compare $\\mathrm {Ca}^{2+}$ signals\nacross experimental conditions.\n", "versions": [{"version": "v1", "created": "Tue, 28 Sep 2010 09:16:19 GMT"}], "update_date": "2010-09-30", "authors_parsed": [["Martinez", "Josue G.", ""], ["Huang", "Jianhua Z.", ""], ["Burghardt", "Robert C.", ""], ["Barhoumi", "Rola", ""], ["Carroll", "Raymond J.", ""]]}, {"id": "1009.5778", "submitter": "Sarat C. Dass", "authors": "Sarat C. Dass, Mingfei Li", "title": "Hierarchical mixture models for assessing fingerprint individuality", "comments": "Published in at http://dx.doi.org/10.1214/09-AOAS266 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 4, 1448-1466", "doi": "10.1214/09-AOAS266", "report-no": "IMS-AOAS-AOAS266", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of fingerprint individuality aims to determine to what extent a\nfingerprint uniquely identifies an individual. Recent court cases have\nhighlighted the need for measures of fingerprint individuality when a person is\nidentified based on fingerprint evidence. The main challenge in studies of\nfingerprint individuality is to adequately capture the variability of\nfingerprint features in a population. In this paper hierarchical mixture models\nare introduced to infer the extent of individualization. Hierarchical mixtures\nutilize complementary aspects of mixtures at different levels of the hierarchy.\nAt the first (top) level, a mixture is used to represent homogeneous groups of\nfingerprints in the population, whereas at the second level, nested mixtures\nare used as flexible representations of distributions of features from each\nfingerprint. Inference for hierarchical mixtures is more challenging since the\nnumber of unknown mixture components arise in both the first and second levels\nof the hierarchy. A Bayesian approach based on reversible jump Markov chain\nMonte Carlo methodology is developed for the inference of all unknown\nparameters of hierarchical mixtures. The methodology is illustrated on\nfingerprint images from the NIST database and is used to make inference on\nfingerprint individuality estimates from this population.\n", "versions": [{"version": "v1", "created": "Wed, 29 Sep 2010 06:05:19 GMT"}], "update_date": "2010-09-30", "authors_parsed": [["Dass", "Sarat C.", ""], ["Li", "Mingfei", ""]]}, {"id": "1009.5785", "submitter": "Chao A. Hsiung", "authors": "Li-Chu Chien, I-Shou Chang, Shih Sheng Jiang, Pramod K. Gupta,\n  Chi-Chung Wen, Yuh-Jenn Wu, Chao A. Hsiung", "title": "Profiling time course expression of virus genes---an illustration of\n  Bayesian inference under shape restrictions", "comments": "Published in at http://dx.doi.org/10.1214/09-AOAS258 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 4, 1542-1565", "doi": "10.1214/09-AOAS258", "report-no": "IMS-AOAS-AOAS258", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There have been several studies of the genome-wide temporal transcriptional\nprogram of viruses, based on microarray experiments, which are generally useful\nin the construction of gene regulation network. It seems that biological\ninterpretations in these studies are directly based on the normalized data and\nsome crude statistics, which provide rough estimates of limited features of the\nprofile and may incur biases. This paper introduces a hierarchical Bayesian\nshape restricted regression method for making inference on the time course\nexpression of virus genes. Estimates of many salient features of the expression\nprofile like onset time, inflection point, maximum value, time to maximum\nvalue, area under curve, etc. can be obtained immediately by this method.\nApplying this method to a baculovirus microarray time course expression data\nset, we indicate that many biological questions can be formulated\nquantitatively and we are able to offer insights into the baculovirus biology.\n", "versions": [{"version": "v1", "created": "Wed, 29 Sep 2010 06:57:37 GMT"}], "update_date": "2010-09-30", "authors_parsed": [["Chien", "Li-Chu", ""], ["Chang", "I-Shou", ""], ["Jiang", "Shih Sheng", ""], ["Gupta", "Pramod K.", ""], ["Wen", "Chi-Chung", ""], ["Wu", "Yuh-Jenn", ""], ["Hsiung", "Chao A.", ""]]}, {"id": "1009.5818", "submitter": "Michiel Debruyne", "authors": "Michiel Debruyne", "title": "An outlier map for Support Vector Machine classification", "comments": "Published in at http://dx.doi.org/10.1214/09-AOAS256 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 4, 1566-1580", "doi": "10.1214/09-AOAS256", "report-no": "IMS-AOAS-AOAS256", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support Vector Machines are a widely used classification technique. They are\ncomputationally efficient and provide excellent predictions even for\nhigh-dimensional data. Moreover, Support Vector Machines are very flexible due\nto the incorporation of kernel functions. The latter allow to model\nnonlinearity, but also to deal with nonnumerical data such as protein strings.\nHowever, Support Vector Machines can suffer a lot from unclean data containing,\nfor example, outliers or mislabeled observations. Although several outlier\ndetection schemes have been proposed in the literature, the selection of\noutliers versus nonoutliers is often rather ad hoc and does not provide much\ninsight in the data. In robust multivariate statistics outlier maps are quite\npopular tools to assess the quality of data under consideration. They provide a\nvisual representation of the data depicting several types of outliers. This\npaper proposes an outlier map designed for Support Vector Machine\nclassification. The Stahel--Donoho outlyingness measure from multivariate\nstatistics is extended to an arbitrary kernel space. A trimmed version of\nSupport Vector Machines is defined trimming part of the samples with largest\noutlyingness. Based on this classifier, an outlier map is constructed\nvisualizing data in any type of high-dimensional kernel space. The outlier map\nis illustrated on 4 biological examples showing its use in exploratory data\nanalysis.\n", "versions": [{"version": "v1", "created": "Wed, 29 Sep 2010 09:34:00 GMT"}], "update_date": "2010-09-30", "authors_parsed": [["Debruyne", "Michiel", ""]]}, {"id": "1009.5854", "submitter": "Iain M. Johnstone", "authors": "Iain M. Johnstone", "title": "Approximate null distribution of the largest root in multivariate\n  analysis", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS220 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 4, 1616-1633", "doi": "10.1214/08-AOAS220", "report-no": "IMS-AOAS-AOAS220", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The greatest root distribution occurs everywhere in classical multivariate\nanalysis, but even under the null hypothesis the exact distribution has\nrequired extensive tables or special purpose software. We describe a simple\napproximation, based on the Tracy--Widom distribution, that in many cases can\nbe used instead of tables or software, at least for initial screening. The\nquality of approximation is studied, and its use illustrated in a variety of\nsetttings.\n", "versions": [{"version": "v1", "created": "Wed, 29 Sep 2010 11:55:00 GMT"}], "update_date": "2010-09-30", "authors_parsed": [["Johnstone", "Iain M.", ""]]}, {"id": "1009.5861", "submitter": "Xuming He", "authors": "Xingdong Feng, Xuming He", "title": "Inference on low-rank data matrices with applications to microarray data", "comments": "Published in at http://dx.doi.org/10.1214/09-AOAS262 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 4, 1634-1654", "doi": "10.1214/09-AOAS262", "report-no": "IMS-AOAS-AOAS262", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probe-level microarray data are usually stored in matrices, where the row and\ncolumn correspond to array and probe, respectively. Scientists routinely\nsummarize each array by a single index as the expression level of each probe\nset (gene). We examine the adequacy of a unidimensional summary for\ncharacterizing the data matrix of each probe set. To do so, we propose a\nlow-rank matrix model for the probe-level intensities, and develop a useful\nframework for testing the adequacy of unidimensionality against targeted\nalternatives. This is an interesting statistical problem where inference has to\nbe made based on one data matrix whose entries are not i.i.d. We analyze the\nasymptotic properties of the proposed test statistics, and use Monte Carlo\nsimulations to assess their small sample performance. Applications of the\nproposed tests to GeneChip data show that evidence against a unidimensional\nmodel is often indicative of practically relevant features of a probe set.\n", "versions": [{"version": "v1", "created": "Wed, 29 Sep 2010 12:24:10 GMT"}], "update_date": "2010-09-30", "authors_parsed": [["Feng", "Xingdong", ""], ["He", "Xuming", ""]]}, {"id": "1009.5869", "submitter": "James G. Scott", "authors": "James G. Scott", "title": "Nonparametric Bayesian multiple testing for longitudinal performance\n  stratification", "comments": "Published in at http://dx.doi.org/10.1214/09-AOAS252 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 4, 1655-1674", "doi": "10.1214/09-AOAS252", "report-no": "IMS-AOAS-AOAS252", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a framework for flexible multiple hypothesis testing of\nautoregressive time series. The modeling approach is Bayesian, though a blend\nof frequentist and Bayesian reasoning is used to evaluate procedures.\nNonparametric characterizations of both the null and alternative hypotheses\nwill be shown to be the key robustification step necessary to ensure reasonable\nType-I error performance. The methodology is applied to part of a large\ndatabase containing up to 50 years of corporate performance statistics on\n24,157 publicly traded American companies, where the primary goal of the\nanalysis is to flag companies whose historical performance is significantly\ndifferent from that expected due to chance.\n", "versions": [{"version": "v1", "created": "Wed, 29 Sep 2010 12:45:35 GMT"}], "update_date": "2010-09-30", "authors_parsed": [["Scott", "James G.", ""]]}]