[{"id": "1202.0048", "submitter": "Jonathan Tuke", "authors": "J. Tuke, G. F. V. Glonek, and P. J. Solomon", "title": "P-values, q-values and posterior probabilities for equivalence in\n  genomics studies", "comments": "26 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST q-bio.QM stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Equivalence testing is of emerging importance in genomics studies but has\nhitherto been little studied in this content. In this paper, we define the\nnotion of equivalence of gene expression and determine a `strength of evidence'\nmeasure for gene equivalence. It is common practice in genome-wide studies to\nrank genes according to observed gene-specific P-values or adjusted P-values,\nwhich are assumed to measure the strength of evidence against the null\nhypothesis of no differential gene expression. We show here, both empirically\nand formally, that the equivalence P-value does not satisfy the basic\nconsistency requirements for a valid strength of evidence measure for\nequivalence. This means that the widely-used q-value (Storey, 2002) defined for\neach gene to be the minimum positive false discovery rate that would result in\nthe inclusion of the corresponding P-value in the discovery set, cannot be\ntranslated to the equivalence testing framework. However, when represented as a\nposterior probability, we find that the q-value does satisfy some basic\nconsistency requirements needed to be a credible measure of evidence for\nequivalence. We propose a simple estimate for the q-value from posterior\nprobabilities of equivalence, and analyse data from a mouse stem cell\nmicroarray experiment which demonstrate the theory and methods presented here.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2012 23:10:04 GMT"}], "update_date": "2012-02-03", "authors_parsed": [["Tuke", "J.", ""], ["Glonek", "G. F. V.", ""], ["Solomon", "P. J.", ""]]}, {"id": "1202.0055", "submitter": "Sergiy Vorobyov A.", "authors": "Aboulnasr Hassanien, Sergiy A. Vorobyov, and Alex B. Gershman", "title": "Moving Target Parameters Estimation in Non-Coherent MIMO Radar Systems", "comments": "17 pages, 4 figures, Submitted to the IEEE Trans. Signal Processing\n  in Aug. 2011", "journal-ref": "A. Hassanien, S.A. Vorobyov, and A.B. Gershman, \"Moving target\n  parameters estimation in non-coherent MIMO radar systems,\" IEEE Trans. Signal\n  Processing, vol. 60, no. 5, pp. 2354-2361, May 2012", "doi": "10.1109/TSP.2012.2187290", "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of estimating the parameters of a moving target in multiple-input\nmultiple-output (MIMO) radar is considered and a new approach for estimating\nthe moving target parameters by making use of the phase information associated\nwith each transmit-receive path is introduced. It is required for this\ntechnique that different receive antennas have the same time reference, but no\nsynchronization of initial phases of the receive antennas is needed and,\ntherefore, the estimation process is non-coherent. We model the target motion\nwithin a certain processing interval as a polynomial of general order. The\nfirst three coefficients of such a polynomial correspond to the initial\nlocation, velocity, and acceleration of the target, respectively. A new maximum\nlikelihood (ML) technique for estimating the target motion coefficients is\ndeveloped. It is shown that the considered ML problem can be interpreted as the\nclassic \"overdetermined\" nonlinear least-squares problem. The proposed ML\nestimator requires multi-dimensional search over the unknown polynomial\ncoefficients. The Cram\\'er-Rao Bound (CRB) for the proposed parameter\nestimation problem is derived. The performance of the proposed estimator is\nvalidated by simulation results and is shown to achieve the CRB.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2012 00:03:08 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Hassanien", "Aboulnasr", ""], ["Vorobyov", "Sergiy A.", ""], ["Gershman", "Alex B.", ""]]}, {"id": "1202.0373", "submitter": "Yue Yu", "authors": "Yue Yu, Zhijie Sun", "title": "Partial Sliced Inverse Regression for Quality-Relevant Multivariate\n  Statistical Process Monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a popular dimension reduction method, sliced inverse\nregression (SIR), into multivariate statistical process monitoring. Provides an\nextension of SIR for the single-index model by adopting the idea from partial\nleast squares (PLS). Our partial sliced inverse regression (PSIR) method has\nthe merit of incorporating information from both predictors (x) and responses\n(y), and it has capability of handling large, nonlinear, or \"n<p\" dataset. Two\nstatistics with their corresponding distributions and control limits are given\nbased on the X-space decomposition of PSIR for the purpose of fault detection\nin process monitoring. Simulations showed PSIR outperformed over PLS and SIR\nfor both linear and nonlinear model.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2012 06:55:44 GMT"}], "update_date": "2012-02-03", "authors_parsed": [["Yu", "Yue", ""], ["Sun", "Zhijie", ""]]}, {"id": "1202.0500", "submitter": "Matthew Salganik", "authors": "Matthew J. Salganik and Karen E. C. Levy", "title": "Wiki surveys: Open and quantifiable social data collection", "comments": "24 pages, 8 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the social sciences, there is a longstanding tension between data\ncollection methods that facilitate quantification and those that are open to\nunanticipated information. Advances in technology now enable new, hybrid\nmethods that combine some of the benefits of both approaches. Drawing\ninspiration from online information aggregation systems like Wikipedia and from\ntraditional survey research, we propose a new class of research instruments\ncalled wiki surveys. Just as Wikipedia evolves over time based on contributions\nfrom participants, we envision an evolving survey driven by contributions from\nrespondents. We develop three general principles that underlie wiki surveys:\nthey should be greedy, collaborative, and adaptive. Building on these\nprinciples, we develop methods for data collection and data analysis for one\ntype of wiki survey, a pairwise wiki survey. Using two proof-of-concept case\nstudies involving our free and open-source website www.allourideas.org, we show\nthat pairwise wiki surveys can yield insights that would be difficult to obtain\nwith other methods.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2012 17:36:49 GMT"}, {"version": "v2", "created": "Thu, 2 Oct 2014 02:54:11 GMT"}], "update_date": "2014-10-03", "authors_parsed": [["Salganik", "Matthew J.", ""], ["Levy", "Karen E. C.", ""]]}, {"id": "1202.0501", "submitter": "Leo Lahti", "authors": "Leo Lahti, Juha E. A. Knuuttila, Samuel Kaski", "title": "Global modeling of transcriptional responses in interaction networks", "comments": "19 pages, 13 figures", "journal-ref": "Global modeling of transcriptional responses in interaction\n  networks. Leo Lahti, Juha E.A. Knuuttila, and Samuel Kaski. Bioinformatics\n  26(21):2713-2720, 2010", "doi": "10.1093/bioinformatics/btq500", "report-no": null, "categories": "q-bio.MN cs.CE q-bio.QM stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Motivation: Cell-biological processes are regulated through a complex network\nof interactions between genes and their products. The processes, their\nactivating conditions, and the associated transcriptional responses are often\nunknown. Organism-wide modeling of network activation can reveal unique and\nshared mechanisms between physiological conditions, and potentially as yet\nunknown processes. We introduce a novel approach for organism-wide discovery\nand analysis of transcriptional responses in interaction networks. The method\nsearches for local, connected regions in a network that exhibit coordinated\ntranscriptional response in a subset of conditions. Known interactions between\ngenes are used to limit the search space and to guide the analysis. Validation\non a human pathway network reveals physiologically coherent responses,\nfunctional relatedness between physiological conditions, and coordinated,\ncontext-specific regulation of the genes. Availability: Implementation is\nfreely available in R and Matlab at http://netpro.r-forge.r-project.org\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2012 17:40:14 GMT"}], "update_date": "2012-02-03", "authors_parsed": [["Lahti", "Leo", ""], ["Knuuttila", "Juha E. A.", ""], ["Kaski", "Samuel", ""]]}, {"id": "1202.0517", "submitter": "Zuofeng Shang", "authors": "Zuofeng Shang and Murray K. Clayton", "title": "An Application of Bayesian Variable Selection to Spatial Concurrent\n  Linear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial concurrent linear models, in which the model coefficients are spatial\nprocesses varying at a local level, are flexible and useful tools for analyzing\nspatial data. One approach places stationary Gaussian process priors on the\nspatial processes, but in applications the data may display strong\nnonstationary patterns. In this article, we propose a Bayesian variable\nselection approach based on wavelet tools to address this problem. The proposed\napproach does not involve any stationarity assumptions on the priors, and\ninstead we impose a mixture prior directly on each wavelet coefficient. We\nintroduce an option to control the priors such that high resolution\ncoefficients are more likely to be zero. Computationally efficient MCMC\nprocedures are provided to address posterior sampling, and uncertainty in the\nestimation is assessed through posterior means and standard deviations.\nExamples based on simulated data demonstrate the estimation accuracy and\nadvantages of the proposed method. We also illustrate the performance of the\nproposed method for real data obtained through remote sensing.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2012 19:10:19 GMT"}], "update_date": "2012-02-03", "authors_parsed": [["Shang", "Zuofeng", ""], ["Clayton", "Murray K.", ""]]}, {"id": "1202.0836", "submitter": "Gael Varoquaux", "authors": "Ga\\\"el Varoquaux (LNAO, INRIA Saclay - Ile de France), Alexandre\n  Gramfort (LNAO, INRIA Saclay - Ile de France), Jean Baptiste Poline (LNAO,\n  INRIA Saclay - Ile de France), Bertrand Thirion (LNAO, INRIA Saclay - Ile de\n  France)", "title": "Markov models for fMRI correlation structure: is brain functional\n  connectivity small world, or decomposable into networks?", "comments": null, "journal-ref": "Journal of Physiology - Paris (2012)", "doi": "10.1016/j.jphysparis.2012.01.001", "report-no": null, "categories": "stat.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlations in the signal observed via functional Magnetic Resonance Imaging\n(fMRI), are expected to reveal the interactions in the underlying neural\npopulations through hemodynamic response. In particular, they highlight\ndistributed set of mutually correlated regions that correspond to brain\nnetworks related to different cognitive functions. Yet graph-theoretical\nstudies of neural connections give a different picture: that of a highly\nintegrated system with small-world properties: local clustering but with short\npathways across the complete structure. We examine the conditional independence\nproperties of the fMRI signal, i.e. its Markov structure, to find realistic\nassumptions on the connectivity structure that are required to explain the\nobserved functional connectivity. In particular we seek a decomposition of the\nMarkov structure into segregated functional networks using decomposable graphs:\na set of strongly-connected and partially overlapping cliques. We introduce a\nnew method to efficiently extract such cliques on a large, strongly-connected\ngraph. We compare methods learning different graph structures from functional\nconnectivity by testing the goodness of fit of the model they learn on new\ndata. We find that summarizing the structure as strongly-connected networks can\ngive a good description only for very large and overlapping networks. These\nresults highlight that Markov models are good tools to identify the structure\nof brain connectivity from fMRI signals, but for this purpose they must reflect\nthe small-world properties of the underlying neural systems.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2012 22:31:39 GMT"}], "update_date": "2012-02-07", "authors_parsed": [["Varoquaux", "Ga\u00ebl", "", "LNAO, INRIA Saclay - Ile de France"], ["Gramfort", "Alexandre", "", "LNAO, INRIA Saclay - Ile de France"], ["Poline", "Jean Baptiste", "", "LNAO,\n  INRIA Saclay - Ile de France"], ["Thirion", "Bertrand", "", "LNAO, INRIA Saclay - Ile de\n  France"]]}, {"id": "1202.0944", "submitter": "Michel Broniatowski", "authors": "Michel Broniatowski (LSTA), Virgile Caron (LSTA)", "title": "Conditional inference in parametric models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new approach to conditional inference, based on the\nsimulation of samples conditioned by a statistics of the data. Also an explicit\nexpression for the approximation of the conditional likelihood of long runs of\nthe sample given the observed statistics is provided. It is shown that when the\nconditioning statistics is sufficient for a given parameter, the approximating\ndensity is still invariant with respect to the parameter. A new\nRao-Blackwellisation procedure is proposed and simulation shows that Lehmann\nScheff\\'{e} Theorem is valid for this approximation. Conditional inference for\nexponential families with nuisance parameter is also studied, leading to Monte\ncarlo tests. Finally the estimation of the parameter of interest through\nconditional likelihood is considered. Comparison with the parametric bootstrap\nmethod is discussed.\n", "versions": [{"version": "v1", "created": "Sun, 5 Feb 2012 07:35:12 GMT"}], "update_date": "2012-02-07", "authors_parsed": [["Broniatowski", "Michel", "", "LSTA"], ["Caron", "Virgile", "", "LSTA"]]}, {"id": "1202.1425", "submitter": "Lotfi Chaari", "authors": "Lotfi Chaari and Thomas Vincent and Florence Forbes and Michel Dojat\n  and Philippe Ciuciu", "title": "Fast joint detection-estimation of evoked brain activity in\n  event-related fMRI using a variational approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In standard clinical within-subject analyses of event-related fMRI data, two\nsteps are usually performed separately: detection of brain activity and\nestimation of the hemodynamic response. Because these two steps are inherently\nlinked, we adopt the so-called region-based Joint Detection-Estimation (JDE)\nframework that addresses this joint issue using a multivariate inference for\ndetection and estimation. JDE is built by making use of a regional bilinear\ngenerative model of the BOLD response and constraining the parameter estimation\nby physiological priors using temporal and spatial information in a Markovian\nmodeling. In contrast to previous works that use Markov Chain Monte Carlo\n(MCMC) techniques to approximate the resulting intractable posterior\ndistribution, we recast the JDE into a missing data framework and derive a\nVariational Expectation-Maximization (VEM) algorithm for its inference. A\nvariational approximation is used to approximate the Markovian model in the\nunsupervised spatially adaptive JDE inference, which allows fine automatic\ntuning of spatial regularisation parameters. It follows a new algorithm that\nexhibits interesting properties compared to the previously used MCMC-based\napproach. Experiments on artificial and real data show that VEM-JDE is robust\nto model mis-specification and provides computational gain while maintaining\ngood performance in terms of activation detection and hemodynamic shape\nrecovery.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2012 13:53:08 GMT"}], "update_date": "2012-02-08", "authors_parsed": [["Chaari", "Lotfi", ""], ["Vincent", "Thomas", ""], ["Forbes", "Florence", ""], ["Dojat", "Michel", ""], ["Ciuciu", "Philippe", ""]]}, {"id": "1202.1561", "submitter": "Yong Wang", "authors": "Yong Wang, Ilze Ziedins, Mark Holmes, Neil Challands", "title": "Tree models for difference and change detection in a complex environment", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS548 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 3, 1162-1184", "doi": "10.1214/12-AOAS548", "report-no": "IMS-AOAS-AOAS548", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new family of tree models is proposed, which we call \"differential trees.\"\nA differential tree model is constructed from multiple data sets and aims to\ndetect distributional differences between them. The new methodology differs\nfrom the existing difference and change detection techniques in its\nnonparametric nature, model construction from multiple data sets, and\napplicability to high-dimensional data. Through a detailed study of an arson\ncase in New Zealand, where an individual is known to have been laying\nvegetation fires within a certain time period, we illustrate how these models\ncan help detect changes in the frequencies of event occurrences and uncover\nunusual clusters of events in a complex environment.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2012 23:28:39 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2012 13:19:50 GMT"}], "update_date": "2012-10-01", "authors_parsed": [["Wang", "Yong", ""], ["Ziedins", "Ilze", ""], ["Holmes", "Mark", ""], ["Challands", "Neil", ""]]}, {"id": "1202.1696", "submitter": "Max Hinne", "authors": "M. Hinne, T. Heskes, M. A. J. van Gerven", "title": "Bayesian Inference of Whole-Brain Networks", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In structural brain networks the connections of interest consist of\nwhite-matter fibre bundles between spatially segregated brain regions. The\npresence, location and orientation of these white matter tracts can be derived\nusing diffusion MRI in combination with probabilistic tractography.\nUnfortunately, as of yet no approaches have been suggested that provide an\nundisputed way of inferring brain networks from tractography. In this paper, we\nprovide a computational framework which we refer to as Bayesian connectomics.\nRather than applying an arbitrary threshold to obtain a single network, we\nconsider the posterior distribution of networks that are supported by the data,\ncombined with an exponential random graph (ERGM) prior that captures a priori\nknowledge concerning the graph-theoretical properties of whole-brain networks.\nWe show that, on simulated probabilistic tractography data, our approach is\nable to reconstruct whole-brain networks. In addition, our approach directly\nsupports multi-model data fusion and group-level network inference.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2012 13:52:05 GMT"}], "update_date": "2012-02-09", "authors_parsed": [["Hinne", "M.", ""], ["Heskes", "T.", ""], ["van Gerven", "M. A. J.", ""]]}, {"id": "1202.2009", "submitter": "Jakob Stoeber", "authors": "Jakob Stoeber and Claudia Czado", "title": "Detecting regime switches in the dependence structure of high\n  dimensional financial data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Misperceptions about extreme dependencies between different financial assets\nhave been an im- portant element of the recent financial crisis. This paper\nstudies inhomogeneity in dependence structures using Markov switching regular\nvine copulas. These account for asymmetric depen- dencies and tail dependencies\nin high dimensional data. We develop methods for fast maximum likelihood as\nwell as Bayesian inference. Our algorithms are validated in simulations and\napplied to financial data. We find that regime switches are present in the\ndependence structure of various data sets and show that regime switching models\ncould provide tools for the accurate description of inhomogeneity during times\nof crisis.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2012 14:58:47 GMT"}], "update_date": "2012-02-10", "authors_parsed": [["Stoeber", "Jakob", ""], ["Czado", "Claudia", ""]]}, {"id": "1202.2370", "submitter": "Burcu Aydin", "authors": "Burcu Ayd{\\i}n, G\\'abor Pataki, Haonan Wang, Alim Ladha, Elizabeth\n  Bullitt, J.S. Marron", "title": "New Approaches to Principal Component Analysis for Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object Oriented Data Analysis is a new area in statistics that studies\npopulations of general data objects. In this article we consider populations of\ntree-structured objects as our focus of interest. We develop improved analysis\ntools for data lying in a binary tree space analogous to classical Principal\nComponent Analysis methods in Euclidean space. Our extensions of PCA are\nanalogs of one dimensional subspaces that best fit the data. Previous work was\nbased on the notion of tree-lines.\n  In this paper, a generalization of the previous tree-line notion is proposed:\nk-tree-lines. Previously proposed tree-lines are k-tree-lines where k=1. New\nsub-cases of k-tree-lines studied in this work are the 2-tree-lines and\ntree-curves, which explain much more variation per principal component than\ntree-lines. The optimal principal component tree-lines were computable in\nlinear time. Because 2-tree-lines and tree-curves are more complex, they are\ncomputationally more expensive, but yield improved data analysis results.\n  We provide a comparative study of all these methods on a motivating data set\nconsisting of brain vessel structures of 98 subjects.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2012 21:04:33 GMT"}], "update_date": "2012-02-14", "authors_parsed": [["Ayd\u0131n", "Burcu", ""], ["Pataki", "G\u00e1bor", ""], ["Wang", "Haonan", ""], ["Ladha", "Alim", ""], ["Bullitt", "Elizabeth", ""], ["Marron", "J. S.", ""]]}, {"id": "1202.2559", "submitter": "Salima El Kolei", "authors": "Salima El Kolei", "title": "Parametric estimation of hidden stochastic model by contrast\n  minimization and deconvolution: application to the Stochastic Volatility\n  Model", "comments": "46 pages", "journal-ref": "Metrika, journal 184 article 430, 2013", "doi": "10.1007/s00184-013-0430-3.", "report-no": null, "categories": "stat.AP q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a new parametric approach for particular hidden stochastic models\nsuch as the Stochastic Volatility model. This method is based on contrast\nminimization and deconvolution. After proving consistency and asymptotic\nnormality of the estimation leading to asymptotic confidence intervals, we\nprovide a thorough numerical study, which compares most of the classical\nmethods that are used in practice (Quasi Maximum Likelihood estimator,\nSimulated Expectation Maximization Likelihood estimator and Bayesian\nestimators). We prove that our estimator clearly outperforms the Maximum\nLikelihood Estimator in term of computing time, but also most of the other\nmethods. We also show that this contrast method is the most robust with respect\nto non Gaussianity of the error and also does not need any tuning parameter.\n", "versions": [{"version": "v1", "created": "Sun, 12 Feb 2012 19:27:37 GMT"}, {"version": "v2", "created": "Thu, 14 Mar 2013 19:06:33 GMT"}], "update_date": "2013-03-15", "authors_parsed": [["Kolei", "Salima El", ""]]}, {"id": "1202.2688", "submitter": "Arni S.R. Srinivasa Rao", "authors": "Arni S. R. Srinivasa Rao", "title": "Understanding Theoretically The Impact of Reporting of Disease Cases in\n  Epidemiology", "comments": "21 pages, 2 figures. To appear in Journal of Theoretical Biology\n  (Elsevier)", "journal-ref": "(2012) Journal of Theoretical Biology 302:89-95", "doi": "10.1016/j.jtbi.2012.02.026", "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In conducting preliminary analysis during an epidemic, data on reported\ndisease cases offer key information in guiding the direction to the in-depth\nanalysis. Models for growth and transmission dynamics are heavily dependent on\npreliminary analysis results. When a particular disease case is reported more\nthan once or alternatively is never reported or detected in the population,\nthen in such a situation, there is a possibility of existence of multiple\nreporting or under reporting in the population. In this work, a theoretical\napproach for studying reporting error in epidemiology is explored. The upper\nbound for the error that arises due to multiple reporting is higher than that\nwhich arises due to under reporting. Numerical examples are provided to support\nthe arguments. This article mainly treats reporting error as deterministic and\none can explore a stochastic model for the same.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2012 11:01:10 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2012 19:29:58 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Rao", "Arni S. R. Srinivasa", ""]]}, {"id": "1202.2849", "submitter": "Aleksey Polunchenko", "authors": "Aleksey S. Polunchenko and Alexander G. Tartakovsky and Nitis\n  Mukhopadhyay", "title": "Nearly Optimal Change-Point Detection with an Application to\n  Cybersecurity", "comments": "minor typos and formatting issues fixed, 23 pages, submitted to\n  Sequential Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the sequential change-point detection problem for the Gaussian\nmodel where baseline distribution is Gaussian with variance \\sigma^2 and mean\n\\mu such that \\sigma^2=a\\mu, where a>0 is a known constant; the change is in\n\\mu from one known value to another. First, we carry out a comparative\nperformance analysis of four detection procedures: the CUSUM procedure, the\nShiryaev-Roberts (SR) procedure, and two its modifications - the\nShiryaev-Roberts-Pollak and Shiryaev-Roberts-r procedures. The performance is\nbenchmarked via Pollak's maximal average delay to detection and Shiryaev's\nstationary average delay to detection, each subject to a fixed average run\nlength to false alarm. The analysis shows that in practically interesting cases\nthe accuracy of asymptotic approximations is \"reasonable\" to \"excellent\". We\nalso consider an application of change-point detection to cybersecurity - for\nrapid anomaly detection in computer networks. Using real network data we show\nthat statistically traffic's intensity can be well-described by the proposed\nGaussian model with \\sigma^2=a\\mu instead of the traditional Poisson model,\nwhich requires \\sigma^2=\\mu. By successively devising the SR and CUSUM\nprocedures to \"catch\" a low-contrast network anomaly (caused by an ICMP\nreflector attack), we then show that the SR rule is quicker. We conclude that\nthe SR procedure is a better cyber \"watch dog\" than the popular CUSUM\nprocedure.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2012 20:59:32 GMT"}, {"version": "v2", "created": "Sat, 3 Mar 2012 22:06:04 GMT"}], "update_date": "2012-03-06", "authors_parsed": [["Polunchenko", "Aleksey S.", ""], ["Tartakovsky", "Alexander G.", ""], ["Mukhopadhyay", "Nitis", ""]]}, {"id": "1202.2982", "submitter": "Udaysinh T. Bhosale", "authors": "Udaysinh T. Bhosale, Steven Tomsovic, Arul Lakshminarayan", "title": "Entanglement between two subsystems, the Wigner semicircle and extreme\n  value statistics", "comments": "Substantially improved version (now 43 pages, 10 figures) that is\n  accepted for publication in Phys. Rev. A", "journal-ref": "Phys. Rev. A 85, 062331 (2012)", "doi": "10.1103/PhysRevA.85.062331", "report-no": "Preprint No. IITM/PH/TH/2012/2", "categories": "quant-ph math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The entanglement between two arbitrary subsystems of random pure states is\nstudied via properties of the density matrix's partial transpose,\n$\\rho_{12}^{T_2}$. The density of states of $\\rho_{12}^{T_2}$ is close to the\nsemicircle law when both subsystems have dimensions which are not too small and\nare of the same order. A simple random matrix model for the partial transpose\nis found to capture the entanglement properties well, including a transition\nacross a critical dimension. Log-negativity is used to quantify entanglement\nbetween subsystems and analytic formulas for this are derived based on the\nsimple model. The skewness of the eigenvalue density of $\\rho_{12}^{T_2}$ is\nderived analytically, using the average of the third moment over the ensemble\nof random pure states. The third moment after partial transpose is also shown\nto be related to a generalization of the Kempe invariant. The smallest\neigenvalue after partial transpose is found to follow the extreme value\nstatistics of random matrices, namely the Tracy-Widom distribution. This\ndistribution, with relevant parameters obtained from the model, is found to be\nuseful in calculating the fraction of entangled states at critical dimensions.\nThese results are tested in a quantum dynamical system of three coupled\nstandard maps, where one finds that if the parameters represent a strongly\nchaotic system, the results are close to those of random states, although there\nare some systematic deviations at critical dimensions.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 10:26:03 GMT"}, {"version": "v2", "created": "Sat, 16 Jun 2012 13:57:57 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Bhosale", "Udaysinh T.", ""], ["Tomsovic", "Steven", ""], ["Lakshminarayan", "Arul", ""]]}, {"id": "1202.3302", "submitter": "Daniel Samarov", "authors": "Daniel Samarov, J. S. Marron, Yufeng Liu, Christopher Grulke,\n  Alexander Tropsha", "title": "Local kernel canonical correlation analysis with application to virtual\n  drug screening", "comments": "Published in at http://dx.doi.org/10.1214/11-AOAS472 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 3, 2169-2196", "doi": "10.1214/11-AOAS472", "report-no": "IMS-AOAS-AOAS472", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drug discovery is the process of identifying compounds which have potentially\nmeaningful biological activity. A major challenge that arises is that the\nnumber of compounds to search over can be quite large, sometimes numbering in\nthe millions, making experimental testing intractable. For this reason\ncomputational methods are employed to filter out those compounds which do not\nexhibit strong biological activity. This filtering step, also called virtual\nscreening reduces the search space, allowing for the remaining compounds to be\nexperimentally tested. In this paper we propose several novel approaches to the\nproblem of virtual screening based on Canonical Correlation Analysis (CCA) and\non a kernel-based extension. Spectral learning ideas motivate our proposed new\nmethod called Indefinite Kernel CCA (IKCCA). We show the strong performance of\nthis approach both for a toy problem as well as using real world data with\ndramatic improvements in predictive accuracy of virtual screening over an\nexisting methodology.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2012 13:49:24 GMT"}], "update_date": "2012-02-16", "authors_parsed": [["Samarov", "Daniel", ""], ["Marron", "J. S.", ""], ["Liu", "Yufeng", ""], ["Grulke", "Christopher", ""], ["Tropsha", "Alexander", ""]]}, {"id": "1202.3861", "submitter": "Michael Schreiber", "authors": "Michael Schreiber", "title": "Inconsistencies of Recently Proposed Citation Impact Indicators and how\n  to Avoid Them", "comments": "14 pages, 9 figures, accepted by Journal of the American Society for\n  Information Science and Technology Final version with slightly changed\n  figures, new scoring rule, extended discussion", "journal-ref": "J. Am. Soc. Inf. Sci. Techn. 63(10), 2062-2073, (2012)", "doi": null, "report-no": null, "categories": "stat.AP cs.DL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is shown that under certain circumstances in particular for small datasets\nthe recently proposed citation impact indicators I3(6PR) and R(6,k) behave\ninconsistently when additional papers or citations are taken into\nconsideration. Three simple examples are presented, in which the indicators\nfluctuate strongly and the ranking of scientists in the evaluated group is\nsometimes completely mixed up by minor changes in the data base. The erratic\nbehavior is traced to the specific way in which weights are attributed to the\nsix percentile rank classes, specifically for the tied papers. For 100\npercentile rank classes the effects will be less serious. For the 6 classes it\nis demonstrated that a different way of assigning weights avoids these\nproblems, although the non-linearity of the weights for the different\npercentile rank classes can still lead to (much less frequent) changes in the\nranking. This behavior is not undesired, because it can be used to correct for\ndifferences in citation behavior in different fields. Remaining deviations from\nthe theoretical value R(6,k) = 1.91 can be avoided by a new scoring rule, the\nfractional scoring. Previously proposed consistency criteria are amended by\nanother property of strict independence which a performance indicator should\naim at.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2012 10:05:04 GMT"}, {"version": "v2", "created": "Wed, 4 Apr 2012 08:33:52 GMT"}], "update_date": "2013-01-31", "authors_parsed": [["Schreiber", "Michael", ""]]}, {"id": "1202.3956", "submitter": "Annette M\\\"oller", "authors": "Annette M\\\"oller, Alex Lenkoski, and Thordis L. Thorarinsdottir", "title": "Multivariate probabilistic forecasting using Bayesian model averaging\n  and copulas", "comments": "17 pages, 4 figures", "journal-ref": null, "doi": "10.1002/qj.2009", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for post-processing an ensemble of multivariate forecasts\nin order to obtain a joint predictive distribution of weather. Our method\nutilizes existing univariate post-processing techniques, in this case ensemble\nBayesian model averaging (BMA), to obtain estimated marginal distributions.\nHowever, implementing these methods individually offers no information\nregarding the joint distribution. To correct this, we propose the use of a\nGaussian copula, which offers a simple procedure for recovering the dependence\nthat is lost in the estimation of the ensemble BMA marginals. Our method is\napplied to 48-h forecasts of a set of five weather quantities using the\n8-member University of Washington mesoscale ensemble. We show that our method\nrecovers many well-understood dependencies between weather quantities and\nsubsequently improves calibration and sharpness over both the raw ensemble and\na method which does not incorporate joint distributional information.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2012 16:38:02 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["M\u00f6ller", "Annette", ""], ["Lenkoski", "Alex", ""], ["Thorarinsdottir", "Thordis L.", ""]]}, {"id": "1202.4198", "submitter": "Bala Rajaratnam", "authors": "Richard A. Olshen and Bala Rajaratnam", "title": "Successive Standardization of Rectangular Arrays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we illustrate and develop further with mathematics and examples,\nthe work on successive standardization (or normalization) that is studied\nearlier by the same authors in Olshen and Rajaratnam (2010) and Olshen and\nRajaratnam (2011). Thus, we deal with successive iterations applied to\nrectangular arrays of numbers, where to avoid technical difficulties an array\nhas at least three rows and at least three columns. Without loss, an iteration\nbegins with operations on columns: first subtract the mean of each column; then\ndivide by its standard deviation. The iteration continues with the same two\noperations done successively for rows. These four operations applied in\nsequence completes one iteration. One then iterates again, and again, and\nagain,.... In Olshen and Rajaratnam (2010) it was argued that if arrays are\nmade up of real numbers, then the set for which convergence of these successive\niterations fails has Lebesgue measure 0. The limiting array has row and column\nmeans 0, row and column standard deviations 1. A basic result on convergence\ngiven in Olshen and Rajaratnam (2010) is true, though the argument in Olshen\nand Rajaratnam (2010) is faulty. The result is stated in the form of a theorem\nhere, and the argument for the theorem is correct. Moreover, many graphics\ngiven in Olshen and Rajaratnam (2010) suggest that but for a set of entries of\nany array with Lebesgue measure 0, convergence is very rapid, eventually\nexponentially fast in the number of iterations. Because we learned this set of\nrules from Bradley Efron, we call it \"Efron's algorithm\". More importantly, the\nrapidity of convergence is illustrated by numerical examples.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2012 00:27:10 GMT"}], "update_date": "2012-02-21", "authors_parsed": [["Olshen", "Richard A.", ""], ["Rajaratnam", "Bala", ""]]}, {"id": "1202.4442", "submitter": "S\\'andor Baran", "authors": "S\\'andor Baran, D\\'ora Nemoda and Andr\\'as Hor\\'anyi", "title": "Probabilistic wind speed forecasting in Hungary", "comments": "17 pages, 10 figures", "journal-ref": "Meteorologische Zeitschrift 22 (2013), no. 3, 273-282", "doi": "10.1127/0941-2948/2013/0428", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction of various weather quantities is mostly based on deterministic\nnumerical weather forecasting models. Multiple runs of these models with\ndifferent initial conditions result ensembles of forecasts which are applied\nfor estimating the distribution of future weather quantities. However, the\nensembles are usually under-dispersive and uncalibrated, so post-processing is\nrequired.\n  In the present work Bayesian Model Averaging (BMA) is applied for calibrating\nensembles of wind speed forecasts produced by the operational Limited Area\nModel Ensemble Prediction System of the Hungarian Meteorological Service (HMS).\n  We describe two possible BMA models for wind speed data of the HMS and show\nthat BMA post-processing significantly improves the calibration and precision\nof forecasts.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2012 20:29:12 GMT"}, {"version": "v2", "created": "Thu, 23 Feb 2012 19:00:33 GMT"}, {"version": "v3", "created": "Sat, 17 Mar 2012 19:19:23 GMT"}, {"version": "v4", "created": "Mon, 7 Jan 2013 09:45:55 GMT"}], "update_date": "2014-04-09", "authors_parsed": [["Baran", "S\u00e1ndor", ""], ["Nemoda", "D\u00f3ra", ""], ["Hor\u00e1nyi", "Andr\u00e1s", ""]]}, {"id": "1202.4751", "submitter": "Wonsang You", "authors": "Wonsang You, Joerg Stadler", "title": "Fractal-based Correlation Analysis for Resting State Functional\n  Connectivity of the Rat Brain in Functional MRI", "comments": "CBBS Educational Workshop on Resting State fMRI 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most studies on functional connectivity have been done by analyzing the\nbrain's hemodynamic response to a stimulation. On the other hand, the\nlow-frequency spontaneous fluctuations in the blood oxygen level dependent\n(BOLD) signals of functional MRI have been observed in the resting state.\nHowever, the BOLD signals in resting state are significantly corrupted by huge\nnoises arising from cardiac pulsation, respiration, subject motion, scanner,\nand so forth. Especially, the noise compounds are stronger in the rat brain\nthan in the human brain. To overcome such an artifact, we assumed that fractal\nbehavior in BOLD signals reflects low frequency neural activity, and applied\nthe theorem such that the wavelet correlation spectrum between long memory\nprocesses is scale-invariant over low frequency scales. Here, we report an\nexperiment that shows special correlation patterns not only in correlation of\nscaling coefficients in very low-frequency band (less than 0.0078Hz) but also\nin asymptotic wavelet correlation. In addition, we show the distribution of the\nHurst exponents in the rat brain.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2012 20:55:07 GMT"}], "update_date": "2012-02-22", "authors_parsed": [["You", "Wonsang", ""], ["Stadler", "Joerg", ""]]}, {"id": "1202.4962", "submitter": "Assaf Oron", "authors": "Assaf P. Oron and Peter D. Hoff", "title": "Small-Sample Behavior of Novel Phase I Cancer Trial Designs", "comments": "Somewhat modified version of the version accepted pending final\n  modifications at Clinical Trials. The supplement is in the back", "journal-ref": "Clinical Trials 2013, 10(1):63-92 (including comments and\n  rejoinder)", "doi": "10.1177/1740774512469311", "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novel dose-finding designs, using estimation to assign the best estimated\nmaximum- tolerated-dose (MTD) at each point in the experiment, most commonly\nvia Bayesian techniques, have recently entered large-scale implementation in\nPhase I cancer clinical trials. We examine the small-sample behavior of these\n\"Bayesian Phase I\" (BP1) designs, and also of non-Bayesian designs sharing the\nsame main \"long-memory\" traits (hereafter: LMP1s).\n  For all LMP1s examined, the number of cohorts treated at the true MTD\n(denoted here as n*) was highly variable between numerical runs drawn from the\nsame toxicity-threshold distribution, especially when compared with\n\"up-and-down\" (U&D) short-memory designs. Further investigation using the same\nset of thresholds in permuted order, produced a nearly-identical magnitude of\nvariability in n*. Therefore, this LMP1 behavior is driven by a strong\nsensitivity to the order in which toxicity thresholds appear in the experiment.\nWe suggest that the sensitivity is related to LMP1's tendency to \"settle\" early\non a specific dose level - a tendency caused by the repeated likelihood-based\n\"winner-takes-all\" dose assignment rule, which grants the early cohorts a\ndisproportionately large influence upon experimental trajectories.\n  Presently, U&D designs offer a simpler and more stable alternative, with\nroughly equivalent MTD estimation performance. A promising direction for\ncombining the two approaches is briefly discussed (note: the '3+3' protocol is\nnot a U&D design).\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2012 16:35:24 GMT"}, {"version": "v2", "created": "Thu, 20 Sep 2012 21:31:20 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Oron", "Assaf P.", ""], ["Hoff", "Peter D.", ""]]}, {"id": "1202.5064", "submitter": "Zhongyang Zhang", "authors": "Zhongyang Zhang, Kenneth Lange, Chiara Sabatti", "title": "Reconstructing DNA copy number by joint segmentation of multiple\n  sequences", "comments": "54 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The variation in DNA copy number carries information on the modalities of\ngenome evolution and misregulation of DNA replication in cancer cells; its\nstudy can be helpful to localize tumor suppressor genes, distinguish different\npopulations of cancerous cell, as well identify genomic variations responsible\nfor disease phenotypes. A number of different high throughput technologies can\nbe used to identify copy number variable sites, and the literature documents\nmultiple effective algorithms. We focus here on the specific problem of\ndetecting regions where variation in copy number is relatively common in the\nsample at hand: this encompasses the cases of copy number polymorphisms,\nrelated samples, technical replicates, and cancerous sub-populations from the\nsame individual. We present an algorithm based on regularization approaches\nwith significant computational advantages and competitive accuracy. We\nillustrate its applicability with simulated and real data sets.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2012 23:42:08 GMT"}, {"version": "v2", "created": "Wed, 14 Mar 2012 08:21:02 GMT"}], "update_date": "2012-03-20", "authors_parsed": [["Zhang", "Zhongyang", ""], ["Lange", "Kenneth", ""], ["Sabatti", "Chiara", ""]]}, {"id": "1202.5299", "submitter": "Matjaz Perc", "authors": "Jianbo Gao, Jing Hu, Xiang Mao, Matjaz Perc", "title": "Culturomics meets random fractal theory: Insights into long-range\n  correlations of social and natural phenomena over the past two centuries", "comments": "8 two-column pages, 5 figures; accepted for publication in Journal of\n  the Royal Society Interface [data available at\n  http://books.google.com/ngrams]", "journal-ref": "J. R. Soc. Interface 9 (2012) 1956-1964", "doi": "10.1098/rsif.2011.0846", "report-no": null, "categories": "physics.soc-ph cond-mat.stat-mech cs.DL cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Culturomics was recently introduced as the application of high-throughput\ndata collection and analysis to the study of human culture. Here we make use of\nthis data by investigating fluctuations in yearly usage frequencies of specific\nwords that describe social and natural phenomena, as derived from books that\nwere published over the course of the past two centuries. We show that the\ndetermination of the Hurst parameter by means of fractal analysis provides\nfundamental insights into the nature of long-range correlations contained in\nthe culturomic trajectories, and by doing so, offers new interpretations as to\nwhat might be the main driving forces behind the examined phenomena. Quite\nremarkably, we find that social and natural phenomena are governed by\nfundamentally different processes. While natural phenomena have properties that\nare typical for processes with persistent long-range correlations, social\nphenomena are better described as nonstationary, on-off intermittent, or Levy\nwalk processes.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2012 20:54:23 GMT"}], "update_date": "2012-07-05", "authors_parsed": [["Gao", "Jianbo", ""], ["Hu", "Jing", ""], ["Mao", "Xiang", ""], ["Perc", "Matjaz", ""]]}, {"id": "1202.5334", "submitter": "Tlemcani Mounir", "authors": "Zohra Benkamra, Mekki Terbeche, Mounir Tlemcani", "title": "An allocation scheme for estimating the reliability of a parallel-series\n  system", "comments": "16 pages, 4 figures", "journal-ref": "Advances in Decision Sciences Volume 2012 (2012), Article ID\n  289035", "doi": "10.1155/2012/289035", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We give a hybrid two stage design which can be useful to estimate the\nreliability of a parallel-series and/or by duality a series-parallel system,\nwhen the component reliabilities are unknown as well as the total numbers of\nunits allowed to be tested in each subsystem. When a total sample size is fixed\nlarge, asymptotic optimality is proved systematically and validated via Monte\nCarlo simulation.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2012 21:52:57 GMT"}, {"version": "v2", "created": "Tue, 6 Mar 2012 22:24:02 GMT"}, {"version": "v3", "created": "Thu, 17 May 2012 14:52:40 GMT"}, {"version": "v4", "created": "Mon, 24 Dec 2012 23:09:29 GMT"}], "update_date": "2012-12-27", "authors_parsed": [["Benkamra", "Zohra", ""], ["Terbeche", "Mekki", ""], ["Tlemcani", "Mounir", ""]]}, {"id": "1202.5450", "submitter": "Omar De la Cruz", "authors": "Omar De la Cruz, Susan Holmes", "title": "The duality diagram in data analysis: Examples of modern applications", "comments": "Published in at http://dx.doi.org/10.1214/10-AOAS408 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 4, 2266-2277", "doi": "10.1214/10-AOAS408", "report-no": "IMS-AOAS-AOAS408", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's data-heavy research environment requires the integration of different\nsources of information into structured data sets that can not be analyzed as\nsimple matrices. We introduce an old technique, known in the European data\nanalyses circles as the Duality Diagram Approach, put to new uses through the\nuse of a variety of metrics and ways of combining different diagrams together.\nThis issue of the Annals of Applied Statistics contains contemporary examples\nof how this approach provides solutions to hard problems in data integration.\nWe present here the genesis of the technique and how it can be seen as a\nprecursor of the modern kernel based approaches.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2012 13:36:50 GMT"}], "update_date": "2012-02-27", "authors_parsed": [["De la Cruz", "Omar", ""], ["Holmes", "Susan", ""]]}, {"id": "1202.5473", "submitter": "Jean Thioulouse", "authors": "Jean Thioulouse", "title": "Simultaneous analysis of a sequence of paired ecological tables: A\n  comparison of several methods", "comments": "Published in at http://dx.doi.org/10.1214/10-AOAS372 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 4, 2300-2325", "doi": "10.1214/10-AOAS372", "report-no": "IMS-AOAS-AOAS372", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A pair of ecological tables is made of one table containing environmental\nvariables (in columns) and another table containing species data (in columns).\nThe rows of these two tables are identical and correspond to the sites where\nenvironmental variables and species data have been measured. Such data are used\nto analyze the relationships between species and their environment. If sampling\nis repeated over time for both tables, one obtains a sequence of pairs of\necological tables. Analyzing this type of data is a way to assess changes in\nspecies-environment relationships, which can be important for conservation\nEcology or for global change studies. We present a new data analysis method\nadapted to the study of this type of data, and we compare it with two other\nmethods on the same data set. All three methods are implemented in the ade4\npackage for the R environment.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2012 15:04:28 GMT"}], "update_date": "2012-02-27", "authors_parsed": [["Thioulouse", "Jean", ""]]}, {"id": "1202.5846", "submitter": "Alex Lenkoski", "authors": "Anna Karl and Alex Lenkoski", "title": "Instrumental Variable Bayesian Model Averaging via Conditional Bayes\n  Factors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a method to perform model averaging in two-stage linear regression\nsystems subject to endogeneity. Our method extends an existing Gibbs sampler\nfor instrumental variables to incorporate a component of model uncertainty.\nDirect evaluation of model probabilities is intractable in this setting. We\nshow that by nesting model moves inside the Gibbs sampler, model comparison can\nbe performed via conditional Bayes factors, leading to straightforward\ncalculations. This new Gibbs sampler is only slightly more involved than the\noriginal algorithm and exhibits no evidence of mixing difficulties. We conclude\nwith a study of two different modeling challenges: incorporating uncertainty\ninto the determinants of macroeconomic growth, and estimating a demand function\nby instrumenting wholesale on retail prices.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2012 08:00:30 GMT"}, {"version": "v2", "created": "Tue, 6 Mar 2012 10:43:52 GMT"}, {"version": "v3", "created": "Mon, 19 Mar 2012 09:26:34 GMT"}], "update_date": "2012-03-20", "authors_parsed": [["Karl", "Anna", ""], ["Lenkoski", "Alex", ""]]}, {"id": "1202.5858", "submitter": "Alex Lenkoski", "authors": "A. Lenkoski, T. S. Eicher and A. E. Raftery", "title": "Two-Stage Bayesian Model Averaging in Endogenous Variable Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Economic modeling in the presence of endogeneity is subject to model\nuncertainty at both the instrument and covariate level. We propose a Two-Stage\nBayesian Model Averaging (2SBMA) methodology that extends the Two-Stage Least\nSquares (2SLS) estimator. By constructing a Two-Stage Unit Information Prior in\nthe endogenous variable model, we are able to efficiently combine established\nmethods for addressing model uncertainty in regression models with the classic\ntechnique of 2SLS. To assess the validity of instruments in the 2SBMA context,\nwe develop Bayesian tests of the identification restriction that are based on\nmodel averaged posterior predictive p-values. A simulation study showed that\n2SBMA has the ability to recover structure in both the instrument and covariate\nset, and substantially improves the sharpness of resulting coefficient\nestimates in comparison to 2SLS using the full specification in an automatic\nfashion. Due to the increased parsimony of the 2SBMA estimate, the Bayesian\nSargan test had a power of 50 percent in detecting a violation of the\nexogeneity assumption, while the method based on 2SLS using the full\nspecification had negligible power. We apply our approach to the problem of\ndevelopment accounting, and find support not only for institutions, but also\nfor geography and integration as development determinants, once both model\nuncertainty and endogeneity have been jointly addressed.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2012 08:47:32 GMT"}], "update_date": "2012-02-28", "authors_parsed": [["Lenkoski", "A.", ""], ["Eicher", "T. S.", ""], ["Raftery", "A. E.", ""]]}, {"id": "1202.5880", "submitter": "Elizabeth Purdom", "authors": "Elizabeth Purdom", "title": "Analysis of a data matrix and a graph: Metagenomic data and the\n  phylogenetic tree", "comments": "Published in at http://dx.doi.org/10.1214/10-AOAS402 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 4, 2326-2358", "doi": "10.1214/10-AOAS402", "report-no": "IMS-AOAS-AOAS402", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In biological experiments researchers often have information in the form of a\ngraph that supplements observed numerical data. Incorporating the knowledge\ncontained in these graphs into an analysis of the numerical data is an\nimportant and nontrivial task. We look at the example of metagenomic\ndata---data from a genomic survey of the abundance of different species of\nbacteria in a sample. Here, the graph of interest is a phylogenetic tree\ndepicting the interspecies relationships among the bacteria species. We\nillustrate that analysis of the data in a nonstandard inner-product space\neffectively uses this additional graphical information and produces more\nmeaningful results.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2012 10:22:19 GMT"}], "update_date": "2012-02-28", "authors_parsed": [["Purdom", "Elizabeth", ""]]}, {"id": "1202.5901", "submitter": "Stefano Conti", "authors": "Stefano Conti, Anne M. Presanis, Maaike G. van Veen, Maria Xiridou,\n  Martin C. Donoghoe, Annemarie Rinder Stengaard, Daniela De Angelis", "title": "Modeling of the HIV infection epidemic in the Netherlands: A\n  multi-parameter evidence synthesis approach", "comments": "Published in at http://dx.doi.org/10.1214/11-AOAS488 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 4, 2359-2384", "doi": "10.1214/11-AOAS488", "report-no": "IMS-AOAS-AOAS488", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-parameter evidence synthesis (MPES) is receiving growing attention from\nthe epidemiological community as a coherent and flexible analytical framework\nto accommodate a disparate body of evidence available to inform disease\nincidence and prevalence estimation. MPES is the statistical methodology\nadopted by the Health Protection Agency in the UK for its annual national\nassessment of the HIV epidemic, and is acknowledged by the World Health\nOrganization and UNAIDS as a valuable technique for the estimation of adult HIV\nprevalence from surveillance data. This paper describes the results of\nutilizing a Bayesian MPES approach to model HIV prevalence in the Netherlands\nat the end of 2007, using an array of field data from different study designs\non various population risk subgroups and with a varying degree of regional\ncoverage. Auxiliary data and expert opinion were additionally incorporated to\nresolve issues arising from biased, insufficient or inconsistent evidence. This\ncase study offers a demonstration of the ability of MPES to naturally integrate\nand critically reconcile disparate and heterogeneous sources of evidence, while\nproducing reliable estimates of HIV prevalence used to support public health\ndecision-making.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2012 11:50:31 GMT"}], "update_date": "2012-02-28", "authors_parsed": [["Conti", "Stefano", ""], ["Presanis", "Anne M.", ""], ["van Veen", "Maaike G.", ""], ["Xiridou", "Maria", ""], ["Donoghoe", "Martin C.", ""], ["Stengaard", "Annemarie Rinder", ""], ["De Angelis", "Daniela", ""]]}, {"id": "1202.5914", "submitter": "Luis Guti\\'{e}rrez", "authors": "Luis Guti\\'errez, Fernando A. Quintana", "title": "Multivariate Bayesian semiparametric models for authentication of food\n  and beverages", "comments": "Published in at http://dx.doi.org/10.1214/11-AOAS492 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 4, 2385-2402", "doi": "10.1214/11-AOAS492", "report-no": "IMS-AOAS-AOAS492", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Food and beverage authentication is the process by which foods or beverages\nare verified as complying with its label description, for example, verifying if\nthe denomination of origin of an olive oil bottle is correct or if the variety\nof a certain bottle of wine matches its label description. The common way to\ndeal with an authentication process is to measure a number of attributes on\nsamples of food and then use these as input for a classification problem. Our\nmotivation stems from data consisting of measurements of nine chemical\ncompounds denominated Anthocyanins, obtained from samples of Chilean red wines\nof grape varieties Cabernet Sauvignon, Merlot and Carm\\'{e}n\\`{e}re. We\nconsider a model-based approach to authentication through a semiparametric\nmultivariate hierarchical linear mixed model for the mean responses, and\ncovariance matrices that are specific to the classification categories.\nSpecifically, we propose a model of the ANOVA-DDP type, which takes advantage\nof the fact that the available covariates are discrete in nature. The results\nsuggest that the model performs well compared to other parametric alternatives.\nThis is also corroborated by application to simulated data.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2012 12:45:10 GMT"}], "update_date": "2012-02-28", "authors_parsed": [["Guti\u00e9rrez", "Luis", ""], ["Quintana", "Fernando A.", ""]]}, {"id": "1202.5933", "submitter": "Jacob Bien", "authors": "Jacob Bien, Robert Tibshirani", "title": "Prototype selection for interpretable classification", "comments": "Published in at http://dx.doi.org/10.1214/11-AOAS495 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org). arXiv admin note: text\n  overlap with arXiv:0908.2284", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 4, 2403-2424", "doi": "10.1214/11-AOAS495", "report-no": "IMS-AOAS-AOAS495", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prototype methods seek a minimal subset of samples that can serve as a\ndistillation or condensed view of a data set. As the size of modern data sets\ngrows, being able to present a domain specialist with a short list of\n\"representative\" samples chosen from the data set is of increasing\ninterpretative value. While much recent statistical research has been focused\non producing sparse-in-the-variables methods, this paper aims at achieving\nsparsity in the samples. We discuss a method for selecting prototypes in the\nclassification setting (in which the samples fall into known discrete\ncategories). Our method of focus is derived from three basic properties that we\nbelieve a good prototype set should satisfy. This intuition is translated into\na set cover optimization problem, which we solve approximately using standard\napproaches. While prototype selection is usually viewed as purely a means\ntoward building an efficient classifier, in this paper we emphasize the\ninherent value of having a set of prototypical elements. That said, by using\nthe nearest-neighbor rule on the set of prototypes, we can of course discuss\nour method as a classifier as well.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2012 13:45:29 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Bien", "Jacob", ""], ["Tibshirani", "Robert", ""]]}, {"id": "1202.5999", "submitter": "John Dawson", "authors": "John A. Dawson and Christina Kendziorski", "title": "Survival-supervised latent Dirichlet allocation models for genomic\n  analysis of time-to-event outcomes", "comments": "21 pages, including 6 figures and 5 pages of appendices", "journal-ref": null, "doi": null, "report-no": "TR 225", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two challenging problems in the clinical study of cancer are the\ncharacterization of cancer subtypes and the classification of individual\npatients according to those subtypes. Statistical approaches addressing these\nproblems are hampered by population heterogeneity and challenges inherent in\ndata integration across high-dimensional, diverse covariates. We have developed\na survival-supervised latent Dirichlet allocation (survLDA) modeling framework\nto address these concerns. LDA models have proven extremely effective at\nidentifying themes common across large collections of text, but applications to\ngenomics have been limited. Our framework extends LDA to the genome by\nconsidering each patient as a `document' with `text' constructed from clinical\nand high-dimensional genomic measurements. We then further extend the framework\nto allow for supervision by a time-to-event response. The model enables the\nefficient identification of collections of clinical and genomic features that\nco-occur within patient subgroups, and then characterizes each patient by those\nfeatures. An application of survLDA to The Cancer Genome Atlas (TCGA) ovarian\nproject identifies informative patient subgroups that are characterized by\ndifferent propensities for exhibiting abnormal mRNA expression and\nmethylations, corresponding to differential rates of survival from primary\ntherapy.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2012 17:10:38 GMT"}], "update_date": "2012-02-28", "authors_parsed": [["Dawson", "John A.", ""], ["Kendziorski", "Christina", ""]]}, {"id": "1202.6133", "submitter": "Adam R. Brentnall", "authors": "Adam R. Brentnall, Stephen W. Duffy, Martin J. Crowder, Maureen G. C.\n  Gillan, Susan M. Astley, Matthew G. Wallis, Jonathan James, Caroline R. M.\n  Boggis, Fiona J. Gilbert", "title": "A method for exploratory repeated-measures analysis applied to a\n  breast-cancer screening study", "comments": "Published in at http://dx.doi.org/10.1214/11-AOAS481 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 4, 2448-2469", "doi": "10.1214/11-AOAS481", "report-no": "IMS-AOAS-AOAS481", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a model may be fitted separately to each individual statistical unit,\ninspection of the point estimates may help the statistician to understand\nbetween-individual variability and to identify possible relationships. However,\nsome information will be lost in such an approach because estimation\nuncertainty is disregarded. We present a comparative method for exploratory\nrepeated-measures analysis to complement the point estimates that was motivated\nby and is demonstrated by analysis of data from the CADET II breast-cancer\nscreening study. The approach helped to flag up some unusual reader behavior,\nto assess differences in performance, and to identify potential random-effects\nmodels for further analysis.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2012 06:30:41 GMT"}], "update_date": "2012-02-29", "authors_parsed": [["Brentnall", "Adam R.", ""], ["Duffy", "Stephen W.", ""], ["Crowder", "Martin J.", ""], ["Gillan", "Maureen G. C.", ""], ["Astley", "Susan M.", ""], ["Wallis", "Matthew G.", ""], ["James", "Jonathan", ""], ["Boggis", "Caroline R. M.", ""], ["Gilbert", "Fiona J.", ""]]}, {"id": "1202.6172", "submitter": "Brian J. Reich", "authors": "Brian J. Reich, Jo Eidsvik, Michele Guindani, Amy J. Nail, Alexandra\n  M. Schmidt", "title": "A class of covariate-dependent spatiotemporal covariance functions for\n  the analysis of daily ozone concentration", "comments": "Published in at http://dx.doi.org/10.1214/11-AOAS482 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 4, 2425-2447", "doi": "10.1214/11-AOAS482", "report-no": "IMS-AOAS-AOAS482", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In geostatistics, it is common to model spatially distributed phenomena\nthrough an underlying stationary and isotropic spatial process. However, these\nassumptions are often untenable in practice because of the influence of local\neffects in the correlation structure. Therefore, it has been of prolonged\ninterest in the literature to provide flexible and effective ways to model\nnonstationarity in the spatial effects. Arguably, due to the local nature of\nthe problem, we might envision that the correlation structure would be highly\ndependent on local characteristics of the domain of study, namely, the\nlatitude, longitude and altitude of the observation sites, as well as other\nlocally defined covariate information. In this work, we provide a flexible and\ncomputationally feasible way for allowing the correlation structure of the\nunderlying processes to depend on local covariate information. We discuss the\nproperties of the induced covariance functions and methods to assess its\ndependence on local covariate information. The proposed method is used to\nanalyze daily ozone in the southeast United States.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2012 10:33:00 GMT"}], "update_date": "2012-02-29", "authors_parsed": [["Reich", "Brian J.", ""], ["Eidsvik", "Jo", ""], ["Guindani", "Michele", ""], ["Nail", "Amy J.", ""], ["Schmidt", "Alexandra M.", ""]]}, {"id": "1202.6201", "submitter": "Daniela M. Witten", "authors": "Daniela M. Witten", "title": "Classification and clustering of sequencing data using a Poisson model", "comments": "Published in at http://dx.doi.org/10.1214/11-AOAS493 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 4, 2493-2518", "doi": "10.1214/11-AOAS493", "report-no": "IMS-AOAS-AOAS493", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, advances in high throughput sequencing technology have led\nto a need for specialized methods for the analysis of digital gene expression\ndata. While gene expression data measured on a microarray take on continuous\nvalues and can be modeled using the normal distribution, RNA sequencing data\ninvolve nonnegative counts and are more appropriately modeled using a discrete\ncount distribution, such as the Poisson or the negative binomial. Consequently,\nanalytic tools that assume a Gaussian distribution (such as classification\nmethods based on linear discriminant analysis and clustering methods that use\nEuclidean distance) may not perform as well for sequencing data as methods that\nare based upon a more appropriate distribution. Here, we propose new approaches\nfor performing classification and clustering of observations on the basis of\nsequencing data. Using a Poisson log linear model, we develop an analog of\ndiagonal linear discriminant analysis that is appropriate for sequencing data.\nWe also propose an approach for clustering sequencing data using a new\ndissimilarity measure that is based upon the Poisson model. We demonstrate the\nperformances of these approaches in a simulation study, on three publicly\navailable RNA sequencing data sets, and on a publicly available chromatin\nimmunoprecipitation sequencing data set.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2012 12:55:23 GMT"}], "update_date": "2012-02-29", "authors_parsed": [["Witten", "Daniela M.", ""]]}, {"id": "1202.6294", "submitter": "Nicolas Dobigeon", "authors": "Jos\\'e M. Bioucas-Dias, Antonio Plaza, Nicolas Dobigeon, Mario\n  Parente, Qian Du, Paul Gader and Jocelyn Chanussot", "title": "Hyperspectral Unmixing Overview: Geometrical, Statistical, and Sparse\n  Regression-Based Approaches", "comments": "This work has been accepted for publication in IEEE Journal of\n  Selected Topics in Applied Earth Observations and Remote Sensing", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an math.OC stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imaging spectrometers measure electromagnetic energy scattered in their\ninstantaneous field view in hundreds or thousands of spectral channels with\nhigher spectral resolution than multispectral cameras. Imaging spectrometers\nare therefore often referred to as hyperspectral cameras (HSCs). Higher\nspectral resolution enables material identification via spectroscopic analysis,\nwhich facilitates countless applications that require identifying materials in\nscenarios unsuitable for classical spectroscopic analysis. Due to low spatial\nresolution of HSCs, microscopic material mixing, and multiple scattering,\nspectra measured by HSCs are mixtures of spectra of materials in a scene. Thus,\naccurate estimation requires unmixing. Pixels are assumed to be mixtures of a\nfew materials, called endmembers. Unmixing involves estimating all or some of:\nthe number of endmembers, their spectral signatures, and their abundances at\neach pixel. Unmixing is a challenging, ill-posed inverse problem because of\nmodel inaccuracies, observation noise, environmental conditions, endmember\nvariability, and data set size. Researchers have devised and investigated many\nmodels searching for robust, stable, tractable, and accurate unmixing\nalgorithms. This paper presents an overview of unmixing methods from the time\nof Keshava and Mustard's unmixing tutorial [1] to the present. Mixing models\nare first discussed. Signal-subspace, geometrical, statistical, sparsity-based,\nand spatial-contextual unmixing algorithms are described. Mathematical problems\nand potential solutions are described. Algorithm characteristics are\nillustrated experimentally.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2012 17:30:39 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2012 11:52:08 GMT"}], "update_date": "2012-04-25", "authors_parsed": [["Bioucas-Dias", "Jos\u00e9 M.", ""], ["Plaza", "Antonio", ""], ["Dobigeon", "Nicolas", ""], ["Parente", "Mario", ""], ["Du", "Qian", ""], ["Gader", "Paul", ""], ["Chanussot", "Jocelyn", ""]]}, {"id": "1202.6418", "submitter": "Douglas Cochran", "authors": "Bill Moran, Stephen D. Howard, Douglas Cochran", "title": "An Information-geometric Approach to Sensor Management", "comments": "4 pages, 3 figures, to appear in Proceedings of the IEEE\n  International Conference on Acoustics, Speech, and Signal Processing, March\n  2012", "journal-ref": null, "doi": "10.1109/ICASSP.2012.6289107", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An information-geometric approach to sensor management is introduced that is\nbased on following geodesic curves in a manifold of possible sensor\nconfigurations. This perspective arises by observing that, given a parameter\nestimation problem to be addressed through management of sensor assets, any\nparticular sensor configuration corresponds to a Riemannian metric on the\nparameter manifold. With this perspective, managing sensors involves navigation\non the space of all Riemannian metrics on the parameter manifold, which is\nitself a Riemannian manifold. Existing work assumes the metric on the parameter\nmanifold is one that, in statistical terms, corresponds to a Jeffreys prior on\nthe parameter to be estimated. It is observed that informative priors, as arise\nin sensor management, can also be accommodated. Given an initial sensor\nconfiguration, the trajectory along which to move in sensor configuration space\nto gather most information is seen to be locally defined by the geodesic\nstructure of this manifold. Further, divergences based on Fisher and Shannon\ninformation lead to the same Riemannian metric and geodesics.\n", "versions": [{"version": "v1", "created": "Wed, 29 Feb 2012 00:49:44 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Moran", "Bill", ""], ["Howard", "Stephen D.", ""], ["Cochran", "Douglas", ""]]}, {"id": "1202.6420", "submitter": "Douglas Cochran", "authors": "Douglas Cochran, Stephen D. Howard, Bill Moran, Harry A. Schmitt", "title": "Maximum-entropy Surrogation in Network Signal Detection", "comments": "4 pages, submitted to IEEE Statistical Signal Processing Workshop,\n  August 2012", "journal-ref": null, "doi": "10.1109/SSP.2012.6319686", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple-channel detection is considered in the context of a sensor network\nwhere raw data are shared only by nodes that have a common edge in the network\ngraph. Established multiple-channel detectors, such as those based on\ngeneralized coherence or multiple coherence, use pairwise measurements from\nevery pair of sensors in the network and are thus directly applicable only to\nnetworks whose graphs are completely connected. An approach introduced here\nuses a maximum-entropy technique to formulate surrogate values for missing\nmeasurements corresponding to pairs of nodes that do not share an edge in the\nnetwork graph. The broader potential merit of maximum-entropy baselines in\nquantifying the value of information in sensor network applications is also\nnoted.\n", "versions": [{"version": "v1", "created": "Wed, 29 Feb 2012 01:05:02 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Cochran", "Douglas", ""], ["Howard", "Stephen D.", ""], ["Moran", "Bill", ""], ["Schmitt", "Harry A.", ""]]}, {"id": "1202.6475", "submitter": "Victor M. Panaretos", "authors": "Victor M. Panaretos, Kjell Konis", "title": "Sparse approximations of protein structure from noisy random projections", "comments": "Published in at http://dx.doi.org/10.1214/11-AOAS479 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 4, 2572-2602", "doi": "10.1214/11-AOAS479", "report-no": "IMS-AOAS-AOAS479", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-particle electron microscopy is a modern technique that biophysicists\nemploy to learn the structure of proteins. It yields data that consist of noisy\nrandom projections of the protein structure in random directions, with the\nadded complication that the projection angles cannot be observed. In order to\nreconstruct a three-dimensional model, the projection directions need to be\nestimated by use of an ad-hoc starting estimate of the unknown particle. In\nthis paper we propose a methodology that does not rely on knowledge of the\nprojection angles, to construct an objective data-dependent low-resolution\napproximation of the unknown structure that can serve as such a starting\nestimate. The approach assumes that the protein admits a suitable sparse\nrepresentation, and employs discrete $L^1$-regularization (LASSO) as well as\nnotions from shape theory to tackle the peculiar challenges involved in the\nassociated inverse problem. We illustrate the approach by application to the\nreconstruction of an E. coli protein component called the Klenow fragment.\n", "versions": [{"version": "v1", "created": "Wed, 29 Feb 2012 07:54:59 GMT"}], "update_date": "2012-03-01", "authors_parsed": [["Panaretos", "Victor M.", ""], ["Konis", "Kjell", ""]]}, {"id": "1202.6485", "submitter": "St\\'{e}phane Dray", "authors": "St\\'ephane Dray, Thibaut Jombart", "title": "Revisiting Guerry's data: Introducing spatial constraints in\n  multivariate analysis", "comments": "Published in at http://dx.doi.org/10.1214/10-AOAS356 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 4, 2278-2299", "doi": "10.1214/10-AOAS356", "report-no": "IMS-AOAS-AOAS356", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard multivariate analysis methods aim to identify and summarize the main\nstructures in large data sets containing the description of a number of\nobservations by several variables. In many cases, spatial information is also\navailable for each observation, so that a map can be associated to the\nmultivariate data set. Two main objectives are relevant in the analysis of\nspatial multivariate data: summarizing covariation structures and identifying\nspatial patterns. In practice, achieving both goals simultaneously is a\nstatistical challenge, and a range of methods have been developed that offer\ntrade-offs between these two objectives. In an applied context, this\nmethodological question has been and remains a major issue in community\necology, where species assemblages (i.e., covariation between species\nabundances) are often driven by spatial processes (and thus exhibit spatial\npatterns). In this paper we review a variety of methods developed in community\necology to investigate multivariate spatial patterns. We present different ways\nof incorporating spatial constraints in multivariate analysis and illustrate\nthese different approaches using the famous data set on moral statistics in\nFrance published by Andr\\'{e}-Michel Guerry in 1833. We discuss and compare the\nproperties of these different approaches both from a practical and theoretical\nviewpoint.\n", "versions": [{"version": "v1", "created": "Wed, 29 Feb 2012 08:50:09 GMT"}], "update_date": "2012-03-01", "authors_parsed": [["Dray", "St\u00e9phane", ""], ["Jombart", "Thibaut", ""]]}, {"id": "1202.6487", "submitter": "Robert Alan Clements", "authors": "Robert Alan Clements, Frederic Paik Schoenberg, Danijel Schorlemmer", "title": "Residual analysis methods for space--time point processes with\n  applications to earthquake forecast models in California", "comments": "Published in at http://dx.doi.org/10.1214/11-AOAS487 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 4, 2549-2571", "doi": "10.1214/11-AOAS487", "report-no": "IMS-AOAS-AOAS487", "categories": "stat.AP physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern, powerful techniques for the residual analysis of spatial-temporal\npoint process models are reviewed and compared. These methods are applied to\nCalifornia earthquake forecast models used in the Collaboratory for the Study\nof Earthquake Predictability (CSEP). Assessments of these earthquake\nforecasting models have previously been performed using simple, low-power means\nsuch as the L-test and N-test. We instead propose residual methods based on\nrescaling, thinning, superposition, weighted K-functions and deviance\nresiduals. Rescaled residuals can be useful for assessing the overall fit of a\nmodel, but as with thinning and superposition, rescaling is generally\nimpractical when the conditional intensity $\\lambda$ is volatile. While\nresidual thinning and superposition may be useful for identifying spatial\nlocations where a model fits poorly, these methods have limited power when the\nmodeled conditional intensity assumes extremely low or high values somewhere in\nthe observation region, and this is commonly the case for earthquake\nforecasting models. A recently proposed hybrid method of thinning and\nsuperposition, called super-thinning, is a more powerful alternative.\n", "versions": [{"version": "v1", "created": "Wed, 29 Feb 2012 09:02:45 GMT"}], "update_date": "2012-03-01", "authors_parsed": [["Clements", "Robert Alan", ""], ["Schoenberg", "Frederic Paik", ""], ["Schorlemmer", "Danijel", ""]]}, {"id": "1202.6515", "submitter": "Jianxin Yin", "authors": "Jianxin Yin, Hongzhe Li", "title": "A sparse conditional Gaussian graphical model for analysis of genetical\n  genomics data", "comments": "Published in at http://dx.doi.org/10.1214/11-AOAS494 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 4, 2630-2650", "doi": "10.1214/11-AOAS494", "report-no": "IMS-AOAS-AOAS494", "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genetical genomics experiments have now been routinely conducted to measure\nboth the genetic markers and gene expression data on the same subjects. The\ngene expression levels are often treated as quantitative traits and are subject\nto standard genetic analysis in order to identify the gene expression\nquantitative loci (eQTL). However, the genetic architecture for many gene\nexpressions may be complex, and poorly estimated genetic architecture may\ncompromise the inferences of the dependency structures of the genes at the\ntranscriptional level. In this paper we introduce a sparse conditional Gaussian\ngraphical model for studying the conditional independent relationships among a\nset of gene expressions adjusting for possible genetic effects where the gene\nexpressions are modeled with seemingly unrelated regressions. We present an\nefficient coordinate descent algorithm to obtain the penalized estimation of\nboth the regression coefficients and the sparse concentration matrix. The\ncorresponding graph can be used to determine the conditional independence among\na group of genes while adjusting for shared genetic effects. Simulation\nexperiments and asymptotic convergence rates and sparsistency are used to\njustify our proposed methods. By sparsistency, we mean the property that all\nparameters that are zero are actually estimated as zero with probability\ntending to one. We apply our methods to the analysis of a yeast eQTL data set\nand demonstrate that the conditional Gaussian graphical model leads to a more\ninterpretable gene network than a standard Gaussian graphical model based on\ngene expression data alone.\n", "versions": [{"version": "v1", "created": "Wed, 29 Feb 2012 11:14:12 GMT"}], "update_date": "2012-03-01", "authors_parsed": [["Yin", "Jianxin", ""], ["Li", "Hongzhe", ""]]}, {"id": "1202.6524", "submitter": "Enrique F. Schisterman", "authors": "Enrique F. Schisterman, Albert Vexler, Aijun Ye, Neil J. Perkins", "title": "A combined efficient design for biomarker data subject to a limit of\n  detection due to measuring instrument sensitivity", "comments": "Published in at http://dx.doi.org/10.1214/11-AOAS490 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 4, 2651-2667", "doi": "10.1214/11-AOAS490", "report-no": "IMS-AOAS-AOAS490", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pooling specimens, a well-accepted sampling strategy in biomedical research,\ncan be applied to reduce the cost of studying biomarkers. Even if the cost of a\nsingle assay is not a major restriction in evaluating biomarkers, pooling can\nbe a powerful design that increases the efficiency of estimation based on data\nthat is censored due to an instrument's lower limit of detection (LLOD).\nHowever, there are situations when the pooling design strongly aggravates the\ndetection limit problem. To combine the benefits of pooled assays and\nindividual assays, hybrid designs that involve taking a sample of both pooled\nand individual specimens have been proposed. We examine the efficiency of these\nhybrid designs in estimating parameters of two systems subject to a LLOD: (1)\nnormally distributed biomarker with normally distributed measurement error and\npooling error; (2) Gamma distributed biomarker with double exponentially\ndistributed measurement error and pooling error. Three-assay design and\ntwo-assay design with replicates are applied to estimate the measurement and\npooling error. The Maximum likelihood method is used to estimate the\nparameters. We found that the simple one-pool design, where all assays but one\nare random individuals and a single pooled assay includes the remaining\nspecimens, under plausible conditions, is very efficient and can be recommended\nfor practical use.\n", "versions": [{"version": "v1", "created": "Wed, 29 Feb 2012 12:10:19 GMT"}], "update_date": "2012-03-01", "authors_parsed": [["Schisterman", "Enrique F.", ""], ["Vexler", "Albert", ""], ["Ye", "Aijun", ""], ["Perkins", "Neil J.", ""]]}, {"id": "1202.6536", "submitter": "Hui Shen", "authors": "Hui Shen, William J. Welch, Jacqueline M. Hughes-Oliver", "title": "Efficient, adaptive cross-validation for tuning and comparing models,\n  with application to drug discovery", "comments": "Published in at http://dx.doi.org/10.1214/11-AOAS491 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 4, 2668-2687", "doi": "10.1214/11-AOAS491", "report-no": "IMS-AOAS-AOAS491", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-validation (CV) is widely used for tuning a model with respect to\nuser-selected parameters and for selecting a \"best\" model. For example, the\nmethod of $k$-nearest neighbors requires the user to choose $k$, the number of\nneighbors, and a neural network has several tuning parameters controlling the\nnetwork complexity. Once such parameters are optimized for a particular data\nset, the next step is often to compare the various optimized models and choose\nthe method with the best predictive performance. Both tuning and model\nselection boil down to comparing models, either across different values of the\ntuning parameters or across different classes of statistical models and/or sets\nof explanatory variables. For multiple large sets of data, like the PubChem\ndrug discovery cheminformatics data which motivated this work, reliable CV\ncomparisons are computationally demanding, or even infeasible. In this paper we\ndevelop an efficient sequential methodology for model comparison based on CV.\nIt also takes into account the randomness in CV. The number of models is\nreduced via an adaptive, multiplicity-adjusted sequential algorithm, where poor\nperformers are quickly eliminated. By exploiting matching of individual\nobservations, it is sometimes even possible to establish the statistically\nsignificant inferiority of some models with just one execution of CV.\n", "versions": [{"version": "v1", "created": "Wed, 29 Feb 2012 13:12:51 GMT"}], "update_date": "2012-03-01", "authors_parsed": [["Shen", "Hui", ""], ["Welch", "William J.", ""], ["Hughes-Oliver", "Jacqueline M.", ""]]}]