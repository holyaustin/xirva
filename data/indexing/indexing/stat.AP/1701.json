[{"id": "1701.00166", "submitter": "J. Nathan Kutz", "authors": "Lucas Stolerman, and Pedro Maia, and J. Nathan Kutz", "title": "Data-Driven Forecast of Dengue Outbreaks in Brazil: A Critical\n  Assessment of Climate Conditions for Different Capitals", "comments": "29 pages, 12 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local climate conditions play a major role in the development of the mosquito\npopulation responsible for transmitting Dengue Fever. Since the {\\em Aedes\nAegypti} mosquito is also a primary vector for the recent Zika and Chikungunya\nepidemics across the Americas, a detailed monitoring of periods with favorable\nclimate conditions for mosquito profusion may improve the timing of\nvector-control efforts and other urgent public health strategies. We apply\ndimensionality reduction techniques and machine-learning algorithms to climate\ntime series data and analyze their connection to the occurrence of Dengue\noutbreaks for seven major cities in Brazil. Specifically, we have identified\ntwo key variables and a period during the annual cycle that are highly\npredictive of epidemic outbreaks. The key variables are the frequency of\nprecipitation and temperature during an approximately two month window of the\nwinter season preceding the outbreak. Thus simple climate signatures may be\ninfluencing Dengue outbreaks even months before their occurrence. Some of the\nmore challenging datasets required usage of compressive-sensing procedures to\nestimate missing entries for temperature and precipitation records. Our results\nindicate that each Brazilian capital considered has a unique frequency of\nprecipitation and temperature signature in the winter preceding a Dengue\noutbreak. Such climate contributions on vector populations are key factors in\ndengue dynamics which could lead to more accurate prediction models and early\nwarning systems. Finally, we show that critical temperature and precipitation\nsignatures may vary significantly from city to city, suggesting that the\ninterplay between climate variables and dengue outbreaks is more complex than\ngenerally appreciated.\n", "versions": [{"version": "v1", "created": "Sat, 31 Dec 2016 20:55:47 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Stolerman", "Lucas", ""], ["Maia", "Pedro", ""], ["Kutz", "J. Nathan", ""]]}, {"id": "1701.00284", "submitter": "Milan Janosov", "authors": "Mil\\'an Janosov, Csaba Vir\\'agh, G\\'abor V\\'as\\'arhelyi, Tam\\'as\n  Vicsek", "title": "Group chasing tactics: how to catch a faster prey?", "comments": null, "journal-ref": null, "doi": "10.1088/1367-2630/aa69e7", "report-no": null, "categories": "physics.bio-ph q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a bio-inspired, agent-based approach to describe the natural\nphenomenon of group chasing in both two and three dimensions. Using a set of\nlocal interaction rules we created a continuous-space and discrete-time model\nwith time delay, external noise and limited acceleration. We implemented a\nunique collective chasing strategy, optimized its parameters and studied its\nproperties when chasing a much faster, erratic escaper. We show that collective\nchasing strategies can significantly enhance the chasers' success rate. Our\nrealistic approach handles group chasing within closed, soft boundaries -\ncontrasting most of those published in the literature with periodic ones -- and\nresembles several properties of pursuits observed in nature, such as the\nemergent encircling or the escaper's zigzag motion.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jan 2017 20:01:52 GMT"}, {"version": "v2", "created": "Thu, 23 Feb 2017 14:15:04 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Janosov", "Mil\u00e1n", ""], ["Vir\u00e1gh", "Csaba", ""], ["V\u00e1s\u00e1rhelyi", "G\u00e1bor", ""], ["Vicsek", "Tam\u00e1s", ""]]}, {"id": "1701.00900", "submitter": "Xiufang Shi", "authors": "Xiufang Shi, Guoqiang Mao, Brian.D.O. Anderson, Zaiyue Yang and Jiming\n  Chen", "title": "Robust Localization Using Range Measurements with Unknown and Bounded\n  Errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cooperative geolocation has attracted significant research interests in\nrecent years. A large number of localization algorithms rely on the\navailability of statistical knowledge of measurement errors, which is often\ndifficult to obtain in practice. Compared with the statistical knowledge of\nmeasurement errors, it can often be easier to obtain the measurement error\nbound. This work investigates a localization problem assuming unknown\nmeasurement error distribution except for a bound on the error. We first\nformulate this localization problem as an optimization problem to minimize the\nworst-case estimation error, which is shown to be a non-convex optimization\nproblem. Then, relaxation is applied to transform it into a convex one.\nFurthermore, we propose a distributed algorithm to solve the problem, which\nwill converge in a few iterations. Simulation results show that the proposed\nalgorithms are more robust to large measurement errors than existing algorithms\nin the literature. Geometrical analysis providing additional insights is also\nprovided.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 05:27:13 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Shi", "Xiufang", ""], ["Mao", "Guoqiang", ""], ["Anderson", "Brian. D. O.", ""], ["Yang", "Zaiyue", ""], ["Chen", "Jiming", ""]]}, {"id": "1701.01055", "submitter": "Zhiyong Zhou", "authors": "Zhiyong Zhou and Jun Yu", "title": "Estimation of block sparsity in compressive sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explicitly using the block structure of the unknown signal can achieve better\nreconstruction performance in compressive sensing. Theoretically, an unknown\nsignal with block structure can be accurately recovered from a few number of\nunder-determined linear measurements provided that it is sufficiently block\nsparse. From the practical point of view, a severe concern is that the block\nsparse level appears often unknown. In this paper, we introduce a soft measure\nof block sparsity\n$k_\\alpha(\\mathbf{x})=\\left(\\lVert\\mathbf{x}\\rVert_{2,\\alpha}/\\lVert\\mathbf{x}\\rVert_{2,1}\\right)^{\\frac{\\alpha}{1-\\alpha}}$\nwith $\\alpha\\in[0,\\infty]$, and propose an estimation procedure by using\nmultivariate centered isotropic symmetric $\\alpha$-stable random projections.\nThe limiting distribution of the estimator is established. Simulations are\nconducted to illustrate our theoretical results.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 15:57:58 GMT"}, {"version": "v2", "created": "Fri, 7 Apr 2017 08:11:41 GMT"}, {"version": "v3", "created": "Thu, 3 Jun 2021 03:13:52 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Zhou", "Zhiyong", ""], ["Yu", "Jun", ""]]}, {"id": "1701.01140", "submitter": "Alexander Peysakhovich", "authors": "Alexander Peysakhovich and Dean Eckles", "title": "Learning causal effects from many randomized experiments using\n  regularized instrumental variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific and business practices are increasingly resulting in large\ncollections of randomized experiments. Analyzed together, these collections can\ntell us things that individual experiments in the collection cannot. We study\nhow to learn causal relationships between variables from the kinds of\ncollections faced by modern data scientists: the number of experiments is\nlarge, many experiments have very small effects, and the analyst lacks metadata\n(e.g., descriptions of the interventions). Here we use experimental groups as\ninstrumental variables (IV) and show that a standard method (two-stage least\nsquares) is biased even when the number of experiments is infinite. We show how\na sparsity-inducing l0 regularization can --- in a reversal of the standard\nbias--variance tradeoff in regularization --- reduce bias (and thus error) of\ninterventional predictions. Because we are interested in interventional loss\nminimization we also propose a modified cross-validation procedure (IVCV) to\nfeasibly select the regularization parameter. We show, using a trick from Monte\nCarlo sampling, that IVCV can be done using summary statistics instead of raw\ndata. This makes our full procedure simple to use in many real-world\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 20:04:55 GMT"}, {"version": "v2", "created": "Tue, 28 Mar 2017 04:32:32 GMT"}, {"version": "v3", "created": "Thu, 1 Jun 2017 17:20:48 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Peysakhovich", "Alexander", ""], ["Eckles", "Dean", ""]]}, {"id": "1701.01185", "submitter": "Simon Clinet", "authors": "Simon Clinet and Yoann Potiron", "title": "Efficient asymptotic variance reduction when estimating volatility in\n  high frequency data", "comments": "60 pages, 8 tables, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows how to carry out efficient asymptotic variance reduction\nwhen estimating volatility in the presence of stochastic volatility and\nmicrostructure noise with the realized kernels (RK) from [Barndorff-Nielsen et\nal., 2008] and the quasi-maximum likelihood estimator (QMLE) studied in [Xiu,\n2010]. To obtain such a reduction, we chop the data into B blocks, compute the\nRK (or QMLE) on each block, and aggregate the block estimates. The ratio of\nasymptotic variance over the bound of asymptotic efficiency converges as B\nincreases to the ratio in the parametric version of the problem, i.e. 1.0025 in\nthe case of the fastest RK Tukey-Hanning 16 and 1 for the QMLE. The impact of\nstochastic sampling times and jump in the price process is examined carefully.\nThe finite sample performance of both estimators is investigated in\nsimulations, while empirical work illustrates the gain in practice.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 00:13:36 GMT"}, {"version": "v2", "created": "Fri, 7 Jul 2017 06:16:29 GMT"}, {"version": "v3", "created": "Wed, 27 Jun 2018 01:57:31 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Clinet", "Simon", ""], ["Potiron", "Yoann", ""]]}, {"id": "1701.01206", "submitter": "Nan Xu", "authors": "Nan Xu, Peter C Doerschuk", "title": "Reconstruction of stochastic 3-D signals with symmetric statistics from\n  2-D projection images motivated by cryo-electron microscopy", "comments": "16 pages, 10 figures", "journal-ref": "IEEE Transactions on Image Processing On page(s): 1-16 Print ISSN:\n  1057-7149 Online ISSN: 1941-0042", "doi": "10.1109/TIP.2019.2915631", "report-no": null, "categories": "stat.AP physics.data-an q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cryo-electron microscopy provides 2-D projection images of the 3-D electron\nscattering intensity of many instances of the particle under study (e.g., a\nvirus). Both symmetry (rotational point groups) and heterogeneity are important\naspects of biological particles and both aspects can be combined by describing\nthe electron scattering intensity of the particle as a stochastic process with\na symmetric probability law and therefore symmetric moments. A maximum\nlikelihood estimator implemented by an expectation-maximization algorithm is\ndescribed which estimates the unknown statistics of the electron scattering\nintensity stochastic process from images of instances of the particle. The\nalgorithm is demonstrated on the bacteriophage HK97 and the virus N$\\omega$V.\nThe results are contrasted with existing algorithms which assume that each\ninstance of the particle has the symmetry rather than the less restrictive\nassumption that the probability law has the symmetry.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 04:30:35 GMT"}, {"version": "v2", "created": "Fri, 6 Jul 2018 05:29:47 GMT"}, {"version": "v3", "created": "Fri, 8 Feb 2019 22:13:33 GMT"}, {"version": "v4", "created": "Thu, 16 May 2019 05:22:16 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Xu", "Nan", ""], ["Doerschuk", "Peter C", ""]]}, {"id": "1701.01219", "submitter": "Geoffrey Goodhill", "authors": "Geoffrey J Goodhill", "title": "Is neuroscience facing up to statistical power?", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been demonstrated that the statistical power of many neuroscience\nstudies is very low, so that the results are unlikely to be robustly\nreproducible. How are neuroscientists and the journals in which they publish\nresponding to this problem? Here I review the sample size justifications\nprovided for all 15 papers published in one recent issue of the leading journal\nNature Neuroscience. Of these, only one claimed it was adequately powered. The\nothers mostly appealed to the sample sizes used in earlier studies, despite a\nlack of evidence that these earlier studies were adequately powered. Thus,\nconcerns regarding statistical power in neuroscience have mostly not yet been\naddressed.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 06:07:48 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Goodhill", "Geoffrey J", ""]]}, {"id": "1701.01286", "submitter": "Thea Bj{\\o}rnland", "authors": "Thea Bj{\\o}rnland, Anja Bye, Einar Ryeng, Ulrik Wisl{\\o}ff, Mette\n  Langaas", "title": "Powerful extreme phenotype sampling designs and score tests for genetic\n  association studies", "comments": null, "journal-ref": "Statistics in Medicine. 2018; 37: 4234-4251", "doi": "10.1002/sim.7914", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider cross-sectional genetic association studies (common and rare\nvariants) where non-genetic information is available, or feasible to obtain for\n$N$ individuals, but where it is infeasible to genotype all $N$ individuals. We\nconsider continuously measurable Gaussian traits (phenotypes). Genotyping $n<N$\nextreme phenotype individuals can yield better power to detect\nphenotype-genotype associations, as compared to randomly selecting $n$\nindividuals. We define a person as having an extreme phenotype if the observed\nphenotype is above a specified threshold or below a specified thresholds. We\nconsider a model where these thresholds can be tailored to each individual. The\nclassical extreme sampling design is to set equal thresholds for all\nindividuals. We introduce a design ($z$-extreme sampling) where personalized\nthresholds are defined based on the residuals of a regression model including\nonly non-genetic (fully available) information. We derive score tests for the\nsituation where only $n$ extremes are analyzed (complete case analysis), and\nfor the situation where the non-genetic information on $N-n$ non-extremes is\nincluded in the analysis (all case analysis). For the classical design, all\ncase analysis is generally more powerful than complete case analysis. For the\n$z$-extreme sample, we show that all case and complete case tests are equally\npowerful. Simulations and data analysis also show that $z$-extreme sampling is\nat least as powerful as the classical extreme sampling design and the classical\ndesign is shown to be at times less powerful than random sampling. The method\nof dichotomizing extreme phenotypes is also discussed.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 11:59:04 GMT"}, {"version": "v2", "created": "Wed, 5 Feb 2020 09:37:17 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Bj\u00f8rnland", "Thea", ""], ["Bye", "Anja", ""], ["Ryeng", "Einar", ""], ["Wisl\u00f8ff", "Ulrik", ""], ["Langaas", "Mette", ""]]}, {"id": "1701.01315", "submitter": "Guillermo Gallardo", "authors": "Guillermo Gallardo (ATHENA), William Wells Iii (HMS), Rachid Deriche\n  (ATHENA), Demian Wassermann (ATHENA)", "title": "Groupwise Structural Parcellation of the Cortex: A Sound Approach Based\n  on Logistic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current theories hold that brain function is highly related to long-range\nphysical connections through axonal bundles, namely extrinsic connectiv-ity.\nHowever, obtaining a groupwise cortical parcellation based on extrinsic\nconnectivity remains challenging. Current parcellation methods are\ncompu-tationally expensive; need tuning of several parameters or rely on ad-hoc\nconstraints. Furthermore, none of these methods present a model for the\ncortical extrinsic connectivity of the cortex. To tackle these problems, we\npropose a parsimonious model for the extrinsic connectivity and an efficient\nparceling technique based on clustering of tractograms. Our technique allows\nthe creation of single subject and groupwise parcellations of the whole cortex.\nThe parcellations obtained with our technique are in agreement with structural\nand functional parcellations in the literature. In particular, the motor and\nsensory cortex are subdivided in agreement with the human ho-munculus of\nPenfield. We illustrate this by comparing our resulting parcels with the motor\nstrip mapping included in the Human Connectome Project data.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 13:52:57 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Gallardo", "Guillermo", "", "ATHENA"], ["Wells", "William", "Iii", "HMS"], ["Deriche", "Rachid", "", "ATHENA"], ["Wassermann", "Demian", "", "ATHENA"]]}, {"id": "1701.01352", "submitter": "Thakshila Wimalajeewa", "authors": "Thakshila Wimalajeewa and Pramod K. Varshney", "title": "Compressive Sensing-Based Detection with Multimodal Dependent Data", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2017.2770100", "report-no": null, "categories": "stat.AP cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection with high dimensional multimodal data is a challenging problem when\nthere are complex inter- and intra- modal dependencies. While several\napproaches have been proposed for dependent data fusion (e.g., based on copula\ntheory), their advantages come at a high price in terms of computational\ncomplexity. In this paper, we treat the detection problem with compressive\nsensing (CS) where compression at each sensor is achieved via low dimensional\nrandom projections. CS has recently been exploited to solve detection problems\nunder various assumptions on the signals of interest, however, its potential\nfor dependent data fusion has not been explored adequately. We exploit the\ncapability of CS to capture statistical properties of uncompressed data in\norder to compute decision statistics for detection in the compressed domain.\nFirst, a Gaussian approximation is employed to perform likelihood ratio (LR)\nbased detection with compressed data. In this approach, inter-modal dependence\nis captured via a compressed version of the covariance matrix of the\nconcatenated (temporally and spatially) uncompressed data vector. We show that,\nunder certain conditions, this approach with a small number of compressed\nmeasurements per node leads to enhanced performance compared to detection with\nuncompressed data using widely considered suboptimal approaches. Second, we\ndevelop a nonparametric approach where a decision statistic based on the second\norder statistics of uncompressed data is computed in the compressed domain. The\nsecond approach is promising over other related nonparametric approaches and\nthe first approach when multimodal data is highly correlated at the expense of\nslightly increased computational complexity.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 15:29:01 GMT"}, {"version": "v2", "created": "Thu, 20 Jul 2017 20:16:34 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Wimalajeewa", "Thakshila", ""], ["Varshney", "Pramod K.", ""]]}, {"id": "1701.01558", "submitter": "Seung Jun Shin", "authors": "Seung Jun Shin, Ying Yuan, Louise C. Strong, Jasmina Bojadzieva, Wenyi\n  Wang", "title": "Bayesian Semiparametric Estimation of Cancer-specific Age-at-onset\n  Penetrance with Application to Li-Fraumeni Syndrome", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Penetrance, which plays a key role in genetic research, is defined as the\nproportion of individuals with the genetic variants (i.e., {genotype}) that\ncause a particular trait and who have clinical symptoms of the trait (i.e.,\n{phenotype}). We propose a Bayesian semiparametric approach to estimate the\ncancer-specific age-at-onset penetrance in the presence of the competing risk\nof multiple cancers. We employ a Bayesian semiparametric competing risk model\nto model the duration until individuals in a high-risk group develop different\ncancers, and accommodate family data using family-wise likelihoods. We tackle\nthe ascertainment bias arising when family data are collected through probands\nin a high-risk population in which disease cases are more likely to be\nobserved. We apply the proposed method to a cohort of 186 families with\nLi-Fraumeni syndrome identified through probands with sarcoma treated at MD\nAnderson Cancer Center from 1944 to 1982.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2017 07:20:28 GMT"}, {"version": "v2", "created": "Fri, 20 Jan 2017 06:28:16 GMT"}, {"version": "v3", "created": "Fri, 27 Oct 2017 06:10:12 GMT"}, {"version": "v4", "created": "Thu, 3 May 2018 01:48:48 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Shin", "Seung Jun", ""], ["Yuan", "Ying", ""], ["Strong", "Louise C.", ""], ["Bojadzieva", "Jasmina", ""], ["Wang", "Wenyi", ""]]}, {"id": "1701.01668", "submitter": "Marco Lorenzi", "authors": "Marco Lorenzi, Maurizio Filippone, Daniel C. Alexander, Sebastien\n  Ourselin", "title": "Disease Progression Modeling and Prediction through Random Effect\n  Gaussian Processes and Time Transformation", "comments": "13 pages, 2 figures", "journal-ref": "NeuroImage 2017", "doi": "10.1016/j.neuroimage.2017.08.059", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of statistical approaches for the joint modelling of the\ntemporal changes of imaging, biochemical, and clinical biomarkers is of\nparamount importance for improving the understanding of neurodegenerative\ndisorders, and for providing a reference for the prediction and quantification\nof the pathology in unseen individuals. Nonetheless, the use of disease\nprogression models for probabilistic predictions still requires investigation,\nfor example for accounting for missing observations in clinical data, and for\naccurate uncertainty quantification. We tackle this problem by proposing a\nnovel Gaussian process-based method for the joint modeling of imaging and\nclinical biomarker progressions from time series of individual observations.\nThe model is formulated to account for individual random effects and time\nreparameterization, allowing non-parametric estimates of the biomarker\nevolution, as well as high flexibility in specifying correlation structure, and\ntime transformation models. Thanks to the Bayesian formulation, the model\nnaturally accounts for missing data, and allows for uncertainty quantification\nin the estimate of evolutions, as well as for probabilistic prediction of\ndisease staging in unseen patients. The experimental results show that the\nproposed model provides a biologically plausible description of the evolution\nof Alzheimer's pathology across the whole disease time-span as well as\nremarkable predictive performance when tested on a large clinical cohort with\nmissing observations.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2017 15:38:17 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Lorenzi", "Marco", ""], ["Filippone", "Maurizio", ""], ["Alexander", "Daniel C.", ""], ["Ourselin", "Sebastien", ""]]}, {"id": "1701.01787", "submitter": "Hana Sevcikova", "authors": "Hana Sevcikova, Adrian E. Raftery, Patrick Gerland", "title": "Probabilistic Projection of Subnational Total Fertility Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of probabilistic projection of the total fertility\nrate (TFR) for subnational regions. We seek a method that is consistent with\nthe UN's recently adopted Bayesian method for probabilistic TFR projections for\nall countries, and works well for all countries. We assess various possible\nmethods using subnational TFR data for 47 countries. We find that the method\nthat performs best in terms of out-of-sample predictive performance and also in\nterms of reproducing the within-country correlation in TFR is a method that\nscales the national trajectory by a region-specific scale factor that is\nallowed to vary slowly over time. This supports the hypothesis of Watkins\n(1990, 1991) that within-country TFR converges over time in response to\ncountry-specific factors, and extends the Watkins hypothesis to the last 50\nyears and to a much wider range of countries around the world.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jan 2017 02:15:13 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Sevcikova", "Hana", ""], ["Raftery", "Adrian E.", ""], ["Gerland", "Patrick", ""]]}, {"id": "1701.01917", "submitter": "Lina Zhu", "authors": "Xun Zhou, Changle Li, Zhe Liu, Tom H. Luan, Zhifang Miao, Lina Zhu and\n  Lei Xiong", "title": "See the Near Future: A Short-Term Predictive Methodology to Traffic Load\n  in ITS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Intelligent Transportation System (ITS) targets to a coordinated traffic\nsystem by applying the advanced wireless communication technologies for road\ntraffic scheduling. Towards an accurate road traffic control, the short-term\ntraffic forecasting to predict the road traffic at the particular site in a\nshort period is often useful and important. In existing works, Seasonal\nAutoregressive Integrated Moving Average (SARIMA) model is a popular approach.\nThe scheme however encounters two challenges: 1) the analysis on related data\nis insufficient whereas some important features of data may be neglected; and\n2) with data presenting different features, it is unlikely to have one\npredictive model that can fit all situations. To tackle above issues, in this\nwork, we develop a hybrid model to improve accuracy of SARIMA. In specific, we\nfirst explore the autocorrelation and distribution features existed in traffic\nflow to revise structure of the time series model. Based on the Gaussian\ndistribution of traffic flow, a hybrid model with a Bayesian learning algorithm\nis developed which can effectively expand the application scenarios of SARIMA.\nWe show the efficiency and accuracy of our proposal using both analysis and\nexperimental studies. Using the real-world trace data, we show that the\nproposed predicting approach can achieve satisfactory performance in practice.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jan 2017 06:11:34 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Zhou", "Xun", ""], ["Li", "Changle", ""], ["Liu", "Zhe", ""], ["Luan", "Tom H.", ""], ["Miao", "Zhifang", ""], ["Zhu", "Lina", ""], ["Xiong", "Lei", ""]]}, {"id": "1701.02245", "submitter": "Takashi Shinzato", "authors": "Takashi Shinzato", "title": "Property Safety Stock Policy for Correlated Commodities Based on\n  Probability Inequality", "comments": "16 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC q-fin.CP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deriving the optimal safety stock quantity with which to meet customer\nsatisfaction is one of the most important topics in stock management. However,\nit is difficult to control the stock management of correlated marketable\nmerchandise when using an inventory control method that was developed under the\nassumption that the demands are not correlated. For this, we propose a\ndeterministic approach that uses a probability inequality to derive a\nreasonable safety stock for the case in which we know the correlation between\nvarious commodities. Moreover, over a given lead time, the relation between the\nappropriate safety stock and the allowable stockout rate is analytically\nderived, and the potential of our proposed procedure is validated by numerical\nexperiments.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 16:28:11 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Shinzato", "Takashi", ""]]}, {"id": "1701.02359", "submitter": "Tapio Pahikkala", "authors": "Markus Viljanen, Antti Airola, Jukka Heikkonen, Tapio Pahikkala", "title": "Playtime Measurement with Survival Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximizing product use is a central goal of many businesses, which makes\nretention and monetization two central analytics metrics in games. Player\nretention may refer to various duration variables quantifying product use:\ntotal playtime or session playtime are popular research targets, and active\nplaytime is well-suited for subscription games. Such research often has the\ngoal of increasing player retention or conversely decreasing player churn.\nSurvival analysis is a framework of powerful tools well suited for retention\ntype data. This paper contributes new methods to game analytics on how playtime\ncan be analyzed using survival analysis without covariates. Survival and hazard\nestimates provide both a visual and an analytic interpretation of the playtime\nphenomena as a funnel type nonparametric estimate. Metrics based on the\nsurvival curve can be used to aggregate this playtime information into a single\nstatistic. Comparison of survival curves between cohorts provides a scientific\nAB-test. All these methods work on censored data and enable computation of\nconfidence intervals. This is especially important in time and sample limited\ndata which occurs during game development. Throughout this paper, we illustrate\nthe application of these methods to real world game development problems on the\nHipster Sheep mobile game.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 10:25:04 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Viljanen", "Markus", ""], ["Airola", "Antti", ""], ["Heikkonen", "Jukka", ""], ["Pahikkala", "Tapio", ""]]}, {"id": "1701.02373", "submitter": "Bertrand Iooss", "authors": "G\\'eraud Blatman (EDF R\\&D), Thibault Delage (EDF R\\&D), Bertrand\n  Iooss (EDF R\\&D, IMT, GdR MASCOT-NUM), Nadia P\\'erot (GdR MASCOT-NUM, DER)", "title": "Probabilistic risk bounds for the characterization of radiological\n  contamination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The radiological characterization of contaminated elements (walls, grounds,\nobjects) from nuclear facilities often suffers from a too small number of\nmeasurements. In order to determine risk prediction bounds on the level of\ncontamination, some classic statistical methods may then reveal unsuited as\nthey rely upon strong assumptions (e.g. that the underlying distribution is\nGaussian) which cannot be checked. Considering that a set of measurements or\ntheir average value arise from a Gaussian distribution can sometimes lead to\nerroneous conclusion, possibly underconservative. This paper presents several\nalternative statistical approaches which are based on much weaker hypotheses\nthan Gaussianity. They result from general probabilistic inequalities and\norder-statistics based formula. Given a data sample, these inequalities make it\npossible to derive prediction intervals for a random variable, which can be\ndirectly interpreted as probabilistic risk bounds. For the sake of validation,\nthey are first applied to synthetic data samples generated from several known\ntheoretical distributions. In a second time, the proposed methods are applied\nto two data sets obtained from real radiological contamination measurements.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 14:33:57 GMT"}, {"version": "v2", "created": "Sat, 27 May 2017 10:31:47 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Blatman", "G\u00e9raud", "", "EDF R\\&D"], ["Delage", "Thibault", "", "EDF R\\&D"], ["Iooss", "Bertrand", "", "EDF R\\&D, IMT, GdR MASCOT-NUM"], ["P\u00e9rot", "Nadia", "", "GdR MASCOT-NUM, DER"]]}, {"id": "1701.02643", "submitter": "Pantelis Samartsidis", "authors": "Pantelis Samartsidis, Claudia R. Eickhoff, Simon B. Eickhoff, Tor D.\n  Wager, Lisa Feldman Barrett, Shir Atzil, Timothy D. Johnson and Thomas E.\n  Nichols", "title": "Bayesian log-Gaussian Cox process regression: applications to\n  meta-analysis of neuroimaging working memory studies", "comments": null, "journal-ref": "JRSSC (Applied Statistics) 68, Part 1, 217-234 (2019)", "doi": "10.1111/rssc.12295", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Working memory (WM) was one of the first cognitive processes studied with\nfunctional magnetic resonance imaging. With now over 20 years of studies on WM,\neach study with tiny sample sizes, there is a need for meta-analysis to\nidentify the brain regions that are consistently activated by WM tasks, and to\nunderstand the interstudy variation in those activations. However, current\nmethods in the field cannot fully account for the spatial nature of\nneuroimaging meta-analysis data or the heterogeneity observed among WM studies.\nIn this work, we propose a fully Bayesian random-effects metaregression model\nbased on log-Gaussian Cox processes, which can be used for meta-analysis of\nneuroimaging studies. An efficient Markov chain Monte Carlo scheme for\nposterior simulations is presented which makes use of some recent advances in\nparallel computing using graphics processing units. Application of the proposed\nmodel to a real data set provides valuable insights regarding the function of\nthe WM.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 15:43:27 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 18:04:38 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Samartsidis", "Pantelis", ""], ["Eickhoff", "Claudia R.", ""], ["Eickhoff", "Simon B.", ""], ["Wager", "Tor D.", ""], ["Barrett", "Lisa Feldman", ""], ["Atzil", "Shir", ""], ["Johnson", "Timothy D.", ""], ["Nichols", "Thomas E.", ""]]}, {"id": "1701.02856", "submitter": "Tracy Holsclaw", "authors": "Tracy Holsclaw, Arthur M. Greene, Andrew W. Robertson, Padhraic Smyth", "title": "Bayesian Non-Homogeneous Markov Models via Polya-Gamma Data Augmentation\n  with Applications to Rainfall Modeling", "comments": "40 pages, 26 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete-time hidden Markov models are a broadly useful class of\nlatent-variable models with applications in areas such as speech recognition,\nbioinformatics, and climate data analysis. It is common in practice to\nintroduce temporal non-homogeneity into such models by making the transition\nprobabilities dependent on time-varying exogenous input variables via a\nmultinomial logistic parametrization. We extend such models to introduce\nadditional non-homogeneity into the emission distribution using a generalized\nlinear model (GLM), with data augmentation for sampling-based inference.\nHowever, the presence of the logistic function in the state transition model\nsignificantly complicates parameter inference for the overall model,\nparticularly in a Bayesian context. To address this we extend the\nrecently-proposed Polya-Gamma data augmentation approach to handle\nnon-homogeneous hidden Markov models (NHMMs), allowing the development of an\nefficient Markov chain Monte Carlo (MCMC) sampling scheme. We apply our model\nand inference scheme to 30 years of daily rainfall in India, leading to a\nnumber of insights into rainfall-related phenomena in the region. Our proposed\napproach allows for fully Bayesian analysis of relatively complex NHMMs on a\nscale that was not possible with previous methods. Software implementing the\nmethods described in the paper is available via the R package NHMM.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 06:07:55 GMT"}, {"version": "v2", "created": "Fri, 13 Jan 2017 02:47:42 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Holsclaw", "Tracy", ""], ["Greene", "Arthur M.", ""], ["Robertson", "Andrew W.", ""], ["Smyth", "Padhraic", ""]]}, {"id": "1701.02942", "submitter": "Thomas Nichols", "authors": "Thomas E. Nichols, Anders Eklund, Hans Knutsson", "title": "A defense of using resting state fMRI as null data for estimating false\n  positive rates", "comments": "Update: Title changed to be more informative, abstract expanded. Body\n  text unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent Editorial by Slotnick (2017) reconsiders the findings of our paper\non the accuracy of false positive rate control with cluster inference in fMRI\n(Eklund et al, 2016), in particular criticising our use of resting state fMRI\ndata as a source for null data in the evaluation of task fMRI methods. We\ndefend this use of resting fMRI data, as while there is much structure in this\ndata, we argue it is representative of task data noise and as such analysis\nsoftware should be able to accommodate this noise. We also discuss a potential\nproblem with Slotnick's own method.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 12:12:33 GMT"}, {"version": "v2", "created": "Thu, 12 Jan 2017 13:36:18 GMT"}, {"version": "v3", "created": "Wed, 18 Jan 2017 13:33:16 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Nichols", "Thomas E.", ""], ["Eklund", "Anders", ""], ["Knutsson", "Hans", ""]]}, {"id": "1701.02950", "submitter": "Antonio Canale", "authors": "Antonio Canale, Daniele Durante, David Dunson", "title": "Convex Mixture Regression for Quantitative Risk Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is wide interest in studying how the distribution of a continuous\nresponse changes with a predictor. We are motivated by environmental\napplications in which the predictor is the dose of an exposure and the response\nis a health outcome. A main focus in these studies is inference on dose levels\nassociated with a given increase in risk relative to a baseline. Popular\nmethods either dichotomize the continuous response or focus on modeling changes\nwith the dose in the expectation of the outcome. Such choices may lead to\ninformation loss and provide inaccurate inference on dose-response\nrelationships. We instead propose a Bayesian convex mixture regression model\nthat allows the entire distribution of the health outcome to be unknown and\nchanging with the dose. To balance flexibility and parsimony, we rely on a\nmixture model for the density at the extreme doses, and express the conditional\ndensity at each intermediate dose via a convex combination of these extremal\ndensities. This representation generalizes classical dose-response models for\nquantitative outcomes, and provides a more parsimonious, but still powerful,\nformulation compared to nonparametric methods, thereby improving\ninterpretability and efficiency in inference on risk functions. A Markov chain\nMonte Carlo algorithm for posterior inference is developed, and the benefits of\nour methods are outlined in simulations, along with a study on the impact of\nDDT exposure on gestational age.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 12:30:53 GMT"}, {"version": "v2", "created": "Wed, 1 Feb 2017 10:26:15 GMT"}, {"version": "v3", "created": "Sat, 12 Aug 2017 21:11:20 GMT"}, {"version": "v4", "created": "Thu, 30 Nov 2017 08:56:10 GMT"}, {"version": "v5", "created": "Tue, 24 Apr 2018 19:53:53 GMT"}, {"version": "v6", "created": "Wed, 9 May 2018 14:54:28 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Canale", "Antonio", ""], ["Durante", "Daniele", ""], ["Dunson", "David", ""]]}, {"id": "1701.03095", "submitter": "Panagiotis Papastamoulis", "authors": "Panagiotis Papastamoulis and Magnus Rattray", "title": "Bayesian estimation of Differential Transcript Usage from RNA-seq data", "comments": "Revised version, accepted to Statistical Applications in Genetics and\n  Molecular Biology", "journal-ref": null, "doi": "10.1515/sagmb-2017-0005", "report-no": null, "categories": "q-bio.GN stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Next generation sequencing allows the identification of genes consisting of\ndifferentially expressed transcripts, a term which usually refers to changes in\nthe overall expression level. A specific type of differential expression is\ndifferential transcript usage (DTU) and targets changes in the relative within\ngene expression of a transcript. The contribution of this paper is to: (a)\nextend the use of cjBitSeq to the DTU context, a previously introduced Bayesian\nmodel which is originally designed for identifying changes in overall\nexpression levels and (b) propose a Bayesian version of DRIMSeq, a frequentist\nmodel for inferring DTU. cjBitSeq is a read based model and performs fully\nBayesian inference by MCMC sampling on the space of latent state of each\ntranscript per gene. BayesDRIMSeq is a count based model and estimates the\nBayes Factor of a DTU model against a null model using Laplace's approximation.\nThe proposed models are benchmarked against the existing ones using a recent\nindependent simulation study as well as a real RNA-seq dataset. Our results\nsuggest that the Bayesian methods exhibit similar performance with DRIMSeq in\nterms of precision/recall but offer better calibration of False Discovery Rate.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 18:51:54 GMT"}, {"version": "v2", "created": "Thu, 28 Sep 2017 14:05:32 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Papastamoulis", "Panagiotis", ""], ["Rattray", "Magnus", ""]]}, {"id": "1701.03139", "submitter": "Luke Miratrix", "authors": "Luke Miratrix, Jane Furey, Avi Feller, Todd Grindal, Lindsay C. Page", "title": "Bounding, an accessible method for estimating principal causal effects,\n  examined and explained", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating treatment effects for subgroups defined by post-treatment behavior\n(i.e., estimating causal effects in a principal stratification framework) can\nbe technically challenging and heavily reliant on strong assumptions. We\ninvestigate an alternative path: using bounds to identify ranges of possible\neffects that are consistent with the data. This simple approach relies on fewer\nassumptions and yet can result in policy-relevant findings. As we show,\ncovariates can be used to substantially tighten bounds in a straightforward\nmanner. Via simulation, we demonstrate which types of covariates are maximally\nbeneficial. We conclude with an analysis of a multi-site experimental study of\nEarly College High Schools. When examining the program's impact on students\ncompleting the ninth grade \"on-track\" for college, we find little impact for\nECHS students who would otherwise attend a high quality high school, but\nsubstantial effects for those who would not. This suggests potential benefit in\nexpanding these programs in areas primarily served by lower quality schools.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 19:42:33 GMT"}, {"version": "v2", "created": "Wed, 16 Aug 2017 11:49:15 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Miratrix", "Luke", ""], ["Furey", "Jane", ""], ["Feller", "Avi", ""], ["Grindal", "Todd", ""], ["Page", "Lindsay C.", ""]]}, {"id": "1701.03159", "submitter": "Royi Jacobovic", "authors": "Royi Jacobovic", "title": "Are Thousands of Samples Really Needed to Generate Robust Gene-List for\n  Prediction of Cancer Outcome?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prediction of cancer prognosis and metastatic potential immediately after\nthe initial diagnoses is a major challenge in current clinical research. The\nrelevance of such a signature is clear, as it will free many patients from the\nagony and toxic side-effects associated with the adjuvant chemotherapy\nautomatically and sometimes carelessly subscribed to them. Motivated by this\nissue, Ein-Dor (2006) and Zuk (2007) presented a Bayesian model which leads to\nthe following conclusion: Thousands of samples are needed to generate a robust\ngene list for predicting outcome. This conclusion is based on existence of some\nstatistical assumptions. The current work raises doubts over this determination\nby showing that: (1) These assumptions are not consistent with additional\nassumptions such as sparsity and Gaussianity. (2) The empirical Bayes\nmethodology which was suggested in order to test the relevant assumptions\ndoesn't detect severe violations of the model assumptions and consequently an\noverestimation of the required sample size might be incurred.\n", "versions": [{"version": "v1", "created": "Mon, 26 Dec 2016 09:43:06 GMT"}, {"version": "v2", "created": "Sun, 15 Oct 2017 11:59:04 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Jacobovic", "Royi", ""]]}, {"id": "1701.03161", "submitter": "Avishai Wagner", "authors": "Avishai Wagner, Naama Fixler, Yehezkel S. Resheff", "title": "A Wavelet-Based Approach To Monitoring Parkinson's Disease Symptoms", "comments": "ICASSP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parkinson's disease is a neuro-degenerative disorder affecting tens of\nmillions of people worldwide. Lately, there has been considerable interest in\nsystems for at-home monitoring of patients, using wearable devices which\ncontain inertial measurement units. We present a new wavelet-based approach for\nanalysis of data from single wrist-worn smart-watches, and show high detection\nperformance for tremor, bradykinesia, and dyskinesia, which have been the major\ntargets for monitoring in this context. We also discuss the implication of our\ncontrolled-experiment results for uncontrolled home monitoring of freely\nbehaving patients.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 18:03:44 GMT"}, {"version": "v2", "created": "Mon, 6 Feb 2017 19:25:16 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Wagner", "Avishai", ""], ["Fixler", "Naama", ""], ["Resheff", "Yehezkel S.", ""]]}, {"id": "1701.03162", "submitter": "Yifan Yang", "authors": "Yifan Yang and Tian Qin and Yu-Heng Lei", "title": "Real-time eSports Match Result Prediction", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we try to predict the winning team of a match in the\nmultiplayer eSports game Dota 2. To address the weaknesses of previous work, we\nconsider more aspects of prior (pre-match) features from individual players'\nmatch history, as well as real-time (during-match) features at each minute as\nthe match progresses. We use logistic regression, the proposed Attribute\nSequence Model, and their combinations as the prediction models. In a dataset\nof 78362 matches where 20631 matches contain replay data, our experiments show\nthat adding more aspects of prior features improves accuracy from 58.69% to\n71.49%, and introducing real-time features achieves up to 93.73% accuracy when\npredicting at the 40th minute.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2016 06:30:25 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Yang", "Yifan", ""], ["Qin", "Tian", ""], ["Lei", "Yu-Heng", ""]]}, {"id": "1701.03550", "submitter": "Yong Huang", "authors": "Yong Huang, James L. Beck and Hui Li", "title": "Bayesian System Identification based on Hierarchical Sparse Bayesian\n  Learning and Gibbs Sampling with Application to Structural Damage Assessment", "comments": "12 figures", "journal-ref": null, "doi": "10.1016/j.cma.2017.01.030", "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The focus in this paper is Bayesian system identification based on noisy\nincomplete modal data where we can impose spatially-sparse stiffness changes\nwhen updating a structural model. To this end, based on a similar hierarchical\nsparse Bayesian learning model from our previous work, we propose two Gibbs\nsampling algorithms. The algorithms differ in their strategies to deal with the\nposterior uncertainty of the equation-error precision parameter, but both\nsample from the conditional posterior probability density functions (PDFs) for\nthe structural stiffness parameters and system modal parameters. The effective\ndimension for the Gibbs sampling is low because iterative sampling is done from\nonly three conditional posterior PDFs that correspond to three parameter\ngroups, along with sampling of the equation-error precision parameter from\nanother conditional posterior PDF in one of the algorithms where it is not\nintegrated out as a \"nuisance\" parameter. A nice feature from a computational\nperspective is that it is not necessary to solve a nonlinear eigenvalue problem\nof a structural model. The effectiveness and robustness of the proposed\nalgorithms are illustrated by applying them to the IASE-ASCE Phase II simulated\nand experimental benchmark studies. The goal is to use incomplete modal data\nidentified before and after possible damage to detect and assess\nspatially-sparse stiffness reductions induced by any damage. Our past and\ncurrent focus on meeting challenges arising from Bayesian inference of\nstructural stiffness serve to strengthen the capability of vibration-based\nstructural system identification but our methods also have much broader\napplicability for inverse problems in science and technology where system\nmatrices are to be inferred from noisy partial information about their\neigenquantities.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 02:51:37 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Huang", "Yong", ""], ["Beck", "James L.", ""], ["Li", "Hui", ""]]}, {"id": "1701.03569", "submitter": "Debasis Kundu Professor", "authors": "Vahid Nekoukhou and Debasis Kundu", "title": "Bivariate Discrete Generalized Exponential Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop a bivariate discrete generalized exponential\ndistribution, whose marginals are discrete generalized exponential distribution\nas proposed by Nekoukhou, Alamatsaz and Bidram (\"Discrete generalized\nexponential distribution of a second type\", Statistics, 47, 876 - 887, 2013).\nIt is observed that the proposed bivariate distribution is a very flexible\ndistribution and the bivariate geometric distribution can be obtained as a\nspecial case of this distribution. The proposed distribution can be seen as a\nnatural discrete analogue of the bivariate generalized exponential distribution\nproposed by Kundu and Gupta (\"Bivariate generalized exponential distribution\",\nJournal of Multivariate Analysis, 100, 581 - 593, 2009). We study different\nproperties of this distribution and explore its dependence structures. We\npropose a new EM algorithm to compute the maximum likelihood estimators of the\nunknown parameters which can be implemented very efficiently, and discuss some\ninferential issues also. The analysis of one data set has been performed to\nshow the effectiveness of the proposed model. Finally we propose some open\nproblems and conclude the paper.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 06:03:45 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Nekoukhou", "Vahid", ""], ["Kundu", "Debasis", ""]]}, {"id": "1701.03770", "submitter": "Dominik Hartmann", "authors": "Dominik Hartmann, Cristian Jara-Figueroa, Miguel Guevara, Alex Simoes,\n  C\\'esar A. Hidalgo", "title": "The structural constraints of income inequality in Latin America", "comments": null, "journal-ref": "Integration & Trade Journal, No. 40, June 2016, p.70-85", "doi": null, "report-no": null, "categories": "q-fin.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that a country's productive structure constrains its\nlevel of economic growth and income inequality. Here, we compare the productive\nstructure of countries in Latin America and the Caribbean (LAC) with that of\nChina and other High-Performing Asian Economies (HPAE) to expose the increasing\ngap in their productive capabilities. Moreover, we use the product space and\nthe Product Gini Index to reveal the structural constraints on income\ninequality. Our network maps reveal that HPAE have managed to diversify into\nproducts typically produced by countries with low levels of income inequality,\nwhile LAC economies have remained dependent on products related to high levels\nof income inequality. We also introduce the Xgini, a coefficient that captures\nthe constraints on income inequality imposed by the mix of products a country\nmakes. Finally, we argue that LAC countries need to emphasize a smart\ncombination of social and economic policies to overcome the structural\nconstraints for inclusive growth.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 18:35:12 GMT"}, {"version": "v2", "created": "Tue, 17 Jan 2017 08:04:37 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Hartmann", "Dominik", ""], ["Jara-Figueroa", "Cristian", ""], ["Guevara", "Miguel", ""], ["Simoes", "Alex", ""], ["Hidalgo", "C\u00e9sar A.", ""]]}, {"id": "1701.03822", "submitter": "Lazhar Benkhelifa", "authors": "Lazhar Benkhelifa", "title": "Efficient estimation in the Topp-Leone distribution", "comments": "12pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the current paper, the estimation of the probability density function and\nthe cumulative distribution function of the Topp-Leone distribution is\nconsidered. We derive the following estimators: maximum likelihood estimator,\nuniformly minimum variance unbiased estimator, percentile estimator, least\nsquares estimator and weighted least squares estimator. A simulation study\nshows that the maximum likelihood estimator is more efficient than the others\nestimators.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 20:15:38 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Benkhelifa", "Lazhar", ""]]}, {"id": "1701.03935", "submitter": "Pieter Segaert", "authors": "V\\'aclav Plevka, Pieter Segaert, Chris M. J. Tamp\\`ere, Mia Hubert", "title": "Analysis of travel activity determinants using robust statistics", "comments": null, "journal-ref": "Transportation (2016) 43:979", "doi": "10.1007/s11116-016-9718-2", "report-no": null, "categories": "stat.AP physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study investigates travel behavior determinants based on a multiday\ntravel survey conducted in the region of Ghent, Belgium. Due to the limited\ndata reliability of the data sample and the influence of outliers exerted on\nclassical principal component analysis, robust principal component analysis\n(ROBPCA) is employed in order to reveal the explanatory variables responsible\nfor most of the variability. Interpretation of the results is eased by\nutilizing ROSPCA. The application of ROSPCA reveals six distinct principal\ncomponents where each is determined by a few variables. Among others, our\nresults suggest a key role of variable categories such as journey\npurpose-related impedance and journey inherent constraints. Surprisingly, the\nvariables associated with journey timing turn out to be less important.\nFinally, our findings reveal the critical role of outliers in travel behavior\nanalysis. This suggests that a systematic understanding of how outliers\ncontribute to observed mobility behavior patterns, as derived from travel\nsurveys, is needed. In this regard, the proposed methods serve for processing\nraw data typically used in activity-based modelling.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jan 2017 15:40:36 GMT"}], "update_date": "2017-02-12", "authors_parsed": [["Plevka", "V\u00e1clav", ""], ["Segaert", "Pieter", ""], ["Tamp\u00e8re", "Chris M. J.", ""], ["Hubert", "Mia", ""]]}, {"id": "1701.03959", "submitter": "Oliver Urs Lenz", "authors": "Oliver Urs Lenz, Daniel L Oberski", "title": "A test for monitoring under- and overtreatment in Dutch hospitals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over- and undertreatment harm patients and society and confound other\nhealthcare quality measures. Despite a growing body of research covering\nspecific conditions, we lack tools to systematically detect and measure over-\nand undertreatment in hospitals. We demonstrate a test used to monitor over-\nand undertreatment in Dutch hospitals, and illustrate its results applied to\nthe aggregated administrative treatment data of 1,836,349 patients at 89\nhospitals in 2013. We employ a random effects model to create risk-adjusted\nfunnel plots that account for natural variation among hospitals, allowing us to\nestimate a measure of overtreatment and undertreatment when hospitals fall\noutside the control limits. The results of this test are not definitive,\nfindings were discussed with hospitals to improve the model and to enable the\nhospitals to make informed treatment decisions.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jan 2017 19:40:13 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Lenz", "Oliver Urs", ""], ["Oberski", "Daniel L", ""]]}, {"id": "1701.04177", "submitter": "Peng Ding", "authors": "Peng Ding, Tyler VanderWeele, James Robins", "title": "Instrumental variables as bias amplifiers with general outcome and\n  confounding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drawing causal inference with observational studies is the central pillar of\nmany disciplines. One sufficient condition for identifying the causal effect is\nthat the treatment-outcome relationship is unconfounded conditional on the\nobserved covariates. It is often believed that the more covariates we condition\non, the more plausible this unconfoundedness assumption is. This belief has had\na huge impact on practical causal inference, suggesting that we should adjust\nfor all pretreatment covariates. However, when there is unmeasured confounding\nbetween the treatment and outcome, estimators adjusting for some pretreatment\ncovariate might have greater bias than estimators without adjusting for this\ncovariate. This kind of covariate is called a bias amplifier, and includes\ninstrumental variables that are independent of the confounder, and affect the\noutcome only through the treatment. Previously, theoretical results for this\nphenomenon have been established only for linear models. We fill in this gap in\nthe literature by providing a general theory, showing that this phenomenon\nhappens under a wide class of models satisfying certain monotonicity\nassumptions. We further show that when the treatment follows an additive or\nmultiplicative model conditional on the instrumental variable and the\nconfounder, these monotonicity assumptions can be interpreted as the signs of\nthe arrows of the causal diagrams.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 06:00:36 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Ding", "Peng", ""], ["VanderWeele", "Tyler", ""], ["Robins", "James", ""]]}, {"id": "1701.04387", "submitter": "Murilo Soares Pinheiro", "authors": "Murilo S. Pinheiro, Alu\\'isio S. Pinheiro, Denilon S. Carvalho", "title": "A CUSUM approach to the detection of copy-number neutral loss of\n  heterozygosity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several genetic alterations are involved in the genesis and development of\ncancers. The determination of whether and how each genetic alterations\ncontributes to cancer development is fundamental for a complete understanding\nof the human cancer etiology. Loss of heterozygosity (LOH) is one of such\ngenetic phenomenon linked to a variate of diseases and characterized by the\nchange from heterozygosity (the presence of both alleles of a gene) to to\nhomozygosity (presence of only one type of allele) in a particular DNA locus.\nThus identification of DNA regions where LOH has taken place is a important\nissue in the health sciences. In this article we formulate the LOH detection as\nthe identification of change-points in the parameters of a mixture model and\npresent a detection algorithm based on the cumulative sums (CUSUM) method. We\nfound that even under mild contamination our proposal is a fast and reliable\nmethod.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 18:30:16 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Pinheiro", "Murilo S.", ""], ["Pinheiro", "Alu\u00edsio S.", ""], ["Carvalho", "Denilon S.", ""]]}, {"id": "1701.04405", "submitter": "Murilo Soares Pinheiro", "authors": "Murilo S. Pinheiro, Benilton S. Carvalho, Alu\\'isio S. Pinheiro", "title": "Screening and merging algorithm for the detection of copy-number\n  alterations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.GN stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We call change-point problem (CPP) the identification of changes in the\nprobabilistic behavior of a sequence of observations. Solving the CPP involves\ndetecting the number and position of such changes. In genetics the study of how\nand what characteristics of a individual's genetic content might contribute to\nthe occurrence and evolution of cancer has fundamental importance in the\ndiagnosis and treatment of such diseases and can be formulated in the framework\nof chage-point analysis. In this article we propose a modification to a\nexisting method of segmentation with the objective of producing a algorithm\nthat is robust to a variety of sampling distributions and that is adequate for\nmore recent method of accessing DNA copy-number which might require a\nrestriction on the minimum length of a altered segment.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 18:31:40 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Pinheiro", "Murilo S.", ""], ["Carvalho", "Benilton S.", ""], ["Pinheiro", "Alu\u00edsio S.", ""]]}, {"id": "1701.04423", "submitter": "Xiaoqi Zhang", "authors": "Xiaoqi Zhang and John Ringland", "title": "Stochastic Process and Health Data: A Full Maximum Likelihood Method to\n  Hospital Charge and Length of Stay Data", "comments": "29 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the model used in Gardiner et al. (2002) and Polverejan et al.\n(2003) through deriving an explicit expression for the joint probability\ndensity function of hospital charge and length of stay (LOS) under a general\nclass of conditions. Using this joint density function, we can apply the full\nmaximum likelihood method (FML) to estimate the effect of covariates on charge\nand LOS. By FML, the endogeneity issues arisen from the dependence between\ncharge and LOS can be efficiently resolved. As an illustrative example, we\napply our method to real charge and LOS data sampled from New York State\nStatewide Planning and Research Cooperative System 2013 (SPARCS 2013). We\ncompare our fitting result with the fitting to the marginal LOS data generated\nby the widely used Phase-Type model, and conclude that our method is more\nefficient in fitting.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 19:11:33 GMT"}, {"version": "v2", "created": "Tue, 2 May 2017 20:28:25 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Zhang", "Xiaoqi", ""], ["Ringland", "John", ""]]}, {"id": "1701.04438", "submitter": "Akash Malhotra", "authors": "Akash Malhotra, Shailesh Krishna", "title": "A Statistical Analysis of Bowling Performance in Cricket", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a widespread notion in cricketing world that with increasing pace\nthe performance of a bowler improves. Additionally, many commentators believe\nlower order batters to be more vulnerable to pace. The present study puts these\ntwo ubiquitous notions under test by statistically analysing the differences in\nperformance of bowlers from three subpopulations based on average release\nvelocities. Results from one-way ANOVA reveal faster bowlers to be performing\nbetter, in terms of Average and Strike-rate, but no significant differences in\nthe case of Economy rate and CBR. Lower and Middle order batsmen were found to\nbe more vulnerable against faster bowling. However, there was no statistically\nsignificant difference in performance of Fast and Fast-Medium bowlers against a\ntop-order batter.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 19:49:27 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Malhotra", "Akash", ""], ["Krishna", "Shailesh", ""]]}, {"id": "1701.04518", "submitter": "Rohitash Chandra", "authors": "Rohitash Chandra", "title": "Towards prediction of rapid intensification in tropical cyclones with\n  recurrent neural networks", "comments": "Technical Report: Artificial Intelligence and Cybernetics Research\n  Group, Software Foundation, Nausori, Fiji", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem where a tropical cyclone intensifies dramatically within a short\nperiod of time is known as rapid intensification. This has been one of the\nmajor challenges for tropical weather forecasting. Recurrent neural networks\nhave been promising for time series problems which makes them appropriate for\nrapid intensification. In this paper, recurrent neural networks are used to\npredict rapid intensification cases of tropical cyclones from the South Pacific\nand South Indian Ocean regions. A class imbalanced problem is encountered which\nmakes it very challenging to achieve promising performance. A simple strategy\nwas proposed to include more positive cases for detection where the false\npositive rate was slightly improved. The limitations of building an efficient\nsystem remains due to the challenges of addressing the class imbalance problem\nencountered for rapid intensification prediction. This motivates further\nresearch in using innovative machine learning methods.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 03:08:12 GMT"}], "update_date": "2017-02-12", "authors_parsed": [["Chandra", "Rohitash", ""]]}, {"id": "1701.04782", "submitter": "Rossana Mastrandrea", "authors": "Rossana Mastrandrea, Andrea Gabrielli, Fabrizio Piras, Gianfranco\n  Spalletta, Guido Caldarelli and Tommaso Gili", "title": "Organization and hierarchy of the human functional brain network lead to\n  a chain-like core", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC physics.bio-ph physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The brain is a paradigmatic example of a complex system as its functionality\nemerges as a global property of local mesoscopic and microscopic interactions.\nComplex network theory allows to elicit the functional architecture of the\nbrain in terms of links (correlations) between nodes (grey matter regions) and\nto extract information out of the noise. Here we present the analysis of\nfunctional magnetic resonance imaging data from forty healthy humans during the\nresting condition for the investigation of the basal scaffold of the functional\nbrain network organization. We show how brain regions tend to coordinate by\nforming a highly hierarchical chain-like structure of homogeneously clustered\nanatomical areas. A maximum spanning tree approach revealed the centrality of\nthe occipital cortex and the peculiar aggregation of cerebellar regions to form\na closed core. We also report the hierarchy of network segregation and the\nlevel of clusters integration as a function of the connectivity strength\nbetween brain regions.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 17:44:16 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Mastrandrea", "Rossana", ""], ["Gabrielli", "Andrea", ""], ["Piras", "Fabrizio", ""], ["Spalletta", "Gianfranco", ""], ["Caldarelli", "Guido", ""], ["Gili", "Tommaso", ""]]}, {"id": "1701.04858", "submitter": "Joseph Roy", "authors": "Christopher Eager and Joseph Roy", "title": "Mixed Effects Models are Sometimes Terrible", "comments": "Write up for poster presented at Linguistic Society of America 2017:\n  Eager, Christopher and Joseph Roy. Mixed Effects are Sometimes Terrible.\n  Linguistic Society of America, Poster (January 5-8, 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed-effects models have emerged as the gold standard of statistical\nanalysis in different sub-fields of linguistics (Baayen, Davidson & Bates,\n2008; Johnson, 2009; Barr, et al, 2013; Gries, 2015). One problematic feature\nof these models is their failure to converge under maximal (or even\nnear-maximal) random effects structures. The lack of convergence is relatively\nunaddressed in linguistics and when it is addressed has resulted in statistical\npractices (e.g. Jaeger, 2009; Gries, 2015; Bates, et al, 2015b) that are\npremised on the idea that non-convergence is an indication that a random\neffects structure is over-specified (or not parsimonious), the parsimonious\nconvergence hypothesis (PCH). We test the PCH by running simulations in lme4\nunder two sets of assumptions for both a linear dependent variable and a binary\ndependent variable in order to assess the rate of non-convergence for both\ntypes of mixed effects models when a known maximal effect structure is used to\ngenerate the data (i.e. when non-convergence cannot be explained by random\neffects with zero variance). Under the PCH, lack of convergence is treated as\nevidence against a more maximal random effects structure, but that result is\nnot upheld with our simulations. We provide an alternative model, fully\nspecified Bayesian models implemented in rstan (Stan Development Team, 2016;\nCarpenter, et al, in press) that removed the convergence problems almost\nentirely in simulations of the same conditions. These results indicate that\nwhen there is known non-zero variance for all slopes and intercepts, under\nrealistic distributions of data and with moderate to severe imbalance, mixed\neffects models in lme4 have moderate to high non-convergence rates which can\ncause linguistic researchers to wrongfully exclude random effect terms.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 19:10:22 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Eager", "Christopher", ""], ["Roy", "Joseph", ""]]}, {"id": "1701.05177", "submitter": "Christoph Stadtfeld", "authors": "Christoph Stadtfeld and Tom A. B. Snijders and Christian Steglich and\n  Marijtje A. J. van Duijn", "title": "Statistical Power in Longitudinal Network Studies", "comments": null, "journal-ref": null, "doi": "10.1177/0049124118769113", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Longitudinal social network studies can easily suffer from insufficient\nstatistical power. Studies that simultaneously investigate change of network\nties and change of nodal attributes (selection and influence studies) are\nparticularly at risk because the number of nodal observations is typically much\nlower than the number of observed tie variables. This paper presents a\nsimulation-based procedure to evaluate statistical power of longitudinal social\nnetwork studies in which stochastic actor-oriented models (SAOMs) are to be\napplied. Two detailed case studies illustrate how statistical power is strongly\naffected by network size, number of data collection waves, effect sizes,\nmissing data, and participant turnover. These issues should thus be explored in\nthe design phase of longitudinal social network studies.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 18:38:08 GMT"}, {"version": "v2", "created": "Sun, 4 Mar 2018 23:40:23 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Stadtfeld", "Christoph", ""], ["Snijders", "Tom A. B.", ""], ["Steglich", "Christian", ""], ["van Duijn", "Marijtje A. J.", ""]]}, {"id": "1701.05426", "submitter": "Gen Li", "authors": "Gen Li, Dereje D. Jima, Fred A. Wright, Andrew B. Nobel", "title": "HT-eQTL: Integrative Expression Quantitative Trait Loci Analysis in a\n  Large Number of Human Tissues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expression quantitative trait loci (eQTL) analysis identifies genetic markers\nassociated with the expression of a gene. Most existing eQTL analyses and\nmethods investigate association in a single, readily available tissue, such as\nblood. Joint analysis of eQTL in multiple tissues has the potential to improve,\nand expand the scope of, single-tissue analyses. Large-scale collaborative\nefforts such as the Genotype-Tissue Expression (GTEx) program are currently\ngenerating high quality data in a large number of tissues. However,\ncomputational constraints limit genome-wide multi-tissue eQTL analysis. We\ndevelop an integrative method under a hierarchical Bayesian framework for eQTL\nanalysis in a large number of tissues. The model fitting procedure is highly\nscalable, and the computing time is a polynomial function of the number of\ntissues. Multi-tissue eQTLs are identified through a local false discovery rate\napproach, which rigorously controls the false discovery rate. Using simulation\nand GTEx real data studies, we show that the proposed method has superior\nperformance to existing methods in terms of computing time and the power of\neQTL discovery. We provide a scalable method for eQTL analysis in a large\nnumber of tissues. The method enables the identification of eQTL with different\nconfigurations and facilitates the characterization of tissue specificity.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 14:19:57 GMT"}, {"version": "v2", "created": "Fri, 20 Jan 2017 04:14:57 GMT"}, {"version": "v3", "created": "Thu, 7 Sep 2017 03:21:04 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Li", "Gen", ""], ["Jima", "Dereje D.", ""], ["Wright", "Fred A.", ""], ["Nobel", "Andrew B.", ""]]}, {"id": "1701.05455", "submitter": "Amir Payandeh Dr", "authors": "Amir T. Payandeh Najafabadi, Ghobad Barmalzan, and Shahla Aghaei", "title": "A Weighted Model Confidence Set: Applications to Local and Mixture Model\n  Confidence Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article provides a weighted model confidence set, whenever underling\nmodel has been misspecified and some part of support of random variable $X$\nconveys some important information about underling true model. Application of\nsuch weighted model confidence set for local and mixture model confidence sets\nhave been given. Two simulation studies have been conducted to show practical\napplication of our findings.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 15:04:54 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Najafabadi", "Amir T. Payandeh", ""], ["Barmalzan", "Ghobad", ""], ["Aghaei", "Shahla", ""]]}, {"id": "1701.05530", "submitter": "Frank Marrs", "authors": "Frank W. Marrs and Bailey K. Fosdick and Tyler H. McCormick", "title": "Regression of exchangeable relational arrays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational arrays represent measures of association between pairs of actors,\noften in varied contexts or over time. Such data appear as trade flows between\ncountries, financial transactions between individuals, contact frequencies\nbetween school children in classrooms, and dynamic protein-protein\ninteractions. Elements of a relational array are often modeled as a linear\nfunction of observable covariates, where the regression coefficients are the\nsubjects of inference. The structure of the relational array engenders\ndependence among relations that involve the same actor. Uncertainty estimates\nfor regression coefficient estimators -- and ideally the coefficient estimators\nthemselves -- must account for this relational dependence. Existing estimators\nof standard errors that recognize such relational dependence rely on estimating\nextremely complex, heterogeneous structure across actors. This paper proposes a\nnew class of parsimonious coefficient and standard error estimators for\nregressions of relational arrays. We leverage an exchangeability assumption to\nderive standard error estimators that pool information across actors and are\nsubstantially more accurate than existing estimators in a variety of settings.\nThis exchangeability assumption is pervasive in network and array models in the\nstatistics literature, but not previously considered when adjusting for\ndependence in a regression setting with relational data. We demonstrate\nimprovements in inference theoretically, via a simulation study, and by\nanalysis of a data set involving international trade.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 17:56:10 GMT"}, {"version": "v2", "created": "Mon, 8 May 2017 18:23:46 GMT"}, {"version": "v3", "created": "Fri, 13 Apr 2018 15:48:43 GMT"}, {"version": "v4", "created": "Tue, 16 Feb 2021 14:33:20 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Marrs", "Frank W.", ""], ["Fosdick", "Bailey K.", ""], ["McCormick", "Tyler H.", ""]]}, {"id": "1701.05593", "submitter": "Peyman Tavallali", "authors": "Peyman Tavallali, Marianne Razavi, Sean Brady", "title": "Parameter Selection Algorithm For Continuous Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose a new algorithm for supervised learning methods,\nby which one can both capture the non-linearity in data and also find the best\nsubset model. To produce an enhanced subset of the original variables, an ideal\nselection method should have the potential of adding a supplementary level of\nregression analysis that would capture complex relationships in the data via\nmathematical transformation of the predictors and exploration of synergistic\neffects of combined variables. The method that we present here has the\npotential to produce an optimal subset of variables, rendering the overall\nprocess of model selection to be more efficient. The core objective of this\npaper is to introduce a new estimation technique for the classical least square\nregression framework. This new automatic variable transformation and model\nselection method could offer an optimal and stable model that minimizes the\nmean square error and variability, while combining all possible subset\nselection methodology and including variable transformations and interaction.\nMoreover, this novel method controls multicollinearity, leading to an optimal\nset of explanatory variables.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 20:35:31 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Tavallali", "Peyman", ""], ["Razavi", "Marianne", ""], ["Brady", "Sean", ""]]}, {"id": "1701.05638", "submitter": "Francisco Javier Rubio", "authors": "Cristiano Villa and Francisco J. Rubio", "title": "Objective priors for the number of degrees of freedom of a multivariate\n  t distribution and the t-copula", "comments": "To appear in Computational Statistics and Data Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An objective Bayesian approach to estimate the number of degrees of freedom\n$(\\nu)$ for the multivariate $t$ distribution and for the $t$-copula, when the\nparameter is considered discrete, is proposed. Inference on this parameter has\nbeen problematic for the multivariate $t$ and, for the absence of any method,\nfor the $t$-copula. An objective criterion based on loss functions which allows\nto overcome the issue of defining objective probabilities directly is employed.\nThe support of the prior for $\\nu$ is truncated, which derives from the\nproperty of both the multivariate $t$ and the $t$-copula of convergence to\nnormality for a sufficiently large number of degrees of freedom. The\nperformance of the priors is tested on simulated scenarios. The R codes and the\nreplication material are available as a supplementary material of the\nelectronic version of the paper and on real data: daily logarithmic returns of\nIBM and of the Center for Research in Security Prices Database.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 23:09:08 GMT"}, {"version": "v2", "created": "Sat, 15 Jul 2017 12:38:04 GMT"}, {"version": "v3", "created": "Thu, 1 Mar 2018 08:59:19 GMT"}, {"version": "v4", "created": "Tue, 13 Mar 2018 08:17:31 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Villa", "Cristiano", ""], ["Rubio", "Francisco J.", ""]]}, {"id": "1701.05691", "submitter": "Lei Lin", "authors": "Lei Lin, Qian Wang, Adel W. Sadek", "title": "Real-time Traffic Accident Risk Prediction based on Frequent Pattern\n  Tree", "comments": "OPT-i 2014 - 1st International Conference on Engineering and Applied\n  Sciences Optimization, Proceedings", "journal-ref": null, "doi": null, "report-no": "2-s2.0-84911904129", "categories": "stat.AP cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traffic accident data are usually noisy, contain missing values, and\nheterogeneous. How to select the most important variables to improve real-time\ntraffic accident risk prediction has become a concern of many recent studies.\nThis paper proposes a novel variable selection method based on the Frequent\nPattern tree (FP tree) algorithm. First, all the frequent patterns in the\ntraffic accident dataset are discovered. Then for each frequent pattern, a new\ncriterion, called the Relative Object Purity Ratio (ROPR) which we proposed, is\ncalculated. This ROPR is added to the importance score of the variables that\ndifferentiate one frequent pattern from the others. To test the proposed\nmethod, a dataset was compiled from the traffic accidents records detected by\nonly one detector on interstate highway I-64 in Virginia in 2005. This dataset\nwas then linked to other variables such as real-time traffic information and\nweather conditions. Both the proposed method based on the FP tree algorithm, as\nwell as the widely utilized, random forest method, were then used to identify\nthe important variables or the Virginia dataset. The results indicate that\nthere are some differences between the variables deemed important by the FP\ntree and those selected by the random forest method. Following this, two\nbaseline models (i.e. a nearest neighbor (k-NN) method and a Bayesian network)\nwere developed to predict accident risk based on the variables identified by\nboth the FP tree method and the random forest method. The results show that the\nmodels based on the variable selection using the FP tree performed better than\nthose based on the random forest method for several versions of the k-NN and\nBayesian network models.The best results were derived from a Bayesian network\nmodel using variables from FP tree. That model could predict 61.11% of\naccidents accurately while having a false alarm rate of 38.16%.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 05:05:20 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 17:13:09 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Lin", "Lei", ""], ["Wang", "Qian", ""], ["Sadek", "Adel W.", ""]]}, {"id": "1701.05736", "submitter": "Alison Parton", "authors": "Alison Parton and Paul G. Blackwell", "title": "Bayesian inference for multistate `step and turn' animal movement in\n  continuous time", "comments": "34 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mechanistic modelling of animal movement is often formulated in discrete time\ndespite problems with scale invariance, such as handling irregularly timed\nobservations. A natural solution is to formulate in continuous time, yet uptake\nof this has been slow. This lack of implementation is often excused by a\ndifficulty in interpretation. Here we aim to bolster usage by developing a\ncontinuous-time model with interpretable parameters, similar to those of\npopular discrete-time models that use turning angles and step lengths. Movement\nis defined by a joint bearing and speed process, with parameters dependent on a\ncontinuous-time behavioural switching process, creating a flexible class of\nmovement models.\n  Methodology is presented for Markov chain Monte Carlo inference given\nirregular observations, involving augmenting observed locations with a\nreconstruction of the underlying movement process. This is applied to well\nknown GPS data from elk (\\emph{Cervus elaphus}), which have previously been\nmodelled in discrete time. We demonstrate the interpretable nature of the\ncontinuous-time model, finding clear differences in behaviour over time and\ninsights into short term behaviour that could not have been obtained in\ndiscrete time.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 09:42:38 GMT"}, {"version": "v2", "created": "Thu, 18 May 2017 14:38:36 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Parton", "Alison", ""], ["Blackwell", "Paul G.", ""]]}, {"id": "1701.05763", "submitter": "Jussi Korpela", "authors": "Jussi Korpela and Emilia Oikarinen and Kai Puolam\\\"aki and Antti\n  Ukkonen", "title": "Multivariate Confidence Intervals", "comments": "A short version of this paper appeared in the 2017 SIAM International\n  Conference on Data Mining, SDM'17. This extended version contains proofs of\n  theorems in the appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Confidence intervals are a popular way to visualize and analyze data\ndistributions. Unlike p-values, they can convey information both about\nstatistical significance as well as effect size. However, very little work\nexists on applying confidence intervals to multivariate data. In this paper we\ndefine confidence intervals for multivariate data that extend the\none-dimensional definition in a natural way. In our definition every variable\nis associated with its own confidence interval as usual, but a data vector can\nbe outside of a few of these, and still be considered to be within the\nconfidence area. We analyze the problem and show that the resulting confidence\nareas retain the good qualities of their one-dimensional counterparts: they are\ninformative and easy to interpret. Furthermore, we show that the problem of\nfinding multivariate confidence intervals is hard, but provide efficient\napproximate algorithms to solve the problem.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 11:19:50 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Korpela", "Jussi", ""], ["Oikarinen", "Emilia", ""], ["Puolam\u00e4ki", "Kai", ""], ["Ukkonen", "Antti", ""]]}, {"id": "1701.05863", "submitter": "Shinichiro Shirota Dr", "authors": "Shinichiro Shirota, Alan E Gelfand, Jorge Mateu", "title": "Analyzing Car Thefts and Recoveries with Connections to Modeling\n  Origin-Destination Point Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a given region, we have a dataset composed of car theft locations along\nwith a linked dataset of recovery locations which, due to partial recovery, is\na relatively small subset of the set of theft locations. For an investigator\nseeking to understand the behavior of car thefts and recoveries in the region,\nseveral questions are addressed. Viewing the set of theft locations as a point\npattern, can we propose useful models to explain the pattern? What types of\npredictive models can be built to learn about recovery location given theft\nlocation? Can the dependence between theft locations and recovery locations be\nformalized? Can the flow between theft sites and recovery sites be captured?\nOrigin-destination modeling offers a natural framework for such problems.\nHowever, here the data is not for areal units but rather is a pair of point\npatterns, with the recovery point pattern only partially observed. We offer\nmodeling approaches for investigating the questions above and apply the\napproaches to two datasets. One is small from the state of Neza in Mexico with\nareal covariate information regarding population features and crime type. A\nsecond, much larger one, is from Belo Horizonte in Brazil but lacks covariates.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 17:11:32 GMT"}, {"version": "v2", "created": "Sat, 31 Mar 2018 06:49:06 GMT"}, {"version": "v3", "created": "Fri, 21 Sep 2018 22:35:59 GMT"}, {"version": "v4", "created": "Fri, 3 Apr 2020 23:33:03 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Shirota", "Shinichiro", ""], ["Gelfand", "Alan E", ""], ["Mateu", "Jorge", ""]]}, {"id": "1701.05870", "submitter": "Davide Pigoli", "authors": "Alessandra Cabassi, Davide Pigoli, Piercesare Secchi, Patrick A.\n  Carter", "title": "Permutation tests for the equality of covariance operators of functional\n  data with applications to evolutionary biology", "comments": null, "journal-ref": null, "doi": "10.1214/17-EJS1347", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we generalize the metric-based permutation test for the\nequality of covariance operators proposed by Pigoli et al. (2014) to the case\nof multiple samples of functional data. To this end, the non-parametric\ncombination methodology of Pesarin and Salmaso (2010) is used to combine all\nthe pairwise comparisons between samples into a global test. Different\ncombining functions and permutation strategies are reviewed and analyzed in\ndetail. The resulting test allows to make inference on the equality of the\ncovariance operators of multiple groups and, if there is evidence to reject the\nnull hypothesis, to identify the pairs of groups having different covariances.\nIt is shown that, for some combining functions, step-down adjusting procedures\nare available to control for the multiple testing problem in this setting. The\nempirical power of this new test is then explored via simulations and compared\nwith those of existing alternative approaches in different scenarios. Finally,\nthe proposed methodology is applied to data from wheel running activity\nexperiments, that used selective breeding to study the evolution of locomotor\nbehavior in mice.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 17:47:04 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Cabassi", "Alessandra", ""], ["Pigoli", "Davide", ""], ["Secchi", "Piercesare", ""], ["Carter", "Patrick A.", ""]]}, {"id": "1701.05976", "submitter": "Michael Lopez", "authors": "Michael J. Lopez, Gregory J. Matthews, Benjamin S. Baumer", "title": "How often does the best team win? A unified approach to understanding\n  randomness in North American sport", "comments": "40 pages, 20 figures, 5 tables, code available at\n  https://github.com/bigfour/competitiveness", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical applications in sports have long centered on how to best separate\nsignal (e.g. team talent) from random noise. However, most of this work has\nconcentrated on a single sport, and the development of meaningful cross-sport\ncomparisons has been impeded by the difficulty of translating luck from one\nsport to another. In this manuscript, we develop Bayesian state-space models\nusing betting market data that can be uniformly applied across sporting\norganizations to better understand the role of randomness in game outcomes.\nThese models can be used to extract estimates of team strength, the\nbetween-season, within-season, and game-to-game variability of team strengths,\nas well each team's home advantage. We implement our approach across a decade\nof play in each of the National Football League (NFL), National Hockey League\n(NHL), National Basketball Association (NBA), and Major League Baseball (MLB),\nfinding that the NBA demonstrates both the largest dispersion in talent and the\nlargest home advantage, while the NHL and MLB stand out for their relative\nrandomness in game outcomes. We conclude by proposing new metrics for judging\ncompetitiveness across sports leagues, both within the regular season and using\ntraditional postseason tournament formats. Although we focus on sports, we\ndiscuss a number of other situations in which our generalizable models might be\nusefully applied.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jan 2017 03:28:57 GMT"}, {"version": "v2", "created": "Tue, 13 Jun 2017 01:16:14 GMT"}, {"version": "v3", "created": "Wed, 22 Nov 2017 05:55:04 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Lopez", "Michael J.", ""], ["Matthews", "Gregory J.", ""], ["Baumer", "Benjamin S.", ""]]}, {"id": "1701.06445", "submitter": "Jianfeng Wang", "authors": "Jianfeng Wang, Anders Garpebring, Patrik Brynolfsson, Xijia Liu, Jun\n  Yu", "title": "Contrast Agent Quantification by Using Spatial Information in Dynamic\n  Contrast Enhanced MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this study is to investigate a method, using simulations, to\nimprove contrast agent quantification in Dynamic Contrast Enhanced MRI.\nBayesian hierarchical models (BHMs) are applied to smaller images\n($10\\times10\\times10$) such that spatial information can be incorporated. Then\nexploratory analysis is done for larger images ($64\\times64\\times64$) by using\nmaximum a posteriori (MAP).\n  For smaller images: the estimators of proposed BHMs show improvements in\nterms of the root mean squared error compared to the estimators in existing\nmethod for a noise level equivalent of a 12-channel head coil at 3T. Moreover,\nLeroux model outperforms Besag models. For larger images: MAP estimators also\nshow improvements by assigning Leroux prior.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 15:19:48 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Wang", "Jianfeng", ""], ["Garpebring", "Anders", ""], ["Brynolfsson", "Patrik", ""], ["Liu", "Xijia", ""], ["Yu", "Jun", ""]]}, {"id": "1701.06619", "submitter": "Jaewoo Park", "authors": "Jaewoo Park and Murali Haran", "title": "Bayesian Inference in the Presence of Intractable Normalizing Functions", "comments": "main paper (40 pages), supplementary (13 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models with intractable normalizing functions arise frequently in statistics.\nCommon examples of such models include exponential random graph models for\nsocial networks and Markov point processes for ecology and disease modeling.\nInference for these models is complicated because the normalizing functions of\ntheir probability distributions include the parameters of interest. In Bayesian\nanalysis they result in so-called doubly intractable posterior distributions\nwhich pose significant computational challenges. Several Monte Carlo methods\nhave emerged in recent years to address Bayesian inference for such models. We\nprovide a framework for understanding the algorithms and elucidate connections\namong them. Through multiple simulated and real data examples, we compare and\ncontrast the computational and statistical efficiency of these algorithms and\ndiscuss their theoretical bases. Our study provides practical recommendations\nfor practitioners along with directions for future research for MCMC\nmethodologists.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 20:26:42 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2018 17:31:51 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Park", "Jaewoo", ""], ["Haran", "Murali", ""]]}, {"id": "1701.06720", "submitter": "Stephen Collins-Elliott", "authors": "Stephen A. Collins-Elliott", "title": "Bayesian inference with Monte Carlo approximation: Measuring regional\n  differentiation in ceramic and glass vessel assemblages in Republican Italy,\n  ca. 200 BCE - 20 CE", "comments": null, "journal-ref": null, "doi": "10.1016/j.jas.2017.01.006", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods of measuring differentiation in archaeological assemblages have long\nbeen based on attribute-level analyses of assemblages. This paper considers a\nmethod of comparing assemblages as probability distributions via the Hellinger\ndistance, as calculated through a Dirichlet-categorical model of inference\nusing Monte Carlo methods of approximation. This method has application within\npractice-theory traditions of archaeology, an approach which seeks to measure\nand associate different factors that comprise the habitus of society. It is\nimplemented here focusing on the question of regional food consumption habits\nin Republican Italy in the last two centuries BCE, toward informing a\nperspective on mass social change.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 03:43:09 GMT"}, {"version": "v2", "created": "Sat, 18 Feb 2017 04:14:38 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Collins-Elliott", "Stephen A.", ""]]}, {"id": "1701.06754", "submitter": "Chee-Ming Ting PhD", "authors": "Chee-Ming Ting, Hernando Ombao, S. Balqis Samdin, Sh-Hussain Salleh", "title": "Estimating Time-Varying Effective Connectivity in High-Dimensional fMRI\n  Data Using Regime-Switching Factor Models", "comments": "21 pages", "journal-ref": "IEEE Trans. Medical Imaging 37 (2018) 1011-1023", "doi": "10.1109/TMI.2017.2780185", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies on analyzing dynamic brain connectivity rely on sliding-window\nanalysis or time-varying coefficient models which are unable to capture both\nsmooth and abrupt changes simultaneously. Emerging evidence suggests\nstate-related changes in brain connectivity where dependence structure\nalternates between a finite number of latent states or regimes. Another\nchallenge is inference of full-brain networks with large number of nodes. We\nemploy a Markov-switching dynamic factor model in which the state-driven\ntime-varying connectivity regimes of high-dimensional fMRI data are\ncharacterized by lower-dimensional common latent factors, following a\nregime-switching process. It enables a reliable, data-adaptive estimation of\nchange-points of connectivity regimes and the massive dependencies associated\nwith each regime. We consider the switching VAR to quantity the dynamic\neffective connectivity. We propose a three-step estimation procedure: (1)\nextracting the factors using principal component analysis (PCA) and (2)\nidentifying dynamic connectivity states using the factor-based switching vector\nautoregressive (VAR) models in a state-space formulation using Kalman filter\nand expectation-maximization (EM) algorithm, and (3) constructing the\nhigh-dimensional connectivity metrics for each state based on subspace\nestimates. Simulation results show that our proposed estimator outperforms the\nK-means clustering of time-windowed coefficients, providing more accurate\nestimation of regime dynamics and connectivity metrics in high-dimensional\nsettings. Applications to analyzing resting-state fMRI data identify dynamic\nchanges in brain states during rest, and reveal distinct directed connectivity\npatterns and modular organization in resting-state networks across different\nstates.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 07:37:01 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Ting", "Chee-Ming", ""], ["Ombao", "Hernando", ""], ["Samdin", "S. Balqis", ""], ["Salleh", "Sh-Hussain", ""]]}, {"id": "1701.06976", "submitter": "Haiming Zhou", "authors": "Haiming Zhou and Timothy Hanson", "title": "A unified framework for fitting Bayesian semiparametric models to\n  arbitrarily censored survival data, including spatially-referenced data", "comments": "To appear in Journal of the American Statistical Association", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A comprehensive, unified approach to modeling arbitrarily censored spatial\nsurvival data is presented for the three most commonly-used semiparametric\nmodels: proportional hazards, proportional odds, and accelerated failure time.\nUnlike many other approaches, all manner of censored survival times are\nsimultaneously accommodated including uncensored, interval censored,\ncurrent-status, left and right censored, and mixtures of these. Left-truncated\ndata are also accommodated leading to models for time-dependent covariates.\nBoth georeferenced (location exactly observed) and areally observed (location\nknown up to a geographic unit such as a county) spatial locations are handled;\nformal variable selection makes model selection especially easy. Model fit is\nassessed with conditional Cox-Snell residual plots, and model choice is carried\nout via LPML and DIC. Baseline survival is modeled with a novel transformed\nBernstein polynomial prior. All models are fit via a new function which calls\nefficient compiled C++ in the R package spBayesSurv. The methodology is broadly\nillustrated with simulations and real data applications. An important finding\nis that proportional odds and accelerated failure time models often fit\nsignificantly better than the commonly-used proportional hazards model.\nSupplementary materials are available online.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 16:47:47 GMT"}, {"version": "v2", "created": "Mon, 3 Jul 2017 02:47:45 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Zhou", "Haiming", ""], ["Hanson", "Timothy", ""]]}, {"id": "1701.07011", "submitter": "Lingfei Wang", "authors": "Lingfei Wang and Tom Michoel", "title": "Controlling false discoveries in Bayesian gene networks with lasso\n  regression p-values", "comments": "9 pages, 6 figures, 3 tables. Supplementary info: 2 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.MN q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian networks can represent directed gene regulations and therefore are\nfavored over co-expression networks. However, hardly any Bayesian network study\nconcerns the false discovery control (FDC) of network edges, leading to low\naccuracies due to systematic biases from inconsistent false discovery levels in\nthe same study. We design four empirical tests to examine the FDC of Bayesian\nnetworks from three p-value based lasso regression variable selections --- two\nexisting and one we originate. Our method, lassopv, computes p-values for the\ncritical regularization strength at which a predictor starts to contribute to\nlasso regression. Using null and Geuvadis datasets, we find that lassopv\nobtains optimal FDC in Bayesian gene networks, whilst existing methods have\ndefective p-values. The FDC concept and tests extend to most network inference\nscenarios and will guide the design and improvement of new and existing\nmethods. Our novel variable selection method with lasso regression also allows\nFDC on other datasets and questions, even beyond network inference and\ncomputational biology. Lassopv is implemented in R and freely available at\nhttps://github.com/lingfeiwang/lassopv and\nhttps://cran.r-project.org/package=lassopv\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 18:56:45 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2018 19:45:20 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Wang", "Lingfei", ""], ["Michoel", "Tom", ""]]}, {"id": "1701.07021", "submitter": "Ali Nassif", "authors": "Ali Bou Nassif", "title": "Short Term Power Demand Prediction Using Stochastic Gradient Boosting", "comments": null, "journal-ref": "5th International Conference on Electronic Devices, Systems and\n  Applications, IEEE, 2016, pages 1-4", "doi": "10.1109/icedsa.2016.7818510", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power prediction demand is vital in power system and delivery engineering\nfields. By efficiently predicting the power demand, we can forecast the total\nenergy to be consumed in a certain city or district. Thus, exact resources\nrequired to produce the demand power can be allocated. In this paper, a\nStochastic Gradient Boosting (aka Treeboost) model is used to predict the short\nterm power demand for the Emirate of Sharjah in the United Arab Emirates (UAE).\nResults show that the proposed model gives promising results in comparison to\nthe model used by Sharjah Electricity and Water Authority (SEWA).\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 13:24:54 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Nassif", "Ali Bou", ""]]}, {"id": "1701.07152", "submitter": "Worapree Ole Maneesoonthorn", "authors": "Rub\\'en Loaiza-Maya, Michael S. Smith and Worapree Maneesoonthorn", "title": "Time Series Copulas for Heteroskedastic Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose parametric copulas that capture serial dependence in stationary\nheteroskedastic time series. We develop our copula for first order Markov\nseries, and extend it to higher orders and multivariate series. We derive the\ncopula of a volatility proxy, based on which we propose new measures of\nvolatility dependence, including co-movement and spillover in multivariate\nseries. In general, these depend upon the marginal distributions of the series.\nUsing exchange rate returns, we show that the resulting copula models can\ncapture their marginal distributions more accurately than univariate and\nmultivariate GARCH models, and produce more accurate value at risk forecasts.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 04:01:35 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Loaiza-Maya", "Rub\u00e9n", ""], ["Smith", "Michael S.", ""], ["Maneesoonthorn", "Worapree", ""]]}, {"id": "1701.07203", "submitter": "Apratim Ganguly", "authors": "Apratim Ganguly, Eric Kolaczyk", "title": "Estimation of Vertex Degrees in a Sampled Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need to produce accurate estimates of vertex degree in a large network,\nbased on observation of a subnetwork, arises in a number of practical settings.\nWe study a formalized version of this problem, wherein the goal is, given a\nrandomly sampled subnetwork from a large parent network, to estimate the actual\ndegree of the sampled nodes. Depending on the sampling scheme, trivial method\nof moments estimators (MMEs) can be used. However, the MME is not expected, in\ngeneral, to use all relevant network information. In this study, we propose a\nhandful of novel estimators derived from a risk-theoretic perspective, which\nmake more sophisticated use of the information in the sampled network.\nTheoretical assessment of the new estimators characterizes under what\nconditions they can offer improvement over the MME, while numerical comparisons\nshow that when such improvement obtains, it can be substantial. Illustration is\nprovided on a human trafficking network.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 08:41:12 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Ganguly", "Apratim", ""], ["Kolaczyk", "Eric", ""]]}, {"id": "1701.07316", "submitter": "Thierry Duchesne", "authors": "David Beaudoin and Thierry Duchesne", "title": "Prediction of the margin of victory only from team rankings for regular\n  season games in NCAA men's basketball", "comments": null, "journal-ref": null, "doi": "10.1177/1754337117754181", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main objective of this paper is to investigate the extent to which the\nmargin of victory can be predicted solely by the rankings of the opposing teams\nin NCAA Division I men's basketball games. Several past studies have modeled\nthis relationship for the games played during the March Madness tournament, and\nthis work aims at verifying if the models advocated in these papers still\nperform well for regular season games. Indeed, most previous articles have\nshown that a simple quadratic regression model provides fairly accurate\npredictions of the margin of victory when team rankings only range from 1 to\n16. Does that still hold true when team rankings can go as high as 351? Do the\nmodel assumptions hold? Can we find semi- or non-parametric methods that yield\neven better results (i.e. predicted margins of victory that more closely\nresemble actual results)? The analyses presented in this paper suggest that the\nanswer is \"yes\" on all three counts!\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 14:07:52 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Beaudoin", "David", ""], ["Duchesne", "Thierry", ""]]}, {"id": "1701.07402", "submitter": "Santosh Kumar", "authors": "Santosh Kumar, Bharath Sambasivam and Shashank Anand", "title": "Smallest eigenvalue density for regular or fixed-trace complex\n  Wishart-Laguerre ensemble and entanglement in coupled kicked tops", "comments": "Published version", "journal-ref": "Journal of Physics A: Mathematical and Theoretical, Volume 50,\n  Number 34, Page - 345201, Year 2017", "doi": "10.1088/1751-8121/aa7d0e", "report-no": null, "categories": "math-ph cond-mat.stat-mech math.MP math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical behaviour of the smallest eigenvalue has important\nimplications for systems which can be modeled using a Wishart-Laguerre\nensemble, the regular one or the fixed trace one. For example, the density of\nthe smallest eigenvalue of the Wishart-Laguerre ensemble plays a crucial role\nin characterizing multiple channel telecommunication systems. Similarly, in the\nquantum entanglement problem, the smallest eigenvalue of the fixed trace\nensemble carries information regarding the nature of entanglement.\n  For real Wishart-Laguerre matrices, there exists an elegant recurrence scheme\nsuggested by Edelman to directly obtain the exact expression for the smallest\neigenvalue density. In the case of complex Wishart-Laguerre matrices, for\nfinding exact and explicit expressions for the smallest eigenvalue density,\nexisting results based on determinants become impractical when the determinants\ninvolve large-size matrices. In this work, we derive a recurrence scheme for\nthe complex case which is analogous to that of Edelman's for the real case.\nThis is used to obtain exact results for the smallest eigenvalue density for\nboth the regular, and the fixed trace complex Wishart-Laguerre ensembles. We\nvalidate our analytical results using Monte Carlo simulations. We also study\nscaled Wishart-Laguerre ensemble and investigate its efficacy in approximating\nthe fixed-trace ensemble. Eventually, we apply our result for the fixed-trace\nensemble to investigate the behaviour of the smallest eigenvalue in the\nparadigmatic system of coupled kicked tops.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 17:47:32 GMT"}, {"version": "v2", "created": "Sat, 29 Jul 2017 08:28:53 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Kumar", "Santosh", ""], ["Sambasivam", "Bharath", ""], ["Anand", "Shashank", ""]]}, {"id": "1701.07483", "submitter": "Ashwin Venkataraman", "authors": "Srikanth Jagabathula, Lakshminarayanan Subramanian, Ashwin\n  Venkataraman", "title": "A Model-based Projection Technique for Segmenting Customers", "comments": "51 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of segmenting a large population of customers into\nnon-overlapping groups with similar preferences, using diverse preference\nobservations such as purchases, ratings, clicks, etc. over subsets of items. We\nfocus on the setting where the universe of items is large (ranging from\nthousands to millions) and unstructured (lacking well-defined attributes) and\neach customer provides observations for only a few items. These data\ncharacteristics limit the applicability of existing techniques in marketing and\nmachine learning. To overcome these limitations, we propose a model-based\nprojection technique, which transforms the diverse set of observations into a\nmore comparable scale and deals with missing data by projecting the transformed\ndata onto a low-dimensional space. We then cluster the projected data to obtain\nthe customer segments. Theoretically, we derive precise necessary and\nsufficient conditions that guarantee asymptotic recovery of the true customer\nsegments. Empirically, we demonstrate the speed and performance of our method\nin two real-world case studies: (a) 84% improvement in the accuracy of new\nmovie recommendations on the MovieLens data set and (b) 6% improvement in the\nperformance of similar item recommendations algorithm on an offline dataset at\neBay. We show that our method outperforms standard latent-class and\ndemographic-based techniques.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 20:47:40 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["Jagabathula", "Srikanth", ""], ["Subramanian", "Lakshminarayanan", ""], ["Venkataraman", "Ashwin", ""]]}, {"id": "1701.07496", "submitter": "Max Tolkoff", "authors": "Max R. Tolkoff, Michael L. Alfaro, Guy Baele, Philippe Lemey, and Marc\n  A. Suchard", "title": "Phylogenetic Factor Analysis", "comments": "51 pages (42 main, 9 supplemental), 9 figures (5 main, 4\n  supplemental), 4 tables (2 main, 2 supplemental), submitted to Systematic\n  Biology", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phylogenetic comparative methods explore the relationships between\nquantitative traits adjusting for shared evolutionary history. This adjustment\noften occurs through a Brownian diffusion process along the branches of the\nphylogeny that generates model residuals or the traits themselves. For\nhigh-dimensional traits, inferring all pair-wise correlations within the\nmultivariate diffusion is limiting. To circumvent this problem, we propose\nphylogenetic factor analysis (PFA) that assumes a small unknown number of\nindependent evolutionary factors arise along the phylogeny and these factors\ngenerate clusters of dependent traits. Set in a Bayesian framework, PFA\nprovides measures of uncertainty on the factor number and groupings, combines\nboth continuous and discrete traits, integrates over missing measurements and\nincorporates phylogenetic uncertainty with the help of molecular sequences. We\ndevelop Gibbs samplers based on dynamic programming to estimate the PFA\nposterior distribution, over three-fold faster than for multivariate diffusion\nand a further order-of-magnitude more efficiently in the presence of latent\ntraits. We further propose a novel marginal likelihood estimator for previously\nimpractical models with discrete data and find that PFA also provides a better\nfit than multivariate diffusion in evolutionary questions in columbine flower\ndevelopment, placental reproduction transitions and triggerfish fin\nmorphometry.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 21:43:03 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["Tolkoff", "Max R.", ""], ["Alfaro", "Michael L.", ""], ["Baele", "Guy", ""], ["Lemey", "Philippe", ""], ["Suchard", "Marc A.", ""]]}, {"id": "1701.07555", "submitter": "Gery Geenens", "authors": "Gery Geenens, Thomas Cuddihy", "title": "Robust analysis of second-leg home advantage in UEFA football through\n  better nonparametric confidence intervals for binary regression functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In international football (soccer), two-legged knockout ties, with each team\nplaying at home in one leg and the final outcome decided on aggregate, are\ncommon. Many players, managers and followers seem to believe in the `second-leg\nhome advantage', i.e. that it is beneficial to play at home on the second leg.\nA more complex effect than the usual and well-established home advantage, it is\nharder to identify, and previous statistical studies did not prove conclusive\nabout its actuality. Yet, given the amount of money handled in international\nfootball competitions nowadays, the question of existence or otherwise of this\neffect is of real import. As opposed to previous research, this paper addresses\nit from a purely nonparametric perspective and brings a very objective answer,\nnot based on any particular model specification which could orientate the\nanalysis in one or the other direction. Along the way, the paper reviews the\nwell-known shortcomings of the Wald confidence interval for a proportion,\nsuggests new nonparametric confidence intervals for conditional probability\nfunctions, revisits the problem of the bias when building confidence intervals\nin nonparametric regression, and provides a novel bootstrap-based solution to\nit. Finally, the new intervals are used in a careful analysis of game outcome\ndata for the UEFA Champions and Europa leagues from 2009/10 to 2014/15. A\nslight `second-leg home advantage' is evidenced.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 02:49:06 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["Geenens", "Gery", ""], ["Cuddihy", "Thomas", ""]]}, {"id": "1701.07638", "submitter": "Zbigniew Michna", "authors": "Zbigniew Michna, Stephen M. Disney and Peter Nielsen", "title": "The impact of stochastic lead times on the bullwhip effect under\n  correlated demand and moving average forecasts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We quantify the bullwhip effect (which measures how the variance in\nreplenishment orders is amplified as the orders move up the supply chain) when\nrandom demands and random lead times are estimated using the industrially\npopular moving average forecasting method. We assume that the lead times\nconstitute a sequence of independent identically distributed random variables\nand correlated demands are described by a first order autoregressive process.\nWe obtain an expression that reveals the impact of demand and lead time\nforecasting on the bullwhip effect. We draw a number of conclusions on the\nbehavior of the bullwhip effect with respect to the demand auto-correlation and\nthe number of past lead times and demands used in the forecasts. Furthermore we\nfind the maxima and minima of the bullwhip measure as a function of the demand\nauto-correlation.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 10:17:07 GMT"}, {"version": "v2", "created": "Sat, 4 Feb 2017 09:45:36 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Michna", "Zbigniew", ""], ["Disney", "Stephen M.", ""], ["Nielsen", "Peter", ""]]}, {"id": "1701.07899", "submitter": "Emilie Devijver", "authors": "Emilie Devijver, M\\'elina Gallopin and Emeline Perthame", "title": "Nonlinear network-based quantitative trait prediction from\n  transcriptomic data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitatively predicting phenotype variables by the expression changes in a\nset of candidate genes is of great interest in molecular biology but it is also\na challenging task for several reasons. First, the collected biological\nobservations might be heterogeneous and correspond to different biological\nmechanisms. Secondly, the gene expression variables used to predict the\nphenotype are potentially highly correlated since genes interact though unknown\nregulatory networks. In this paper, we present a novel approach designed to\npredict quantitative trait from transcriptomic data, taking into account the\nheterogeneity in biological samples and the hidden gene regulatory networks\nunderlying different biological mechanisms. The proposed model performs well on\nprediction but it is also fully parametric, which facilitates the downstream\nbiological interpretation. The model provides clusters of individuals based on\nthe relation between gene expression data and the phenotype, and also leads to\ninfer a gene regulatory network specific for each cluster of individuals. We\nperform numerical simulations to demonstrate that our model is competitive with\nother prediction models, and we demonstrate the predictive performance and the\ninterpretability of our model to predict alcohol sensitivity from\ntranscriptomic data on real data from Drosophila Melanogaster Genetic Reference\nPanel (DGRP).\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 23:05:40 GMT"}, {"version": "v2", "created": "Thu, 23 Feb 2017 18:53:25 GMT"}, {"version": "v3", "created": "Fri, 12 May 2017 13:14:53 GMT"}, {"version": "v4", "created": "Wed, 19 Jul 2017 11:23:30 GMT"}, {"version": "v5", "created": "Thu, 20 Jul 2017 16:45:07 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Devijver", "Emilie", ""], ["Gallopin", "M\u00e9lina", ""], ["Perthame", "Emeline", ""]]}, {"id": "1701.07910", "submitter": "Daniel Eck", "authors": "Daniel J. Eck, Charles J. Geyer, and R. Dennis Cook", "title": "Combining Envelope Methodology and Aster Models for Variance Reduction\n  in Life History Analyses", "comments": "Title changed from \"An Application of Envelope Methodology and Aster\n  Models\" to \"Combining Envelope Methodology and Aster Models for Variance\n  Reduction in Life History Analyses\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precise estimation of expected Darwinian fitness, the expected lifetime\nnumber of offspring of organism, is a central component of life history\nanalysis. The aster model serves as a defensible statistical model for\ndistributions of Darwinian fitness. The aster model is equipped to incorporate\nthe major life stages an organism travels through which separately may effect\nDarwinian fitness. Envelope methodology reduces asymptotic variability by\nestablishing a link between unknown parameters of interest and the asymptotic\ncovariance matrices of their estimators. It is known both theoretically and in\napplications that incorporation of envelope methodology reduces asymptotic\nvariability. We develop an envelope framework, including a new envelope\nestimator, that is appropriate for aster analyses. The level of precision\nprovided from our methods allows researchers to draw stronger conclusions about\nthe driving forces of Darwinian fitness from their life history analyses than\nthey could with the aster model alone. Our methods are illustrated on a\nsimulated dataset and a life history analysis of \\emph{Mimulus guttatus}\nflowers is provided. Useful variance reduction is obtained in both analyses.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 00:22:42 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 16:08:36 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Eck", "Daniel J.", ""], ["Geyer", "Charles J.", ""], ["Cook", "R. Dennis", ""]]}, {"id": "1701.08043", "submitter": "Magne Aldrin", "authors": "Magne Aldrin, Ragnar Bang Huseby, Audun Stien, Randi Nygaard\n  Gr{\\o}ntvedt, Hildegunn Viljugrein and Peder Andreas Jansen", "title": "A stage-structured Bayesian hierarchical model for salmon lice\n  populations at individual salmon farms - Estimated from multiple farm data\n  sets", "comments": null, "journal-ref": "Ecological Modelling, 2017", "doi": "10.1016/j.ecolmodel.2017.05.019", "report-no": null, "categories": "q-bio.PE q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Salmon farming has become a prosperous international industry over the last\ndecades. Along with growth in the production farmed salmon, however, an\nincreasing threat by pathogens has emerged. Of special concern is the\npropagation and spread of the salmon louse, Lepeophtheirus salmonis. In order\nto gain insight into this parasites population dynamics in large scale salmon\nfarming system, we present a fully mechanistic stage-structured population\nmodel for the salmon louse, also allowing for complexities involved in the\nhierarchical structure of full scale salmon farming. The model estimates\nparameters controlling a wide range of processes, including temperature\ndependent demographic rates, fish size and abundance effects on louse\ntransmission rates, effects sizes of various salmon louse control measures, and\ndistance based between farm transmission rates. Model parameters were estimated\nfrom data including 32 salmon farms, except the last production months for five\nfarms which were used to evaluate model predictions. We used a Bayesian\nestimation approach, combining the prior distributions and the data likelihood\ninto a joint posterior distribution for all model parameters. The model\ngenerated expected values that fitted the observed infection levels of the\nchalimus, adult female and other mobile stages of salmon lice, reasonably well.\nPredictions for the time periods not used for fitting the model were also\nconsistent with the observational data. We argue that the present model for the\npopulation dynamics of the salmon louse in aquaculture farm systems may\ncontribute to resolve the complexity of processes that drive that drive this\nhost-parasite relationship, and hence may improve strategies to control the\nparasite in this production system.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 13:07:49 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Aldrin", "Magne", ""], ["Huseby", "Ragnar Bang", ""], ["Stien", "Audun", ""], ["Gr\u00f8ntvedt", "Randi Nygaard", ""], ["Viljugrein", "Hildegunn", ""], ["Jansen", "Peder Andreas", ""]]}, {"id": "1701.08055", "submitter": "Franz J. Kir\\'aly", "authors": "Franz J. Kir\\'aly and Zhaozhi Qian", "title": "Modelling Competitive Sports: Bradley-Terry-\\'{E}l\\H{o} Models for\n  Supervised and On-Line Learning of Paired Competition Outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction and modelling of competitive sports outcomes has received much\nrecent attention, especially from the Bayesian statistics and machine learning\ncommunities. In the real world setting of outcome prediction, the seminal\n\\'{E}l\\H{o} update still remains, after more than 50 years, a valuable baseline\nwhich is difficult to improve upon, though in its original form it is a\nheuristic and not a proper statistical \"model\". Mathematically, the \\'{E}l\\H{o}\nrating system is very closely related to the Bradley-Terry models, which are\nusually used in an explanatory fashion rather than in a predictive supervised\nor on-line learning setting.\n  Exploiting this close link between these two model classes and some newly\nobserved similarities, we propose a new supervised learning framework with\nclose similarities to logistic regression, low-rank matrix completion and\nneural networks. Building on it, we formulate a class of structured log-odds\nmodels, unifying the desirable properties found in the above: supervised\nprobabilistic prediction of scores and wins/draws/losses, batch/epoch and\non-line learning, as well as the possibility to incorporate features in the\nprediction, without having to sacrifice simplicity, parsimony of the\nBradley-Terry models, or computational efficiency of \\'{E}l\\H{o}'s original\napproach.\n  We validate the structured log-odds modelling approach in synthetic\nexperiments and English Premier League outcomes, where the added expressivity\nyields the best predictions reported in the state-of-art, close to the quality\nof contemporary betting odds.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 14:01:53 GMT"}], "update_date": "2017-01-30", "authors_parsed": [["Kir\u00e1ly", "Franz J.", ""], ["Qian", "Zhaozhi", ""]]}, {"id": "1701.08107", "submitter": "Ahmed Karam Eldaly MSc", "authors": "Ahmed Karam Eldaly, Yoann Altmann, Antonios Perperidis, Nikola\n  Krstajic, Tushar Choudhary, Kevin Dhaliwal, and Stephen McLaughlin", "title": "Deconvolution and Restoration of Optical Endomicroscopy Images", "comments": null, "journal-ref": null, "doi": "10.1109/TCI.2018.2811939", "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical endomicroscopy (OEM) is an emerging technology platform with\npreclinical and clinical imaging applications. Pulmonary OEM via fibre bundles\nhas the potential to provide in vivo, in situ molecular signatures of disease\nsuch as infection and inflammation. However, enhancing the quality of data\nacquired by this technique for better visualization and subsequent analysis\nremains a challenging problem. Cross coupling between fiber cores and sparse\nsampling by imaging fiber bundles are the main reasons for image degradation,\nand poor detection performance (i.e., inflammation, bacteria, etc.). In this\nwork, we address the problem of deconvolution and restoration of OEM data. We\npropose a hierarchical Bayesian model to solve this problem and compare three\nestimation algorithms to exploit the resulting joint posterior distribution.\nThe first method is based on Markov chain Monte Carlo (MCMC) methods, however,\nit exhibits a relatively long computational time. The second and third\nalgorithms deal with this issue and are based on a variational Bayes (VB)\napproach and an alternating direction method of multipliers (ADMM) algorithm\nrespectively. Results on both synthetic and real datasets illustrate the\neffectiveness of the proposed methods for restoration of OEM images.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 16:37:03 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 10:12:20 GMT"}, {"version": "v3", "created": "Tue, 28 Aug 2018 13:44:54 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Eldaly", "Ahmed Karam", ""], ["Altmann", "Yoann", ""], ["Perperidis", "Antonios", ""], ["Krstajic", "Nikola", ""], ["Choudhary", "Tushar", ""], ["Dhaliwal", "Kevin", ""], ["McLaughlin", "Stephen", ""]]}, {"id": "1701.08142", "submitter": "Clara Grazian", "authors": "Clara Grazian, Fabrizio Leisen, Brunero Liseo", "title": "Modelling Preference Data with the Wallenius Distribution", "comments": "3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Wallenius distribution is a generalisation of the Hypergeometric\ndistribution where weights are assigned to balls of different colours. This\nnaturally defines a model for ranking categories which can be used for\nclassification purposes. Since, in general, the resulting likelihood is not\nanalytically available, we adopt an approximate Bayesian computational (ABC)\napproach for estimating the importance of the categories. We illustrate the\nperformance of the estimation procedure on simulated datasets. Finally, we use\nthe new model for analysing two datasets about movies ratings and Italian\nacademic statisticians' journal preferences. The latter is a novel dataset\ncollected by the authors.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 18:30:09 GMT"}, {"version": "v2", "created": "Thu, 9 Feb 2017 14:45:06 GMT"}, {"version": "v3", "created": "Fri, 3 Mar 2017 15:54:59 GMT"}, {"version": "v4", "created": "Wed, 7 Feb 2018 12:03:47 GMT"}, {"version": "v5", "created": "Thu, 28 Jun 2018 14:46:09 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Grazian", "Clara", ""], ["Leisen", "Fabrizio", ""], ["Liseo", "Brunero", ""]]}, {"id": "1701.08145", "submitter": "Myung Soon Song", "authors": "Myung Soon Song", "title": "Synthesizing Correlations with Computational Likelihood Approach:\n  Vitamin C Data", "comments": "15 pages, 2 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that the primary source of dietary vitamin C is fruit and\nvegetables and the plasma level of vitamin C has been considered a good\nsurrogate biomarker of vitamin C intake by fruit and vegetable consumption. To\ncombine the information about association between vitamin C intake and the\nplasma level of vitamin C, numerical approximation methods for likelihood\nfunction of correlation coefficient are studied. The least squares approach is\nused to estimate a log-likelihood function by a function from a space of\nB-splines having desirable mathematical properties. The likelihood interval\nfrom the Highest Likelihood Regions (HLR) is used for further inference. This\napproach can be easily extended to the realm of meta-analysis involving sample\ncorrelations from different studies by use of an approximated combined\nlikelihood function. The sample correlations between vitamin C intake and serum\nlevel of vitamin C from many studies are used to illustrate application of this\napproach.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jan 2017 02:52:27 GMT"}, {"version": "v2", "created": "Mon, 22 May 2017 02:53:23 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Song", "Myung Soon", ""]]}, {"id": "1701.08230", "submitter": "Sam Corbett-Davies", "authors": "Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz\n  Huq", "title": "Algorithmic decision making and the cost of fairness", "comments": "To appear in Proceedings of KDD'17", "journal-ref": null, "doi": "10.1145/3097983.309809", "report-no": null, "categories": "cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms are now regularly used to decide whether defendants awaiting trial\nare too dangerous to be released back into the community. In some cases, black\ndefendants are substantially more likely than white defendants to be\nincorrectly classified as high risk. To mitigate such disparities, several\ntechniques recently have been proposed to achieve algorithmic fairness. Here we\nreformulate algorithmic fairness as constrained optimization: the objective is\nto maximize public safety while satisfying formal fairness constraints designed\nto reduce racial disparities. We show that for several past definitions of\nfairness, the optimal algorithms that result require detaining defendants above\nrace-specific risk thresholds. We further show that the optimal unconstrained\nalgorithm requires applying a single, uniform threshold to all defendants. The\nunconstrained algorithm thus maximizes public safety while also satisfying one\nimportant understanding of equality: that all individuals are held to the same\nstandard, irrespective of race. Because the optimal constrained and\nunconstrained algorithms generally differ, there is tension between improving\npublic safety and satisfying prevailing notions of algorithmic fairness. By\nexamining data from Broward County, Florida, we show that this trade-off can be\nlarge in practice. We focus on algorithms for pretrial release decisions, but\nthe principles we discuss apply to other domains, and also to human decision\nmakers carrying out structured decision rules.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jan 2017 00:42:00 GMT"}, {"version": "v2", "created": "Wed, 1 Feb 2017 18:50:03 GMT"}, {"version": "v3", "created": "Sat, 18 Feb 2017 18:48:36 GMT"}, {"version": "v4", "created": "Sat, 10 Jun 2017 00:01:23 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Corbett-Davies", "Sam", ""], ["Pierson", "Emma", ""], ["Feller", "Avi", ""], ["Goel", "Sharad", ""], ["Huq", "Aziz", ""]]}, {"id": "1701.08299", "submitter": "Viktor Witkovsky", "authors": "Viktor Witkovsky, Gejza Wimmer, Tomas Duby", "title": "Computing the aggregate loss distribution based on numerical inversion\n  of the compound empirical characteristic function of frequency and severity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A non-parametric method for evaluation of the aggregate loss distribution\n(ALD) by combining and numerically inverting the empirical characteristic\nfunctions (CFs) is presented and illustrated. This approach to evaluate ALD is\nbased on purely non-parametric considerations, i.e., based on the empirical CFs\nof frequency and severity of the claims in the actuarial risk applications.\nThis approach can be, however, naturally generalized to a more complex\nsemi-parametric modeling approach, e.g., by incorporating the generalized\nPareto distribution fit of the severity distribution heavy tails, and/or by\nconsidering the weighted mixture of the parametric CFs (used to model the\nexpert knowledge) and the empirical CFs (used to incorporate the knowledge\nbased on the historical data - internal and/or external). Here we present a\nsimple and yet efficient method and algorithms for numerical inversion of the\nCF, suitable for evaluation of the ALDs and the associated measures of interest\nimportant for applications, as, e.g., the value at risk (VaR). The presented\napproach is based on combination of the Gil-Pelaez inversion formulae for\nderiving the probability distribution (PDF and CDF) from the compound\n(empirical) CF and the trapezoidal rule used for numerical integration. The\napplicability of the suggested approach is illustrated by analysis of a well\nknow insurance dataset, the Danish fire loss data.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jan 2017 16:31:25 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Witkovsky", "Viktor", ""], ["Wimmer", "Gejza", ""], ["Duby", "Tomas", ""]]}, {"id": "1701.08312", "submitter": "Ronald Rivest", "authors": "Ronald L. Rivest", "title": "ClipAudit: A Simple Risk-Limiting Post-Election Audit", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple risk-limiting audit for elections, ClipAudit. To\ndetermine whether candidate A (the reported winner) actually beat candidate B\nin a plurality election, ClipAudit draws ballots at random, without\nreplacement, until either all cast ballots have been drawn, or until \\[ a - b\n\\ge \\beta \\sqrt{a+b}\n  \\] where $a$ is the number of ballots in the sample for the reported winner\nA, and $b$ is the number of ballots in the sample for opponent B, and where\n$\\beta$ is a constant determined a priori as a function of the number $n$ of\nballots cast and the risk-limit $\\alpha$. ClipAudit doesn't depend on the\nunofficial margin (as does Bravo). We show how to extend ClipAudit to contests\nwith multiple winners or losers, or to multiple contests.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jan 2017 18:51:10 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Rivest", "Ronald L.", ""]]}, {"id": "1701.08365", "submitter": "Ali Reza Taheriyoun", "authors": "Azam Saadatjouy and Ali R. Taheriyoun and Mohammad Q. Vahidi-Asl", "title": "Testing the Zonal Stationarity of Spatial Point Processes: Applied to\n  prostate tissues and trees locations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of testing the stationarity and isotropy of a spatial\npoint pattern based on the concept of local spectra. Using a logarithmic\ntransformation, the mechanism of the proposed test is approximately identical\nto a simple two factor analysis of variance procedure when the variance of\nresiduals is known. This procedure is also used for testing the stationarity in\nneighborhood of a particular point of the window of observation. The same idea\nis used in post-hoc tests to cluster the point pattern into stationary and\nnonstationary sub-windows. The performance of the proposed method is examined\nvia a simulation study and applied in a practical data.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jan 2017 11:42:03 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Saadatjouy", "Azam", ""], ["Taheriyoun", "Ali R.", ""], ["Vahidi-Asl", "Mohammad Q.", ""]]}, {"id": "1701.08588", "submitter": "Erik Schlicht", "authors": "Erik J. Schlicht and Nichole L. Morris", "title": "Estimating the risk associated with transportation technology using\n  multifidelity simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a quantitative method for estimating the risk associated\nwith candidate transportation technology, before it is developed and deployed.\nThe proposed solution extends previous methods that rely exclusively on\nlow-fidelity human-in-the-loop experimental data, or high-fidelity traffic\ndata, by adopting a multifidelity approach that leverages data from both low-\nand high-fidelity sources. The multifidelity method overcomes limitations\ninherent to existing approaches by allowing a model to be trained\ninexpensively, while still assuring that its predictions generalize to the\nreal-world. This allows for candidate technologies to be evaluated at the stage\nof conception, and enables a mechanism for only the safest and most effective\ntechnology to be developed and released.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 13:38:56 GMT"}, {"version": "v2", "created": "Tue, 31 Jan 2017 21:48:50 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Schlicht", "Erik J.", ""], ["Morris", "Nichole L.", ""]]}, {"id": "1701.08923", "submitter": "Bilal Khan", "authors": "Bilal Khan, Hsuan-Wei Lee and Kirk Dombrowski", "title": "One-step Estimation of Networked Population Size with Anonymity Using\n  Respondent-Driven Capture-Recapture and Hashing", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimates of population size for hidden and hard-to-reach individuals are of\nparticular interest to health officials when health problems are concentrated\nin such populations. Efforts to derive these estimates are often frustrated by\na range of factors including social stigma or an association with illegal\nactivities that ordinarily preclude conventional survey strategies. This paper\nbuilds on and extends prior work that proposed a method to meet these\nchallenges. Here we describe a rigorous formalization of a one-step,\nnetwork-based population estimation procedure that can be employed under\nconditions of anonymity. The estimation procedure is designed to be implemented\nalongside currently accepted strategies for research with hidden populations.\nSimulation experiments are described that test the efficacy of the method\nacross a range of implementation conditions and hidden population sizes. The\nresults of these experiments show that reliable population estimates can be\nderived for hidden, networked population as large as 12,500 and perhaps larger\nfor one family of random graphs. As such, the method shows potential for\ncost-effective implementation health and disease surveillance officials\nconcerned with hidden populations. Limitations and future work are discussed in\nthe concluding section.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 05:43:39 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Khan", "Bilal", ""], ["Lee", "Hsuan-Wei", ""], ["Dombrowski", "Kirk", ""]]}, {"id": "1701.09143", "submitter": "Casey Kneale", "authors": "Casey Kneale, Karl S. Booksh", "title": "Effective Calibration Transfer via M\\\"obius and Affine Transformations", "comments": "To be revised and submitted to the Journal of Chemometrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel technique for calibration transfer called the Modified Four Point\nInterpolant (MFPI) method is introduced for near infrared spectra. The method\nis founded on physical intuition and utilizes a series of quasiconformal maps\nin the frequency domain to transfer spectra from a slave instrument to a master\ninstrument's approximated space. Comparisons between direct standardization\n(DS), piecewise direct standardization (PDS), and MFPI for two publicly\navailable datasets are detailed herein. The results suggest that MFPI can\noutperform DS and PDS with respect to root mean squared errors of transfer and\nprediction. Combinations of MFPI with DS/PDS are also shown to reduce\npredictive errors after transfer.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 17:27:21 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Kneale", "Casey", ""], ["Booksh", "Karl S.", ""]]}]