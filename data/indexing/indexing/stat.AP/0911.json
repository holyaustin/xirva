[{"id": "0911.1189", "submitter": "Bertrand Iooss", "authors": "Amandine Marrel (IFP), Bertrand Iooss (M\\'ethodes d'Analyse\n  Stochastique des Codes et Traitements Num\\'eriques), Michel Jullien, Beatrice\n  Laurent (IMT), Elena Volkova", "title": "Global sensitivity analysis for models with spatially dependent outputs", "comments": null, "journal-ref": "Environmentrics 22 (2011) 383-397", "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The global sensitivity analysis of a complex numerical model often calls for\nthe estimation of variance-based importance measures, named Sobol' indices.\nMetamodel-based techniques have been developed in order to replace the cpu\ntime-expensive computer code with an inexpensive mathematical function, which\npredicts the computer code output. The common metamodel-based sensitivity\nanalysis methods are well-suited for computer codes with scalar outputs.\nHowever, in the environmental domain, as in many areas of application, the\nnumerical model outputs are often spatial maps, which may also vary with time.\nIn this paper, we introduce an innovative method to obtain a spatial map of\nSobol' indices with a minimal number of numerical model computations. It is\nbased upon the functional decomposition of the spatial output onto a wavelet\nbasis and the metamodeling of the wavelet coefficients by the Gaussian process.\nAn analytical example is presented to clarify the various steps of our\nmethodology. This technique is then applied to a real hydrogeological case: for\neach model input variable, a spatial map of Sobol' indices is thus obtained.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2009 07:33:31 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2010 08:12:40 GMT"}, {"version": "v3", "created": "Tue, 6 Jul 2010 16:25:22 GMT"}, {"version": "v4", "created": "Thu, 23 Sep 2010 11:58:53 GMT"}], "update_date": "2011-04-22", "authors_parsed": [["Marrel", "Amandine", "", "IFP"], ["Iooss", "Bertrand", "", "M\u00e9thodes d'Analyse\n  Stochastique des Codes et Traitements Num\u00e9riques"], ["Jullien", "Michel", "", "IMT"], ["Laurent", "Beatrice", "", "IMT"], ["Volkova", "Elena", ""]]}, {"id": "0911.1262", "submitter": "Jean-Fran\\c{c}ois Giovannelli", "authors": "Vincent Samson, Fr\\'ed\\'eric Champagnat, Jean-Fran\\c{c}ois Giovannelli", "title": "Point target detection and subpixel position estimation in optical\n  imagery", "comments": null, "journal-ref": "Applied optics, Special Issue on Image processing for EO sensors,\n  vol. 43, no.2, pp. 257-263, January 2004", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the issue of detecting point objects in a clutter\nbackground and estimating their position by image processing. We are interested\nin the specific context where the object signature significantly varies with\nits random subpixel location because of aliasing. Conventional matched filter\nneglects this phenomenon and causes consistent loss of detection performance.\nThus, alternative detectors are proposed and numerical results show the\nimprovement brought by approximate and generalized likelihood ratio tests in\ncomparison with pixel matched filtering. We also study the performance of two\ntypes of subpixel position estimators. Finally, we put forward the major\ninfluence of sensor design on both estimation and point object detection\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2009 14:22:56 GMT"}], "update_date": "2009-11-09", "authors_parsed": [["Samson", "Vincent", ""], ["Champagnat", "Fr\u00e9d\u00e9ric", ""], ["Giovannelli", "Jean-Fran\u00e7ois", ""]]}, {"id": "0911.1472", "submitter": "Bastiaan Geelhoed", "authors": "Bastiaan Geelhoed", "title": "Variable Second-Order Inclusion Probabilities as a Tool to Predict the\n  Sampling Variance", "comments": "This work was presented at the Third World Conference on Sampling and\n  Blending, October 2007, Porto Alegre, Brasil", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A generalization of Gy's theory for the variance of the fundamental sampling\nerror is reviewed. Practical situations where the generalized model potentially\nleads to more accurate variance estimates are identified as: clustering of\nparticles, differences in densities or sizes of the particles or repulsive\ninter-particle forces. Two general approaches for estimating an input parameter\nfor the generalized model are discussed. The first approach consists of\nmodelling based on physical properties of particles such as size, density and\nelectrostatic forces between particles. The second approach uses image analysis\nof actual samples. Further research into both methods is proposed and a\nsuggestion is made to use line-intercept sampling combined with Markov Chain\nmodelling in the second approach.\n  It is concluded that although, at the moment, it is too early for a routine\napplication of the generalized theory, the generalization has the potential of\nproviding more accurate variance estimates than are possible in the theory of\nGy. Therefore, further research into the development and expansion of the\ngeneralized theory is worthwhile.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2009 22:29:18 GMT"}], "update_date": "2009-11-10", "authors_parsed": [["Geelhoed", "Bastiaan", ""]]}, {"id": "0911.1697", "submitter": "Patrick J. Wolfe", "authors": "Daniel Rudoy, Thomas F. Quatieri, and Patrick J. Wolfe", "title": "Time-Varying Autoregressions in Speech: Detection Theory and\n  Applications", "comments": "12 pages, 12 figures; revised version", "journal-ref": "IEEE Transactions on Audio, Speech, and Language Processing, vol.\n  19, pp. 977-989, 2011", "doi": "10.1109/TASL.2010.2073704", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article develops a general detection theory for speech analysis based on\ntime-varying autoregressive models, which themselves generalize the classical\nlinear predictive speech analysis framework. This theory leads to a\ncomputationally efficient decision-theoretic procedure that may be applied to\ndetect the presence of vocal tract variation in speech waveform data. A\ncorresponding generalized likelihood ratio test is derived and studied both\nempirically for short data records, using formant-like synthetic examples, and\nasymptotically, leading to constant false alarm rate hypothesis tests for\nchanges in vocal tract configuration. Two in-depth case studies then serve to\nillustrate the practical efficacy of this procedure across different time\nscales of speech dynamics: first, the detection of formant changes on the scale\nof tens of milliseconds of data, and second, the identification of glottal\nopening and closing instants on time scales below ten milliseconds.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2009 15:10:11 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2010 11:55:17 GMT"}], "update_date": "2011-08-25", "authors_parsed": [["Rudoy", "Daniel", ""], ["Quatieri", "Thomas F.", ""], ["Wolfe", "Patrick J.", ""]]}, {"id": "0911.1768", "submitter": "James Scott", "authors": "James G. Scott", "title": "Benchmarking Historical Corporate Performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper uses Bayesian tree models for statistical benchmarking in data\nsets with awkward marginals and complicated dependence structures. The method\nis applied to a very large database on corporate performance over the last four\ndecades. The results of this study provide a formal basis for making\ncross-peer-group comparisons among companies in very different industries and\noperating environments. This is done by using models for Bayesian multiple\nhypothesis testing to determine which firms, if any, have systematically\noutperformed their peer groups over time. We conclude that systematic\noutperformance, while it seems to exist, is quite rare worldwide.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2009 00:35:27 GMT"}, {"version": "v2", "created": "Mon, 25 Oct 2010 19:03:49 GMT"}], "update_date": "2010-10-26", "authors_parsed": [["Scott", "James G.", ""]]}, {"id": "0911.2716", "submitter": "Dalia Chakrabarty Dr.", "authors": "Dalia Chakrabarty", "title": "Non-parametric Deprojection of Surface Brightness Profiles of Galaxies\n  in Generalised Geometries", "comments": "Accepted for publication in A&A; 15 pages of text; 15 figures", "journal-ref": null, "doi": "10.1051/0004-6361/200912008", "report-no": null, "categories": "astro-ph.IM astro-ph.GA stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new Bayesian non-parametric deprojection algorithm DOPING\n(Deprojection of Observed Photometry using and INverse Gambit), that is\ndesigned to extract 3-D luminosity density distributions $\\rho$ from observed\nsurface brightness maps $I$, in generalised geometries, while taking into\naccount changes in intrinsic shape with radius, using a penalised likelihood\napproach and an MCMC optimiser. We provide the most likely solution to the\nintegral equation that represents deprojection of the measured $I$ to $\\rho$.\nIn order to keep the solution modular, we choose to express $\\rho$ as a\nfunction of the line-of-sight (LOS) coordinate $z$. We calculate the extent of\nthe system along the ${\\bf z}$-axis, for a given point on the image that lies\nwithin an identified isophotal annulus. The extent along the LOS is binned and\ndensity is held a constant over each such $z$-bin. The code begins with a seed\ndensity and at the beginning of an iterative step, the trial $\\rho$ is updated.\nComparison of the projection of the current choice of $\\rho$ and the observed\n$I$ defines the likelihood function (which is supplemented by Laplacian\nregularisation), the maximal region of which is sought by the optimiser\n(Metropolis Hastings). The algorithm is successfully tested on a set of test\ngalaxies, the morphology of which ranges from an elliptical galaxy with varying\neccentricity to an infinitesimally thin disk galaxy marked by an abruptly\nvarying eccentricity profile. Applications are made to faint dwarf elliptical\ngalaxy Ic~3019 and another dwarf elliptical that is characterised by a central\nspheroidal nuclear component superimposed upon a more extended flattened\ncomponent. The result of deprojection of the X-ray image of triaxial cluster\nA1413 is also presented.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2009 22:13:17 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Chakrabarty", "Dalia", ""]]}, {"id": "0911.3349", "submitter": "Alyssa A. Goodman", "authors": "Alyssa Goodman", "title": "Seeing Science", "comments": "4 pages, including 3 figures. To appear in Proceedings of the\n  International Festival of Scientific Visualization, held in Tokyo, Japan,\n  March 2009. Publisher will be Universal Academy Press", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.CV cs.GR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to represent scientific data and concepts visually is becoming\nincreasingly important due to the unprecedented exponential growth of\ncomputational power during the present digital age. The data sets and\nsimulations scientists in all fields can now create are literally thousands of\ntimes as large as those created just 20 years ago. Historically successful\nmethods for data visualization can, and should, be applied to today's huge data\nsets, but new approaches, also enabled by technology, are needed as well.\nIncreasingly, \"modular craftsmanship\" will be applied, as relevant\nfunctionality from the graphically and technically best tools for a job are\ncombined as-needed, without low-level programming.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2009 17:01:33 GMT"}], "update_date": "2009-11-18", "authors_parsed": [["Goodman", "Alyssa", ""]]}, {"id": "0911.3462", "submitter": "Jonathan Touboul", "authors": "Jonathan Touboul, Olivier Faugeras", "title": "A Markovian event-based framework for stochastic spiking neural networks", "comments": null, "journal-ref": null, "doi": "10.1007/s10827-011-0327-y", "report-no": null, "categories": "stat.AP math.ST q-bio.NC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spiking neural networks, the information is conveyed by the spike times,\nthat depend on the intrinsic dynamics of each neuron, the input they receive\nand on the connections between neurons. In this article we study the Markovian\nnature of the sequence of spike times in stochastic neural networks, and in\nparticular the ability to deduce from a spike train the next spike time, and\ntherefore produce a description of the network activity only based on the spike\ntimes regardless of the membrane potential process.\n  To study this question in a rigorous manner, we introduce and study an\nevent-based description of networks of noisy integrate-and-fire neurons, i.e.\nthat is based on the computation of the spike times. We show that the firing\ntimes of the neurons in the networks constitute a Markov chain, whose\ntransition probability is related to the probability distribution of the\ninterspike interval of the neurons in the network. In the cases where the\nMarkovian model can be developed, the transition probability is explicitly\nderived in such classical cases of neural networks as the linear\nintegrate-and-fire neuron models with excitatory and inhibitory interactions,\nfor different types of synapses, possibly featuring noisy synaptic integration,\ntransmission delays and absolute and relative refractory period. This covers\nmost of the cases that have been investigated in the event-based description of\nspiking deterministic neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2009 04:22:08 GMT"}, {"version": "v2", "created": "Fri, 18 Feb 2011 09:42:36 GMT"}], "update_date": "2012-11-07", "authors_parsed": [["Touboul", "Jonathan", ""], ["Faugeras", "Olivier", ""]]}, {"id": "0911.3944", "submitter": "Patrick J. Wolfe", "authors": "Christopher M. White, Sanjeev P. Khudanpur, and Patrick J. Wolfe", "title": "Likelihood-based semi-supervised model selection with applications to\n  speech processing", "comments": "11 pages, 2 figures; submitted for publication", "journal-ref": "IEEE Journal of Selected Topics in Signal Processing, vol. 4, pp.\n  1016-1026, 2010", "doi": "10.1109/JSTSP.2010.2076050", "report-no": null, "categories": "stat.ML cs.CL cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In conventional supervised pattern recognition tasks, model selection is\ntypically accomplished by minimizing the classification error rate on a set of\nso-called development data, subject to ground-truth labeling by human experts\nor some other means. In the context of speech processing systems and other\nlarge-scale practical applications, however, such labeled development data are\ntypically costly and difficult to obtain. This article proposes an alternative\nsemi-supervised framework for likelihood-based model selection that leverages\nunlabeled data by using trained classifiers representing each model to\nautomatically generate putative labels. The errors that result from this\nautomatic labeling are shown to be amenable to results from robust statistics,\nwhich in turn provide for minimax-optimal censored likelihood ratio tests that\nrecover the nonparametric sign test as a limiting case. This approach is then\nvalidated experimentally using a state-of-the-art automatic speech recognition\nsystem to select between candidate word pronunciations using unlabeled speech\ndata that only potentially contain instances of the words under test. Results\nprovide supporting evidence for the utility of this approach, and suggest that\nit may also find use in other applications of machine learning.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2009 01:30:36 GMT"}], "update_date": "2011-08-25", "authors_parsed": [["White", "Christopher M.", ""], ["Khudanpur", "Sanjeev P.", ""], ["Wolfe", "Patrick J.", ""]]}, {"id": "0911.4207", "submitter": "Rafael S.  Calsaverini", "authors": "Rafael S. Calsaverini, Renato Vicente", "title": "An information theoretic approach to statistical dependence: copula\n  information", "comments": "to appear in Europhysics Letters", "journal-ref": "Europ. Phys. Lett. 88 68003 (2009)", "doi": "10.1209/0295-5075/88/68003", "report-no": null, "categories": "q-fin.ST cs.IT math.IT physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the connection between information and copula theories by showing\nthat a copula can be employed to decompose the information content of a\nmultivariate distribution into marginal and dependence components, with the\nlatter quantified by the mutual information. We define the information excess\nas a measure of deviation from a maximum entropy distribution. The idea of\nmarginal invariant dependence measures is also discussed and used to show that\nempirical linear correlation underestimates the amplitude of the actual\ncorrelation in the case of non-Gaussian marginals. The mutual information is\nshown to provide an upper bound for the asymptotic empirical log-likelihood of\na copula. An analytical expression for the information excess of T-copulas is\nprovided, allowing for simple model identification within this family. We\nillustrate the framework in a financial data set.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2009 22:41:35 GMT"}], "update_date": "2011-10-26", "authors_parsed": [["Calsaverini", "Rafael S.", ""], ["Vicente", "Renato", ""]]}, {"id": "0911.4503", "submitter": "Shane Jensen", "authors": "Blakeley B. McShane, Alexander Braunstein, James Piette, and Shane T.\n  Jensen", "title": "A Bayesian Variable Selection Approach to Major League Baseball Hitting\n  Metrics", "comments": null, "journal-ref": "Journal of Quantitative Analysis in Sports 2011, Vol. 7, No. 4,\n  Art. 2", "doi": "10.2202/1559-0410.1323", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous statistics have been proposed for the measure of offensive ability\nin major league baseball. While some of these measures may offer moderate\npredictive power in certain situations, it is unclear which simple offensive\nmetrics are the most reliable or consistent. We address this issue with a\nBayesian hierarchical model for variable selection to capture which offensive\nmetrics are most predictive within players across time. Our sophisticated\nmethodology allows for full estimation of the posterior distributions for our\nparameters and automatically adjusts for multiple testing, providing a distinct\nadvantage over alternative approaches. We implement our model on a set of 50\ndifferent offensive metrics and discuss our results in the context of\ncomparison to other variable selection techniques. We find that 33/50 metrics\ndemonstrate signal. However, these metrics are highly correlated with one\nanother and related to traditional notions of performance (e.g., plate\ndiscipline, power, and ability to make contact).\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2009 22:25:58 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["McShane", "Blakeley B.", ""], ["Braunstein", "Alexander", ""], ["Piette", "James", ""], ["Jensen", "Shane T.", ""]]}, {"id": "0911.4650", "submitter": "Gael Varoquaux", "authors": "Ga\\\"el Varoquaux (INRIA Saclay - Ile de France, LNAO), Sepideh\n  Sadaghiani (LCogn), Jean Baptiste Poline (LNAO), Bertrand Thirion (INRIA\n  Saclay - Ile de France, LNAO)", "title": "CanICA: Model-based extraction of reproducible group-level ICA patterns\n  from fMRI time series", "comments": null, "journal-ref": "Medical Image Computing and Computer Aided Intervention, London :\n  United Kingdom (2009)", "doi": null, "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial Independent Component Analysis (ICA) is an increasingly used\ndata-driven method to analyze functional Magnetic Resonance Imaging (fMRI)\ndata. To date, it has been used to extract meaningful patterns without prior\ninformation. However, ICA is not robust to mild data variation and remains a\nparameter-sensitive algorithm. The validity of the extracted patterns is hard\nto establish, as well as the significance of differences between patterns\nextracted from different groups of subjects. We start from a generative model\nof the fMRI group data to introduce a probabilistic ICA pattern-extraction\nalgorithm, called CanICA (Canonical ICA). Thanks to an explicit noise model and\ncanonical correlation analysis, our method is auto-calibrated and identifies\nthe group-reproducible data subspace before performing ICA. We compare our\nmethod to state-of-the-art multi-subject fMRI ICA methods and show that the\nfeatures extracted are more reproducible.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2009 15:25:38 GMT"}], "update_date": "2009-11-25", "authors_parsed": [["Varoquaux", "Ga\u00ebl", "", "INRIA Saclay - Ile de France, LNAO"], ["Sadaghiani", "Sepideh", "", "LCogn"], ["Poline", "Jean Baptiste", "", "LNAO"], ["Thirion", "Bertrand", "", "INRIA\n  Saclay - Ile de France, LNAO"]]}]