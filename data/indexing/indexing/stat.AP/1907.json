[{"id": "1907.00070", "submitter": "Rostislav Serota", "authors": "M. Dashti Moghaddam, Jiong Liu, John G. Holden and R. A. Serota", "title": "Modeling Response Time Distributions with Generalized Beta Prime", "comments": "15 pages, 11 figure, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use Generalized Beta Prime distribution, also known as GB2, for fitting\nresponse time distributions. This distribution, characterized by one scale and\nthree shape parameters, is incredibly flexible in that it can mimic behavior of\nmany other distributions. GB2 exhibits power-law behavior at both front and\ntail ends and is a steady-state distribution of a simple stochastic\ndifferential equation. We apply GB2 in contrast studies between two distinct\ngroups -- in this case children with dyslexia and a control group -- and show\nthat it provides superior fitting. We compare aggregate response time\ndistributions of the two groups for scale and shape differences (including\nseveral scale-independent measures of variability, such as Hoover index), which\nmay in turn reflect on cognitive dynamics differences. In this approach,\nresponse time distribution of an individual can be considered as a random\nvariate of that individual's group distribution.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 20:46:21 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Moghaddam", "M. Dashti", ""], ["Liu", "Jiong", ""], ["Holden", "John G.", ""], ["Serota", "R. A.", ""]]}, {"id": "1907.00093", "submitter": "Matthew Thomas", "authors": "Matthew L. Thomas and Gavin Shaddick and Daniel Simpson and Kees de\n  Hoogh and James V. Zidek", "title": "Data integration for high-resolution, continental-scale estimation of\n  air pollution concentrations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Air pollution constitutes the highest environmental risk factor in relation\nto heath. In order to provide the evidence required for health impact analyses,\nto inform policy and to develop potential mitigation strategies comprehensive\ninformation is required on the state of air pollution. Information on air\npollution traditionally comes from ground monitoring (GM) networks but these\nmay not be able to provide sufficient coverage and may need to be supplemented\nwith information from other sources (e.g. chemical transport models; CTMs).\nHowever, these may only be available on grids and may not capture micro-scale\nfeatures that may be important in assessing air quality in areas of high\npopulation. We develop a model that allows calibration between multiple data\nsources available at different levels of support by allowing the coefficients\nof calibration equations to vary over space and time, enabling downscaling\nwhere the data is sufficient to support it. The model is used to produce\nhigh-resolution (1km $\\times$ 1km) estimates of NO$_2$ and PM$_{2.5}$ across\nWestern Europe for 2010-2016. Concentrations of both pollutants are decreasing\nduring this period, however there remain large populations exposed to levels\nexceeding the WHO Air Quality Guidelines and thus air pollution remains a\nserious threat to health.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 21:57:38 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 11:06:20 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Thomas", "Matthew L.", ""], ["Shaddick", "Gavin", ""], ["Simpson", "Daniel", ""], ["de Hoogh", "Kees", ""], ["Zidek", "James V.", ""]]}, {"id": "1907.00111", "submitter": "Xueqi Zhao", "authors": "Xueqi Zhao, Enrique del Castillo", "title": "An Intrinsic Geometrical Approach for Statistical Process Control of\n  Surface and Manifold Data", "comments": null, "journal-ref": null, "doi": "10.1080/00401706.2020.1772114", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for statistical process control (SPC) of a discrete\npart manufacturing system based on intrinsic geometrical properties of the\nparts, estimated from three-dimensional sensor data. An intrinsic method has\nthe computational advantage of avoiding the difficult part registration\nproblem, necessary in previous SPC approaches of three-dimensional geometrical\ndata, but inadequate if noncontact sensors are used. The approach estimates the\nspectrum of the Laplace-Beltrami (LB) operator of the scanned parts and uses a\nmultivariate nonparametric control chart for online process control. Our\nproposal brings SPC closer to computer vision and computer graphics methods\naimed to detect large differences in shape (but not in size). However, the SPC\nproblem differs in that small changes in either shape or size of the parts need\nto be detected, keeping a controllable false alarm rate and without completely\nfiltering noise. An online or \"Phase II\" method and a scheme for starting up in\nthe absence of prior data (\"Phase I\") are presented. Comparison with earlier\napproaches that require registration shows the LB spectrum method to be more\nsensitive to rapidly detect small changes in shape and size, including the\npractical case when the sequence of part datasets is in the form of large,\nunequal size meshes. A post-alarm diagnostic method to investigate the location\nof defects on the surface of a part is also presented. While we focus in this\narticle on surface (triangulation) data, the methods can also be applied to\npoint cloud and voxel metrology data.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 22:53:28 GMT"}, {"version": "v2", "created": "Tue, 2 Jul 2019 18:04:56 GMT"}, {"version": "v3", "created": "Fri, 10 Jul 2020 05:11:01 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Zhao", "Xueqi", ""], ["del Castillo", "Enrique", ""]]}, {"id": "1907.00161", "submitter": "Kristian Brock", "authors": "Kristian Brock", "title": "trialr: Bayesian Clinical Trial Designs in R and Stan", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This manuscript introduces an \\proglang{R} package called \\pkg{trialr} that\nimplements a collection of clinical trial methods in \\proglang{Stan} and\n\\proglang{R}. In this article, we explore three methods in detail. The first is\nthe continual reassessment method for conducting phase I dose-finding trials\nthat seek a maximum tolerable dose. The second is EffTox, a dose-finding design\nthat scrutinises doses by joint efficacy and toxicity outcomes. The third is\nthe augmented binary method for modelling the probability of treatment success\nin phase II oncology trials with reference to repeated measures of continuous\ntumour size and binary indicators of treatment failure. We emphasise in this\narticle the benefits that stem from having access to posterior samples,\nincluding flexible inference and powerful visualisation. We hope that this\npackage encourages the use of Bayesian methods in clinical trials.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 07:33:20 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Brock", "Kristian", ""]]}, {"id": "1907.00287", "submitter": "Jelena Bradic", "authors": "Jue Hou and Jelena Bradic and Ronghui Xu", "title": "Estimating Treatment Effect under Additive Hazards Models with\n  High-dimensional Covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating causal effects for survival outcomes in the high-dimensional\nsetting is an extremely important topic for many biomedical applications as\nwell as areas of social sciences. We propose a new orthogonal score method for\ntreatment effect estimation and inference that results in asymptotically valid\nconfidence intervals assuming only good estimation properties of the hazard\noutcome model and the conditional probability of treatment. This guarantee\nallows us to provide valid inference for the conditional treatment effect under\nthe high-dimensional additive hazards model under considerably more generality\nthan existing approaches. In addition, we develop a new Hazards Difference\n(HDi), estimator. We showcase that our approach has double-robustness\nproperties in high dimensions: with cross-fitting, the HDi estimate is\nconsistent under a wide variety of treatment assignment models; the HDi\nestimate is also consistent when the hazards model is misspecified and instead\nthe true data generating mechanism follows a partially linear additive hazards\nmodel. We further develop a novel sparsity doubly robust result, where either\nthe outcome or the treatment model can be a fully dense high-dimensional model.\nWe apply our methods to study the treatment effect of radical prostatectomy\nversus conservative management for prostate cancer patients using the\nSEER-Medicare Linked Data.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 22:15:59 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Hou", "Jue", ""], ["Bradic", "Jelena", ""], ["Xu", "Ronghui", ""]]}, {"id": "1907.00304", "submitter": "Peida Tian", "authors": "Peida Tian, Victoria Kostina", "title": "Nonstationary Gauss-Markov Processes: Parameter Estimation and\n  Dispersion", "comments": "25 pages, 5 figures", "journal-ref": "in IEEE Transactions on Information Theory, vol. 67, no. 4, pp.\n  2426-2449, April 2021", "doi": "10.1109/TIT.2021.3050342", "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a precise error analysis for the maximum likelihood\nestimate $\\hat{a}_{\\text{ML}}(u_1^n)$ of the parameter $a$ given samples $u_1^n\n= (u_1, \\ldots, u_n)'$ drawn from a nonstationary Gauss-Markov process $U_i = a\nU_{i-1} + Z_i,~i\\geq 1$, where $U_0 = 0$, $a> 1$, and $Z_i$'s are independent\nGaussian random variables with zero mean and variance $\\sigma^2$. We show a\ntight nonasymptotic exponentially decaying bound on the tail probability of the\nestimation error. Unlike previous works, our bound is tight already for a\nsample size of the order of hundreds. We apply the new estimation bound to find\nthe dispersion for lossy compression of nonstationary Gauss-Markov sources. We\nshow that the dispersion is given by the same integral formula that we derived\npreviously for the asymptotically stationary Gauss-Markov sources, i.e., $|a| <\n1$. New ideas in the nonstationary case include separately bounding the maximum\neigenvalue (which scales exponentially) and the other eigenvalues (which are\nbounded by constants that depend only on $a$) of the covariance matrix of the\nsource sequence, and new techniques in the derivation of our estimation error\nbound.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 01:40:20 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 01:51:08 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Tian", "Peida", ""], ["Kostina", "Victoria", ""]]}, {"id": "1907.00307", "submitter": "Hongwei Wang", "authors": "Hongwei Wang, Wei Zhang, Junyi Zuo and Heping Wang", "title": "Outlier-robust Kalman filters with mixture correntropy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the robust filtering problem for a nonlinear state-space model\nwith outliers in measurements. To improve the robustness of the traditional\nKalman filtering algorithm, we propose in this work two robust filters based on\nmixture correntropy, especially the double-Gaussian mixture correntropy and\nLaplace-Gaussian mixture correntropy. We have formulated the robust filtering\nproblem by adopting the mixture correntropy induced cost to replace the\nquadratic one in the conventional Kalman filter for measurement fitting errors.\nIn addition, a tradeoff weight coefficient is introduced to make sure the\nproposed approaches can provide reasonable state estimates in scenarios where\nmeasurement fitting errors are small. The formulated robust filtering problems\nare iteratively solved by utilizing the cubature Kalman filtering framework\nwith a reweighted measurement covariance. Numerical results show that the\nproposed methods can achieve a performance improvement over existing robust\nsolutions.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 02:52:50 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 02:12:50 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Wang", "Hongwei", ""], ["Zhang", "Wei", ""], ["Zuo", "Junyi", ""], ["Wang", "Heping", ""]]}, {"id": "1907.00502", "submitter": "John Malik", "authors": "Yu-Ting Lin, John Malik, Hau-Tieng Wu", "title": "Wave-shape oscillatory model for nonstationary periodic time series\n  analysis", "comments": "35 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The oscillations observed in many time series, particularly in biomedicine,\nexhibit morphological variations over time. These morphological variations are\ncaused by intrinsic or extrinsic changes to the state of the generating system,\nhenceforth referred to as dynamics. To model these time series (including and\nspecifically pathophysiological ones) and estimate the underlying dynamics, we\nprovide a novel wave-shape oscillatory model. In this model, time-dependent\nvariations in cycle shape occur along a manifold called the wave-shape\nmanifold. To estimate the wave-shape manifold associated with an oscillatory\ntime series, study the dynamics, and visualize the time-dependent changes along\nthe wave-shape manifold, we propose a novel algorithm coined Dynamic Diffusion\nmap (DDmap) by applying the well-established diffusion maps (DM) algorithm to\nthe set of all observed oscillations. We provide a theoretical guarantee on the\ndynamical information recovered by the DDmap algorithm under the proposed\nmodel. Applying the proposed model and algorithm to arterial blood pressure\n(ABP) signals recorded during general anesthesia leads to the extraction of\nnociception information. Applying the wave-shape oscillatory model and the\nDDmap algorithm to cardiac cycles in the electrocardiogram (ECG) leads to\nectopy detection and a new ECG-derived respiratory signal, even when the\nsubject has atrial fibrillation.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 00:05:48 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 17:37:45 GMT"}, {"version": "v3", "created": "Sun, 28 Mar 2021 16:48:10 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Lin", "Yu-Ting", ""], ["Malik", "John", ""], ["Wu", "Hau-Tieng", ""]]}, {"id": "1907.00510", "submitter": "Amir Karami", "authors": "Amir Karami, Suzanne C. Swan, Cynthia Nicole White, Kayla Ford", "title": "Hidden in Plain Sight For Too Long: Using Text Mining Techniques to\n  Shine a Light on Workplace Sexism and Sexual Harassment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: The goal of this study is to understand how people experience\nsexism and sexual harassment in the workplace by discovering themes in 2,362\nexperiences posted on the Everyday Sexism Project's website everydaysexism.com.\nMethod: This study used both quantitative and qualitative methods. The\nquantitative method was a computational framework to collect and analyze a\nlarge number of workplace sexual harassment experiences. The qualitative method\nwas the analysis of the topics generated by a text mining method. Results:\nTwenty-three topics were coded and then grouped into three overarching themes\nfrom the sex discrimination and sexual harassment literature. The Sex\nDiscrimination theme included experiences in which women were treated\nunfavorably due to their sex, such as being passed over for promotion, denied\nopportunities, paid less than men, and ignored or talked over in meetings. The\nSex Discrimination and Gender harassment theme included stories about sex\ndiscrimination and gender harassment, such as sexist hostility behaviors\nranging from insults and jokes invoking misogynistic stereotypes to bullying\nbehavior. The last theme, Unwanted Sexual Attention, contained stories\ndescribing sexual comments and behaviors used to degrade women. Unwanted\ntouching was the highest weighted topic, indicating how common it was for\nwebsite users to endure being touched, hugged or kissed, groped, and grabbed.\nConclusions: This study illustrates how researchers can use automatic processes\nto go beyond the limits of traditional research methods and investigate\nnaturally occurring large scale datasets on the internet to achieve a better\nunderstanding of everyday workplace sexism experiences.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 01:48:49 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Karami", "Amir", ""], ["Swan", "Suzanne C.", ""], ["White", "Cynthia Nicole", ""], ["Ford", "Kayla", ""]]}, {"id": "1907.00552", "submitter": "Luai Al-Labadi Dr.", "authors": "Luai Al Labadi, Hishyar Khalil and Nida Siddiqui", "title": "Male Under-performance in Undergraduate Engineering Mathematical\n  Courses: Causes and Solution Strategy", "comments": "11", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.HO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of students in Mathematics and its allied fields has been a\ntopic of great interest for mathematical educationalists worldwide. In this\npaper, we study the student performance in one of the most math heavy\nfields-Engineering. An analysis of the performance of students using a sample\nfrom calculus courses across all fields within Engineering at the University of\nSharjah, UAE reveled a trend of girls performing better than the boys. To be\nable to apply corrective strategies to handle this issue, a survey was carried\nout focusing on micro issues which we as educationalist could deal with at the\ncollege level and the university level. The results of the survey pinpointed\nout clear reasons for this grade disparity among the genders, thus allowing us\nto propose some immediate and practical solutions to deal with this scenario.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 05:20:36 GMT"}, {"version": "v2", "created": "Tue, 2 Jul 2019 18:16:30 GMT"}, {"version": "v3", "created": "Thu, 6 Aug 2020 21:40:02 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Labadi", "Luai Al", ""], ["Khalil", "Hishyar", ""], ["Siddiqui", "Nida", ""]]}, {"id": "1907.00603", "submitter": "Sebastian Weber", "authors": "Sebastian Weber, Yue Li, John Seaman, Tomoyuki Kakizume, Heinz\n  Schmidli", "title": "Applying Meta-Analytic-Predictive Priors with the R Bayesian evidence\n  synthesis tools", "comments": "27 pages, 3 figures, RBesT R package on CRAN\n  https://cran.r-project.org/package=RBesT", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Use of historical data in clinical trial design and analysis has shown\nvarious advantages such as reduction of within-study placebo-treated number of\nsubjects and increase of study power. The meta-analytic-predictive (MAP)\napproach accounts with a hierarchical model for between-trial heterogeneity in\norder to derive an informative prior from historical (often control) data. In\nthis paper, we introduce the package RBesT (R Bayesian Evidence Synthesis\nTools) which implements the MAP approach with normal (known sampling standard\ndeviation), binomial and Poisson endpoints. The hierarchical MAP model is\nevaluated by MCMC. The numerical MCMC samples representing the MAP prior are\napproximated with parametric mixture densities which are obtained with the\nexpectation maximization algorithm. The parametric mixture density\nrepresentation facilitates easy communication of the MAP prior and enables via\nfast and accurate analytical procedures to evaluate properties of trial designs\nwith informative MAP priors. The paper first introduces the framework of robust\nBayesian evidence synthesis in this setting and then explains how RBesT\nfacilitates the derivation and evaluation of an informative MAP prior from\nhistorical control data. In addition we describe how the meta-analytic\nframework relates to further applications including probability of success\ncalculations.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 08:26:39 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 16:02:27 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Weber", "Sebastian", ""], ["Li", "Yue", ""], ["Seaman", "John", ""], ["Kakizume", "Tomoyuki", ""], ["Schmidli", "Heinz", ""]]}, {"id": "1907.00846", "submitter": "Ari Ercole", "authors": "Ari Ercole", "title": "ICU Disparnumerophobia and Triskaidekaphobia: The 'Irrational Care\n  Unit'?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whilst evidence-based medicine is the cornerstone of modern practice, it is\nlikely that clinicians are influenced by cultural biases. This work set out to\nlook for evidence of number preference in invasive mechanical ventilatory\ntherapy as a concrete example of subconscious treatment bias. A retrospective\nobservational intensive care electronic medical record database search and\nanalysis was carried out in adult general, specialist neurosciences and\npaediatric intensive care units within a tertiary referral hospital. All\nadmitted, invasively mechanically ventilated patients between October 2014 and\nAugust 2015 were included. Set positive end-expiratory pressure (PEEP),\nrespiratory rate (RR) and inspiratory pressure (Pinsp) settings were extracted.\nStatistical analysis using conventional testing and a novel Monte Carlo method\nwere used to look for evidence of two culturally prevalent superstitions:\nOdd/even preference and aversion to the number 13. Patients spent significantly\nlonger with odd choices for PEEP ($OR=0.16$, $p<2\\times10^{-16}$), RR\n($OR=0.31$, $p<2\\times10^{-16}$) and Pinsp (OR=0.48, $p=2.9\\times10^{-7}$). An\naversion to the number 13 was detected for choices of RR ($p=0.00024$) and\nPinsp ($p=3.9\\times10^{-5}$). However a PEEP of 13 was more prevalent than\nexpected by chance ($p=0.00028$). These findings suggest superstitious\npreferences in intensive care therapy do exist and practitioners should be\nalert to guard against other, less obvious but perhaps more clinically\nsignificant decision-making biases. The methodology described may be useful for\ndetecting statistically significant number preferences in other domains.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 15:13:49 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Ercole", "Ari", ""]]}, {"id": "1907.01175", "submitter": "Xinyu Song", "authors": "Xinyu Song, Donggyu Kim, Huiling Yuan, Xiangyu Cui, Zhiping Lu, Yong\n  Zhou, Yazhen Wang", "title": "Volatility Analysis with Realized GARCH-Ito Models", "comments": "39 pages, 4 tables, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a unified approach for modeling high-frequency\nfinancial data that can accommodate both the continuous-time jump-diffusion and\ndiscrete-time realized GARCH model by embedding the discrete realized GARCH\nstructure in the continuous instantaneous volatility process. The key feature\nof the proposed model is that the corresponding conditional daily integrated\nvolatility adopts an autoregressive structure where both integrated volatility\nand jump variation serve as innovations. We name it as the realized GARCH-Ito\nmodel. Given the autoregressive structure in the conditional daily integrated\nvolatility, we propose a quasi-likelihood function for parameter estimation and\nestablish its asymptotic properties. To improve the parameter estimation, we\npropose a joint quasi-likelihood function that is built on the marriage of\ndaily integrated volatility estimated by high-frequency data and nonparametric\nvolatility estimator obtained from option data. We conduct a simulation study\nto check the finite sample performance of the proposed methodologies and an\nempirical study with the S&P500 stock index and option data.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 05:26:36 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 16:26:35 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Song", "Xinyu", ""], ["Kim", "Donggyu", ""], ["Yuan", "Huiling", ""], ["Cui", "Xiangyu", ""], ["Lu", "Zhiping", ""], ["Zhou", "Yong", ""], ["Wang", "Yazhen", ""]]}, {"id": "1907.01184", "submitter": "Linh Nguyen PhD", "authors": "Linh Nguyen, Jaime Valls Miro, Lei Shi and Teresa Vidal-Calleja", "title": "Gaussian Mixture Marginal Distributions for Modelling Remaining Pipe\n  Wall Thickness of Critical Water Mains in Non-Destructive Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapidly estimating the remaining wall thickness (RWT) is paramount for the\nnon-destructive condition assessment evaluation of large critical metallic\npipelines. A robotic vehicle with embedded magnetism-based sensors has been\ndeveloped to traverse the inside of a pipeline and conduct inspections at the\nlocation of a break. However its sensing speed is constrained by the magnetic\nprinciple of operation, thus slowing down the overall operation in seeking\ndense RWT mapping. To ameliorate this drawback, this work proposes the partial\nscanning of the pipe and then employing Gaussian Processes (GPs) to infer RWT\nat the unseen pipe sections. Since GP prediction assumes to have normally\ndistributed input data - which does correspond with real RWT measurements -\nGaussian mixture (GM) models are proven in this work as fitting marginal\ndistributions to effectively capture the probability of any RWT value in the\ninspected data. The effectiveness of the proposed approach is extensively\nvalidated from real-world data collected in collaboration with a water utility\nfrom a cast iron water main pipeline in Sydney, Australia.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 06:07:12 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Nguyen", "Linh", ""], ["Miro", "Jaime Valls", ""], ["Shi", "Lei", ""], ["Vidal-Calleja", "Teresa", ""]]}, {"id": "1907.01196", "submitter": "Xinyu Song", "authors": "Xinyu Song", "title": "Large Volatility Matrix Prediction with High-Frequency Data", "comments": "Research method similar to the one covered in the manuscript has been\n  examined already", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a novel method for large volatility matrix prediction with\nhigh-frequency data by applying eigen-decomposition to daily realized\nvolatility matrix estimators and capturing eigenvalue dynamics with ARMA\nmodels. Given a sequence of daily volatility matrix estimators, we compute the\naggregated eigenvectors and obtain the corresponding eigenvalues. Eigenvalues\nin the same relative magnitude form a time series and the ARMA models are\nfurther employed to model the dynamics within each eigenvalue time series to\nproduce a predictor. We predict future large volatility matrix based on the\npredicted eigenvalues and the aggregated eigenvectors, and demonstrate the\nadvantages of the proposed method in volatility prediction and portfolio\nallocation problems.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 06:54:21 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 12:24:22 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Song", "Xinyu", ""]]}, {"id": "1907.01390", "submitter": "Fei Feng", "authors": "Fei Feng and Jiajia Luo", "title": "CSSegNet: Fine-Grained Cardiac Structures Segmentation Using Dilated\n  Pyramid Pooling in U-net", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiac structure segmentation plays an important role in medical analysis\nprocedures. Images' blurred boundaries issue always limits the segmentation\nperformance. To address this difficult problem, we presented a novel network\nstructure which embedded dilated pyramid pooling block in the skip connections\nbetween networks' encoding and decoding stage. A dilated pyramid pooling block\nis made up of convolutions and pooling operations with different vision scopes.\nEquipped the model with such module, it could be endowed with multi-scales\nvision ability. Together combining with other techniques, it included a\nmulti-scales initial features extraction and a multi-resolutions' prediction\naggregation module. As for backbone feature extraction network, we referred to\nthe basic idea of Xception network which benefited from separable convolutions.\nEvaluated on the Post 2017 MICCAI-ACDC challenge phase data, our proposed model\ncould achieve state-of-the-art performance in left ventricle (LVC) cavities and\nright ventricle cavities (RVC) segmentation tasks. Results revealed that our\nmethod has advantages on both geometrical (Dice coefficient, Hausdorff\ndistance) and clinical evaluation (Ejection Fraction, Volume), which represent\ncloser boundaries and more statistically significant separately.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 14:17:31 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Feng", "Fei", ""], ["Luo", "Jiajia", ""]]}, {"id": "1907.01406", "submitter": "Jwala Dhamala", "authors": "Jwala Dhamala, Sandesh Ghimire, John L. Sapp, B. Milan Horacek, Linwei\n  Wang", "title": "Bayesian Optimization on Large Graphs via a Graph Convolutional\n  Generative Model: Application in Cardiac Model Personalization", "comments": "9 pages, 5 figures, MICCAI", "journal-ref": null, "doi": "10.1007/978-3-030-32245-8_51", "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalization of cardiac models involves the optimization of organ tissue\nproperties that vary spatially over the non-Euclidean geometry model of the\nheart. To represent the high-dimensional (HD) unknown of tissue properties,\nmost existing works rely on a low-dimensional (LD) partitioning of the\ngeometrical model. While this exploits the geometry of the heart, it is of\nlimited expressiveness to allow partitioning that is small enough for effective\noptimization. Recently, a variational auto-encoder (VAE) was utilized as a more\nexpressive generative model to embed the HD optimization into the LD latent\nspace. Its Euclidean nature, however, neglects the rich geometrical information\nin the heart. In this paper, we present a novel graph convolutional VAE to\nallow generative modeling of non-Euclidean data, and utilize it to embed\nBayesian optimization of large graphs into a small latent space. This approach\nbridges the gap of previous works by introducing an expressive generative model\nthat is able to incorporate the knowledge of spatial proximity and hierarchical\ncompositionality of the underlying geometry. It further allows transferring of\nthe learned features across different geometries, which was not possible with a\nregular VAE. We demonstrate these benefits of the presented method in synthetic\nand real data experiments of estimating tissue excitability in a cardiac\nelectrophysiological model.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 17:47:21 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 16:01:35 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Dhamala", "Jwala", ""], ["Ghimire", "Sandesh", ""], ["Sapp", "John L.", ""], ["Horacek", "B. Milan", ""], ["Wang", "Linwei", ""]]}, {"id": "1907.01458", "submitter": "Uri Keich", "authors": "Kristen Emery, Syamand Hasam, William Stafford Noble, Uri Keich", "title": "Multiple competition-based FDR control for peptide detection", "comments": "Numerous changes from the initial submission including an expanded\n  section on peptide detection (context/motivation and results), refocused and\n  streamlined methods development section, revised and more selective figures\n  reflecting the most recent analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Competition-based FDR control has been commonly used for over a decade in the\ncomputational mass spectrometry community (Elias and Gygi, 2007). Recently, the\napproach has gained significant popularity in other fields after Barber and\nCandes (2015) laid its theoretical foundation in a more general setting that\nincluded the feature selection problem. In both cases, the competition is based\non a head-to-head comparison between an observed score and a corresponding\ndecoy / knockoff. Keich and Noble (2017b) recently demonstrated some advantages\nof using multiple rather than a single decoy when addressing the problem of\nassigning peptide sequences to observed mass spectra. In this work, we consider\na related problem -- detecting peptides based on a collection of mass spectra\n-- and we develop a new framework for competition-based FDR control using\nmultiple null scores. Within this framework, we offer several methods, all of\nwhich are based on a novel procedure that rigorously controls the FDR in the\nfinite sample setting. Using real data to study the peptide detection problem\nwe show that, relative to existing single-decoy methods, our approach can\nincrease the number of discovered peptides by up to 50% at small FDR\nthresholds.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 15:39:57 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2019 10:32:44 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Emery", "Kristen", ""], ["Hasam", "Syamand", ""], ["Noble", "William Stafford", ""], ["Keich", "Uri", ""]]}, {"id": "1907.01505", "submitter": "Umberto Simola Mr.", "authors": "Umberto Simola, Jessica Cisewski-Kehe, Michael U. Gutmann, Jukka\n  Corander", "title": "Adaptive Approximate Bayesian Computation Tolerance Selection", "comments": "26 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian Computation (ABC) methods are increasingly used for\ninference in situations in which the likelihood function is either\ncomputationally costly or intractable to evaluate. Extensions of the basic ABC\nrejection algorithm have improved the computational efficiency of the procedure\nand broadened its applicability. The ABC-Population Monte Carlo (ABC-PMC)\napproach of Beaumont et al. (2009) has become a popular choice for approximate\nsampling from the posterior. ABC-PMC is a sequential sampler with an\niteratively decreasing value of the tolerance, which specifies how close the\nsimulated data need to be to the real data for acceptance. We propose a method\nfor adaptively selecting a sequence of tolerances that improves the\ncomputational efficiency of the algorithm over other common techniques. In\naddition we define a stopping rule as a by-product of the adaptation procedure,\nwhich assists in automating termination of sampling. The proposed automatic\nABC-PMC algorithm can be easily implemented and we present several examples\ndemonstrating its benefits in terms of computational efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 16:02:58 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 07:47:45 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Simola", "Umberto", ""], ["Cisewski-Kehe", "Jessica", ""], ["Gutmann", "Michael U.", ""], ["Corander", "Jukka", ""]]}, {"id": "1907.01589", "submitter": "Roy R. Lederman", "authors": "Roy R. Lederman, Joakim And\\'en, Amit Singer", "title": "Hyper-Molecules: on the Representation and Recovery of Dynamical\n  Structures, with Application to Flexible Macro-Molecular Structures in\n  Cryo-EM", "comments": null, "journal-ref": null, "doi": "10.1088/1361-6420/ab5ede", "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cryo-electron microscopy (cryo-EM), the subject of the 2017 Nobel Prize in\nChemistry, is a technology for determining the 3-D structure of macromolecules\nfrom many noisy 2-D projections of instances of these macromolecules, whose\norientations and positions are unknown. The molecular structures are not rigid\nobjects, but flexible objects involved in dynamical processes. The different\nconformations are exhibited by different instances of the macromolecule\nobserved in a cryo-EM experiment, each of which is recorded as a particle\nimage. The range of conformations and the conformation of each particle are not\nknown a priori; one of the great promises of cryo-EM is to map this\nconformation space. Remarkable progress has been made in determining rigid\nstructures from homogeneous samples of molecules in spite of the unknown\norientation of each particle image and significant progress has been made in\nrecovering a few distinct states from mixtures of rather distinct\nconformations, but more complex heterogeneous samples remain a major challenge.\nWe introduce the ``hyper-molecule'' framework for modeling structures across\ndifferent states of heterogeneous molecules, including continuums of states.\nThe key idea behind this framework is representing heterogeneous macromolecules\nas high-dimensional objects, with the additional dimensions representing the\nconformation space. This idea is then refined to model properties such as\nlocalized heterogeneity. In addition, we introduce an algorithmic framework for\nrecovering such maps of heterogeneous objects from experimental data using a\nBayesian formulation of the problem and Markov chain Monte Carlo (MCMC)\nalgorithms to address the computational challenges in recovering these high\ndimensional hyper-molecules. We demonstrate these ideas in a prototype applied\nto synthetic data.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 19:16:12 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Lederman", "Roy R.", ""], ["And\u00e9n", "Joakim", ""], ["Singer", "Amit", ""]]}, {"id": "1907.01723", "submitter": "Yihuang Kang", "authors": "Yihuang Kang, I-Ling Cheng, Wenjui Mao, Bowen Kuo, Pei-Ju Lee", "title": "Towards Interpretable Deep Extreme Multi-label Learning", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many Machine Learning algorithms, such as deep neural networks, have long\nbeen criticized for being \"black-boxes\"-a kind of models unable to provide how\nit arrive at a decision without further efforts to interpret. This problem has\nraised concerns on model applications' trust, safety, nondiscrimination, and\nother ethical issues. In this paper, we discuss the machine learning\ninterpretability of a real-world application, eXtreme Multi-label Learning\n(XML), which involves learning models from annotated data with many pre-defined\nlabels. We propose a two-step XML approach that combines deep non-negative\nautoencoder with other multi-label classifiers to tackle different data\napplications with a large number of labels. Our experimental result shows that\nthe proposed approach is able to cope with many-label problems as well as to\nprovide interpretable label hierarchies and dependencies that helps us\nunderstand how the model recognizes the existences of objects in an image.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 03:51:31 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Kang", "Yihuang", ""], ["Cheng", "I-Ling", ""], ["Mao", "Wenjui", ""], ["Kuo", "Bowen", ""], ["Lee", "Pei-Ju", ""]]}, {"id": "1907.01736", "submitter": "Luai Al-Labadi Dr.", "authors": "Luai Al-Labadi, Forough Fazeli Asl and Zahra Saberi", "title": "A Bayesian Semiparametric Gaussian Copula Approach to a Multivariate\n  Normality Test", "comments": "30 pages, 4 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a Bayesian semiparametric copula approach is used to model the\nunderlying multivariate distribution $F_{true}$. First, the Dirichlet process\nis constructed on the unknown marginal distributions of $F_{true}$. Then a\nGaussian copula model is utilized to capture the dependence structure of\n$F_{true}$. As a result, a Bayesian multivariate normality test is developed by\ncombining the relative belief ratio and the Energy distance. Several\ninteresting theoretical results of the approach are derived. Finally, through\nseveral simulated examples and a real data set, the proposed approach reveals\nexcellent performance.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 04:45:54 GMT"}, {"version": "v2", "created": "Thu, 4 Jul 2019 04:50:09 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Al-Labadi", "Luai", ""], ["Asl", "Forough Fazeli", ""], ["Saberi", "Zahra", ""]]}, {"id": "1907.01781", "submitter": "Lucie Bernard", "authors": "Lucie Bernard (IDP), Philippe Leduc (ST-TOURS)", "title": "Estimating a probability of failure with the convex order in computer\n  experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the estimation of a failure probability of an\nindustrial product. To be more specific, it is defined as the probability that\nthe output of a physical model, with random input variables, exceeds a\nthreshold. The model corresponds with an expensive to evaluate black-box\nfunction, so that classical Monte Carlo simulation methods cannot be applied.\nBayesian principles of the Kriging method are then used to design an estimator\nof the failure probability. From a numerical point of view, the practical use\nof this estimator is restricted. An alternative estimator is proposed, which is\nequivalent in term of bias. The main result of this paper concerns the\nexistence of a convex order inequality between these two estimators. This\ninequality allows to compare their efficiency and to quantify the uncertainty\non the results that these estimators provide. A sequential procedure for the\nconstruction of a design of computer experiments, based on the principle of the\nStepwise Uncertainty Reduction strategies, also results of the convex order\ninequality. The interest of this approach is highlighted through the study of a\nreal case from the company STMicroelectronics.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 07:50:38 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Bernard", "Lucie", "", "IDP"], ["Leduc", "Philippe", "", "ST-TOURS"]]}, {"id": "1907.01894", "submitter": "Francis Bunnin", "authors": "F.O.Bunnin and J.Q.Smith", "title": "A Bayesian Hierarchical Model for Criminal Investigations", "comments": "57 pages, 20 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Potential violent criminals will often need to go through a sequence of\npreparatory steps before they can execute their plans. During this escalation\nprocess police have the opportunity to evaluate the threat posed by such people\nthrough what they know, observe and learn from intelligence reports about their\nactivities. In this paper we customise a three-level Bayesian hierarchical\nmodel to describe this process. This is able to propagate both routine and\nunexpected evidence in real time. We discuss how to set up such a model so that\nit calibrates to domain expert judgments. The model illustrations include a\nhypothetical example based on a potential vehicle based terrorist attack.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 12:40:42 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 14:39:31 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Bunnin", "F. O.", ""], ["Smith", "J. Q.", ""]]}, {"id": "1907.01898", "submitter": "Amit Moscovich", "authors": "Amit Moscovich and Amit Halevi and Joakim And\\'en and Amit Singer", "title": "Cryo-EM reconstruction of continuous heterogeneity by Laplacian spectral\n  volumes", "comments": "33 pages, 10 figures", "journal-ref": "Inverse Problems, 36:2 (2020)", "doi": "10.1088/1361-6420/ab4f55", "report-no": null, "categories": "eess.IV cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-particle electron cryomicroscopy is an essential tool for\nhigh-resolution 3D reconstruction of proteins and other biological\nmacromolecules. An important challenge in cryo-EM is the reconstruction of\nnon-rigid molecules with parts that move and deform. Traditional reconstruction\nmethods fail in these cases, resulting in smeared reconstructions of the moving\nparts. This poses a major obstacle for structural biologists, who need\nhigh-resolution reconstructions of entire macromolecules, moving parts\nincluded. To address this challenge, we present a new method for the\nreconstruction of macromolecules exhibiting continuous heterogeneity. The\nproposed method uses projection images from multiple viewing directions to\nconstruct a graph Laplacian through which the manifold of three-dimensional\nconformations is analyzed. The 3D molecular structures are then expanded in a\nbasis of Laplacian eigenvectors, using a novel generalized tomographic\nreconstruction algorithm to compute the expansion coefficients. These\ncoefficients, which we name spectral volumes, provide a high-resolution\nvisualization of the molecular dynamics. We provide a theoretical analysis and\nevaluate the method empirically on several simulated data sets.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 22:58:05 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2019 21:00:48 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Moscovich", "Amit", ""], ["Halevi", "Amit", ""], ["And\u00e9n", "Joakim", ""], ["Singer", "Amit", ""]]}, {"id": "1907.01952", "submitter": "Erik \\v{S}trumbelj", "authors": "Jure Dem\\v{s}ar, Grega Repov\\v{s}, Erik \\v{S}trumbelj", "title": "bayes4psy -- an Open Source R Package for Bayesian Statistics in\n  Psychology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.MS stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in psychology generates interesting data sets and unique statistical\nmodelling tasks. However, these tasks, while important, are often very\nspecific, so appropriate statistical models and methods cannot be found in\naccessible Bayesian tools. As a result, the use of Bayesian methods is limited\nto those that have the technical and statistical fundamentals that are required\nfor probabilistic programming. Such knowledge is not part of the typical\npsychology curriculum and is a difficult obstacle for psychology students and\nresearchers to overcome. The goal of the bayes4psy package is to bridge this\ngap and offer a collection of models and methods to be used for data analysis\nthat arises from psychology experiments and as a teaching tool for Bayesian\nstatistics in psychology. The package contains Bayesian t-test and\nbootstrapping and models for analyzing reaction times, success rates, and\ncolors. It also provides all the diagnostic, analytic and visualization tools\nfor the modern Bayesian data analysis workflow.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 14:01:23 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Dem\u0161ar", "Jure", ""], ["Repov\u0161", "Grega", ""], ["\u0160trumbelj", "Erik", ""]]}, {"id": "1907.02048", "submitter": "Nathan Hara", "authors": "N. C. Hara, G. Bou\\'e, J. Laskar, J.B. Delisle, N. Unger", "title": "Bias and robustness of eccentricity estimates from radial velocity data", "comments": "Accepted for publication in MNRAS", "journal-ref": null, "doi": "10.1093/mnras/stz1849", "report-no": null, "categories": "astro-ph.EP astro-ph.IM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eccentricity is a parameter of particular interest as it is an informative\nindicator of the past of planetary systems. It is however not always clear\nwhether the eccentricity fitted on radial velocity data is real or if it is an\nartefact of an inappropriate modelling. In this work, we address this question\nin two steps: we first assume that the model used for inference is correct and\npresent interesting features of classical estimators. Secondly, we study\nwhether the eccentricity estimates are to be trusted when the data contain\nincorrectly modelled signals, such as missed planetary companions, non Gaussian\nnoises, correlated noises with unknown covariance, etc. Our main conclusion is\nthat data analysis via posterior distributions, with a model including a free\nerror term gives reliable results provided two conditions. First, convergence\nof the numerical methods needs to be ascertained. Secondly, the noise power\nspectrum should not have a particularly strong peak at the semi period of the\nplanet of interest. As a consequence, it is difficult to determine if the\nsignal of an apparently eccentric planet might be due to another inner\ncompanion in 2:1 mean motion resonance. We study the use of Bayes factors to\ndisentangle these cases. Finally, we suggest methods to check if there are\nhints of an incorrect model in the residuals. We show on simulated data the\nperformance of our methods and comment on the eccentricities of Proxima b and\n55 Cnc f.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 17:29:44 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Hara", "N. C.", ""], ["Bou\u00e9", "G.", ""], ["Laskar", "J.", ""], ["Delisle", "J. B.", ""], ["Unger", "N.", ""]]}, {"id": "1907.02175", "submitter": "Ali Reza Fotouhi", "authors": "Ali Reza Fotouhi", "title": "Bayesian analysis of extreme values in economic indexes and climate\n  data: Simulation and application", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed modeling of extreme values and random effects is relatively unexplored\ntopic. Computational difficulties in using the maximum likelihood method for\nmixed models and the fact that maximum likelihood method uses available data\nand does not use the prior information motivate us to use Bayesian method. Our\nsimulation studies indicate that random effects modeling produces more reliable\nestimates when heterogeneity is present. The application of the proposed model\nto the climate data and return values of some economic indexes reveals the same\npattern as the simulation results and confirms the usefulness of mixed modeling\nof random effects and extremes. As the nature of climate and economic data are\nmassive and there is always a possibility of missing a considerable part of\ndata, saving the information included in past data is useful. Our simulation\nstudies and applications show the benefit of Bayesian method to save the\ninformation from the past data into the posterior distributions of the\nparameters to be used as informative prior distributions to fit the future\ndata. We show that informative prior distributions obtained from the past data\nhelp to estimate the return level in Block Maxima method and Value-at-Risk and\nExpected Shortfall in Peak Over Threshold method with less bias than using\nuninformative prior distributions.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 01:02:43 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Fotouhi", "Ali Reza", ""]]}, {"id": "1907.02179", "submitter": "Hayden Moffat", "authors": "Hayden Moffat, Markus Hainy, Nikos E. Papanikolaou and Christopher\n  Drovandi", "title": "Sequential Experimental Design for Predator-Prey Functional Response\n  Experiments", "comments": "Main Text: 23 pages, 7 Figures - Supplementary Text: 11 pages, 5\n  Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding functional response within a predator-prey dynamic is a\ncornerstone for many quantitative ecological studies. Over the past 60 years,\nthe methodology for modelling functional response has gradually transitioned\nfrom the classic mechanistic models to more statistically oriented models. To\nobtain inferences on these statistical models, a substantial number of\nexperiments need to be conducted. The obvious disadvantages of collecting this\nvolume of data include cost, time and the sacrificing of animals. Therefore,\noptimally designed experiments are useful as they may reduce the total number\nof experimental runs required to attain the same statistical results. In this\npaper, we develop the first sequential experimental design method for\npredator-prey functional response experiments. To make inferences on the\nparameters in each of the statistical models we consider, we use sequential\nMonte Carlo, which is computationally efficient and facilitates convenient\nestimation of important utility functions. It provides coverage of experimental\ngoals including parameter estimation, model discrimination as well as a\ncombination of these. The results of our simulation study illustrate that for\npredator-prey functional response experiments sequential design outperforms\nstatic design for our experimental goals. R code for implementing the\nmethodology is available via\nhttps://github.com/haydenmoffat/sequential_design_for_predator_prey_experiments.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 01:18:20 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 13:03:22 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Moffat", "Hayden", ""], ["Hainy", "Markus", ""], ["Papanikolaou", "Nikos E.", ""], ["Drovandi", "Christopher", ""]]}, {"id": "1907.02447", "submitter": "Arthur Guillaumin", "authors": "Arthur P. Guillaumin, Adam M. Sykulski, Sofia C. Olhede, Frederik J.\n  Simons", "title": "The Debiased Spatial Whittle Likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a computationally and statistically efficient method for\nestimating the parameters of a stochastic Gaussian model observed on a regular\nspatial grid in any number of dimensions. Our proposed method, which we call\nthe debiased spatial Whittle likelihood, makes important corrections to the\nwell-known Whittle likelihood to account for large sources of bias caused by\nboundary effects and aliasing. We generalise the approach to flexibly allow for\nsignificant volumes of missing data, for the usage of irregular sampling\nschemes including those with lower-dimensional substructure, and for irregular\nsampling boundaries. We build a theoretical framework under relatively weak\nassumptions which ensures consistency and asymptotic normality in numerous\npractical settings. We provide detailed implementation guidelines which ensure\nthe estimation procedure can still be conducted in $\\mathcal{O}(n\\log n)$\noperations, where $n$ is the number of points of the encapsulating rectangular\ngrid, thus keeping the computational scalability of Fourier and Whittle-based\nmethods for large data sets. We validate our procedure over a range of\nsimulated and real world settings, and compare with state-of-the-art\nalternatives, demonstrating the enduring significant practical appeal of\nFourier-based methods, provided they are corrected by the constructive\nprocedures developed in this paper.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 15:11:36 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2019 14:33:14 GMT"}, {"version": "v3", "created": "Thu, 13 Aug 2020 21:17:15 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Guillaumin", "Arthur P.", ""], ["Sykulski", "Adam M.", ""], ["Olhede", "Sofia C.", ""], ["Simons", "Frederik J.", ""]]}, {"id": "1907.02493", "submitter": "Tommaso Rigon", "authors": "Tommaso Rigon", "title": "An enriched mixture model for functional clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increasingly rich literature about Bayesian nonparametric models\nfor clustering functional observations. However, most of the recent proposals\nrely on infinite-dimensional characterizations that might lead to overly\ncomplex cluster solutions. In addition, while prior knowledge about the\nfunctional shapes is typically available, its practical exploitation might be a\ndifficult modeling task. Motivated by an application in e-commerce, we propose\na novel enriched Dirichlet mixture model for functional data. Our proposal\naccommodates the incorporation of functional constraints while bounding the\nmodel complexity. To clarify the underlying partition mechanism, we\ncharacterize the prior process through a P\\'olya urn scheme. These features\nlead to a very interpretable clustering method compared to available\ntechniques. To overcome computational bottlenecks, we employ a variational\nBayes approximation for tractable posterior inference.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 17:09:04 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Rigon", "Tommaso", ""]]}, {"id": "1907.02569", "submitter": "George Leckie", "authors": "George Leckie", "title": "Cross-classified multilevel models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-classified multilevel modelling is an extension of standard multilevel\nmodelling for non-hierarchical data that have cross-classified structures.\nTraditional multilevel models involve hierarchical data structures whereby\nlower level units such as students are nested within higher level units such as\nschools and where these higher level units may in turn be nested within further\ngroupings or clusters such as school districts, regions, and countries. With\nhierarchical data structures, there is an exact nesting of each lower level\nunit in one and only one higher level unit. For example, each student attends\none school, each school is located within one school district, and so on.\nHowever, social reality is more complicated than this, and so social and\nbehavioural data often do not follow pure or strict hierarchies. Two types of\nnon-hierarchical data structures which often appear in practice are\ncross-classified and multiple membership structures. In this article, we\ndescribe cross-classified data structures and cross-classified hierarchical\nlinear modelling which can be used to analyse them.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 19:45:08 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Leckie", "George", ""]]}, {"id": "1907.02666", "submitter": "Huiling Yuan", "authors": "Huiling Yuan, Yong Zhou, Zhiyuan Zhang, Xiangyu Cui", "title": "Forecasting security's volatility using low-frequency historical data,\n  high-frequency historical data and option-implied volatility", "comments": "37 pages,9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-frequency historical data, high-frequency historical data and option data\nare three major sources, which can be used to forecast the underlying\nsecurity's volatility. In this paper, we propose two econometric models, which\nintegrate three information sources. In GARCH-It\\^{o}-OI model, we assume that\nthe option-implied volatility can influence the security's future volatility,\nand the option-implied volatility is treated as an observable exogenous\nvariable. In GARCH-It\\^{o}-IV model, we assume that the option-implied\nvolatility can not influence the security's volatility directly, and the\nrelationship between the option-implied volatility and the security's\nvolatility is constructed to extract useful information of the underlying\nsecurity. After providing the quasi-maximum likelihood estimators for the\nparameters and establishing their asymptotic properties, we also conduct a\nseries of simulation analysis and empirical analysis to compare the proposed\nmodels with other popular models in the literature. We find that when the\nsampling interval of the high-frequency data is 5 minutes, the GARCH-It\\^{o}-OI\nmodel and GARCH-It\\^{o}-IV model has better forecasting performance than other\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 03:44:36 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Yuan", "Huiling", ""], ["Zhou", "Yong", ""], ["Zhang", "Zhiyuan", ""], ["Cui", "Xiangyu", ""]]}, {"id": "1907.02706", "submitter": "Unn Dahlen", "authors": "Unn Dahlen and Johan Linstr\\\"om and Marko Scholze", "title": "Spatio-Temporal Reconstructions of Global CO2-Fluxes using Gaussian\n  Markov Random Fields", "comments": "Article: 37 pages, 11 figures, including references and appendix.\n  Supplemental material: 11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Atmospheric inverse modelling is a method for reconstructing historical\nfluxes of green-house gas between land and atmosphere, using observed\natmospheric concentrations and an atmospheric tracer transport model. The small\nnumber of observed atmospheric concentrations in relation to the number of\nunknown flux components makes the inverse problem ill-conditioned, and\nassumptions on the fluxes are needed to constrain the solution. A common\npractise is to model the fluxes using latent Gaussian fields with a mean\nstructure based on estimated fluxes from combinations of process modelling\n(natural fluxes) and statistical bookkeeping (anthropogenic emissions). Here,\nwe reconstruct global \\CO flux fields by modelling fluxes using Gaussian Markov\nRandom Fields (GMRF), resulting in a flexible and computational beneficial\nmodel with a Mat\\'ern-like spatial covariance, and a temporal covariance\ndefined through an auto-regressive model with seasonal dependence.\n  In contrast to previous inversions, the flux is defined on a spatially\ncontinuous domain, and the traditionally discrete flux representation is\nreplaced by integrated fluxes at the resolution specified by the transport\nmodel. This formulation removes aggregation errors in the flux covariance, due\nto the traditional representation of area integrals by fluxes at discrete\npoints, and provides a model closer resembling real-life space-time continuous\nfluxes.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 07:31:30 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Dahlen", "Unn", ""], ["Linstr\u00f6m", "Johan", ""], ["Scholze", "Marko", ""]]}, {"id": "1907.02764", "submitter": "Kellyn Arnold", "authors": "Peter W. G. Tennant, Kellyn F. Arnold, George T. H. Ellison, Mark S.\n  Gilthorpe", "title": "Analyses of 'change scores' do not estimate causal effects in\n  observational data", "comments": "15 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: In longitudinal data, it is common to create 'change scores' by\nsubtracting measurements taken at baseline from those taken at follow-up, and\nthen to analyse the resulting 'change' as the outcome variable. In\nobservational data, this approach can produce misleading causal effect\nestimates. The present article uses directed acyclic graphs (DAGs) and simple\nsimulations to provide an accessible explanation of why change scores do not\nestimate causal effects in observational data.\n  Methods: Data were simulated to match three general scenarios where the\nvariable representing measurements of the outcome at baseline was a 1)\ncompeting exposure, 2) confounder, or 3) mediator for the total causal effect\nof the exposure on the variable representing measurements of the outcome at\nfollow-up. Regression coefficients were compared between change-score analyses\nand DAG-informed analyses.\n  Results: Change-score analyses do not provide meaningful causal effect\nestimates unless the variable representing measurements of the outcome at\nbaseline is a competing exposure, as in a randomised experiment. Where such\nvariables (i.e. baseline measurements of the outcome) are confounders or\nmediators, the conclusions drawn from analyses of change scores diverge\n(potentially substantially) from those of DAG-informed analyses.\n  Conclusions: Future observational studies that seek causal effect estimates\nshould avoid analysing change scores and adopt alternative analytical\nstrategies.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 10:42:38 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Tennant", "Peter W. G.", ""], ["Arnold", "Kellyn F.", ""], ["Ellison", "George T. H.", ""], ["Gilthorpe", "Mark S.", ""]]}, {"id": "1907.02829", "submitter": "Adam Brentnall", "authors": "Adam R Brentnall, Jack Cuzick", "title": "Risk models for breast cancer and their validation", "comments": null, "journal-ref": "Statist. Sci., Volume 35, Number 1 (2020), 14-30", "doi": "10.1214/19-STS729", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Strategies to prevent cancer and diagnose it early when it is most treatable\nare needed to reduce the public health burden from rising disease incidence.\nRisk assessment is playing an increasingly important role in targeting\nindividuals in need of such interventions. For breast cancer many individual\nrisk factors have been well understood for a long time, but the development of\na fully comprehensive risk model has not been straightforward, in part because\nthere have been limited data where joint effects of an extensive set of risk\nfactors may be estimated with precision. In this article we first review the\napproach taken to develop the IBIS (Tyrer-Cuzick) model, and describe recent\nupdates. We then review and develop methods to assess calibration of models\nsuch as this one, where the risk of disease allowing for competing mortality\nover a long follow-up time or lifetime is estimated. The breast cancer risk\nmodel model and calibration assessment methods are demonstrated using a cohort\nof 132 139 women attending mammography screening in Washington, USA.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 13:43:00 GMT"}, {"version": "v2", "created": "Tue, 9 Jul 2019 16:29:19 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Brentnall", "Adam R", ""], ["Cuzick", "Jack", ""]]}, {"id": "1907.02936", "submitter": "Vasiliki Liakoni", "authors": "Vasiliki Liakoni, Alireza Modirshanechi, Wulfram Gerstner, Johanni\n  Brea", "title": "Learning in Volatile Environments with the Bayes Factor Surprise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surprise-based learning allows agents to rapidly adapt to non-stationary\nstochastic environments characterized by sudden changes. We show that exact\nBayesian inference in a hierarchical model gives rise to a surprise-modulated\ntrade-off between forgetting old observations and integrating them with the new\nones. The modulation depends on a probability ratio, which we call \"Bayes\nFactor Surprise\", that tests the prior belief against the current belief. We\ndemonstrate that in several existing approximate algorithms the Bayes Factor\nSurprise modulates the rate of adaptation to new observations. We derive three\nnovel surprised-based algorithms, one in the family of particle filters, one in\nthe family of variational learning, and the other in the family of message\npassing, that have constant scaling in observation sequence length and\nparticularly simple update dynamics for any distribution in the exponential\nfamily. Empirical results show that these surprise-based algorithms estimate\nparameters better than alternative approximate approaches and reach levels of\nperformance comparable to computationally more expensive algorithms. The Bayes\nFactor Surprise is related to but different from Shannon Surprise. In two\nhypothetical experiments, we make testable predictions for physiological\nindicators that dissociate the Bayes Factor Surprise from Shannon Surprise. The\ntheoretical insight of casting various approaches as surprise-based learning,\nas well as the proposed online algorithms, may be applied to the analysis of\nanimal and human behavior, and to reinforcement learning in non-stationary\nenvironments.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 17:07:18 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 13:50:28 GMT"}, {"version": "v3", "created": "Wed, 23 Sep 2020 19:55:12 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Liakoni", "Vasiliki", ""], ["Modirshanechi", "Alireza", ""], ["Gerstner", "Wulfram", ""], ["Brea", "Johanni", ""]]}, {"id": "1907.03119", "submitter": "Pavlos Kolias", "authors": "Pavlos Kolias and Alexandra Papadopoulou", "title": "Investigating some attributes of periodicity in DNA sequences via\n  semi-Markov modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DNA segments and sequences have been studied thoroughly during the past\ndecades. One of the main problems in computational biology is the\nidentification of exon-intron structures inside genes using mathematical\ntechniques. Previous studies have used different methods, such as Fourier\nanalysis and hidden-Markov models, in order to be able to predict which parts\nof a gene correspond to a protein encoding area. In this paper, a semi-Markov\nmodel is applied to 3-base periodic sequences, which characterize the\nprotein-coding regions of the gene. Analytic forms of the related probabilities\nand the corresponding indexes are provided, which yield a description of the\nunderlying periodic pattern. Last, the previous theoretical results are\nillustrated with DNA sequences of synthetic and real data.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 11:58:51 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Kolias", "Pavlos", ""], ["Papadopoulou", "Alexandra", ""]]}, {"id": "1907.03153", "submitter": "Cl\\'emence Karmann Mrs", "authors": "Anne G\\'egout-Petit, Aur\\'elie Gueudin-Muller, Cl\\'emence Karmann", "title": "The revisited knockoffs method for variable selection in L1-penalised\n  regressions", "comments": "arXiv admin note: text overlap with arXiv:1805.10097", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of variable selection in regression models. In\nparticular, we are interested in selecting explanatory covariates linked with\nthe response variable and we want to determine which covariates are relevant,\nthat is which covariates are involved in the model. In this framework, we deal\nwith L1-penalised regression models. To handle the choice of the penalty\nparameter to perform variable selection, we develop a new method based on the\nknockoffs idea. This revisited knockoffs method is general, suitable for a wide\nrange of regressions with various types of response variables. Besides, it also\nworks when the number of observations is smaller than the number of covariates\nand gives an order of importance of the covariates. Finally, we provide many\nexperimental results to corroborate our method and compare it with other\nvariable selection methods.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 16:44:07 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["G\u00e9gout-Petit", "Anne", ""], ["Gueudin-Muller", "Aur\u00e9lie", ""], ["Karmann", "Cl\u00e9mence", ""]]}, {"id": "1907.03186", "submitter": "Wei Shi", "authors": "Junxian Geng, Wei Shi, Guanyu Hu", "title": "Bayesian Nonparametric Nonhomogeneous Poisson Process with Applications\n  to USGS Earthquake Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Intensity estimation is a common problem in statistical analysis of spatial\npoint pattern data. This paper proposes a nonparametric Bayesian method for\nestimating the spatial point process intensity based on mixture of finite\nmixture (MFM) model. MFM approach leads to a consistent estimate of the\nintensity of spatial point patterns in different areas while considering\nheterogeneity. An efficient Markov chain Monte Carlo (MCMC) algorithm is\nproposed for our method. Extensive simulation studies are carried out to\nexamine empirical performance of the proposed method. The usage of our proposed\nmethod is further illustrated with the analysis of the Earthquake Hazards\nProgram of United States Geological Survey (USGS) earthquake data.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 21:05:12 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Geng", "Junxian", ""], ["Shi", "Wei", ""], ["Hu", "Guanyu", ""]]}, {"id": "1907.03206", "submitter": "Ben Moews", "authors": "Ben Moews, Jaime R. Argueta Jr., Antonia Gieschen", "title": "Filaments of crime: Informing policing via thresholded ridge estimation", "comments": "17 pages, 3 figures", "journal-ref": null, "doi": "10.1016/j.dss.2021.113518", "report-no": null, "categories": "stat.AP cs.CY stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objectives: We introduce a new method for reducing crime in hot spots and\nacross cities through ridge estimation. In doing so, our goal is to explore the\napplication of density ridges to hot spots and patrol optimization, and to\ncontribute to the policing literature in police patrolling and crime reduction\nstrategies.\n  Methods: We make use of the subspace-constrained mean shift algorithm, a\nrecently introduced approach for ridge estimation further developed in\ncosmology, which we modify and extend for geospatial datasets and hot spot\nanalysis. Our experiments extract density ridges of Part I crime incidents from\nthe City of Chicago during the year 2018 and early 2019 to demonstrate the\napplication to current data.\n  Results: Our results demonstrate nonlinear mode-following ridges in agreement\nwith broader kernel density estimates. Using early 2019 incidents with\npredictive ridges extracted from 2018 data, we create multi-run confidence\nintervals and show that our patrol templates cover around 94% of incidents for\n0.1-mile envelopes around ridges, quickly rising to near-complete coverage. We\nalso develop and provide researchers, as well as practitioners, with a\nuser-friendly and open-source software for fast geospatial density ridge\nestimation.\n  Conclusions: We show that ridges following crime report densities can be used\nto enhance patrolling capabilities. Our empirical tests show the stability of\nridges based on past data, offering an accessible way of identifying routes\nwithin hot spots instead of patrolling epicenters. We suggest further research\ninto the application and efficacy of density ridges for patrolling.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 23:59:22 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Moews", "Ben", ""], ["Argueta", "Jaime R.", "Jr."], ["Gieschen", "Antonia", ""]]}, {"id": "1907.03211", "submitter": "Andrew Song", "authors": "Bahareh Tolooshams, Andrew H. Song, Simona Temereanca, Demba Ba", "title": "Convolutional dictionary learning based auto-encoders for natural\n  exponential-family distributions", "comments": null, "journal-ref": "International Conference on Machine Learning (ICML) 2020", "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a class of auto-encoder neural networks tailored to data from\nthe natural exponential family (e.g., count data). The architectures are\ninspired by the problem of learning the filters in a convolutional generative\nmodel with sparsity constraints, often referred to as convolutional dictionary\nlearning (CDL). Our work is the first to combine ideas from convolutional\ngenerative models and deep learning for data that are naturally modeled with a\nnon-Gaussian distribution (e.g., binomial and Poisson). This perspective\nprovides us with a scalable and flexible framework that can be re-purposed for\na wide range of tasks and assumptions on the generative model. Specifically,\nthe iterative optimization procedure for solving CDL, an unsupervised task, is\nmapped to an unfolded and constrained neural network, with iterative\nadjustments to the inputs to account for the generative distribution. We also\nshow that the framework can easily be extended for discriminative training,\nappropriate for a supervised task. We demonstrate 1) that fitting the\ngenerative model to learn, in an unsupervised fashion, the latent stimulus that\nunderlies neural spiking data leads to better goodness-of-fit compared to other\nbaselines, 2) competitive performance compared to state-of-the-art algorithms\nfor supervised Poisson image denoising, with significantly fewer parameters,\nand 3) gradient dynamics of shallow binomial auto-encoder.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2019 01:45:42 GMT"}, {"version": "v2", "created": "Thu, 24 Oct 2019 23:36:18 GMT"}, {"version": "v3", "created": "Tue, 11 Feb 2020 11:55:04 GMT"}, {"version": "v4", "created": "Sun, 28 Jun 2020 23:35:04 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Tolooshams", "Bahareh", ""], ["Song", "Andrew H.", ""], ["Temereanca", "Simona", ""], ["Ba", "Demba", ""]]}, {"id": "1907.03518", "submitter": "Pablo Dorta-Gonzalez", "authors": "Sara M. Gonz\\'alez-Betancor, Pablo Dorta-Gonz\\'alez", "title": "Publication modalities 'article in press' and 'open access' in relation\n  to journal average citation", "comments": "23 pages, 2 figures, 6 tables", "journal-ref": null, "doi": "10.1007/s11192-019-03156-2", "report-no": null, "categories": "cs.DL stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There has been a generalization in the use of two publication practices by\nscientific journals during the past decade: 1. 'article in press' or early\nview, which allows access to the accepted paper before its formal publication\nin an issue; 2. 'open access', which allows readers to obtain it freely and\nfree of charge. This paper studies the influence of both publication modalities\non the average impact of the journal and its evolution over time. It tries to\nidentify the separate effect of access on citation into two major parts: early\nview and selection effect, managing to provide some evidence of the positive\neffect of both. Scopus is used as the database and CiteScore as the measure of\njournal impact. The prevalence of both publication modalities is quantified.\nDifferences in the average impact factor of group of journals, according to\ntheir publication modalities, are tested. The evolution over time of the\ncitation influence, from 2011 to 2016, is also analysed. Finally, a linear\nregression to explain the correlation of these publication practices with the\nCiteScore in 2016, in a ceteris paribus context, is estimated. Our main\nfindings show evidence of a positive correlation between average journal impact\nand advancing the publication of accepted articles, moreover this correlation\nincreases over time. The open access modality, in a ceteris paribus context,\nalso correlates positively with average journal impact.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 11:19:53 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Gonz\u00e1lez-Betancor", "Sara M.", ""], ["Dorta-Gonz\u00e1lez", "Pablo", ""]]}, {"id": "1907.03555", "submitter": "Linda Mhalla", "authors": "Linda Mhalla, Val\\'erie Chavez-Demoulin, Debbie J. Dupuis", "title": "Causal mechanism of extreme river discharges in the upper Danube basin\n  network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extreme hydrological events in the Danube river basin may severely impact\nhuman populations, aquatic organisms, and economic activity. One often\ncharacterizes the joint structure of the extreme events using the theory of\nmultivariate and spatial extremes and its asymptotically justified models.\nThere is interest however in cascading extreme events and whether one event\ncauses another. In this paper, we argue that an improved understanding of the\nmechanism underlying severe events is achieved by combining extreme value\nmodelling and causal discovery. We construct a causal inference method relying\non the notion of the Kolmogorov complexity of extreme conditional quantiles.\nTail quantities are derived using multivariate extreme value models and\ncausal-induced asymmetries in the data are explored through the minimum\ndescription length principle. Our CausEV, for Causality for Extreme Values,\napproach uncovers causal relations between summer extreme river discharges in\nthe upper Danube basin and finds significant causal links between the Danube\nand its Alpine tributary Lech.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 12:26:34 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 12:49:26 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Mhalla", "Linda", ""], ["Chavez-Demoulin", "Val\u00e9rie", ""], ["Dupuis", "Debbie J.", ""]]}, {"id": "1907.03807", "submitter": "Fang Xie", "authors": "Fang Xie and Johannes Lederer", "title": "Aggregating Knockoffs for False Discovery Rate Control with an\n  Application to Gut Microbiome Data", "comments": null, "journal-ref": "Entropy 23(2021) 230", "doi": "10.3390/e23020230", "report-no": null, "categories": "stat.ME q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent discoveries suggest that our gut microbiome plays an important role in\nour health and wellbeing. However, the gut microbiome data are intricate; for\nexample, the microbial diversity in the gut makes the data high-dimensional.\nWhile there are dedicated high-dimensional methods, such as the lasso\nestimator, they always come with the risk of false discoveries. Knockoffs are a\nrecent approach to control the number of false discoveries. In this paper, we\nshow that knockoffs can be aggregated to increase power while retaining sharp\ncontrol over the false discoveries. We support our method both in theory and\nsimulations, and we show that it can lead to new discoveries on microbiome data\nfrom the American Gut Project. In particular, our results indicate that several\nphyla that have been overlooked so far are associated with obesity.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 18:48:57 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 09:36:26 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Xie", "Fang", ""], ["Lederer", "Johannes", ""]]}, {"id": "1907.03808", "submitter": "Lu Yu", "authors": "Lu Yu and Tobias Kaufmann and Johannes Lederer", "title": "False Discovery Rates in Biological Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing availability of data has generated unprecedented prospects for\nnetwork analyses in many biological fields, such as neuroscience (e.g., brain\nnetworks), genomics (e.g., gene-gene interaction networks), and ecology (e.g.,\nspecies interaction networks). A powerful statistical framework for estimating\nsuch networks is Gaussian graphical models, but standard estimators for the\ncorresponding graphs are prone to large numbers of false discoveries. In this\npaper, we introduce a novel graph estimator based on knockoffs that imitate the\npartial correlation structures of unconnected nodes. We show that this new\nestimator guarantees accurate control of the false discovery rate in theory,\nsimulations, and biological applications, and we provide easy-to-use R code.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 18:49:09 GMT"}, {"version": "v2", "created": "Sun, 24 Nov 2019 14:44:43 GMT"}, {"version": "v3", "created": "Thu, 4 Feb 2021 15:40:29 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Yu", "Lu", ""], ["Kaufmann", "Tobias", ""], ["Lederer", "Johannes", ""]]}, {"id": "1907.03814", "submitter": "Zhepu Xu", "authors": "Zhepu Xu and Qun Yang", "title": "Road Maintenance Operation Start Time Optimization Based on Real-time\n  Traffic Map Data", "comments": "14 pages, in Chinese, 12 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimizing the maintenance operation start time can greatly reduce the delays\ncaused by the maintenance operations. A real-time traffic status data\nacquisition method based on real-time traffic map was first proposed, and then\na method that can convert real-time traffic status into real-time traffic\nvolume was put forward. Based on this real-time traffic volume data and the\nclassic delay calculation method based on queuing theory, the delays caused by\nmaintenance operations at different start time can be calculated and compared,\nand therefore the optimal maintenance operation start time can be obtained. The\nfeasibility of the real-time traffic status data to real-time traffic volume\ndata conversion method and the feasibility of optimizing the maintenance\noperation start time based on real-time traffic map data are verified by actual\ncases.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 18:59:23 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Xu", "Zhepu", ""], ["Yang", "Qun", ""]]}, {"id": "1907.03827", "submitter": "An Yan", "authors": "An Yan, Bill Howe", "title": "FairST: Equitable Spatial and Temporal Demand Prediction for New\n  Mobility Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Emerging transportation modes, including car-sharing, bike-sharing, and\nride-hailing, are transforming urban mobility but have been shown to reinforce\nsocioeconomic inequities. Spatiotemporal demand prediction models for these new\nmobility regimes must therefore consider fairness as a first-class design\nrequirement. We present FairST, a fairness-aware model for predicting demand\nfor new mobility systems. Our approach utilizes 1D, 2D and 3D convolutions to\nintegrate various urban features and learn the spatial-temporal dynamics of a\nmobility system, but we include fairness metrics as a form of regularization to\nmake the predictions more equitable across demographic groups. We propose two\nnovel spatiotemporal fairness metrics, a region-based fairness gap (RFG) and an\nindividual-based fairness gap (IFG). Both quantify equity in a spatiotemporal\ncontext, but vary by whether demographics are labeled at the region level (RFG)\nor whether population distribution information is available (IFG). Experimental\nresults on real bike share and ride share datasets demonstrate the\neffectiveness of the proposed model: FairST not only reduces the fairness gap\nby more than 80%, but can surprisingly achieve better accuracy than\nstate-of-the-art yet fairness-oblivious methods including LSTMs, ConvLSTMs, and\n3D CNN.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 19:33:16 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Yan", "An", ""], ["Howe", "Bill", ""]]}, {"id": "1907.03929", "submitter": "Mohsen Joneidi", "authors": "Mohsen Joneidi", "title": "Functional Brain Networks Discovery Using Dictionary Learning with\n  Correlated Sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP eess.IV q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of data from functional magnetic resonance imaging (fMRI) results in\nconstructing functional brain networks. Principal component analysis (PCA) and\nindependent component analysis (ICA) are widely used to generate functional\nbrain networks. Moreover, dictionary learning and sparse representation provide\nsome latent patterns that rules brain activities and they can be interpreted as\nbrain networks. However, these methods lack modeling dependencies of the\ndiscovered networks. In this study an alternative to these conventional methods\nis presented in which dependencies of the networks are considered via\ncorrelated sparsity patterns. We formulate this challenge as a new dictionary\nlearning problem and propose two approaches to solve the problem effectively.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 01:18:11 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2019 21:29:05 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Joneidi", "Mohsen", ""]]}, {"id": "1907.03933", "submitter": "Zicheng Liu", "authors": "Zicheng Liu, Dominique Lesselier, Bruno Sudret, Joe Wiart", "title": "Surrogate modeling of indoor down-link human exposure based on sparse\n  polynomial chaos expansion", "comments": null, "journal-ref": null, "doi": "10.1615/Int.J.UncertaintyQuantification.2020031452", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human exposure induced by wireless communication systems increasingly draws\nthe public attention. Here, an indoor down-link scenario is concerned and the\nexposure level is statistically analyzed. The electromagnetic field (EMF)\nemitted by a WiFi box is measured and electromagnetic dosimetry features are\nevaluated from the whole-body specific absorption rate as computed with a\nFinite-Difference Time-Domain (a.k.a. FDTD) code. Due to computational cost, a\nstatistical analysis is performed based on a surrogate model, which is\nconstructed by means of so-called sparse polynomial chaos expansion (PCE),\nwhere the inner cross validation (ICV) is used to select the optimal\nhyperparameters during the model construction and assess the model performance.\nHowever, the ICV error is optimized and the model assessment tends to be overly\noptimistic with small data sets. The method of cross-model validation is used\nand outer cross validation is carried out for the model assessment. The effects\nof the data preprocessing are investigated as well. Based on the surrogate\nmodel, the global sensitivity of the exposure to input parameters is analyzed\nfrom Sobol' indices.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 13:29:57 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 16:00:42 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Liu", "Zicheng", ""], ["Lesselier", "Dominique", ""], ["Sudret", "Bruno", ""], ["Wiart", "Joe", ""]]}, {"id": "1907.04148", "submitter": "George Leckie", "authors": "George Leckie", "title": "Multiple membership multilevel models", "comments": "arXiv admin note: substantial text overlap with arXiv:1907.02569", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple membership multilevel models are an extension of standard multilevel\nmodels for non-hierarchical data that have multiple membership structures.\nTraditional multilevel models involve hierarchical data structures whereby\nlower-level units such as students are nested within higher-level units such as\nschools and where these higher-level units may in turn be nested within further\ngroupings or clusters such as school districts, regions, and countries. With\nhierarchical data structures, there is an exact nesting of each lower-level\nunit in one and only one higher-level unit. For example, each student attends\none school, each school is located within one school district, and so on.\nHowever, social reality is more complicated than this, and so social and\nbehavioural data often do not follow pure or strict hierarchies. Two types of\nnon-hierarchical data structures which often appear in practice are\ncross-classified and multiple membership structures. In this article, we\ndescribe multiple membership data structures and multiple membership models\nwhich can be used to analyse them.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 19:46:34 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Leckie", "George", ""]]}, {"id": "1907.04185", "submitter": "Sebastian Weber", "authors": "Beat Neuenschwander, Sebastian Weber, Heinz Schmidli, Anthony O'Hagan", "title": "Predictively Consistent Prior Effective Sample Sizes", "comments": "19 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining the sample size of an experiment can be challenging, even more so\nwhen incorporating external information via a prior distribution. Such\ninformation is increasingly used to reduce the size of the control group in\nrandomized clinical trials. Knowing the amount of prior information, expressed\nas an equivalent prior effective sample size (ESS), clearly facilitates trial\ndesigns. Various methods to obtain a prior's ESS have been proposed recently.\nThey have been justified by the fact that they give the standard ESS for\none-parameter exponential families. However, despite being based on similar\ninformation-based metrics, they may lead to surprisingly different ESS for\nnon-conjugate settings, which complicates many designs with prior information.\nWe show that current methods fail a basic predictive consistency criterion,\nwhich requires the expected posterior-predictive ESS for a sample of size $N$\nto be the sum of the prior ESS and $N$. The expected local-information-ratio\nESS is introduced and shown to be predictively consistent. It corrects the ESS\nof current methods, as shown for normally distributed data with a heavy-tailed\nStudent-t prior and exponential data with a generalized Gamma prior. Finally,\ntwo applications are discussed: the prior ESS for the control group derived\nfrom historical data, and the posterior ESS for hierarchical subgroup analyses.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 14:26:48 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Neuenschwander", "Beat", ""], ["Weber", "Sebastian", ""], ["Schmidli", "Heinz", ""], ["O'Hagan", "Anthony", ""]]}, {"id": "1907.04461", "submitter": "Przemyslaw Biecek", "authors": "Przemyslaw Biecek", "title": "Model Development Process", "comments": "6 pages, process definition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive modeling has an increasing number of applications in various\nfields. High demand for predictive models drives creation of tools that\nautomate and support work of data scientist on the model development. To better\nunderstand what can be automated we need first a description of the model\nlife-cycle. In this paper we propose a generic Model Development Process (MDP).\nThis process is inspired by Rational Unified Process (RUP) which was designed\nfor software development. There are other approached to process description,\nlike CRISP DM or ASUM DM, in this paper we discuss similarities and differences\nbetween these methodologies. We believe that the proposed open standard for\nmodel development will facilitate creation of tools for automation of model\ntraining, testing and maintaining.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 23:41:29 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Biecek", "Przemyslaw", ""]]}, {"id": "1907.04763", "submitter": "Raphael Huser", "authors": "\\'Arni V. Johannesson, Stefan Siegert, Rapha\\\"el Huser, Haakon Bakka\n  and Birgir Hrafnkelsson", "title": "Approximate Bayesian inference for analysis of spatio-temporal flood\n  frequency data", "comments": "33 pages, 12 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extreme floods cause casualties, and widespread damage to property and vital\ncivil infrastructure. We here propose a Bayesian approach for predicting\nextreme floods using the generalized extreme-value (GEV) distribution within\ngauged and ungauged catchments. A major methodological challenge is to find a\nsuitable parametrization for the GEV distribution when covariates or latent\nspatial effects are involved. Other challenges involve balancing model\ncomplexity and parsimony using an appropriate model selection procedure, and\nmaking inference using a reliable and computationally efficient approach. Our\napproach relies on a latent Gaussian modeling framework with a novel\nmultivariate link function designed to separate the interpretation of the\nparameters at the latent level and to avoid unreasonable estimates of the shape\nand time trend parameters. Structured additive regression models are proposed\nfor the four parameters at the latent level. For computational efficiency with\nlarge datasets and richly parametrized models, we exploit an accurate and fast\napproximate Bayesian inference approach. We applied our proposed methodology to\nannual peak river flow data from 554 catchments across the United Kingdom (UK).\nOur model performed well in terms of flood predictions for both gauged and\nungauged catchments. The results show that the spatial model components for the\ntransformed location and scale parameters, and the time trend, are all\nimportant. Posterior estimates of the time trend parameters correspond to an\naverage increase of about $1.5\\%$ per decade and reveal a spatial structure\nacross the UK. To estimate return levels for spatial aggregates, we further\ndevelop a novel copula-based post-processing approach of posterior predictive\nsamples, in order to mitigate the effect of the conditional independence\nassumption at the data level, and we show that our approach provides accurate\nresults.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 14:48:49 GMT"}, {"version": "v2", "created": "Sun, 1 Mar 2020 16:23:27 GMT"}, {"version": "v3", "created": "Tue, 6 Apr 2021 12:50:32 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Johannesson", "\u00c1rni V.", ""], ["Siegert", "Stefan", ""], ["Huser", "Rapha\u00ebl", ""], ["Bakka", "Haakon", ""], ["Hrafnkelsson", "Birgir", ""]]}, {"id": "1907.04838", "submitter": "Adrian Dobra", "authors": "Adrian Dobra, Katherine Buhikire and Joachim G. Voss", "title": "Identifying mediating variables with graphical models: an application to\n  the study of causal pathways in people living with HIV", "comments": "17 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We empirically demonstrate that graphical models can be a valuable tool in\nthe identification of mediating variables in causal pathways. We make use of\ngraphical models to elucidate the causal pathway through which the treatment\ninfluences the levels of fatigue and weakness in people living with HIV (PLHIV)\nbased on a secondary analysis of a categorical dataset collected in a\nbehavioral clinical trial: is weakness a mediator for the treatment and\nfatigue, or is fatigue a mediator for the treatment and weakness? Causal\nmediation analysis could not offer any definite answers to these questions.\\\\\nKEYWORDS: Contingency tables; graphical models; loglinear models; HIV;\nmediation\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 17:36:33 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Dobra", "Adrian", ""], ["Buhikire", "Katherine", ""], ["Voss", "Joachim G.", ""]]}, {"id": "1907.04842", "submitter": "Deborshee Sen", "authors": "Andres F. Barrientos, Deborshee Sen, Garritt L Page, and David B\n  Dunson", "title": "Bayesian inferences on uncertain ranks and orderings: Application to\n  ranking players and lineups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common to be interested in rankings or order relationships among\nentities. In complex settings where one does not directly measure a univariate\nstatistic upon which to base ranks, such inferences typically rely on\nstatistical models having entity-specific parameters. These can be treated as\nrandom effects in hierarchical models characterizing variation among the\nentities. In this paper, we are particularly interested in the problem of\nranking basketball players in terms of their contribution to team performance.\nUsing data from the United States National Basketball Association (NBA), we\nfind that many players have similar latent ability levels, making any single\nestimated ranking highly misleading. The current literature fails to provide\nsummaries of order relationships that adequately account for uncertainty.\nMotivated by this, we propose a Bayesian strategy for characterizing\nuncertainty in inferences on order relationships among players and lineups. Our\napproach adapts to scenarios in which uncertainty in ordering is high by\nproducing more conservative results that improve interpretability. This is\nachieved through a reward function within a decision theoretic framework. We\napply our approach to data from the 2009-10 NBA season.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 17:41:55 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 04:06:46 GMT"}, {"version": "v3", "created": "Tue, 6 Apr 2021 01:52:54 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Barrientos", "Andres F.", ""], ["Sen", "Deborshee", ""], ["Page", "Garritt L", ""], ["Dunson", "David B", ""]]}, {"id": "1907.04911", "submitter": "Michaela Mila", "authors": "Michaela Hardt, Alvin Rajkomar, Gerardo Flores, Andrew Dai, Michael\n  Howell, Greg Corrado, Claire Cui and Moritz Hardt", "title": "Explaining an increase in predicted risk for clinical alerts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much work aims to explain a model's prediction on a static input. We consider\nexplanations in a temporal setting where a stateful dynamical model produces a\nsequence of risk estimates given an input at each time step. When the estimated\nrisk increases, the goal of the explanation is to attribute the increase to a\nfew relevant inputs from the past. While our formal setup and techniques are\ngeneral, we carry out an in-depth case study in a clinical setting. The goal\nhere is to alert a clinician when a patient's risk of deterioration rises. The\nclinician then has to decide whether to intervene and adjust the treatment.\nGiven a potentially long sequence of new events since she last saw the patient,\na concise explanation helps her to quickly triage the alert. We develop methods\nto lift static attribution techniques to the dynamical setting, where we\nidentify and address challenges specific to dynamics. We then experimentally\nassess the utility of different explanations of clinical alerts through expert\nevaluation.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 20:26:43 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Hardt", "Michaela", ""], ["Rajkomar", "Alvin", ""], ["Flores", "Gerardo", ""], ["Dai", "Andrew", ""], ["Howell", "Michael", ""], ["Corrado", "Greg", ""], ["Cui", "Claire", ""], ["Hardt", "Moritz", ""]]}, {"id": "1907.04913", "submitter": "Amir Mosavi", "authors": "Danial Mohammadzadeh, Seyed-Farzan Kazemi, Amir Mosavi, Ehsan\n  Nasseralshariati, Joseph H. M. Tah", "title": "Prediction of Compression Index of Fine-Grained Soils Using a Gene\n  Expression Programming Model", "comments": "8 figures, 5 tables, 12 pages", "journal-ref": "Infrastructures 2019, 4, 26", "doi": "10.3390/infrastructures4020026", "report-no": null, "categories": "stat.AP cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In construction projects, estimation of the settlement of fine-grained soils\nis of critical importance, and yet is a challenging task. The coefficient of\nconsolidation for the compression index (Cc) is a key parameter in modeling the\nsettlement of fine-grained soil layers. However, the estimation of this\nparameter is costly, time-consuming, and requires skilled technicians. To\novercome these drawbacks, we aimed to predict Cc through other soil parameters,\ni.e., the liquid limit (LL), plastic limit (PL), and initial void ratio (e0).\nUsing these parameters is more convenient and requires substantially less time\nand cost compared to the conventional tests to estimate Cc. This study presents\na novel prediction model for the Cc of fine-grained soils using gene expression\nprogramming (GEP). A database consisting of 108 different data points was used\nto develop the model. A closed-form equation solution was derived to estimate\nCc based on LL, PL, and e0. The performance of the developed GEP-based model\nwas evaluated through the coefficient of determination (R2), the root mean\nsquared error (RMSE), and the mean average error (MAE). The proposed model\nperformed better in terms of R2, RMSE, and MAE compared to the other models.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 07:12:35 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Mohammadzadeh", "Danial", ""], ["Kazemi", "Seyed-Farzan", ""], ["Mosavi", "Amir", ""], ["Nasseralshariati", "Ehsan", ""], ["Tah", "Joseph H. M.", ""]]}, {"id": "1907.05026", "submitter": "Rodolfo Metulini", "authors": "Rodolfo Metulini, Maurizio Carpita", "title": "The HOG-FDA Approach with Mobile Phone Data to Modeling the Dynamic of\n  People's Presences in the City", "comments": "5 pages, 3 figures, 9th International Conference IES 2019 -\n  Innovation & Society - Book of short papers, Statistical evaluation systems\n  at 360{\\deg}: techniques, technologies and new frontiers - organized by\n  Statistics for the Evaluation and Quality in Services Group of the Italian\n  Statistical Society and European University of Rome", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of Smart City, the dynamic of the presence of people can be\nanalysed using high-dimensional spatio-temporal mobile phone data. In order to\nfind regularities and detect anomalies in the daily profiles, we propose an\napproach that considers the spatial structure by means of Histogram of Oriented\nGradients (HOG) method and the temporal evolution using a Model-Based\nClustering Functional Data Analysis (FDA). An application to the case study of\nthe Municipality of Brescia is provided. Similarities among days, that follow a\nseasonal or a days of the week trend, exist. The number of users in the city,\ndepending on the season, the day of the week and the time of the day, varies\nfrom 30 to 60 thousands of people\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 07:17:06 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Metulini", "Rodolfo", ""], ["Carpita", "Maurizio", ""]]}, {"id": "1907.05234", "submitter": "Chainarong Amornbunchornvej", "authors": "Chainarong Amornbunchornvej, Navaporn Surasvadi, Anon Plangprasopchok,\n  and Suttipong Thajchayapong", "title": "Identifying Linear Models in Multi-Resolution Population Data using\n  Minimum Description Length Principle to Predict Household Income", "comments": "This is the accepted manuscript for publication in TKDD. The R\n  package is available at https://github.com/DarkEyes/MRReg", "journal-ref": "ACM Transactions on Knowledge Discovery from Data (TKDD), 15(2),\n  15 (2021)", "doi": "10.1145/3424670", "report-no": null, "categories": "cs.LG cs.CY stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One shirt size cannot fit everybody, while we cannot make a unique shirt that\nfits perfectly for everyone because of resource limitation. This analogy is\ntrue for the policy making. Policy makers cannot establish a single policy to\nsolve all problems for all regions because each region has its own unique\nissue. In the other extreme, policy makers also cannot create a policy for each\nsmall village due to the resource limitation. Would it be better if we can find\na set of largest regions such that the population of each region within this\nset has common issues and we can establish a single policy for them? In this\nwork, we propose a framework using regression analysis and minimum description\nlength (MDL) to find a set of largest areas that have common indicators, which\ncan be used to predict household incomes efficiently. Given a set of household\nfeatures, and a multi-resolution partition that represents administrative\ndivisions, our framework reports a set C* of largest subdivisions that have a\ncommon model for population-income prediction. We formalize a problem of\nfinding C* and propose the algorithm as a solution. We use both simulation\ndatasets as well as a real-world dataset of Thailand's population household\ninformation to demonstrate our framework performance and application. The\nresults show that our framework performance is better than the baseline\nmethods. We show the results of our method can be used to find indicators of\nincome prediction for many areas in Thailand. By increasing these indicator\nvalues, we expect people in these areas to gain more incomes. Hence, the policy\nmakers can plan to establish the policies by using these indicators in our\nresults as a guideline to solve low-income issues. Our framework can be used to\nsupport policy makers to establish policies regarding any other dependent\nvariable beyond incomes in order to combat poverty and other issues.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 03:08:31 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 15:32:25 GMT"}, {"version": "v3", "created": "Wed, 6 Jan 2021 08:39:38 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Amornbunchornvej", "Chainarong", ""], ["Surasvadi", "Navaporn", ""], ["Plangprasopchok", "Anon", ""], ["Thajchayapong", "Suttipong", ""]]}, {"id": "1907.05302", "submitter": "Marjolein Fokkema", "authors": "Marjolein Fokkema and Carolin Strobl", "title": "Fitting Prediction Rule Ensembles to Psychological Research Data: An\n  Introduction and Tutorial", "comments": "Published in Psychological Methods", "journal-ref": null, "doi": "10.1037/met0000256", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction rule ensembles (PREs) are a relatively new statistical learning\nmethod, which aim to strike a balance between predictive accuracy and\ninterpretability. Starting from a decision tree ensemble, like a boosted tree\nensemble or a random forest, PREs retain a small subset of tree nodes in the\nfinal predictive model. These nodes can be written as simple rules of the form\nif [condition] then [prediction]. As a result, PREs are often much less complex\nthan full decision tree ensembles, while they have been found to provide\nsimilar predictive accuracy in many situations. The current paper introduces\nthe methodology and shows how PREs can be fitted using the R package pre\nthrough several real-data examples from psychological research. The examples\nalso illustrate a number of features of package \\textbf{pre} that may be\nparticularly useful for applications in psychology: support for categorical,\nmultivariate and count responses, application of (non-)negativity constraints,\ninclusion of confirmatory rules and standardized variable importance measures.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 15:23:31 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2019 17:07:43 GMT"}, {"version": "v3", "created": "Fri, 27 Mar 2020 23:50:44 GMT"}, {"version": "v4", "created": "Sun, 21 Feb 2021 23:50:19 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Fokkema", "Marjolein", ""], ["Strobl", "Carolin", ""]]}, {"id": "1907.05326", "submitter": "Chinchin Wang", "authors": "Chinchin Wang, Jorge Trejo Vargas, Tyrel Stokes, Russell Steele, Ian\n  Shrier", "title": "The acute:chronic workload ratio: challenges and prospects for\n  improvement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Injuries occur when an athlete performs a greater amount of activity\n(workload) than what their body can absorb. To maximize the positive effects of\ntraining while avoiding injuries, athletes and coaches need to determine safe\nworkload levels. The International Olympic Committee has recommended using the\nacute:chronic workload ratio (ACRatio) to monitor injury risk, and has provided\nthresholds to minimize risk. However, there are several limitations to the\nACRatio which may impact the validity of current recommendations. In this\nreview, we discuss previously published and novel challenges with the ACRatio,\nand possible strategies to address them. These challenges include 1)\nformulating the ACRatio as a proportion rather than a measure of change, 2) its\nuse of unweighted averages to measure activity loads, 3) inapplicability of the\nACRatio to sports where athletes taper their activity, 4) discretization of the\nACRatio prior to model selection, 5) the establishment of the model using\nsparse data, 6) potential bias in the ACRatio of injured athletes, 7)\nunmeasured confounding, and 8) application of the ACRatio to subsequent\ninjuries.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 16:01:59 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Wang", "Chinchin", ""], ["Vargas", "Jorge Trejo", ""], ["Stokes", "Tyrel", ""], ["Steele", "Russell", ""], ["Shrier", "Ian", ""]]}, {"id": "1907.05364", "submitter": "Felix Batsch", "authors": "Felix Batsch, Alireza Daneshkhah, Madeline Cheah, Stratis Kanarachos,\n  Anthony Baxendale", "title": "Performance Boundary Identification for the Evaluation of Automated\n  Vehicles using Gaussian Process Classification", "comments": "6 pages, 5 figures, accepted at 2019 IEEE Intelligent Transportation\n  Systems Conference - ITSC 2019, Auckland, New Zealand, October 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Safety is an essential aspect in the facilitation of automated vehicle\ndeployment. Current testing practices are not enough, and going beyond them\nleads to infeasible testing requirements, such as needing to drive billions of\nkilometres on public roads. Automated vehicles are exposed to an indefinite\nnumber of scenarios. Handling of the most challenging scenarios should be\ntested, which leads to the question of how such corner cases can be determined.\nWe propose an approach to identify the performance boundary, where these corner\ncases are located, using Gaussian Process Classification. We also demonstrate\nthe classification on an exemplary traffic jam approach scenario, showing that\nit is feasible and would lead to more efficient testing practices.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 16:35:59 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Batsch", "Felix", ""], ["Daneshkhah", "Alireza", ""], ["Cheah", "Madeline", ""], ["Kanarachos", "Stratis", ""], ["Baxendale", "Anthony", ""]]}, {"id": "1907.05377", "submitter": "Joe Kileel", "authors": "Nir Sharon, Joe Kileel, Yuehaw Khoo, Boris Landa, Amit Singer", "title": "Method of moments for 3-D single particle ab initio modeling with\n  non-uniform distribution of viewing angles", "comments": "41 pages. v2: additional numerical experiments, appendices edited,\n  other updates", "journal-ref": null, "doi": "10.1088/1361-6420/ab6139", "report-no": null, "categories": "math.NA cs.NA math.OC q-bio.BM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-particle reconstruction in cryo-electron microscopy (cryo-EM) is an\nincreasingly popular technique for determining the 3-D structure of a molecule\nfrom several noisy 2-D projections images taken at unknown viewing angles. Most\nreconstruction algorithms require a low-resolution initialization for the 3-D\nstructure, which is the goal of ab initio modeling. Suggested by Zvi Kam in\n1980, the method of moments (MoM) offers one approach, wherein low-order\nstatistics of the 2-D images are computed and a 3-D structure is estimated by\nsolving a system of polynomial equations. Unfortunately, Kam's method suffers\nfrom restrictive assumptions, most notably that viewing angles should be\ndistributed uniformly. Often unrealistic, uniformity entails the computation of\nhigher-order correlations, as in this case first and second moments fail to\ndetermine the 3-D structure. In the present paper, we remove this hypothesis,\nby permitting an unknown, non-uniform distribution of viewing angles in MoM.\nPerhaps surprisingly, we show that this case is statistically easier than the\nuniform case, as now first and second moments generically suffice to determine\nlow-resolution expansions of the molecule. In the idealized setting of a known,\nnon-uniform distribution, we find an efficient provable algorithm inverting\nfirst and second moments. For unknown, non-uniform distributions, we use\nnon-convex optimization methods to solve for both the molecule and\ndistribution.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 16:53:34 GMT"}, {"version": "v2", "created": "Sat, 23 Nov 2019 17:44:17 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Sharon", "Nir", ""], ["Kileel", "Joe", ""], ["Khoo", "Yuehaw", ""], ["Landa", "Boris", ""], ["Singer", "Amit", ""]]}, {"id": "1907.05385", "submitter": "Lamar Hunt", "authors": "Lamar Hunt III, Irene B. Murimi, Jodi B. Segal, Marissa J. Seamans,\n  Daniel O. Scharfstein, Ravi Varadhan", "title": "Brand vs. Generic: Addressing Non-Adherence, Secular Trends, and\n  Non-Overlap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While generic drugs offer a cost-effective alternative to brand name drugs,\nregulators need a method to assess therapeutic equivalence in a post market\nsetting. We develop such a method in the context of assessing the therapeutic\nequivalence of immediate release (IM) venlafaxine, based on a large insurance\nclaims dataset provided by OptumLabs\\textsuperscript{\\textregistered}. To\nproperly address this question, our methodology must deal with issues of\nnon-adherence, secular trends in health outcomes, and lack of treatment overlap\ndue to sharp uptake of the generic once it becomes available. We define,\nidentify (under assumptions) and estimate (using G-computation) a causal effect\nfor a time-to-event outcome by extending regression discontinuity to survival\ncurves. We do not find evidence for a lack of therapeutic equivalence of brand\nand generic IM venlafaxine.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2019 15:32:43 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Hunt", "Lamar", "III"], ["Murimi", "Irene B.", ""], ["Segal", "Jodi B.", ""], ["Seamans", "Marissa J.", ""], ["Scharfstein", "Daniel O.", ""], ["Varadhan", "Ravi", ""]]}, {"id": "1907.05386", "submitter": "Frederic Lavancier", "authors": "Fr\\'ed\\'eric Lavancier and Thierry P\\'ecot and Liu Zengzhen and\n  Charles Kervrann", "title": "Testing independence between two random sets for the analysis of\n  colocalization in bio-imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP eess.IV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colocalization aims at characterizing spatial associations between two\nfluorescently-tagged biomolecules by quantifying the co-occurrence and\ncorrelation between the two channels acquired in fluorescence microscopy.\nColocalization is presented either as the degree of overlap between the two\nchannels or the overlays of the red and green images, with areas of yellow\nindicating colocalization of the molecules. This problem remains an open issue\nin diffraction-limited microscopy and raises new challenges with the emergence\nof super-resolution imaging, a microscopic technique awarded by the 2014 Nobel\nprize in chemistry. We propose GcoPS, for Geo-coPositioning System, an original\nmethod that exploits the random sets structure of the tagged molecules to\nprovide an explicit testing procedure. Our simulation study shows that GcoPS\nunequivocally outperforms the best competitive methods in adverse situations\n(noise, irregularly shaped fluorescent patterns, different optical\nresolutions). GcoPS is also much faster, a decisive advantage to face the huge\namount of data in super-resolution imaging. We demonstrate the performances of\nGcoPS on two biological real datasets, obtained by conventional\ndiffraction-limited microscopy technique and by super-resolution technique,\nrespectively.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 12:25:05 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Lavancier", "Fr\u00e9d\u00e9ric", ""], ["P\u00e9cot", "Thierry", ""], ["Zengzhen", "Liu", ""], ["Kervrann", "Charles", ""]]}, {"id": "1907.05387", "submitter": "Jairo F\\'uquene", "authors": "Julieth Casta\\~neda and Cristian Tellez and Jairo Fuquene", "title": "An alternative for the average income estimation using small area\n  methods", "comments": "in Spanish", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The average household income is one of the most important indexes for\ndecision making and the modelling of economic inequity and poverty. In this\nwork we propose a practical procedure to estimate the average income using\nsmall area methods. We illustrate our proposal using information from a\nmultipurpose survey and suitable economic and demographic variables such as the\nmultidimensional poverty and the valorization indexes and the official\npopulation projections. We find that the standard relative errors for the\nincome average estimates improve substantially when the proposed methodology is\nimplemented.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 23:45:41 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Casta\u00f1eda", "Julieth", ""], ["Tellez", "Cristian", ""], ["Fuquene", "Jairo", ""]]}, {"id": "1907.05409", "submitter": "Solt Kov\\'acs", "authors": "Malte Londschien, Solt Kov\\'acs, Peter B\\\"uhlmann", "title": "Change point detection for graphical models in the presence of missing\n  values", "comments": "14 pages, 6 figures, 3 tables, hdcd R package; added explanations and\n  clarifications, methodology and simulation results unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose estimation methods for change points in high-dimensional\ncovariance structures with an emphasis on challenging scenarios with missing\nvalues. We advocate three imputation like methods and investigate their\nimplications on common losses used for change point detection. We also discuss\nhow model selection methods have to be adapted to the setting of incomplete\ndata. The methods are compared in a simulation study and applied to a time\nseries from an environmental monitoring system. An implementation of our\nproposals within the R-package hdcd is available via the Supplementary\nmaterials.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 17:50:47 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 20:51:57 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Londschien", "Malte", ""], ["Kov\u00e1cs", "Solt", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "1907.05458", "submitter": "Matthew Malloy", "authors": "Swapnil Shinde, Jukka Ranta, Paul Deitrick, Matthew Malloy", "title": "Scalable Panel Fusion Using Distributed Min Cost Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern audience measurement requires combining observations from disparate\npanel datasets. Connecting and relating such panel datasets is a process termed\npanel fusion. This paper formalizes the panel fusion problem and presents a\nnovel approach to solve it. We cast the panel fusion as a network flow problem,\nallowing the application of a rich body of research. In the context of digital\naudience measurement, where panel sizes can grow into the tens of millions, we\npropose an efficient algorithm to partition the network into sub-problems.\nWhile the algorithm solves a relaxed version of the original problem, we\nprovide conditions under which it guarantees optimality. We demonstrate our\napproach by fusing two real-world panel datasets in a distributed computing\nenvironment.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 19:27:37 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Shinde", "Swapnil", ""], ["Ranta", "Jukka", ""], ["Deitrick", "Paul", ""], ["Malloy", "Matthew", ""]]}, {"id": "1907.05526", "submitter": "Shu Wei", "authors": "Shu Wei", "title": "The Geography of Love: Decoding the Spatial Pattern and Digital Self in\n  Chinese Online Courtship", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The evolution of Chinese online courtship culture is a mixed product under\ndynamic interaction between the feudal tradition, urban development and\ngovernment policies. The presented research examines the regional online dating\npopularity, spatial homogamy and the self-presentation pattern in the cyber\nlove market, with a special focus on the geographical factors. More than 30\nthousand recent data are collected from China's biggest online dating site\njiayuan.com, and analyzed through the ordered logit modeling, Katz centrality\nanalysis and Word2Vec linguistic modeling. The results indicate that the high\nuser density spots are widely spread among different parts of China rather than\nonly clustering in the developed areas. While major users expect a within-city\ndistance with the potential dater, the increasing social mobility and\ncross-city mating trend is also driven by the motivation of gaining localized\naccess to better social and welfare resources. Moreover, people tend to\nemphasize the soft qualities of caring, reliable and family engagement in the\nonline self-presentation, with the marriage pragmatism overweighing the\npersonalized individualism and emotional self.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 23:56:51 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Wei", "Shu", ""]]}, {"id": "1907.05621", "submitter": "Anirudh Tomer", "authors": "Anirudh Tomer, Dimitris Rizopoulos, Daan Nieboer, Frank-Jan Drost,\n  Monique J. Roobol, Ewout W. Steyerberg", "title": "Personalized Decision Making for Biopsies in Prostate Cancer Active\n  Surveillance Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Background: Low-risk prostate cancer patients enrolled in active surveillance\nprograms commonly undergo biopsies for examination of cancer progression.\nBiopsies are conducted as per a fixed and frequent schedule (e.g., annual\nbiopsies). Since biopsies are burdensome, patients do not always comply with\nthe schedule, which increases the risk of delayed detection of cancer\nprogression.\n  Objective: Our aim is to better balance the number of biopsies (burden) and\nthe delay in detection of cancer progression (less is beneficial), by\npersonalizing the decision of conducting biopsies.\n  Data Sources: We use patient data of the world's largest active surveillance\nprogram (PRIAS). It enrolled 5270 patients, had 866 cancer progressions, and an\naverage of nine prostate-specific antigen (PSA) and five digital rectal\nexamination (DRE) measurements per patient.\n  Methods: Using joint models for time-to-event and longitudinal data, we model\nthe historical DRE and PSA measurements, and biopsy results of a patient at\neach follow-up visit. This results in a visit and patient-specific cumulative\nrisk of cancer progression. If this risk is above a certain threshold, we\nschedule a biopsy. We compare this personalized approach with the currently\npracticed biopsy schedules via an extensive and realistic simulation study,\nbased on a replica of the patients from the PRIAS program.\n  Results: The personalized approach saved a median of six biopsies (median: 4,\nIQR: 2-5), compared to the annual schedule (median: 10, IQR: 3-10). However,\nthe delay in detection of progression (years) is similar for the personalized\n(median: 0.7, IQR: 0.3-1.0) and the annual schedule (median: 0.5, IQR:\n0.3-0.8).\n  Conclusions: We conclude that personalized schedules provide substantially\nbetter balance in the number of biopsies per detected progression for men with\nlow-risk prostate cancer.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 08:39:06 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Tomer", "Anirudh", ""], ["Rizopoulos", "Dimitris", ""], ["Nieboer", "Daan", ""], ["Drost", "Frank-Jan", ""], ["Roobol", "Monique J.", ""], ["Steyerberg", "Ewout W.", ""]]}, {"id": "1907.05750", "submitter": "Kate Saunders Saunders", "authors": "K.R. Saunders, A.G. Stephenson and D.J. Karoly", "title": "A Regionalisation Approach for Rainfall based on Extremal Dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To mitigate the risk posed by extreme rainfall events, we require statistical\nmodels that reliably capture extremes in continuous space with dependence.\nHowever, assuming a stationary dependence structure in such models is often\nerroneous, particularly over large geographical domains. Furthermore, there are\nlimitations on the ability to fit existing models, such as max-stable\nprocesses, to a large number of locations. To address these modelling\nchallenges, we present a regionalisation method that partitions stations into\nregions of similar extremal dependence using clustering. To demonstrate our\nregionalisation approach, we consider a study region of Australia and discuss\nthe results with respect to known climate and topographic features. To\nvisualise and evaluate the effectiveness of the partitioning, we fit max-stable\nmodels to each of the regions. This work serves as a prelude to how one might\nconsider undertaking a project where spatial dependence is non-stationary and\nis modelled on a large geographical scale.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 14:00:26 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Saunders", "K. R.", ""], ["Stephenson", "A. G.", ""], ["Karoly", "D. J.", ""]]}, {"id": "1907.06118", "submitter": "Geoff Boeing", "authors": "Geoff Boeing", "title": "Online Rental Housing Market Representation and the Digital Reproduction\n  of Urban Inequality", "comments": null, "journal-ref": "Environment and Planning A: Economy and Space, 2019", "doi": "10.1177/0308518X19869678", "report-no": null, "categories": "econ.GN q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the rental housing market moves online, the Internet offers divergent\npossible futures: either the promise of more-equal access to information for\npreviously marginalized homeseekers, or a reproduction of longstanding\ninformation inequalities. Biases in online listings' representativeness could\nimpact different communities' access to housing search information, reinforcing\ntraditional information segregation patterns through a digital divide. They\ncould also circumscribe housing practitioners' and researchers' ability to draw\nbroad market insights from listings to understand rental supply and\naffordability. This study examines millions of Craigslist rental listings\nacross the US and finds that they spatially concentrate and over-represent\nwhiter, wealthier, and better-educated communities. Other significant\ndemographic differences exist in age, language, college enrollment, rent,\npoverty rate, and household size. Most cities' online housing markets are\ndigitally segregated by race and class, and we discuss various implications for\nresidential mobility, community legibility, gentrification, housing voucher\nutilization, and automated monitoring and analytics in the smart cities\nparadigm. While Craigslist contains valuable crowdsourced data to better\nunderstand affordability and available rental supply in real-time, it does not\nevenly represent all market segments. The Internet promises information\ndemocratization, and online listings can reduce housing search costs and\nincrease choice sets. However, technology access/preferences and information\nchannel segregation can concentrate such information-broadcasting benefits in\nalready-advantaged communities, reproducing traditional inequalities and\nreinforcing residential sorting and segregation dynamics. Technology platforms\nlike Craigslist construct new institutions with the power to shape spatial\neconomies.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 18:56:57 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 13:01:33 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Boeing", "Geoff", ""]]}, {"id": "1907.06145", "submitter": "Olanrewaju Akande", "authors": "Olanrewaju Akande, Gabriel Madson, D. Sunshine Hillygus and Jerome P.\n  Reiter", "title": "Leveraging Auxiliary Information on Marginal Distributions in\n  Nonignorable Models for Item and Unit Nonresponse", "comments": "23 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Often, government agencies and survey organizations know the population\ncounts or percentages for some of the variables in a survey. These may be\navailable from auxiliary sources, for example, administrative databases or\nother high quality surveys. We present and illustrate a model-based framework\nfor leveraging such auxiliary marginal information when handling unit and item\nnonresponse. We show how one can use the margins to specify different\nmissingness mechanisms for each type of nonresponse. We use the framework to\nimpute missing values in voter turnout in a subset of data from the U.S.\\\nCurrent Population Survey (CPS). In doing so, we examine the sensitivity of\nresults to different assumptions about the unit and item nonresponse.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 23:02:47 GMT"}, {"version": "v2", "created": "Sun, 2 Aug 2020 19:57:14 GMT"}, {"version": "v3", "created": "Tue, 10 Nov 2020 23:56:32 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Akande", "Olanrewaju", ""], ["Madson", "Gabriel", ""], ["Hillygus", "D. Sunshine", ""], ["Reiter", "Jerome P.", ""]]}, {"id": "1907.06148", "submitter": "Martin Treiber", "authors": "D. Ngoduy, S. Lee, M. Treiber, M. Keyvan-Ekbatani, and H. L. Vu", "title": "Langevin method for a continuous stochastic car-following model and its\n  stability conditions", "comments": null, "journal-ref": "Transportation Research Part C: Emerging Technologies Volume 105,\n  August 2019, 599-610", "doi": "10.1016/j.trc.2019.06.005", "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In car-following models, the driver reacts according to his physical and\npsychological abilities which may change over time. However, most car-following\nmodels are deterministic and do not capture the stochastic nature of human\nperception. It is expected that purely deterministic traffic models may produce\nunrealistic results due to the stochastic driving behaviors of drivers. This\npaper is devoted to the development of a distinct car-following model where a\nstochastic process is adopted to describe the time-varying random acceleration\nwhich essentially reflects the random individual perception of driver behavior\nwith respect to the leading vehicle over time. In particular, we apply coupled\nLangevin equations to model complex human driver behavior. In the proposed\nmodel, an extended Cox-Ingersoll-Ross (CIR) stochastic process will be used to\ndescribe the stochastic speed of the follower in response to the stimulus of\nthe leader. An important property of the extended CIR process is to enhance the\nnon-negative properties of the stochastic traffic variables (e.g. non-negative\nspeed) for any arbitrary model parameters. Based on stochastic process\ntheories, we derive stochastic linear stability conditions which, for the first\ntime, theoretically capture the effect of the random parameter on traffic\ninstabilities. Our stability results conform to the empirical results that the\ntraffic instability is related to the stochastic nature of traffic flow at the\nlow speed conditions, even when traffic is deemed to be stable from\ndeterministic models.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 23:34:07 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Ngoduy", "D.", ""], ["Lee", "S.", ""], ["Treiber", "M.", ""], ["Keyvan-Ekbatani", "M.", ""], ["Vu", "H. L.", ""]]}, {"id": "1907.06244", "submitter": "Daniel McDonald", "authors": "Daniel J. McDonald and Michael McBride and Yupeng Gu and Christopher\n  Raphael", "title": "Markov-switching State Space Models for Uncovering Musical\n  Interpretation", "comments": "33 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For concertgoers, musical interpretation is the most important factor in\ndetermining whether or not we enjoy a classical performance. Every performance\nincludes mistakes---intonation issues, a lost note, an unpleasant sound---but\nthese are all easily forgotten (or unnoticed) when a performer engages her\naudience, imbuing a piece with novel emotional content beyond the vague\ninstructions inscribed on the printed page. While music teachers use imagery or\nheuristic guidelines to motivate interpretive decisions, combining these vague\ninstructions to create a convincing performance remains the domain of the\nperformer, subject to the whims of the moment, technical fluency, and taste. In\nthis research, we use data from the CHARM Mazurka Project---forty-six\nprofessional recordings of Chopin's Mazurka Op. 63 No. 3 by consumate\nartists---with the goal of elucidating musically interpretable performance\ndecisions. Using information on the inter-onset intervals of the note attacks\nin the recordings, we apply functional data analysis techniques enriched with\nprior information gained from music theory to discover relevant features and\nperform hierarchical clustering. The resulting clusters suggest methods for\ninforming music instruction, discovering listening preferences, and analyzing\nperformances.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2019 16:25:08 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["McDonald", "Daniel J.", ""], ["McBride", "Michael", ""], ["Gu", "Yupeng", ""], ["Raphael", "Christopher", ""]]}, {"id": "1907.06303", "submitter": "Francis Diebold", "authors": "Francis X. Diebold and Glenn D. Rudebusch", "title": "On the Evolution of U.S. Temperature Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Climate change is a massive multidimensional shift. Temperature shifts, in\nparticular, have important implications for urbanization, agriculture, health,\nproductivity, and poverty, among other things. While much research has\ndocumented rising mean temperature \\emph{levels}, we also examine range-based\nmeasures of daily temperature \\emph{volatility}. Specifically, using data for\nselect U.S. cities over the past half-century, we compare the evolving time\nseries dynamics of the average temperature level, AVG, and the diurnal\ntemperature range, DTR (the difference between the daily maximum and minimum\ntemperatures). We characterize trend and seasonality in these two series using\nlinear models with time-varying coefficients. These straightforward yet\nflexible approximations provide evidence of evolving DTR seasonality and stable\nAVG seasonality.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 00:27:20 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2021 15:14:43 GMT"}, {"version": "v3", "created": "Wed, 6 Jan 2021 18:30:24 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Diebold", "Francis X.", ""], ["Rudebusch", "Glenn D.", ""]]}, {"id": "1907.06455", "submitter": "Radu Stoica", "authors": "R. Stoica (Universit\\'e de Lorraine), Madalina Deaconu\n  (TOSCA-NGE-POST), Anne Philippe (UN), Lluis Hurtado", "title": "Shadow Simulated Annealing algorithm: a new tool for global optimisation\n  and statistical inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a new global optimisation method that applies to a family\nof criteria that are not entirely known. This family includes the criteria\nobtained from the class of posteriors that have nor-malising constants that are\nanalytically not tractable. The procedure applies to posterior probability\ndensities that are continuously differen-tiable with respect to their\nparameters. The proposed approach avoids the re-sampling needed for the\nclassical Monte Carlo maximum likelihood inference, while providing the missing\nconvergence properties of the ABC based methods. Results on simulated data and\nreal data are presented. The real data application fits an inhomogeneous area\ninteraction point process to cosmological data. The obtained results validate\ntwo important aspects of the galaxies distribution in our Universe : proximity\nof the galaxies from the cosmic filament network together with territorial\nclustering at given range of interactions. Finally, conclusions and\nperspectives are depicted.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 11:58:33 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Stoica", "R.", "", "Universit\u00e9 de Lorraine"], ["Deaconu", "Madalina", "", "TOSCA-NGE-POST"], ["Philippe", "Anne", "", "UN"], ["Hurtado", "Lluis", ""]]}, {"id": "1907.06481", "submitter": "Gabriel Michau Dr.", "authors": "Gabriel Michau and Olga Fink", "title": "Unsupervised Fault Detection in Varying Operating Conditions", "comments": null, "journal-ref": "Proceedings of the 2019 IEEE International Conference on\n  Prognostics and Health Management, San Francisco", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training data-driven approaches for complex industrial system health\nmonitoring is challenging. When data on faulty conditions are rare or not\navailable, the training has to be performed in a unsupervised manner. In\naddition, when the observation period, used for training, is kept short, to be\nable to monitor the system in its early life, the training data might not be\nrepresentative of all the system normal operating conditions. In this paper, we\npropose five approaches to perform fault detection in such context. Two\napproaches rely on the data from the unit to be monitored only: the baseline is\ntrained on the early life of the unit. An incremental learning procedure tries\nto learn new operating conditions as they arise. Three other approaches take\nadvantage of data from other similar units within a fleet. In two cases, units\nare directly compared to each other with similarity measures, and the data from\nsimilar units are combined in the training set. We propose, in the third case,\na new deep-learning methodology to perform, first, a feature alignment of\ndifferent units with an Unsupervised Feature Alignment Network (UFAN). Then,\nfeatures of both units are combined in the training set of the fault detection\nneural network.\n  The approaches are tested on a fleet comprising 112 units, observed over one\nyear of data. All approaches proposed here are an improvement to the baseline,\ntrained with two months of data only. As units in the fleet are found to be\nvery dissimilar, the new architecture UFAN, that aligns units in the feature\nspace, is outperforming others.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 13:01:26 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Michau", "Gabriel", ""], ["Fink", "Olga", ""]]}, {"id": "1907.06560", "submitter": "Brady West PhD", "authors": "Brady T. West, James Wagner, Stephanie Coffey, Michael R. Elliott", "title": "Deriving Priors for Bayesian Prediction of Daily Response Propensity in\n  Responsive Survey Design: Historical Data Analysis vs. Literature Review", "comments": "42 pages, 9 figures, one table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Responsive Survey Design (RSD) aims to increase the efficiency of survey data\ncollection via live monitoring of paradata and the introduction of protocol\nchanges when survey errors and increased costs seem imminent. Daily predictions\nof response propensity for all active sampled cases are among the most\nimportant quantities for live monitoring of data collection outcomes, making\nsound predictions of these propensities essential for the success of RSD.\nBecause it relies on real-time updates of prior beliefs about key design\nquantities, such as predicted response propensities, RSD stands to benefit from\nBayesian approaches. However, empirical evidence of the merits of these\napproaches is lacking in the literature, and the derivation of informative\nprior distributions is required for these approaches to be effective. In this\npaper, we evaluate the ability of two approaches to deriving prior\ndistributions for the coefficients defining daily response propensity models to\nimprove predictions of daily response propensity in a real data collection\nemploying RSD. The first approach involves analyses of historical data from the\nsame survey, and the second approach involves literature review. We find that\nBayesian methods based on these two approaches result in higher-quality\npredictions of response propensity than more standard approaches ignoring prior\ninformation. This is especially true during the early-to-middle periods of data\ncollection when interventions are often considered in an RSD framework.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 16:05:00 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 22:16:08 GMT"}, {"version": "v3", "created": "Fri, 7 Aug 2020 20:54:11 GMT"}, {"version": "v4", "created": "Wed, 17 Mar 2021 13:17:42 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["West", "Brady T.", ""], ["Wagner", "James", ""], ["Coffey", "Stephanie", ""], ["Elliott", "Michael R.", ""]]}, {"id": "1907.06567", "submitter": "Shirley Liao", "authors": "Shirley Liao, Lucas Henneman, Corwin Zigler", "title": "Posterior Predictive Treatment Assignment Methods for Causal Inference\n  in the Context of Time-Varying Treatments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Marginal structural models (MSM) with inverse probability weighting (IPW) are\nused to estimate causal effects of time-varying treatments, but can result in\nerratic finite-sample performance when there is low overlap in covariate\ndistributions across different treatment patterns. Modifications to IPW which\ntarget the average treatment effect (ATE) estimand either introduce bias or\nrely on unverifiable parametric assumptions and extrapolation. This paper\nextends an alternate estimand, the average treatment effect on the overlap\npopulation (ATO) which is estimated on a sub-population with a reasonable\nprobability of receiving alternate treatment patterns in time-varying treatment\nsettings. To estimate the ATO within a MSM framework, this paper extends a\nstochastic pruning method based on the posterior predictive treatment\nassignment (PPTA) as well as a weighting analogue to the time-varying treatment\nsetting. Simulations demonstrate the performance of these extensions compared\nagainst IPW and stabilized weighting with regard to bias, efficiency and\ncoverage. Finally, an analysis using these methods is performed on Medicare\nbeneficiaries residing across 18,480 zip codes in the U.S. to evaluate the\neffect of coal-fired power plant emissions exposure on ischemic heart disease\nhospitalization, accounting for seasonal patterns that lead to change in\ntreatment over time.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 16:17:13 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Liao", "Shirley", ""], ["Henneman", "Lucas", ""], ["Zigler", "Corwin", ""]]}, {"id": "1907.06614", "submitter": "Argyris Kalogeratos", "authors": "Ioannis Bargiotas, Argyris Kalogeratos, Myrto Limnios, Pierre-Paul\n  Vidal, Damien Ricard, Nicolas Vayatis", "title": "Revealing posturographic features associated with the risk of falling in\n  patients with Parkinsonian syndromes via machine learning", "comments": "16 pages, 11 figures (plots, tables, algorithms)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Falling in Parkinsonian syndromes (PS) is associated with postural\ninstability and consists a common cause of disability among PS patients.\nCurrent posturographic practices record the body's center-of-pressure\ndisplacement (statokinesigram) while the patient stands on a force platform.\nStatokinesigrams, after appropriate signal processing, can offer numerous\nposturographic features, which however challenges the efforts for valid\nstatistics via standard univariate approaches. In this work, we present the\nts-AUC, a non-parametric multivariate two-sample test, which we employ to\nanalyze statokinesigram differences among PS patients that are fallers (PSf)\nand non-fallers (PSNF). We included 123 PS patients who were classified into\nPSF or PSNF based on clinical assessment and underwent simple Romberg Test\n(eyes open/eyes closed). We analyzed posturographic features using both\nmultiple testing with p-value adjustment and the ts-AUC. While the ts-AUC\nshowed significant difference between groups (p-value = 0.01), multiple testing\ndid not show any such difference. Interestingly, significant difference between\nthe two groups was found only using the open-eyes protocol. PSF showed\nsignificantly increased antero-posterior movements as well as increased\nposturographic area, compared to PSNF. Our study demonstrates the superiority\nof the ts-AUC test compared to standard statistical tools in distinguishing PSF\nand PSNF in the multidimensional feature space. This result highlights more\ngenerally the fact that machine learning-based statistical tests can be seen as\na natural extension of classical statistical approaches and should be\nconsidered, especially when dealing with multifactorial assessments.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 17:21:13 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Bargiotas", "Ioannis", ""], ["Kalogeratos", "Argyris", ""], ["Limnios", "Myrto", ""], ["Vidal", "Pierre-Paul", ""], ["Ricard", "Damien", ""], ["Vayatis", "Nicolas", ""]]}, {"id": "1907.06622", "submitter": "Christopher Walters", "authors": "Patrick Kline and Christopher Walters", "title": "Audits as Evidence: Experiments, Ensembles, and Enforcement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop tools for utilizing correspondence experiments to detect illegal\ndiscrimination by individual employers. Employers violate US employment law if\ntheir propensity to contact applicants depends on protected characteristics\nsuch as race or sex. We establish identification of higher moments of the\ncausal effects of protected characteristics on callback rates as a function of\nthe number of fictitious applications sent to each job ad. These moments are\nused to bound the fraction of jobs that illegally discriminate. Applying our\nresults to three experimental datasets, we find evidence of significant\nemployer heterogeneity in discriminatory behavior, with the standard deviation\nof gaps in job-specific callback probabilities across protected groups\naveraging roughly twice the mean gap. In a recent experiment manipulating\nracially distinctive names, we estimate that at least 85% of jobs that contact\nboth of two white applications and neither of two black applications are\nengaged in illegal discrimination. To assess the tradeoff between type I and II\nerrors presented by these patterns, we consider the performance of a series of\ndecision rules for investigating suspicious callback behavior under a simple\ntwo-type model that rationalizes the experimental data. Though, in our\npreferred specification, only 17% of employers are estimated to discriminate on\nthe basis of race, we find that an experiment sending 10 applications to each\njob would enable accurate detection of 7-10% of discriminators while falsely\naccusing fewer than 0.2% of non-discriminators. A minimax decision rule\nacknowledging partial identification of the joint distribution of callback\nrates yields higher error rates but more investigations than our baseline\ntwo-type model. Our results suggest illegal labor market discrimination can be\nreliably monitored with relatively small modifications to existing audit\ndesigns.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 17:43:02 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 16:26:53 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Kline", "Patrick", ""], ["Walters", "Christopher", ""]]}, {"id": "1907.06932", "submitter": "Amanda Lenzi", "authors": "Amanda Lenzi, Stefano Castruccio, Haavard Rue and Marc G. Genton", "title": "Improving Bayesian Local Spatial Models in Large Data Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Environmental processes resolved at a sufficiently small scale in space and\ntime will inevitably display non-stationary behavior. Such processes are both\nchallenging to model and computationally expensive when the data size is large.\nInstead of modeling the global non-stationarity explicitly, local models can be\napplied to disjoint regions of the domain. The choice of the size of these\nregions is dictated by a bias-variance trade-off; large regions will have\nsmaller variance and larger bias, whereas small regions will have higher\nvariance and smaller bias. From both the modeling and computational point of\nview, small regions are preferable to better accommodate the non-stationarity.\nHowever, in practice, large regions are necessary to control the variance. We\npropose a novel Bayesian three-step approach that allows for smaller regions\nwithout compromising the increase of the variance that would follow. We are\nable to propagate the uncertainty from one step to the next without issues\ncaused by reusing the data. The improvement in inference also results in\nimproved prediction, as our simulated example shows. We illustrate this new\napproach on a data set of simulated high-resolution wind speed data over Saudi\nArabia.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 10:40:55 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2020 14:15:32 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Lenzi", "Amanda", ""], ["Castruccio", "Stefano", ""], ["Rue", "Haavard", ""], ["Genton", "Marc G.", ""]]}, {"id": "1907.06988", "submitter": "Vitalii Makogin", "authors": "Denis Dresvyanskiy and Tatiana Karaseva and Vitalii Makogin and Sergei\n  Mitrofanov and Claudia Redenbach and Evgeny Spodarev", "title": "Detecting anomalies in fibre systems using 3-dimensional image data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of detecting anomalies in the directional\ndistribution of fibre materials observed in 3D images. We divide the image into\na set of scanning windows and classify them into two clusters: homogeneous\nmaterial and anomaly. Based on a sample of estimated local fibre directions,\nfor each scanning window we compute several classification attributes, namely\nthe coordinate wise means of local fibre directions, the entropy of the\ndirectional distribution, and a combination of them. We also propose a new\nspatial modification of the Stochastic Approximation Expectation-Maximization\n(SAEM) algorithm. Besides the clustering we also consider testing the\nsignificance of anomalies. To this end, we apply a change point technique for\nrandom fields and derive the exact inequalities for tail probabilities of a\ntest statistics. The proposed methodology is first validated on simulated\nimages. Finally, it is applied to a 3D image of a fibre reinforced polymer.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 13:36:10 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Dresvyanskiy", "Denis", ""], ["Karaseva", "Tatiana", ""], ["Makogin", "Vitalii", ""], ["Mitrofanov", "Sergei", ""], ["Redenbach", "Claudia", ""], ["Spodarev", "Evgeny", ""]]}, {"id": "1907.06994", "submitter": "Faicel Chamroukhi", "authors": "Bao Tuyen Huynh and Faicel Chamroukhi", "title": "Estimation and Feature Selection in Mixtures of Generalized Linear\n  Experts Models", "comments": "arXiv admin note: text overlap with arXiv:1810.12161", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixtures-of-Experts (MoE) are conditional mixture models that have shown\ntheir performance in modeling heterogeneity in data in many statistical\nlearning approaches for prediction, including regression and classification, as\nwell as for clustering. Their estimation in high-dimensional problems is still\nhowever challenging. We consider the problem of parameter estimation and\nfeature selection in MoE models with different generalized linear experts\nmodels, and propose a regularized maximum likelihood estimation that\nefficiently encourages sparse solutions for heterogeneous data with\nhigh-dimensional predictors. The developed proximal-Newton EM algorithm\nincludes proximal Newton-type procedures to update the model parameter by\nmonotonically maximizing the objective function and allows to perform efficient\nestimation and feature selection. An experimental study shows the good\nperformance of the algorithms in terms of recovering the actual sparse\nsolutions, parameter estimation, and clustering of heterogeneous regression\ndata, compared to the main state-of-the art competitors.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2019 10:58:31 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Huynh", "Bao Tuyen", ""], ["Chamroukhi", "Faicel", ""]]}, {"id": "1907.07015", "submitter": "Rose Baker", "authors": "Rose Baker", "title": "Outliers in meta-analysis: an asymmetric trimmed-mean approach", "comments": "10 pages, 4 tables, no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The adaptive asymmetric trimmed mean is a known way of estimating central\nlocation, usually in conjunction with the bootstrap. It is here modified and\napplied to meta-analysis, as a way of dealing with outlying results by\ndown-weighting the corresponding studies. This requires a modified bootstrap\nand a method of down-weighting studies, as opposed to removing single\nobservations. This methodology is shown in analysis of some well-travelled\ndatasets to down-weight outliers in agreement with other methods, and\nMonte-Carlo studies show that it does does not appreciably down-weight studies\nwhen outliers are absent. Conceptually simple, it does not make parametric\nassumptions about the outliers.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 14:04:42 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Baker", "Rose", ""]]}, {"id": "1907.07439", "submitter": "Andriy Olenko", "authors": "Daniel Fryer, Andriy Olenko", "title": "Spherical data handling and analysis with R package rcosmo", "comments": "15 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The R package rcosmo was developed for handling and analysing Hierarchical\nEqual Area isoLatitude Pixelation(HEALPix) and Cosmic Microwave Background(CMB)\nradiation data. It has more than 100 functions. rcosmo was initially developed\nfor CMB, but also can be used for other spherical data. This paper discusses\ntransformations into rcosmo formats and handling of three types of non-CMB\ndata: continuous geographic, point pattern and star-shaped. For each type of\ndata we provide a brief description of the corresponding statistical model,\ndata example and ready-to-use R code. Some statistical functionality of rcosmo\nis demonstrated for the example data converted into the HEALPix format. The\npaper can serve as the first practical guideline to transforming data into the\nHEALPix format and statistical analysis with rcosmo for geo-statisticians, GIS\nand R users and researches dealing with spherical data in non-HEALPix formats.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 11:15:46 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Fryer", "Daniel", ""], ["Olenko", "Andriy", ""]]}, {"id": "1907.07476", "submitter": "Binod Kharel", "authors": "Binod Kharel, Onel L. Alcaraz L\\'opez, Hirley Alves, Matti Latva-aho", "title": "Achieving Ultra-Reliable Communication via CRAN-Enabled Diversity\n  Schemes", "comments": "5 pages,6 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet of things is in progress to build a smart society, and wireless\nnetworks are critical enablers for many of its use cases. In this paper, we\npresent a multi-coordinated transmission scheme to achieve ultra-reliability\nfor critical machine-type wireless communication networks. We take advantage of\ndiversity, which is fundamental for dealing with fading channel impairments,\nand for achieving ultra-reliable region of operation in order of five 9's as\ndefined by 3GPP standardization bodies. We evaluate an interference-limited\nnetwork composed of multiple remote radio heads that are allowed to cooperate,\nby keeping silence thus reducing interference, or by performing more elaborated\nstrategies such as maximal ratio transmission, in order to serve a user\nequipment with ultra-reliability. We provide extensive numerical analysis and\ndiscuss the gains of cooperation by the centralized radio access network.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 12:38:04 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Kharel", "Binod", ""], ["L\u00f3pez", "Onel L. Alcaraz", ""], ["Alves", "Hirley", ""], ["Latva-aho", "Matti", ""]]}, {"id": "1907.07514", "submitter": "Peter Cotton", "authors": "Peter Cotton", "title": "Self Organizing Supply Chains for Micro-Prediction: Present and Future\n  uses of the ROAR Protocol", "comments": "Thirty-sixth International Conference on Machine Learning Workshop on\n  AI in Finance: Applications and Infrastructure for Multi-Agent Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY q-fin.ST stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multi-agent system is trialed as a means of crowd-sourcing inexpensive but\nhigh quality streams of predictions. Each agent is a microservice embodying\nstatistical models and endowed with economic self-interest. The ability to fork\nand modify simple agents is granted to a large number of employees in a firm\nand empirical lessons are reported. We suggest that one plausible trajectory\nfor this project is the creation of a Prediction Web.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 13:40:15 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Cotton", "Peter", ""]]}, {"id": "1907.07523", "submitter": "Anne Sabourin", "authors": "Ma\\\"el Chiapino (LTCI), St\\'ephan Cl\\'emen\\c{c}on (LTCI), Vincent\n  Feuillard, Anne Sabourin (LTCI)", "title": "A Multivariate Extreme Value Theory Approach to Anomaly Clustering and\n  Visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a wide variety of situations, anomalies in the behaviour of a complex\nsystem, whose health is monitored through the observation of a random vector X\n= (X1,. .. , X d) valued in R d , correspond to the simultaneous occurrence of\nextreme values for certain subgroups $\\alpha$ $\\subset$ {1,. .. , d} of\nvariables Xj. Under the heavy-tail assumption, which is precisely appropriate\nfor modeling these phenomena, statistical methods relying on multivariate\nextreme value theory have been developed in the past few years for identifying\nsuch events/subgroups. This paper exploits this approach much further by means\nof a novel mixture model that permits to describe the distribution of extremal\nobservations and where the anomaly type $\\alpha$ is viewed as a latent\nvariable. One may then take advantage of the model by assigning to any extreme\npoint a posterior probability for each anomaly type $\\alpha$, defining\nimplicitly a similarity measure between anomalies. It is explained at length\nhow the latter permits to cluster extreme observations and obtain an\ninformative planar representation of anomalies using standard graph-mining\ntools. The relevance and usefulness of the clustering and 2-d visual display\nthus designed is illustrated on simulated datasets and on real observations as\nwell, in the aeronautics application domain.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 13:48:58 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Chiapino", "Ma\u00ebl", "", "LTCI"], ["Cl\u00e9men\u00e7on", "St\u00e9phan", "", "LTCI"], ["Feuillard", "Vincent", "", "LTCI"], ["Sabourin", "Anne", "", "LTCI"]]}, {"id": "1907.07548", "submitter": "Santosh Kumar", "authors": "Ayana Sarkar, Manuja Kothiyal, Santosh Kumar", "title": "Distribution of the ratio of two consecutive level spacings in\n  orthogonal to unitary crossover ensembles", "comments": "Published version", "journal-ref": "Phys. Rev. E 101, 012216 (2020)", "doi": "10.1103/PhysRevE.101.012216", "report-no": null, "categories": "math-ph math.MP physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ratio of two consecutive level spacings has emerged as a very useful\nmetric in investigating universal features exhibited by complex spectra. It\ndoes not require the knowledge of density of states and is therefore quite\nconvenient to compute in analyzing the spectrum of a general system. The\nWigner-surmise-like results for the ratio distribution are known for the\ninvariant classes of Gaussian random matrices. However, for the crossover\nensembles, which are useful in modeling systems with partially broken\nsymmetries, corresponding results have remained unavailable so far. In this\nwork, we derive exact results for the distribution and average of the ratio of\ntwo consecutive level spacings in the Gaussian orthogonal to unitary crossover\nensemble using a $3\\times 3$ random matrix model. This crossover is useful in\nmodeling time-reversal symmetry breaking in quantum chaotic systems. Although\nbased on a $3\\times 3$ matrix model, our results can also be applied in the\nstudy of large spectra, provided the symmetry-breaking parameter facilitating\nthe crossover is suitably scaled. We substantiate this claim by considering\nGaussian and Laguerre crossover ensembles comprising large matrices. Moreover,\nwe apply our result to investigate the violation of time-reversal invariance in\nthe quantum kicked rotor system.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 14:38:26 GMT"}, {"version": "v2", "created": "Sun, 2 Feb 2020 16:21:36 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Sarkar", "Ayana", ""], ["Kothiyal", "Manuja", ""], ["Kumar", "Santosh", ""]]}, {"id": "1907.07552", "submitter": "Themistoklis Sapsis", "authors": "Themistoklis P. Sapsis", "title": "Output-weighted optimal sampling for Bayesian regression and rare event\n  statistics using few samples", "comments": "34 pages; 13 figures", "journal-ref": null, "doi": "10.1098/rspa.2019.0834", "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many important problems the quantity of interest is an unknown function\nof the parameters, which is a random vector with known statistics. Since the\ndependence of the output on this random vector is unknown, the challenge is to\nidentify its statistics, using the minimum number of function evaluations. This\nproblem can been seen in the context of active learning or optimal experimental\ndesign. We employ Bayesian regression to represent the derived model\nuncertainty due to finite and small number of input-output pairs. In this\ncontext we evaluate existing methods for optimal sample selection, such as\nmodel error minimization and mutual information maximization. We show that for\nthe case of known output variance, the commonly employed criteria in the\nliterature do not take into account the output values of the existing\ninput-output pairs, while for the case of unknown output variance this\ndependence can be very weak. We introduce a criterion that takes into account\nthe values of the output for the existing samples and adaptively selects inputs\nfrom regions of the parameter space which have important contribution to the\noutput. The new method allows for application to high-dimensional inputs,\npaving the way for optimal experimental design in high-dimensions.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 14:51:11 GMT"}, {"version": "v2", "created": "Sat, 30 Nov 2019 16:14:43 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Sapsis", "Themistoklis P.", ""]]}, {"id": "1907.07580", "submitter": "Juan M. Morales Dr.", "authors": "Miguel \\'A. Mu\\~noz, Juan M. Morales and Salvador Pineda", "title": "Feature-driven Improvement of Renewable Energy Forecasting and Trading", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired from recent insights into the common ground of machine learning,\noptimization and decision-making, this paper proposes an easy-to-implement, but\neffective procedure to enhance both the quality of renewable energy forecasts\nand the competitive edge of renewable energy producers in electricity markets\nwith a dual-price settlement of imbalances. The quality and economic gains\nbrought by the proposed procedure essentially stem from the utilization of\nvaluable predictors (also known as features) in a data-driven newsvendor model\nthat renders a computationally inexpensive linear program. We illustrate the\nproposed procedure and numerically assess its benefits on a realistic case\nstudy that considers the aggregate wind power production in the Danish DK1\nbidding zone as the variable to be predicted and traded. Within this context,\nour procedure leverages, among others, spatial information in the form of wind\npower forecasts issued by transmission system operators (TSO) in surrounding\nbidding zones and publicly available in online platforms. We show that our\nmethod is able to improve the quality of the wind power forecast issued by the\nDanish TSO by several percentage points (when measured in terms of the mean\nabsolute or the root mean square error) and to significantly reduce the\nbalancing costs incurred by the wind power producer.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 15:23:32 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2019 12:01:41 GMT"}, {"version": "v3", "created": "Thu, 16 Jan 2020 17:51:06 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Mu\u00f1oz", "Miguel \u00c1.", ""], ["Morales", "Juan M.", ""], ["Pineda", "Salvador", ""]]}, {"id": "1907.07603", "submitter": "Renjie Chen", "authors": "Renjie Chen, Jingyue Zhang, Nalini Ravishanker, Karthik Konduri", "title": "Clustering Activity-Travel Behavior Time Series using Topological Data\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Over the last few years, traffic data has been exploding and the\ntransportation discipline has entered the era of big data. It brings out new\nopportunities for doing data-driven analysis, but it also challenges\ntraditional analytic methods. This paper proposes a new Divide and Combine\nbased approach to do K means clustering on activity-travel behavior time series\nusing features that are derived using tools in Time Series Analysis and\nTopological Data Analysis. Clustering data from five waves of the National\nHousehold Travel Survey ranging from 1990 to 2017 suggests that activity-travel\npatterns of individuals over the last three decades can be grouped into three\nclusters. Results also provide evidence in support of recent claims about\ndifferences in activity-travel patterns of different survey cohorts. The\nproposed method is generally applicable and is not limited only to\nactivity-travel behavior analysis in transportation studies. Driving behavior,\ntravel mode choice, household vehicle ownership, when being characterized as\ncategorical time series, can all be analyzed using the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 16:05:37 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Chen", "Renjie", ""], ["Zhang", "Jingyue", ""], ["Ravishanker", "Nalini", ""], ["Konduri", "Karthik", ""]]}, {"id": "1907.07669", "submitter": "Faezeh Movahedi", "authors": "Faezeh Movahedi, Robert L. Kormos, Lisa Lohmueller, Laura Seese,\n  Manreet Kanwar, Srinivas Murali, Yiye Zhang, Rema Padman, James F. Antaki", "title": "Sequential Pattern mining of Longitudinal Adverse Events After Left\n  Ventricular Assist Device Implant", "comments": "\"under review\" IEEE Journal of Biomedical and Health Informatics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Left ventricular assist devices (LVADs) are an increasingly common therapy\nfor patients with advanced heart failure. However, implantation of the LVAD\nincreases the risk of stroke, infection, bleeding, and other serious adverse\nevents (AEs). Most post-LVAD AEs studies have focused on individual AEs in\nisolation, neglecting the possible interrelation, or causality between AEs.\nThis study is the first to conduct an exploratory analysis to discover common\nsequential chains of AEs following LVAD implantation that are correlated with\nimportant clinical outcomes. This analysis was derived from 58,575 recorded AEs\nfor 13,192 patients in International Registry for Mechanical Circulatory\nSupport (INTERMACS) who received a continuousflow LVAD between 2006 and 2015.\nThe pattern mining procedure involved three main steps: (1) creating a bank of\nAE sequences by converting the AEs for each patient into a single,\nchronologically sequenced record, (2) grouping patients with similar AE\nsequences using hierarchical clustering, and (3) extracting temporal chains of\nAEs for each group of patients using Markov modeling. The mined results\nindicate the existence of seven groups of sequential chains of AEs,\ncharacterized by common types of AEs that occurred in a unique order. The\ngroups were identified as: GRP1: Recurrent bleeding, GRP2: Trajectory of device\nmalfunction & explant, GRP3: Infection, GRP4: Trajectories to transplant, GRP5:\nCardiac arrhythmia, GRP6: Trajectory of neurological dysfunction & death, and\nGRP7: Trajectory of respiratory failure, renal dysfunction & death. These\npatterns of sequential post-LVAD AEs disclose potential interdependence between\nAEs and may aid prediction, and prevention, of subsequent AEs in future\nstudies.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 18:47:18 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Movahedi", "Faezeh", ""], ["Kormos", "Robert L.", ""], ["Lohmueller", "Lisa", ""], ["Seese", "Laura", ""], ["Kanwar", "Manreet", ""], ["Murali", "Srinivas", ""], ["Zhang", "Yiye", ""], ["Padman", "Rema", ""], ["Antaki", "James F.", ""]]}, {"id": "1907.07789", "submitter": "Momiao Xiong", "authors": "Rong Jiao, Xiangning Chen, Eric Boerwinkle and Momiao Xiong", "title": "Genome-wide Causation Studies of Complex Diseases", "comments": "61 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant progress in dissecting the genetic architecture of\ncomplex diseases by genome-wide association studies (GWAS), the signals\nidentified by association analysis may not have specific pathological relevance\nto diseases so that a large fraction of disease causing genetic variants is\nstill hidden. Association is used to measure dependence between two variables\nor two sets of variables. Genome-wide association studies test association\nbetween a disease and SNPs (or other genetic variants) across the genome.\nAssociation analysis may detect superficial patterns between disease and\ngenetic variants. Association signals provide limited information on the causal\nmechanism of diseases. The use of association analysis as a major analytical\nplatform for genetic studies of complex diseases is a key issue that hampers\ndiscovery of the mechanism of diseases, calling into question the ability of\nGWAS to identify loci underlying diseases. It is time to move beyond\nassociation analysis toward techniques enabling the discovery of the underlying\ncausal genetic strctures of complex diseases. To achieve this, we propose a\nconcept of a genome-wide causation studies (GWCS) as an alternative to GWAS and\ndevelop additive noise models (ANMs) for genetic causation analysis. Type I\nerror rates and power of the ANMs to test for causation are presented. We\nconduct GWCS of schizophrenia. Both simulation and real data analysis show that\nthe proportion of the overlapped association and causation signals is small.\nThus, we hope that our analysis will stimulate discussion of GWAS and GWCS.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 22:01:16 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Jiao", "Rong", ""], ["Chen", "Xiangning", ""], ["Boerwinkle", "Eric", ""], ["Xiong", "Momiao", ""]]}, {"id": "1907.07809", "submitter": "Lu Xia", "authors": "Lu Xia, Kevin He, Yanming Li, John D. Kalbfleisch", "title": "Accounting for total variation and robustness in profiling health care\n  providers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monitoring outcomes of health care providers, such as patient deaths,\nhospitalizations and hospital readmissions, helps in assessing the quality of\nhealth care. We consider a large database on patients being treated at dialysis\nfacilities in the United States, and the problem of identifying facilities with\noutcomes that are better than or worse than expected. Analyses of such data\nhave been commonly based on random or fixed facility effects, which have\nshortcomings that can lead to unfair assessments. A primary issue is that they\ndo not appropriately account for variation between providers that is outside\nthe providers' control due, for example, to unobserved patient characteristics\nthat vary between providers. In this article, we propose a smoothed empirical\nnull approach that accounts for the total variation and adapts to different\nprovider sizes. The linear model provides an illustration that extends easily\nto other nonlinear models for survival or binary outcomes, for example. The\nempirical null method is generalized to allow for some variation being due to\nquality of care. These methods are examined with numerical simulations and\napplied to the monitoring of survival in the dialysis facility data.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 23:34:52 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 05:40:23 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Xia", "Lu", ""], ["He", "Kevin", ""], ["Li", "Yanming", ""], ["Kalbfleisch", "John D.", ""]]}, {"id": "1907.07813", "submitter": "Andrew Zammit-Mangion", "authors": "Andrew Zammit-Mangion and Jonathan Rougier", "title": "Multi-Scale Process Modelling and Distributed Computation for Spatial\n  Data", "comments": "33 pages, 10 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen a huge development in spatial modelling and prediction\nmethodology, driven by the increased availability of remote-sensing data and\nthe reduced cost of distributed-processing technology. It is well known that\nmodelling and prediction using infinite-dimensional process models is not\npossible with large data sets, and that both approximate models and, often,\napproximate-inference methods, are needed. The problem of fitting simple global\nspatial models to large data sets has been solved through the likes of\nmulti-resolution approximations and nearest-neighbour techniques. Here we\ntackle the next challenge, that of fitting complex, nonstationary, multi-scale\nmodels to large data sets. We propose doing this through the use of\nsuperpositions of spatial processes with increasing spatial scale and\nincreasing degrees of nonstationarity. Computation is facilitated through the\nuse of Gaussian Markov random fields and parallel Markov chain Monte Carlo\nbased on graph colouring. The resulting model allows for both distributed\ncomputing and distributed data. Importantly, it provides opportunities for\ngenuine model and data scaleability and yet is still able to borrow strength\nacross large spatial scales. We illustrate a two-scale version on a data set of\nsea-surface temperature containing on the order of one million observations,\nand compare our approach to state-of-the-art spatial modelling and prediction\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 23:43:05 GMT"}, {"version": "v2", "created": "Sun, 16 Feb 2020 22:09:28 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Zammit-Mangion", "Andrew", ""], ["Rougier", "Jonathan", ""]]}, {"id": "1907.07850", "submitter": "Dilanka Shenal Dedduwakumara", "authors": "Dilanka S. Dedduwakumara, Luke A. Prendergast", "title": "Interval estimators for inequality measures using grouped data", "comments": "17 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Income inequality measures are often used as an indication of economic\nhealth. How to obtain reliable confidence intervals for these measures based on\nsampled data has been studied extensively in recent years. To preserve\nconfidentiality, income data is often made available in summary form only (i.e.\nhistograms, frequencies between quintiles, etc.). In this paper, we show that\ngood coverage can be achieved for bootstrap and Wald-type intervals for\nquantile-based measures when only grouped (binned) data are available. These\ncoverages are typically superior to those that we have been able to achieve for\nintervals for popular measures such as the Gini index in this grouped data\nsetting. To facilitate the bootstrapping, we use the Generalized Lambda\nDistribution and also a linear interpolation approximation method to\napproximate the underlying density. The latter is possible when groups means\nare available. We also apply our methods to real data sets.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 02:52:17 GMT"}, {"version": "v2", "created": "Fri, 19 Jul 2019 03:42:13 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Dedduwakumara", "Dilanka S.", ""], ["Prendergast", "Luke A.", ""]]}, {"id": "1907.08114", "submitter": "Gerd B\\\"urger", "authors": "Gerd B\\\"urger", "title": "Artificial skill in monsoon onset prediction: two recent examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For two cases of empirical monsoon onset prediction it is argued that current\nverification practice leads to optimistically biased skill, caused by the\nintricacy of the model setup. For the case of the operational forecasts by the\nIndian Meteorological Department (IMD) it leads to an overlap of model\ndefinition and verification data. A more seriously flawed verification was used\nin a recent method based on trend extrapolations of 'tipping elements' (TE).\nClaims of TE of predicting onset 2 weeks earlier than other methods are\nunjustified. On the contrary, the correlation between TE forecasts and\nobservations is as low as 0.24 and compares poorly to the reported IMD\ncorrelation of 0.78. That latter value likely being artificially inflated,\ncurrently the best and most reliable monsoon onset predictions come from a\ndynamical model with more reliable skill values of about 0.7.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 15:32:58 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["B\u00fcrger", "Gerd", ""]]}, {"id": "1907.08237", "submitter": "Huijing Jiang", "authors": "Ta-Hsin Li, Huijing Jiang, Kevin Tran, Gigi Yuen-Reed, Bob Kelley,\n  Thomas Halvorson", "title": "A Systematic Approach to Detect Hierarchical Healthcare Cost Drivers and\n  Interpretable Change Patterns", "comments": "2019 KDD DS Healthcare workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is strong interest among payers to identify emerging healthcare cost\ndrivers to support early intervention. However, many challenges arise in\nanalyzing large, high dimensional, and noisy healthcare data. In this paper, we\npropose a systematic approach that utilizes hierarchical and multi-resolution\nsearch strategies using enhanced statistical process control (SPC) algorithms\nto surface high impact cost drivers. Our approach aims to provide\ninterpretable, detailed, and actionable insights of detected change patterns\nattributing to multiple demographic and clinical factors. We also proposed an\nalgorithm to identify comparable treatment offsets at the population level and\nquantify the cost impact on their utilization changes.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 18:32:32 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Li", "Ta-Hsin", ""], ["Jiang", "Huijing", ""], ["Tran", "Kevin", ""], ["Yuen-Reed", "Gigi", ""], ["Kelley", "Bob", ""], ["Halvorson", "Thomas", ""]]}, {"id": "1907.08242", "submitter": "Zad Rafi", "authors": "Zad Rafi", "title": "Misplaced Confidence in Observed Power", "comments": "1 page; 0 figures; 7 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recently published randomized controlled trial in JAMA investigated the\nimpact of the selective serotonin reuptake inhibitor, escitalopram, on the risk\nof major adverse events (MACE). The authors estimated a hazard ratio (HR) of\n0.69 (95% CI: 0.49, 0.96; $p$ = 0.03) and then attempted to calculate how much\nstatistical power their study (test) had attained, and used this measure to\nassess how reliable their results were. Here, we discuss why this approach,\nalong with other post-hoc power analyses, are highly misleading.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 18:46:24 GMT"}, {"version": "v2", "created": "Tue, 13 Aug 2019 23:17:45 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2020 06:55:15 GMT"}, {"version": "v4", "created": "Fri, 2 Oct 2020 01:17:11 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Rafi", "Zad", ""]]}, {"id": "1907.08245", "submitter": "Leonardo Bottolo", "authors": "Angelos Alexopoulos and Leonardo Bottolo", "title": "Bayesian Variable Selection for Gaussian copula regression models", "comments": "39 pages main paper and 21 pages Supplementary Material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop a novel Bayesian method to select important predictors in\nregression models with multiple responses of diverse types. A sparse Gaussian\ncopula regression model is used to account for the multivariate dependencies\nbetween any combination of discrete and/or continuous responses and their\nassociation with a set of predictors. We utilize the parameter expansion for\ndata augmentation strategy to construct a Markov chain Monte Carlo algorithm\nfor the estimation of the parameters and the latent variables of the model.\nBased on a centered parametrization of the Gaussian latent variables, we design\na fixed-dimensional proposal distribution to update jointly the latent binary\nvectors of important predictors and the corresponding non-zero regression\ncoefficients. For Gaussian responses and for outcomes that can be modeled as a\ndependent version of a Gaussian response, this proposal leads to a\nMetropolis-Hastings step that allows an efficient exploration of the\npredictors' model space. The proposed strategy is tested on simulated data and\napplied to real data sets in which the responses consist of low-intensity\ncounts, binary, ordinal and continuous variables.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 18:55:21 GMT"}, {"version": "v2", "created": "Sun, 20 Sep 2020 17:23:00 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Alexopoulos", "Angelos", ""], ["Bottolo", "Leonardo", ""]]}, {"id": "1907.08260", "submitter": "Caroline Moosm\\\"uller", "authors": "Caroline Moosm\\\"uller, Felix Dietrich, Ioannis G. Kevrekidis", "title": "A geometric approach to the transport of discontinuous densities", "comments": "26 pages, 15 figures, updated funding information", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different observations of a relation between inputs (\"sources\") and outputs\n(\"targets\") are often reported in terms of histograms (discretizations of the\nsource and the target densities). Transporting these densities to each other\nprovides insight regarding the underlying relation. In (forward) uncertainty\nquantification, one typically studies how the distribution of inputs to a\nsystem affects the distribution of the system responses. Here, we focus on the\nidentification of the system (the transport map) itself, once the input and\noutput distributions are determined, and suggest a modification of current\npractice by including data from what we call \"an observation process\". We\nhypothesize that there exists a smooth manifold underlying the relation; the\nsources and the targets are then partial observations (possibly projections) of\nthis manifold. Knowledge of such a manifold implies knowledge of the relation,\nand thus of \"the right\" transport between source and target observations. When\nthe source-target observations are not bijective (when the manifold is not the\ngraph of a function over both observation spaces, either because folds over\nthem give rise to density singularities, or because it marginalizes over\nseveral observables), recovery of the manifold is obscured. Using ideas from\nattractor reconstruction in dynamical systems, we demonstrate how additional\ninformation in the form of short histories of an observation process can help\nus recover the underlying manifold. The types of additional information\nemployed and the relation to optimal transport based solely on density\nobservations is illustrated and discussed, along with limitations in the\nrecovery of the true underlying relation.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 19:33:25 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2019 16:55:30 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Moosm\u00fcller", "Caroline", ""], ["Dietrich", "Felix", ""], ["Kevrekidis", "Ioannis G.", ""]]}, {"id": "1907.08264", "submitter": "\\'Alvaro Ignacio Riquelme", "authors": "Alvaro I. Riquelme and Julian M. Ortiz", "title": "A general approach to the assessment of uncertainty in volumes by using\n  the multi-Gaussian model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this research is to derive an approach to assess uncertainty in\nan arbitrary volume conditioned by sampling data, without using geostatistical\nsimulation. We have accomplished this goal by deriving an numerical tool\nsuitable for any probabilistic distribution of the sample data. For this, we\nhave worked with an extension of the traditional multi-Gaussian model, allowing\nus to obtain a formulation that makes explicit the dependence of the\nuncertainty in the arbitrary volume from the grades within the volume, the\nspatial correlation of the data and the conditioning values. A Kriging of the\nGaussian values is the only requirement to obtain not only conditional local\nmeans and variances but also the complete local distributions at any support,\nin an easy and straightforward way.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 20:06:19 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Riquelme", "Alvaro I.", ""], ["Ortiz", "Julian M.", ""]]}, {"id": "1907.08387", "submitter": "Antonio Calcagn\\`i", "authors": "Antonio Calcagn\\`i, Luigi Lombardi, Marco D'Alessandro", "title": "A state space approach to dynamic modeling of mouse-tracking data", "comments": "The manuscript consists of 29 pages, 10 figures, and 5 tables. It\n  also contains Supplementary Materials (10 pages, 8 figures, and 3 tables)\n  providing extended results along with a simulation study", "journal-ref": "Frontiers in Psychology - Quantitative Psychology and Measurement,\n  2019, 10:2716", "doi": "10.3389/fpsyg.2019.02716", "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mouse-tracking recording techniques are becoming very attractive in\nexperimental psychology. They provide an effective means of enhancing the\nmeasurement of some real-time cognitive processes involved in categorization,\ndecision-making, and lexical decision tasks. Mouse-tracking data are commonly\nanalysed using a two-step procedure which first summarizes individuals' hand\ntrajectories with independent measures, and then applies standard statistical\nmodels on them. However, this approach can be problematic in many cases. In\nparticular, it does not provide a direct way to capitalize the richness of hand\nmovement variability within a consistent and unified representation. In this\narticle we present a novel, unified framework for mouse-tracking data. Unlike\nstandard approaches to mouse-tracking, our proposal uses stochastic state-space\nmodeling to represent the observed trajectories in terms of both individual\nmovement dynamics and experimental variables. The model is estimated via a\nMetropolis-Hastings algorithm coupled with a non-linear recursive filter. The\ncharacteristics and potentials of the proposed approach are illustrated using a\nlexical decision case study. The results highlighted how dynamic modeling of\nmouse-tracking data can considerably improve the analysis of mouse-tracking\ntasks and the conclusions researchers can draw from them.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 06:50:00 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2019 06:01:28 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Calcagn\u00ec", "Antonio", ""], ["Lombardi", "Luigi", ""], ["D'Alessandro", "Marco", ""]]}, {"id": "1907.08522", "submitter": "Martin Magris", "authors": "Martin Magris", "title": "A Vine-copula extension for the HAR model", "comments": "24 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The heterogeneous autoregressive (HAR) model is revised by modeling the joint\ndistribution of the four partial-volatility terms therein involved. Namely,\ntoday's, yesterday's, last week's and last month's volatility components. The\njoint distribution relies on a (C-) Vine copula construction, allowing to\nconveniently extract volatility forecasts based on the conditional expectation\nof today's volatility given its past terms. The proposed empirical application\ninvolves more than seven years of high-frequency transaction prices for ten\nstocks and evaluates the in-sample, out-of-sample and one-step-ahead forecast\nperformance of our model for daily realized-kernel measures. The model proposed\nin this paper is shown to outperform the HAR counterpart under different models\nfor marginal distributions, copula construction methods, and forecasting\nsettings.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 14:18:50 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Magris", "Martin", ""]]}, {"id": "1907.08566", "submitter": "Peter Tait A", "authors": "Peter A. Tait, Paul D. McNicholas and Joyce Obeid", "title": "Clustering Higher Order Data: An Application to Pediatric Multi-variable\n  Longitudinal Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physical activity levels are an important predictor of cardiovascular health\nand increasingly being measured by sensors, like accelerometers. Accelerometers\nproduce rich multivariate data that can inform important clinical decisions\nrelated to individual patients and public health. The CHAMPION study, a study\nof youth with chronic inflammatory conditions, aims to determine the links\nbetween heart health, inflammation, physical activity, and fitness. The\naccelerometer data from CHAMPION is represented as 4-dimensional arrays, and a\nfinite mixture of multidimensional arrays model is developed for clustering.\nThe use of model-based clustering for multidimensional arrays has thus far been\nlimited to two-dimensional arrays, i.e., matrices or order-two tensors, and the\nwork in this paper can also be seen as an approach for clustering D-dimensional\narrays for D > 2 or, in other words, for clustering order-D tensors.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 16:40:01 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 21:33:56 GMT"}, {"version": "v3", "created": "Fri, 4 Dec 2020 17:09:19 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Tait", "Peter A.", ""], ["McNicholas", "Paul D.", ""], ["Obeid", "Joyce", ""]]}, {"id": "1907.08733", "submitter": "Wenjie Zhao", "authors": "Wenjie Zhao, Raquel Prado", "title": "Efficient Bayesian PARCOR Approaches for Dynamic Modeling of\n  Multivariate Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Bayesian lattice filtering and smoothing approach is proposed for fast and\naccurate modeling and inference in multivariate non-stationary time series.\nThis approach offers computational feasibility and interpretable time-frequency\nanalysis in the multivariate context. The proposed framework allows us to\nobtain posterior estimates of the time-varying spectral densities of individual\ntime series components, as well as posterior measurements of the time-frequency\nrelationships across multiple components, such as time-varying coherence and\npartial coherence.\n  The proposed formulation considers multivariate dynamic linear models (MDLMs)\non the forward and backward time-varying partial autocorrelation coefficients\n(TV-VPARCOR). Computationally expensive schemes for posterior inference on the\nmultivariate dynamic PARCOR model are avoided using approximations in the MDLM\ncontext. Approximate inference on the corresponding time-varying vector\nautoregressive (TV-VAR) coefficients is obtained via Whittle's algorithm. A key\naspect of the proposed TV-VPARCOR representations is that they are of lower\ndimension, and therefore more efficient, than TV-VAR representations. The\nperformance of the TV-VPARCOR models is illustrated in simulation studies and\nin the analysis of multivariate non-stationary temporal data arising in\nneuroscience and environmental applications. Model performance is evaluated\nusing goodness-of-fit measurements in the time-frequency domain and also by\nassessing the quality of short-term forecasting.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2019 01:20:19 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Zhao", "Wenjie", ""], ["Prado", "Raquel", ""]]}, {"id": "1907.08738", "submitter": "Taehee Lee", "authors": "Taehee Lee, Lorraine E. Lisiecki, Devin Rand, Geoffrey Gebbie, Charles\n  E. Lawrence", "title": "Bayesian Inference Gaussian Process Multiproxy Alignment of Continuous\n  Signals (BIGMACS): Applications for Paleoceanography", "comments": "This article has been submitted to \"Bayesian Analysis\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We first introduce a novel profile-based alignment algorithm, the multiple\ncontinuous Signal Alignment algorithm with Gaussian Process Regression profiles\n(SA-GPR). SA-GPR addresses the limitations of currently available signal\nalignment methods by adopting a hybrid of the particle smoothing and\nMarkov-chain Monte Carlo (MCMC) algorithms to align signals, and by applying\nthe Gaussian process regression to construct profiles to be aligned\ncontinuously. SA-GPR shares all the strengths of the existing alignment\nalgorithms that depend on profiles but is more exact in the sense that profiles\ndo not need to be discretized as sequential bins. The uncertainty of\nperformance over the resolution of such bins is thereby eliminated. This\nmethodology produces alignments that are consistent, that regularize extreme\ncases, and that properly reflect the inherent uncertainty.\n  Then we extend SA-GPR to a specific problem in the field of paleoceanography\nwith a method called Bayesian Inference Gaussian Process Multiproxy Alignment\nof Continuous Signals (BIGMACS). The goal of BIGMACS is to infer continuous\nages for ocean sediment cores using two classes of age proxies: proxies that\nexplicitly return calendar ages (e.g., radiocarbon) and those used to\nsynchronize ages in multiple marine records (e.g., an oxygen isotope based\nmarine proxy known as benthic ${\\delta}^{18}{\\rm O}$). BIGMACS integrates these\ntwo proxies by iteratively performing two steps: profile construction from\nbenthic ${\\delta}^{18}{\\rm O}$ age models and alignment of each core to the\nprofile also reflecting radiocarbon dates. We use BIGMACS to construct a new\nDeep Northeastern Atlantic stack (i.e., a profile from a particular benthic\n${\\delta}^{18}{\\rm O}$ records) of five ocean sediment cores. We conclude by\nconstructing multiproxy age models for two additional cores from the same\nregion by aligning them to the stack.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2019 02:44:00 GMT"}, {"version": "v2", "created": "Tue, 24 Dec 2019 05:25:07 GMT"}, {"version": "v3", "created": "Fri, 27 Dec 2019 05:56:21 GMT"}, {"version": "v4", "created": "Sun, 13 Jun 2021 09:11:58 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Lee", "Taehee", ""], ["Lisiecki", "Lorraine E.", ""], ["Rand", "Devin", ""], ["Gebbie", "Geoffrey", ""], ["Lawrence", "Charles E.", ""]]}, {"id": "1907.09007", "submitter": "Issa Annamoradnejad", "authors": "Issa Annamoradnejad, Jafar Habibi", "title": "A Comprehensive Analysis of Twitter Trending Topics", "comments": "6 pages, 8 figures, 3 tables, conference paper", "journal-ref": "2019 5th International Conference on Web Research (ICWR), Tehran,\n  Iran, 2019, pp. 22-27", "doi": "10.1109/ICWR.2019.8765252", "report-no": null, "categories": "cs.SI cs.CL cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Twitter, a name, phrase, or topic that is mentioned at a greater rate than\nothers is called a \"trending topic\" or simply \"trend\". Twitter trends list has\na powerful ability to promote public events such as natural events, political\nscandals, market changes and other types of breaking news. Nevertheless, there\nhave been very few works focused on the dynamics of these trending topics. In\nthis article, we thoroughly examined the Twitter's trending topics of 2018. To\nthis end, we automatically accessed Twitter's trends API and stored the\nresulting 50 top trending topics in a novel dataset. We propose and analyze our\ndataset according to six criteria: lexical analysis, time to reach, trend\nreoccurrence, trending time, tweets count, and language analysis. Based on our\nresults, 77.6% of the topics that reached the Top-10 list were trending with\nless than 100k tweets. More than 50% of the topics could not hold the position\nfor more than an hour. English and Arabic languages comprised close to 40% and\n20% of the first rank topics, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2019 17:07:07 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2020 12:31:30 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Annamoradnejad", "Issa", ""], ["Habibi", "Jafar", ""]]}, {"id": "1907.09043", "submitter": "Ernesto Pasten-Zapata", "authors": "Ernesto Pasten-Zapata, Julie Jones, Helen Moggridge, Martin Widmann", "title": "Evaluation of the performance of Euro-CORDEX RCMs for assessing\n  hydrological climate change impacts in Great Britain: a comparison of\n  different spatial resolutions and quantile mapping bias correction methods", "comments": "48 pages, 13 figures Submitted to Journal of Hydrology", "journal-ref": null, "doi": "10.1016/j.jhydrol.2020.124653", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regional Climate Models (RCMs) are an essential tool for analysing regional\nclimate change impacts as they provide simulations with more small-scale\ndetails and expected smaller errors than global climate models. There has been\nmuch effort to increase the spatial resolution and simulation skill of RCMs,\nyet the extent to which this improves the projection of hydrological change is\nunclear. Here, we evaluate the skill of five reanalysis-driven Euro-CORDEX RCMs\nin simulating precipitation and temperature, and as drivers of a hydrological\nmodel to simulate river flow on four UK catchments covering different physical,\nclimatic and hydrological characteristics. We test whether high-resolution RCMs\nprovide added value, through analysis of two RCM resolutions, 50 km and 12.5\nkm, which are also bias-corrected employing the parametric quantile-mapping\n(QM) method, using the normal distribution for temperature, and the Gamma (GQM)\nand Double Gamma (DGQM) distributions for precipitation. In a small catchment\nwith complex topography, the 12.5 km RCMs outperform their 50 km version for\nprecipitation and temperature, but when used in combination with the\nhydrological model, fail to capture the observed river flow distribution. In\nthe other (larger) catchments, only one high-resolution RCM consistently\noutperforms its low-resolution version, implying that in general there is no\nadded value from using the high-resolution RCMs in those catchments. GQM\ndecreases most of the simulation biases, except for extreme precipitation and\nhigh flows, which are further decreased by DGQM. Bias correction does not\nimprove the representation of daily temporal variability, but it does for\nmonthly variability, in particular when applying DGQM. Overall, an increase in\nRCM resolution does not imply a better simulation of hydrology and\nbias-correction represents an alternative to ease decision-making.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2019 22:09:29 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Pasten-Zapata", "Ernesto", ""], ["Jones", "Julie", ""], ["Moggridge", "Helen", ""], ["Widmann", "Martin", ""]]}, {"id": "1907.09192", "submitter": "Laure Sansonnet", "authors": "C. Denis, E. Lebarbier, C. L\\'evy-Leduc, O. Martin, L. Sansonnet", "title": "A novel regularized approach for functional data clustering: An\n  application to milking kinetics in dairy goats", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by an application to the clustering of milking kinetics of dairy\ngoats, we propose in this paper a novel approach for functional data\nclustering. This issue is of growing interest in precision livestock farming\nthat has been largely based on the development of data acquisition automation\nand on the development of interpretative tools to capitalize on high-throughput\nraw data and to generate benchmarks for phenotypic traits. The method that we\npropose in this paper falls in this context. Our methodology relies on a\npiecewise linear estimation of curves based on a novel regularized change-point\nestimation method and on the k-means algorithm applied to a vector of\ncoefficients summarizing the curves. The statistical performance of our method\nis assessed through numerical experiments and is thoroughly compared with\nexisting ones. Our technique is finally applied to milk emission kinetics data\nwith the aim of a better characterization of inter-animal variability and\ntoward a better understanding of the lactation process.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 09:01:59 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Denis", "C.", ""], ["Lebarbier", "E.", ""], ["L\u00e9vy-Leduc", "C.", ""], ["Martin", "O.", ""], ["Sansonnet", "L.", ""]]}, {"id": "1907.09204", "submitter": "Gabriel Michau Dr.", "authors": "Gabriel Michau and Olga Fink", "title": "Domain Adaptation for One-Class Classification: Monitoring the Health of\n  Critical Systems Under Limited Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The failure of a complex and safety critical industrial asset can have\nextremely high consequences. Close monitoring for early detection of abnormal\nsystem conditions is therefore required. Data-driven solutions to this problem\nhave been limited for two reasons: First, safety critical assets are designed\nand maintained to be highly reliable and faults are rare. Fault detection can\nthus not be solved with supervised learning. Second, complex industrial systems\nusually have long lifetime during which they face very different operating\nconditions. In the early life of the system, the collected data is probably not\nrepresentative of future operating conditions, making it challenging to train a\nrobust model.\n  In this paper, we propose a methodology to monitor the systems in their early\nlife. To do so, we enhance the training dataset with other units from a fleet,\nfor which longer observations are available. Since each unit has its own\nspecificity, we propose to extract features made independent of their origin by\nthree unsupervised feature alignment techniques. First, using a variational\nencoder, we impose a shared probabilistic encoder/decoder for both units.\nSecond, we introduce a new loss designed to conserve inter-point spacial\nrelationships between the input and the learned features. Last, we propose to\ntrain in an adversarial manner a discriminator on the origin of the features.\nOnce aligned, the features are fed to a one-class classifier to monitor the\nhealth of the system. By exploring the different combinations of the proposed\nalignment strategies, and by testing them on a real case study, a fleet\ncomposed of 112 power plants operated in different geographical locations and\nunder very different operating regimes, we demonstrate that this alignment is\nnecessary and beneficial.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 09:49:50 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2019 07:24:00 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Michau", "Gabriel", ""], ["Fink", "Olga", ""]]}, {"id": "1907.09435", "submitter": "L\\'aszl\\'o N\\'emeth", "authors": "L\\'aszl\\'o N\\'emeth, Zuzana H\\\"ubnerov\\'a, Andr\\'as Zempl\\'eni", "title": "Trend detection in GEV models", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent environmental studies extreme events have a great impact. The\nyearly and monthly maxima of environment related indices can be analysed by the\ntools of extreme value theory. For instance, the monthly maxima of the fire\nweather index in British Columbian forests might be modelled by GEV\ndistribution, but the stationarity of the time series is questionable. This\nproperty can lead us to different approaches to test if there is a significant\ntrend in past few years data or not. An approach is a likelihood ratio based\nprocedure which has favourable asymptotic properties, but for realistic sample\nsizes it might have a large error. In this paper we analyse the properties of\nthe likelihood ratio test for extremes by bootstrap simulations and aim to\ndetermine a minimal required sample size. With the theoretical results we\nre-asses the trends of fire weather index in British Columbian forests.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 17:20:10 GMT"}, {"version": "v2", "created": "Sat, 24 Aug 2019 09:38:20 GMT"}, {"version": "v3", "created": "Thu, 19 Mar 2020 07:00:05 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["N\u00e9meth", "L\u00e1szl\u00f3", ""], ["H\u00fcbnerov\u00e1", "Zuzana", ""], ["Zempl\u00e9ni", "Andr\u00e1s", ""]]}, {"id": "1907.09474", "submitter": "Vicent Blanes-Selva", "authors": "Vicent Blanes-Selva, Vicente Ruiz-Garc\\'ia, Salvador Tortajada,\n  Jos\\'e-Miguel Bened\\'i, Bernardo Valdivieso, Juan M. Garc\\'ia-G\\'omez", "title": "Design of one-year mortality forecast at hospital admission based: a\n  machine learning approach", "comments": null, "journal-ref": null, "doi": "10.1177/1460458220987580", "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Palliative care is referred to a set of programs for patients\nthat suffer life-limiting illnesses. These programs aim to guarantee a minimum\nlevel of quality of life (QoL) for the last stage of life. They are currently\nbased on clinical evaluation of risk of one-year mortality.\n  Objectives: The main objective of this work is to develop and validate\nmachine-learning based models to predict the exitus of a patient within the\nnext year using data gathered at hospital admission.\n  Methods: Five machine learning techniques were applied in our study to\ndevelop machine-learning predictive models: Support Vector Machines,\nK-neighbors Classifier, Gradient Boosting Classifier, Random Forest and\nMultilayer Perceptron. All models were trained and evaluated using the\nretrospective dataset. The evaluation was performed with five metrics computed\nby a resampling strategy: Accuracy, the area under the ROC curve, Specificity,\nSensitivity, and the Balanced Error Rate.\n  Results: All models for forecasting one-year mortality achieved an AUC ROC\nfrom 0.858 to 0.911. Specifically, Gradient Boosting Classifier was the best\nmodel, producing an AUC ROC of 0.911 (CI 95%, 0.911 to 0.912), a sensitivity of\n0.858 (CI 95%, 0.856 to 0.86) and a specificity of 0.807 (CI 95%, 0.806 to\n0808) and a BER of 0.168 (CI 95%, 0.167 to 0.169).\n  Conclusions: The analysis of common information at hospital admission\ncombined with machine learning techniques produced models with competitive\ndiscriminative power. Our models reach the best results reported in state of\nthe art. These results demonstrate that they can be used as an accurate\ndata-driven palliative care criteria inclusion.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 14:18:04 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Blanes-Selva", "Vicent", ""], ["Ruiz-Garc\u00eda", "Vicente", ""], ["Tortajada", "Salvador", ""], ["Bened\u00ed", "Jos\u00e9-Miguel", ""], ["Valdivieso", "Bernardo", ""], ["Garc\u00eda-G\u00f3mez", "Juan M.", ""]]}, {"id": "1907.09553", "submitter": "Andrew Brown", "authors": "Carl Ehrett, D. Andrew Brown, Evan Chodora, Christopher Kitchens, and\n  Sez Atamturktur", "title": "Coupling material and mechanical design processes via computer model\n  calibration", "comments": "20 pages, 7 figures. Supplementary material and computer code\n  available from the authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer model calibration typically operates by choosing parameter values in\na computer model so that the model output faithfully predicts reality. By using\nperformance targets in place of observed data, we show that calibration\ntechniques can be repurposed to wed engineering and material design, two\nprocesses that are traditionally carried out separately. This allows materials\nto be designed with specific engineering targets in mind while quantifying the\nassociated sources of uncertainty. We demonstrate our proposed approach by\n\"calibrating\" material design settings to performance targets for a wind\nturbine blade.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 20:16:55 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 16:44:59 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Ehrett", "Carl", ""], ["Brown", "D. Andrew", ""], ["Chodora", "Evan", ""], ["Kitchens", "Christopher", ""], ["Atamturktur", "Sez", ""]]}, {"id": "1907.09565", "submitter": "Ranjan Maitra", "authors": "Geoffrey Z. Thompson and Ranjan Maitra and William Q. Meeker and\n  Ashraf Bastawros", "title": "Classification with the matrix-variate-$t$ distribution", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": "10.1080/10618600.2019.1696208", "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix-variate distributions can intuitively model the dependence structure\nof matrix-valued observations that arise in applications with multivariate time\nseries, spatio-temporal or repeated measures. This paper develops an\nExpectation-Maximization algorithm for discriminant analysis and classification\nwith matrix-variate $t$-distributions. The methodology shows promise on\nsimulated datasets or when applied to the forensic matching of fractured\nsurfaces or the classification of functional Magnetic Resonance, satellite or\nhand gestures images.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 20:44:34 GMT"}, {"version": "v2", "created": "Sun, 20 Oct 2019 22:07:55 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Thompson", "Geoffrey Z.", ""], ["Maitra", "Ranjan", ""], ["Meeker", "William Q.", ""], ["Bastawros", "Ashraf", ""]]}, {"id": "1907.09594", "submitter": "Jungseock Joo", "authors": "Nan Xi, Di Ma, Marcus Liou, Zachary C. Steinert-Threlkeld, Jason\n  Anastasopoulos, Jungseock Joo", "title": "Understanding the Political Ideology of Legislators from Social Media\n  Images", "comments": "To appear in the Proceedings of International AAAI Conference on Web\n  and Social Media (ICWSM 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CV cs.HC cs.MM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we seek to understand how politicians use images to express\nideological rhetoric through Facebook images posted by members of the U.S.\nHouse and Senate. In the era of social media, politics has become saturated\nwith imagery, a potent and emotionally salient form of political rhetoric which\nhas been used by politicians and political organizations to influence public\nsentiment and voting behavior for well over a century. To date, however, little\nis known about how images are used as political rhetoric. Using deep learning\ntechniques to automatically predict Republican or Democratic party affiliation\nsolely from the Facebook photographs of the members of the 114th U.S. Congress,\nwe demonstrate that predicted class probabilities from our model function as an\naccurate proxy of the political ideology of images along a left-right\n(liberal-conservative) dimension. After controlling for the gender and race of\npoliticians, our method achieves an accuracy of 59.28% from single photographs\nand 82.35% when aggregating scores from multiple photographs (up to 150) of the\nsame person. To better understand image content distinguishing liberal from\nconservative images, we also perform in-depth content analyses of the\nphotographs. Our findings suggest that conservatives tend to use more images\nsupporting status quo political institutions and hierarchy maintenance,\nfeaturing individuals from dominant social groups, and displaying greater\nhappiness than liberals.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 21:43:49 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Xi", "Nan", ""], ["Ma", "Di", ""], ["Liou", "Marcus", ""], ["Steinert-Threlkeld", "Zachary C.", ""], ["Anastasopoulos", "Jason", ""], ["Joo", "Jungseock", ""]]}, {"id": "1907.09637", "submitter": "Alice Richardson", "authors": "Aidan Zellner, Alice M. Richardson, Brett A. Lidbury, Peter Hobson and\n  Tony Badrick", "title": "An Investigation into Outlier Elimination and Calculation Methods in the\n  Determination of Reference Intervals using Serum Immunoglobulin A as a Model\n  Data Collection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Reference intervals are essential to interpret diagnostic tests,\nbut their determination has become controversial. Methods: In this paper\nparametric, non-parametric and robust reference intervals with Tukey and block\nelimination are calculated from a dataset of over 32,000 serum immunoglobulin A\n(IgA) measurements. Results: The outlier elimination method was significantly\nmore determinative of the reference intervals than the calculation method. The\nTukey elimination procedure consistently eliminated significantly more values\nthan the block method of Dixon and Reed across all age ranges. If Tukey\nelimination was applied, variation between reference intervals produced by the\ndifferent calculation methods was minimal. Block elimination rarely eliminated\nvalues. The non-parametric reference intervals were more sensitive to outliers,\nwhich in the IgA context, led to higher and wider reference intervals for the\nolder age groups. There were only minimal differences between robust and\nparametric reference intervals. Conclusions: This suggests that Tukey\nelimination should be preferred over the block D/R method for datasets similar\nto the one used in this study. These are predominantly new observations, as\nprevious literature has focused on the calculation technique and not discussed\noutlier elimination. This suggests the robust method is not advantageous over\nthe parametric method and therefore due to its complexity is not particularly\nuseful, contrary to CLSI Guidelines.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 00:09:01 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Zellner", "Aidan", ""], ["Richardson", "Alice M.", ""], ["Lidbury", "Brett A.", ""], ["Hobson", "Peter", ""], ["Badrick", "Tony", ""]]}, {"id": "1907.09771", "submitter": "Sophie Donnet Dr", "authors": "Sophie Donnet, St\\'ephane Robin", "title": "Bayesian inference for network Poisson models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is motivated by the analysis of ecological interaction networks.\nPoisson stochastic blockmodels are widely used in this field to decipher the\nstructure that underlies a weighted network, while accounting for covariate\neffects. Efficient algorithms based on variational approximations exist for\nfrequentist inference, but without statistical guaranties as for the resulting\nestimates. In absence of variational Bayes estimates, we show that a good proxy\nof the posterior distribution can be straightforwardly derived from the\nfrequentist variational estimation procedure, using a Laplace approximation. We\nuse this proxy to sample from the true posterior distribution via a sequential\nMonte-Carlo algorithm. As shown in the simulation study, the efficiency of the\nposterior sampling is greatly improved by the accuracy of the approximate\nposterior distribution. The proposed procedure can be easily extended to other\nlatent variable models. We use this methodology to assess the influence of\navailable covariates on the organization of two ecological networks, as well as\nthe existence of a residual interaction structure.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 09:02:30 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Donnet", "Sophie", ""], ["Robin", "St\u00e9phane", ""]]}, {"id": "1907.10009", "submitter": "Fabrizio De Vico Fallani", "authors": "Catalina Obando, Charlotte Rosso, Joshua Siegel, Maurizio Corbetta and\n  Fabrizio De Vico Fallani", "title": "Temporal connection signatures of human brain networks after stroke", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plasticity after stroke is a complex phenomenon initiated by the functional\nreorganization of the brain, especially in the perilesional tissue. At\nmacroscales, the reestablishment of segregation within the affected hemisphere\nand interhemispheric integration has been extensively documented in the\nreconfiguration of brain networks. However, the local connection mechanisms\ngenerating such global network changes are still largely unknown as well as\ntheir potential to better predict the outcome of patients. To address this\nquestion, time must be considered as a formal variable of the problem and not\njust a simple repeated observation. Here, we hypothesize that the temporal\nformation of basic connection blocks such as intermodule edges and intramodule\ntriangles would be sufficient to determine the large-scale brain reorganization\nafter stroke. To test our hypothesis, we adopted a statistical approach based\non temporal exponential random graph models (tERGMs). First, we validated the\noverall performance on synthetic time-varying networks simulating the\nreconfiguration process after stroke. Then, using longitudinal functional\nconnectivity measurements of resting-state brain activity, we showed that both\nthe formation of triangles within the affected hemisphere and interhemispheric\nlinks are sufficient to reproduce the longitudinal brain network changes from 2\nweeks to 1 year after the stroke. Finally, we showed that these temporal\nconnection mechanisms are over-expressed in the subacute phase as compared to\nhealthy controls and predicted the chronic language and visual outcome\nrespectively in patients with subcortical and cortical lesions, whereas static\napproaches failed to do so. Our results indicate the importance of considering\ntime-varying connection properties when modeling dynamic complex systems and\nprovide fresh insights into the network mechanisms of stroke recovery.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 17:07:50 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Obando", "Catalina", ""], ["Rosso", "Charlotte", ""], ["Siegel", "Joshua", ""], ["Corbetta", "Maurizio", ""], ["Fallani", "Fabrizio De Vico", ""]]}, {"id": "1907.10013", "submitter": "Hannah Roberts", "authors": "Hannah Roberts, Caspar van Lissa, Paulien Hagedoorn, Ian Kellar, Marco\n  Helbich", "title": "The effect of short-term exposure to the natural environment on\n  depressive mood: A systematic review and meta-analysis", "comments": null, "journal-ref": null, "doi": "10.1016/j.envres.2019.108606", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Research suggests that exposure to the natural environment can improve mood,\nhowever, current reviews are limited in scope and there is little understanding\nof moderators. We aimed to conduct a comprehensive systematic review and\nmeta-analysis of the evidence for the effect of short-term exposure to the\nnatural environment on depressive mood. Five databases were systematically\nsearched for relevant studies published up to March 2018. Risk of bias was\nevaluated using the Cochrane Risk of Bias (ROB) tool 1.0 and the Risk of Bias\nin Non-Randomised Studies of Interventions (ROBINS-I) tool where appropriate.\nThe GRADE approach was used to assess the quality of evidence overall. A\nrandom-effects meta-analysis was performed. 20 potential moderators of the\neffect size were coded and the machine learning-based MetaForest algorithm was\nused to identify relevant moderators. These were then entered into a\nmeta-regression. 33 studies met the inclusion criteria. Effect sizes ranged\nfrom -2.30 to 0.84, with a pooled effect size of $\\gamma$ = -0.30 95% CI [-0.50\nto -0.10]. However, there was significant residual heterogeneity between\nstudies and risk of bias was high. Type of natural environment, type of built\nenvironment, gender mix of the sample, and region of study origin, among\nothers, were identified as relevant moderators but were not significant when\nentered in a meta-regression. Quality of evidence was rated very low to low. An\nassessment of publication bias was inconclusive. A small effect was found for\nreduction in depressive mood following exposure to the natural environment.\nHowever, the high risk of bias and low quality of studies limits confidence in\nthe results. The variation in effect size also remains largely unexplained. It\nis recommended that future studies make use of reporting guidelines and aim to\nreduce the potential for bias where possible.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 17:16:06 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2019 11:26:46 GMT"}, {"version": "v3", "created": "Tue, 22 Oct 2019 11:32:36 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Roberts", "Hannah", ""], ["van Lissa", "Caspar", ""], ["Hagedoorn", "Paulien", ""], ["Kellar", "Ian", ""], ["Helbich", "Marco", ""]]}, {"id": "1907.10109", "submitter": "Andrew Finley Dr.", "authors": "Shinichiro Shirota, Andrew O. Finley, Bruce D. Cook, Sudipto Banerjee", "title": "Conjugate Nearest Neighbor Gaussian Process Models for Efficient\n  Statistical Interpolation of Large Spatial Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key challenge in spatial statistics is the analysis for massive\nspatially-referenced data sets. Such analyses often proceed from Gaussian\nprocess specifications that can produce rich and robust inference, but involve\ndense covariance matrices that lack computationally exploitable structures. The\nmatrix computations required for fitting such models involve floating point\noperations in cubic order of the number of spatial locations and dynamic memory\nstorage in quadratic order. Recent developments in spatial statistics offer a\nvariety of massively scalable approaches. Bayesian inference and hierarchical\nmodels, in particular, have gained popularity due to their richness and\nflexibility in accommodating spatial processes. Our current contribution is to\nprovide computationally efficient exact algorithms for spatial interpolation of\nmassive data sets using scalable spatial processes. We combine low-rank\nGaussian processes with efficient sparse approximations. Following recent work\nby [1], we model the low-rank process using a Gaussian predictive process (GPP)\nand the residual process as a sparsity-inducing nearest-neighbor Gaussian\nprocess (NNGP). A key contribution here is to implement these models using\nexact conjugate Bayesian modeling to avoid expensive iterative algorithms.\nThrough the simulation studies, we evaluate performance of the proposed\napproach and the robustness of our models, especially for long range\nprediction. We implement our approaches for remotely sensed light detection and\nranging (LiDAR) data collected over the US Forest Service Tanana Inventory Unit\n(TIU) in a remote portion of Interior Alaska.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 19:36:27 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Shirota", "Shinichiro", ""], ["Finley", "Andrew O.", ""], ["Cook", "Bruce D.", ""], ["Banerjee", "Sudipto", ""]]}, {"id": "1907.10115", "submitter": "Sofia Ruiz Suarez", "authors": "Sofia Ruiz-Suarez, Vianey Leos-Barajas, Ignacio Alvarez-Castro, Juan\n  M. Morales", "title": "Approximate Bayesian inference for a \"steps and turns\" continuous-time\n  random walk observed at regular time intervals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of animal movement is challenging because it is a process modulated\nby many factors acting at different spatial and temporal scales. Several models\nhave been proposed which differ primarily in the temporal conceptualization,\nnamely continuous and discrete time formulations. Naturally, animal movement\noccurs in continuous time but we tend to observe it at fixed time intervals. To\naccount for the temporal mismatch between observations and movement decisions,\nwe used a state-space model where movement decisions (steps and turns) are made\nin continuous time. The movement process is then observed at regular time\nintervals. As the likelihood function of this state-space model turned out to\nbe complex to calculate yet simulating data is straightforward, we conduct\ninference using a few variations of Approximate Bayesian Computation (ABC). We\nexplore the applicability of these methods as a function of the discrepancy\nbetween the temporal scale of the observations and that of the movement process\nin a simulation study. We demonstrate the application of this model to a real\ntrajectory of a sheep that was reconstructed in high resolution using\ninformation from magnetometer and GPS devices. Our results suggest that\naccurate estimates can be obtained when the observations are less than 5 times\nthe average time between changes in movement direction. The state-space model\nused here allowed us to connect the scales of the observations and movement\ndecisions in an intuitive and easy to interpret way. Our findings underscore\nthe idea that the time scale at which animal movement decisions are made needs\nto be considered when designing data collection protocols, and that sometimes\nhigh-frequency data may not be necessary to have good estimates of certain\nmovement processes.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 20:01:02 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Ruiz-Suarez", "Sofia", ""], ["Leos-Barajas", "Vianey", ""], ["Alvarez-Castro", "Ignacio", ""], ["Morales", "Juan M.", ""]]}, {"id": "1907.10152", "submitter": "Michael Weylandt", "authors": "Michael Weylandt and Yu Han and Katherine B. Ensor", "title": "Multivariate Modeling of Natural Gas Spot Trading Hubs Incorporating\n  Futures Market Realized Volatility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Financial markets for Liquified Natural Gas (LNG) are an important and\nrapidly-growing segment of commodities markets. Like other commodities markets,\nthere is an inherent spatial structure to LNG markets, with different price\ndynamics for different points of delivery hubs. Certain hubs support highly\nliquid markets, allowing efficient and robust price discovery, while others are\nhighly illiquid, limiting the effectiveness of standard risk management\ntechniques. We propose a joint modeling strategy, which uses high-frequency\ninformation from thickly-traded hubs to improve volatility estimation and risk\nmanagement at thinly traded hubs. The resulting model has superior in- and\nout-of-sample predictive performance, particularly for several commonly used\nrisk management metrics, demonstrating that joint modeling is indeed possible\nand useful. To improve estimation, a Bayesian estimation strategy is employed\nand data-driven weakly informative priors are suggested. Our model is robust to\nsparse data and can be effectively used in any market with similar irregular\npatterns of data availability.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 21:53:41 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Weylandt", "Michael", ""], ["Han", "Yu", ""], ["Ensor", "Katherine B.", ""]]}, {"id": "1907.10173", "submitter": "William Briggs", "authors": "William M. Briggs and Jaap Hanekamp", "title": "Uncertainty in the MAN Data Calibration & Trend Estimates", "comments": "32 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate trend identification in the LML and MAN atmospheric ammonia\ndata. The signals are mixed in the LML data, with just as many positive,\nnegative, and no trends found. The start date for trend identification is\ncrucial, with the trends claimed changing sign and significance depending on\nthe start date. The MAN data is calibrated to the LML data. This calibration\nintroduces uncertainty never heretofore accounted for in any downstream\nanalysis, such as identifying trends. We introduce a method to do this, and\nfind that the number of trends identified in the MAN data drop by about 50%.\nThe missing data at MAN stations is also imputed; we show that this imputation\nagain changes the number of trends identified, with more positive and fewer\nsignificant trends claimed. The sign and significance of the trends identified\nin the MAN data change with the introduction of the calibration and then again\nwith the imputation. The conclusion is that great over-certainty exists in\ncurrent methods of trend identification.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 23:18:38 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Briggs", "William M.", ""], ["Hanekamp", "Jaap", ""]]}, {"id": "1907.10207", "submitter": "Mityl Biswas", "authors": "Mityl Biswas, Arnab Maity", "title": "Hypothesis Testing in Nonlinear Function on Scalar Regression with\n  Application to Child Growth Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a kernel machine based hypothesis testing procedure in nonlinear\nfunction-on-scalar regression model. Our research is motivated by the Newborn\nEpigenetic Study (NEST) where the question of interest is whether a\npre-specified group of toxic metals or methylation at any of 9 differentially\nmethylated regions (DMRs) is associated with child growth. We take the child\ngrowth trajectory as the functional response, and model the toxic metal\nmeasurements jointly using a nonlinear function. We use a kernel machine\napproach to model the unknown function and transform the hypothesis of no\neffect to an appropriate variance component test. We demonstrate our proposed\nmethodology using a simulation study and by applying it to analyze the NEST\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 02:06:59 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 05:48:13 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Biswas", "Mityl", ""], ["Maity", "Arnab", ""]]}, {"id": "1907.10287", "submitter": "Jiannan Lu", "authors": "Jiannan Lu, Yunshu Zhang and Peng Ding", "title": "Sharp bounds on the relative treatment effect for ordinal outcomes", "comments": "Accepted by Biometrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For ordinal outcomes, the average treatment effect is often ill-defined and\nhard to interpret. Echoing Agresti and Kateri (2017), we argue that the\nrelative treatment effect can be a useful measure especially for ordinal\noutcomes, which is defined as $\\gamma = \\mathrm{pr}\\{ Y_i(1) > Y_i(0) \\} -\n\\mathrm{pr}\\{ Y_i(1) < Y_i(0) \\}$, with $Y_i(1)$ and $Y_i(0)$ being the\npotential outcomes of unit $i$ under treatment and control, respectively. Given\nthe marginal distributions of the potential outcomes, we derive the sharp\nbounds on $\\gamma,$ which are identifiable parameters based on the observed\ndata. Agresti and Kateri (2017) focused on modeling strategies under the\nassumption of independent potential outcomes, but we allow for arbitrary\ndependence.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 07:57:33 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2019 16:03:25 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Lu", "Jiannan", ""], ["Zhang", "Yunshu", ""], ["Ding", "Peng", ""]]}, {"id": "1907.10306", "submitter": "Petr Koldanov Alexander", "authors": "Petr Koldanov", "title": "Testing new property of elliptical model for stock returns distribution", "comments": "15 pages, 2 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wide class of elliptically contoured distributions is a popular model of\nstock returns distribution. However the important question of adequacy of the\nmodel is open. There are some results which reject and approve such model. Such\nresults are obtained by testing some properties of elliptical model for each\npair of stocks from some markets. New property of equality of $\\tau$ Kendall\ncorrelation coefficient and probability of sign coincidence for any pair of\nrandom variables with elliptically contoured distribution is proved in the\npaper. Distribution free statistical tests for testing this property for any\npair of stocks are constructed. Holm multiple hypotheses testing procedure\nbased on the individual tests is constructed and applied for stock markets data\nfor the concrete year. New procedure of testing the elliptical model for stock\nreturns distribution for all years of observation for some period is proposed.\nThe procedure is applied for the stock markets data of China, USA, Great\nBritain and Germany for the period from 2003 to 2014. It is shown that for USA,\nGreat Britain and Germany stock markets the hypothesis of elliptical model of\nstock returns distribution could be accepted but for Chinese stock market is\nrejected for some cases.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 08:53:45 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Koldanov", "Petr", ""]]}, {"id": "1907.10485", "submitter": "Xin Shi", "authors": "Xin Shi, Robert Qiu", "title": "Early Anomaly Detection in Power Systems Based on Random Matrix Theory", "comments": "8 pages, IEEE Trans on Smart Grid, submitted. arXiv admin note:\n  substantial text overlap with arXiv:1801.01669; text overlap with\n  arXiv:1810.08962", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is important for detecting the anomaly in power systems before it expands\nand causes serious faults such as power failures or system blackout. With the\ndeployments of phasor measurement units (PMUs), massive amounts of\nsynchrophasor measurements are collected, which makes it possible for the\nreal-time situation awareness of the entire system. In this paper, based on\nrandom matrix theory (RMT), a data-driven approach is proposed for anomaly\ndetection in power systems. First, spatio-temporal data set is formulated by\narranging high-dimensional synchrophasor measurements in chronological order.\nBased on the Ring Law in RMT for the empirical spectral analysis of\n`signal+noise' matrix, the mean spectral radius (MSR) is introduced to indicate\nthe system states from the macroscopic perspective. In order to realize anomaly\ndeclare automatically, an anomaly indicator based on the MSR is designed and\nthe corresponding confidence level $1-\\alpha$ is calculated. The proposed\napproach is capable of detecting the anomaly in an early phase and robust\nagainst random fluctuations and measuring errors. Cases on the synthetic data\ngenerated from IEEE 300-bus, 118-bus and 57-bus test systems validate the\neffectiveness and advantages of the approach.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2019 04:30:56 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Shi", "Xin", ""], ["Qiu", "Robert", ""]]}, {"id": "1907.10524", "submitter": "Raju Rimal Mr", "authors": "Raju Rimal, Trygve Alm{\\o}y and Solve S{\\ae}b{\\o}", "title": "Comparison of Multi-response Estimation Methods", "comments": "24 Pages", "journal-ref": null, "doi": "10.1016/j.chemolab.2020.104093", "report-no": null, "categories": "stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Prediction performance does not always reflect the estimation behaviour of a\nmethod. High error in estimation may necessarily not result in high prediction\nerror, but can lead to an unreliable prediction if test data lie in a slightly\ndifferent subspace than the training data. In addition, high estimation error\noften leads to unstable estimates, and consequently, the estimated effect of\npredictors on the response can not have a valid interpretation. Many research\nfields show more interest in the effect of predictor variables than actual\nprediction performance. This study compares some newly-developed (envelope) and\nwell-established (PCR, PLS) estimation methods using simulated data with\nspecifically designed properties such as Multicollinearity in the predictor\nvariables, the correlation between multiple responses and the position of\nprincipal components corresponding to predictors that are relevant for the\nresponse. This study aims to give some insights into these methods and help the\nresearchers to understand and use them for further study. Here we have, not\nsurprisingly, found that no single method is superior to others, but each has\nits strength for some specific nature of data. In addition, the newly developed\nenvelope method has shown impressive results in finding relevant information\nfrom data using significantly fewer components than the other methods.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 15:37:30 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 08:31:00 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Rimal", "Raju", ""], ["Alm\u00f8y", "Trygve", ""], ["S\u00e6b\u00f8", "Solve", ""]]}, {"id": "1907.10684", "submitter": "Thomas Muehlenstaedt", "authors": "Thomas Muehlenstaedt, Maria Lanzerath", "title": "Teaching Split Plot Experiments With a Boomerang Tin", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents an example on how to teach split-plot experimental\ndesigns based on a hands-on exercise. This is a toy called boomerang tin which\nutilizes a rubber band to store and release energy. The set up and mechanisms\nof the hands-on example are explained followed by a description of how it can\nbe leveraged in teaching split plot DoE. An actual design is set up, analyzed\nand discussed to provide reference for its usage in class.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 19:30:10 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Muehlenstaedt", "Thomas", ""], ["Lanzerath", "Maria", ""]]}, {"id": "1907.10722", "submitter": "Benjamin Ackerman", "authors": "Benjamin Ackerman, Juned Siddique, Elizabeth A. Stuart", "title": "Transportability of Outcome Measurement Error Correction: from\n  Validation Studies to Intervention Trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many lifestyle intervention trials depend on collecting self-reported\noutcomes, like dietary intake, to assess the intervention's effectiveness.\nSelf-reported outcome measures are subject to measurement error, which could\nimpact treatment effect estimation. External validation studies measure both\nself-reported outcomes and an accompanying biomarker, and can therefore be used\nfor measurement error correction. Most validation data, though, are only\nrelevant for outcomes under control conditions. Statistical methods have been\ndeveloped to use external validation data to correct for outcome measurement\nerror under control, and then conduct sensitivity analyses around the error\nunder treatment to obtain estimates of the corrected average treatment effect.\nHowever, an assumption underlying this approach is that the measurement error\nstructure of the outcome is the same in both the validation sample and the\nintervention trial, so that the error correction is transportable to the trial.\nThis may not always be a valid assumption to make. In this paper, we propose an\napproach that adjusts the validation sample to better resemble the trial sample\nand thus leads to more transportable measurement error corrections. We also\nformally investigate when bias due to poor transportability may arise. Lastly,\nwe examine the method performance using simulation, and illustrate them using\nPREMIER, a multi-arm lifestyle intervention trial measuring self-reported\nsodium intake as an outcome, and OPEN, a validation study that measures both\nself-reported diet and urinary biomarkers.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 20:58:53 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 16:11:55 GMT"}, {"version": "v3", "created": "Thu, 2 Apr 2020 19:04:59 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Ackerman", "Benjamin", ""], ["Siddique", "Juned", ""], ["Stuart", "Elizabeth A.", ""]]}, {"id": "1907.10762", "submitter": "Bart Spencer", "authors": "Bartholomew Spencer, Karl Jackson, Sam Robertson", "title": "Fitting motion models to contextual player behavior", "comments": "8 pages, 4 figures, IACSS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this study was to incorporate contextual information into\nthe modelling of player movements. This was achieved by combining the\ndistributions of forthcoming passing contests that players committed to and\nthose they did not. The resultant array measures the probability a player would\ncommit to forthcoming contests in their vicinity. Commitment-based motion\nmodels were fit on 46220 samples of player behavior in the Australian Football\nLeague. It was found that the shape of commitment-based models differed greatly\nto displacement-based models for Australian footballers. Player commitment\narrays were used to measure the spatial occupancy and dominance of the\nattacking team. The spatial characteristics of pass receivers were extracted\nfor 2934 passes. Positional trends in passing were identified. Furthermore,\npasses were clustered into three components using Gaussian mixture models.\nPasses in the AFL are most commonly to one-on-one contests or unmarked players.\nFurthermore, passes were rarely greater than 25 m.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 22:32:39 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Spencer", "Bartholomew", ""], ["Jackson", "Karl", ""], ["Robertson", "Sam", ""]]}, {"id": "1907.10940", "submitter": "Ziwen An", "authors": "Ziwen An, Leah F South and Christopher Drovandi", "title": "BSL: An R Package for Efficient Parameter Estimation for\n  Simulation-Based Models via Bayesian Synthetic Likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian synthetic likelihood (BSL) is a popular method for estimating the\nparameter posterior distribution for complex statistical models and stochastic\nprocesses that possess a computationally intractable likelihood function.\nInstead of evaluating the likelihood, BSL approximates the likelihood of a\njudiciously chosen summary statistic of the data via model simulation and\ndensity estimation. Compared to alternative methods such as approximate\nBayesian computation (ABC), BSL requires little tuning and requires less model\nsimulations than ABC when the chosen summary statistic is high-dimensional. The\noriginal synthetic likelihood relies on a multivariate normal approximation of\nthe intractable likelihood, where the mean and covariance are estimated by\nsimulation. An extension of BSL considers replacing the sample covariance with\na penalised covariance estimator to reduce the number of required model\nsimulations. Further, a semi-parametric approach has been developed to relax\nthe normality assumption. In this paper, we present an R package called BSL\nthat amalgamates the aforementioned methods and more into a single, easy-to-use\nand coherent piece of software. The R package also includes several examples to\nillustrate how to use the package and demonstrate the utility of the methods.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 10:06:08 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["An", "Ziwen", ""], ["South", "Leah F", ""], ["Drovandi", "Christopher", ""]]}, {"id": "1907.11051", "submitter": "Diego Mesa", "authors": "Thomas A. Lasko and Diego A. Mesa", "title": "Computational Phenotype Discovery via Probabilistic Independence", "comments": "Presented at KDD Workshop on Applied Data Science for Healthcare 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational Phenotype Discovery research has taken various pragmatic\napproaches to disentangling phenotypes from the episodic observations in\nElectronic Health Records. In this work, we use transformation into continuous,\nlongitudinal curves to abstract away the sparse irregularity of the data, and\nwe introduce probabilistic independence as a guiding principle for\ndisentangling phenotypes into patterns that may more closely match true\npathophysiologic mechanisms. We use the identification of liver disease\npatterns that presage development of Hepatocellular Carcinoma as a\nproof-of-concept demonstration.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 13:48:28 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Lasko", "Thomas A.", ""], ["Mesa", "Diego A.", ""]]}, {"id": "1907.11077", "submitter": "Adam Walder", "authors": "Adam Walder and Ephraim M. Hanks", "title": "Bayesian Analysis of Spatial Generalized Linear Mixed Models with\n  Laplace Random Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian random field (GRF) models are widely used in spatial statistics to\ncapture spatially correlated error. We investigate the results of replacing\nGaussian processes with Laplace moving averages (LMAs) in spatial generalized\nlinear mixed models (SGLMMs). We demonstrate that LMAs offer improved\npredictive power when the data exhibits localized spikes in the response.\nSGLMMs with LMAs are shown to maintain analogous parameter inference and\nsimilar computing to Gaussian SGLMMs. We propose a novel discrete space LMA\nmodel for irregular lattices and construct conjugate samplers for LMAs with\ngeoreferenced and areal support. We provide a Bayesian analysis of SGLMMs with\nLMAs and GRFs over multiple data support and response types.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 14:13:44 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Walder", "Adam", ""], ["Hanks", "Ephraim M.", ""]]}, {"id": "1907.11129", "submitter": "Casey Kneale Ph.D", "authors": "Casey Kneale, Kolia Sadeghi", "title": "Semisupervised Adversarial Neural Networks for Cyber Security Transfer\n  Learning", "comments": "14 figures, 17 pages, Draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.NE stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On the path to establishing a global cybersecurity framework where each\nenterprise shares information about malicious behavior, an important question\narises. How can a machine learning representation characterizing a cyber attack\non one network be used to detect similar attacks on other enterprise networks\nif each networks has wildly different distributions of benign and malicious\ntraffic? We address this issue by comparing the results of naively transferring\na model across network domains and using CORrelation ALignment, to our novel\nadversarial Siamese neural network. Our proposed model learns attack\nrepresentations that are more invariant to each network's particularities via\nan adversarial approach. It uses a simple ranking loss that prioritizes the\nlabeling of the most egregious malicious events correctly over average\naccuracy. This is appropriate for driving an alert triage workflow wherein an\nanalyst only has time to inspect the top few events ranked highest by the\nmodel. In terms of accuracy, the other approaches fail completely to detect any\nmalicious events when models were trained on one dataset are evaluated on\nanother for the first 100 events. While, the method presented here retrieves\nsizable proportions of malicious events, at the expense of some training\ninstabilities due in adversarial modeling. We evaluate these approaches using 2\npublicly available networking datasets, and suggest areas for future research.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 15:14:38 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Kneale", "Casey", ""], ["Sadeghi", "Kolia", ""]]}, {"id": "1907.11195", "submitter": "Xiao Wang", "authors": "Xiao Wang, Zhijie Wang, Yolande M. Pengetnze, Barry S. Lachman, Vikas\n  Chowdhry", "title": "Deep Learning Models to Predict Pediatric Asthma Emergency Department\n  Visits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pediatric asthma is the most prevalent chronic childhood illness, afflicting\nabout 6.2 million children in the United States. However, asthma could be\nbetter managed by identifying and avoiding triggers, educating about\nmedications and proper disease management strategies. This research utilizes\ndeep learning methodologies to predict asthma-related emergency department (ED)\nvisit within 3 months using Medicaid claims data. We compare prediction results\nagainst traditional statistical classification model - penalized Lasso logistic\nregression, which we trained and have deployed since 2015. The results have\nindicated that deep learning model Artificial Neural Networks (ANN) slightly\noutperforms (with AUC = 0.845) the Lasso logistic regression (with AUC =\n0.842). The reason may come from the nonlinear nature of ANN.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 16:56:56 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Wang", "Xiao", ""], ["Wang", "Zhijie", ""], ["Pengetnze", "Yolande M.", ""], ["Lachman", "Barry S.", ""], ["Chowdhry", "Vikas", ""]]}, {"id": "1907.11264", "submitter": "Mansour Naslcheraghi", "authors": "Leila Marandi, Mansour Naslcheraghi, Seyed Ali Ghorashi, Mohammad\n  Shikh-Bahaei", "title": "Delay Analysis in Full-Duplex Heterogeneous Cellular Networks", "comments": "This paper has been accepted for publication in the IEEE Transactions\n  on Vehicular Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous networks (HetNets) as a combination of macro cells and small\ncells are used to increase the cellular network's capacity, and present a\nperfect solution for high-speed communications. Increasing area spectrum\nefficiency and capacity of HetNets largely depends on the high speed of\nbackhaul links. One effective way which is currently utilized in HetNets is the\nuse of full-duplex (FD) technology that potentially doubles the spectral\nefficiency without the need for additional spectrum. On the other hand, one of\nthe most critical network design requirements is delay, which is a key\nrepresentation of the quality of service (QoS) in modern cellular networks. In\nthis paper, by utilizing tools from the stochastic geometry, we analyze the\nlocal delay for downlink (DL) channel, which is typically defined as the mean\nnumber of required time slots for a successful communication. Given imperfect\nself-interference (SI) cancellation in practical FD communications, we utilize\nduplex mode (half-duplex (HD) or FD) for each user based on the distance from\nits serving base station (BS). Further, we aim to investigate the energy\nefficiency (EE) for both duplexing modes, i.e., HD and FD, by considering local\ndelay. We conduct extensive simulations to validate system performance in terms\nof local delay versus different system key parameters.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 18:25:50 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Marandi", "Leila", ""], ["Naslcheraghi", "Mansour", ""], ["Ghorashi", "Seyed Ali", ""], ["Shikh-Bahaei", "Mohammad", ""]]}, {"id": "1907.11313", "submitter": "Piyush Pandita", "authors": "Piyush Pandita, Jesper Kristensen and Liping Wang", "title": "Towards Scalable Gaussian Process Modeling", "comments": "15 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous engineering problems of interest to the industry are often\ncharacterized by expensive black-box objective experiments or computer\nsimulations. Obtaining insight into the problem or performing subsequent\noptimizations requires hundreds of thousands of evaluations of the objective\nfunction which is most often a practically unachievable task. Gaussian Process\n(GP) surrogate modeling replaces the expensive function with a\ncheap-to-evaluate data-driven probabilistic model. While the GP does not assume\na functional form of the problem, it is defined by a set of parameters, called\nhyperparameters. The hyperparameters define the characteristics of the\nobjective function, such as smoothness, magnitude, periodicity, etc. Accurately\nestimating these hyperparameters is a key ingredient in developing a reliable\nand generalizable surrogate model. Markov chain Monte Carlo (MCMC) is a\nubiquitously used Bayesian method to estimate these hyperparameters. At the GE\nGlobal Research Center, a customized industry-strength Bayesian hybrid modeling\nframework utilizing the GP, called GEBHM, has been employed and validated over\nmany years. GEBHM is very effective on problems of small and medium size,\ntypically less than 1000 training points. However, the GP does not scale well\nin time with a growing dataset and problem dimensionality which can be a major\nimpediment in such problems. In this work, we extend and implement in GEBHM an\nAdaptive Sequential Monte Carlo (ASMC) methodology for training the GP enabling\nthe modeling of large-scale industry problems. This implementation saves\ncomputational time (especially for large-scale problems) while not sacrificing\npredictability over the current MCMC implementation. We demonstrate the\neffectiveness and accuracy of GEBHM with ASMC on four mathematical problems and\non two challenging industry applications of varying complexity.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 21:15:57 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Pandita", "Piyush", ""], ["Kristensen", "Jesper", ""], ["Wang", "Liping", ""]]}, {"id": "1907.11325", "submitter": "Cec\\'ilia Nunes", "authors": "Cec\\'ilia Nunes, H\\'el\\`ene Langet, Mathieu De Craene, Oscar Camara,\n  Bart Bijnens, Anders Jonsson", "title": "Decision Tree Learning for Uncertain Clinical Measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Clinical decision requires reasoning in the presence of imperfect data. DTs\nare a well-known decision support tool, owing to their interpretability,\nfundamental in safety-critical contexts such as medical diagnosis. However,\nlearning DTs from uncertain data leads to poor generalization, and generating\npredictions for uncertain data hinders prediction accuracy. Several methods\nhave suggested the potential of probabilistic decisions at the internal nodes\nin making DTs robust to uncertainty. Some approaches only employ probabilistic\nthresholds during evaluation. Others also consider the uncertainty in the\nlearning phase, at the expense of increased computational complexity or reduced\ninterpretability. The existing methods have not clarified the merit of a\nprobabilistic approach in the distinct phases of DT learning, nor when the\nuncertainty is present in the training or the test data. We present a\nprobabilistic DT approach that models measurement uncertainty as a noise\ndistribution, independently realized: (1) when searching for the split\nthresholds, (2) when splitting the training instances, and (3) when generating\npredictions for unseen data. The soft training approaches (1, 2) achieved a\nregularizing effect, leading to significant reductions in DT size, while\nmaintaining accuracy, for increased noise. Soft evaluation (3) showed no\nbenefit in handling noise.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 22:28:07 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Nunes", "Cec\u00edlia", ""], ["Langet", "H\u00e9l\u00e8ne", ""], ["De Craene", "Mathieu", ""], ["Camara", "Oscar", ""], ["Bijnens", "Bart", ""], ["Jonsson", "Anders", ""]]}, {"id": "1907.11447", "submitter": "Adrian Bowman Prof.", "authors": "Mark Livingston and Francesca Pannullo and Adrian Bowman and Marian\n  Scott and Nick Bailey", "title": "Exploiting new forms of data to study the private rented sector:\n  strengths and limitations of a database of rental listings", "comments": null, "journal-ref": "Journal of the Royal Statistical Society, Series A, 2021", "doi": "10.1111/rssa.12643", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reviews of official statistics for UK housing have noted that developments\nhave not kept pace with real-world change, particularly the rapid growth of\nprivate renting. This paper examines the potential value of big data in this\ncontext. We report on the construction of a dataset from the on-line adverts of\none national lettings agency, describing the content of the dataset and efforts\nto validate it against external sources. Focussing on one urban area, we\nillustrate how the dataset can shed new light on local changes. Lastly, we\ndiscuss the issues involved in making more routine use of this kind of data.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 09:16:54 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Livingston", "Mark", ""], ["Pannullo", "Francesca", ""], ["Bowman", "Adrian", ""], ["Scott", "Marian", ""], ["Bailey", "Nick", ""]]}, {"id": "1907.11526", "submitter": "Kiana Roshan Zamir", "authors": "Kiana Roshan Zamir (Ph.D. Candidate, Department of Civil and\n  Environmental Engineering, University of Maryland, College Park, USA), Iryna\n  Bondarenko (Master of Community Planning, School of Architecture, Planning,\n  and Preservation, University of Maryland, College Park, MD), Arefeh Nasri\n  (Faculty Research Scientist, National Center for Smart Growth Research and\n  Education, Maryland Transportation Institute, University of Maryland, College\n  Park, MD), Stefanie Brodie (Research Program Specialist, District Department\n  of Transportation, Washington, DC, USA), Kimberly Lucas (Supervisory\n  Transportation Management Planner, Bicycle and Pedestrian Program Specialist,\n  District Department of Transportation, Washington, DC, USA)", "title": "Comparative Analysis of User Behavior of Dock-Based vs. Dockless\n  Bikeshare and Scootershare in Washington, D.C", "comments": "Extended abstract of this paper is presented at Transportation\n  Research Board 98th Annual Meeting (No. 19-03300)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2017, dockless bikeshare systems were introduced in the United States,\nfollowed by dockless scootershare in early 2018. These new mobility options are\nexpected to complement the existing station-based bikeshare systems, which are\nbound to static origin and destination points at docking stations. The three\nsystems attract different users with different travel behavior mobility\npatterns. The present research provides a comparative analysis of users'\nbehavior for these three shared mobility systems during March-May 2018 in the\nDistrict of Columbia. Our study identifies similarities/differences between the\ntwo systems aiming for better planning, operating, and decision-making of these\nemerging personal shared mobility systems in the future. It uses logistic\nregression and random forest modeling to delineate between \"member\" behavior,\nwhich aligns most closely with commuter behavior, and \"casual\" behavior that\nrepresents more recreational behavior. The results show that 63.8% of dockless\nbike users and 69.6% of dockless scooter users demonstrated \"member\" behavior,\nwhich is slightly lower than the actual percentage of trips made by members\nwithin the conventional bikeshare system (73.3%). Dockless systems users also\nshowed to have short trip durations similar to conventional bikeshare system's\nregistered members, with no significant difference between trips during\nweekdays and weekends. Overall, this study provides a methodology to understand\nusers' behavior for the dockless bikeshare system and provides sufficient\nevidence that these new shared mobility systems can potentially make positive\ncontributions to urban multi-modal infrastructure by promoting bicycle usage\nfor urban daily travel.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 04:46:17 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Zamir", "Kiana Roshan", "", "Ph.D. Candidate, Department of Civil and\n  Environmental Engineering, University of Maryland, College Park, USA"], ["Bondarenko", "Iryna", "", "Master of Community Planning, School of Architecture, Planning,\n  and Preservation, University of Maryland, College Park, MD"], ["Nasri", "Arefeh", "", "Faculty Research Scientist, National Center for Smart Growth Research and\n  Education, Maryland Transportation Institute, University of Maryland, College\n  Park, MD"], ["Brodie", "Stefanie", "", "Research Program Specialist, District Department\n  of Transportation, Washington, DC, USA"], ["Lucas", "Kimberly", "", "Supervisory\n  Transportation Management Planner, Bicycle and Pedestrian Program Specialist,\n  District Department of Transportation, Washington, DC, USA"]]}, {"id": "1907.11581", "submitter": "Somak Dutta", "authors": "Xiaojun Mao, Somak Dutta, Raymond K. W. Wong, Dan Nettleton", "title": "Adjusting for Spatial Effects in Genomic Prediction", "comments": "22 pages, 6 figures, 10 tables", "journal-ref": null, "doi": "10.1007/s13253-020-00396-1", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the problem of adjusting for spatial effects in\ngenomic prediction. Despite being seldomly considered in genomic prediction,\nspatial effects often affect phenotypic measurements of plants. We consider a\nGaussian random field model with an additive covariance structure that\nincorporates genotype effects, spatial effects and subpopulation effects. An\nempirical study shows the existence of spatial effects and heterogeneity across\ndifferent subpopulation families, while simulations illustrate the improvement\nin selecting genotypically superior plants by adjusting for spatial effects in\ngenomic prediction.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 14:00:29 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2020 07:58:34 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Mao", "Xiaojun", ""], ["Dutta", "Somak", ""], ["Wong", "Raymond K. W.", ""], ["Nettleton", "Dan", ""]]}, {"id": "1907.11762", "submitter": "Soumya Dutta", "authors": "Soumya Dutta and Ayan Biswas and James Ahrens", "title": "Multivariate Pointwise Information-Driven Data Sampling and\n  Visualization", "comments": "25 pages", "journal-ref": "Entropy, Volume 21, Issue 7, Year 2019", "doi": "10.3390/e21070699", "report-no": null, "categories": "cs.HC cs.GR cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With increasing computing capabilities of modern supercomputers, the size of\nthe data generated from the scientific simulations is growing rapidly. As a\nresult, application scientists need effective data summarization techniques\nthat can reduce large-scale multivariate spatiotemporal data sets while\npreserving the important data properties so that the reduced data can answer\ndomain-specific queries involving multiple variables with sufficient accuracy.\nWhile analyzing complex scientific events, domain experts often analyze and\nvisualize two or more variables together to obtain a better understanding of\nthe characteristics of the data features. Therefore, data summarization\ntechniques are required to analyze multi-variable relationships in detail and\nthen perform data reduction such that the important features involving multiple\nvariables are preserved in the reduced data. To achieve this, in this work, we\npropose a data sub-sampling algorithm for performing statistical data\nsummarization that leverages pointwise information theoretic measures to\nquantify the statistical association of data points considering multiple\nvariables and generates a sub-sampled data that preserves the statistical\nassociation among multi-variables. Using such reduced sampled data, we show\nthat multivariate feature query and analysis can be done effectively. The\nefficacy of the proposed multivariate association driven sampling algorithm is\npresented by applying it on several scientific data sets.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 19:32:53 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Dutta", "Soumya", ""], ["Biswas", "Ayan", ""], ["Ahrens", "James", ""]]}, {"id": "1907.11884", "submitter": "Wei Pan", "authors": "Colin Cotter, Dan Crisan, Darryl D. Holm, Wei Pan, Igor Shevchenko", "title": "A Particle Filter for Stochastic Advection by Lie Transport (SALT): A\n  case study for the damped and forced incompressible 2D Euler equation", "comments": "51 pages, 20 figures. New numerical results. Added more details and\n  references to the Introduction. Added Remark 3 to Sec 3.3", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we combine a stochastic model reduction with a particle filter\naugmented with tempering and jittering, and apply the combined algorithm to a\ndamped and forced incompressible 2D Euler dynamics defined on a simply\nconnected bounded domain.\n  We show that using the combined algorithm, we are able to assimilate data\nfrom a reference system state (the ``truth\") modelled by a highly resolved\nnumerical solution of the flow that has roughly $3.1\\times10^6$ degrees of\nfreedom, into a stochastic system having two orders of magnitude less degrees\nof freedom, which is able to approximate the true state reasonably accurately\nfor $5$ large scale eddy turnover times, using modest computational hardware.\n  The model reduction is performed through the introduction of a stochastic\nadvection by Lie transport (SALT) model as the signal on a coarser resolution.\nThe SALT approach was introduced as a general theory using a geometric\nmechanics framework from Holm, Proc. Roy. Soc. A (2015). This work follows on\nthe numerical implementation for SALT presented by Cotter et al, SIAM\nMultiscale Model. Sim. (2019) for the flow in consideration. The model\nreduction is substantial: The reduced SALT model has $4.9\\times 10^4$ degrees\nof freedom.\n  Results from reliability tests on the assimilated system are also presented.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 09:59:04 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2020 09:10:49 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Cotter", "Colin", ""], ["Crisan", "Dan", ""], ["Holm", "Darryl D.", ""], ["Pan", "Wei", ""], ["Shevchenko", "Igor", ""]]}, {"id": "1907.11925", "submitter": "Dietmar Pfeifer Prof. Dr.", "authors": "Dietmar Pfeifer", "title": "Modellvalidierung mit Hilfe von Quantil-Quantil-Plots unter Solvency II\n  (Model validation on the basis of quantile-quantile-plots under Solvency II)", "comments": "15 pages, German (with English summary), 24 figures, 8 tables, 18\n  references, revised version", "journal-ref": "ZVersWiss 2019, 307 - 325", "doi": "10.1007/s12297-019-00451-y", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After several years of development, the Solvency II-project has finally been\nset to work in the European Union with the beginning of the year 2016. This has\ncaused massive changes in the regional legislative supervisory acts. One new\naspect of regulation is the requirement of an analysis and judgement concerning\npossible deviations of the company's risk profile from the assumptions\nunderlying the standard formula in Solvency II. In particular, for the reserve\nand premium risk and the corresponding combined ratios, resp. a lognormal\ndistribution is implicitly assumed. In this paper, we present a simple, but\nnevertheless mathematically accurate method on the basis of\nquantile-quantile-plots which is suitable to perform suvh kind of analyses.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 14:38:20 GMT"}, {"version": "v2", "created": "Thu, 1 Aug 2019 18:32:03 GMT"}, {"version": "v3", "created": "Mon, 6 Jan 2020 11:48:57 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Pfeifer", "Dietmar", ""]]}, {"id": "1907.11970", "submitter": "Ranjan Maitra", "authors": "Fan Dai, Somak Dutta and Ranjan Maitra", "title": "A Matrix--free Likelihood Method for Exploratory Factor Analysis of\n  High-dimensional Gaussian Data", "comments": "10 pages, 5 figures, 4 tables", "journal-ref": null, "doi": "10.1080/10618600.2019.1704296", "report-no": null, "categories": "stat.ME q-bio.QM stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel profile likelihood method for estimating the\ncovariance parameters in exploratory factor analysis of high-dimensional\nGaussian datasets with fewer observations than number of variables. An\nimplicitly restarted Lanczos algorithm and a limited-memory quasi-Newton method\nare implemented to develop a matrix-free framework for likelihood maximization.\nSimulation results show that our method is substantially faster than the\nexpectation-maximization solution without sacrificing accuracy. Our method is\napplied to fit factor models on data from suicide attempters, suicide ideators\nand a control group.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 20:04:55 GMT"}, {"version": "v2", "created": "Sun, 10 Nov 2019 19:25:43 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Dai", "Fan", ""], ["Dutta", "Somak", ""], ["Maitra", "Ranjan", ""]]}, {"id": "1907.12389", "submitter": "Xin Lu", "authors": "Xin Lu, Shuo Qin, Petter Holme, Fanhui Meng, Yanqing Hu, Fredrik\n  Liljeros and Gad Allon", "title": "Beyond the Coverage of Information Spreading: Analytical and Empirical\n  Evidence of Re-exposure in Large-scale Online Social Networks", "comments": "16 pages,7 figures,1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Peer influence and social contagion are key denominators in the adoption and\nparticipation of information spreading, such as news propagation, word-of-mouth\nor viral marketing. In this study, we argue that it is biased to only focus on\nthe scale and coverage of information spreading, and propose that the level of\ninfluence reinforcement, quantified by the re-exposure rate, i.e., the rate of\nindividuals who are repeatedly exposed to the same information, should be\nconsidered together to measure the effectiveness of spreading. We show that\nlocal network structural characteristics significantly affects the probability\nof being exposed or re-exposed to the same information. After analyzing\ntrending news on the super large-scale online network of Sina Weibo (China's\nTwitter) with 430 million connected users, we find a class of users with\nextremely low exposure rate, even they are following tens of thousands of\nothers; and the re-exposure rate is substantially higher for news with more\ntransmission waves and stronger secondary forwarding. While exposure and\nre-exposure rate typically grow together with the scale of spreading, we find\nexceptional cases where it is possible to achieve a high exposure rate while\nmaintaining low re-exposure rate, or vice versa.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 02:05:12 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Lu", "Xin", ""], ["Qin", "Shuo", ""], ["Holme", "Petter", ""], ["Meng", "Fanhui", ""], ["Hu", "Yanqing", ""], ["Liljeros", "Fredrik", ""], ["Allon", "Gad", ""]]}, {"id": "1907.12510", "submitter": "Konstantinos Kaloudis", "authors": "Spyridon J. Hatjispyros and Konstantinos Kaloudis", "title": "A Bayesian nonparametric approach to the approximation of the global\n  stable manifold", "comments": null, "journal-ref": null, "doi": "10.1063/1.5122187", "report-no": null, "categories": "stat.AP nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian nonparametric model based on Markov Chain Monte Carlo\n(MCMC) methods for unveiling the structure of the invariant global stable\nmanifold from observed time-series data. The underlying unknown dynamical\nprocess is possibly contaminated by additive noise. We introduce the Stable\nManifold Geometric Stick Breaking Reconstruction (SM-GSBR) model with which we\nreconstruct the unknown dynamic equations and in parallel we estimate the\nglobal structure of the perturbed stable manifold. Our method works for\nnoninvertible maps without modifications. The stable manifold estimation\nprocedure is demonstrated specifically in the case of polynomial maps.\nSimulations based on synthetic time series are presented.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 16:21:40 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Hatjispyros", "Spyridon J.", ""], ["Kaloudis", "Konstantinos", ""]]}, {"id": "1907.12833", "submitter": "V\\'ictor Navas-Portella", "authors": "V\\'ictor Navas-Portella, \\'Alvaro Gonz\\'alez, Isabel Serra, Eduard\n  Vives, and \\'Alvaro Corral", "title": "Universality of power-law exponents by means of maximum likelihood\n  estimation", "comments": "14 pages, 4 figures", "journal-ref": "Phys. Rev. E 100, 062106 (2019)", "doi": "10.1103/PhysRevE.100.062106", "report-no": null, "categories": "physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power-law type distributions are extensively found when studying the\nbehaviour of many complex systems. However, due to limitations in data\nacquisition, empirical datasets often only cover a narrow range of observation,\nmaking it difficult to establish power-law behaviour unambiguously. In this\nwork we present a statistical procedure to merge different datasets with the\naim of obtaining a broader fitting range for the statistics of different\nexperiments or observations of the same system or the same universality class.\nThis procedure is applied to the Gutenberg-Richter law for earthquakes and for\nsynthetic earthquakes (acoustic emission events) generated in the laboratory:\nlabquakes. Different earthquake catalogs have been merged finding a\nGutenberg-Ricther law holding for more than eight orders of magnitude in\nseismic moment. The value of the exponent of the energy distribution of\nlabquakes depends on the material used in the compression experiments. By means\nof the procedure exposed in this manuscript, it has been found that the\nGutenberg-Richter law for earthquakes and charcoal labquakes can be\ncharacterized by the same power-law exponent.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 11:11:10 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Navas-Portella", "V\u00edctor", ""], ["Gonz\u00e1lez", "\u00c1lvaro", ""], ["Serra", "Isabel", ""], ["Vives", "Eduard", ""], ["Corral", "\u00c1lvaro", ""]]}, {"id": "1907.12853", "submitter": "Souvik Banerjee", "authors": "Souvik Banerjee, Gajendra K. Vishwakarma and Atanu Bhattacharjee", "title": "Classification Algorithm for High Dimensional Protein Markers in\n  Time-course Data", "comments": "Require major revisions and refinements", "journal-ref": "Statistics in Medicine, 2020", "doi": "10.1002/sim.8720", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identification of biomarkers is an emerging area in Oncology. In this\narticle, we develop an efficient statistical procedure for classification of\nprotein markers according to their effect on cancer progression. A\nhigh-dimensional time-course dataset of protein markers for 80 patients\nmotivates us for developing the model. We obtain the optimal threshold values\nfor markers using Cox proportional hazard model. The optimal threshold value is\ndefined as a level of a marker having maximum impact on cancer progression. The\nclassification was validated by comparing random components using both\nproportional hazard and accelerated failure time frailty models. The study\nelucidates the application of two separate joint modeling techniques using auto\nregressive-type model and mixed effect model for time-course data and\nproportional hazard model for survival data with proper utilization of Bayesian\nmethodology. Also, a prognostic score has been developed on the basis of few\nselected genes with application on patients. The complete analysis is performed\nby R programming code. This study facilitates to identify relevant biomarkers\nfrom a set of markers.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 12:14:43 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 12:02:11 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Banerjee", "Souvik", ""], ["Vishwakarma", "Gajendra K.", ""], ["Bhattacharjee", "Atanu", ""]]}, {"id": "1907.12961", "submitter": "Maria Kateri", "authors": "Marcus Johnen, Simon Pitzen, Udo Kamps, Maria Kateri and Dirk Uwe\n  Sauer", "title": "Modeling long-term capacity degradation of lithium-ion batteries", "comments": "15 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capacity degradation of lithium-ion batteries under long-term cyclic aging is\nmodelled via a flexible sigmoidal-type regression set-up, where the regression\nparameters can be interpreted. Different approaches known from the literature\nare discussed and compared with the new proposal. Statistical procedures, such\nas parameter estimation, confidence and prediction intervals are presented and\napplied to real data. The long-term capacity degradation model may be applied\nin second-life scenarios of batteries. Using some prior information or training\ndata on the complete degradation path, the model can be fitted satisfactorily\neven if only short-term degradation data is available. The training data may\narise from a single battery.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 14:09:58 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Johnen", "Marcus", ""], ["Pitzen", "Simon", ""], ["Kamps", "Udo", ""], ["Kateri", "Maria", ""], ["Sauer", "Dirk Uwe", ""]]}, {"id": "1907.13050", "submitter": "Stan Zachary", "authors": "Amy L Wilson and Stan Zachary", "title": "Using extreme value theory for the estimation of risk metrics for\n  capacity adequacy assessment", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the use of extreme value theory for modelling the\ndistribution of demand-net-of-wind for capacity adequacy assessment. Extreme\nvalue theory approaches are well-established and mathematically justified\nmethods for estimating the tails of a distribution and so are ideally suited\nfor problems in capacity adequacy, where normally only the tails of the\nrelevant distributions are significant. The extreme value theory peaks over\nthreshold approach is applied directly to observations of demand-net-of-wind,\nmeaning that no assumption is needed about the nature of any dependence between\ndemand and wind.\n  The methodology is tested on data from Great Britain and compared to two\nalternative approaches: use of the empirical distribution of demand-net-of-wind\nand use of a model which assumes independence between demand and wind. Extreme\nvalue theory is shown to produce broadly similar estimates of risk metrics to\nthe use of the above empirical distribution but with smaller sampling\nuncertainty. Estimates of risk metrics differ when the approach assuming\nindependence is used, especially when data across different historical years\nare pooled, suggesting that assuming independence might result in the over- or\nunder-estimation of risk metrics.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 16:21:10 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Wilson", "Amy L", ""], ["Zachary", "Stan", ""]]}, {"id": "1907.13489", "submitter": "Jean Abi Rizk", "authors": "Jean Rizk, Kevin Burke, Cathal Walsh", "title": "An Alternative Formulation of Coxian Phase-type Distributions with\n  Covariates: Application to Emergency Department Length of Stay", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new methodology to model patient transitions and\nlength of stay in the emergency department using a series of conditional Coxian\nphase-type distributions, with covariates. We reformulate the Coxian models\n(standard Coxian, Coxian with multiple absorbing states, joint Coxian, and\nconditional Coxian) to take into account heterogeneity in patient\ncharacteristics such as arrival mode, time of admission and age. The approach\ndiffers from previous research in that it reduces the computational time, and\nit allows the inclusion of patient covariate information directly into the\nmodel. The model is applied to emergency department data from University\nHospital Limerick in Ireland.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 13:22:46 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Rizk", "Jean", ""], ["Burke", "Kevin", ""], ["Walsh", "Cathal", ""]]}, {"id": "1907.13554", "submitter": "Won Chang", "authors": "Won Chang, Bledar A. Konomi, Georgios Karagiannis, Yawen Guan, Murali\n  Haran", "title": "Ice Model Calibration Using Semi-continuous Spatial Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid changes in Earth's cryosphere caused by human activity can lead to\nsignificant environmental impacts. Computer models provide a useful tool for\nunderstanding the behavior and projecting the future of Arctic and Antarctic\nice sheets. However, these models are typically subject to large parametric\nuncertainties due to poorly constrained model input parameters that govern the\nbehavior of simulated ice sheets. Computer model calibration provides a formal\nstatistical framework to infer parameters using observational data, and to\nquantify the uncertainty in projections due to the uncertainty in these\nparameters. Calibration of ice sheet models is often challenging because the\nrelevant model output and observational data take the form of semi-continuous\nspatial data, with a point mass at zero and a right-skewed continuous\ndistribution for positive values. Current calibration approaches cannot handle\nsuch data. Here we introduce a hierarchical latent variable model that handles\nbinary spatial patterns and positive continuous spatial patterns as separate\ncomponents. To overcome challenges due to high-dimensionality we use\nlikelihood-based generalized principal component analysis to impose\nlow-dimensional structures on the latent variables for spatial dependence. We\napply our methodology to calibrate a physical model for the Antarctic ice sheet\nand demonstrate that we can overcome the aforementioned modeling and\ncomputational challenges. As a result of our calibration, we obtain improved\nfuture ice-volume change projections.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 15:30:36 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Chang", "Won", ""], ["Konomi", "Bledar A.", ""], ["Karagiannis", "Georgios", ""], ["Guan", "Yawen", ""], ["Haran", "Murali", ""]]}, {"id": "1907.13563", "submitter": "Francisco Javier Rubio", "authors": "David Rossell and Francisco Javier Rubio", "title": "Additive Bayesian variable selection under censoring and\n  misspecification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the role of misspecification and censoring on Bayesian model\nselection in the contexts of right-censored survival and concave log-likelihood\nregression. Misspecification includes wrongly assuming the censoring mechanism\nto be non-informative. Emphasis is placed on additive accelerated failure time,\nCox proportional hazards and probit models. We offer a theoretical treatment\nthat includes local and non-local priors, and a general non-linear effect\ndecomposition to improve power-sparsity trade-offs. We discuss a fundamental\nquestion: what solution can one hope to obtain when (inevitably) models are\nmisspecified, and how to interpret it? Asymptotically, covariates that do not\nhave predictive power for neither the outcome nor (for survival data) censoring\ntimes, in the sense of reducing a likelihood-associated loss, are discarded.\nMisspecification and censoring have an asymptotically negligible effect on\nfalse positives, but their impact on power is exponential. We show that it can\nbe advantageous to consider simple models that are computationally practical\nyet attain good power to detect potentially complex effects, including the use\nof finite-dimensional basis to detect truly non-parametric effects. We also\ndiscuss algorithms to capitalize on sufficient statistics and fast likelihood\napproximations for Gaussian-based survival and binary models.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 15:43:40 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 22:19:24 GMT"}, {"version": "v3", "created": "Wed, 6 May 2020 18:39:21 GMT"}, {"version": "v4", "created": "Tue, 30 Mar 2021 15:18:10 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Rossell", "David", ""], ["Rubio", "Francisco Javier", ""]]}, {"id": "1907.13620", "submitter": "Haiyan Zheng", "authors": "Haiyan Zheng, Lisa V. Hampson", "title": "A Bayesian decision-theoretic approach to incorporate preclinical\n  information into phase I oncology trials", "comments": "26 pages, 6 figures; accepted by Biometrical Journal", "journal-ref": "Biometrical Journal 2020; 1-19", "doi": "10.1002/bimj.201900161", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Leveraging preclinical animal data for a phase I first-in-man trial is\nappealing yet challenging. A prior based on animal data may place large\nprobability mass on values of the dose-toxicity model parameter(s), which\nappear infeasible in light of data accrued from the ongoing phase I clinical\ntrial. In this paper, we seek to use animal data to improve decision making in\na model-based dose-escalation procedure for phase I oncology trials.\nSpecifically, animal data are incorporated via a robust mixture prior for the\nparameters of the dose-toxicity relationship. This prior changes dynamically as\nthe trial progresses. After completion of treatment for each cohort, the weight\nallocated to the informative component, obtained based on animal data alone, is\nupdated using a decision-theoretic approach to assess the commensurability of\nthe animal data with the human toxicity data observed thus far. In particular,\nwe measure commensurability as a function of the utility of optimal prior\npredictions for the human responses (toxicity or no toxicity) on each\nadministered dose. The proposed methodology is illustrated through several\nexamples and an extensive simulation study. Results show that our proposal can\naddress difficulties in coping with prior-data conflict commencing in\nsequential trials with a small sample size.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 10:50:54 GMT"}, {"version": "v2", "created": "Thu, 6 Feb 2020 15:29:11 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Zheng", "Haiyan", ""], ["Hampson", "Lisa V.", ""]]}, {"id": "1907.13629", "submitter": "Jeremy Koster", "authors": "Jeremy Koster, George Leckie, Brandy Aven, Christopher Charlton", "title": "Methods and Software for the Multilevel Social Relations Model: A\n  Tutorial", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This tutorial demonstrates the estimation and interpretation of the\nMultilevel Social Relations Model for dyadic data. The Social Relations Model\nis appropriate for data structures in which individuals appear multiple times\nas both the source and recipient of dyadic outcomes. Estimated using Stat-JR\nstatistical software, the models are fitted to multiple outcome types:\ncontinuous, count, and binary outcomes. In addition, models are demonstrated\nfor dyadic data from a single group and from multiple groups. The modeling\napproaches are illustrated via a series of case studies, and the data and\nsoftware to replicate these analyses are available as supplemental files.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 13:32:09 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Koster", "Jeremy", ""], ["Leckie", "George", ""], ["Aven", "Brandy", ""], ["Charlton", "Christopher", ""]]}]