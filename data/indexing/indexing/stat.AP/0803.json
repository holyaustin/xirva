[{"id": "0803.1364", "submitter": "Matus Medo", "authors": "Matus Medo, Yury M. Pis'mak, Yi-Cheng Zhang", "title": "Diversification and limited information in the Kelly game", "comments": "11 pages, 4 figures", "journal-ref": "Physica A 387, 6151-6158 (2008)", "doi": "10.1016/j.physa.2008.07.007", "report-no": null, "categories": "q-fin.PM physics.data-an physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Financial markets, with their vast range of different investment\nopportunities, can be seen as a system of many different simultaneous games\nwith diverse and often unknown levels of risk and reward. We introduce\ngeneralizations to the classic Kelly investment game [Kelly (1956)] that\nincorporates these features, and use them to investigate the influence of\ndiversification and limited information on Kelly-optimal portfolios. In\nparticular we present approximate formulas for optimizing diversified\nportfolios and exact results for optimal investment in unknown games where the\nonly available information is past outcomes.\n", "versions": [{"version": "v1", "created": "Mon, 10 Mar 2008 08:57:31 GMT"}, {"version": "v2", "created": "Mon, 7 Jul 2008 10:04:36 GMT"}], "update_date": "2008-12-10", "authors_parsed": [["Medo", "Matus", ""], ["Pis'mak", "Yury M.", ""], ["Zhang", "Yi-Cheng", ""]]}, {"id": "0803.2549", "submitter": "Betsabe Blas", "authors": "Betsab\\'e G. Blas Achic and M\\^onica C. Sandoval", "title": "Heteroscedastic controlled calibration model applied to analytical\n  chemistry", "comments": "LaTex, 12 pages and 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In chemical analysis made by laboratories one has the problem of determining\nthe concentration of a chemical element in a sample. In order to tackle this\nproblem the guide EURACHEM/CITAC recommends the application of the linear\ncalibration model, so implicitly assume that there is no measurement error in\nthe independent variable $X$. In this work, it is proposed a new calibration\nmodel assuming that the independent variable is controlled. This assumption is\nappropriate in chemical analysis where the process tempting to attain the fixed\nknown value $X$ generates an error and the resulting value is $x$, which is not\nan observable. However, observations on its surrogate $X$ are available. A\nsimulation study is carried out in order to verify some properties of the\nestimators derived for the new model and it is also considered the usual\ncalibration model to compare it with the new approach. Three applications are\nconsidered to verify the performance of the new approach.\n", "versions": [{"version": "v1", "created": "Mon, 17 Mar 2008 23:17:04 GMT"}], "update_date": "2008-03-19", "authors_parsed": [["Achic", "Betsab\u00e9 G. Blas", ""], ["Sandoval", "M\u00f4nica C.", ""]]}, {"id": "0803.2622", "submitter": "Francois-Xavier Dupe", "authors": "Fran\\c{c}ois-Xavier Dup\\'e (GREYC), Jalal Fadili (GREYC), Jean Luc\n  Starck (SEDI)", "title": "Deconvolution of confocal microscopy images using proximal iteration and\n  sparse representations", "comments": null, "journal-ref": "Biomedical Imaging: From Nano to Macro, 2008. ISBI 2008. 5th IEEE\n  International Symposium on, Paris : France (2008)", "doi": "10.1109/ISBI.2008.4541101", "report-no": null, "categories": "stat.AP math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deconvolution algorithm for images blurred and degraded by a\nPoisson noise. The algorithm uses a fast proximal backward-forward splitting\niteration. This iteration minimizes an energy which combines a\n\\textit{non-linear} data fidelity term, adapted to Poisson noise, and a\nnon-smooth sparsity-promoting regularization (e.g $\\ell_1$-norm) over the image\nrepresentation coefficients in some dictionary of transforms (e.g. wavelets,\ncurvelets). Our results on simulated microscopy images of neurons and cells are\nconfronted to some state-of-the-art algorithms. They show that our approach is\nvery competitive, and as expected, the importance of the non-linearity due to\nPoisson noise is more salient at low and medium intensities. Finally an\nexperiment on real fluorescent confocal microscopy data is reported.\n", "versions": [{"version": "v1", "created": "Tue, 18 Mar 2008 13:04:20 GMT"}, {"version": "v2", "created": "Fri, 13 Jun 2008 07:07:55 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Dup\u00e9", "Fran\u00e7ois-Xavier", "", "GREYC"], ["Fadili", "Jalal", "", "GREYC"], ["Starck", "Jean Luc", "", "SEDI"]]}, {"id": "0803.2623", "submitter": "Francois-Xavier Dupe", "authors": "Fran\\c{c}ois-Xavier Dup\\'e (GREYC), Jalal Fadili (GREYC), Jean Luc\n  Starck (SEDI)", "title": "A proximal iteration for deconvolving Poisson noisy images using sparse\n  representations", "comments": null, "journal-ref": "IEEE Transactions on Image Processing (2008) \\`a para\\^itre", "doi": "10.1109/TIP.2008.2008223", "report-no": null, "categories": "stat.AP math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an image deconvolution algorithm when the data is contaminated by\nPoisson noise. The image to restore is assumed to be sparsely represented in a\ndictionary of waveforms such as the wavelet or curvelet transforms. Our key\ncontributions are: First, we handle the Poisson noise properly by using the\nAnscombe variance stabilizing transform leading to a {\\it non-linear}\ndegradation equation with additive Gaussian noise. Second, the deconvolution\nproblem is formulated as the minimization of a convex functional with a\ndata-fidelity term reflecting the noise properties, and a non-smooth\nsparsity-promoting penalties over the image representation coefficients (e.g.\n$\\ell_1$-norm). Third, a fast iterative backward-forward splitting algorithm is\nproposed to solve the minimization problem. We derive existence and uniqueness\nconditions of the solution, and establish convergence of the iterative\nalgorithm. Finally, a GCV-based model selection procedure is proposed to\nobjectively select the regularization parameter. Experimental results are\ncarried out to show the striking benefits gained from taking into account the\nPoisson statistics of the noise. These results also suggest that using\nsparse-domain regularization may be tractable in many deconvolution\napplications with Poisson noise such as astronomy and microscopy.\n", "versions": [{"version": "v1", "created": "Tue, 18 Mar 2008 13:13:20 GMT"}, {"version": "v2", "created": "Wed, 27 Aug 2008 13:55:12 GMT"}], "update_date": "2009-11-13", "authors_parsed": [["Dup\u00e9", "Fran\u00e7ois-Xavier", "", "GREYC"], ["Fadili", "Jalal", "", "GREYC"], ["Starck", "Jean Luc", "", "SEDI"]]}, {"id": "0803.2679", "submitter": "Giacomo Aletti", "authors": "Giacomo Aletti, Enea G. Bongiorno, Vincenzo Capasso", "title": "Statistical aspects of birth--and--growth stochastic processes", "comments": "simpler notations typos", "journal-ref": null, "doi": "10.1016/j.fss.2008.12.011", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The paper considers a particular family of set--valued stochastic processes\nmodeling birth--and--growth processes. The proposed setting allows us to\ninvestigate the nucleation and the growth processes. A decomposition theorem is\nestablished to characterize the nucleation and the growth. As a consequence,\ndifferent consistent set--valued estimators are studied for growth process.\nMoreover, the nucleation process is studied via the hitting function, and a\nconsistent estimator of the nucleation hitting function is derived.\n", "versions": [{"version": "v1", "created": "Tue, 18 Mar 2008 17:16:49 GMT"}, {"version": "v2", "created": "Sat, 19 Jul 2008 16:56:36 GMT"}, {"version": "v3", "created": "Thu, 25 Sep 2008 10:25:45 GMT"}], "update_date": "2011-09-29", "authors_parsed": [["Aletti", "Giacomo", ""], ["Bongiorno", "Enea G.", ""], ["Capasso", "Vincenzo", ""]]}, {"id": "0803.3343", "submitter": "Wafik Hachicha", "authors": "Wafik Hachicha (U2MP), Faouzi Masmoudi (U2MP), Mohamed Haddar (U2MP)", "title": "Principal component analysis model for machine-part cell formation\n  problem in group technology", "comments": null, "journal-ref": "The International Conference on Advances in Mechanical Engineering\n  and Mechanics (ICAMEM 2006) Tunisie (2006)", "doi": null, "report-no": null, "categories": "stat.AP physics.class-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of forming machine cell in cellular\nmanufacturing (CM). The major problem in the design of a CM system is to\nidentify the part families and machine groups and consequently to form\nmanufacturing cells. The aim of this article is to formulate a multivariate\napproach based on a correlation analysis for solving cell formation problem.\nThe proposed approach is carried out in two phases. In the first phase, the\ncorrelation matrix is used as an original similarity coefficient matrix. In the\nsecond phase, Principal Component Analysis (PCA) is applied to find the\neigenvalues and eigenvectors on the correlation similarity matrix. A scatter\nplot analysis as a cluster analysis is applied to make machine groups while\nmaximizing correlation between elements. A numerical example for the design of\ncell structures is provided in order to illustrate the proposed approach. The\nresults of a comparative study based on multiple performance criteria show that\nthe present approach is very effective, efficient and practical\n", "versions": [{"version": "v1", "created": "Sun, 23 Mar 2008 20:28:27 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Hachicha", "Wafik", "", "U2MP"], ["Masmoudi", "Faouzi", "", "U2MP"], ["Haddar", "Mohamed", "", "U2MP"]]}, {"id": "0803.3436", "submitter": "Nataliya Malyshkina", "authors": "Nataliya V. Malyshkina", "title": "Influence of Speed Limit on Roadway Safety in Indiana", "comments": "MS thesis (with additional data included), 161 pages, 16 figures, 20\n  tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The influence of speed limits on roadway safety is an extremely important\nsocial issue and is subject to an extensive debate in the State of Indiana and\nnationwide. With around 800-900 fatalities and thousands of injuries annually\nin Indiana, traffic accidents place an incredible social and economic burden on\nthe state. Still, speed limits posted on highways and other roads are routinely\nexceeded as individual drivers try to balance safety and mobility (speed). This\nresearch explores the relationship between speed limits and roadway safety.\nNamely, the research focuses on the influence of the posted speed limit on the\ncausation and severity of accidents. Data on individual accidents from the\nIndiana Electronic Vehicle Crash Record System is used in the research, and\nappropriate statistical models are estimated for causation and severity of\ndifferent types of accidents on all road classes. The results of the modeling\nshow that speed limits do not have a statistically significant adverse effect\non unsafe-speed-related causation of accidents on all roads, but generally\nincrease the severity of accidents on the majority of roads other than highways\n(the accident severity on highways is unaffected by speed limits). Our findings\ncan perhaps save both lives and travel time by helping the Indiana Department\nof Transportation determine optimal speed limit policies in the state.\n", "versions": [{"version": "v1", "created": "Mon, 24 Mar 2008 17:22:02 GMT"}], "update_date": "2008-03-25", "authors_parsed": [["Malyshkina", "Nataliya V.", ""]]}, {"id": "0803.3675", "submitter": "Jean-Marc Bardet", "authors": "Imen Kammoun (CES, SAMOS), V\\'eronique Billat (LEPHE), Jean-Marc\n  Bardet (CES, SAMOS)", "title": "A new stochastic process to model Heart Rate series during exhaustive\n  run and an estimator of its fractality parameter", "comments": null, "journal-ref": "Journal of Applied Statistics (2011) 1-24", "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to interpret and explain the physiological signal behaviors, it can\nbe interesting to find some constants among the fluctuations of these data\nduring all the effort or during different stages of the race (which can be\ndetected using a change points detection method). Several recent papers have\nproposed the long-range dependence (Hurst) parameter as such a constant.\nHowever, their results induce two main problems. Firstly, DFA method is usually\napplied for estimating this parameter. Clearly, such a method does not provide\nthe most efficient estimator and moreover it is not at all robust even in the\ncase of smooth trends. Secondly, this method often gives estimated Hurst\nparameters larger than 1, which is the larger possible value for long memory\nstationary processes. In this article we propose solutions for both these\nproblems and we define a new model allowing such estimated parameters.\n", "versions": [{"version": "v1", "created": "Wed, 26 Mar 2008 07:47:55 GMT"}], "update_date": "2011-12-06", "authors_parsed": [["Kammoun", "Imen", "", "CES, SAMOS"], ["Billat", "V\u00e9ronique", "", "LEPHE"], ["Bardet", "Jean-Marc", "", "CES, SAMOS"]]}, {"id": "0803.3678", "submitter": "Stephen E. Fienberg", "authors": "Stephen E. Fienberg", "title": "Editorial: Statistics and \"The lost tomb of Jesus\"", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS162 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2008, Vol. 2, No. 1, 1-2", "doi": "10.1214/08-AOAS162", "report-no": "IMS-AOAS-AOAS162", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What makes a problem suitable for statistical analysis? Are historical and\nreligious questions addressable using statistical calculations? Such issues\nhave long been debated in the statistical community and statisticians and\nothers have used historical information and texts to analyze such questions as\nthe economics of slavery, the authorship of the Federalist Papers and the\nquestion of the existence of God. But what about historical and religious\nattributions associated with information gathered from archeological finds? In\n1980, a construction crew working in the Jerusalem neighborhood of East Talpiot\nstumbled upon a crypt. Archaeologists from the Israel Antiquities Authority\ncame to the scene and found 10 limestone burial boxes, known as ossuaries, in\nthe crypt. Six of these had inscriptions. The remains found in the ossuaries\nwere reburied, as required by Jewish religious tradition, and the ossuaries\nwere catalogued and stored in a warehouse. The inscriptions on the ossuaries\nwere catalogued and published by Rahmani (1994) and by Kloner (1996) but there\nreports did not receive widespread public attention. Fast forward to March\n2007, when a television ``docudrama'' aired on The Discovery Channel entitled\n``The Lost Tomb of Jesus'' touched off a public and religious controversy--one\nonly need think about the title to see why there might be a controversy! The\nprogram, and a simultaneously published book [Jacobovici and Pellegrino\n(2007)], described the ``rediscovery'' of the East Talpiot archeological find\nand they presented interpretations of the ossuary inscriptions from a number of\nperspectives. Among these was a statistical calculation attributed to the\nstatistician Andrey Feuerverger: ``that the odds that all six names would\nappear together in one tomb are 1 in 600, calculated conservatively--or\npossibly even as much as one in one million.''\n", "versions": [{"version": "v1", "created": "Wed, 26 Mar 2008 08:51:38 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Fienberg", "Stephen E.", ""]]}, {"id": "0803.3697", "submitter": "Lawrence D. Brown", "authors": "Lawrence D. Brown", "title": "In-season prediction of batting averages: A field test of empirical\n  Bayes and Bayes methodologies", "comments": "Published in at http://dx.doi.org/10.1214/07-AOAS138 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2008, Vol. 2, No. 1, 113-152", "doi": "10.1214/07-AOAS138", "report-no": "IMS-AOAS-AOAS138", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batting average is one of the principle performance measures for an\nindividual baseball player. It is natural to statistically model this as a\nbinomial-variable proportion, with a given (observed) number of qualifying\nattempts (called ``at-bats''), an observed number of successes (``hits'')\ndistributed according to the binomial distribution, and with a true (but\nunknown) value of $p_i$ that represents the player's latent ability. This is a\ncommon data structure in many statistical applications; and so the\nmethodological study here has implications for such a range of applications. We\nlook at batting records for each Major League player over the course of a\nsingle season (2005). The primary focus is on using only the batting records\nfrom an earlier part of the season (e.g., the first 3 months) in order to\nestimate the batter's latent ability, $p_i$, and consequently, also to predict\ntheir batting-average performance for the remainder of the season. Since we are\nusing a season that has already concluded, we can then validate our estimation\nperformance by comparing the estimated values to the actual values for the\nremainder of the season. The prediction methods to be investigated are\nmotivated from empirical Bayes and hierarchical Bayes interpretations. A newly\nproposed nonparametric empirical Bayes procedure performs particularly well in\nthe basic analysis of the full data set, though less well with analyses\ninvolving more homogeneous subsets of the data. In those more homogeneous\nsituations better performance is obtained from appropriate versions of more\nfamiliar methods. In all situations the poorest performing choice is the\nna\\\"{{\\i}}ve predictor which directly uses the current average to predict the\nfuture average.\n", "versions": [{"version": "v1", "created": "Wed, 26 Mar 2008 11:00:46 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Brown", "Lawrence D.", ""]]}, {"id": "0803.3740", "submitter": "Armin Schwartzman", "authors": "Armin Schwartzman, Robert F. Dougherty, Jonathan E. Taylor", "title": "False discovery rate analysis of brain diffusion direction maps", "comments": "Published in at http://dx.doi.org/10.1214/07-AOAS133 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2008, Vol. 2, No. 1, 153-175", "doi": "10.1214/07-AOAS133", "report-no": "IMS-AOAS-AOAS133", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion tensor imaging (DTI) is a novel modality of magnetic resonance\nimaging that allows noninvasive mapping of the brain's white matter. A\nparticular map derived from DTI measurements is a map of water principal\ndiffusion directions, which are proxies for neural fiber directions. We\nconsider a study in which diffusion direction maps were acquired for two groups\nof subjects. The objective of the analysis is to find regions of the brain in\nwhich the corresponding diffusion directions differ between the groups. This is\nattained by first computing a test statistic for the difference in direction at\nevery brain location using a Watson model for directional data. Interesting\nlocations are subsequently selected with control of the false discovery rate.\nMore accurate modeling of the null distribution is obtained using an empirical\nnull density based on the empirical distribution of the test statistics across\nthe brain. Further, substantial improvements in power are achieved by local\nspatial averaging of the test statistic map. Although the focus is on one\nparticular study and imaging technology, the proposed inference methods can be\napplied to other large scale simultaneous hypothesis testing problems with a\ncontinuous underlying spatial structure.\n", "versions": [{"version": "v1", "created": "Wed, 26 Mar 2008 14:56:57 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Schwartzman", "Armin", ""], ["Dougherty", "Robert F.", ""], ["Taylor", "Jonathan E.", ""]]}, {"id": "0803.3757", "submitter": "David A. Freedman", "authors": "David A. Freedman", "title": "On regression adjustments in experiments with several treatments", "comments": "Published in at http://dx.doi.org/10.1214/07-AOAS143 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2008, Vol. 2, No. 1, 176-196", "doi": "10.1214/07-AOAS143", "report-no": "IMS-AOAS-AOAS143", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression adjustments are often made to experimental data. Since\nrandomization does not justify the models, bias is likely; nor are the usual\nvariance calculations to be trusted. Here, we evaluate regression adjustments\nusing Neyman's nonparametric model. Previous results are generalized, and more\nintuitive proofs are given. A bias term is isolated, and conditions are given\nfor unbiased estimation in finite samples.\n", "versions": [{"version": "v1", "created": "Wed, 26 Mar 2008 15:54:26 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Freedman", "David A.", ""]]}, {"id": "0803.3863", "submitter": "Bradley Efron", "authors": "Bradley Efron", "title": "Simultaneous inference: When should hypothesis testing problems be\n  combined?", "comments": "Published in at http://dx.doi.org/10.1214/07-AOAS141 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2008, Vol. 2, No. 1, 197-223", "doi": "10.1214/07-AOAS141", "report-no": "IMS-AOAS-AOAS141", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern statisticians are often presented with hundreds or thousands of\nhypothesis testing problems to evaluate at the same time, generated from new\nscientific technologies such as microarrays, medical and satellite imaging\ndevices, or flow cytometry counters. The relevant statistical literature tends\nto begin with the tacit assumption that a single combined analysis, for\ninstance, a False Discovery Rate assessment, should be applied to the entire\nset of problems at hand. This can be a dangerous assumption, as the examples in\nthe paper show, leading to overly conservative or overly liberal conclusions\nwithin any particular subclass of the cases. A simple Bayesian theory yields a\nsuccinct description of the effects of separation or combination on false\ndiscovery rate analyses. The theory allows efficient testing within small\nsubclasses, and has applications to ``enrichment,'' the detection of multi-case\neffects.\n", "versions": [{"version": "v1", "created": "Thu, 27 Mar 2008 07:01:08 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Efron", "Bradley", ""]]}, {"id": "0803.3872", "submitter": "Ji Zhu", "authors": "Elizaveta Levina, Adam Rothman, Ji Zhu", "title": "Sparse estimation of large covariance matrices via a nested Lasso\n  penalty", "comments": "Published in at http://dx.doi.org/10.1214/07-AOAS139 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2008, Vol. 2, No. 1, 245-263", "doi": "10.1214/07-AOAS139", "report-no": "IMS-AOAS-AOAS139", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper proposes a new covariance estimator for large covariance matrices\nwhen the variables have a natural ordering. Using the Cholesky decomposition of\nthe inverse, we impose a banded structure on the Cholesky factor, and select\nthe bandwidth adaptively for each row of the Cholesky factor, using a novel\npenalty we call nested Lasso. This structure has more flexibility than regular\nbanding, but, unlike regular Lasso applied to the entries of the Cholesky\nfactor, results in a sparse estimator for the inverse of the covariance matrix.\nAn iterative algorithm for solving the optimization problem is developed. The\nestimator is compared to a number of other covariance estimators and is shown\nto do best, both in simulations and on a real data example. Simulations show\nthat the margin by which the estimator outperforms its competitors tends to\nincrease with dimension.\n", "versions": [{"version": "v1", "created": "Thu, 27 Mar 2008 09:03:19 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Levina", "Elizaveta", ""], ["Rothman", "Adam", ""], ["Zhu", "Ji", ""]]}, {"id": "0803.3875", "submitter": "Charles F. Manski", "authors": "Charles F. Manski, Francesca Molinari", "title": "Skip sequencing: A decision problem in questionnaire design", "comments": "Published in at http://dx.doi.org/10.1214/07-AOAS134 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2008, Vol. 2, No. 1, 264-285", "doi": "10.1214/07-AOAS134", "report-no": "IMS-AOAS-AOAS134", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies questionnaire design as a formal decision problem,\nfocusing on one element of the design process: skip sequencing. We propose that\na survey planner use an explicit loss function to quantify the trade-off\nbetween cost and informativeness of the survey and aim to make a design choice\nthat minimizes loss. We pose a choice between three options: ask all\nrespondents about an item of interest, use skip sequencing, thereby asking the\nitem only of respondents who give a certain answer to an opening question, or\ndo not ask the item at all. The first option is most informative but also most\ncostly. The use of skip sequencing reduces respondent burden and the cost of\ninterviewing, but may spread data quality problems across survey items, thereby\nreducing informativeness. The last option has no cost but is completely\nuninformative about the item of interest. We show how the planner may choose\namong these three options in the presence of two inferential problems, item\nnonresponse and response error.\n", "versions": [{"version": "v1", "created": "Thu, 27 Mar 2008 09:29:30 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Manski", "Charles F.", ""], ["Molinari", "Francesca", ""]]}, {"id": "0803.3876", "submitter": "Tong Tong Wu", "authors": "Tong Tong Wu, Kenneth Lange", "title": "Coordinate descent algorithms for lasso penalized regression", "comments": "Published in at http://dx.doi.org/10.1214/07-AOAS147 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2008, Vol. 2, No. 1, 224-244", "doi": "10.1214/07-AOAS147", "report-no": "IMS-AOAS-AOAS147", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imposition of a lasso penalty shrinks parameter estimates toward zero and\nperforms continuous model selection. Lasso penalized regression is capable of\nhandling linear regression problems where the number of predictors far exceeds\nthe number of cases. This paper tests two exceptionally fast algorithms for\nestimating regression coefficients with a lasso penalty. The previously known\n$\\ell_2$ algorithm is based on cyclic coordinate descent. Our new $\\ell_1$\nalgorithm is based on greedy coordinate descent and Edgeworth's algorithm for\nordinary $\\ell_1$ regression. Each algorithm relies on a tuning constant that\ncan be chosen by cross-validation. In some regression problems it is natural to\ngroup parameters and penalize parameters group by group rather than separately.\nIf the group penalty is proportional to the Euclidean norm of the parameters of\nthe group, then it is possible to majorize the norm and reduce parameter\nestimation to $\\ell_2$ regression with a lasso penalty. Thus, the existing\nalgorithm can be extended to novel settings. Each of the algorithms discussed\nis tested via either simulated or real data or both. The Appendix proves that a\ngreedy form of the $\\ell_2$ algorithm converges to the minimum value of the\nobjective function.\n", "versions": [{"version": "v1", "created": "Thu, 27 Mar 2008 09:35:00 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Wu", "Tong Tong", ""], ["Lange", "Kenneth", ""]]}, {"id": "0803.3881", "submitter": "William T. Barry", "authors": "William T. Barry, Andrew B. Nobel, Fred A. Wright", "title": "A statistical framework for testing functional categories in microarray\n  data", "comments": "Published in at http://dx.doi.org/10.1214/07-AOAS146 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2008, Vol. 2, No. 1, 286-315", "doi": "10.1214/07-AOAS146", "report-no": "IMS-AOAS-AOAS146", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ready access to emerging databases of gene annotation and functional pathways\nhas shifted assessments of differential expression in DNA microarray studies\nfrom single genes to groups of genes with shared biological function. This\npaper takes a critical look at existing methods for assessing the differential\nexpression of a group of genes (functional category), and provides some\nsuggestions for improved performance. We begin by presenting a general\nframework, in which the set of genes in a functional category is compared to\nthe complementary set of genes on the array. The framework includes tests for\noverrepresentation of a category within a list of significant genes, and\nmethods that consider continuous measures of differential expression. Existing\ntests are divided into two classes. Class 1 tests assume gene-specific measures\nof differential expression are independent, despite overwhelming evidence of\npositive correlation. Analytic and simulated results are presented that\ndemonstrate Class 1 tests are strongly anti-conservative in practice. Class 2\ntests account for gene correlation, typically through array permutation that by\nconstruction has proper Type I error control for the induced null. However,\nboth Class 1 and Class 2 tests use a null hypothesis that all genes have the\nsame degree of differential expression. We introduce a more sensible and\ngeneral (Class 3) null under which the profile of differential expression is\nthe same within the category and complement. Under this broader null, Class 2\ntests are shown to be conservative. We propose standard bootstrap methods for\ntesting against the Class 3 null and demonstrate they provide valid Type I\nerror control and more power than array permutation in simulated datasets and\nreal microarray experiments.\n", "versions": [{"version": "v1", "created": "Thu, 27 Mar 2008 10:24:55 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Barry", "William T.", ""], ["Nobel", "Andrew B.", ""], ["Wright", "Fred A.", ""]]}, {"id": "0803.3891", "submitter": "Maarten J. L. F. Cruyff", "authors": "Maarten J. L. F. Cruyff, Ulf B\\\"ockenholt, Ardo van den Hout, Peter G.\n  M. van der Heijden", "title": "Accounting for self-protective responses in randomized response data\n  from a social security survey using the zero-inflated Poisson model", "comments": "Published in at http://dx.doi.org/10.1214/07-AOAS135 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2008, Vol. 2, No. 1, 316-331", "doi": "10.1214/07-AOAS135", "report-no": "IMS-AOAS-AOAS135", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2004 the Dutch Department of Social Affairs conducted a survey to assess\nthe extent of noncompliance with social security regulations. The survey was\nconducted among 870 recipients of social security benefits and included a\nseries of sensitive questions about regulatory noncompliance. Due to the\nsensitive nature of the questions the randomized response design was used.\nAlthough randomized response protects the privacy of the respondent, it is\nunlikely that all respondents followed the design. In this paper we introduce a\nmodel that allows for respondents displaying self-protective response behavior\nby consistently giving the nonincriminating response, irrespective of the\noutcome of the randomizing device. The dependent variable denoting the total\nnumber of incriminating responses is assumed to be generated by the application\nof randomized response to a latent Poisson variable denoting the true number of\nrule violations. Since self-protective responses result in an excess of\nobserved zeros in relation to the Poisson randomized response distribution,\nthese are modeled as observed zero-inflation. The model includes predictors of\nthe Poisson parameters, as well as predictors of the probability of\nself-protective response behavior.\n", "versions": [{"version": "v1", "created": "Thu, 27 Mar 2008 11:12:38 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Cruyff", "Maarten J. L. F.", ""], ["B\u00f6ckenholt", "Ulf", ""], ["Hout", "Ardo van den", ""], ["van der Heijden", "Peter G. M.", ""]]}, {"id": "0803.3904", "submitter": "Nancy R. Zhang", "authors": "Nancy R. Zhang, Mary C. Wildermuth, Terence P. Speed", "title": "Transcription factor binding site prediction with multivariate gene\n  expression data", "comments": "Published in at http://dx.doi.org/10.1214/10.1214/07-AOAS142 the\n  Annals of Applied Statistics (http://www.imstat.org/aoas/) by the Institute\n  of Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2008, Vol. 2, No. 1, 332-365", "doi": "10.1214/10.1214/07-AOAS142", "report-no": "IMS-AOAS-AOAS142", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-sample microarray experiments have become a standard experimental\nmethod for studying biological systems. A frequent goal in such studies is to\nunravel the regulatory relationships between genes. During the last few years,\nregression models have been proposed for the de novo discovery of cis-acting\nregulatory sequences using gene expression data. However, when applied to\nmulti-sample experiments, existing regression based methods model each\nindividual sample separately. To better capture the dynamic relationships in\nmulti-sample microarray experiments, we propose a flexible method for the joint\nmodeling of promoter sequence and multivariate expression data. In higher order\neukaryotic genomes expression regulation usually involves combinatorial\ninteraction between several transcription factors. Experiments have shown that\nspacing between transcription factor binding sites can significantly affect\ntheir strength in activating gene expression. We propose an adaptive model\nbuilding procedure to capture such spacing dependent cis-acting regulatory\nmodules. We apply our methods to the analysis of microarray time-course\nexperiments in yeast and in Arabidopsis. These experiments exhibit very\ndifferent dynamic temporal relationships. For both data sets, we have found all\nof the well-known cis-acting regulatory elements in the related context, as\nwell as being able to predict novel elements.\n", "versions": [{"version": "v1", "created": "Thu, 27 Mar 2008 12:31:37 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Zhang", "Nancy R.", ""], ["Wildermuth", "Mary C.", ""], ["Speed", "Terence P.", ""]]}, {"id": "0803.3911", "submitter": "Tathagata Banerjee", "authors": "Tathagata Banerjee, Rahul Mukerjee", "title": "Optimal factorial designs for cDNA microarray experiments", "comments": "Published in at http://dx.doi.org/10.1214/07-AOAS144 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2007, Vol. 2, No. 1, 366-385", "doi": "10.1214/07-AOAS144", "report-no": "IMS-AOAS-AOAS144", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider cDNA microarray experiments when the cell populations have a\nfactorial structure, and investigate the problem of their optimal designing\nunder a baseline parametrization where the objects of interest differ from\nthose under the more common orthogonal parametrization. First, analytical\nresults are given for the $2\\times 2$ factorial. Since practical applications\noften involve a more complex factorial structure, we next explore general\nfactorials and obtain a collection of optimal designs in the saturated, that\nis, most economic, case. This, in turn, is seen to yield an approach for\nfinding optimal or efficient designs in the practically more important nearly\nsaturated cases. Thereafter, the findings are extended to the more intricate\nsituation where the underlying model incorporates dye-coloring effects, and the\nrole of dye-swapping is critically examined.\n", "versions": [{"version": "v1", "created": "Thu, 27 Mar 2008 12:54:40 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Banerjee", "Tathagata", ""], ["Mukerjee", "Rahul", ""]]}, {"id": "0803.3919", "submitter": "Li Qin", "authors": "Li Qin, Peter B. Gilbert, Dean Follmann, Dongfeng Li", "title": "Assessing surrogate endpoints in vaccine trials with case-cohort\n  sampling and the Cox model", "comments": "Published in at http://dx.doi.org/10.1214/07-AOAS132 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2008, Vol. 2, No. 1, 386-407", "doi": "10.1214/07-AOAS132", "report-no": "IMS-AOAS-AOAS132", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing immune responses to study vaccines as surrogates of protection\nplays a central role in vaccine clinical trials. Motivated by three ongoing or\npending HIV vaccine efficacy trials, we consider such surrogate endpoint\nassessment in a randomized placebo-controlled trial with case-cohort sampling\nof immune responses and a time to event endpoint. Based on the principal\nsurrogate definition under the principal stratification framework proposed by\nFrangakis and Rubin [Biometrics 58 (2002) 21--29] and adapted by Gilbert and\nHudgens (2006), we introduce estimands that measure the value of an immune\nresponse as a surrogate of protection in the context of the Cox proportional\nhazards model. The estimands are not identified because the immune response to\nvaccine is not measured in placebo recipients. We formulate the problem as a\nCox model with missing covariates, and employ novel trial designs for\npredicting the missing immune responses and thereby identifying the estimands.\nThe first design utilizes information from baseline predictors of the immune\nresponse, and bridges their relationship in the vaccine recipients to the\nplacebo recipients. The second design provides a validation set for the\nunmeasured immune responses of uninfected placebo recipients by immunizing them\nwith the study vaccine after trial closeout. A maximum estimated likelihood\napproach is proposed for estimation of the parameters. Simulated data examples\nare given to evaluate the proposed designs and study their properties.\n", "versions": [{"version": "v1", "created": "Thu, 27 Mar 2008 13:26:33 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Qin", "Li", ""], ["Gilbert", "Peter B.", ""], ["Follmann", "Dean", ""], ["Li", "Dongfeng", ""]]}, {"id": "0803.3942", "submitter": "Hongzhe Li", "authors": "Zhi Wei, Hongzhe Li", "title": "A hidden spatial-temporal Markov random field model for network-based\n  analysis of time course gene expression data", "comments": "Published in at http://dx.doi.org/10.1214/07--AOAS145 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2008, Vol. 2, No. 1, 408-429", "doi": "10.1214/07--AOAS145", "report-no": "IMS-AOAS-AOAS145", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microarray time course (MTC) gene expression data are commonly collected to\nstudy the dynamic nature of biological processes. One important problem is to\nidentify genes that show different expression profiles over time and pathways\nthat are perturbed during a given biological process. While methods are\navailable to identify the genes with differential expression levels over time,\nthere is a lack of methods that can incorporate the pathway information in\nidentifying the pathways being modified/activated during a biological process.\nIn this paper we develop a hidden spatial-temporal Markov random field\n(hstMRF)-based method for identifying genes and subnetworks that are related to\nbiological processes, where the dependency of the differential expression\npatterns of genes on the networks are modeled over time and over the network of\npathways. Simulation studies indicated that the method is quite effective in\nidentifying genes and modified subnetworks and has higher sensitivity than the\ncommonly used procedures that do not use the pathway structure or time\ndependency information, with similar false discovery rates. Application to a\nmicroarray gene expression study of systemic inflammation in humans identified\na core set of genes on the KEGG pathways that show clear differential\nexpression patterns over time. In addition, the method confirmed that the\nTOLL-like signaling pathway plays an important role in immune response to\nendotoxins.\n", "versions": [{"version": "v1", "created": "Thu, 27 Mar 2008 14:37:35 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Wei", "Zhi", ""], ["Li", "Hongzhe", ""]]}, {"id": "0803.4055", "submitter": "John E. Fiorentino", "authors": "John E. Fiorentino", "title": "Letter to the editor. NAA and JFK: Can revisionism take us home?", "comments": "Published in at http://dx.doi.org/10.1214/07-AOAS153 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2008, Vol. 2, No. 1, 430-431", "doi": "10.1214/07-AOAS153", "report-no": "IMS-AOAS-AOAS153", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Occasionally during the course of the human learning experience we are faced\nwith an anomaly. An aberration of sorts, which try as we might, defies\nappropriate classification. The recent paper by Spiegelman et al.--Chemical and\nforensic analysis of JFK assassination bullet lots: Is a second shooter\npossible?--is one such aberration. It is riddled with both misconceptions and\nerrors of fact. Purporting to cast doubt on the NAA (neutron activation\nanalysis) work conducted by Dr. Vincent Guinn in the investigation of the\nassassination of President John F. Kennedy, it fails miserably. The paper\noffers two central conclusions, one which is demonstrably false, and the other\nwhich is specious. The authors opine; ``If the assassination fragments are\nderived from three or more separate bullets, then a second assassin is likely,\nas the additional bullet would not be attributable to the main suspect, Mr.\nOswald.'' This statement relating to the likelihood of a second assassin based\non the premise of three or more separate bullets is demonstrably false. The\navailable evidence indicates that Oswald fired three shots, one of which is\nbelieved to have missed. However, on the off chance that all three shots hit\n(even though there is absolutely no other supporting forensic evidence for such\na notion) those three shots alone in no way would indicate then that ``a second\nassassin is likely.'' The authors' erroneous conclusion was achieved because\nthey have either been misled (which I personally believe is the case) or they\nsimply aren't familiar with the evidence.\n", "versions": [{"version": "v1", "created": "Fri, 28 Mar 2008 07:47:41 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Fiorentino", "John E.", ""]]}, {"id": "0803.4057", "submitter": "Clifford Spiegelman", "authors": "Clifford Spiegelman, S. J. Sheather, W. A. Tobin, W. D. James, S.\n  Wexler, D. M. Roundhill", "title": "Response to the Letter to the Editor", "comments": "Published in at http://dx.doi.org/10.1214/07-AOAS154 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2008, Vol. 2, No. 1, 432-433", "doi": "10.1214/07-AOAS154", "report-no": "IMS-AOAS-AOAS154", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper has attracted interest around the world from the media (both TV\nand newspapers). In addition, we have received letters, emails and telephone\ncalls. One of our favorites was a voicemail message asking us to return a call\nto Australia at which point we would learn who really killed JFK. We welcome\nthe opportunity to respond to the letter to the editor from Mr. Fiorentino. Mr.\nFiorentino claims that our ``statement relating to the likelihood of a second\nassassin based on the premise of three or more separate bullets is demonstrably\nfalse.'' In response we would like to simply quote from page 327 of Gerald\nPosner's book Case Closed, one of the most well known works supporting the\nsingle assassin theory: ``If Connally was hit by another bullet, it had to be\nfired from a second shooter, since the Warren Commission's own reconstructions\nshowed that Oswald could not have operated the bolt and refired in 1.4\nseconds.'' Mr. Fiorentino also claims that the ``second fatal flaw is the use\nof a rather uncomplicated formula based on Bayes Theorem.'' Let $E$ denote the\nevidence and $T$ denote the theory that there were just two bullets (and hence\na single shooter). We used Bayes Theorem to hypothetically calculate $P(T|E)$\nfrom $P(E|T)$ and the prior probability $P(T)$. In order to make $P(T|E)$ ten\ntimes more likely than $P(\\bar{T}|E)$, the ratio of the prior probabilities\n[i.e., $P(T) / P(\\bar{T})$] would have to be greater than 15. Thus, we again\nconclude that this casts serious doubt on Dr. Guinn's conclusion that the\nevidence supported just two bullets. Sadly, this is far from the first time\nthat probability has been misunderstood and/or misapplied in a case of public\ninterest. A notable British example is the Clark case. See Nobles and Schiff\n(2005) for details. Finally, we welcome and, in fact, encourage members of the\nscientific community to provide alternative analyses of the data.\n", "versions": [{"version": "v1", "created": "Fri, 28 Mar 2008 08:00:30 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Spiegelman", "Clifford", ""], ["Sheather", "S. J.", ""], ["Tobin", "W. A.", ""], ["James", "W. D.", ""], ["Wexler", "S.", ""], ["Roundhill", "D. M.", ""]]}, {"id": "0803.4065", "submitter": "Yulan Liang", "authors": "Yulan Liang, Arpad Kelemen", "title": "Statistical advances and challenges for analyzing correlated high\n  dimensional SNP data in genomic study for complex diseases", "comments": "Published in at http://dx.doi.org/10.1214/07-SS026 the Statistics\n  Surveys (http://www.i-journals.org/ss/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistics Surveys 2008, Vol. 2, No. 0, 43-60", "doi": "10.1214/07-SS026", "report-no": "IMS-SS-SS_2007_26", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances of information technology in biomedical sciences and other\napplied areas have created numerous large diverse data sets with a high\ndimensional feature space, which provide us a tremendous amount of information\nand new opportunities for improving the quality of human life. Meanwhile, great\nchallenges are also created driven by the continuous arrival of new data that\nrequires researchers to convert these raw data into scientific knowledge in\norder to benefit from it. Association studies of complex diseases using SNP\ndata have become more and more popular in biomedical research in recent years.\nIn this paper, we present a review of recent statistical advances and\nchallenges for analyzing correlated high dimensional SNP data in genomic\nassociation studies for complex diseases. The review includes both general\nfeature reduction approaches for high dimensional correlated data and more\nspecific approaches for SNPs data, which include unsupervised haplotype\nmapping, tag SNP selection, and supervised SNPs selection using statistical\ntesting/scoring, statistical modeling and machine learning methods with an\nemphasis on how to identify interacting loci.\n", "versions": [{"version": "v1", "created": "Fri, 28 Mar 2008 08:46:51 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Liang", "Yulan", ""], ["Kelemen", "Arpad", ""]]}]