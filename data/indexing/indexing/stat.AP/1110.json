[{"id": "1110.0062", "submitter": "Murphy Choy", "authors": "Murphy Choy and Michelle L.F. Cheong", "title": "Identification of Demand through Statistical Distribution Modeling for\n  Improved Demand Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Demand functions for goods are generally cyclical in nature with\ncharacteristics such as trend or stochasticity. Most existing demand\nforecasting techniques in literature are designed to manage and forecast this\ntype of demand functions. However, if the demand function is lumpy in nature,\nthen the general demand forecasting techniques may fail given the unusual\ncharacteristics of the function. Proper identification of the underlying demand\nfunction and using the most appropriate forecasting technique becomes critical.\nIn this paper, we will attempt to explore the key characteristics of the\ndifferent types of demand function and relate them to known statistical\ndistributions. By fitting statistical distributions to actual past demand data,\nwe are then able to identify the correct demand functions, so that the the most\nappropriate forecasting technique can be applied to obtain improved forecasting\nresults. We applied the methodology to a real case study to show the reduction\nin forecasting errors obtained.\n", "versions": [{"version": "v1", "created": "Sat, 1 Oct 2011 01:07:57 GMT"}], "update_date": "2011-10-04", "authors_parsed": [["Choy", "Murphy", ""], ["Cheong", "Michelle L. F.", ""]]}, {"id": "1110.0205", "submitter": "Tewfik Lounis", "authors": "Tewfik Lounis (LMNO)", "title": "Asymptotically Optimal Tests when Parameters are Estimated", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main purpose of this paper is to provide an asymptotically optimal test.\nThe proposed statistic is of Neyman-Pearson-type when the parameters are\nestimated with a particular kind of estimators. It is shown that the proposed\nestimators enable us to achieve this end. Two particular cases, AR(1) and ARCH\nmodels were studied and the asymptotic power function was derived.\n", "versions": [{"version": "v1", "created": "Sun, 2 Oct 2011 16:44:29 GMT"}], "update_date": "2011-10-04", "authors_parsed": [["Lounis", "Tewfik", "", "LMNO"]]}, {"id": "1110.0532", "submitter": "Caleb Phillips", "authors": "Caleb Phillips, Lee Becker, and Elizabeth Bradley", "title": "Strange Beta: An Assistance System for Indoor Rock Climbing Route\n  Setting Using Chaotic Variations and Machine Learning", "comments": "University of Colorado Computer Science Department Technical Report", "journal-ref": "Chaos 22, 013130 (2012)", "doi": "10.1063/1.3693047", "report-no": "CU-CS-1087-11", "categories": "cs.AI cs.HC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper applies machine learning and the mathematics of chaos to the task\nof designing indoor rock-climbing routes. Chaotic variation has been used to\ngreat advantage on music and dance, but the challenges here are quite\ndifferent, beginning with the representation. We present a formalized system\nfor transcribing rock climbing problems, then describe a variation generator\nthat is designed to support human route-setters in designing new and\ninteresting climbing problems. This variation generator, termed Strange Beta,\ncombines chaos and machine learning, using the former to introduce novelty and\nthe latter to smooth transitions in a manner that is consistent with the style\nof the climbs This entails parsing the domain-specific natural language that\nrock climbers use to describe routes and movement and then learning the\npatterns in the results. We validated this approach with a pilot study in a\nsmall university rock climbing gym, followed by a large blinded study in a\ncommercial climbing gym, in cooperation with experienced climbers and expert\nroute setters. The results show that {\\sc Strange Beta} can help a human setter\nproduce routes that are at least as good as, and in some cases better than,\nthose produced in the traditional manner.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2011 22:23:46 GMT"}], "update_date": "2014-05-23", "authors_parsed": [["Phillips", "Caleb", ""], ["Becker", "Lee", ""], ["Bradley", "Elizabeth", ""]]}, {"id": "1110.0641", "submitter": "Vladimir Nikulin", "authors": "Vladimir Nikulin", "title": "Identifying relationships between drugs and medical conditions: winning\n  experience in the Challenge 2 of the OMOP 2010 Cup", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing interest in using a longitudinal observational databases\nto detect drug safety signal. In this paper we present a novel method, which we\nused online during the OMOP Cup. We consider homogeneous ensembling, which is\nbased on random re-sampling (known, also, as bagging) as a main innovation\ncompared to the previous publications in the related field. This study is based\non a very large simulated database of the 10 million patients records, which\nwas created by the Observational Medical Outcomes Partnership (OMOP). Compared\nto the traditional classification problem, the given data are unlabelled. The\nobjective of this study is to discover hidden associations between drugs and\nconditions. The main idea of the approach, which we used during the OMOP Cup is\nto compare the numbers of observed and expected patterns. This comparison may\nbe organised in several different ways, and the outcomes (base learners) may be\nquite different as well. It is proposed to construct the final decision\nfunction as an ensemble of the base learners. Our method was recognised\nformally by the Organisers of the OMOP Cup as a top performing method for the\nChallenge N2.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2011 11:17:04 GMT"}], "update_date": "2011-10-05", "authors_parsed": [["Nikulin", "Vladimir", ""]]}, {"id": "1110.0883", "submitter": "Laszlo Korsos", "authors": "Laszlo Korsos and Nicholas G. Polson", "title": "Analyzing Risky Choices: Q-Learning for Deal-No Deal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive an optimal strategy in the popular Deal or No Deal game show.\nQ-learning quantifies the continuation value inherent in sequential decision\nmaking and we use this to analyze contestants risky choices. Given their\nchoices and optimal strategy, we invert to find implied bounds on their levels\nof risk aversion. In risky decision making, previous empirical evidence has\nsuggested that past outcomes affect future choices and that contestants have\ntime-varying risk aversion. We demonstrate that the strategies of two players\n(Suzanne and Frank) from the European version of the game are consistent with\nconstant risk aversion levels except for their last risk-seeking choice.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2011 02:53:10 GMT"}], "update_date": "2011-10-06", "authors_parsed": [["Korsos", "Laszlo", ""], ["Polson", "Nicholas G.", ""]]}, {"id": "1110.1338", "submitter": "Johannes Rauh", "authors": "Johannes Rauh and Nihat Ay", "title": "Robustness and Conditional Independence Ideals", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study notions of robustness of Markov kernels and probability distribution\nof a system that is described by $n$ input random variables and one output\nrandom variable. Markov kernels can be expanded in a series of potentials that\nallow to describe the system's behaviour after knockouts. Robustness imposes\nstructural constraints on these potentials. Robustness of probability\ndistributions is defined via conditional independence statements. These\nstatements can be studied algebraically. The corresponding conditional\nindependence ideals are related to binary edge ideals. The set of robust\nprobability distributions lies on an algebraic variety. We compute a Gr\\\"obner\nbasis of this ideal and study the irreducible decomposition of the variety.\nThese algebraic results allow to parametrize the set of all robust probability\ndistributions.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2011 18:00:56 GMT"}], "update_date": "2011-10-07", "authors_parsed": [["Rauh", "Johannes", ""], ["Ay", "Nihat", ""]]}, {"id": "1110.1694", "submitter": "Ethan Anderes", "authors": "Ethan Anderes and Debashis Paul", "title": "Shrinking the Quadratic Estimator", "comments": null, "journal-ref": null, "doi": "10.1103/PhysRevD.85.103003", "report-no": null, "categories": "astro-ph.IM astro-ph.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a regression characterization for the quadratic estimator of weak\nlensing, developed by Hu and Okamoto (2001,2002), for cosmic microwave\nbackground observations. This characterization motivates a modification of the\nquadratic estimator by an adaptive Wiener filter which uses the robust Bayesian\ntechniques described in Strawderman (1971) and Berger (1980). This technique\nrequires the user to propose a fiducial model for the spectral density of the\nunknown lensing potential but the resulting estimator is developed to be robust\nto misspecification of this model. The role of the fiducial spectral density is\nto give the estimator superior statistical performance in a \"neighborhood of\nthe fiducial model\" while controlling the statistical errors when the fiducial\nspectral density is drastically wrong. Our estimate also highlights some\nadvantages provided by a Bayesian analysis of the quadratic estimator.\n", "versions": [{"version": "v1", "created": "Sat, 8 Oct 2011 04:19:47 GMT"}], "update_date": "2015-05-30", "authors_parsed": [["Anderes", "Ethan", ""], ["Paul", "Debashis", ""]]}, {"id": "1110.1972", "submitter": "Manuel J. A. Eugster", "authors": "Manuel J. A. Eugster", "title": "Archetypal Athletes", "comments": null, "journal-ref": null, "doi": null, "report-no": "TR-113", "categories": "stat.AP physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussions on outstanding---positively and/or negatively---athletes are\ncommon practice. The rapidly grown amount of collected sports data now allow to\nsupport such discussions with state of the art statistical methodology. Given a\n(multivariate) data set with collected data of athletes within a specific\nsport, outstanding athletes are values on the data set boundary. In the present\npaper we propose archetypal analysis to compute these extreme values. The\nso-called archetypes, i.e., archetypal athletes, approximate the observations\nas convex combinations. We interpret the archetypal athletes and their\ncharacteristics, and, furthermore, the composition of all athletes based on the\narchetypal athletes. The application of archetypal analysis is demonstrated on\nbasketball statistics and soccer skill ratings.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2011 09:31:40 GMT"}], "update_date": "2011-10-11", "authors_parsed": [["Eugster", "Manuel J. A.", ""]]}, {"id": "1110.2435", "submitter": "Tuhin Sahai", "authors": "Amit Surana, Tuhin Sahai and Andrzej Banaszuk", "title": "Iterative Methods for Scalable Uncertainty Quantification in Complex\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of uncertainty management for robust\ndesign, and verification of large dynamic networks whose performance is\naffected by an equally large number of uncertain parameters. Many such networks\n(e.g. power, thermal and communication networks) are often composed of weakly\ninteracting subnetworks. We propose intrusive and non-intrusive iterative\nschemes that exploit such weak interconnections to overcome dimensionality\ncurse associated with traditional uncertainty quantification methods (e.g.\ngeneralized Polynomial Chaos, Probabilistic Collocation) and accelerate\nuncertainty propagation in systems with large number of uncertain parameters.\nThis approach relies on integrating graph theoretic methods and waveform\nrelaxation with generalized Polynomial Chaos, and Probabilistic Collocation,\nrendering these techniques scalable. We analyze convergence properties of this\nscheme and illustrate it on several examples.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2011 17:05:00 GMT"}], "update_date": "2011-10-12", "authors_parsed": [["Surana", "Amit", ""], ["Sahai", "Tuhin", ""], ["Banaszuk", "Andrzej", ""]]}, {"id": "1110.2724", "submitter": "Greg Ver Steeg", "authors": "Greg Ver Steeg and Aram Galstyan", "title": "Information Transfer in Social Media", "comments": "8 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has explored the increasingly important role of social media\nby examining the dynamics of individual and group behavior, characterizing\npatterns of information diffusion, and identifying influential individuals. In\nthis paper we suggest a measure of causal relationships between nodes based on\nthe information-theoretic notion of transfer entropy, or information transfer.\nThis theoretically grounded measure is based on dynamic information, captures\nfine-grain notions of influence, and admits a natural, predictive\ninterpretation. Causal networks inferred by transfer entropy can differ\nsignificantly from static friendship networks because most friendship links are\nnot useful for predicting future dynamics. We demonstrate through analysis of\nsynthetic and real-world data that transfer entropy reveals meaningful hidden\nnetwork structures. In addition to altering our notion of who is influential,\ntransfer entropy allows us to differentiate between weak influence over large\ngroups and strong influence over small groups.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2011 18:11:16 GMT"}], "update_date": "2011-10-13", "authors_parsed": [["Steeg", "Greg Ver", ""], ["Galstyan", "Aram", ""]]}, {"id": "1110.3018", "submitter": "Sewoong Oh", "authors": "Amin Karbasi and Sewoong Oh", "title": "Robust Localization from Incomplete Local Information", "comments": "40 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of localizing wireless devices in an ad-hoc network\nembedded in a d-dimensional Euclidean space. Obtaining a good estimation of\nwhere wireless devices are located is crucial in wireless network applications\nincluding environment monitoring, geographic routing and topology control. When\nthe positions of the devices are unknown and only local distance information is\ngiven, we need to infer the positions from these local distance measurements.\nThis problem is particularly challenging when we only have access to\nmeasurements that have limited accuracy and are incomplete. We consider the\nextreme case of this limitation on the available information, namely only the\nconnectivity information is available, i.e., we only know whether a pair of\nnodes is within a fixed detection range of each other or not, and no\ninformation is known about how far apart they are. Further, to account for\ndetection failures, we assume that even if a pair of devices is within the\ndetection range, it fails to detect the presence of one another with some\nprobability and this probability of failure depends on how far apart those\ndevices are. Given this limited information, we investigate the performance of\na centralized positioning algorithm MDS-MAP introduced by Shang et al., and a\ndistributed positioning algorithm, introduced by Savarese et al., called\nHOP-TERRAIN. In particular, for a network consisting of n devices positioned\nrandomly, we provide a bound on the resulting error for both algorithms. We\nshow that the error is bounded, decreasing at a rate that is proportional to\nR/Rc, where Rc is the critical detection range when the resulting random\nnetwork starts to be connected, and R is the detection range of each device.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2011 18:09:11 GMT"}], "update_date": "2011-10-14", "authors_parsed": [["Karbasi", "Amin", ""], ["Oh", "Sewoong", ""]]}, {"id": "1110.3257", "submitter": "Gavin Shaddick", "authors": "Gavin Shaddick, Haojie Yan, Danielle Vienneau", "title": "Modelling the impact of human activity on nitrogen dioxide\n  concentrations in Europe", "comments": "22 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ambient concentrations of many pollutants are associated with emissions due\nto human activity, such as road transport and other combustion sources. In this\npaper we consider air pollution as a multi--level phenomenon within a Bayesian\nhierarchical model. We examine different scales of variation in pollution\nconcentrations ranging from large scale transboundary effects to more localised\neffects which are directly related to human activity. Specifically, in the\nfirst stage of the model, we isolate underlying patterns in pollution\nconcentrations due to global factors such as underlying climate and topography,\nwhich are modelled together with spatial structure. At this stage measurements\nfrom monitoring sites located within rural areas are used which, as far as\npossible, are chosen to reflect background concentrations. Having isolated\nthese global effects, in the second stage we assess the effects of human\nactivity on pollution in urban areas. The proposed model was applied to\nconcentrations of nitrogen dioxide measured throughout the EU for which\nsignificant increases are found to be associated with human activity in urban\nareas. The approach proposed here provides valuable information that could be\nused in performing health impact assessments and to inform policy.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2011 16:01:57 GMT"}], "update_date": "2011-10-17", "authors_parsed": [["Shaddick", "Gavin", ""], ["Yan", "Haojie", ""], ["Vienneau", "Danielle", ""]]}, {"id": "1110.3390", "submitter": "Konstantin Zuev M", "authors": "Konstantin M. Zuev, James L. Beck, Siu-Kui Au, and Lambros S.\n  Katafygiotis", "title": "Bayesian Post-Processor and other Enhancements of Subset Simulation for\n  Estimating Failure Probabilities in High Dimensions", "comments": "35 pages, 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of small failure probabilities is one of the most important and\nchallenging computational problems in reliability engineering. The failure\nprobability is usually given by an integral over a high-dimensional uncertain\nparameter space that is difficult to evaluate numerically. This paper focuses\non enhancements to Subset Simulation (SS), proposed by Au and Beck, which\nprovides an efficient algorithm based on MCMC (Markov chain Monte Carlo)\nsimulation for computing small failure probabilities for general\nhigh-dimensional reliability problems. First, we analyze the Modified\nMetropolis algorithm (MMA), an MCMC technique, which is used in SS for sampling\nfrom high-dimensional conditional distributions. We present some observations\non the optimal scaling of MMA, and develop an optimal scaling strategy for this\nalgorithm when it is employed within SS. Next, we provide a theoretical basis\nfor the optimal value of the conditional failure probability $p_0$, an\nimportant parameter one has to choose when using SS. Finally, a Bayesian\npost-processor SS+ for the original SS method is developed where the uncertain\nfailure probability that one is estimating is modeled as a stochastic variable\nwhose possible values belong to the unit interval. Simulated samples from SS\nare viewed as informative data relevant to the system's reliability. Instead of\na single real number as an estimate, SS+ produces the posterior PDF of the\nfailure probability, which takes into account both prior information and the\ninformation in the sampled data. This PDF quantifies the uncertainty in the\nvalue of the failure probability and it may be further used in risk analyses to\nincorporate this uncertainty. The relationship between the original SS and SS+\nis also discussed\n", "versions": [{"version": "v1", "created": "Sat, 15 Oct 2011 06:21:37 GMT"}], "update_date": "2011-10-18", "authors_parsed": [["Zuev", "Konstantin M.", ""], ["Beck", "James L.", ""], ["Au", "Siu-Kui", ""], ["Katafygiotis", "Lambros S.", ""]]}, {"id": "1110.3437", "submitter": "Salim Bouzebda", "authors": "Salim Bouzebda and Tarek Zari", "title": "A Strong Invariance Theorem of the Tail Empirical Copula Processes", "comments": null, "journal-ref": "Communications in Statistics. Theory and Methods - 42 (2013), no.\n  1, 11-27", "doi": "10.1080/03610926.2011.575514", "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the behavior of bivariate empirical copula process\n$\\mathbb{G}_n(\\cdot,\\cdot)$ on pavements $[0,k_n/n]^2$ of $[0,1]^2,$ where\n$k_n$ is a sequence of positive constants fulfilling some conditions. We\nprovide a upper bound for the strong approximation of\n$\\mathbb{G}_n(\\cdot,\\cdot)$ by a Gaussian process when $k_n/n \\searrow \\gamma$\nas $n\\rightarrow \\infty,$ where $0 \\leq \\gamma \\leq 1.$\n", "versions": [{"version": "v1", "created": "Sat, 15 Oct 2011 20:14:00 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Bouzebda", "Salim", ""], ["Zari", "Tarek", ""]]}, {"id": "1110.3689", "submitter": "Feng Li", "authors": "Feng Li and Mattias Villani", "title": "Efficient Bayesian Multivariate Surface Regression", "comments": null, "journal-ref": "Scandinavian Journal of Statistics, (2013), 40(4)", "doi": "10.1111/sjos.12022", "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for choosing a fixed set of knot locations in additive spline models\nare fairly well established in the statistical literature. While most of these\nmethods are in principle directly extendable to non-additive surface models,\nthey are less likely to be successful in that setting because of the curse of\ndimensionality, especially when there are more than a couple of covariates. We\npropose a regression model for a multivariate Gaussian response that combines\nboth additive splines and interactive splines, and a highly efficient MCMC\nalgorithm that updates all the knot locations jointly. We use shrinkage priors\nto avoid overfitting with different estimated shrinkage factors for the\nadditive and surface part of the model, and also different shrinkage parameters\nfor the different response variables. This makes it possible for the model to\nadapt to varying degrees of nonlinearity in different parts of the data in a\nparsimonious way. Simulated data and an application to firm leverage data show\nthat the approach is computationally efficient, and that allowing for freely\nestimated knot locations can offer a substantial improvement in out-of-sample\npredictive performance.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2011 15:00:42 GMT"}, {"version": "v2", "created": "Thu, 4 Oct 2012 16:56:39 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Li", "Feng", ""], ["Villani", "Mattias", ""]]}, {"id": "1110.3860", "submitter": "Zack Almquist", "authors": "Zack W. Almquist and Carter T. Butts", "title": "Contending Parties: A Logistic Choice Analysis of Inter- and Intra-group\n  Blog Citation Dynamics in the 2004 US Presidential Election", "comments": null, "journal-ref": null, "doi": null, "report-no": "MBS 11-06", "categories": "cs.SI physics.soc-ph stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 2004 US Presidential Election cycle marked the debut of Internet-based\nmedia such as blogs and social networking websites as institutionally\nrecognized features of the American political landscape. Using a longitudinal\nsample of all DNC/RNC-designated blog-citation networks we are able to test the\ninfluence of various strategic, institutional, and balance-theoretic mechanisms\nand exogenous factors such as seasonality and political events on the\npropensity of blogs to cite one another over time. Capitalizing on the temporal\nresolution of our data, we utilize an autoregressive network regression\nframework to carry out inference for a logistic choice process. Using a\ncombination of deviance-based model selection criteria and simulation-based\nmodel adequacy tests, we identify the combination of processes that best\ncharacterizes the choice behavior of the contending blogs.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2011 01:35:32 GMT"}], "update_date": "2011-10-19", "authors_parsed": [["Almquist", "Zack W.", ""], ["Butts", "Carter T.", ""]]}, {"id": "1110.4128", "submitter": "Pablo Tamayo", "authors": "Pablo Tamayo, George Steinhardt, Arthur Liberzon, and Jill P. Mesirov", "title": "The Limitations of Simple Gene Set Enrichment Analysis Assuming Gene\n  Independence", "comments": "Submitted to Statistical Methods in Medical Research", "journal-ref": null, "doi": "10.1016/j.jbi.2011.12.002", "report-no": null, "categories": "stat.ME q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since its first publication in 2003, the Gene Set Enrichment Analysis (GSEA)\nmethod, based on the Kolmogorov-Smirnov statistic, has been heavily used,\nmodified, and also questioned. Recently a simplified approach, using a one\nsample t test score to assess enrichment and ignoring gene-gene correlations\nwas proposed by Irizarry et al. 2009 as a serious contender. The argument\ncriticizes GSEA's nonparametric nature and its use of an empirical null\ndistribution as unnecessary and hard to compute. We refute these claims by\ncareful consideration of the assumptions of the simplified method and its\nresults, including a comparison with GSEA's on a large benchmark set of 50\ndatasets. Our results provide strong empirical evidence that gene-gene\ncorrelations cannot be ignored due to the significant variance inflation they\nproduced on the enrichment scores and should be taken into account when\nestimating gene set enrichment significance. In addition, we discuss the\nchallenges that the complex correlation structure and multi-modality of gene\nsets pose more generally for gene set enrichment methods.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2011 21:24:51 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2012 16:28:04 GMT"}], "update_date": "2012-10-12", "authors_parsed": [["Tamayo", "Pablo", ""], ["Steinhardt", "George", ""], ["Liberzon", "Arthur", ""], ["Mesirov", "Jill P.", ""]]}, {"id": "1110.4139", "submitter": "Logan Grosenick", "authors": "Logan Grosenick, Brad Klingenberg, Kiefer Katovich, Brian Knutson,\n  Jonathan E. Taylor", "title": "Whole-brain Prediction Analysis with GraphNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate machine learning methods are increasingly used to analyze\nneuroimaging data, often replacing more traditional \"mass univariate\"\ntechniques that fit data one voxel at a time. In the functional magnetic\nresonance imaging (fMRI) literature, this has led to broad application of\n\"off-the-shelf\" classification and regression methods. These generic approaches\nallow investigators to use ready-made algorithms to accurately decode\nperceptual, cognitive, or behavioral states from distributed patterns of neural\nactivity. However, when applied to correlated whole-brain fMRI data these\nmethods suffer from coefficient instability, are sensitive to outliers, and\nyield dense solutions that are hard to interpret without arbitrary\nthresholding. Here, we develop variants of the the Graph-constrained Elastic\nNet (GraphNet), ..., we (1) extend GraphNet to include robust loss functions\nthat confer insensitivity to outliers, (2) equip them with \"adaptive\" penalties\nthat asymptotically guarantee correct variable selection, and (3) develop a\nnovel sparse structured Support Vector GraphNet classifier (SVGN). When applied\nto previously published data, these efficient whole-brain methods significantly\nimproved classification accuracy over previously reported VOI-based analyses on\nthe same data while discovering task-related regions not documented in the\noriginal VOI approach. Critically, GraphNet estimates generalize well to\nout-of-sample data collected more than three years later on the same task but\nwith different subjects and stimuli. By enabling robust and efficient selection\nof important voxels from whole-brain data taken over multiple time points\n(>100,000 \"features\"), these methods enable data-driven selection of brain\nareas that accurately predict single-trial behavior within and across\nindividuals.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2011 22:36:43 GMT"}, {"version": "v2", "created": "Wed, 26 Dec 2012 19:12:52 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Grosenick", "Logan", ""], ["Klingenberg", "Brad", ""], ["Katovich", "Kiefer", ""], ["Knutson", "Brian", ""], ["Taylor", "Jonathan E.", ""]]}, {"id": "1110.4705", "submitter": "Qunhua Li", "authors": "Qunhua Li, James B. Brown, Haiyan Huang, Peter J. Bickel", "title": "Measuring reproducibility of high-throughput experiments", "comments": "Published in at http://dx.doi.org/10.1214/11-AOAS466 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 3, 1752-1779", "doi": "10.1214/11-AOAS466", "report-no": "IMS-AOAS-AOAS466", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reproducibility is essential to reliable scientific discovery in\nhigh-throughput experiments. In this work we propose a unified approach to\nmeasure the reproducibility of findings identified from replicate experiments\nand identify putative discoveries using reproducibility. Unlike the usual\nscalar measures of reproducibility, our approach creates a curve, which\nquantitatively assesses when the findings are no longer consistent across\nreplicates. Our curve is fitted by a copula mixture model, from which we derive\na quantitative reproducibility score, which we call the \"irreproducible\ndiscovery rate\" (IDR) analogous to the FDR. This score can be computed at each\nset of paired replicate ranks and permits the principled setting of thresholds\nboth for assessing reproducibility and combining replicates. Since our approach\npermits an arbitrary scale for each replicate, it provides useful descriptive\nmeasures in a wide variety of situations to be explored. We study the\nperformance of the algorithm using simulations and give a heuristic analysis of\nits theoretical properties. We demonstrate the effectiveness of our method in a\nChIP-seq experiment.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2011 05:57:29 GMT"}], "update_date": "2011-10-24", "authors_parsed": [["Li", "Qunhua", ""], ["Brown", "James B.", ""], ["Huang", "Haiyan", ""], ["Bickel", "Peter J.", ""]]}, {"id": "1110.5002", "submitter": "Eric Burns", "authors": "Eric Burns, Wade Fisher", "title": "Testing the approximations described in \"Asymptotic formulae for\n  likelihood-based tests of new physics\"", "comments": "16 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-ex physics.data-an stat.AP", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  \"Asymptotic formulae for likelihood-based tests of new physics\" presents a\nmathematical formalism for a new approximation for hypothesis testing in high\nenergy physics. The approximations are designed to greatly reduce the\ncomputational burden for such problems. We seek to test the conditions under\nwhich the approximations described remain valid. To do so, we perform parallel\ncalculations for a range of scenarios and compare the full calculation to the\napproximations to determine the limits and robustness of the approximation. We\ncompare this approximation against values calculated with the Collie framework,\nwhich for our analysis we assume produces true values.\n", "versions": [{"version": "v1", "created": "Sat, 22 Oct 2011 20:59:40 GMT"}], "update_date": "2011-10-25", "authors_parsed": [["Burns", "Eric", ""], ["Fisher", "Wade", ""]]}, {"id": "1110.5254", "submitter": "Edward L. Ionides", "authors": "Edward L. Ionides, Zhen Wang, Jos\\'e A. Tapia Granados", "title": "Macroeconomic effects on mortality revealed by panel analysis with\n  nonlinear trends", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS624 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 3, 1362-1385", "doi": "10.1214/12-AOAS624", "report-no": "IMS-AOAS-AOAS624", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many investigations have used panel methods to study the relationships\nbetween fluctuations in economic activity and mortality. A broad consensus has\nemerged on the overall procyclical nature of mortality: perhaps\ncounter-intuitively, mortality typically rises above its trend during\nexpansions. This consensus has been tarnished by inconsistent reports on the\nspecific age groups and mortality causes involved. We show that these\ninconsistencies result, in part, from the trend specifications used in previous\npanel models. Standard econometric panel analysis involves fitting regression\nmodels using ordinary least squares, employing standard errors which are robust\nto temporal autocorrelation. The model specifications include a fixed effect,\nand possibly a linear trend, for each time series in the panel. We propose\nalternative methodology based on nonlinear detrending. Applying our methodology\non data for the 50 US states from 1980 to 2006, we obtain more precise and\nconsistent results than previous studies. We find procyclical mortality in all\nage groups. We find clear procyclical mortality due to respiratory disease and\ntraffic injuries. Predominantly procyclical cardiovascular disease mortality\nand countercyclical suicide are subject to substantial state-to-state\nvariation. Neither cancer nor homicide have significant macroeconomic\nassociation.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2011 15:09:58 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2013 09:10:54 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Ionides", "Edward L.", ""], ["Wang", "Zhen", ""], ["Granados", "Jos\u00e9 A. Tapia", ""]]}, {"id": "1110.5342", "submitter": "Engin Masazade", "authors": "Engin Masazade, Ruixin Niu, Pramod K. Varshney", "title": "Dynamic Bit Allocation for Object Tracking in Bandwidth Limited Sensor\n  Networks", "comments": "Original manusprit is submitted to IEEE Transactions on Signal\n  Processing. Part of this work was presented at the Fusion'11 conference held\n  at Chicago, IL, July 5-8, 2011", "journal-ref": null, "doi": "10.1109/TSP.2012.2204257", "report-no": null, "categories": "stat.AP cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the target tracking problem in wireless sensor\nnetworks (WSNs) using quantized sensor measurements under limited bandwidth\navailability. At each time step of tracking, the available bandwidth $R$ needs\nto be distributed among the $N$ sensors in the WSN for the next time step. The\noptimal solution for the bandwidth allocation problem can be obtained by using\na combinatorial search which may become computationally prohibitive for large\n$N$ and $R$. Therefore, we develop two new computationally efficient suboptimal\nbandwidth distribution algorithms which are based on convex relaxation and\napproximate dynamic programming (A-DP). We compare the mean squared error (MSE)\nand computational complexity performances of convex relaxation and A-DP with\nother existing suboptimal bandwidth distribution schemes based on generalized\nBreiman, Friedman, Olshen, and Stone (GBFOS) algorithm and greedy search.\nSimulation results show that, A-DP, convex optimization and GBFOS yield similar\nMSE performance, which is very close to that based on the optimal exhaustive\nsearch approach and they outperform greedy search and nearest neighbor based\nbandwidth allocation approaches significantly. Computationally, A-DP is more\nefficient than the bandwidth allocation schemes based on convex relaxation and\nGBFOS, especially for a large sensor network.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2011 17:31:39 GMT"}], "update_date": "2015-05-30", "authors_parsed": [["Masazade", "Engin", ""], ["Niu", "Ruixin", ""], ["Varshney", "Pramod K.", ""]]}, {"id": "1110.5429", "submitter": "Egil Ferkingstad", "authors": "Egil Ferkingstad, Anders L{\\o}land, Mathilde Wilhelmsen", "title": "Causal modeling and inference for electricity markets", "comments": null, "journal-ref": "Energy Economics (2011), 33(3):404-412", "doi": "10.1016/j.eneco.2010.10.006", "report-no": null, "categories": "stat.AP q-fin.ST stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How does dynamic price information flow among Northern European electricity\nspot prices and prices of major electricity generation fuel sources? We use\ntime series models combined with new advances in causal inference to answer\nthese questions. Applying our methods to weekly Nordic and German electricity\nprices, and oil, gas and coal prices, with German wind power and Nordic water\nreservoir levels as exogenous variables, we estimate a causal model for the\nprice dynamics, both for contemporaneous and lagged relationships. In\ncontemporaneous time, Nordic and German electricity prices are interlinked\nthrough gas prices. In the long run, electricity prices and British gas prices\nadjust themselves to establish the equlibrium price level, since oil, coal,\ncontinental gas and EUR/USD are found to be weakly exogenous.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2011 07:59:28 GMT"}], "update_date": "2011-10-26", "authors_parsed": [["Ferkingstad", "Egil", ""], ["L\u00f8land", "Anders", ""], ["Wilhelmsen", "Mathilde", ""]]}, {"id": "1110.5789", "submitter": "James Scott", "authors": "Nicholas G. Polson and James G. Scott", "title": "An empirical test for Eurozone contagion using an asset-pricing model\n  with heavy-tailed stochastic volatility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an empirical test of financial contagion in European\nequity markets during the tumultuous period of 2008-2011. Our analysis shows\nthat traditional GARCH and Gaussian stochastic-volatility models are unable to\nexplain two key stylized features of global markets during presumptive\ncontagion periods: shocks to aggregate market volatility can be sudden and\nexplosive, and they are associated with specific directional biases in the\ncross-section of country-level returns. Our model repairs this deficit by\nassuming that the random shocks to volatility are heavy-tailed and correlated\ncross-sectionally, both with each other and with returns. The fundamental\nconclusion of our analysis is that great care is needed in modeling volatility\nif one wishes to characterize the relationship between volatility and contagion\nthat is predicted by economic theory.\n  In analyzing daily data, we find evidence for significant contagion effects\nduring the major EU crisis periods of May 2010 and August 2011, where contagion\nis defined as excess correlation in the residuals from a factor model\nincorporating global and regional market risk factors. Some of this excess\ncorrelation can be explained by quantifying the impact of shocks to aggregate\nvolatility in the cross-section of expected returns - but only, it turns out,\nif one is extremely careful in accounting for the explosive nature of these\nshocks. We show that global markets have time-varying cross-sectional\nsensitivities to these shocks, and that high sensitivities strongly predict\nperiods of financial crisis. Moreover, the pattern of temporal changes in\ncorrelation structure between volatility and returns is readily interpretable\nin terms of the major events of the periods in question.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2011 13:43:59 GMT"}, {"version": "v2", "created": "Mon, 26 Mar 2012 20:50:05 GMT"}], "update_date": "2012-03-28", "authors_parsed": [["Polson", "Nicholas G.", ""], ["Scott", "James G.", ""]]}, {"id": "1110.6019", "submitter": "Yongtao Guan", "authors": "Yongtao Guan, Matthew Stephens", "title": "Bayesian variable selection regression for genome-wide association\n  studies and other large-scale problems", "comments": "Published in at http://dx.doi.org/10.1214/11-AOAS455 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 3, 1780-1815", "doi": "10.1214/11-AOAS455", "report-no": "IMS-AOAS-AOAS455", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider applying Bayesian Variable Selection Regression, or BVSR, to\ngenome-wide association studies and similar large-scale regression problems.\nCurrently, typical genome-wide association studies measure hundreds of\nthousands, or millions, of genetic variants (SNPs), in thousands or tens of\nthousands of individuals, and attempt to identify regions harboring SNPs that\naffect some phenotype or outcome of interest. This goal can naturally be cast\nas a variable selection regression problem, with the SNPs as the covariates in\nthe regression. Characteristic features of genome-wide association studies\ninclude the following: (i) a focus primarily on identifying relevant variables,\nrather than on prediction; and (ii) many relevant covariates may have tiny\neffects, making it effectively impossible to confidently identify the complete\n\"correct\" subset of variables. Taken together, these factors put a premium on\nhaving interpretable measures of confidence for individual covariates being\nincluded in the model, which we argue is a strength of BVSR compared with\nalternatives such as penalized regression methods. Here we focus primarily on\nanalysis of quantitative phenotypes, and on appropriate prior specification for\nBVSR in this setting, emphasizing the idea of considering what the priors imply\nabout the total proportion of variance in outcome explained by relevant\ncovariates. We also emphasize the potential for BVSR to estimate this\nproportion of variance explained, and hence shed light on the issue of \"missing\nheritability\" in genome-wide association studies.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2011 09:15:15 GMT"}], "update_date": "2011-10-28", "authors_parsed": [["Guan", "Yongtao", ""], ["Stephens", "Matthew", ""]]}, {"id": "1110.6135", "submitter": "Yue Yu", "authors": "Yue Yu, Zhihong Chen, Jie Yang", "title": "Cluster-Based Regularized Sliced Inverse Regression for Forecasting\n  Macroeconomic Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article concerns the dimension reduction in regression for large data\nset. We introduce a new method based on the sliced inverse regression approach,\ncalled cluster-based regularized sliced inverse regression. Our method not only\nkeeps the merit of considering both response and predictors' information, but\nalso enhances the capability of handling highly correlated variables. It is\njustified under certain linearity conditions. An empirical application on a\nmacroeconomic data set shows that our method has outperformed the dynamic\nfactor model and other shrinkage methods.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2011 16:28:57 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2013 02:05:11 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Yu", "Yue", ""], ["Chen", "Zhihong", ""], ["Yang", "Jie", ""]]}, {"id": "1110.6178", "submitter": "James Newling", "authors": "James Newling, Bruce. A. Bassett, Ren\\'ee Hlozek, Martin Kunz, Mathew\n  Smith, Melvin Varughese", "title": "Parameter Estimation with BEAMS in the presence of biases and\n  correlations", "comments": "10 figures, 14 pages", "journal-ref": null, "doi": "10.1111/j.1365-2966.2011.20147.x", "report-no": null, "categories": "astro-ph.IM astro-ph.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The original formulation of BEAMS - Bayesian Estimation Applied to Multiple\nSpecies - showed how to use a dataset contaminated by points of multiple\nunderlying types to perform unbiased parameter estimation. An example is\ncosmological parameter estimation from a photometric supernova sample\ncontaminated by unknown Type Ibc and II supernovae. Where other methods require\ndata cuts to increase purity, BEAMS uses all of the data points in conjunction\nwith their probabilities of being each type. Here we extend the BEAMS formalism\nto allow for correlations between the data and the type probabilities of the\nobjects as can occur in realistic cases. We show with simple simulations that\nthis extension can be crucial, providing a 50% reduction in parameter\nestimation variance when such correlations do exist. We then go on to perform\ntests to quantify the importance of the type probabilities, one of which\nillustrates the effect of biasing the probabilities in various ways. Finally, a\ngeneral presentation of the selection bias problem is given, and discussed in\nthe context of future photometric supernova surveys and BEAMS, which lead to\nspecific recommendations for future supernova surveys.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2011 20:00:05 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Newling", "James", ""], ["Bassett", "Bruce. A.", ""], ["Hlozek", "Ren\u00e9e", ""], ["Kunz", "Martin", ""], ["Smith", "Mathew", ""], ["Varughese", "Melvin", ""]]}, {"id": "1110.6451", "submitter": "Roman Jandarov", "authors": "Roman Jandarov, Murali Haran, Ottar Bj{\\o}rnstad and Bryan Grenfell", "title": "Emulating a gravity model to infer the spatiotemporal dynamics of an\n  infectious disease", "comments": "31 pages, 8 figures and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic models for infectious disease dynamics are useful for\nunderstanding the mechanism underlying the spread of infection. When the\nlikelihood function for these models is expensive to evaluate, traditional\nlikelihood-based inference may be computationally intractable. Furthermore,\ntraditional inference may lead to poor parameter estimates and the fitted model\nmay not capture important biological characteristics of the observed data. We\npropose a novel approach for resolving these issues that is inspired by recent\nwork in emulation and calibration for complex computer models. Our motivating\nexample is the gravity time series susceptible-infected-recovered (TSIR) model.\nOur approach focuses on the characteristics of the process that are of\nscientific interest. We find a Gaussian process approximation to the gravity\nmodel using key summary statistics obtained from model simulations. We\ndemonstrate via simulated examples that the new approach is computationally\nexpedient, provides accurate parameter inference, and results in a good model\nfit. We apply our method to analyze measles outbreaks in England and Wales in\ntwo periods, the pre-vaccination period from 1944-1965 and the vaccination\nperiod from 1966-1994. Based on our results, we are able to obtain important\nscientific insights about the transmission of measles. In general, our method\nis applicable to problems where traditional likelihood-based inference is\ncomputationally intractable or produces a poor model fit. It is also an\nalternative to approximate Bayesian computation (ABC) when simulations from the\nmodel are expensive.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2011 20:05:44 GMT"}, {"version": "v2", "created": "Fri, 6 Jan 2012 00:35:39 GMT"}, {"version": "v3", "created": "Thu, 14 Feb 2013 22:52:49 GMT"}], "update_date": "2013-02-18", "authors_parsed": [["Jandarov", "Roman", ""], ["Haran", "Murali", ""], ["Bj\u00f8rnstad", "Ottar", ""], ["Grenfell", "Bryan", ""]]}, {"id": "1110.6460", "submitter": "Dylan Harp", "authors": "Dylan R. Harp and Velimir V. Vesselinov", "title": "Contaminant remediation decision analysis using information gap theory", "comments": null, "journal-ref": null, "doi": null, "report-no": "LA-UR-11-10250", "categories": "physics.geo-ph physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision making under severe lack of information is a ubiquitous situation in\nnearly every applied field of engineering, policy, and science. A severe lack\nof information precludes our ability to determine a frequency of occurrence of\nevents or conditions that impact the decision; therefore, decision\nuncertainties due to a severe lack of information cannot be characterized\nprobabilistically. To circumvent this problem, information gap (info-gap)\ntheory has been developed to explicitly recognize and quantify the implications\nof information gaps in decision making. This paper presents a decision analysis\nbased on info-gap theory developed for a contaminant remediation scenario. The\nanalysis provides decision support in determining the fraction of contaminant\nmass to remove from the environment in the presence of a lack of information\nrelated to the contaminant mass flux into an aquifer. An info-gap uncertainty\nmodel is developed to characterize uncertainty due to a lack of information\nconcerning the contaminant flux. The info-gap uncertainty model groups nested,\nconvex sets of functions defining contaminant flux over time based on their\nlevel of deviation from a nominal contaminant flux. The nominal contaminant\nflux defines a reasonable contaminant flux over time based on existing\ninformation. A robustness function is derived to quantify the maximum level of\ndeviation from nominal that still ensures compliance for each decision. An\nopportuneness function is derived to characterize the possibility of meeting a\ndesired contaminant concentration level. The decision analysis evaluates how\nthe robustness and opportuneness change as a function of time since remediation\nand as a function of the fraction of contaminant mass removed.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2011 20:41:15 GMT"}], "update_date": "2011-11-01", "authors_parsed": [["Harp", "Dylan R.", ""], ["Vesselinov", "Velimir V.", ""]]}, {"id": "1110.6560", "submitter": "Xi Luo", "authors": "Xi Luo, Dylan S. Small, Chiang-shan R. Li, Paul R. Rosenbaum", "title": "Inference with interference between units in an fMRI experiment of motor\n  inhibition", "comments": "Published by Journal of the American Statistical Association at\n  http://www.tandfonline.com/doi/full/10.1080/01621459.2012.655954 . R package\n  cin (Causal Inference for Neuroscience) implementing the proposed method is\n  freely available on CRAN at https://CRAN.R-project.org/package=cin", "journal-ref": "Journal of the American Statistical Association 107, no. 498\n  (2012): 530-541", "doi": "10.1080/01621459.2012.655954", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An experimental unit is an opportunity to randomly apply or withhold a\ntreatment. There is interference between units if the application of the\ntreatment to one unit may also affect other units. In cognitive neuroscience, a\ncommon form of experiment presents a sequence of stimuli or requests for\ncognitive activity at random to each experimental subject and measures\nbiological aspects of brain activity that follow these requests. Each subject\nis then many experimental units, and interference between units within an\nexperimental subject is likely, in part because the stimuli follow one another\nquickly and in part because human subjects learn or become experienced or\nprimed or bored as the experiment proceeds. We use a recent fMRI experiment\nconcerned with the inhibition of motor activity to illustrate and further\ndevelop recently proposed methodology for inference in the presence of\ninterference. A simulation evaluates the power of competing procedures.\n", "versions": [{"version": "v1", "created": "Sat, 29 Oct 2011 22:11:41 GMT"}, {"version": "v2", "created": "Tue, 10 Jan 2017 16:05:21 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Luo", "Xi", ""], ["Small", "Dylan S.", ""], ["Li", "Chiang-shan R.", ""], ["Rosenbaum", "Paul R.", ""]]}, {"id": "1110.6631", "submitter": "Viet Chi Tran", "authors": "Marc Constant, Viet Chi Tran (LPP), Bernard Beno\\^it, Francis Vasseur", "title": "New first trimester crown-rump length's equations optimized by\n  structured data collection from a French general population", "comments": null, "journal-ref": "Fetal Diagnosis and Therapy 32, 4 (2012) 277-287", "doi": "10.1159/000339272", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  --- Objectives --- Prior to foetal karyotyping, the likelihood of Down's\nsyndrome is often determined combining maternal age, serum free beta-HCG,\nPAPP-A levels and embryonic measurements of crown-rump length and nuchal\ntranslucency for gestational ages between 11 and 13 weeks. It appeared\nimportant to get a precise knowledge of these scan parameters' normal values\nduring the first trimester. This paper focused on crown-rump length. ---\nMETHODS --- 402 pregnancies from in-vitro fertilization allowing a precise\nestimation of foetal ages (FA) were used to determine the best model that\ndescribes crown-rump length (CRL) as a function of FA. Scan measures by a\nsingle operator from 3846 spontaneous pregnancies representative of the general\npopulation from Northern France were used to build a mathematical model linking\nFA and CRL in a context as close as possible to normal scan screening used in\nDown's syndrome likelihood determination. We modeled both CRL as a function of\nFA and FA as a function of CRL. For this, we used a clear methodology and\nperformed regressions with heteroskedastic corrections and robust regressions.\nThe results were compared by cross-validation to retain the equations with the\nbest predictive power. We also studied the errors between observed and\npredicted values. --- Results --- Data from 513 spontaneous pregnancies allowed\nto model CRL as a function of age of foetal age. The best model was a\npolynomial of degree 2. Datation with our equation that models spontaneous\npregnancies from a general population was in quite agreement with objective\ndatations obtained from 402 IVF pregnancies and thus support the validity of\nour model. The most precise measure of CRL was when the SD was minimal\n(1.83mm), for a CRL of 23.6 mm where our model predicted a 49.4 days of foetal\nage. Our study allowed to model the SD from 30 to 90 days of foetal age and\noffers the opportunity of using Zscores in the future to detect growth\nabnormalities. --- Conclusion --- With powerful statistical tools we report a\ngood modeling of the first trimester embryonic growth in the general population\nallowing a better knowledge of the date of fertilization useful in the\nultrasound screening of Down's syndrome. The optimal period to measure CRL and\npredict foetal age was 49.4 days (9 weeks of gestational age). Our results open\nthe way to the detection of foetal growth abnormalities using CRL Zscores\nthroughout the first trimester.\n", "versions": [{"version": "v1", "created": "Sun, 30 Oct 2011 18:19:50 GMT"}], "update_date": "2013-02-14", "authors_parsed": [["Constant", "Marc", "", "LPP"], ["Tran", "Viet Chi", "", "LPP"], ["Beno\u00eet", "Bernard", ""], ["Vasseur", "Francis", ""]]}]