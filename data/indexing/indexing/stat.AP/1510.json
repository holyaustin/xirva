[{"id": "1510.00028", "submitter": "Le Bao", "authors": "David R. Hunter, Le Bao, Mary Poss", "title": "Assignment of endogenous retrovirus integration sites using a mixture\n  model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural variation occurs in the genomes of individuals because of the\ndifferent positions occupied by repetitive genome elements like endogenous\nretroviruses, or ERVs. The presence or absence of ERVs can be determined by\nidentifying the junction with the host genome using high-throughput sequence\ntechnology and a clustering algorithm. The resulting data give the number of\nsequence reads assigned to each ERV-host junction sequence for each sampled\nindividual. Variability in the number of reads from an individual integration\nsite makes it difficult to determine whether a site is present for low read\ncounts. We present a novel two-component mixture of negative binomial\ndistributions to model these counts and assign a probability that a given ERV\nis present in a given individual. We explain how our approach is superior to\nexisting alternatives, including another form of two-component mixture model\nand the much more common approach of selecting a threshold count for declaring\nthe presence of an ERV. We apply our method to a data set of ERV integrations\nin mule deer [Odocoileus hemionus], a species for which no genomic resources\nare available, and demonstrate that the discovered patterns of shared\nintegration sites contain information about animal relatedness.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 20:32:10 GMT"}, {"version": "v2", "created": "Tue, 3 Jan 2017 19:46:13 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Hunter", "David R.", ""], ["Bao", "Le", ""], ["Poss", "Mary", ""]]}, {"id": "1510.00474", "submitter": "Xiaopeng Wang", "authors": "Xiaopeng Wang and Zihuai Lin", "title": "Microwave Surveillance based on Ghost Imaging and Distributed Antennas", "comments": "4 pages, 11 figures, submitted for possible journal publication", "journal-ref": null, "doi": "10.1109/LAWP.2016.2538787", "report-no": null, "categories": "stat.AP cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this letter, we proposed a ghost imaging (GI) and distributed antennas\nbased microwave surveillance scheme. By analyzing its imaging resolution and\nsampling requirement, the potential of employing microwave GI to achieve\nhigh-quality surveillance performance with low system complexity has been\ndemonstrated. The theoretical analysis and effectiveness of the proposed\nmicrowave surveillance method are also validated via simulations.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2015 02:48:34 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Wang", "Xiaopeng", ""], ["Lin", "Zihuai", ""]]}, {"id": "1510.00646", "submitter": "Daniele Durante", "authors": "Daniele Durante, Sally Paganin, Bruno Scarpa, David B. Dunson", "title": "Bayesian modeling of networks in complex business intelligence problems", "comments": null, "journal-ref": "Journal of the Royal Statistical Society: Series C (2017). 66,\n  555-580", "doi": "10.1111/rssc.12168", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex network data problems are increasingly common in many fields of\napplication. Our motivation is drawn from strategic marketing studies\nmonitoring customer choices of specific products, along with co-subscription\nnetworks encoding multiple purchasing behavior. Data are available for several\nagencies within the same insurance company, and our goal is to efficiently\nexploit co-subscription networks to inform targeted advertising of cross-sell\nstrategies to currently mono-product customers. We address this goal by\ndeveloping a Bayesian hierarchical model, which clusters agencies according to\ncommon mono-product customer choices and co-subscription networks. Within each\ncluster, we efficiently model customer behavior via a cluster-dependent mixture\nof latent eigenmodels. This formulation provides key information on\nmono-product customer choices and multiple purchasing behavior within each\ncluster, informing targeted cross-sell strategies. We develop simple algorithms\nfor tractable inference, and assess performance in simulations and an\napplication to business intelligence.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2015 17:13:33 GMT"}, {"version": "v2", "created": "Mon, 28 Mar 2016 18:05:44 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Durante", "Daniele", ""], ["Paganin", "Sally", ""], ["Scarpa", "Bruno", ""], ["Dunson", "David B.", ""]]}, {"id": "1510.00842", "submitter": "Edward George", "authors": "E.I. George, V. Rockova, P.R. Rosenbaum, V.A. Satopaa and J.H. Silber", "title": "Mortality Rate Estimation and Standardization for Public Reporting:\n  Medicare's Hospital Compare", "comments": "Main paper: 31 pages, 7 figures, 4 tables Supplemental Material: 4\n  pages, 2 figures, 1 table", "journal-ref": "Journal of the American Statistical Association (2017), 112:519,\n  933-947", "doi": "10.1080/01621459.2016.1276021", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian models are increasing fit to large administrative data sets and then\nused to make individualized recommendations. For instance, Medicare's Hospital\nCompare webpage provides information to patients about specific hospital\nmortality rates for a heart attack or Acute Myocardial Infarction (AMI).\nHospital Compare's current recommendations are based on a random effects logit\nmodel with a random hospital indicator and patient risk factors. By checking\nthe out of sample calibration of their individualized predictions against\ngeneral empirical advice, we are led to substantial revisions of the Hospital\nCompare model for AMI mortality. As opposed to Hospital Compare, our revised\nmodels incorporate information about hospital volume, nursing staff, medical\nresidents, and the hospital's ability to perform cardiovascular procedures,\ninformation that is clearly needed if a model is to make appropriately\ncalibrated predictions. Additionally, we contrast several methods for\nsummarizing a model's predictions for use by the public. We find that indirect\nstandardization, as currently used by Hospital Compare, fails to adequately\ncontrol for differences in patient risk factors, whereas direct standardization\nprovides good control and is easy to interpret.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2015 16:02:09 GMT"}, {"version": "v2", "created": "Thu, 14 Jul 2016 17:24:10 GMT"}, {"version": "v3", "created": "Sun, 18 Dec 2016 16:19:36 GMT"}, {"version": "v4", "created": "Tue, 8 Aug 2017 21:04:35 GMT"}, {"version": "v5", "created": "Sat, 31 Mar 2018 10:42:16 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["George", "E. I.", ""], ["Rockova", "V.", ""], ["Rosenbaum", "P. R.", ""], ["Satopaa", "V. A.", ""], ["Silber", "J. H.", ""]]}, {"id": "1510.00918", "submitter": "Koushiki Sarkar", "authors": "Koushiki Sarkar, Diganta Mukherjee", "title": "Testing for Characteristics of Attribute Linked Infinite Networks based\n  on Small Samples", "comments": "Working Paper (To be presented in LSCNA, in conjunction with IEEE\n  ANTS, ISI Kolkata)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this paper is to study the characteristics (geometric and\notherwise) of very large attribute based undirected networks. Real-world\nnetworks are often very large and fast evolving. Their analysis and\nunderstanding present a great challenge. An Attribute based network is a graph\nin which the edges depend on certain properties of the vertices on which they\nare incident. In context of a social network, the existence of links between\ntwo individuals may depend on certain attributes of the two of them. We use the\nLovasz type sampling strategy of observing a certain random process on a graph\nlocally , i.e., in the neighborhood of a node, and deriving information about\nglobal properties of the graph. The corresponding adjacency matrix is our\nprimary object of interest. We study the efficiency of recently proposed\nsampling strategies, modified to our set up, to estimate the degree\ndistribution, centrality measures, planarity etc. The limiting distributions\nare derived using recently developed probabilistic techniques for random\nmatrices and hence we devise relevant test statistics and confidence intervals\nfor different parameters / hypotheses of interest. We hope that our work will\nbe useful for social and computer scientists for designing sampling strategies\nand computational algorithms appropriate to their respective domains of\ninquiry. Extensive simulations studies are done to empirically verify the\nprobabilistic statements made in the paper.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2015 09:40:45 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Sarkar", "Koushiki", ""], ["Mukherjee", "Diganta", ""]]}, {"id": "1510.01298", "submitter": "Andr\\'e Beauducel", "authors": "Andre Beauducel", "title": "A note on structured means analysis for a single group", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The calculation of common factor means in structured means analysis (SMM) is\nconsidered. The SMM equations imply that the unique factors are defined as\nhaving zero means. It was shown within the one factor solution that this\ndefinition implies larger absolute common factor loadings to co-occur with\nlarger absolute expectations of the observed variables in the single group\ncase. This result was illustrated by means of a small simulation study. It is\nargued that the proportionality of factor loadings and observed means should be\ncritically examined in the context of SMM. It is recommended that researchers\nshould freely estimate the observed expectations, whenever, the proportionality\nof loadings and observed means does not make sense. It was also shown that for\na given size of observed expectations, smaller common factor loadings result in\nlarger common factor mean estimates. It should therefore be checked whether\nvariables with very small factor loadings occur when SMM results in very large\nabsolute common factor means.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2015 19:37:21 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Beauducel", "Andre", ""]]}, {"id": "1510.01417", "submitter": "Jason Loeppky Dr.", "authors": "Abigail Arnold and Jason Loeppky", "title": "The Problem with Assessing Statistical Methods", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the problem of assessing statistical methods\nand effectively summarizing results from simulations. Specifically, we consider\nproblems of the type where multiple methods are compared on a reasonably large\ntest set of problems. These simulation studies are typically used to provide\nadvice on an effective method for analyzing future untested problems. Most of\nthese simulation studies never apply statistical methods to find which\nmethod(s) are expected to perform best. Instead, conclusions are based on a\nqualitative assessment of poorly chosen graphical and numerical summaries of\nthe results. We illustrate that the Empirical Cumulative Distribution Function\nwhen used appropriately is an extremely effective tool for assessing what\nmatters in large scale statistical simulations.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2015 03:09:48 GMT"}], "update_date": "2015-10-07", "authors_parsed": [["Arnold", "Abigail", ""], ["Loeppky", "Jason", ""]]}, {"id": "1510.01676", "submitter": "Won Chang", "authors": "Won Chang, Murali Haran, Patrick Applegate, David Pollard", "title": "Improving Ice Sheet Model Calibration Using Paleoclimate and Modern Data", "comments": null, "journal-ref": "The Annals of Applied Statistics, 10 (4), 2274-2302 (2016)", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-induced climate change may cause significant ice volume loss from the\nWest Antarctic Ice Sheet (WAIS). Projections of ice volume change from\nice-sheet models and corresponding future sea-level rise have large\nuncertainties due to poorly constrained input parameters. In most future\napplications to date, model calibration has utilized only modern or recent\n(decadal) observations, leaving input parameters that control the long-term\nbehavior of WAIS largely unconstrained. Many paleo-observations are in the form\nof localized time series, while modern observations are non-Gaussian spatial\ndata; combining information across these types poses non-trivial statistical\nchallenges. Here we introduce a computationally efficient calibration approach\nthat utilizes both modern and paleo-observations to generate better-constrained\nice volume projections. Using fast emulators built upon principal component\nanalysis and a reduced dimension calibration model, we can efficiently handle\nhigh-dimensional and non-Gaussian data. We apply our calibration approach to\nthe PSU3D-ICE model which can realistically simulate long-term behavior of\nWAIS. Our results show that using paleo observations in calibration\nsignificantly reduces parametric uncertainty, resulting in sharper projections\nabout the future state of WAIS. One benefit of using paleo observations is\nfound to be that unrealistic simulations with overshoots in past ice retreat\nand projected future regrowth are eliminated.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2015 17:40:46 GMT"}, {"version": "v2", "created": "Fri, 20 May 2016 04:38:39 GMT"}, {"version": "v3", "created": "Fri, 19 Aug 2016 03:20:38 GMT"}, {"version": "v4", "created": "Wed, 24 Aug 2016 15:07:53 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Chang", "Won", ""], ["Haran", "Murali", ""], ["Applegate", "Patrick", ""], ["Pollard", "David", ""]]}, {"id": "1510.01772", "submitter": "Andrew L. Johnson", "authors": "Jos\\'e Luis Preciado Arreola and Andrew L. Johnson", "title": "Estimating Stochastic Production Frontiers: A One-stage Multivariate\n  Semi-Nonparametric Bayesian Concave Regression Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a method to estimate a production frontier that\nsatisfies the axioms of monotonicity and concavity in a non-parametric Bayesian\nsetting. An inefficiency term that allows for significant departure from prior\ndistributional assumptions is jointly estimated in a single stage with\nparametric prior assumptions. We introduce heteroscedasticity into the\ninefficiency terms by local hyperplane-specific shrinkage hyperparameters and\nimpose monotonicity using bound-constrained local nonlinear regression. Our\nminimum-of-hyperplanes estimator imposes concavity. Our Monte Carlo simulation\nexperiments demonstrate that the frontier and efficiency estimations are\ncompetitive, economically sound, and allow for the analysis of larger datasets\nthan existing nonparametric methods. We validate the proposed method using data\nfrom 2007-2010 for Japan's concrete industry. The results show that the\nefficiency levels remain relatively high over the time period.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2015 22:19:48 GMT"}], "update_date": "2015-10-08", "authors_parsed": [["Arreola", "Jos\u00e9 Luis Preciado", ""], ["Johnson", "Andrew L.", ""]]}, {"id": "1510.01811", "submitter": "Maryam Sohrabi Dr", "authors": "Maryam Sohrabi and Mahmoud Zarepour", "title": "Bootstrapping the Mean Vector for the Observations in the Domain of\n  Attraction of a Multivariate Stable Law", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a robust estimation of the mean vector for a sequence of i.i.d.\nobservations in the domain of attraction of a stable law with different indices\nof stability, $DS(\\alpha_1, \\ldots, \\alpha_p)$, such that $1<\\alpha_{i}\\leq 2$,\n$i=1,\\ldots,p$. The suggested estimator is asymptotically Gaussian with unknown\nparameters. We apply an asymptotically valid bootstrap to construct a\nconfidence region for the mean vector. A simulation study is performed to show\nthat the estimation method is efficient for conducting inference about the mean\nvector for multivariate heavy-tailed distributions.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2015 03:50:37 GMT"}, {"version": "v2", "created": "Mon, 12 Dec 2016 17:53:23 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Sohrabi", "Maryam", ""], ["Zarepour", "Mahmoud", ""]]}, {"id": "1510.02172", "submitter": "Matt Taddy", "authors": "Robert B. Gramacy, Matt Taddy, and Sen Tian", "title": "Hockey Player Performance via Regularized Logistic Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A hockey player's plus-minus measures the difference between goals scored by\nand against that player's team while the player was on the ice. This measures\nonly a marginal effect, failing to account for the influence of the others he\nis playing with and against. A better approach would be to jointly model the\neffects of all players, and any other confounding information, in order to\ninfer a partial effect for this individual: his influence on the box score\nregardless of who else is on the ice.\n  This chapter describes and illustrates a simple algorithm for recovering such\npartial effects. There are two main ingredients. First, we provide a logistic\nregression model that can predict which team has scored a given goal as a\nfunction of who was on the ice, what teams were playing, and details of the\ngame situation (e.g. full-strength or power-play). Since the resulting model is\nso high dimensional that standard maximum likelihood estimation techniques\nfail, our second ingredient is a scheme for regularized estimation. This adds a\npenalty to the objective that favors parsimonious models and stabilizes\nestimation. Such techniques have proven useful in fields from genetics to\nfinance over the past two decades, and have demonstrated an impressive ability\nto gracefully handle large and highly imbalanced data sets. The latest software\npackages accompanying this new methodology -- which exploit parallel computing\nenvironments, sparse matrices, and other features of modern data structures --\nare widely available and make it straightforward for interested analysts to\nexplore their own models of player contribution.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 00:19:20 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2016 22:48:36 GMT"}], "update_date": "2016-01-27", "authors_parsed": [["Gramacy", "Robert B.", ""], ["Taddy", "Matt", ""], ["Tian", "Sen", ""]]}, {"id": "1510.02267", "submitter": "Patrick Heas", "authors": "Patrick H\\'eas and C\\'edric Herzet", "title": "Reduced-Order Modeling Of Hidden Dynamics", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this paper is to investigate how noisy and incomplete\nobservations can be integrated in the process of building a reduced-order\nmodel.\n  This problematic arises in many scientific domains where there exists a need\nfor accurate low-order descriptions of highly-complex phenomena, which can not\nbe directly and/or deterministically observed. Within this context, the paper\nproposes a probabilistic framework for the construction of \"POD-Galerkin\"\nreduced-order models. Assuming a hidden Markov chain, the inference integrates\nthe uncertainty of the hidden states relying on their posterior distribution.\nSimulations show the benefits obtained by exploiting the proposed framework.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 10:11:52 GMT"}, {"version": "v2", "created": "Thu, 17 May 2018 13:45:57 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["H\u00e9as", "Patrick", ""], ["Herzet", "C\u00e9dric", ""]]}, {"id": "1510.02425", "submitter": "Vahed Maroufy", "authors": "Vahed Maroufy and Paul Marriott", "title": "Generalizing the Frailty Assumptions in Survival Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies Cox's regression hazard model with an unobservable random\nfrailty where no specific distribution is postulated for the frailty variable,\nand the marginal lifetime distribution allows both parametric and\nnon-parametric models. Laplace's approximation method and gradient search on\nsmooth manifolds embedded in Euclidean space are applied, and a non-iterative\nprofile likelihood optimization method is proposed for estimating the\nregression coefficients. The proposed method is compared with the\nExpected-Maximization method developed based on a gamma frailty assumption, and\nalso in the case when the frailty model is misspecified.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 17:58:51 GMT"}], "update_date": "2015-10-09", "authors_parsed": [["Maroufy", "Vahed", ""], ["Marriott", "Paul", ""]]}, {"id": "1510.02427", "submitter": "Luis Mu\\~noz-Gonz\\'alez", "authors": "Luis Mu\\~noz-Gonz\\'alez, Daniele Sgandurra, Mart\\'in Barr\\`ere, Emil\n  Lupu", "title": "Exact Inference Techniques for the Analysis of Bayesian Attack Graphs", "comments": "14 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attack graphs are a powerful tool for security risk assessment by analysing\nnetwork vulnerabilities and the paths attackers can use to compromise network\nresources. The uncertainty about the attacker's behaviour makes Bayesian\nnetworks suitable to model attack graphs to perform static and dynamic\nanalysis. Previous approaches have focused on the formalization of attack\ngraphs into a Bayesian model rather than proposing mechanisms for their\nanalysis. In this paper we propose to use efficient algorithms to make exact\ninference in Bayesian attack graphs, enabling the static and dynamic network\nrisk assessments. To support the validity of our approach we have performed an\nextensive experimental evaluation on synthetic Bayesian attack graphs with\ndifferent topologies, showing the computational advantages in terms of time and\nmemory use of the proposed techniques when compared to existing approaches.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 18:01:54 GMT"}, {"version": "v2", "created": "Fri, 4 Nov 2016 18:03:38 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Mu\u00f1oz-Gonz\u00e1lez", "Luis", ""], ["Sgandurra", "Daniele", ""], ["Barr\u00e8re", "Mart\u00edn", ""], ["Lupu", "Emil", ""]]}, {"id": "1510.02430", "submitter": "Linbo Wang", "authors": "Thomas S. Richardson, James M. Robins, Linbo Wang", "title": "On Modeling and Estimation for the Relative Risk and Risk Difference", "comments": "To appear in Journal of the American Statistical Association: Theory\n  and Methods", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common problem in formulating models for the relative risk and risk\ndifference is the variation dependence between these parameters and the\nbaseline risk, which is a nuisance model. We address this problem by proposing\nthe conditional log odds-product as a preferred nuisance model. This novel\nnuisance model facilitates maximum-likelihood estimation, but also permits\ndoubly-robust estimation for the parameters of interest. Our approach is\nillustrated via simulations and a data analysis.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 18:12:19 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2015 02:25:48 GMT"}, {"version": "v3", "created": "Wed, 13 Apr 2016 16:44:38 GMT"}, {"version": "v4", "created": "Fri, 18 Nov 2016 14:19:20 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Richardson", "Thomas S.", ""], ["Robins", "James M.", ""], ["Wang", "Linbo", ""]]}, {"id": "1510.02510", "submitter": "Robert Castelo", "authors": "Alberto Roverato and Robert Castelo", "title": "The networked partial correlation and its application to the analysis of\n  genetic interactions", "comments": "23 pages, 5 figures; major revision of the paper after journal\n  review; 24 pages, 7 figures; figures enlarged, added DOI", "journal-ref": "Journal of the Royal Statistical Society Series C -Applied\n  Statistics, 66(3):647-665, 2017", "doi": "10.1111/rssc.12166", "report-no": null, "categories": "q-bio.QM q-bio.GN q-bio.MN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genetic interactions confer robustness on cells in response to genetic\nperturbations. This often occurs through molecular buffering mechanisms that\ncan be predicted using, among other features, the degree of coexpression\nbetween genes, commonly estimated through marginal measures of association such\nas Pearson or Spearman correlation coefficients. However, marginal correlations\nare sensitive to indirect effects and often partial correlations are used\ninstead. Yet, partial correlations convey no information about the (linear)\ninfluence of the coexpressed genes on the entire multivariate system, which may\nbe crucial to discriminate functional associations from genetic interactions.\nTo address these two shortcomings, here we propose to use the edge weight\nderived from the covariance decomposition over the paths of the associated gene\nnetwork. We call this new quantity the networked partial correlation and use it\nto analyze genetic interactions in yeast.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 21:12:49 GMT"}, {"version": "v2", "created": "Thu, 5 May 2016 09:50:57 GMT"}, {"version": "v3", "created": "Fri, 8 Jul 2016 16:59:01 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Roverato", "Alberto", ""], ["Castelo", "Robert", ""]]}, {"id": "1510.02541", "submitter": "Hau-tieng Wu", "authors": "Christophe L. Herry and Martin Frasch and Andrew JE Seely and\n  Hau-tieng Wu", "title": "Heart beat classification from single-lead ECG using the\n  Synchrosqueezing Transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The processing of ECG signal provides a wealth of information on cardiac\nfunction and overall cardiovascular health. While multi-lead ECG recordings are\noften necessary for a proper assessment of cardiac rhythms, they are not always\navailable or practical, for example in fetal ECG applications. Moreover, a wide\nrange of small non-obtrusive single-lead ECG ambulatory monitoring devices are\nnow available, from which heart rate variability (HRV) and other health-related\nmetrics are derived. Proper beat detection and classification of abnormal\nrhythms is important for reliable HRV assessment and can be challenging in\nsingle-lead ECG monitoring devices. In this manuscript, we modeled the heart\nrate signal as an adaptive non-harmonic model and used the newly developed\nsynchrosqueezing transform (SST) to characterize ECG patterns. We show how the\nproposed model can be used to enhance heart beat detection and classification\nbetween normal and abnormal rhythms. In particular, using the Massachusetts\nInstitute of Technology-Beth Israel Hospital (MIT-BIH) arrhythmia database and\nthe Association for the Advancement of Medical Instrumentation (AAMI) beat\nclasses, we trained and validated a support vector machine (SVM) classifier on\na portion of the annotated beat database using the SST-derived instantaneous\nphase, the R-peak amplitudes and R-peak to R-peak interval durations, based on\na single ECG lead. We obtained sensitivities and positive predictive values\ncomparable to other published algorithms using multiple leads and many more\nfeatures.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2015 01:39:25 GMT"}, {"version": "v2", "created": "Thu, 7 Jul 2016 23:07:37 GMT"}, {"version": "v3", "created": "Sat, 17 Sep 2016 00:33:38 GMT"}, {"version": "v4", "created": "Tue, 22 Nov 2016 02:39:30 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Herry", "Christophe L.", ""], ["Frasch", "Martin", ""], ["Seely", "Andrew JE", ""], ["Wu", "Hau-tieng", ""]]}, {"id": "1510.02557", "submitter": "Matthew Schofield", "authors": "Matthew R. Schofield, Richard J. Barker, Andrew Gelman, Edward R. Cook\n  and Keith R. Briffa", "title": "A Model-Based Approach to Climate Reconstruction Using Tree-Ring Data", "comments": "To appear in the Journal of the American Statistical Association", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantifying long-term historical climate is fundamental to understanding\nrecent climate change. Most instrumentally recorded climate data are only\navailable for the past 200 years, so proxy observations from natural archives\nare often considered. We describe a model-based approach to reconstructing\nclimate defined in terms of raw tree-ring measurement data that simultaneously\naccounts for non-climatic and climatic variability. In this approach we specify\na joint model for the tree-ring data and climate variable that we fit using\nBayesian inference. We consider a range of prior densities and compare the\nmodeling approach to current methodology using an example case of Scots pine\nfrom Tornetrask, Sweden to reconstruct growing season temperature. We describe\nhow current approaches translate into particular model assumptions. We explore\nhow changes to various components in the model-based approach affect the\nresulting reconstruction. We show that minor changes in model specification can\nhave little effect on model fit but lead to large changes in the predictions.\nIn particular, the periods of relatively warmer and cooler temperatures are\nrobust between models, but the magnitude of the resulting temperatures are\nhighly model dependent. Such sensitivity may not be apparent with traditional\napproaches because the underlying statistical model is often hidden or poorly\ndescribed.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2015 03:02:40 GMT"}], "update_date": "2015-10-12", "authors_parsed": [["Schofield", "Matthew R.", ""], ["Barker", "Richard J.", ""], ["Gelman", "Andrew", ""], ["Cook", "Edward R.", ""], ["Briffa", "Keith R.", ""]]}, {"id": "1510.02790", "submitter": "Hoi-Jeong Lim", "authors": "Sung-Bin Hong, Budi Kusnoto, Eunjeong Kim, Ellen A BeGole, Hyeon-Shik\n  Hwang, Hoi-Jeong Lim", "title": "Prognostic factors associated with success rates of posterior\n  orthodontic miniscrew implant", "comments": "accepted by KJO journal, preprinted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three electronic searches were conducted to obtain articles in English\nlimited to clinical human studies published prior to March 2015. The outcome\nmeasure was the success of MIs. Patient factors included age, gender, and jaw;\nthe MI factors included length and diameter. A metaanalysis was then performed\nbased on 17 individual studies. The quality of each study was assessed for\nnonrandomized studies and quantified using the NewcastleOttawa Scale. The\noutcome of the metaanalysis was a combined OR. Subgroup and sensitivity\nanalyses based on the study design, study quality, and sample size of MI were\nperformed.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2015 19:54:21 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2015 10:51:27 GMT"}], "update_date": "2015-10-26", "authors_parsed": [["Hong", "Sung-Bin", ""], ["Kusnoto", "Budi", ""], ["Kim", "Eunjeong", ""], ["BeGole", "Ellen A", ""], ["Hwang", "Hyeon-Shik", ""], ["Lim", "Hoi-Jeong", ""]]}, {"id": "1510.02863", "submitter": "Karl Broman", "authors": "Jianan Tian, Mark P. Keller, Aimee Teo Broman, Christina Kendziorski,\n  Brian S. Yandell, Alan D. Attie, Karl W. Broman", "title": "The dissection of expression quantitative trait locus hotspots", "comments": "40 pages, 6 figures, 3 supplemental figures, and a separate PDF file\n  (FileS1.pdf) with an additional 35 pages of figures; made small changes to\n  text on pages 26-31 in response to reviewers' comments; corrected a number of\n  typographical errors and added acknowledgment of an additional grant", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.GN", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Studies of the genetic loci that contribute to variation in gene expression\nfrequently identify loci with broad effect on gene expression: expression\nquantitative trait locus (eQTL) hotspots. We describe a set of exploratory\ngraphical methods as well as a formal likelihood-based test for assessing\nwhether a given hotspot is due to one or multiple polymorphisms. We first look\nat the pattern of effects of the locus on the expression traits that map to the\nlocus: the direction of the effects, as well as the degree of dominance. A\nsecond technique is to focus on the individuals that exhibit no recombination\nevent in the region, apply dimensionality reduction (such as with linear\ndiscriminant analysis) and compare the phenotype distribution in the\nnon-recombinants to that in the recombinant individuals: If the recombinant\nindividuals display a different expression pattern than the non-recombinants,\nthis indicates the presence of multiple causal polymorphisms. In the formal\nlikelihood-based test, we compare a two-locus model, with each expression trait\naffected by one or the other locus, to a single-locus model. We apply our\nmethods to a large mouse intercross with gene expression microarray data on six\ntissues.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2015 02:15:13 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2016 15:01:12 GMT"}, {"version": "v3", "created": "Wed, 17 Feb 2016 17:32:23 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Tian", "Jianan", ""], ["Keller", "Mark P.", ""], ["Broman", "Aimee Teo", ""], ["Kendziorski", "Christina", ""], ["Yandell", "Brian S.", ""], ["Attie", "Alan D.", ""], ["Broman", "Karl W.", ""]]}, {"id": "1510.02934", "submitter": "Cheng Koay", "authors": "Cheng Guan Koay, Ping-Hong Yeh, John M. Ollinger, M. Okan\n  \\.Irfano\\u{g}lu, Carlo Pierpaoli, Peter J. Basser, Terrence R. Oakes, Gerard\n  Riedy", "title": "Tract Orientation and Angular Dispersion Deviation Indicator (TOADDI): A\n  framework for single-subject analysis in diffusion tensor imaging", "comments": "49 pages, 6 figures, 2 tables", "journal-ref": null, "doi": "10.1016/j.neuroimage.2015.11.046", "report-no": null, "categories": "physics.med-ph cs.CV stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this work is to develop a framework for single-subject\nanalysis of diffusion tensor imaging (DTI) data. This framework (termed TOADDI)\nis capable of testing whether an individual tract as represented by the major\neigenvector of the diffusion tensor and its corresponding angular dispersion\nare significantly different from a group of tracts on a voxel-by-voxel basis.\nThis work develops two complementary statistical tests based on the elliptical\ncone of uncertainty (COU), which is a model of uncertainty or dispersion of the\nmajor eigenvector of the diffusion tensor. The orientation deviation test\nexamines whether the major eigenvector from a single subject is within the\naverage elliptical COU formed by a collection of elliptical COUs. The shape\ndeviation test is based on the two-tailed Wilcoxon-Mann-Whitney two-sample test\nbetween the normalized shape measures (area and circumference) of the\nelliptical cones of uncertainty of the single subject against a group of\ncontrols. The False Discovery Rate (FDR) and False Non-discovery Rate (FNR)\nwere incorporated in the orientation deviation test. The shape deviation test\nuses FDR only. TOADDI was found to be numerically accurate and statistically\neffective. Clinical data from two Traumatic Brain Injury (TBI) patients and one\nnon-TBI subject were tested against the data obtained from a group of 45\nnon-TBI controls to illustrate the application of the proposed framework in\nsingle-subject analysis. The frontal portion of the superior longitudinal\nfasciculus seemed to be implicated in both tests as significantly different\nfrom that of the control group. The TBI patients and the single non-TBI subject\nwere well separated under the shape deviation test at the chosen FDR level of\n0.0005. TOADDI is a simple but novel geometrically based statistical framework\nfor analyzing DTI data.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2015 13:54:07 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2015 01:35:14 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Koay", "Cheng Guan", ""], ["Yeh", "Ping-Hong", ""], ["Ollinger", "John M.", ""], ["\u0130rfano\u011flu", "M. Okan", ""], ["Pierpaoli", "Carlo", ""], ["Basser", "Peter J.", ""], ["Oakes", "Terrence R.", ""], ["Riedy", "Gerard", ""]]}, {"id": "1510.03040", "submitter": "Sadegh Movahed", "authors": "Z. Koohi Lai, S. Vasheghani Farahani, S.M.S. Movahed and G.R. Jafari", "title": "Coupled uncertainty provided by a multifractal random walker", "comments": "9 pages and 4 figures", "journal-ref": "Physics Letters A 379 (2015) 2284-2290", "doi": "10.1016/j.physleta.2015.07.044", "report-no": null, "categories": "physics.data-an cond-mat.stat-mech q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim here is to study the concept of pairing multifractality between time\nseries possessing non-Gaussian distributions. The increasing number of rare\nevents creates \"criticality\". We show how the pairing between two series is\naffected by rare events, which we call \"coupled criticality\". A method is\nproposed for studying the coupled criticality born out of the interaction\nbetween two series, using the bivariate multifractal random walk (BiMRW). This\nmethod allows studying dependence of the coupled criticality on the criticality\nof each individual system. This approach is applied to data sets of gold and\noil markets, and inflation and unemployment.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2015 11:30:31 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Lai", "Z. Koohi", ""], ["Farahani", "S. Vasheghani", ""], ["Movahed", "S. M. S.", ""], ["Jafari", "G. R.", ""]]}, {"id": "1510.03229", "submitter": "Anirudh Acharya", "authors": "Anirudh Acharya, Theodore Kypraios, Madalin Guta", "title": "Statistically efficient tomography of low rank states with incomplete\n  measurements", "comments": "19 pages, 6 figures ; V2: updated figure 5, added references, changed\n  title, updated abstract", "journal-ref": "New Journal of Physics, Volume 18, April 2016", "doi": "10.1088/1367-2630/18/4/043018", "report-no": null, "categories": "quant-ph math-ph math.MP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The construction of physically relevant low dimensional state models, and the\ndesign of appropriate measurements are key issues in tackling quantum state\ntomography for large dimensional systems. We consider the statistical problem\nof estimating low rank states in the set-up of multiple ions tomography, and\ninvestigate how the estimation error behaves with a reduction in the number of\nmeasurement settings, compared with the standard ion tomography setup. We\npresent extensive simulation results showing that the error is robust with\nrespect to the choice of states of a given rank, the random selection of\nsettings, and that the number of settings can be significantly reduced with\nonly a negligible increase in error. We present an argument to explain these\nfindings based on a concentration inequality for the Fisher information matrix.\nIn the more general setup of random basis measurements we use this argument to\nshow that for certain rank $r$ states it suffices to measure in $O(r\\log d)$\nbases to achieve the average Fisher information over all bases. We present\nnumerical evidence for states upto 8 atoms, supporting a conjecture on a lower\nbound for the Fisher information which, if true, would imply a similar\nbehaviour in the case of Pauli bases. The relation to similar problems in\ncompressed sensing is also discussed.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2015 11:11:43 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2015 15:19:41 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Acharya", "Anirudh", ""], ["Kypraios", "Theodore", ""], ["Guta", "Madalin", ""]]}, {"id": "1510.03349", "submitter": "Wenjie Zheng", "authors": "Wenjie Zheng", "title": "Toward a Better Understanding of Leaderboard", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The leaderboard in machine learning competitions is a tool to show the\nperformance of various participants and to compare them. However, the\nleaderboard quickly becomes no longer accurate, due to hack or overfitting.\nThis article gives two pieces of advice to prevent easy hack or overfitting. By\nfollowing these advice, we reach the conclusion that something like the Ladder\nleaderboard introduced in [blum2015ladder] is inevitable. With this\nunderstanding, we naturally simplify Ladder by eliminating its redundant\ncomputation and explain how to choose the parameter and interpret it. We also\nprove that the sample complexity is cubic to the desired precision of the\nleaderboard.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2015 16:12:49 GMT"}, {"version": "v2", "created": "Wed, 7 Jun 2017 13:12:56 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Zheng", "Wenjie", ""]]}, {"id": "1510.03385", "submitter": "David Puelz", "authors": "David Puelz, Carlos M. Carvalho, P. Richard Hahn", "title": "Optimal ETF Selection for Passive Investing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of isolating a small number of exchange\ntraded funds (ETFs) that suffice to capture the fundamental dimensions of\nvariation in U.S. financial markets. First, the data is fit to a vector-valued\nBayesian regression model, which is a matrix-variate generalization of the well\nknown stochastic search variable selection (SSVS) of George and McCulloch\n(1993). ETF selection is then performed using the decoupled shrinkage and\nselection (DSS) procedure described in Hahn and Carvalho (2015), adapted in two\nways: to the vector-response setting and to incorporate stochastic covariates.\nThe selected set of ETFs is obtained under a number of different penalty and\nmodeling choices. Optimal portfolios are constructed from selected ETFs by\nmaximizing the Sharpe ratio posterior mean, and they are compared to the\n(unknown) optimal portfolio based on the full Bayesian model. We compare our\nselection results to popular ETF advisor Wealthfront.com. Additionally, we\nconsider selecting ETFs by modeling a large set of mutual funds.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2015 18:34:35 GMT"}, {"version": "v2", "created": "Sat, 28 Nov 2015 12:47:31 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Puelz", "David", ""], ["Carvalho", "Carlos M.", ""], ["Hahn", "P. Richard", ""]]}, {"id": "1510.03840", "submitter": "Vasileios Kapinas", "authors": "Alexandros E. Paschos, Vasileios M. Kapinas, Georgia D. Ntouni,\n  Leontios J. Hadjileontiadis, and George K. Karagiannidis", "title": "Dynamic Spectrum Sensing Through Accelerated Particle Swarm Optimization", "comments": "4 pages, 3 figures, 2 algorithms, 1 table", "journal-ref": "Proc. Telecommunications Forum (TELFOR), pp. 1-4, Belgrade,\n  Serbia, November 2017", "doi": "10.1109/TELFOR.2017.8249365", "report-no": null, "categories": "math.OC cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel optimization algorithm, called the acceleration-aided\nparticle swarm optimization (AAPSO), is proposed for reliable dynamic spectrum\nsensing in cognitive radio networks. In A-APSO, the acceleration variable of\nthe particles in the swarm is also considered in the search space of the\noptimization problem. We show that the proposed A-APSO based spectrum sensing\ntechnique is more efficient in terms of performance than the corresponding one\nbased on the standard particle swarm optimization algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2015 21:17:36 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2015 15:39:52 GMT"}, {"version": "v3", "created": "Mon, 4 Jan 2016 09:16:45 GMT"}, {"version": "v4", "created": "Thu, 4 Feb 2016 00:45:52 GMT"}, {"version": "v5", "created": "Mon, 30 May 2016 20:04:46 GMT"}, {"version": "v6", "created": "Sat, 10 Mar 2018 18:32:09 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Paschos", "Alexandros E.", ""], ["Kapinas", "Vasileios M.", ""], ["Ntouni", "Georgia D.", ""], ["Hadjileontiadis", "Leontios J.", ""], ["Karagiannidis", "George K.", ""]]}, {"id": "1510.03924", "submitter": "Steffen Moritz", "authors": "Steffen Moritz, Alexis Sard\\'a, Thomas Bartz-Beielstein, Martin\n  Zaefferer, J\\\"org Stork", "title": "Comparison of different Methods for Univariate Time Series Imputation in\n  R", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.OH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing values in datasets are a well-known problem and there are quite a lot\nof R packages offering imputation functions. But while imputation in general is\nwell covered within R, it is hard to find functions for imputation of\nunivariate time series. The problem is, most standard imputation techniques can\nnot be applied directly. Most algorithms rely on inter-attribute correlations,\nwhile univariate time series imputation needs to employ time dependencies. This\npaper provides an overview of univariate time series imputation in general and\nan in-detail insight into the respective implementations within R packages.\nFurthermore, we experimentally compare the R functions on different time series\nusing four different ratios of missing data. Our results show that either an\ninterpolation with seasonal kalman filter from the zoo package or a linear\ninterpolation on seasonal loess decomposed data from the forecast package were\nthe most effective methods for dealing with missing data in most of the\nscenarios assessed in this paper.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2015 23:16:10 GMT"}], "update_date": "2015-10-15", "authors_parsed": [["Moritz", "Steffen", ""], ["Sard\u00e1", "Alexis", ""], ["Bartz-Beielstein", "Thomas", ""], ["Zaefferer", "Martin", ""], ["Stork", "J\u00f6rg", ""]]}, {"id": "1510.04162", "submitter": "Pranay Seshadri", "authors": "Pranay Seshadri, Geoffrey Parks, Shahrokh Shahpar", "title": "Density-Matching for Turbomachinery Optimization Under Uncertainty", "comments": "9 pages", "journal-ref": null, "doi": "10.1016/j.cma.2016.03.006", "report-no": null, "categories": "stat.AP math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A monotonic, non-kernel density variant of the density-matching technique for\noptimization under uncertainty is developed. The approach is suited for\nturbomachinery problems which, by and large, tend to exhibit monotonic\nvariations in the circumferentially and radially mass-averaged quantities--such\nas pressure ratio, efficiency and capacity--with common aleatory turbomachinery\nuncertainties. The method is successfully applied to de-sensitize the effect of\nan uncertainty in rear-seal leakage flows on the fan stage of a modern jet\nengine.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2015 15:41:30 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Seshadri", "Pranay", ""], ["Parks", "Geoffrey", ""], ["Shahpar", "Shahrokh", ""]]}, {"id": "1510.04180", "submitter": "Andrew Hart PhD", "authors": "Andrew Hart and Servet Mart\\'inez", "title": "An Entropy-Based Technique for Classifying Bacterial Chromosomes\n  According to Synonymous Codon Usage", "comments": "22 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework based on conditional entropy and the Dirichlet\ndistribution for classifying chromosomes based on the degree to which they use\nsynonymous codons uniformly or preferentially, that is, whether or not codons\nthat code for an amino acid appear with the same relative frequency. Applying\nthe approach to a large collection of annotated bacterial chromosomes reveals\nthree distinct groups of bacteria.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2015 16:03:09 GMT"}], "update_date": "2015-10-15", "authors_parsed": [["Hart", "Andrew", ""], ["Mart\u00ednez", "Servet", ""]]}, {"id": "1510.04597", "submitter": "Damien Fay", "authors": "Damien Fay", "title": "Predictive partitioning for efficient BFS traversal in social networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show how graph structure can be used to drastically reduce\nthe computational bottleneck of the Breadth First Search algorithm (the\nfoundation of many graph traversal techniques). In particular, we address\nparallel implementations where the bottleneck is the number of messages between\nprocessors emitted at the peak iteration. First, we derive an expression for\nthe expected degree distribution of vertices in the frontier of the algorithm\nwhich is shown to be highly skewed. Subsequently, we derive an expression for\nthe expected message along an edge in a particular iteration. This skew\nsuggests a weighted, iteration based, partition would be advantageous.\nEmploying the METIS algorithm we then show empirically that such partitions can\nreduce the message overhead by up to 50% in some particular instances and in\nthe order of 20% on average. These results have implications for graph\nprocessing in multiprocessor and distributed computing environments.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 15:50:51 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Fay", "Damien", ""]]}, {"id": "1510.05154", "submitter": "Christopher Gatti", "authors": "Christopher J. Gatti, James D. Brooks, and Sarah G. Nurre", "title": "A Historical Analysis of the Field of OR/MS using Topic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study investigates the content of the published scientific literature in\nthe fields of operations research and management science (OR/MS) since the\nearly 1950s. Our study is based on 80,757 published journal abstracts from 37\nof the leading OR/MS journals. We have developed a topic model, using Latent\nDirichlet Allocation (LDA), and extend this analysis to reveal the temporal\ndynamics of the field, journals, and topics. Our analysis shows the generality\nor specificity of each of the journals, and we identify groups of journals with\nsimilar content, which are both consistent and inconsistent with intuition. We\nalso show how journals have become more or less unique in their scope. A more\ndetailed analysis of each journals' topics over time shows significant temporal\ndynamics, especially for journals with niche content. This study presents an\nobservational, yet objective, view of the published literature from OR/MS that\nwould be of interest to authors, editors, journals, and publishers.\nFurthermore, this work can be used by new entrants to the fields of OR/MS to\nunderstand the content landscape, as a starting point for discussions and\ninquiry of the field at large, or as a model for other fields to perform\nsimilar analyses.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2015 18:52:24 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Gatti", "Christopher J.", ""], ["Brooks", "James D.", ""], ["Nurre", "Sarah G.", ""]]}, {"id": "1510.05234", "submitter": "Fr\\'ed\\'eric Pro\\\"ia", "authors": "Fr\\'ed\\'eric Pro\\\"ia, Alix Pernet, Tatiana Thouroude, Gilles Michel,\n  J\\'er\\'emy Clotault", "title": "On the characterization of flowering curves using Gaussian mixture\n  models", "comments": "28 pages, 27 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a statistical methodology applied to the\ncharacterization of flowering curves using Gaussian mixture models. Our study\nrelies on a set of rosebushes flowering data, and Gaussian mixture models are\nmainly used to quantify the reblooming properties of each one. In this regard,\nwe also suggest our own selection criterion to take into account the lack of\nsymmetry of most of the flowering curves. Three classes are created on the\nbasis of a principal component analysis conducted on a set of reblooming\nindicators, and a subclassification is made using a longitudinal $k$--means\nalgorithm which also highlights the role played by the precocity of the\nflowering. In this way, we obtain an overview of the correlations between the\nfeatures we decided to retain on each curve. In particular, results suggest the\nlack of correlation between reblooming and flowering precocity. The pertinent\nindicators obtained in this study will be a first step towards the\ncomprehension of the environmental and genetic control of these biological\nprocesses.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2015 12:40:18 GMT"}, {"version": "v2", "created": "Mon, 18 Apr 2016 19:16:17 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Pro\u00efa", "Fr\u00e9d\u00e9ric", ""], ["Pernet", "Alix", ""], ["Thouroude", "Tatiana", ""], ["Michel", "Gilles", ""], ["Clotault", "J\u00e9r\u00e9my", ""]]}, {"id": "1510.05391", "submitter": "Daniele Durante", "authors": "Daniele Durante, Madelaine Daianu, Neda Jahanshad, Paul M. Thompson,\n  David B. Dunson", "title": "Unifying inference on brain network variations in neurological diseases:\n  The Alzheimer's case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is growing interest in understanding how the structural\ninterconnections among brain regions change with the occurrence of neurological\ndiseases. Diffusion weighted MRI imaging has allowed researchers to\nnon-invasively estimate a network of structural cortical connections made by\nwhite matter tracts, but current statistical methods for relating such networks\nto the presence or absence of a disease cannot exploit this rich network\ninformation. Standard practice considers each edge independently or summarizes\nthe network with a few simple features. We enable dramatic gains in biological\ninsight via a novel unifying methodology for inference on brain network\nvariations associated to the occurrence of neurological diseases. The key of\nthis approach is to define a probabilistic generative mechanism directly on the\nspace of network configurations via dependent mixtures of low-rank\nfactorizations, which efficiently exploit network information and allow the\nprobability mass function for the brain network-valued random variable to vary\nflexibly across the group of patients characterized by a specific neurological\ndisease and the one comprising age-matched cognitively healthy individuals.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2015 08:49:52 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Durante", "Daniele", ""], ["Daianu", "Madelaine", ""], ["Jahanshad", "Neda", ""], ["Thompson", "Paul M.", ""], ["Dunson", "David B.", ""]]}, {"id": "1510.05553", "submitter": "Radu Stoica", "authors": "R. S. Stoica, S. Liu, L. J. Liivam\\\"agi, E. Saar, E. Tempel, F.\n  Deleflie, M. Fouchard, D. Hestroffer, I. Kovalenko and A. Vienne", "title": "An integrative approach based on probabilistic modelling and statistical\n  inference for morpho-statistical characterization of astronomical data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes several applications in astronomy and cosmology that are\naddressed using probabilistic modelling and statistical inference.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2015 16:01:02 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Stoica", "R. S.", ""], ["Liu", "S.", ""], ["Liivam\u00e4gi", "L. J.", ""], ["Saar", "E.", ""], ["Tempel", "E.", ""], ["Deleflie", "F.", ""], ["Fouchard", "M.", ""], ["Hestroffer", "D.", ""], ["Kovalenko", "I.", ""], ["Vienne", "A.", ""]]}, {"id": "1510.05569", "submitter": "Amit Sharma", "authors": "Amit Sharma, Jake M. Hofman, Duncan J. Watts", "title": "Estimating the Causal Impact of Recommendation Systems from\n  Observational Data", "comments": "ACM Conference on Economics and Computation (EC 2015)", "journal-ref": null, "doi": "10.1145/2764468.2764488", "report-no": null, "categories": "cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation systems are an increasingly prominent part of the web,\naccounting for up to a third of all traffic on several of the world's most\npopular sites. Nevertheless, little is known about how much activity such\nsystems actually cause over and above activity that would have occurred via\nother means (e.g., search) if recommendations were absent. Although the ideal\nway to estimate the causal impact of recommendations is via randomized\nexperiments, such experiments are costly and may inconvenience users. In this\npaper, therefore, we present a method for estimating causal effects from purely\nobservational data. Specifically, we show that causal identification through an\ninstrumental variable is possible when a product experiences an instantaneous\nshock in direct traffic and the products recommended next to it do not. We then\napply our method to browsing logs containing anonymized activity for 2.1\nmillion users on Amazon.com over a 9 month period and analyze over 4,000 unique\nproducts that experience such shocks. We find that although recommendation\nclick-throughs do account for a large fraction of traffic among these products,\nat least 75% of this activity would likely occur in the absence of\nrecommendations. We conclude with a discussion about the assumptions under\nwhich the method is appropriate and caveats around extrapolating results to\nother products, sites, or settings.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2015 16:27:49 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Sharma", "Amit", ""], ["Hofman", "Jake M.", ""], ["Watts", "Duncan J.", ""]]}, {"id": "1510.05677", "submitter": "Jonas Haslbeck", "authors": "Jonas M. B. Haslbeck, Lourens J. Waldorp", "title": "Structure estimation for mixed graphical models in high-dimensional data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Undirected graphical models are a key component in the analysis of complex\nobservational data in a large variety of disciplines. In many of these\napplications one is interested in estimating the undirected graphical model\nunderlying a distribution over variables with different domains. Despite the\npervasive need for such an estimation method, to date there is no such method\nthat models all variables on their proper domain. We close this methodological\ngap by combining a new class of mixed graphical models with a structure\nestimation approach based on generalized covariance matrices. We report the\nperformance of our methods using simulations, illustrate the method with a\ndataset on Autism Spectrum Disorder (ASD) and provide an implementation as an\nR-package.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2015 20:36:35 GMT"}], "update_date": "2015-10-21", "authors_parsed": [["Haslbeck", "Jonas M. B.", ""], ["Waldorp", "Lourens J.", ""]]}, {"id": "1510.05968", "submitter": "Sylvain Robbiano", "authors": "Sylvain Robbiano, Matthieu Saumard and Michel Cur\\'e", "title": "Improving prediction performance of stellar parameters using functional\n  models", "comments": null, "journal-ref": null, "doi": "10.1080/02664763.2015.1106448", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the problem of prediction of stellar parameters,\nbased on the star's electromagnetic spectrum. The knowledge of these parameters\npermits to infer on the evolutionary state of the star. From a statistical\npoint of view, the spectra of different stars can be represented as functional\ndata. Therefore, a two-step procedure decomposing the spectra in a functional\nbasis combined with a regression method of prediction is proposed. We also use\na bootstrap methodology to build prediction intervals for the stellar\nparameters. A practical application is also provided to illustrate the\nnumerical performance of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2015 17:08:02 GMT"}], "update_date": "2015-10-21", "authors_parsed": [["Robbiano", "Sylvain", ""], ["Saumard", "Matthieu", ""], ["Cur\u00e9", "Michel", ""]]}, {"id": "1510.06021", "submitter": "Anthony J  Webster", "authors": "Anthony J. Webster", "title": "Lightning Strikes and Attribution of Climatic Change", "comments": "10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using lightning strikes as an example, two possible schemes are discussed for\nthe attribution of changes in event frequency to climate change, and estimating\nthe cost associated with them. The schemes determine the fraction of events\nthat should be attributed to climatic change, and the fraction that should be\nattributed to natural chance. They both allow for the expected increase in\nclaims and the fluctuations about this expected value. Importantly, the\nattribution fraction proposed in the second of these schemes is necessarily\ndifferent to that found in epidemiological studies. This ensures that the\nstatistically expected fraction of attributed claims is correctly equal to the\nexpected increase in claims. The analysis of lightning data highlights two\nparticular difficulties with data-driven, as opposed to modeled, attribution\nstudies. The first is the possibility of unknown \"confounding\" variables that\ncan influence the strike frequency. This is partly accounted for here by\nconsidering the influence of temperature changes within a given month, so as to\nstandardise the data to allow for cyclical climatic influences. The second is\nthe possibility suggested by the data presented here, that climate change may\nlead to qualitatively different climate patterns, with a different relationship\nbetween e.g. strike frequency and temperature.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2015 15:48:22 GMT"}], "update_date": "2015-10-21", "authors_parsed": [["Webster", "Anthony J.", ""]]}, {"id": "1510.06091", "submitter": "Nicolas Kim", "authors": "Nicolas Kim", "title": "The Effect of Data Swapping on Analyses of American Community Survey\n  Data", "comments": "19 pages, 7 figures. Submitted to the Journal of Privacy and\n  Confidentiality", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers from a growing range of fields and industries rely on\npublic-access census data. These data are altered by census-taking agencies to\nminimize the risk of identification; one such disclosure avoidance measure is\nthe data swapping procedure. I study the effects of data swapping on\ncontingency tables using a dummy dataset, public-use American Community Survey\n(ACS) data, and restricted-use ACS data accessed within the U.S.\\ Census\nBureau. These simulations demonstrate that as the rate of swapping is varied,\nthe effect on joint distributions of categorical variables is no longer\nunderstandable when the data swapping procedure attempts to target at-risk\nindividuals for swapping using a simple targeting criterion.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2015 00:09:48 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2015 02:04:10 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Kim", "Nicolas", ""]]}, {"id": "1510.06731", "submitter": "Nassim Nicholas Taleb", "authors": "Nassim Nicholas Taleb, Pasquale Cirillo", "title": "On the shadow moments of apparently infinite-mean phenomena", "comments": null, "journal-ref": "Unifying Themes in Complex Systems IX (2018), Proceedings of the\n  Ninth International Conference on Complex Systems, Alfredo J. Morales ,\n  Carlos Gershenson, Dan Braha, Ali A. Minai, & Yaneer Bar-Yam, Eds., Springer,\n  pp 155-164", "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach to compute the conditional moments of fat-tailed\nphenomena that, only looking at data, could be mistakenly considered as having\ninfinite mean. This type of problems manifests itself when a random variable Y\nhas a heavy- tailed distribution with an extremely wide yet bounded support. We\nintroduce the concept of dual distribution, by means of a log-transformation\nthat removes the upper bound. The tail of the dual distribution can then be\nstudied using extreme value theory, without making excessive parametric\nassumptions, and the estimates one obtains can be used to study the original\ndistribution and compute its moments by reverting the log- transformation. The\ncentral difference between our approach and a simple truncation is in the\nsmoothness of the transformation between the original and the dual\ndistribution, allowing use of extreme value theory. War casualties, operational\nrisk, environment blight, complex networks and many other econophysics\nphenomena are possible fields of application.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2015 19:48:31 GMT"}, {"version": "v2", "created": "Sun, 7 Feb 2016 15:15:33 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Taleb", "Nassim Nicholas", ""], ["Cirillo", "Pasquale", ""]]}, {"id": "1510.06732", "submitter": "Ali Bozdogan", "authors": "Ali Onder Bozdogan, Roy Streit, Murat Efe", "title": "Reduced Palm Intensity for Track Extraction", "comments": "Manuscript is accepted by the IEEE Transactions on Aerospace and\n  Electronic Systems and scheduled for publication in the December 2016 issue", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pair correlation function is introduced to target tracking filters that\nuse a finite point process target model as a means to investigate interactions\nin the Bayes posterior target process. It is shown that the Bayes posterior\ntarget point process of the probability hypothesis density (PHD) filter-before\nusing the Poisson point process approximation to close the recursion-is a\nspatially correlated process with weakly repulsive pair interactions. The\nreduced Palm target point process is introduced to define the conditional\ntarget point process given the state of one or more known targets. Using the\nintensity function of the reduced Palm process, an approximate two-stage pseudo\nmaximum a posteriori track extractor is developed. The proposed track extractor\nis formulated for the PHD filter and implemented in a numerical study that\ninvolves tracking two close-by targets. Numerical results demonstrate\nimprovement in the mean optimal subpattern assignment statistic for the\nproposed track extractor in comparison to the Gaussian mixture PHD filter's\nstate extractor.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2015 19:50:46 GMT"}, {"version": "v2", "created": "Sun, 5 Jun 2016 17:29:41 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Bozdogan", "Ali Onder", ""], ["Streit", "Roy", ""], ["Efe", "Murat", ""]]}, {"id": "1510.06787", "submitter": "Agnieszka Werpachowska", "authors": "Agnieszka Werpachowska and Roman Werpachowski", "title": "Cross-sectional Markov model for trend analysis of observed discrete\n  distributions of population characteristics", "comments": "21 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a stochastic model of population dynamics exploiting\ncross-sectional data in trend analysis and forecasts for groups and cohorts of\na population. While sharing the convenient features of classic Markov models,\nit alleviates the practical problems experienced in longitudinal studies. Based\non statistical and information-theoretical analysis, we adopt maximum\nlikelihood estimation to determine model parameters, facilitating the use of a\nrange of model selection methods. Their application to several synthetic and\nempirical datasets shows that the proposed approach is robust, stable and\nsuperior to a regression-based one. We extend the basic framework to simulate\nageing cohorts, processes with finite memory, distinguishing their short and\nlong-term trends, introduce regularisation to avoid the ecological fallacy, and\ngeneralise it to mixtures of cross-sectional and (possibly incomplete)\nlongitudinal data. The presented model illustrations yield new and interesting\nresults, such as an implied common driving factor in obesity for all\ngenerations of the English population and \"yo-yo\" dieting in the U.S. data.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2015 23:22:23 GMT"}, {"version": "v2", "created": "Sat, 17 Jun 2017 22:41:52 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Werpachowska", "Agnieszka", ""], ["Werpachowski", "Roman", ""]]}, {"id": "1510.06852", "submitter": "Aristidis K. Nikoloulopoulos", "authors": "Aristidis K. Nikoloulopoulos", "title": "Correlation structure and variable selection in generalized estimating\n  equations via composite likelihood information criteria", "comments": null, "journal-ref": "Statistics in Medicine, 2016, 35(14), 2377--2390", "doi": "10.1002/sim.6871", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method of generalized estimating equations (GEE) is popular in the\nbiostatistics literature for analyzing longitudinal binary and count data. It\nassumes a generalized linear model (GLM) for the outcome variable, and a\nworking correlation among repeated measurements. In this paper, we introduce a\nviable competitor: the weighted scores method for GLM margins. We weight the\nunivariate score equations using a working discretized multivariate normal\nmodel that is a proper multivariate model. Since the weighted scores method is\na parametric method based on likelihood, we propose composite likelihood\ninformation criteria as an intermediate step for model selection. The same\ncriteria can be used for both correlation structure and variable selection.\nSimulations studies and the application example show that our method\noutperforms other existing model selection methods in GEE. From the example, it\ncan be seen that our methods allow for correct analysis, and may change the\ninferential results.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2015 08:13:13 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2015 18:04:51 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Nikoloulopoulos", "Aristidis K.", ""]]}, {"id": "1510.06871", "submitter": "Jonas Haslbeck", "authors": "Jonas M. B. Haslbeck, Lourens J. Waldorp", "title": "mgm: Estimating Time-Varying Mixed Graphical Models in High-Dimensional\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the R-package mgm for the estimation of k-order Mixed Graphical\nModels (MGMs) and mixed Vector Autoregressive (mVAR) models in high-dimensional\ndata. These are a useful extensions of graphical models for only one variable\ntype, since data sets consisting of mixed types of variables (continuous,\ncount, categorical) are ubiquitous. In addition, we allow to relax the\nstationarity assumption of both models by introducing time-varying versions\nMGMs and mVAR models based on a kernel weighting approach. Time-varying models\noffer a rich description of temporally evolving systems and allow to identify\nexternal influences on the model structure such as the impact of interventions.\nWe provide the background of all implemented methods and provide fully\nreproducible examples that illustrate how to use the package.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2015 09:48:51 GMT"}, {"version": "v2", "created": "Tue, 26 Apr 2016 23:45:26 GMT"}, {"version": "v3", "created": "Fri, 26 May 2017 13:15:33 GMT"}, {"version": "v4", "created": "Tue, 20 Jun 2017 08:37:41 GMT"}, {"version": "v5", "created": "Sun, 10 Jun 2018 01:21:02 GMT"}, {"version": "v6", "created": "Mon, 7 Jan 2019 10:48:09 GMT"}, {"version": "v7", "created": "Mon, 21 Oct 2019 09:57:18 GMT"}, {"version": "v8", "created": "Wed, 12 Feb 2020 11:07:11 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Haslbeck", "Jonas M. B.", ""], ["Waldorp", "Lourens J.", ""]]}, {"id": "1510.07129", "submitter": "Abhirup Datta", "authors": "Abhirup Datta, Hui Zou and Sudipto Banerjee", "title": "Bayesian Inference for High Dimensional Changing Linear Regression with\n  Application to Minnesota House Price Index Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, the dataset under investigation exhibits heterogeneous\nregimes that are more appropriately modeled using piece-wise linear models for\neach of the data segments separated by change-points. Although there have been\nmuch work on change point linear regression for the low dimensional case,\nhigh-dimensional change point regression is severely underdeveloped. Motivated\nby the analysis of Minnesota House Price Index data, we propose a fully\nBayesian framework for fitting changing linear regression models in\nhigh-dimensional settings. Using segment-specific shrinkage and diffusion\npriors, we deliver full posterior inference for the change points and\nsimultaneously obtain posterior probabilities of variable selection in each\nsegment via an efficient Gibbs sampler. Additionally, our method can detect an\nunknown number of change points and accommodate different variable selection\nconstraints like grouping or partial selection. We substantiate the accuracy of\nour method using simulation experiments for a wide range of scenarios. We apply\nour approach for a macro-economic analysis of Minnesota house price index data.\nThe results strongly favor the change point model over a homogeneous (no change\npoint) high-dimensional regression model.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2015 10:28:14 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Datta", "Abhirup", ""], ["Zou", "Hui", ""], ["Banerjee", "Sudipto", ""]]}, {"id": "1510.07130", "submitter": "Abhirup Datta", "authors": "Abhirup Datta, Sudipto Banerjee, Andrew O. Finley, Nicholas A.S. Hamm\n  and Martijn Schaap", "title": "Non-separable Dynamic Nearest-Neighbor Gaussian Process Models for Large\n  spatio-temporal Data With an Application to Particulate Matter Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particulate matter (PM) is a class of malicious environmental pollutants\nknown to be detrimental to human health. Regulatory efforts aimed at curbing PM\nlevels in different countries often require high resolution space-time maps\nthat can identify red-flag regions exceeding statutory concentration limits.\nContinuous spatio-temporal Gaussian Process (GP) models can deliver maps\ndepicting predicted PM levels and quantify predictive uncertainty. However, GP\nbased approaches are usually thwarted by computational challenges posed by\nlarge datasets. We construct a novel class of scalable Dynamic Nearest Neighbor\nGaussian Process (DNNGP) models that can provide a sparse approximation to any\nspatio-temporal GP (e.g., with non-separable covariance structures). The DNNGP\nwe develop here can be used as a sparsity-inducing prior for spatio-temporal\nrandom effects in any Bayesian hierarchical model to deliver full posterior\ninference. Storage and memory requirements for a DNNGP model are linear in the\nsize of the dataset thereby delivering massive scalability without sacrificing\ninferential richness. Extensive numerical studies reveal that the DNNGP\nprovides substantially superior approximations to the underlying process than\nlow rank approximations. Finally, we use the DNNGP to analyze a massive air\nquality dataset to substantially improve predictions of PM levels across Europe\nin conjunction with the LOTOS-EUROS chemistry transport models (CTMs).\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2015 10:52:07 GMT"}, {"version": "v2", "created": "Sun, 10 Apr 2016 17:06:31 GMT"}, {"version": "v3", "created": "Thu, 14 Apr 2016 03:42:20 GMT"}], "update_date": "2016-04-15", "authors_parsed": [["Datta", "Abhirup", ""], ["Banerjee", "Sudipto", ""], ["Finley", "Andrew O.", ""], ["Hamm", "Nicholas A. S.", ""], ["Schaap", "Martijn", ""]]}, {"id": "1510.07302", "submitter": "Elvan Ceyhan", "authors": "E. Ceyhan, T. Nishino, K.N. Botteron, M.I. Miller, J.T. Ratnanather", "title": "Analysis of Cortical Morphometric Variability Using Labeled Cortical\n  Distance Maps", "comments": "40 pages, 16 figures, 12 tables", "journal-ref": null, "doi": null, "report-no": "KU-EC-15-1", "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Morphometric differences in the anatomy of cortical structures are associated\nwith neuro-developmental and neuropsychiatric disorders. Such differences can\nbe quantized and detected by a powerful tool called Labeled Cortical Distance\nMap (LCDM). The LCDM method pro-vides distances of labeled gray matter (GM)\nvoxels from the GM/white matter (WM) surface for specific cortical structures\n(or tissues). Here we describe a method to analyze morphometric variability in\nthe particular tissue using LCDM distances. To extract more of the information\nprovided by LCDM distances, we perform pooling and censoring of LCDM distances.\nIn particular, we employ Brown-Forsythe (BF) test of homogeneity of variance\n(HOV) on the LCDM distances. HOV analysis of pooled distances provides an\noverall analysis of morphometric variability of the LCDMs due to the disease in\nquestion, while the HOV analysis of censored distances suggests the location(s)\nof significant variation in these differences (i.e., at which distance from the\nGM/WM surface the morphometric variability starts to be significant). We also\ncheck for the influence of assumption violations on the HOV analysis of LCDM\ndistances. In particular, we demonstrate that BF HOV test is robust to\nassumption violations such as the non-normality and within sample dependence of\nthe residuals from the median for pooled and censored distances and are robust\nto data aggregation which occurs in analysis of censored distances. We\nillustrate the methodology on a real data example, namely, LCDM distances of GM\nvoxels in ventral medial prefrontal cortices (VMPFCs) to see the effects of\ndepression or being of high risk to depression on the morphometry of VMPFCs.\nThe methodology used here is also valid for morphometric analysis of other\ncortical structures.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2015 20:59:58 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Ceyhan", "E.", ""], ["Nishino", "T.", ""], ["Botteron", "K. N.", ""], ["Miller", "M. I.", ""], ["Ratnanather", "J. T.", ""]]}, {"id": "1510.07376", "submitter": "Aristidis K. Nikoloulopoulos", "authors": "Aristidis K. Nikoloulopoulos", "title": "Weighted scores method for longitudinal ordinal data", "comments": "arXiv admin note: text overlap with arXiv:1510.06852", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extending generalized estimating equations (GEE) to ordinal response data\nrequires a conversion of the ordinal response to a vector of binary category\nindicators. That leads to a rather complicated association structure, and the\nintroduction of large matrices when the number of categories and dimension of\nthe cluster are large. To allow a richer specification of working correlation\nassumptions, we adopt the weighted scores method which is essentially an\nextension of the GEE approach, since it can also be applied to families that\nare not in the GLM class. The weighted scores method stems from the lack of a\ntheoretically sound methodology for analyzing multivariate discrete data based\nonly on moments up to second order and it is robust to dependence and nearly as\nefficient as maximum likelihood. There is no need to convert the ordinal\nresponse to binary indicators, thus the weight matrices have smaller dimensions\nand it is not necessary to guess the correlations of indicator variables for\ndifferent categories. We focus on important issues that would interest the data\nanalyst, such as choice of the structure of the correlation matrix and of\nexplanatory variables, comparison of results obtained from our methods versus\nGEE, and insights provided by our method that would be missed with the GEE\nmethod. Our modelling framework is implemented in the package weightedScores\nwithin the open source statistical environment R.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2015 06:30:39 GMT"}, {"version": "v2", "created": "Mon, 12 Dec 2016 09:51:07 GMT"}, {"version": "v3", "created": "Fri, 19 May 2017 18:12:24 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Nikoloulopoulos", "Aristidis K.", ""]]}, {"id": "1510.07714", "submitter": "Rebecca Steorts", "authors": "Peter Sadosky, Anshumali Shrivastava, Megan Price, and Rebecca C.\n  Steorts", "title": "Blocking Methods Applied to Casualty Records from the Syrian Conflict", "comments": "25 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of death counts and associated standard errors is of great\nimportance in armed conflict such as the ongoing violence in Syria, as well as\nhistorical conflicts in Guatemala, Per\\'u, Colombia, Timor Leste, and Kosovo.\nFor example, statistical estimates of death counts were cited as important\nevidence in the trial of General Efra\\'in R\\'ios Montt for acts of genocide in\nGuatemala. Estimation relies on both record linkage and multiple systems\nestimation. A key first step in this process is identifying ways to partition\nthe records such that they are computationally manageable. This step is\nreferred to as blocking and is a major challenge for the Syrian database since\nit is sparse in the number of duplicate records and feature poor in its\nattributes. As a consequence, we propose locality sensitive hashing (LSH)\nmethods to overcome these challenges. We demonstrate the computational\nsuperiority and error rates of these methods by comparing our proposed approach\nwith others in the literature. We conclude with a discussion of many challenges\nof merging LSH with record linkage to achieve an estimate of the number of\nuniquely documented deaths in the Syrian conflict.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2015 22:59:44 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Sadosky", "Peter", ""], ["Shrivastava", "Anshumali", ""], ["Price", "Megan", ""], ["Steorts", "Rebecca C.", ""]]}, {"id": "1510.07819", "submitter": "Zhihao Wu", "authors": "Zhihao Wu, Youfang Lin, Jing Wang and Steve Gregory", "title": "Link Prediction with Node Clustering Coefficient", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": "10.1016/j.physa.2016.01.038", "report-no": null, "categories": "cs.SI physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting missing links in incomplete complex networks efficiently and\naccurately is still a challenging problem. The recently proposed CAR\n(Cannistrai-Alanis-Ravai) index shows the power of local link/triangle\ninformation in improving link-prediction accuracy. With the information of\nlevel-2 links, which are links between common-neighbors, most classical\nsimilarity indices can be improved. Nevertheless, calculating the number of\nlevel-2 links makes CAR index not efficient enough. Inspired by the idea of\nemploying local link/triangle information, we propose a new similarity index\nwith more local structure information. In our method, local link/triangle\nstructure information can be conveyed by clustering coefficient of common\nneighbors directly. The reason why clustering coefficient has good\neffectiveness in estimating the contribution of a common-neighbor is because\nthat it employs links existing between neighbors of the common-neighbor and\nthese links have the same structural position with the candidate link to this\ncommon-neighbor. Ten real-world networks drawn from five various fields are\nused to test the performance of our method against to classical similarity\nindices and recently proposed CAR index. Two estimators: precision and AUP, are\nused to evaluate the accuracy of link prediction algorithms. Generally\nspeaking, our new index only performs competitively with CAR, but it is a good\ncomplement to CAR for networks with not very high LCP-corr, which is a measure\nto estimate the correlation between number of common-neighbors and number of\nlinks between common-neighbors. Besides, the proposed index is also more\nefficient than CAR index.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2015 09:24:39 GMT"}, {"version": "v2", "created": "Sat, 16 Jan 2016 09:37:23 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Wu", "Zhihao", ""], ["Lin", "Youfang", ""], ["Wang", "Jing", ""], ["Gregory", "Steve", ""]]}, {"id": "1510.08437", "submitter": "Omkar Muralidharan", "authors": "Omkar Muralidharan and Amir Najmi", "title": "Second Order Calibration: A Simple Way to Get Approximate Posteriors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many large-scale machine learning problems involve estimating an unknown\nparameter $\\theta_{i}$ for each of many items. For example, a key problem in\nsponsored search is to estimate the click through rate (CTR) of each of\nbillions of query-ad pairs. Most common methods, though, only give a point\nestimate of each $\\theta_{i}$. A posterior distribution for each $\\theta_{i}$\nis usually more useful but harder to get.\n  We present a simple post-processing technique that takes point estimates or\nscores $t_{i}$ (from any method) and estimates an approximate posterior for\neach $\\theta_{i}$. We build on the idea of calibration, a common\npost-processing technique that estimates\n$\\mathrm{E}\\left(\\theta_{i}\\!\\!\\bigm|\\!\\! t_{i}\\right)$. Our method, second\norder calibration, uses empirical Bayes methods to estimate the distribution of\n$\\theta_{i}\\!\\!\\bigm|\\!\\! t_{i}$ and uses the estimated distribution as an\napproximation to the posterior distribution of $\\theta_{i}$. We show that this\ncan yield improved point estimates and useful accuracy estimates. The method\nscales to large problems - our motivating example is a CTR estimation problem\ninvolving tens of billions of query-ad pairs.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2015 19:58:31 GMT"}], "update_date": "2015-11-27", "authors_parsed": [["Muralidharan", "Omkar", ""], ["Najmi", "Amir", ""]]}, {"id": "1510.08508", "submitter": "David Keator", "authors": "David B. Keator, Alexander Ihler", "title": "An Evaluation of Sparse Inverse Covariance Models for Group Functional\n  Connectivity in Molecular Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating the functional relationships between brain regions measured with\nneuroimaging provides insight into how the brain is sharing information at a\nmacro scale. Many functional connectivity methods have been developed for\ndynamic imaging modalities such as functional MRI (fMRI), while less work has\nfocused on models for static molecular imaging techniques such as FDG-PET and\nTc-99m HMPAO SPECT across groups of individuals. In this work we provide a\nquantitative assessment of how well three functional connec- tivity models\nbased on sparse inverse covariance estimation can accurately recover gold\nstandard connectivity patterns across multiple cohorts and data set sizes. We\ncompare the accuracies of learning regularized inverse covariance matrices\nacross cohorts independently with those learned using two different group-based\nregular- ization models. By using large cohorts of SPECT scans, we are able to\nprovide a quantitative assessment of the accuracy of the models in recovering\nthe gold standard functional conn\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2015 21:58:59 GMT"}], "update_date": "2015-10-30", "authors_parsed": [["Keator", "David B.", ""], ["Ihler", "Alexander", ""]]}, {"id": "1510.08802", "submitter": "Aaron Fisher", "authors": "Aaron J Fisher, R Yates Coley, Scott L Zeger", "title": "Fast Out-of-Sample Predictions for Bayesian Hierarchical Models of\n  Latent Health States", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical Bayesian models can be especially useful in precision medicine\nsettings, where clinicians are interested in estimating the patient-level\nlatent variables associated with an individual's current health state and its\ntrajectory. Such models are often fit using batch Markov Chain Monte Carlo\n(MCMC). However, the slow speed of batch MCMC computation makes it difficult to\nimplement in clinical settings, where immediate latent variable estimates are\noften desired in response to new patient data. In this report, we discuss how\nimportance sampling (IS) can instead be used to obtain fast, in-clinic\nestimates of patient-level latent variables. We apply IS to the hierarchical\nmodel proposed in Coley et al (2015) for predicting an individual's underlying\nprostate cancer state. We find that latent variable estimates via IS can\ntypically be obtained in 1-10 seconds per person and have high agreement with\nestimates coming from longer-running batch MCMC methods. Alternative options\nfor out-of-sample fitting and online updating are also discussed.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2015 18:01:52 GMT"}], "update_date": "2015-10-30", "authors_parsed": [["Fisher", "Aaron J", ""], ["Coley", "R Yates", ""], ["Zeger", "Scott L", ""]]}, {"id": "1510.09130", "submitter": "Mingjun Zhong", "authors": "Mingjun Zhong, Nigel Goddard, Charles Sutton", "title": "Latent Bayesian melding for integrating individual and population models", "comments": "11 pages, Advances in Neural Information Processing Systems (NIPS),\n  2015. (Spotlight Presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many statistical problems, a more coarse-grained model may be suitable for\npopulation-level behaviour, whereas a more detailed model is appropriate for\naccurate modelling of individual behaviour. This raises the question of how to\nintegrate both types of models. Methods such as posterior regularization follow\nthe idea of generalized moment matching, in that they allow matching\nexpectations between two models, but sometimes both models are most\nconveniently expressed as latent variable models. We propose latent Bayesian\nmelding, which is motivated by averaging the distributions over populations\nstatistics of both the individual-level and the population-level models under a\nlogarithmic opinion pool framework. In a case study on electricity\ndisaggregation, which is a type of single-channel blind source separation\nproblem, we show that latent Bayesian melding leads to significantly more\naccurate predictions than an approach based solely on generalized moment\nmatching.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2015 15:39:06 GMT"}], "update_date": "2015-11-02", "authors_parsed": [["Zhong", "Mingjun", ""], ["Goddard", "Nigel", ""], ["Sutton", "Charles", ""]]}]