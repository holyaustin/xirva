[{"id": "1401.0087", "submitter": "Iuliana Teodorescu", "authors": "Iuliana Teodorescu and Chris Tsokos", "title": "Contributors of carbon dioxide in the atmosphere in Europe: the surface\n  response analysis", "comments": "arXiv admin note: substantial text overlap with arXiv:1312.7827", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a continuation of the statistical modeling of the nonlinear\nrelationship between atmospheric CO2 and attributable variables that can\naccount for emissions, based on data from EU countries, in order to compare the\nrelevant findings to those obtained in the case of US data, in [1, 2]. The\ncurrent study was initiated in [3], leading to the optimal second-order model,\nbased on three linear terms and five second-order terms. We conclude this study\nin the present work, by finding the canonical decomposition of the nonlinear\nmodel, and by computing the specific two-dimensional confidence regions that it\nleads to. We then use the model in order to quantify the net effect of various\nrisk factors, and compare to the results obtained in the US case.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2013 04:10:40 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Teodorescu", "Iuliana", ""], ["Tsokos", "Chris", ""]]}, {"id": "1401.0132", "submitter": "Nil Kamal Hazra", "authors": "Nil Kamal Hazra, Asok K. Nanda", "title": "General Standby Component Allocation in Series and Parallel Systems", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimum lifetime of a series (resp. parallel) system with general standby\ncomponent(s) always depends on allocation strategy of standby component(s) into\nthe system. Here, we discuss three different models of one or more standby\ncompo- nents. In each model, we compare different series (resp. parallel)\nsystems (which are formed through different allocation strategies of standby\ncomponent(s)) with respect to the usual stochastic and the stochastic\nprecedence orders.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2013 11:32:34 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Hazra", "Nil Kamal", ""], ["Nanda", "Asok K.", ""]]}, {"id": "1401.0207", "submitter": "Galen Wilkerson", "authors": "Galen Wilkerson, Ramin Khalili, Stefan Schmid", "title": "Urban Mobility Scaling: Lessons from `Little Data'", "comments": "6 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.CY cs.SI physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent mobility scaling research, using new data sources, often relies on\naggregated data alone. Hence, these studies face difficulties characterizing\nthe influence of factors such as transportation mode on mobility patterns. This\npaper attempts to complement this research by looking at a category-rich\nmobility data set. In order to shed light on the impact of categories, as a\ncase study, we use conventionally collected German mobility data. In contrast\nto `check-in'-based data, our results are not biased by Euclidean distance\napproximations. In our analysis, we show that aggregation can hide crucial\ndifferences between trip length distributions, when subdivided by categories.\nFor example, we see that on an urban scale (0 to ~15 km), walking, versus\ndriving, exhibits a highly different scaling exponent, thus universality class.\nMoreover, mode share and trip length are responsive to day-of-week and\ntime-of-day. For example, in Germany, although driving is relatively less\nfrequent on Sundays than on Wednesdays, trips seem to be longer. In addition,\nour work may shed new light on the debate between distance-based and\nintervening-opportunity mechanisms affecting mobility patterns, since mode may\nbe chosen both according to trip length and urban form.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2013 19:13:48 GMT"}, {"version": "v2", "created": "Mon, 6 Jan 2014 10:02:25 GMT"}], "update_date": "2014-01-07", "authors_parsed": [["Wilkerson", "Galen", ""], ["Khalili", "Ramin", ""], ["Schmid", "Stefan", ""]]}, {"id": "1401.0211", "submitter": "Yang Feng", "authors": "Jianqing Fan, Yang Feng, Jiancheng Jiang and Xin Tong", "title": "Feature Augmentation via Nonparametrics and Selection (FANS) in High\n  Dimensional Classification", "comments": "30 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a high dimensional classification method that involves\nnonparametric feature augmentation. Knowing that marginal density ratios are\nthe most powerful univariate classifiers, we use the ratio estimates to\ntransform the original feature measurements. Subsequently, penalized logistic\nregression is invoked, taking as input the newly transformed or augmented\nfeatures. This procedure trains models equipped with local complexity and\nglobal simplicity, thereby avoiding the curse of dimensionality while creating\na flexible nonlinear decision boundary. The resulting method is called Feature\nAugmentation via Nonparametrics and Selection (FANS). We motivate FANS by\ngeneralizing the Naive Bayes model, writing the log ratio of joint densities as\na linear combination of those of marginal densities. It is related to\ngeneralized additive models, but has better interpretability and computability.\nRisk bounds are developed for FANS. In numerical analysis, FANS is compared\nwith competing methods, so as to provide a guideline on its best application\ndomain. Real data analysis demonstrates that FANS performs very competitively\non benchmark email spam and gene expression data sets. Moreover, FANS is\nimplemented by an extremely fast algorithm through parallel computing.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2013 19:53:11 GMT"}, {"version": "v2", "created": "Fri, 2 Jan 2015 17:27:38 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Fan", "Jianqing", ""], ["Feng", "Yang", ""], ["Jiang", "Jiancheng", ""], ["Tong", "Xin", ""]]}, {"id": "1401.0759", "submitter": "Nicholas J. Horton", "authors": "Nicholas J. Horton, Daniell Toth, Polly Phipps", "title": "Adjusting models of ordered multinomial outcomes for nonignorable\n  nonresponse in the occupational employment statistics survey", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS714 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 2, 956-973", "doi": "10.1214/14-AOAS714", "report-no": "IMS-AOAS-AOAS714", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An establishment's average wage, computed from administrative wage data, has\nbeen found to be related to occupational wages. These occupational wages are a\nprimary outcome variable for the Bureau of Labor Statistics Occupational\nEmployment Statistics survey. Motivated by the fact that nonresponse in this\nsurvey is associated with average wage even after accounting for other\nestablishment characteristics, we propose a method that uses the administrative\ndata for imputing missing occupational wage values due to nonresponse. This\nimputation is complicated by the structure of the data. Since occupational wage\ndata is collected in the form of counts of employees in predefined wage ranges\nfor each occupation, weighting approaches to deal with nonresponse do not\nadequately adjust the estimates for certain domains of estimation. To preserve\nthe current data structure, we propose a method to impute each missing\nestablishment's wage interval count data as an ordered multinomial random\nvariable using a separate survival model for each occupation. Each model\nincorporates known auxiliary information for each establishment associated with\nthe distribution of the occupational wage data, including geographic and\nindustry characteristics. This flexible model allows the baseline hazard to\nvary by occupation while allowing predictors to adjust the probabilities of an\nemployee's salary falling within the specified ranges. An empirical study and\nsimulation results suggest that the method imputes missing OES wages that are\nassociated with the average wage of the establishment in a way that more\nclosely resembles the observed association.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2014 00:36:07 GMT"}, {"version": "v2", "created": "Thu, 3 Jul 2014 22:13:01 GMT"}, {"version": "v3", "created": "Thu, 31 Jul 2014 13:00:10 GMT"}], "update_date": "2014-08-01", "authors_parsed": [["Horton", "Nicholas J.", ""], ["Toth", "Daniell", ""], ["Phipps", "Polly", ""]]}, {"id": "1401.0803", "submitter": "Jean-Luc Marichal", "authors": "Jean-Luc Marichal", "title": "Structure functions and minimal path sets", "comments": null, "journal-ref": "IEEE Transactions on Reliability 65 (2) (2016) 763-768", "doi": "10.1109/TR.2015.2513017", "report-no": null, "categories": "stat.AP math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this short note we give and discuss a general multilinear expression of\nthe structure function of an arbitrary semicoherent system in terms of its\nminimal path and cut sets. We also examine the link between the number of\nminimal path and cut sets consisting of 1 or 2 components and the concept of\nstructure signature of the system.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2014 12:06:57 GMT"}, {"version": "v2", "created": "Tue, 21 Jun 2016 09:27:07 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Marichal", "Jean-Luc", ""]]}, {"id": "1401.0871", "submitter": "Sakellarios Zairis", "authors": "Amy Rebecca Gansell, Jan-Willem van de Meent, Sakellarios Zairis,\n  Chris H. Wiggins", "title": "Stylistic Clusters and the Syrian/South Syrian Tradition of\n  First-Millennium BCE Levantine Ivory Carving: A Machine Learning Approach", "comments": "28 pages, 16 figures, accepted for publication in the Journal of\n  Archaeological Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thousands of first-millennium BCE ivory carvings have been excavated from\nNeo-Assyrian sites in Mesopotamia (primarily Nimrud, Khorsabad, and Arslan\nTash) hundreds of miles from their Levantine production contexts. At present,\ntheir specific manufacture dates and workshop localities are unknown. Relying\non subjective, visual methods, scholars have grappled with their classification\nand regional attribution for over a century. This study combines visual\napproaches with machine-learning techniques to offer data-driven perspectives\non the classification and attribution of this early Iron Age corpus. The study\nsample consisted of 162 sculptures of female figures. We have developed an\nalgorithm that clusters the ivories based on a combination of descriptive and\nanthropometric data. The resulting categories, which are based on purely\nstatistical criteria, show good agreement with conventional art historical\nclassifications, while revealing new perspectives, especially with regard to\nthe contested Syrian/South Syrian/Intermediate tradition. Specifically, we have\nidentified that objects of the Syrian/South Syrian/Intermediate tradition may\nbe more closely related to Phoenician objects than to North Syrian objects; we\noffer a reconsideration of a subset of Phoenician objects, and we confirm\nSyrian/South Syrian/Intermediate stylistic subgroups that might distinguish\nnetworks of acquisition among the sites of Nimrud, Khorsabad, Arslan Tash and\nthe Levant. We have also identified which features are most significant in our\ncluster assignments and might thereby be most diagnostic of regional carving\ntraditions. In short, our study both corroborates traditional visual\nclassification methods and demonstrates how machine-learning techniques may be\nemployed to reveal complementary information not accessible through the\nexclusively visual analysis of an archaeological corpus.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2014 08:15:54 GMT"}], "update_date": "2014-01-07", "authors_parsed": [["Gansell", "Amy Rebecca", ""], ["van de Meent", "Jan-Willem", ""], ["Zairis", "Sakellarios", ""], ["Wiggins", "Chris H.", ""]]}, {"id": "1401.0942", "submitter": "Andrew Miller", "authors": "Andrew Miller, Luke Bornn, Ryan Adams, Kirk Goldsberry", "title": "Factorized Point Process Intensities: A Spatial Analysis of Professional\n  Basketball", "comments": "13 pages, 6 figures, fixed formatting issues", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a machine learning approach to represent and analyze the\nunderlying spatial structure that governs shot selection among professional\nbasketball players in the NBA. Typically, NBA players are discussed and\ncompared in an heuristic, imprecise manner that relies on unmeasured intuitions\nabout player behavior. This makes it difficult to draw comparisons between\nplayers and make accurate player specific predictions. Modeling shot attempt\ndata as a point process, we create a low dimensional representation of\noffensive player types in the NBA. Using non-negative matrix factorization\n(NMF), an unsupervised dimensionality reduction technique, we show that a\nlow-rank spatial decomposition summarizes the shooting habits of NBA players.\nThe spatial representations discovered by the algorithm correspond to intuitive\ndescriptions of NBA player types, and can be used to model other spatial\neffects, such as shooting accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2014 21:31:03 GMT"}, {"version": "v2", "created": "Wed, 8 Jan 2014 01:41:03 GMT"}], "update_date": "2014-01-09", "authors_parsed": [["Miller", "Andrew", ""], ["Bornn", "Luke", ""], ["Adams", "Ryan", ""], ["Goldsberry", "Kirk", ""]]}, {"id": "1401.1052", "submitter": "Dalia Chakrabarty Dr.", "authors": "Dalia Chakrabarty and Prasenjit Saha", "title": "Inverse Bayesian Estimation of Gravitational Mass Density in Galaxies\n  from Missing Kinematic Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP astro-ph.GA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we focus on a type of inverse problem in which the data is\nexpressed as an unknown function of the sought and unknown model function (or\nits discretised representation as a model parameter vector). In particular, we\ndeal with situations in which training data is not available. Then we cannot\nmodel the unknown functional relationship between data and the unknown model\nfunction (or parameter vector) with a Gaussian Process of appropriate\ndimensionality. A Bayesian method based on state space modelling is advanced\ninstead. Within this framework, the likelihood is expressed in terms of the\nprobability density function ($pdf$) of the state space variable and the sought\nmodel parameter vector is embedded within the domain of this $pdf$. As the\nmeasurable vector lives only inside an identified sub-volume of the system\nstate space, the $pdf$ of the state space variable is projected onto the space\nof the measurables, and it is in terms of the projected state space density\nthat the likelihood is written; the final form of the likelihood is achieved\nafter convolution with the distribution of measurement errors. Application\nmotivated vague priors are invoked and the posterior probability density of the\nmodel parameter vectors, given the data is computed. Inference is performed by\ntaking posterior samples with adaptive MCMC. The method is illustrated on\nsynthetic as well as real galactic data.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2014 11:58:05 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Chakrabarty", "Dalia", ""], ["Saha", "Prasenjit", ""]]}, {"id": "1401.1165", "submitter": "Bruno Valente", "authors": "Bruno Dourado Valente, Gota Morota, Guilherme Jordao Magalhaes Rosa,\n  Daniel Gianola, Kent Weigel", "title": "The causal meaning of genomic predictors and how it affects the\n  construction and comparison of genome-enabled selection models", "comments": null, "journal-ref": null, "doi": "10.1534/genetics.114.169490", "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The additive genetic effect is arguably the most important quantity inferred\nin animal and plant breeding analyses. The term effect indicates that it\nrepresents causal information, which is different from standard statistical\nconcepts as regression coefficient and association. The process of inferring\ncausal information is also different from standard statistical learning, as the\nformer requires causal (i.e. non-statistical) assumptions and involves extra\ncomplexities. Remarkably, the task of inferring genetic effects is largely seen\nas a standard regression/prediction problem, contradicting its label. This\nwidely accepted analysis approach is by itself insufficient for causal\nlearning, suggesting that causality is not the point for selection. Given this\nincongruence, it is important to verify if genomic predictors need to represent\ncausal effects to be relevant for selection decisions, especially because\napplying regression studies to answer causal questions may lead to wrong\nconclusions. The answer to this question defines if genomic selection models\nshould be constructed aiming maximum genomic predictive ability or aiming\nidentifiability of genetic causal effects. Here, we demonstrate that selection\nrelies on a causal effect from genotype to phenotype, and that genomic\npredictors are only useful for selection if they distinguish such effect from\nother sources of association. Conversely, genomic predictors capturing\nnon-causal signals provide information that is less relevant for selection\nregardless of the resulting predictive ability. Focusing on covariate choice\ndecision, simulated examples are used to show that predictive ability, which is\nthe criterion normally used to compare models, may not indicate the quality of\ngenomic predictors for selection. Additionally, we propose using alternative\ncriteria to construct models aiming for the identification of the genetic\ncausal effects.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2013 21:45:05 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Valente", "Bruno Dourado", ""], ["Morota", "Gota", ""], ["Rosa", "Guilherme Jordao Magalhaes", ""], ["Gianola", "Daniel", ""], ["Weigel", "Kent", ""]]}, {"id": "1401.1264", "submitter": "Peng Ding", "authors": "Peng Ding, and Zhi Geng", "title": "Identifiability of Subgroup Causal Effects in Randomized Experiments\n  with Nonignorable Missing Covariates", "comments": "Statistics in Medicine (2014)", "journal-ref": null, "doi": "10.1002/sim.6014", "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although randomized experiments are widely regarded as the gold standard for\nestimating causal effects, missing data of the pretreatment covariates makes it\nchallenging to estimate the subgroup causal effects. When the missing data\nmechanism of the covariates is nonignorable, the parameters of interest are\ngenerally not pointly identifiable, and we can only get bounds for the\nparameters of interest, which may be too wide for practical use. In some real\ncases, we have prior knowledge that some restrictions may be plausible. We show\nthe identifiability of the causal effects and joint distributions for four\ninterpretable missing data mechanisms, and evaluate the performance of the\nstatistical inference via simulation studies. One application of our methods to\na real data set from a randomized clinical trial shows that one of the\nnonignorable missing data mechanisms fits better than the ignorable missing\ndata mechanism, and the results conform to the study's original expert\nopinions. We also illustrate the potential applications of our methods to\nobservational studies using a data set from a job-training program.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2014 04:18:22 GMT"}], "update_date": "2014-01-08", "authors_parsed": [["Ding", "Peng", ""], ["Geng", "Zhi", ""]]}, {"id": "1401.1267", "submitter": "Peng Ding", "authors": "Peng Ding", "title": "Three Occurrences of the Hyperbolic-Secant Distribution", "comments": "The American Statistician (2014)", "journal-ref": null, "doi": "10.1080/00031305.2013.867902", "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although it is the generator distribution of the sixth natural exponential\nfamily with quadratic variance function, the Hyperbolic-Secant distribution is\nmuch less known than other distributions in the exponential families. Its lack\nof familiarity is due to its isolation from many widely-used statistical\nmodels. We fill in the gap by showing three examples naturally generating the\nHyperbolic-Secant distribution, including Fisher's analysis of similarity\nbetween twins, the Jeffreys' prior for contingency tables, and invalid\ninstrumental variables.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2014 04:24:29 GMT"}], "update_date": "2014-01-08", "authors_parsed": [["Ding", "Peng", ""]]}, {"id": "1401.1269", "submitter": "Peng Ding", "authors": "Peng Ding", "title": "Bayesian Robust Inference of Sample Selection Using Selection-t Models", "comments": "Journal of Multivariate Analysis (2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heckman selection model is the most popular econometric model in analysis of\ndata with sample selection. However, selection models with Normal errors cannot\naccommodate heavy tails in the error distribution. Recently, Marchenko and\nGenton proposed a selection-t model to perform frequentist' robust analysis of\nsample selection. Instead of using their maximum likelihood estimates, our\npaper develops new Bayesian procedures for the selection-t models with either\ncontinuous or binary outcomes. By exploiting the Normal mixture representation\nof the t distribution, we can use data augmentation to impute the missing data,\nand use parameter expansion to sample the restricted covariance matrices. The\nBayesian procedures only involve simple steps, without calculating analytical\nor numerical derivatives of the complicated log likelihood functions.\nSimulation studies show the vulnerability of the selection models with Normal\nerrors, as well as the robustness of the selection models with t errors.\nInterestingly, we find evidence of heavy-tailedness in three real examples\nanalyzed by previous studies, and the conclusions about the existence of\nselection effect are very sensitive to the distributional assumptions of the\nerror terms.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2014 04:32:47 GMT"}], "update_date": "2014-01-08", "authors_parsed": [["Ding", "Peng", ""]]}, {"id": "1401.1301", "submitter": "Laura Anderlucci", "authors": "Laura Anderlucci, Cinzia Viroli", "title": "Covariance pattern mixture models for the analysis of multivariate\n  heterogeneous longitudinal data", "comments": "Published at http://dx.doi.org/10.1214/15-AOAS816 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 2, 777-800", "doi": "10.1214/15-AOAS816", "report-no": "IMS-AOAS-AOAS816", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for modeling multivariate longitudinal data in\nthe presence of unobserved heterogeneity for the analysis of the Health and\nRetirement Study (HRS) data. Our proposal can be cast within the framework of\nlinear mixed models with discrete individual random intercepts; however,\ndifferently from the standard formulation, the proposed Covariance Pattern\nMixture Model (CPMM) does not require the usual local independence assumption.\nThe model is thus able to simultaneously model the heterogeneity, the\nassociation among the responses and the temporal dependence structure. We focus\non the investigation of temporal patterns related to the cognitive functioning\nin retired American respondents. In particular, we aim to understand whether it\ncan be affected by some individual socio-economical characteristics and whether\nit is possible to identify some homogenous groups of respondents that share a\nsimilar cognitive profile. An accurate description of the detected groups\nallows government policy interventions to be opportunely addressed. Results\nidentify three homogenous clusters of individuals with specific cognitive\nfunctioning, consistent with the class conditional distribution of the\ncovariates. The flexibility of CPMM allows for a different contribution of each\nregressor on the responses according to group membership. In so doing, the\nidentified groups receive a global and accurate phenomenological\ncharacterization.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2014 08:01:19 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2015 12:51:50 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Anderlucci", "Laura", ""], ["Viroli", "Cinzia", ""]]}, {"id": "1401.1489", "submitter": "Romain H\\'erault", "authors": "John Komar and Romain H\\'erault and Ludovic Seifert", "title": "Key point selection and clustering of swimmer coordination through\n  Sparse Fisher-EM", "comments": "Presented at ECML/PKDD 2013 Workshop on Machine Learning and Data\n  Mining for Sports Analytics (MLSA2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To answer the existence of optimal swimmer learning/teaching strategies, this\nwork introduces a two-level clustering in order to analyze temporal dynamics of\nmotor learning in breaststroke swimming. Each level have been performed through\nSparse Fisher-EM, a unsupervised framework which can be applied efficiently on\nlarge and correlated datasets. The induced sparsity selects key points of the\ncoordination phase without any prior knowledge.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2014 20:16:05 GMT"}], "update_date": "2014-01-08", "authors_parsed": [["Komar", "John", ""], ["H\u00e9rault", "Romain", ""], ["Seifert", "Ludovic", ""]]}, {"id": "1401.1490", "submitter": "Bahman Afsari", "authors": "Bahman Afsari, Ulisses M. Braga-Neto, Donald Geman", "title": "Rank discriminants for predicting phenotypes from RNA expression", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS738 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 3, 1469-1491", "doi": "10.1214/14-AOAS738", "report-no": "IMS-AOAS-AOAS738", "categories": "q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical methods for analyzing large-scale biomolecular data are\ncommonplace in computational biology. A notable example is phenotype prediction\nfrom gene expression data, for instance, detecting human cancers,\ndifferentiating subtypes and predicting clinical outcomes. Still, clinical\napplications remain scarce. One reason is that the complexity of the decision\nrules that emerge from standard statistical learning impedes biological\nunderstanding, in particular, any mechanistic interpretation. Here we explore\ndecision rules for binary classification utilizing only the ordering of\nexpression among several genes; the basic building blocks are then two-gene\nexpression comparisons. The simplest example, just one comparison, is the TSP\nclassifier, which has appeared in a variety of cancer-related discovery\nstudies. Decision rules based on multiple comparisons can better accommodate\nclass heterogeneity, and thereby increase accuracy, and might provide a link\nwith biological mechanism. We consider a general framework (\"rank-in-context\")\nfor designing discriminant functions, including a data-driven selection of the\nnumber and identity of the genes in the support (\"context\"). We then specialize\nto two examples: voting among several pairs and comparing the median expression\nin two groups of genes. Comprehensive experiments assess accuracy relative to\nother, more complex, methods, and reinforce earlier observations that simple\nclassifiers are competitive.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2014 20:26:59 GMT"}, {"version": "v2", "created": "Mon, 7 Apr 2014 22:58:29 GMT"}, {"version": "v3", "created": "Fri, 21 Nov 2014 08:18:00 GMT"}], "update_date": "2014-11-24", "authors_parsed": [["Afsari", "Bahman", ""], ["Braga-Neto", "Ulisses M.", ""], ["Geman", "Donald", ""]]}, {"id": "1401.1608", "submitter": "Mark A. Newell", "authors": "Mark A. Newell, Dianne Cook, Heike Hofmann, Jean-Luc Jannink", "title": "An algorithm for deciding the number of clusters and validation using\n  simulated data with application to exploring crop population structure", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS671 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 4, 1898-1916", "doi": "10.1214/13-AOAS671", "report-no": "IMS-AOAS-AOAS671", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A first step in exploring population structure in crop plants and other\norganisms is to define the number of subpopulations that exist for a given data\nset. The genetic marker data sets being generated have become increasingly\nlarge over time and commonly are of the high-dimension, low sample size (HDLSS)\nsituation. An algorithm for deciding the number of clusters is proposed, and is\nvalidated on simulated data sets varying in both the level of structure and the\nnumber of clusters covering the range of variation observed empirically. The\nalgorithm was then tested on six empirical data sets across three small grain\nspecies. The algorithm uses bootstrapping, three methods of clustering, and\ndefines the optimum number of clusters based on a common criterion, the\nHubert's gamma statistic. Validation on simulated sets coupled with testing on\nempirical sets suggests that the algorithm can be used for a wide variety of\ngenetic data sets.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2014 08:50:01 GMT"}], "update_date": "2014-01-09", "authors_parsed": [["Newell", "Mark A.", ""], ["Cook", "Dianne", ""], ["Hofmann", "Heike", ""], ["Jannink", "Jean-Luc", ""]]}, {"id": "1401.1631", "submitter": "Ming-Hung Kao", "authors": "Ming-Hung Kao, Dibyen Majumdar, Abhyuday Mandal, John Stufken", "title": "Maximin and maximin-efficient event-related fMRI designs under a\n  nonlinear model", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS658 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 4, 1940-1959", "doi": "10.1214/13-AOAS658", "report-no": "IMS-AOAS-AOAS658", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous studies on event-related functional magnetic resonance imaging\nexperimental designs are primarily based on linear models, in which a known\nshape of the hemodynamic response function (HRF) is assumed. However, the HRF\nshape is usually uncertain at the design stage. To address this issue, we\nconsider a nonlinear model to accommodate a wide spectrum of feasible HRF\nshapes, and propose efficient approaches for obtaining maximin and\nmaximin-efficient designs. Our approaches involve a reduction in the parameter\nspace and a search algorithm that helps to efficiently search over a restricted\nclass of designs for good designs. The obtained designs are compared with\ntraditional designs widely used in practice. We also demonstrate the usefulness\nof our approaches via a motivating example.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2014 09:35:31 GMT"}], "update_date": "2014-01-09", "authors_parsed": [["Kao", "Ming-Hung", ""], ["Majumdar", "Dibyen", ""], ["Mandal", "Abhyuday", ""], ["Stufken", "John", ""]]}, {"id": "1401.1640", "submitter": "B\\\"{a}rbel Finkenst\\\"{a}dt", "authors": "B\\\"arbel Finkenst\\\"adt, Dan J. Woodcock, Michal Komorowski, Claire V.\n  Harper, Julian R. E. Davis, Mike R. H. White, David A. Rand", "title": "Quantifying intrinsic and extrinsic noise in gene transcription using\n  the linear noise approximation: An application to single cell data", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS669 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 4, 1960-1982", "doi": "10.1214/13-AOAS669", "report-no": "IMS-AOAS-AOAS669", "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central challenge in computational modeling of dynamic biological systems\nis parameter inference from experimental time course measurements. However, one\nwould not only like to infer kinetic parameters but also study their\nvariability from cell to cell. Here we focus on the case where single-cell\nfluorescent protein imaging time series data are available for a population of\ncells. Based on van Kampen's linear noise approximation, we derive a dynamic\nstate space model for molecular populations which is then extended to a\nhierarchical model. This model has potential to address the sources of\nvariability relevant to single-cell data, namely, intrinsic noise due to the\nstochastic nature of the birth and death processes involved in reactions and\nextrinsic noise arising from the cell-to-cell variation of kinetic parameters.\nIn order to infer such a model from experimental data, one must also quantify\nthe measurement process where one has to allow for nonmeasurable molecular\nspecies as well as measurement noise of unknown level and variance. The\navailability of multiple single-cell time series data here provides a unique\ntestbed to fit such a model and quantify these different sources of variation\nfrom experimental data.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2014 10:06:25 GMT"}], "update_date": "2014-01-09", "authors_parsed": [["Finkenst\u00e4dt", "B\u00e4rbel", ""], ["Woodcock", "Dan J.", ""], ["Komorowski", "Michal", ""], ["Harper", "Claire V.", ""], ["Davis", "Julian R. E.", ""], ["White", "Mike R. H.", ""], ["Rand", "David A.", ""]]}, {"id": "1401.1663", "submitter": "Jeroen Pannekoek", "authors": "Jeroen Pannekoek, Natalie Shlomo, Ton De Waal", "title": "Calibrated imputation of numerical data under linear edit restrictions", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS664 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 4, 1983-2006", "doi": "10.1214/13-AOAS664", "report-no": "IMS-AOAS-AOAS664", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common problem faced by statistical institutes is that data may be missing\nfrom collected data sets. The typical way to overcome this problem is to impute\nthe missing data. The problem of imputing missing data is complicated by the\nfact that statistical data often have to satisfy certain edit rules and that\nvalues of variables across units sometimes have to sum up to known totals. For\nnumerical data, edit rules are most often formulated as linear restrictions on\nthe variables. For example, for data on enterprises edit rules could be that\nthe profit and costs of an enterprise should sum up to its turnover and that\nthe turnover should be at least zero. The totals of some variables across units\nmay already be known from administrative data (e.g., turnover from a tax\nregister) or estimated from other sources. Standard imputation methods for\nnumerical data as described in the literature generally do not take such edit\nrules and totals into account. In this article we describe algorithms for\nimputing missing numerical data that take edit restrictions into account and\nensure that sums are calibrated to known totals. These algorithms are based on\na sequential regression approach that uses regression predictions to impute the\nvariables one by one. To assess the performance of the imputation methods, a\nsimulation study is carried out as well as an evaluation study based on a real\ndata set.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2014 11:15:40 GMT"}], "update_date": "2014-01-09", "authors_parsed": [["Pannekoek", "Jeroen", ""], ["Shlomo", "Natalie", ""], ["De Waal", "Ton", ""]]}, {"id": "1401.1683", "submitter": "Elizabeth A. Handorf", "authors": "Elizabeth A. Handorf, Justin E. Bekelman, Daniel F. Heitjan, Nandita\n  Mitra", "title": "Evaluating costs with unmeasured confounding: A sensitivity analysis for\n  the treatment effect", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS665 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 4, 2062-2080", "doi": "10.1214/13-AOAS665", "report-no": "IMS-AOAS-AOAS665", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimates of the effects of treatment on cost from observational studies are\nsubject to bias if there are unmeasured confounders. It is therefore advisable\nin practice to assess the potential magnitude of such biases. We derive a\ngeneral adjustment formula for loglinear models of mean cost and explore\nspecial cases under plausible assumptions about the distribution of the\nunmeasured confounder. We assess the performance of the adjustment by\nsimulation, in particular, examining robustness to a key assumption of\nconditional independence between the unmeasured and measured covariates given\nthe treatment indicator. We apply our method to SEER-Medicare cost data for a\nstage II/III muscle-invasive bladder cancer cohort. We evaluate the costs for\nradical cystectomy vs. combined radiation/chemotherapy, and find that the\nsignificance of the treatment effect is sensitive to plausible unmeasured\nBernoulli, Poisson and Gamma confounders.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2014 12:49:13 GMT"}], "update_date": "2014-01-09", "authors_parsed": [["Handorf", "Elizabeth A.", ""], ["Bekelman", "Justin E.", ""], ["Heitjan", "Daniel F.", ""], ["Mitra", "Nandita", ""]]}, {"id": "1401.1689", "submitter": "Andrey Feuerverger", "authors": "Andrey Feuerverger", "title": "The tomb next door: An update to ``Statistical analysis of an\n  archeological find''", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS677 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 4, 2081-2105", "doi": "10.1214/13-AOAS677", "report-no": "IMS-AOAS-AOAS677", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In June of 2010 access via robotic means was obtained to a tomb adjacent to\nthe one studied in Feuerverger [Ann. Appl. Stat. 2 (2008) 3-54]. In this\nupdate, we lay out and attempt to interpret the remarkable findings from this\nsecond tomb and comment on the statistical and scientific significance of these\nnew data and of their possible inferential connections to the data from the\nfirst tomb. Readers are then invited to formulate their own conclusions.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2014 13:21:27 GMT"}], "update_date": "2014-01-09", "authors_parsed": [["Feuerverger", "Andrey", ""]]}, {"id": "1401.1701", "submitter": "Alisa J. Stephens", "authors": "Alisa J. Stephens, Eric J. Tchetgen Tchetgen, Victor De Gruttola", "title": "Flexible covariate-adjusted exact tests of randomized treatment effects\n  with application to a trial of HIV education", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS679 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 4, 2106-2137", "doi": "10.1214/13-AOAS679", "report-no": "IMS-AOAS-AOAS679", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The primary goal of randomized trials is to compare the effects of different\ninterventions on some outcome of interest. In addition to the treatment\nassignment and outcome, data on baseline covariates, such as demographic\ncharacteristics or biomarker measurements, are typically collected.\nIncorporating such auxiliary covariates in the analysis of randomized trials\ncan increase power, but questions remain about how to preserve type I error\nwhen incorporating such covariates in a flexible way, particularly when the\nnumber of randomized units is small. Using the Young Citizens study, a\ncluster-randomized trial of an educational intervention to promote HIV\nawareness, we compare several methods to evaluate intervention effects when\nbaseline covariates are incorporated adaptively. To ascertain the validity of\nthe methods shown in small samples, extensive simulation studies were\nconducted. We demonstrate that randomization inference preserves type I error\nunder model selection while tests based on asymptotic theory may yield invalid\nresults. We also demonstrate that covariate adjustment generally increases\npower, except at extremely small sample sizes using liberal selection\nprocedures. Although shown within the context of HIV prevention research, our\nconclusions have important implications for maximizing efficiency and\nrobustness in randomized trials with small samples across disciplines.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2014 13:55:15 GMT"}], "update_date": "2014-01-09", "authors_parsed": [["Stephens", "Alisa J.", ""], ["Tchetgen", "Eric J. Tchetgen", ""], ["De Gruttola", "Victor", ""]]}, {"id": "1401.1706", "submitter": "Suyu Liu", "authors": "Suyu Liu, Guosheng Yin, Ying Yuan", "title": "Bayesian data augmentation dose finding with continual reassessment\n  method and delayed toxicity", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS661 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 4, 2138-2156", "doi": "10.1214/13-AOAS661", "report-no": "IMS-AOAS-AOAS661", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major practical impediment when implementing adaptive dose-finding designs\nis that the toxicity outcome used by the decision rules may not be observed\nshortly after the initiation of the treatment. To address this issue, we\npropose the data augmentation continual reassessment method (DA-CRM) for dose\nfinding. By naturally treating the unobserved toxicities as missing data, we\nshow that such missing data are nonignorable in the sense that the missingness\ndepends on the unobserved outcomes. The Bayesian data augmentation approach is\nused to sample both the missing data and model parameters from their posterior\nfull conditional distributions. We evaluate the performance of the DA-CRM\nthrough extensive simulation studies and also compare it with other existing\nmethods. The results show that the proposed design satisfactorily resolves the\nissues related to late-onset toxicities and possesses desirable operating\ncharacteristics: treating patients more safely and also selecting the maximum\ntolerated dose with a higher probability. The new DA-CRM is illustrated with\ntwo phase I cancer clinical trials.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2014 14:09:10 GMT"}], "update_date": "2014-01-09", "authors_parsed": [["Liu", "Suyu", ""], ["Yin", "Guosheng", ""], ["Yuan", "Ying", ""]]}, {"id": "1401.1749", "submitter": "Colin Worby", "authors": "Colin J. Worby, Philip D. O'Neill, Theodore Kypraios, Julie V.\n  Robotham, Daniela De Angelis, Edward J. P. Cartwright, Sharon J. Peacock, Ben\n  S. Cooper", "title": "Reconstructing transmission trees for communicable diseases using\n  densely sampled genetic data", "comments": null, "journal-ref": "Ann. Appl. Stat. 10 (2016) 395-417", "doi": "10.1214/15-AOAS898", "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whole genome sequencing of pathogens from multiple hosts in an epidemic\noffers the potential to investigate who infected whom with unparalleled\nresolution, potentially yielding important insights into disease dynamics and\nthe impact of control measures. We considered disease outbreaks in a setting\nwith dense genomic sampling, and formulated stochastic epidemic models to\ninvestigate person-to-person transmission, based on observed genomic and\nepidemiological data. We constructed models in which the genetic distance\nbetween sampled genotypes depends on the epidemiological relationship between\nthe hosts. A data augmented Markov chain Monte Carlo algorithm was used to\nsample over the transmission trees, providing a posterior probability for any\ngiven transmission route. We investigated the predictive performance of our\nmethodology using simulated data, demonstrating high sensitivity and\nspecificity, particularly for rapidly mutating pathogens with low\ntransmissibility. We then analyzed data collected during an outbreak of\nmethicillin-resistant Staphylococcus aureus in a hospital, identifying probable\ntransmission routes and estimating epidemiological parameters. Our approach\novercomes limitations of previous methods, providing a framework with the\nflexibility to allow for unobserved infection times, multiple independent\nintroductions of the pathogen, and within-host genetic diversity, as well as\nallowing forward simulation.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2014 17:01:50 GMT"}, {"version": "v2", "created": "Sun, 6 Dec 2015 21:40:25 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Worby", "Colin J.", ""], ["O'Neill", "Philip D.", ""], ["Kypraios", "Theodore", ""], ["Robotham", "Julie V.", ""], ["De Angelis", "Daniela", ""], ["Cartwright", "Edward J. P.", ""], ["Peacock", "Sharon J.", ""], ["Cooper", "Ben S.", ""]]}, {"id": "1401.1867", "submitter": "Jessi Cisewski", "authors": "Jessi Cisewski, Rupert A. C. Croft, Peter E. Freeman, Christopher R.\n  Genovese, Nishikanta Khandai, Melih Ozbek, Larry Wasserman", "title": "Nonparametric 3D map of the IGM using the Lyman-alpha forest", "comments": null, "journal-ref": null, "doi": "10.1093/mnras/stu475", "report-no": null, "categories": "astro-ph.IM astro-ph.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualizing the high-redshift Universe is difficult due to the dearth of\navailable data; however, the Lyman-alpha forest provides a means to map the\nintergalactic medium at redshifts not accessible to large galaxy surveys.\nLarge-scale structure surveys, such as the Baryon Oscillation Spectroscopic\nSurvey (BOSS), have collected quasar (QSO) spectra that enable the\nreconstruction of HI density fluctuations. The data fall on a collection of\nlines defined by the lines-of-sight (LOS) of the QSO, and a major issue with\nproducing a 3D reconstruction is determining how to model the regions between\nthe LOS. We present a method that produces a 3D map of this relatively\nuncharted portion of the Universe by employing local polynomial smoothing, a\nnonparametric methodology. The performance of the method is analyzed on\nsimulated data that mimics the varying number of LOS expected in real data, and\nthen is applied to a sample region selected from BOSS. Evaluation of the\nreconstruction is assessed by considering various features of the predicted 3D\nmaps including visual comparison of slices, PDFs, counts of local minima and\nmaxima, and standardized correlation functions. This 3D reconstruction allows\nfor an initial investigation of the topology of this portion of the Universe\nusing persistent homology.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2014 00:44:32 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Cisewski", "Jessi", ""], ["Croft", "Rupert A. C.", ""], ["Freeman", "Peter E.", ""], ["Genovese", "Christopher R.", ""], ["Khandai", "Nishikanta", ""], ["Ozbek", "Melih", ""], ["Wasserman", "Larry", ""]]}, {"id": "1401.1915", "submitter": "Xun Jiang", "authors": "Xun Jiang, Dipak K. Dey, Rachel Prunier, Adam M. Wilson, Kent E.\n  Holsinger", "title": "A new class of flexible link functions with application to species\n  co-occurrence in cape floristic region", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS663 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 4, 2180-2204", "doi": "10.1214/13-AOAS663", "report-no": "IMS-AOAS-AOAS663", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the mechanisms that allow biological species to co-occur is of\ngreat interest to ecologists. Here we investigate the factors that influence\nco-occurrence of members of the genus Protea in the Cape Floristic Region of\nsouthwestern Africa, a global hot spot of biodiversity. Due to the binomial\nnature of our response, a critical issue is to choose appropriate link\nfunctions for the regression model. In this paper we propose a new family of\nflexible link functions for modeling binomial response data. By introducing a\npower parameter into the cumulative distribution function (c.d.f.)\ncorresponding to a symmetric link function and its mirror reflection, greater\nflexibility in skewness can be achieved in both positive and negative\ndirections. Through simulated data sets and analysis of the Protea\nco-occurrence data, we show that the proposed link function is quite flexible\nand performs better against link misspecification than standard link functions.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2014 07:54:05 GMT"}], "update_date": "2014-01-10", "authors_parsed": [["Jiang", "Xun", ""], ["Dey", "Dipak K.", ""], ["Prunier", "Rachel", ""], ["Wilson", "Adam M.", ""], ["Holsinger", "Kent E.", ""]]}, {"id": "1401.2126", "submitter": "Susan M. Paddock", "authors": "Susan M. Paddock", "title": "Editorial", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS689 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 4, 1837-1837", "doi": "10.1214/13-AOAS689", "report-no": "IMS-AOAS-AOAS689", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clauset and Woodard (2013) ask, \"What is the likelihood of another September\n11th-sized or larger terrorist event, worldwide, over the next decade?\" This\nquestion has implications for numerous policy arenas - national security,\ninternational relations, public safety, disaster preparedness, and so on - but\nit also has resonance for any individual who remembers 9/11. Thus, the Area\nEditors chose this paper to engage the audience at the 2013 Joint Statistical\nMeetings (JSM) in Montreal. The discussions in this issue of The Annals of\nApplied Statistics include those presented formally at JSM 2013 as well as\nothers from individuals who were unable to attend JSM 2013 or who contributed\ndiscussions after the meeting.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2014 08:02:42 GMT"}], "update_date": "2014-01-10", "authors_parsed": [["Paddock", "Susan M.", ""]]}, {"id": "1401.2254", "submitter": "Lutz Bornmann Dr.", "authors": "Richard Williams and Lutz Bornmann", "title": "Sampling Issues in Bibliometric Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.DL physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bibliometricians face several issues when drawing and analyzing samples of\ncitation records for their research. Drawing samples that are too small may\nmake it difficult or impossible for studies to achieve their goals, while\ndrawing samples that are too large may drain resources that could be better\nused for other purposes. This paper considers three common situations and\noffers advice for dealing with each. First, an entire population of records is\navailable for an institution. We argue that, even though all records have been\ncollected, the use of inferential statistics, significance testing, and\nconfidence intervals is both common and desirable. Second, because of limited\nresources or other factors, a sample of records needs to be drawn. We\ndemonstrate how power analyses can be used to determine in advance how large\nthe sample needs to be to achieve the study's goals. Third, the sample size may\nalready be determined, either because the data have already been collected or\nbecause resources are limited. We show how power analyses can again be used to\ndetermine how large effects need to be in order to find effects that are\nstatistically significant. Such information can then help bibliometricians to\ndevelop reasonable expectations as to what their analysis can accomplish. While\nwe focus on issues of interest to bibliometricians, our recommendations and\nprocedures can easily be adapted for other fields of study.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2014 08:37:20 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2015 07:51:27 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Williams", "Richard", ""], ["Bornmann", "Lutz", ""]]}, {"id": "1401.2278", "submitter": "Zhigen Zhao", "authors": "Zhigen Zhao, Wei Wang, Zhi Wei", "title": "An empirical Bayes testing procedure for detecting variants in analysis\n  of next generation sequencing data", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS660 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 4, 2229-2248", "doi": "10.1214/13-AOAS660", "report-no": "IMS-AOAS-AOAS660", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because of the decreasing cost and high digital resolution, next-generation\nsequencing (NGS) is expected to replace the traditional hybridization-based\nmicroarray technology. For genetics study, the first-step analysis of NGS data\nis often to identify genomic variants among sequenced samples. Several\nstatistical models and tests have been developed for variant calling in NGS\nstudy. The existing approaches, however, are based on either conventional\nBayesian or frequentist methods, which are unable to address the multiplicity\nand testing efficiency issues simultaneously. In this paper, we derive an\noptimal empirical Bayes testing procedure to detect variants for NGS study. We\nutilize the empirical Bayes technique to exploit the across-site information\namong many testing sites in NGS data. We prove that our testing procedure is\nvalid and optimal in the sense of rejecting the maximum number of nonnulls\nwhile the Bayesian false discovery rate is controlled at a given nominal level.\nWe show by both simulation studies and real data analysis that our testing\nefficiency can be greatly enhanced over the existing frequentist approaches\nthat fail to pool and utilize information across the multiple testing sites.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2014 10:28:56 GMT"}], "update_date": "2014-01-13", "authors_parsed": [["Zhao", "Zhigen", ""], ["Wang", "Wei", ""], ["Wei", "Zhi", ""]]}, {"id": "1401.2282", "submitter": "Zhi-Sheng Ye", "authors": "Zhi-Sheng Ye, Yili Hong, Yimeng Xie", "title": "How do heterogeneities in operating environments affect field failure\n  predictions and test planning?", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS666 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 4, 2249-2271", "doi": "10.1214/13-AOAS666", "report-no": "IMS-AOAS-AOAS666", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main objective of accelerated life tests (ALTs) is to predict fraction\nfailings of products in the field. However, there are often discrepancies\nbetween the predicted fraction failing from the lab testing data and that from\nthe field failure data, due to the yet unobserved heterogeneities in usage and\noperating conditions. Most previous research on ALT planning and data analysis\nignores the discrepancies, resulting in inferior test plans and biased\npredictions. In this paper we model the heterogeneous environments together\nwith their effects on the product failures as a frailty term to link the lab\nfailure time distribution and field failure time distribution of a product. We\nshow that in the presence of the heterogeneous operating conditions, the hazard\nrate function of the field failure time distribution exhibits a range of\nshapes. Statistical inference procedure for the frailty models is developed\nwhen both the ALT data and the field failure data are available. Based on the\nfrailty models, optimal ALT plans aimed at predicting the field failure time\ndistribution are obtained. The developed methods are demonstrated through a\nreal life example.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2014 10:55:39 GMT"}], "update_date": "2014-01-13", "authors_parsed": [["Ye", "Zhi-Sheng", ""], ["Hong", "Yili", ""], ["Xie", "Yimeng", ""]]}, {"id": "1401.2293", "submitter": "George Mohler", "authors": "George Mohler", "title": "Discussion of \"Estimating the historical and future probabilities of\n  large terrorist events\" by Aaron Clauset and Ryan Woodard", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS614A the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 4, 1866-1870", "doi": "10.1214/13-AOAS614A", "report-no": "IMS-AOAS-AOAS614A", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Estimating the historical and future probabilities of large\nterrorist events\" by Aaron Clauset and Ryan Woodard [arXiv:1209.0089].\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2014 11:49:13 GMT"}], "update_date": "2014-01-13", "authors_parsed": [["Mohler", "George", ""]]}, {"id": "1401.2298", "submitter": "Brian J. Reich", "authors": "Brian J. Reich, Michael D. Porter", "title": "Discussion of \"Estimating the historical and future probabilities of\n  large terrorist event\" by Aaron Clauset and Ryan Woodard", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS614B the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 4, 1871-1875", "doi": "10.1214/13-AOAS614B", "report-no": "IMS-AOAS-AOAS614B", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Estimating the historical and future probabilities of large\nterrorist events\" by Aaron Clauset and Ryan Woodard [arXiv:1209.0089].\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2014 12:06:48 GMT"}], "update_date": "2014-01-13", "authors_parsed": [["Reich", "Brian J.", ""], ["Porter", "Michael D.", ""]]}, {"id": "1401.2303", "submitter": "Gentry White", "authors": "Gentry White", "title": "Discussion of \"Estimating the historical and future probabilities of\n  large terrorist events\" by Aaron Clauset and Ryan Woodard", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS614C the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 4, 1876-1880", "doi": "10.1214/13-AOAS614C", "report-no": "IMS-AOAS-AOAS614C", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Estimating the historical and future probabilities of large\nterrorist events\" by Aaron Clauset and Ryan Woodard [arXiv:1209.0089].\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2014 12:16:53 GMT"}], "update_date": "2014-01-13", "authors_parsed": [["White", "Gentry", ""]]}, {"id": "1401.2308", "submitter": "Jeff Gill", "authors": "Jeff Gill", "title": "Discussion of \"Estimating the historical and future probabilities of\n  large terrorist events\" by Aaron Clauset and Ryan Woodard", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS614D the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 4, 1881-1887", "doi": "10.1214/13-AOAS614D", "report-no": "IMS-AOAS-AOAS614D", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Estimating the historical and future probabilities of large\nterrorist events\" by Aaron Clauset and Ryan Woodard [arXiv:1209.0089].\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2014 12:29:21 GMT"}], "update_date": "2014-01-13", "authors_parsed": [["Gill", "Jeff", ""]]}, {"id": "1401.2309", "submitter": "Frederic Paik Schoenberg", "authors": "Frederic Paik Schoenberg", "title": "Discussion of \"Estimating the historical and future probabilities of\n  large terrorist events\" by Aaron Clauset and Ryan Woodard", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS614E the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 4, 1888-1890", "doi": "10.1214/13-AOAS614E", "report-no": "IMS-AOAS-AOAS614E", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Estimating the historical and future probabilities of large\nterrorist events\" by Aaron Clauset and Ryan Woodard [arXiv:1209.0089].\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2014 12:32:44 GMT"}], "update_date": "2014-01-13", "authors_parsed": [["Schoenberg", "Frederic Paik", ""]]}, {"id": "1401.2316", "submitter": "Qiurong Cui", "authors": "Qiurong Cui, Karl Rohe, Zhengjun Zhang", "title": "Discussion of \"Estimating the historical and future probabilities of\n  large terrorist events\" by Aaron Clauset and Ryan Woodard", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS614F the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 4, 1891-1894", "doi": "10.1214/13-AOAS614F", "report-no": "IMS-AOAS-AOAS614F", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Estimating the historical and future probabilities of large\nterrorist events\" by Aaron Clauset and Ryan Woodard [arXiv:1209.0089].\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2014 12:49:31 GMT"}], "update_date": "2014-01-13", "authors_parsed": [["Cui", "Qiurong", ""], ["Rohe", "Karl", ""], ["Zhang", "Zhengjun", ""]]}, {"id": "1401.2317", "submitter": "Aaron Clauset", "authors": "Aaron Clauset, Ryan Woodard", "title": "Rejoinder of \"Estimating the historical and future probabilities of\n  large terrorist events\" by Aaron Clauset and Ryan Woodard", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS614R the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 4, 1895-1897", "doi": "10.1214/13-AOAS614R", "report-no": "IMS-AOAS-AOAS614R", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rejoinder of \"Estimating the historical and future probabilities of large\nterrorist events\" by Aaron Clauset and Ryan Woodard [arXiv:1209.0089].\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2014 12:53:53 GMT"}], "update_date": "2014-01-13", "authors_parsed": [["Clauset", "Aaron", ""], ["Woodard", "Ryan", ""]]}, {"id": "1401.2324", "submitter": "Philip S. Boonstra", "authors": "Philip S. Boonstra, Bhramar Mukherjee, Jeremy M. G. Taylor", "title": "Bayesian shrinkage methods for partially observed data with many\n  predictors", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS668 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 4, 2272-2292", "doi": "10.1214/13-AOAS668", "report-no": "IMS-AOAS-AOAS668", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the increasing use of and rapid changes in array technologies,\nwe consider the prediction problem of fitting a linear regression relating a\ncontinuous outcome $Y$ to a large number of covariates $\\mathbf {X}$, for\nexample, measurements from current, state-of-the-art technology. For most of\nthe samples, only the outcome $Y$ and surrogate covariates, $\\mathbf {W}$, are\navailable. These surrogates may be data from prior studies using older\ntechnologies. Owing to the dimension of the problem and the large fraction of\nmissing information, a critical issue is appropriate shrinkage of model\nparameters for an optimal bias-variance trade-off. We discuss a variety of\nfully Bayesian and Empirical Bayes algorithms which account for uncertainty in\nthe missing data and adaptively shrink parameter estimates for superior\nprediction. These methods are evaluated via a comprehensive simulation study.\nIn addition, we apply our methods to a lung cancer data set, predicting\nsurvival time ($Y$) using qRT-PCR ($\\mathbf {X}$) and microarray ($\\mathbf\n{W}$) measurements.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2014 13:29:39 GMT"}], "update_date": "2014-01-13", "authors_parsed": [["Boonstra", "Philip S.", ""], ["Mukherjee", "Bhramar", ""], ["Taylor", "Jeremy M. G.", ""]]}, {"id": "1401.2344", "submitter": "Alessandra Mattei", "authors": "Alessandra Mattei, Fan Li, Fabrizia Mealli", "title": "Exploiting multiple outcomes in Bayesian principal stratification\n  analysis with application to the evaluation of a job training program", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS674 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 4, 2336-2360", "doi": "10.1214/13-AOAS674", "report-no": "IMS-AOAS-AOAS674", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The causal effect of a randomized job training program, the JOBS II study, on\ntrainees' depression is evaluated. Principal stratification is used to deal\nwith noncompliance to the assigned treatment. Due to the latent nature of the\nprincipal strata, strong structural assumptions are often invoked to identify\nprincipal causal effects. Alternatively, distributional assumptions may be\ninvoked using a model-based approach. These often lead to weakly identified\nmodels with substantial regions of flatness in the posterior distribution of\nthe causal effects. Information on multiple outcomes is routinely collected in\npractice, but is rarely used to improve inference. This article develops a\nBayesian approach to exploit multivariate outcomes to sharpen inferences in\nweakly identified principal stratification models. We show that inference for\nthe causal effect on depression is significantly improved by using the\nre-employment status as a secondary outcome in the JOBS II study. Simulation\nstudies are also performed to illustrate the potential gains in the estimation\nof principal causal effects from jointly modeling more than one outcome. This\napproach can also be used to assess plausibility of structural assumptions and\nsensitivity to deviations from these structural assumptions. Two model checking\nprocedures via posterior predictive checks are also discussed.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2014 14:34:12 GMT"}], "update_date": "2014-01-13", "authors_parsed": [["Mattei", "Alessandra", ""], ["Li", "Fan", ""], ["Mealli", "Fabrizia", ""]]}, {"id": "1401.2503", "submitter": "Tao Xiong", "authors": "Tao Xiong, Yukun Bao, Zhongyi Hu", "title": "Does Restraining End Effect Matter in EMD-Based Modeling Framework for\n  Time Series Prediction? Some Experimental Evidences", "comments": "28 pages", "journal-ref": "Neurocomputing. 123, 2013: 174-184", "doi": "10.1016/j.neucom.2013.07.004", "report-no": null, "categories": "cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the \"decomposition-and-ensemble\" principle, the empirical mode\ndecomposition (EMD)-based modeling framework has been widely used as a\npromising alternative for nonlinear and nonstationary time series modeling and\nprediction. The end effect, which occurs during the sifting process of EMD and\nis apt to distort the decomposed sub-series and hurt the modeling process\nfollowed, however, has been ignored in previous studies. Addressing the end\neffect issue, this study proposes to incorporate end condition methods into\nEMD-based decomposition and ensemble modeling framework for one- and multi-step\nahead time series prediction. Four well-established end condition methods,\nMirror method, Coughlin's method, Slope-based method, and Rato's method, are\nselected, and support vector regression (SVR) is employed as the modeling\ntechnique. For the purpose of justification and comparison, well-known NN3\ncompetition data sets are used and four well-established prediction models are\nselected as benchmarks. The experimental results demonstrated that significant\nimprovement can be achieved by the proposed EMD-based SVR models with end\ncondition methods. The EMD-SBM-SVR model and EMD-Rato-SVR model, in particular,\nachieved the best prediction performances in terms of goodness of forecast\nmeasures and equality of accuracy of competing forecasts test.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jan 2014 06:08:04 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Xiong", "Tao", ""], ["Bao", "Yukun", ""], ["Hu", "Zhongyi", ""]]}, {"id": "1401.2642", "submitter": "Reinhard Furrer", "authors": "Michaela Paul and Paul R. Torgerson and Johan H\\\"oglund and Reinhard\n  Furrer", "title": "Hierarchical modelling of faecal egg counts to assess anthelmintic\n  efficacy", "comments": "14 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counting the number of parasite eggs in faecal samples is a widely used\ndiagnostic method to evaluate parasite burden. Typically a sub-sample of the\ndiluted faeces is examined for eggs. The resulting egg counts are multiplied by\na specific correction factor to estimate the mean parasite burden. To detect\nanthelmintic resistance, the mean parasite burden from treated and untreated\nanimals are compared. However, this standard method has some limitations. In\nparticular, the analysis of repeated samples may produce quite variable results\nas the sampling variability due to the counting technique is ignored. We\npropose a hierarchical model that takes this sampling variability as well as\nbetween-animal variation into account. Bayesian inference is done via Markov\nchain Monte Carlo sampling. The performance of the hierarchical model is\nillustrated by a re-analysis of faecal egg count data from a Swedish study\nassessing the anthelmintic resistance of nematode parasite in sheep. A\nsimulation study shows that the hierarchical model provides better\nclassification of anthelmintic resistance compared to the standard method.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2014 16:53:43 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Paul", "Michaela", ""], ["Torgerson", "Paul R.", ""], ["H\u00f6glund", "Johan", ""], ["Furrer", "Reinhard", ""]]}, {"id": "1401.2649", "submitter": "Marco Scutari", "authors": "Marco Scutari", "title": "Personalised Medicine: Taking a New Look at the Patient", "comments": "Workshop on Foundations of Biomedical Knowledge Representation,\n  October 29 - November 2, 2012 Leiden, The Netherlands", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalised medicine strives to identify the right treatment for the right\npatient at the right time, integrating different types of biological and\nenvironmental information. Such information come from a variety of sources:\nomics data (genomic, proteomic, metabolomic, etc.), live molecular diagnostics,\nand other established diagnostics routinely used by medical doctors.\nIntegrating these different kinds of data, which are all high-dimensional,\npresents significant challenges in knowledge representation and subsequent\nreasoning. The ultimate goal of such a modelling effort is to elucidate the\nflow of information that links genes, protein signalling and other\nphysiological responses to external stimuli such as environmental conditions or\nthe progress of a disease.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2014 17:46:46 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Scutari", "Marco", ""]]}, {"id": "1401.2722", "submitter": "Yuval Benjamini", "authors": "Yuval Benjamini, Bin Yu", "title": "The shuffle estimator for explainable variance in fMRI experiments", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS681 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 4, 2007-2033", "doi": "10.1214/13-AOAS681", "report-no": "IMS-AOAS-AOAS681", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computational neuroscience, it is important to estimate well the\nproportion of signal variance in the total variance of neural activity\nmeasurements. This explainable variance measure helps neuroscientists assess\nthe adequacy of predictive models that describe how images are encoded in the\nbrain. Complicating the estimation problem are strong noise correlations, which\nmay confound the neural responses corresponding to the stimuli. If not properly\ntaken into account, the correlations could inflate the explainable variance\nestimates and suggest false possible prediction accuracies. We propose a novel\nmethod to estimate the explainable variance in functional MRI (fMRI) brain\nactivity measurements when there are strong correlations in the noise. Our\nshuffle estimator is nonparametric, unbiased, and built upon the random effect\nmodel reflecting the randomization in the fMRI data collection process.\nLeveraging symmetries in the measurements, our estimator is obtained by\nappropriately permuting the measurement vector in such a way that the noise\ncovariance structure is intact but the explainable variance is changed after\nthe permutation. This difference is then used to estimate the explainable\nvariance. We validate the properties of the proposed method in simulation\nexperiments. For the image-fMRI data, we show that the shuffle estimates can\nexplain the variation in prediction accuracy for voxels within the primary\nvisual cortex (V1) better than alternative parametric methods.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2014 06:25:57 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Benjamini", "Yuval", ""], ["Yu", "Bin", ""]]}, {"id": "1401.2728", "submitter": "Jonathan Gruhl", "authors": "Jonathan Gruhl, Elena A. Erosheva, Paul K. Crane", "title": "A semiparametric approach to mixed outcome latent variable models:\n  Estimating the association between cognition and regional brain volumes", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS675 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 4, 2361-2383", "doi": "10.1214/13-AOAS675", "report-no": "IMS-AOAS-AOAS675", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate data that combine binary, categorical, count and continuous\noutcomes are common in the social and health sciences. We propose a\nsemiparametric Bayesian latent variable model for multivariate data of\narbitrary type that does not require specification of conditional\ndistributions. Drawing on the extended rank likelihood method by Hoff [Ann.\nAppl. Stat. 1 (2007) 265-283], we develop a semiparametric approach for latent\nvariable modeling with mixed outcomes and propose associated Markov chain Monte\nCarlo estimation methods. Motivated by cognitive testing data, we focus on\nbifactor models, a special case of factor analysis. We employ our\nsemiparametric Bayesian latent variable model to investigate the association\nbetween cognitive outcomes and MRI-measured regional brain volumes.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2014 07:07:41 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Gruhl", "Jonathan", ""], ["Erosheva", "Elena A.", ""], ["Crane", "Paul K.", ""]]}, {"id": "1401.2756", "submitter": "Edoardo M. Airoldi", "authors": "Edoardo M. Airoldi, Xiaopei Wang, Xiaodong Lin", "title": "Multi-way blockmodels for analyzing coordinated high-dimensional\n  responses", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS643 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 4, 2431-2457", "doi": "10.1214/13-AOAS643", "report-no": "IMS-AOAS-AOAS643", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of quantifying temporal coordination between multiple\nhigh-dimensional responses. We introduce a family of multi-way stochastic\nblockmodels suited for this problem, which avoids preprocessing steps such as\nbinning and thresholding commonly adopted for this type of data, in biology. We\ndevelop two inference procedures based on collapsed Gibbs sampling and\nvariational methods. We provide a thorough evaluation of the proposed methods\non simulated data, in terms of membership and blockmodel estimation,\npredictions out-of-sample and run-time. We also quantify the effects of\ncensoring procedures such as binning and thresholding on the estimation tasks.\nWe use these models to carry out an empirical analysis of the functional\nmechanisms driving the coordination between gene expression and metabolite\nconcentrations during carbon and nitrogen starvation, in S. cerevisiae.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2014 09:05:00 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Airoldi", "Edoardo M.", ""], ["Wang", "Xiaopei", ""], ["Lin", "Xiaodong", ""]]}, {"id": "1401.2760", "submitter": "Giwhyun Lee", "authors": "Giwhyun Lee, Eunshin Byon, Lewis Ntaimo, Yu Ding", "title": "Bayesian spline method for assessing extreme loads on wind turbines", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS670 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 4, 2034-2061", "doi": "10.1214/13-AOAS670", "report-no": "IMS-AOAS-AOAS670", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study presents a Bayesian parametric model for the purpose of estimating\nthe extreme load on a wind turbine. The extreme load is the highest stress\nlevel imposed on a turbine structure that the turbine would experience during\nits service lifetime. A wind turbine should be designed to resist such a high\nload to avoid catastrophic structural failures. To assess the extreme load,\nturbine structural responses are evaluated by conducting field measurement\ncampaigns or performing aeroelastic simulation studies. In general, data\nobtained in either case are not sufficient to represent various loading\nresponses under all possible weather conditions. An appropriate extrapolation\nis necessary to characterize the structural loads in a turbine's service life.\nThis study devises a Bayesian spline method for this extrapolation purpose,\nusing load data collected in a period much shorter than a turbine's service\nlife. The spline method is applied to three sets of turbine's load response\ndata to estimate the corresponding extreme loads at the roots of the turbine\nblades. Compared to the current industry practice, the spline method appears to\nprovide better extreme load assessment.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2014 09:11:59 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Lee", "Giwhyun", ""], ["Byon", "Eunshin", ""], ["Ntaimo", "Lewis", ""], ["Ding", "Yu", ""]]}, {"id": "1401.2822", "submitter": "Alexandru  Am\\u{a}rioarei", "authors": "Alexandru Amarioarei and Cristian Preda", "title": "Approximations for two-dimensional discrete scan statistics in some\n  block-factor type dependent models", "comments": "17 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the two-dimensional discrete scan statistic generated by a\nblock-factor type model obtained from i.i.d. sequence. We present an\napproximation for the distribution of the scan statistics and the corresponding\nerror bounds. A simulation study illustrates our methodology.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2014 12:57:57 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Amarioarei", "Alexandru", ""], ["Preda", "Cristian", ""]]}, {"id": "1401.2832", "submitter": "Yuping Zhang", "authors": "Yuping Zhang, Ronald Davis", "title": "Principal trend analysis for time-course data with applications in\n  genomic medicine", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS659 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 4, 2205-2228", "doi": "10.1214/13-AOAS659", "report-no": "IMS-AOAS-AOAS659", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-course high-throughput gene expression data are emerging in genomic and\ntranslational medicine. Extracting interesting time-course patterns from a\npatient cohort can provide biological insights for further clinical research\nand patient treatment. We propose principal trend analysis (PTA) to extract\nprincipal trends of time-course gene expression data from a group of patients,\nand identify genes that make dominant contributions to the principal trends.\nThrough simulations, we demonstrate the utility of PTA for dimension reduction,\ntime-course signal recovery and feature selection with high-dimensional data.\nMoreover, PTA derives new insights in real biological and clinical research. We\ndemonstrate the usefulness of PTA by applying it to longitudinal gene\nexpression data of a circadian regulation system and burn patients. These\napplications show that PTA can extract interesting time-course trends with\nbiological significance, which helps the understanding of biological mechanisms\nof circadian regulation systems as well as the recovery of burn patients.\nOverall, the proposed PTA approach will benefit the genomic medicine research.\nOur method is implemented into an R-package: PTA (Principal Trend Analysis).\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2014 13:26:55 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Zhang", "Yuping", ""], ["Davis", "Ronald", ""]]}, {"id": "1401.2894", "submitter": "Theodore  Kypraios", "authors": "Christopher J. Fallaize and Theodore Kypraios", "title": "Exact Bayesian Inference for the Bingham Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with making Bayesian inference from data that are\nassumed to be drawn from a Bingham distribution. A barrier to the Bayesian\napproach is the parameter-dependent normalising constant of the Bingham\ndistribution, which, even when it can be evaluated or accurately approximated,\nwould have to be calculated at each iteration of an MCMC scheme, thereby\ngreatly increasing the computational burden. We propose a method which enables\nexact (in Monte Carlo sense) Bayesian inference for the unknown parameters of\nthe Bingham distribution by completely avoiding the need to evaluate this\nconstant. We apply the method to simulated and real data, and illustrate that\nit is simpler to implement, faster, and performs better than an alternative\nalgorithm that has recently been proposed in the literature.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2014 16:10:07 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Fallaize", "Christopher J.", ""], ["Kypraios", "Theodore", ""]]}, {"id": "1401.2957", "submitter": "Wagner Bonat", "authors": "Wagner Hugo Bonat and Paulo Justiniano Ribeiro Jr and Silvia emiko\n  Shimakura", "title": "Bayesian analysis for a class of beta mixed models", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized linear mixed models (GLMM) encompass large class of statistical\nmodels, with a vast range of applications areas. GLMM extends the linear mixed\nmodels allowing for different types of response variable. Three most common\ndata types are continuous, counts and binary and standard distributions for\nthese types of response variables are Gaussian, Poisson and Binomial,\nrespectively. Despite that flexibility, there are situations where the response\nvariable is continuous, but bounded, such as rates, percentages, indexes and\nproportions. In such situations the usual GLMM's are not adequate because\nbounds are ignored and the beta distribution can be used. Likelihood and\nBayesian inference for beta mixed models are not straightforward demanding a\ncomputational overhead. Recently, a new algorithm for Bayesian inference called\nINLA (Integrated Nested Laplace Approximation) was proposed.INLA allows\ncomputation of many Bayesian GLMMs in a reasonable amount time allowing\nextensive comparison among models. We explore Bayesian inference for beta mixed\nmodels by INLA. We discuss the choice of prior distributions, sensitivity\nanalysis and model selection measures through a real data set. The results\nobtained from INLA are compared with those obtained by an MCMC algorithm and\nlikelihood analysis. We analyze data from an study on a life quality index of\nindustry workers collected according to a hierarchical sampling scheme. Results\nshow that the INLA approach is suitable and faster to fit the proposed beta\nmixed models producing results similar to alternative algorithms and with\neasier handling of modeling alternatives. Sensitivity analysis, measures of\ngoodness of fit and model choice are discussed.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2014 19:06:40 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2014 12:47:47 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Bonat", "Wagner Hugo", ""], ["Ribeiro", "Paulo Justiniano", "Jr"], ["Shimakura", "Silvia emiko", ""]]}, {"id": "1401.3211", "submitter": "Julian Faraway", "authors": "Julian Faraway, Ashish Mahabal, Jiayang Sun, Xiaofeng Wang, Yi (Grace)\n  Wang, Lingsong Zhang", "title": "Modeling Light Curves for Improved Classification", "comments": "16 pages, 4 Figures", "journal-ref": null, "doi": "10.1002/sam.11305", "report-no": null, "categories": "stat.AP astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many synoptic surveys are observing large parts of the sky multiple times.\nThe resulting lightcurves provide a wonderful window to the dynamic nature of\nthe universe. However, there are many significant challenges in analyzing these\nlight curves. These include heterogeneity of the data, irregularly sampled\ndata, missing data, censored data, known but variable measurement errors, and\nmost importantly, the need to classify in astronomical objects in real time\nusing these imperfect light curves. We describe a modeling-based approach using\nGaussian process regression for generating critical measures representing\nfeatures for the classification of such lightcurves. We demonstrate that our\napproach performs better by comparing it with past methods. Finally, we provide\nfuture directions for use in sky-surveys that are getting even bigger by the\nday.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2014 15:03:13 GMT"}, {"version": "v2", "created": "Tue, 1 Jul 2014 08:47:53 GMT"}], "update_date": "2016-02-04", "authors_parsed": [["Faraway", "Julian", "", "Grace"], ["Mahabal", "Ashish", "", "Grace"], ["Sun", "Jiayang", "", "Grace"], ["Wang", "Xiaofeng", "", "Grace"], ["Yi", "", "", "Grace"], ["Wang", "", ""], ["Zhang", "Lingsong", ""]]}, {"id": "1401.3229", "submitter": "Ngoc Mai Tran", "authors": "Ngoc Mai Tran, Maria Osipenko, and Wolfgang Karl Haerdle", "title": "Principal Component Analysis in an Asymmetric Norm", "comments": "31 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is a widely used dimension reduction tool\nin the analysis of many kind of high-dimensional data. It is used in signal\nprocessing, mechanical engineering, psychometrics, and other fields under\ndifferent names. It still bears the same mathematical idea: the decomposition\nof variation of a high dimensional object into uncorrelated factors or\ncomponents. However, in many of the above applications, one is interested in\ncapturing the tail variables of the data rather than variation around the mean.\nSuch applications include weather related event curves, expected shortfalls,\nand speeding analysis among others. These are all high dimensional tail objects\nwhich one would like to study in a PCA fashion. The tail character though\nrequires to do the dimension reduction in an asymmetric norm rather than the\nclassical $L_2$-type orthogonal projection. We develop an analogue of PCA in an\nasymmetric norm. These norms cover both quantiles and expectiles, another tail\nevent measure. The difficulty is that there is no natural basis, no `principal\ncomponents', to the $k$-dimensional subspace found. We propose two definitions\nof principal components and provide algorithms based on iterative least\nsquares. We prove upper bounds on their convergence times, and compare their\nperformances in a simulation study. We apply the algorithms to a Chinese\nweather dataset with a view to weather derivative pricing\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2014 15:45:45 GMT"}], "update_date": "2014-01-15", "authors_parsed": [["Tran", "Ngoc Mai", ""], ["Osipenko", "Maria", ""], ["Haerdle", "Wolfgang Karl", ""]]}, {"id": "1401.3290", "submitter": "Brett T. McClintock", "authors": "Brett T. McClintock, Larissa L. Bailey, Brian P. Dreher, William A.\n  Link", "title": "Probit models for capture-recapture data subject to imperfect detection,\n  individual heterogeneity and misidentification", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS783 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 4, 2461-2484", "doi": "10.1214/14-AOAS783", "report-no": "IMS-AOAS-AOAS783", "categories": "q-bio.QM q-bio.PE stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As noninvasive sampling techniques for animal populations have become more\npopular, there has been increasing interest in the development of\ncapture-recapture models that can accommodate both imperfect detection and\nmisidentification of individuals (e.g., due to genotyping error). However,\ncurrent methods do not allow for individual variation in parameters, such as\ndetection or survival probability. Here we develop misidentification models for\ncapture-recapture data that can simultaneously account for temporal variation,\nbehavioral effects and individual heterogeneity in parameters. To facilitate\nBayesian inference using our approach, we extend standard probit regression\ntechniques to latent multinomial models where the dimension and zeros of the\nresponse cannot be observed. We also present a novel Metropolis-Hastings within\nGibbs algorithm for fitting these models using Markov chain Monte Carlo. Using\nclosed population abundance models for illustration, we re-visit a DNA\ncapture-recapture population study of black bears in Michigan, USA and find\nevidence of misidentification due to genotyping error, as well as temporal,\nbehavioral and individual variation in detection probability. We also estimate\na salamander population of known size from laboratory experiments evaluating\nthe effectiveness of a marking technique commonly used for amphibians and fish.\nOur model was able to reliably estimate the size of this population and\nprovided evidence of individual heterogeneity in misidentification probability\nthat is attributable to variable mark quality. Our approach is more\ncomputationally demanding than previously proposed methods, but it provides the\nflexibility necessary for a much broader suite of models to be explored while\nproperly accounting for uncertainty introduced by misidentification and\nimperfect detection. In the absence of misidentification, our probit\nformulation also provides a convenient and efficient Gibbs sampler for Bayesian\nanalysis of traditional closed population capture-recapture data.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2014 19:14:31 GMT"}, {"version": "v2", "created": "Mon, 11 Aug 2014 18:44:53 GMT"}, {"version": "v3", "created": "Thu, 2 Oct 2014 20:59:51 GMT"}, {"version": "v4", "created": "Tue, 3 Feb 2015 12:47:54 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["McClintock", "Brett T.", ""], ["Bailey", "Larissa L.", ""], ["Dreher", "Brian P.", ""], ["Link", "William A.", ""]]}, {"id": "1401.3407", "submitter": "Tadilo E Bogale", "authors": "Tadilo Endeshaw Bogale and Luc Vandendorpe", "title": "USRP Implementation of Max-Min SNR Signal Energy based Spectrum Sensing\n  Algorithms for Cognitive Radio Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the Universal Software Radio Peripheral (USRP)\nexperimental results of the Max-Min signal to noise ratio (SNR) Signal Energy\nbased Spectrum Sensing Algorithms for Cognitive Radio Networks which is\nrecently proposed in \\cite{BogaMaxMinSNRJournal2013}. Extensive experiments are\nperformed for different set of parameters. In particular, the effects of SNR,\nnumber of samples and roll-off factor on the detection performances of the\nlatter algorithms are examined briefly. We have observed that the experimental\nresults fit well with those of the theory. We also confirm that these\nalgorithms are indeed robust against carrier frequency offset, symbol timing\noffset and noise variance uncertainty.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 02:09:47 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Bogale", "Tadilo Endeshaw", ""], ["Vandendorpe", "Luc", ""]]}, {"id": "1401.3408", "submitter": "George Moustakides", "authors": "George V. Moustakides", "title": "Multiple optimality properties of the Shewhart test", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the problem of sequential detection of changes, we adopt the probability\nmaximizing approach in place of the classical minimization of the average\ndetection delay, and propose modified versions of the Shiryaev, Lorden and\nPollak performance measures. For these alternative formulations, we demonstrate\nthat the optimum sequential detection scheme is the simple Shewhart rule.\nInterestingly, we can also solve problems which under the classical setup have\nbeen open for many years, as optimum change detection with time varying\nobservations or with multiple post-change probability measures. For the last\ncase, we also offer the exact solution for Lorden's original setup when the\naverage false alarm period is within certain limits.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 02:12:01 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Moustakides", "George V.", ""]]}, {"id": "1401.3518", "submitter": "Cedric Ginestet", "authors": "Wes Viles, Cedric E. Ginestet, Ariana Tang, Mark A. Kramer, Eric D.\n  Kolaczyk", "title": "Percolation under Noise: Detecting Explosive Percolation Using the\n  Second Largest Component", "comments": "9 pages and 8 figures. Submitted to Physics Review, Series E", "journal-ref": "Phys. Rev. E 93, 052301 (2016)", "doi": "10.1103/PhysRevE.93.052301", "report-no": null, "categories": "stat.AP q-bio.NC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We consider the problem of distinguishing classical (Erd\\H{o}s-R\\'{e}nyi)\npercolation from explosive (Achlioptas) percolation, under noise. A statistical\nmodel of percolation is constructed allowing for the birth and death of edges\nas well as the presence of noise in the observations. This graph-valued\nstochastic process is composed of a latent and an observed non-stationary\nprocess, where the observed graph process is corrupted by Type I and Type II\nerrors. This produces a hidden Markov graph model. We show that for certain\nchoices of parameters controlling the noise, the classical (ER) percolation is\nvisually indistinguishable from the explosive (Achlioptas) percolation model.\nIn this setting, we compare two different criteria for discriminating between\nthese two percolation models, based on a quantile difference (QD) of the first\ncomponent's size and on the maximal size of the second largest component. We\nshow through data simulations that this second criterion outperforms the QD of\nthe first component's size, in terms of discriminatory power. The maximal size\nof the second component therefore provides a useful statistic for\ndistinguishing between the ER and Achlioptas models of percolation, under\nphysically motivated conditions for the birth and death of edges, and under\nnoise. The potential application of the proposed criteria for percolation\ndetection in clinical neuroscience is also discussed.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 08:52:29 GMT"}], "update_date": "2016-05-11", "authors_parsed": [["Viles", "Wes", ""], ["Ginestet", "Cedric E.", ""], ["Tang", "Ariana", ""], ["Kramer", "Mark A.", ""], ["Kolaczyk", "Eric D.", ""]]}, {"id": "1401.3567", "submitter": "Youssef Khmou", "authors": "Y.Khmou, S.Safi", "title": "2D Direction Of Arrival Estimation with Modified Propagator", "comments": "4 pages, one latex file, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a fast algorithm for the Direction Of Arrival (DOA) estimation\nof radiating sources, based on partial covariance matrix and without eigende-\ncomposition of incoming signals is extended to two dimensional problem of joint\nazimuth and elevation estimation angles using Uniform Circular Array (UCA) in\ncase of non coherent narrowband signals. Simulation results are presented with\nboth Additive White Gaussian Noise (AWGN) and real symmetric Toeplitz noise.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 13:07:52 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Khmou", "Y.", ""], ["Safi", "S.", ""]]}, {"id": "1401.3790", "submitter": "William  Marshall", "authors": "William Marshall and Paul Marriott", "title": "Detection of Phase Shift Events", "comments": "20 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of change-point estimation of the instantaneous phase\nof an observed time series. Such change points, or phase shifts, can be markers\nof information transfer in complex systems; their analysis occurring in\ngeology, biology and physics, but most notably in neuroscience. We develop two\nnon-parametric approaches to this problem: the cumulative summation (CUSUM) and\nphase derivative (PD) estimators. In general the CUSUM estimator has higher\npower for identifying single shift events, while the PD estimator has better\ntemporal resolution for multiple ones. A system of weakly coupled Rossler\nattractors provides an application in which there are high levels of systematic\nand time-dependent noise. Shift identification is also performed on beta-band\nactivity from electroencephalogram recordings of a visual attention task, an\nunsupervised application which requires high temporal resolution.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 23:41:31 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Marshall", "William", ""], ["Marriott", "Paul", ""]]}, {"id": "1401.3813", "submitter": "Heather Patsolic", "authors": "Heather Patsolic, Sancar Adali, Joshua T. Vogelstein, Youngser Park,\n  Carey E. Friebe, Gongkai Li, Vince Lyzinski", "title": "Seeded Graph Matching Via Joint Optimization of Fidelity and\n  Commensurability", "comments": "26 pages, 7 figures. Updated content and added application of\n  simultaneous matching for several time-steps for zebrafish connectomes", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approximate graph matching algorithm that incorporates\nseeded data into the graph matching paradigm. Our Joint Optimization of\nFidelity and Commensurability (JOFC) algorithm embeds two graphs into a common\nEuclidean space where the matching inference task can be performed. Through\nreal and simulated data examples, we demonstrate the versatility of our\nalgorithm in matching graphs with various characteristics--weightedness,\ndirectedness, loopiness, many-to-one and many-to-many matchings, and soft\nseedings.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 02:33:44 GMT"}, {"version": "v2", "created": "Sun, 8 Dec 2019 20:01:37 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Patsolic", "Heather", ""], ["Adali", "Sancar", ""], ["Vogelstein", "Joshua T.", ""], ["Park", "Youngser", ""], ["Friebe", "Carey E.", ""], ["Li", "Gongkai", ""], ["Lyzinski", "Vince", ""]]}, {"id": "1401.3911", "submitter": "Gael Martin Prof", "authors": "Worapree Maneesoonthorn, Catherine S. Forbes and Gael M. Martin", "title": "Inference on Self-Exciting Jumps in Prices and Volatility using High\n  Frequency Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic jumps in the price and volatility of an asset are modelled using a\njoint Hawkes process in conjunction with a bivariate jump diffusion. A state\nspace representation is used to link observed returns, plus nonparametric\nmeasures of integrated volatility and price jumps, to the specified model\ncomponents; with Bayesian inference conducted using a Markov chain Monte Carlo\nalgorithm. An evaluation of marginal likelihoods for the proposed model\nrelative to a large number of alternative models, including some that have\nfeatured in the literature, is provided. An extensive empirical investigation\nis undertaken using data on the S&P500 market index over the 1996 to 2014\nperiod, with substantial support for dynamic jump intensities - including in\nterms of predictive accuracy - documented.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:46:36 GMT"}, {"version": "v2", "created": "Tue, 23 Dec 2014 22:55:00 GMT"}, {"version": "v3", "created": "Wed, 9 Mar 2016 03:38:11 GMT"}], "update_date": "2016-03-10", "authors_parsed": [["Maneesoonthorn", "Worapree", ""], ["Forbes", "Catherine S.", ""], ["Martin", "Gael M.", ""]]}, {"id": "1401.4122", "submitter": "Carsten Allefeld", "authors": "Carsten Allefeld, John-Dylan Haynes", "title": "Searchlight-based multi-voxel pattern analysis of fMRI by\n  cross-validated MANOVA", "comments": null, "journal-ref": "NeuroImage, 89:345-357, 2014", "doi": "10.1016/j.neuroimage.2013.11.043", "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-voxel pattern analysis (MVPA) is a fruitful and increasingly popular\ncomplement to traditional univariate methods of analyzing neuroimaging data. We\npropose to replace the standard 'decoding' approach to searchlight-based MVPA,\nmeasuring the performance of a classifier by its accuracy, with a method based\non the multivariate form of the general linear model. Following the\nwell-established methodology of multivariate analysis of variance (MANOVA), we\ndefine a measure that directly characterizes the structure of multi-voxel data,\nthe pattern distinctness $D$. Our measure is related to standard multivariate\nstatistics, but we apply cross-validation to obtain an unbiased estimate of its\npopulation value, independent of the amount of data or its partitioning into\n'training' and 'test' sets. The estimate $\\hat D$ can therefore serve not only\nas a test statistic, but as an interpretable measure of multivariate effect\nsize. The pattern distinctness generalizes the Mahalanobis distance to an\narbitrary number of classes, but also the case where there are no classes of\ntrials because the design is described by parametric regressors. It is defined\nfor arbitrary estimable contrasts, including main effects (pattern differences)\nand interactions (pattern changes). In this way, our approach makes the full\nanalytical power of complex factorial designs known from univariate fMRI\nanalyses available to MVPA studies. Moreover, we show how the results of a\nfactorial analysis can be used to obtain a measure of pattern stability, the\nequivalent of 'cross-decoding'.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 18:44:28 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2014 15:05:10 GMT"}], "update_date": "2014-02-10", "authors_parsed": [["Allefeld", "Carsten", ""], ["Haynes", "John-Dylan", ""]]}, {"id": "1401.4128", "submitter": "Charles-Henri Cappelaere", "authors": "Charles-Henri Cappelaere, R. Dubois, P. Roussel, G. Dreyfus", "title": "Towards the selection of patients requiring ICD implantation by\n  automatic classification from Holter monitoring indices", "comments": "Computing in Cardiology, Saragosse : Espagne (2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this study is to optimize the selection of prophylactic\ncardioverter defibrillator implantation candidates. Currently, the main\ncriterion for implantation is a low Left Ventricular Ejection Fraction (LVEF)\nwhose specificity is relatively poor. We designed two classifiers aimed to\npredict, from long term ECG recordings (Holter), whether a low-LVEF patient is\nlikely or not to undergo ventricular arrhythmia in the next six months. One\nclassifier is a single hidden layer neural network whose variables are the most\nrelevant features extracted from Holter recordings, and the other classifier\nhas a structure that capitalizes on the physiological decomposition of the\narrhythmogenic factors into three disjoint groups: the myocardial substrate,\nthe triggers and the autonomic nervous system (ANS). In this ad hoc network,\nthe features were assigned to each group; one neural network classifier per\ngroup was designed and its complexity was optimized. The outputs of the\nclassifiers were fed to a single neuron that provided the required probability\nestimate. The latter was thresholded for final discrimination A dataset\ncomposed of 186 pre-implantation 30-mn Holter recordings of patients equipped\nwith an implantable cardioverter defibrillator (ICD) in primary prevention was\nused in order to design and test this classifier. 44 out of 186 patients\nunderwent at least one treated ventricular arrhythmia during the six-month\nfollow-up period. Performances of the designed classifier were evaluated using\na cross-test strategy that consists in splitting the database into several\ncombinations of a training set and a test set. The average arrhythmia\nprediction performances of the ad-hoc classifier are NPV = 77% $\\pm$ 13% and\nPPV = 31% $\\pm$ 19% (Negative Predictive Value $\\pm$ std, Positive Predictive\nValue $\\pm$ std). According to our study, improving prophylactic\nICD-implantation candidate selection by automatic classification from ECG\nfeatures may be possible, but the availability of a sizable dataset appears to\nbe essential to decrease the number of False Negatives.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 18:54:43 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Cappelaere", "Charles-Henri", ""], ["Dubois", "R.", ""], ["Roussel", "P.", ""], ["Dreyfus", "G.", ""]]}, {"id": "1401.4215", "submitter": "Michael Evans", "authors": "Saman Muthukumarana and Michael Evans", "title": "Bayesian Hypothesis Assessment in Two-arm Trials Using Relative Belief\n  Ratios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a Bayesian approach for assessing equivalence and\nnon-inferiority hypotheses in two-arm trials using relative belief ratios. A\nrelative belief ratio is a measure of statistical evidence and can indicate\nevidence either for or against a hypothesis. In addition to the relative belief\nratio, we also compute a measure of the strength of this evidence as a\ncalibration of the relative belief ratio. Furthermore, we make use of the\nrelative belief ratio as a measure of evidence, to assess whether a given prior\ninduces bias either for or against a hypothesis. Prior elicitation, model\nchecking and checking for prior-data conflict procedures are developed to\nensure that the choices of model and prior made are relevant to the specific\napplication. We highlight the applicability of the approach and illustrate the\nproposed method by applying it to a data set obtained from a two-arm clinical\ntrial.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2014 01:20:13 GMT"}], "update_date": "2014-01-20", "authors_parsed": [["Muthukumarana", "Saman", ""], ["Evans", "Michael", ""]]}, {"id": "1401.4342", "submitter": "Nicole Augustin H", "authors": "Nicole H. Augustin, Calum Mattocks, Julian J. Faraway, Sonja Greven\n  and Andy R. Ness", "title": "Modelling a response as a function of high frequency count data: the\n  association between physical activity and fat mass", "comments": null, "journal-ref": null, "doi": "10.1177/0962280215595832", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new statistical modelling approach where the response is a\nfunction of high frequency count data. Our application is about investigating\nthe relationship between the health outcome fat mass and physical activity (PA)\nmeasured by accelerometer. The accelerometer quantifies the intensity of\nphysical activity as counts per epoch over a given period of time. We use data\nfrom the Avon longitudinal study of parents and children (ALSPAC) where\naccelerometer data is available as a time series of accelerometer counts per\nminute over seven days for a subset of children. In order to compare\naccelerometer profiles between individuals and to reduce the high dimension a\nfunctional summary of the profiles is used. We use the histogram as a\nfunctional summary due to its simplicity, suitability and ease of\ninterpretation. Our model is an extension of generalised regression of scalars\non functions or signal regression. It allows also multi-dimensional functional\npredictors and additive non-linear predictors for metric covariates. The\nadditive multidimensional functional predictors allow investigating specific\nquestions about whether the effect of PA varies over its intensity, by gender,\nby time of day or by day of the week. The key feature of the model is that it\nutilises the full profile of measured PA without requiring cut-points defining\nintensity levels for light, moderate and vigorous activity. We show that the\n(not necessarily causal) effect of PA is not linear and not constant over the\nactivity intensity. Also, there is little evidence to suggest that the effect\nof PA intensity varies by gender or whether it happens on weekdays or on\nweekends.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2014 13:42:44 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Augustin", "Nicole H.", ""], ["Mattocks", "Calum", ""], ["Faraway", "Julian J.", ""], ["Greven", "Sonja", ""], ["Ness", "Andy R.", ""]]}, {"id": "1401.4644", "submitter": "Cyril Voyant", "authors": "Cyril Voyant (SPE), Pierrick Haurant (CETHIL), Marc Muselli (SPE),\n  Christophe Paoli (SPE), Marie Laure Nivet (SPE)", "title": "Time series modeling and large scale global solar radiation forecasting\n  from geostationary satellites data", "comments": "Solar Energy (2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE physics.comp-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a territory is poorly instrumented, geostationary satellites data can be\nuseful to predict global solar radiation. In this paper, we use geostationary\nsatellites data to generate 2-D time series of solar radiation for the next\nhour. The results presented in this paper relate to a particular territory, the\nCorsica Island, but as data used are available for the entire surface of the\nglobe, our method can be easily exploited to another place. Indeed 2-D hourly\ntime series are extracted from the HelioClim-3 surface solar irradiation\ndatabase treated by the Heliosat-2 model. Each point of the map have been used\nas training data and inputs of artificial neural networks (ANN) and as inputs\nfor two persistence models (scaled or not). Comparisons between these models\nand clear sky estimations were proceeded to evaluate the performances. We found\na normalized root mean square error (nRMSE) close to 16.5% for the two best\npredictors (scaled persistence and ANN) equivalent to 35-45% related to ground\nmeasurements. Finally in order to validate our 2-D predictions maps, we\nintroduce a new error metric called the gamma index which is a criterion for\ncomparing data from two matrixes in medical physics. As first results, we found\nthat in winter and spring, scaled persistence gives the best results (gamma\nindex test passing rate is respectively 67.7% and 86%), in autumn simple\npersistence is the best predictor (95.3%) and ANN is the best in summer\n(99.8%).\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2014 08:02:29 GMT"}], "update_date": "2014-01-21", "authors_parsed": [["Voyant", "Cyril", "", "SPE"], ["Haurant", "Pierrick", "", "CETHIL"], ["Muselli", "Marc", "", "SPE"], ["Paoli", "Christophe", "", "SPE"], ["Nivet", "Marie Laure", "", "SPE"]]}, {"id": "1401.4661", "submitter": "Jean-Christophe Mourrat", "authors": "Jean-Christophe Mourrat", "title": "Significance level and positivity bias as causes for high rate of\n  non-reproducible scientific results?", "comments": "5 pages + 9 pages of supplementary material, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The high fraction of published results that turn out to be incorrect is a\nmajor concern of today's science. This paper contributes to the understanding\nof this problem in two independent directions. First, Johnson's recent claim\nthat hypothesis testing with a significance level of 0.05 can alone lead to an\nunacceptably large proportion of false positives among all results is shown to\nbe unfounded. Second, a way to quantify the effect of \"positivity bias\" (the\ntendency to consider only positive results as worthwhile) is introduced. We\nestimate the proportion of false positives among positive results in terms of\nthe significance level used and the positivity ratio. The latter quantity is\nthe fraction of positive results over all results, be they positive or not,\npublished or not. In particular, if one uses a significance level of 0.05, and\nproduces 4 (possibly unpublished) negative results for every positive result,\nthen the proportion of false positives among positive results can climb to a\nhigh 21%.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2014 12:23:11 GMT"}, {"version": "v2", "created": "Thu, 6 Feb 2014 20:03:29 GMT"}, {"version": "v3", "created": "Thu, 4 Sep 2014 20:21:27 GMT"}, {"version": "v4", "created": "Thu, 6 Nov 2014 20:04:52 GMT"}], "update_date": "2014-11-07", "authors_parsed": [["Mourrat", "Jean-Christophe", ""]]}, {"id": "1401.4769", "submitter": "Sheng-Mao Chang", "authors": "Sheng-Mao Chang", "title": "Variable Screenings in Binary Response Regressions with Multivariate\n  Normal Predictors", "comments": "13 pages, 1 figures", "journal-ref": "Journal of Chinese Statistical Association (2013), 51, 427-444", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Screening before model building is a reasonable strategy to reduce the\ndimension of regression problems. Sure independence screening is an efficient\napproach to this purpose. It applies the slope estimate of a simple linear\nregression as a surrogate measure of the association between the response and\nthe predictor so that the final model can be built by those predictors with\nsteep slopes. However, if the response is truly affected by a nontrivial linear\ncombination of some predictors then the simple linear regression model is a\nmisspecified model. In this work, we investigate the performance of the sure\nindependence screening in the view of model misspecification for binary\nresponse regressions. Both maximum likelihood screening and least square\nscreening are studied with the assumption that predictors follow multivariate\nnormal distribution and the true and the working link function belong to a\nclass of scale mixtures of normal distributions.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2014 01:58:25 GMT"}], "update_date": "2014-01-21", "authors_parsed": [["Chang", "Sheng-Mao", ""]]}, {"id": "1401.4785", "submitter": "Takafumi Ono", "authors": "Satoshi Hara, Takafumi Ono, Ryo Okamoto, Takashi Washio and Shigeki\n  Takeuchi", "title": "Anomaly detection in reconstructed quantum states using a\n  machine-learning technique", "comments": "Accepted for Physical Review A", "journal-ref": null, "doi": "10.1103/PhysRevA.89.022104", "report-no": null, "categories": "quant-ph stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accurate detection of small deviations in given density matrices is\nimportant for quantum information processing. Here we propose a new method\nbased on the concept of data mining. We demonstrate that the proposed method\ncan more accurately detect small erroneous deviations in reconstructed density\nmatrices, which contain intrinsic fluctuations due to the limited number of\nsamples, than a naive method of checking the trace distance from the average of\nthe given density matrices. This method has the potential to be a key tool in\nbroad areas of physics where the detection of small deviations of quantum\nstates reconstructed using a limited number of samples are essential.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2014 03:44:08 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Hara", "Satoshi", ""], ["Ono", "Takafumi", ""], ["Okamoto", "Ryo", ""], ["Washio", "Takashi", ""], ["Takeuchi", "Shigeki", ""]]}, {"id": "1401.4787", "submitter": "Xianhua Peng", "authors": "Steven Kou and Xianhua Peng", "title": "On the Measurement of Economic Tail Risk", "comments": "51 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.ST stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper attempts to provide a decision-theoretic foundation for the\nmeasurement of economic tail risk, which is not only closely related to utility\ntheory but also relevant to statistical model uncertainty. The main result is\nthat the only risk measures that satisfy a set of economic axioms for the\nChoquet expected utility and the statistical property of elicitability (i.e.\nthere exists an objective function such that minimizing the expected objective\nfunction yields the risk measure) are the mean functional and the median\nshortfall, which is the median of tail loss distribution. Elicitability is\nimportant for backtesting. We also extend the result to address model\nuncertainty by incorporating multiple scenarios. As an application, we argue\nthat median shortfall is a better alternative than expected shortfall for\nsetting capital requirements in Basel Accords.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2014 03:55:15 GMT"}, {"version": "v2", "created": "Tue, 11 Feb 2014 12:49:48 GMT"}, {"version": "v3", "created": "Sun, 16 Aug 2015 14:48:15 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Kou", "Steven", ""], ["Peng", "Xianhua", ""]]}, {"id": "1401.5035", "submitter": "Quentin Grimonprez", "authors": "Quentin Grimonprez (INRIA Lille - Nord Europe), Alain Celisse (INRIA\n  Lille - Nord Europe), Meyling Cheok, Martin Figeac, Guillemette Marot (INRIA\n  Lille - Nord Europe, CERIM)", "title": "MPAgenomics : An R package for multi-patients analysis of genomic\n  markers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MPAgenomics, standing for multi-patients analysis (MPA) of genomic markers,\nis an R-package devoted to: (i) efficient segmentation, and (ii) genomic marker\nselection from multi-patient copy number and SNP data profiles. It provides\nwrappers from commonly used packages to facilitate their repeated (sometimes\ndifficult) use, offering an easy-to-use pipeline for beginners in R. The\nsegmentation of successive multiple profiles (finding losses and gains) is\nbased on a new automatic choice of influential parameters since default ones\nwere misleading in the original packages. Considering multiple profiles in the\nsame time, MPAgenomics wraps efficient penalized regression methods to select\nrelevant markers associated with a given response.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2014 19:58:38 GMT"}], "update_date": "2014-01-21", "authors_parsed": [["Grimonprez", "Quentin", "", "INRIA Lille - Nord Europe"], ["Celisse", "Alain", "", "INRIA\n  Lille - Nord Europe"], ["Cheok", "Meyling", "", "INRIA\n  Lille - Nord Europe, CERIM"], ["Figeac", "Martin", "", "INRIA\n  Lille - Nord Europe, CERIM"], ["Marot", "Guillemette", "", "INRIA\n  Lille - Nord Europe, CERIM"]]}, {"id": "1401.5193", "submitter": "Fei Yu", "authors": "Fei Yu, Stephen E. Fienberg, Aleksandra Slavkovi\\'c, Caroline Uhler", "title": "Scalable Privacy-Preserving Data Sharing Methodology for Genome-Wide\n  Association Studies", "comments": "28 pages, 2 figures, source code available upon request", "journal-ref": null, "doi": "10.1016/j.jbi.2014.01.008", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The protection of privacy of individual-level information in genome-wide\nassociation study (GWAS) databases has been a major concern of researchers\nfollowing the publication of \"an attack\" on GWAS data by Homer et al. (2008)\nTraditional statistical methods for confidentiality and privacy protection of\nstatistical databases do not scale well to deal with GWAS data, especially in\nterms of guarantees regarding protection from linkage to external information.\nThe more recent concept of differential privacy, introduced by the\ncryptographic community, is an approach that provides a rigorous definition of\nprivacy with meaningful privacy guarantees in the presence of arbitrary\nexternal information, although the guarantees may come at a serious price in\nterms of data utility. Building on such notions, Uhler et al. (2013) proposed\nnew methods to release aggregate GWAS data without compromising an individual's\nprivacy. We extend the methods developed in Uhler et al. (2013) for releasing\ndifferentially-private $\\chi^2$-statistics by allowing for arbitrary number of\ncases and controls, and for releasing differentially-private allelic test\nstatistics. We also provide a new interpretation by assuming the controls' data\nare known, which is a realistic assumption because some GWAS use publicly\navailable data as controls. We assess the performance of the proposed methods\nthrough a risk-utility analysis on a real data set consisting of DNA samples\ncollected by the Wellcome Trust Case Control Consortium and compare the methods\nwith the differentially-private release mechanism proposed by Johnson and\nShmatikov (2013).\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2014 06:34:32 GMT"}], "update_date": "2014-02-10", "authors_parsed": [["Yu", "Fei", ""], ["Fienberg", "Stephen E.", ""], ["Slavkovi\u0107", "Aleksandra", ""], ["Uhler", "Caroline", ""]]}, {"id": "1401.5343", "submitter": "Damien McParland", "authors": "Damien McParland, Isobel Claire Gormley, Tyler H. McCormick, Samuel J.\n  Clark, Chodziwadziwa Whiteson Kabudula, Mark A. Collinson", "title": "Clustering South African households based on their asset status using\n  latent variable models", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS726 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 2, 747-776", "doi": "10.1214/14-AOAS726", "report-no": "IMS-AOAS-AOAS726", "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Agincourt Health and Demographic Surveillance System has since 2001\nconducted a biannual household asset survey in order to quantify household\nsocio-economic status (SES) in a rural population living in northeast South\nAfrica. The survey contains binary, ordinal and nominal items. In the absence\nof income or expenditure data, the SES landscape in the study population is\nexplored and described by clustering the households into homogeneous groups\nbased on their asset status. A model-based approach to clustering the Agincourt\nhouseholds, based on latent variable models, is proposed. In the case of\nmodeling binary or ordinal items, item response theory models are employed. For\nnominal survey items, a factor analysis model, similar in nature to a\nmultinomial probit model, is used. Both model types have an underlying latent\nvariable structure - this similarity is exploited and the models are combined\nto produce a hybrid model capable of handling mixed data types. Further, a\nmixture of the hybrid models is considered to provide clustering capabilities\nwithin the context of mixed binary, ordinal and nominal response data. The\nproposed model is termed a mixture of factor analyzers for mixed data (MFA-MD).\nThe MFA-MD model is applied to the survey data to cluster the Agincourt\nhouseholds into homogeneous groups. The model is estimated within the Bayesian\nparadigm, using a Markov chain Monte Carlo algorithm. Intuitive groupings\nresult, providing insight to the different socio-economic strata within the\nAgincourt region.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2014 15:23:37 GMT"}, {"version": "v2", "created": "Thu, 31 Jul 2014 12:20:19 GMT"}], "update_date": "2014-08-01", "authors_parsed": [["McParland", "Damien", ""], ["Gormley", "Isobel Claire", ""], ["McCormick", "Tyler H.", ""], ["Clark", "Samuel J.", ""], ["Kabudula", "Chodziwadziwa Whiteson", ""], ["Collinson", "Mark A.", ""]]}, {"id": "1401.5375", "submitter": "Marco Iglesias", "authors": "Marco A. Iglesias", "title": "Iterative regularization for ensemble data assimilation in reservoir\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the application of iterative regularization for the development of\nensemble methods for solving Bayesian inverse problems. In concrete, we\nconstruct (i) a variational iterative regularizing ensemble Levenberg-Marquardt\nmethod (IR-enLM) and (ii) a derivative-free iterative ensemble Kalman smoother\n(IR-ES). The aim of these methods is to provide a robust ensemble approximation\nof the Bayesian posterior. The proposed methods are based on fundamental ideas\nfrom iterative regularization methods that have been widely used for the\nsolution of deterministic inverse problems [21]. In this work we are interested\nin the application of the proposed ensemble methods for the solution of\nBayesian inverse problems that arise in reservoir modeling applications. The\nproposed ensemble methods use key aspects of the regularizing\nLevenberg-Marquardt scheme developed by Hanke [16] and that we recently applied\nfor history matching in [18].\n  In the case where the forward operator is linear and the prior is Gaussian,\nwe show that the proposed IR-enLM and IR-ES coincide with standard randomized\nmaximum likelihood (RML) and the ensemble smoother (ES) respectively. For the\ngeneral nonlinear case, we develop a numerical framework to assess the\nperformance of the proposed ensemble methods at capturing the posterior. This\nframework consists of using a state-of-the art MCMC method for resolving the\nBayesian posterior from synthetic experiments. The resolved posterior via MCMC\nthen provides a gold standard against to which compare the proposed IR-enLM and\nIR-ES. We show that for the careful selection of regularization parameters,\nrobust approximations of the posterior can be accomplished in terms of mean and\nvariance. Our numerical experiments showcase the advantage of using iterative\nregularization for obtaining more robust and stable approximation of the\nposterior than standard unregularized methods.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2014 16:31:49 GMT"}, {"version": "v2", "created": "Tue, 24 Jun 2014 10:05:02 GMT"}], "update_date": "2014-06-25", "authors_parsed": [["Iglesias", "Marco A.", ""]]}, {"id": "1401.5547", "submitter": "Zhengyi Zhou", "authors": "Zhengyi Zhou, David S. Matteson, Dawn B. Woodard, Shane G. Henderson\n  and Athanasios C. Micheas", "title": "A Spatio-Temporal Point Process Model for Ambulance Demand", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ambulance demand estimation at fine time and location scales is critical for\nfleet management and dynamic deployment. We are motivated by the problem of\nestimating the spatial distribution of ambulance demand in Toronto, Canada, as\nit changes over discrete 2-hour intervals. This large-scale dataset is sparse\nat the desired temporal resolutions and exhibits location-specific serial\ndependence, daily and weekly seasonality. We address these challenges by\nintroducing a novel characterization of time-varying Gaussian mixture models.\nWe fix the mixture component distributions across all time periods to overcome\ndata sparsity and accurately describe Toronto's spatial structure, while\nrepresenting the complex spatio-temporal dynamics through time-varying mixture\nweights. We constrain the mixture weights to capture weekly seasonality, and\napply a conditionally autoregressive prior on the mixture weights of each\ncomponent to represent location-specific short-term serial dependence and daily\nseasonality. While estimation may be performed using a fixed number of mixture\ncomponents, we also extend to estimate the number of components using\nbirth-and-death Markov chain Monte Carlo. The proposed model is shown to give\nhigher statistical predictive accuracy and to reduce the error in predicting\nEMS operational performance by as much as two-thirds compared to a typical\nindustry practice.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2014 03:29:08 GMT"}, {"version": "v2", "created": "Tue, 27 May 2014 21:49:17 GMT"}], "update_date": "2014-05-29", "authors_parsed": [["Zhou", "Zhengyi", ""], ["Matteson", "David S.", ""], ["Woodard", "Dawn B.", ""], ["Henderson", "Shane G.", ""], ["Micheas", "Athanasios C.", ""]]}, {"id": "1401.5613", "submitter": "Krzysztof Szajowski", "authors": "A. Ochman-Gozdek, W. Sarnowski and K.J. Szajowski", "title": "A precision of the sequential change point detection", "comments": "8 pages. The research has been supported by grant S30103/I-18. This\n  paper was presented in part at 59th ISI World Statistics Congress 25-30\n  August 2013, Hong Kong Special Administrative Region, China in the session\n  CPS018", "journal-ref": "Applicationes Mathematicae. 2017, vol. 44, nr 2, s. 267-280", "doi": "10.4064/am2278-5-2017", "report-no": null, "categories": "math.ST math.PR stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A random sequence having two segments being the homogeneous Markov processes\nis registered. Each segment has his own transition probability law and the\nlength of the segment is unknown and random. The transition probabilities of\neach process are known and a priori distribution of the disorder moment is\ngiven. The decision maker aim is to detect the moment of the transition\nprobabilities change. The detection of the disorder rarely is precise. The\ndecision maker accepts some deviation in estimation of the disorder moment. In\nthe considered model the aim is to indicate the change point with fixed,\nbounded error with maximal probability. The case with various precision for\nover and under estimation of this point is analysed. The case when the disorder\ndoes not appears with positive probability is also included. The results\ninsignificantly extends range of application, explain the structure of optimal\ndetector in various circumstances and shows new details of the solution\nconstruction. The motivation for this investigation is the modelling of the\nattacks in the node of networks. The objectives is to detect one of the attack\nimmediately or in very short time before or after it appearance with highest\nprobability. The problem is reformulated to optimal stopping of the observed\nsequences. The detailed analysis of the problem is presented to show the form\nof optimal decision function.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2014 10:33:27 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Ochman-Gozdek", "A.", ""], ["Sarnowski", "W.", ""], ["Szajowski", "K. J.", ""]]}, {"id": "1401.5925", "submitter": "Raj Thilak Rajan", "authors": "Raj Thilak Rajan and Geert Leus and Alle-Jan van der Veen", "title": "Joint relative position and velocity estimation for an anchorless\n  network of mobile nodes", "comments": "In submission", "journal-ref": null, "doi": "10.1016/j.sigpro.2015.02.023", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Localization is a fundamental challenge for any wireless network of nodes, in\nparticular when the nodes are mobile. We present an extension of the classical\nMultidimensional scaling (MDS) for an anchorless network of mobile nodes,\nwherein the solutions to the time-varying relative node positions are shown to\nlie in the derivatives of the time-varying inter-nodal pairwise distances.\nMoreover, we show that the relative position of a mobile node at each time\ninstance is only dependent on the initial relative position, relative velocity\nand a common rotation matrix of the respective node, which are estimated using\nMDS-like and least squares estimators. Simulations are conducted to evaluate\nthe performance of the proposed solutions and the results are presented.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2014 10:09:20 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Rajan", "Raj Thilak", ""], ["Leus", "Geert", ""], ["van der Veen", "Alle-Jan", ""]]}, {"id": "1401.5931", "submitter": "Raj Thilak Rajan", "authors": "Raj Thilak Rajan and Alle-Jan van der Veen", "title": "Joint non-linear ranging and affine synchronization basis for a network\n  of mobile nodes", "comments": null, "journal-ref": "EURASIP EUSIPCO, September 9-13, 2013, Marrakech, Morocco", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Synchronization and localization are critical challenges for the coherent\nfunctioning of a wireless network of mobile nodes. In this paper, a novel joint\nnon-linear range and affine time model is presented based on two way time stamp\nexchanges, extending an existing affine time-range model. For a pair of nodes,\na closed form pairwise least squares solution is proposed for estimating\npairwise range parameters, namely relative range, range rate and rate of range\nrate between the nodes, in addition to estimating the clock skews and the clock\noffsets. Extending these pair wise solutions to network wide ranging and clock\nsynchronization, we present a central data fusion based global least squares\nsolution. Furthermore, a new Constrained Cramer Rao Bound (CCRB) is derived for\nthe joint time-range model and the proposed algorithms are shown to approach\nthe theoretical limits.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2014 10:43:03 GMT"}], "update_date": "2014-01-24", "authors_parsed": [["Rajan", "Raj Thilak", ""], ["van der Veen", "Alle-Jan", ""]]}, {"id": "1401.6002", "submitter": "Cyril Voyant", "authors": "Cyril Voyant (SPE), Gilles Notton (SPE), Christophe Paoli (SPE), Marie\n  Laure Nivet (SPE), Marc Muselli (SPE), Kahina Dahmani (LRIA)", "title": "Numerical weather prediction or stochastic modeling: an objective\n  criterion of choice for the global radiation forecasting", "comments": "International Journal of Energy Technology and Policy (2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous methods exist and were developed for global radiation forecasting.\nThe two most popular types are the numerical weather predictions (NWP) and the\npredictions using stochastic approaches. We propose to compute a parameter\nnoted constructed in part from the mutual information which is a quantity that\nmeasures the mutual dependence of two variables. Both of these are calculated\nwith the objective to establish the more relevant method between NWP and\nstochastic models concerning the current problem.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2014 19:33:20 GMT"}], "update_date": "2014-01-24", "authors_parsed": [["Voyant", "Cyril", "", "SPE"], ["Notton", "Gilles", "", "SPE"], ["Paoli", "Christophe", "", "SPE"], ["Nivet", "Marie Laure", "", "SPE"], ["Muselli", "Marc", "", "SPE"], ["Dahmani", "Kahina", "", "LRIA"]]}, {"id": "1401.6410", "submitter": "Christian Steinruecken", "authors": "Christian Steinruecken", "title": "Compressing Sets and Multisets of Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes lossless compression algorithms for multisets of\nsequences, taking advantage of the multiset's unordered structure. Multisets\nare a generalisation of sets where members are allowed to occur multiple times.\nA multiset can be encoded na\\\"ively by simply storing its elements in some\nsequential order, but then information is wasted on the ordering. We propose a\ntechnique that transforms the multiset into an order-invariant tree\nrepresentation, and derive an arithmetic code that optimally compresses the\ntree. Our method achieves compression even if the sequences in the multiset are\nindividually incompressible (such as cryptographic hash sums). The algorithm is\ndemonstrated practically by compressing collections of SHA-1 hash sums, and\nmultisets of arbitrary, individually encodable objects.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2014 17:36:32 GMT"}], "update_date": "2014-01-27", "authors_parsed": [["Steinruecken", "Christian", ""]]}, {"id": "1401.6449", "submitter": "Viet Chi Tran", "authors": "St\\'ephan Cl\\'emen\\c{c}on, Hector De Arazoza (MATCOM), Fabrice Rossi,\n  Viet Chi Tran", "title": "A statistical network analysis of the HIV/AIDS epidemics in Cuba", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Cuban contact-tracing detection system set up in 1986 allowed the\nreconstruction and analysis of the sexual network underlying the epidemic\n(5,389 vertices and 4,073 edges, giant component of 2,386 nodes and 3,168\nedges), shedding light onto the spread of HIV and the role of contact-tracing.\nClustering based on modularity optimization provides a better visualization and\nunderstanding of the network, in combination with the study of covariates. The\ngraph has a globally low but heterogeneous density, with clusters of high\nintraconnectivity but low interconnectivity. Though descriptive, our results\npave the way for incorporating structure when studying stochastic SIR epidemics\nspreading on social networks.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2014 20:41:20 GMT"}, {"version": "v2", "created": "Mon, 17 Mar 2014 18:34:07 GMT"}, {"version": "v3", "created": "Thu, 21 May 2015 09:34:01 GMT"}, {"version": "v4", "created": "Fri, 22 May 2015 08:26:56 GMT"}], "update_date": "2015-05-25", "authors_parsed": [["Cl\u00e9men\u00e7on", "St\u00e9phan", "", "MATCOM"], ["De Arazoza", "Hector", "", "MATCOM"], ["Rossi", "Fabrice", ""], ["Tran", "Viet Chi", ""]]}, {"id": "1401.6595", "submitter": "Leila Wehbe", "authors": "Leila Wehbe, Aaditya Ramdas, Rebecca C. Steorts, Cosma Rohilla Shalizi", "title": "Regularized brain reading with shrinkage and smoothing", "comments": "Published at http://dx.doi.org/10.1214/15-AOAS837 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 4, 1997-2022", "doi": "10.1214/15-AOAS837", "report-no": "IMS-AOAS-AOAS837", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional neuroimaging measures how the brain responds to complex stimuli.\nHowever, sample sizes are modest, noise is substantial, and stimuli are high\ndimensional. Hence, direct estimates are inherently imprecise and call for\nregularization. We compare a suite of approaches which regularize via\nshrinkage: ridge regression, the elastic net (a generalization of ridge\nregression and the lasso), and a hierarchical Bayesian model based on small\narea estimation (SAE). We contrast regularization with spatial smoothing and\ncombinations of smoothing and shrinkage. All methods are tested on functional\nmagnetic resonance imaging (fMRI) data from multiple subjects participating in\ntwo different experiments related to reading, for both predicting neural\nresponse to stimuli and decoding stimuli from responses. Interestingly, when\nthe regularization parameters are chosen by cross-validation independently for\nevery voxel, low/high regularization is chosen in voxels where the\nclassification accuracy is high/low, indicating that the regularization\nintensity is a good tool for identification of relevant voxels for the\ncognitive task. Surprisingly, all the regularization methods work about equally\nwell, suggesting that beating basic smoothing and shrinkage will take not only\nclever methods, but also careful modeling.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jan 2014 23:40:17 GMT"}, {"version": "v2", "created": "Tue, 30 Dec 2014 17:02:52 GMT"}, {"version": "v3", "created": "Thu, 4 Feb 2016 14:36:31 GMT"}], "update_date": "2016-02-05", "authors_parsed": [["Wehbe", "Leila", ""], ["Ramdas", "Aaditya", ""], ["Steorts", "Rebecca C.", ""], ["Shalizi", "Cosma Rohilla", ""]]}, {"id": "1401.6640", "submitter": "Richard D. Gill", "authors": "Ian Freckelton, Richard D. Gill, and Johannes F. Nijboer", "title": "Forensic Statistics and Justice: the Leiden Consensus", "comments": "Based on discussions (work-groups, round-table, etc) at conference\n  \"Science meets justice\", Lorentz Center, Leiden, 26-29 April, 2011. This\n  version dated 5 March, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Between 26 and 29 April 2011 a four-day interdisciplinary workshop \"Science\nmeets Law\" was held at the Lorentz Center (University of Leiden, NL). It was\norganised by the Lorentz Center in collaboration with the Netherlands Institute\nfor Advanced Study in the Humanities and Social Sciences (\"NIAS\", Wassenaar,\nNL). The programme was prepared by Richard Gill and Johannes F. Nijboer and\nconnected to Richard Gill's 2010-2011 stay at NIAS as Distinguished Lorentz\nFellow.\n  This document was prepared by three of us and represents our personal\nsynthesis of the discussions which took place during the meeting. Comments are\nwelcome.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jan 2014 11:16:00 GMT"}], "update_date": "2014-01-28", "authors_parsed": [["Freckelton", "Ian", ""], ["Gill", "Richard D.", ""], ["Nijboer", "Johannes F.", ""]]}, {"id": "1401.7088", "submitter": "Monowar Hasan", "authors": "Hina Tabassum, Uzma Siddique, Ekram Hossain, and Md. Jahangir Hossain", "title": "Cellular Downlink Performance with Base Station Sleeping, User\n  Association, and Scheduling", "comments": "Submitted to IEEE Transactions on Wireless Communications", "journal-ref": null, "doi": "10.1109/TWC.2014.2336249", "report-no": null, "categories": "cs.NI cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Base station (BS) sleeping has emerged as a viable solution to enhance the\noverall network energy efficiency by inactivating the underutilized BSs.\nHowever, it affects the performance of users in sleeping cells depending on\ntheir BS association criteria, their channel conditions towards the active BSs,\nand scheduling criteria and traffic loads at the active BSs. This paper\ncharacterizes the performance of cellular systems with BS sleeping by\ndeveloping a systematic framework to derive the spectral efficiency and outage\nprobability of downlink transmission to the sleeping cell users taking into\naccount the aforementioned factors. In this context, we develop a user\nassociation scheme in which a typical user in a sleeping cell selects a BS with\n\\textbf{M}aximum best-case \\textbf{M}ean channel \\textbf{A}ccess\n\\textbf{P}robability (MMAP) which is calculated by all active BSs based on\ntheir existing traffic loads. We consider both greedy and round-robin schemes\nat active BSs for scheduling users in a channel. Once the association is\nperformed, the exact access probability for a typical sleeping cell user and\nthe statistics of its received signal and interference powers are derived to\nevaluate the spectral and energy efficiencies of transmission. For the sleeping\ncell users, we also consider the conventional \\textbf{M}aximum\n\\textbf{R}eceived \\textbf{S}ignal \\textbf{P}ower (MRSP)-based user association\nscheme along with greedy and round-robin schemes at the BSs. The impact of\ncell-zooming is incorporated in the derivations to analyze its feasibility in\nreducing the coverage holes created by BS sleeping. Numerical results show the\ntrade-offs between spectral efficiency and energy efficiency in various network\nscenarios. The accuracy of the analysis is verified through Monte-Carlo\nsimulations.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2014 05:39:31 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Tabassum", "Hina", ""], ["Siddique", "Uzma", ""], ["Hossain", "Ekram", ""], ["Hossain", "Md. Jahangir", ""]]}, {"id": "1401.7359", "submitter": "Peng Shi", "authors": "Parag A. Pathak and Peng Shi", "title": "Demand Modeling, Forecasting, and Counterfactuals, Part I", "comments": "Also available as NBER Working Paper No. 19859", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are relatively few systematic comparisons of the ex ante counterfactual\npredictions from structural models to what occurs ex post. This paper uses a\nlarge-scale policy change in Boston in 2014 to investigate the performance of\ndiscrete choice models of demand compared to simpler alternatives. In 2013,\nBoston Public Schools (BPS) proposed alternative zone configurations in their\nschool choice plan, each of which alters the set of schools participants are\nallowed to rank. Pathak Shi (2013) estimated discrete choice models of demand\nusing families' historical choices and these demand models were used to\nforecast the outcomes under alternative plans. BPS, the school committee, and\nthe public used these forecasts to compare alternatives and eventually adopt a\nnew plan for Spring 2014. This paper updates the forecasts using the most\nrecently available historical data on participants' submitted preferences and\nalso makes forecasts based on an alternative statistical model not based a\nrandom utility foundation. We describe our analysis plan, the methodology, and\nthe target forecast outcomes. Our ex ante forecasts eliminate any scope for\npost-analysis bias because they are made before new preferences are submitted.\nPart II will use newly submitted preference data to evaluate these forecasts\nand assess the strengths and limitations of discrete choice models of demand in\nour context.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2014 22:42:06 GMT"}, {"version": "v2", "created": "Thu, 1 Jan 2015 00:58:31 GMT"}, {"version": "v3", "created": "Mon, 12 Jan 2015 19:23:55 GMT"}, {"version": "v4", "created": "Wed, 14 Jan 2015 15:53:23 GMT"}], "update_date": "2015-01-15", "authors_parsed": [["Pathak", "Parag A.", ""], ["Shi", "Peng", ""]]}, {"id": "1401.7953", "submitter": "Deniz Akdemir", "authors": "Deniz Akdemir", "title": "Training population selection for (breeding value) prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training population selection for genomic selection has captured a great deal\nof interest in animal and plant breeding. In this article, we derive a\ncomputationally efficient statistic to measure the reliability of estimates of\ngenetic breeding values for a fixed set of genotypes based on a given training\nset of genotypes and phenotypes. We adopt a genetic algorithm scheme to find a\ntraining set of certain size from a larger set of candidate genotypes that\noptimizes this reliability measure. Our results show that, compared to a random\nsample of the same size, phenotyping individuals selected by our method results\nin models with better accuracies. We implement the proposed training selection\nmethodology on four data sets, namely, the arabidopsis, wheat, rice and the\nmaize data sets. Our results indicate that dynamic model building process which\nuses genotypes of the individuals in the test sample into account while\nselecting the training individuals improves the performance of GS models.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2014 19:03:30 GMT"}, {"version": "v2", "created": "Wed, 21 May 2014 19:07:14 GMT"}], "update_date": "2014-05-22", "authors_parsed": [["Akdemir", "Deniz", ""]]}, {"id": "1401.8115", "submitter": "Amelia Sim\\'o", "authors": "M. \\'Angeles Gallego, M. Victoria Ib\\'a\\~nez and Amelia Sim\\'o", "title": "Inhomogeneous K-function for germ-grain models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a generalization to germ-grain models of the\ninhomogeneous K-function of Point Processes. We apply them to a sample of\nimages of peripheral blood smears obtained from patients with Sickle Cell\nDisease, in order to decide whether the sample belongs to the thin, thick or\nmorphological region.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2014 10:25:44 GMT"}], "update_date": "2014-02-03", "authors_parsed": [["Gallego", "M. \u00c1ngeles", ""], ["Ib\u00e1\u00f1ez", "M. Victoria", ""], ["Sim\u00f3", "Amelia", ""]]}, {"id": "1401.8274", "submitter": "Mikael Kuusela", "authors": "Mikael Kuusela and Victor M. Panaretos", "title": "Empirical Bayes unfolding of elementary particle spectra at the Large\n  Hadron Collider", "comments": "40 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP hep-ex physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the so-called unfolding problem in experimental high energy\nphysics, where the goal is to estimate the true spectrum of elementary\nparticles given observations distorted by measurement error due to the limited\nresolution of a particle detector. This an important statistical inverse\nproblem arising in the analysis of data at the Large Hadron Collider at CERN.\nMathematically, the problem is formalized as one of estimating the intensity\nfunction of an indirectly observed Poisson point process. Particle physicists\nare particularly keen on unfolding methods that feature a principled way of\nchoosing the regularization strength and allow for the quantification of the\nuncertainty inherent in the solution. Though there are many approaches that\nhave been considered by experimental physicists, it can be argued that few --\nif any -- of these deal with these two key issues in a satisfactory manner. In\nthis paper, we propose to attack the unfolding problem within the framework of\nempirical Bayes estimation: we consider Bayes estimators of the coefficients of\na basis expansion of the unknown intensity, using a regularizing prior; and\nemploy a Monte Carlo expectation-maximization algorithm to find the marginal\nmaximum likelihood estimate of the hyperparameter controlling the strength of\nthe regularization. Due to the data-driven choice of the hyperparameter,\ncredible intervals derived using the empirical Bayes posterior lose their\nsubjective Bayesian interpretation. Since the properties and meaning of such\nintervals are poorly understood, we explore instead the use of bootstrap\nresampling for constructing purely frequentist confidence bands for the true\nintensity. The performance of the proposed methodology is demonstrated using\nboth simulations and real data from the Large Hadron Collider.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2014 20:01:23 GMT"}], "update_date": "2014-02-03", "authors_parsed": [["Kuusela", "Mikael", ""], ["Panaretos", "Victor M.", ""]]}]