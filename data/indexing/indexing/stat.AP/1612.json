[{"id": "1612.00083", "submitter": "Christian Carmona", "authors": "Christian Carmona, Luis Nieto-Barajas and Antonio Canale", "title": "Model based approach for household clustering with mixed scale variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Ministry of Social Development in Mexico is in charge of creating and\nassigning social programmes targeting specific needs in the population for the\nimprovement of quality of life. To better target the social programmes, the\nMinistry is aimed to find clusters of households with the same needs based on\ndemographic characteristics as well as poverty conditions of the household.\nAvailable data consists of continuous, ordinal, and nominal variables and the\nobservations are not iid but come from a survey sample based on a complex\ndesign. We propose a Bayesian nonparametric mixture model that jointly models\nthis mixed scale data and accommodates for the different sampling\nprobabilities. The performance of the model is assessed via simulated data. A\nfull analysis of socio-economic conditions in households in the State of Mexico\nis presented.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 00:01:01 GMT"}, {"version": "v2", "created": "Thu, 23 Nov 2017 16:19:22 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Carmona", "Christian", ""], ["Nieto-Barajas", "Luis", ""], ["Canale", "Antonio", ""]]}, {"id": "1612.00129", "submitter": "Hyemin Han", "authors": "Hyemin Han, Kangwook Lee and Firat Soylu", "title": "Predicting Long-term Outcomes of Educational Interventions Using the\n  Evolutionary Causal Matrices and Markov Chain Based on Educational\n  Neuroscience", "comments": null, "journal-ref": "Trends.Neurosci.Educ. 5 (2016) 157-165", "doi": "10.1016/j.tine.2016.11.003", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We developed a prediction model based on the evolutionary causal matrices\n(ECM) and the Markov Chain to predict long-term influences of educational\ninterventions on adolescents development. Particularly, we created a\ncomputational model predicting longitudinal influences of different types of\nstories of moral exemplars on adolescents voluntary service participation. We\ntested whether the developed prediction model can properly predict a long-term\nlongitudinal trend of change in voluntary service participation rate by\ncomparing prediction results and surveyed data. Furthermore, we examined which\ntype of intervention would most effectively promote service engagement and what\nis the minimum required frequency of intervention to produce a large effect. We\ndiscussed the implications of the developed prediction model in educational\ninterventions based on educational neuroscience.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 03:32:13 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Han", "Hyemin", ""], ["Lee", "Kangwook", ""], ["Soylu", "Firat", ""]]}, {"id": "1612.00388", "submitter": "Wesley Tansey", "authors": "Wesley Tansey and Edward W. Lowe Jr. and James G. Scott", "title": "Diet2Vec: Multi-scale analysis of massive dietary data", "comments": "Accepted to the NIPS 2016 Workshop on Machine Learning for Health", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart phone apps that enable users to easily track their diets have become\nwidespread in the last decade. This has created an opportunity to discover new\ninsights into obesity and weight loss by analyzing the eating habits of the\nusers of such apps. In this paper, we present diet2vec: an approach to modeling\nlatent structure in a massive database of electronic diet journals. Through an\niterative contract-and-expand process, our model learns real-valued embeddings\nof users' diets, as well as embeddings for individual foods and meals. We\ndemonstrate the effectiveness of our approach on a real dataset of 55K users of\nthe popular diet-tracking app LoseIt\\footnote{http://www.loseit.com/}. To the\nbest of our knowledge, this is the largest fine-grained diet tracking study in\nthe history of nutrition and obesity research. Our results suggest that\ndiet2vec finds interpretable results at all levels, discovering intuitive\nrepresentations of foods, meals, and diets.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 19:21:22 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Tansey", "Wesley", ""], ["Lowe", "Edward W.", "Jr."], ["Scott", "James G.", ""]]}, {"id": "1612.00497", "submitter": "Kris Sankaran", "authors": "Kris Sankaran, Suzanne Tamang, Ami Bhatt", "title": "Opioid Atlas: Mapping Access to Pain Medication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Opiates are some of the most effective pain relief medications available for\npatients suffering from cancer and surgery-related pain. Despite the\naffordability and effectiveness of these medications, access to opiates is\nhighly geographically variable. Pain researchers have attributed geographic\nvariation to various factors including the fear of opioid addiction, diversion\nof legal opiods to the underground market and pharmaceutical industry\ninfluences. However, the extent to which there is inequity in untreated cancer\nand surgery-related pain is unknown. To help opioid investigators study these\nquestions, we designed a tool, the Opioid Atlas, for exploring data on legal\nopioid consumption, by country and time, collected by the International\nNarcotics Control Board. Our design borrows ideas from the data visualization\nand multivariate statistics communities, especially the principles of linking\nand dimensionality reduction. Our work is relevant to policymakers and pain\nresearchers who wish to systematically assess country-level factors that\ncontribute to differences in opioid access for patients with cancer and\nsurgery-related pain. The Opioid Atlas, and the code behind it, is freely\navailable with an open source license.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 22:17:28 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Sankaran", "Kris", ""], ["Tamang", "Suzanne", ""], ["Bhatt", "Ami", ""]]}, {"id": "1612.00503", "submitter": "Art Owen", "authors": "Art B. Owen and Tristan Launay", "title": "Multibrand geographic experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a geographic experiment to measure advertising effectiveness, some regions\n(hereafter GEOs) get increased advertising while others do not. This paper\nlooks at running $B>1$ such experiments simultaneously on $B$ different brands\nin $G$ GEOs, and then using shrinkage methods to estimate returns to\nadvertising. There are important practical gains from doing this. Data from any\none brand helps to estimate the return of all other brands. We see this in both\na frequentist and Bayesian formulation. As a result, each individual experiment\ncould be made smaller and less expensive when they are analyzed together. We\nalso provide an experimental design for multibrand experiments where half of\nthe brands have increased spend in each GEO while half of the GEOs have\nincreased spend for each brand. For $G>B$ the design is a two level factorial\nfor each brand and simultaneously a supersaturated design for the GEOs.\nMultiple simultaneous experiments also allow one to identify GEOs in which\nadvertising is generally more effective. That cannot be done in the single\nbrand experiments we consider.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 22:50:33 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Owen", "Art B.", ""], ["Launay", "Tristan", ""]]}, {"id": "1612.00520", "submitter": "David Kohn", "authors": "David Kohn, Sally Cripps, Nick Glozier, Hugh Durrant-Whyte", "title": "A Bayesian Approach to Predicting Disengaged Youth", "comments": "5 pages, 2 figures, 2 tables, NIPS 2016 Workshop on Machine Learning\n  for Health", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a Bayesian approach for predicting and identifying the\nfactors which most influence an individual's propensity to fall into the\ncategory of Not in Employment Education or Training (NEET). The approach\npartitions the covariates into two groups: those which have the potential to be\nchanged as a result of an intervention strategy and those which must be\ncontrolled for. This partition allows us to develop models and identify\nimportant factors conditional on the control covariates, which is useful for\nclinicians and policy makers who wish to identify potential intervention\nstrategies. Using the data obtained by O'Dea (2014) we compare the results from\nthis approach with the results from O'Dea (2014) and with the results obtained\nusing the Bayesian variable selection procedure of Lamnisos (2009) when the\ncovariates are not partitioned. We find that the relative importance of\npredictive factors varies greatly depending upon the control covariates. This\nhas enormous implications when deciding on what interventions are most useful\nto prevent young people from being NEET.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 00:02:34 GMT"}, {"version": "v2", "created": "Wed, 7 Dec 2016 12:17:27 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Kohn", "David", ""], ["Cripps", "Sally", ""], ["Glozier", "Nick", ""], ["Durrant-Whyte", "Hugh", ""]]}, {"id": "1612.00585", "submitter": "Soumi Chaki", "authors": "Soumi Chaki, Aurobinda Routray, William K. Mohanty, Mamata Jenamani", "title": "Development of a hybrid learning system based on SVM, ANFIS and domain\n  knowledge: DKFIS", "comments": "6 pages, 5 figures, 3tables Presented at Indicon 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the development of a hybrid learning system based on\nSupport Vector Machines (SVM), Adaptive Neuro-Fuzzy Inference System (ANFIS)\nand domain knowledge to solve prediction problem. The proposed two-stage Domain\nKnowledge based Fuzzy Information System (DKFIS) improves the prediction\naccuracy attained by ANFIS alone. The proposed framework has been implemented\non a noisy and incomplete dataset acquired from a hydrocarbon field located at\nwestern part of India. Here, oil saturation has been predicted from four\ndifferent well logs i.e. gamma ray, resistivity, density, and clay volume. In\nthe first stage, depending on zero or near zero and non-zero oil saturation\nlevels the input vector is classified into two classes (Class 0 and Class 1)\nusing SVM. The classification results have been further fine-tuned applying\nexpert knowledge based on the relationship among predictor variables i.e. well\nlogs and target variable - oil saturation. Second, an ANFIS is designed to\npredict non-zero (Class 1) oil saturation values from predictor logs. The\npredicted output has been further refined based on expert knowledge. It is\napparent from the experimental results that the expert intervention with\nqualitative judgment at each stage has rendered the prediction into the\nfeasible and realistic ranges. The performance analysis of the prediction in\nterms of four performance metrics such as correlation coefficient (CC), root\nmean square error (RMSE), and absolute error mean (AEM), scatter index (SI) has\nestablished DKFIS as a useful tool for reservoir characterization.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 07:56:23 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Chaki", "Soumi", ""], ["Routray", "Aurobinda", ""], ["Mohanty", "William K.", ""], ["Jenamani", "Mamata", ""]]}, {"id": "1612.00664", "submitter": "Christoph Kurz", "authors": "Christoph Kurz", "title": "Survival Prediction with Limited Features: a Top Performing Approach\n  from the DREAM ALS Stratification Prize4Life Challenge", "comments": "accepted for NIPS 2016 ML4HC workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Survival prediction with small sets of features is a highly relevant topic\nfor decision-making in clinical practice. I describe a method for predicting\nsurvival of amyotrophic lateral sclerosis (ALS) patients that was developed as\na submission to the DREAM ALS Stratification Prize4Life Challenge held in\nsummer 2015 to find the most accurate prediction of ALS progression and\nsurvival. ALS is a neurodegenerative disease with very heterogeneous survival\ntimes. Based on patient data from two national registries, solvers were asked\nto predict survival for three different time intervals, which was then\nevaluated on undisclosed information from additional data. I describe methods\nused to generate new features from existing ones from longitudinal data,\nselecting the most predictive features, and developing the best survival model.\nI show that easily obtainable engineered features can significantly improve\nprediction and could be incorporated into clinical practice. Furthermore, my\nprediction model confirms previous reports suggesting that past disease\nprogression measured by the ALSFRS (ALS functional rating scale score), time\nsince disease onset, onset site, and age are strong predictors for survival.\nRegarding prediction accuracy, this approach ranked second.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 12:54:20 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Kurz", "Christoph", ""]]}, {"id": "1612.00667", "submitter": "Santi Puch", "authors": "Santi Puch, Asier Aduriz, Adri\\`a Casamitjana, Veronica Vilaplana,\n  Paula Petrone, Gr\\'egory Operto, Raffaele Cacciaglia, Stavros Skouras, Carles\n  Falcon, Jos\\'e Luis Molinuevo, Juan Domingo Gispert", "title": "Voxelwise nonlinear regression toolbox for neuroimage analysis:\n  Application to aging and neurodegenerative disease modeling", "comments": "4 pages + 1 page for acknowledgements and references. NIPS 2016\n  Workshop on Machine Learning for Health (NIPS ML4HC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a new neuroimaging analysis toolbox that allows for the\nmodeling of nonlinear effects at the voxel level, overcoming limitations of\nmethods based on linear models like the GLM. We illustrate its features using a\nrelevant example in which distinct nonlinear trajectories of Alzheimer's\ndisease related brain atrophy patterns were found across the full biological\nspectrum of the disease. The open-source toolbox presented in this paper is\navailable at https://github.com/imatge-upc/VNeAT.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 12:59:11 GMT"}, {"version": "v2", "created": "Sun, 5 Mar 2017 10:58:16 GMT"}, {"version": "v3", "created": "Tue, 18 Apr 2017 20:12:16 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Puch", "Santi", ""], ["Aduriz", "Asier", ""], ["Casamitjana", "Adri\u00e0", ""], ["Vilaplana", "Veronica", ""], ["Petrone", "Paula", ""], ["Operto", "Gr\u00e9gory", ""], ["Cacciaglia", "Raffaele", ""], ["Skouras", "Stavros", ""], ["Falcon", "Carles", ""], ["Molinuevo", "Jos\u00e9 Luis", ""], ["Gispert", "Juan Domingo", ""]]}, {"id": "1612.00690", "submitter": "Anders Eklund", "authors": "Anders Eklund, Martin A. Lindquist, Mattias Villani", "title": "A Bayesian Heteroscedastic GLM with Application to fMRI Data with Motion\n  Spikes", "comments": null, "journal-ref": "NeuroImage, Volume 155, 354-369 (2017)", "doi": "10.1016/j.neuroimage.2017.04.069", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a voxel-wise general linear model with autoregressive noise and\nheteroscedastic noise innovations (GLMH) for analyzing functional magnetic\nresonance imaging (fMRI) data. The model is analyzed from a Bayesian\nperspective and has the benefit of automatically down-weighting time points\nclose to motion spikes in a data-driven manner. We develop a highly efficient\nMarkov Chain Monte Carlo (MCMC) algorithm that allows for Bayesian variable\nselection among the regressors to model both the mean (i.e., the design matrix)\nand variance. This makes it possible to include a broad range of explanatory\nvariables in both the mean and variance (e.g., time trends, activation stimuli,\nhead motion parameters and their temporal derivatives), and to compute the\nposterior probability of inclusion from the MCMC output. Variable selection is\nalso applied to the lags in the autoregressive noise process, making it\npossible to infer the lag order from the data simultaneously with all other\nmodel parameters. We use both simulated data and real fMRI data from OpenfMRI\nto illustrate the importance of proper modeling of heteroscedasticity in fMRI\ndata analysis. Our results show that the GLMH tends to detect more brain\nactivity, compared to its homoscedastic counterpart, by allowing the variance\nto change over time depending on the degree of head motion.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 14:36:34 GMT"}, {"version": "v2", "created": "Sat, 11 Mar 2017 13:53:30 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Eklund", "Anders", ""], ["Lindquist", "Martin A.", ""], ["Villani", "Mattias", ""]]}, {"id": "1612.00778", "submitter": "David Bailey", "authors": "David C. Bailey", "title": "Not Normal: the uncertainties of scientific measurements", "comments": "17 pages, 5 figures. Auxiliary Excel file\n  (UncertaintyDataDescription.xls) lists sources of data", "journal-ref": "Royal Society Open Science, 4, 160600 (2017)", "doi": "10.1098/rsos.160600", "report-no": null, "categories": "stat.AP physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Judging the significance and reproducibility of quantitative research\nrequires a good understanding of relevant uncertainties, but it is often\nunclear how well these have been evaluated and what they imply. Reported\nscientific uncertainties were studied by analysing 41000 measurements of 3200\nquantities from medicine, nuclear and particle physics, and interlaboratory\ncomparisons ranging from chemistry to toxicology. Outliers are common, with\n5{\\sigma} disagreements up to five orders of magnitude more frequent than\nnaively expected. Uncertainty-normalized differences between multiple\nmeasurements of the same quantity are consistent with heavy-tailed Student-t\ndistributions that are often almost Cauchy, far from a Gaussian Normal bell\ncurve. Medical research uncertainties are generally as well evaluated as those\nin physics, but physics uncertainty improves more rapidly, making feasible\nsimple significance criteria such as the 5{\\sigma} discovery convention in\nparticle physics. Contributions to measurement uncertainty from mistakes and\nunknown problems are not completely unpredictable. Such errors appear to have\npower-law distributions consistent with how designed complex systems fail, and\nhow unknown systematic errors are constrained by researchers. This better\nunderstanding may help improve analysis and meta-analysis of data, and help\nscientists and the public have more realistic expectations of what scientific\nresults imply.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 18:09:32 GMT"}, {"version": "v2", "created": "Thu, 19 Jan 2017 16:51:36 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Bailey", "David C.", ""]]}, {"id": "1612.00840", "submitter": "Soumi Chaki", "authors": "Soumi Chaki, Aurobinda Routray, William K. Mohanty, Mamata Jenamani", "title": "A novel multiclassSVM based framework to classify lithology from well\n  logs: a real-world application", "comments": "5 pages, 5 figures, 4 tables Presented at INDICON 2015 at New Delhi,\n  India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support vector machines (SVMs) have been recognized as a potential tool for\nsupervised classification analyses in different domains of research. In\nessence, SVM is a binary classifier. Therefore, in case of a multiclass\nproblem, the problem is divided into a series of binary problems which are\nsolved by binary classifiers, and finally the classification results are\ncombined following either the one-against-one or one-against-all strategies. In\nthis paper, an attempt has been made to classify lithology using a multiclass\nSVM based framework using well logs as predictor variables. Here, the lithology\nis classified into four classes such as sand, shaly sand, sandy shale and shale\nbased on the relative values of sand and shale fractions as suggested by an\nexpert geologist. The available dataset consisting well logs (gamma ray,\nneutron porosity, density, and P-sonic) and class information from four closely\nspaced wells from an onshore hydrocarbon field is divided into training and\ntesting sets. We have used one-against-all strategy to combine the results of\nmultiple binary classifiers. The reported results established the superiority\nof multiclass SVM compared to other classifiers in terms of classification\naccuracy. The selection of kernel function and associated parameters has also\nbeen investigated here. It can be envisaged from the results achieved in this\nstudy that the proposed framework based on multiclass SVM can further be used\nto solve classification problems. In future research endeavor, seismic\nattributes can be introduced in the framework to classify the lithology\nthroughout a study area from seismic inputs.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 07:55:16 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Chaki", "Soumi", ""], ["Routray", "Aurobinda", ""], ["Mohanty", "William K.", ""], ["Jenamani", "Mamata", ""]]}, {"id": "1612.01014", "submitter": "Zhengwu Zhang", "authors": "Zhengwu Zhang and Maxime Descoteaux and David B. Dunson", "title": "Nonparametric Bayes Models of Fiber Curves Connecting Brain Regions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In studying structural inter-connections in the human brain, it is common to\nfirst estimate fiber bundles connecting different regions of the brain relying\non diffusion MRI. These fiber bundles act as highways for neural activity and\ncommunication, snaking through the brain and connecting different regions.\nCurrent statistical methods for analyzing these fibers reduce the rich\ninformation into an adjacency matrix, with the elements containing a count of\nthe number of fibers or a mean diffusion feature (such as fractional\nanisotropy) along the fibers. The goal of this article is to avoid discarding\nthe rich functional data on the shape, size and orientation of fibers,\ndeveloping flexible models for characterizing the population distribution of\nfibers between brain regions of interest within and across different\nindividuals. We start by decomposing each fiber in each individual's brain into\na corresponding rotation matrix, shape and translation from a global reference\ncurve. These components can be viewed as data lying on a product space composed\nof different Euclidean spaces and manifolds. To non-parametrically model the\ndistribution within and across individuals, we rely on a hierarchical mixture\nof product kernels specific to the component spaces. Taking a Bayesian approach\nto inference, we develop an efficient method for posterior sampling. The\napproach automatically produces clusters of fibers within and across\nindividuals, and yields interesting new insights into variation in fiber\ncurves, while providing a useful starting point for more elaborate models\nrelating fibers to covariates and neuropsychiatric traits.\n", "versions": [{"version": "v1", "created": "Sat, 3 Dec 2016 20:25:19 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Zhang", "Zhengwu", ""], ["Descoteaux", "Maxime", ""], ["Dunson", "David B.", ""]]}, {"id": "1612.01055", "submitter": "Anna Goldenberg", "authors": "Lauren Erdman, Ekansh Sharma, Eva Unternahrer, Shantala Hari Dass,\n  Kieran ODonnell, Sara Mostafavi, Rachel Edgar, Michael Kobor, Helene\n  Gaudreau, Michael Meaney, Anna Goldenberg", "title": "Modeling trajectories of mental health: challenges and opportunities", "comments": "extended abstract for ML4HC at NIPS 2016, 4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  More than two thirds of mental health problems have their onset during\nchildhood or adolescence. Identifying children at risk for mental illness later\nin life and predicting the type of illness is not easy. We set out to develop a\nplatform to define subtypes of childhood social-emotional development using\nlongitudinal, multifactorial trait-based measures. Subtypes discovered through\nthis study could ultimately advance psychiatric knowledge of the early\nbehavioural signs of mental illness. To this extent we have examined two types\nof models: latent class mixture models and GP-based models. Our findings\nindicate that while GP models come close in accuracy of predicting future\ntrajectories, LCMMs predict the trajectories as well in a fraction of the time.\nUnfortunately, neither of the models are currently accurate enough to lead to\nimmediate clinical impact. The available data related to the development of\nchildhood mental health is often sparse with only a few time points measured\nand require novel methods with improved efficiency and accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 03:20:54 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Erdman", "Lauren", ""], ["Sharma", "Ekansh", ""], ["Unternahrer", "Eva", ""], ["Dass", "Shantala Hari", ""], ["ODonnell", "Kieran", ""], ["Mostafavi", "Sara", ""], ["Edgar", "Rachel", ""], ["Kobor", "Michael", ""], ["Gaudreau", "Helene", ""], ["Meaney", "Michael", ""], ["Goldenberg", "Anna", ""]]}, {"id": "1612.01089", "submitter": "Xing He", "authors": "Zenan Ling, Robert C. Qiu, Xing He and Chu Lei", "title": "A Novel Approach for Big Data Analytics in Future Grids Based on Free\n  Probability", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on the random matrix model, we can build statistical models using\nmassive datasets across the power grid, and employ hypothesis testing for\nanomaly detection. First, the aim of this paper is to make the first attempt to\napply the recent free probability result in extracting big data analytics, in\nparticular data fusion. The nature of this work is basic in that new algorithms\nand analytics tools are proposed to pave the way for the future's research.\nSecond, using the new analytic tool, we are able to make some discovery related\nto anomaly detection that is very difficult for other approaches. To our best\nknowledge, there is no similar report in the literature. Third, both linear and\nnonlinear polynomials of large random matrices can be handled in this new\nframework. Simulations demonstrate the following: Compared with the linearity,\nnonlinearity is more flexible in problem modeling and closer to the nature of\nthe reality. In some sense, some other nonlinear matrix polynomials may be more\neffective for the power grid\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 09:16:50 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Ling", "Zenan", ""], ["Qiu", "Robert C.", ""], ["He", "Xing", ""], ["Lei", "Chu", ""]]}, {"id": "1612.01316", "submitter": "Konstantinos Sechidis", "authors": "Konstantinos Sechidis, Emily Turner, Paul D. Metcalfe, James\n  Weatherall and Gavin Brown", "title": "Ranking Biomarkers Through Mutual Information", "comments": "Accepted at NIPS 2016 Workshop on Machine Learning for Health", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study information theoretic methods for ranking biomarkers. In clinical\ntrials there are two, closely related, types of biomarkers: predictive and\nprognostic, and disentangling them is a key challenge. Our first step is to\nphrase biomarker ranking in terms of optimizing an information theoretic\nquantity. This formalization of the problem will enable us to derive rankings\nof predictive/prognostic biomarkers, by estimating different, high dimensional,\nconditional mutual information terms. To estimate these terms, we suggest\nefficient low dimensional approximations, and we derive an empirical Bayes\nestimator, which is suitable for small or sparse datasets. Finally, we\nintroduce a new visualisation tool that captures the prognostic and the\npredictive strength of a set of biomarkers. We believe this representation will\nprove to be a powerful tool in biomarker discovery.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 11:44:32 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Sechidis", "Konstantinos", ""], ["Turner", "Emily", ""], ["Metcalfe", "Paul D.", ""], ["Weatherall", "James", ""], ["Brown", "Gavin", ""]]}, {"id": "1612.01349", "submitter": "Soumi Chaki", "authors": "Soumi Chaki, Akhilesh Kumar Verma, Aurobinda Routray, William K.\n  Mohanty, Mamata Jenamani", "title": "A One class Classifier based Framework using SVDD : Application to an\n  Imbalanced Geological Dataset", "comments": "presented at IEEE Students Technology Symposium (TechSym), 28\n  February to 2 March 2014, IIT Kharagpur, India. 6 pages, 7 figures, 2tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluation of hydrocarbon reservoir requires classification of petrophysical\nproperties from available dataset. However, characterization of reservoir\nattributes is difficult due to the nonlinear and heterogeneous nature of the\nsubsurface physical properties. In this context, present study proposes a\ngeneralized one class classification framework based on Support Vector Data\nDescription (SVDD) to classify a reservoir characteristic water saturation into\ntwo classes (Class high and Class low) from four logs namely gamma ray, neutron\nporosity, bulk density, and P sonic using an imbalanced dataset. A comparison\nis carried out among proposed framework and different supervised classification\nalgorithms in terms of g metric means and execution time. Experimental results\nshow that proposed framework has outperformed other classifiers in terms of\nthese performance evaluators. It is envisaged that the classification analysis\nperformed in this study will be useful in further reservoir modeling.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 07:54:23 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Chaki", "Soumi", ""], ["Verma", "Akhilesh Kumar", ""], ["Routray", "Aurobinda", ""], ["Mohanty", "William K.", ""], ["Jenamani", "Mamata", ""]]}, {"id": "1612.01403", "submitter": "Ilja Klebanov", "authors": "Ilja Klebanov, Alexander Sikorski, Christof Sch\\\"utte, Susanna\n  R\\\"oblitz", "title": "Empirical Bayes Methods for Prior Estimation in Systems Medicine", "comments": "20 pages, 8 figures. arXiv admin note: text overlap with\n  arXiv:1612.00064", "journal-ref": null, "doi": null, "report-no": "ZR-16-57", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main goals of mathematical modeling in systems medicine related to\nmedical applications is to obtain patient-specific parameterizations and model\npredictions. In clinical practice, however, the number of available\nmeasurements for single patients is usually limited due to time and cost\nrestrictions. This hampers the process of making patient-specific predictions\nabout the outcome of a treatment. On the other hand, data are often available\nfor many patients, in particular if extensive clinical studies have been\nperformed. Therefore, before applying Bayes' rule \\emph{separately} to the data\nof each patient (which is typically performed using a non-informative prior),\nit is meaningful to use empirical Bayes methods in order to construct an\ninformative prior from all available data. We compare the performance of four\npriors -- a non-informative prior and priors chosen by nonparametric maximum\nlikelihood estimation (NPMLE), by maximum penalized likelihood estimation\n(MPLE) and by doubly-smoothed maximum likelihood estimation (DS-MLE) -- by\napplying them to a low-dimensional parameter estimation problem in a toy model\nas well as to a high-dimensional ODE model of the human menstrual cycle, which\nrepresents a typical example from systems biology modeling.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 15:47:40 GMT"}, {"version": "v2", "created": "Wed, 14 Dec 2016 14:09:19 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Klebanov", "Ilja", ""], ["Sikorski", "Alexander", ""], ["Sch\u00fctte", "Christof", ""], ["R\u00f6blitz", "Susanna", ""]]}, {"id": "1612.01408", "submitter": "Samuel Clark", "authors": "Samuel J. Clark", "title": "A General Age-Specific Mortality Model with An Example Indexed by Child\n  or Child/Adult Mortality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BACKGROUND. The majority of countries in Africa and nearly one third of all\ncountries require mortality models to infer complete age schedules of\nmortality, required for population estimates, projections/forecasts and many\nother tasks in demography and epidemiology. Models that relate child mortality\nto mortality at other ages are important because all countries have measures of\nchild mortality.\n  OBJECTIVE. 1) Design a general model for age-specific mortality that provides\na standard way to relate covariates to age-specific mortality. 2) Calibrate\nthat model using the relationship between child or child/adult mortality and\nmortality at other ages. 3) Validate the calibrated model and compare its\nperformance to existing models.\n  METHODS. A general, parametrizable component model of mortality is designed\nusing the singular value decomposition (SVD-Comp) and calibrated to the\nrelationship between child or child/adult mortality and mortality at other ages\nin the observed mortality schedules of the Human Mortality Database. Cross\nvalidation is used to validate the model, and the predictive performance of the\nmodel is compared to that of the Log-Quad model, designed to do the same thing.\n  RESULTS. Prediction and cross validation tests indicate that the child\nmortality-calibrated SVD-Comp is able to accurately represent the observed\nmortality schedules in the Human Mortality Database, is robust to the selection\nof mortality schedules used to calibrate it, and performs better than the\nLog-Quad Model.\n  CONCLUSIONS. The child mortality-calibrated SVD-Comp is a useful tool that\ncan be used where child mortality is available but mortality at other ages is\nunknown. Together with earlier work on an HIV prevalence-calibrated version of\nSVD-Comp, this work suggests that this approach is truly general and could be\nused to develop a wide range of additional useful models.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 03:37:13 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Clark", "Samuel J.", ""]]}, {"id": "1612.01409", "submitter": "Santosh Tirunagari", "authors": "Norman Poh, Simon Bull, Santosh Tirunagari, Nicholas Cole, Simon de\n  Lusignan", "title": "Probabilistic Broken-Stick Model: A Regression Algorithm for Irregularly\n  Sampled Data with Application to eGFR", "comments": "Preprint submitted to Journal of Biomedical Informatics", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order for clinicians to manage disease progression and make effective\ndecisions about drug dosage, treatment regimens or scheduling follow up\nappointments, it is necessary to be able to identify both short and long-term\ntrends in repeated biomedical measurements. However, this is complicated by the\nfact that these measurements are irregularly sampled and influenced by both\ngenuine physiological changes and external factors. In their current forms,\nexisting regression algorithms often do not fulfil all of a clinician's\nrequirements for identifying short-term events while still being able to\nidentify long-term trends in disease progression. Therefore, in order to\nbalance both short term interpretability and long term flexibility, an\nextension to broken-stick regression models is proposed in order to make them\nmore suitable for modelling clinical time series. The proposed probabilistic\nbroken-stick model can robustly estimate both short-term and long-term trends\nsimultaneously, while also accommodating the unequal length and irregularly\nsampled nature of clinical time series. Moreover, since the model is parametric\nand completely generative, its first derivative provides a long-term non-linear\nestimate of the annual rate of change in the measurements more reliably than\nlinear regression. The benefits of the proposed model are illustrated using\nestimated glomerular filtration rate as a case study for managing patients with\nchronic kidney disease.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 17:50:44 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Poh", "Norman", ""], ["Bull", "Simon", ""], ["Tirunagari", "Santosh", ""], ["Cole", "Nicholas", ""], ["de Lusignan", "Simon", ""]]}, {"id": "1612.01454", "submitter": "Yawen Guan", "authors": "Yawen Guan, Murali Haran and David Pollard", "title": "Inferring Ice Thickness from a Glacier Dynamics Model and Multiple\n  Surface Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The future behavior of the West Antarctic Ice Sheet (WAIS) may have a major\nimpact on future climate. For instance, ice sheet melt may contribute\nsignificantly to global sea level rise. Understanding the current state of WAIS\nis therefore of great interest. WAIS is drained by fast-flowing glaciers which\nare major contributors to ice loss. Hence, understanding the stability and\ndynamics of glaciers is critical for predicting the future of the ice sheet.\nGlacier dynamics are driven by the interplay between the topography,\ntemperature and basal conditions beneath the ice. A glacier dynamics model\ndescribes the interactions between these processes. We develop a hierarchical\nBayesian model that integrates multiple ice sheet surface data sets with a\nglacier dynamics model. Our approach allows us to (1) infer important\nparameters describing the glacier dynamics, (2) learn about ice sheet\nthickness, and (3) account for errors in the observations and the model.\nBecause we have relatively dense and accurate ice thickness data from the\nThwaites Glacier in West Antarctica, we use these data to validate the proposed\napproach. The long-term goal of this work is to have a general model that may\nbe used to study multiple glaciers in the Antarctic.\n  Keywords: ice sheet, glacier dynamics, hierarchical Bayes, Gaussian process,\nMarkov chain Monte Carlo, West Antarctic ice sheet.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 16:17:03 GMT"}, {"version": "v2", "created": "Fri, 30 Jun 2017 20:37:08 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Guan", "Yawen", ""], ["Haran", "Murali", ""], ["Pollard", "David", ""]]}, {"id": "1612.01773", "submitter": "Anna Kiriliouk", "authors": "Anna Kiriliouk and Holger Rootz\\'en and Johan Segers and Jennifer L.\n  Wadsworth", "title": "Peaks over thresholds modelling with multivariate generalized Pareto\n  distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When assessing the impact of extreme events, it is often not just a single\ncomponent, but the combined behaviour of several components which is important.\nStatistical modelling using multivariate generalized Pareto (GP) distributions\nconstitutes the multivariate analogue of univariate peaks over thresholds\nmodelling, which is widely used in finance and engineering. We develop general\nmethods for construction of multivariate GP distributions and use them to\ncreate a variety of new statistical models. A censored likelihood procedure is\nproposed to make inference on these models, together with a threshold selection\nprocedure, goodness-of-fit diagnostics, and a computationally tractable\nstrategy for model selection. The models are fitted to returns of stock prices\nof four UK-based banks and to rainfall data in the context of landslide risk\nestimation. Supplementary materials and codes are available online.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 12:14:59 GMT"}, {"version": "v2", "created": "Tue, 6 Feb 2018 09:46:57 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Kiriliouk", "Anna", ""], ["Rootz\u00e9n", "Holger", ""], ["Segers", "Johan", ""], ["Wadsworth", "Jennifer L.", ""]]}, {"id": "1612.01791", "submitter": "Nigul Olspert", "authors": "Nigul Olspert, Maarit K\\\"apyl\\\"a and Jaan Pelt", "title": "Method for estimating cycle lengths from multidimensional time series:\n  Test cases and application to a massive \"in silico\" dataset", "comments": null, "journal-ref": null, "doi": "10.1109/BigData.2016.7840977", "report-no": null, "categories": "astro-ph.SR astro-ph.IM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real world systems exhibit cyclic behavior that is, for example, due to\nthe nearly harmonic oscillations being perturbed by the strong fluctuations\npresent in the regime of significant non-linearities. For the investigation of\nsuch sys- tems special techniques relaxing the assumption to periodicity are\nrequired. In this paper, we present the generalization of one of such\ntechniques, namely the D2 phase dispersion statistic, to multidimensional\ndatasets, especially suited for the analysis of the outputs from\nthree-dimensional numerical simulations of the full magnetohydrodynamic\nequations. We present the motivation and need for the usage of such a method\nwith simple test cases, and present an application to a solar-like semi-global\nnumerical dynamo simulation covering nearly 150 magnetic cycles.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 13:03:08 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Olspert", "Nigul", ""], ["K\u00e4pyl\u00e4", "Maarit", ""], ["Pelt", "Jaan", ""]]}, {"id": "1612.02023", "submitter": "Virginie Ollier", "authors": "Virginie Ollier, Mohammed Nabil El Korso, R\\'emy Boyer, Pascal\n  Larzabal, Marius Pesavento", "title": "Robust Calibration of Radio Interferometers in Non-Gaussian Environment", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2017.2733496", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of new phased array systems in radio astronomy, as the low\nfrequency array (LOFAR) and the square kilometre array (SKA), formed of a large\nnumber of small and flexible elementary antennas, has led to significant\nchallenges. Among them, model calibration is a crucial step in order to provide\naccurate and thus meaningful images and requires the estimation of all the\nperturbation effects introduced along the signal propagation path, for a\nspecific source direction and antenna position. Usually, it is common to\nperform model calibration using the a priori knowledge regarding a small number\nof known strong calibrator sources but under the assumption of Gaussianity of\nthe noise. Nevertheless, observations in the context of radio astronomy are\nknown to be affected by the presence of outliers which are due to several\ncauses, e.g., weak non-calibrator sources or man made radio frequency\ninterferences. Consequently, the classical Gaussian noise assumption is\nviolated leading to severe degradation in performances. In order to take into\naccount the outlier effects, we assume that the noise follows a spherically\ninvariant random distribution. Based on this modeling, a robust calibration\nalgorithm is presented in this paper. More precisely, this new scheme is based\non the design of an iterative relaxed concentrated maximum likelihood\nestimation procedure which allows to obtain closed-form expressions for the\nunknown parameters with a reasonable computational cost. This is of importance\nas the number of estimated parameters depends on the number of antenna\nelements, which is large for the new generation of radio interferometers.\nNumerical simulations reveal that the proposed algorithm outperforms the\nstate-of-the-art calibration techniques.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 21:14:43 GMT"}, {"version": "v2", "created": "Wed, 14 Dec 2016 20:32:33 GMT"}, {"version": "v3", "created": "Tue, 25 Jul 2017 08:18:05 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Ollier", "Virginie", ""], ["Korso", "Mohammed Nabil El", ""], ["Boyer", "R\u00e9my", ""], ["Larzabal", "Pascal", ""], ["Pesavento", "Marius", ""]]}, {"id": "1612.02024", "submitter": "Marinho Bertanha", "authors": "Marinho Bertanha and Marcelo J. Moreira", "title": "Impossible Inference in Econometrics: Theory and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST q-fin.EC stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies models in which hypothesis tests have trivial power, that\nis, power smaller than size. This testing impossibility, or impossibility type\nA, arises when any alternative is not distinguishable from the null. We also\nstudy settings in which it is impossible to have almost surely bounded\nconfidence sets for a parameter of interest. This second type of impossibility\n(type B) occurs under a condition weaker than the condition for type A\nimpossibility: the parameter of interest must be nearly unidentified. Our\ntheoretical framework connects many existing publications on impossible\ninference that rely on different notions of topologies to show models are not\ndistinguishable or nearly unidentified. We also derive both types of\nimpossibility using the weak topology induced by convergence in distribution.\nImpossibility in the weak topology is often easier to prove, it is applicable\nfor many widely-used tests, and it is useful for robust hypothesis testing. We\nconclude by demonstrating impossible inference in multiple economic\napplications of models with discontinuity and time-series models.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 21:15:33 GMT"}, {"version": "v2", "created": "Fri, 9 Dec 2016 03:35:34 GMT"}, {"version": "v3", "created": "Wed, 21 Feb 2018 16:42:39 GMT"}, {"version": "v4", "created": "Fri, 16 Nov 2018 15:10:42 GMT"}, {"version": "v5", "created": "Mon, 17 Feb 2020 21:38:24 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Bertanha", "Marinho", ""], ["Moreira", "Marcelo J.", ""]]}, {"id": "1612.02130", "submitter": "Niek Tax", "authors": "Niek Tax, Ilya Verenich, Marcello La Rosa, Marlon Dumas", "title": "Predictive Business Process Monitoring with LSTM Neural Networks", "comments": "Accepted at the International Conference on Advanced Information\n  Systems Engineering (CAiSE) 2017", "journal-ref": "Lecture Notes in Computer Science, 10253 (2017) 477-492", "doi": "10.1007/978-3-319-59536-8_30", "report-no": null, "categories": "stat.AP cs.DB cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive business process monitoring methods exploit logs of completed\ncases of a process in order to make predictions about running cases thereof.\nExisting methods in this space are tailor-made for specific prediction tasks.\nMoreover, their relative accuracy is highly sensitive to the dataset at hand,\nthus requiring users to engage in trial-and-error and tuning when applying them\nin a specific setting. This paper investigates Long Short-Term Memory (LSTM)\nneural networks as an approach to build consistently accurate models for a wide\nrange of predictive process monitoring tasks. First, we show that LSTMs\noutperform existing techniques to predict the next event of a running case and\nits timestamp. Next, we show how to use models for predicting the next task in\norder to predict the full continuation of a running case. Finally, we apply the\nsame approach to predict the remaining time, and show that this approach\noutperforms existing tailor-made methods.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 07:04:17 GMT"}, {"version": "v2", "created": "Tue, 16 May 2017 18:51:41 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Tax", "Niek", ""], ["Verenich", "Ilya", ""], ["La Rosa", "Marcello", ""], ["Dumas", "Marlon", ""]]}, {"id": "1612.02189", "submitter": "Evrim Acar", "authors": "Evrim Acar, Yuri Levin-Schwartz, Vince D. Calhoun and T\\\"ulay Adal{\\i}", "title": "Tensor-Based Fusion of EEG and FMRI to Understand Neurological Changes\n  in Schizophrenia", "comments": null, "journal-ref": null, "doi": "10.1109/ISCAS.2017.8050303", "report-no": null, "categories": "stat.AP q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuroimaging modalities such as functional magnetic resonance imaging (fMRI)\nand electroencephalography (EEG) provide information about neurological\nfunctions in complementary spatiotemporal resolutions; therefore, fusion of\nthese modalities is expected to provide better understanding of brain activity.\nIn this paper, we jointly analyze fMRI and multi-channel EEG signals collected\nduring an auditory oddball task with the goal of capturing brain activity\npatterns that differ between patients with schizophrenia and healthy controls.\nRather than selecting a single electrode or matricizing the third-order tensor\nthat can be naturally used to represent multi-channel EEG signals, we preserve\nthe multi-way structure of EEG data and use a coupled matrix and tensor\nfactorization (CMTF) model to jointly analyze fMRI and EEG signals. Our\nanalysis reveals that (i) joint analysis of EEG and fMRI using a CMTF model can\ncapture meaningful temporal and spatial signatures of patterns that behave\ndifferently in patients and controls, and (ii) these differences and the\ninterpretability of the associated components increase by including multiple\nelectrodes from frontal, motor and parietal areas, but not necessarily by\nincluding all electrodes in the analysis.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 10:40:16 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Acar", "Evrim", ""], ["Levin-Schwartz", "Yuri", ""], ["Calhoun", "Vince D.", ""], ["Adal\u0131", "T\u00fclay", ""]]}, {"id": "1612.02195", "submitter": "Jerzy Rydlewski", "authors": "Daniel Kosiorowski and Dominik Mielczarek and Jerzy P. Rydlewski and\n  Ma{\\l}gorzata Snarska", "title": "Generalized Exponential smoothing in prediction of hierarchical time\n  series", "comments": null, "journal-ref": "STATISTICS IN TRANSITION new series, June 2018 Vol. 19, No. 2, pp.\n  331-350", "doi": "10.21307/stattrans-2018-019", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shang and Hyndman (2017) proposed a grouped functional time series\nforecasting approach as a combination of individual forecasts obtained using\ngeneralized least squares method. We modify their methodology using generalized\nexponential smoothing technique for the most disaggregated functional time\nseries in order to obtain more robust predictor. We discuss some properties of\nour proposals basing on results obtained via simulation studies and analysis of\nreal data related to a prediction of a demand for electricity in Australia in\n2016.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 11:06:15 GMT"}, {"version": "v2", "created": "Wed, 19 Apr 2017 11:54:08 GMT"}, {"version": "v3", "created": "Fri, 23 Mar 2018 16:12:39 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Kosiorowski", "Daniel", ""], ["Mielczarek", "Dominik", ""], ["Rydlewski", "Jerzy P.", ""], ["Snarska", "Ma\u0142gorzata", ""]]}, {"id": "1612.02300", "submitter": "Eyal Fisher", "authors": "Eyal Fisher, Regev Schweiger and Saharon Rosset", "title": "Efficient Construction of Test-Inversion Confidence Intervals Using\n  Quantile Regression, With Application To Population Genetics", "comments": "21 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern problems in statistics tend to include estimators of high\ncomputational complexity and with complicated distributions. Statistical\ninference on such estimators usually relies on asymptotic normality\nassumptions, however, such assumptions are often not applicable for available\nsample sizes, due to dependencies in the data and other causes. A common\nalternative is the use of re-sampling procedures, such as the bootstrap, but\nthese may be computationally intensive to an extent that renders them\nimpractical for modern problems. In this paper we develop a method for fast\nconstruction of test-inversion bootstrap confidence intervals. Our approach\nuses quantile regression to model the quantile of an estimator conditional on\nthe true value of the parameter, and we apply it on the Watterson estimator of\nmutation rate in a standard coalescent model. We demonstrate an improved\nefficiency of up to 40% from using quantile regression compared to state of the\nart methods based on stochastic approximation, as measured by the number of\nsimulations required to achieve comparable accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 15:45:09 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Fisher", "Eyal", ""], ["Schweiger", "Regev", ""], ["Rosset", "Saharon", ""]]}, {"id": "1612.02382", "submitter": "Malcolm Itter", "authors": "Malcolm S. Itter, Andrew O. Finley, Mevin B. Hooten, Philip E.\n  Higuera, Jennifer R. Marlon, Ryan Kelly, Jason S. McLachlan", "title": "A Model-Based Approach to Wildland Fire Reconstruction Using Sediment\n  Charcoal Records", "comments": "26 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lake sediment charcoal records are used in paleoecological analyses to\nreconstruct fire history including the identification of past wildland fires.\nOne challenge of applying sediment charcoal records to infer fire history is\nthe separation of charcoal associated with local fire occurrence and charcoal\noriginating from regional fire activity. Despite a variety of methods to\nidentify local fires from sediment charcoal records, an integrated statistical\nframework for fire reconstruction is lacking. We develop a Bayesian point\nprocess model to estimate probability of fire associated with charcoal counts\nfrom individual-lake sediments and estimate mean fire return intervals. A\nmultivariate extension of the model combines records from multiple lakes to\nreduce uncertainty in local fire identification and estimate a regional mean\nfire return interval. The univariate and multivariate models are applied to 13\nlakes in the Yukon Flats region of Alaska. Both models resulted in similar mean\nfire return intervals (100-350 years) with reduced uncertainty under the\nmultivariate model due to improved estimation of regional charcoal deposition.\nThe point process model offers an integrated statistical framework for\npaleo-fire reconstruction and extends existing methods to infer regional fire\nhistory from multiple lake records with uncertainty following directly from\nposterior distributions.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 19:17:55 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Itter", "Malcolm S.", ""], ["Finley", "Andrew O.", ""], ["Hooten", "Mevin B.", ""], ["Higuera", "Philip E.", ""], ["Marlon", "Jennifer R.", ""], ["Kelly", "Ryan", ""], ["McLachlan", "Jason S.", ""]]}, {"id": "1612.02460", "submitter": "Fahad Alhasoun", "authors": "Fahad Alhasoun, May Alhazzani, Marta C. Gonz\\'alez", "title": "Demographical Priors for Health Conditions Diagnosis Using Medicare Data", "comments": "NIPS 2016 Workshop on Machine Learning for Health", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an example of how demographical characteristics of\npatients influence their susceptibility to certain medical conditions. In this\npaper, we investigate the association of health conditions to age of patients\nin a heterogeneous population. We show that besides the symptoms a patients is\nhaving, the age has the potential of aiding the diagnostic process in\nhospitals. Working with Electronic Health Records (EHR), we show that medical\nconditions group into clusters that share distinctive population age densities.\nWe use Electronic Health Records from Brazil for a period of 15 months from\nMarch of 2013 to July of 2014. The number of patients in the data is 1.7\nmillion patients and the number of records is 47 million records. The findings\nhas the potential of helping in a setting where an automated system undergoes\nthe task of predicting the condition of a patient given their symptoms and\ndemographical information.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 21:27:36 GMT"}, {"version": "v2", "created": "Mon, 9 Jan 2017 18:32:14 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Alhasoun", "Fahad", ""], ["Alhazzani", "May", ""], ["Gonz\u00e1lez", "Marta C.", ""]]}, {"id": "1612.02490", "submitter": "An Qu", "authors": "An Qu and Cheng Zhang and Paul Ackermann and Hedvig Kjellstr\\\"om", "title": "Bridging Medical Data Inference to Achilles Tendon Rupture\n  Rehabilitation", "comments": "Workshop on Machine Learning for Healthcare, NIPS 2016, Barcelona,\n  Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imputing incomplete medical tests and predicting patient outcomes are crucial\nfor guiding the decision making for therapy, such as after an Achilles Tendon\nRupture (ATR). We formulate the problem of data imputation and prediction for\nATR relevant medical measurements into a recommender system framework. By\napplying MatchBox, which is a collaborative filtering approach, on a real\ndataset collected from 374 ATR patients, we aim at offering personalized\nmedical data imputation and prediction. In this work, we show the feasibility\nof this approach and discuss potential research directions by conducting\ninitial qualitative evaluations.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 23:58:36 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Qu", "An", ""], ["Zhang", "Cheng", ""], ["Ackermann", "Paul", ""], ["Kjellstr\u00f6m", "Hedvig", ""]]}, {"id": "1612.02527", "submitter": "Gentry White", "authors": "Gentry White and Fabrizio Ruggeri and Michael D. Porter", "title": "Modelling the Proliferation of Terrorism via Diffusion and Contagion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of terrorism is a serious concern in national and\ninternational security, as its spread is seen as an existential threat to\nWestern liberal democracies. Understanding and effectively modelling the spread\nof terrorism provides useful insight into formulating effective responses. A\nmathematical model capturing the theoretical constructs of contagion and\ndiffusion is constructed for explaining the spread of terrorist activity and\nused to analyse data from the Global Terrorism Database from 2000--2016 for\nAfghanistan, Iraq, and Israel.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 04:24:19 GMT"}, {"version": "v2", "created": "Mon, 19 Dec 2016 07:36:58 GMT"}, {"version": "v3", "created": "Fri, 27 Apr 2018 23:13:08 GMT"}, {"version": "v4", "created": "Tue, 12 Feb 2019 01:19:27 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["White", "Gentry", ""], ["Ruggeri", "Fabrizio", ""], ["Porter", "Michael D.", ""]]}, {"id": "1612.02705", "submitter": "Yanxun Xu", "authors": "Yanxun Xu, Peter Mueller, Apostolia M Tsimberidou, Donald Berry", "title": "A Nonparametric Bayesian Basket Trial Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Targeted therapies on the basis of genomic aberrations analysis of the tumor\nhave shown promising results in cancer prognosis and treatment. Regardless of\ntumor type, trials that match patients to targeted therapies for their\nparticular genomic aberrations have become a mainstream direction of\ntherapeutic management of patients with cancer. Therefore, finding the\nsubpopulation of patients who can most benefit from an aberration-specific\ntargeted therapy across multiple cancer types is important. We propose an\nadaptive Bayesian clinical trial design for patient allocation and\nsubpopulation identification. We start with a decision theoretic approach,\nincluding a utility function and a probability model across all possible\nsubpopulation models. The main features of the proposed design and population\nfinding methods are that we allow for variable sets of covariates to be\nrecorded by different patients, adjust for missing data, allow high order\ninteractions of covariates, and the adaptive allocation of each patient to\ntreatment arms using the posterior predictive probability of which arm is best\nfor each patient. The new method is demonstrated via extensive simulation\nstudies.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 16:00:33 GMT"}, {"version": "v2", "created": "Sun, 19 Feb 2017 00:50:24 GMT"}, {"version": "v3", "created": "Tue, 17 Apr 2018 13:54:20 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Xu", "Yanxun", ""], ["Mueller", "Peter", ""], ["Tsimberidou", "Apostolia M", ""], ["Berry", "Donald", ""]]}, {"id": "1612.02812", "submitter": "Shihao Yang", "authors": "Shihao Yang, S. C. Kou, Fred Lu, John S. Brownstein, Nicholas Brooke,\n  Mauricio Santillana", "title": "Advances in using Internet searches to track dengue", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dengue is a mosquito-borne disease that threatens more than half of the\nworld's population. Despite being endemic to over 100 countries, government-led\nefforts and mechanisms to timely identify and track the emergence of new\ninfections are still lacking in many affected areas. Multiple methodologies\nthat leverage the use of Internet-based data sources have been proposed as a\nway to complement dengue surveillance efforts. Among these, the trends in\ndengue-related Google searches have been shown to correlate with dengue\nactivity. We extend a methodological framework, initially proposed and\nvalidated for flu surveillance, to produce near real-time estimates of dengue\ncases in five countries/regions: Mexico, Brazil, Thailand, Singapore and\nTaiwan. Our result shows that our modeling framework can be used to improve the\ntracking of dengue activity in multiple locations around the world.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 20:55:27 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Yang", "Shihao", ""], ["Kou", "S. C.", ""], ["Lu", "Fred", ""], ["Brownstein", "John S.", ""], ["Brooke", "Nicholas", ""], ["Santillana", "Mauricio", ""]]}, {"id": "1612.03073", "submitter": "Timoth\\'ee Stumpf-F\\'etizon", "authors": "Jos\\'e Garc\\'ia Montalvo and Omiros Papaspiliopoulos and Timoth\\'ee\n  Stumpf-F\\'etizon", "title": "Bayesian forecasting of electoral outcomes with new parties' competition", "comments": "Multilevel models, Bayesian machine learning, inverse regression,\n  evidence synthesis, elections", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposed a methodology to forecast electoral outcomes using the\nresult of the combination of a fundamental model and a model-based aggregation\nof polls. We propose a Bayesian hierarchical structure for the fundamental\nmodel that synthesises data at the provincial, regional and national level. We\nuse a Bayesian strategy to combine the fundamental model with the information\ncoming for recent polls. This model can naturally be updated every time new\ninformation, for instance a new poll, becomes available. This methodology is\nwell suited to deal with increasingly frequent situations in which new\npolitical parties enter an electoral competition, although our approach is\ngeneral enough to accommodate any other electoral situation. We illustrate the\nadvantages of our method using the 2015 Spanish Congressional Election in which\ntwo new parties ended up receiving 30\\% of the votes. We compare the predictive\nperformance of our model versus alternative models. In general the predictions\nof our model outperform the alternative specifications, including hybrid models\nthat combine fundamental and polls models. Our predictions are, in relative\nterms, particularly accurate in predicting the seats obtained by each political\nparty.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 16:13:22 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2019 14:45:26 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Montalvo", "Jos\u00e9 Garc\u00eda", ""], ["Papaspiliopoulos", "Omiros", ""], ["Stumpf-F\u00e9tizon", "Timoth\u00e9e", ""]]}, {"id": "1612.03278", "submitter": "Samir Bhatt Dr", "authors": "Samir Bhatt, Ewan Cameron, Seth R Flaxman, Daniel J Weiss, David L\n  Smith and Peter W Gething", "title": "Improved prediction accuracy for disease risk mapping using Gaussian\n  Process stacked generalisation", "comments": "Under Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Maps of infectious disease---charting spatial variations in the force of\ninfection, degree of endemicity, and the burden on human health---provide an\nessential evidence base to support planning towards global health targets.\nContemporary disease mapping efforts have embraced statistical modelling\napproaches to properly acknowledge uncertainties in both the available\nmeasurements and their spatial interpolation. The most common such approach is\nthat of Gaussian process regression, a mathematical framework comprised of two\ncomponents: a mean function harnessing the predictive power of multiple\nindependent variables, and a covariance function yielding spatio-temporal\nshrinkage against residual variation from the mean. Though many techniques have\nbeen developed to improve the flexibility and fitting of the covariance\nfunction, models for the mean function have typically been restricted to simple\nlinear terms. For infectious diseases, known to be driven by complex\ninteractions between environmental and socio-economic factors, improved\nmodelling of the mean function can greatly boost predictive power. Here we\npresent an ensemble approach based on stacked generalisation that allows for\nmultiple, non-linear algorithmic mean functions to be jointly embedded within\nthe Gaussian process framework. We apply this method to mapping Plasmodium\nfalciparum prevalence data in Sub-Saharan Africa and show that the generalised\nensemble approach markedly out-performs any individual method.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2016 11:00:36 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Bhatt", "Samir", ""], ["Cameron", "Ewan", ""], ["Flaxman", "Seth R", ""], ["Weiss", "Daniel J", ""], ["Smith", "David L", ""], ["Gething", "Peter W", ""]]}, {"id": "1612.03373", "submitter": "Luyan Ji", "authors": "Jie Wang, Luyan Ji, Xiaomeng Huang, Haohuan Fu, Shiming Xu, Congcong\n  Li", "title": "A probabilistic graphical model approach in 30 m land cover mapping with\n  multiple data sources", "comments": "38 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a trend to acquire high accuracy land-cover maps using multi-source\nclassification methods, most of which are based on data fusion, especially\npixel- or feature-level fusions. A probabilistic graphical model (PGM) approach\nis proposed in this research for 30 m resolution land-cover mapping with\nmulti-temporal Landsat and MODerate Resolution Imaging Spectroradiometer\n(MODIS) data. Independent classifiers were applied to two single-date Landsat 8\nscenes and the MODIS time-series data, respectively, for probability\nestimation. A PGM was created for each pixel in Landsat 8 data. Conditional\nprobability distributions were computed based on data quality and reliability\nby using information selectively. Using the administrative territory of Beijing\nCity (Area-1) and a coastal region of Shandong province, China (Area-2) as\nstudy areas, multiple land-cover maps were generated for comparison.\nQuantitative results show the effectiveness of the proposed method. Overall\naccuracies promoted from 74.0% (maps acquired from single-temporal Landsat\nimages) to 81.8% (output of the PGM) for Area-1. Improvements can also be seen\nwhen using MODIS data and only a single-temporal Landsat image as input\n(overall accuracy: 78.4% versus 74.0% for Area-1, and 86.8% versus 83.0% for\nArea-2). Information from MODIS data did not help much when the PGM was applied\nto cloud free regions of. One of the advantages of the proposed method is that\nit can be applied where multi-temporal data cannot be simply stacked as a\nmulti-layered image.\n", "versions": [{"version": "v1", "created": "Sun, 11 Dec 2016 06:02:41 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Wang", "Jie", ""], ["Ji", "Luyan", ""], ["Huang", "Xiaomeng", ""], ["Fu", "Haohuan", ""], ["Xu", "Shiming", ""], ["Li", "Congcong", ""]]}, {"id": "1612.03554", "submitter": "Briton Park", "authors": "Derek Lo and Briton Park", "title": "Modeling the spread of the Zika virus using topological data analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zika virus (ZIKV), a disease spread primarily through the Aedes aegypti\nmosquito, was identified in Brazil in 2015 and was declared a global health\nemergency by the World Health Organization (WHO). Epidemiologists often use\ncommon state-level attributes such as population density and temperature to\ndetermine the spread of disease. By applying techniques from topological data\nanalysis, we believe that epidemiologists will be able to better predict how\nZIKV will spread. We use the Vietoris-Rips filtration on high-density mosquito\nlocations in Brazil to create simplicial complexes, from which we extract\nhomology group generators. Previously epidemiologists have not relied on\ntopological data analysis to model disease spread. Evaluating our model on ZIKV\ncase data in the states of Brazil demonstrates the value of these techniques\nfor the improved assessment of vector-borne diseases.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 06:37:04 GMT"}, {"version": "v2", "created": "Tue, 13 Dec 2016 19:20:16 GMT"}, {"version": "v3", "created": "Tue, 24 Jan 2017 03:41:17 GMT"}, {"version": "v4", "created": "Wed, 25 Jan 2017 20:57:10 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["Lo", "Derek", ""], ["Park", "Briton", ""]]}, {"id": "1612.03561", "submitter": "Monica Alexander", "authors": "Monica Alexander and Leontine Alkema", "title": "Global Estimation of Neonatal Mortality using a Bayesian Hierarchical\n  Splines Regression Model", "comments": null, "journal-ref": null, "doi": "10.4054/DemRes.2018.38.15", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, much of the focus in monitoring child mortality has been on\nassessing changes in the under-five mortality rate (U5MR). However, as the U5MR\ndecreases, the share of neonatal deaths (within the first month) tends to\nincrease, warranting increased efforts in monitoring this indicator in addition\nto the U5MR. A Bayesian splines regression model is presented for estimating\nneonatal mortality rates (NMR) for all countries. In the model, the\nrelationship between NMR and U5MR is assessed and used to inform estimates, and\nspline regression models are used to capture country-specific trends. As such,\nthe resulting NMR estimates incorporate trends in overall child mortality while\nalso capturing data-driven trends. The model is fitted to 195 countries using\nthe database from the United Nations Interagency Group for Child Mortality\nEstimation, producing estimates from 1990, or earlier if data are available,\nuntil 2015. The results suggest that, above a U5MR of 34 deaths per 1000 live\nbirths, at the global level, a 1 per cent increase in the U5MR leads to a 0.6\nper cent decrease in the ratio of NMR to U5MR. Below a U5MR of 34 deaths per\n1000 live births, the proportion of deaths under-five that are neonatal is\nconstant at around 54 per cent. However, the relationship between U5MR and NMR\nvaries across countries. The model has now been adopted by the United Nations\nInter-agency Group for Child Mortality Estimation.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 07:40:50 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Alexander", "Monica", ""], ["Alkema", "Leontine", ""]]}, {"id": "1612.04074", "submitter": "Aristides Moustakas", "authors": "Aristides Moustakas", "title": "Spatio-temporal data mining in ecological and veterinary epidemiology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the spread of any disease is a highly complex and\ninterdisciplinary exercise as biological, social, geographic, economic, and\nmedical factors may shape the way a disease moves through a population and\noptions for its eventual control or eradication. Disease spread poses a serious\nthreat in animal and plant health and has implications for ecosystem\nfunctioning and species extinctions as well as implications in society through\nfood security and potential disease spread in humans. Space-time epidemiology\nis based on the concept that various characteristics of the pathogenic agents\nand the environment interact in order to alter the probability of disease\noccurrence and form temporal or spatial patterns. Epidemiology aims to identify\nthese patterns and factors, to assess the relevant uncertainty sources, and to\ndescribe disease in the population. Thus disease spread at the population level\ndiffers from the approach traditionally taken by veterinary practitioners that\nare principally concerned with the health status of the individual. Patterns of\ndisease occurrence provide insights into which factors may be affecting the\nhealth of the population, through investigating which individuals are affected,\nwhere are these individuals located and when did they become infected. With the\nrapid development of smart sensors, social networks, as well as digital maps\nand remotely-sensed imagery spatio-temporal data are more ubiquitous and richer\nthan ever before. The availability of such large datasets (Big data) poses\ngreat challenges in data analysis. In addition, increased availability of\ncomputing power facilitates the use of computationally-intensive methods for\nthe analysis of such data. Thus new methods as well as case studies are needed\nto understand veterinary and ecological epidemiology. A special issue aimed to\naddress this topic.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 09:42:39 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Moustakas", "Aristides", ""]]}, {"id": "1612.04091", "submitter": "Eric Beutner", "authors": "Eric Beutner, Simon Reese, Jean-Pierre Urbain", "title": "Identifiability issues of age-period and age-period-cohort models of the\n  Lee-Carter type", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The predominant way of modelling mortality rates is the Lee-Carter model and\nits many extensions. The Lee-Carter model and its many extensions use a latent\nprocess to forecast. These models are estimated using a two-step procedure that\ncauses an inconsistent view on the latent variable. This paper considers\nidentifiability issues of these models from a perspective that acknowledges the\nlatent variable as a stochastic process from the beginning. We call this\nperspective the plug-in age-period or plug-in age-period-cohort model. Defining\na parameter vector that includes the underlying parameters of this process\nrather than its realisations, we investigate whether the expected values and\ncovariances of the plug-in Lee-Carter models are identifiable. It will be seen,\nfor example, that even if in both steps of the estimation procedure we have\nidentifiability in a certain sense it does not necessarily carry over to the\nplug-in models.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 10:44:24 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Beutner", "Eric", ""], ["Reese", "Simon", ""], ["Urbain", "Jean-Pierre", ""]]}, {"id": "1612.04174", "submitter": "Bruno Nicenboim", "authors": "Bruno Nicenboim and Shravan Vasishth", "title": "Models of retrieval in sentence comprehension: A computational\n  evaluation using Bayesian hierarchical modeling", "comments": "Accepted in Journal of Memory and Language", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on interference has provided evidence that the formation of\ndependencies between non-adjacent words relies on a cue-based retrieval\nmechanism. Two different models can account for one of the main predictions of\ninterference, i.e., a slowdown at a retrieval site, when several items share a\nfeature associated with a retrieval cue: Lewis and Vasishth's (2005)\nactivation-based model and McElree's (2000) direct access model. Even though\nthese two models have been used almost interchangeably, they are based on\ndifferent assumptions and predict differences in the relationship between\nreading times and response accuracy. The activation-based model follows the\nassumptions of ACT-R, and its retrieval process behaves as a lognormal race\nbetween accumulators of evidence with a single variance. Under this model,\naccuracy of the retrieval is determined by the winner of the race and retrieval\ntime by its rate of accumulation. In contrast, the direct access model assumes\na model of memory where only the probability of retrieval varies between items;\nin this model, differences in latencies are a by-product of the possibility and\nrepairing incorrect retrievals. We implemented both models in a Bayesian\nhierarchical framework in order to evaluate them and compare them. We show that\nsome aspects of the data are better fit under the direct access model than\nunder the activation-based model. We suggest that this finding does not rule\nout the possibility that retrieval may be behaving as a race model with\nassumptions that follow less closely the ones from the ACT-R framework. We show\nthat by introducing a modification of the activation model, i.e, by assuming\nthat the accumulation of evidence for retrieval of incorrect items is not only\nslower but noisier (i.e., different variances for the correct and incorrect\nitems), the model can provide a fit as good as the one of the direct access\nmodel.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 13:55:39 GMT"}, {"version": "v2", "created": "Thu, 24 Aug 2017 11:05:55 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Nicenboim", "Bruno", ""], ["Vasishth", "Shravan", ""]]}, {"id": "1612.04185", "submitter": "Hau-tieng Wu", "authors": "Yin-Yan Lin, Hau-tieng Wu, Chi-An Hsu, Po-Chiun Huang, Yuan-Hao Huang,\n  Yu-Lun Lo", "title": "Sleep Apnea Detection Based on Thoracic and Abdominal Movement Signals\n  of Wearable Piezo-Electric Bands", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physiologically, the thoracic (THO) and abdominal (ABD) movement signals,\ncaptured using wearable piezo-electric bands, provide information about various\ntypes of apnea, including central sleep apnea (CSA) and obstructive sleep apnea\n(OSA). However, the use of piezo-electric wearables in detecting sleep apnea\nevents has been seldom explored in the literature. This study explored the\npossibility of identifying sleep apnea events, including OSA and CSA, by solely\nanalyzing {one or both the THO and ABD signals. An adaptive non-harmonic model\nwas introduced to model the THO and ABD signals, which allows us to design\nfeatures for sleep apnea events. To confirm the suitability of the extracted\nfeatures, a support vector machine was applied to classify three categories --\nnormal and hypopnea, OSA, and CSA. According to a database of} 34 subjects, the\noverall classification accuracies were on average $75.9\\%\\pm 11.7\\%$ and\n$73.8\\%\\pm 4.4\\%$, respectively, based on the cross validation. When the\nfeatures determined from the THO and ABD signals were combined, the overall\nclassification accuracy became $81.8\\%\\pm 9.4\\%$. These features were applied\nfor designing a state machine for online apnea event detection. Two\nevent-by-event accuracy indices, S and I, were proposed for evaluating the\nperformance {of the state machine. For the same database, the} S index was\n$84.01\\%\\pm 9.06\\%$, and the I index was $77.21\\%\\pm 19.01\\%$. The results\nindicate the considerable potential of applying the proposed algorithm to\nclinical examinations for both screening and homecare purposes.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 02:09:25 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Lin", "Yin-Yan", ""], ["Wu", "Hau-tieng", ""], ["Hsu", "Chi-An", ""], ["Huang", "Po-Chiun", ""], ["Huang", "Yuan-Hao", ""], ["Lo", "Yu-Lun", ""]]}, {"id": "1612.04345", "submitter": "Daniel Mirman", "authors": "Daniel Mirman, Jon-Frederick Landrigan, Spiro Kokolis, Sean Verillo,\n  Casey Ferrara, Dorian Pustina", "title": "Corrections for multiple comparisons in voxel-based lesion-symptom\n  mapping", "comments": "20 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Voxel-based lesion-symptom mapping (VLSM) is an important method for basic\nand translational human neuroscience research. VLSM leverages modern\nneuroimaging analysis techniques to build on the classic approach of examining\nthe relationship between location of brain damage and cognitive deficits.\nTesting an association between deficit severity and lesion status in each voxel\ninvolves very many individual tests and requires statistical correction for\nmultiple comparisons. Several strategies have been adapted from analysis of\nfunctional neuroimaging data, though VLSM faces a more difficult trade-off\nbetween avoiding false positives and statistical power (missing true effects).\nNon-parametric, permutation-based methods are generally preferable because they\ndo not make assumptions that are likely to be violated by skewed distributions\nof behavioral deficit (symptom) scores and by the necessary spatial contiguity\nof stroke lesions. We used simulated and real deficit scores from a sample of\napproximately 100 individuals with left hemisphere stroke to evaluate two such\npermutation-based approaches. Using permutation to set a minimum cluster size\nidentified a region that systematically extended well beyond the true region,\neven under the most conservative settings tested here, making it ill-suited to\nidentifying brain-behavior relationships. In contrast, generalizing the\nstandard permutation-based family-wise error correction approach provided a\nprincipled way to balance false positives and false negatives. An\nimplementation of this continuous permutation-based FWER correction method is\navailable at https://gist.github.com/dmirman/05a92e0e9e0027f6fe6e528c648143d7\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 20:29:00 GMT"}, {"version": "v2", "created": "Wed, 14 Dec 2016 16:52:37 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Mirman", "Daniel", ""], ["Landrigan", "Jon-Frederick", ""], ["Kokolis", "Spiro", ""], ["Verillo", "Sean", ""], ["Ferrara", "Casey", ""], ["Pustina", "Dorian", ""]]}, {"id": "1612.04535", "submitter": "Kari Krizak Halle", "authors": "Kari Krizak Halle, Srdjan Djurovic, Ole Andreas Andreassen, Mette\n  Langaas", "title": "Is the familywise error rate in genomics controlled by methods based on\n  the effective number of independent tests?", "comments": "20 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In genome-wide association (GWA) studies the goal is to detect association\nbetween one or more genetic markers and a given phenotype. The number of\ngenetic markers in a GWA study can be in the order hundreds of thousands and\ntherefore multiple testing methods are needed. This paper presents a set of\npopular methods to be used to correct for multiple testing in GWA studies. All\nare based on the concept of estimating an effective number of independent\ntests. We compare these methods using simulated data and data from the TOP\nstudy, and show that the effective number of independent tests is not additive\nover blocks of independent genetic markers unless we assume a common value for\nthe local significance level. We also show that the reviewed methods based on\nestimating the effective number of independent tests in general do not control\nthe familywise error rate.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 08:45:05 GMT"}, {"version": "v2", "created": "Wed, 21 Dec 2016 09:58:41 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Halle", "Kari Krizak", ""], ["Djurovic", "Srdjan", ""], ["Andreassen", "Ole Andreas", ""], ["Langaas", "Mette", ""]]}, {"id": "1612.04555", "submitter": "S{\\o}ren F{\\o}ns Vind Nielsen", "authors": "Jesper L. Hinrich, S{\\o}ren F. V. Nielsen, Nicolai A. B. Riis, Casper\n  T. Eriksen, Jacob Fr{\\o}sig, Marco D. F. Kristensen, Mikkel N. Schmidt,\n  Kristoffer H. Madsen and Morten M{\\o}rup", "title": "Scalable Group Level Probabilistic Sparse Factor Analysis", "comments": "10 pages plus 5 pages appendix, Submitted to ICASSP 17", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many data-driven approaches exist to extract neural representations of\nfunctional magnetic resonance imaging (fMRI) data, but most of them lack a\nproper probabilistic formulation. We propose a group level scalable\nprobabilistic sparse factor analysis (psFA) allowing spatially sparse maps,\ncomponent pruning using automatic relevance determination (ARD) and subject\nspecific heteroscedastic spatial noise modeling. For task-based and resting\nstate fMRI, we show that the sparsity constraint gives rise to components\nsimilar to those obtained by group independent component analysis. The noise\nmodeling shows that noise is reduced in areas typically associated with\nactivation by the experimental design. The psFA model identifies sparse\ncomponents and the probabilistic setting provides a natural way to handle\nparameter uncertainties. The variational Bayesian framework easily extends to\nmore complex noise models than the presently considered.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 09:59:51 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Hinrich", "Jesper L.", ""], ["Nielsen", "S\u00f8ren F. V.", ""], ["Riis", "Nicolai A. B.", ""], ["Eriksen", "Casper T.", ""], ["Fr\u00f8sig", "Jacob", ""], ["Kristensen", "Marco D. F.", ""], ["Schmidt", "Mikkel N.", ""], ["Madsen", "Kristoffer H.", ""], ["M\u00f8rup", "Morten", ""]]}, {"id": "1612.04710", "submitter": "Karen Fuchs", "authors": "Karen Fuchs (1 and 2), Wolfgang P\\\"o{\\ss}necker (2), Gerhard Tutz (2)\n  ((1) Siemens AG, CT RDA SII CPS-DE, Munich, (2) Department of Statistics,\n  Ludwig-Maximilians-Universit\\\"at M\\\"unchen)", "title": "Classification of Functional Data with k-Nearest-Neighbor Ensembles by\n  Fitting Constrained Multinomial Logit Models", "comments": "The first replacement is due to an update of the data links. No other\n  changes took place with respect to the original submission. To reproduce\n  results of the cell chip or phoneme data application, files can now be\n  downloaded from\n  http://agfda.userweb.mwn.de/files/Fuchs_Poessnecker_Tutz_pcMLM_code_cellchip.zip\n  or\n  http://agfda.userweb.mwn.de/files/Fuchs_Poessnecker_Tutz_pcMLM_code_phonemes.zip", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the last decades, many methods for the analysis of functional data\nincluding classification methods have been developed. Nonetheless, there are\nissues that have not been adressed satisfactorily by currently available\nmethods, as, for example, feature selection combined with variable selection\nwhen using multiple functional covariates. In this paper, a functional ensemble\nis combined with a penalized and constrained multinomial logit model. It is\nshown that this synthesis yields a powerful classification tool for functional\ndata (possibly mixed with non-functional predictors), which also provides\nautomatic variable selection. The choice of an appropriate, sparsity-inducing\npenalty allows to estimate most model coefficients to exactly zero, and permits\nclass-specific coefficients in multiclass problems, such that feature selection\nis obtained. An additional constraint within the multinomial logit model\nensures that the model coefficients can be considered as weights. Thus, the\nestimation results become interpretable with respect to the discriminative\nimportance of the selected features, which is rated by a feature importance\nmeasure. In two application examples, data of a cell chip used for water\nquality monitoring experiments and phoneme data used for speech recognition,\nthe interpretability as well as the selection results are examined. The\nclassification performance is compared to various other classification\napproaches which are in common use.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 16:19:30 GMT"}, {"version": "v2", "created": "Mon, 6 Feb 2017 21:15:20 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Fuchs", "Karen", "", "1 and 2"], ["P\u00f6\u00dfnecker", "Wolfgang", ""], ["Tutz", "Gerhard", ""]]}, {"id": "1612.04838", "submitter": "Sebastian D\\\"ohler", "authors": "Sebastian D\\\"ohler", "title": "A discrete modification of the Benjamini-Yekutieli procedure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Benjamini-Yekutieli procedure is a multiple testing method that controls\nthe false discovery rate under arbitrary dependence of the $p$-values. A\nmodification of this and related procedures is proposed for the case when the\ntest statistics are discrete. It is shown that taking discreteness into account\ncan improve upon known procedures. The performance of this new procedure is\nevaluated for pharmacovigilance data and in a simulation study.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 21:05:20 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["D\u00f6hler", "Sebastian", ""]]}, {"id": "1612.05021", "submitter": "Jaeyong An", "authors": "Jaeyong An, P. R. Kumar, and Le Xie", "title": "Dynamic Modeling of Price Responsive Demand in Real-time Electricity\n  Market: Empirical Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the price responsiveness of electricity consumption\nfrom empirical commercial and industrial load data obtained from Texas.\nEmploying a dynamical system perspective, we show that price responsive demand\ncan be modeled as a hybrid of a Hammerstein model with delay following a price\nsurge, and a linear ARX model under moderate price changes. It is observed that\nelectricity consumption therefore has unique characteristics including (1)\nqualitatively distinct response between moderate and extremely high prices; and\n(2) a time delay associated with the response to high prices. It is shown that\nthese observed features may render traditional approaches to demand response\nand retail pricing based on classical economic theories ineffective. In\nparticular, ultimate real-time retail pricing may be limitedly beneficial than\nas considered in classical economic theories.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 11:23:52 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["An", "Jaeyong", ""], ["Kumar", "P. R.", ""], ["Xie", "Le", ""]]}, {"id": "1612.05198", "submitter": "Kaushik Jana Dr.", "authors": "Kaushik Jana, Debasis Sengupta, Subrata Kundu, Arindam Chakraborty and\n  Purnima Shaw", "title": "The Statistical Face of a Region under Monsoon Rainfall in Eastern India", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A region under rainfall is a contiguous spatial area receiving positive\nprecipitation at a particular time. The probabilistic behavior of such a region\nis an issue of interest in meteorological studies. A region under rainfall can\nbe viewed as a shape object of a special kind, where scale and rotational\ninvariance are not necessarily desirable attributes of a mathematical\nrepresentation. For modeling variation in objects of this type, we propose an\napproximation of the boundary that can be represented as a real valued\nfunction, and arrive at further approximation through functional principal\ncomponent analysis, after suitable adjustment for asymmetry and incompleteness\nin the data. The analysis of an open access satellite data set on monsoon\nprecipitation over Eastern India leads to explanation of most of the variation\nin shapes of the regions under rainfall through a handful of interpretable\nfunctions that can be further approximated parametrically. The most important\naspect of shape is found to be the size followed by contraction/elongation,\nmostly along two pairs of orthogonal axes. The different modes of variation are\nremarkably stable across calendar years and across different thresholds for\nminimum size of the region.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 19:19:05 GMT"}, {"version": "v2", "created": "Mon, 19 Dec 2016 15:16:29 GMT"}, {"version": "v3", "created": "Thu, 5 Jan 2017 12:53:03 GMT"}, {"version": "v4", "created": "Mon, 20 Feb 2017 12:35:33 GMT"}, {"version": "v5", "created": "Thu, 27 Apr 2017 09:13:20 GMT"}, {"version": "v6", "created": "Tue, 24 Oct 2017 12:50:18 GMT"}, {"version": "v7", "created": "Fri, 17 Aug 2018 13:59:27 GMT"}, {"version": "v8", "created": "Sat, 2 Mar 2019 12:38:12 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Jana", "Kaushik", ""], ["Sengupta", "Debasis", ""], ["Kundu", "Subrata", ""], ["Chakraborty", "Arindam", ""], ["Shaw", "Purnima", ""]]}, {"id": "1612.05226", "submitter": "Ludmila Brochini", "authors": "Ludmila Brochini, Antonio Galves, Pierre Hodara, Guilherme Ost and\n  Christophe Pouzat", "title": "Estimation of neuronal interaction graph from spike train data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main current issues in Neurobiology concerns the understanding of\ninterrelated spiking activity among multineuronal ensembles and differences\nbetween stimulus-driven and spontaneous activity in neurophysiological\nexperiments. Multi electrode array recordings that are now commonly used\nmonitor neuronal activity in the form of spike trains from many well identified\nneurons. A basic question when analyzing such data is the identification of the\ndirected graph describing \"synaptic coupling\" between neurons. In this article\nwe deal with this matter working with a high quality multielectrode array\nrecording dataset (Pouzat et al., 2015) from the first olfactory relay of the\nlocust, $Schistocerca$ $americana$. From a mathematical point of view this\npaper presents two novelties. First we propose a procedure allowing to deal\nwith the small sample sizes met in actual datasets. Moreover we address the\nsensitive case of partially observed networks. Our starting point is the\nprocedure introduced in Duarte et al. (2016). We evaluate the performance of\nboth original and improved procedures through simulation studies, which are\nalso used for parameter tuning and for exploring the effect of recording only a\nsmall subset of the neurons of a network.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 20:30:15 GMT"}, {"version": "v2", "created": "Wed, 11 Oct 2017 22:48:13 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Brochini", "Ludmila", ""], ["Galves", "Antonio", ""], ["Hodara", "Pierre", ""], ["Ost", "Guilherme", ""], ["Pouzat", "Christophe", ""]]}, {"id": "1612.05229", "submitter": "Patrick Laurie Davies Mr", "authors": "Laurie Davies, Walter Kr\\\"amer", "title": "Stylized Facts and Simulating Long Range Financial Data", "comments": "24 pages 12 figures, Discussion papers SFB 823, Technische\n  Universit\\\"at Dortmund, Germany 2015 48/15", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method (implemented in an R-program) to simulate long-range\ndaily stock-price data. The program reproduces various stylized facts much\nbetter than various parametric models from the extended GARCH-family. In\nparticular, the empirically observed changes in unconditional variance are\ntruthfully mirrored in the simulated data.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 20:38:21 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Davies", "Laurie", ""], ["Kr\u00e4mer", "Walter", ""]]}, {"id": "1612.05844", "submitter": "Bruce Desmarais", "authors": "Skyler J. Cranmer, Bruce A. Desmarais", "title": "What can we Learn from Predictive Modeling?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The large majority of inferences drawn in empirical political research follow\nfrom model-based associations (e.g. regression). Here, we articulate the\nbenefits of predictive modeling as a complement to this approach. Predictive\nmodels aim to specify a probabilistic model that provides a good fit to testing\ndata that were not used to estimate the model's parameters. Our goals are\nthreefold. First, we review the central benefits of this under-utilized\napproach from a perspective uncommon in the existing literature: we focus on\nhow predictive modeling can be used to complement and augment standard\nassociational analyses. Second, we advance the state of the literature by\nlaying out a simple set of benchmark predictive criteria. Third, we illustrate\nour approach through a detailed application to the prediction of interstate\nconflict.\n", "versions": [{"version": "v1", "created": "Sun, 18 Dec 2016 01:51:47 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Cranmer", "Skyler J.", ""], ["Desmarais", "Bruce A.", ""]]}, {"id": "1612.05852", "submitter": "Roger Bilisoly", "authors": "Roger Bilisoly", "title": "Searching for Patterns among Squares Modulo p", "comments": "Joint Statistical Meetings 2016 Proceedings", "journal-ref": "American Statistical Association Proceedings of the Joint\n  Statistical Meetings, 2016, Institute of Mathematical Statistics Section,\n  pages 1094-1100", "doi": null, "report-no": null, "categories": "stat.AP math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although squaring integers is deterministic, squares modulo a prime, $p$,\nappear to be random. First, because they are all generated by the\nmultiplicative linear congruential equation, $x_{i+1} = g^2 x_i \\mod p$, where\n$x_0 = 1$ and $g$ is any primitive root of $p$, a pseudorandom number heuristic\nsuggests that they are, in fact, unpredictable. Moreover, one type of\ncryptography makes use of discrete algorithms, which depends on the difficulty\nof solving $a = g^n$ for $n$ given $a$ and $g$. This suggests that the squares,\nwhich are exactly the even powers of $g$, are hard to identify. On the other\nhand, the Legendre symbol, $(a/p)$, which equals $1$ if a is a square modulo\n$p$ and $-1$ otherwise, has proven patterns. For example, $(ab/p) = (a/p)(b/p)$\nholds true, and this shows that squares modulo $p$ have some structure. This\npaper considers the randomness of the following sequence: $(1/p), (2/p), ...,\n((p-1)/p)$. Because it consists of binary data, the runs test is applied, which\nsuggests that the number of runs is exactly (p-1)/2. This turns out to be a\ntheorem proved by Aladov in 1896 that is not widely known. Consequently, this\nis an example of a number theory fact that is revealed naturally in a\nstatistical setting, but one that has rarely been noted by mathematicians.\n", "versions": [{"version": "v1", "created": "Sun, 18 Dec 2016 03:51:11 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Bilisoly", "Roger", ""]]}, {"id": "1612.05855", "submitter": "Jiang Wu", "authors": "Jiang Wu, Ricardas Zitikis", "title": "Should we opt for the Black Friday discounted price or wait until the\n  Boxing Day?", "comments": "16 pages;5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.TR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive an optimal strategy for minimizing the expected loss in the\ntwo-period economy when a pivotal decision needs to be made during the first\ntime period and cannot be subsequently reversed. Our interest in the problem\nhas been motivated by the classical shopper's dilemma during the Black Friday\npromotion period, and our solution crucially relies on the pioneering work of\nMcDonnell and Abbott on the two-envelope paradox.\n", "versions": [{"version": "v1", "created": "Sun, 18 Dec 2016 04:54:19 GMT"}], "update_date": "2016-12-31", "authors_parsed": [["Wu", "Jiang", ""], ["Zitikis", "Ricardas", ""]]}, {"id": "1612.06451", "submitter": "Chiara Perillo", "authors": "Chiara Perillo (1), Angelos Antonopoulos (2) and Christos Verikoukis\n  (2) ((1) University of Zurich, Department of Banking and Finance, Zurich,\n  Switzerland, (2) Telecommunications Technological Centre of Catalonia (CTTC),\n  Castelldefels, Barcelona, Spain)", "title": "Panel dataset description for econometric analysis of the ISP-OTT\n  relationship in the years 2008-2013", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CY q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The latest technological advancements in the telecommunications domain (e.g.,\nwidespread adoption of mobile devices, introduction of 5G wireless\ncommunications, etc.) have brought new stakeholders into the spotlight. More\nspecifically, Over-the-Top (OTT) providers have recently appeared, offering\ntheir services over the existing deployed telecommunication networks. The entry\nof the new players has changed the dynamics in the domain, as it creates\nconflicting situations with the Internet Service Providers (ISPs), who\ntraditionally dominate the area, motivating the necessity for novel analytical\nstudies for this relationship. However, despite the importance of accessing\nreal observational data, there is no database with the aggregate information\nthat can serve as a solid base for this research. To that end, this document\nprovides a detailed summary report for financial and statistic data for the\nperiod 2008-2013 that can be exploited for realistic econometric models that\nwill provide useful insights on this topic. The document summarizes data from\nvarious sources with regard to the ISP revenues and Capital Expenditures\n(CAPEX), the OTT revenues, the Internet penetration and the Gross Domestic\nProduct (GDP), taking into account three big OTT providers (i.e., Facebook,\nSkype, WhatsApp) and ten major ISPs that operate in seven different countries.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 23:05:35 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Perillo", "Chiara", ""], ["Antonopoulos", "Angelos", ""], ["Verikoukis", "Christos", ""]]}, {"id": "1612.06468", "submitter": "Richard Everitt", "authors": "Richard G Everitt and Richard Culliford and Felipe Medina-Aguayo and\n  Daniel J Wilson", "title": "Sequential Monte Carlo with transformations", "comments": null, "journal-ref": "Statistics and Computing, 2020", "doi": "10.1007/s11222-019-09903-y", "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces methodology for performing Bayesian inference\nsequentially on a sequence of posteriors on spaces of different dimensions. We\nshow how this may be achieved through the use of sequential Monte Carlo (SMC)\nsamplers (Del Moral et al., 2006, 2007), making use of the full flexibility of\nthis framework in order that the method is computationally efficient. In\nparticular, we introduce the innovation of using deterministic transformations\nto move particles effectively between target distributions with different\ndimensions. This approach, combined with adaptive methods, yields an extremely\nflexible and general algorithm for Bayesian model comparison that is suitable\nfor use in applications where the acceptance rate in reversible jump Markov\nchain Monte Carlo (RJMCMC) is low. We demonstrate this approach on the\nwell-studied problem of model comparison for mixture models, and for the novel\napplication of inferring coalescent trees sequentially, as data arrives.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 00:57:02 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2018 15:09:36 GMT"}, {"version": "v3", "created": "Wed, 11 Sep 2019 17:47:01 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Everitt", "Richard G", ""], ["Culliford", "Richard", ""], ["Medina-Aguayo", "Felipe", ""], ["Wilson", "Daniel J", ""]]}, {"id": "1612.06589", "submitter": "Noriyoshi Sukegawa", "authors": "Naoki Nishimura, Noriyoshi Sukegawa, Yuichi Takano, Jiro Iwanaga", "title": "A Latent-class Model for Estimating Product-choice Probabilities from\n  Clickstream Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper analyzes customer product-choice behavior based on the recency and\nfrequency of each customer's page views on e-commerce sites. Recently, we\ndevised an optimization model for estimating product-choice probabilities that\nsatisfy monotonicity, convexity, and concavity constraints with respect to\nrecency and frequency. This shape-restricted model delivered high predictive\nperformance even when there were few training samples. However, typical\ne-commerce sites deal in many different varieties of products, so the\npredictive performance of the model can be further improved by integration of\nsuch product heterogeneity. For this purpose, we develop a novel latent-class\nshape-restricted model for estimating product-choice probabilities for each\nlatent class of products. We also give a tailored expectation-maximization\nalgorithm for parameter estimation. Computational results demonstrate that\nhigher predictive performance is achieved with our latent-class model than with\nthe previous shape-restricted model and common latent-class logistic\nregression.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 10:22:37 GMT"}], "update_date": "2017-01-01", "authors_parsed": [["Nishimura", "Naoki", ""], ["Sukegawa", "Noriyoshi", ""], ["Takano", "Yuichi", ""], ["Iwanaga", "Jiro", ""]]}, {"id": "1612.06738", "submitter": "Sujit Kumar Sahoo Ph.D.", "authors": "Sujit Kumar Sahoo", "title": "Local Sparse Approximation for Image Restoration with Adaptive Block\n  Size Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the problem of image restoration (denoising and inpainting) is\napproached using sparse approximation of local image blocks. The local image\nblocks are extracted by sliding square windows over the image. An adaptive\nblock size selection procedure for local sparse approximation is proposed,\nwhich affects the global recovery of underlying image. Ideally the adaptive\nlocal block selection yields the minimum mean square error (MMSE) in recovered\nimage. This framework gives us a clustered image based on the selected block\nsize, then each cluster is restored separately using sparse approximation. The\nresults obtained using the proposed framework are very much comparable with the\nrecently proposed image restoration techniques.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 16:28:48 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Sahoo", "Sujit Kumar", ""]]}, {"id": "1612.06746", "submitter": "Luis Enrique Correa Rocha Dr", "authors": "Kazuki Fujita and Shigeru Shinomoto and Luis E C Rocha", "title": "Correlations and forecast of death tolls in the Syrian conflict", "comments": "Comments and suggestions welcomed. 8 pages, 7 figures", "journal-ref": "Scientific Reports 7, 15737 (2017)", "doi": "10.1038/s41598-017-15945-x", "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Syrian civil war has been ongoing since 2011 and has already caused\nthousands of deaths. The analysis of death tolls helps to understand the\ndynamics of the conflict and to better allocate resources to the affected\nareas. In this article, we use information on the daily number of deaths to\nstudy temporal and spatial correlations in the data, and exploit this\ninformation to forecast events of deaths. We find that the number of deaths per\nday follows a log-normal distribution during the conflict. We have also\nidentified strong correlations between cities and on consecutive days, implying\nthat major deaths in one location are typically followed by major deaths in\nboth the same location and in other areas. We find that war-related deaths are\nnot random events and observing death tolls in some cities helps to better\npredict these numbers across the system.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 16:45:34 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Fujita", "Kazuki", ""], ["Shinomoto", "Shigeru", ""], ["Rocha", "Luis E C", ""]]}, {"id": "1612.06747", "submitter": "P\\'eter L\\'aszl\\'o Juh\\'asz", "authors": "P\\'eter L. Juh\\'asz and J\\'ozsef St\\'eger and D\\'aniel Kondor and\n  G\\'abor Vattay", "title": "A Bayesian Approach to Identify Bitcoin Users", "comments": null, "journal-ref": "PLoS ONE 13(12): e0207000 (2018)", "doi": "10.1371/journal.pone.0207000", "report-no": null, "categories": "stat.AP cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bitcoin is a digital currency and electronic payment system operating over a\npeer-to-peer network on the Internet. One of its most important properties is\nthe high level of anonymity it provides for its users. The users are identified\nby their Bitcoin addresses, which are random strings in the public records of\ntransactions, the blockchain. When a user initiates a Bitcoin-transaction, his\nBitcoin client program relays messages to other clients through the Bitcoin\nnetwork. Monitoring the propagation of these messages and analyzing them\ncarefully reveal hidden relations. In this paper, we develop a mathematical\nmodel using a probabilistic approach to link Bitcoin addresses and transactions\nto the originator IP address. To utilize our model, we carried out experiments\nby installing more than a hundred modified Bitcoin clients distributed in the\nnetwork to observe as many messages as possible. During a two month observation\nperiod we were able to identify several thousand Bitcoin clients and bind their\ntransactions to geographical locations.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 16:46:27 GMT"}, {"version": "v2", "created": "Wed, 21 Dec 2016 15:33:57 GMT"}, {"version": "v3", "created": "Thu, 22 Dec 2016 13:42:17 GMT"}, {"version": "v4", "created": "Thu, 9 Mar 2017 08:24:23 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Juh\u00e1sz", "P\u00e9ter L.", ""], ["St\u00e9ger", "J\u00f3zsef", ""], ["Kondor", "D\u00e1niel", ""], ["Vattay", "G\u00e1bor", ""]]}, {"id": "1612.06833", "submitter": "Georgios Giasemidis Dr", "authors": "Georgios Giasemidis, Stephen Haben, Tamsin Lee, Colin Singleton, Peter\n  Grindrod", "title": "A Genetic Algorithm Approach for Modelling Low Voltage Network Demands", "comments": "Changes to match published version", "journal-ref": "Applied Energy 203 (2017) 463-473", "doi": "10.1016/j.apenergy.2017.06.057", "report-no": null, "categories": "stat.AP physics.data-an physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distribution network operators (DNOs) are increasingly concerned about the\nimpact of low carbon technologies on the low voltage (LV) networks. More\nadvanced metering infrastructures provide numerous opportunities for more\naccurate load flow analysis of the LV networks. However, such data may not be\nreadily available for DNOs and in any case is likely to be expensive. Modelling\ntools are required which can provide realistic, yet accurate, load profiles as\ninput for a network modelling tool, without needing access to large amounts of\nmonitored customer data. In this paper we outline some simple methods for\naccurately modelling a large number of unmonitored residential customers at the\nLV level. We do this by a process we call buddying, which models unmonitored\ncustomers by assigning them load profiles from a limited sample of monitored\ncustomers who have smart meters. Hence the presented method requires access to\nonly a relatively small amount of domestic customers' data. The method is\nefficiently optimised using a genetic algorithm to minimise a weighted cost\nfunction between matching the substation data and the individual mean daily\ndemands. Hence we can show the effectiveness of substation monitoring in LV\nnetwork modelling. Using real LV network modelling, we show that our methods\nperform significantly better than a comparative Monte Carlo approach, and\nprovide a description of the peak demand behaviour.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 20:19:51 GMT"}, {"version": "v2", "created": "Mon, 26 Jun 2017 20:03:29 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Giasemidis", "Georgios", ""], ["Haben", "Stephen", ""], ["Lee", "Tamsin", ""], ["Singleton", "Colin", ""], ["Grindrod", "Peter", ""]]}, {"id": "1612.06887", "submitter": "Ick Hoon Jin", "authors": "Ick Hoon Jin and Minjeong Jeon", "title": "A Doubly Latent Space Joint Model for Local Item and Person Dependence\n  in the Analysis of Item Response Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Item response theory (IRT) models explain an observed item response as a\nfunction of a respondent's latent trait and the item's property. IRT is one of\nthe most widely utilized tools for item response analysis; however, local item\nand person independence, which is a critical assumption for IRT, is often\nviolated in real testing situations. In this article, we propose a new type of\nanalytical approach for item response data that does not require standard local\nindependence assumptions. By adapting a latent space joint modeling approach,\nour proposed model can estimate pairwise distances to represent the item and\nperson dependence structures, from which item and person clusters in latent\nspaces can be identified. We provide an empirical data analysis to illustrate\nan application of the proposed method. A simulation study was also provided to\nevaluate the performance of the proposed method in comparison to an existing\nmethod.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 21:35:36 GMT"}, {"version": "v2", "created": "Sat, 18 Feb 2017 18:45:39 GMT"}, {"version": "v3", "created": "Tue, 2 May 2017 21:08:37 GMT"}, {"version": "v4", "created": "Sat, 12 May 2018 15:46:17 GMT"}, {"version": "v5", "created": "Fri, 1 Jun 2018 09:03:02 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Jin", "Ick Hoon", ""], ["Jeon", "Minjeong", ""]]}, {"id": "1612.06930", "submitter": "Ishapathik Das Dr", "authors": "I. Das", "title": "Benchmark Dose Estimation using a Family of Link Functions", "comments": "22 pages, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes a method of estimating benchmark dose (BMD) using a\nfamily of link functions in binomial response models dealing with model\nuncertainty problems. Researchers usually estimate the BMD using binomial\nresponse models with a single link function. Several forms of link function\nhave been proposed to fit dose response models to estimate the BMD and the\ncorresponding benchmark dose lower bound (BMDL). However, if the assumed link\nis not correct, then the estimated BMD and BMDL from the fitted model may not\nbe accurate. To account for model uncertainty, model averaging (MA) methods are\nproposed to estimate BMD averaging over a model space containing a finite\nnumber of standard models. Usual model averaging focuses on a pre-specified\nlist of parametric models leading to pitfalls when none of the models in the\nlist is the correct model. Here, an alternative which augments an initial list\nof parametric models with an infinite number of additional models having\nvarying links has been proposed. In addition, different methods for estimating\nBMDL based on the family of link functions are derived. The proposed approach\nis compared with MA in a simulation study and applied to a real data set.\nSimulation studies are also conducted to compare the four methods of estimating\nBMDL.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 00:38:33 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Das", "I.", ""]]}, {"id": "1612.07034", "submitter": "Anders Eklund", "authors": "Bertil Wegmann, Anders Eklund, Mattias Villani", "title": "Bayesian Non-Central Chi Regression For Neuroimaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a regression model for non-central $\\chi$ (NC-$\\chi$) distributed\nfunctional magnetic resonance imaging (fMRI) and diffusion weighted imaging\n(DWI) data, with the heteroscedastic Rician regression model as a prominent\nspecial case. The model allows both parameters in the NC-$\\chi$ distribution to\nbe linked to explanatory variables, with the relevant covariates automatically\nchosen by Bayesian variable selection. A highly efficient Markov chain Monte\nCarlo (MCMC) algorithm is proposed for simulating from the joint Bayesian\nposterior distribution of all model parameters and the binary covariate\nselection indicators. Simulated fMRI data is used to demonstrate that the\nRician model is able to localize brain activity much more accurately than the\ntraditionally used Gaussian model at low signal-to-noise ratios. Using a\ndiffusion dataset from the Human Connectome Project, it is also shown that the\ncommonly used approximate Gaussian noise model underestimates the mean\ndiffusivity (MD) and the fractional anisotropy (FA) in the single-diffusion\ntensor model compared to the theoretically correct Rician model.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 09:52:13 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Wegmann", "Bertil", ""], ["Eklund", "Anders", ""], ["Villani", "Mattias", ""]]}, {"id": "1612.07439", "submitter": "Jie Peng", "authors": "Hao Yan, Owen Carmichael, Debashis Paul and Jie Peng", "title": "Estimating fiber orientation distribution from diffusion MRI with\n  spherical needlets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for estimation of the fiber orientation\ndistribution (FOD) function based on diffusion-weighted Magnetic Resonance\nImaging (D-MRI) data. We formulate the problem of FOD estimation as a\nregression problem through spherical deconvolution and a sparse representation\nof the FOD by a spherical needlets basis that form a multi-resolution tight\nframe for spherical functions. This sparse representation allows us to estimate\nFOD by an $l_1$-penalized regression under a non-negativity constraint. The\nresulting convex optimization problem is solved by an alternating direction\nmethod of multipliers (ADMM) algorithm. The proposed method leads to a\nreconstruction of the FODs that is accurate, has low variability and preserves\nsharp features. Through extensive experiments, we demonstrate the effectiveness\nand favorable performance of the proposed method compared with two existing\nmethods. Particularly, we show the ability of the proposed method in\nsuccessfully resolving fiber crossing at small angles and in automatically\nidentifying isotropic diffusion. We also apply the proposed method to real 3T\nD-MRI data sets of healthy elderly individuals. The results show realistic\ndescriptions of crossing fibers that are more accurate and less noisy than\ncompeting methods even with a relatively small number of gradient directions.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 04:29:43 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Yan", "Hao", ""], ["Carmichael", "Owen", ""], ["Paul", "Debashis", ""], ["Peng", "Jie", ""]]}, {"id": "1612.07801", "submitter": "Luyan Ji", "authors": "Luyan Ji, Jie Wang, Xiurui Geng, Peng Gong", "title": "Probabilistic graphical model based approach for water mapping using\n  GaoFen-2 (GF-2) high resolution imagery and Landsat 8 time series", "comments": "17 pages, 9 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this paper is to evaluate the potential of Gaofen-2 (GF-2)\nhigh resolution multispectral sensor (MS) and panchromatic (PAN) imagery on\nwater mapping. Difficulties of water mapping on high resolution data includes:\n1) misclassification between water and shadows or other low-reflectance ground\nobjects, which is mostly caused by the spectral similarity within the given\nband range; 2) small water bodies with size smaller than the spatial resolution\nof MS image. To solve the confusion between water and low-reflectance objects,\nthe Landsat 8 time series with two shortwave infrared (SWIR) bands is added\nbecause water has extremely strong absorption in SWIR. In order to integrate\nthe three multi-sensor, multi-resolution data sets, the probabilistic graphical\nmodel (PGM) is utilized here with conditional probability distribution defined\nmainly based on the size of each object. For comparison, results from the SVM\nclassifier on the PCA fused and MS data, thresholding method on the PAN image,\nand water index method on the Landsat data are computed. The confusion matrices\nare calculated for all the methods. The results demonstrate that the PGM method\ncan achieve the best performance with the highest overall accuracy. Moreover,\nsmall rivers can also be extracted by adding weight on the PAN result in PGM.\nFinally, the post-classification procedure is applied on the PGM result to\nfurther exclude misclassification in shadow and water-land boundary regions.\nAccordingly, the producer's, user's and overall accuracy are all increased,\nindicating the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 02:59:54 GMT"}], "update_date": "2016-12-26", "authors_parsed": [["Ji", "Luyan", ""], ["Wang", "Jie", ""], ["Geng", "Xiurui", ""], ["Gong", "Peng", ""]]}, {"id": "1612.07938", "submitter": "Soghra Bohlurihajjar", "authors": "S. B. Hajjar and S. Khazaei", "title": "Bayesian Nonparametric Survival Analysis using mixture of Burr XII\n  distributions", "comments": "20 pages, 5 Images and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, the Bayesian nonparametric approach in survival studies attracts\nmuch more attentions. Because of multi modality in survival data, the mixture\nmodels are very common in this field. One of the famous priors on Bayesian\nnonparametric models is Dirichlet process prior. In this paper we introduce a\nBayesian nonparametric mixture model with Burr distribution(Burr type XII) as\nthe kernel of mixture model. Since the Burr distribution shares good properties\nof common distributions on survival analysis, it has more flexibility than\nother distributions. By applying this model to simulated and real failure time\ndata sets, we show the preference of this model and compare it with other\nDirichlet process mixture models with different kernels. And also we show that\nthis model can be applied for the right censored data. For calculating the\nposterior of the parameters for inference and modeling, we used the MCMC\nsimulation methods, especially Gibbs sampling.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 11:19:55 GMT"}], "update_date": "2016-12-26", "authors_parsed": [["Hajjar", "S. B.", ""], ["Khazaei", "S.", ""]]}, {"id": "1612.08062", "submitter": "Minjie Fan", "authors": "Minjie Fan, Debashis Paul, Thomas C.M. Lee, Tomoko Matsuo", "title": "Modeling Tangential Vector Fields on a Sphere", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physical processes that manifest as tangential vector fields on a sphere are\ncommon in geophysical and environmental sciences. These naturally occurring\nvector fields are often subject to physical constraints, such as being\ncurl-free or divergence-free. We construct a new class of parametric models for\ncross-covariance functions of curl-free and divergence-free vector fields that\nare tangential to the unit sphere. These models are constructed by applying the\nsurface gradient or the surface curl operator to scalar random potential fields\ndefined on the unit sphere. We propose a likelihood-based estimation procedure\nfor the model parameters and show that fast computation is possible even for\nlarge data sets when the observations are on a regular latitude-longitude grid.\nCharacteristics and utility of the proposed methodology are illustrated through\nsimulation studies and by applying it to an ocean surface wind velocity data\nset collected through satellite-based scatterometry remote sensing. We also\ncompare the performance of the proposed model with a class of bivariate\nMat\\'ern models in terms of estimation and prediction, and demonstrate that the\nproposed model is superior in capturing certain physical characteristics of the\nwind fields.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 19:01:43 GMT"}], "update_date": "2016-12-26", "authors_parsed": [["Fan", "Minjie", ""], ["Paul", "Debashis", ""], ["Lee", "Thomas C. M.", ""], ["Matsuo", "Tomoko", ""]]}, {"id": "1612.08165", "submitter": "Geoffrey Stewart Morrison", "authors": "Geoffrey Stewart Morrison", "title": "Calculation of forensic likelihood ratios: Use of Monte Carlo\n  simulations to compare the output of score-based approaches with true\n  likelihood-ratio values", "comments": "59 pages. A version of this paper was presented as: Morrison G.S.,\n  Enzinger E., Forensic likelihood ratios should not be based on similarity\n  scores or difference scores, 9th International Conference on Forensic\n  Inference and Statistics (ICFIS), August 2014, Leiden, The Netherlands", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A group of approaches for calculating forensic likelihood ratios first\ncalculates scores which quantify the degree of difference or the degree of\nsimilarity between pairs of samples, then converts those scores to likelihood\nratios. In order for a score-based approach to produce a forensically\ninterpretable likelihood ratio, however, in addition to accounting for the\nsimilarity of the questioned sample with respect to the known sample, it must\nalso account for the typicality of the questioned sample with respect to the\nrelevant population. The present paper explores a number of score-based\napproaches using different types of scores and different procedures for\nconverting scores to likelihood ratios. Monte Carlo simulations are used to\ncompare the output of these approaches to true likelihood-ratio values\ncalculated on the basis of the distribution specified for a simulated\npopulation. The inadequacy of approaches based on similarity-only or\ndifference-only scores is illustrated, and the relative performance of\ndifferent approaches which take account of both similarity and typicality is\nassessed.\n", "versions": [{"version": "v1", "created": "Sat, 24 Dec 2016 11:21:12 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Morrison", "Geoffrey Stewart", ""]]}, {"id": "1612.08194", "submitter": "Mainak Jas", "authors": "Mainak Jas, Denis A. Engemann, Yousra Bekhti, Federico Raimondo,\n  Alexandre Gramfort", "title": "Autoreject: Automated artifact rejection for MEG and EEG data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an automated algorithm for unified rejection and repair of bad\ntrials in magnetoencephalography (MEG) and electroencephalography (EEG)\nsignals. Our method capitalizes on cross-validation in conjunction with a\nrobust evaluation metric to estimate the optimal peak-to-peak threshold -- a\nquantity commonly used for identifying bad trials in M/EEG. This approach is\nthen extended to a more sophisticated algorithm which estimates this threshold\nfor each sensor yielding trial-wise bad sensors. Depending on the number of bad\nsensors, the trial is then repaired by interpolation or by excluding it from\nsubsequent analysis. All steps of the algorithm are fully automated thus\nlending itself to the name Autoreject.\n  In order to assess the practical significance of the algorithm, we conducted\nextensive validation and comparison with state-of-the-art methods on four\npublic datasets containing MEG and EEG recordings from more than 200 subjects.\nComparison include purely qualitative efforts as well as quantitatively\nbenchmarking against human supervised and semi-automated preprocessing\npipelines. The algorithm allowed us to automate the preprocessing of MEG data\nfrom the Human Connectome Project (HCP) going up to the computation of the\nevoked responses. The automated nature of our method minimizes the burden of\nhuman inspection, hence supporting scalability and reliability demanded by data\nanalysis in modern neuroscience.\n", "versions": [{"version": "v1", "created": "Sat, 24 Dec 2016 15:41:23 GMT"}, {"version": "v2", "created": "Thu, 5 Jan 2017 15:39:13 GMT"}, {"version": "v3", "created": "Wed, 12 Jul 2017 21:55:37 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Jas", "Mainak", ""], ["Engemann", "Denis A.", ""], ["Bekhti", "Yousra", ""], ["Raimondo", "Federico", ""], ["Gramfort", "Alexandre", ""]]}, {"id": "1612.08363", "submitter": "Xiaodong Yan", "authors": "Yan Xiao-Dong, Xie Jin-Han, Ding Xian-Wen, Wang Zhi-Qiang and Tang\n  Nian-Sheng", "title": "Fused Mean-variance Filter for Feature Screening", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel model-free screening procedure for ultrahigh\ndimensional data analysis. By utilizing slicing technique which has been\nsuccessfully ap- plied to continuous variables, we construct a new index called\nthe fused mean-variance for feature screening. This method has the following\nmerits: (i) it is model-free, i.e., without specifying regression form of\npredictors and response variable; (ii) it can be used to analyze various types\nof variables including discrete, categorical and continuous vari- ables; (iii)\nit still works well even when the covariates/random errors are heavy-tailed or\nthe predictors are strongly dependent. Under some regularity conditions, we\nestablish the sure screening and rank consistency. Simulation studies are\nconducted to assess the performance of the proposed approach. A real data is\nused to illustrate the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 26 Dec 2016 11:06:43 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Xiao-Dong", "Yan", ""], ["Jin-Han", "Xie", ""], ["Xian-Wen", "Ding", ""], ["Zhi-Qiang", "Wang", ""], ["Nian-Sheng", "Tang", ""]]}, {"id": "1612.08423", "submitter": "Marc Balducci", "authors": "Marc Balducci and Brandon Jones and Alireza Doostan", "title": "Orbit Uncertainty Propagation and Sensitivity Analysis With Separated\n  Representations", "comments": null, "journal-ref": null, "doi": "10.1007/s10569-017-9767-7", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most approximations for stochastic differential equations with\nhigh-dimensional, non-Gaussian inputs suffer from a rapid (e.g., exponential)\nincrease of computational cost, an issue known as the curse of dimensionality.\nIn astrodynamics, this results in reduced accuracy when propagating an\norbit-state probability density function. This paper considers the application\nof separated representations for orbit uncertainty propagation, where future\nstates are expanded into a sum of products of univariate functions of initial\nstates and other uncertain parameters. An accurate generation of separated\nrepresentation requires a number of state samples that is linear in the\ndimension of input uncertainties. The computation cost of a separated\nrepresentation scales linearly with respect to the sample count, thereby\nimproving tractability when compared to methods that suffer from the curse of\ndimensionality. In addition to detailed discussions on their construction and\nuse in sensitivity analysis, this paper presents results for three test cases\nof an Earth orbiting satellite. The first two cases demonstrate that\napproximation via separated representations produces a tractable solution for\npropagating the Cartesian orbit-state uncertainty with up to 20 uncertain\ninputs. The third case, which instead uses Equinoctial elements, reexamines a\nscenario presented in the literature and employs the proposed method for\nsensitivity analysis to more thoroughly characterize the relative effects of\nuncertain inputs on the propagated state.\n", "versions": [{"version": "v1", "created": "Mon, 26 Dec 2016 18:43:23 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Balducci", "Marc", ""], ["Jones", "Brandon", ""], ["Doostan", "Alireza", ""]]}, {"id": "1612.08430", "submitter": "Mario Hellmich", "authors": "Mario Hellmich", "title": "Component Importance Based on Dependence Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the construction of component importance measures for binary\ncoherent reliability systems from known stochastic dependence measures by\nmeasuring the dependence between system and component failures. We treat both\nthe time-dependent case in which the system and its components are described by\nbinary random variables at a fixed instant as well as the continuous time case\nwhere the system and component life times are random variables. As dependence\nmeasures we discuss covariance and mutual information, the latter being based\non Shannon entropy. We prove some basic properties of the resulting importance\nmeasures and obtain results on importance ordering of components.\n", "versions": [{"version": "v1", "created": "Mon, 26 Dec 2016 19:32:33 GMT"}, {"version": "v2", "created": "Fri, 13 Oct 2017 13:25:07 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Hellmich", "Mario", ""]]}, {"id": "1612.08617", "submitter": "Damjan Vukcevic", "authors": "Robert K. Mahar, John B. Carlin, Sarath Ranganathan, Anne-Louise\n  Ponsonby, Peter Vuillermin, Damjan Vukcevic", "title": "Bayesian modelling of lung function data from multiple-breath washout\n  tests", "comments": null, "journal-ref": "Statistics in Medicine (2018) 37(12):2016-2033", "doi": "10.1002/sim.7650", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Paediatric respiratory researchers have widely adopted the multiple-breath\nwashout (MBW) test because it allows assessment of lung function in unsedated\ninfants and is well suited to longitudinal studies of lung development and\ndisease. However, a substantial proportion of MBW tests in infants fail current\nacceptability criteria. We hypothesised that a model-based approach to\nanalysing the data, in place of traditional simple empirical summaries, would\nenable more efficient use of these tests. We therefore developed a novel\nstatistical model for infant MBW data and applied it to 1,197 tests from 432\nindividuals from a large birth cohort study. We focus on Bayesian estimation of\nthe lung clearance index (LCI), the most commonly used summary of lung function\nfrom MBW tests. Our results show that the model provides an excellent fit to\nthe data and shed further light on statistical properties of the standard\nempirical approach. Furthermore, the modelling approach enables LCI to be\nestimated using tests with different degrees of completeness, something not\npossible with the standard approach. Our model therefore allows previously\nunused data to be used rather than discarded, as well as routine use of shorter\ntests without significant loss of precision. Beyond our specific application,\nour work illustrates a number of important aspects of Bayesian modelling in\npractice, such as the importance of hierarchical specifications to account for\nrepeated measurements and the value of model checking via posterior predictive\ndistributions.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 12:59:34 GMT"}, {"version": "v2", "created": "Sun, 5 Nov 2017 12:21:56 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Mahar", "Robert K.", ""], ["Carlin", "John B.", ""], ["Ranganathan", "Sarath", ""], ["Ponsonby", "Anne-Louise", ""], ["Vuillermin", "Peter", ""], ["Vukcevic", "Damjan", ""]]}, {"id": "1612.08699", "submitter": "Kirk Bansak", "authors": "Kirk Bansak", "title": "Comparative Causal Mediation and Relaxing the Assumption of No\n  Mediator-Outcome Confounding: An Application to International Law and\n  Audience Costs", "comments": "This manuscript has been accepted for publication by Political\n  Analysis and will appear in a revised form subject to peer review and/or\n  input from the journal's editor. End-users of this manuscript may only make\n  use of it for private research and study and may not distribute it further", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experiments often include multiple treatments, with the primary goal to\ncompare the causal effects of those treatments. This study focuses on comparing\nthe causal anatomies of multiple treatments through the use of causal mediation\nanalysis. It proposes a novel set of comparative causal mediation (CCM)\nestimands that compare the mediation effects of different treatments via a\ncommon mediator. Further, it derives the properties of a set of estimators for\nthe CCM estimands and shows these estimators to be consistent (or conservative)\nunder assumptions that do not require the absence of unobserved confounding of\nthe mediator-outcome relationship, which is a strong and nonrefutable\nassumption that must typically be made for consistent estimation of individual\ncausal mediation effects. To illustrate the method, the study presents an\noriginal application investigating whether and how the international legal\nstatus of a foreign policy commitment can increase the domestic political\n\"audience costs\" that democratic governments suffer for violating such a\ncommitment. The results provide novel evidence that international legalization\ncan enhance audience costs via multiple causal channels, including by\namplifying the perceived immorality of violating the commitment.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 18:19:42 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 19:53:47 GMT"}, {"version": "v3", "created": "Wed, 4 Apr 2018 20:34:39 GMT"}, {"version": "v4", "created": "Mon, 1 Jul 2019 19:48:17 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Bansak", "Kirk", ""]]}, {"id": "1612.08804", "submitter": "Dane Taylor", "authors": "Dane Taylor, Juan G. Restrepo and Francois G. Meyer", "title": "Ensemble-based estimates of eigenvector error for empirical covariance\n  matrices", "comments": "24 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cond-mat.dis-nn math.PR stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covariance matrices are fundamental to the analysis and forecast of economic,\nphysical and biological systems. Although the eigenvalues $\\{\\lambda_i\\}$ and\neigenvectors $\\{{\\bf u}_i\\}$ of a covariance matrix are central to such\nendeavors, in practice one must inevitably approximate the covariance matrix\nbased on data with finite sample size $n$ to obtain empirical eigenvalues\n$\\{\\tilde{\\lambda}_i\\}$ and eigenvectors $\\{\\tilde{{\\bf u}}_i\\}$, and therefore\nunderstanding the error so introduced is of central importance. We analyze\neigenvector error $\\|{\\bf u}_i - \\tilde{{\\bf u}}_i \\|^2$ while leveraging the\nassumption that the true covariance matrix having size $p$ is drawn from a\nmatrix ensemble with known spectral properties---particularly, we assume the\ndistribution of population eigenvalues weakly converges as $p\\to\\infty$ to a\nspectral density $\\rho(\\lambda)$ and that the spacing between population\neigenvalues is similar to that for the Gaussian orthogonal ensemble. Our\napproach complements previous analyses of eigenvector error that require the\nfull set of eigenvalues to be known, which can be computationally infeasible\nwhen $p$ is large. To provide a scalable approach for uncertainty\nquantification of eigenvector error, we consider a fixed eigenvalue $\\lambda$\nand approximate the distribution of the expected square error $r=\n\\mathbb{E}\\left[\\| {\\bf u}_i - \\tilde{{\\bf u}}_i \\|^2\\right]$ across the matrix\nensemble for all ${\\bf u}_i$ associated with $\\lambda_i=\\lambda$. We find, for\nexample, that for sufficiently large matrix size $p$ and sample size $n>p$, the\nprobability density of $r$ scales as $1/nr^2$. This power-law scaling implies\nthat eigenvector error is extremely heterogeneous---even if $r$ is very small\nfor most eigenvectors, it can be large for others with non-negligible\nprobability. We support this and further results with numerical experiments.\n", "versions": [{"version": "v1", "created": "Wed, 28 Dec 2016 05:04:35 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 19:40:23 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Taylor", "Dane", ""], ["Restrepo", "Juan G.", ""], ["Meyer", "Francois G.", ""]]}, {"id": "1612.09106", "submitter": "Mingjun Zhong", "authors": "Chaoyun Zhang, Mingjun Zhong, Zongzuo Wang, Nigel Goddard, Charles\n  Sutton", "title": "Sequence-to-point learning with neural networks for nonintrusive load\n  monitoring", "comments": "8 pages, 3 figures", "journal-ref": "The Thirty-Second AAAI Conference on Artificial Intelligence\n  (AAAI-18), 2018", "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy disaggregation (a.k.a nonintrusive load monitoring, NILM), a\nsingle-channel blind source separation problem, aims to decompose the mains\nwhich records the whole house electricity consumption into appliance-wise\nreadings. This problem is difficult because it is inherently unidentifiable.\nRecent approaches have shown that the identifiability problem could be reduced\nby introducing domain knowledge into the model. Deep neural networks have been\nshown to be a promising approach for these problems, but sliding windows are\nnecessary to handle the long sequences which arise in signal processing\nproblems, which raises issues about how to combine predictions from different\nsliding windows. In this paper, we propose sequence-to-point learning, where\nthe input is a window of the mains and the output is a single point of the\ntarget appliance. We use convolutional neural networks to train the model.\nInterestingly, we systematically show that the convolutional neural networks\ncan inherently learn the signatures of the target appliances, which are\nautomatically added into the model to reduce the identifiability problem. We\napplied the proposed neural network approaches to real-world household energy\ndata, and show that the methods achieve state-of-the-art performance, improving\ntwo standard error measures by 84% and 92%.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 11:47:23 GMT"}, {"version": "v2", "created": "Fri, 24 Feb 2017 12:42:06 GMT"}, {"version": "v3", "created": "Mon, 18 Sep 2017 08:37:11 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Zhang", "Chaoyun", ""], ["Zhong", "Mingjun", ""], ["Wang", "Zongzuo", ""], ["Goddard", "Nigel", ""], ["Sutton", "Charles", ""]]}, {"id": "1612.09123", "submitter": "Richard Tol", "authors": "Richard S.J. Tol", "title": "Population and trends in the global mean temperature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.EC econ.EM physics.ao-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Fisher Ideal index, developed to measure price inflation, is applied to\ndefine a population-weighted temperature trend. This method has the advantages\nthat the trend is representative for the population distribution throughout the\nsample but without conflating the trend in the population distribution and the\ntrend in the temperature. I show that the trend in the global area-weighted\naverage surface air temperature is different in key details from the\npopulation-weighted trend. I extend the index to include urbanization and the\nurban heat island effect. This substantially changes the trend again. I further\nextend the index to include international migration, but this has a minor\nimpact on the trend.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 17:15:36 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Tol", "Richard S. J.", ""]]}, {"id": "1612.09195", "submitter": "Majnu John", "authors": "Majnu John, Todd Lencz, Anil K Malhotra, Christoph U Correll,\n  Jian-Ping Zhang", "title": "A simulations approach for meta-analysis of genetic association studies\n  based on additive genetic model", "comments": null, "journal-ref": null, "doi": "10.1016/j.mgene.2018.02.004", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genetic association studies are becoming an important component of medical\nresearch. To cite one instance, pharmacogenomics which is gaining prominence as\na useful tool for personalized medicine is heavily reliant on results from\ngenetic association studies. Meta-analysis of genetic association studies is\nbeing increasingly used to assess phenotypic differences between genotype\ngroups. When the underlying genetic model is assumed to be dominant or\nrecessive, assessing the phenotype differences based on summary statistics,\nreported for individual studies in a meta-analysis, is a valid strategy.\nHowever, when the genetic model is additive, a similar strategy based on\nsummary statistics will lead to biased results. This fact about the additive\nmodel is one of the things that we establish in this paper, using simulations.\nThe main goal of this paper is to present an alternate strategy for the\nadditive model based on simulating data for the individual studies. We show\nthat the alternate strategy is far superior to the strategy based on summary\nstatistics.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 16:33:46 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["John", "Majnu", ""], ["Lencz", "Todd", ""], ["Malhotra", "Anil K", ""], ["Correll", "Christoph U", ""], ["Zhang", "Jian-Ping", ""]]}, {"id": "1612.09219", "submitter": "Yuan Tang", "authors": "Yuan Tang and Wenxuan Li", "title": "lfda: An R Package for Local Fisher Discriminant Analysis and\n  Visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Local Fisher discriminant analysis is a localized variant of Fisher\ndiscriminant analysis and it is popular for supervised dimensionality reduction\nmethod. lfda is an R package for performing local Fisher discriminant analysis,\nincluding its variants such as kernel local Fisher discriminant analysis and\nsemi-supervised local Fisher discriminant analysis. It also provides\nvisualization functions to easily visualize the dimension reduction results by\nusing either rgl for 3D visualization or ggfortify for 2D visualization in\nggplot2 style.\n", "versions": [{"version": "v1", "created": "Sun, 11 Dec 2016 02:07:58 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Tang", "Yuan", ""], ["Li", "Wenxuan", ""]]}, {"id": "1612.09340", "submitter": "Amila Sudu Ambegedara", "authors": "Amila Sudu Ambegedara, Jie Sun, Kerop Janoyan and Erik Bollt", "title": "Information theoretical noninvasive damage detection in bridge\n  structures", "comments": "28 pages, 14 figures in Chaos: An interdisciplinary Journal of\n  Nonlinear Science (2016)", "journal-ref": "Chaos: An Interdisciplinary Journal Of Nonlinear Science(2016)", "doi": "10.1063/1.4967920", "report-no": null, "categories": "stat.AP math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Damage detection of mechanical structures such as bridges is an important\nresearch problem in civil engineering. Using spatially distributed sensor time\nseries data collected from a recent experiment on a local bridge in upper state\nNew York, we study noninvasive damage detection using information-theoretical\nmethods. Several findings are in order. First, the time series data, which\nrepresent accelerations measured at the sensors, more closely follow Laplace\ndistribution than normal distribution, allowing us to develop parameter\nestimators for various information-theoretic measures such as entropy and\nmutual information. Secondly, as damage is introduced by the removal of bolts\nof the first diaphragm connection, the interaction between spatially nearby\nsensors as measured by mutual information become weaker, suggesting that the\nbridge is \"loosened\". Finally, using a proposed oMII procedure to prune away\nindirect interactions, we found that the primary direction of interaction or\ninfluence aligns with the traffic direction on the bridge even after damaging\nthe bridge.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 22:56:27 GMT"}], "update_date": "2017-01-02", "authors_parsed": [["Ambegedara", "Amila Sudu", ""], ["Sun", "Jie", ""], ["Janoyan", "Kerop", ""], ["Bollt", "Erik", ""]]}, {"id": "1612.09561", "submitter": "Ricardo Ehlers", "authors": "Breno S. Andrade, Marinho G. Andrade, Ricardo S. Ehlers", "title": "Bayesian Transformed GARMA Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformed Generalized Autoregressive Moving Average (TGARMA) models were\nrecently proposed to deal with non-additivity, non-normality and\nheteroscedasticity in real time series data. In this paper, a Bayesian approach\nis proposed for TGARMA models, thus extending the original model. We conducted\na simulation study to investigate the performance of Bayesian estimation and\nBayesian model selection criteria. In addition, a real dataset was analysed\nusing the proposed approach.\n", "versions": [{"version": "v1", "created": "Fri, 30 Dec 2016 19:00:12 GMT"}], "update_date": "2017-01-02", "authors_parsed": [["Andrade", "Breno S.", ""], ["Andrade", "Marinho G.", ""], ["Ehlers", "Ricardo S.", ""]]}, {"id": "1612.09596", "submitter": "Matt Taddy", "authors": "Jason Hartford, Greg Lewis, Kevin Leyton-Brown, Matt Taddy", "title": "Counterfactual Prediction with Deep Instrumental Variables Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are in the middle of a remarkable rise in the use and capability of\nartificial intelligence. Much of this growth has been fueled by the success of\ndeep learning architectures: models that map from observables to outputs via\nmultiple layers of latent representations. These deep learning algorithms are\neffective tools for unstructured prediction, and they can be combined in AI\nsystems to solve complex automated reasoning problems. This paper provides a\nrecipe for combining ML algorithms to solve for causal effects in the presence\nof instrumental variables -- sources of treatment randomization that are\nconditionally independent from the response. We show that a flexible IV\nspecification resolves into two prediction tasks that can be solved with deep\nneural nets: a first-stage network for treatment prediction and a second-stage\nnetwork whose loss function involves integration over the conditional treatment\ndistribution. This Deep IV framework imposes some specific structure on the\nstochastic gradient descent routine used for training, but it is general enough\nthat we can take advantage of off-the-shelf ML capabilities and avoid extensive\nalgorithm customization. We outline how to obtain out-of-sample causal\nvalidation in order to avoid over-fit. We also introduce schemes for both\nBayesian and frequentist inference: the former via a novel adaptation of\ndropout training, and the latter via a data splitting routine.\n", "versions": [{"version": "v1", "created": "Fri, 30 Dec 2016 20:56:41 GMT"}], "update_date": "2017-01-02", "authors_parsed": [["Hartford", "Jason", ""], ["Lewis", "Greg", ""], ["Leyton-Brown", "Kevin", ""], ["Taddy", "Matt", ""]]}]