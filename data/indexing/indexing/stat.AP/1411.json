[{"id": "1411.0282", "submitter": "Akshay Soni", "authors": "Akshay Soni, Swayambhoo Jain, Jarvis Haupt, and Stefano Gonella", "title": "Noisy Matrix Completion under Sparse Factor Models", "comments": "42 Pages, 7 Figures, Submitted to IEEE Transactions on Information\n  Theory", "journal-ref": null, "doi": "10.1109/TIT.2016.2549040", "report-no": null, "categories": "stat.ML cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines a general class of noisy matrix completion tasks where\nthe goal is to estimate a matrix from observations obtained at a subset of its\nentries, each of which is subject to random noise or corruption. Our specific\nfocus is on settings where the matrix to be estimated is well-approximated by a\nproduct of two (a priori unknown) matrices, one of which is sparse. Such\nstructural models - referred to here as \"sparse factor models\" - have been\nwidely used, for example, in subspace clustering applications, as well as in\ncontemporary sparse modeling and dictionary learning tasks. Our main\ntheoretical contributions are estimation error bounds for sparsity-regularized\nmaximum likelihood estimators for problems of this form, which are applicable\nto a number of different observation noise or corruption models. Several\nspecific implications are examined, including scenarios where observations are\ncorrupted by additive Gaussian noise or additive heavier-tailed (Laplace)\nnoise, Poisson-distributed observations, and highly-quantized (e.g., one-bit)\nobservations. We also propose a simple algorithmic approach based on the\nalternating direction method of multipliers for these tasks, and provide\nexperimental evidence to support our error analyses.\n", "versions": [{"version": "v1", "created": "Sun, 2 Nov 2014 17:29:48 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Soni", "Akshay", ""], ["Jain", "Swayambhoo", ""], ["Haupt", "Jarvis", ""], ["Gonella", "Stefano", ""]]}, {"id": "1411.0408", "submitter": "Nicolas Bousquet", "authors": "Alberto Pasanisi, C\\^ome Roero, Nicolas Bousquet, Emmanuel Remy", "title": "On the practical interest of discrete Inverse Polya and Weibull-1 models\n  in industrial reliability studies", "comments": "18 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Engineers often cope with the problem of assessing the lifetime of industrial\ncomponents, under the basis of observed industrial feedback data. Usually,\nlifetime is modelled as a continuous random variable, for instance\nexponentially or Weibull distributed. However, in some cases, the features of\nthe piece of equipment under investigation rather suggest the use of discrete\nprobabilistic models. This happens for an equipment which only operates on\ncycles or on demand. In these cases, the lifetime is rather measured in number\nof cycles or number of demands before failure, therefore, in theory, discrete\nmodels should be more appropriate. This article aims at bringing some light to\nthe practical interest for the reliability engineer in using two discrete\nmodels among the most popular: the Inverse Polya distribution (IPD), based on a\nPolya urn scheme, and the so-called Weibull-1 (W1) model. It is showed that,\nfor different reasons, the practical use of both models should be restricted to\nspecific industrial situations. In particular, when nothing is a priori known\nover the nature of ageing and/or data are heavily right-censored, they can\nremain of limited interest with respect to more flexible continuous lifetime\nmodels such as the usual Weibull distribution. Nonetheless, the intuitive\nmeaning given to the IPD distribution favors its use by engineers in low\n(decelerated) ageing situations.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 10:06:07 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Pasanisi", "Alberto", ""], ["Roero", "C\u00f4me", ""], ["Bousquet", "Nicolas", ""], ["Remy", "Emmanuel", ""]]}, {"id": "1411.0416", "submitter": "Sebastian Meyer", "authors": "Sebastian Meyer, Leonhard Held, Michael H\\\"ohle", "title": "Spatio-Temporal Analysis of Epidemic Phenomena Using the R Package\n  surveillance", "comments": "53 pages, 20 figures, package homepage:\n  http://surveillance.r-forge.r-project.org/", "journal-ref": "Journal of Statistical Software (2017); 77 (11): 1-55", "doi": "10.18637/jss.v077.i11", "report-no": null, "categories": "stat.CO cs.CE physics.data-an stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The availability of geocoded health data and the inherent temporal structure\nof communicable diseases have led to an increased interest in statistical\nmodels and software for spatio-temporal data with epidemic features. The open\nsource R package surveillance can handle various levels of aggregation at which\ninfective events have been recorded: individual-level time-stamped\ngeo-referenced data (case reports) in either continuous space or discrete\nspace, as well as counts aggregated by period and region. For each of these\ndata types, the surveillance package implements tools for visualization,\nlikelihoood inference and simulation from recently developed statistical\nregression frameworks capturing endemic and epidemic dynamics. Altogether, this\npaper is a guide to the spatio-temporal modeling of epidemic phenomena,\nexemplified by analyses of public health surveillance data on measles and\ninvasive meningococcal disease.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 10:25:14 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2015 03:27:52 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Meyer", "Sebastian", ""], ["Held", "Leonhard", ""], ["H\u00f6hle", "Michael", ""]]}, {"id": "1411.0599", "submitter": "Andrew Finley Dr.", "authors": "Andrew O. Finley, Sudipto Banerjee, Aaron R. Weiskittel, Chad Babcock,\n  and Bruce D. Cook", "title": "Dynamic spatial regression models for space-varying forest stand tables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many forest management planning decisions are based on information about the\nnumber of trees by species and diameter per unit area. This information is\ncommonly summarized in a stand table, where a stand is defined as a group of\nforest trees of sufficiently uniform species composition, age, condition, or\nproductivity to be considered a homogeneous unit for planning purposes.\nTypically information used to construct stand tables is gleaned from observed\nsubsets of the forest selected using a probability-based sampling design. Such\nsampling campaigns are expensive and hence only a small number of sample units\nare typically observed. This data paucity means that stand tables can only be\nestimated for relatively large areal units. Contemporary forest management\nplanning and spatially explicit ecosystem models require stand table input at\nhigher spatial resolution than can be affordably provided using traditional\napproaches. We propose a dynamic multivariate Poisson spatial regression model\nthat accommodates both spatial correlation between observed diameter\ndistributions and also correlation between tree counts across diameter classes\nwithin each location. To improve fit and prediction at unobserved locations,\ndiameter specific intensities can be estimated using auxiliary data such as\nmanagement history or remotely sensed information. The proposed model is used\nto analyze a diverse forest inventory dataset collected on the United States\nForest Service Penobscot Experimental Forest in Bradley, Maine. Results\ndemonstrate that explicitly modeling the residual spatial structure via a\nmultivariate Gaussian process and incorporating information about forest\nstructure from LiDAR covariates improve model fit and can provide high spatial\nresolution stand table maps with associated estimates of uncertainty.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 18:44:28 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Finley", "Andrew O.", ""], ["Banerjee", "Sudipto", ""], ["Weiskittel", "Aaron R.", ""], ["Babcock", "Chad", ""], ["Cook", "Bruce D.", ""]]}, {"id": "1411.0622", "submitter": "Mostafa Rahmani", "authors": "Mostafa Rahmani and George Atia", "title": "A Subspace Method for Array Covariance Matrix Estimation", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a subspace method for the estimation of an array\ncovariance matrix. It is shown that when the received signals are uncorrelated,\nthe true array covariance matrices lie in a specific subspace whose dimension\nis typically much smaller than the dimension of the full space. Based on this\nidea, a subspace based covariance matrix estimator is proposed. The estimator\nis obtained as a solution to a semi-definite convex optimization problem. While\nthe optimization problem has no closed-form solution, a nearly optimal\nclosed-form solution is proposed making it easy to implement. In comparison to\nthe conventional approaches, the proposed method yields higher estimation\naccuracy because it eliminates the estimation error which does not lie in the\nsubspace of the true covariance matrices. The numerical examples indicate that\nthe proposed covariance matrix estimator can significantly improve the\nestimation quality of the covariance matrix.\n", "versions": [{"version": "v1", "created": "Mon, 20 Oct 2014 02:00:58 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Rahmani", "Mostafa", ""], ["Atia", "George", ""]]}, {"id": "1411.0647", "submitter": "Michael Ward", "authors": "Florian M. Hollenbach, Iavor Bojinov, Shahryar Minhas, Nils W.\n  Metternich, Shahryar Minhas, Michael D. Ward, and Alexander Volfovsky", "title": "Multiple Imputation Using Gaussian Copulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing observations are pervasive throughout empirical research, especially\nin the social sciences. Despite multiple approaches to dealing adequately with\nmissing data, many scholars still fail to address this vital issue. In this\npaper, we present a simple-to-use method for generating multiple imputations\nusing a Gaussian copula. The Gaussian copula for multiple imputation (Hoff,\n2007) allows scholars to attain estimation results that have good coverage and\nsmall bias. The use of copulas to model the dependence among variables will\nenable researchers to construct valid joint distributions of the data, even\nwithout knowledge of the actual underlying marginal distributions. Multiple\nimputations are then generated by drawing observations from the resulting\nposterior joint distribution and replacing the missing values. Using simulated\nand observational data from published social science research, we compare\nimputation via Gaussian copulas with two other widely used imputation methods:\nMICE and Amelia II. Our results suggest that the Gaussian copula approach has a\nslightly smaller bias, higher coverage rates, and narrower confidence intervals\ncompared to the other methods. This is especially true when the variables with\nmissing data are not normally distributed. These results, combined with\ntheoretical guarantees and ease-of-use suggest that the approach examined\nprovides an attractive alternative for applied researchers undertaking multiple\nimputations.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 20:21:49 GMT"}, {"version": "v2", "created": "Tue, 16 May 2017 16:40:30 GMT"}, {"version": "v3", "created": "Thu, 4 Oct 2018 20:10:11 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Hollenbach", "Florian M.", ""], ["Bojinov", "Iavor", ""], ["Minhas", "Shahryar", ""], ["Metternich", "Nils W.", ""], ["Minhas", "Shahryar", ""], ["Ward", "Michael D.", ""], ["Volfovsky", "Alexander", ""]]}, {"id": "1411.0919", "submitter": "Scott Kern MD", "authors": "Scott E. Kern", "title": "Inferential statistics, power estimates, and study design formalities\n  continue to suppress biomedical innovation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Innovation is the direct intended product of certain styles in research, but\nnot of others. Fundamental conflicts between descriptive vs inferential\nstatistics, deductive vs inductive hypothesis testing, and exploratory vs\npre-planned confirmatory research designs have been played out over decades,\nwith winners and losers and consequences. Longstanding warnings from both\nacademics and research-funding interests have failed to influence effectively\nthe course of these battles. The NIH publicly studied and diagnosed important\naspects of the problem a decade ago, resulting in outward changes in the grant\nreview process but not a definitive correction. Specific reforms could\ndeliberately abate the damage produced by the current overemphasis on\ninferential statistics, power estimates, and prescriptive study design. Such\nreform would permit a reallocation of resources to historically productive\nrapid exploratory efforts and considerably,increase the chances for\nhigher-impact research discoveries. We can profit from the history and\nfoundation of these conflicts to make specific recommendations for\nadministrative objectives and the process of peer review in decisions regarding\nresearch funding. (C) 2013 S. Kern\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 14:30:05 GMT"}], "update_date": "2014-11-05", "authors_parsed": [["Kern", "Scott E.", ""]]}, {"id": "1411.0924", "submitter": "Alastair Rushworth Dr", "authors": "Alastair Rushworth, Duncan Lee and Christophe Sarran", "title": "An adaptive spatio-temporal smoothing model for estimating trends and\n  step changes in disease risk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical models used to estimate the spatio-temporal pattern in disease\nrisk from areal unit data represent the risk surface for each time period with\nknown covariates and a set of spatially smooth random effects. The latter act\nas a proxy for unmeasured spatial confounding, whose spatial structure is often\ncharacterised by a spatially smooth evolution between some pairs of adjacent\nareal units while other pairs exhibit large step changes. This spatial\nheterogeneity is not consistent with existing global smoothing models, in which\npartial correlation exists between all pairs of adjacent spatial random\neffects. Therefore we propose a novel space-time disease model with an adaptive\nspatial smoothing specification that can identify step changes. The model is\nmotivated by a new study of respiratory and circulatory disease risk across the\nset of Local Authorities in England, and is rigorously tested by simulation to\nassess its efficacy. Results from the England study show that the two diseases\nhave similar spatial patterns in risk, and exhibit a number of common step\nchanges in the unmeasured component of risk between neighbouring local\nauthorities.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 14:35:47 GMT"}, {"version": "v2", "created": "Mon, 18 Apr 2016 16:21:42 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Rushworth", "Alastair", ""], ["Lee", "Duncan", ""], ["Sarran", "Christophe", ""]]}, {"id": "1411.1045", "submitter": "Matthias K\\\"ummerer", "authors": "Matthias K\\\"ummerer and Lucas Theis and Matthias Bethge", "title": "Deep Gaze I: Boosting Saliency Prediction with Feature Maps Trained on\n  ImageNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent results suggest that state-of-the-art saliency models perform far from\noptimal in predicting fixations. This lack in performance has been attributed\nto an inability to model the influence of high-level image features such as\nobjects. Recent seminal advances in applying deep neural networks to tasks like\nobject recognition suggests that they are able to capture this kind of\nstructure. However, the enormous amount of training data necessary to train\nthese networks makes them difficult to apply directly to saliency prediction.\nWe present a novel way of reusing existing neural networks that have been\npretrained on the task of object recognition in models of fixation prediction.\nUsing the well-known network of Krizhevsky et al. (2012), we come up with a new\nsaliency model that significantly outperforms all state-of-the-art models on\nthe MIT Saliency Benchmark. We show that the structure of this network allows\nnew insights in the psychophysics of fixation selection and potentially their\nneural implementation. To train our network, we build on recent work on the\nmodeling of saliency as point processes.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 20:56:51 GMT"}, {"version": "v2", "created": "Thu, 18 Dec 2014 17:22:49 GMT"}, {"version": "v3", "created": "Thu, 26 Feb 2015 19:05:11 GMT"}, {"version": "v4", "created": "Thu, 9 Apr 2015 09:48:11 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["K\u00fcmmerer", "Matthias", ""], ["Theis", "Lucas", ""], ["Bethge", "Matthias", ""]]}, {"id": "1411.1200", "submitter": "Paul Fearnhead", "authors": "Paul Fearnhead, Shoukai Yu, Patrick Biggs, Barbara Holland, Nigel\n  French", "title": "Estimating the relative rate of recombination to mutation in bacteria\n  from single-locus variants using composite likelihood methods", "comments": "Published at http://dx.doi.org/10.1214/14-AOAS795 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 1, 200-224", "doi": "10.1214/14-AOAS795", "report-no": "IMS-AOAS-AOAS795", "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of studies have suggested using comparisons between DNA sequences of\nclosely related bacterial isolates to estimate the relative rate of\nrecombination to mutation for that bacterial species. We consider such an\napproach which uses single-locus variants: pairs of isolates whose DNA differ\nat a single gene locus. One way of deriving point estimates for the relative\nrate of recombination to mutation from such data is to use composite likelihood\nmethods. We extend recent work in this area so as to be able to construct\nconfidence intervals for our estimates, without needing to resort to\ncomputationally-intensive bootstrap procedures, and to develop a test for\nwhether the relative rate varies across loci. Both our test and method for\nconstructing confidence intervals are obtained by modeling the dependence\nstructure in the data, and then applying asymptotic theory regarding the\ndistribution of estimators obtained using a composite likelihood. We applied\nthese methods to multi-locus sequence typing (MLST) data from eight bacteria,\nfinding strong evidence for considerable rate variation in three of these:\nBacillus cereus, Enterococcus faecium and Klebsiella pneumoniae.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 09:32:04 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2015 04:39:19 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Fearnhead", "Paul", ""], ["Yu", "Shoukai", ""], ["Biggs", "Patrick", ""], ["Holland", "Barbara", ""], ["French", "Nigel", ""]]}, {"id": "1411.1244", "submitter": "Sarat C. Dass", "authors": "Sarat C. Dass, Chae Young Lim, Tapabrata Maiti", "title": "A generalized mixed model framework for assessing fingerprint\n  individuality in presence of varying image quality", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS734 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 3, 1314-1340", "doi": "10.1214/14-AOAS734", "report-no": "IMS-AOAS-AOAS734", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fingerprint individuality refers to the extent of uniqueness of fingerprints\nand is the main criteria for deciding between a match versus nonmatch in\nforensic testimony. Often, prints are subject to varying levels of noise, for\nexample, the image quality may be low when a print is lifted from a crime\nscene. A poor image quality causes human experts as well as automatic systems\nto make more errors in feature detection by either missing true features or\ndetecting spurious ones. This error lowers the extent to which one can claim\nindividualization of fingerprints that are being matched. The aim of this paper\nis to quantify the decrease in individualization as image quality degrades\nbased on fingerprint images in real databases. This, in turn, can be used by\nforensic experts along with their testimony in a court of law. An important\npractical concern is that the databases used typically consist of a large\nnumber of fingerprint images so computational algorithms such as the Gibbs\nsampler can be extremely slow. We develop algorithms based on the Laplace\napproximation of the likelihood and infer the unknown parameters based on this\napproximate likelihood. Two publicly available databases, namely, FVC2002 and\nFVC2006, are analyzed from which estimates of individuality are obtained. From\na statistical perspective, the contribution can be treated as an innovative\napplication of Generalized Linear Mixed Models (GLMMs) to the field of\nfingerprint-based authentication.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 11:57:49 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Dass", "Sarat C.", ""], ["Lim", "Chae Young", ""], ["Maiti", "Tapabrata", ""]]}, {"id": "1411.1285", "submitter": "Benjamin Hofner", "authors": "Benjamin Hofner, Luigi Boccuto, Markus G\\\"oker", "title": "Controlling false discoveries in high-dimensional situations: Boosting\n  with stability selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern biotechnologies often result in high-dimensional data sets with much\nmore variables than observations (n $\\ll$ p). These data sets pose new\nchallenges to statistical analysis: Variable selection becomes one of the most\nimportant tasks in this setting. We assess the recently proposed flexible\nframework for variable selection called stability selection. By the use of\nresampling procedures, stability selection adds a finite sample error control\nto high-dimensional variable selection procedures such as Lasso or boosting. We\nconsider the combination of boosting and stability selection and present\nresults from a detailed simulation study that provides insights into the\nusefulness of this combination. Limitations are discussed and guidance on the\nspecification and tuning of stability selection is given. The interpretation of\nthe used error bounds is elaborated and insights for practical data analysis\nare given. The results will be used to detect differentially expressed\nphenotype measurements in patients with autism spectrum disorders. All methods\nare implemented in the freely available R package stabs.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 14:47:56 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Hofner", "Benjamin", ""], ["Boccuto", "Luigi", ""], ["G\u00f6ker", "Markus", ""]]}, {"id": "1411.1440", "submitter": "Yasin Yilmaz", "authors": "Yasin Yilmaz, Shang Li, Xiaodong Wang", "title": "Sequential Joint Detection and Estimation: Optimum Tests and\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We treat the statistical inference problems in which one needs to detect and\nestimate simultaneously using as small number of samples as possible.\nConventional methods treat the detection and estimation subproblems separately,\nignoring the intrinsic coupling between them. However, a joint detection and\nestimation problem should be solved to maximize the overall performance. We\naddress the sample size concern through a sequential and Bayesian setup.\nSpecifically, we seek the optimum triplet of stopping time, detector, and\nestimator(s) that minimizes the number of samples subject to a constraint on\nthe combined detection and estimation cost. A general framework for optimum\nsequential joint detection and estimation is developed. The resulting optimum\ndetector and estimator(s) are strongly coupled with each other, proving that\nthe separate treatment is strictly sub-optimum. The theoretical results derived\nfor a quite general model are then applied to several problems with linear\nquadratic Gaussian (LQG) models, including dynamic spectrum access in cognitive\nradio, and state estimation in smart grid with topological uncertainty.\nNumerical results corroborate the superior overall detection and estimation\nperformance of the proposed schemes over the conventional methods that handle\nthe subproblems separately.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 22:45:45 GMT"}], "update_date": "2014-11-07", "authors_parsed": [["Yilmaz", "Yasin", ""], ["Li", "Shang", ""], ["Wang", "Xiaodong", ""]]}, {"id": "1411.1472", "submitter": "Yaneer Bar-Yam", "authors": "Yaneer Bar-Yam", "title": "Why are public health authorities not concerned about Ebola in the US?\n  Part I. Fat tailed distributions", "comments": "16 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": "New England Complex Systems Institute Report 2014-11-02", "categories": "physics.soc-ph physics.data-an physics.med-ph q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  US public health authorities claim imposing quarantines on healthcare workers\nreturning from West Africa is incorrect according to science. Their positions\nrely upon a set of studies and experience about outbreaks and transmission\nmechanisms in Africa as well as assumptions about what those studies imply\nabout outbreaks in the US. According to this view the probability of a single\ninfection is low and that of a major outbreak is non-existent. In a series of\nbrief reports we will provide insight into why properties of networks of\ncontagion that are not considered in traditional statistics suggest that risks\nare higher than those assumptions suggest. We begin with the difference between\nthin and fat tailed distributions applied to the number of infected individuals\nthat can arise from a single one. Traditional epidemiological models consider\nthe contagion process as described by $R_0$, the average number of new infected\nindividuals arising from a single case. However, in a complex interdependent\nsociety it is possible for the actual number due to a single individual to\ndramatically differ from the average number, with severe consequences for the\nability to contain an outbreak when it is just beginning. Our analysis raises\ndoubts about the scientific validity of policy recommendations of public health\nauthorities. We also point out that existing CDC public health policies and\nactions are inconsistent with their claims.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 02:01:56 GMT"}], "update_date": "2014-11-07", "authors_parsed": [["Bar-Yam", "Yaneer", ""]]}, {"id": "1411.1546", "submitter": "Michael Mahoney", "authors": "Aaron B. Adcock and Blair D. Sullivan and Michael W. Mahoney", "title": "Tree decompositions and social graphs", "comments": "v2 has 44 pages, 21 figures, 7 tables, 107 references. To appear in\n  Internet Mathematics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has established that large informatics graphs such as social and\ninformation networks have non-trivial tree-like structure when viewed at\nmoderate size scales. Here, we present results from the first detailed\nempirical evaluation of the use of tree decomposition (TD) heuristics for\nstructure identification and extraction in social graphs. Although TDs have\nhistorically been used in structural graph theory and scientific computing, we\nshow that---even with existing TD heuristics developed for those very different\nareas---TD methods can identify interesting structure in a wide range of\nrealistic informatics graphs. Our main contributions are the following: we show\nthat TD methods can identify structures that correlate strongly with the\ncore-periphery structure of realistic networks, even when using simple greedy\nheuristics; we show that the peripheral bags of these TDs correlate well with\nlow-conductance communities (when they exist) found using local spectral\ncomputations; and we show that several types of large-scale \"ground-truth\"\ncommunities, defined by demographic metadata on the nodes of the network, are\nwell-localized in the large-scale and/or peripheral structures of the TDs. Our\nother main contributions are the following: we provide detailed empirical\nresults for TD heuristics on toy and synthetic networks to establish a baseline\nto understand better the behavior of the heuristics on more complex real-world\nnetworks; and we prove a theorem providing formal justification for the\nintuition that the only two impediments to low-distortion hyperbolic embedding\nare high tree-width and long geodesic cycles. Our results suggest future\ndirections for improved TD heuristics that are more appropriate for realistic\nsocial graphs.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 09:53:15 GMT"}, {"version": "v2", "created": "Tue, 3 May 2016 19:58:18 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Adcock", "Aaron B.", ""], ["Sullivan", "Blair D.", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1411.1597", "submitter": "Halim Zeghdoudi", "authors": "Halim Zeghdoudi and Sihem Nedjar", "title": "Gamma-Lindley distribution and its application", "comments": "This paper has been withdrawn by the author due to a crucial sign\n  error in theorem 1 and Illustrative examples", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the new distribution named Gamma Lindley distribution (GaLD), of\nwhich the Lindley distribution (LD) is a particular case. In this paper, we\ndiscuss and add more properties. Also, an application of this distribution is\ngiven.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 13:12:34 GMT"}, {"version": "v2", "created": "Tue, 27 Jan 2015 16:29:10 GMT"}], "update_date": "2015-01-28", "authors_parsed": [["Zeghdoudi", "Halim", ""], ["Nedjar", "Sihem", ""]]}, {"id": "1411.1869", "submitter": "Aristidis K. Nikoloulopoulos", "authors": "Aristidis K. Nikoloulopoulos", "title": "Efficient estimation of high-dimensional multivariate normal copula\n  models with discrete spatial responses", "comments": "arXiv admin note: text overlap with arXiv:1304.0905", "journal-ref": "Stochastic Environmental Research and Risk Assessment, 2016,\n  30(2):493--505", "doi": "10.1007/s00477-015-1060-2", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distributional transform (DT) is amongst the computational methods used\nfor estimation of high-dimensional multivariate normal copula models with\ndiscrete responses. Its advantage is that the likelihood can be derived\nconveniently under the theory for copula models with continuous margins, but\nthere has not been a clear analysis of the adequacy of this method. We\ninvestigate the small-sample and asymptotic efficiency of the method for\nestimating high-dimensional multivariate normal copula models with univariate\nBernoulli, Poisson, and negative binomial margins, and show that the DT\napproximation leads to biased estimates when there is more discretisation. For\na high-dimensional discrete response, we implement a maximum simulated\nlikelihood method, which is based on evaluating the multidimensional integrals\nof the likelihood with randomized quasi Monte Carlo methods. Efficiency\ncalculations show that our method is nearly as efficient as maximum likelihood\nfor fully specified high-dimensional multivariate normal copula models. Both\nmethods are illustrated with spatially aggregated count data sets, and it is\nshown that there is a substantial gain on efficiency via the maximum simulated\nlikelihood method.\n", "versions": [{"version": "v1", "created": "Fri, 7 Nov 2014 09:48:51 GMT"}, {"version": "v2", "created": "Tue, 20 Jan 2015 19:21:24 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Nikoloulopoulos", "Aristidis K.", ""]]}, {"id": "1411.1993", "submitter": "Rosemary Braun", "authors": "Rosemary Braun and Sahil Shah", "title": "Network Methods for Pathway Analysis of Genomic Data", "comments": "Review article", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.GN q-bio.MN stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid advances in high-throughput technologies have led to considerable\ninterest in analyzing genome-scale data in the context of biological pathways,\nwith the goal of identifying functional systems that are involved in a given\nphenotype. In the most common approaches, biological pathways are modeled as\nsimple sets of genes, neglecting the network of interactions comprising the\npathway and treating all genes as equally important to the pathway's function.\nRecently, a number of new methods have been proposed to integrate pathway\ntopology in the analyses, harnessing existing knowledge and enabling more\nnuanced models of complex biological systems. However, there is little guidance\navailable to researches choosing between these methods. In this review, we\ndiscuss eight topology-based methods, comparing their methodological approaches\nand appropriate use cases. In addition, we present the results of the\napplication of these methods to a curated set of ten gene expression profiling\nstudies using a common set of pathway annotations. We report the computational\nefficiency of the methods and the consistency of the results across methods and\nstudies to help guide users in choosing a method. We also discuss the\nchallenges and future outlook for improved network analysis methodologies.\n", "versions": [{"version": "v1", "created": "Fri, 7 Nov 2014 17:33:53 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Braun", "Rosemary", ""], ["Shah", "Sahil", ""]]}, {"id": "1411.2051", "submitter": "John Aston", "authors": "Ci-Ren Jiang, John A D Aston and Jane-Ling Wang", "title": "A functional approach to deconvolve dynamic neuroimaging data", "comments": "33 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Positron Emission Tomography (PET) is an imaging technique which can be used\nto investigate chemical changes in human biological processes such as cancer\ndevelopment or neurochemical reactions. Most dynamic PET scans are currently\nanalyzed based on the assumption that linear first order kinetics can be used\nto adequately describe the system under observation. However, there has\nrecently been strong evidence that this is not the case. In order to provide an\nanalysis of PET data which is free from this compartmental assumption, we\npropose a nonparametric deconvolution and analysis model for dynamic PET data\nbased on functional principal component analysis. This yields flexibility in\nthe possible deconvolved functions while still performing well when a linear\ncompartmental model setup is the true data generating mechanism. As the\ndeconvolution needs to be performed on only a relative small number of basis\nfunctions rather than voxel by voxel in the entire 3-D volume, the methodology\nis both robust to typical brain imaging noise levels while also being\ncomputationally efficient. The new methodology is investigated through\nsimulations in both 1-D functions and 2-D images and also applied to a\nneuroimaging study whose goal is the quantification of opioid receptor\nconcentration in the brain.\n", "versions": [{"version": "v1", "created": "Fri, 7 Nov 2014 22:30:42 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Jiang", "Ci-Ren", ""], ["Aston", "John A D", ""], ["Wang", "Jane-Ling", ""]]}, {"id": "1411.2182", "submitter": "Yang Liu", "authors": "Yang Liu, Philip Kokic and K.Shuvo Bakar", "title": "A Censored Bayesian Hierarchical Model For Precipitation", "comments": "Under review at Environmentrics", "journal-ref": null, "doi": null, "report-no": "EP145227", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modelling of precipitation, including extremes, is important for hydrological\nand agricultural applications. Traditionally, because of large sample\nproperties for data over a large threshold value, generalised Pareto (GP)\ndistributions are often used for modelling extreme rainfall. It can be shown\nthat under certain conditions the generalised hyperbolic (GH) distributions can\napproximate the power law decay of the GP distribution in the tails. Given\ntheir flexible form, this raises the possibility that distributions from the GH\nfamily serve as a model for the entire rainfall distribution thus avoiding the\nneed to select a threshold. In this paper, we use a flexible censored\nhierarchical model that leverages the GH distribution to accommodate data\nsubject to heavy tails and an excessive number of zeros. The fitted model\nallows estimation of probabilities and return periods of the rainfall extremes,\nand it produces narrower credible intervals in the tails than the traditional\nGP method. The model not only fits the tails of the rainfall distribution, but\nfits the whole distribution very well. It also efficiently represents\nshort-term dependencies in the data so it is suitable for evaluating duration\nover and below thresholds as well as duration of zero rainfall.\n", "versions": [{"version": "v1", "created": "Sun, 9 Nov 2014 01:43:14 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Liu", "Yang", ""], ["Kokic", "Philip", ""], ["Bakar", "K. Shuvo", ""]]}, {"id": "1411.2183", "submitter": "Daniel Weller", "authors": "Daniel S. Weller, Ayelet Pnueli, Gilad Divon, Ori Radzyner, Yonina C.\n  Eldar, Jeffrey A. Fessler", "title": "Undersampled Phase Retrieval with Outliers", "comments": "11 pages, 9 figures", "journal-ref": null, "doi": "10.1109/TCI.2015.2498402", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general framework for reconstructing transform-sparse images\nfrom undersampled (squared)-magnitude data corrupted with outliers. This\nframework is implemented using a multi-layered approach, combining multiple\ninitializations (to address the nonconvexity of the phase retrieval problem),\nrepeated minimization of a convex majorizer (surrogate for a nonconvex\nobjective function), and iterative optimization using the alternating\ndirections method of multipliers. Exploiting the generality of this framework,\nwe investigate using a Laplace measurement noise model better adapted to\noutliers present in the data than the conventional Gaussian noise model. Using\nsimulations, we explore the sensitivity of the method to both the\nregularization and penalty parameters. We include 1D Monte Carlo and 2D image\nreconstruction comparisons with alternative phase retrieval algorithms. The\nresults suggest the proposed method, with the Laplace noise model, both\nincreases the likelihood of correct support recovery and reduces the mean\nsquared error from measurements containing outliers. We also describe exciting\nextensions made possible by the generality of the proposed framework, including\nregularization using analysis-form sparsity priors that are incompatible with\nmany existing approaches.\n", "versions": [{"version": "v1", "created": "Sun, 9 Nov 2014 01:55:40 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Weller", "Daniel S.", ""], ["Pnueli", "Ayelet", ""], ["Divon", "Gilad", ""], ["Radzyner", "Ori", ""], ["Eldar", "Yonina C.", ""], ["Fessler", "Jeffrey A.", ""]]}, {"id": "1411.2514", "submitter": "Karyn Morrissey Dr", "authors": "Karyn Morrissey, Ferran Espuny, Paul Williamson", "title": "A Multinomial Model for Comorbidity in England of Longstanding CVD,\n  Diabetes, and Obesity", "comments": "Working Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From a public health perspective, previous research on comorbidity tends to\nhave focused on identifying the most prevalent groupings of illnesses that\ndemonstrate comorbidity, particularly among the elderly population, already in\nreceipt of care. In contrast, little attention has been paid to possible\nsocio-economic factors associated with increased rates of comorbidity or to the\npossibility of wider unrevealed need. Given the known relationship between CVD,\ndiabetes and obesity and the strong socio-economic gradients in risk factors\nfor each of the three diseases as single morbidities, this paper uses the\nHealth Survey for England to examine the demographic and socio-economic\ndeterminants of each of the seven disease combinations in the English\npopulation. Using a multinomial logistic model, this research finds that gender\nis a significant predictor for all seven disease combinations. However, gender\nwas not as influential as individual age or socio-economic profile. With regard\nto ethnicity, the black population presents a high obesity, diabetes and\ndiabetes-related comorbidity risk, whilst the Asian population presents a high\nrisk for diabetes and diabetes-related comorbidity but a low risk for obesity\nand comorbidity. Across the seven disease combinations, risk was lowest for\nthose individuals with a high income (4 out of 7), in-work (4 out of 7), home\nowning (3 out of 7) and degree educated (3 out of 7). Finally, smokers have a\nlower risk rate of obesity (and related) than ex-smokers relative to\nindividuals that never smoked (in all cases controlling for all other factors).\nThe important influence of socioeconomic factors has implications for the\nspatial demand for services and the policy solutions adopted to tackle the\nincreasing prevalence of comorbidity.\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2014 17:39:32 GMT"}, {"version": "v2", "created": "Tue, 11 Nov 2014 09:48:56 GMT"}], "update_date": "2014-11-12", "authors_parsed": [["Morrissey", "Karyn", ""], ["Espuny", "Ferran", ""], ["Williamson", "Paul", ""]]}, {"id": "1411.2571", "submitter": "Henrik Singmann", "authors": "Karl Christoph Klauer, Henrik Singmann, and David Kellen", "title": "Parametric Order Constraints in Multinomial Processing Tree Models: An\n  Extension of Knapp and Batchelder (2004)", "comments": "author submitted version accepted for publication in \"Journal of\n  Mathematical Psychology\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multinomial processing tree (MPT) models are tools for disentangling the\ncontributions of latent cognitive processes in a given experimental paradigm.\nThe present note analyzes MPT models subject to order constraints on subsets of\nits parameters. The constraints that we consider frequently arise in cases\nwhere the response categories are ordered in some sense such as in\nconfidence-rating data, Likert scale data, where graded guessing tendencies or\nresponse biases are created via base-rate or payoff manipulations, in the\nanalysis of contingency tables with order constraints, and in many other cases.\nWe show how to construct an MPT model without order constraints that is\nstatistically equivalent to the MPT model with order constraints. This new\nclosure result extends the mathematical analysis of the MPT class, and it\noffers an approach to order-restricted inference that extends the approaches\ndiscussed by Knapp and Batchelder (2004). The usefulness of the method is\nillustrated by means of an analysis of an order-constrained version of the\ntwo-high-threshold model for confidence ratings.\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2014 20:25:04 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Klauer", "Karl Christoph", ""], ["Singmann", "Henrik", ""], ["Kellen", "David", ""]]}, {"id": "1411.2624", "submitter": "Theodore  Kypraios", "authors": "Edward S. Knock and Theodore Kypraios", "title": "Bayesian Non-Parametric Inference for Infectious Disease Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for Bayesian non-parametric estimation of the rate at\nwhich new infections occur assuming that the epidemic is partially observed.\nThe developed methodology relies on modelling the rate at which new infections\noccur as a function which only depends on time. Two different types of prior\ndistributions are proposed namely using step-functions and B-splines. The\nmethodology is illustrated using both simulated and real datasets and we show\nthat certain aspects of the epidemic such as seasonality and super-spreading\nevents are picked up without having to explicitly incorporate them into a\nparametric model.\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2014 21:26:10 GMT"}, {"version": "v2", "created": "Mon, 15 Dec 2014 10:10:42 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Knock", "Edward S.", ""], ["Kypraios", "Theodore", ""]]}, {"id": "1411.2961", "submitter": "Joshua Wiley", "authors": "Joshua F. Wiley, Bei Bei, John Trinder, Rachel Manber", "title": "Variability as a Predictor: A Bayesian Variability Model for Small\n  Samples and Few Repeated Measures", "comments": "47 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whilst most psychological research focuses on differences in means, a growing\nbody of literature demonstrates the value of considering differences in\nintra-individual variability. Compared to the number of methods available for\nanalyzing mean differences, there is a paucity of methods available for\nanalyzing intra-individual variability, particularly when variability is\ntreated as a predictor. In the present article, we first reviewed methods of\nanalyzing intra-individual variability as an outcome, including the individual\nstandard deviation (ISD) and some recent methods. We then introduced a novel\nBayesian method for analyzing intra-individual variability as a predictor. To\nmake this method easily accessible to the research community, we developed an\nopen source R package, VARIAN. To compare the accuracy of parameter estimates\nusing the proposed Bayesian analysis against the ISD as a predictor in a\nregression, we carried out a simulation study. We then demonstrated, using\nempirical data, how the estimated intra-individual variability derived from the\nproposed Bayesian analysis can be used to answer the following two questions:\n(1) is intra-individual variability in daily time-in-bed associated with\nsubjective sleep quality? (2) does subjective sleep quality mediate the\nassociation between time-in-bed variability and depressive symptoms? We\nconcluded with a discussion of methodological and practical considerations that\ncan help guide researchers in choosing methods for evaluating intra-individual\nvariability.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2014 20:56:09 GMT"}], "update_date": "2014-11-12", "authors_parsed": [["Wiley", "Joshua F.", ""], ["Bei", "Bei", ""], ["Trinder", "John", ""], ["Manber", "Rachel", ""]]}, {"id": "1411.3013", "submitter": "Kevin H. Knuth", "authors": "Kevin H. Knuth, Michael Habeck, Nabin K. Malakar, Asim M. Mubeen, Ben\n  Placek", "title": "Bayesian Evidence and Model Selection", "comments": "Arxiv version consists of 58 pages and 9 figures. Features theory,\n  numerical methods and four applications", "journal-ref": "Digital Signal Processing, 47:50-67 (2015)", "doi": "10.1016/j.dsp.2015.06.012", "report-no": null, "categories": "stat.ME astro-ph.IM stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we review the concepts of Bayesian evidence and Bayes factors,\nalso known as log odds ratios, and their application to model selection. The\ntheory is presented along with a discussion of analytic, approximate and\nnumerical techniques. Specific attention is paid to the Laplace approximation,\nvariational Bayes, importance sampling, thermodynamic integration, and nested\nsampling and its recent variants. Analogies to statistical physics, from which\nmany of these techniques originate, are discussed in order to provide readers\nwith deeper insights that may lead to new techniques. The utility of Bayesian\nmodel testing in the domain sciences is demonstrated by presenting four\nspecific practical examples considered within the context of signal processing\nin the areas of signal detection, sensor characterization, scientific model\nselection and molecular force characterization.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2014 23:08:54 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2015 07:13:30 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Knuth", "Kevin H.", ""], ["Habeck", "Michael", ""], ["Malakar", "Nabin K.", ""], ["Mubeen", "Asim M.", ""], ["Placek", "Ben", ""]]}, {"id": "1411.3042", "submitter": "Tyler McCormick", "authors": "Tyler H. McCormick and Zehang Li and Clara Calvert and Amelia C.\n  Crampin and Kathleen Kahn and Samuel J. Clark", "title": "Probabilistic Cause-of-death Assignment using Verbal Autopsies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In regions without complete-coverage civil registration and vital statistics\nsystems there is uncertainty about even the most basic demographic indicators.\nIn such areas the majority of deaths occur outside hospitals and are not\nrecorded. Worldwide, fewer than one-third of deaths are assigned a cause, with\nthe least information available from the most impoverished nations. In\npopulations like this, verbal autopsy (VA) is a commonly used tool to assess\ncause of death and estimate cause-specific mortality rates and the distribution\nof deaths by cause. VA uses an interview with caregivers of the decedent to\nelicit data describing the signs and symptoms leading up to the death. This\npaper develops a new statistical tool known as InSilicoVA to classify cause of\ndeath using information acquired through VA. InSilicoVA shares uncertainty\nbetween cause of death assignments for specific individuals and the\ndistribution of deaths by cause across the population. Using side-by-side\ncomparisons with both observed and simulated data, we demonstrate that\nInSilicoVA has distinct advantages compared to currently available methods.\n", "versions": [{"version": "v1", "created": "Wed, 12 Nov 2014 01:42:58 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2015 19:39:25 GMT"}, {"version": "v3", "created": "Mon, 21 Sep 2015 17:12:04 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["McCormick", "Tyler H.", ""], ["Li", "Zehang", ""], ["Calvert", "Clara", ""], ["Crampin", "Amelia C.", ""], ["Kahn", "Kathleen", ""], ["Clark", "Samuel J.", ""]]}, {"id": "1411.3479", "submitter": "Nikolay Bliznyuk", "authors": "Nikolay Bliznyuk, Christopher J. Paciorek, Joel Schwartz, Brent Coull", "title": "Nonlinear predictive latent process models for integrating\n  spatio-temporal exposure data from multiple sources", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS737 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 3, 1538-1560", "doi": "10.1214/14-AOAS737", "report-no": "IMS-AOAS-AOAS737", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatio-temporal prediction of levels of an environmental exposure is an\nimportant problem in environmental epidemiology. Our work is motivated by\nmultiple studies on the spatio-temporal distribution of mobile source, or\ntraffic related, particles in the greater Boston area. When multiple sources of\nexposure information are available, a joint model that pools information across\nsources maximizes data coverage over both space and time, thereby reducing the\nprediction error. We consider a Bayesian hierarchical framework in which a\njoint model consists of a set of submodels, one for each data source, and a\nmodel for the latent process that serves to relate the submodels to one\nanother. If a submodel depends on the latent process nonlinearly, inference\nusing standard MCMC techniques can be computationally prohibitive. The\nimplications are particularly severe when the data for each submodel are\naggregated at different temporal scales. To make such problems tractable, we\nlinearize the nonlinear components with respect to the latent process and\ninduce sparsity in the covariance matrix of the latent process using compactly\nsupported covariance functions. We propose an efficient MCMC scheme that takes\nadvantage of these approximations. We use our model to address a temporal\nchange of support problem whereby interest focuses on pooling daily and\nmultiday black carbon readings in order to maximize the spatial coverage of the\nstudy region.\n", "versions": [{"version": "v1", "created": "Thu, 13 Nov 2014 09:55:51 GMT"}], "update_date": "2014-11-14", "authors_parsed": [["Bliznyuk", "Nikolay", ""], ["Paciorek", "Christopher J.", ""], ["Schwartz", "Joel", ""], ["Coull", "Brent", ""]]}, {"id": "1411.3513", "submitter": "Arman Sabbaghi", "authors": "Arman Sabbaghi, Tirthankar Dasgupta, Qiang Huang, Jizhe Zhang", "title": "Inference for deformation and interference in 3D printing", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS762 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 3, 1395-1415", "doi": "10.1214/14-AOAS762", "report-no": "IMS-AOAS-AOAS762", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Additive manufacturing, or 3D printing, is a promising manufacturing\ntechnique marred by product deformation due to material solidification in the\nprinting process. Control of printed product deformation can be achieved by a\ncompensation plan. However, little attention has been paid to interference in\ncompensation, which is thought to result from the inevitable discretization of\na compensation plan. We investigate interference with an experiment involving\nthe application of discretized compensation plans to cylinders. Our treatment\nillustrates a principled framework for detecting and modeling interference, and\nultimately provides a new step toward better understanding quality control for\n3D printing.\n", "versions": [{"version": "v1", "created": "Thu, 13 Nov 2014 11:57:26 GMT"}], "update_date": "2014-11-14", "authors_parsed": [["Sabbaghi", "Arman", ""], ["Dasgupta", "Tirthankar", ""], ["Huang", "Qiang", ""], ["Zhang", "Jizhe", ""]]}, {"id": "1411.3637", "submitter": "Derick Rivers", "authors": "Derick L. Rivers and Edward L. Boone", "title": "Dynamic Bayesian Nonlinear Calibration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical calibration where the curve is nonlinear is important in many\nareas, such as analytical chemistry and radiometry. Especially in radiometry,\ninstrument characteristics change over time, thus calibration is a process that\nmust be conducted as long as the instrument is in use. We propose a dynamic\nBayesian method to perform calibration in the presence of a curvilinear\nrelationship between the reference measurements and the response variable. The\ndynamic calibration approach adequately derives time dependent calibration\ndistributions in the presence of drifting regression parameters. The method is\napplied to microwave radiometer data and simulated spectroscopy data based on\nwork by Lundberg and de Mar\\'{e} (1980).\n", "versions": [{"version": "v1", "created": "Thu, 13 Nov 2014 17:59:16 GMT"}], "update_date": "2014-11-14", "authors_parsed": [["Rivers", "Derick L.", ""], ["Boone", "Edward L.", ""]]}, {"id": "1411.3638", "submitter": "Luca Ferreri", "authors": "Luca Ferreri, Mario Giacobini, Paolo Bajardi, Luigi Bertolotti, Luca\n  Bolzoni, Valentina Tagliapietra, Annapaola Rizzoli, Roberto Ros\\`a", "title": "Pattern of tick aggregation on mice: larger than expected distribution\n  tail enhances the spread of tick-borne pathogens", "comments": "32 pages, 13 figures, appears in PLOS Computational Biology 2014", "journal-ref": "PLOS Computational Biology 10 (11): e1003931, 2014", "doi": "10.1371/journal.pcbi.1003931", "report-no": null, "categories": "q-bio.PE physics.bio-ph physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spread of tick-borne pathogens represents an important threat to human\nand animal health in many parts of Eurasia. Here, we analysed a 9-year time\nseries of Ixodes ricinus ticks feeding on Apodemus flavicollis mice (main\nreservoir-competent host for tick-borne encephalitis, TBE) sampled in Trentino\n(Northern Italy). The tail of the distribution of the number of ticks per host\nwas fitted by three theoretical distributions: Negative Binomial (NB),\nPoisson-LogNormal (PoiLN), and Power-Law (PL). The fit with theoretical\ndistributions indicated that the tail of the tick infestation pattern on mice\nis better described by the PL distribution. Moreover, we found that the tail of\nthe distribution significantly changes with seasonal variations in host\nabundance. In order to investigate the effect of different tails of tick\ndistribution on the invasion of a non-systemically transmitted pathogen, we\nsimulated the transmission of a TBE-like virus between susceptible and\ninfective ticks using a stochastic model. Model simulations indicated different\noutcomes of disease spreading when considering different distribution laws of\nticks among hosts. Specifically, we found that the epidemic threshold and the\nprevalence equilibria obtained in epidemiological simulations with PL\ndistribution are a good approximation of those observed in simulations feed by\nthe empirical distribution. Moreover, we also found that the epidemic threshold\nfor disease invasion was lower when considering the seasonal variation of tick\naggregation.\n", "versions": [{"version": "v1", "created": "Thu, 13 Nov 2014 17:59:33 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Ferreri", "Luca", ""], ["Giacobini", "Mario", ""], ["Bajardi", "Paolo", ""], ["Bertolotti", "Luigi", ""], ["Bolzoni", "Luca", ""], ["Tagliapietra", "Valentina", ""], ["Rizzoli", "Annapaola", ""], ["Ros\u00e0", "Roberto", ""]]}, {"id": "1411.4021", "submitter": "Shefali Oza", "authors": "Shefali Oza, Joy E Lawn, Daniel R Hogan, Colin Mathers, Simon Cousens", "title": "Cause-of-death estimates for the early and late neonatal periods for 194\n  countries from 2000-2013", "comments": "123 pages (18 pages of main paper, 105 pages of appendix), in press\n  with Bulletin of the World Health Organization", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Cause-of-death distributions are important for prioritising\ninterventions. We estimated proportions, risks, and numbers of deaths (with\nuncertainty) for programme-relevant causes of neonatal death for 194 countries\nfor 2000-2013, differentiating between the early (days 0-6) and late (days\n7-27) neonatal periods.\n  Methods: For 65 high-quality VR countries, we used the observed early and\nlate neonatal proportional cause distributions. For the remaining 129\ncountries, we used multinomial logistic models to estimate the early and late\nproportional cause distributions. We used separate models, with different\ninputs, for low and high neonatal mortality countries. We applied these\ncause-specific proportions to neonatal death estimates from the United Nations\nby country/year to estimate cause-specific risks and numbers of deaths.\n  Findings: Of the 2.76 million neonatal deaths in 2013, 0.99 (uncertainty:\n0.70-1.31) million (35.7%) were estimated to be from preterm complications,\n0.64 (uncertainty: 0.46-0.84) million (23.4%) from intrapartum-related\ncomplications, and 0.43 (0.22-0.66) million (15.6%) from sepsis. Preterm\n(40.8%) and intrapartum-related (27.0%) complications accounted for the\nmajority of early neonatal deaths while infections caused nearly half of late\nneonatal deaths. In every region, preterm was the leading cause of neonatal\ndeath, with the highest risks in Southern Asia (11.9 per 1000 livebirths) and\nSub-Saharan Africa (9.5).\n  Conclusion: The neonatal cause-of-death distribution differs between the\nearly and late periods, and varies with NMR level and over time. To reduce\nneonatal deaths, this knowledge must be incorporated into policy decisions. The\nEvery Newborn Action Plan provides stimulus for countries to update national\nstrategies and include high-impact interventions to address these causes.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 19:37:03 GMT"}], "update_date": "2014-11-17", "authors_parsed": [["Oza", "Shefali", ""], ["Lawn", "Joy E", ""], ["Hogan", "Daniel R", ""], ["Mathers", "Colin", ""], ["Cousens", "Simon", ""]]}, {"id": "1411.4219", "submitter": "Le Bao", "authors": "Le Bao, Mary Mahy, Xiaoyue Niu, Tim Brown and Peter Ghys", "title": "A Hierarchical Model for Estimating HIV Epidemics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the global HIV pandemic enters its fourth decade, increasing numbers of\nsurveillance sites have been established which allows countries to look into\nthe epidemics at a finer scale, e.g. at sub-national level. However, the\nepidemic models have been applied independently to the sub-national areas\nwithin countries. An important technical barrier is that the availability and\nquality of the data vary widely from area to area, and many areas lack data for\nderiving stable and reliable results. To improve the accuracy of the results in\nareas with little data, we propose a hierarchical model that utilizes\ninformation efficiently by assuming similar characteristics of the epidemics\nacross areas within one country. The joint distribution of the parameters in\nthe hierarchical model can be approximated directly from the results of\nindependent fits without needing to the refit the data. As a result, the\nhierarchical model has better predictive ability than the independent model as\nshown in examples of multiple countries in Sub-Saharan Africa.\n", "versions": [{"version": "v1", "created": "Sun, 16 Nov 2014 05:49:43 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Bao", "Le", ""], ["Mahy", "Mary", ""], ["Niu", "Xiaoyue", ""], ["Brown", "Tim", ""], ["Ghys", "Peter", ""]]}, {"id": "1411.4289", "submitter": "Zbigniew Michna", "authors": "Zbigniew Michna, Peter Nielsen, Izabela Ewa Nielsen", "title": "The impact of stochastic lead times on the bullwhip effect", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we want to review the research state on the bullwhip effect\nin supply chains with stochastic lead times and give a contribution to\nquantifying the bullwhip effect. We analyze the models quantifying the bullwhip\neffect in supply chains with stochastic lead times and find advantages and\ndisadvantages of their approaches to the bullwhip problem. Using real data we\nconfirm that real lead times are stochastic and can be modeled by a sequence of\nindependent identically distributed random variables. Moreover we modify a\nmodel where stochastic lead times and lead time demand forecasting are\nconsidered and give an analytical expression for the bullwhip effect measure\nwhich indicates that the distribution of a lead time and the delay parameter of\nthe lead time demand prediction are the main factors of the bullwhip\nphenomenon. Moreover we analyze a recent paper of Michna and Nielsen adding\nsimulation results.\n", "versions": [{"version": "v1", "created": "Sun, 16 Nov 2014 18:48:13 GMT"}, {"version": "v2", "created": "Tue, 18 Nov 2014 09:30:20 GMT"}, {"version": "v3", "created": "Fri, 20 Mar 2015 16:21:45 GMT"}], "update_date": "2015-03-23", "authors_parsed": [["Michna", "Zbigniew", ""], ["Nielsen", "Peter", ""], ["Nielsen", "Izabela Ewa", ""]]}, {"id": "1411.4314", "submitter": "Nikolai Sinitsyn", "authors": "Benjamin H. Sims, Nikolai Sinitsyn, and Stephan J. Eidenbenz", "title": "Hierarchical and Matrix Structures in a Large Organizational Email\n  Network: Visualization and Modeling Approaches", "comments": "15 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents findings from a study of the email network of a large\nscientific research organization, focusing on methods for visualizing and\nmodeling organizational hierarchies within large, complex network datasets. In\nthe first part of the paper, we find that visualization and interpretation of\ncomplex organizational network data is facilitated by integration of network\ndata with information on formal organizational divisions and levels. By\naggregating and visualizing email traffic between organizational units at\nvarious levels, we derive several insights into how large subdivisions of the\norganization interact with each other and with outside organizations. Our\nanalysis shows that line and program management interactions in this\norganization systematically deviate from the idealized pattern of interaction\nprescribed by \"matrix management.\" In the second part of the paper, we propose\na power law model for predicting degree distribution of organizational email\ntraffic based on hierarchical relationships between managers and employees.\nThis model considers the influence of global email announcements sent from\nmanagers to all employees under their supervision, and the role support staff\nplay in generating email traffic, acting as agents for managers. We also\nanalyze patterns in email traffic volume over the course of a work week.\n", "versions": [{"version": "v1", "created": "Sun, 16 Nov 2014 22:22:54 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Sims", "Benjamin H.", ""], ["Sinitsyn", "Nikolai", ""], ["Eidenbenz", "Stephan J.", ""]]}, {"id": "1411.4550", "submitter": "Andrew Buckley", "authors": "Todd C. Rae, Andy Buckley", "title": "TaxMan: an online facility for the coding of continuous characters for\n  cladistic analysis", "comments": "Unpublished manuscript from 2008/2009, provided for reference and\n  link to implementation of (now superseded) statistical procedure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A consensus is emerging that continuous (or metric) measures can be useful in\nphylogenetic systematics. Many of the methods for coding such characters, how-\never, employ elements that are arbitrary and therefore should be excluded from\nuse in cladistic analysis. The continued use of such potentially inappropriate\nmethods can be attributed to either their simplicity or the availability of\ncomputer programs specifically designed to produce data matrices using these\nmethods. Conversely, one of the most suitable methods, homogeneous subset\ncoding (HSC), is underused, probably due to the lack of a suitable software\nimplementation for this somewhat complex procedure.\n  This paper describes TaxMan, a Web-based facility for the coding of\ncontinuous data using HSC. Data are entered using a form accessible via any\ninternet browser and are automatically converted to a matrix suitable for input\ninto tree-searching software. This implementation of the HSC technique provides\nan uncomplicated procedure for the incorporation of metric data in phylogenetic\nsystematics. The algorithmic implementation of the HSC procedure, and\ninterpolation of the Studentised range and maximum modulus distributions\nrequired by it, is described in detail in appendices.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 17:03:57 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Rae", "Todd C.", ""], ["Buckley", "Andy", ""]]}, {"id": "1411.4564", "submitter": "Matteo Fasiolo", "authors": "Matteo Fasiolo, Natalya Pya and Simon N. Wood", "title": "A comparison of inferential methods for highly non-linear state space\n  models in ecology and epidemiology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Highly non-linear, chaotic or near chaotic, dynamic models are important in\nfields such as ecology and epidemiology: for example, pest species and diseases\noften display highly non-linear dynamics. However, such models are problematic\nfrom the point of view of statistical inference. The defining feature of\nchaotic and near chaotic systems is extreme sensitivity to small changes in\nsystem states and parameters, and this can interfere with inference. There are\ntwo main classes of methods for circumventing these difficulties: information\nreduction approaches, such as Approximate Bayesian Computation or Synthetic\nLikelihood and state space methods, such as Particle Markov chain Monte Carlo,\nIterated Filtering or Parameter Cascading. The purpose of this article is to\ncompare the methods, in order to reach conclusions about how to approach\ninference with such models in practice. We show that neither class of methods\nis universally superior to the other. We show that state space methods can\nsuffer multimodality problems in settings with low process noise or model\nmis-specification, leading to bias toward stable dynamics and high process\nnoise. Information reduction methods avoid this problem but, under the correct\nmodel and with sufficient process noise, state space methods lead to\nsubstantially sharper inference than information reduction methods. More\npractically, there are also differences in the tuning requirements of different\nmethods. Our overall conclusion is that model development and checking should\nprobably be performed using an information reduction method with low tuning\nrequirements, while for final inference it is likely to be better to switch to\na state space method, checking results against the information reduction\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 17:34:25 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2015 10:36:54 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Fasiolo", "Matteo", ""], ["Pya", "Natalya", ""], ["Wood", "Simon N.", ""]]}, {"id": "1411.4715", "submitter": "Yang Liu", "authors": "Yang Liu and Philip Kokic", "title": "Predictive Inference for Spatio-temporal Precipitation Data and Its\n  Extremes", "comments": "Under review at Journal of the American Statistical Association. 27\n  pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modelling of precipitation and its extremes is important for urban and\nagriculture planning purposes. We present a method for producing spatial\npredictions and measures of uncertainty for spatio-temporal data that is\nheavy-tailed and subject to substaintial skewness which often arise in\nmeasurements of many environmental processes, and we apply the method to\nprecipitation data in south-west Western Australia. A generalised hyperbolic\nBayesian hierarchical model is constructed for the intensity, frequency and\nduration of daily precipitation, including the extremes. Unlike models based on\nextreme value theory, which only model maxima of finite-sized blocks or\nexceedances above a large threshold, the proposed model uses all the data\navailable efficiently, and hence not only fits the extremes but also models the\nentire rainfall distribution. It captures spatial and temporal clustering, as\nwell as spatially and temporally varying volatility and skewness. The model\nassumes that the regional precipitation is driven by a latent process\ncharacterised by geographical and climatological covariates. Effects not fully\ndescribed by the covariates are captured by spatial and temporal structure in\nthe hierarchies. Inference is provided by MCMC using a Metropolis-Hastings\nalgorithm and spatial interpolation method, which provide a natural approach\nfor estimating uncertainty. Similarly both spatial and temporal predictions\nwith uncertainty can be produced with the model.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 01:54:21 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Liu", "Yang", ""], ["Kokic", "Philip", ""]]}, {"id": "1411.4763", "submitter": "Faouzi Bellili", "authors": "Faouzi Bellili, Rabii Meftehi, Sofiene Affes, and Alex Stephenne", "title": "Maximum Likelihood SNR Estimation of Linearly-Modulated Signals over\n  Time-Varying Flat-Fading SIMO Channels", "comments": "38 pages, 8 figures, Extended Version of a paper recently accepted\n  for publication in IEEE Trans. on Signal Processing, 2014", "journal-ref": null, "doi": "10.1109/TSP.2014.2364017", "report-no": null, "categories": "stat.AP cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle for the first time the problem of maximum likelihood\n(ML) estimation of the signal-to-noise ratio (SNR) parameter over time-varying\nsingle-input multiple-output (SIMO) channels. Both the data-aided (DA) and the\nnon-data-aided (NDA) schemes are investigated. Unlike classical techniques\nwhere the channel is assumed to be slowly time-varying and, therefore,\nconsidered as constant over the entire observation period, we address the more\nchallenging problem of instantaneous (i.e., short-term or local) SNR estimation\nover fast time-varying channels. The channel variations are tracked locally\nusing a polynomial-in-time expansion. First, we derive in closed-form\nexpressions the DA ML estimator and its bias. The latter is subsequently\nsubtracted in order to obtain a new unbiased DA estimator whose variance and\nthe corresponding Cram\\'er-Rao lower bound (CRLB) are also derived in closed\nform. Due to the extreme nonlinearity of the log-likelihood function (LLF) in\nthe NDA case, we resort to the expectation-maximization (EM) technique to\niteratively obtain the exact NDA ML SNR estimates within very few iterations.\nMost remarkably, the new EM-based NDA estimator is applicable to any\nlinearly-modulated signal and provides sufficiently accurate soft estimates\n(i.e., soft detection) for each of the unknown transmitted symbols. Therefore,\nhard detection can be easily embedded in the iteration loop in order to improve\nits performance at low to moderate SNR levels. We show by extensive computer\nsimulations that the new estimators are able to accurately estimate the\ninstantaneous per-antenna SNRs as they coincide with the DA CRLB over a wide\nrange of practical SNRs.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 08:36:40 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Bellili", "Faouzi", ""], ["Meftehi", "Rabii", ""], ["Affes", "Sofiene", ""], ["Stephenne", "Alex", ""]]}, {"id": "1411.4846", "submitter": "Madhuchhanda Bhattacharjee Dr.", "authors": "Madhuchhanda Bhattacharjee and Elja Arjas", "title": "Modelling and analysis of time in-homogeneous recurrent event processes\n  in a heterogeneous population: A case study of HRTs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a method for the statistical analysis of continually\nmonitored data arising in a recurrent diseases problem. The model enables\nindividual level inference in the presence of time transience and population\nheterogeneity. This is achieved by applying Bayesian hierarchical modelling,\nwhere marked point processes are used as descriptions of the individual data,\nwith latent variables providing a means of modelling long range dependence and\ntransience over time. In addition to providing a sound probabilistic\nformulation of a rather complex data set, the proposed method is also\nsuccessful in prediction of future outcomes. Computational difficulties arising\nfrom the analytic intractability of this Bayesian model were solved by\nimplementing the method into the BUGS software and using standard computational\nfacilities.\n  We illustrate this approach by an analysis of a data set on hormone\nreplacement therapies (HRTs). The data contain, in the form of diaries on\nbleeding patterns maintained by individual patients, detailed information on\nhow they responded to different HRTs. The proposed model is able to capture the\nessential features of these treatments as well as provide realistic individual\nlevel predictions on the future bleeding patterns.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 14:05:23 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Bhattacharjee", "Madhuchhanda", ""], ["Arjas", "Elja", ""]]}, {"id": "1411.4867", "submitter": "Jens Malmros", "authors": "Jens Malmros, Fredrik Liljeros, and Tom Britton", "title": "Respondent-driven sampling and an unusual epidemic", "comments": "20 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.SI math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Respondent-driven sampling (RDS) is frequently used when sampling\nhard-to-reach and/or stigmatized communities. RDS utilizes a peer-driven\nrecruitment mechanism where sampled individuals pass on participation coupons\nto at most $c$ of their acquaintances in the community ($c=3$ being a common\nchoice), who then in turn pass on to their acquaintances if they choose to\nparticipate, and so on. This process of distributing coupons is shown to behave\nlike a new Reed-Frost type network epidemic model, in which becoming infected\ncorresponds to receiving a coupon. The difference from existing network\nepidemic models is that an infected individual can not infect (i.e.\\ sample)\nall of its contacts, but only at most $c$ of them. We calculate $R_0$, the\nprobability of a major \"outbreak\", and the relative size of a major outbreak in\nthe limit of infinite population size and evaluate their adequacy in finite\npopulations. We study the effect of varying $c$ and compare RDS to the\ncorresponding usual epidemic models, i.e.\\ the case of $c=\\infty$. Our results\nsuggest that the number of coupons has a large effect on RDS recruitment.\nAdditionally, we use our findings to explain previous empirical observations.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2014 16:11:33 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Malmros", "Jens", ""], ["Liljeros", "Fredrik", ""], ["Britton", "Tom", ""]]}, {"id": "1411.4873", "submitter": "Colin Fogarty", "authors": "Colin B. Fogarty, Mark E. Mikkelsen, David F. Gaieski, Dylan S. Small", "title": "Discrete Optimization for Interpretable Study Populations and\n  Randomization Inference in an Observational Study of Severe Sepsis Mortality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by an observational study of the effect of hospital ward versus\nintensive care unit admission on severe sepsis mortality, we develop methods to\naddress two common problems in observational studies: (1) when there is a lack\nof covariate overlap between the treated and control groups, how to define an\ninterpretable study population wherein inference can be conducted without\nextrapolating with respect to important variables; and (2) how to use\nrandomization inference to form confidence intervals for the average treatment\neffect with binary outcomes. Our solution to problem (1) incorporates existing\nsuggestions in the literature while yielding a study population that is easily\nunderstood in terms of the covariates themselves, and can be solved using an\nefficient branch-and-bound algorithm. We address problem (2) by solving a\nlinear integer program to utilize the worst case variance of the average\ntreatment effect among values for unobserved potential outcomes that are\ncompatible with the null hypothesis. Our analysis finds no evidence for a\ndifference between the sixty day mortality rates if all individuals were\nadmitted to the ICU and if all patients were admitted to the hospital ward\namong less severely ill patients and among patients with cryptic septic shock.\nWe implement our methodology in R, providing scripts in the supplementary\nmaterial.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 15:34:38 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2015 05:01:23 GMT"}, {"version": "v3", "created": "Wed, 12 Aug 2015 14:08:09 GMT"}], "update_date": "2015-08-13", "authors_parsed": [["Fogarty", "Colin B.", ""], ["Mikkelsen", "Mark E.", ""], ["Gaieski", "David F.", ""], ["Small", "Dylan S.", ""]]}, {"id": "1411.4898", "submitter": "Mauro Bernardi", "authors": "Mauro Bernardi and Antonio Di Ruggiero", "title": "Extracting the Italian output gap: a Bayesian approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the last decades particular effort has been directed towards\nunderstanding and predicting the relevant state of the business cycle with the\nobjective of decomposing permanent shocks from those having only a transitory\nimpact on real output. This trend--cycle decomposition has a relevant impact on\nseveral economic and fiscal variables and constitutes by itself an important\nindicator for policy purposes. This paper deals with trend--cycle decomposition\nfor the Italian economy having some interesting peculiarities which makes it\nattractive to analyse from both a statistic and an historical perspective. We\npropose an univariate model for the quarterly real GDP, subsequently extended\nto include the price dynamics through a Phillips curve. This study considers a\nseries of the Italian quarterly real GDP recently released by OECD which\nincludes both the 1960s and the recent global financial crisis of 2007--2008.\nParameters estimate as well as the signal extraction are performed within the\nBayesian paradigm which effectively handles complex models where the parameters\nenter the log--likelihood function in a strongly nonlinear way. A new Adaptive\nIndependent Metropolis--within--Gibbs sampler is then developed to efficiently\nsimulate the parameters of the unobserved cycle. Our results suggest that\ninflation influences the Output Gap estimate, making the extracted Italian OG\nan important indicator of inflation pressures on the real side of the economy,\nas stated by the Phillips theory. Moreover, our estimate of the sequence of\npeaks and troughs of the Output Gap is in line with the OECD official dating of\nthe Italian business cycle.\n", "versions": [{"version": "v1", "created": "Sat, 1 Nov 2014 14:55:46 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Bernardi", "Mauro", ""], ["Di Ruggiero", "Antonio", ""]]}, {"id": "1411.5076", "submitter": "Vadim Sokolov", "authors": "Nicholas Polson and Vadim Sokolov", "title": "Bayesian Particle Tracking of Traffic Flows", "comments": null, "journal-ref": null, "doi": "10.1109/TITS.2017.2650947", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a Bayesian particle filter for tracking traffic flows that is\ncapable of capturing non-linearities and discontinuities present in flow\ndynamics. Our model includes a hidden state variable that captures sudden\nregime shifts between traffic free flow, breakdown and recovery. We develop an\nefficient particle learning algorithm for real time on-line inference of states\nand parameters. This requires a two step approach, first, resampling the\ncurrent particles, with a mixture predictive distribution and second,\npropagation of states using the conditional posterior distribution. Particle\nlearning of parameters follows from updating recursions for conditional\nsufficient statistics. To illustrate our methodology, we analyze measurements\nof daily traffic flow from the Illinois interstate I-55 highway system. We\ndemonstrate how our filter can be used to inference the change of traffic flow\nregime on a highway road segment based on a measurement from freeway\nsingle-loop detectors. Finally, we conclude with directions for future\nresearch.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2014 00:00:02 GMT"}, {"version": "v2", "created": "Fri, 28 Nov 2014 03:25:39 GMT"}, {"version": "v3", "created": "Mon, 16 Nov 2015 01:02:29 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Polson", "Nicholas", ""], ["Sokolov", "Vadim", ""]]}, {"id": "1411.5086", "submitter": "Natalia Arzeno", "authors": "Natalia M. Arzeno, Karla A. Lawson, Sarah V. Duzinski, Haris Vikalo", "title": "Designing Optimal Mortality Risk Prediction Scores that Preserve\n  Clinical Knowledge", "comments": "30 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many in-hospital mortality risk prediction scores dichotomize predictive\nvariables to simplify the score calculation. However, hard thresholding in\nthese additive stepwise scores of the form \"add x points if variable v is\nabove/below threshold t\" may lead to critical failures. In this paper, we seek\nto develop risk prediction scores that preserve clinical knowledge embedded in\nfeatures and structure of the existing additive stepwise scores while\naddressing limitations caused by variable dichotomization. To this end, we\npropose a novel score structure that relies on a transformation of predictive\nvariables by means of nonlinear logistic functions facilitating smooth\ndifferentiation between critical and normal values of the variables. We develop\nan optimization framework for inferring parameters of the logistic functions\nfor a given patient population via cyclic block coordinate descent. The\nparameters may readily be updated as the patient population and standards of\ncare evolve. We tested the proposed methodology on two populations: (1) brain\ntrauma patients admitted to the intensive care unit of the Dell Children's\nMedical Center of Central Texas between 2007 and 2012, and (2) adult ICU\npatient data from the MIMIC II database. The results are compared with those\nobtained by the widely used PRISM III and SOFA scores. The prediction power of\na score is evaluated using area under ROC curve, Youden's index, and\nprecision-recall balance in a cross-validation study. The results demonstrate\nthat the new framework enables significant performance improvements over PRISM\nIII and SOFA in terms of all three criteria.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2014 01:15:49 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2015 18:20:46 GMT"}], "update_date": "2015-04-30", "authors_parsed": [["Arzeno", "Natalia M.", ""], ["Lawson", "Karla A.", ""], ["Duzinski", "Sarah V.", ""], ["Vikalo", "Haris", ""]]}, {"id": "1411.5179", "submitter": "Constantino Antonio Garc\\'ia", "authors": "Constantino A. Garc\\'ia, Abraham Otero, Xos\\'e Vila, David G.\n  M\\'arquez", "title": "A new algorithm for wavelet-based heart rate variability analysis", "comments": null, "journal-ref": "Biomedical Signal Processing and Control, Volume 8, Issue 6,\n  November 2013, Pages 542-550", "doi": "10.1016/j.bspc.2013.05.006", "report-no": null, "categories": "q-bio.QM physics.med-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most promising non-invasive markers of the activity of the\nautonomic nervous system is Heart Rate Variability (HRV). HRV analysis toolkits\noften provide spectral analysis techniques using the Fourier transform, which\nassumes that the heart rate series is stationary. To overcome this issue, the\nShort Time Fourier Transform is often used (STFT). However, the wavelet\ntransform is thought to be a more suitable tool for analyzing non-stationary\nsignals than the STFT. Given the lack of support for wavelet-based analysis in\nHRV toolkits, such analysis must be implemented by the researcher. This has\nmade this technique underutilized. This paper presents a new algorithm to\nperform HRV power spectrum analysis based on the Maximal Overlap Discrete\nWavelet Packet Transform (MODWPT). The algorithm calculates the power in any\nspectral band with a given tolerance for the band's boundaries. The MODWPT\ndecomposition tree is pruned to avoid calculating unnecessary wavelet\ncoefficients, thereby optimizing execution time. The center of energy shift\ncorrection is applied to achieve optimum alignment of the wavelet coefficients.\nThis algorithm has been implemented in RHRV, an open-source package for HRV\nanalysis. To the best of our knowledge, RHRV is the first HRV toolkit with\nsupport for wavelet-based spectral analysis.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2014 11:11:45 GMT"}], "update_date": "2014-11-20", "authors_parsed": [["Garc\u00eda", "Constantino A.", ""], ["Otero", "Abraham", ""], ["Vila", "Xos\u00e9", ""], ["M\u00e1rquez", "David G.", ""]]}, {"id": "1411.5310", "submitter": "BaoLuo Sun", "authors": "BaoLuo Sun and Eric J. Tchetgen Tchetgen", "title": "On Inverse Probability Weighting for Nonmonotone Missing at Random Data", "comments": null, "journal-ref": "Journal of the American Statistical Association 113(2018) 369-379", "doi": "10.1080/01621459.2016.1256814", "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of coherent missing data models to account for nonmonotone\nmissing at random (MAR) data by inverse probability weighting (IPW) remains to\ndate largely unresolved. As a consequence, IPW has essentially been restricted\nfor use only in monotone missing data settings. We propose a class of models\nfor nonmonotone missing data mechanisms that spans the MAR model, while\nallowing the underlying full data law to remain unrestricted. For parametric\nspecifications within the proposed class, we introduce an unconstrained maximum\nlikelihood estimator for estimating the missing data probabilities which can be\neasily implemented using existing software. To circumvent potential convergence\nissues with this procedure, we also introduce a Bayesian constrained approach\nto estimate the missing data process which is guaranteed to yield inferences\nthat respect all model restrictions. The efficiency of the standard IPW\nestimator is improved by incorporating information from incomplete cases\nthrough an augmented estimating equation which is optimal within a large class\nof estimating equations. We investigate the finite-sample properties of the\nproposed estimators in a simulation study and illustrate the new methodology in\nan application evaluating key correlates of preterm delivery for infants born\nto HIV infected mothers in Botswana, Africa.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2014 18:38:20 GMT"}, {"version": "v2", "created": "Sat, 17 Oct 2015 18:54:27 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Sun", "BaoLuo", ""], ["Tchetgen", "Eric J. Tchetgen", ""]]}, {"id": "1411.5471", "submitter": "Morgan Kelly", "authors": "Morgan Kelly, Cormac \\'O Gr\\'ada", "title": "Change points and temporal dependence in reconstructions of annual\n  temperature: Did Europe experience a Little Ice Age?", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS753 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 3, 1372-1394", "doi": "10.1214/14-AOAS753", "report-no": "IMS-AOAS-AOAS753", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the timing and extent of Northern European temperature falls\nduring the Little Ice Age, using standard temperature reconstructions. However,\nwe can find little evidence of temporal dependence or structural breaks in\nEuropean weather before the twentieth century. Instead, European weather\nbetween the fifteenth and nineteenth centuries resembles uncorrelated draws\nfrom a distribution with a constant mean (although there are occasional decades\nof markedly lower summer temperature) and variance, with the same behavior\nholding more tentatively back to the twelfth century. Our results suggest that\nobserved conditions during the Little Ice Age in Northern Europe are consistent\nwith random climate variability. The existing consensus about apparent cold\nconditions may stem in part from a Slutsky effect, where smoothing data gives\nthe spurious appearance of irregular oscillations when the underlying time\nseries is white noise.\n", "versions": [{"version": "v1", "created": "Thu, 20 Nov 2014 08:40:03 GMT"}], "update_date": "2014-11-21", "authors_parsed": [["Kelly", "Morgan", ""], ["Gr\u00e1da", "Cormac \u00d3", ""]]}, {"id": "1411.5497", "submitter": "Jie Peng", "authors": "Jie Peng, Debashis Paul, Hans-Georg M\\\"uller", "title": "Time-warped growth processes, with applications to the modeling of\n  boom-bust cycles in house prices", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS740 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 3, 1561-1582", "doi": "10.1214/14-AOAS740", "report-no": "IMS-AOAS-AOAS740", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  House price increases have been steady over much of the last 40 years, but\nthere have been occasional declines, most notably in the recent housing bust\nthat started around 2007, on the heels of the preceding housing bubble. We\nintroduce a novel growth model that is motivated by time-warping models in\nfunctional data analysis and includes a nonmonotone time-warping component that\nallows the inclusion and description of boom-bust cycles and facilitates\ninsights into the dynamics of asset bubbles. The underlying idea is to model\nlongitudinal growth trajectories for house prices and other phenomena, where\ntemporal setbacks and deflation may be encountered, by decomposing such\ntrajectories into two components. A first component corresponds to underlying\nsteady growth driven by inflation that anchors the observed trajectories on a\nsimple first order linear differential equation, while a second boom-bust\ncomponent is implemented as time warping. Time warping is a commonly\nencountered phenomenon and reflects random variation along the time axis. Our\napproach to time warping is more general than previous approaches by admitting\nthe inclusion of nonmonotone warping functions. The anchoring of the\ntrajectories on an underlying linear dynamic system also makes the time-warping\ncomponent identifiable and enables straightforward estimation procedures for\nall model components. The application to the dynamics of housing prices as\nobserved for 19 metropolitan areas in the U.S. from December 1998 to July 2013\nreveals that the time setbacks corresponding to nonmonotone time warping vary\nsubstantially across markets and we find indications that they are related to\nmarket-specific growth rates.\n", "versions": [{"version": "v1", "created": "Thu, 20 Nov 2014 10:42:29 GMT"}], "update_date": "2014-11-21", "authors_parsed": [["Peng", "Jie", ""], ["Paul", "Debashis", ""], ["M\u00fcller", "Hans-Georg", ""]]}, {"id": "1411.5510", "submitter": "Abel Rodriguez", "authors": "Abel Rodriguez, David B. Dunson", "title": "Functional clustering in nested designs: Modeling variability in\n  reproductive epidemiology studies", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS751 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 3, 1416-1442", "doi": "10.1214/14-AOAS751", "report-no": "IMS-AOAS-AOAS751", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss functional clustering procedures for nested designs, where\nmultiple curves are collected for each subject in the study. We start by\nconsidering the application of standard functional clustering tools to this\nproblem, which leads to groupings based on the average profile for each\nsubject. After discussing some of the shortcomings of this approach, we present\na mixture model based on a generalization of the nested Dirichlet process that\nclusters subjects based on the distribution of their curves. By using mixtures\nof generalized Dirichlet processes, the model induces a much more flexible\nprior on the partition structure than other popular model-based clustering\nmethods, allowing for different rates of introduction of new clusters as the\nnumber of observations increases. The methods are illustrated using hormone\nprofiles from multiple menstrual cycles collected for women in the Early\nPregnancy Study.\n", "versions": [{"version": "v1", "created": "Thu, 20 Nov 2014 11:43:28 GMT"}], "update_date": "2014-11-21", "authors_parsed": [["Rodriguez", "Abel", ""], ["Dunson", "David B.", ""]]}, {"id": "1411.5634", "submitter": "Daniel Chambers", "authors": "Daniel W. Chambers, Jenny A. Baglivo, John E. Ebel, Alan L. Kafka", "title": "Earthquake Forecasting Using Hidden Markov Models", "comments": null, "journal-ref": "Pure Appl. Geophys. 169 (2012) 625-639", "doi": "10.1007/s00024-011-0315-1", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a novel method, based on hidden Markov models, to\nforecast earthquakes and applies the method to mainshock seismic activity in\nsouthern California and western Nevada. The forecasts are of the probability of\na mainshock within one, five, and ten days in the entire study region or in\nspecific subregions and are based on the observations available at the forecast\ntime, namely the inter event times and locations of the previous mainshocks and\nthe elapsed time since the most recent one. Hidden Markov models have been\napplied to many problems, including earthquake classification; this is the\nfirst application to earthquake forecasting.\n", "versions": [{"version": "v1", "created": "Thu, 20 Nov 2014 18:36:54 GMT"}], "update_date": "2014-11-21", "authors_parsed": [["Chambers", "Daniel W.", ""], ["Baglivo", "Jenny A.", ""], ["Ebel", "John E.", ""], ["Kafka", "Alan L.", ""]]}, {"id": "1411.5754", "submitter": "Michael Schuckers", "authors": "Michael Schuckers and Steve Argeris", "title": "You Can Beat the Market: Estimating the Return on Investment for\n  National Hockey League (NHL) Team Scouting using a Draft Value Pick Chart for\n  the NHL", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scouting is a major part of talent acquisition for any professional sports\nteam. In the National Hockey League (NHL), the market for scouting is set by\nthe NHLs Central Scouting Service which develops a ranking of draft eligible\nplayers. In addition to the Central Scouting rankings, NHL teams use their own\ninternal scouting to augment their knowledge of eligible players and develop\ntheir own rankings. Using a novel statistical approach we show in this paper\nthat the additional information possessed by teams provides better rankings\nthan those of Central Scouting. Using data from the 1998 to 2002 NHL drafts, we\nestimate that the average yearly gain per team from their internal scouting is\nbetween \\$1.8MM and \\$5.2MM. These values are consistent across the three\nmeasures of player productivity that we consider: cumulative Games Played,\ncumulative Time On Ice and cumulative Goals Versus Threshold where we have\naggregated these metrics across the first seven years post draft. We used this\ntime frame since teams generally retain rights to their draft picks for seven\nyears. Further, we find that no individual team outperformed the others in\nterms of draft performance. One byproduct of our analysis is the development of\na Draft Value Pick Chart to assess the worth of an individual selection.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2014 03:22:06 GMT"}], "update_date": "2014-11-24", "authors_parsed": [["Schuckers", "Michael", ""], ["Argeris", "Steve", ""]]}, {"id": "1411.5774", "submitter": "Zhenke Wu", "authors": "Zhenke Wu, Maria Deloria-Knoll, Laura L. Hammitt, Scott L. Zeger", "title": "Partially-Latent Class Models (pLCM) for Case-Control Studies of\n  Childhood Pneumonia Etiology", "comments": "25 pages, 4 figures, 1 supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In population studies on the etiology of disease, one goal is the estimation\nof the fraction of cases attributable to each of several causes. For example,\npneumonia is a clinical diagnosis of lung infection that may be caused by\nviral, bacterial, fungal, or other pathogens. The study of pneumonia etiology\nis challenging because directly sampling from the lung to identify the\netiologic pathogen is not standard clinical practice in most settings. Instead,\nmeasurements from multiple peripheral specimens are made. This paper introduces\nthe statistical methodology designed for estimating the population etiology\ndistribution and the individual etiology probabilities in the Pneumonia\nEtiology Research for Child Health (PERCH) study of 9; 500 children for 7 sites\naround the world. We formulate the scientific problem in statistical terms as\nestimating the mixing weights and latent class indicators under a\npartially-latent class model (pLCM) that combines heterogeneous measurements\nwith different error rates obtained from a case-control study. We introduce the\npLCM as an extension of the latent class model. We also introduce graphical\ndisplays of the population data and inferred latent-class frequencies. The\nmethods are tested with simulated data, and then applied to PERCH data. The\npaper closes with a brief description of extensions of the pLCM to the\nregression setting and to the case where conditional independence among the\nmeasures is relaxed.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2014 06:16:06 GMT"}], "update_date": "2014-11-24", "authors_parsed": [["Wu", "Zhenke", ""], ["Deloria-Knoll", "Maria", ""], ["Hammitt", "Laura L.", ""], ["Zeger", "Scott L.", ""]]}, {"id": "1411.5775", "submitter": "Sixia Chen", "authors": "Sixia Chen, Jae-Kwang Kim", "title": "Two-phase sampling experiment for propensity score estimation in\n  self-selected samples", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS746 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 3, 1492-1515", "doi": "10.1214/14-AOAS746", "report-no": "IMS-AOAS-AOAS746", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-selected samples are frequently obtained due to different levels of\nsurvey participation propensity of the survey individuals. When the survey\nparticipation is related to the survey topic of interest, propensity score\nweighting adjustment using auxiliary information may lead to biased estimation.\nIn this paper, we consider a parametric model for the response probability that\nincludes the study variable itself in the covariates of the model and proposes\na novel application of two-phase sampling to estimate the parameters of the\npropensity model. The proposed method includes an experiment in which data are\ncollected again from a subset of the original self-selected sample. With this\ntwo-phase sampling experiment, we can estimate the parameters in a propensity\nscore model consistently. Then the propensity score adjustment can be applied\nto the self-selected sample to estimate the population parameters. Sensitivity\nof the selection model assumption is investigated from two limited simulation\nstudies. The proposed method is applied to the 2012 Iowa Caucus Survey.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2014 06:16:30 GMT"}], "update_date": "2014-11-24", "authors_parsed": [["Chen", "Sixia", ""], ["Kim", "Jae-Kwang", ""]]}, {"id": "1411.5780", "submitter": "A. Paula Palacios", "authors": "A. Paula Palacios, J. Miguel Mar\\'in, Emiliano J. Quinto, Michael P.\n  Wiper", "title": "Bayesian modeling of bacterial growth for multiple populations", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS720 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 3, 1516-1537", "doi": "10.1214/14-AOAS720", "report-no": "IMS-AOAS-AOAS720", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bacterial growth models are commonly used for the prediction of microbial\nsafety and the shelf life of perishable foods. Growth is affected by several\nenvironmental factors such as temperature, acidity level and salt\nconcentration. In this study, we develop two models to describe bacterial\ngrowth for multiple populations under both equal and different environmental\nconditions. First, a semi-parametric model based on the Gompertz equation is\nproposed. Assuming that the parameters of the Gompertz equation may vary in\nrelation to the running conditions under which the experiment is performed, we\nuse feedforward neural networks to model the influence of these environmental\nfactors on the growth parameters. Second, we propose a more general model which\ndoes not assume any underlying parametric form for the growth function. Thus,\nwe consider a neural network as a primary growth model which includes the\ninfluencing environmental factors as inputs to the network. One of the main\ndisadvantages of neural networks models is that they are often very difficult\nto tune, which complicates fitting procedures. Here, we show that a simple\nBayesian approach to fitting these models can be implemented via the software\npackage WinBugs. Our approach is illustrated using real experimental Listeria\nmonocytogenes growth data.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2014 07:37:34 GMT"}], "update_date": "2014-11-24", "authors_parsed": [["Palacios", "A. Paula", ""], ["Mar\u00edn", "J. Miguel", ""], ["Quinto", "Emiliano J.", ""], ["Wiper", "Michael P.", ""]]}, {"id": "1411.5838", "submitter": "Xi Chen", "authors": "Xi Chen, Simo S\\\"arkk\\\"a, Simon Godsill", "title": "A Bayesian Particle Filtering Method For Brain Source Localisation", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore the multiple source localisation problem in the\ncerebral cortex using magnetoencephalography (MEG) data. We model neural\ncurrents as point-wise dipolar sources which dynamically evolve over time, then\nmodel dipole dynamics using a probabilistic state space model in which dipole\nlocations are strictly constrained to lie within the cortex. Based on the\nproposed models, we develop a Bayesian particle filtering algorithm for\nlocalisation of both known and unknown numbers of dipoles. The algorithm\nconsists of a region of interest (ROI) estimation step for initial dipole\nnumber estimation, a Gibbs multiple particle filter (GMPF) step for individual\ndipole state estimation, and a selection criterion step for selecting the final\nestimates. The estimated results from the ROI estimation are used to adaptively\nadjust particle filter's sample size to reduce the overall computational cost.\nThe proposed models and the algorithm are tested in numerical experiments.\nResults are compared with existing particle filtering methods. The numerical\nresults show that the proposed methods can achieve improved performance metrics\nin terms of dipole number estimation and dipole localisation.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2014 12:25:23 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2015 14:17:09 GMT"}, {"version": "v3", "created": "Wed, 17 Jun 2015 09:39:06 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Chen", "Xi", ""], ["S\u00e4rkk\u00e4", "Simo", ""], ["Godsill", "Simon", ""]]}, {"id": "1411.6028", "submitter": "Caleb Miles", "authors": "Caleb Miles, Ilya Shpitser, Phyllis Kanki, Seema Meloni, and Eric\n  Tchetgen Tchetgen", "title": "Quantifying an Adherence Path-Specific Effect of Antiretroviral Therapy\n  in the Nigeria PEPFAR Program", "comments": "To appear in Journal of the American Statistical Association, 22\n  pages + 4 pages of supplemental material, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the early 2000s, evidence has accumulated for a significant\ndifferential effect of first-line antiretroviral therapy (ART) regimens on\nhuman immunodeficiency virus (HIV) viral load suppression. This finding was\nreplicated in our data from the Harvard President's Emergency Plan for AIDS\nRelief (PEPFAR) program in Nigeria. Investigators were interested in finding\nthe source of these differences, i.e., understanding the mechanisms through\nwhich one regimen outperforms another, particularly via adherence. This\nquestion can be naturally formulated via mediation analysis with adherence\nplaying the role of a mediator. Existing mediation analysis results, however,\nhave relied on an assumption of no exposure-induced confounding of the\nintermediate variable, and generally require an assumption of no unmeasured\nconfounding for nonparametric identification. Both assumptions are violated by\nthe presence of drug toxicity. In this paper, we relax these assumptions and\nshow that certain path-specific effects remain identified under weaker\nconditions. We focus on the path-specific effect solely mediated by adherence\nand not by toxicity and propose an estimator for this effect. We illustrate\nwith simulations and present results from a study applying the methodology to\nthe Harvard PEPFAR data. Supplementary materials are available online.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2014 21:32:14 GMT"}, {"version": "v2", "created": "Sat, 30 Sep 2017 00:50:45 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Miles", "Caleb", ""], ["Shpitser", "Ilya", ""], ["Kanki", "Phyllis", ""], ["Meloni", "Seema", ""], ["Tchetgen", "Eric Tchetgen", ""]]}, {"id": "1411.6144", "submitter": "Wesley Tansey", "authors": "Wesley Tansey and Oluwasanmi Koyejo and Russell A. Poldrack and James\n  G. Scott", "title": "False discovery rate smoothing", "comments": "Added misspecification analysis, added pathological scenario\n  discussions, additional comparisons, new graph fused lasso algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present false discovery rate smoothing, an empirical-Bayes method for\nexploiting spatial structure in large multiple-testing problems. FDR smoothing\nautomatically finds spatially localized regions of significant test statistics.\nIt then relaxes the threshold of statistical significance within these regions,\nand tightens it elsewhere, in a manner that controls the overall\nfalse-discovery rate at a given level. This results in increased power and\ncleaner spatial separation of signals from noise. The approach requires solving\na non-standard high-dimensional optimization problem, for which an efficient\naugmented-Lagrangian algorithm is presented. In simulation studies, FDR\nsmoothing exhibits state-of-the-art performance at modest computational cost.\nIn particular, it is shown to be far more robust than existing methods for\nspatially dependent multiple testing. We also apply the method to a data set\nfrom an fMRI experiment on spatial working memory, where it detects patterns\nthat are much more biologically plausible than those detected by standard\nFDR-controlling methods. All code for FDR smoothing is publicly available in\nPython and R.\n", "versions": [{"version": "v1", "created": "Sat, 22 Nov 2014 17:17:46 GMT"}, {"version": "v2", "created": "Mon, 14 Nov 2016 18:32:39 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Tansey", "Wesley", ""], ["Koyejo", "Oluwasanmi", ""], ["Poldrack", "Russell A.", ""], ["Scott", "James G.", ""]]}, {"id": "1411.6150", "submitter": "Heejung Shim Heejung Shim", "authors": "Heejung Shim and Bret Larget", "title": "BayesCAT: Bayesian Co-estimation of Alignment and Tree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, phylogeny and sequence alignment are estimated separately:\nfirst estimate a multiple sequence alignment and then infer a phylogeny based\non the sequence alignment estimated in the previous step. However, uncertainty\nin the alignment estimation is ignored, resulting, possibly, in overstated\ncertainty in phylogeny estimates. We develop a joint model for co-estimating\nphylogeny and sequence alignment which improves estimates from the traditional\napproach by accounting for uncertainty in the alignment in phylogenetic\ninferences. Our insertion and deletion (indel) model allows arbitrary-length\noverlapping indel events and a general distribution for indel fragment size. We\nemploy a Bayesian approach using MCMC to estimate the joint posterior\ndistribution of a phylogenetic tree and a multiple sequence alignment. Our\napproach has a tree and a complete history of indel events mapped onto the tree\nas the state space of the Markov Chain while alternative previous approaches\nhave a tree and an alignment. A large state space containing a complete history\nof indel events makes our MCMC approach more challenging, but it enables us to\ninfer more information about the indel process. The performances of this joint\nmethod and traditional sequential methods are compared using simulated data as\nwell as real data. Software named BayesCAT (Bayesian Co-estimation of Alignment\nand Tree) is available at https://github.com/heejungshim/BayesCAT.\n", "versions": [{"version": "v1", "created": "Sat, 22 Nov 2014 17:43:03 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Shim", "Heejung", ""], ["Larget", "Bret", ""]]}, {"id": "1411.6179", "submitter": "Adrian Dobra", "authors": "Adrian Dobra, Nathalie E. Williams and Nathan Eagle", "title": "Spatiotemporal Detection of Unusual Human Population Behavior Using\n  Mobile Phone Data", "comments": "40 pages, 32 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0120449", "report-no": null, "categories": "physics.soc-ph cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the aim to contribute to humanitarian response to disasters and violent\nevents, scientists have proposed the development of analytical tools that could\nidentify emergency events in real-time, using mobile phone data. The assumption\nis that dramatic and discrete changes in behavior, measured with mobile phone\ndata, will indicate extreme events. In this study, we propose an efficient\nsystem for spatiotemporal detection of behavioral anomalies from mobile phone\ndata and compare sites with behavioral anomalies to an extensive database of\nemergency and non-emergency events in Rwanda. Our methodology successfully\ncaptures anomalous behavioral patterns associated with a broad range of events,\nfrom religious and official holidays to earthquakes, floods, violence against\ncivilians and protests. Our results suggest that human behavioral responses to\nextreme events are complex and multi-dimensional, including extreme increases\nand decreases in both calling and movement behaviors. We also find significant\ntemporal and spatial variance in responses to extreme events. Our behavioral\nanomaly detection system and extensive discussion of results are a significant\ncontribution to the long-term project of creating an effective real-time event\ndetection system with mobile phone data and we discuss the implications of our\nfindings for future research to this end.\n  KEYWORDS: Big data, call detail record, emergency events, human mobility\n", "versions": [{"version": "v1", "created": "Sat, 22 Nov 2014 23:42:04 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Dobra", "Adrian", ""], ["Williams", "Nathalie E.", ""], ["Eagle", "Nathan", ""]]}, {"id": "1411.6345", "submitter": "Beilin Jia", "authors": "Beilin Jia, Wenli Shi, Feng Zhang", "title": "A Gene Prediction Method Based on Statistics and Signal Processing", "comments": "arXiv admin note: text overlap with arXiv:1304.6615 by other author", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bioinformatics, as an emerging and rapidly developing interdisciplinary, has\nbecome a promising and popular research field in 21st century. Extracting and\nexplaining useful biological information from huge amount of genetic data is an\nurgent issue in post-genome era. In eukaryotic DNA sequences, gene consists of\nexons and introns. To predict the location of exons which carry most genetic\ninformation accurately has become one of the most essential issues in\nbioinformatics. Here, we have used biological characteristics of introns to\nfind the candidate initial and final exon sections. Then we select candidate\nexon sections by using Support Vector Machine (SVM). Next, we predict exon\nsections accurately based on Discrete Fourier Transform (DFT) and using\nthree-base periodicity of DNA sequence signals. This paper provides a gene\nprediction method based on statistics and signal processing and also, the\nimprovement and prospect for this method in the future are discussed.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 04:04:20 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Jia", "Beilin", ""], ["Shi", "Wenli", ""], ["Zhang", "Feng", ""]]}, {"id": "1411.6370", "submitter": "Jun Zhu", "authors": "Jun Zhu, Jianfei Chen, Wenbo Hu, Bo Zhang", "title": "Big Learning with Bayesian Methods", "comments": "21 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explosive growth in data and availability of cheap computing resources have\nsparked increasing interest in Big learning, an emerging subfield that studies\nscalable machine learning algorithms, systems, and applications with Big Data.\nBayesian methods represent one important class of statistic methods for machine\nlearning, with substantial recent developments on adaptive, flexible and\nscalable Bayesian learning. This article provides a survey of the recent\nadvances in Big learning with Bayesian methods, termed Big Bayesian Learning,\nincluding nonparametric Bayesian methods for adaptively inferring model\ncomplexity, regularized Bayesian inference for improving the flexibility via\nposterior regularization, and scalable algorithms and systems based on\nstochastic subsampling and distributed computing for dealing with large-scale\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 07:28:51 GMT"}, {"version": "v2", "created": "Wed, 1 Mar 2017 14:07:26 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Zhu", "Jun", ""], ["Chen", "Jianfei", ""], ["Hu", "Wenbo", ""], ["Zhang", "Bo", ""]]}, {"id": "1411.6506", "submitter": "Daniele Durante", "authors": "Daniele Durante and David B. Dunson", "title": "Bayesian Inference and Testing of Group Differences in Brain Networks", "comments": null, "journal-ref": "Bayesian Analysis (2018). 13, 29-58", "doi": "10.1214/16-BA1030", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network data are increasingly collected along with other variables of\ninterest. Our motivation is drawn from neurophysiology studies measuring brain\nconnectivity networks for a sample of individuals along with their membership\nto a low or high creative reasoning group. It is of paramount importance to\ndevelop statistical methods for testing of global and local changes in the\nstructural interconnections among brain regions across groups. We develop a\ngeneral Bayesian procedure for inference and testing of group differences in\nthe network structure, which relies on a nonparametric representation for the\nconditional probability mass function associated with a network-valued random\nvariable. By leveraging a mixture of low-rank factorizations, we allow simple\nglobal and local hypothesis testing adjusting for multiplicity. An efficient\nGibbs sampler is defined for posterior computation. We provide theoretical\nresults on the flexibility of the model and assess testing performance in\nsimulations. The approach is applied to provide novel insights on the\nrelationships between human brain networks and creativity.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 16:10:13 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2015 12:54:19 GMT"}, {"version": "v3", "created": "Thu, 30 Jul 2015 17:59:24 GMT"}, {"version": "v4", "created": "Sat, 30 Jan 2016 17:21:14 GMT"}, {"version": "v5", "created": "Wed, 17 Aug 2016 11:12:02 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Durante", "Daniele", ""], ["Dunson", "David B.", ""]]}, {"id": "1411.6529", "submitter": "Tiancheng Li", "authors": "Tiancheng Li", "title": "The Optimal Arbitrary-Proportional Finite-Set-Partitioning", "comments": null, "journal-ref": "Frontiers of Information Technology & Electronic Engineering,\n  Volume 16, Issue 11, pp 969-984 (2015)", "doi": "10.1631/FITEE.1500199", "report-no": null, "categories": "cs.NA stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the arbitrary-proportional finite-set-partitioning\nproblem which involves partitioning a finite set into multiple subsets with\nrespect to arbitrary nonnegative proportions. This is the core art of many\nfundamental problems such as determining quotas for different individuals of\ndifferent weights or sampling from a discrete-valued weighted sample set to get\na new identically distributed but non-weighted sample set (e.g. the resampling\nneeded in the particle filter). The challenge raises as the size of each subset\nmust be an integer while its unbiased expectation is often not. To solve this\nproblem, a metric (cost function) is defined on their discrepancies and\ncorrespondingly a solution is proposed to determine the sizes of each subsets,\ngaining the minimal bias. Theoretical proof and simulation demonstrations are\nprovided to demonstrate the optimality of the scheme in the sense of the\nproposed metric.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 16:56:28 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Li", "Tiancheng", ""]]}, {"id": "1411.6652", "submitter": "Paul Bendich", "authors": "Paul Bendich, J.S. Marron, Ezra Miller, Alex Pieloch, and Sean Skwerer", "title": "Persistent homology analysis of brain artery trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New representations of tree-structured data objects, using ideas from\ntopological data analysis, enable improved statistical analyses of a population\nof brain artery trees. A number of representations of each data tree arise from\npersistence diagrams that quantify branching and looping of vessels at multiple\nscales. Novel approaches to the statistical analysis, through various summaries\nof the persistence diagrams, lead to heightened correlations with covariates\nsuch as age and sex, relative to earlier analyses of this data set. The\ncorrelation with age continues to be significant even after controlling for\ncorrelations from earlier significant summaries\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 21:27:54 GMT"}], "update_date": "2014-11-26", "authors_parsed": [["Bendich", "Paul", ""], ["Marron", "J. S.", ""], ["Miller", "Ezra", ""], ["Pieloch", "Alex", ""], ["Skwerer", "Sean", ""]]}, {"id": "1411.6683", "submitter": "Yang Liu", "authors": "Yang Liu, Brian C. Battaile, James V. Zidek and Andrew W. Trites", "title": "Bayesian Melding of the Dead-Reckoned Path and GPS Measurements for an\n  Accurate and High-Resolution Path of Marine Mammals", "comments": "26 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent advances in electrical engineering, devices attached to\nfree-ranging marine mammals today can collect oceanographic data in remarkable\nhigh spatial-temporal resolution. However, those data cannot be fully utilized\nwithout a matching high-resolution and accurate path of the animal, which is\ncurrently missing in this field. In this paper, we develop a Bayesian melding\napproach based on a Brownian Bridge process to combine the fine-resolution but\nseriously biased Dead-Reckoned path and the precise but sparse GPS\nmeasurements, which results in an accurate and high-resolution estimated path\ntogether with credible bands as quantified uncertainty statements. We also\nexploit the properties of underlying processes and some approximations to the\nlikelihood to dramatically reduce the computational burden of handling those\nbig high resolution data sets.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 23:10:21 GMT"}, {"version": "v2", "created": "Sun, 28 Dec 2014 19:14:48 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Liu", "Yang", ""], ["Battaile", "Brian C.", ""], ["Zidek", "James V.", ""], ["Trites", "Andrew W.", ""]]}, {"id": "1411.6719", "submitter": "Dionysios Kalogerias", "authors": "Dionysios S. Kalogerias, Athina P. Petropulu", "title": "Asymptotically Optimal Discrete Time Nonlinear Filters From\n  Stochastically Convergent State Process Approximations", "comments": "EXTENDED version of an original paper published in the IEEE\n  Transactions on Signal Processing; 37 pages", "journal-ref": null, "doi": "10.1109/TSP.2015.2428220", "report-no": null, "categories": "math.ST cs.SY math.OC stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of approximating optimal in the Minimum Mean Squared\nError (MMSE) sense nonlinear filters in a discrete time setting, exploiting\nproperties of stochastically convergent state process approximations. More\nspecifically, we consider a class of nonlinear, partially observable stochastic\nsystems, comprised by a (possibly nonstationary) hidden stochastic process (the\nstate), observed through another conditionally Gaussian stochastic process (the\nobservations). Under general assumptions, we show that, given an approximating\nprocess which, for each time step, is stochastically convergent to the state\nprocess, an approximate filtering operator can be defined, which converges to\nthe true optimal nonlinear filter of the state in a strong and well defined\nsense. In particular, the convergence is compact in time and uniform in a\ncompletely characterized measurable set of probability measure almost unity,\nalso providing a purely quantitative justification of Egoroff's Theorem for the\nproblem at hand. The results presented in this paper can form a common basis\nfor the analysis and characterization of a number of heuristic approaches for\napproximating optimal nonlinear filters, such as approximate grid based\ntechniques, known to perform well in a variety of applications.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 03:49:25 GMT"}, {"version": "v2", "created": "Sun, 25 Jan 2015 11:26:47 GMT"}, {"version": "v3", "created": "Sun, 1 May 2016 18:30:16 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Kalogerias", "Dionysios S.", ""], ["Petropulu", "Athina P.", ""]]}, {"id": "1411.6970", "submitter": "Stephan Saalfeld", "authors": "Philipp Hanslovsky, John A. Bogovic, Stephan Saalfeld (HHMI Janelia\n  Research Campus)", "title": "Post-acquisition image based compensation for thickness variation in\n  microscopy section series", "comments": null, "journal-ref": "IEEE International Symposium on Biomedical Imaging, 2015, pages\n  507--511", "doi": "10.1109/ISBI.2015.7163922", "report-no": null, "categories": "cs.CV q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Serial section Microscopy is an established method for volumetric anatomy\nreconstruction. Section series imaged with Electron Microscopy are currently\nvital for the reconstruction of the synaptic connectivity of entire animal\nbrains such as that of Drosophila melanogaster. The process of removing\nultrathin layers from a solid block containing the specimen, however, is a\nfragile procedure and has limited precision with respect to section thickness.\nWe have developed a method to estimate the relative z-position of each\nindividual section as a function of signal change across the section series.\nFirst experiments show promising results on both serial section Transmission\nElectron Microscopy (ssTEM) data and Focused Ion Beam Scanning Electron\nMicroscopy (FIB-SEM) series. We made our solution available as Open Source\nplugins for the TrakEM2 software and the ImageJ distribution Fiji.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 19:01:12 GMT"}, {"version": "v2", "created": "Wed, 27 May 2015 19:39:10 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Hanslovsky", "Philipp", "", "HHMI Janelia\n  Research Campus"], ["Bogovic", "John A.", "", "HHMI Janelia\n  Research Campus"], ["Saalfeld", "Stephan", "", "HHMI Janelia\n  Research Campus"]]}, {"id": "1411.6980", "submitter": "Andr\\'e Beauducel", "authors": "Andre Beauducel and Norbert Hilger", "title": "Extending the debate between Spearman and Wilson 1929: When do single\n  variables optimally reproduce the common part of the observed covariances?", "comments": null, "journal-ref": "Multivariate Behavioral Research, 50, 555-567, 2015", "doi": "10.1080/00273171.2015.1059311", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Because the covariances of observed variables reproduced from conventional\nfactor score predictors are generally not the same as the covariances\nreproduced from the common factors, it is proposed to find a factor score\npredictor that optimally reproduces the common part of the observed\ncovariances. It is shown that, under some conditions, the single observed\nvariable with highest loading on a factor perfectly reproduces the non-diagonal\nobserved covariances. This refers to Spearman's and Wilson's 1929 debate on the\nuse of single variables as factor score predictors. The implications of this\nfinding were investigated in a population based and in a sample based\nsimulation study confirming that taking a single variable outperforms\nconventional factor score predictors in reproducing the observed covariances\nwhen the salient loading size and the number of salient loadings per factor are\nsmall. Implications of this finding for factor score predictors are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 19:31:55 GMT"}], "update_date": "2015-12-18", "authors_parsed": [["Beauducel", "Andre", ""], ["Hilger", "Norbert", ""]]}, {"id": "1411.7342", "submitter": "Hyunseung Kang", "authors": "Hyunseung Kang, Benno Kreuels, J\\\"urgen May, and Dylan S. Small", "title": "Full Matching Approach to Instrumental Variables Estimation with\n  Application to the Effect of Malaria on Stunting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most previous studies of the causal relationship between malaria and stunting\nhave been studies where potential confounders are controlled via\nregression-based methods, but these studies may have been biased by unobserved\nconfounders. Instrumental variables (IV) regression offers a way to control for\nunmeasured confounders where, in our case, the sickle cell trait can be used as\nan instrument. However, for the instrument to be valid, it may still be\nimportant to account for measured confounders. The most commonly used\ninstrumental variable regression method, two-stage least squares, relies on\nparametric assumptions on the effects of measured confounders to account for\nthem. Additionally, two-stage least squares lacks transparency with respect to\ncovariate balance and weighing of subjects and does not blind the researcher to\nthe outcome data. To address these drawbacks, we propose an alternative method\nfor IV estimation based on full matching. We evaluate our new procedure on\nsimulated data and real data concerning the causal effect of malaria on\nstunting among children. We estimate that the risk of stunting among children\nwith the sickle cell trait decrease by 0.22 times the average number of malaria\nepisodes prevented by the sickle cell trait, a substantial effect of malaria on\nstunting (p-value: 0.011, 95% CI: 0.044, 1).\n", "versions": [{"version": "v1", "created": "Wed, 26 Nov 2014 19:25:12 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2015 18:25:41 GMT"}, {"version": "v3", "created": "Mon, 31 Aug 2015 17:27:05 GMT"}, {"version": "v4", "created": "Tue, 10 Nov 2015 19:49:59 GMT"}], "update_date": "2015-11-11", "authors_parsed": [["Kang", "Hyunseung", ""], ["Kreuels", "Benno", ""], ["May", "J\u00fcrgen", ""], ["Small", "Dylan S.", ""]]}, {"id": "1411.7571", "submitter": "Durba Bhattacharya ms", "authors": "Durba Bhattacharya and Sourabh Bhattacharya", "title": "A Bayesian Semiparametric Approach to Learning About Gene-Gene\n  Interactions in Case-Control Studies", "comments": "To appear in Journal of Applied Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gene-gene interactions are often regarded as playing significant roles in\ninfluencing variabilities of complex traits. Although much research has been\ndevoted to this area, to date a comprehensive statistical model that addresses\nthe various sources of uncertainties, seem to be lacking. In this paper, we\npropose and develop a novel Bayesian semiparametric approach composed of finite\nmixtures based on Dirichlet processes and a hierarchical matrix-normal\ndistribution that can comprehensively account for the unknown number of\nsub-populations and gene-gene interactions. Then, by formulating novel and\nsuitable Bayesian tests of hypotheses we attempt to single out the roles of the\ngenes, individually, and in interaction with other genes, in case-control\nstudies. We also attempt to identify the significant loci associated with the\ndisease. Our model facilitates a highly efficient parallel computing\nmethodology, combining Gibbs sampling and Transformation based MCMC (TMCMC).\nApplication of our ideas to biologically realistic data sets revealed quite\nencouraging performance. We also applied our ideas to a real, myocardial\ninfarction dataset, and obtained interesting results that partly agree with,\nand also complement, the existing works in this area, to reveal the importance\nof sophisticated and realistic modeling of gene-gene interactions.\n", "versions": [{"version": "v1", "created": "Thu, 27 Nov 2014 12:27:58 GMT"}, {"version": "v2", "created": "Thu, 19 May 2016 12:15:00 GMT"}, {"version": "v3", "created": "Tue, 15 Nov 2016 17:22:12 GMT"}, {"version": "v4", "created": "Tue, 14 Mar 2017 17:14:03 GMT"}, {"version": "v5", "created": "Fri, 21 Jul 2017 09:17:29 GMT"}, {"version": "v6", "created": "Tue, 17 Apr 2018 12:57:03 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Bhattacharya", "Durba", ""], ["Bhattacharya", "Sourabh", ""]]}, {"id": "1411.7782", "submitter": "Anne Sabourin", "authors": "Anne Sabourin and Benjamin Renard", "title": "Combining regional estimation and historical floods: a multivariate\n  semi-parametric peaks-over-threshold model with censored data", "comments": null, "journal-ref": null, "doi": "10.1002/2015WR017320", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of extreme flood quantiles is challenging due to the relative\nscarcity of extreme data compared to typical target return periods. Several\napproaches have been developed over the years to face this challenge, including\nregional estimation and the use of historical flood data. This paper\ninvestigates the combination of both approaches using a multivariate\npeaks-over-threshold model, that allows estimating altogether the intersite\ndependence structure and the marginal distributions at each site. The joint\ndistribution of extremes at several sites is constructed using a\nsemi-parametric Dirichlet Mixture model. The existence of partially missing and\ncensored observations (historical data) is accounted for within a data\naugmentation scheme. This model is applied to a case study involving four\ncatchments in Southern France, for which historical data are available since\n1604. The comparison of marginal estimates from four versions of the model\n(with or without regionalizing the shape parameter; using or ignoring\nhistorical floods) highlights significant differences in terms of return level\nestimates. Moreover, the availability of historical data on several nearby\ncatchments allows investigating the asymptotic dependence properties of extreme\nfloods. Catchments display a a significant amount of asymptotic dependence,\ncalling for adapted multivariate statistical models.\n", "versions": [{"version": "v1", "created": "Fri, 28 Nov 2014 09:00:14 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Sabourin", "Anne", ""], ["Renard", "Benjamin", ""]]}, {"id": "1411.7924", "submitter": "Bjarne {\\O}rum Fruergaard", "authors": "Bjarne {\\O}rum Fruergaard", "title": "Predicting clicks in online display advertising with latent features and\n  side-information", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review a method for click-through rate prediction based on the work of\nMenon et al. [11], which combines collaborative filtering and matrix\nfactorization with a side-information model and fuses the outputs to proper\nprobabilities in [0,1]. In addition we provide details, both for the modeling\nas well as the experimental part, that are not found elsewhere. We rigorously\ntest the performance on several test data sets from consecutive days in a\nclick-through rate prediction setup, in a manner which reflects a real-world\npipeline. Our results confirm that performance can be increased using latent\nfeatures, albeit the differences in the measures are small but significant.\n", "versions": [{"version": "v1", "created": "Fri, 28 Nov 2014 16:06:52 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Fruergaard", "Bjarne \u00d8rum", ""]]}, {"id": "1411.7973", "submitter": "Marcos R Vieira", "authors": "Matthias Kormaksson, Luciano Barbosa, Marcos R. Vieira, Bianca\n  Zadrozny", "title": "Bus Travel Time Predictions Using Additive Models", "comments": "11 pages, this is the technical report supporting the IEEE 2014\n  International Conference on Data Mining (ICDM) submission with the same title", "journal-ref": null, "doi": "10.1109/ICDM.2014.107", "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many factors can affect the predictability of public bus services such as\ntraffic, weather and local events. Other aspects, such as day of week or hour\nof day, may influence bus travel times as well, either directly or in\nconjunction with other variables. However, the exact nature of such\nrelationships between travel times and predictor variables is, in most\nsituations, not known. In this paper we develop a framework that allows for\nflexible modeling of bus travel times through the use of Additive Models. In\nparticular, we model travel times as a sum of linear as well as nonlinear terms\nthat are modeled as smooth functions of predictor variables. The proposed class\nof models provides a principled statistical framework that is highly flexible\nin terms of model building. The experimental results demonstrate uniformly\nsuperior performance of our best model as compared to previous prediction\nmethods when applied to a very large GPS data set obtained from buses operating\nin the city of Rio de Janeiro.\n", "versions": [{"version": "v1", "created": "Fri, 28 Nov 2014 18:45:31 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Kormaksson", "Matthias", ""], ["Barbosa", "Luciano", ""], ["Vieira", "Marcos R.", ""], ["Zadrozny", "Bianca", ""]]}]