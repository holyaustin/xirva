[{"id": "0901.0024", "submitter": "Francesco Bartolucci", "authors": "Francesco Bartolucci and Ivonne L. Solis-Trapala", "title": "Multidimensional latent Markov models in a developmental study of\n  inhibitory control and attentional flexibility in early childhood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate the use of a multidimensional extension of the latent Markov\nmodel to analyse data from studies with correlated binary responses in\ndevelopmental psychology. In particular, we consider an experiment based on a\nbattery of tests which was administered to pre-school children, at three time\nperiods, in order to measure their inhibitory control and attentional\nflexibility abilities. Our model represents these abilities by two latent\ntraits which are associated to each state of a latent Markov chain. The\nconditional distribution of the tests outcomes given the latent process depends\non these abilities through a multidimensional two-parameter logistic\nparameterisation. We outline an EM algorithm to conduct likelihood inference on\nthe model parameters; we also focus on likelihood ratio testing of hypotheses\non the dimensionality of the model and on the transition matrices of the latent\nprocess. Through the approach based on the proposed model, we find evidence\nthat supports that inhibitory control and attentional flexibility can be\nconceptualised as distinct constructs. Furthermore, we outline developmental\naspects of participants' performance on these abilities based on inspection of\nthe estimated transition matrices.\n", "versions": [{"version": "v1", "created": "Tue, 30 Dec 2008 22:22:54 GMT"}], "update_date": "2009-01-05", "authors_parsed": [["Bartolucci", "Francesco", ""], ["Solis-Trapala", "Ivonne L.", ""]]}, {"id": "0901.0135", "submitter": "Eric P. Xing", "authors": "Eric P. Xing, Wenjie Fu, Le Song", "title": "A state-space mixed membership blockmodel for dynamic network tomography", "comments": "Published in at http://dx.doi.org/10.1214/09-AOAS311 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2010, Vol. 4, No. 2, 535-566", "doi": "10.1214/09-AOAS311", "report-no": "IMS-AOAS-AOAS311", "categories": "stat.ML q-bio.MN q-bio.QM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a dynamic social or biological environment, the interactions between the\nactors can undergo large and systematic changes. In this paper we propose a\nmodel-based approach to analyze what we will refer to as the dynamic tomography\nof such time-evolving networks. Our approach offers an intuitive but powerful\ntool to infer the semantic underpinnings of each actor, such as its social\nroles or biological functions, underlying the observed network topologies. Our\nmodel builds on earlier work on a mixed membership stochastic blockmodel for\nstatic networks, and the state-space model for tracking object trajectory. It\novercomes a major limitation of many current network inference techniques,\nwhich assume that each actor plays a unique and invariant role that accounts\nfor all its interactions with other actors; instead, our method models the role\nof each actor as a time-evolving mixed membership vector that allows actors to\nbehave differently over time and carry out different roles/functions when\ninteracting with different peers, which is closer to reality. We present an\nefficient algorithm for approximate inference and learning using our model; and\nwe applied our model to analyze a social network between monks (i.e., the\nSampson's network), a dynamic email communication network between the Enron\nemployees, and a rewiring gene interaction network of fruit fly collected\nduring its full life cycle. In all cases, our model reveals interesting\npatterns of the dynamic roles of the actors.\n", "versions": [{"version": "v1", "created": "Wed, 31 Dec 2008 21:27:33 GMT"}, {"version": "v2", "created": "Mon, 8 Nov 2010 09:05:23 GMT"}], "update_date": "2010-11-09", "authors_parsed": [["Xing", "Eric P.", ""], ["Fu", "Wenjie", ""], ["Song", "Le", ""]]}, {"id": "0901.0182", "submitter": "V\\'eronique Maume-Deschamps", "authors": "H. Cossette, E. Marceau, V. Maume-Deschamps", "title": "Adjustment coefficient for risk processes in some dependent contexts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following an article by Muller and Pflug, we study the adjustment coefficient\nof ruin theory in a context of temporal dependency. We provide a consistent\nestimator of this coefficient, and perform some simulations.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jan 2009 16:13:27 GMT"}], "update_date": "2009-01-05", "authors_parsed": [["Cossette", "H.", ""], ["Marceau", "E.", ""], ["Maume-Deschamps", "V.", ""]]}, {"id": "0901.0489", "submitter": "Pascal Pernot", "authors": "Pascal Pernot (LCPO)", "title": "Scaling factors for ab initio vibrational frequencies: comparison of\n  uncertainty models for quantified prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an physics.chem-ph physics.class-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian Model Calibration is used to revisit the problem of scaling factor\ncalibration for semi-empirical correction of ab initio calculations. A\nparticular attention is devoted to uncertainty evaluation for scaling factors,\nand to their effect on prediction of observables involving scaled properties.\nWe argue that linear models used for calibration of scaling factors are\ngenerally not statistically valid, in the sense that they are not able to fit\ncalibration data within their uncertainty limits. Uncertainty evaluation and\nuncertainty propagation by statistical methods from such invalid models are\ndoomed to failure. To relieve this problem, a stochastic function is included\nin the model to account for model inadequacy, according to the Bayesian Model\nCalibration approach. In this framework, we demonstrate that standard\ncalibration summary statistics, as optimal scaling factor and root mean square,\ncan be safely used for uncertainty propagation only when large calibration sets\nof precise data are used. For small datasets containing a few dozens of data, a\nmore accurate formula is provided which involves scaling factor calibration\nuncertainty. For measurement uncertainties larger than model inadequacy, the\nproblem can be reduced to a weighted least squares analysis. For intermediate\ncases, no analytical estimators were found, and numerical Bayesian estimation\nof parameters has to be used.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jan 2009 14:35:30 GMT"}], "update_date": "2009-01-12", "authors_parsed": [["Pernot", "Pascal", "", "LCPO"]]}, {"id": "0901.0638", "submitter": "William Shaw", "authors": "William T. Shaw, Thomas Luu, Nick Brickman", "title": "Quantile Mechanics II: Changes of Variables in Monte Carlo methods and\n  GPU-Optimized Normal Quantiles", "comments": "This revision adds substantial discussion of precision and\n  optimization issues, new code for float and double precision operation.\n  Timings for GTX 285, 480, Quadro 4000, Tesla C2050, and comparisons with most\n  major competing approaches", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP q-fin.RM q-fin.ST stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents differential equations and solution methods for the\nfunctions of the form $Q(x) = F^{-1}(G(x))$, where $F$ and $G$ are cumulative\ndistribution functions. Such functions allow the direct recycling of Monte\nCarlo samples from one distribution into samples from another. The method may\nbe developed analytically for certain special cases, and illuminate the idea\nthat it is a more precise form of the traditional Cornish-Fisher expansion. In\nthis manner the model risk of distributional risk may be assessed free of the\nMonte Carlo noise associated with resampling. Examples are given of equations\nfor converting normal samples to Student t, and converting exponential to\nhyperbolic, variance gamma and normal. In the case of the normal distribution,\nthe change of variables employed allows the sampling to take place to good\naccuracy based on a single rational approximation over a very wide range of the\nsample space. The avoidance of any branching statement is of use in optimal GPU\ncomputations as it avoids the effect of {\\it warp divergence}, and we give\nexamples of branch-free normal quantiles that offer performance improvements in\na GPU environment, while retaining the best precision characteristics of\nwell-known methods. We also offer models based on a low-probability of warp\ndivergence. Comparisons of new and old forms are made on the Nvidia Quadro\n4000, GTX 285 and 480, and Tesla C2050 GPUs. We argue that in single-precision\nmode, the change-of-variables approach offers performance competitive with the\nfastest existing scheme while substantially improving precision, and that in\ndouble-precision mode, this approach offers the most GPU-optimal Gaussian\nquantile yet, and without compromise on precision for Monte Carlo applications,\nworking twice as fast as the CUDA 4 library function with increased precision.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jan 2009 12:14:25 GMT"}, {"version": "v2", "created": "Thu, 8 Jan 2009 17:51:40 GMT"}, {"version": "v3", "created": "Sun, 15 Feb 2009 10:37:04 GMT"}, {"version": "v4", "created": "Thu, 26 Aug 2010 14:20:22 GMT"}, {"version": "v5", "created": "Wed, 7 Dec 2011 19:16:30 GMT"}], "update_date": "2011-12-08", "authors_parsed": [["Shaw", "William T.", ""], ["Luu", "Thomas", ""], ["Brickman", "Nick", ""]]}, {"id": "0901.0684", "submitter": "Gizachew Tiruneh Dr.", "authors": "Gizachew Tiruneh", "title": "Age and Winning Professional Golf Tournaments", "comments": "Introduction and regression analyses added; 16 pages; 14 tables and\n  figures; typos in the conclusions part edited; conclusions sections edited;\n  typo edited in conclusions", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most professional golfers and analysts think that winning on the PGA Tour\npeaks when golfers are in their thirties. Rather than relying on educated\nguesses, we can actually use available statistical data to determine the actual\nages at which golfers peak their golf game. We can also test the hypothesis\nthat age affects winning professional golf tournaments. Using data available\nfrom the Golf Channel, the PGA Tour, and LPGA Tour, I calculated and provided\nthe mean, the median, and the mode ages at which professional golfers on the\nPGA, European PGA, Champions, and LPGA Tours had won over a five-year period.\nMore specifically, the ages at which golfers on the PGA, European PGA,\nChampions Tour, and LPGA Tours peak their wins are 35, 30, 52, and 25,\nrespectively. The regression analyses I conducted seem to support my hypothesis\nthat age affects winning professional golf tournaments.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jan 2009 18:00:13 GMT"}, {"version": "v2", "created": "Tue, 6 Jan 2009 21:37:45 GMT"}, {"version": "v3", "created": "Wed, 7 Jan 2009 22:12:37 GMT"}, {"version": "v4", "created": "Sun, 26 Jul 2009 20:58:07 GMT"}, {"version": "v5", "created": "Sat, 30 Jan 2010 21:51:38 GMT"}], "update_date": "2010-01-30", "authors_parsed": [["Tiruneh", "Gizachew", ""]]}, {"id": "0901.1038", "submitter": "Ricardo Lopez-Ruiz", "authors": "Carmen Pellicer-Lostao and Ricardo Lopez-Ruiz", "title": "Economic Models with Chaotic Money Exchange", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.AO physics.soc-ph q-fin.TR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel study on gas-like models for economic systems.\nThe interacting agents and the amount of exchanged money at each trade are\nselected with different levels of randomness, from a purely random way to a\nmore chaotic one. Depending on the interaction rules, these statistical models\ncan present different asymptotic distributions of money in a community of\nindividuals with a closed economy.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jan 2009 13:20:02 GMT"}], "update_date": "2009-01-09", "authors_parsed": [["Pellicer-Lostao", "Carmen", ""], ["Lopez-Ruiz", "Ricardo", ""]]}, {"id": "0901.1066", "submitter": "Max Shpak", "authors": "Panagis Moschopoulos and Max Shpak", "title": "Taxon Size Distribution in a Time Homogeneous Birth and Death Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of extant individuals within a lineage, as exemplified by counts\nof species numbers across genera in a higher taxonomic category, is known to be\na highly skewed distribution. Because the sublineages (such as genera in a\nclade) themselves follow a random birth process, deriving the distribution of\nlineage sizes involves averaging the solutions to a birth and death process\nover the distribution of time intervals separating the origin of the lineages.\nIn this paper, we show that the resulting distributions can be represented by\nhypergeometric functions of the second kind. We also provide approximations of\nthese distributions up to the second order, and compare these results to the\nasymptotic distributions and numerical approximations used in previous studies.\nFor two limiting cases, one with a relatively high rate of lineage origin, one\nwith a low rate, the cumulative probability densities and percentiles are\ncompared to show that the approximations are robust over a wide rane of\nparameters. It is proposed that the probability density distributions of\nlineage size may have a number of relevant applications to biological problems\nsuch as the coalescence of genetic lineages and in predicting the number of\nspecies in living and extinct higher taxa, as these systems are special\ninstances of the underlying process analyzed in this paper.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jan 2009 19:45:16 GMT"}], "update_date": "2009-01-09", "authors_parsed": [["Moschopoulos", "Panagis", ""], ["Shpak", "Max", ""]]}, {"id": "0901.1945", "submitter": "Michel Fliess", "authors": "Michel Fliess (LIX, INRIA Saclay - Ile de France), C\\'edric Join\n  (INRIA Saclay - Ile de France, CRAN)", "title": "A mathematical proof of the existence of trends in financial time series", "comments": null, "journal-ref": "Systems Theory: Modelling, Analysis and Control (2009) 43-62", "doi": null, "report-no": null, "categories": "q-fin.ST cs.CE math.CA math.PR q-fin.CP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are settling a longstanding quarrel in quantitative finance by proving the\nexistence of trends in financial time series thanks to a theorem due to P.\nCartier and Y. Perrin, which is expressed in the language of nonstandard\nanalysis (Integration over finite sets, F. & M. Diener (Eds): Nonstandard\nAnalysis in Practice, Springer, 1995, pp. 195--204). Those trends, which might\ncoexist with some altered random walk paradigm and efficient market hypothesis,\nseem nevertheless difficult to reconcile with the celebrated Black-Scholes\nmodel. They are estimated via recent techniques stemming from control and\nsignal theory. Several quite convincing computer simulations on the forecast of\nvarious financial quantities are depicted. We conclude by discussing the r\\^ole\nof probability theory.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jan 2009 07:47:18 GMT"}], "update_date": "2009-06-01", "authors_parsed": [["Fliess", "Michel", "", "LIX, INRIA Saclay - Ile de France"], ["Join", "C\u00e9dric", "", "INRIA Saclay - Ile de France, CRAN"]]}, {"id": "0901.2234", "submitter": "Nicole Kraemer", "authors": "Stefan Haufe, Guido Nolte, Klaus-Robert Mueller, Nicole Kraemer", "title": "Sparse Causal Discovery in Multivariate Time Series", "comments": "to appear in Journal of Machine Learning Research, Proceedings of the\n  NIPS'08 workshop on Causality", "journal-ref": "JMLR Workshop and Conference Proceedings 6: Causality: Objectives\n  and Assessment (NIPS 2008), 97 - 106", "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to estimate causal interactions in multivariate time series.\nUsing vector autoregressive (VAR) models, these can be defined based on\nnon-vanishing coefficients belonging to respective time-lagged instances. As in\nmost cases a parsimonious causality structure is assumed, a promising approach\nto causal discovery consists in fitting VAR models with an additional\nsparsity-promoting regularization. Along this line we here propose that\nsparsity should be enforced for the subgroups of coefficients that belong to\neach pair of time series, as the absence of a causal relation requires the\ncoefficients for all time-lags to become jointly zero. Such behavior can be\nachieved by means of l1-l2-norm regularized regression, for which an efficient\nactive set solver has been proposed recently. Our method is shown to outperform\nstandard methods in recovering simulated causality graphs. The results are on\npar with a second novel approach which uses multiple statistical testing.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jan 2009 11:21:33 GMT"}], "update_date": "2010-08-13", "authors_parsed": [["Haufe", "Stefan", ""], ["Nolte", "Guido", ""], ["Mueller", "Klaus-Robert", ""], ["Kraemer", "Nicole", ""]]}, {"id": "0901.2675", "submitter": "Trent McCotter", "authors": "Trent McCotter", "title": "Hitting Streaks Don't Obey Your Rules: Evidence That Hitting Streaks\n  Aren't Just By-Products of Random Variations", "comments": "18 pages, 4 figures; UPDATED with full charts for the permutations\n  involving only starts, and the one involving ALL games", "journal-ref": "The Baseball Research Journal #37, Society for American Baseball\n  Research, Cleveland, p62-70 (2008)", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There have been more hitting streaks in Major League Baseball than we would\nexpect. All batting lines of MLB hitters from 1957-2006 were randomly permuted\n10,000 times and the number of hitting streaks of each length from 2 to 100 was\nmeasured. The average count of each length streak was then compared to the\ncorresponding total from real-life, when the games were in chronological order.\nThe number of streaks in real-life was significantly higher than over the\nrandom permutations. Non-starts (such as pinch-hitting appearances) were\nremoved since these may be unduly reducing the number of streaks in the\npermutations; the number of streaks in the permutations increased but was still\nsignificantly lower than real-life totals. Possible explanations are given for\nwhy more streaks have appeared in real-life than we would expect, including\npossibly the hot hand idea. Contact at trentm@email.unc.edu\n", "versions": [{"version": "v1", "created": "Sun, 18 Jan 2009 02:29:41 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2009 16:27:33 GMT"}, {"version": "v3", "created": "Mon, 10 Aug 2009 19:43:42 GMT"}], "update_date": "2009-08-10", "authors_parsed": [["McCotter", "Trent", ""]]}, {"id": "0901.2880", "submitter": "John Panaretos", "authors": "S. Bersimis, J. Panaretos, and S. Psarakis", "title": "Multivariate Statistical Process Control Charts and the Problem of\n  Interpretation: A Short Overview and Some Applications in Industry", "comments": null, "journal-ref": "Proceedings of the 7th Hellenic European Conference on Computer\n  Mathematics and its Applications, Athens, Greece, 2005", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Woodall and Montgomery [35] in a discussion paper, state that multivariate\nprocess control is one of the most rapidly developing sections of statistical\nprocess control. Nowadays, in industry, there are many situations in which the\nsimultaneous monitoring or control, of two or more related quality - process\ncharacteristics is necessary. Process monitoring problems in which several\nrelated variables are of interest are collectively known as Multivariate\nStatistical Process Control (MSPC).This article has three parts. In the first\npart, we discuss in brief the basic procedures for the implementation of\nmultivariate statistical process control via control charting. In the second\npart we present the most useful procedures for interpreting the out-of-control\nvariable when a control charting procedure gives an out-of-control signal in a\nmultivariate process. Finally, in the third part, we present applications of\nmultivariate statistical process control in the area of industrial process\ncontrol, informatics, and business.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jan 2009 15:20:54 GMT"}], "update_date": "2009-01-20", "authors_parsed": [["Bersimis", "S.", ""], ["Panaretos", "J.", ""], ["Psarakis", "S.", ""]]}, {"id": "0901.3469", "submitter": "Michael L. Stein", "authors": "Montserrat Fuentes, Peter Guttorp, Michael L. Stein", "title": "Special section on statistics in the atmospheric sciences", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS209 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2008, Vol. 2, No. 4, 1143-1147", "doi": "10.1214/08-AOAS209", "report-no": "IMS-AOAS-AOAS209", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the possible exception of gambling, meteorology, particularly\nprecipitation forecasting, may be the area with which the general public is\nmost familiar with probabilistic assessments of uncertainty. Despite the heavy\nuse of stochastic models and statistical methods in weather forecasting and\nother areas of the atmospheric sciences, papers in these areas have\ntraditionally been somewhat uncommon in statistics journals. We see signs of\nthis changing in recent years and we have sought to highlight some present\nresearch directions at the interface of statistics and the atmospheric sciences\nin this special section.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2009 12:39:49 GMT"}], "update_date": "2009-01-23", "authors_parsed": [["Fuentes", "Montserrat", ""], ["Guttorp", "Peter", ""], ["Stein", "Michael L.", ""]]}, {"id": "0901.3478", "submitter": "Montserrat Fuentes", "authors": "Montserrat Fuentes, Brian Reich, Gyuwon Lee", "title": "Spatial--temporal mesoscale modeling of rainfall intensity using gage\n  and radar data", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS166 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2008, Vol. 2, No. 4, 1148-1169", "doi": "10.1214/08-AOAS166", "report-no": "IMS-AOAS-AOAS166", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gridded estimated rainfall intensity values at very high spatial and temporal\nresolution levels are needed as main inputs for weather prediction models to\nobtain accurate precipitation forecasts, and to verify the performance of\nprecipitation forecast models. These gridded rainfall fields are also the main\ndriver for hydrological models that forecast flash floods, and they are\nessential for disaster prediction associated with heavy rain. Rainfall\ninformation can be obtained from rain gages that provide relatively accurate\nestimates of the actual rainfall values at point-referenced locations, but they\ndo not characterize well enough the spatial and temporal structure of the\nrainfall fields. Doppler radar data offer better spatial and temporal coverage,\nbut Doppler radar measures effective radar reflectivity ($Ze$) rather than\nrainfall rate ($R$). Thus, rainfall estimates from radar data suffer from\nvarious uncertainties due to their measuring principle and the conversion from\n$Ze$ to $R$. We introduce a framework to combine radar reflectivity and gage\ndata, by writing the different sources of rainfall information in terms of an\nunderlying unobservable spatial temporal process with the true rainfall values.\nWe use spatial logistic regression to model the probability of rain for both\nsources of data in terms of the latent true rainfall process. We characterize\nthe different sources of bias and error in the gage and radar data and we\nestimate the true rainfall intensity with its posterior predictive\ndistribution, conditioning on the observed data.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2009 13:22:27 GMT"}], "update_date": "2009-01-23", "authors_parsed": [["Fuentes", "Montserrat", ""], ["Reich", "Brian", ""], ["Lee", "Gyuwon", ""]]}, {"id": "0901.3484", "submitter": "Tilmann Gneiting", "authors": "Veronica J. Berrocal, Adrian E. Raftery, Tilmann Gneiting", "title": "Probabilistic quantitative precipitation field forecasting using a\n  two-stage spatial model", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS203 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2008, Vol. 2, No. 4, 1170-1193", "doi": "10.1214/08-AOAS203", "report-no": "IMS-AOAS-AOAS203", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Short-range forecasts of precipitation fields are needed in a wealth of\nagricultural, hydrological, ecological and other applications. Forecasts from\nnumerical weather prediction models are often biased and do not provide\nuncertainty information. Here we present a postprocessing technique for such\nnumerical forecasts that produces correlated probabilistic forecasts of\nprecipitation accumulation at multiple sites simultaneously. The statistical\nmodel is a spatial version of a two-stage model that represents the\ndistribution of precipitation by a mixture of a point mass at zero and a Gamma\ndensity for the continuous distribution of precipitation accumulation. Spatial\ncorrelation is captured by assuming that two Gaussian processes drive\nprecipitation occurrence and precipitation amount, respectively. The first\nprocess is latent and drives precipitation occurrence via a threshold. The\nsecond process explains the spatial correlation in precipitation accumulation.\nIt is related to precipitation via a site-specific transformation function, so\nas to retain the marginal right-skewed distribution of precipitation while\nmodeling spatial dependence. Both processes take into account the information\ncontained in the numerical weather forecast and are modeled as stationary\nisotropic spatial processes with an exponential correlation function. The\ntwo-stage spatial model was applied to 48-hour-ahead forecasts of daily\nprecipitation accumulation over the Pacific Northwest in 2004. The predictive\ndistributions from the two-stage spatial model were calibrated and sharp, and\noutperformed reference forecasts for spatially composite and areally averaged\nquantities.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2009 14:36:16 GMT"}], "update_date": "2009-01-23", "authors_parsed": [["Berrocal", "Veronica J.", ""], ["Raftery", "Adrian E.", ""], ["Gneiting", "Tilmann", ""]]}, {"id": "0901.3494", "submitter": "Huiyan Sang", "authors": "Huiyan Sang, Alan E. Gelfand, Chris Lennard, Gabriele Hegerl, Bruce\n  Hewitson", "title": "Interpreting self-organizing maps through space--time data models", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS174 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2008, Vol. 2, No. 4, 1194-1216", "doi": "10.1214/08-AOAS174", "report-no": "IMS-AOAS-AOAS174", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-organizing maps (SOMs) are a technique that has been used with\nhigh-dimensional data vectors to develop an archetypal set of states (nodes)\nthat span, in some sense, the high-dimensional space. Noteworthy applications\ninclude weather states as described by weather variables over a region and\nspeech patterns as characterized by frequencies in time. The SOM approach is\nessentially a neural network model that implements a nonlinear projection from\na high-dimensional input space to a low-dimensional array of neurons. In the\nprocess, it also becomes a clustering technique, assigning to any vector in the\nhigh-dimensional data space the node (neuron) to which it is closest (using,\nsay, Euclidean distance) in the data space. The number of nodes is thus equal\nto the number of clusters. However, the primary use for the SOM is as a\nrepresentation technique, that is, finding a set of nodes which\nrepresentatively span the high-dimensional space. These nodes are typically\ndisplayed using maps to enable visualization of the continuum of the data\nspace. The technique does not appear to have been discussed in the statistics\nliterature so it is our intent here to bring it to the attention of the\ncommunity. The technique is implemented algorithmically through a training set\nof vectors. However, through the introduction of stochasticity in the form of a\nspace--time process model, we seek to illuminate and interpret its performance\nin the context of application to daily data collection. That is, the observed\ndaily state vectors are viewed as a time series of multivariate process\nrealizations which we try to understand under the dimension reduction achieved\nby the SOM procedure.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2009 15:12:59 GMT"}], "update_date": "2009-01-23", "authors_parsed": [["Sang", "Huiyan", ""], ["Gelfand", "Alan E.", ""], ["Lennard", "Chris", ""], ["Hegerl", "Gabriele", ""], ["Hewitson", "Bruce", ""]]}, {"id": "0901.3531", "submitter": "Matthias Kohl", "authors": "Matthias Kohl, Peter Ruckdeschel, Helmut Rieder", "title": "Infinitesimally Robust Estimation in General Smoothly Parametrized\n  Models", "comments": null, "journal-ref": "Statistical Methods and Application 2010", "doi": "10.1007/s10260-010-0133-0", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the shrinking neighborhood approach of Robust Statistics, which\napplies to general smoothly parametrized models, especially, exponential\nfamilies. Equal generality is achieved by object oriented implementation of the\noptimally robust estimators. We evaluate the estimates on real datasets from\nliterature by means of our R packages ROptEst and RobLox.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2009 18:11:02 GMT"}], "update_date": "2010-08-04", "authors_parsed": [["Kohl", "Matthias", ""], ["Ruckdeschel", "Peter", ""], ["Rieder", "Helmut", ""]]}, {"id": "0901.3665", "submitter": "Dorin Drignei", "authors": "Dorin Drignei, Chris E. Forest, Doug Nychka", "title": "Parameter estimation for computationally intensive nonlinear regression\n  with an application to climate modeling", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS210 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2008, Vol. 2, No. 4, 1217-1230", "doi": "10.1214/08-AOAS210", "report-no": "IMS-AOAS-AOAS210", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear regression is a useful statistical tool, relating observed data and\na nonlinear function of unknown parameters. When the parameter-dependent\nnonlinear function is computationally intensive, a straightforward regression\nanalysis by maximum likelihood is not feasible. The method presented in this\npaper proposes to construct a faster running surrogate for such a\ncomputationally intensive nonlinear function, and to use it in a related\nnonlinear statistical model that accounts for the uncertainty associated with\nthis surrogate. A pivotal quantity in the Earth's climate system is the climate\nsensitivity: the change in global temperature due to doubling of atmospheric\n$\\mathrm{CO}_2$ concentrations. This, along with other climate parameters, are\nestimated by applying the statistical method developed in this paper, where the\ncomputationally intensive nonlinear function is the MIT 2D climate model.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jan 2009 12:29:51 GMT"}], "update_date": "2009-01-26", "authors_parsed": [["Drignei", "Dorin", ""], ["Forest", "Chris E.", ""], ["Nychka", "Doug", ""]]}, {"id": "0901.3670", "submitter": "Anders Malmberg", "authors": "Anders Malmberg, Avelino Arellano, David P. Edwards, Natasha Flyer,\n  Doug Nychka, Christopher Wikle", "title": "Interpolating fields of carbon monoxide data using a hybrid\n  statistical-physical model", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS168 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2008, Vol. 2, No. 4, 1231-1248", "doi": "10.1214/08-AOAS168", "report-no": "IMS-AOAS-AOAS168", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Atmospheric Carbon Monoxide (CO) provides a window on the chemistry of the\natmosphere since it is one of few chemical constituents that can be remotely\nsensed, and it can be used to determine budgets of other greenhouse gases such\nas ozone and OH radicals. Remote sensing platforms in geostationary Earth orbit\nwill soon provide regional observations of CO at several vertical layers with\nhigh spatial and temporal resolution. However, cloudy locations cannot be\nobserved and estimates of the complete CO concentration fields have to be\nestimated based on the cloud-free observations. The current state-of-the-art\nsolution of this interpolation problem is to combine cloud-free observations\nwith prior information, computed by a deterministic physical model, which might\nintroduce uncertainties that do not derive from data. While sharing features\nwith the physical model, this paper suggests a Bayesian hierarchical model to\nestimate the complete CO concentration fields. The paper also provides a direct\ncomparison to state-of-the-art methods. To our knowledge, such a model and\ncomparison have not been considered before.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jan 2009 12:54:20 GMT"}], "update_date": "2009-01-26", "authors_parsed": [["Malmberg", "Anders", ""], ["Arellano", "Avelino", ""], ["Edwards", "David P.", ""], ["Flyer", "Natasha", ""], ["Nychka", "Doug", ""], ["Wikle", "Christopher", ""]]}, {"id": "0901.3806", "submitter": "Yangxin Huang", "authors": "Yangxin Huang, Tao Lu", "title": "Modeling long-term longitudinal HIV dynamics with application to an AIDS\n  clinical study", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS192 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2008, Vol. 2, No. 4, 1384-1408", "doi": "10.1214/08-AOAS192", "report-no": "IMS-AOAS-AOAS192", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A virologic marker, the number of HIV RNA copies or viral load, is currently\nused to evaluate antiretroviral (ARV) therapies in AIDS clinical trials. This\nmarker can be used to assess the ARV potency of therapies, but is easily\naffected by drug exposures, drug resistance and other factors during the\nlong-term treatment evaluation process. HIV dynamic studies have significantly\ncontributed to the understanding of HIV pathogenesis and ARV treatment\nstrategies. However, the models of these studies are used to quantify\nshort-term HIV dynamics ($<$ 1 month), and are not applicable to describe\nlong-term virological response to ARV treatment due to the difficulty of\nestablishing a relationship of antiviral response with multiple treatment\nfactors such as drug exposure and drug susceptibility during long-term\ntreatment. Long-term therapy with ARV agents in HIV-infected patients often\nresults in failure to suppress the viral load. Pharmacokinetics (PK), drug\nresistance and imperfect adherence to prescribed antiviral drugs are important\nfactors explaining the resurgence of virus. To better understand the factors\nresponsible for the virological failure, this paper develops the\nmechanism-based nonlinear differential equation models for characterizing\nlong-term viral dynamics with ARV therapy. The models directly incorporate drug\nconcentration, adherence and drug susceptibility into a function of treatment\nefficacy and, hence, fully integrate virologic, PK, drug adherence and\nresistance from an AIDS clinical trial into the analysis. A Bayesian nonlinear\nmixed-effects modeling approach in conjunction with the rescaled version of\ndynamic differential equations is investigated to estimate dynamic parameters\nand make inference. In addition, the correlations of baseline factors with\nestimated dynamic parameters are explored and some biologically meaningful\ncorrelation results are presented. Further, the estimated dynamic parameters in\npatients with virologic success were compared to those in patients with\nvirologic failure and significantly important findings were summarized. These\nresults suggest that viral dynamic parameters may play an important role in\nunderstanding HIV pathogenesis, designing new treatment strategies for\nlong-term care of AIDS patients.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2009 14:57:23 GMT"}], "update_date": "2009-01-27", "authors_parsed": [["Huang", "Yangxin", ""], ["Lu", "Tao", ""]]}, {"id": "0901.3877", "submitter": "Li Qin", "authors": "Li Qin, Yuedong Wang", "title": "Nonparametric spectral analysis with applications to seizure\n  characterization using EEG time series", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS185 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2008, Vol. 2, No. 4, 1432-1451", "doi": "10.1214/08-AOAS185", "report-no": "IMS-AOAS-AOAS185", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the seizure initiation process and its propagation pattern(s)\nis a critical task in epilepsy research. Characteristics of the pre-seizure\nelectroencephalograms (EEGs) such as oscillating powers and high-frequency\nactivities are believed to be indicative of the seizure onset and spread\npatterns. In this article, we analyze epileptic EEG time series using\nnonparametric spectral estimation methods to extract information on\nseizure-specific power and characteristic frequency [or frequency band(s)].\nBecause the EEGs may become nonstationary before seizure events, we develop\nmethods for both stationary and local stationary processes. Based on penalized\nWhittle likelihood, we propose a direct generalized maximum likelihood (GML)\nand generalized approximate cross-validation (GACV) methods to estimate\nsmoothing parameters in both smoothing spline spectrum estimation of a\nstationary process and smoothing spline ANOVA time-varying spectrum estimation\nof a locally stationary process. We also propose permutation methods to test if\na locally stationary process is stationary. Extensive simulations indicate that\nthe proposed direct methods, especially the direct GML, are stable and perform\nbetter than other existing methods. We apply the proposed methods to the\nintracranial electroencephalograms (IEEGs) of an epileptic patient to gain\ninsights into the seizure generation process.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2009 16:04:39 GMT"}], "update_date": "2009-01-27", "authors_parsed": [["Qin", "Li", ""], ["Wang", "Yuedong", ""]]}, {"id": "0901.3980", "submitter": "Mikyoung Jun", "authors": "Mikyoung Jun, Michael L. Stein", "title": "Nonstationary covariance models for global data", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS183 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2008, Vol. 2, No. 4, 1271-1289", "doi": "10.1214/08-AOAS183", "report-no": "IMS-AOAS-AOAS183", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the widespread availability of satellite-based instruments, many\ngeophysical processes are measured on a global scale and they often show strong\nnonstationarity in the covariance structure. In this paper we present a\nflexible class of parametric covariance models that can capture the\nnonstationarity in global data, especially strong dependency of covariance\nstructure on latitudes. We apply the Discrete Fourier Transform to data on\nregular grids, which enables us to calculate the exact likelihood for large\ndata sets. Our covariance model is applied to global total column ozone level\ndata on a given day. We discuss how our covariance model compares with some\nexisting models.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2009 11:51:50 GMT"}], "update_date": "2009-01-27", "authors_parsed": [["Jun", "Mikyoung", ""], ["Stein", "Michael L.", ""]]}, {"id": "0901.3988", "submitter": "Hui Zou", "authors": "Hui Zou, Ji Zhu, Trevor Hastie", "title": "New multicategory boosting algorithms based on multicategory\n  Fisher-consistent losses", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS198 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2008, Vol. 2, No. 4, 1290-1306", "doi": "10.1214/08-AOAS198", "report-no": "IMS-AOAS-AOAS198", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fisher-consistent loss functions play a fundamental role in the construction\nof successful binary margin-based classifiers. In this paper we establish the\nFisher-consistency condition for multicategory classification problems. Our\napproach uses the margin vector concept which can be regarded as a\nmulticategory generalization of the binary margin. We characterize a wide class\nof smooth convex loss functions that are Fisher-consistent for multicategory\nclassification. We then consider using the margin-vector-based loss functions\nto derive multicategory boosting algorithms. In particular, we derive two new\nmulticategory boosting algorithms by using the exponential and logistic\nregression losses.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2009 12:27:15 GMT"}], "update_date": "2009-01-27", "authors_parsed": [["Zou", "Hui", ""], ["Zhu", "Ji", ""], ["Hastie", "Trevor", ""]]}, {"id": "0901.3999", "submitter": "Qing Zhou", "authors": "Qing Zhou, Wing Hung Wong", "title": "Reconstructing the energy landscape of a distribution from Monte Carlo\n  samples", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS196 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2008, Vol. 2, No. 4, 1307-1331", "doi": "10.1214/08-AOAS196", "report-no": "IMS-AOAS-AOAS196", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Defining the energy function as the negative logarithm of the density, we\nexplore the energy landscape of a distribution via the tree of sublevel sets of\nits energy. This tree represents the hierarchy among the connected components\nof the sublevel sets. We propose ways to annotate the tree so that it provides\ninformation on both topological and statistical aspects of the distribution,\nsuch as the local energy minima (local modes), their local domains and volumes,\nand the barriers between them. We develop a computational method to estimate\nthe tree and reconstruct the energy landscape from Monte Carlo samples\nsimulated at a wide energy range of a distribution. This method can be applied\nto any arbitrary distribution on a space with defined connectedness. We test\nthe method on multimodal distributions and posterior distributions to show that\nour estimated trees are accurate compared to theoretical values. When used to\nperform Bayesian inference of DNA sequence segmentation, this approach reveals\nmuch more information than the standard approach based on marginal posterior\ndistributions.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2009 13:11:47 GMT"}], "update_date": "2009-01-27", "authors_parsed": [["Zhou", "Qing", ""], ["Wong", "Wing Hung", ""]]}, {"id": "0901.4007", "submitter": "Armin Schwartzman", "authors": "Armin Schwartzman", "title": "Empirical null and false discovery rate inference for exponential\n  families", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS184 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2008, Vol. 2, No. 4, 1332-1359", "doi": "10.1214/08-AOAS184", "report-no": "IMS-AOAS-AOAS184", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In large scale multiple testing, the use of an empirical null distribution\nrather than the theoretical null distribution can be critical for correct\ninference. This paper proposes a ``mode matching'' method for fitting an\nempirical null when the theoretical null belongs to any exponential family.\nBased on the central matching method for $z$-scores, mode matching estimates\nthe null density by fitting an appropriate exponential family to the histogram\nof the test statistics by Poisson regression in a region surrounding the mode.\nThe empirical null estimate is then used to estimate local and tail false\ndiscovery rate (FDR) for inference. Delta-method covariance formulas and\napproximate asymptotic bias formulas are provided, as well as simulation\nstudies of the effect of the tuning parameters of the procedure on the\nbias-variance trade-off. The standard FDR estimates are found to be biased down\nat the far tails. Correlation between test statistics is taken into account in\nthe covariance estimates, providing a generalization of Efron's ``wing\nfunction'' for exponential families. Applications with $\\chi^2$ statistics are\nshown in a family-based genome-wide association study from the Framingham Heart\nStudy and an anatomical brain imaging study of dyslexia in children.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2009 13:43:48 GMT"}], "update_date": "2009-01-27", "authors_parsed": [["Schwartzman", "Armin", ""]]}, {"id": "0901.4011", "submitter": "Andrew Gelman", "authors": "Andrew Gelman, Aleks Jakulin, Maria Grazia Pittau, Yu-Sung Su", "title": "A weakly informative default prior distribution for logistic and other\n  regression models", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS191 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2008, Vol. 2, No. 4, 1360-1383", "doi": "10.1214/08-AOAS191", "report-no": "IMS-AOAS-AOAS191", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new prior distribution for classical (nonhierarchical) logistic\nregression models, constructed by first scaling all nonbinary variables to have\nmean 0 and standard deviation 0.5, and then placing independent Student-$t$\nprior distributions on the coefficients. As a default choice, we recommend the\nCauchy distribution with center 0 and scale 2.5, which in the simplest setting\nis a longer-tailed version of the distribution attained by assuming one-half\nadditional success and one-half additional failure in a logistic regression.\nCross-validation on a corpus of datasets shows the Cauchy class of prior\ndistributions to outperform existing implementations of Gaussian and Laplace\npriors. We recommend this prior distribution as a default choice for routine\napplied use. It has the advantage of always giving answers, even when there is\ncomplete separation in logistic regression (a common problem, even when the\nsample size is large and the number of predictors is small), and also\nautomatically applying more shrinkage to higher-order interactions. This can be\nuseful in routine data analysis as well as in automated procedures such as\nchained equations for missing-data imputation. We implement a procedure to fit\ngeneralized linear models in R with the Student-$t$ prior distribution by\nincorporating an approximate EM algorithm into the usual iteratively weighted\nleast squares. We illustrate with several applications, including a series of\nlogistic regressions predicting voting preferences, a small bioassay\nexperiment, and an imputation model for a public health data set.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2009 14:20:43 GMT"}], "update_date": "2009-01-27", "authors_parsed": [["Gelman", "Andrew", ""], ["Jakulin", "Aleks", ""], ["Pittau", "Maria Grazia", ""], ["Su", "Yu-Sung", ""]]}, {"id": "0901.4025", "submitter": "Yang Yang", "authors": "Yang Yang, Peter Gilbert, Ira M. Longini, Jr., M. Elizabeth Halloran", "title": "A Bayesian framework for estimating vaccine efficacy per infectious\n  contact", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS193 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2008, Vol. 2, No. 4, 1409-1431", "doi": "10.1214/08-AOAS193", "report-no": "IMS-AOAS-AOAS193", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In vaccine studies for infectious diseases such as human immunodeficiency\nvirus (HIV), the frequency and type of contacts between study participants and\ninfectious sources are among the most informative risk factors, but are often\nnot adequately adjusted for in standard analyses. Such adjustment can improve\nthe assessment of vaccine efficacy as well as the assessment of risk factors.\nIt can be attained by modeling transmission per contact with infectious\nsources. However, information about contacts that rely on self-reporting by\nstudy participants are subject to nontrivial measurement error in many studies.\nWe develop a Bayesian hierarchical model fitted using Markov chain Monte Carlo\n(MCMC) sampling to estimate the vaccine efficacy controlled for exposure to\ninfection, while adjusting for measurement error in contact-related factors.\nOur method is used to re-analyze two recent HIV vaccine studies, and the\nresults are compared with the published primary analyses that used standard\nmethods. The proposed method could also be used for other vaccines where\ncontact information is collected, such as human papilloma virus vaccines.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2009 15:36:14 GMT"}], "update_date": "2012-08-27", "authors_parsed": [["Yang", "Yang", ""], ["Gilbert", "Peter", ""], ["Longini,", "Ira M.", "Jr."], ["Halloran", "M. Elizabeth", ""]]}, {"id": "0901.4203", "submitter": "Thomas Brendan Murphy", "authors": "Isobel Claire Gormley, Thomas Brendan Murphy", "title": "A mixture of experts model for rank data with applications in election\n  studies", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS178 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2008, Vol. 2, No. 4, 1452-1477", "doi": "10.1214/08-AOAS178", "report-no": "IMS-AOAS-AOAS178", "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A voting bloc is defined to be a group of voters who have similar voting\npreferences. The cleavage of the Irish electorate into voting blocs is of\ninterest. Irish elections employ a ``single transferable vote'' electoral\nsystem; under this system voters rank some or all of the electoral candidates\nin order of preference. These rank votes provide a rich source of preference\ninformation from which inferences about the composition of the electorate may\nbe drawn. Additionally, the influence of social factors or covariates on the\nelectorate composition is of interest. A mixture of experts model is a mixture\nmodel in which the model parameters are functions of covariates. A mixture of\nexperts model for rank data is developed to provide a model-based method to\ncluster Irish voters into voting blocs, to examine the influence of social\nfactors on this clustering and to examine the characteristic preferences of the\nvoting blocs. The Benter model for rank data is employed as the family of\ncomponent densities within the mixture of experts model; generalized linear\nmodel theory is employed to model the influence of covariates on the mixing\nproportions. Model fitting is achieved via a hybrid of the EM and MM\nalgorithms. An example of the methodology is illustrated by examining an Irish\npresidential election. The existence of voting blocs in the electorate is\nestablished and it is determined that age and government satisfaction levels\nare important factors in influencing voting in this election.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jan 2009 09:16:05 GMT"}], "update_date": "2014-02-26", "authors_parsed": [["Gormley", "Isobel Claire", ""], ["Murphy", "Thomas Brendan", ""]]}, {"id": "0901.4208", "submitter": "Paul Gustafson", "authors": "Paul Gustafson, Genevi\\`eve Lefebvre", "title": "Bayesian multinomial regression with class-specific predictor selection", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS188 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2008, Vol. 2, No. 4, 1478-1502", "doi": "10.1214/08-AOAS188", "report-no": "IMS-AOAS-AOAS188", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a multinomial regression model where the response, which indicates a\nunit's membership in one of several possible unordered classes, is associated\nwith a set of predictor variables. Such models typically involve a matrix of\nregression coefficients, with the $(j,k)$ element of this matrix modulating the\neffect of the $k$th predictor on the propensity of the unit to belong to the\n$j$th class. Thus, a supposition that only a subset of the available predictors\nare associated with the response corresponds to some of the columns of the\ncoefficient matrix being zero. Under the Bayesian paradigm, the subset of\npredictors which are associated with the response can be treated as an unknown\nparameter, leading to typical Bayesian model selection and model averaging\nprocedures. As an alternative, we investigate model selection and averaging,\nwhereby a subset of individual elements of the coefficient matrix are zero.\nThat is, the subset of predictors associated with the propensity to belong to a\nclass varies with the class. We refer to this as class-specific predictor\nselection. We argue that such a scheme can be attractive on both conceptual and\ncomputational grounds.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jan 2009 09:45:16 GMT"}], "update_date": "2009-01-28", "authors_parsed": [["Gustafson", "Paul", ""], ["Lefebvre", "Genevi\u00e8ve", ""]]}, {"id": "0901.4213", "submitter": "Edwin E. Lewis", "authors": "Hsieh Fushing, Li Zhu, David I. Shapiro-Ilan, James F. Campbell, Edwin\n  E. Lewis", "title": "State-space based mass event-history model I: many decision-making\n  agents with one target", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS189 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2008, Vol. 2, No. 4, 1503-1522", "doi": "10.1214/08-AOAS189", "report-no": "IMS-AOAS-AOAS189", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A dynamic decision-making system that includes a mass of indistinguishable\nagents could manifest impressive heterogeneity. This kind of nonhomogeneity is\npostulated to result from macroscopic behavioral tactics employed by almost all\ninvolved agents. A State-Space Based (SSB) mass event-history model is\ndeveloped here to explore the potential existence of such macroscopic\nbehaviors. By imposing an unobserved internal state-space variable into the\nsystem, each individual's event-history is made into a composition of a common\nstate duration and an individual specific time to action. With the common state\nmodeling of the macroscopic behavior, parametric statistical inferences are\nderived under the current-status data structure and conditional independence\nassumptions. Identifiability and computation related problems are also\naddressed. From the dynamic perspectives of system-wise heterogeneity, this SSB\nmass event-history model is shown to be very distinct from a random effect\nmodel via the Principle Component Analysis (PCA) in a numerical experiment.\nReal data showing the mass invasion by two species of parasitic nematode into\ntwo species of host larvae are also analyzed. The analysis results not only are\nfound coherent in the context of the biology of the nematode as a parasite, but\nalso include new quantitative interpretations.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jan 2009 10:08:58 GMT"}], "update_date": "2009-01-28", "authors_parsed": [["Fushing", "Hsieh", ""], ["Zhu", "Li", ""], ["Shapiro-Ilan", "David I.", ""], ["Campbell", "James F.", ""], ["Lewis", "Edwin E.", ""]]}, {"id": "0901.4219", "submitter": "Alessandra Luati", "authors": "Tommaso Proietti, Alessandra Luati", "title": "Real time estimation in local polynomial regression, with application to\n  trend-cycle analysis", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS195 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2008, Vol. 2, No. 4, 1523-1553", "doi": "10.1214/08-AOAS195", "report-no": "IMS-AOAS-AOAS195", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper focuses on the adaptation of local polynomial filters at the end of\nthe sample period. We show that for real time estimation of signals (i.e.,\nexactly at the boundary of the time support) we cannot rely on the automatic\nadaptation of the local polynomial smoothers, since the direct real time filter\nturns out to be strongly localized, and thereby yields extremely volatile\nestimates. As an alternative, we evaluate a general family of asymmetric\nfilters that minimizes the mean square revision error subject to polynomial\nreproduction constraints; in the case of the Henderson filter it nests the\nwell-known Musgrave's surrogate filters. The class of filters depends on\nunknown features of the series such as the slope and the curvature of the\nunderlying signal, which can be estimated from the data. Several empirical\nexamples illustrate the effectiveness of our proposal.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jan 2009 10:55:57 GMT"}], "update_date": "2009-01-28", "authors_parsed": [["Proietti", "Tommaso", ""], ["Luati", "Alessandra", ""]]}, {"id": "0901.4647", "submitter": "Dana Draghicescu", "authors": "Dana Draghicescu, Rosaria Ignaccolo", "title": "Modeling threshold exceedance probabilities of spatially correlated time\n  series", "comments": "Published in at http://dx.doi.org/10.1214/08-EJS252 the Electronic\n  Journal of Statistics (http://www.i-journals.org/ejs/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Electronic Journal of Statistics 2009, Vol. 3, 149-164", "doi": "10.1214/08-EJS252", "report-no": "IMS-EJS-EJS_2008_252", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Commission of the European Union, as well the United States Environmental\nProtection Agency, have set limit values for some pollutants in the ambient air\nthat have been shown to have adverse effects on human and environmental health.\nIt is therefore important to identify regions where the probability of\nexceeding those limits is high. We propose a two-step procedure for estimating\nthe probability of exceeding the legal limits that combines smoothing in the\ntime domain with spatial interpolation. For illustration, we show an\napplication to particulate matter with diameter less than 10 microns\n(PM$_{10}$) in the North-Italian region Piemonte.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jan 2009 10:48:51 GMT"}], "update_date": "2009-01-30", "authors_parsed": [["Draghicescu", "Dana", ""], ["Ignaccolo", "Rosaria", ""]]}]