[{"id": "1811.00074", "submitter": "Negin Alemazkoor", "authors": "Negin Alemazkoor, Hadi Meidani", "title": "Efficient Collection of Connected Vehicles Data with Precision\n  Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Connected vehicles disseminate detailed data, including their position and\nspeed, at a very high frequency. Such data can be used for accurate real-time\nanalysis, prediction and control of transportation systems. The outstanding\nchallenge for such analysis is how to continuously collect and process\nextremely large volumes of data. To address this challenge, efficient\ncollection of data is critical to prevent overburdening the communication\nsystems and overreaching computational and memory capacity. In this work, we\npropose an efficient data collection scheme that selects and transmits only a\nsmall subset of data to alleviate data transmission burden. As a demonstration,\nwe have used the proposed approach to select data points to be transmitted from\n10,000 connected vehicles trips available in the Safety Pilot Model Deployment\ndataset. The presented results show that collection ratio can be as small as\n0.05 depending on the required precision. Moreover, a simulation study was\nperformed to evaluate the travel time estimation accuracy using the proposed\ndata collection approach. Results show that the proposed data collection\napproach can significantly improve travel time estimation accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 19:23:23 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Alemazkoor", "Negin", ""], ["Meidani", "Hadi", ""]]}, {"id": "1811.00457", "submitter": "Elea McDonnell Feit", "authors": "Elea McDonnell Feit and Ron Berman", "title": "Test & Roll: Profit-Maximizing A/B Tests", "comments": null, "journal-ref": null, "doi": "10.1287/mksc.2019.1194", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Marketers often use A/B testing as a tool to compare marketing treatments in\na test stage and then deploy the better-performing treatment to the remainder\nof the consumer population. While these tests have traditionally been analyzed\nusing hypothesis testing, we re-frame them as an explicit trade-off between the\nopportunity cost of the test (where some customers receive a sub-optimal\ntreatment) and the potential losses associated with deploying a sub-optimal\ntreatment to the remainder of the population.\n  We derive a closed-form expression for the profit-maximizing test size and\nshow that it is substantially smaller than typically recommended for a\nhypothesis test, particularly when the response is noisy or when the total\npopulation is small. The common practice of using small holdout groups can be\nrationalized by asymmetric priors. The proposed test design achieves nearly the\nsame expected regret as the flexible, yet harder-to-implement multi-armed\nbandit under a wide range of conditions.\n  We demonstrate the benefits of the method in three different marketing\ncontexts -- website design, display advertising and catalog tests -- in which\nwe estimate priors from past data. In all three cases, the optimal sample sizes\nare substantially smaller than for a traditional hypothesis test, resulting in\nhigher profit.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 15:53:28 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 22:22:41 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Feit", "Elea McDonnell", ""], ["Berman", "Ron", ""]]}, {"id": "1811.00591", "submitter": "Miki Verma", "authors": "Miki E. Verma, Robert A. Bridges", "title": "Defining a Metric Space of Host Logs and Operational Use Cases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Host logs, in particular, Windows Event Logs, are a valuable source of\ninformation often collected by security operation centers (SOCs). The\nsemi-structured nature of host logs inhibits automated analytics, and while\nmanual analysis is common, the sheer volume makes manual inspection of all logs\nimpossible. Although many powerful algorithms for analyzing time-series and\nsequential data exist, utilization of such algorithms for most cyber security\napplications is either infeasible or requires tailored, research-intensive\npreparations. In particular, basic mathematic and algorithmic developments for\nproviding a generalized, meaningful similarity metric on system logs is needed\nto bridge the gap between many existing sequential data mining methods and this\ncurrently available but under-utilized data source. In this paper, we provide a\nrigorous definition of a metric product space on Windows Event Logs, providing\nan embedding that allows for the application of established machine learning\nand time-series analysis methods. We then demonstrate the utility and\nflexibility of this embedding with multiple use-cases on real data: (1)\ncomparing known infected to new host log streams for attack detection and\nforensics, (2) collapsing similar streams of logs into semantically-meaningful\ngroups (by user, by role), thereby reducing the quantity of data but not the\ncontent, (3) clustering logs as well as short sequences of logs to identify and\nvisualize user behaviors and background processes over time. Overall, we\nprovide a metric space framework for general host logs and log sequences that\nrespects semantic similarity and facilitates a wide variety of data science\nanalytics to these logs without data-specific preparations for each.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 19:00:29 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Verma", "Miki E.", ""], ["Bridges", "Robert A.", ""]]}, {"id": "1811.00673", "submitter": "Daniel Gilbert", "authors": "Daniel E. Gilbert and Martin T. Wells", "title": "Ludometrics: Luck, and How to Measure It", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Game theory is the study of tractable games which may be used to model more\ncomplex systems. Board games, video games and sports, however, are intractable\nby design, so \"ludological\" theories about these games as complex phenomena\nshould be grounded in empiricism. A first \"ludometric\" concern is the empirical\nmeasurement of the amount of luck in various games. We argue against a narrow\nview of luck which includes only factors outside any player's control, and\nadvocate for a holistic definition of luck as complementary to the variation in\neffective skill within a population of players. We introduce two metrics for\nluck in a game for a given population - one information theoretical, and one\nBayesian, and discuss the estimation of these metrics using sparse,\nhigh-dimensional regression techniques. Finally, we apply these techniques to\ncompare the amount of luck between various professional sports, between Chess\nand Go, and between two hobby board games: Race for the Galaxy and Seasons.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 23:14:07 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Gilbert", "Daniel E.", ""], ["Wells", "Martin T.", ""]]}, {"id": "1811.00724", "submitter": "Satwik Acharyya", "authors": "Satwik Acharyya, Zhengwu Zhang, Anirban Bhattacharya, Debdeep Pati", "title": "Bayesian Hierarchical Modeling on Covariance Valued Data", "comments": "Some key references are missing in the old version which are\n  corrected in this version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of structural and functional connectivity (FC) of human brains is of\npivotal importance for diagnosis of cognitive ability. The Human Connectome\nProject (HCP) provides an excellent source of neural data across different\nregions of interest (ROIs) of the living human brain. Individual specific data\nwere available from an existing analysis (Dai et al., 2017) in the form of time\nvarying covariance matrices representing the brain activity as the subjects\nperform a specific task. As a preliminary objective of studying the\nheterogeneity of brain connectomics across the population, we develop a\nprobabilistic model for a sample of covariance matrices using a scaled Wishart\ndistribution. We stress here that our data units are available in the form of\ncovariance matrices, and we use the Wishart distribution to create our\nlikelihood function rather than its more common usage as a prior on covariance\nmatrices. Based on empirical explorations suggesting the data matrices to have\nlow effective rank, we further model the center of the Wishart distribution\nusing an orthogonal factor model type decomposition. We encourage shrinkage\ntowards a low rank structure through a novel shrinkage prior and discuss\nstrategies to sample from the posterior distribution using a combination of\nGibbs and slice sampling. We extend our modeling framework to a dynamic setting\nto detect change points. The efficacy of the approach is explored in various\nsimulation settings and exemplified on several case studies including our\nmotivating HCP data. We extend our modeling framework to a dynamic setting to\ndetect change points.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 03:24:22 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2020 21:46:08 GMT"}, {"version": "v3", "created": "Fri, 21 Feb 2020 22:04:44 GMT"}, {"version": "v4", "created": "Thu, 9 Jul 2020 05:48:08 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Acharyya", "Satwik", ""], ["Zhang", "Zhengwu", ""], ["Bhattacharya", "Anirban", ""], ["Pati", "Debdeep", ""]]}, {"id": "1811.00731", "submitter": "Cynthia Rudin", "authors": "Cynthia Rudin, Caroline Wang, and Beau Coker", "title": "The age of secrecy and unfairness in recidivism prediction", "comments": null, "journal-ref": "Harvard Data Science Review 2(1), 2020", "doi": null, "report-no": null, "categories": "stat.AP cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In our current society, secret algorithms make important decisions about\nindividuals. There has been substantial discussion about whether these\nalgorithms are unfair to groups of individuals. While noble, this pursuit is\ncomplex and ultimately stagnating because there is no clear definition of\nfairness and competing definitions are largely incompatible. We argue that the\nfocus on the question of fairness is misplaced, as these algorithms fail to\nmeet a more important and yet readily obtainable goal: transparency. As a\nresult, creators of secret algorithms can provide incomplete or misleading\ndescriptions about how their models work, and various other kinds of errors can\neasily go unnoticed. By partially reverse engineering the COMPAS algorithm -- a\nrecidivism-risk scoring algorithm used throughout the criminal justice system\n-- we show that it does not seem to depend linearly on the defendant's age,\ndespite statements to the contrary by the algorithm's creator. Furthermore, by\nsubtracting from COMPAS its (hypothesized) nonlinear age component, we show\nthat COMPAS does not necessarily depend on race, contradicting ProPublica's\nanalysis, which assumed linearity in age. In other words, faulty assumptions\nabout a proprietary algorithm lead to faulty conclusions that go unchecked\nwithout careful reverse engineering. Were the algorithm transparent in the\nfirst place, this would likely not have occurred. The most important result in\nthis work is that we find that there are many defendants with low risk score\nbut long criminal histories, suggesting that data inconsistencies occur\nfrequently in criminal justice databases. We argue that transparency satisfies\na different notion of procedural fairness by providing both the defendants and\nthe public with the opportunity to scrutinize the methodology and calculations\nbehind risk scores for recidivism.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 04:14:09 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2019 04:30:19 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Rudin", "Cynthia", ""], ["Wang", "Caroline", ""], ["Coker", "Beau", ""]]}, {"id": "1811.00964", "submitter": "Bo Chen", "authors": "Bo Chen, Radu V. Craiu, Lisa J. Strug, Lei Sun", "title": "The X Factor: A Robust and Powerful Approach to X-chromosome-Inclusive\n  Whole-genome Association Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The X-chromosome is often excluded from genome-wide association studies\nbecause of analytical challenges. Some of the problems, such as the random,\nskewed or no X-inactivation model uncertainty, have been investigated. Other\nconsiderations have received little to no attention, such as the value in\nconsidering non-additive and gene-sex interaction effects, and the inferential\nconsequence of choosing different baseline alleles (i.e.\\ the reference vs.\\\nthe alternative allele). Here we propose a unified and flexible\nregression-based association test for X-chromosomal variants. We provide\ntheoretical justifications for its robustness in the presence of various model\nuncertainties, as well as for its improved power when compared with the\nexisting approaches under certain scenarios. For completeness, we also revisit\nthe autosomes and show that the proposed framework leads to a more robust\napproach than the standard method. Finally, we provide supporting evidence by\nrevisiting several published association studies. Supplementary materials for\nthis article are available online.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 16:27:15 GMT"}, {"version": "v2", "created": "Sun, 17 Mar 2019 01:46:09 GMT"}, {"version": "v3", "created": "Fri, 6 Sep 2019 01:35:47 GMT"}, {"version": "v4", "created": "Sat, 15 May 2021 02:41:57 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Chen", "Bo", ""], ["Craiu", "Radu V.", ""], ["Strug", "Lisa J.", ""], ["Sun", "Lei", ""]]}, {"id": "1811.01261", "submitter": "Jonathan Levy", "authors": "Jonathan Levy", "title": "Canonical Least Favorable Submodels:A New TMLE Procedure for\n  Multidimensional Parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a fundamental addition to the world of targeted maximum\nlikelihood estimation (TMLE) (or likewise, targeted minimum loss estimation)\nfor simultaneous estimation of multi-dimensional parameters of interest. TMLE,\nas part of the targeted learning framework, offers a crucial step in\nconstructing efficient plug-in estimators for nonparametric or semiparametric\nmodels. The so-called targeting step of targeted learning, involves fluctuating\nthe initial fit of the model in a way that maximally adjusts the plug-in\nestimate per change in the log likelihood. Previously for multidimensional\nparameters of interest, iterative TMLE's were constructed using locally least\nfavorable submodels as defined in van der Laan and Gruber, 2016, which are\nindexed by a multidimensional fluctuation parameter. In this paper we define a\ncanonical least favorable submodel in terms of a single dimensional epsilon for\na $d$-dimensional parameter of interest. One can view the clfm as the iterative\nanalog to the one-step TMLE as constructed in van der Laan and Gruber, 2016. It\nis currently implemented in several software packages we provide in the last\nsection. Using a single epsilon for the targeting step in TMLE could be useful\nfor high dimensional parameters, where using a fluctuation parameter of the\nsame dimension as the parameter of interest could suffer the consequences of\ncurse of dimensionality. The clfm also enables placing the so-called clever\ncovariate denominator as an inverse weight in an offset intercept model. It has\nbeen shown that such weighting mitigates the effect of large inverse weights\nsometimes caused by near positivity violations.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 18:06:44 GMT"}, {"version": "v2", "created": "Fri, 9 Nov 2018 03:02:41 GMT"}, {"version": "v3", "created": "Thu, 11 Apr 2019 04:29:09 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Levy", "Jonathan", ""]]}, {"id": "1811.01314", "submitter": "Xilei Zhao", "authors": "Xilei Zhao, James C. Spall", "title": "Modeling Traffic Networks Using Integrated Route and Link Data", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time navigation services, such as Google Maps and Waze, are widely used\nin daily life. These services provide rich data resources in real-time traffic\nconditions and travel time predictions; however, they have not been fully\napplied in transportation modeling. This paper aims to use traffic data from\nGoogle Maps and applying cutting-edge technologies in maximum likelihood\nestimation to model traffic networks and travel time reliability. This paper\nintegrates Google Maps travel time data for routes and traffic condition data\nfor links to model the complexities of traffic networks. We then formulate the\nFisher information matrix and apply the asymptotic normality to obtain the\nprobability distribution of the travel time estimates for a random route within\nthe network of interest. We also derive the travel time reliability by\nconsidering two levels of uncertainties, i.e., the uncertainty of the route's\ntravel time and the uncertainty of its travel time estimates. The proposed\nmethod could provide a more realistic and precise travel time reliability\nestimate. The methodology is applied to a small network in the downtown\nBaltimore area, where we propose a link data collection strategy and provide\nempirical evidence to show data independence by following this strategy. We\nalso show results for maximum likelihood estimates and travel time reliability\nmeasures for different routes within the network. Furthermore, we use the\nhistorical data from a different network to validate this approach, showing our\nmethod provides a more accurate and precise estimate compared to the sample\nmean of the empirical data.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 02:53:03 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Zhao", "Xilei", ""], ["Spall", "James C.", ""]]}, {"id": "1811.01315", "submitter": "Xilei Zhao", "authors": "Xilei Zhao, Xiang Yan, Alan Yu, Pascal Van Hentenryck", "title": "Modeling Stated Preference for Mobility-on-Demand Transit: A Comparison\n  of Machine Learning and Logit Models", "comments": "30 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logit models are usually applied when studying individual travel behavior,\ni.e., to predict travel mode choice and to gain behavioral insights on traveler\npreferences. Recently, some studies have applied machine learning to model\ntravel mode choice and reported higher out-of-sample predictive accuracy than\ntraditional logit models (e.g., multinomial logit). However, little research\nfocuses on comparing the interpretability of machine learning with logit\nmodels. In other words, how to draw behavioral insights from the\nhigh-performance \"black-box\" machine-learning models remains largely unsolved\nin the field of travel behavior modeling.\n  This paper aims at providing a comprehensive comparison between the two\napproaches by examining the key similarities and differences in model\ndevelopment, evaluation, and behavioral interpretation between logit and\nmachine-learning models for travel mode choice modeling. To complement the\ntheoretical discussions, the paper also empirically evaluates the two\napproaches on the stated-preference survey data for a new type of transit\nsystem integrating high-frequency fixed-route services and ridesourcing. The\nresults show that machine learning can produce significantly higher predictive\naccuracy than logit models. Moreover, machine learning and logit models largely\nagree on many aspects of behavioral interpretations. In addition, machine\nlearning can automatically capture the nonlinear relationship between the input\nfeatures and choice outcomes. The paper concludes that there is great potential\nin merging ideas from machine learning and conventional statistical methods to\ndevelop refined models for travel behavior research and suggests some new\nresearch directions.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 02:55:49 GMT"}, {"version": "v2", "created": "Mon, 1 Apr 2019 19:40:52 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Zhao", "Xilei", ""], ["Yan", "Xiang", ""], ["Yu", "Alan", ""], ["Van Hentenryck", "Pascal", ""]]}, {"id": "1811.01467", "submitter": "Michelle Edwards", "authors": "Michelle Edwards, Lewis Mitchell, Jonathan Tuke and Matthew Roughan", "title": "The one comparing narrative social network extraction techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysing narratives through their social networks is an expanding field in\nquantitative literary studies. Manually extracting a social network from any\nnarrative can be time consuming, so automatic extraction methods of varying\ncomplexity have been developed. However, the effect of different extraction\nmethods on the analysis is unknown. Here we model and compare three extraction\nmethods for social networks in narratives: manual extraction, co-occurrence\nautomated extraction and automated extraction using machine learning. Although\nthe manual extraction method produces more precise results in the network\nanalysis, it is much more time consuming and the automatic extraction methods\nyield comparable conclusions for density, centrality measures and edge weights.\nOur results provide evidence that social networks extracted automatically are\nreliable for many analyses. We also describe which aspects of analysis are not\nreliable with such a social network. We anticipate that our findings will make\nit easier to analyse more narratives, which help us improve our understanding\nof how stories are written and evolve, and how people interact with each other.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 00:50:18 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Edwards", "Michelle", ""], ["Mitchell", "Lewis", ""], ["Tuke", "Jonathan", ""], ["Roughan", "Matthew", ""]]}, {"id": "1811.01469", "submitter": "Xudong Zhang", "authors": "Xudong Zhang", "title": "Monte Carlo Simulations on robustness of functional location estimator\n  based on several functional depth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional data analysis has been a growing field of study in recent decades,\nand one fundamental task in functional data analysis is estimating the sample\nlocation. A notion called statistical depth has been extended from multivariate\ndata to functional data, and it can provide a center-outward order for each\nobservation within a sample of functional curves. Making use of this intuitive\nnature of depth methods, a depth-based trimmed mean where curves with lower\ndepth values are excluded can be used as a robust location estimator for the\nsample. In this project, we first introduced several state-of-the-art depth\napproaches for functional data. These depths were half region depth, functional\nmajority depth, band depth, modified band depth and functional spatial depth.\nThen we described a robust location estimator based on functional depth, and\nstudied performances of these estimators based on different functional depth\napproaches via simulation tests. Finally, the test results showed that\nestimators based on functional spatial depth and modified band depth exhibited\nsuperior performances.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 01:05:02 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Zhang", "Xudong", ""]]}, {"id": "1811.01689", "submitter": "Kaveh Dehghanpour", "authors": "Yuxuan Yuan and Kaveh Dehghanpour and Fankun Bu and Zhaoyu Wang", "title": "A Data-Driven Customer Segmentation Strategy Based on Contribution to\n  System Peak Demand", "comments": null, "journal-ref": null, "doi": "10.1109/TPWRS.2020.2979943", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advanced metering infrastructure (AMI) enables utilities to obtain granular\nenergy consumption data, which offers a unique opportunity to design customer\nsegmentation strategies based on their impact on various operational metrics in\ndistribution grids. However, performing utility-scale segmentation for\nunobservable customers with only monthly billing information, remains a\nchallenging problem. To address this challenge, we propose a new metric, the\ncoincident monthly peak contribution (CMPC), that quantifies the contribution\nof individual customers to system peak demand. Furthermore, a novel multi-state\nmachine learning-based segmentation method is developed that estimates CMPC for\ncustomers without smart meters (SMs): first, a clustering technique is used to\nbuild a databank containing typical daily load patterns in different seasons\nusing the SM data of observable customers. Next, to associate unobservable\ncustomers with the discovered typical load profiles, a classification approach\nis leveraged to compute the likelihood of daily consumption patterns for\ndifferent unobservable households. In the third stage, a weighted clusterwise\nregression (WCR) model is utilized to estimate the CMPC of unobservable\ncustomers using their monthly billing data and the outcomes of the\nclassification module. The proposed segmentation methodology has been tested\nand verified using real utility data.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 15:13:16 GMT"}, {"version": "v2", "created": "Mon, 25 Feb 2019 17:40:04 GMT"}, {"version": "v3", "created": "Thu, 7 Mar 2019 16:46:12 GMT"}, {"version": "v4", "created": "Thu, 26 Sep 2019 21:44:58 GMT"}, {"version": "v5", "created": "Sun, 8 Mar 2020 18:51:25 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Yuan", "Yuxuan", ""], ["Dehghanpour", "Kaveh", ""], ["Bu", "Fankun", ""], ["Wang", "Zhaoyu", ""]]}, {"id": "1811.02021", "submitter": "Jacob Fiksel", "authors": "Jacob Fiksel, Johanna S. Hardin, Leah R. Jager, and Margaret A. Taub", "title": "Using GitHub Classroom To Teach Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Git and GitHub are common tools for keeping track of multiple versions of\ndata analytic content, which allow for more than one person to simultaneously\nwork on a project. GitHub Classroom aims to provide a way for students to work\non and submit their assignments via Git and GitHub, giving teachers an\nopportunity to teach these version control tools as part of their course. In\nthe Fall 2017 semester, we implemented GitHub Classroom in two educational\nsettings--an introductory computational statistics lab and a more advanced\ncomputational statistics course. We found many educational benefits of\nimplementing GitHub Classroom, such as easily providing coding feedback during\nassignments and making students more confident in their ability to collaborate\nand use version control tools for future data science work. To encourage and\nease the transition into using GitHub Classroom, we provide free and publicly\navailable resources--both for students to begin using Git/GitHub and for\nteachers to use GitHub Classroom for their own courses.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 20:30:49 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Fiksel", "Jacob", ""], ["Hardin", "Johanna S.", ""], ["Jager", "Leah R.", ""], ["Taub", "Margaret A.", ""]]}, {"id": "1811.02069", "submitter": "Frederic Pascal", "authors": "Gordana Draskovic and Arnaud Breloy and Frederic Pascal", "title": "On the asymptotics of Maronna's robust PCA", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2019.2932877", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The eigenvalue decomposition (EVD) parameters of the second order statistics\nare ubiquitous in statistical analysis and signal processing. Notably, the EVD\nof robust scatter $M$-estimators is a popular choice to perform robust\nprobabilistic PCA or other dimension reduction related applications. Towards\nthe goal of characterizing the behavior of these quantities, this paper\nproposes new asymptotics for the EVD parameters (i.e. eigenvalues, eigenvectors\nand principal subspace) of the scatter $M$-estimator in the context of complex\nelliptically symmetric distributions. First, their Gaussian asymptotic\ndistribution is obtained by extending standard results on the sample covariance\nmatrix in a Gaussian context. Second, their convergence rate towards the EVD\nparameters of a Gaussian-Core Wishart Equivalent is derived. This second result\nrepresents the main contribution in the sense that it quantifies when it is\nacceptable to directly plug-in well-established results on the EVD of\nWishart-distributed matrix for characterizing the EVD of $M$-estimators.\nEventually, some examples (low-rank adaptive filtering and Intrinsic bias\nanalysis) are provided to illustrate where the obtained results can be\nleveraged.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 22:38:03 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Draskovic", "Gordana", ""], ["Breloy", "Arnaud", ""], ["Pascal", "Frederic", ""]]}, {"id": "1811.02106", "submitter": "Adam Ploszaj", "authors": "Adam Ploszaj, Xiaoran Yan, Katy Borner", "title": "The impact of air transport availability on research collaboration: A\n  case study of four universities", "comments": "3 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper analyzes the impact of air transport connectivity and\naccessibility on scientific collaboration.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 01:05:38 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Ploszaj", "Adam", ""], ["Yan", "Xiaoran", ""], ["Borner", "Katy", ""]]}, {"id": "1811.02215", "submitter": "Colin Leverger", "authors": "Colin Leverger (LACODAM), Vincent Lemaire, Simon Malinowski (UR1,\n  LinkMedia), Thomas Guyet (LACODAM), Laurence Roz\\'e (LACODAM, INSA Rennes)", "title": "Day-ahead time series forecasting: application to capacity planning", "comments": null, "journal-ref": "AALTD'18 at ECML 2018, Sep 2018, Dublin, Ireland", "doi": null, "report-no": null, "categories": "cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of capacity planning, forecasting the evolution of informatics\nservers usage enables companies to better manage their computational resources.\nWe address this problem by collecting key indicator time series and propose to\nforecast their evolution a day-ahead. Our method assumes that data is\nstructured by a daily seasonality, but also that there is typical evolution of\nindicators within a day. Then, it uses the combination of a clustering\nalgorithm and Markov Models to produce day-ahead forecasts. Our experiments on\nreal datasets show that the data satisfies our assumption and that, in the case\nstudy, our method outperforms classical approaches (AR, Holt-Winters).\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 08:14:01 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Leverger", "Colin", "", "LACODAM"], ["Lemaire", "Vincent", "", "UR1,\n  LinkMedia"], ["Malinowski", "Simon", "", "UR1,\n  LinkMedia"], ["Guyet", "Thomas", "", "LACODAM"], ["Roz\u00e9", "Laurence", "", "LACODAM, INSA Rennes"]]}, {"id": "1811.02255", "submitter": "Annette M\\\"oller", "authors": "Annette M\\\"oller, Ludovica Spazzini, Daniel Kraus, Thomas Nagler and\n  Claudia Czado", "title": "Vine copula based post-processing of ensemble forecasts for temperature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today weather forecasting is conducted using numerical weather prediction\n(NWP) models, consisting of a set of differential equations describing the\ndynamics of the atmosphere. The output of such NWP models are single\ndeterministic forecasts of future atmospheric states. To assess uncertainty in\nNWP forecasts so-called forecast ensembles are utilized. They are generated by\nemploying a NWP model for distinct variants. However, as forecast ensembles are\nnot able to capture the full amount of uncertainty in an NWP model, they often\nexhibit biases and dispersion errors. Therefore it has become common practise\nto employ statistical post processing models which correct for biases and\nimprove calibration. We propose a novel post processing approach based on\nD-vine copulas, representing the predictive distribution by its quantiles.\nThese models allow for much more general dependence structures than the\nstate-of-the-art EMOS model and is highly data adapted. Our D-vine quantile\nregression approach shows excellent predictive performance in comparative\nstudies of temperature forecasts over Europe with different forecast horizons\nbased on the 52-member ensemble of the European Centre for Medium-Range Weather\nForecasting (ECMWF). Specifically for larger forecast horizons the method\nclearly improves over the benchmark EMOS model.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 09:39:52 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["M\u00f6ller", "Annette", ""], ["Spazzini", "Ludovica", ""], ["Kraus", "Daniel", ""], ["Nagler", "Thomas", ""], ["Czado", "Claudia", ""]]}, {"id": "1811.02504", "submitter": "Christian R\\\"over", "authors": "T. Friede, M. Posch, S. Zohar, C. Alberti, N. Benda, E. Comets, S.\n  Day, A. Dmitrenko, A. Graf, B. K. G\\\"unhan, S. W. Hee, F. Lentz, J. Madan, F.\n  Miller, T. Ondra, M. Pearce, C. R\\\"over, A. Tournazi, S. Unkel, M. Ursino, G.\n  Wassmer, N. Stallard", "title": "Recent advances in methodology for clinical trials in small populations:\n  the InSPiRe project", "comments": "9 pages, 3 figures", "journal-ref": "Orphanet Journal of Rare Diseases, 13:136, 2018", "doi": "10.1186/s13023-018-0919-y", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Where there are a limited number of patients, such as in a rare disease,\nclinical trials in these small populations present several challenges,\nincluding statistical issues. This led to an EU FP7 call for proposals in 2013.\nOne of the three projects funded was the Innovative Methodology for Small\nPopulations Research (InSPiRe) project. This paper summarizes the main results\nof the project, which was completed in 2017. The InSPiRe project has led to\ndevelopment of novel statistical methodology for clinical trials in small\npopulations in four areas. We have explored new decision-making methods for\nsmall population clinical trials using a Bayesian decision-theoretic framework\nto compare costs with potential benefits, developed approaches for targeted\ntreatment trials, enabling simultaneous identification of subgroups and\nconfirmation of treatment effect for these patients, worked on early phase\nclinical trial design and on extrapolation from adult to pediatric studies,\ndeveloping methods to enable use of pharmacokinetics and pharmacodynamics data,\nand also developed improved robust meta-analysis methods for a small number of\ntrials to support the planning, analysis and interpretation of a trial as well\nas enabling extrapolation between patient groups. In addition to scientific\npublications, we have contributed to regulatory guidance and produced free\nsoftware in order to facilitate implementation of the novel methods.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 08:04:44 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Friede", "T.", ""], ["Posch", "M.", ""], ["Zohar", "S.", ""], ["Alberti", "C.", ""], ["Benda", "N.", ""], ["Comets", "E.", ""], ["Day", "S.", ""], ["Dmitrenko", "A.", ""], ["Graf", "A.", ""], ["G\u00fcnhan", "B. K.", ""], ["Hee", "S. W.", ""], ["Lentz", "F.", ""], ["Madan", "J.", ""], ["Miller", "F.", ""], ["Ondra", "T.", ""], ["Pearce", "M.", ""], ["R\u00f6ver", "C.", ""], ["Tournazi", "A.", ""], ["Unkel", "S.", ""], ["Ursino", "M.", ""], ["Wassmer", "G.", ""], ["Stallard", "N.", ""]]}, {"id": "1811.02750", "submitter": "Bridianne O'Dea", "authors": "B. ODea, T.W. Boonstra, M.E. Larsen, T. Nguyen, S. Venkatesh, H.\n  Christensen", "title": "The relationship between linguistic expression and symptoms of\n  depression, anxiety, and suicidal thoughts: A longitudinal study of blog\n  content", "comments": "29 pages, 6 figures", "journal-ref": "PLoS ONE 16(5): e0251787, 2021", "doi": "10.1371/journal.pone.0251787", "report-no": null, "categories": "cs.CL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to its popularity and availability, social media data may present a new\nway to identify individuals who are experiencing mental illness. By analysing\nblog content, this study aimed to investigate the associations between\nlinguistic features and symptoms of depression, generalised anxiety, and\nsuicidal ideation. This study utilised a longitudinal study design. Individuals\nwho blogged were invited to participate in a study in which they completed\nfortnightly mental health questionnaires including the PHQ9 and GAD7 for a\nperiod of 36 weeks. Linguistic features were extracted from blog data using the\nLIWC tool. Bivariate and multivariate analyses were performed to investigate\nthe correlations between the linguistic features and mental health scores\nbetween subjects. We then used the multivariate regression model to predict\nlongitudinal changes in mood within subjects. A total of 153 participants\nconsented to taking part, with 38 participants completing the required number\nof questionnaires and blog posts during the study period. Between-subject\nanalysis revealed that several linguistic features, including tentativeness and\nnon-fluencies, were significantly associated with depression and anxiety\nsymptoms, but not suicidal thoughts. Within-subject analysis showed no robust\ncorrelations between linguistic features and changes in mental health score.\nThis study provides further support for the relationship between linguistic\nfeatures within social media data and symptoms of depression and anxiety. The\nlack of robust within-subject correlations indicate that the relationship\nobserved at the group level may not generalise to individual changes over time.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 04:11:15 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["ODea", "B.", ""], ["Boonstra", "T. W.", ""], ["Larsen", "M. E.", ""], ["Nguyen", "T.", ""], ["Venkatesh", "S.", ""], ["Christensen", "H.", ""]]}, {"id": "1811.02756", "submitter": "Kursat Rasim Mestav", "authors": "Kursat Rasim Mestav, Jaime Luengo-Rozas and Lang Tong", "title": "Bayesian State Estimation for Unobservable Distribution Systems via Deep\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of state estimation for unobservable distribution systems is\nconsidered. A deep learning approach to Bayesian state estimation is proposed\nfor real-time applications. The proposed technique consists of distribution\nlearning of stochastic power injection, a Monte Carlo technique for the\ntraining of a deep neural network for state estimation, and a Bayesian bad-data\ndetection and filtering algorithm. Structural characteristics of the deep\nneural networks are investigated. Simulations illustrate the accuracy of\nBayesian state estimation for unobservable systems and demonstrate the benefit\nof employing a deep neural network. Numerical results show the robustness of\nBayesian state estimation against modeling and estimation errors and the\npresence of bad and missing data. Comparing with pseudo-measurement techniques,\ndirect Bayesian state estimation via deep learning neural network outperforms\nexisting benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 04:37:33 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 05:11:27 GMT"}, {"version": "v3", "created": "Mon, 17 Dec 2018 20:04:58 GMT"}, {"version": "v4", "created": "Mon, 25 Feb 2019 00:37:22 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Mestav", "Kursat Rasim", ""], ["Luengo-Rozas", "Jaime", ""], ["Tong", "Lang", ""]]}, {"id": "1811.02809", "submitter": "Tingting Huang", "authors": "Huiwen Wang, Tingting Huang, Shanshan Wang", "title": "A Flexible Spatial Autoregressive Modelling Framework for Mixed\n  Covariates of Multiple Data Types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed spatial autoregressive (SAR) models with numerical covariates have been\nwell studied. However, as non-numerical data, such as functional data and\ncompositional data, receive substantial amounts of attention and are applied to\neconomics, medicine and meteorology, it becomes necessary to develop flexible\nSAR models with multiple data types. In this article, we integrate three types\nof covariates, functional, compositional and numerical, in an SAR model. The\nnew model has the merits of classical functional linear models and\ncompositional linear models with scalar responses. Moreover, we develop an\nestimation method for the proposed model, which is based on functional\nprincipal component analysis (FPCA), the isometric logratio (ilr)\ntransformation and the maximum likelihood estimation method. Monte Carlo\nexperiments demonstrate the effectiveness of the estimators. A real dataset is\nalso used to illustrate the utility of the proposed model.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 09:44:54 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Wang", "Huiwen", ""], ["Huang", "Tingting", ""], ["Wang", "Shanshan", ""]]}, {"id": "1811.02893", "submitter": "Bruno M\\'eriaux", "authors": "Bruno M\\'eriaux (SONDRA/CentraleSup\\'elec, FRANCE), Xin Zhang (AI\n  Department, Echiev Autonomous Driving Technology, CHINA), Mohammed Nabil El\n  Korso (LEME/Paris Nanterre University, FRANCE) and Marius Pesavento\n  (Communication Systems Group, Technische Universit\\\"at Darmstadt, GERMANY)", "title": "Iterative Marginal Maximum Likelihood DOD and DOA Estimation for MIMO\n  Radar in the Presence of SIRP Clutter", "comments": null, "journal-ref": "Signal Processing 155 (2019) 384-390", "doi": null, "report-no": null, "categories": "eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spherically invariant random process (SIRP) clutter model is commonly\nused in scenarios where the radar clutter cannot be correctly modeled as a\nGaussian process. In this short communication, we devise a novel\nMaximum-Likelihood (ML)-based iterative estimator for direction-of-departure\nand direction-of-arrival estimation in the Multiple-input multiple-output\n(MIMO) radar context in the presence of SIRP clutter. The proposed estimator\nemploys a stepwise numerical concentration approach w.r.t. the objective\nfunction related to the marginal likelihood of the observation data. Our\nestimator leads to superior performance, as our simulations show, w.r.t. to the\nexisting likelihood based methods, namely, the conventional, the conditional\nand the joint likelihood based estimators, and w.r.t. the robust subspace\ndecomposition based methods. Finally, interconnections and comparison between\nthe Iterative Marginal ML Estimator (IMMLE), Iterative Joint ML Estimator\n(IJMLE) and Iterative Conditional ML Estimator (ICdMLE) are provided.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 14:02:04 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["M\u00e9riaux", "Bruno", "", "SONDRA/CentraleSup\u00e9lec, FRANCE"], ["Zhang", "Xin", "", "AI\n  Department, Echiev Autonomous Driving Technology, CHINA"], ["Korso", "Mohammed Nabil El", "", "LEME/Paris Nanterre University, FRANCE"], ["Pesavento", "Marius", "", "Communication Systems Group, Technische Universit\u00e4t Darmstadt, GERMANY"]]}, {"id": "1811.02973", "submitter": "Peter Poloskei", "authors": "Peter Zsolt Poloskei, Gergely Papp, Gabor Por, Laszlo Horvath and\n  Gergo I. Pokol", "title": "Bicoherence analysis of nonstationary and nonlinear processes", "comments": "8 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bicoherence analysis is a well established method for identifying the\nquadratic nonlinearity of stationary processes. However, it is often applied\nwithout checking the basic assumptions of stationarity and convergence. The\nclassic bicoherence, unfortunately, tends to give false positives -- high\nbicoherence values without actual nonlinear coupling of different frequency\ncomponents -- for signals exhibiting rapidly changing amplitudes and limited\nlength. The effect of false positive values can lead to misinterpretation of\nresults, therefore a more prudent analysis is necessary in such cases. This\npaper analyses the properties of bispectrum and bicoherence in detail,\ngeneralizing these quantities to nonstationary processes. A step-by-step method\nis proposed to filter out false positives at a given confidence level for the\ncase of nonstationary signals. We present a number of test cases, where the\nmethod is demonstrated on simple physics-based numerical systems. The approach\nand methodology introduced in the paper can be generalized to lower and higher\norder coherence calculations.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 16:35:34 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Poloskei", "Peter Zsolt", ""], ["Papp", "Gergely", ""], ["Por", "Gabor", ""], ["Horvath", "Laszlo", ""], ["Pokol", "Gergo I.", ""]]}, {"id": "1811.03014", "submitter": "Adrienne Mendrik", "authors": "Adrienne M. Mendrik, Stephen R. Aylward", "title": "Beyond the Leaderboard: Insight and Deployment Challenges to Address\n  Research Problems", "comments": "This two-page abstract was accepted for the NIPS 2018 Challenges in\n  Machine Learning (CiML) workshop \"Machine Learning competitions \"in the\n  wild\": Playing in the real world or in real time\" on Saturday December 8,\n  2018 in Palais des congres de Montreal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the medical image analysis field, organizing challenges with associated\nworkshops at international conferences began in 2007 and has grown to include\nover 150 challenges. Several of these challenges have had a major impact in the\nfield. However, whereas well-designed challenges have the potential to unite\nand focus the field on creating solutions to important problems, poorly\ndesigned and documented challenges can equally impede a field and lead to\npursuing incremental improvements in metric scores with no theoretic or\nclinical significance. This is supported by a critical assessment of challenges\nat the international MICCAI conference. In this assessment the main observation\nwas that small changes to the underlying challenge data can drastically change\nthe ranking order on the leaderboard. Related to this is the practice of\nleaderboard climbing, which is characterized by participants focusing on\nincrementally improving metric results rather than advancing science or solving\nthe driving problem of a challenge. In this abstract we look beyond the\nleaderboard of a challenge and instead look at the conclusions that can be\ndrawn from a challenge with respect to the research problem that it is\naddressing. Research study design is well described in other research areas and\ncan be translated to challenge design when viewing challenges as research\nstudies on algorithm performance that address a research problem. Based on the\ntwo main types of scientific research study design, we propose two main\nchallenge types, which we think would benefit other research areas as well: 1)\nan insight challenge that is based on a qualitative study design and 2) a\ndeployment challenge that is based on a quantitative study design. In addition\nwe briefly touch upon related considerations with respect to statistical\nsignificance versus practical significance, generalizability and data\nsaturation.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 10:18:07 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Mendrik", "Adrienne M.", ""], ["Aylward", "Stephen R.", ""]]}, {"id": "1811.03192", "submitter": "Roman Olson", "authors": "Roman Olson, Soon-Il An, Yanan Fan and Jason P. Evans", "title": "Accounting for Skill in Trend, Variability, and Autocorrelation\n  Facilitates Better Multi-Model Projections: Application to the AMOC and\n  Temperature Time Series", "comments": null, "journal-ref": "PLoS ONE (2019) 14(4): e0214535.\n  https://doi.org/10.1371/journal.pone.0214535", "doi": "10.1371/journal.pone.0214535", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel quasi-Bayesian method to weight multiple dynamical models\nby their skill at capturing both potentially non-linear trends and first-order\nautocorrelated variability of the underlying process, and to make weighted\nprobabilistic projections. We validate the method using a suite of\none-at-a-time cross-validation experiments involving Atlantic meridional\noverturning circulation (AMOC), its temperature-based index, as well as Korean\nsummer mean maximum temperature. In these experiments the method tends to\nexhibit superior skill over a trend-only Bayesian model averaging weighting\nmethod in terms of weight assignment and probabilistic forecasts. Specifically,\nmean credible interval width, and mean absolute error of the projections tend\nto improve. We apply the method to a problem of projecting summer mean maximum\ntemperature change over Korea by the end of the 21st century using a\nmulti-model ensemble. Compared to the trend-only method, the new method\nappreciably sharpens the probability distribution function (pdf) and increases\nfuture most likely, median, and mean warming in Korea. The method is flexible,\nwith a potential to improve forecasts in geosciences and other fields.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 23:54:02 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2018 08:55:17 GMT"}, {"version": "v3", "created": "Wed, 17 Apr 2019 05:06:36 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Olson", "Roman", ""], ["An", "Soon-Il", ""], ["Fan", "Yanan", ""], ["Evans", "Jason P.", ""]]}, {"id": "1811.03334", "submitter": "H\\'el\\`ene Ruffieux", "authors": "H\\'el\\`ene Ruffieux, Anthony C. Davison, J\\\"org Hager, Jamie Inshaw,\n  Benjamin P. Fairfax, Sylvia Richardson, Leonardo Bottolo", "title": "A global-local approach for detecting hotspots in multiple-response\n  regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle modelling and inference for variable selection in regression\nproblems with many predictors and many responses. We focus on detecting\nhotspots, i.e., predictors associated with several responses. Such a task is\ncritical in statistical genetics, as hotspot genetic variants shape the\narchitecture of the genome by controlling the expression of many genes and may\ninitiate decisive functional mechanisms underlying disease endpoints. Existing\nhierarchical regression approaches designed to model hotspots suffer from two\nlimitations: their discrimination of hotspots is sensitive to the choice of\ntop-level scale parameters for the propensity of predictors to be hotspots, and\nthey do not scale to large predictor and response vectors, e.g., of dimensions\n$10^3-10^5$ in genetic applications. We address these shortcomings by\nintroducing a flexible hierarchical regression framework that is tailored to\nthe detection of hotspots and scalable to the above dimensions. Our proposal\nimplements a fully Bayesian model for hotspots based on the horseshoe shrinkage\nprior. Its global-local formulation shrinks noise globally and hence\naccommodates the highly sparse nature of genetic analyses, while being robust\nto individual signals, thus leaving the effects of hotspots unshrunk. Inference\nis carried out using a fast variational algorithm coupled with a novel\nsimulated annealing procedure that allows efficient exploration of multimodal\ndistributions.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 09:50:46 GMT"}, {"version": "v2", "created": "Thu, 13 Jun 2019 10:08:43 GMT"}, {"version": "v3", "created": "Fri, 15 May 2020 17:30:59 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Ruffieux", "H\u00e9l\u00e8ne", ""], ["Davison", "Anthony C.", ""], ["Hager", "J\u00f6rg", ""], ["Inshaw", "Jamie", ""], ["Fairfax", "Benjamin P.", ""], ["Richardson", "Sylvia", ""], ["Bottolo", "Leonardo", ""]]}, {"id": "1811.03362", "submitter": "Mariangela Guidolin", "authors": "Mariangela Guidolin, Renato Guseo", "title": "On inverse product cannibalisation: a new Lotka-Volterra model for\n  asymmetric competition in the ICTs", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Product cannibalisation is a well-known phenomenon in marketing and\ntechnological research and describes the case when a new product steals sales\nfrom another product under the same brand. A very special case of\ncannibalisation may occur when the older product react to the competitive\nstrength of the newer one, absorbing the corresponding market shares. Given its\nspecial character, we call this phenomenon inverse product cannibalisation. We\nsuppose that a case of inverse cannibalisation is observed between two products\nof Apple Inc., the iPhone and the more recent iPad, and the first has been able\nto succeed at the expense of the second. To explore this hypothesis, within a\ndiffusion of innovations perspective, we propose a modified Lotka-Volterra\nmodel for mean trajectories in asymmetric competition, allowing us to test the\npresence and the extent of the inverse cannibalisation phenomenon. A SARMAX\nrefinement integrates the short term predictions with seasonal and\nautodependent components. A non-dimensional representation of the proposed\nmodel shows that the penetration of the second technology has been beneficial\nfor the first, both in terms of market size and life cycle length.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 11:28:53 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Guidolin", "Mariangela", ""], ["Guseo", "Renato", ""]]}, {"id": "1811.03577", "submitter": "Guillermo Cabrera-Vives", "authors": "Guillermo Cabrera-Vives and Christopher J. Miller and Jeff Schneider", "title": "Labeling Bias in Galaxy Morphologies", "comments": null, "journal-ref": null, "doi": "10.3847/1538-3881/aae9f4", "report-no": null, "categories": "astro-ph.GA stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a metric to quantify systematic labeling bias in galaxy morphology\ndata sets stemming from the quality of the labeled data. This labeling bias is\nindependent from labeling errors and requires knowledge about the intrinsic\nproperties of the data with respect to the observed properties. We conduct a\nrelative comparison of label bias for different low redshift galaxy morphology\ndata sets. We show our metric is able to recover previous de-biasing procedures\nbased on redshift as biasing parameter. By using the image resolution instead,\nwe find biases that have not been addressed. We find that the morphologies\nbased on supervised machine-learning trained over features such as colors,\nshape, and concentration show significantly less bias than morphologies based\non expert or citizen-science classifiers. This result holds even when there is\nunderlying bias present in the training sets used in the supervised machine\nlearning process. We use catalog simulations to validate our bias metric, and\nshow how to bin the multidimensional intrinsic and observed galaxy properties\nused in the bias quantification. Our approach is designed to work on any other\nlabeled multidimensional data sets and the code is publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 17:58:18 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Cabrera-Vives", "Guillermo", ""], ["Miller", "Christopher J.", ""], ["Schneider", "Jeff", ""]]}, {"id": "1811.03687", "submitter": "Dennis Becker", "authors": "Dennis Becker", "title": "Variational Bayesian hierarchical regression for data analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Collected data, which is used for analysis or prediction tasks, often have a\nhierarchical structure, for example, data from various people performing the\nsame task. Modeling the data's structure can improve the reliability of the\nderived results and prediction performance of newly unobserved data. Bayesian\nmodeling provides a tool-kit for designing hierarchical models. However, Markov\nChain Monte Carlo methods which are commonly used for parameter estimation are\ncomputationally expensive. This often renders its use for many applications not\napplicable. However, variational Bayesian methods allow to derive an\napproximation with much less computational effort. This document describes the\nderivation of a variational approximation for a hierarchical linear Bayesian\nregression and demonstrates its application to data analysis.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 21:47:48 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Becker", "Dennis", ""]]}, {"id": "1811.03745", "submitter": "Jonathan Levy", "authors": "Jonathan Levy and Mark van der Laan and Alan Hubbard and Romain\n  Pirracchio", "title": "A Fundamental Measure of Treatment Effect Heterogeneity", "comments": "Presented at JSM 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We offer a non-parametric plug-in estimator for an important measure of\ntreatment effect variability and provide minimum conditions under which the\nestimator is asymptotically efficient. The stratum specific treatment effect\nfunction or so-called blip function, is the average treatment effect for a\nrandomly drawn stratum of confounders. The mean of the blip function is the\naverage treatment effect (ATE), whereas the variance of the blip function\n(VTE), the main subject of this paper, measures overall clinical effect\nheterogeneity, perhaps providing a strong impetus to refine treatment based on\nthe confounders. VTE is also an important measure for assessing reliability of\nthe treatment for an individual. The CV-TMLE provides simultaneous plug-in\nestimates and inference for both ATE and VTE, guaranteeing asymptotic\nefficiency under one less condition than for TMLE. This condition is difficult\nto guarantee a priori, particularly when using highly adaptive machine learning\nthat we need to employ in order to eliminate bias. Even in defiance of this\ncondition, CV-TMLE sampling distributions maintain normality, not guaranteed\nfor TMLE, and have a lower mean squared error than their TMLE counterparts. In\naddition to verifying the theoretical properties of TMLE and CV-TMLE through\nsimulations, we point out some of the challenges in estimating VTE, which lacks\ndouble robustness and might be unavoidably biased if the true VTE is small and\nsample size insufficient. We will provide an application of the estimator on a\ndata set for treatment of acute trauma patients.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 02:39:38 GMT"}, {"version": "v2", "created": "Sat, 17 Nov 2018 08:07:58 GMT"}, {"version": "v3", "created": "Sun, 23 Dec 2018 09:58:22 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Levy", "Jonathan", ""], ["van der Laan", "Mark", ""], ["Hubbard", "Alan", ""], ["Pirracchio", "Romain", ""]]}, {"id": "1811.03860", "submitter": "Sipat Triukose", "authors": "Suthara Aramcharoen (1), Ponlapat Satian (2), Ponlachart Chotikarn\n  (3), Sipat Triukose (4 and 5) ((1) Thungsong Hospital, Nakhonsrithammarat,\n  Thailand (2) Lansaka Hospital, Nakhonsrithammarat, Thailand (3) Marine and\n  Coastal Resources Institute, Prince of Songkla University, Thailand (4)\n  Chulalongkorn University Big Data Analytics and IoT Center (CUBIC),\n  Chulalongkorn University, Thailand (5) Research group on Applied Computer\n  Engineering Technology for Medicine and Healthcare (ATM), Chulalongkorn\n  University, Thailand)", "title": "An external validation of Thais' cardiovascular 10-year risk assessment\n  in the southern Thailand", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiovascular diseases (CVDs) is a number one cause of death globally. WHO\nestimated that CVD is a cause of 17.9 million deaths (or 31% of all global\ndeaths) in 2016. It may seem surprising, CVDs can be easily prevented by\naltering lifestyle to avoid risk factors. The only requirement needed is to\nknow your risk prior. Thai CV Risk score is a trustworthy tool to forecast risk\nof having cardiovascular event in the future for Thais. This study is an\nexternal validation of the Thai CV risk score. We aim to answer two key\nquestions. Firstly, Can Thai CV Risk score developed using dataset of people\nfrom central and north western parts of Thailand is applicable to people from\nother parts of the country? Secondly, Can Thai CV Risk score developed for\ngeneral public works for hospital's patients who tend to have higher risk? We\nanswer these two questions using a dataset of 1,025 patients (319 males, 35-70\nyears old) from Lansaka Hospital in the southern Thailand. In brief, we find\nthat the Thai CV risk score works for southern Thais population including\npatients in the hospital. It generally works well for low CV risk group.\nHowever, the score tends to overestimate moderate and high risks. Fortunately,\nthis poses no serious concern for general public as it only makes people be\nmore careful about their lifestyle. The doctor should be careful when using the\nscore with other factors to make treatment decision.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 11:02:26 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Aramcharoen", "Suthara", "", "4 and 5"], ["Satian", "Ponlapat", "", "4 and 5"], ["Chotikarn", "Ponlachart", "", "4 and 5"], ["Triukose", "Sipat", "", "4 and 5"]]}, {"id": "1811.03930", "submitter": "Peter Gilbert", "authors": "Peter B. Gilbert, Bryan S. Blette, Bryan E. Shepherd, Michael G.\n  Hudgens", "title": "Post-randomization Biomarker Effect Modification in an HIV Vaccine\n  Clinical Trial", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the HVTN 505 trial showed no overall efficacy of the tested vaccine to\nprevent HIV infection over placebo, previous studies, biological theories, and\nthe finding that immune response markers strongly correlated with infection in\nvaccine recipients generated the hypothesis that a qualitative interaction\noccurred. This hypothesis can be assessed with statistical methods for studying\ntreatment effect modification by an intermediate response variable (i.e.,\nprincipal stratification effect modification (PSEM) methods). However,\navailable PSEM methods make untestable structural risk assumptions, such that\nassumption-lean versions of PSEM methods are needed in order to surpass the\nhigh bar of evidence to demonstrate a qualitative interaction. Fortunately, the\nsurvivor average causal effect (SACE) literature is replete with\nassumption-lean methods that can be readily adapted to the PSEM application for\nthe special case of a binary intermediate response variable. We map this\nadaptation, opening up a host of new PSEM methods for a binary intermediate\nvariable measured via two-phase sampling, for a dichotomous or failure time\nfinal outcome and including or excluding the SACE monotonicity assumption. The\nnew methods support that the vaccine partially protected vaccine recipients\nwith a high polyfunctional CD8+ T cell response, an important new insight for\nthe HIV vaccine field.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 14:42:11 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Gilbert", "Peter B.", ""], ["Blette", "Bryan S.", ""], ["Shepherd", "Bryan E.", ""], ["Hudgens", "Michael G.", ""]]}, {"id": "1811.03939", "submitter": "Konstantin Klemmer", "authors": "Konstantin Klemmer, Daniel B. Neill, Stephen A. Jarvis", "title": "Modeling Rape Reporting Delays Using Spatial, Temporal and Social\n  Features", "comments": "Workshop on Modeling and Decision-Making in the Spatiotemporal\n  Domain, 32nd Conference on Neural Information Processing Systems (NeurIPS\n  2018), Montreal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to estimate the delay observed between the\noccurrence and reporting of rape crimes. We explore spatial, temporal and\nsocial effects in sparse aggregated (area-level) and high-dimensional\ndisaggregated (event-level) data for New York and Los Angeles. Focusing on\ninference, we apply Gradient Boosting and Random Forests to assess predictor\nimportance, as well as Gaussian Processes to model spatial disparities in\nreporting times. Our results highlight differences and similarities between the\ntwo cities. We identify at-risk populations and communities which may be\ntargeted with focused policies and interventions to support rape victims,\napprehend perpetrators, and prevent future crimes.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 14:58:07 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 17:56:47 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Klemmer", "Konstantin", ""], ["Neill", "Daniel B.", ""], ["Jarvis", "Stephen A.", ""]]}, {"id": "1811.04028", "submitter": "Philipp Blandfort", "authors": "Philipp Blandfort, J\\\"orn Hees, Desmond U. Patton", "title": "An Overview of Computational Approaches for Interpretation Analysis", "comments": "Preprint submitted to Digital Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is said that beauty is in the eye of the beholder. But how exactly can we\ncharacterize such discrepancies in interpretation? For example, are there any\nspecific features of an image that makes person A regard an image as beautiful\nwhile person B finds the same image displeasing? Such questions ultimately aim\nat explaining our individual ways of interpretation, an intention that has been\nof fundamental importance to the social sciences from the beginning. More\nrecently, advances in computer science brought up two related questions: First,\ncan computational tools be adopted for analyzing ways of interpretation?\nSecond, what if the \"beholder\" is a computer model, i.e., how can we explain a\ncomputer model's point of view? Numerous efforts have been made regarding both\nof these points, while many existing approaches focus on particular aspects and\nare still rather separate. With this paper, in order to connect these\napproaches we introduce a theoretical framework for analyzing interpretation,\nwhich is applicable to interpretation of both human beings and computer models.\nWe give an overview of relevant computational approaches from various fields,\nand discuss the most common and promising application areas. The focus of this\npaper lies on interpretation of text and image data, while many of the\npresented approaches are applicable to other types of data as well.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 17:25:25 GMT"}, {"version": "v2", "created": "Wed, 22 May 2019 09:14:59 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Blandfort", "Philipp", ""], ["Hees", "J\u00f6rn", ""], ["Patton", "Desmond U.", ""]]}, {"id": "1811.04144", "submitter": "Javier Linkolk L\\'opez Gonzales JLlg", "authors": "Javier L. L. Gonzales, Rodrigo F. Calili, Reinaldo C. Souza, Felipe L.\n  Coelho da Silva", "title": "Simulation of the energy efficiency auction prices in Brazil", "comments": "The paper was presented at the International Conference on Renewable\n  Energies and Power Quality (ICREPQ), ICREPQ'16-Spain. Link:\n  http://www.icrepq.com/16-396.htm", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The electricity consumption behavior in Brazil has been extensively\ninvestigated over the years due to financial and social problems. In this\ncontext, it is important to simulate the energy prices of the energy efficiency\nauctions in the Brazilian regulated environment. This paper presents an\napproach to generate samples of auction energy prices in energy efficiency\nmarket, using Markov chain Monte Carlo method, through the Metropolis-Hastings\nalgorithm. The obtained results show that this approach can be used to generate\nenergy price samples.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 21:45:30 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Gonzales", "Javier L. L.", ""], ["Calili", "Rodrigo F.", ""], ["Souza", "Reinaldo C.", ""], ["da Silva", "Felipe L. Coelho", ""]]}, {"id": "1811.04290", "submitter": "Haiyan Liu", "authors": "Haiyan Liu, Francesco Del Galdo, Jeanine Houwing-Duistermaat", "title": "Prediction and forecasting models based on patient's history and\n  biomarkers with application to Scleroderma disease", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at predicting lung function values based on patients\nhistorical lung function values and serum biomarkers in Scleroderma patients.\nThe progression of disease is measured by three lung function indexes (FVC,\nTLC, DLCO). Values of four biomarkers (TIMP1, P3NP, HA, NT-proBNP) are\navailable. The data are sparse (6 months intervals) and irregular (many visits\nare missed). We consider two modeling approaches to achieve our goal, namely,\nthe mixed effects model which is the standard approach in epidemiological\nstudies and the functional principal component analysis model which is\ntypically used for dense temporal datasets. We find that functional data\nmethodology was able to recover the trajectories of three lung function indexes\nand to predict the future values very well.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 18:28:20 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Liu", "Haiyan", ""], ["Del Galdo", "Francesco", ""], ["Houwing-Duistermaat", "Jeanine", ""]]}, {"id": "1811.04295", "submitter": "Yixiu Kong", "authors": "Rui-jie Wu, Yi-Xiu Kong, Gui-yuan Shi and Yi-Cheng Zhang", "title": "Using NonBacktracking Expansion to Analyze k-core Pruning Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph math-ph math.MP physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We induce the NonBacktracking Expansion Branch method to analyze the k-core\npruning process on the monopartite graph G which does not contain any self-loop\nor multi-edge. Different from the traditional approaches like the generating\nfunctions or the degree distribution evolution equations which are\nmathematically difficult to solve, this method provides a simple and intuitive\nsolution of the k-core pruning process. Besides, this method can be naturally\nextended to study the k-core pruning process on correlated networks, which is\namong the few attempts to analytically solve the problem.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 19:19:46 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Wu", "Rui-jie", ""], ["Kong", "Yi-Xiu", ""], ["Shi", "Gui-yuan", ""], ["Zhang", "Yi-Cheng", ""]]}, {"id": "1811.04446", "submitter": "Niels Olsen", "authors": "Niels Lundtorp Olsen, Pascal Herren, Bo Markussen, Annette Bruun\n  Jensen, J{\\o}rgen Eilenberg", "title": "Statistical modelling of conidial discharge of entomophthoralean fungi\n  using a newly discovered Pandora species", "comments": "23 pages including supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entomophthoralean fungi are insect pathogenic fungi and are characterized by\ntheir active discharge of infective conidia that infect insects. Our aim was to\nstudy the effects of temperature on the discharge and to characterize the\nvariation in the associated temporal pattern of a newly discovered Pandora\nspecies with focus on peak location and shape of the discharge. Mycelia were\nincubated at various temperatures in darkness, and conidial discharge was\nmeasured over time. We used a novel modification of a statistical model\n(pavpop), that simultaneously estimates phase and amplitude effects, into a\nsetting of generalized linear models. This model is used to test hypotheses of\npeak location and discharge of conidia. The statistical analysis showed that\nhigh temperature leads to an early and fast decreasing peak, whereas there were\nno significant differences in total number of discharged conidia. Using the\nproposed model we also quantified the biological variation in the timing of the\npeak location at a fixed temperature.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 18:48:05 GMT"}, {"version": "v2", "created": "Sun, 21 Apr 2019 20:39:18 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Olsen", "Niels Lundtorp", ""], ["Herren", "Pascal", ""], ["Markussen", "Bo", ""], ["Jensen", "Annette Bruun", ""], ["Eilenberg", "J\u00f8rgen", ""]]}, {"id": "1811.04653", "submitter": "Johan Falkenjack", "authors": "Johan Falkenjack, Mattias Villani, and Arne J\\\"onsson", "title": "Modeling Text Complexity using a Multi-Scale Probit", "comments": "21 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel model for text complexity analysis which can be fitted to\nordered categorical data measured on multiple scales, e.g. a corpus with binary\nresponses mixed with a corpus with more than two ordered outcomes. The multiple\nscales are assumed to be driven by the same underlying latent variable\ndescribing the complexity of the text. We propose an easily implemented Gibbs\nsampler to sample from the posterior distribution by a direct extension of\nestablished data augmentation schemes. By being able to combine multiple\ncorpora with different annotation schemes we can get around the common problem\nof having more text features than annotated documents, i.e. an example of the\n$p>n$ problem. The predictive performance of the model is evaluated using both\nsimulated and real world readability data with very promising results.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 10:51:12 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Falkenjack", "Johan", ""], ["Villani", "Mattias", ""], ["J\u00f6nsson", "Arne", ""]]}, {"id": "1811.04881", "submitter": "Aureli Alabert", "authors": "Aureli Alabert and Merc\\`e Farr\\'e", "title": "The doctrinal paradox: ROC analysis in a probabilistic framework", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The doctrinal paradox is analysed from a probabilistic point of view assuming\na simple parametric model for the committee's behaviour. The well known\nissue-by-issue and case-by-case majority rules are compared in this model, by\nmeans of the concepts of false positive rate (FPR), false negative rate (FNR)\nand Receiver Operating Characteristics (ROC) space. We introduce also a new\nrule that we call path-by-path, which is somehow halfway between the other two.\nUnder our model assumptions, the issue-by-issue rule is shown to be the best of\nthe three according to an optimality criterion based in ROC maps, for all\nvalues of the model parameters (committee size and competence of its members),\nwhen equal weight is given to FPR an FNR. For unequal weights, the relative\ngoodness of the rules depends on the values of the competence and the weights,\nin a way which is precisely described. The results are illustrated with some\nnumerical examples.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 17:56:56 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Alabert", "Aureli", ""], ["Farr\u00e9", "Merc\u00e8", ""]]}, {"id": "1811.04987", "submitter": "Rui Wang", "authors": "Sicheng Hao, Rui Wang, Yu Zhang, Hui Zhan", "title": "Prediction of Alzheimer's disease-associated genes by integration of\n  GWAS summary data and expression data", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alzheimer's disease is the most common cause of dementia. It is the\nfifth-leading cause of death among elderly people. With high genetic\nheritability (79%), finding disease causal genes is a crucial step in find\ntreatment for AD. Following the International Genomics of Alzheimer's Project\n(IGAP), many disease-associated genes have been identified; however, we don't\nhave enough knowledge about how those disease-associated genes affect gene\nexpression and disease-related pathways. We integrated GWAS summary data from\nIGAP and five different expression level data by using TWAS method and\nidentified 15 disease causal genes under strict multiple testing (alpha<0.05),\n4 genes are newly identified; identified additional 29 potential disease causal\ngenes under false discovery rate(alpha < 0.05), 21 of them are newly\nidentified. Many genes we identified are also associated with some autoimmune\ndisorder.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 20:14:04 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Hao", "Sicheng", ""], ["Wang", "Rui", ""], ["Zhang", "Yu", ""], ["Zhan", "Hui", ""]]}, {"id": "1811.05063", "submitter": "Lewis Mitchell", "authors": "Peter Mathews, Caitlin Gray, Lewis Mitchell, Giang T. Nguyen, Nigel\n  G.Bean", "title": "SMERC: Social media event response clustering using textual and temporal\n  information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tweet clustering for event detection is a powerful modern method to automate\nthe real-time detection of events. In this work we present a new tweet\nclustering approach, using a probabilistic approach to incorporate temporal\ninformation. By analysing the distribution of time gaps between tweets we show\nthat the gaps between pairs of related tweets exhibit exponential decay,\nwhereas the gaps between unrelated tweets are approximately uniform. Guided by\nthis insight, we use probabilistic arguments to estimate the likelihood that a\npair of tweets are related, and build an improved clustering method. Our method\nSocial Media Event Response Clustering (SMERC) creates clusters of tweets based\non their tendency to be related to a single event. We evaluate our method at\nthree levels: through traditional event prediction from tweet clustering, by\nmeasuring the improvement in quality of clusters created, and also comparing\nthe clustering precision and recall with other methods. By applying SMERC to\ntweets collected during a number of sporting events, we demonstrate that\nincorporating temporal information leads to state of the art clustering\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 01:58:36 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Mathews", "Peter", ""], ["Gray", "Caitlin", ""], ["Mitchell", "Lewis", ""], ["Nguyen", "Giang T.", ""], ["Bean", "Nigel G.", ""]]}, {"id": "1811.05405", "submitter": "Priyam Das", "authors": "Priyam Das, Christine Peterson, Kim-Anh Do, Rehan Akbani, Veerabhadran\n  Baladandayuthapani", "title": "NExUS: Bayesian simultaneous network estimation across unequal sample\n  sizes", "comments": "8 pages, 8 figues", "journal-ref": null, "doi": "10.1093/bioinformatics/btz636", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network-based analyses of high-throughput genomics data provide a holistic,\nsystems-level understanding of various biological mechanisms for a common\npopulation. However, when estimating multiple networks across heterogeneous\nsub-populations, varying sample sizes pose a challenge in the estimation and\ninference, as network differences may be driven by differences in power. We are\nparticularly interested in addressing this challenge in the context of\nproteomic networks for related cancers, as the number of subjects available for\nrare cancer (sub-)types is often limited. We develop NExUS (Network Estimation\nacross Unequal Sample sizes), a Bayesian method that enables joint learning of\nmultiple networks while avoiding artefactual relationship between sample size\nand network sparsity. We demonstrate through simulations that NExUS outperforms\nexisting network estimation methods in this context, and apply it to learn\nnetwork similarity and shared pathway activity for groups of cancers with\nrelated origins represented in The Cancer Genome Atlas (TCGA) proteomic data.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 22:50:55 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Das", "Priyam", ""], ["Peterson", "Christine", ""], ["Do", "Kim-Anh", ""], ["Akbani", "Rehan", ""], ["Baladandayuthapani", "Veerabhadran", ""]]}, {"id": "1811.05561", "submitter": "Deovrat Kakde", "authors": "Deovrat Kakde, Arin Chaudhuri and Diana Shaw", "title": "A New SVDD-Based Multivariate Non-parametric Process Capability Index", "comments": null, "journal-ref": null, "doi": "10.1109/ICPHM.2018.8448517", "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process capability index (PCI) is a commonly used statistic to measure\nability of a process to operate within the given specifications or to produce\nproducts which meet the required quality specifications. PCI can be univariate\nor multivariate depending upon the number of process specifications or quality\ncharacteristics of interest. Most PCIs make distributional assumptions which\nare often unrealistic in practice.\n  This paper proposes a new multivariate non-parametric process capability\nindex. This index can be used when distribution of the process or quality\nparameters is either unknown or does not follow commonly used distributions\nsuch as multivariate normal.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 23:05:38 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Kakde", "Deovrat", ""], ["Chaudhuri", "Arin", ""], ["Shaw", "Diana", ""]]}, {"id": "1811.05566", "submitter": "Donatello Telesca", "authors": "Qian Li, John Shamshoian, Damla Senturk, Catherine Sugar, Shafali\n  Jeste, Charlotte DiStefano, Donatello Telesca", "title": "Region-Referenced Spectral Power Dynamics of EEG Signals: A Hierarchical\n  Modeling Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional brain imaging through electroencephalography (EEG) relies upon the\nanalysis and interpretation of high-dimensional, spatially organized time\nseries. We propose to represent time-localized frequency domain\ncharacterizations of EEG data as region-referenced functional data. This\nrepresentation is coupled with a hierarchical modeling approach to multivariate\nfunctional observations. Within this familiar setting, we discuss how several\nprior models relate to structural assumptions about multivariate covariance\noperators. An overarching modeling framework, based on infinite factorial\ndecompositions, is finally proposed to balance flexibility and efficiency in\nestimation. The motivating application stems from a study of implicit auditory\nlearning, in which typically developing (TD) children, and children with autism\nspectrum disorder (ASD) were exposed to a continuous speech stream. Using the\nproposed model, we examine differential band power dynamics as brain function\nis interrogated throughout the duration of a computer-controlled experiment.\nOur work offers a novel look at previous findings in psychiatry, and provides\nfurther insights into the understanding of ASD. Our approach to inference is\nfully Bayesian and implemented in a highly optimized Rcpp package.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 23:13:47 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 18:04:08 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Li", "Qian", ""], ["Shamshoian", "John", ""], ["Senturk", "Damla", ""], ["Sugar", "Catherine", ""], ["Jeste", "Shafali", ""], ["DiStefano", "Charlotte", ""], ["Telesca", "Donatello", ""]]}, {"id": "1811.05648", "submitter": "Vahid Tadayon", "authors": "Vahid Tadayon", "title": "Analysis of Gaussian Spatial Models with Covariate Measurement Error", "comments": "http://jss.irstat.ir/article-1-457-en.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty is an inherent characteristic of biological and geospatial data\nwhich is almost made by measurement error in the observed values of the\nquantity of interest. Ignoring measurement error can lead to biased estimates\nand inflated variances and so an inappropriate inference. In this paper, the\nGaussian spatial model is fitted based on covariate measurement error. For this\npurpose, we adopt the Bayesian approach and utilize the Markov chain Monte\nCarlo algorithms and data augmentations to carry out calculations. The\nmethodology is illustrated using simulated data.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 05:21:06 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2018 05:58:29 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Tadayon", "Vahid", ""]]}, {"id": "1811.05821", "submitter": "S\\'andor Baran", "authors": "S\\'andor Baran, Martin Leutbecher, Marianna Szab\\'o and Zied Ben\n  Bouall\\`egue", "title": "Statistical post-processing of dual-resolution ensemble forecasts", "comments": "25 pages, 12 figures, 2 tables", "journal-ref": "Quarterly Journal of the Royal Meteorological Society 145 (2019),\n  1705-1720", "doi": "10.1002/qj.3521", "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational cost as well as the probabilistic skill of ensemble\nforecasts depends on the spatial resolution of the numerical weather prediction\nmodel and the ensemble size. Periodically, e.g. when more computational\nresources become available, it is appropriate to reassess the balance between\nresolution and ensemble size. Recently, it has been proposed to investigate\nthis balance in the context of dual-resolution ensembles, which use members\nwith two different resolutions to make probabilistic forecasts. This study\ninvestigates whether statistical post-processing of such dual-resolution\nensemble forecasts changes the conclusions regarding the optimal\ndual-resolution configuration.\n  Medium-range dual-resolution ensemble forecasts of 2-metre temperature have\nbeen calibrated using ensemble model output statistics. The forecasts are\nproduced with ECMWF's Integrated Forecast System and have horizontal\nresolutions between 18 km and 45 km. The ensemble sizes range from 8 to 254\nmembers. The forecasts are verified with SYNOP station data. Results show that\nscore differences between various single and dual-resolution configurations are\nstrongly reduced by statistical post-processing. Therefore, the benefit of some\ndual-resolution configurations over single resolution configurations appears to\nbe less pronounced than for raw forecasts. Moreover, the ranking of the\nensemble configurations can be affected by the statistical post-processing.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 14:55:54 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Baran", "S\u00e1ndor", ""], ["Leutbecher", "Martin", ""], ["Szab\u00f3", "Marianna", ""], ["Bouall\u00e8gue", "Zied Ben", ""]]}, {"id": "1811.05975", "submitter": "Fredrik D. Johansson", "authors": "Fredrik D. Johansson", "title": "Machine Learning Analysis of Heterogeneity in the Effect of Student\n  Mindset Interventions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study heterogeneity in the effect of a mindset intervention on\nstudent-level performance through an observational dataset from the National\nStudy of Learning Mindsets (NSLM). Our analysis uses machine learning (ML) to\naddress the following associated problems: assessing treatment group overlap\nand covariate balance, imputing conditional average treatment effects, and\ninterpreting imputed effects. By comparing several different model families we\nillustrate the flexibility of both off-the-shelf and purpose-built estimators.\nWe find that the mindset intervention has a positive average effect of 0.26,\n95%-CI [0.22, 0.30], and that heterogeneity in the range of [0.1, 0.4] is\nmoderated by school-level achievement level, poverty concentration, urbanicity,\nand student prior expectations.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 13:43:39 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Johansson", "Fredrik D.", ""]]}, {"id": "1811.05997", "submitter": "Rachel Nethery", "authors": "Rachel C. Nethery, Yue Yang, Anna J. Brown, Francesca Dominici", "title": "A causal inference framework for cancer cluster investigations using\n  publicly available data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Often, a community becomes alarmed when high rates of cancer are noticed, and\nresidents suspect that the cancer cases could be caused by a known source of\nhazard. In response, the CDC recommends that departments of health perform a\nstandardized incidence ratio (SIR) analysis to determine whether the observed\ncancer incidence is higher than expected. This approach has several limitations\nthat are well documented in the literature. In this paper we propose a novel\ncausal inference approach to cancer cluster investigations, rooted in the\npotential outcomes framework. Assuming that a source of hazard representing a\npotential cause of increased cancer rates in the community is identified a\npriori, we introduce a new estimand called the causal SIR (cSIR). The cSIR is a\nratio defined as the expected cancer incidence in the exposed population\ndivided by the expected cancer incidence under the (counterfactual) scenario of\nno exposure. To estimate the cSIR we need to overcome two main challenges: 1)\nidentify unexposed populations that are as similar as possible to the exposed\none to inform estimation under the counterfactual scenario of no exposure, and\n2) make inference on cancer incidence in these unexposed populations using\npublicly available data that are often available at a much higher level of\nspatial aggregation than what is desired. We overcome the first challenge by\nrelying on matching. We overcome the second challenge by developing a Bayesian\nhierarchical model that borrows information from other sources to impute cancer\nincidence at the desired finer level of spatial aggregation. We apply our\nproposed approach to determine whether trichloroethylene vapor exposure has\ncaused increased cancer incidence in Endicott, NY.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 19:01:52 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2018 20:36:52 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Nethery", "Rachel C.", ""], ["Yang", "Yue", ""], ["Brown", "Anna J.", ""], ["Dominici", "Francesca", ""]]}, {"id": "1811.06534", "submitter": "Edouard Fournier", "authors": "Edouard Fournier (ENAC, IMT), St\\'ephane Grihon, Christian Bes (ICA),\n  Thierry Klein (LSP, ENAC, IMT)", "title": "Prediction of Preliminary Maximum Wing Bending Moments under Discrete\n  Gust", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many methodologies have been proposed to quickly identify among a very large\nnumber of flight conditions and maneuvers (i.e., steady, quasi-steady and\nunsteady loads cases) the ones which give the worst values for structural\nsizing (e.g., bending moments, shear forces, torques,...). All of these methods\nuse both the simulation model of the aircraft under development and efficient\nalgorithms to find out the critical points of the flight envelope. At the\npreliminary structural design phases detailed models are not available and\nairframe's loads are estimated by empirical relationships or engineering\njudgments. These approximations can induce load uncertainties and may lead to\nexpensive redesign activities through the upcoming detailed sizing process. In\nthe context of preliminary design phase for a weight aircraft variant without\ngeometric change, to overcome this likely drawback, we propose a method based\non the huge and reliable database of an initial aircraft from which the weight\nvariant belongs. More precisely, from the load cases of this initial database,\nresponse surfaces are identified as functions of preliminary parameters (flight\nconditions and structural parameters). Then, these response surfaces are used\nto predict quickly the weight aircraft variant quantities of interest for\npreliminary structural design studies. Although the proposed method can be\nreadily extended to any structural quantity of interest and to any flight\nconditions and maneuvers, it is presented here for the prediction of the\nbending moments due to discrete gust at different locations along a wing span.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 09:09:22 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Fournier", "Edouard", "", "ENAC, IMT"], ["Grihon", "St\u00e9phane", "", "ICA"], ["Bes", "Christian", "", "ICA"], ["Klein", "Thierry", "", "LSP, ENAC, IMT"]]}, {"id": "1811.06687", "submitter": "Yaniv Romano", "authors": "Yaniv Romano, Matteo Sesia, Emmanuel J. Cand\\`es", "title": "Deep Knockoffs", "comments": "37 pages, 23 figures, 1 table", "journal-ref": "J. Am. Stat. Assoc., Volume 0, Issue 0, 17 Oct 2019, Pages 1-12", "doi": "10.1080/01621459.2019.1660174", "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a machine for sampling approximate model-X knockoffs\nfor arbitrary and unspecified data distributions using deep generative models.\nThe main idea is to iteratively refine a knockoff sampling mechanism until a\ncriterion measuring the validity of the produced knockoffs is optimized; this\ncriterion is inspired by the popular maximum mean discrepancy in machine\nlearning and can be thought of as measuring the distance to pairwise\nexchangeability between original and knockoff features. By building upon the\nexisting model-X framework, we thus obtain a flexible and model-free\nstatistical tool to perform controlled variable selection. Extensive numerical\nexperiments and quantitative tests confirm the generality, effectiveness, and\npower of our deep knockoff machines. Finally, we apply this new method to a\nreal study of mutations linked to changes in drug resistance in the human\nimmunodeficiency virus.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 06:26:33 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Romano", "Yaniv", ""], ["Sesia", "Matteo", ""], ["Cand\u00e8s", "Emmanuel J.", ""]]}, {"id": "1811.06692", "submitter": "Changho Shin", "authors": "Changho Shin, Sunghwan Joo, Jaeryun Yim, Hyoseop Lee, Taesup Moon,\n  Wonjong Rhee", "title": "Subtask Gated Networks for Non-Intrusive Load Monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-intrusive load monitoring (NILM), also known as energy disaggregation, is\na blind source separation problem where a household's aggregate electricity\nconsumption is broken down into electricity usages of individual appliances. In\nthis way, the cost and trouble of installing many measurement devices over\nnumerous household appliances can be avoided, and only one device needs to be\ninstalled. The problem has been well-known since Hart's seminal paper in 1992,\nand recently significant performance improvements have been achieved by\nadopting deep networks. In this work, we focus on the idea that appliances have\non/off states, and develop a deep network for further performance improvements.\nSpecifically, we propose a subtask gated network that combines the main\nregression network with an on/off classification subtask network. Unlike\ntypical multitask learning algorithms where multiple tasks simply share the\nnetwork parameters to take advantage of the relevance among tasks, the subtask\ngated network multiply the main network's regression output with the subtask's\nclassification probability. When standby-power is additionally learned, the\nproposed solution surpasses the state-of-the-art performance for most of the\nbenchmark cases. The subtask gated network can be very effective for any\nproblem that inherently has on/off states.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 07:38:48 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Shin", "Changho", ""], ["Joo", "Sunghwan", ""], ["Yim", "Jaeryun", ""], ["Lee", "Hyoseop", ""], ["Moon", "Taesup", ""], ["Rhee", "Wonjong", ""]]}, {"id": "1811.06766", "submitter": "Arman Hassanniakalager", "authors": "Georgios Sermpinis, Arman Hassanniakalager, Charalampos Stasinakis,\n  Ioannis Psaradellis", "title": "Technical Analysis and Discrete False Discovery Rate: Evidence from MSCI\n  Indices", "comments": "72 pages, 2 figues, 14 (main) and 13 (appendix) tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the performance of dynamic portfolios constructed using more\nthan 21,000 technical trading rules on 12 categorical and country-specific\nmarkets over the 2004-2015 study period, on rolling forward structures of\ndifferent lengths. We also introduce a discrete false discovery rate (DFRD+/-)\nmethod for controlling data snooping bias. Compared to the existing methods,\nDFRD+/- is adaptive and more powerful, and accommodates for discrete p-values.\nThe profitability, persistence and robustness of the technical rules are\nexamined. Technical analysis still has short-term value in advanced, emerging\nand frontier markets. Financial stress, the economic environment and market\ndevelopment seem to affect the performance of trading rules. A cross-validation\nexercise highlights the importance of frequent rebalancing and the variability\nof profitability in trading with technical analysis.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 11:41:39 GMT"}, {"version": "v2", "created": "Thu, 13 Jun 2019 13:40:00 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Sermpinis", "Georgios", ""], ["Hassanniakalager", "Arman", ""], ["Stasinakis", "Charalampos", ""], ["Psaradellis", "Ioannis", ""]]}, {"id": "1811.06782", "submitter": "Anne G\\'egout-Petit", "authors": "Anne G\\'egout-Petit and Lucia Gu\\'erin-Dubrana and Shuxian Li", "title": "A new centered spatio-temporal autologistic regression model.\n  Application to spatio-temporal analysis of esca disease in a vineyard", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new centered autologistic spatio-temporal model for binary data\non a lattice. The centering allows the interpretation of the autoregression\ncoefficients in separating the large scale structure of the model corresponding\nto an expected mean and the small-scale structure corresponding to the\nauto-correlation. We discuss the existence of the joint law of the process and\nshow by simulation the interest of this kind of centering. We propose and show\nthe efficiency of the maximum pseudo-likelihood estimator and also a method to\nchoose the best structure of neighborhood. Method is applied to model and fit\nepidemiological data about Esca disease on a vineyard of the Bordeaux region.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 12:35:30 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["G\u00e9gout-Petit", "Anne", ""], ["Gu\u00e9rin-Dubrana", "Lucia", ""], ["Li", "Shuxian", ""]]}, {"id": "1811.06819", "submitter": "Carlos Caminha Neto", "authors": "Carlos Caminha", "title": "Descoberta de rela\\c{c}\\~oes alom\\'etricas entre popula\\c{c}\\~ao e crime\n  dentro de uma grande metr\\'opole", "comments": "114 pages, in Portuguese, 31 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently humanity has just crossed an important landmark in its history with\nthe majority of people now living in large cities. This population\nconcentration is capable of boosting the growth of positive indicators such as\ninnovation, the production of new patents and supercreative employment, but\nincreases the spread of diseases and the occurrence of crimes. Faced with the\nrealization that crime rates grow year after year in these large urban centers,\nwe sought to understand the dynamics of crime within cities. We investigate at\nthe subscale of the neighborhoods of a highly populated city the incidence of\nproperty crimes in terms of both the resident and the floating population. Our\nresults show that a relevant allometric relation could only be observed between\nproperty crimes and floating population. More precisely, the evidence of a\nsuperlinear behavior indicates that a disproportional number of property crimes\noccurs in regions where an increased flow of people takes place in the city.\nFor comparison, we also found that the number of crimes of peace disturbance\nonly correlates well, and in a superlinear fashion too, with the resident\npopulation. Our study raises the interesting possibility that the\nsuperlinearity observed in previous studies [Bettencourt et al., Proc. Natl.\nAcad. Sci. USA 104, 7301 (2007) and Melo et al., Sci. Rep. 4, 6239 (2014)] for\nhomicides versus population at the city scale could have its origin in the fact\nthat the floating population, and not the resident one, should be taken as the\nrelevant variable determining the intrinsic microdynamical behavior of the\nsystem. This finding was the motivation for the codification of a framework\nthat supports the analysis of population and crime data to propose city\ndivisions that allow the allocation of police by floating population and\nresident population statistics.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 18:24:29 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Caminha", "Carlos", ""]]}, {"id": "1811.06856", "submitter": "Joshua Rapp", "authors": "Joshua Rapp, Robin M. A. Dawson, Vivek K Goyal", "title": "Estimation from Quantized Gaussian Measurements: When and How to Use\n  Dither", "comments": "Revision with added references, figures, and appendices", "journal-ref": null, "doi": "10.1109/TSP.2019.2916046", "report-no": null, "categories": "stat.AP eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subtractive dither is a powerful method for removing the signal dependence of\nquantization noise for coarsely-quantized signals. However, estimation from\ndithered measurements often naively applies the sample mean or midrange, even\nwhen the total noise is not well described with a Gaussian or uniform\ndistribution. We show that the generalized Gaussian distribution approximately\ndescribes subtractively-dithered, quantized samples of a Gaussian signal.\nFurthermore, a generalized Gaussian fit leads to simple estimators based on\norder statistics that match the performance of more complicated maximum\nlikelihood estimators requiring iterative solvers. The order statistics-based\nestimators outperform both the sample mean and midrange for nontrivial sums of\nGaussian and uniform noise. Additional analysis of the generalized Gaussian\napproximation yields rules of thumb for determining when and how to apply\ndither to quantized measurements. Specifically, we find subtractive dither to\nbe beneficial when the ratio between the Gaussian standard deviation and\nquantization interval length is roughly less than 1/3. If that ratio is also\ngreater than 0.822/$K^{0.930}$ for the number of measurements $K>20$, we\npresent estimators more efficient than the midrange.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 15:28:41 GMT"}, {"version": "v2", "created": "Fri, 22 Feb 2019 18:41:56 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Rapp", "Joshua", ""], ["Dawson", "Robin M. A.", ""], ["Goyal", "Vivek K", ""]]}, {"id": "1811.07058", "submitter": "William Long", "authors": "William Long, Joe Zhou, Zhirui Hu, Yuntian Deng", "title": "Statistical Impact of New York Health Legislation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  As the US Government plays an increasing role in health care, it becomes\nessential to understand the impact of expensive legislation on actual outcomes.\nNew York, having spent the last decade heavily legislating health-related\nbehavior, represents a unique test case to gain insight about what factors\ncause health care legislation to succeed or fail. We present a longitudinal\nstudy comparing bills across 13 Health Areas to measure the effect legislation\nin that Area had on 311 hotline service complaints. We find that there is\nstatistically significant evidence with p-value $p=0.05$ that legislation in\nthe Hazardous Materials Health Area correlated with a positive change in\noutcomes. The other Health Areas correlated with changes, but were not\nstatistically significant.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 23:02:04 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Long", "William", ""], ["Zhou", "Joe", ""], ["Hu", "Zhirui", ""], ["Deng", "Yuntian", ""]]}, {"id": "1811.07259", "submitter": "Jun Hee Kim", "authors": "Jun Hee Kim", "title": "Modeling Baseball Outcomes as Higher-Order Markov Chains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Baseball is one of the few sports in which each team plays a game nearly\neveryday. For instance, in the baseball league in South Korea, namely the KBO\n(Korea Baseball Organization) league, every team has a game everyday except for\nMondays. This consecutiveness of the KBO league schedule could make a team's\nmatch outcome be associated to the results of recent games. This paper deals\nwith modeling the match outcomes of each of the ten teams in the KBO league as\na higher-order Markov chain, where the possible states are win ($\"W\"$), draw\n($\"D\"$), and loss ($\"L\"$). For each team, the value of $k$ in which the\n$k^{\\text{th}}$ order Markov chain model best describes the match outcome\nsequence is computed. Further, whether there are any patterns between such a\nvalue of k and the team's overall performance in the league is examined. We\nfind that for the top three teams in the league, lower values of $k$ tend to\nhave the $k^{th}$ order Markov chain to better model their outcome, but the\nother teams don't reveal such patterns.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 02:38:27 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Kim", "Jun Hee", ""]]}, {"id": "1811.07625", "submitter": "Christos Merkatas", "authors": "Spyridon J. Hatjispyros, Christos Merkatas", "title": "Joint reconstruction and prediction of random dynamical systems under\n  borrowing of strength", "comments": null, "journal-ref": null, "doi": "10.1063/1.5054656", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian nonparametric model based on Markov Chain Monte Carlo\n(MCMC) methods for the joint reconstruction and prediction of discrete time\nstochastic dynamical systems, based on $m$-multiple time-series data, perturbed\nby additive dynamical noise. We introduce the Pairwise Dependent Geometric\nStick-Breaking Reconstruction (PD-GSBR) model, which relies on the construction\nof a $m$-variate nonparametric prior over the space of densities supported over\n$\\mathbb{R}^m$. We are focusing in the case where at least one of the\ntime-series has a sufficiently large sample size representation for an\nindependent and accurate Geometric Stick-Breaking estimation, as defined in\nMerkatas et al. (2017). Our contention, is that whenever the dynamical error\nprocesses perturbing the underlying dynamical systems share common\ncharacteristics, underrepresented data sets can benefit in terms of model\nestimation accuracy. The PD-GSBR estimation and prediction procedure is\ndemonstrated specifically in the case of maps with polynomial nonlinearities of\nan arbitrary degree. Simulations based on synthetic time-series are presented.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 11:34:27 GMT"}, {"version": "v2", "created": "Sat, 16 Feb 2019 07:05:32 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Hatjispyros", "Spyridon J.", ""], ["Merkatas", "Christos", ""]]}, {"id": "1811.07740", "submitter": "Timon Elmer", "authors": "Timon Elmer, Christoph Stadtfeld", "title": "Social interaction networks and depressive symptoms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face-to-face social interactions are an important aspect of peoples' social\nlives. A lack of interactions can explain how individuals develop depressive\nsymptoms, but depressive symptoms can also explain how individuals engage in\nsocial interactions. Understanding in detail how depression affects\nindividuals' social interaction networks is important to break this vicious\ncycle of social isolation and depression. This article tackles two central\nmethodological challenges in understanding the micro-level mechanisms between\ndepressive symptoms and social interactions. The first contribution is the\napplication of novel data collection strategies that employ RFID sensors to\nrecord behavioral data on interpersonal interaction, which we combine with\nself-reported depressive symptoms and sociometric data on friendship relations.\nThe second contribution is the analysis of these data with statistical social\nnetwork methodology that allows testing hypotheses on the duration of\ninteractions between pairs of individuals. With this unique approach, we test\nfour social network hypotheses in an empirical setting of two first-year\nundergraduate student cohorts (N(pairs) = 2,454, N(individuals) = 123) spending\na weekend together in a remote camp house. We conclude that depressive symptoms\nare associated with (1) spending less time in social interaction, (2) spending\ntime with similarly depressed others, (3) spending time in pair-wise\ninteractions rather than group interactions, and with (4) spending more time\nwith reciprocal friends (but not with unilaterally perceived friends). Our\nfindings offer new insights into social consequences of depressive symptoms and\ncall for the development of social network-oriented intervention strategies to\nprevent depressed individuals from being socially isolated.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 14:58:12 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Elmer", "Timon", ""], ["Stadtfeld", "Christoph", ""]]}, {"id": "1811.07761", "submitter": "Maciej Pawlikowski", "authors": "Maciej Pawlikowski, Agata Chorowska", "title": "Weighted Ensemble of Statistical Models", "comments": "10 pages + 7 pages of appendix, 4 tables, 7 figures. Preprint\n  submitted to International Journal of Forecasting", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a detailed description of our submission for the M4 forecasting\ncompetition, in which it ranked 3rd overall. Our solution utilizes several\ncommonly used statistical models, which are weighted according to their\nperformance on historical data. We cluster series within each type of frequency\nwith respect to the existence of trend and seasonality. Every class of series\nis assigned a different set of models to combine. Combination weights are\nchosen separately for each series. We conduct experiments with a holdout set to\nmanually pick pools of models that perform best for a given series type, as\nwell as to choose the combination approaches.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 15:53:51 GMT"}, {"version": "v2", "created": "Thu, 10 Jan 2019 15:38:39 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Pawlikowski", "Maciej", ""], ["Chorowska", "Agata", ""]]}, {"id": "1811.07827", "submitter": "Jong-Hyeon Jeong Prof.", "authors": "Jong-Hyeon Jeong", "title": "Domain of Inverse Double Arcsine Transformation", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To combine the proportions from different studies for meta-analysis, Freeman\nand Tukey double arcsine tranformation can be useful for normalization and\nvariance stabilization. The inverse function of the double arcsine\ntransformation has been also derived in the literature to recover the original\nscale of the proportion after aggregation. In this brief note, we present the\ndomain and range of the inverse double arcsine transformation both analytically\nand graphically. We notice an erratic behavior in the mathematical formula for\nthe inverse double arcsine tranformation at both limits of its domain, and\npropose approximation methods for both small and large samples. We also propose\na simple accuracy measure, the maximum percent error (MPE), of the large sample\napproximation, which can be used to determine the sample size that would\nprovide a certain accuracy level, and conversely to determine the accuracy\nlevel of the approximation given a sample size.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 17:38:56 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Jeong", "Jong-Hyeon", ""]]}, {"id": "1811.07867", "submitter": "Shira Mitchell", "authors": "Shira Mitchell, Eric Potash, Solon Barocas, Alexander D'Amour,\n  Kristian Lum", "title": "Prediction-Based Decisions and Fairness: A Catalogue of Choices,\n  Assumptions, and Definitions", "comments": null, "journal-ref": "Annual Review of Statistics and Its Application 2021 8:1", "doi": "10.1146/annurev-statistics-042720-125902", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent flurry of research activity has attempted to quantitatively define\n\"fairness\" for decisions based on statistical and machine learning (ML)\npredictions. The rapid growth of this new field has led to wildly inconsistent\nterminology and notation, presenting a serious challenge for cataloguing and\ncomparing definitions. This paper attempts to bring much-needed order.\n  First, we explicate the various choices and assumptions made---often\nimplicitly---to justify the use of prediction-based decisions. Next, we show\nhow such choices and assumptions can raise concerns about fairness and we\npresent a notationally consistent catalogue of fairness definitions from the ML\nliterature. In doing so, we offer a concise reference for thinking through the\nchoices, assumptions, and fairness considerations of prediction-based decision\nsystems.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 18:40:53 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2019 16:47:20 GMT"}, {"version": "v3", "created": "Fri, 24 Apr 2020 20:06:41 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Mitchell", "Shira", ""], ["Potash", "Eric", ""], ["Barocas", "Solon", ""], ["D'Amour", "Alexander", ""], ["Lum", "Kristian", ""]]}, {"id": "1811.07897", "submitter": "Miki Verma", "authors": "Miki E. Verma, Robert A. Bridges, Samuel C. Hollifield", "title": "ACTT: Automotive CAN Tokenization and Translation", "comments": "5th Annual Conference on Computational Science & Computational\n  Intelligence (CSCI'18)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern vehicles contain scores of Electrical Control Units (ECUs) that\nbroadcast messages over a Controller Area Network (CAN). Vehicle manufacturers\nrely on security through obscurity by concealing their unique mapping of CAN\nmessages to vehicle functions which differs for each make, model, year, and\neven trim. This poses a major obstacle for after-market modifications notably\nperformance tuning and in-vehicle network security measures. We present ACTT:\nAutomotive CAN Tokenization and Translation, a novel, vehicle-agnostic,\nalgorithm that leverages available diagnostic information to parse CAN data\ninto meaningful messages, simultaneously cutting binary messages into tokens,\nand learning the translation to map these contiguous bits to the value of the\nvehicle function communicated.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 17:02:16 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Verma", "Miki E.", ""], ["Bridges", "Robert A.", ""], ["Hollifield", "Samuel C.", ""]]}, {"id": "1811.08102", "submitter": "Nora Speicher", "authors": "Nora K. Speicher and Nico Pfeifer", "title": "An interpretable multiple kernel learning approach for the discovery of\n  integrative cancer subtypes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to the complexity of cancer, clustering algorithms have been used to\ndisentangle the observed heterogeneity and identify cancer subtypes that can be\ntreated specifically. While kernel based clustering approaches allow the use of\nmore than one input matrix, which is an important factor when considering a\nmultidimensional disease like cancer, the clustering results remain hard to\nevaluate and, in many cases, it is unclear which piece of information had which\nimpact on the final result. In this paper, we propose an extension of multiple\nkernel learning clustering that enables the characterization of each identified\npatient cluster based on the features that had the highest impact on the\nresult. To this end, we combine feature clustering with multiple kernel\ndimensionality reduction and introduce FIPPA, a score which measures the\nfeature cluster impact on a patient cluster. Results: We applied the approach\nto different cancer types described by four different data types with the aim\nof identifying integrative patient subtypes and understanding which features\nwere most important for their identification. Our results show that our method\ndoes not only have state-of-the-art performance according to standard measures\n(e.g., survival analysis), but, based on the high impact features, it also\nproduces meaningful explanations for the molecular bases of the subtypes. This\ncould provide an important step in the validation of potential cancer subtypes\nand enable the formulation of new hypotheses concerning individual patient\ngroups. Similar analysis are possible for other disease phenotypes.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 07:28:13 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Speicher", "Nora K.", ""], ["Pfeifer", "Nico", ""]]}, {"id": "1811.08472", "submitter": "Giri Gopalan", "authors": "Giri Gopalan, Birgir Hrafnkelsson, Christopher K. Wikle, H{\\aa}vard\n  Rue, Gu{\\dh}finna A{\\dh}algeirsd\\'ottir, Alexander H. Jarosch, Finnur\n  P\\'alsson", "title": "A Hierarchical Spatio-Temporal Statistical Model Motivated by Glaciology", "comments": "Revision accepted for publication by the Journal of Agricultural,\n  Biological, and Environmental Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we extend and analyze a Bayesian hierarchical spatio-temporal\nmodel for physical systems. A novelty is to model the discrepancy between the\noutput of a computer simulator for a physical process and the actual process\nvalues with a multivariate random walk. For computational efficiency, linear\nalgebra for bandwidth limited matrices is utilized, and first-order emulator\ninference allows for the fast emulation of a numerical partial differential\nequation (PDE) solver. A test scenario from a physical system motivated by\nglaciology is used to examine the speed and accuracy of the computational\nmethods used, in addition to the viability of modeling assumptions. We conclude\nby discussing how the model and associated methodology can be applied in other\nphysical contexts besides glaciology.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 20:29:44 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 23:16:07 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Gopalan", "Giri", ""], ["Hrafnkelsson", "Birgir", ""], ["Wikle", "Christopher K.", ""], ["Rue", "H\u00e5vard", ""], ["A\u00f0algeirsd\u00f3ttir", "Gu\u00f0finna", ""], ["Jarosch", "Alexander H.", ""], ["P\u00e1lsson", "Finnur", ""]]}, {"id": "1811.08524", "submitter": "Vivek Srikrishnan", "authors": "Vivek Srikrishnan and Klaus Keller", "title": "Small increases in agent-based model complexity can result in large\n  increases in required calibration data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Agent-based models (ABMs) are widely used to model coupled natural-human\nsystems. Descriptive models require careful calibration with observed data.\nHowever, ABMs are often not calibrated in a statistical sense. Here we examine\nthe impact of data record structure on the calibration of an ABM for housing\nabandonment in the presence of flood risk. Using a perfect model experiment, we\nexamine the impact of data record structures on (i) model calibration and (ii)\nthe ability to distinguish a model with inter-agent interactions from one\nwithout. We show how limited data sets may not constrain a model with just four\nparameters. This indicates that many ABMs may require informative prior\ndistributions to be descriptive. We also illustrate how spatially-aggregated\ndata can be insufficient to identify the correct model structure. This\nemphasizes the need for utilizing independent lines of evidence to select sound\nand informative priors.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 22:57:49 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 14:17:45 GMT"}, {"version": "v3", "created": "Wed, 30 Oct 2019 21:19:49 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Srikrishnan", "Vivek", ""], ["Keller", "Klaus", ""]]}, {"id": "1811.08604", "submitter": "Florian Ziel", "authors": "Christopher Kath, Florian Ziel", "title": "The value of forecasts: Quantifying the economic gains of accurate\n  quarter-hourly electricity price forecasts", "comments": null, "journal-ref": "Energy Economics, 76 (2018) 411-423", "doi": "10.1016/j.eneco.2018.10.005", "report-no": null, "categories": "q-fin.ST econ.EM q-fin.PM q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a multivariate elastic net regression forecast model for German\nquarter-hourly electricity spot markets. While the literature is diverse on\nday-ahead prediction approaches, both the intraday continuous and intraday\ncall-auction prices have not been studied intensively with a clear focus on\npredictive power. Besides electricity price forecasting, we check for the\nimpact of early day-ahead (DA) EXAA prices on intraday forecasts. Another\nnovelty of this paper is the complementary discussion of economic benefits. A\nprecise estimation is worthless if it cannot be utilized. We elaborate possible\ntrading decisions based upon our forecasting scheme and analyze their monetary\neffects. We find that even simple electricity trading strategies can lead to\nsubstantial economic impact if combined with a decent forecasting technique.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 06:05:38 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Kath", "Christopher", ""], ["Ziel", "Florian", ""]]}, {"id": "1811.08769", "submitter": "Yacouba Boubacar Mainassara", "authors": "Yacouba Boubacar Ma\\\"inassara (UFC), Othman Kadmiri, Bruno Saussereau\n  (LMB)", "title": "Portmanteau test for the asymmetric power GARCH model when the power is\n  unknown", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is now widely accepted that, to model the dynamics of daily financial\nreturns, volatility models have to incorporate the so-called leverage effect.\nWe derive the asymptotic behaviour of the squared residuals autocovariances for\nthe class of asymmetric power GARCH model when the power is unknown and is\njointly estimated with the model's parameters. We then deduce a portmanteau\nadequacy test based on the autocovariances of the squared residuals. These\nasymptotic results are illustrated by Monte Carlo experiments. An application\nto real financial data is also proposed.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 15:04:58 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Ma\u00efnassara", "Yacouba Boubacar", "", "UFC"], ["Kadmiri", "Othman", "", "LMB"], ["Saussereau", "Bruno", "", "LMB"]]}, {"id": "1811.08793", "submitter": "Zhicheng Chen", "authors": "Zhicheng Chen, Yuequan Bao, Hui Li and Billie F. Spencer Jr", "title": "LQD-RKHS-based distribution-to-distribution regression methodology for\n  restoring the probability distributions of missing SHM data", "comments": "This is a manuscript. Readers are suggested to read the formal\n  version published in the journal \"Mechanical Systems and Signal Processing\",\n  https://doi.org/10.1016/j.ymssp.2018.11.052", "journal-ref": "Mechanical Systems and Signal Processing 2019;121:655-674", "doi": "10.1016/j.ymssp.2018.11.052", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data loss is a critical problem in structural health monitoring (SHM).\nProbability distributions play a highly important role in many applications.\nImproving the quality of distribution estimations made using incomplete samples\nis highly important. Missing samples can be compensated for by applying\nconventional missing data restoration methods; however, ensuring that restored\nsamples roughly follow underlying distributions of true missing data remains a\nchallenge. Another strategy involves directly restoring the probability density\nfunction (PDF) for a sensor when samples are missing by leveraging distribution\ninformation from another sensor with complete data using distribution\nregression techniques; existing methods include the conventional\ndistribution-to-distribution regression (DDR) and distribution-to-warping\nfunction regression (DWR) methods. Due to constraints on PDFs and warping\nfunctions, the regression functions of both methods are estimated from the\nNadaraya-Watson kernel estimator with relatively low degrees of precision. This\narticle proposes a new indirect distribution-to-distribution regression method\nin the context of functional data analysis for restoring distributions of\nmissing SHM data. PDFs are transformed to ordinary functions residing in a\nHilbert space via the newly proposed log-quantile-density (LQD) transformation;\nthe regression for distributions is realized in the transformed space via a\nfunctional regression model constructed based on the theory of Reproducing\nKernel Hilbert Space (RKHS), corresponding result is subsequently mapped back\nto the density space through the inverse LQD transformation. Test results using\nfield monitoring data indicate that the new method significantly outperforms\nconventional methods in general cases; however, in extrapolation cases, the new\nmethod is inferior to the distribution-to-warping function regression method.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 15:44:54 GMT"}, {"version": "v2", "created": "Sat, 8 Dec 2018 08:52:03 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Chen", "Zhicheng", ""], ["Bao", "Yuequan", ""], ["Li", "Hui", ""], ["Spencer", "Billie F.", "Jr"]]}, {"id": "1811.09016", "submitter": "Takumi Suzuki", "authors": "Takumi Suzuki and Nakahiro Yoshida", "title": "Penalized least squares approximation methods and their applications to\n  stochastic processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct an objective function that consists of a quadratic approximation\nterm and a penalty term. Thanks to the quadratic approximation, we can deal\nwith various kinds of loss functions into a unified way, and by taking\nadvantage of the penalty term, we can simultaneously execute variable selection\nand parameter estimation. In this article, we show that our estimator has\noracle properties, and even better property. We also treat an stochastic\nprocesses as applications.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 04:07:14 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Suzuki", "Takumi", ""], ["Yoshida", "Nakahiro", ""]]}, {"id": "1811.09204", "submitter": "Dalia Chakrabarty Dr.", "authors": "Cedric Spire and Dalia Chakrabarty", "title": "Learning in the Absence of Training Data -- a Galactic Application", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are multiple real-world problems in which training data is unavailable,\nand still, the ambition is to learn values of the system parameters, at which\ntest data on an observable is realised, subsequent to the learning of the\nfunctional relationship between these variables. We present a novel Bayesian\nmethod to deal with such a problem, in which we learn a system function of a\nstationary dynamical system, for which only test data on a vector-valued\nobservable is available, and training data is unavailable. This exercise\nborrows heavily from the state space probability density function ($pdf$), that\nwe also learn. As there is no training data available for either sought\nfunction, we cannot learn its correlation structure, and instead, perform\ninference (using Metropolis-within-Gibbs), on the discretised form of the\nsought system function and of the ${pdf}$, where this $pdf$ is constructed such\nthat the unknown system parameters are embedded within its support. Likelihood\nof the unknowns given the available data, is defined in terms of such a\n${pdf}$. We make an application to the learning of the density of all\ngravitational matter in a real galaxy.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 15:17:42 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Spire", "Cedric", ""], ["Chakrabarty", "Dalia", ""]]}, {"id": "1811.09240", "submitter": "George Leckie", "authors": "George Leckie, Harvey Goldstein", "title": "Should we adjust for pupil background in school value-added models? A\n  study of Progress 8 and school accountability in England", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the UK, US and elsewhere, school accountability systems increasingly\ncompare schools using value-added measures of school performance derived from\npupil scores in high-stakes standardised tests. Rather than naively comparing\nschool average scores, which largely reflect school intake differences in prior\nattainment, these measures attempt to compare the average progress or\nimprovement pupils make during a year or phase of schooling. Schools, however,\nalso differ in terms of their pupil demographic and socioeconomic\ncharacteristics and these also predict why some schools subsequently score\nhigher than others. Many therefore argue that value-added measures unadjusted\nfor pupil background are biased in favour of schools with more 'educationally\nadvantaged' intakes. But, others worry that adjusting for pupil background\nentrenches socioeconomic inequities and excuses low performing schools. In this\narticle we explore these theoretical arguments and their practical importance\nin the context of the 'Progress 8' secondary school accountability system in\nEngland which has chosen to ignore pupil background. We reveal how the reported\nlow or high performance of many schools changes dramatically once adjustments\nare made for pupil background and these changes also affect the reported\ndifferential performances of region and of different school types. We conclude\nthat accountability systems which choose to ignore pupil background are likely\nto reward and punish the wrong schools and this will likely have detrimental\neffects on pupil learning. These findings, especially when coupled with more\ngeneral concerns surrounding high-stakes testing and school value-added models,\nraise serious doubts about their use in school accountability systems.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 17:12:32 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Leckie", "George", ""], ["Goldstein", "Harvey", ""]]}, {"id": "1811.09309", "submitter": "Mihnea Stefan Andrei", "authors": "Mihnea S. Andrei and John S.J. Hsu", "title": "Bayesian Alternatives to the Black-Litterman Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST q-fin.PM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Black-Litterman model combines investors' personal views with historical\ndata and gives optimal portfolio weights. In this paper we will introduce the\noriginal Black-Litterman model (section 1), we will modify the model such that\nit fits in a Bayesian framework by considering the investors' personal views to\nbe a direct prior on the means of the returns and by adding a typical Inverse\nWishart prior on the covariance matrix of the returns (section 2). Lastly, we\nwill use Leonard and Hsu's (1992) idea of adding a prior on the logarithm of\nthe covariance matrix (section 3). Sensitivity simulations for the level of\nconfidence that the investor has in their own personal views were performed and\nperformance of the models was assessed on a test data set consisting of returns\nover the month of January 2018.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 21:09:31 GMT"}, {"version": "v2", "created": "Mon, 26 Nov 2018 17:09:41 GMT"}, {"version": "v3", "created": "Tue, 25 Dec 2018 05:00:42 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Andrei", "Mihnea S.", ""], ["Hsu", "John S. J.", ""]]}, {"id": "1811.09433", "submitter": "Burak K\\\"ursad G\\\"unhan", "authors": "Burak K\\\"ursad G\\\"unhan, Sebastian Weber, Abdelkader Seroutou, Tim\n  Friede", "title": "A Bayesian time-to-event pharmacokinetic model for sequential phase I\n  dose-escalation trials with multiple schedules", "comments": null, "journal-ref": "BMC medical research methodology, 2021", "doi": "10.1186/s12874-021-01218-9", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phase I dose-escalation trials constitute the first step in investigating the\nsafety of potentially promising drugs in humans. Conventional methods for phase\nI dose-escalation trials are based on a single treatment schedule only. More\nrecently, however, multiple schedules are more frequently investigated in the\nsame trial. Here, we consider sequential phase I trials, where the trial\nproceeds with a new schedule (e.g. daily or weekly dosing) once the dose\nescalation with another schedule has been completed. The aim is to utilize the\ninformation from both the completed and the ongoing dose-escalation trial to\ninform decisions on the dose level for the next dose cohort. For this purpose,\nwe adapted the time-to-event pharmacokinetics (TITE-PK) model, which were\noriginally developed for simultaneous investigation of multiple schedules.\nTITE-PK integrates information from multiple schedules using a pharmacokinetics\n(PK) model. In a simulation study, the developed appraoch is compared to the\nbridging continual reassessment method and the Bayesian logistic regression\nmodel using a meta-analytic-prior. TITE-PK results in better performance than\ncomparators in terms of recommending acceptable dose and avoiding overly toxic\ndoses for sequential phase I trials in most of the scenarios considered.\nFurthermore, better performance of TITE-PK is achieved while requiring similar\nnumber of patients in the simulated trials. For the scenarios involving one\nschedule, TITE-PK displays similar performance with alternatives in terms of\nacceptable dose recommendations. The \\texttt{R} and \\texttt{Stan} code for the\nimplementation of an illustrative sequential phase I trial example is publicly\navailable at https://github.com/gunhanb/TITEPK_sequential.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 11:24:38 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 07:40:42 GMT"}, {"version": "v3", "created": "Thu, 20 Aug 2020 20:21:40 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["G\u00fcnhan", "Burak K\u00fcrsad", ""], ["Weber", "Sebastian", ""], ["Seroutou", "Abdelkader", ""], ["Friede", "Tim", ""]]}, {"id": "1811.09443", "submitter": "Filippo Elba", "authors": "Filippo Elba", "title": "Una valutazione di copertura, qualita' ed efficienza dei servizi\n  sanitari regionali tra 2010 e 2013", "comments": "Italian", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An application of Multiplicative Non-Parametric Corporate Performance Model\n(based on Data Envelopment Analysis) by Emrouznejad and Cabanda is here\npresented and applied so as to evaluate effectiveness, efficiency and quality\nof Regional Health Systems in Italy. A Quadrant Analysis is used too in order\nto visualize analysis results. In general, Regional Systems characterized by\nhigh effectiveness scores are much efficient too. Emilia Romagna seems to be\nthe best Region under all three points of view. The Southern Regions register\nthe worst performances.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 12:07:31 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2018 09:45:37 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Elba", "Filippo", ""]]}, {"id": "1811.09451", "submitter": "Carlos Pineda", "authors": "Germinal Cocho, R. F. Rodr\\'iguez, Sergio S\\'anchez, Jorge Flores,\n  Carlos Pineda, Carlos Gershenson", "title": "Rank-frequency distribution of natural languages: a difference of\n  probabilities approach", "comments": "11 pages", "journal-ref": "Physica A 532, 121795 (2019)", "doi": "10.1016/j.physa.2019.121795", "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The time variation of the rank $k$ of words for six Indo-European languages\nis obtained using data from Google Books. For low ranks the distinct languages\nbehave differently, maybe due to syntaxis rules, whereas for $k>50$ the law of\nlarge numbers predominates. The dynamics of $k$ is described stochastically\nthrough a master equation governing the time evolution of its probability\ndensity, which is approximated by a Fokker-Planck equation that is solved\nanalytically. The difference between the data and the asymptotic solution is\nidentified with the transient solution, and good agreement is obtained.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 12:25:41 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Cocho", "Germinal", ""], ["Rodr\u00edguez", "R. F.", ""], ["S\u00e1nchez", "Sergio", ""], ["Flores", "Jorge", ""], ["Pineda", "Carlos", ""], ["Gershenson", "Carlos", ""]]}, {"id": "1811.09610", "submitter": "Radha Nagarajan", "authors": "Radhakrishnan Nagarajan, Jeffery Talbert", "title": "Network Abstractions of Prescription Patterns in a Medicaid Population", "comments": "9 Pages, 3 Figures, 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding prescription patterns have relied largely on aggregate\nstatistical measures. Evidence of doctor-shopping, inappropriate prescribing,\ndrug diversion and patient seeking prescription drugs across multiple\nprescribers demand understanding the concerted working of prescribers and\nprescriber communities as opposed to treating them as independent entities. We\nmodel potential associations between prescribers as prescriber-prescriber\nnetwork (PPN) and subsequently investigate its properties across Schedule II,\nIII, IV drugs in a single month in a Medicaid population. Community structure\ndetection algorithms and geo-spatial layouts revealed characteristic patterns\nin PPN markedly different from their random graph surrogate counterparts\nrejecting them as potential generative mechanism. Outlier detection with\nrecommended thresholds also revealed a subset of prescriber specialties to be\nconstitutively flagged across Schedule II, III, IV drugs. Presence of\nprescriber communities may assist in targeted monitoring and their deviation\nfrom random graphs may serve as a metric in assessing PPN evolution temporally\nand pre-/post- interventions.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 21:28:55 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Nagarajan", "Radhakrishnan", ""], ["Talbert", "Jeffery", ""]]}, {"id": "1811.09734", "submitter": "Won Chang", "authors": "Won Chang, Sunghoon Kim, Heewon Chae", "title": "A Regularized Spatial Market Segmentation Method with Dirichlet Process\n  Gaussian Mixture Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatially referenced data are increasingly available thanks to the\ndevelopment of modern GPS technology. They also provide rich opportunities for\nspatial analytics in the field of marketing science. Our main interest is to\npropose a new efficient statistical framework to conduct spatial segmentation\nanalysis for restaurants located in a metropolitan area in the U.S. The spatial\nsegmentation problem poses important statistical challenges: selecting the\noptimal number of underlying structures of market segments, capturing complex\nand flexible spatial structures, and resolving any possible small n and large p\nissue which can be typical in latent class analysis. Existing approaches try to\ntackle these issues in heuristic ways or seem silent on them. To overcome these\nchallenges, we propose a new statistical framework based on regularized\nBayesian spatial mixture regressions with Dirichlet process integrating ridge\nor lasso regularization. Our simulation study demonstrates that the proposed\nmodels successfully recover the underlying spatial clustering structures and\noutperforms two existing benchmark models. In the empirical analysis using\nonline customer satisfaction data from the Yelp, our models provides\ninteresting insights on segment-level key drivers of customer satisfaction and\ninterpretable relationships between regional demographics and restaurants'\ncharacteristics.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 01:01:44 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Chang", "Won", ""], ["Kim", "Sunghoon", ""], ["Chae", "Heewon", ""]]}, {"id": "1811.09911", "submitter": "Hemant Gehlot", "authors": "Hemant Gehlot, Arif Mohaimin Sadri, Satish V. Ukkusuri", "title": "Joint modeling of evacuation departure and travel times in hurricanes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hurricanes are costly natural disasters periodically faced by households in\ncoastal and to some extent, inland areas. A detailed understanding of\nevacuation behavior is fundamental to the development of efficient emergency\nplans. Once a household decides to evacuate, a key behavioral issue is the time\nat which individuals depart to reach their destination. An accurate estimation\nof evacuation departure time is useful to predict evacuation demand over time\nand develop effective evacuation strategies. In addition, the time it takes for\nevacuees to reach their preferred destinations is important. A holistic\nunderstanding of the factors that affect travel time is useful to emergency\nofficials in controlling road traffic and helps in preventing adverse\nconditions like traffic jams. Past studies suggest that departure time and\ntravel time can be related. Hence, an important question arises whether there\nis an interdependence between evacuation departure time and travel time? Does\ndeparting close to the landfall increases the possibility of traveling short\ndistances? Are people more likely to depart early when destined to longer\ndistances? In this study, we present a model to jointly estimate departure and\ntravel times during hurricane evacuations. Empirical results underscore the\nimportance of accommodating an inter-relationship among these dimensions of\nevacuation behavior. This paper also attempts to empirically investigate the\ninfluence of social ties of individuals on joint estimation of evacuation\ndeparture and travel times. Survey data from Hurricane Sandy is used for\ncomputing empirical results. Results indicate significant role of social\nnetworks in addition to other key factors on evacuation departure and travel\ntimes during hurricanes.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 23:50:35 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Gehlot", "Hemant", ""], ["Sadri", "Arif Mohaimin", ""], ["Ukkusuri", "Satish V.", ""]]}, {"id": "1811.09926", "submitter": "Donghui Yan", "authors": "Xiaochun Chen, Honggang Wang and Donghui Yan", "title": "Clustering of Transcriptomic Data for the Identification of Cancer\n  Subtypes", "comments": "10 pages, 3 figures. The 4th International Conference on Fuzzy\n  Systems and Data Mining, 2018", "journal-ref": null, "doi": "10.3233/978-1-61499-927-0-387", "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cancer is a number of related yet highly heterogeneous diseases. Correct\nidentification of cancer subtypes is critical for clinical decisions. The\nadvance in sequencing technologies has made it possible to study cancer based\non abundant genomics and transcriptomic (-omics) data. Such a data-driven\napproach is expected to address limitations and issues with traditional methods\nin identifying cancer subtypes. We evaluate the suitability of clustering--a\ndata mining tool to study heterogenous data when there is a lack of sufficient\nunderstanding of the subject matters--in the identification of cancer subtypes.\nA number of popular clustering algorithms and their consensus are explored, and\nwe find cancer subtypes identified by consensus clustering agree well with\nclinical studies.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 01:56:58 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Chen", "Xiaochun", ""], ["Wang", "Honggang", ""], ["Yan", "Donghui", ""]]}, {"id": "1811.10122", "submitter": "Udit Bhatia", "authors": "Udit Bhatia, Auroop Ratan Ganguly", "title": "Precipitation extremes and depth-duration-frequency under internal\n  climate variability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural climate variability, captured through multiple initial condition\nensembles, may be comparable to the variability caused by knowledge gaps in\nfuture emissions trajectories and in the physical science basis, especially at\nadaptation-relevant scales and projection horizons. The relations to chaos\ntheory, including sensitivity to initial conditions, have caused the resulting\nvariability in projections to be viewed as the irreducible uncertainty\ncomponent of climate. The multiplier effect of ensembles from\nemissions-trajectories, multiple-models and initial-conditions contribute to\nthe challenge. We show that ignoring this variability results in\nunderestimation of precipitation extremes return periods leading to\nmaladaptation. However, we show that concatenating initial-condition ensembles\nresults in reduction of hydroclimate uncertainty. We show how this reduced\nuncertainty in precipitation extremes percolates to\nadaptation-relevant-Depth-Duration Frequency curves. Hence, generation of\nadditional initial condition ensembles therefore no longer needs to be viewed\nas an uncertainty explosion problem but as a solution that can lead to\nuncertainty reduction in assessment of extremes.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 00:10:10 GMT"}, {"version": "v2", "created": "Fri, 21 Jun 2019 07:52:13 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Bhatia", "Udit", ""], ["Ganguly", "Auroop Ratan", ""]]}, {"id": "1811.10558", "submitter": "Michel Vellekoop", "authors": "Torsten Kleinow and Michel Vellekoop", "title": "Minimum reversion in multivariate time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new multivariate time series model in which we assume that each\ncomponent has a tendency to revert to the minimum of all components. Such a\nspecification is useful to describe phenomena where each member in a population\nwhich is subjected to random noise mimics the behaviour of the best performing\nmember.\n  We show that the proposed dynamics generate co-integrated processes.We\ncharacterize the model's asymptotic properties for the case of two populations\nand show a stabilizing effect on long term dynamics in simulation studies. An\nempirical study involving human survival data in different countries provides\nan example which confirms the occurrence of the phenomenon of reversion to the\nminimum in real data.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 18:05:05 GMT"}, {"version": "v2", "created": "Wed, 5 Dec 2018 17:37:48 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Kleinow", "Torsten", ""], ["Vellekoop", "Michel", ""]]}, {"id": "1811.10690", "submitter": "Matthias Parey", "authors": "Richard Blundell, Joel Horowitz, Matthias Parey", "title": "Estimation of a Heterogeneous Demand Function with Berkson Errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Berkson errors are commonplace in empirical microeconomics. In consumer\ndemand this form of measurement error occurs when the price an individual pays\nis measured by the (weighted) average price paid by individuals in a specified\ngroup (e.g., a county), rather than the true transaction price. We show the\nimportance of such measurement errors for the estimation of demand in a setting\nwith nonseparable unobserved heterogeneity. We develop a consistent estimator\nusing external information on the true distribution of prices. Examining the\ndemand for gasoline in the U.S., we document substantial within-market price\nvariability, and show that there are significant spatial differences in the\nmagnitude of Berkson errors across regions of the U.S. Accounting for Berkson\nerrors is found to be quantitatively important for estimating price effects and\nfor welfare calculations. Imposing the Slutsky shape constraint greatly reduces\nthe sensitivity to Berkson errors.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 21:11:10 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 12:15:44 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Blundell", "Richard", ""], ["Horowitz", "Joel", ""], ["Parey", "Matthias", ""]]}, {"id": "1811.10882", "submitter": "Alastair Gregory", "authors": "Alastair Gregory, Din-Houn Lau, Mark Girolami, Liam Butler and\n  Mohammed Elshafie", "title": "The synthesis of data from instrumented structures and physics-based\n  models via Gaussian processes", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2019.04.065", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent development which is poised to disrupt current structural\nengineering practice is the use of data obtained from physical structures such\nas bridges, viaducts and buildings. These data can represent how the structure\nresponds to various stimuli over time when in operation, providing engineers\nwith a unique insight into how their designs are performing. With the advent of\nadvanced sensing technologies and the Internet of Things, the efficient\ninterpretation of structural health monitoring data has become a big data\nchallenge. Many models have been proposed in literature to represent such data,\nsuch as linear statistical models. Based upon these models, the health of the\nstructure is reasoned about, e.g. through damage indices, changes in likelihood\nand statistical parameter estimates. On the other hand, physics-based models\nare typically used when designing structures to predict how the structure will\nrespond to operational stimuli. What remains unclear in the literature is how\nto combine the observed data with information from the idealised physics-based\nmodel into a model that describes the responses of the operational structure.\nThis paper introduces a new approach which fuses together observed data from a\nphysical structure during operation and information from a mathematical model.\nThe observed data are combined with data simulated from the physics-based model\nusing a multi-output Gaussian process formulation. The novelty of this method\nis how the information from observed data and the physics-based model is\nbalanced to obtain a representative model of the structures response to\nstimuli. We present our method using data obtained from a fibre-optic sensor\nnetwork installed on experimental railway sleepers. We discuss how this\napproach can be used to reason about changes in the structures behaviour over\ntime using simulations and experimental data.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 09:10:10 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2019 08:37:20 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Gregory", "Alastair", ""], ["Lau", "Din-Houn", ""], ["Girolami", "Mark", ""], ["Butler", "Liam", ""], ["Elshafie", "Mohammed", ""]]}, {"id": "1811.10958", "submitter": "Marko J\\\"arvenp\\\"a\\\"a", "authors": "Marko J\\\"arvenp\\\"a\\\"a, Mohamad R. Abdul Sater, Georgia K. Lagoudas,\n  Paul C. Blainey, Loren G. Miller, James A. McKinnell, Susan S. Huang, Yonatan\n  H. Grad, Pekka Marttinen", "title": "A Bayesian model of acquisition and clearance of bacterial colonization", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/87", "categories": "q-bio.PE cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bacterial populations that colonize a host play important roles in host\nhealth, including serving as a reservoir that transmits to other hosts and from\nwhich invasive strains emerge, thus emphasizing the importance of understanding\nrates of acquisition and clearance of colonizing populations. Studies of\ncolonization dynamics have been based on assessment of whether serial samples\nrepresent a single population or distinct colonization events. A common\nsolution to estimate acquisition and clearance rates is to use a fixed genetic\ndistance threshold. However, this approach is often inadequate to account for\nthe diversity of the underlying within-host evolving population, the time\nintervals between consecutive measurements, and the uncertainty in the\nestimated acquisition and clearance rates. Here, we summarize recently\nsubmitted work \\cite{jarvenpaa2018named} and present a Bayesian model that\nprovides probabilities of whether two strains should be considered the same,\nallowing to determine bacterial clearance and acquisition from genomes sampled\nover time. We explicitly model the within-host variation using population\ngenetic simulation, and the inference is done by combining information from\nmultiple data sources by using a combination of Approximate Bayesian\nComputation (ABC) and Markov Chain Monte Carlo (MCMC). We use the method to\nanalyse a collection of methicillin resistant Staphylococcus aureus (MRSA)\nisolates.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 13:17:44 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["J\u00e4rvenp\u00e4\u00e4", "Marko", ""], ["Sater", "Mohamad R. Abdul", ""], ["Lagoudas", "Georgia K.", ""], ["Blainey", "Paul C.", ""], ["Miller", "Loren G.", ""], ["McKinnell", "James A.", ""], ["Huang", "Susan S.", ""], ["Grad", "Yonatan H.", ""], ["Marttinen", "Pekka", ""]]}, {"id": "1811.11025", "submitter": "Wenying Deng", "authors": "Wenying Deng, Jeremiah Zhe Liu, Erin Lake, Brent A. Coull", "title": "CVEK: Robust Estimation and Testing for Nonlinear Effects using Kernel\n  Machine Ensemble", "comments": "5 figures. arXiv admin note: text overlap with arXiv:1710.01406", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The R package CVEK introduces a suite of flexible machine learning models and\nrobust hypothesis tests for learning the joint nonlinear effects of multiple\ncovariates in limited samples. It implements the Cross-validated Ensemble of\nKernels (CVEK)(Liu and Coull 2017), an ensemble-based kernel machine learning\nmethod that adaptively learns the joint nonlinear effect of multiple covariates\nfrom data, and provides powerful hypothesis tests for both main effects of\nfeatures and interactions among features. The R Package CVEK provides a\nflexible, easy-to-use implementation of CVEK, and offers a wide range of\nchoices for the kernel family (for instance, polynomial, radial basis\nfunctions, Mat\\'ern, neural network, and others), model selection criteria,\nensembling method (averaging, exponential weighting, cross-validated stacking),\nand the type of hypothesis test (asymptotic or parametric bootstrap). Through\nextensive simulations we demonstrate the validity and robustness of this\napproach, and provide practical guidelines on how to design an estimation\nstrategy for optimal performance in different data scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 09:20:47 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 23:04:02 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Deng", "Wenying", ""], ["Liu", "Jeremiah Zhe", ""], ["Lake", "Erin", ""], ["Coull", "Brent A.", ""]]}, {"id": "1811.11031", "submitter": "Eliane Pinheiro", "authors": "Eliane C. Pinheiro, Silvia L.P. Ferrari and Francisco M.C. Medeiros", "title": "Higher-order approximate confidence intervals", "comments": "25 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard confidence intervals employed in applied statistical analysis are\nusually based on asymptotic approximations. Such approximations can be\nconsiderably inaccurate in small and moderate sized samples. We derive accurate\nconfidence intervals based on higher-order approximate quantiles of the score\nfunction. The coverage approximation error is $O(n^{-3/2})$ while the\napproximation error of confidence intervals based on the asymptotic normality\nof MLEs is $O(n^{-1/2})$. Monte Carlo simulations confirm the theoretical\nfindings. An implementation for regression models and real data applications\nare provided.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 14:46:11 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 14:37:08 GMT"}, {"version": "v3", "created": "Thu, 10 Dec 2020 21:55:13 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Pinheiro", "Eliane C.", ""], ["Ferrari", "Silvia L. P.", ""], ["Medeiros", "Francisco M. C.", ""]]}, {"id": "1811.11038", "submitter": "Samuel I. Berchuck", "authors": "Samuel I. Berchuck, Jean-Claude Mwanza, Joshua L. Warren", "title": "A spatially varying change points model for monitoring glaucoma\n  progression using visual field data", "comments": "This is a preprint of an article submitted for publication in Spatial\n  Statistics (https://www.journals.elsevier.com/spatial-statistics). The\n  article contains 42 pages, 4 figures, 5 tables and 1 video", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Glaucoma disease progression, as measured by visual field (VF) data, is often\ndefined by periods of relative stability followed by an abrupt decrease in\nvisual ability at some point in time. Determining the transition point of the\ndisease trajectory to a more severe state is important clinically for disease\nmanagement and for avoiding irreversible vision loss. Based on this, we present\na unified statistical modeling framework that permits prediction of the timing\nand spatial location of future vision loss and informs clinical decisions\nregarding disease progression. The developed method incorporates anatomical\ninformation to create a biologically plausible data-generating model. We\naccomplish this by introducing a spatially varying coefficients model that\nincludes spatially varying change points to detect structural shifts in both\nthe mean and variance process of VF data across both space and time. The VF\nlocation-specific change point represents the underlying, and potentially\ncensored, timing of true change in disease trajectory while a multivariate\nspatial boundary detection structure is introduced that accounts for the\ncomplex spatial connectivity of the VF and optic disc. We show that our method\nimproves estimation and prediction of multiple aspects of disease management in\ncomparison to existing methods through simulation and real data application.\nThe R package spCP implements the new methodology.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 14:57:37 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Berchuck", "Samuel I.", ""], ["Mwanza", "Jean-Claude", ""], ["Warren", "Joshua L.", ""]]}, {"id": "1811.11072", "submitter": "Luis Fernando Campos", "authors": "Luis F. Campos, Mark E. Glickman, Kristen B. Hunter", "title": "Measuring Effects of Medication Adherence on Time-Varying Health\n  Outcomes using Bayesian Dynamic Linear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the most significant barriers to medication treatment is patients'\nnon-adherence to a prescribed medication regimen. The extent of the impact of\npoor adherence on resulting health measures is often unknown, and typical\nanalyses ignore the time-varying nature of adherence. This paper develops a\nmodeling framework for longitudinally recorded health measures modeled as a\nfunction of time-varying medication adherence or other time-varying covariates.\nOur framework, which relies on normal Bayesian dynamic linear models (DLMs),\naccounts for time-varying covariates such as adherence and non-dynamic\ncovariates such as baseline health characteristics. Given the inefficiencies\nusing standard inferential procedures for DLMs associated with infrequent and\nirregularly recorded response data, we develop an approach that relies on\nfactoring the posterior density into a product of two terms; a marginal\nposterior density for the non-dynamic parameters, and a multivariate normal\nposterior density of the dynamic parameters conditional on the non-dynamic\nones. This factorization leads to a two-stage process for inference in which\nthe non-dynamic parameters can be inferred separately from the time-varying\nparameters. We demonstrate the application of this model to the time-varying\neffect of anti-hypertensive medication on blood pressure levels from a cohort\nof patients diagnosed with hypertension. Our model results are compared to ones\nin which adherence is incorporated through non-dynamic summaries.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 16:08:45 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 22:15:05 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Campos", "Luis F.", ""], ["Glickman", "Mark E.", ""], ["Hunter", "Kristen B.", ""]]}, {"id": "1811.11074", "submitter": "John Malik", "authors": "Yao Lu, Hau-tieng Wu, John Malik", "title": "Recycling cardiogenic artifacts in impedance pneumography", "comments": "21 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Biomedical sensors often exhibit cardiogenic artifacts which, while\ndistorting the signal of interest, carry useful hemodynamic information. We\npropose an algorithm to remove and extract hemodynamic information from these\ncardiogenic artifacts. Methods: We apply a nonlinear time-frequency analysis\ntechnique, the de-shape synchrosqueezing transform (dsSST), to adaptively\nisolate the high- and low-frequency components of a single-channel signal. We\ndemonstrate this technique's effectiveness by removing and deriving hemodynamic\ninformation from the cardiogenic artifact in an impedance pneumography (IP).\nResults: The instantaneous heart rate is extracted, and the cardiac and\nrespiratory signals are reconstructed. Conclusions: The dsSST is suitable for\ngenerating useful hemodynamic information from the cardiogenic artifact in a\nsingle-channel IP. We propose that the usefulness of the dsSST as a recycling\ntool extends to other biomedical sensors exhibiting cardiogenic artifacts.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 16:16:37 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 14:14:55 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Lu", "Yao", ""], ["Wu", "Hau-tieng", ""], ["Malik", "John", ""]]}, {"id": "1811.11154", "submitter": "Xiaojie Mao", "authors": "Jiahao Chen, Nathan Kallus, Xiaojie Mao, Geoffry Svacha, Madeleine\n  Udell", "title": "Fairness Under Unawareness: Assessing Disparity When Protected Class Is\n  Unobserved", "comments": "13 pages, 11 figures, FAT*' 19: Conference on Fairness,\n  Accountability, and Transparency (FAT*' 19), January 29-31, 2019, Atlanta,\n  GA, USA", "journal-ref": null, "doi": "10.1145/3287560.3287594", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing the fairness of a decision making system with respect to a\nprotected class, such as gender or race, is challenging when class membership\nlabels are unavailable. Probabilistic models for predicting the protected class\nbased on observable proxies, such as surname and geolocation for race, are\nsometimes used to impute these missing labels for compliance assessments.\nEmpirically, these methods are observed to exaggerate disparities, but the\nreason why is unknown. In this paper, we decompose the biases in estimating\noutcome disparity via threshold-based imputation into multiple interpretable\nbias sources, allowing us to explain when over- or underestimation occurs. We\nalso propose an alternative weighted estimator that uses soft classification,\nand show that its bias arises simply from the conditional covariance of the\noutcome with the true class membership. Finally, we illustrate our results with\nnumerical simulations and a public dataset of mortgage applications, using\ngeolocation as a proxy for race. We confirm that the bias of threshold-based\nimputation is generally upward, but its magnitude varies strongly with the\nthreshold chosen. Our new weighted estimator tends to have a negative bias that\nis much simpler to analyze and reason about.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 18:39:09 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Chen", "Jiahao", ""], ["Kallus", "Nathan", ""], ["Mao", "Xiaojie", ""], ["Svacha", "Geoffry", ""], ["Udell", "Madeleine", ""]]}, {"id": "1811.11709", "submitter": "Yuchen Zhou", "authors": "Pixu Shi and Yuchen Zhou and Anru R. Zhang", "title": "High-dimensional Log-Error-in-Variable Regression with Applications to\n  Microbial Compositional Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In microbiome and genomic studies, the regression of compositional data has\nbeen a crucial tool for identifying microbial taxa or genes that are associated\nwith clinical phenotypes. To account for the variation in sequencing depth, the\nclassic log-contrast model is often used where read counts are normalized into\ncompositions. However, zero read counts and the randomness in covariates remain\ncritical issues. In this article, we introduce a surprisingly simple,\ninterpretable, and efficient method for the estimation of compositional data\nregression through the lens of a novel high-dimensional log-error-in-variable\nregression model. The proposed method provides both corrections on sequencing\ndata with possible overdispersion and simultaneously avoids any subjective\nimputation of zero read counts. We provide theoretical justifications with\nmatching upper and lower bounds for the estimation error. The merit of the\nprocedure is illustrated through real data analysis and simulation studies.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 17:57:59 GMT"}, {"version": "v2", "created": "Sat, 15 Dec 2018 01:29:35 GMT"}, {"version": "v3", "created": "Wed, 10 Mar 2021 17:59:25 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Shi", "Pixu", ""], ["Zhou", "Yuchen", ""], ["Zhang", "Anru R.", ""]]}, {"id": "1811.11850", "submitter": "L\\'aszl\\'o Csat\\'o", "authors": "L\\'aszl\\'o Csat\\'o", "title": "Optimal tournament design: lessons from the men's handball Champions\n  League", "comments": "25 pages, 11 figures, 3 tables", "journal-ref": "Journal of Sports Economics, 21(8): 848-868, 2020", "doi": "10.1177/1527002520944442", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many sports tournaments are organised in a hybrid design consisting of a\nround-robin group stage followed by a knock-out phase. The traditional seeding\nregime aims to create balanced groups roughly at the same competition level but\nmay result in several uneven matches when the quality of the teams varies\ngreatly. Our paper is the first challenging this classical solution through the\nexample of the men's EHF (European Handball Federation) Champions League, the\nmost prestigious men's handball club competition in Europe, which has used\nunbalanced groups between the 2015/16 and 2019/20 seasons. Its particular\ndesign is compared to an alternative format with equally strong groups, as well\nas to the previous scheme of the EHF Champions League. We find that it is\npossible to increase the quality of all matches played together with raising\nthe uncertainty of outcome, essentially without sacrificing fairness. Our\nresults have useful implications for the governing bodies of major sports.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 21:35:26 GMT"}, {"version": "v2", "created": "Fri, 18 Jan 2019 09:22:41 GMT"}, {"version": "v3", "created": "Tue, 22 Jan 2019 13:58:17 GMT"}, {"version": "v4", "created": "Fri, 31 May 2019 12:15:31 GMT"}, {"version": "v5", "created": "Wed, 9 Oct 2019 19:49:16 GMT"}, {"version": "v6", "created": "Wed, 26 Feb 2020 14:26:28 GMT"}, {"version": "v7", "created": "Mon, 11 May 2020 15:53:47 GMT"}, {"version": "v8", "created": "Fri, 17 Jul 2020 09:27:16 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Csat\u00f3", "L\u00e1szl\u00f3", ""]]}, {"id": "1811.11920", "submitter": "Elias Chaibub Neto", "authors": "Elias Chaibub Neto, Abhishek Pratap, Thanneer M Perumal, Meghasyam\n  Tummalacherla, Brian M Bot, Lara Mangravite, Larsson Omberg", "title": "Using permutations to assess confounding in machine learning\n  applications for digital health", "comments": "This workshop article draws some material from arXiv:1805.07465. Main\n  text and Supplement. Machine Learning for Health (ML4H) Workshop at NeurIPS\n  2018 arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/193", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical machine learning applications are often plagued with confounders\nthat can impact the generalizability and predictive performance of the\nlearners. Confounding is especially problematic in remote digital health\nstudies where the participants self-select to enter the study, thereby making\nit challenging to balance the demographic characteristics of participants. One\neffective approach to combat confounding is to match samples with respect to\nthe confounding variables in order to balance the data. This procedure,\nhowever, leads to smaller datasets and hence impact the inferences drawn from\nthe learners. Alternatively, confounding adjustment methods that make more\nefficient use of the data (e.g., inverse probability weighting) usually rely on\nmodeling assumptions, and it is unclear how robust these methods are to\nviolations of these assumptions. Here, rather than proposing a new approach to\ncontrol for confounding, we develop novel permutation based statistical methods\nto detect and quantify the influence of observed confounders, and estimate the\nunconfounded performance of the learner. Our tools can be used to evaluate the\neffectiveness of existing confounding adjustment methods. We illustrate their\napplication using real-life data from a Parkinson's disease mobile health study\ncollected in an uncontrolled environment.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 02:02:10 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Neto", "Elias Chaibub", ""], ["Pratap", "Abhishek", ""], ["Perumal", "Thanneer M", ""], ["Tummalacherla", "Meghasyam", ""], ["Bot", "Brian M", ""], ["Mangravite", "Lara", ""], ["Omberg", "Larsson", ""]]}, {"id": "1811.12106", "submitter": "arXiv admin", "authors": "V. Tadayon", "title": "Health Effects Estimation Attributed to Particulate Matter", "comments": "This article has been removed by arXiv administrators due to\n  falsified authorship", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article has been removed by arXiv administrators due to falsified\nauthorship.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 12:49:31 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 08:03:13 GMT"}, {"version": "v3", "created": "Fri, 7 Dec 2018 08:01:15 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Tadayon", "V.", ""]]}, {"id": "1811.12295", "submitter": "Sim\\'on Ram\\'irez-Amaya", "authors": "Adolfo Quiroz, Sim\\'on Ram\\'irez-Amaya, \\'Alvaro Riascos", "title": "Regression by clustering using Metropolis-Hastings", "comments": "Presented at NIPS 2018 Workshop on Machine Learning for the\n  Developing World", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High quality risk adjustment in health insurance markets weakens insurer\nincentives to engage in inefficient behavior to attract lower-cost enrollees.\nWe propose a novel methodology based on Markov Chain Monte Carlo methods to\nimprove risk adjustment by clustering diagnostic codes into risk groups optimal\nfor health expenditure prediction. We test the performance of our methodology\nagainst common alternatives using panel data from 500 thousand enrollees of the\nColombian Healthcare System. Results show that our methodology outperforms\ncommon alternatives and suggest that it has potential to improve access to\nquality healthcare for the chronically ill.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 16:36:32 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2019 19:06:17 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Quiroz", "Adolfo", ""], ["Ram\u00edrez-Amaya", "Sim\u00f3n", ""], ["Riascos", "\u00c1lvaro", ""]]}, {"id": "1811.12314", "submitter": "Zhiqin Xu", "authors": "Zhi-Qin John Xu, Douglas Zhou, David Cai", "title": "Swift Two-sample Test on High-dimensional Neural Spiking Data", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To understand how neural networks process information, it is important to\ninvestigate how neural network dynamics varies with respect to different\nstimuli. One challenging task is to design efficient statistical approaches to\nanalyze multiple spike train data obtained from a short recording time. Based\non the development of high-dimensional statistical methods, it is able to deal\nwith data whose dimension is much larger than the sample size. However, these\nmethods often require statistically independent samples to start with, while\nneural data are correlated over consecutive sampling time bins. We develop an\napproach to pretreat neural data to become independent samples over time by\ntransferring the correlation of dynamics for each neuron in different sampling\ntime bins into the correlation of dynamics among different dimensions within\neach sampling time bin. We verify the method using simulation data generated\nfrom Integrate-and-fire neuron network models and a large-scale network model\nof primary visual cortex within a short time, e.g., a few seconds. Our method\nmay offer experimenters to use the advantage of the development of statistical\nmethods to analyze high-dimensional neural data.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 20:40:41 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Xu", "Zhi-Qin John", ""], ["Zhou", "Douglas", ""], ["Cai", "David", ""]]}, {"id": "1811.12466", "submitter": "Henry Bendekgey", "authors": "Henry Bendekgey", "title": "Consistency of Forecasts for the U.S. House of Representatives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the performance of the foremost academic House of Representatives\nforecasting models in the 2018 elections. In creating open-source\nimplementations of these models, we outline key underlying assumptions. We find\nthat although the results were unsurprising, they indicate a weakening of many\ntraditional forecasting indicators.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 20:20:38 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Bendekgey", "Henry", ""]]}, {"id": "1811.12520", "submitter": "Eli Sherman", "authors": "Eli Sherman, Hitinder Gurm, Ulysses Balis, Scott Owens, Jenna Wiens", "title": "Leveraging Clinical Time-Series Data for Prediction: A Cautionary Tale", "comments": "In Proceedings of American Medical Informatics Annual Symposium 2017\n  PMID: 29854227", "journal-ref": "AMIA Annu Symp Proc. 2018 Apr 16;2017:1571-1580. eCollection 2017", "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In healthcare, patient risk stratification models are often learned using\ntime-series data extracted from electronic health records. When extracting data\nfor a clinical prediction task, several formulations exist, depending on how\none chooses the time of prediction and the prediction horizon. In this paper,\nwe show how the formulation can greatly impact both model performance and\nclinical utility. Leveraging a publicly available ICU dataset, we consider two\nclinical prediction tasks: in-hospital mortality, and hypokalemia. Through\nthese case studies, we demonstrate the necessity of evaluating models using an\noutcome-independent reference point, since choosing the time of prediction\nrelative to the event can result in unrealistic performance. Further, an\noutcome-independent scheme outperforms an outcome-dependent scheme on both\ntasks (In-Hospital Mortality AUROC .882 vs. .831; Serum Potassium: AUROC .829\nvs. .740) when evaluated on test sets that mimic real-world use.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 22:43:23 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Sherman", "Eli", ""], ["Gurm", "Hitinder", ""], ["Balis", "Ulysses", ""], ["Owens", "Scott", ""], ["Wiens", "Jenna", ""]]}, {"id": "1811.12610", "submitter": "Vishal Sharma", "authors": "Vishal Sharma", "title": "An Energy-Efficient Transaction Model for the Blockchain-enabled\n  Internet of Vehicles (IoV)", "comments": "4 Pages, 4 Figures, IEEE Communications Letters", "journal-ref": null, "doi": "10.1109/LCOMM.2018.2883629", "report-no": null, "categories": "cs.NI cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The blockchain is a safe, reliable and innovative mechanism for managing\nnumerous vehicles seeking connectivity. However, following the principles of\nthe blockchain, the number of transactions required to update ledgers pose\nserious issues for vehicles as these may consume the maximum available energy.\nTo resolve this, an efficient model is presented in this letter which is\ncapable of handling the energy demands of the blockchain-enabled Internet of\nVehicles (IoV) by optimally controlling the number of transactions through\ndistributed clustering. Numerical results suggest that the proposed approach is\n40.16% better in terms of energy conservation and 82.06% better in terms of the\nnumber of transactions required to share the entire blockchain-data compared\nwith the traditional blockchain.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 04:41:57 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Sharma", "Vishal", ""]]}, {"id": "1811.12682", "submitter": "Elena Pesce", "authors": "Elena Pesce and Eva Riccomagno", "title": "Large Datasets, Bias and Model Oriented Optimal Design of Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review recent literature that proposes to adapt ideas from classical model\nbased optimal design of experiments to problems of data selection of large\ndatasets. Special attention is given to bias reduction and to protection\nagainst confounders. Some new results are presented. Theoretical and\ncomputational comparisons are made.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 09:28:39 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Pesce", "Elena", ""], ["Riccomagno", "Eva", ""]]}, {"id": "1811.12718", "submitter": "Umberto Simola Mr.", "authors": "Umberto Simola and Xavier Dumusque and Jessi Cisewski-Kehe", "title": "Measuring precise radial velocities and cross-correlation function\n  line-profile variations using a Skew Normal density", "comments": null, "journal-ref": "A&A 622, A131 (2019)", "doi": "10.1051/0004-6361/201833895", "report-no": null, "categories": "astro-ph.EP astro-ph.SR stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stellar activity is one of the primary limitations to the detection of\nlow-mass exoplanets using the radial-velocity (RV) technique. We propose to\nestimate the variations in shape of the CCF by fitting a Skew Normal (SN)\ndensity which, unlike the commonly employed Normal density, includes a skewness\nparameter to capture the asymmetry of the CCF induced by stellar activity and\nthe convective blueshift. The performances of the proposed method are compared\nto the commonly employed Normal density using both simulations and real\nobservations, with different levels of activity and signal-to-noise ratio. When\nconsidering real observations, the correlation between the RV and the asymmetry\nof the CCF and between the RV and the width of the CCF are stronger when using\nthe parameters estimated with the SN density rather than the ones obtained with\nthe commonly employed Normal density. Using the proposed SN approach, the\nuncertainties estimated on the RV defined as the median of the SN are on\naverage 10% smaller than the uncertainties calculated on the mean of the\nNormal. The uncertainties estimated on the asymmetry parameter of the SN are on\naverage 15% smaller than the uncertainties measured on the Bisector Inverse\nSlope Span (BIS SPAN), which is the commonly used parameter to evaluate the\nasymmetry of the CCF. We also propose a new model to account for stellar\nactivity when fitting a planetary signal to RV data. Based on simple\nsimulations, we were able to demonstrate that this new model improves the\nplanetary detection limits by 12% compared to the model commonly used to\naccount for stellar activity. The SN density is a better model than the Normal\ndensity for characterizing the CCF since the correlations used to probe stellar\nactivity are stronger and the uncertainties of the RV estimate and the\nasymmetry of the CCF are both smaller.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 10:54:10 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Simola", "Umberto", ""], ["Dumusque", "Xavier", ""], ["Cisewski-Kehe", "Jessi", ""]]}, {"id": "1811.12723", "submitter": "Fabian Guignard", "authors": "Fabian Guignard, Dasaraden Mauree, Mikhail Kanevski, Luciano Telesca", "title": "Wavelet variance scale-dependence as a dynamics discriminating tool in\n  high-frequency urban wind speed time series", "comments": "17 pages, 6 figures", "journal-ref": null, "doi": "10.1016/j.physa.2019.04.021", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High frequency wind time series measured at different heights from the ground\n(from 1.5 to 25.5 meters) in an urban area were investigated by using the\nvariance of the coefficients of their wavelet transform. Two ranges of scales\nwere identified, sensitive to two different dynamical behavior of the wind\nspeed: the lower anemometers show higher wavelet variance at smaller scales,\nwhile the higher ones are characterized by higher wavelet variance at larger\nscales. Due to the relationship between wavelet scale and frequency, the\nresults suggest the existence of two frequency ranges, where the wind speed\nvariability change according to the position of the anemometer from the ground.\nThis study contributes to better understanding of the high frequency wind speed\nin urban areas and to a better knowledge of the underlying mechanism governing\nthe wind fluctuations at different heights from the ground in particular in\nurban area.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 11:03:29 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Guignard", "Fabian", ""], ["Mauree", "Dasaraden", ""], ["Kanevski", "Mikhail", ""], ["Telesca", "Luciano", ""]]}, {"id": "1811.12788", "submitter": "Jerome Stenger", "authors": "Jerome Stenger, Fabrice Gamboa, Merlin Keller, Bertrand Iooss", "title": "Optimal Uncertainty Quantification on moment class using canonical\n  moments", "comments": "21 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We gain robustness on the quantification of a risk measurement by accounting\nfor all sources of uncertainties tainting the inputs of a computer code. We\nevaluate the maximum quantile over a class of distributions defined only by\nconstraints on their moments. The methodology is based on the theory of\ncanonical moments that appears to be a well-suited framework for practical\noptimization.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 13:38:04 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Stenger", "Jerome", ""], ["Gamboa", "Fabrice", ""], ["Keller", "Merlin", ""], ["Iooss", "Bertrand", ""]]}, {"id": "1811.12880", "submitter": "Sim\\'on Ram\\'irez-Amaya", "authors": "Mateo Dulce, Sim\\'on Ram\\'irez-Amaya, \\'Alvaro Riascos", "title": "Efficient allocation of law enforcement resources using predictive\n  police patrolling", "comments": "Presented at NIPS 2018 Workshop on Machine Learning for the\n  Developing World", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient allocation of scarce law enforcement resources is a hard problem to\ntackle. In a previous study (forthcoming Barreras et.al (2019)) it has been\nshown that a simplified version of the self-exciting point process explained in\nMohler et.al (2011), performs better predicting crime in the city of Bogot\\'{a}\n- Colombia, than other standard hotspot models such as plain KDE or ellipses\nmodels. This paper fully implements the Mohler et.al (2011) model in the city\nof Bogot\\'{a} and explains its technological deployment for the city as a tool\nfor the efficient allocation of police resources.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 16:40:08 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Dulce", "Mateo", ""], ["Ram\u00edrez-Amaya", "Sim\u00f3n", ""], ["Riascos", "\u00c1lvaro", ""]]}, {"id": "1811.12895", "submitter": "arXiv admin", "authors": "V. Tadayon", "title": "Spatial modeling of particulate matters and emergency room visits", "comments": "This article has been removed by arXiv administrators due to\n  falsified authorship", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article has been removed by arXiv administrators due to falsified\nauthorship.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 17:04:52 GMT"}, {"version": "v2", "created": "Fri, 7 Dec 2018 08:14:30 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Tadayon", "V.", ""]]}]