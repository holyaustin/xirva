[{"id": "1807.00373", "submitter": "Ran Rubin", "authors": "Ran Rubin", "title": "New Heuristics for Parallel and Scalable Bayesian Optimization", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization has emerged as a strong candidate tool for global\noptimization of functions with expensive evaluation costs. However, due to the\ndynamic nature of research in Bayesian approaches, and the evolution of\ncomputing technology, using Bayesian optimization in a parallel computing\nenvironment remains a challenge for the non-expert. In this report, I review\nthe state-of-the-art in parallel and scalable Bayesian optimization methods. In\naddition, I propose practical ways to avoid a few of the pitfalls of Bayesian\noptimization, such as oversampling of edge parameters and over-exploitation of\nhigh performance parameters. Finally, I provide relatively simple, heuristic\nalgorithms, along with their open source software implementations, that can be\nimmediately and easily deployed in any computing environment.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jul 2018 19:09:15 GMT"}, {"version": "v2", "created": "Thu, 23 Aug 2018 01:39:44 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Rubin", "Ran", ""]]}, {"id": "1807.00445", "submitter": "Erdem Varol", "authors": "Erdem Varol, Aristeidis Sotiras, Ke Zeng, Christos Davatzikos", "title": "Generative discriminative models for multivariate inference and\n  statistical mapping in medical imaging", "comments": "To appear in MICCAI 2018 proceedings", "journal-ref": null, "doi": "10.1007/978-3-030-00931-1_62", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper presents a general framework for obtaining interpretable\nmultivariate discriminative models that allow efficient statistical inference\nfor neuroimage analysis. The framework, termed generative discriminative\nmachine (GDM), augments discriminative models with a generative regularization\nterm. We demonstrate that the proposed formulation can be optimized in closed\nform and in dual space, allowing efficient computation for high dimensional\nneuroimaging datasets. Furthermore, we provide an analytic estimation of the\nnull distribution of the model parameters, which enables efficient statistical\ninference and p-value computation without the need for permutation testing. We\ncompared the proposed method with both purely generative and discriminative\nlearning methods in two large structural magnetic resonance imaging (sMRI)\ndatasets of Alzheimer's disease (AD) (n=415) and Schizophrenia (n=853). Using\nthe AD dataset, we demonstrated the ability of GDM to robustly handle\nconfounding variations. Using Schizophrenia dataset, we demonstrated the\nability of GDM to handle multi-site studies. Taken together, the results\nunderline the potential of the proposed approach for neuroimaging analyses.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 03:02:13 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Varol", "Erdem", ""], ["Sotiras", "Aristeidis", ""], ["Zeng", "Ke", ""], ["Davatzikos", "Christos", ""]]}, {"id": "1807.01094", "submitter": "Nikita Churikov", "authors": "N. S. Churikov, N. G. Grafeeva", "title": "Recovering gaps in the gamma-ray logging method", "comments": "7 pages, 3 figures, SGEM conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The gamma-ray logging method is one of the mandatory well logging methods for\ngeophysical exploration of wells. However, during the conduct of such a study,\nthe sensor, for one reason or another, may stop recording observations in the\nwell. If a small number of values are missing, you can restore these values\nusing standard methods to fill in gaps like in time series. If data miss a\nlarge number of values, observations usually are made again, which leads to\nadditional financial costs. This work proposes an alternative solution, in the\nform of filling missed observations in data with the help of machine learning\nmethods. The main idea of this method is to construct a simple two- layer\nneural network that is trained on data from the well, and then synthesise the\nmissing values based on the trained neural network. This work evaluates the\neffectiveness of the proposed method, and gives reasons for the appropriateness\nof using different methods of filling gaps, depending on the number of missed\nvalues.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 11:43:13 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Churikov", "N. S.", ""], ["Grafeeva", "N. G.", ""]]}, {"id": "1807.01104", "submitter": "Yunus Kologlu", "authors": "Yunus Kologlu, Hasan Birinci, Sevde Ilgaz Kanalmaz, Burhan Ozyilmaz", "title": "A Multiple Linear Regression Approach For Estimating the Market Value of\n  Football Players in Forward Position", "comments": "10 Pages + Reference Page, 9 Figures, Article of the term project", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, market values of the football players in the forward positions\nare estimated using multiple linear regression by including the physical and\nperformance factors in 2017-2018 season. Players from 4 major leagues of Europe\nare examined, and by applying the test for homoscedasticity, a reasonable\nregression model within 0.10 significance level is built, and the most and the\nleast affecting factors are explained in detail.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 12:04:01 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Kologlu", "Yunus", ""], ["Birinci", "Hasan", ""], ["Kanalmaz", "Sevde Ilgaz", ""], ["Ozyilmaz", "Burhan", ""]]}, {"id": "1807.01111", "submitter": "Mahendra Saha", "authors": "Abhimanyu Singh Yadav, Sudhansu S. Maiti, Mahendra Saha and Arvind\n  Pandey", "title": "The inverse xgamma distribution: statistical properties and different\n  methods of estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper proposed a new probability distribution named as inverse xgamma\ndistribution (IXGD). Different mathematical and statistical properties,viz.,\nreliability characteristics, moments, inverse moments, stochastic ordering and\norder statistics of the proposed distribution have been derived and discussed.\nThe estimation of the parameter of IXGD has been approached by different\nmethods of estimation, namely, maximum likelihood method of estimation (MLE),\nLeast square method of estimation (LSE), Weighted least square method of\nestimation (WLSE), Cram'er-von-Mises method of estimation (CME) and maximum\nproduct spacing method of estimation (MPSE). Asymptotic confidence interval\n(ACI) of the parameter is also obtained. A simulation study has been carried\nout to compare the performance of the obtained estimators and corresponding ACI\nin terms of average widths and corresponding coverage probabilities. Finally,\ntwo real data sets have been used to demonstrate the applicability of IXGD in\nreal life situations.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 12:19:34 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Yadav", "Abhimanyu Singh", ""], ["Maiti", "Sudhansu S.", ""], ["Saha", "Mahendra", ""], ["Pandey", "Arvind", ""]]}, {"id": "1807.01137", "submitter": "Debasis Kundu Professor", "authors": "Nandini Kannan and Debasis Kundu", "title": "Simple Step-Stress Models with a Cure Fraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we consider models for time-to-event data obtained from\nexperiments in which stress levels are altered at intermediate stages during\nthe observation period. These experiments, known as step-stress tests, belong\nto the larger class of accelerated tests used extensively in the reliability\nliterature. The analysis of data from step-stress tests largely relies on the\npopular cumulative exposure model. However, despite its simple form, the\nutility of the model is limited, as it is assumed that the hazard function of\nthe underlying distribution is discontinuous at the points at which the stress\nlevels are changed, which may not be very reasonable. Due to this deficiency,\nKannan et al. \\cite{KKNT:2010} introduced the cumulative risk model, where the\nhazard function is continuous. In this paper we propose a class of parametric\nmodels based on the cumulative risk model assuming the underlying population\ncontains long-term survivors or `cured' fraction. An EM algorithm to compute\nthe maximum likelihood estimators of the unknown parameters is proposed. This\nresearch is motivated by a study on altitude decompression sickness. The\nperformance of different parametric models will be evaluated using data from\nthis study.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 13:06:38 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Kannan", "Nandini", ""], ["Kundu", "Debasis", ""]]}, {"id": "1807.01200", "submitter": "Dr Abhimanyu Singh Yadav", "authors": "Abhimanyu Singh Yadav, Hassan S. Bakouch, Sanjay Kumar Singh and Umesh\n  Singh", "title": "Power Maxwell distribution: Statistical Properties, Estimation and\n  Application", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this article, we proposed a new probability distribution named as power\nMaxwell distribution (PMaD). It is another extension of Maxwell distribution\n(MaD) which would lead more flexibility to analyze the data with non-monotone\nfailure rate. Different statistical properties such as reliability\ncharacteristics, moments, quantiles, mean deviation, generating function,\nconditional moments, stochastic ordering, residual lifetime function and\nvarious entropy measures have been derived. The estimation of the parameters\nfor the proposed probability distribution has been addressed by maximum\nlikelihood estimation method and Bayes estimation method. The Bayes estimates\nare obtained under gamma prior using squared error loss function. Lastly,\nreal-life application for the proposed distribution has been illustrated\nthrough different lifetime data.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 14:16:24 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Yadav", "Abhimanyu Singh", ""], ["Bakouch", "Hassan S.", ""], ["Singh", "Sanjay Kumar", ""], ["Singh", "Umesh", ""]]}, {"id": "1807.01239", "submitter": "Reihaneh Entezari", "authors": "Reihaneh Entezari, Patrick E. Brown, and Jeffrey S. Rosenthal", "title": "Bayesian Spatial Analysis of Hardwood Tree Counts in Forests via MCMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we perform Bayesian Inference to analyze spatial tree count\ndata from the Timiskaming and Abitibi River forests in Ontario, Canada. We\nconsider a Bayesian Generalized Linear Geostatistical Model and implement a\nMarkov Chain Monte Carlo algorithm to sample from its posterior distribution.\nHow spatial predictions for new sites in the forests change as the amount of\ntraining data is reduced is studied and compared with a Logistic Regression\nmodel without a spatial effect. Finally, we discuss a stratified sampling\napproach for selecting subsets of data that allows for potential better\npredictions.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 15:35:52 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Entezari", "Reihaneh", ""], ["Brown", "Patrick E.", ""], ["Rosenthal", "Jeffrey S.", ""]]}, {"id": "1807.01269", "submitter": "Agatha Rodrigues Mrs.", "authors": "Agatha Rodrigues and Pascal Kerschke and Carlos Alberto de B. Pereira\n  and Heike Trautmann and Carolin Wagner and Bernd Hellingrath and Adriano\n  Polpo", "title": "Estimation of component reliability from superposed renewal processes\n  with masked cause of failure by means of latent variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a system, there are identical replaceable components working for a given\ntask and a failed component is replaced by a functioning one in the\ncorresponding position, which characterizes a repairable system. Assuming that\na replaced component lifetime has the same lifetime distribution as the old\none, a single component position can be represented by a renewal process and\nthe multiple components positions for a single system form a superposed renewal\nprocess. When the interest consists in estimating the component lifetime\ndistribution, there are a considerable amount of works that deal with\nestimation methods for this kind of problem. However, the information about the\nexact position of the replaced component is not available, that is, a masked\ncause of failure. In this work, we propose two methods, a Bayesian and a\nmaximum likelihood function approaches, for estimating the failure time\ndistribution of components in a repairable system with a masked cause of\nfailure. As our proposed estimators consider latent variables, they yield\nbetter performance results compared to commonly used estimators from the\nliterature. The proposed models are generic and straightforward for any\nprobability distribution. Aside from point estimates, interval estimates are\npresented for both approaches. Using several simulations, the performances of\nthe proposed methods are illustrated and their efficiency and applicability are\nshown based on the so-called cylinder problem.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 03:29:04 GMT"}, {"version": "v2", "created": "Wed, 1 May 2019 18:06:57 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Rodrigues", "Agatha", ""], ["Kerschke", "Pascal", ""], ["Pereira", "Carlos Alberto de B.", ""], ["Trautmann", "Heike", ""], ["Wagner", "Carolin", ""], ["Hellingrath", "Bernd", ""], ["Polpo", "Adriano", ""]]}, {"id": "1807.01305", "submitter": "Marta Bofill Roig", "authors": "Marta Bofill Roig and Guadalupe G\\'omez Melis", "title": "A new approach for sizing trials with composite binary endpoints using\n  anticipated marginal values and accounting for the correlation between\n  components", "comments": null, "journal-ref": "Statistics in Medicine, 2019", "doi": "10.1002/sim.8092", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Composite binary endpoints are increasingly used as primary endpoints in\nclinical trials. When designing a trial, it is crucial to determine the\nappropriate sample size for testing the statistical differences between\ntreatment groups for the primary endpoint. As shown in this work, when using a\ncomposite binary endpoint to size a trial, one needs to specify the event rates\nand the effect sizes of the composite components as well as the correlation\nbetween them. In practice, the marginal parameters of the components can be\nobtained from previous studies or pilot trials, however, the correlation is\noften not previously reported and thus usually unknown. We first show that the\nsample size for composite binary endpoints is strongly dependent on the\ncorrelation and, second, that slight deviations in the prior information on the\nmarginal parameters may result in underpowered trials for achieving the study\nobjectives at a pre-specified significance level. We propose a general strategy\nfor calculating the required sample size when the correlation is not specified,\nand accounting for uncertainty in the marginal parameter values. We present the\nweb platform CompARE to characterize composite endpoints and to calculate the\nsample size just as we propose in this paper. We evaluate the performance of\nthe proposal with a simulation study, and illustrate it by means of a real case\nstudy using CompARE.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 17:46:48 GMT"}, {"version": "v2", "created": "Fri, 16 Nov 2018 09:37:08 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Roig", "Marta Bofill", ""], ["Melis", "Guadalupe G\u00f3mez", ""]]}, {"id": "1807.01334", "submitter": "Reihaneh Entezari", "authors": "Reihaneh Entezari", "title": "Breast Cancer Diagnosis via Classification Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze the Wisconsin Diagnostic Breast Cancer Data using\nMachine Learning classification techniques, such as the SVM, Bayesian Logistic\nRegression (Variational Approximation), and K-Nearest-Neighbors. We describe\neach model, and compare their performance through different measures. We\nconclude that SVM has the best performance among all other classifiers, while\nit competes closely with the Bayesian Logistic Regression that is ranked second\nbest method for this dataset.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 18:13:55 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Entezari", "Reihaneh", ""]]}, {"id": "1807.01510", "submitter": "Pieter Segaert", "authors": "Pieter Segaert, Marta B. Lopes, Sandra Casimiro, Susana Vinga, Peter\n  J. Rousseeuw", "title": "Robust Identification of Target Genes and Outliers in Triple-negative\n  Breast Cancer Data", "comments": null, "journal-ref": "Statistical Methods in Medical Research, 2019, Vol. 28, 3042-3056", "doi": "10.1177/0962280218794722", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correct classification of breast cancer sub-types is of high importance as it\ndirectly affects the therapeutic options. We focus on triple-negative breast\ncancer (TNBC) which has the worst prognosis among breast cancer types. Using\ncutting edge methods from the field of robust statistics, we analyze Breast\nInvasive Carcinoma (BRCA) transcriptomic data publicly available from The\nCancer Genome Atlas (TCGA) data portal. Our analysis identifies statistical\noutliers that may correspond to misdiagnosed patients. Furthermore, it is\nillustrated that classical statistical methods may fail in the presence of\nthese outliers, prompting the need for robust statistics. Using robust sparse\nlogistic regression we obtain 36 relevant genes, of which ca. 60\\% have been\npreviously reported as biologically relevant to TNBC, reinforcing the validity\nof the method. The remaining 14 genes identified are new potential biomarkers\nfor TNBC. Out of these, JAM3, SFT2D2 and PAPSS1 were previously associated to\nbreast tumors or other types of cancer. The relevance of these genes is\nconfirmed by the new DetectDeviatingCells (DDC) outlier detection technique. A\ncomparison of gene networks on the selected genes showed significant\ndifferences between TNBC and non-TNBC data. The individual role of FOXA1 in\nTNBC and non-TNBC, and the strong FOXA1-AGR2 connection in TNBC stand out. Not\nonly will our results contribute to the breast cancer/TNBC understanding and\nultimately its management, they also show that robust regression and outlier\ndetection constitute key strategies to cope with high-dimensional clinical data\nsuch as omics data.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 10:32:54 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Segaert", "Pieter", ""], ["Lopes", "Marta B.", ""], ["Casimiro", "Sandra", ""], ["Vinga", "Susana", ""], ["Rousseeuw", "Peter J.", ""]]}, {"id": "1807.01623", "submitter": "Alkeos Tsokos", "authors": "Alkeos Tsokos, Santhosh Narayanan, Ioannis Kosmidis, Gianluca Baio,\n  Mihai Cucuringu, Gavin Whitaker, Franz J. Kir\\'aly", "title": "Modeling outcomes of soccer matches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare various extensions of the Bradley-Terry model and a hierarchical\nPoisson log-linear model in terms of their performance in predicting the\noutcome of soccer matches (win, draw, or loss). The parameters of the\nBradley-Terry extensions are estimated by maximizing the log-likelihood, or an\nappropriately penalized version of it, while the posterior densities of the\nparameters of the hierarchical Poisson log-linear model are approximated using\nintegrated nested Laplace approximations. The prediction performance of the\nvarious modeling approaches is assessed using a novel, context-specific\nframework for temporal validation that is found to deliver accurate estimates\nof the test error. The direct modeling of outcomes via the various\nBradley-Terry extensions and the modeling of match scores using the\nhierarchical Poisson log-linear model demonstrate similar behavior in terms of\npredictive performance.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 14:50:00 GMT"}, {"version": "v2", "created": "Fri, 3 Aug 2018 12:50:59 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Tsokos", "Alkeos", ""], ["Narayanan", "Santhosh", ""], ["Kosmidis", "Ioannis", ""], ["Baio", "Gianluca", ""], ["Cucuringu", "Mihai", ""], ["Whitaker", "Gavin", ""], ["Kir\u00e1ly", "Franz J.", ""]]}, {"id": "1807.01635", "submitter": "Xinran Li", "authors": "Xinran Li, Peng Ding, Qian Lin, Dawei Yang, Jun S. Liu", "title": "Randomization Inference for Peer Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many previous causal inference studies require no interference, that is, the\npotential outcomes of a unit do not depend on the treatments of other units.\nHowever, this no-interference assumption becomes unreasonable when a unit\ninteracts with other units in the same group or cluster. In a motivating\napplication, a university in China admits students through two channels: the\ncollege entrance exam (also known as Gaokao) and recommendation (often based on\nOlympiads in various subjects). The university randomly assigns students to\ndorms, each of which hosts four students. Students within the same dorm live\ntogether and have extensive interactions. Therefore, it is likely that peer\neffects exist and the no-interference assumption does not hold. It is important\nto understand peer effects, because they give useful guidance for future\nroommate assignment to improve the performance of students. We define peer\neffects using potential outcomes. We then propose a randomization-based\ninference framework to study peer effects with arbitrary numbers of peers and\npeer types. Our inferential procedure does not assume any parametric model on\nthe outcome distribution. Our analysis gives useful practical guidance for\npolicy makers of the university in China.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 15:19:19 GMT"}, {"version": "v2", "created": "Mon, 30 Jul 2018 21:33:03 GMT"}, {"version": "v3", "created": "Fri, 21 Dec 2018 02:11:56 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Li", "Xinran", ""], ["Ding", "Peng", ""], ["Lin", "Qian", ""], ["Yang", "Dawei", ""], ["Liu", "Jun S.", ""]]}, {"id": "1807.01776", "submitter": "Joseph Hackman", "authors": "Joseph V. Hackman (1), Daniel J. Hruschka (2) ((1) University of Utah,\n  Department of Anthropology (2) Arizona State University, School of Human\n  Evolution and Social Change)", "title": "Disentangling basal and accrued height-for-age for cross-population\n  comparisons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objectives: Current standards for comparing stunting across human populations\nassume a universal model of child growth. Such comparisons ignore population\ndifferences that are independent of deprivation and health outcomes. This paper\npartitions variation in height-for-age that is specifically associated with\ndeprivation and health outcomes to provide a basis for cross-population\ncomparisons.\n  Materials & Methods: Using a multi-level model with a sigmoid relationship of\nresources and growth, we partition variation in height-for-age z-scores (HAZ)\nfrom 1,522,564 children across 70 countries into two components: 1) \"accrued\nHAZ\" shaped by environmental inputs (e.g., undernutrition, infectious disease,\ninadequate sanitation, poverty), and 2) a country-specific \"basal HAZ\"\nindependent of such inputs. We validate these components against\npopulation-level infant mortality rates, and assess how these basal differences\nmay affect cross-population comparisons of stunting.\n  Results: Basal HAZ differs reliably across countries (range of 1.5 SD) and is\nindependent of measures of infant mortality. By contrast, accrued HAZ captures\nstunting as impaired growth due to deprivation and is more closely associated\nwith infant mortality than observed HAZ. Ranking populations by accrued HAZ\nsuggest that populations in West Africa and the Caribbean suffer much greater\nlevels of stunting than suggested by observed HAZ.\n  Discussion: Current universal standards may dramatically underestimate\nstunting in populations with taller basal HAZ. Relying on observed HAZ rather\nthan accrued HAZ may also lead to inappropriate cross-population comparisons,\nsuch as concluding that Haitian children enjoy better conditions for growth\nthan do Indian or Guatemalan children.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 20:13:15 GMT"}, {"version": "v2", "created": "Fri, 3 Aug 2018 20:42:35 GMT"}, {"version": "v3", "created": "Tue, 7 May 2019 20:12:11 GMT"}, {"version": "v4", "created": "Sat, 17 Aug 2019 00:31:08 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Hackman", "Joseph V.", ""], ["Hruschka", "Daniel J.", ""]]}, {"id": "1807.01902", "submitter": "H{\\aa}kon Tjelmeland", "authors": "H{\\aa}kon Tjelmeland, Xin Luo and Torstein Fjeldstad", "title": "A Bayesian model for lithology/fluid class prediction using a Markov\n  mesh prior fitted from a training image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a Bayesian model for inversion of observed amplitude variation\nwith offset (AVO) data into lithology/fluid classes, and study in particular\nhow the choice of prior distribution for the lithology/fluid classes influences\nthe inversion results. Two distinct prior distributions are considered, a\nsimple manually specified Markov random field prior with a first order\nneighborhood and a Markov mesh model with a much larger neighborhood estimated\nfrom a training image. They are chosen to model both horisontal connectivity\nand vertical thickness distribution of the lithology/fluid classes, and are\ncompared on an offshore clastic oil reservoir in the North Sea. We combine both\npriors with the same linearised Gaussian likelihood function based on a\nconvolved linearised Zoeppritz relation and estimate properties of the\nresulting two posterior distributions by simulating from these distributions\nwith the Metropolis-Hastings algorithm.\n  The influence of the prior on the marginal posterior probabilities for the\nlithology/fluid classes is clearly observable, but modest. The importance of\nthe prior on the connectivity properties in the posterior realisations,\nhowever, is much stronger. The larger neighborhood of the Markov mesh prior\nenables it to identify and model connectivity and curvature much better than\nwhat can be done by the first order neighborhood Markov random field prior. As\na result, we conclude that the posterior realisations based on the Markov mesh\nprior appear with much higher lateral connectivity, which is geologically\nplausible.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 09:00:27 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Tjelmeland", "H\u00e5kon", ""], ["Luo", "Xin", ""], ["Fjeldstad", "Torstein", ""]]}, {"id": "1807.02175", "submitter": "Lucas Theis", "authors": "Katherine Storrs, Sebastiaan Van Leuven, Steve Kojder, Lucas Theis,\n  Ferenc Husz\\'ar", "title": "Adaptive Paired-Comparison Method for Subjective Video Quality\n  Assessment on Mobile Devices", "comments": null, "journal-ref": "Picture Coding Symposium, 2018", "doi": null, "report-no": null, "categories": "stat.AP eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To effectively evaluate subjective visual quality in weakly-controlled\nenvironments, we propose an Adaptive Paired Comparison method based on particle\nfiltering. As our approach requires each sample to be rated only once, the test\ntime compared to regular paired comparison can be reduced. The method works\nwith non-experts and improves reliability compared to MOS and DS-MOS methods.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 20:34:41 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Storrs", "Katherine", ""], ["Van Leuven", "Sebastiaan", ""], ["Kojder", "Steve", ""], ["Theis", "Lucas", ""], ["Husz\u00e1r", "Ferenc", ""]]}, {"id": "1807.02215", "submitter": "Chanseok Park", "authors": "Chanseok Park and Min Wang", "title": "Empirical distributions of the robustified $t$-test statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on the median and the median absolute deviation estimators, and the\nHodges-Lehmann and Shamos estimators, robustified analogues of the conventional\n$t$-test statistic are proposed. The asymptotic distributions of these\nstatistics are recently provided. However, when the sample size is small, it is\nnot appropriate to use the asymptotic distribution of the robustified $t$-test\nstatistics for making a statistical inference including hypothesis testing,\nconfidence interval, p-value, etc.\n  In this article, through extensive Monte Carlo simulations, we obtain the\nempirical distributions of the robustified $t$-test statistics and their\nquantile values. Then these quantile values can be used for making a\nstatistical inference.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 01:33:42 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Park", "Chanseok", ""], ["Wang", "Min", ""]]}, {"id": "1807.02228", "submitter": "Nada Abdalla", "authors": "Nada Abdalla and Sudipto Banerjee and Gurumurthy Ramachandran and\n  Susan Arnold", "title": "Bayesian State Space Modeling of Physical Processes in Industrial\n  Hygiene", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exposure assessment models are deterministic models derived from\nphysical-chemical laws. In real workplace settings, chemical concentration\nmeasurements can be noisy and indirectly measured. In addition, inference on\nimportant parameters such as generation and ventilation rates are usually of\ninterest since they are difficult to obtain. In this paper we outline a\nflexible Bayesian framework for parameter inference and exposure prediction. In\nparticular, we propose using Bayesian state space models by discretizing the\ndifferential equation models and incorporating information from observed\nmeasurements and expert prior knowledge. At each time point, a new measurement\nis available that contains some noise, so using the physical model and the\navailable measurements, we try to obtain a more accurate state estimate, which\ncan be called filtering. We consider Monte Carlo sampling methods for parameter\nestimation and inference under nonlinear and non-Gaussian assumptions. The\nperformance of the different methods is studied on computer-simulated and\ncontrolled laboratory-generated data. We consider some commonly used exposure\nmodels representing different physical hypotheses.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 02:54:01 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Abdalla", "Nada", ""], ["Banerjee", "Sudipto", ""], ["Ramachandran", "Gurumurthy", ""], ["Arnold", "Susan", ""]]}, {"id": "1807.02230", "submitter": "Nada Abdalla", "authors": "Nada Abdalla, Sudipto Banerjee, Gurumurthy Ramachandran, Mark Stenzel\n  and Patricia A. Stewart", "title": "Coastline Kriging: A Bayesian Approach", "comments": null, "journal-ref": "Annals of Work Exposures and Health, 2018", "doi": "10.1093/annweh/wxy058", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical interpolation of chemical concentrations at new locations is an\nimportant step in assessing a worker's exposure level. When measurements are\navailable from coastlines, as is the case in coastal clean-up operations in oil\nspills, one may need a mechanism to carry out spatial interpolation at new\nlocations along the coast. In this paper we present a simple model for\nanalyzing spatial data that is observed over a coastline. We demonstrate four\ndifferent models using two different representations of the coast using curves.\nThe four models were demonstrated on simulated data and one of them was also\ndemonstrated on a dataset from the GuLF STUDY. Our contribution here is to\noffer practicing hygienists and exposure assessors with a simple and easy\nmethod to implement Bayesian hierarchical models for analyzing and\ninterpolating coastal chemical concentrations.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 03:02:58 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Abdalla", "Nada", ""], ["Banerjee", "Sudipto", ""], ["Ramachandran", "Gurumurthy", ""], ["Stenzel", "Mark", ""], ["Stewart", "Patricia A.", ""]]}, {"id": "1807.02239", "submitter": "Sepehr Akhavan Masouleh", "authors": "Sepehr Akhavan Masouleh, Tracy Holsclaw, Babak Shahbaba, Daniel L.\n  Gillen", "title": "A Flexible Joint Longitudinal-Survival Model for Analysis of End-Stage\n  Renal Disease Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a flexible joint longitudinal-survival framework to examine the\nassociation between longitudinally collected biomarkers and a time-to-event\nendpoint. More specifically, we use our method for analyzing the survival\noutcome of end-stage renal disease patients with time-varying serum albumin\nmeasurements. Our proposed method is robust to common parametric assumptions in\nthat it avoids explicit distributional assumptions on longitudinal measures and\nallows for subject-specific baseline hazard in the survival component. Fully\njoint estimation is performed to account for the uncertainty in the estimated\nlongitudinal biomarkers included in the survival model.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 03:34:18 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Masouleh", "Sepehr Akhavan", ""], ["Holsclaw", "Tracy", ""], ["Shahbaba", "Babak", ""], ["Gillen", "Daniel L.", ""]]}, {"id": "1807.02348", "submitter": "Marcel M{\\l}y\\'nczak", "authors": "Marcel M{\\l}y\\'nczak", "title": "Data-driven causal path discovery without prior knowledge - a benchmark\n  study", "comments": "18 pages along with 8 ones as supplementary; 5 figures; 4 tables; 27\n  references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal discovery broadens the inference possibilities, as correlation does\nnot inform about the relationship direction. The common approaches were\nproposed for cases in which prior knowledge is desired, when the impact of a\ntreatment/intervention variable is discovered or to analyze time-related\ndependencies. In some practical applications, more universal techniques are\nneeded and have already been presented. Therefore, the aim of the study was to\nassess the accuracies in determining causal paths in a dataset without\nconsidering the ground truth and the contextual information. This benchmark was\nperformed on the database with cause-effect pairs, using a framework consisting\nof generalized correlations (GC), kernel regression gradients (GR) and absolute\nresiduals criteria (AR), along with causal additive modeling (CAM). The best\noverall accuracy, 80%, was achieved for the (majority voting) combination of\nGC, AR, and CAM, however, the most similar sensitivity and specificity values\nwere obtained for AR. Bootstrap simulation established the probability of\ncorrect causal path determination (which pairs should remain indeterminate).\nThe mean accuracy was then improved to 83% for the selected subset of pairs.\nThe described approach can be used for preliminary dependence assessment, as an\ninitial step for commonly used causality assessment frameworks or for\ncomparison with prior assumptions.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 10:43:32 GMT"}, {"version": "v2", "created": "Tue, 17 Jul 2018 14:48:27 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["M\u0142y\u0144czak", "Marcel", ""]]}, {"id": "1807.02616", "submitter": "Vadim Sokolov", "authors": "Vadim Sokolov, David W. Etherington, Christian Schmid, Dominik\n  Karbowski, Aymeric Rousseau, Muhammad Imran", "title": "Effects of Predictive Real-Time Traffic Signal Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper analyzes the impact of providing car drivers with predictive\ninformation on traffic signal timing in real-time, including time-to-green and\ngreen-wave speed recommendations. Over a period of six months, the behavior of\nthese 121 drivers in everyday urban driving was analyzed with and without\naccess to live traffic signal information. In a first period, drivers had the\ninformation providing service disabled in order to establish a baseline\nbehavior; after that initial phase, the service was activated. In both cases,\ndata from smartphone and vehicle sensors was collected, including speed,\nacceleration, fuel rate, acceleration and brake pedal positions. We estimated\nthe changes in the driving behavior which result from drivers' receiving the\ntraffic signal timing information by carefully comparing distributions of\nacceleration/deceleration patterns through statistical analysis. Our analysis\ndemonstrates that there is a positive effect of providing traffic signal\ninformation timing to the drivers.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jul 2018 05:45:39 GMT"}, {"version": "v2", "created": "Sat, 10 Nov 2018 01:32:02 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Sokolov", "Vadim", ""], ["Etherington", "David W.", ""], ["Schmid", "Christian", ""], ["Karbowski", "Dominik", ""], ["Rousseau", "Aymeric", ""], ["Imran", "Muhammad", ""]]}, {"id": "1807.02671", "submitter": "Philip Sansom", "authors": "Philip G. Sansom, David B. Stephenson, Daniel B. Williamson", "title": "State-space modeling of intra-seasonal persistence in daily climate\n  indices: a data-driven approach for seasonal forecasting", "comments": "16 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods for diagnosing predictability in climate indices often make\na number of unjustified assumptions about the climate system that can lead to\nmisleading conclusions. We present a flexible family of state-space models\ncapable of separating the effects of external forcing on inter-annual time\nscales, from long-term trends and decadal variability, short term weather\nnoise, observational errors and changes in autocorrelation. Standard potential\npredictability models only estimate the fraction of the total variance in the\nindex attributable to external forcing. In addition, our methodology allows us\nto partition individual seasonal means into forced, slow, fast and error\ncomponents. Changes in the predictable signal within the season can also be\nestimated. The model can also be used in forecast mode to assess both intra-\nand inter-seasonal predictability.\n  We apply the proposed methodology to a North Atlantic Oscillation index for\nthe years 1948-2017. Around 60% of the inter-annual variance in the\nDecember-January-February mean North Atlantic Oscillation is attributable to\nexternal forcing, and 8% to trends on longer time-scales. In some years, the\nexternal forcing remains relatively constant throughout the winter season, in\nothers it changes during the season. Skillful statistical forecasts of the\nDecember-January-February mean North Atlantic Oscillation are possible from the\nend of November onward and predictability extends into March. Statistical\nforecasts of the December-January-February mean achieve a correlation with the\nobservations of 0.48.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jul 2018 14:33:03 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Sansom", "Philip G.", ""], ["Stephenson", "David B.", ""], ["Williamson", "Daniel B.", ""]]}, {"id": "1807.02814", "submitter": "Eric Blankmeyer", "authors": "Eric Blankmeyer", "title": "Measurement Errors as Bad Leverage Points", "comments": "20 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Errors-in-variables is a long-standing, difficult issue in linear regression;\nand progress depends in part on new identifying assumptions. I characterize\nmeasurement error as bad-leverage points and assume that fewer than half the\nsample observations are heavily contaminated, in which case a high-breakdown\nrobust estimator may be able to isolate and down weight or discard the\nproblematic data. In simulations of simple and multiple regression where eiv\naffects 25% of the data and R-squared is mediocre, certain high-breakdown\nestimators have small bias and reliable confidence intervals.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jul 2018 13:25:45 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 11:15:57 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Blankmeyer", "Eric", ""]]}, {"id": "1807.02850", "submitter": "Sathish Vurukonda", "authors": "Siuli Mukhopadhyay, V. Sathish", "title": "Predictive Likelihood for Coherent Forecasting of Count Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new forecasting method based on the concept of the profile predictive the\nlikelihood function is proposed for discrete-valued processes. In particular,\ngeneralized autoregressive and moving average (GARMA) models for Poisson\ndistributed data are explored in details. Highest density regions are used to\nconstruct forecasting regions. The proposed forecast estimates and regions are\ncoherent. Large sample results are derived for the forecasting distribution.\nNumerical studies using simulations and a real data set are used to establish\nthe performance of the proposed forecasting method. Robustness of the proposed\nmethod to possible misspecification in the model is also studied.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jul 2018 16:41:26 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Mukhopadhyay", "Siuli", ""], ["Sathish", "V.", ""]]}, {"id": "1807.03152", "submitter": "Marcel M{\\l}y\\'nczak", "authors": "Marcel M{\\l}y\\'nczak, Hubert Krysztofiak", "title": "Discovery of causal paths in cardiorespiratory parameters: a\n  time-independent approach in elite athletes", "comments": "Accepted for publication on September 25, 2018; 24 pages; 16 figures;\n  3 tables; 68 references", "journal-ref": "www.frontiersin.org/articles/10.3389/fphys.2018.01455/", "doi": "10.3389/fphys.2018.01455", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training of elite athletes requires regular physiological and medical\nmonitoring to plan the schedule, intensity and volume of training, and\nsubsequent recovery. In sports medicine, ECG-based analyses are well\nestablished. However, they rarely consider the correspondence of respiratory\nand cardiac activity. Given such mutual influence, we hypothesize that athlete\nmonitoring might be developed with causal inference and that detailed,\ntime-related techniques should be preceded by a more general, time-independent\napproach that considers the whole group of participants and parameters\ndescribing whole signals. The aim of this study was to discover general causal\npaths among cardiac and respiratory variables in elite athletes in two body\npositions (supine and standing), at rest. ECG and impedance pneumography\nsignals were obtained from 100 elite athletes. The mean HR, the RMSSD, its\nnatural logarithm, the mean respiratory rate, the breathing activity\ncoefficients, and the resulting breathing regularity were estimated. Several\ncausal discovery frameworks were applied: generalized correlations, CAM, FGES,\nGFCI, and two Bayesian network learning algorithms: Hill-Climbing and Tabu. The\nmain, still mild, rules best supported by data are: for supine - tidal volume\ncauses heart activity variation, which causes HR, which causes respiratory\ntiming; and for standing - normalized respiratory activity variation causes\naverage heart activity. The presented approach allows data-driven and\ntime-independent analysis of elite athletes as a particular population, without\nconsidering prior knowledge. However, the results seem to be consistent with\nthe medical background. Causality inference is an interesting mathematical\napproach to the analysis of biological responses, which are complex. One can\nuse it to profile athletes and plan appropriate training.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 13:53:02 GMT"}, {"version": "v2", "created": "Wed, 26 Sep 2018 12:55:32 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["M\u0142y\u0144czak", "Marcel", ""], ["Krysztofiak", "Hubert", ""]]}, {"id": "1807.03292", "submitter": "Aiyou Chen", "authors": "Aiyou Chen, David Chan, Mike Perry, Yuxue Jin, Yunting Sun, Yueqing\n  Wang, Jim Koehler", "title": "Bias Correction For Paid Search In Media Mix Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Evaluating the return on ad spend (ROAS), the causal effect of advertising on\nsales, is critical to advertisers for understanding the performance of their\nexisting marketing strategy as well as how to improve and optimize it. Media\nMix Modeling (MMM) has been used as a convenient analytical tool to address the\nproblem using observational data. However it is well recognized that MMM\nsuffers from various fundamental challenges: data collection, model\nspecification and selection bias due to ad targeting, among others\n\\citep{chan2017,wolfe2016}.\n  In this paper, we study the challenge associated with measuring the impact of\nsearch ads in MMM, namely the selection bias due to ad targeting. Using causal\ndiagrams of the search ad environment, we derive a statistically principled\nmethod for bias correction based on the \\textit{back-door} criterion\n\\citep{pearl2013causality}. We use case studies to show that the method\nprovides promising results by comparison with results from randomized\nexperiments. We also report a more complex case study where the advertiser had\nspent on more than a dozen media channels but results from a randomized\nexperiment are not available. Both our theory and empirical studies suggest\nthat in some common, practical scenarios, one may be able to obtain an\napproximately unbiased estimate of search ad ROAS.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 17:47:25 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Chen", "Aiyou", ""], ["Chan", "David", ""], ["Perry", "Mike", ""], ["Jin", "Yuxue", ""], ["Sun", "Yunting", ""], ["Wang", "Yueqing", ""], ["Koehler", "Jim", ""]]}, {"id": "1807.03456", "submitter": "Jeremy Diaz", "authors": "Jeremy Diaz and Maxwell Joseph", "title": "Predicting property damage from tornadoes with zero-inflated neural\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tornadoes are the most violent of all atmospheric storms. In a typical year,\nthe United States experiences hundreds of tornadoes with associated damages on\nthe order of one billion dollars. Community preparation and resilience would\nbenefit from accurate predictions of these economic losses, particularly as\npopulations in tornado-prone areas increase in density and extent. Here, we use\na zero-inflated modeling approach and artificial neural networks to predict\ntornado-induced property damage using publicly available data. We developed a\nneural network that predicts whether a tornado will cause property damage\n(out-of-sample accuracy = 0.821 and area under the receiver operating\ncharacteristic curve, AUROC, = 0.872). Conditional on a tornado causing damage,\nanother neural network predicts the amount of damage (out-of-sample mean\nsquared error = 0.0918 and R2 = 0.432). When used together, these two models\nfunction as a zero-inflated log-normal regression with hidden layers. From the\nbest-performing models, we provide static and interactive gridded maps of\nmonthly predicted probabilities of damage and property damages for the year\n2019. Two primary weaknesses include (1) model fitting requires log-scale data\nwhich leads to large natural-scale residuals and (2) beginning tornado\ncoordinates were utilized rather than tornado paths. Ultimately, this is the\nfirst known study to directly model tornado-induced property damages, and all\ndata, code, and tools are publicly available. The predictive capacity of this\nmodel along with an interactive interface may provide an opportunity for\nscience-informed tornado disaster planning.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 02:39:33 GMT"}, {"version": "v2", "created": "Wed, 5 Jun 2019 22:32:08 GMT"}, {"version": "v3", "created": "Fri, 19 Jul 2019 17:54:24 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Diaz", "Jeremy", ""], ["Joseph", "Maxwell", ""]]}, {"id": "1807.03469", "submitter": "Yang Feng", "authors": "Sihan Huang and Yang Feng", "title": "Pairwise Covariates-adjusted Block Model for Community Detection", "comments": "41 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most fundamental problems in network study is community detection.\nThe stochastic block model (SBM) is one widely used model for network data with\ndifferent estimation methods developed with their community detection\nconsistency results unveiled. However, the SBM is restricted by the strong\nassumption that all nodes in the same community are stochastically equivalent,\nwhich may not be suitable for practical applications. We introduce a pairwise\ncovariates-adjusted stochastic block model (PCABM), a generalization of SBM\nthat incorporates pairwise covariate information. We study the maximum\nlikelihood estimates of the coefficients for the covariates as well as the\ncommunity assignments. It is shown that both the coefficient estimates of the\ncovariates and the community assignments are consistent under suitable sparsity\nconditions. Spectral clustering with adjustment (SCWA) is introduced to\nefficiently solve PCABM. Under certain conditions, we derive the error bound of\ncommunity estimation under SCWA and show that it is community detection\nconsistent. PCABM compares favorably with the SBM or degree-corrected\nstochastic block model (DCBM) under a wide range of simulated and real networks\nwhen covariate information is accessible.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 03:37:55 GMT"}, {"version": "v2", "created": "Sat, 20 Jun 2020 18:01:26 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Huang", "Sihan", ""], ["Feng", "Yang", ""]]}, {"id": "1807.03544", "submitter": "Lucas Macri", "authors": "Wenlong Yuan, Lucas M. Macri, Atefeh Javadi, Zhenfeng Lin and Jianhua\n  Z. Huang", "title": "Near-infrared Mira Period-Luminosity Relations in M33", "comments": null, "journal-ref": "The Astronomical Journal, 156, 3 (2018)", "doi": "10.3847/1538-3881/aad330", "report-no": null, "categories": "astro-ph.SR astro-ph.GA stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze sparsely-sampled near-infrared (JHKs) light curves of a sample of\n1781 Mira variable candidates in M33, originally discovered using I-band\ntime-series observations. We extend our single-band semi-parametric Gaussian\nprocess modeling of Mira light curves to a multi-band version and obtain\nimproved period determinations. We use our previous results on near-infrared\nproperties of candidate Miras in the LMC to classify the majority of the M33\nsample into Oxygen- or Carbon-rich subsets. We derive Period-Luminosity\nrelations for O-rich Miras and determine a distance modulus for M33 of 24.80 +-\n0.06 mag.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 09:20:39 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Yuan", "Wenlong", ""], ["Macri", "Lucas M.", ""], ["Javadi", "Atefeh", ""], ["Lin", "Zhenfeng", ""], ["Huang", "Jianhua Z.", ""]]}, {"id": "1807.03592", "submitter": "Christoph Dalitz", "authors": "Christoph Dalitz", "title": "Estimating Wealth Distribution: Top Tail and Inequality", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": "Technical Report No. 2016-01, Hochschule Niederrhein, Fachbereich\n  Elektrotechnik & Informatik (2016)", "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article describes mathematical methods for estimating the top-tail of\nthe wealth distribution and therefrom the share of total wealth that the\nrichest $p$ percent hold, which is an intuitive measure of inequality. As the\ndata base for the top-tail of the wealth distribution is inevitably less\ncomplete than the data for lower wealth, the top-tail distribution is replaced\nby a parametric model based on a Pareto distribution. The different methods for\nestimating the parameters are compared and new simulations are presented which\nfavor the maximum-likelihood estimator for the Pareto parameter $\\alpha$. New\ncriteria for the choice of other parameters are presented which have not yet\nbeen discussed in the literature before. The methods are applied to the 2012\ndata from the ECB Household and Consumption Survey (HFCS) for Germany and the\ncorresponding rich list from the Manager Magazin. In addition to a presentation\nof all formulas, R scripts implementing them are provided by the author.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 12:34:02 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Dalitz", "Christoph", ""]]}, {"id": "1807.03860", "submitter": "Peter Casey", "authors": "Peter C. Casey, Kevin H. Wilson, David Yokum", "title": "A Cautionary Tail: A Framework and Case Study for Testing Predictive\n  Model Validity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Data scientists frequently train predictive models on administrative data.\nHowever, the process that generates this data can bias predictive models,\nmaking it important to test models against their intended use. We provide a\nfield assessment framework that we use to validate a model predicting rat\ninfestations in Washington, D.C. The model was developed with data from the\ncity's 311 service request system. Although the model performs well against new\n311 data, we find that it does not perform well when predicting the outcomes of\ninspections in our field assessment. We recommend that data scientists expand\nthe use of field assessments to test their models.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 20:58:28 GMT"}, {"version": "v2", "created": "Tue, 17 Jul 2018 14:16:43 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Casey", "Peter C.", ""], ["Wilson", "Kevin H.", ""], ["Yokum", "David", ""]]}, {"id": "1807.03861", "submitter": "Mohsen Kamrani", "authors": "Mohsen Kamrani, Ramin Arvin and Asad J. Khattak", "title": "Analyzing Highly Volatile Driving Trips Taken by Alternative Fuel\n  Vehicles", "comments": "Presented at the 97th Annual Meeting Transportation Research Board,\n  January 2018. Washington D.C", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Volatile driving, characterized by fluctuations in speed and accelerations\nand aggressive lane changing/merging, is known to contribute to transportation\ncrashes. To fully understand driving volatility with the intention of reducing\nit, the objective of this study is to identify its key correlates, while\nfocusing on highly volatile trips. First, a measure of driving volatility based\non vehicle speed is applied to trip data collected in the California Household\nTravel Survey during 2012-2013. Specifically, the trips containing driving\ncycles (N=62839 trips) were analyzed to obtain driving volatility. Second,\ncorrelations of volatility with the trip, vehicle, and person level variables\nwere quantified using Ordinary Least Squares and quantile regression models.\nThe results of the 90th percentile regression (which distinguishes the 10%\nhighly volatile trips from the rest) show that trips taken by pickup trucks,\nhatchbacks, convertibles, and minivans are less volatile when compared to the\ntrips taken by sedans. Moreover, longer trips have less driving volatility. In\naddition, younger drivers are more volatile drivers than old ones. Overall, the\nresults of this study are reasonable and beneficial in identifying correlates\nof driving volatility, especially in terms of understanding factors that\ndifferentiate highly volatile trips from other trips. Reductions in driving\nvolatility have positive implications for transportation safety. From a\nmethodological standpoint, this study is an example of how to extract useful\n(volatility) information from raw vehicle speed data and use it to calm down\ndrivers and ultimately improve transportation safety.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 20:59:04 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Kamrani", "Mohsen", ""], ["Arvin", "Ramin", ""], ["Khattak", "Asad J.", ""]]}, {"id": "1807.03935", "submitter": "Philip White", "authors": "Philip A. White, Alan E. Gelfand, Eliane R. Rodrigues, Guadalupe\n  Tzintzun", "title": "Pollution State Modeling for Mexico City", "comments": null, "journal-ref": "J. R. Stat. Soc. A., 182(3), 1039-1060 (2019)", "doi": "10.1111/rssa.12444", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ground-level ozone and particulate matter pollutants are associated with a\nvariety of health issues and increased mortality. For this reason, Mexican\nenvironmental agencies regulate pollutant levels. In addition, Mexico City\ndefines pollution emergencies using thresholds that rely on regional maxima for\nozone and particulate matter with diameter less than 10 micrometers\n($\\text{PM}_{10}$). To predict local pollution emergencies and to assess\ncompliance to Mexican ambient air quality standards, we analyze hourly ozone\nand $\\text{PM}_{10}$ measurements from 24 stations across Mexico City from 2017\nusing a bivariate spatiotemporal model. Using this model, we predict future\npollutant levels using current weather conditions and recent pollutant\nconcentrations. Using hourly pollutant projections, we predict regional maxima\nneeded to estimate the probability of future pollution emergencies. We discuss\nhow predicted compliance to legislated pollution limits varies across regions\nwithin Mexico City in 2017. We find that predicted probability of pollution\nemergencies is limited to a few time periods. In contrast, we show that\npredicted exceedance of Mexican ambient air quality standards is a common,\nnearly daily occurrence.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 02:51:10 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["White", "Philip A.", ""], ["Gelfand", "Alan E.", ""], ["Rodrigues", "Eliane R.", ""], ["Tzintzun", "Guadalupe", ""]]}, {"id": "1807.04003", "submitter": "Peida Zhan", "authors": "Peida Zhan, Hong Jiao, Wen-Chung Wang, Kaiwen Man", "title": "A Multidimensional Hierarchical Framework for Modeling Speed and Ability\n  in Computer-based Multidimensional Tests", "comments": "27 pages, 3 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In psychological and educational computer-based multidimensional tests,\nlatent speed, a rate of the amount of labor performed on the items with respect\nto time, may also be multidimensional. To capture the multidimensionality of\nlatent speed, this study firstly proposed a multidimensional log-normal\nresponse time (RT) model to consider the potential multidimensional latent\nspeed. Further, to simultaneously take into account the response accuracy (RA)\nand RTs in multidimensional tests, a multidimensional hierarchical modeling\nframework was proposed. The framework is an extension of the van der Linden\n(2007; doi:10.1007/s11336-006-1478-z) and allows a \"plug-and-play approach\"\nwith alternative choices of multidimensional models for RA and RT. The model\nparameters within the framework were estimated using the Bayesian Markov chain\nMonte Carlo method. The 2012 Program for International Student Assessment\ncomputer-based mathematics data were analyzed first to illustrate the\nimplications and applications of the proposed models. The results indicated\nthat it is appropriate to simultaneously consider the multidimensionality of\nlatent speed and latent ability for multidimensional tests. A brief simulation\nstudy was conducted to evaluate the parameter recovery of the proposed model\nand the consequences of ignoring the multidimensionality of latent speed.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 08:49:55 GMT"}, {"version": "v2", "created": "Sat, 14 Jul 2018 04:20:11 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Zhan", "Peida", ""], ["Jiao", "Hong", ""], ["Wang", "Wen-Chung", ""], ["Man", "Kaiwen", ""]]}, {"id": "1807.04177", "submitter": "Mark Risser", "authors": "Mark D. Risser and Christopher J. Paciorek and Michael F. Wehner and\n  Travis A. O'Brien and William D. Collins", "title": "A probabilistic gridded product for daily precipitation extremes over\n  the United States", "comments": null, "journal-ref": null, "doi": "10.1007/s00382-019-04636-0", "report-no": null, "categories": "stat.AP physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gridded data products, for example interpolated daily measurements of\nprecipitation from weather stations, are commonly used as a convenient\nsubstitute for direct observations because these products provide a spatially\nand temporally continuous and complete source of data. However, when the goal\nis to characterize climatological features of extreme precipitation over a\nspatial domain (e.g., a map of return values) at the native spatial scales of\nthese phenomena, then gridded products may lead to incorrect conclusions\nbecause daily precipitation is a fractal field and hence any smoothing\ntechnique will dampen local extremes. To address this issue, we create a new\n\"probabilistic\" gridded product specifically designed to characterize the\nclimatological properties of extreme precipitation by applying spatial\nstatistical analyses to daily measurements of precipitation from the GHCN over\nCONUS. The essence of our method is to first estimate the climatology of\nextreme precipitation based on station data and then use a data-driven\nstatistical approach to interpolate these estimates to a fine grid. We argue\nthat our method yields an improved characterization of the climatology within a\ngrid cell because the probabilistic behavior of extreme precipitation is much\nbetter behaved (i.e., smoother) than daily weather. Furthermore, the spatial\nsmoothing innate to our approach significantly increases the signal-to-noise\nratio in the estimated extreme statistics relative to an analysis without\nsmoothing. Finally, by deriving a data-driven approach for translating extreme\nstatistics to a spatially complete grid, the methodology outlined in this paper\nresolves the issue of how to properly compare station data with output from\nearth system models. We conclude the paper by comparing our probabilistic\ngridded product with a standard extreme value analysis of the Livneh gridded\ndaily precipitation product.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 14:52:44 GMT"}, {"version": "v2", "created": "Wed, 2 Jan 2019 20:29:22 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Risser", "Mark D.", ""], ["Paciorek", "Christopher J.", ""], ["Wehner", "Michael F.", ""], ["O'Brien", "Travis A.", ""], ["Collins", "William D.", ""]]}, {"id": "1807.04516", "submitter": "Maxime Rischard", "authors": "Maxime Rischard, Zach Branson, Luke Miratrix, and Luke Bornn", "title": "A Bayesian Nonparametric Approach to Geographic Regression Discontinuity\n  Designs: Do School Districts Affect NYC House Prices?", "comments": "40 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most research on regression discontinuity designs (RDDs) has focused on\nunivariate cases, where only those units with a \"forcing\" variable on one side\nof a threshold value receive a treatment. Geographical regression discontinuity\ndesigns (GeoRDDs) extend the RDD to multivariate settings with spatial forcing\nvariables. We propose a framework for analysing GeoRDDs, which we implement\nusing Gaussian process regression. This yields a Bayesian posterior\ndistribution of the treatment effect at every point along the border. We\naddress nuances of having a functional estimand defind on a border with\npotentially intricate topology, particularly when defining and estimating\ncausal estimands of the local average treatment effect (LATE). The Bayesian\nestimate of the LATE can also be used as a test statistic in a hypothesis test\nwith good frequentist properties, which we validate using simulations and\nplacebo tests. We demonstrate our methodology with a dataset of property sales\nin New York City, to assess whether there is a discontinuity in housing prices\nat the border between two school district. We find a statistically significant\ndifference in price across the border between the districts with $p$=0.002, and\nestimate a 20% higher price on average for a house on the more desirable side.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 09:57:29 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Rischard", "Maxime", ""], ["Branson", "Zach", ""], ["Miratrix", "Luke", ""], ["Bornn", "Luke", ""]]}, {"id": "1807.04667", "submitter": "Ryan McConville", "authors": "Ryan McConville, Gareth Archer, Ian Craddock, Herman ter Horst, Robert\n  Piechocki, James Pope, Raul Santos-Rodriguez", "title": "Online Heart Rate Prediction using Acceleration from a Wrist Worn\n  Wearable", "comments": "MLMH 2018: 2018 KDD Workshop on Machine Learning for Medicine and\n  Healthcare", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the prediction of heart rate from acceleration using a\nwrist worn wearable. Although existing photoplethysmography (PPG) heart rate\nsensors provide reliable measurements, they use considerably more energy than\naccelerometers and have a major impact on battery life of wearable devices. By\nusing energy-efficient accelerometers to predict heart rate, significant energy\nsavings can be made. Further, we are interested in understanding patient\nrecovery after a heart rate intervention, where we expect a variation in heart\nrate over time. Therefore, we propose an online approach to tackle the concept\nas time passes. We evaluate the methods on approximately 4 weeks of free living\ndata from three patients over a number of months. We show that our approach can\nachieve good predictive performance (e.g., 2.89 Mean Absolute Error) while\nusing the PPG heart rate sensor infrequently (e.g., 20.25% of the samples).\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 17:08:52 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["McConville", "Ryan", ""], ["Archer", "Gareth", ""], ["Craddock", "Ian", ""], ["ter Horst", "Herman", ""], ["Piechocki", "Robert", ""], ["Pope", "James", ""], ["Santos-Rodriguez", "Raul", ""]]}, {"id": "1807.04883", "submitter": "Alistair Reid", "authors": "Alistair Reid, Xinyue Wang, Simon O'Callaghan, Daniel Steinberg,\n  Lachlan McCalman", "title": "Probabilistic Re-aggregation Algorithm [First Draft]", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial data about individuals or businesses is often aggregated over\npolygonal regions to preserve privacy, provide useful insight and support\ndecision making. Given a particular aggregation of data (say into local\ngovernment areas), the re-aggregation problem is to estimate how that same data\nwould aggregate over a different set of polygonal regions (say electorates)\nwithout having access to the original unit records. Data61 is developing new\nre-aggregation algorithms that both estimate confidence intervals of their\npredictions and utilize additional related datasets when available to improve\naccuracy. The algorithms are an improvement over the current re-aggregation\nprocedure in use by the ABS, which is manually applied by the data user, less\naccurate in validation experiments and provides a single best guess answer. The\nalgorithms are deployed in an accessible web service that automatically learns\na model and applies it to user-data. This report formulates the re-aggregation\nproblem, describes Data61's new algorithms, and presents preliminary validation\nexperiments.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 01:38:24 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Reid", "Alistair", ""], ["Wang", "Xinyue", ""], ["O'Callaghan", "Simon", ""], ["Steinberg", "Daniel", ""], ["McCalman", "Lachlan", ""]]}, {"id": "1807.05466", "submitter": "Philip White", "authors": "Philip A. White, C. Shane Reese, William F. Christensen, Summer Rupper", "title": "Non-separable Nearest-Neighbor Gaussian Process Model for Antarctic\n  Surface Mass Balance and Ice Core Site Selection", "comments": null, "journal-ref": "Environmetrics. 2019;e2579", "doi": "10.1002/env.2579", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surface mass balance (SMB) is an important factor in the estimation of sea\nlevel change, and data are collected to estimate models for prediction of SMB\nover the Antarctic ice sheets. Using a quality-controlled aggregate dataset of\nSMB field measurements with significantly more observations than previous\nanalyses, a fully Bayesian nearest-neighbor Gaussian process model is posed to\nestimate Antarctic SMB and propose new field measurement locations. A\ncorresponding Antarctic SMB map is rendered using this model and is compared\nwith previous estimates. A prediction uncertainty map is created to identify\nregions of high SMB uncertainty. The model estimates net SMB to be 2345 Gton\n$\\text{yr}^{-1}$, with 95% credible interval (2273,2413) Gton $\\text{yr}^{-1}$.\nOverall, these results suggest lower Antarctic SMB than previously reported.\nUsing the model's uncertainty quantification, we propose 25 new measurement\nsites for field study utilizing a design to minimize integrated mean squared\nerror.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jul 2018 23:44:24 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["White", "Philip A.", ""], ["Reese", "C. Shane", ""], ["Christensen", "William F.", ""], ["Rupper", "Summer", ""]]}, {"id": "1807.05600", "submitter": "Philip White", "authors": "Philip A. White, Emilio Porcu", "title": "Modeling Daily Seasonality of Mexico City Ozone using Nonseparable\n  Covariance Models on Circles Cross Time", "comments": null, "journal-ref": "Environmetrics. 2019;e2558", "doi": "10.1002/env.2558", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mexico City tracks ground-level ozone levels to assess compliance with\nnational ambient air quality standards and to prevent environmental health\nemergencies. Ozone levels show distinct daily patterns, within the city, and\nover the course of the year. To model these data, we use covariance models over\nspace, circular time, and linear time. We review existing models and develop\nnew classes of nonseparable covariance models of this type, models appropriate\nfor quasi-periodic data collected at many locations. With these covariance\nmodels, we use nearest-neighbor Gaussian processes to predict hourly ozone\nlevels at unobserved locations in April and May, the peak ozone season, to\ninfer compliance to Mexican air quality standards and to estimate respiratory\nhealth risk associated with ozone. Predicted compliance with air quality\nstandards and estimated respiratory health risk vary greatly over space and\ntime. In some regions, we predict exceedance of national standards for more\nthan a third of the hours in April and May. On many days, we predict that\nnearly all of Mexico City exceeds nationally legislated ozone thresholds at\nleast once. In peak regions, we estimate respiratory risk for ozone to be 55%\nhigher on average than the annual average risk and as much at 170% higher on\nsome days.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jul 2018 19:35:46 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["White", "Philip A.", ""], ["Porcu", "Emilio", ""]]}, {"id": "1807.05737", "submitter": "Kota Ogasawara", "authors": "Kota Ogasawara", "title": "Consumption smoothing in the working-class households of interwar Japan", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I analyze Osaka factory worker households in the early 1920s, whether\nidiosyncratic income shocks were shared efficiently, and which consumption\ncategories were robust to shocks. While the null hypothesis of full\nrisk-sharing of total expenditures was rejected, factory workers maintained\ntheir households, in that they paid for essential expenditures (rent,\nutilities, and commutation) during economic hardship. Additionally, children's\neducation expenditures were possibly robust to idiosyncratic income shocks. The\nresults suggest that temporary income is statistically significantly increased\nif disposable income drops due to idiosyncratic shocks. Historical documents\nsuggest microfinancial lending and saving institutions helped mitigate\nrisk-based vulnerabilities.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 09:02:14 GMT"}, {"version": "v2", "created": "Mon, 6 Aug 2018 06:55:12 GMT"}, {"version": "v3", "created": "Mon, 25 Mar 2019 09:02:30 GMT"}, {"version": "v4", "created": "Mon, 24 Jun 2019 14:00:18 GMT"}, {"version": "v5", "created": "Tue, 23 Jul 2019 08:43:27 GMT"}, {"version": "v6", "created": "Thu, 19 Sep 2019 08:51:04 GMT"}, {"version": "v7", "created": "Sat, 27 Jun 2020 06:41:31 GMT"}, {"version": "v8", "created": "Tue, 8 Dec 2020 12:59:24 GMT"}, {"version": "v9", "created": "Thu, 11 Feb 2021 08:13:26 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Ogasawara", "Kota", ""]]}, {"id": "1807.05834", "submitter": "Yehezkel Resheff", "authors": "Yehezkel S. Resheff and Moni Shahar", "title": "A Statistical Approach to Inferring Business Locations Based on Purchase\n  Behavior", "comments": "IEEE BigData 2018 (Intelligent Data Mining)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transaction data obtained by Personal Financial Management (PFM) services\nfrom financial institutes such as banks and credit card companies contain a\ndescription string from which the merchant, and an encoded store identifier may\nbe parsed. However, the physical location of the purchase is absent from this\ndescription. In this paper we present a method designed to recover this\nvaluable spatial information and map merchant and identifier tuples to physical\nmap locations. We begin by constructing a graph of customer sharing between\nbusinesses, and based on a small set of known \"seed\" locations we formulate\nthis task as a maximum likelihood problem based on a model of customer sharing\nbetween nearby businesses. We test our method extensively on real world data\nand provide statistics on the displacement error in many cities.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 13:08:35 GMT"}, {"version": "v2", "created": "Sun, 4 Nov 2018 21:27:59 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Resheff", "Yehezkel S.", ""], ["Shahar", "Moni", ""]]}, {"id": "1807.05846", "submitter": "Zheng Chen", "authors": "Jinbao Chen, Yawen Hou and Zheng Chen", "title": "Statistical inference methods for cumulative incidence function curves\n  at a fixed point in time", "comments": null, "journal-ref": "Communications in Statistics - Simulation and Computation, 2018", "doi": "10.1080/03610918.2018.1476697", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Competing risks data arise frequently in clinical trials. When the\nproportional subdistribution hazard assumption is violated or two cumulative\nincidence function (CIF) curves cross, rather than comparing the overall\ntreatment effects, researchers may be interested in focusing on a comparison of\nclinical utility at some fixed time points. This paper extend a series of tests\nthat are constructed based on a pseudo-value regression technique or different\ntransformation functions for CIFs and their variances based on Gaynor's or\nAalen's work, and the differences among CIFs at a given time point are\ncompared.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 13:27:35 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2018 10:11:46 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Chen", "Jinbao", ""], ["Hou", "Yawen", ""], ["Chen", "Zheng", ""]]}, {"id": "1807.05872", "submitter": "Soumyabrata Dev", "authors": "Soumyabrata Dev, Tarek AlSkaif, Murhaf Hossari, Radu Godina, Atse\n  Louwen, and Wilfried van Sark", "title": "Solar Irradiance Forecasting Using Triple Exponential Smoothing", "comments": "Published in International Conference on Smart Energy Systems and\n  Technologies (SEST) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Owing to the growing concern of global warming and over-dependence on fossil\nfuels, there has been a huge interest in last years in the deployment of\nPhotovoltaic (PV) systems for generating electricity. The output power of a PV\narray greatly depends, among other parameters, on solar irradiation. However,\nsolar irradiation has an intermittent nature and suffers from rapid\nfluctuations. This creates challenges when integrating PV systems in the\nelectricity grid and calls for accurate forecasting methods of solar\nirradiance. In this paper, we propose a triple exponential-smoothing based\nforecasting methodology for intra-hour forecasting of the solar irradiance at\nfuture lead times. We use time-series data of measured solar irradiance,\ntogether with clear-sky solar irradiance, to forecast solar irradiance up-to a\nperiod of 20 minutes. The numerical evaluation is performed using 1 year of\nweather data, collected by a PV outdoor test facility on the roof of an office\nbuilding in Utrecht University, Utrecht, The Netherlands. We benchmark our\nproposed approach with two other common forecasting approaches: persistence\nforecasting and average forecasting. Results show that the TES method has a\nbetter forecasting performance as it can capture the rapid fluctuations of\nsolar irradiance with a fair degree of accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 14:01:01 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Dev", "Soumyabrata", ""], ["AlSkaif", "Tarek", ""], ["Hossari", "Murhaf", ""], ["Godina", "Radu", ""], ["Louwen", "Atse", ""], ["van Sark", "Wilfried", ""]]}, {"id": "1807.05944", "submitter": "Bert Gunter", "authors": "Bert Gunter", "title": "Use Factorial Design To Improve Experimental Reproducibility", "comments": "18 pages, 4 Figures, 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Systematic differences in experimental materials, methods, measurements, and\ndata handling between labs, over time, and among personnel can sabotage\nexperimental reproducibility. Uncovering such differences can be difficult and\ntime consuming. Unfortunately, it is made more so when scientists employ\ntraditional experimental procedures to explore possible sources of systematic\nvariability by sequentially changing them one at a time to determine the\nmagnitude of their effects. We use two simple examples to show how and why well\nknown methods of factorial experimentation in which multiple potential sources\nare simultaneously varied provide a better alternative, can be understood as a\nstraightforward extension of standard practice, and could be embedded into the\nquality control procedures of routine experimental practice. Doing so, we\nargue, would help mitigate at least some of the problems fueling the current\n\"reproducibility crisis\" roiling some scientific disciplines.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 16:12:23 GMT"}, {"version": "v2", "created": "Tue, 17 Jul 2018 17:19:29 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Gunter", "Bert", ""]]}, {"id": "1807.05949", "submitter": "Daniel Kostner", "authors": "Daniel Kostner", "title": "Multi-criteria decision making via multivariate quantiles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel approach for solving a multiple judge, multiple criteria decision\nmaking (MCDM) problem is proposed. The ranking of alternatives that are\nevaluated based on multiple criteria is difficult, since the presence of\nmultiple criteria leads to a non-total order relation. This issue is handled by\nreinterpreting the MCDM problem as a multivariate statistics one and by solving\nit via set optimization methods. A function that ranks alternatives as well as\nadditional functions that categorize alternatives into sets of \"good\" and \"bad\"\nchoices are presented. Moreover, the paper shows that the properties of these\nfunctions ensure a logical and reasonable decision making process.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 16:16:13 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Kostner", "Daniel", ""]]}, {"id": "1807.06063", "submitter": "Tin Lok James Ng", "authors": "Tin Lok James Ng, Thomas Brendan Murphy, Ted Westling, Tyler H.\n  McCormick, Bailey K. Fosdick", "title": "Modeling the social media relationships of Irish politicians using a\n  generalized latent space stochastic blockmodel", "comments": "31 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  D\\'ail \\'Eireann is the principal chamber of the Irish parliament. The 31st\nD\\'ail \\'Eireann is the principal chamber of the Irish parliament. The 31st\nD\\'ail was in session from March 11th, 2011 to February 6th, 2016. Many of the\nmembers of the D\\'ail were active on social media and many were Twitter users\nwho followed other members of the D\\'ail. The pattern of following amongst\nthese politicians provides insights into political alignment within the D\\'ail.\nWe propose a new model, called the generalized latent space stochastic\nblockmodel, which extends and generalizes both the latent space model and the\nstochastic blockmodel to study social media connections between members of the\nD\\'ail. The probability of an edge between two nodes in a network depends on\ntheir respective class labels as well as latent positions in an unobserved\nlatent space. The proposed model is capable of representing transitivity,\nclustering, as well as disassortative mixing. A Bayesian method with Markov\nchain Monte Carlo sampling is proposed for estimation of model parameters.\nModel selection is performed using the WAIC criterion and models of different\nnumber of classes or dimensions of latent space are compared. We use the model\nto study Twitter following relationships of members of the D\\'ail and interpret\nstructure found in these relationships. We find that the following\nrelationships amongst politicians is mainly driven by past and present\npolitical party membership. We also find that the modeling outputs are\ninformative when studying voting within the D\\'ail.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 19:10:04 GMT"}, {"version": "v2", "created": "Sun, 13 Dec 2020 06:54:50 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Ng", "Tin Lok James", ""], ["Murphy", "Thomas Brendan", ""], ["Westling", "Ted", ""], ["McCormick", "Tyler H.", ""], ["Fosdick", "Bailey K.", ""]]}, {"id": "1807.06143", "submitter": "Shaofeng Zou", "authors": "Shaofeng Zou, Venugopal V. Veeravalli, Jian Li, Don Towsley", "title": "Quickest Detection of Dynamic Events in Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of quickest detection of dynamic events in networks is studied.\nAt some unknown time, an event occurs, and a number of nodes in the network are\naffected by the event, in that they undergo a change in the statistics of their\nobservations. It is assumed that the event is dynamic, in that it can propagate\nalong the edges in the network, and affect more and more nodes with time. The\nevent propagation dynamics is assumed to be unknown. The goal is to design a\nsequential algorithm that can detect a \"significant\" event, i.e., when the\nevent has affected no fewer than $\\eta$ nodes, as quickly as possible, while\ncontrolling the false alarm rate. Fully connected networks are studied first,\nand the results are then extended to arbitrarily connected networks. The\ndesigned algorithms are shown to be adaptive to the unknown propagation\ndynamics, and their first-order asymptotic optimality is demonstrated as the\nfalse alarm rate goes to zero. The algorithms can be implemented with linear\ncomputational complexity in the network size at each time step, which is\ncritical for online implementation. Numerical simulations are provided to\nvalidate the theoretical results.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 22:46:53 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Zou", "Shaofeng", ""], ["Veeravalli", "Venugopal V.", ""], ["Li", "Jian", ""], ["Towsley", "Don", ""]]}, {"id": "1807.06190", "submitter": "Orestes Manzanilla-Salazar M.Sc.", "authors": "Orestes Manzanilla-Salazar (1) and Brunilde Sans\\`o (1) ((1)\n  Polytechnique Montr\\'eal)", "title": "Privacy-preserving classifiers recognize shared mobility behaviours from\n  WiFi network imperfect data", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CR cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proves the concept that it is feasible to accurately recognize\nspecific human mobility shared patterns, based solely on the connection logs\nbetween portable devices and WiFi Access Points (APs), while preserving user's\nprivacy. We gathered data from the Eduroam WiFi network of Polytechnique\nMontreal, making omission of device tracking or physical layer data. The\nbehaviors we chose to detect were the movements associated to the end of an\nacademic class, and the patterns related to the small break periods between\nclasses. Stringent conditions were self-imposed in our experiments. The data is\nknown to have errors noise, and be susceptible to information loss. No\ncountermeasures were adopted to mitigate any of these issues. Data\npre-processing consists of basic statistics that were used in aggregating the\ndata in time intervals. We obtained accuracy values of 93.7 % and 83.3 % (via\nBagged Trees) when recognizing behaviour patterns of breaks between classes and\nend-of-classes, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 02:51:52 GMT"}, {"version": "v2", "created": "Wed, 24 Oct 2018 18:39:56 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Manzanilla-Salazar", "Orestes", ""], ["Sans\u00f2", "Brunilde", ""]]}, {"id": "1807.06312", "submitter": "Gloria Cecchini", "authors": "Gloria Cecchini, Bjoern Schelter", "title": "Analytical approach to network inference: Investigating degree\n  distribution", "comments": null, "journal-ref": null, "doi": "10.1103/PhysRevE.98.022311", "report-no": null, "categories": "physics.data-an math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When the network is reconstructed, two types of errors can occur: false\npositive and false negative errors about the presence or absence of links. In\nthis paper, the influence of these two errors on the vertex degree distribution\nis analytically analysed. Moreover, an analytic formula of the density of the\nbiased vertex degree distribution is found. In the inverse problem, we find a\nreliable procedure to reconstruct analytically the density of the vertex degree\ndistribution of any network based on the inferred network and estimates for the\nfalse positive and false negative errors based on, e.g., simulation studies.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 10:07:51 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Cecchini", "Gloria", ""], ["Schelter", "Bjoern", ""]]}, {"id": "1807.06350", "submitter": "David Howey", "authors": "Robert R. Richardson and Michael A. Osborne and David A. Howey", "title": "Battery health prediction under generalized conditions using a Gaussian\n  process transition model", "comments": null, "journal-ref": null, "doi": "10.1016/j.est.2019.03.022", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately predicting the future health of batteries is necessary to ensure\nreliable operation, minimise maintenance costs, and calculate the value of\nenergy storage investments. The complex nature of degradation renders\ndata-driven approaches a promising alternative to mechanistic modelling. This\nstudy predicts the changes in battery capacity over time using a Bayesian\nnon-parametric approach based on Gaussian process regression. These changes can\nbe integrated against an arbitrary input sequence to predict capacity fade in a\nvariety of usage scenarios, forming a generalised health model. The approach\nnaturally incorporates varying current, voltage and temperature inputs, crucial\nfor enabling real world application. A key innovation is the feature selection\nstep, where arbitrary length current, voltage and temperature measurement\nvectors are mapped to fixed size feature vectors, enabling them to be\nefficiently used as exogenous variables. The approach is demonstrated on the\nopen-source NASA Randomised Battery Usage Dataset, with data of 26 cells aged\nunder randomized operational conditions. Using half of the cells for training,\nand half for validation, the method is shown to accurately predict non-linear\ncapacity fade, with a best case normalised root mean square error of 4.3%,\nincluding accurate estimation of prediction uncertainty.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 11:21:53 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Richardson", "Robert R.", ""], ["Osborne", "Michael A.", ""], ["Howey", "David A.", ""]]}, {"id": "1807.06770", "submitter": "Wenfei Du", "authors": "Wenfei Du and Rob Tibshirani", "title": "A pliable lasso for the Cox model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a pliable lasso method for estimation of interaction effects in\nthe Cox proportional hazards model framework. The pliable lasso is a linear\nmodel that includes interactions between covariates X and a set of modifying\nvariables Z and assumes sparsity of the main effects and interaction effects.\nThe hierarchical penalty excludes interaction effects when the corresponding\nmain effects are zero: this avoids overfitting and an explosion of model\ncomplexity. We extend this method to the Cox model for survival data,\nincorporating modifiers that are either fixed or varying in time into the\npartial likelihood. For example, this allows modeling of survival times that\ndiffer based on interactions of genes with age, gender, or other demographic\ninformation. The optimization is done by blockwise coordinate descent on a\nsecond order approximation of the objective.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 04:09:47 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Du", "Wenfei", ""], ["Tibshirani", "Rob", ""]]}, {"id": "1807.06911", "submitter": "Marcel Ausloos", "authors": "Marcel Ausloos (U. Leicester) and Roy Cerqueti (U. Macerata)", "title": "Intriguing yet simple skewness - kurtosis relation in economic and\n  demographic data distributions; pointing to preferential attachment processes", "comments": "29 pages, 5 tables, 10 figures, 56 references", "journal-ref": "Journal of Applied Statistics, 45:12, 2202-2218 (2018)", "doi": "10.1080/02664763.2017.1413077", "report-no": null, "categories": "stat.AP math.ST physics.soc-ph stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose that relations between high order moments of data\ndistributions, for example between the skewness (S) and kurtosis (K), allow to\npoint to theoretical models with understandable structural parameters. The\nillustrative data concerns two cases: (i) the distribution of income taxes and\n(ii) that of inhabitants, after aggregation over each city in each province of\nItaly in 2011. Moreover, from the rank-size relationship, for either S or K, in\nboth cases, it is shown that one obtains the parameters of the underlying\n(hypothetical) modeling distribution: in the present cases, the 2-parameter\nBeta function, - itself related to the Yule-Simon distribution function, whence\nsuggesting a growth model based on the preferential attachment process.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 13:15:57 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Ausloos", "Marcel", "", "U. Leicester"], ["Cerqueti", "Roy", "", "U. Macerata"]]}, {"id": "1807.06994", "submitter": "Debraj Roy", "authors": "Debraj Roy, David Bernal, Michael Lees", "title": "An exploratory factor analysis model for slum severity index in Mexico\n  City", "comments": "Submitted to Urban Studies", "journal-ref": null, "doi": "10.1177/0042098019869769", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Mexico, 25 per cent of the urban population now lives in informal\nsettlements with varying degree of depravity. Although some informal\nneighbourhoods have contributed to the upward mobility of the inhabitants, the\nmajority still lack basic services. Mexico City and the conurbation around it,\nform a mega city of 21 million people that has been growing in a manner\nqualified as \"highly unproductive, (that) deepens inequality, raises pollution\nlevels\" and contains the largest slum in the world, Neza-Chalco-Izta. Urban\nreforms are now aiming to better the conditions in these slums and therefore it\nis very important to have reliable measurement tools to assess the changes that\nare undergoing. In this paper, we use exploratory factor analysis to define an\nindex of depravity in Mexico City, namely the Slum Severity Index (SSI), based\non the UN-HABITATs definition of a slum. We apply this novel approach to the\nCensus survey of Mexico and measure the housing deprivation levels types from\n1990 - 2010. The analysis highlights high variability in housing conditions\nwithin Mexico City. We find that the SSI decreased significantly between 1990 -\n2000 due to several policy reforms, but increased between 2000 - 2010. We also\nshow correlations of the SSI with other social factors such as education,\nhealth and migration. We present a validation of the SSI using Grey Level\nCo-occurrence Matrix (GLCM) features extracted from Very-High Resolution (VHR)\nremote-sensed satellite images. Finally, we show that the SSI can present a\ncardinally meaningful assessment of the extent of the difference in depravity\nas compared to a similar index defined by CONEVAL, a government institution\nthat studies poverty in Mexico.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 15:27:43 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Roy", "Debraj", ""], ["Bernal", "David", ""], ["Lees", "Michael", ""]]}, {"id": "1807.07024", "submitter": "Brennan Klein", "authors": "Stefano Balietti, Brennan Klein, Christoph Riedl", "title": "Optimal design of experiments to identify latent behavioral types", "comments": null, "journal-ref": "published in Experimental Economics (2020)", "doi": "10.1007/s10683-020-09680-w", "report-no": null, "categories": "stat.AP cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimal experiments that maximize the information gained from\ncollected data are critical to efficiently identify behavioral models. We\nextend a seminal method for designing Bayesian optimal experiments by\nintroducing two computational improvements that make the procedure tractable:\n(1) a search algorithm from artificial intelligence that efficiently explores\nthe space of possible design parameters, and (2) a sampling procedure which\nevaluates each design parameter combination more efficiently. We apply our\nprocedure to a game of imperfect information to evaluate and quantify the\ncomputational improvements. We then collect data across five different\nexperimental designs to compare the ability of the optimal experimental design\nto discriminate among competing behavioral models against the experimental\ndesigns chosen by a \"wisdom of experts\" prediction experiment. We find that\ndata from the experiment suggested by the optimal design approach requires\nsignificantly less data to distinguish behavioral models (i.e., test\nhypotheses) than data from the experiment suggested by experts. Substantively,\nwe find that reinforcement learning best explains human decision-making in the\nimperfect information game and that behavior is not adequately described by the\nBayesian Nash equilibrium. Our procedure is general and computationally\nefficient and can be applied to dynamically optimize online experiments.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 09:14:37 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2018 23:59:53 GMT"}, {"version": "v3", "created": "Thu, 6 Aug 2020 21:31:32 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Balietti", "Stefano", ""], ["Klein", "Brennan", ""], ["Riedl", "Christoph", ""]]}, {"id": "1807.07120", "submitter": "Tommaso Nesti", "authors": "Ana Radovanovic, Tommaso Nesti, Bokan Chen", "title": "A Holistic Approach to Forecasting Wholesale Energy Market Prices", "comments": "14 pages, 14 figures. Accepted for publication in IEEE Transactions\n  on Power Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electricity market price predictions enable energy market participants to\nshape their consumption or supply while meeting their economic and\nenvironmental objectives. By utilizing the basic properties of the\nsupply-demand matching process performed by grid operators, known as Optimal\nPower Flow (OPF), we develop a methodology to recover energy market's structure\nand predict the resulting nodal prices by using only publicly available data,\nspecifically grid-wide generation type mix, system load, and historical prices.\nOur methodology uses the latest advancements in statistical learning to cope\nwith high dimensional and sparse real power grid topologies, as well as scarce,\npublic market data, while exploiting structural characteristics of the\nunderlying OPF mechanism. Rigorous validations using the Southwest Power Pool\n(SPP) market data reveal a strong correlation between the grid level mix and\ncorresponding market prices, resulting in accurate day-ahead predictions of\nreal time prices. The proposed approach demonstrates remarkable proximity to\nthe state-of-the-art industry benchmark while assuming a fully decentralized,\nmarket-participant perspective. Finally, we recognize the limitations of the\nproposed and other evaluated methodologies in predicting large price spike\nvalues.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 19:44:25 GMT"}, {"version": "v2", "created": "Fri, 7 Jun 2019 19:41:10 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Radovanovic", "Ana", ""], ["Nesti", "Tommaso", ""], ["Chen", "Bokan", ""]]}, {"id": "1807.07133", "submitter": "Philipp Hunziker", "authors": "Philipp Hunziker (Northeastern University), Julian Wucherpfennig\n  (Hertie School of Governance), Aya Kachi (University of Basel) and\n  Nils-Christian Bormann (University of Exeter)", "title": "A Scalable MCEM Estimator for Spatio-Temporal Autoregressive Models", "comments": "29 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Very large spatio-temporal lattice data are becoming increasingly common\nacross a variety of disciplines. However, estimating interdependence across\nspace and time in large areal datasets remains challenging, as existing\napproaches are often (i) not scalable, (ii) designed for conditionally Gaussian\noutcome data, or (iii) are limited to cross-sectional and univariate outcomes.\nThis paper proposes an MCEM estimation strategy for a family of latent-Gaussian\nmultivariate spatio-temporal models that addresses these issues. The proposed\nestimator is applicable to a wide range of non-Gaussian outcomes, and\nimplementations for binary and count outcomes are discussed explicitly. The\nmethodology is illustrated on simulated data, as well as on weekly data of\nIS-related events in Syrian districts.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 20:20:05 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Hunziker", "Philipp", "", "Northeastern University"], ["Wucherpfennig", "Julian", "", "Hertie School of Governance"], ["Kachi", "Aya", "", "University of Basel"], ["Bormann", "Nils-Christian", "", "University of Exeter"]]}, {"id": "1807.07268", "submitter": "Emi Tanaka", "authors": "Emi Tanaka", "title": "Simple robust genomic prediction and outlier detection for a\n  multi-environmental field trial", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of plant breeding trials is often to identify germplasms that are\nwell adapt to target environments. These germplasms are identified through\ngenomic prediction from the analysis of multi-environmental field trial (MET)\nusing linear mixed models. The occurrence of outliers in MET are common and\nknown to adversely impact accuracy of genomic prediction yet the detection of\noutliers, and subsequently its treatment, are often neglected. A number of\nreasons stand for this - complex data such as MET give rise to distinct levels\nof residuals and thus offers additional challenges of an outlier detection\nmethod and many linear mixed model software are ill-equipped for robust\nprediction. We present outlier detection methods using a holistic approach that\nborrows the strength across trials. We furthermore evaluate a simple robust\ngenomic prediction that is applicable to any linear mixed model software. These\nare demonstrated using simulation based on two real bread wheat yield METs with\na partially replicated design and an alpha lattice design.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 07:36:21 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Tanaka", "Emi", ""]]}, {"id": "1807.07536", "submitter": "Konstantinos Pelechrinis", "authors": "Konstantinos Pelechrinis and Wayne Winston", "title": "A Skellam Regression Model for Quantifying Positional Value in Soccer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Soccer is undeniably the most popular sport world-wide and everyone from\ngeneral managers and coaching staff to fans and media are interested in\nevaluating players' performance. Metrics applied successfully in other sports,\nsuch as the (adjusted) +/- that allows for division of credit among a\nbasketball team's players, exhibit several challenges when applied to soccer\ndue to severe co-linearities. Recently, a number of player evaluation metrics\nhave been developed utilizing optical tracking data, but they are based on\nproprietary data. In this work, our objective is to develop an open framework\nthat can estimate the expected contribution of a soccer player to his team's\nwinning chances using publicly available data. In particular, using data from\n(i) approximately 20,000 games from 11 European leagues over 8 seasons, and,\n(ii) player ratings from the FIFA video game, we estimate through a Skellam\nregression model the importance of every line (attackers, midfielders,\ndefenders and goalkeeping) in winning a soccer game. We consequently translate\nthe model to expected league points added above a replacement player (eLPAR).\nThis model can further be used as a guide for allocating a team's salary budget\nto players based on their expected contributions on the pitch. We showcase\nsimilar applications using annual salary data from the English Premier League\nand identify evidence that in our dataset the market appears to under-value\ndefensive line players relative to goalkeepers.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 16:57:52 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2018 13:40:02 GMT"}, {"version": "v3", "created": "Mon, 20 Aug 2018 15:18:45 GMT"}, {"version": "v4", "created": "Wed, 23 Jan 2019 00:55:08 GMT"}, {"version": "v5", "created": "Tue, 1 Dec 2020 04:15:29 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Pelechrinis", "Konstantinos", ""], ["Winston", "Wayne", ""]]}, {"id": "1807.07646", "submitter": "Nikita Basov", "authors": "Nikita Basov (Centre for German and European Studies, St. Petersburg\n  State University)", "title": "Socio-Material Network Analysis: A Mixed Method Study of Five European\n  Artistic Collectives", "comments": null, "journal-ref": null, "doi": "10.1016/j.socnet.2018.02.003", "report-no": null, "categories": "cs.CY stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, I argue that we can better understand the relationship between\nsocial structure and materiality by combining qualitative analysis of practices\nin shared physical space with statistical analysis. Drawing on the two-mode\napproach, I treat social and material structures together with the relationship\nbetween them as a two-level socio-material network. In a mixed method study,\nformalized ethnographic data on such networks in five European artistic\ncollectives are subjected to multilevel exponential random graph modelling. It\nsheds light on how different types of interpersonal ties condition the\nengagement of individuals with similar materiality over time.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 13:10:15 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Basov", "Nikita", "", "Centre for German and European Studies, St. Petersburg\n  State University"]]}, {"id": "1807.07911", "submitter": "Hans Dembinski", "authors": "Hans Dembinski and Michael Schmelling and Roland Waldi", "title": "Application of the Iterated Weighted Least-Squares Fit to counting\n  experiments", "comments": "Accepted by NIMA", "journal-ref": null, "doi": "10.1016/j.nima.2019.05.086", "report-no": null, "categories": "physics.data-an hep-ex stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Least-squares fits are an important tool in many data analysis applications.\nIn this paper, we review theoretical results, which are relevant for their\napplication to data from counting experiments. Using a simple example, we\nillustrate the well known fact that commonly used variants of the least-squares\nfit applied to Poisson-distributed data produce biased estimates. The bias can\nbe overcome with an iterated weighted least-squares method, which produces\nresults identical to the maximum-likelihood method. For linear models, the\niterated weighted least-squares method converges faster than the equivalent\nmaximum-likelihood method, and does not require problem-specific starting\nvalues, which may be a practical advantage. The equivalence of both methods\nalso holds for binomially distributed data. We further show that the unbinned\nmaximum-likelihood method can be derived as a limiting case of the iterated\nleast-squares fit when the bin width goes to zero, which demonstrates a deep\nconnection between the two methods.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 16:01:39 GMT"}, {"version": "v2", "created": "Tue, 6 Nov 2018 16:41:34 GMT"}, {"version": "v3", "created": "Thu, 20 Dec 2018 14:35:14 GMT"}, {"version": "v4", "created": "Wed, 9 Jan 2019 09:54:40 GMT"}, {"version": "v5", "created": "Mon, 14 Jan 2019 13:33:46 GMT"}, {"version": "v6", "created": "Mon, 11 Feb 2019 11:53:16 GMT"}, {"version": "v7", "created": "Fri, 15 Feb 2019 13:16:19 GMT"}, {"version": "v8", "created": "Tue, 4 Jun 2019 09:29:15 GMT"}, {"version": "v9", "created": "Thu, 6 Jun 2019 13:00:04 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Dembinski", "Hans", ""], ["Schmelling", "Michael", ""], ["Waldi", "Roland", ""]]}, {"id": "1807.07958", "submitter": "Marco Geraci", "authors": "Marco Geraci, Nansi S. Boghossian, Alessio Farcomeni, Jeffrey D.\n  Horbar", "title": "Quantile contours and allometric modelling for risk classification of\n  abnormal ratios with an application to asymmetric growth-restriction in\n  preterm infants", "comments": "31 pages, 3 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an approach to risk classification based on quantile contours and\nallometric modelling of multivariate anthropometric measurements. We propose\nthe definition of allometric direction tangent to the directional quantile\nenvelope, which divides ratios of measurements into half-spaces. This in turn\nprovides an operational definition of directional quantile that can be used as\ncutoff for risk assessment. We show the application of the proposed approach\nusing a large dataset from the Vermont Oxford Network containing observations\nof birthweight (BW) and head circumference (HC) for more than 150,000 preterm\ninfants. Our analysis suggests that disproportionately growth-restricted\ninfants with a larger HC-to-BW ratio are at increased mortality risk as\ncompared to proportionately growth-restricted infants. The role of maternal\nhypertension is also investigated.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 19:49:28 GMT"}, {"version": "v2", "created": "Fri, 7 Jun 2019 12:01:31 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Geraci", "Marco", ""], ["Boghossian", "Nansi S.", ""], ["Farcomeni", "Alessio", ""], ["Horbar", "Jeffrey D.", ""]]}, {"id": "1807.08030", "submitter": "Mevin Hooten", "authors": "Mevin B. Hooten, Henry R. Scharf, Juan M. Morales", "title": "Running on empty: Recharge dynamics from animal movement data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vital rates such as survival and recruitment have always been important in\nthe study of population and community ecology. At the individual level,\nphysiological processes such as energetics are critical in understanding\nbiomechanics and movement ecology and also scale up to influence food webs and\ntrophic cascades. Although vital rates and population-level characteristics are\ntied with individual-level animal movement, most statistical models for\ntelemetry data are not equipped to provide inference about these relationships\nbecause they lack the explicit, mechanistic connection to physiological\ndynamics. We present a framework for modeling telemetry data that explicitly\nincludes an aggregated physiological process associated with decision making\nand movement in heterogeneous environments. Our framework accommodates a wide\nrange of movement and physiological process specifications. We illustrate a\nspecific model formulation in continuous-time to provide direct inference about\ngains and losses associated with physiological processes based on movement. Our\napproach can also be extended to accommodate auxiliary data when available. We\ndemonstrate our model to infer mountain lion (in Colorado, USA) and African\nbuffalo (in Kruger National Park, South Africa) recharge dynamics.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 21:10:33 GMT"}, {"version": "v2", "created": "Mon, 8 Oct 2018 03:23:10 GMT"}, {"version": "v3", "created": "Fri, 12 Oct 2018 18:08:48 GMT"}, {"version": "v4", "created": "Wed, 14 Nov 2018 18:59:19 GMT"}, {"version": "v5", "created": "Thu, 30 Jan 2020 19:58:36 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Hooten", "Mevin B.", ""], ["Scharf", "Henry R.", ""], ["Morales", "Juan M.", ""]]}, {"id": "1807.08125", "submitter": "Xinwei Sun", "authors": "Xinwei Sun, Lingjing Hu, Fandong Zhang, Yuan Yao and Yizhou Wang", "title": "FDR-HS: An Empirical Bayesian Identification of Heterogenous Features in\n  Neuroimage Analysis", "comments": "Accepted in Miccai, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies found that in voxel-based neuroimage analysis, detecting and\ndifferentiating \"procedural bias\" that are introduced during the preprocessing\nsteps from lesion features, not only can help boost accuracy but also can\nimprove interpretability. To the best of our knowledge, GSplit LBI is the first\nmodel proposed in the literature to simultaneously capture both procedural bias\nand lesion features. Despite the fact that it can improve prediction power by\nleveraging the procedural bias, it may select spurious features due to the\nmulticollinearity in high dimensional space. Moreover, it does not take into\naccount the heterogeneity of these two types of features. In fact, the\nprocedural bias and lesion features differ in terms of volumetric change and\nspatial correlation pattern. To address these issues, we propose a \"two-groups\"\nEmpirical-Bayes method called \"FDR-HS\" (False-Discovery-Rate Heterogenous\nSmoothing). Such method is able to not only avoid multicollinearity, but also\nexploit the heterogenous spatial patterns of features. In addition, it enjoys\nthe simplicity in implementation by introducing hidden variables, which turns\nthe problem into a convex optimization scheme and can be solved efficiently by\nthe expectation-maximum (EM) algorithm. Empirical experiments have been\nevaluated on the Alzheimer's Disease Neuroimage Initiative (ADNI) database. The\nadvantage of the proposed model is verified by improved interpretability and\nprediction power using selected features by FDR-HS.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jul 2018 10:52:31 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Sun", "Xinwei", ""], ["Hu", "Lingjing", ""], ["Zhang", "Fandong", ""], ["Yao", "Yuan", ""], ["Wang", "Yizhou", ""]]}, {"id": "1807.08458", "submitter": "Dhafer Malouche", "authors": "Rim Lahmandi-Ayed and Dhafer Malouche", "title": "More investment in Research and Development for better Education in the\n  future?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The question in this paper is whether R&D efforts affect education\nperformance in small classes. Merging two datasets collected from the PISA\nstudies and the World Development Indicators and using Learning Bayesian\nNetworks, we prove the existence of a statistical causal relationship between\ninvestment in R&D of a country and its education performance (PISA scores). We\nalso prove that the effect of R\\&D on Education is long term as a country has\nto invest at least 10 years before beginning to improve the level of young\npupils.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 07:26:41 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Lahmandi-Ayed", "Rim", ""], ["Malouche", "Dhafer", ""]]}, {"id": "1807.08513", "submitter": "Luigi Lombardo", "authors": "Luigi Lombardo, Haakon Bakka, Hakan Tanyas, Cees van Westen, P. Martin\n  Mai, Raphael Huser", "title": "Geostatistical modeling to capture seismic-shaking patterns from\n  earthquake-induced landslides", "comments": null, "journal-ref": null, "doi": "10.1029/2019JF005056", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate earthquake-induced landslides using a\ngeostatistical model that includes a latent spatial effect (LSE). The LSE\nrepresents the spatially structured residuals in the data, which are\ncomplementary to the information carried by the covariates. To determine\nwhether the LSE can capture the residual signal from a given trigger, we test\nwhether the LSE is able to capture the pattern of seismic shaking caused by an\nearthquake from the distribution of seismically induced landslides, without\nprior knowledge of the earthquake being included in the statistical model. We\nassess the landslide intensity, i.e., the expected number of landslide\nactivations per mapping unit, for the area in which landslides triggered by the\nWenchuan (M 7.9, May 12, 2008) and Lushan (M 6.6, April 20, 2013) earthquakes\noverlap. We chose an area of overlapping landslides in order to test our method\non landslide inventories located in the near and far fields of the earthquake.\nWe generated three different models for each earthquake-induced landslide\nscenario: i) seismic parameters only (as a proxy for the trigger); ii) the LSE\nonly; and iii) both seismic parameters and the LSE. The three configurations\nshare the same set of morphometric covariates. This allowed us to study the\npattern in the LSE and assess whether it adequately approximated the effects of\nseismic wave propagation. Moreover, it allowed us to check whether the LSE\ncaptured effects that are not explained by the shaking levels, such as\ntopographic amplification. Our results show that the LSE reproduced the shaking\npatterns in space for both earthquakes with a level of spatial detail even\ngreater than the seismic parameters. In addition, the models including the LSE\nperform better than conventional models featuring seismic parameters only.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 10:21:10 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Lombardo", "Luigi", ""], ["Bakka", "Haakon", ""], ["Tanyas", "Hakan", ""], ["van Westen", "Cees", ""], ["Mai", "P. Martin", ""], ["Huser", "Raphael", ""]]}, {"id": "1807.08537", "submitter": "Tomoki Tokuda", "authors": "Tomoki Tokuda and Hirohiko Shimada", "title": "Classes of low-frequency earthquakes based on inter-time distribution\n  reveal a precursor event for the 2011 Great Tohoku Earthquake", "comments": null, "journal-ref": "Scientific Reports, 9, 2019", "doi": "10.1038/s41598-019-45765-0", "report-no": null, "categories": "physics.geo-ph cond-mat.stat-mech stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, slow earthquakes (slow EQ) have received much attention relative to\nunderstanding the mechanisms underlying large earthquakes and to detecting\ntheir precursors. Low-frequency earthquakes (LFE) are a specific type of slow\nEQ. In the present paper, we reveal the relevance of LFEs to the 2011 Great\nTohoku Earthquake (Tohoku-oki EQ) by means of cluster analysis. We classified\nLFEs in northern Japan in a data-driven manner, based on inter-time, the time\ninterval between neighboring LFEs occurring within 10 km. We found that there\nare four classes of LFE that are characterized by median inter-times of 24\nseconds, 27 minutes, 2.0 days, and 35 days, respectively. Remarkably, in\nexamining the relevance of these classes to the Tohoku-oki EQ, we found that\nactivity in the shortest inter-time class (median 23 seconds) diminished\nsignificantly at least three months before the Tohoku-oki EQ, and became\ncompletely quiescent 30 days before the event (p-value = 0.00014). Further\nstatistical analysis implies that this class, together with a similar class of\nvolcanic tremor, may have served as a precursor of the Tohoku-oki EQ. We\ndiscuss a generative model for these classes of LFE, in which the shortest\ninter-time class is characterized by a generalized gamma distribution with the\nproduct of shape parameters 1.54 in the domain of inter-time close to zero. We\ngive a possible geodetic interpretation for the relevance of LFE to the\nTohoku-oki EQ.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 11:26:15 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Tokuda", "Tomoki", ""], ["Shimada", "Hirohiko", ""]]}, {"id": "1807.08926", "submitter": "James Watson", "authors": "Oliver Watson and Isidro Cortes-Ciriano and Aimee Taylor and James A\n  Watson", "title": "A decision theoretic approach to model evaluation in computational drug\n  discovery", "comments": "11 pages and 3 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence, trained via machine learning or computational\nstatistics algorithms, holds much promise for the improvement of small molecule\ndrug discovery. However, structure-activity data are high dimensional with low\nsignal-to-noise ratios and proper validation of predictive methods is\ndifficult. It is poorly understood which, if any, of the currently available\nmachine learning algorithms will best predict new candidate drugs. 25 publicly\navailable molecular datasets were extracted from ChEMBL. Neural nets, random\nforests, support vector machines (regression) and ridge regression were then\nfitted to the structure-activity data. A new validation method, based on\nquantile splits on the activity distribution function, is proposed for the\nconstruction of training and testing sets. Model validation based on random\npartitioning of available data favours models which overfit and `memorize' the\ntraining set, namely random forests and deep neural nets. Partitioning based on\nquantiles of the activity distribution correctly penalizes models which can\nextrapolate onto structurally different molecules outside of the training data.\nThis approach favours more constrained models, namely ridge regression and\nsupport vector regression. In addition, our new rank-based loss functions give\nconsiderably different results from mean squared error highlighting the\nnecessity to define model optimality with respect to the decision task at hand.\nModel performance should be evaluated from a decision theoretic perspective\nwith subjective loss functions. Data-splitting based on the separation of high\nand low activity data provides a robust methodology for determining the best\nextrapolating model. Simpler, traditional statistical methods such as ridge\nregression outperform state-of-the-art machine learning methods in this\nsetting.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 06:50:45 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Watson", "Oliver", ""], ["Cortes-Ciriano", "Isidro", ""], ["Taylor", "Aimee", ""], ["Watson", "James A", ""]]}, {"id": "1807.08959", "submitter": "Bruno Torresani", "authors": "Marie-Christine Roubaud (I2M), Jean-Marc Lina (ETS), Julie Carrier\n  (CEAMS), B Torr\\'esani (I2M)", "title": "Space-Time Extension of the MEM Approach for Electromagnetic\n  Neuroimaging", "comments": null, "journal-ref": "IEEE International Workshop on Machine Learning for Signal\n  Processing, Sep 2018, Aalborg, Denmark", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The wavelet Maximum Entropy on the Mean (wMEM) approach to the MEG inverse\nproblem is revisited and extended to infer brain activity from full space-time\ndata. The resulting dimensionality increase is tackled using a collection of\ntechniques , that includes time and space dimension reduction (using\nrespectively wavelet and spatial filter based reductions), Kronecker product\nmodeling for covariance matrices, and numerical manipulation of the free energy\ndirectly in matrix form. This leads to a smooth numerical optimization problem\nof reasonable dimension, solved using standard approaches. The method is\napplied to the MEG inverse problem. Results of a simulation study in the\ncontext of slow wave localization from sleep MEG data are presented and\ndiscussed.\n  Index Terms: MEG inverse problem, maximum entropy on the mean, wavelet\ndecomposition, spatial filters, Kronecker covariance factorization, sleep slow\nwaves.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 08:42:58 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Roubaud", "Marie-Christine", "", "I2M"], ["Lina", "Jean-Marc", "", "ETS"], ["Carrier", "Julie", "", "CEAMS"], ["Torr\u00e9sani", "B", "", "I2M"]]}, {"id": "1807.09120", "submitter": "Mohamad Kazem Shirani Faradonbeh", "authors": "Mohamad Kazem Shirani Faradonbeh, Ambuj Tewari, and George Michailidis", "title": "Finite Time Adaptive Stabilization of LQ Systems", "comments": "arXiv admin note: substantial text overlap with arXiv:1711.07230", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY math.PR stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stabilization of linear systems with unknown dynamics is a canonical problem\nin adaptive control. Since the lack of knowledge of system parameters can cause\nit to become destabilized, an adaptive stabilization procedure is needed prior\nto regulation. Therefore, the adaptive stabilization needs to be completed in\nfinite time. In order to achieve this goal, asymptotic approaches are not very\nhelpful. There are only a few existing non-asymptotic results and a full\ntreatment of the problem is not currently available.\n  In this work, leveraging the novel method of random linear feedbacks, we\nestablish high probability guarantees for finite time stabilization. Our\nresults hold for remarkably general settings because we carefully choose a\nminimal set of assumptions. These include stabilizability of the underlying\nsystem and restricting the degree of heaviness of the noise distribution. To\nderive our results, we also introduce a number of new concepts and technical\ntools to address regularity and instability of the closed-loop matrix.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jul 2018 22:36:42 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Faradonbeh", "Mohamad Kazem Shirani", ""], ["Tewari", "Ambuj", ""], ["Michailidis", "George", ""]]}, {"id": "1807.09157", "submitter": "Geng Chen", "authors": "Geng Chen, Pei Zhang, Ke Li, Chong-Yaw Wee, Wenliang Pan, Yafeng Wu,\n  Panteleimon Giannakopoulos, Sven Haller, Dinggang Shen, Pew-Thian Yap", "title": "Robust Group Comparison Using Non-Parametric Block-Based Statistics", "comments": "17 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Voxel-based analysis methods localize brain structural differences by\nperforming voxel-wise statistical comparisons on two groups of images aligned\nto a common space. This procedure requires highly accurate registration as well\nas a sufficiently large dataset. However, in practice, the registration\nalgorithms are not perfect due to noise, artifacts, and complex structural\nvariations. The sample size is also limited due to low disease prevalence,\nrecruitment difficulties, and demographic matching issues. To address these\nissues, in this paper, we propose a method, called block-based statistic (BBS),\nfor robust group comparison. BBS consists of two major components: Block\nmatching and permutation test. Specifically, based on two group of images\naligned to a common space, we first perform block matching so that structural\nmisalignments can be corrected. Then, based on results given by block matching,\nwe conduct robust non-parametric statistical inference based on permutation\ntest. Extensive experiments were performed on synthetic data and the real\ndiffusion MR data of mild cognitive impairment patients. The experimental\nresults indicate that BBS significantly improves statistical power,\nnotwithstanding the small sample size.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 14:43:05 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Chen", "Geng", ""], ["Zhang", "Pei", ""], ["Li", "Ke", ""], ["Wee", "Chong-Yaw", ""], ["Pan", "Wenliang", ""], ["Wu", "Yafeng", ""], ["Giannakopoulos", "Panteleimon", ""], ["Haller", "Sven", ""], ["Shen", "Dinggang", ""], ["Yap", "Pew-Thian", ""]]}, {"id": "1807.09237", "submitter": "Elizabeth Lorenzi", "authors": "Elizabeth Lorenzi, Ricardo Henao, Katherine Heller", "title": "Hierarchical infinite factor model for improving the prediction of\n  surgical complications for geriatric patients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a hierarchical infinite latent factor model (HIFM) to\nappropriately account for the covariance structure across subpopulations in\ndata. We propose a novel Hierarchical Dirichlet Process shrinkage prior on the\nloadings matrix that flexibly captures the underlying structure of our data\nacross subpopulations while sharing information to improve inference and\nprediction. The stick-breaking construction of the prior assumes infinite\nnumber of factors and allows for each subpopulation to utilize different\nsubsets of the factor space and select the number of factors needed to best\nexplain the variation. Theoretical results are provided to show support of the\nprior. We develop the model into a latent factor regression method that excels\nat prediction and inference of regression coefficients. Simulations are used to\nvalidate this strong performance compared to baseline methods. We apply this\nwork to the problem of predicting surgical complications using electronic\nhealth record data for geriatric patients at Duke University Health System\n(DUHS). We utilize additional surgical encounters at DUHS to enhance learning\nfor the targeted patients. Using HIFM to identify high risk patients improves\nthe sensitivity of predicting death to 91% from 35% based on the currently used\nheuristic.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 17:15:01 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Lorenzi", "Elizabeth", ""], ["Henao", "Ricardo", ""], ["Heller", "Katherine", ""]]}, {"id": "1807.09655", "submitter": "Pablo de Oliveira Castro", "authors": "Devan Sohier, Pablo de Oliveira Castro, Fran\\c{c}ois F\\'evotte, Bruno\n  Lathuili\\`ere, Eric Petit, Olivier Jamond", "title": "Confidence Intervals for Stochastic Arithmetic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantifying errors and losses due to the use of Floating-Point (FP)\ncalculations in industrial scientific computing codes is an important part of\nthe Verification, Validation and Uncertainty Quantification (VVUQ) process.\nStochastic Arithmetic is one way to model and estimate FP losses of accuracy,\nwhich scales well to large, industrial codes. It exists in different flavors,\nsuch as CESTAC or MCA, implemented in various tools such as CADNA, Verificarlo\nor Verrou. These methodologies and tools are based on the idea that FP losses\nof accuracy can be modeled via randomness. Therefore, they share the same need\nto perform a statistical analysis of programs results in order to estimate the\nsignificance of the results. In this paper, we propose a framework to perform a\nsolid statistical analysis of Stochastic Arithmetic. This framework unifies all\nexisting definitions of the number of significant digits (CESTAC and MCA), and\nalso proposes a new quantity of interest: the number of digits contributing to\nthe accuracy of the results. Sound confidence intervals are provided for all\nestimators, both in the case of normally distributed results, and in the\ngeneral case. The use of this framework is demonstrated by two case studies of\nlarge, industrial codes: Europlexus and code_aster.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 09:18:06 GMT"}, {"version": "v2", "created": "Wed, 19 Sep 2018 09:54:28 GMT"}, {"version": "v3", "created": "Fri, 30 Apr 2021 07:39:07 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Sohier", "Devan", ""], ["Castro", "Pablo de Oliveira", ""], ["F\u00e9votte", "Fran\u00e7ois", ""], ["Lathuili\u00e8re", "Bruno", ""], ["Petit", "Eric", ""], ["Jamond", "Olivier", ""]]}, {"id": "1807.09665", "submitter": "Alexander Bauer", "authors": "Alexander Bauer, Andreas Bender, Andr\\'e Klima, Helmut K\\\"uchenhoff", "title": "KOALA: A new paradigm for election coverage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common election poll reporting is often misleading as sample uncertainty is\naddressed insufficiently or not covered at all. Furthermore, main interest\nusually lies beyond the simple party shares. For a more comprehensive opinion\npoll and election coverage, we propose shifting the focus towards the reporting\nof survey-based probabilities for specific events of interest. We present such\nan approach for multi-party electoral systems, focusing on probabilities of\ncoalition majorities. A Monte Carlo approach based on a Bayesian\nMultinomial-Dirichlet model is used for estimation. Probabilities are\nestimated, assuming the election was held today (''now-cast''), not accounting\nfor potential shifts in the electorate until election day ''fore-cast''. Since\nour method is based on the posterior distribution of party shares, the approach\ncan be used to answer a variety of questions related to the outcome of an\nelection. We also introduce visualization techniques that facilitate a more\nadequate depiction of relevant quantities as well as respective uncertainties.\nThe benefits of our approach are discussed by application to the German federal\nelections in 2013 and 2017. An open source implementation of our methods is\nfreely available in the R package coalitions.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jul 2018 13:18:22 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Bauer", "Alexander", ""], ["Bender", "Andreas", ""], ["Klima", "Andr\u00e9", ""], ["K\u00fcchenhoff", "Helmut", ""]]}, {"id": "1807.09864", "submitter": "Eric Benhamou", "authors": "Eric Benhamou and Beatrice Guez", "title": "Incremental Sharpe and other performance ratios", "comments": "18 pages", "journal-ref": "Journal of Statistical and Econometric Methods, vol.7, no.4, 2018,\n  19-37", "doi": null, "report-no": null, "categories": "q-fin.PM q-fin.RM q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new methodology of computing incremental contribution for\nperformance ratios for portfolio like Sharpe, Treynor, Calmar or Sterling\nratios. Using Euler's homogeneous function theorem, we are able to decompose\nthese performance ratios as a linear combination of individual modified\nperformance ratios. This allows understanding the drivers of these performance\nratios as well as deriving a condition for a new asset to provide incremental\nperformance for the portfolio. We provide various numerical examples of this\nperformance ratio decomposition.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 16:30:47 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2018 01:12:25 GMT"}, {"version": "v3", "created": "Tue, 4 Sep 2018 05:47:16 GMT"}, {"version": "v4", "created": "Sun, 16 Sep 2018 10:52:25 GMT"}, {"version": "v5", "created": "Mon, 17 Dec 2018 07:19:59 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Benhamou", "Eric", ""], ["Guez", "Beatrice", ""]]}, {"id": "1807.10021", "submitter": "Hugues Mercier", "authors": "Hugues Mercier and Sandro Heiniger", "title": "Judging the Judges: Evaluating the Performance of International\n  Gymnastics Judges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Judging a gymnastics routine is a noisy process, and the performance of\njudges varies widely. In collaboration with the F\\'ed\\'eration Internationale\nde Gymnastique (FIG) and Longines, we are designing and implementing an\nimproved statistical engine to analyze the performance of gymnastics judges\nduring and after major competitions like the Olympic Games and the World\nChampionships. The engine, called the Judge Evaluation Program (JEP), has three\nobjectives: (1) provide constructive feedback to judges, executive committees\nand national federations; (2) assign the best judges to the most important\ncompetitions; (3) detect bias and outright cheating.\n  Using data from international gymnastics competitions held during the\n2013-2016 Olympic cycle, we first develop a marking score evaluating the\naccuracy of the marks given by gymnastics judges. Judging a gymnastics routine\nis a random process, and we can model this process very accurately using\nheteroscedastic random variables. The marking score scales the difference\nbetween the mark of a judge and the theoretical performance of a gymnast as a\nfunction of the standard deviation of the judging error estimated from data for\neach apparatus. This dependence between judging variability and performance\nquality has never been properly studied. We then study ranking scores assessing\nto what extent judges rate gymnasts in the correct order, and explain why we\nultimately chose not to implement them. We also study outlier detection to\npinpoint gymnasts who were poorly evaluated by judges. Finally, we discuss\ninteresting observations and discoveries that led to recommendations and rule\nchanges at the FIG.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 09:07:11 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2018 07:04:58 GMT"}, {"version": "v3", "created": "Fri, 16 Aug 2019 00:57:46 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Mercier", "Hugues", ""], ["Heiniger", "Sandro", ""]]}, {"id": "1807.10033", "submitter": "Hugues Mercier", "authors": "Sandro Heiniger and Hugues Mercier", "title": "National Bias of International Gymnastics Judges during the 2013-2016\n  Olympic Cycle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  National bias in sports judging is a well-known issue and has been observed\nin several sports: judges, in the aggregate, give higher marks to athletes of\nthe same nationality. In this work, we study the national bias of international\ngymnastics judges during the 2013-2016 Olympic cycle. As opposed to prior work,\nour analysis leverages the intrinsic variance of the judging error based on the\nperformance level of the gymnasts for each apparatus and discipline. The\nmagnitude of the national bias varies across judges, nations and disciplines.\nWhile acrobatics and trampoline do not exhibit any bias, we observe\nconsiderable bias by aerobics, artistics and rhythmics judges. In aerobic and\nartistic gymnastics this bias further increases for the best athletes competing\nin the finals. On the positive side, we show that judges are unbiased against\ndirect competitors of their own gymnasts. Our approach could easily be applied\nto other sports that incorporate a judging panel and objective judging\nguidelines. It could help sports federations and the public at large understand\nthe extent of national bias and identify particularly prone judges or nations.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 09:31:15 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2018 07:06:56 GMT"}, {"version": "v3", "created": "Fri, 16 Aug 2019 13:10:40 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Heiniger", "Sandro", ""], ["Mercier", "Hugues", ""]]}, {"id": "1807.10055", "submitter": "Hugues Mercier", "authors": "Sandro Heiniger and Hugues Mercier", "title": "Judging the Judges: A General Framework for Evaluating the Performance\n  of International Sports Judges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The monitoring of judges and referees in sports has become an important topic\ndue to the increasing media exposure of international sporting events and the\nlarge monetary sums involved. In this article, we present a method to assess\nthe accuracy of sports judges and estimate their bias. Our method is broadly\napplicable to all sports where panels of judges evaluate athletic performances\non a finite scale. We analyze judging scores from eight different sports with\ncomparable judging systems: diving, dressage, figure skating, freestyle skiing\n(aerials), freestyle snowboard (halfpipe, slopestyle), gymnastics, ski jumping\nand synchronized swimming. With the notable exception of dressage, we identify,\nfor each aforementioned sport, a general and accurate pattern of the intrinsic\njudging error as a function of the performance level of the athlete. This\nintrinsic judging inaccuracy is heteroscedastic and can be approximated by a\nquadratic curve, indicating increased consensus among judges towards the best\nathletes. Using this observation, the framework developed to assess the\nperformance of international gymnastics judges is applicable to all these\nsports: we can evaluate the performance of judges compared to their peers and\ndistinguish cheating from unintentional misjudging. Our analysis also leads to\nvaluable insights about the judging practices of the sports under\nconsideration. In particular, it reveals a systemic judging problem in\ndressage, where judges disagree on what constitutes a good performance.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 10:30:19 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2018 07:08:37 GMT"}, {"version": "v3", "created": "Fri, 16 Aug 2019 21:58:41 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Heiniger", "Sandro", ""], ["Mercier", "Hugues", ""]]}, {"id": "1807.10083", "submitter": "Maryna Prus", "authors": "Maryna Prus, Norbert Benda, Rainer Schwabe", "title": "Optimal Design in Hierarchical Models with application in Multi-center\n  Trials", "comments": null, "journal-ref": "Journal of Statistical Theory and Practice (2020) 14:24", "doi": "10.1007/s42519-020-00090-y", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical random effect models are used for different purposes in clinical\nresearch and other areas. In general, the main focus is on population\nparameters related to the expected treatment effects or group differences among\nall units of an upper level (e.g. subjects in many settings). Optimal design\nfor estimation of population parameters are well established for many models.\nHowever, optimal designs for the prediction for the individual units may be\ndifferent. Several settings are identiffed in which individual prediction may\nbe of interest. In this paper we determine optimal designs for the individual\npredictions, e.g. in multi-center trials, and compare them to a conventional\nbalanced design with respect to treatment allocation. Our investigations show,\nthat balanced designs are far from optimal if the treatment effects vary\nstrongly as compared to the residual error and more subjects should be\nrecruited to the active (new) treatment in multi-center trials. Nevertheless,\neffciency loss may be limited resulting in a moderate sample size increase when\nindividual predictions are foreseen with a balanced allocation.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 12:05:13 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Prus", "Maryna", ""], ["Benda", "Norbert", ""], ["Schwabe", "Rainer", ""]]}, {"id": "1807.10333", "submitter": "Mar\\'ia Gabriela Palacio", "authors": "M. G. Palacio, S. B. Ferrero and A. C. Frery", "title": "Revisiting the effect of spatial resolution on information content based\n  on classification results", "comments": "16 pages, 9 figures, Accepted for publication in the International\n  Journal of Remote Sensing", "journal-ref": null, "doi": "10.1080/01431161.2019.1569278", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polarimetric Synthetic Aperture Radar (PolSAR) images are an important source\nof information. Speckle noise gives SAR images a granular appearance that makes\ninterpretation and analysis hard tasks. A major issue is the assessment of\ninformation content in these kind of images, and how it is affected by usual\nprocessing techniques. Previous works have resulted in various approaches for\nquantifying image information content. As Narayanan, Desetty, and\nReichenbach(2002) we study this problem from the classification accuracy\nviewpoint, focusing in the filtering and the classification stages. Thus,\nthrough classified images we verify how changing properties of the input data\naffects their quality. Our input is an actual PolSAR image, the control\nparameters are the filter (Local Mean or Model Based PolSAR, MBPolSAR), the\nsize of them and the classification method (Maximum Likelihood, ML, or Support\nVector Machine, SVM), and the output are the classification precision obtained\napplying the classification algorithm to the filtered data. To expand the\nconclusions, this study deals not only with Classification Accuracy, but also\nwith Kappa and Overall Accuracy as measures of map precision. Experiments were\nconducted on two airborne PolSAR images. Unless Narayanan, Desetty, and\nReichenbach(2002) almost all measure values are good and increase with\ndegradation, i.e. the filtering algorithm that we used always improves the\nclassification results at least up to 7x7.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 19:27:26 GMT"}, {"version": "v2", "created": "Tue, 14 Aug 2018 00:07:09 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Palacio", "M. G.", ""], ["Ferrero", "S. B.", ""], ["Frery", "A. C.", ""]]}, {"id": "1807.10434", "submitter": "Peter Jan van Leeuwen", "authors": "Peter Jan van Leeuwen, Hans R. K\\\"unsch, Lars Nerger, Roland Potthast,\n  and Sebastian Reich", "title": "Particle filters for high-dimensional geoscience applications: a review", "comments": "Review paper, 36 pages, 9 figures, Resubmitted to Q.J.Royal Meteorol.\n  Soc", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle filters contain the promise of fully nonlinear data assimilation.\nThey have been applied in numerous science areas, but their application to the\ngeosciences has been limited due to their inefficiency in high-dimensional\nsystems in standard settings. However, huge progress has been made, and this\nlimitation is disappearing fast due to recent developments in proposal\ndensities, the use of ideas from (optimal) transportation, the use of\nlocalisation and intelligent adaptive resampling strategies. Furthermore,\npowerful hybrids between particle filters and ensemble Kalman filters and\nvariational methods have been developed. We present a state of the art\ndiscussion of present efforts of developing particle filters for highly\nnonlinear geoscience state-estimation problems with an emphasis on atmospheric\nand oceanic applications, including many new ideas, derivations, and\nunifications, highlighting hidden connections, and generating a valuable tool\nand guide for the community. Initial experiments show that particle filters can\nbe competitive with present-day methods for numerical weather prediction\nsuggesting that they will become mainstream soon.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 05:17:26 GMT"}, {"version": "v2", "created": "Sat, 13 Apr 2019 21:23:15 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["van Leeuwen", "Peter Jan", ""], ["K\u00fcnsch", "Hans R.", ""], ["Nerger", "Lars", ""], ["Potthast", "Roland", ""], ["Reich", "Sebastian", ""]]}, {"id": "1807.10542", "submitter": "Philip Jonathan", "authors": "Matthew Jones, David Randell, Kevin Ewans, Philip Jonathan", "title": "Statistics of extreme ocean environments: Non-stationary inference for\n  directionality and other covariate effects", "comments": "37 pages, 12 figures", "journal-ref": "Ocean Engineering 2016 v119 pp30-46", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous approaches are proposed in the literature for non-stationarity\nmarginal extreme value inference, including different model parameterisations\nwith respect to covariate, and different inference schemes. The objective of\nthis article is to compare some of these procedures critically. We generate\nsample realisations from generalised Pareto distributions, the parameters of\nwhich are smooth functions of a single smooth periodic covariate, specified to\nreflect the characteristics of actual samples from the tail of the distribution\nof significant wave height with direction, considered in the literature in the\nrecent past. We estimate extreme values models (a) using Constant, Fourier,\nB-spline and Gaussian Process parameterisations for the functional forms of\ngeneralised Pareto shape and (adjusted) scale with respect to covariate and (b)\nmaximum likelihood and Bayesian inference procedures. We evaluate the relative\nquality of inferences by estimating return value distributions for the response\ncorresponding to a time period of $10 \\times$ the (assumed) period of the\noriginal sample, and compare estimated return values distributions with the\ntruth using Kullback-Leibler, Cramer-von Mises and Kolmogorov-Smirnov\nstatistics. We find that Spline and Gaussian Process parameterisations\nestimated by Markov chain Monte Carlo inference using the mMALA algorithm,\nperform equally well in terms of quality of inference and computational\nefficiency, and generally perform better than alternatives in those respects.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 11:56:58 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Jones", "Matthew", ""], ["Randell", "David", ""], ["Ewans", "Kevin", ""], ["Jonathan", "Philip", ""]]}, {"id": "1807.10558", "submitter": "Jordan Weiss", "authors": "Jordan Weiss, Amanda R. Rabinowitz, Sameer K. Deshpande, Raiden B.\n  Hasegawa, Dylan S. Small", "title": "Protocol for an Observational Study on the Effects of Early-Life\n  Participation in Contact Sports on Later-Life Cognition in a Sample of\n  Monozygotic and Dizygotic Swedish Twins Reared Together and Twins Reared\n  Apart", "comments": "Updated methodology and tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large body of work links traumatic brain injury (TBI) in adulthood to the\nonset of Alzheimer's disease (AD). AD is the chief cause of dementia, leading\nto reduced cognitive capacity and autonomy and increased mortality risk. More\nrecently, researchers have sought to investigate whether TBI experienced in\nearly-life may influence trajectories of cognitive dysfunction in adulthood. It\nhas been speculated that early-life participation in collision sports may lead\nto poor cognitive and mental health outcomes. However, to date, the few studies\nto investigate this relationship have produced mixed results. We propose to\nextend this literature by conducting a prospective study on the effects of\nearly-life participation in collision sports on later-life cognitive health\nusing the Swedish Adoption/Twin Study on Aging (SATSA). The SATSA is unique in\nits sampling of monozygotic and dizygotic twins reared together (respectively\nMZT, DZT) and twins reared apart (respectively MZA, DZA). The proposed analysis\nis a prospective study of 660 individuals comprised of 270 twin pairs and 120\nsingletons. Seventy-eight (11.8% individuals reported participation in\ncollision sports. Our primary outcome will be an indicator of cognitive\nimpairment determined by scores on the Mini-Mental State Examination (MMSE). We\nwill also consider several secondary cognitive outcomes including verbal and\nspatial ability, memory, and processing speed. Our sample will be restricted to\nindividuals with at least one MMSE score out of seven repeated assessments\nspaced approximately three years apart. We will adjust for age, sex, and\neducation in each of our models.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 12:43:43 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 12:43:28 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Weiss", "Jordan", ""], ["Rabinowitz", "Amanda R.", ""], ["Deshpande", "Sameer K.", ""], ["Hasegawa", "Raiden B.", ""], ["Small", "Dylan S.", ""]]}, {"id": "1807.10820", "submitter": "Ansgar Steland", "authors": "Evgenii Sovetkin, Ansgar Steland", "title": "Automatic Processing and Solar Cell Detection in Photovoltaic\n  Electroluminescence Images", "comments": null, "journal-ref": "Integrated Computer-Aided Engineering, vol. 26, no. 2, pp.\n  123-137, 2019", "doi": "10.3233/ICA-180588", "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electroluminescence (EL) imaging is a powerful and established technique for\nassessing the quality of photovoltaic (PV) modules, which consist of many\nelectrically connected solar cells arranged in a grid. The analysis of\nimperfect real-world images requires reliable methods for preprocessing,\ndetection and extraction of the cells. We propose several methods for those\ntasks, which, however, can be modified to related imaging problems where\nsimilar geometric objects need to be detected accurately. Allowing for images\ntaken under difficult outdoor conditions, we present methods to correct for\nrotation and perspective distortions. The next important step is the extraction\nof the solar cells of a PV module, for instance to pass them to a procedure to\ndetect and analyze defects on their surface. We propose a method based on\nspecialized Hough transforms, which allows to extract the cells even when the\nmodule is surrounded by disturbing background and a fast method based on\ncumulated sums (CUSUM) change detection to extract the cell area of single-cell\nmini-module, where the correction of perspective distortion is implicitly done.\nThe methods are highly automatized to allow for big data analyses. Their\napplication to a large database of EL images substantiates that the methods\nwork reliably on a large scale for real-world images. Simulations show that the\napproach achieves high accuracy, reliability and robustness. This even holds\nfor low contrast images as evaluated by comparing the simulated accuracy for a\nlow and a high contrast image.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 09:58:34 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Sovetkin", "Evgenii", ""], ["Steland", "Ansgar", ""]]}, {"id": "1807.10840", "submitter": "Mengyang Gu", "authors": "Mengyang Gu, Debarun Bhattacharjya and Dharmashankar Subramanian", "title": "Nonparametric estimation of utility functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring a decision maker's utility function typically involves an\nelicitation phase where the decision maker responds to a series of elicitation\nqueries, followed by an estimation phase where the state-of-the-art is to\neither fit the response data to a parametric form (such as the exponential or\npower function) or perform linear interpolation. We introduce a Bayesian\nnonparametric method involving Gaussian stochastic processes for estimating a\nutility function. Advantages include the flexibility to fit a large class of\nfunctions, favorable theoretical properties, and a fully probabilistic view of\nthe decision maker's preference properties including risk attitude. Using\nextensive simulation experiments as well as two real datasets from the\nliterature, we demonstrate that the proposed approach yields estimates with\nlower mean squared errors. While our focus is primarily on single-attribute\nutility functions, one of the real datasets involves three attributes; the\nresults indicate that nonparametric methods also seem promising for\nmulti-attribute utility function estimation.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 22:03:27 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Gu", "Mengyang", ""], ["Bhattacharjya", "Debarun", ""], ["Subramanian", "Dharmashankar", ""]]}, {"id": "1807.10853", "submitter": "Emma Jingfei Zhang", "authors": "Jingfei Zhang, Xuening Zhu, Hansheng Wang and Yongtao Guan", "title": "Modeling Social Media User Content Generation Using Interpretable Point\n  Process Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we study the activity patterns of modern social media users\non platforms such as Twitter and Facebook. To characterize the complex patterns\nwe observe in users' interactions with social media, we describe a new class of\npoint process models. The components in the model have straightforward\ninterpretations and can thus provide meaningful insights into user activity\npatterns. A composite likelihood approach and a composite EM estimation\nprocedure are developed to overcome the challenges that arise in parameter\nestimation. Using the proposed method, we analyze Donald Trump's Twitter data\nand study if and how his tweeting behavior evolved before, during and after the\npresidential campaign. Additionally, we analyze a large-scale social media data\nfrom Sina Weibo and identify interesting groups of users with distinct\nbehaviors; in this analysis, we also discuss the effect of social ties on a\nuser's online content generating behavior.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 23:11:34 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 18:47:27 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Zhang", "Jingfei", ""], ["Zhu", "Xuening", ""], ["Wang", "Hansheng", ""], ["Guan", "Yongtao", ""]]}, {"id": "1807.10869", "submitter": "Xiang Zhou", "authors": "Xiang Zhou, Geoffrey T. Wodtke", "title": "Residual Balancing: A Method of Constructing Weights for Marginal\n  Structural Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When making causal inferences, post-treatment confounders complicate analyses\nof time-varying treatment effects. Conditioning on these variables naively to\nestimate marginal effects may inappropriately block causal pathways and may\ninduce spurious associations between treatment and the outcome, leading to\nbias. To avoid such bias, researchers often use marginal structural models\n(MSMs) with inverse probability weighting (IPW). However, IPW requires models\nfor the conditional distributions of treatment and is highly sensitive to their\nmisspecification. Moreover, IPW is relatively inefficient, susceptible to\nfinite-sample bias, and difficult to use with continuous treatments. We\nintroduce an alternative method of constructing weights for MSMs, which we call\n\"residual balancing.\" In contrast to IPW, it requires modeling the conditional\nmeans of the post-treatment confounders rather than the conditional\ndistributions of treatment, and it is therefore easier to use with continuous\nexposures. Numeric simulations suggest that residual balancing is both more\nefficient and more robust to model misspecification than IPW and its variants.\nWe illustrate the method by estimating (a) the cumulative effect of negative\nadvertising on election outcomes and (b) the controlled direct effect of shared\ndemocracy on public support for war. Open source software is available for\nimplementing the proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jul 2018 01:46:12 GMT"}, {"version": "v2", "created": "Fri, 29 Mar 2019 18:28:52 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Zhou", "Xiang", ""], ["Wodtke", "Geoffrey T.", ""]]}, {"id": "1807.10987", "submitter": "Manoel Santos Neto", "authors": "M\\'ario F. Desousa, Helton Saulo, Manoel Santos-Neto and V\\'ictor\n  Leiva", "title": "A new mixture-based fixed-effect model for a biometrical case-study\n  related to immunogenecity with highly censored data", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": "10.1080/00949655.2020.1790560", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new continuous-discrete mixture regression model which is useful\nfor describing highly censored data. We motivate our investigation based on a\ncase-study in biometry related to measles vaccines in Haiti. In this\ncase-study, the neutralization antibody level is explained by the type of\nvaccine used, level of the dosage and gender of the patient. This mixture model\nallows us to account for excess of censored observations and consists of the\nBirnbaum-Saunders and Bernoulli distributions. These distributions describe the\nantibody level and the point mass of the censoring observations. We estimate\nthe model parameters with the maximum likelihood method. Numerical evaluation\nof the model is performed by Monte Carlo simulations and by an illustration\nwith biometrical data, both of which show its good performance and its\npotential applications.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jul 2018 01:09:26 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Desousa", "M\u00e1rio F.", ""], ["Saulo", "Helton", ""], ["Santos-Neto", "Manoel", ""], ["Leiva", "V\u00edctor", ""]]}, {"id": "1807.11097", "submitter": "Dominic Magirr Dr", "authors": "Dominic Magirr and Carl-Fredrik Burman", "title": "Modestly Weighted Logrank Tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new class of weighted logrank tests (WLRT) that control the risk\nof concluding that a new drug is more efficacious than standard of care, when,\nin fact, it is uniformly inferior. Perhaps surprisingly, this risk is not\ncontrolled for WLRT in general. Tests from this new class can be constructed to\nhave high power under a delayed-onset treatment effect scenario, as well as\nbeing almost as efficient as the standard logrank test under proportional\nhazards.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jul 2018 18:34:45 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Magirr", "Dominic", ""], ["Burman", "Carl-Fredrik", ""]]}, {"id": "1807.11177", "submitter": "Kairui Feng", "authors": "Kairui Feng and Ning Lin", "title": "A reconstruction of Florida Traffic Flow During Hurricane Irma (2017)", "comments": "12 pages, 3 figures, submitted to transportation research board", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent Hurricane Irma (2017) created the most extensive scale of evacuation\nin Florida's history, involving about 6.5 million people on mandatory\nevacuation order and 4 million evacuation vehicles. To understand the hurricane\nevacuation process, the spatial and temporal evolution of the traffic flow is a\ncritical piece of information, but it is usually not fully observed. Based on\nthe game theory, this paper employs the available traffic observation on main\nhighways (20 cameras; including parts of Route 1, 27 and I-75, 95, 4,10) during\nIrma to reconstruct the traffic flow in Florida during Irma. The model is\nvalidated with self-reported twitters. The traffic reconstruction estimates the\ntraffic demand (about 4 million cars in total) and the temporal and spatial\ndistribution of congestion during the evacuation. The results compare well with\navailable information from news reports and Twitter records. The reconstructed\ndata can be used to analyze hurricane evacuation decisions and travel behavior.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 05:15:42 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Feng", "Kairui", ""], ["Lin", "Ning", ""]]}, {"id": "1807.11287", "submitter": "Alberto Sorrentino", "authors": "Federica Sciacchitano, Silvio Lugaro and Alberto Sorrentino", "title": "Sparse Bayesian Imaging of Solar Flares", "comments": "accepted on Siam Journal on Imaging Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM math.NA stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider imaging of solar flares from NASA RHESSI data as a parametric\nimaging problem, where flares are represented as a finite collection of\ngeometric shapes. We set up a Bayesian model in which the number of objects\nforming the image is a priori unknown, as well as their shapes. We use a\nSequential Monte Carlo algorithm to explore the corresponding posterior\ndistribution. We apply the method to synthetic and experimental data, largely\nknown in the RHESSI community. The method reconstructs improved images of solar\nflares, with the additional advantage of providing uncertainty quantification\nof the estimated parameters.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 11:07:08 GMT"}, {"version": "v2", "created": "Thu, 22 Nov 2018 16:45:34 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Sciacchitano", "Federica", ""], ["Lugaro", "Silvio", ""], ["Sorrentino", "Alberto", ""]]}, {"id": "1807.11314", "submitter": "Virginie Ollier", "authors": "Virginie Ollier, Mohammed Nabil El Korso, Andr\\'e Ferrari, R\\'emy\n  Boyer, Pascal Larzabal", "title": "Robust Calibration of Radio Interferometers in Multi-Frequency Scenario", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP astro-ph.IM eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates calibration of sensor arrays in the radio astronomy\ncontext. Current and future radio telescopes require computationally efficient\nalgorithms to overcome the new technical challenges as large collecting area,\nwide field of view and huge data volume. Specifically, we study the calibration\nof radio interferometry stations with significant direction dependent\ndistortions. We propose an iterative robust calibration algorithm based on a\nrelaxed maximum likelihood estimator for a specific context: i) observations\nare affected by the presence of outliers and ii) parameters of interest have a\nspecific structure depending on frequency. Variation of parameters across\nfrequency is addressed through a distributed procedure, which is consistent\nwith the new radio synthesis arrays where the full observing bandwidth is\ndivided into multiple frequency channels. Numerical simulations reveal that the\nproposed robust distributed calibration estimator outperforms the conventional\nnon-robust algorithm and/or the mono-frequency case.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 12:31:58 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Ollier", "Virginie", ""], ["Korso", "Mohammed Nabil El", ""], ["Ferrari", "Andr\u00e9", ""], ["Boyer", "R\u00e9my", ""], ["Larzabal", "Pascal", ""]]}, {"id": "1807.11382", "submitter": "Virginie Ollier", "authors": "Virginie Ollier, Mohammed Nabil El Korso, Andr\\'e Ferrari, R\\'emy\n  Boyer, Pascal Larzabal", "title": "Bayesian Calibration using Different Prior Distributions: an Iterative\n  Maximum A Posteriori Approach for Radio Interferometers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP astro-ph.IM eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim to design robust estimation techniques based on the\ncompound-Gaussian (CG) process and adapted for calibration of radio\ninterferometers. The motivation beyond this is due to the presence of outliers\nleading to an unrealistic traditional Gaussian noise assumption. Consequently,\nto achieve robustness, we adopt a maximum a posteriori (MAP) approach which\nexploits Bayesian statistics and follows a sequential updating procedure here.\nThe proposed algorithm is applied in a multi-frequency scenario in order to\nenhance the estimation and correction of perturbation effects. Numerical\nsimulations assess the performance of the proposed algorithm for different\nnoise models, Student's t, K, Laplace, Cauchy and inverse-Gaussian\ncompound-Gaussian distributions w.r.t. the classical non-robust Gaussian noise\nassumption.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 15:00:02 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Ollier", "Virginie", ""], ["Korso", "Mohammed Nabil El", ""], ["Ferrari", "Andr\u00e9", ""], ["Boyer", "R\u00e9my", ""], ["Larzabal", "Pascal", ""]]}, {"id": "1807.11591", "submitter": "Noah Tuchow", "authors": "Noah W. Tuchow, Eric B. Ford, Theodore Papamarkou, and Alexey Lindo", "title": "The Efficiency of Geometric Samplers for Exoplanet Transit Timing\n  Variation Models", "comments": null, "journal-ref": null, "doi": "10.1093/mnras/stz247", "report-no": null, "categories": "astro-ph.IM astro-ph.EP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transit timing variations (TTVs) are a valuable tool to determine the masses\nand orbits of transiting planets in multi-planet systems. TTVs can be readily\nmodeled given knowledge of the interacting planets' orbital configurations and\nplanet-star mass ratios, but such models are highly nonlinear and difficult to\ninvert. Markov chain Monte Carlo (MCMC) methods are often used to explore the\nposterior distribution for model parameters, but, due to the high correlations\nbetween parameters, nonlinearity, and potential multi-modality in the\nposterior, many samplers perform very inefficiently. Therefore, we assess the\nperformance of several MCMC samplers that use varying degrees of geometric\ninformation about the target distribution. We generate synthetic datasets from\nmultiple models, including the TTVFaster model and a simple sinusoidal model,\nand test the efficiencies of various MCMC samplers. We find that sampling\nefficiency can be greatly improved for all models by sampling from a parameter\nspace transformed using an estimate of the covariance and means of the target\ndistribution. No one sampler performs the best for all datasets, but several\nsamplers, such as Differential Evolution Monte Carlo and Geometric adaptive\nMonte Carlo, have consistently efficient performance. For datasets with near\nGaussian posteriors, Hamiltonian Monte Carlo samplers with 2 or 3 leapfrog\nsteps obtained the highest efficiencies. Based on differences in effective\nsample sizes per time, we show that the right choice of sampler can improve\nsampling efficiencies by several orders of magnitude.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 21:42:23 GMT"}, {"version": "v2", "created": "Mon, 28 Jan 2019 03:43:00 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Tuchow", "Noah W.", ""], ["Ford", "Eric B.", ""], ["Papamarkou", "Theodore", ""], ["Lindo", "Alexey", ""]]}, {"id": "1807.11660", "submitter": "Cesar Yahia", "authors": "Cesar N. Yahia, Shannon E. Scott, Stephen D. Boyles, Christian G.\n  Claudel", "title": "Unmanned Aerial Vehicle Path Planning for Traffic Estimation and\n  Detection of Non-Recurrent Congestion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unmanned aerial vehicles (UAVs) provide a novel means of extracting road and\ntraffic information via video data. Specifically, by analyzing objects in a\nvideo frame, UAVs can be used to detect traffic characteristics and road\nincidents. Under congested conditions, the UAVs can supply accurate incident\ninformation where it is otherwise difficult to infer the road state from\ntraditional speed-density measurements. Leveraging the mobility and detection\ncapabilities of UAVs, we investigate navigation algorithms that seek to\nmaximize information on the road/traffic state under non-recurrent congestion.\nWe propose an active exploration framework that (1) assimilates UAV\nobservations with speed-density sensor data, (2) quantifies uncertainty on the\nroad/traffic state, and (3) adaptively navigates the UAV to minimize this\nuncertainty. The navigation algorithm uses the A-optimal information measure\n(mean uncertainty) and it depends on covariance matrices generated by an\nensemble Kalman filter (EnKF). In the EnKF procedure, we incorporate nonlinear\ntraffic observations through model diagnostic variables, and we present a\nparameter update procedure that maintains a monotonic relationship between\nstates and measurements. We compare the traffic and incident state estimates\nresulting from the coupled UAV navigation-estimation procedure against\ncorresponding estimates that do not use targeted UAV observations. Our results\nindicate that UAVs aid in detection of incidents under congested conditions\nwhere speed-density data are not informative.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 04:49:54 GMT"}, {"version": "v2", "created": "Sat, 2 Mar 2019 01:17:40 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Yahia", "Cesar N.", ""], ["Scott", "Shannon E.", ""], ["Boyles", "Stephen D.", ""], ["Claudel", "Christian G.", ""]]}, {"id": "1807.11738", "submitter": "Virginie Ollier", "authors": "Virginie Ollier, Mohammed Nabil El Korso, Andr\\'e Ferrari, R\\'emy\n  Boyer, Pascal Larzabal", "title": "Robust distributed calibration of radio interferometers with direction\n  dependent distortions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In radio astronomy, accurate calibration is of crucial importance for the new\ngeneration of radio interferometers. More specifically, because of the\npotential presence of outliers which affect the measured data, robustness needs\nto be ensured. On the other hand, calibration is improved by taking advantage\nof these new instruments and exploiting the known structure of parameters of\ninterest across frequency. Therefore, we propose in this paper an iterative\nrobust multi-frequency calibration algorithm based on a distributed and\nconsensus optimization scheme which aims to estimate the complex gains of the\nreceivers and the directional perturbations caused by the ionosphere. Numerical\nsimulations reveal that the proposed distributed calibration technique\noutperforms the conventional non-robust algorithm and per-channel calibration.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 10:09:08 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Ollier", "Virginie", ""], ["Korso", "Mohammed Nabil El", ""], ["Ferrari", "Andr\u00e9", ""], ["Boyer", "R\u00e9my", ""], ["Larzabal", "Pascal", ""]]}, {"id": "1807.11743", "submitter": "Ma{\\l}gorzata Snarska", "authors": "Jarek Duda, Ma{\\l}gorzata Snarska", "title": "Modeling joint probability distribution of yield curve parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  US Yield curve has recently collapsed to its most flattened level since\nsubprime crisis and is close to the inversion. This fact has gathered attention\nof investors around the world and revived the discussion of proper modeling and\nforecasting yield curve, since changes in interest rate structure are believed\nto represent investors expectations about the future state of economy and have\nforeshadowed recessions in the United States. While changes in term structure\nof interest rates are relatively easy to interpret they are however very\ndifficult to model and forecast due to no proper economic theory underlying\nsuch events. Yield curves are usually represented by multivariate sparse time\nseries, at any point in time infinite dimensional curve is portrayed via\nrelatively few points in a multivariate space of data and as a consequence\nmultimodal statistical dependencies behind these curves are relatively hard to\nextract and forecast via typical multivariate statistical methods.We propose to\nmodel yield curves via reconstruction of joint probability distribution of\nparameters in functional space as a high degree polynomial. Thanks to adoption\nof an orthonormal basis, the MSE estimation of coefficients of a given function\nis an average over a data sample in the space of functions. Since such\npolynomial coefficients are independent and have cumulant-like interpretation\nie.describe corresponding perturbation from an uniform joint distribution, our\napproach can also be extended to any d-dimensional space of yield curve\nparameters (also in neighboring times) due to controllable accuracy. We believe\nthat this approach to modeling of local behavior of a sparse multivariate\ncurved time series can complement prediction from standard models like ARIMA,\nthat are using long range dependencies, but provide only inaccurate prediction\nof probability distribution, often as just Gaussian with constant width.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 10:17:16 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Duda", "Jarek", ""], ["Snarska", "Ma\u0142gorzata", ""]]}, {"id": "1807.11849", "submitter": "Fabian Guignard", "authors": "Fabian Guignard, Michele Lovallo, Mohamed Laib, Jean Golay, Mikhail\n  Kanevski, Nora Helbig, Luciano Telesca", "title": "Investigating the time dynamics of wind speed in complex terrains by\n  using the Fisher-Shannon method", "comments": "25 pages, 12 figures", "journal-ref": null, "doi": "10.1016/j.physa.2019.02.048", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the time dynamics of the daily means of wind speed measured in\ncomplex mountainous regions are investigated. For 293 measuring stations\ndistributed over all Switzerland, the Fisher information measure and the\nShannon entropy power are calculated. The results reveal a clear relationship\nbetween the computed measures and both the elevation of the wind stations and\nthe slope of the measuring sites. In particular, the Shannon entropy power and\nthe Fisher information measure have their highest (respectively lowest) values\nin the Alps, where the time dynamics of wind speed follows a more disordered\npattern. The spatial mapping of the calculated quantities allows the\nidentification of two regions, which is in agreement with the topography of the\nSwiss territory. The present study could contribute to a better\ncharacterization of the temporal dynamics of wind speed in complex mountainous\nterrain.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 15:01:15 GMT"}, {"version": "v2", "created": "Mon, 25 Feb 2019 11:33:27 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Guignard", "Fabian", ""], ["Lovallo", "Michele", ""], ["Laib", "Mohamed", ""], ["Golay", "Jean", ""], ["Kanevski", "Mikhail", ""], ["Helbig", "Nora", ""], ["Telesca", "Luciano", ""]]}, {"id": "1807.11887", "submitter": "Tingran Gao", "authors": "Tingran Gao, Shahar Z. Kovalsky, Doug M. Boyer, and Ingrid Daubechies", "title": "Gaussian Process Landmarking for Three-Dimensional Geometric\n  Morphometrics", "comments": "41 pages, 17 figures, 3 tables. Some portions of this work appeared\n  earlier as arXiv:1802.03479, which was split into 2 parts during the\n  refereeing process. This version combines the main text with the supplemental\n  materials. Figure sizes have been reduced to meet arxiv size limit", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate applications of the Gaussian process-based landmarking\nalgorithm proposed in [T. Gao, S.Z. Kovalsky, and I. Daubechies, SIAM Journal\non Mathematics of Data Science (2019)] to geometric morphometrics, a branch of\nevolutionary biology centered at the analysis and comparisons of anatomical\nshapes, and compares the automatically sampled landmarks with the \"ground\ntruth\" landmarks manually placed by evolutionary anthropologists; the results\nsuggest that Gaussian process landmarks perform equally well or better, in\nterms of both spatial coverage and downstream statistical analysis. We provide\na detailed exposition of numerical procedures and feature filtering algorithms\nfor computing high-quality and semantically meaningful diffeomorphisms between\ndisk-type anatomical surfaces.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 15:59:29 GMT"}, {"version": "v2", "created": "Fri, 28 Dec 2018 16:40:59 GMT"}, {"version": "v3", "created": "Tue, 8 Jan 2019 20:23:53 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Gao", "Tingran", ""], ["Kovalsky", "Shahar Z.", ""], ["Boyer", "Doug M.", ""], ["Daubechies", "Ingrid", ""]]}]