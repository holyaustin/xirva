[{"id": "1403.0065", "submitter": "Christian Robert Y", "authors": "Alexis Bienven\\\"ue and Christian Y. Robert", "title": "Likelihood based inference for high-dimensional extreme value\n  distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate extreme value statistical analysis is concerned with\nobservations on several variables which are thought to possess some degree of\ntail-dependence. In areas such as the modeling of financial and insurance\nrisks, or as the modeling of spatial variables, extreme value models in high\ndimensions (up to fifty or more) with their statistical inference procedures\nare needed. In this paper, we consider max-stable models for which the spectral\nrandom vectors have absolutely continuous distributions. For random samples\nwith max-stable distributions we provide quasi-explicit analytical expressions\nof the full likelihoods. When the full likelihood becomes numerically\nintractable because of a too large dimension, it is however necessary to split\nthe components into subgroups and to consider a composite likelihood approach.\nFor random samples in the max-domain of attraction of a max-stable\ndistribution, two approaches that use simpler likelihoods are possible: (i) a\nthreshold approach that is combined with a censoring scheme, (ii) a block\nmaxima approach that exploits the information on the occurrence times of the\ncomponentwise maxima. The asymptotic properties of the estimators are given and\nthe utility of the methods is examined via simulation. The estimators are also\ncompared with those derived from the pairwise composite likelihood method which\nhas been previously proposed in the spatial extreme value literature.\n", "versions": [{"version": "v1", "created": "Sat, 1 Mar 2014 08:43:28 GMT"}, {"version": "v2", "created": "Mon, 6 Oct 2014 13:23:01 GMT"}, {"version": "v3", "created": "Tue, 30 Dec 2014 14:39:58 GMT"}], "update_date": "2014-12-31", "authors_parsed": [["Bienven\u00fce", "Alexis", ""], ["Robert", "Christian Y.", ""]]}, {"id": "1403.0211", "submitter": "Rebecca Steorts", "authors": "Rebecca C. Steorts, Rob Hall, and Stephen E. Fienberg", "title": "SMERED: A Bayesian Approach to Graphical Record Linkage and\n  De-duplication", "comments": "AISTATS (2014), to appear; 9 pages with references, 2 page\n  supplement, 4 figures. Shorter version of arXiv:1312.4645", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel unsupervised approach for linking records across\narbitrarily many files, while simultaneously detecting duplicate records within\nfiles. Our key innovation is to represent the pattern of links between records\nas a {\\em bipartite} graph, in which records are directly linked to latent true\nindividuals, and only indirectly linked to other records. This flexible new\nrepresentation of the linkage structure naturally allows us to estimate the\nattributes of the unique observable people in the population, calculate $k$-way\nposterior probabilities of matches across records, and propagate the\nuncertainty of record linkage into later analyses. Our linkage structure lends\nitself to an efficient, linear-time, hybrid Markov chain Monte Carlo algorithm,\nwhich overcomes many obstacles encountered by previously proposed methods of\nrecord linkage, despite the high dimensional parameter space. We assess our\nresults on real and simulated data.\n", "versions": [{"version": "v1", "created": "Sun, 2 Mar 2014 14:21:20 GMT"}], "update_date": "2014-03-04", "authors_parsed": [["Steorts", "Rebecca C.", ""], ["Hall", "Rob", ""], ["Fienberg", "Stephen E.", ""]]}, {"id": "1403.0289", "submitter": "C\\'edric Richard", "authors": "Rita Ammanouil, Andr\\'e Ferrari, C\\'edric Richard, David Mary", "title": "Blind and fully constrained unmixing of hyperspectral images", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2014.2362056", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of blind and fully constrained unmixing of\nhyperspectral images. Unmixing is performed without the use of any dictionary,\nand assumes that the number of constituent materials in the scene and their\nspectral signatures are unknown. The estimated abundances satisfy the desired\nsum-to-one and nonnegativity constraints. Two models with increasing complexity\nare developed to achieve this challenging task, depending on how noise\ninteracts with hyperspectral data. The first one leads to a convex optimization\nproblem, and is solved with the Alternating Direction Method of Multipliers.\nThe second one accounts for signal-dependent noise, and is addressed with a\nReweighted Least Squares algorithm. Experiments on synthetic and real data\ndemonstrate the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 3 Mar 2014 02:06:36 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Ammanouil", "Rita", ""], ["Ferrari", "Andr\u00e9", ""], ["Richard", "C\u00e9dric", ""], ["Mary", "David", ""]]}, {"id": "1403.0315", "submitter": "Conrad Sanderson", "authors": "Johanna Carvajal, Chris McCool, Conrad Sanderson", "title": "Summarisation of Short-Term and Long-Term Videos using Texture and\n  Colour", "comments": "IEEE Winter Conference on Applications of Computer Vision (WACV),\n  2014", "journal-ref": null, "doi": "10.1109/WACV.2014.6836025", "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to video summarisation that makes use of a\nBag-of-visual-Textures (BoT) approach. Two systems are proposed, one based\nsolely on the BoT approach and another which exploits both colour information\nand BoT features. On 50 short-term videos from the Open Video Project we show\nthat our BoT and fusion systems both achieve state-of-the-art performance,\nobtaining an average F-measure of 0.83 and 0.86 respectively, a relative\nimprovement of 9% and 13% when compared to the previous state-of-the-art. When\napplied to a new underwater surveillance dataset containing 33 long-term\nvideos, the proposed system reduces the amount of footage by a factor of 27,\nwith only minor degradation in the information content. This order of magnitude\nreduction in video data represents significant savings in terms of time and\npotential labour cost when manually reviewing such footage.\n", "versions": [{"version": "v1", "created": "Mon, 3 Mar 2014 05:19:10 GMT"}], "update_date": "2014-08-27", "authors_parsed": [["Carvajal", "Johanna", ""], ["McCool", "Chris", ""], ["Sanderson", "Conrad", ""]]}, {"id": "1403.0419", "submitter": "Gian Luca Lippi", "authors": "G.L. Lippi", "title": "A priori analysis: an application to the estimate of the uncertainty in\n  course grades", "comments": "5 pages", "journal-ref": "Eur. J. Phys. 35, 045012 (2014)", "doi": "10.1088/0143-0807/35/4/045012", "report-no": null, "categories": "physics.ed-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The a priori analysis (APA) is discussed as a tool to assess the reliability\nof grades in standard curricular courses. This unusual, but striking\napplication is presented when teaching the section on data treatment of a\nLaboratory Course to illustrate the characteristics of the APA and its\npotential for widespread use, beyond the traditional Physics Curriculum. The\nconditions necessary for this kind of analysis are discussed, the general\nframework is set out and a specific example is given to illustrate its various\naspects. Students are often struck by this unusual application and are more apt\nto remember the APA. Instructors may also benefit from some of the gathered\ninformation, as discussed in the paper.\n", "versions": [{"version": "v1", "created": "Mon, 3 Mar 2014 13:08:49 GMT"}], "update_date": "2014-09-29", "authors_parsed": [["Lippi", "G. L.", ""]]}, {"id": "1403.0510", "submitter": "Dalia Chakrabarty Dr.", "authors": "Dalia Chakrabarty, Fabio Rigat, Nare Gabrielyan, Richard Beanland and\n  Shashi Paul", "title": "Bayesian Density Estimation via Multiple Sequential Inversions of 2-D\n  Images with Application in Electron Microscopy", "comments": "Accepted for Publication in Technometrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cond-mat.mtrl-sci", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new Bayesian methodology to learn the unknown material density\nof a given sample by inverting its two-dimensional images that are taken with a\nScanning Electron Microscope. An image results from a sequence of projections\nof the convolution of the density function with the unknown microscopy\ncorrection function that we also learn from the data. We invoke a novel design\nof experiment, involving imaging at multiple values of the parameter that\ncontrols the sub-surface depth from which information about the density\nstructure is carried, to result in the image. Real-life material density\nfunctions are characterised by high density contrasts and typically are highly\ndiscontinuous, implying that they exhibit correlation structures that do not\nvary smoothly. In the absence of training data, modelling such correlation\nstructures of real material density functions is not possible. So we discretise\nthe material sample and treat values of the density function at chosen\nlocations inside it as independent and distribution-free parameters. Resolution\nof the available image dictates the discretisation length of the model; three\nmodels pertaining to distinct resolution classes are developed. We develop\npriors on the material density, such that these priors adapt to the sparsity\ninherent in the density function. The likelihood is defined in terms of the\ndistance between the convolution of the unknown functions and the image data.\nThe posterior probability density of the unknowns given the data is expressed\nusing the developed priors on the density and priors on the microscopy\ncorrection function as elicitated from the Microscopy literature. We achieve\nposterior samples using an adaptive Metropolis-within-Gibbs inference scheme.\nThe method is applied to learn the material density of a 3-D sample of a real\nnano-structure and of simulated alloy samples.\n", "versions": [{"version": "v1", "created": "Mon, 3 Mar 2014 18:22:01 GMT"}], "update_date": "2014-03-06", "authors_parsed": [["Chakrabarty", "Dalia", ""], ["Rigat", "Fabio", ""], ["Gabrielyan", "Nare", ""], ["Beanland", "Richard", ""], ["Paul", "Shashi", ""]]}, {"id": "1403.0532", "submitter": "Alejandro Frery", "authors": "R. Ospina and A. M. Larangeiras and A. C. Frery", "title": "Visualization of Skewed Data: A Tool in R", "comments": "Submitted to the Revista Colombiana de Estad\\'istica", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a visualization tool specifically tailored to deal\nwith skewed data. The technique is based upon the use of two types of notched\nboxplots (the usual one, and one which is tuned for the skewness of the data),\nthe violin plot, the histogram and a nonparametric estimate of the density. The\ndata is assumed to lie on the same line, so the plots are compatible. We show\nthat a good deal of information can be extracted from the inspection of this\ntool; in particular, we apply the technique to analyze data from synthetic\naperture radar images. We provide the implementation in R.\n", "versions": [{"version": "v1", "created": "Mon, 3 Mar 2014 19:27:33 GMT"}], "update_date": "2014-03-04", "authors_parsed": [["Ospina", "R.", ""], ["Larangeiras", "A. M.", ""], ["Frery", "A. C.", ""]]}, {"id": "1403.0566", "submitter": "Stephen E. Fienberg", "authors": "Stephen E. Fienberg, Rebecca C. Steorts", "title": "Discussion of \"Estimating the Distribution of Dietary Consumption\n  Patterns\"", "comments": "Published in at http://dx.doi.org/10.1214/13-STS448 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 1, 95-96", "doi": "10.1214/13-STS448", "report-no": "IMS-STS-STS448", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Estimating the Distribution of Dietary Consumption Patterns\"\nby Raymond J. Carroll [arXiv:1405.4667].\n", "versions": [{"version": "v1", "created": "Mon, 3 Mar 2014 20:59:55 GMT"}, {"version": "v2", "created": "Tue, 20 May 2014 12:19:06 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Fienberg", "Stephen E.", ""], ["Steorts", "Rebecca C.", ""]]}, {"id": "1403.0612", "submitter": "Issac Shams", "authors": "Issac Shams, Saeede Ajorlou, Kai Yang", "title": "Modeling clustered non-stationary Poisson processes for stochastic\n  simulation inputs", "comments": "29 pages, 2 figures, 4 tables, published in Computers and Industrial\n  Engineering. arXiv admin note: text overlap with arXiv:1402.7112", "journal-ref": "Computers and Industrial Engineering, 64.4 (2013): 1074-1083", "doi": "10.1016/j.cie.2013.02.002", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A validated simulation model primarily requires performing an appropriate\ninput analysis mainly by determining the behavior of real-world processes using\nprobability distributions. In many practical cases, probability distributions\nof the random inputs vary over time in such a way that the functional forms of\nthe distributions and/or their parameters depend on time. This paper answers\nthe question whether a sequence of observations from a process follow the same\nstatistical distribution, and if not, where the exact change points are, so\nthat observations within two consecutive change points follow the same\ndistribution. We propose two different methods based on likelihood ratio test\nand cluster analysis to detect multiple change points when observations follow\nnon-stationary Poisson process with diverse occurrence rates over time. Results\nfrom a comprehensive Monte Carlo study indicate satisfactory performance for\nthe proposed methods. A well-known example is also considered to show the\napplication of our findings in real world cases.\n", "versions": [{"version": "v1", "created": "Mon, 3 Mar 2014 21:58:15 GMT"}], "update_date": "2014-03-05", "authors_parsed": [["Shams", "Issac", ""], ["Ajorlou", "Saeede", ""], ["Yang", "Kai", ""]]}, {"id": "1403.0623", "submitter": "Saptarshi Das", "authors": "Indranil Pan, Daya Shankar Pandey, Saptarshi Das", "title": "Global solar irradiation prediction using a multi-gene genetic\n  programming approach", "comments": "31 pages, 16 figures, 5 tables", "journal-ref": "Journal of Renewable and Sustainable Energy, vol. 5, no. 6, pp.\n  063129, 2013", "doi": "10.1063/1.4850495", "report-no": null, "categories": "cs.NE cs.CE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a nonlinear symbolic regression technique using an\nevolutionary algorithm known as multi-gene genetic programming (MGGP) is\napplied for a data-driven modelling between the dependent and the independent\nvariables. The technique is applied for modelling the measured global solar\nirradiation and validated through numerical simulations. The proposed modelling\ntechnique shows improved results over the fuzzy logic and artificial neural\nnetwork (ANN) based approaches as attempted by contemporary researchers. The\nmethod proposed here results in nonlinear analytical expressions, unlike those\nwith neural networks which is essentially a black box modelling approach. This\nadditional flexibility is an advantage from the modelling perspective and helps\nto discern the important variables which affect the prediction. Due to the\nevolutionary nature of the algorithm, it is able to get out of local minima and\nconverge to a global optimum unlike the back-propagation (BP) algorithm used\nfor training neural networks. This results in a better percentage fit than the\nones obtained using neural networks by contemporary researchers. Also a\nhold-out cross validation is done on the obtained genetic programming (GP)\nresults which show that the results generalize well to new data and do not\nover-fit the training samples. The multi-gene GP results are compared with\nthose, obtained using its single-gene version and also the same with four\nclassical regression models in order to show the effectiveness of the adopted\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 3 Mar 2014 22:34:28 GMT"}], "update_date": "2014-03-05", "authors_parsed": [["Pan", "Indranil", ""], ["Pandey", "Daya Shankar", ""], ["Das", "Saptarshi", ""]]}, {"id": "1403.0668", "submitter": "Issac Shams", "authors": "Issac Shams, Saeede Ajorlou, and Kai Yang", "title": "Estimating Multiple Step Shifts in a Gaussian Process Mean with an\n  Application to Phase I Control Chart Analysis", "comments": "5 pages, to be submitted in IEEE CASE 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In preliminary analysis of control charts, one may encounter multiple shifts\nand/or outliers especially with a large number of observations. The following\npaper addresses this problem. A statistical model for detecting and estimating\nmultiple change points in a finite batch of retrospective (phase I)data is\nproposed based on likelihood ratio test. We consider a univariate normal\ndistribution with multiple step shifts occurred in predefined locations of\nprocess mean. A numerical example is performed to illustrate the efficiency of\nour method. Finally, performance comparisons, based on accuracy measures and\nprecision measures, are explored through simulation studies.\n", "versions": [{"version": "v1", "created": "Tue, 4 Mar 2014 02:54:30 GMT"}], "update_date": "2014-03-05", "authors_parsed": [["Shams", "Issac", ""], ["Ajorlou", "Saeede", ""], ["Yang", "Kai", ""]]}, {"id": "1403.0674", "submitter": "Issac Shams", "authors": "Issac Shams, Saeede Ajorlou, Kai Yang", "title": "A multivariate hierarchical Bayesian framework for healthcare\n  predictions with application to medical home study in the Department of\n  Veteran Affairs", "comments": "5 pages, 3 tables, one figure, to be submitted to IEEE CASE 2014.\n  arXiv admin note: text overlap with arXiv:1402.6666", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently the patient centered medical home (PCMH) model has become a popular\napproach to deliver better care to patients. Current research shows that the\nmost important key for succession of this method is to make balance between\nhealthcare supply and demand. Without such balance in clinical supply and\ndemand, issues such as excessive under and over utilization of physicians, long\nwaiting time for receiving the appropriate treatment, and non continuity of\ncare will eliminate many advantages of the medical home strategy. To reach this\nend we need to have information about both supply and demand in healthcare\nsystem. Healthcare supply can be calculated easily based on head counts and\navailable hours which is offered by professionals for a specific time period\nwhile healthcare demand is not easy to calculate, and it is affected by some\nhealthcare, diagnostic and demographic attributes. In this paper, by extending\nthe hierarchical generalized linear model to include multivariate responses, we\ndevelop a clinical workload prediction model for care portfolio demands in a\nBayesian framework. Our analyses of a recent data from Veteran Health\nAdministration indicate that our prediction model works for clinical data with\nhigh performance.\n", "versions": [{"version": "v1", "created": "Tue, 4 Mar 2014 03:42:34 GMT"}], "update_date": "2014-03-05", "authors_parsed": [["Shams", "Issac", ""], ["Ajorlou", "Saeede", ""], ["Yang", "Kai", ""]]}, {"id": "1403.1057", "submitter": "Asis Chattopadhyay", "authors": "Tuli De, Tanuka Chattopadhyay and Asis Kumar Chattopadhyay", "title": "Use of spatial cross correlation function to study formation mechanism\n  of massive elliptical galaxies", "comments": "16 pages 4 figures", "journal-ref": null, "doi": "10.1017/pasa.2014.42", "report-no": null, "categories": "stat.AP astro-ph.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial clustering nature of galaxies have been studied previously through\nauto correlation function. The same type of cross correlation function has been\nused to investigate parametric clustering nature of galaxies e.g. with respect\nto masses and sizes of galaxies.\n  Here formation and evolution of several components of nearby massive early\ntype galaxies have been envisaged through cross correlations, in the mass-size\nparametric plane, with high redshift early type galaxies (hereafter ETG).It is\nfound that the inner most components of nearby ETGs have significant\ncorrelation with ETGs in the highest redshift range called red nuggets whereas\nintermediate components are highly correlated with ETGs in the redshift range\nwith z value greater than 0.5 and less than 0.75. The outer most part has no\ncorrelation in any range, suggesting a scenario through in situ accretion. The\nabove formation scenario is consistent with the previous results obtained for\nNGC5128 (Chattopadhyay et al. (2009), Chattopadhyay et al. (2013)) and to some\nextent for nearby elliptical galaxies (Huang et al. (2013)) after considering a\nsample of ETGs at high redshift with stellar masses greater than or equal to\n108.73 M-Sun. So the present work indicates a three phase formation of massive\nnearby elliptical galaxies instead of two as discussed in previous works.\n", "versions": [{"version": "v1", "created": "Wed, 5 Mar 2014 09:44:45 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["De", "Tuli", ""], ["Chattopadhyay", "Tanuka", ""], ["Chattopadhyay", "Asis Kumar", ""]]}, {"id": "1403.1210", "submitter": "Saeede Ajorlou", "authors": "Saeede Ajorlou, Issac Shams, Kai Yang", "title": "Predicting patient risk of readmission with frailty models in the\n  Department of Veteran Affairs", "comments": "6 pages, to be submitted in IEEE CASE 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reducing potentially preventable readmissions has been identified as an\nimportant issue for decreasing Medicare costs and improving quality of care\nprovided by hospitals. Based on previous research by medical professionals,\npreventable readmissions are caused by such factors as flawed patient\ndischarging process, inadequate follow-ups after discharging, and noncompliance\nof patients on discharging and follow up instructions. It is also found that\nthe risk of preventable readmission also may relate to some patient's\ncharacteristics, such as age, health condition, diagnosis, and even treatment\nspecialty. In this study, using both general demographic information and\nindividual past history of readmission records, we develop a risk prediction\nmodel based on hierarchical nonlinear mixed effect framework to extract\nsignificant prognostic factors associated with patient risk of 30-day\nreadmission. The effectiveness of our proposed approach is validated based on a\nreal dataset from four VA facilities in the State of Michigan. Simultaneously\nexplaining both patient and population based variations of readmission process,\nsuch an accurate model can be used to recognize patients with high likelihood\nof discharging non-compliances, and then targeted post-care actions can be\ndesigned to reduce further rehospitalization.\n", "versions": [{"version": "v1", "created": "Wed, 5 Mar 2014 18:23:50 GMT"}], "update_date": "2014-03-06", "authors_parsed": [["Ajorlou", "Saeede", ""], ["Shams", "Issac", ""], ["Yang", "Kai", ""]]}, {"id": "1403.1412", "submitter": "Katri Pulliyakode Saishankar", "authors": "K.P. Saishankar, Sheetal Kalyani, K. Narendran", "title": "Rate Prediction and Selection in LTE systems using Modified Source\n  Encoding Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In current wireless systems, the base-Station (eNodeB) tries to serve its\nuser-equipment (UE) at the highest possible rate that the UE can reliably\ndecode. The eNodeB obtains this rate information as a quantized feedback from\nthe UE at time n and uses this, for rate selection till the next feedback is\nreceived at time n + {\\delta}. The feedback received at n can become outdated\nbefore n + {\\delta}, because of a) Doppler fading, and b) Change in the set of\nactive interferers for a UE. Therefore rate prediction becomes essential.\nSince, the rates belong to a discrete set, we propose a discrete sequence\nprediction approach, wherein, frequency trees for the discrete sequences are\nbuilt using source encoding algorithms like Prediction by Partial Match (PPM).\nFinding the optimal depth of the frequency tree used for prediction is cast as\na model order selection problem. The rate sequence complexity is analysed to\nprovide an upper bound on model order. Information-theoretic criteria are then\nused to solve the model order problem. Finally, two prediction algorithms are\nproposed, using the PPM with optimal model order and system level simulations\ndemonstrate the improvement in packet loss and throughput due to these\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 6 Mar 2014 11:32:00 GMT"}, {"version": "v2", "created": "Fri, 14 Mar 2014 06:25:30 GMT"}, {"version": "v3", "created": "Wed, 30 Apr 2014 15:55:38 GMT"}, {"version": "v4", "created": "Fri, 23 May 2014 11:16:58 GMT"}, {"version": "v5", "created": "Fri, 8 Aug 2014 11:10:18 GMT"}], "update_date": "2014-08-11", "authors_parsed": [["Saishankar", "K. P.", ""], ["Kalyani", "Sheetal", ""], ["Narendran", "K.", ""]]}, {"id": "1403.1562", "submitter": "Krzysztof Bartoszek", "authors": "Krzysztof Bartoszek", "title": "The Laplace Motion in Phylogenetic Comparative Methods", "comments": "http://kkzmbm.mimuw.edu.pl/?pageId=4&sprawId=18", "journal-ref": "Proceedings of the Eighteenth National Conference on Applications\n  of Mathematics in Biology and Medicine, 2012", "doi": null, "report-no": null, "categories": "q-bio.PE math.PR stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of current phylogenetic comparative methods assume that the\nstochastic evolutionary process is homogeneous over the phylogeny or offer\nrelaxations of this in rather limited and usually parameter expensive ways.\nHere we make a preliminary investigation, by means of a numerical experiment,\nwhether the Laplace motion process can offer an alternative approach.\n", "versions": [{"version": "v1", "created": "Thu, 6 Mar 2014 20:24:38 GMT"}], "update_date": "2014-03-07", "authors_parsed": [["Bartoszek", "Krzysztof", ""]]}, {"id": "1403.1783", "submitter": "Chrisovaladis Malesios", "authors": "C. Malesios, N. Demiris, K. Kalogeropoulos and I. Ntzoufras", "title": "Bayesian spatio-temporal epidemic models with applications to sheep pox", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Epidemic data often possess certain characteristics, such as the presence of\nmany zeros, the spatial nature of the disease spread mechanism or environmental\nnoise. This paper addresses these issues via suitable Bayesian modelling. In\ndoing so we utilise stochastic regression models appropriate for\nspatio-temporal count data with an excess number of zeros. The developed\nregression framework can incorporate serial correlation and time varying\ncovariates through an Ornstein Uhlenbeck process formulation. In addition, we\nexplore the effect of different priors, including default options and\ntechniques based upon variations of mixtures of $g$-priors. The effect of\ndifferent distance kernels for the epidemic model component is investigated. We\nproceed by developing branching process-based methods for testing scenarios for\ndisease control, thus linking traditional spatio-temporal models with epidemic\nprocesses, useful in policy-focused decision making. The approach is\nillustrated with an application to a sheep pox dataset from the Evros region,\nGreece.\n", "versions": [{"version": "v1", "created": "Fri, 7 Mar 2014 15:37:59 GMT"}], "update_date": "2014-03-10", "authors_parsed": [["Malesios", "C.", ""], ["Demiris", "N.", ""], ["Kalogeropoulos", "K.", ""], ["Ntzoufras", "I.", ""]]}, {"id": "1403.1891", "submitter": "Lihong Li", "authors": "Lihong Li and Shunbao Chen and Jim Kleban and Ankur Gupta", "title": "Counterfactual Estimation and Optimization of Click Metrics for Search\n  Engines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimizing an interactive system against a predefined online metric is\nparticularly challenging, when the metric is computed from user feedback such\nas clicks and payments. The key challenge is the counterfactual nature: in the\ncase of Web search, any change to a component of the search engine may result\nin a different search result page for the same query, but we normally cannot\ninfer reliably from search log how users would react to the new result page.\nConsequently, it appears impossible to accurately estimate online metrics that\ndepend on user feedback, unless the new engine is run to serve users and\ncompared with a baseline in an A/B test. This approach, while valid and\nsuccessful, is unfortunately expensive and time-consuming. In this paper, we\npropose to address this problem using causal inference techniques, under the\ncontextual-bandit framework. This approach effectively allows one to run\n(potentially infinitely) many A/B tests offline from search log, making it\npossible to estimate and optimize online metrics quickly and inexpensively.\nFocusing on an important component in a commercial search engine, we show how\nthese ideas can be instantiated and applied, and obtain very promising results\nthat suggest the wide applicability of these techniques.\n", "versions": [{"version": "v1", "created": "Fri, 7 Mar 2014 22:54:52 GMT"}, {"version": "v2", "created": "Wed, 12 Mar 2014 06:36:02 GMT"}], "update_date": "2014-03-13", "authors_parsed": [["Li", "Lihong", ""], ["Chen", "Shunbao", ""], ["Kleban", "Jim", ""], ["Gupta", "Ankur", ""]]}, {"id": "1403.2003", "submitter": "Sadi Seker E", "authors": "M. Lutfi Arslan, Sadi Evren Seker", "title": "The Impact of Employment Web Sites' Traffic on Unemployment: A Cross\n  Country Comparison", "comments": "9 pages", "journal-ref": "International Journal of Social Sciences and Humanity Studies Vol\n  5, No 2, 2013 ISSN: 1309-8063 (Online)", "doi": null, "report-no": null, "categories": "stat.AP cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although employment web sites have recently become the main source for re-\ncruitment and selection process, the relation between those sites and unemploy-\nment rates is seldom addressed. Deriving data from 32 countries and 427 web\nsites, this study explores the correlation between unemployment rates of\nEuropean countries and the attractiveness of country specific employment web\nsites. It also compares the changes in unemployment rates and traffic on all\nthe aforementioned web sites. The results showed that there is a strong\ncorrelation between web sites traffic and unemployment rates.\n", "versions": [{"version": "v1", "created": "Sat, 8 Mar 2014 19:51:19 GMT"}], "update_date": "2014-03-11", "authors_parsed": [["Arslan", "M. Lutfi", ""], ["Seker", "Sadi Evren", ""]]}, {"id": "1403.2137", "submitter": "Wanchuang Zhu", "authors": "Wanchuang Zhu, Yanan Fan", "title": "Relabelling Algorithms for Large Dataset Mixture Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture models are flexible tools in density estimation and classification\nproblems. Bayesian estimation of such models typically relies on sampling from\nthe posterior distribution using Markov chain Monte Carlo. Label switching\narises because the posterior is invariant to permutations of the component\nparameters. Methods for dealing with label switching have been studied fairly\nextensively in the literature, with the most popular approaches being those\nbased on loss functions. However, many of these algorithms turn out to be too\nslow in practice, and can be infeasible as the size and dimension of the data\ngrow. In this article, we review earlier solutions which can scale up well for\nlarge data sets, and compare their performances on simulated and real datasets.\nIn addition, we propose a new, and computationally efficient algorithm based on\na loss function interpretation, and show that it can scale up well in larger\nproblems. We conclude with some discussions and recommendations of all the\nmethods studied.\n", "versions": [{"version": "v1", "created": "Mon, 10 Mar 2014 05:09:31 GMT"}], "update_date": "2014-03-11", "authors_parsed": [["Zhu", "Wanchuang", ""], ["Fan", "Yanan", ""]]}, {"id": "1403.2272", "submitter": "Daniele Durante", "authors": "Daniele Durante and David B. Dunson", "title": "Bayesian dynamic financial networks with time-varying predictors", "comments": null, "journal-ref": "Statistics & Probability Letters (2014). 93, 19-26", "doi": "10.1016/j.spl.2014.06.015", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian nonparametric model including time-varying predictors\nin dynamic network inference. The model is applied to infer the dependence\nstructure among financial markets during the global financial crisis,\nestimating effects of verbal and material cooperation efforts. We interestingly\nlearn contagion effects, with increasing influence of verbal relations during\nthe financial crisis and opposite results during the United States housing\nbubble.\n", "versions": [{"version": "v1", "created": "Mon, 10 Mar 2014 16:17:24 GMT"}], "update_date": "2014-07-08", "authors_parsed": [["Durante", "Daniele", ""], ["Dunson", "David B.", ""]]}, {"id": "1403.3028", "submitter": "Gunnar Stefansson", "authors": "Sigrun Helga Lund, Asgeir Sigurdsson, Sigurjon Axel Gudjonsson, Julius\n  Gudmundsson, Daniel Fannar Gudbjartsson, Thorunn Rafnar, Kari Stefansson and\n  Gunnar Stefansson", "title": "Estimating robustness of the tileShuffle method with repeated probes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the TileShuffle method is evaluated as a search method for\ncandidate lncRNAs at 8q24.2. The method is run on three microarrays.\nMicroarrays which all contained the same sample and repeated copies of tiled\nprobes. This allows the coherence of the selection method within and between\nmicroarrays to be estimated by Monte Carlo simulations on the repeated probes.\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2014 16:57:14 GMT"}], "update_date": "2014-03-13", "authors_parsed": [["Lund", "Sigrun Helga", ""], ["Sigurdsson", "Asgeir", ""], ["Gudjonsson", "Sigurjon Axel", ""], ["Gudmundsson", "Julius", ""], ["Gudbjartsson", "Daniel Fannar", ""], ["Rafnar", "Thorunn", ""], ["Stefansson", "Kari", ""], ["Stefansson", "Gunnar", ""]]}, {"id": "1403.3048", "submitter": "Karl Broman", "authors": "Il-Youp Kwak, Candace R. Moore, Edgar P. Spalding, Karl W. Broman", "title": "A simple regression-based method to map quantitative trait loci\n  underlying function-valued phenotypes", "comments": "Considerable changes in response to reviewers' comments. Revised the\n  initial simulation study. Moved one figure to supplement; added another\n  figure plus three additional supplemental figures", "journal-ref": "Kwak I-Y, Moore CR, Spalding EP, Broman KW (2014) A simple\n  regression-based method to map quantitative trait loci underlying\n  function-valued phenotypes. Genetics 197: 1409-1416", "doi": "10.1534/genetics.114.166306", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Most statistical methods for QTL mapping focus on a single phenotype.\nHowever, multiple phenotypes are commonly measured, and recent technological\nadvances have greatly simplified the automated acquisition of numerous\nphenotypes, including function-valued phenotypes, such as growth measured over\ntime. While there exist methods for QTL mapping with function-valued\nphenotypes, they are generally computationally intensive and focus on\nsingle-QTL models. We propose two simple, fast methods that maintain high power\nand precision and are amenable to extensions with multiple-QTL models using a\npenalized likelihood approach. After identifying multiple QTL by these\napproaches, we can view the function-valued QTL effects to provide a deeper\nunderstanding of the underlying processes. Our methods have been implemented as\na package for R, funqtl.\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2014 17:56:28 GMT"}, {"version": "v2", "created": "Thu, 15 May 2014 19:23:42 GMT"}], "update_date": "2014-08-11", "authors_parsed": [["Kwak", "Il-Youp", ""], ["Moore", "Candace R.", ""], ["Spalding", "Edgar P.", ""], ["Broman", "Karl W.", ""]]}, {"id": "1403.3260", "submitter": "Luis Barboza", "authors": "Luis Barboza, Bo Li, Martin P. Tingley, Frederi G. Viens", "title": "Reconstructing past temperatures from natural proxies and estimated\n  climate forcings using short- and long-memory models", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS785 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 4, 1966-2001", "doi": "10.1214/14-AOAS785", "report-no": "IMS-AOAS-AOAS785", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We produce new reconstructions of Northern Hemisphere annually averaged\ntemperature anomalies back to 1000 AD, and explore the effects of including\nexternal climate forcings within the reconstruction and of accounting for\nshort-memory and long-memory features. Our reconstructions are based on two\nlinear models, with the first linking the latent temperature series to three\nmain external forcings (solar irradiance, greenhouse gas concentration and\nvolcanism), and the second linking the observed temperature proxy data (tree\nrings, sediment record, ice cores, etc.) to the unobserved temperature series.\nUncertainty is captured with additive noise, and a rigorous statistical\ninvestigation of the correlation structure in the regression errors is\nconducted through systematic comparisons between reconstructions that assume no\nmemory, short-memory autoregressive models, and long-memory fractional Gaussian\nnoise models. We use Bayesian estimation to fit the model parameters and to\nperform separate reconstructions of land-only and combined land-and-marine\ntemperature anomalies. For model formulations that include forcings, both\nexploratory and Bayesian data analysis provide evidence against models with no\nmemory. Model assessments indicate that models with no memory underestimate\nuncertainty. However, no single line of evidence is sufficient to favor\nshort-memory models over long-memory ones, or to favor the opposite choice.\nWhen forcings are not included, the long-memory models appear to be necessary.\nWhile including external climate forcings substantially improves the\nreconstruction, accurate reconstructions that exclude these forcings are vital\nfor testing the fidelity of climate models used for future projections.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2014 13:37:58 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2015 08:35:43 GMT"}], "update_date": "2015-03-05", "authors_parsed": [["Barboza", "Luis", ""], ["Li", "Bo", ""], ["Tingley", "Martin P.", ""], ["Viens", "Frederi G.", ""]]}, {"id": "1403.3371", "submitter": "Hamed Firouzi", "authors": "Hamed Firouzi, Dennis Wei, Alfred O. Hero III", "title": "Spectral Correlation Hub Screening of Multivariate Time Series", "comments": "32 pages, To appear in Excursions in Harmonic Analysis: The February\n  Fourier Talks at the Norbert Wiener Center", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter discusses correlation analysis of stationary multivariate\nGaussian time series in the spectral or Fourier domain. The goal is to identify\nthe hub time series, i.e., those that are highly correlated with a specified\nnumber of other time series. We show that Fourier components of the time series\nat different frequencies are asymptotically statistically independent. This\nproperty permits independent correlation analysis at each frequency,\nalleviating the computational and statistical challenges of high-dimensional\ntime series. To detect correlation hubs at each frequency, an existing\ncorrelation screening method is extended to the complex numbers to accommodate\ncomplex-valued Fourier components. We characterize the number of hub\ndiscoveries at specified correlation and degree thresholds in the regime of\nincreasing dimension and fixed sample size. The theory specifies appropriate\nthresholds to apply to sample correlation matrices to detect hubs and also\nallows statistical significance to be attributed to hub discoveries. Numerical\nresults illustrate the accuracy of the theory and the usefulness of the\nproposed spectral framework.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2014 19:01:28 GMT"}, {"version": "v2", "created": "Wed, 9 Apr 2014 16:25:31 GMT"}], "update_date": "2014-04-10", "authors_parsed": [["Firouzi", "Hamed", ""], ["Wei", "Dennis", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1403.3909", "submitter": "Nesreen Ahmed", "authors": "Nesreen K. Ahmed, Nick Duffield, Jennifer Neville, Ramana Kompella", "title": "Graph Sample and Hold: A Framework for Big-Graph Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sampling is a standard approach in big-graph analytics; the goal is to\nefficiently estimate the graph properties by consulting a sample of the whole\npopulation. A perfect sample is assumed to mirror every property of the whole\npopulation. Unfortunately, such a perfect sample is hard to collect in complex\npopulations such as graphs (e.g. web graphs, social networks etc), where an\nunderlying network connects the units of the population. Therefore, a good\nsample will be representative in the sense that graph properties of interest\ncan be estimated with a known degree of accuracy. While previous work focused\nparticularly on sampling schemes used to estimate certain graph properties\n(e.g. triangle count), much less is known for the case when we need to estimate\nvarious graph properties with the same sampling scheme. In this paper, we\npropose a generic stream sampling framework for big-graph analytics, called\nGraph Sample and Hold (gSH). To begin, the proposed framework samples from\nmassive graphs sequentially in a single pass, one edge at a time, while\nmaintaining a small state. We then show how to produce unbiased estimators for\nvarious graph properties from the sample. Given that the graph analysis\nalgorithms will run on a sample instead of the whole population, the runtime\ncomplexity of these algorithm is kept under control. Moreover, given that the\nestimators of graph properties are unbiased, the approximation error is kept\nunder control. Finally, we show the performance of the proposed framework (gSH)\non various types of graphs, such as social graphs, among others.\n", "versions": [{"version": "v1", "created": "Sun, 16 Mar 2014 12:26:45 GMT"}], "update_date": "2014-03-18", "authors_parsed": [["Ahmed", "Nesreen K.", ""], ["Duffield", "Nick", ""], ["Neville", "Jennifer", ""], ["Kompella", "Ramana", ""]]}, {"id": "1403.4086", "submitter": "Hande Topa", "authors": "Hande Topa, \\'Agnes J\\'on\\'as, Robert Kofler, Carolin Kosiol, Antti\n  Honkela", "title": "Gaussian process test for high-throughput sequencing time series:\n  application to experimental evolution", "comments": "41 pages, 29 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE q-bio.GN q-bio.QM stat.AP", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Motivation: Recent advances in high-throughput sequencing (HTS) have made it\npossible to monitor genomes in great detail. New experiments not only use HTS\nto measure genomic features at one time point but to monitor them changing over\ntime with the aim of identifying significant changes in their abundance. In\npopulation genetics, for example, allele frequencies are monitored over time to\ndetect significant frequency changes that indicate selection pressures.\nPrevious attempts at analysing data from HTS experiments have been limited as\nthey could not simultaneously include data at intermediate time points,\nreplicate experiments and sources of uncertainty specific to HTS such as\nsequencing depth.\n  Results: We present the beta-binomial Gaussian process (BBGP) model for\nranking features with significant non-random variation in abundance over time.\nThe features are assumed to represent proportions, such as proportion of an\nalternative allele in a population. We use the beta-binomial model to capture\nthe uncertainty arising from finite sequencing depth and combine it with a\nGaussian process model over the time series. In simulations that mimic the\nfeatures of experimental evolution data, the proposed method clearly\noutperforms classical testing in average precision of finding selected alleles.\nWe also present simulations exploring different experimental design choices and\nresults on real data from Drosophila experimental evolution experiment in\ntemperature adaptation.\n  Availability: R software implementing the test is available at\nhttps://github.com/handetopa/BBGP\n", "versions": [{"version": "v1", "created": "Mon, 17 Mar 2014 13:08:06 GMT"}, {"version": "v2", "created": "Tue, 18 Mar 2014 14:14:25 GMT"}, {"version": "v3", "created": "Thu, 18 Sep 2014 07:59:09 GMT"}], "update_date": "2014-09-19", "authors_parsed": [["Topa", "Hande", ""], ["J\u00f3n\u00e1s", "\u00c1gnes", ""], ["Kofler", "Robert", ""], ["Kosiol", "Carolin", ""], ["Honkela", "Antti", ""]]}, {"id": "1403.4171", "submitter": "Giuseppe Arbia", "authors": "Giuseppe arbia", "title": "Least quartic Regression Criterion with Application to Finance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes a new method for the estimation of the parameters of a\nsimple linear regression model which accounts for the role of co-moments in\nnon-Gaussian distributions being based on the minimization of a quartic loss\nfunction. Although the proposed method is very general, we examine its\napplication to finance. In fact, in this field the contribution of the\nco-moments in explaining the return-generating process is of paramount\nimportance when evaluating the systematic risk of an asset within the framework\nof the Capital Asset Pricing Model (CAPM). The suggested new method contributes\nto this literature by showing that, in the presence of non-normality, the\nregression slope can be expressed as a function of the co-kurtosis between the\nreturns of a risky asset and the market proxy. The paper provides an\nillustration of the method based on some empirical financial data referring to\n40 industrial sector assets rates of the Italian stock market.\n", "versions": [{"version": "v1", "created": "Mon, 17 Mar 2014 17:06:29 GMT"}], "update_date": "2014-03-18", "authors_parsed": [["arbia", "Giuseppe", ""]]}, {"id": "1403.4291", "submitter": "Mathieu Cambou", "authors": "Philipp Arbenz, Mathieu Cambou and Marius Hofert", "title": "An importance sampling approach for copula models in insurance", "comments": "24 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An importance sampling approach for sampling copula models is introduced. We\npropose two algorithms that improve Monte Carlo estimators when the functional\nof interest depends mainly on the behaviour of the underlying random vector\nwhen at least one of the components is large. Such problems often arise from\ndependence models in finance and insurance. The importance sampling framework\nwe propose is general and can be easily implemented for all classes of copula\nmodels from which sampling is feasible. We show how the proposal distribution\nof the two algorithms can be optimized to reduce the sampling error. In a case\nstudy inspired by a typical multivariate insurance application, we obtain\nvariance reduction factors between 10 and 30 in comparison to standard Monte\nCarlo estimators.\n", "versions": [{"version": "v1", "created": "Mon, 17 Mar 2014 22:28:14 GMT"}, {"version": "v2", "created": "Mon, 15 Dec 2014 22:09:12 GMT"}, {"version": "v3", "created": "Tue, 7 Apr 2015 17:57:36 GMT"}], "update_date": "2015-04-08", "authors_parsed": [["Arbenz", "Philipp", ""], ["Cambou", "Mathieu", ""], ["Hofert", "Marius", ""]]}, {"id": "1403.4309", "submitter": "Oleksandr Savenkov", "authors": "A. Savenkov, S. Wu, D. Neal", "title": "Testing for Efficacy in Single-Subject Trials with Intervention Analysis", "comments": "wrong file", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single subject or n-of-1 research designs have been widely used to evaluate\ntreatment interventions. Many statistical procedures such as split-middle trend\nlines, regression trend line, Shewart-chart trend line, binomial tests,\nrandomization tests and Tryon C-statistics have been used to analyze\nsingle-subject data, but they fail to control Type I error due to\nserially-dependent time-series observations. The interrupted time series\nanalysis maintains Type I error but assumes that the intervention effect to be\na linear trend change from baseline. In this paper, we consider an improved\nintervention analysis model (Box and Tiao, 1975) for dynamic characteristics of\nan intervention effect in a short series of single-subject data. The maximum\nlikelihood estimates are derived and a hypothesis testing procedure is\nproposed. The method is illustrated with a real clinical trial on constraint\ninduced language therapy for aphasia patients.\n", "versions": [{"version": "v1", "created": "Tue, 18 Mar 2014 00:14:32 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2015 14:59:21 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Savenkov", "A.", ""], ["Wu", "S.", ""], ["Neal", "D.", ""]]}, {"id": "1403.4370", "submitter": "Kun  Yang", "authors": "Kun Yang and Wing Hung Wong", "title": "Discovering and Visualizing Hierarchy in Multivariate Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to extract useful insights from data is always a challenge, especially if\nthe data is multidimensional. Often, the data can be organized according to\ncertain hierarchical structure that are stemmed either from data collection\nprocess or from the information and phenomena carried by the data itself. The\ncurrent study attempts to discover and visualize these underlying hierarchies.\nBy regarding each observation in the data as a draw from a (hypothetical)\nmultidimensional joint density, our first goal is to approximate this unknown\ndensity with a piecewise constant function via binary partition, our\nnon-parametric approach makes no assumptions on the form of the density. Given\nthe piecewise constant density function and its corresponding binary partition,\nour second goal is to construct a connected graph and build up a tree\nrepresentation of the data by level sets. To demonstrate that our method is a\ngeneral data mining and visualization tool which can provide \"multi-resolution\"\nsummaries and reveal different levels of information of the data, we apply it\nto two real data sets from Flow Cytometry and Social Network.\n", "versions": [{"version": "v1", "created": "Tue, 18 Mar 2014 08:43:52 GMT"}, {"version": "v2", "created": "Wed, 26 Mar 2014 07:59:55 GMT"}, {"version": "v3", "created": "Fri, 28 Mar 2014 06:37:35 GMT"}, {"version": "v4", "created": "Wed, 20 Apr 2016 16:17:11 GMT"}], "update_date": "2016-04-21", "authors_parsed": [["Yang", "Kun", ""], ["Wong", "Wing Hung", ""]]}, {"id": "1403.4512", "submitter": "Vilson Vieira da Silva Junior", "authors": "Vilson Vieira, Renato Fabbri, David Sbrissa, Luciano da Fontoura\n  Costa, Gonzalo Travieso", "title": "A Quantitative Approach to Painting Styles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.OH", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This research extends a method previously applied to music and\nphilosophy,representing the evolution of art as a time-series where relations\nlike dialectics are measured quantitatively. For that, a corpus of paintings of\n12 well-known artists from baroque and modern art is analyzed. A set of 93\nfeatures is extracted and the features which most contributed to the\nclassification of painters are selected. The projection space obtained provides\nthe basis to the analysis of measurements. This quantitative measures underlie\nrevealing observations about the evolution of painting styles, specially when\ncompared with other humanity fields already analyzed: while music evolved along\na master-apprentice tradition (high dialectics) and philosophy by opposition,\npainting presents another pattern: constant increasing skewness, low opposition\nbetween members of the same movement and opposition peaks in the transition\nbetween movements. Differences between baroque and modern movements are also\nobserved in the projected \"painting space\": while baroque paintings are\npresented as an overlapped cluster, the modern paintings present minor\noverlapping and are disposed more widely in the projection than the baroque\ncounterparts. This finding suggests that baroque painters shared aesthetics\nwhile modern painters tend to \"break rules\" and develop their own style.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2013 00:15:47 GMT"}], "update_date": "2014-03-19", "authors_parsed": [["Vieira", "Vilson", ""], ["Fabbri", "Renato", ""], ["Sbrissa", "David", ""], ["Costa", "Luciano da Fontoura", ""], ["Travieso", "Gonzalo", ""]]}, {"id": "1403.4513", "submitter": "Vilson Vieira da Silva Junior", "authors": "Vilson Vieira, Renato Fabbri, Gonzalo Travieso, Osvaldo N. Oliveira\n  Jr., Luciano da Fontoura Costa", "title": "A quantitative approach to evolution of music and philosophy", "comments": "arXiv admin note: substantial text overlap with arXiv:1109.4653", "journal-ref": "J. Stat. Mech. (2012) P08010", "doi": "10.1088/1742-5468/2012/08/P08010", "report-no": null, "categories": "stat.AP cs.SD", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The development of new statistical and computational methods is increasingly\nmaking it possible to bridge the gap between hard sciences and humanities. In\nthis study, we propose an approach based on a quantitative evaluation of\nattributes of objects in fields of humanities, from which concepts such as\ndialectics and opposition are formally defined mathematically. As case studies,\nwe analyzed the temporal evolution of classical music and philosophy by\nobtaining data for 8 features characterizing the corresponding fields for 7\nwell-known composers and philosophers, which were treated with multivariate\nstatistics and pattern recognition methods. A bootstrap method was applied to\navoid statistical bias caused by the small sample data set, with which hundreds\nof artificial composers and philosophers were generated, influenced by the 7\nnames originally chosen. Upon defining indices for opposition, skewness and\ncounter-dialectics, we confirmed the intuitive analysis of historians in that\nclassical music evolved according to a master-apprentice tradition, while in\nphilosophy changes were driven by opposition. Though these case studies were\nmeant only to show the possibility of treating phenomena in humanities\nquantitatively, including a quantitative measure of concepts such as dialectics\nand opposition the results are encouraging for further application of the\napproach presented here to many other areas, since it is entirely generic.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2013 01:18:31 GMT"}], "update_date": "2014-03-19", "authors_parsed": [["Vieira", "Vilson", ""], ["Fabbri", "Renato", ""], ["Travieso", "Gonzalo", ""], ["Oliveira", "Osvaldo N.", "Jr."], ["Costa", "Luciano da Fontoura", ""]]}, {"id": "1403.4698", "submitter": "Xi Luo", "authors": "Xi Luo", "title": "A Hierarchical Graphical Model for Big Inverse Covariance Estimation\n  with an Application to fMRI", "comments": "An R package of the proposed method will be publicly available on\n  CRAN. This paper has been presented orally at Yale University on Feburary 18,\n  2014, and at the Eastern North American Region Meeting of the International\n  Biometric Society on March 18, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain networks has attracted the interests of many neuroscientists. From\nfunctional MRI (fMRI) data, statistical tools have been developed to recover\nbrain networks. However, the dimensionality of whole-brain fMRI, usually in\nhundreds of thousands, challenges the applicability of these methods. We\ndevelop a hierarchical graphical model (HGM) to remediate this difficulty. This\nmodel introduces a hidden layer of networks based on sparse Gaussian graphical\nmodels, and the observed data are sampled from individual network nodes. In\nfMRI, the network layer models the underlying signals of different brain\nfunctional units, and how these units directly interact with each other. The\nintroduction of this hierarchical structure not only provides a formal and\ninterpretable approach, but also enables efficient computation for inferring\nbig networks with hundreds of thousands of nodes. Based on the conditional\nconvexity of our formulation, we develop an alternating update algorithm to\ncompute the HGM model parameters simultaneously. The effectiveness of this\napproach is demonstrated on simulated data and a real dataset from a stop/go\nfMRI experiment.\n", "versions": [{"version": "v1", "created": "Wed, 19 Mar 2014 05:43:43 GMT"}, {"version": "v2", "created": "Mon, 7 Apr 2014 18:49:10 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Luo", "Xi", ""]]}, {"id": "1403.4700", "submitter": "Ryosuke Yano", "authors": "Ryosuke Yano, Arnaud Martin", "title": "Kinetic modeling of opinion formation of peoples via multiple political\n  parties", "comments": "Further considerations are essential for binary exchange of opinions.\n  Present version is insufficient", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the opinion formation among the peoples and multiple political\nparties using the one dimensional relativistic Boltzmann-Vlasov equation for\nmulti-components. A political party is constituted of politicians. The opinion\nformation depends on self-thinkings of peoples and politicians, and the\nconstraint of the political party over opinions of politicians, when we\nrestrict ourselves to the conciliatory exchange of opinions between two\nindividuals. In particular, shock like profiles are obtained in the\ndistribution of opinions of peoples, when the self-thinking of politicians are\nabsent at the binary exchange of opinions between two politicians in the same\npolitical party.\n", "versions": [{"version": "v1", "created": "Wed, 19 Mar 2014 06:10:17 GMT"}, {"version": "v2", "created": "Fri, 16 May 2014 04:33:15 GMT"}], "update_date": "2014-05-19", "authors_parsed": [["Yano", "Ryosuke", ""], ["Martin", "Arnaud", ""]]}, {"id": "1403.4732", "submitter": "Tommi Suvitaival", "authors": "Tommi Suvitaival, Simon Rogers, Samuel Kaski", "title": "Stronger findings from mass spectral data through multi-peak modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.BM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mass spectrometry-based metabolomic analysis depends upon the identification\nof spectral peaks by their mass and retention time. Statistical analysis that\nfollows the identification currently relies on one main peak of each compound.\nHowever, a compound present in the sample typically produces several spectral\npeaks due to its isotopic properties and the ionization process of the mass\nspectrometer device. In this work, we investigate the extent to which these\nadditional peaks can be used to increase the statistical strength of\ndifferential analysis.\n  We present a Bayesian approach for integrating data of multiple detected\npeaks that come from one compound. We demonstrate the approach through a\nsimulated experiment and validate it on ultra performance liquid\nchromatography-mass spectrometry (UPLC-MS) experiments for metabolomics and\nlipidomics. Peaks that are likely to be associated with one compound can be\nclustered by the similarity of their chromatographic shape. Changes of\nconcentration between sample groups can be inferred more accurately when\nmultiple peaks are available.\n  When the sample-size is limited, the proposed multi-peak approach improves\nthe accuracy at inferring covariate effects. An R implementation, data and the\nsupplementary material are available at\nhttp://research.ics.aalto.fi/mi/software/peakANOVA/ .\n", "versions": [{"version": "v1", "created": "Wed, 19 Mar 2014 08:44:18 GMT"}], "update_date": "2014-03-20", "authors_parsed": [["Suvitaival", "Tommi", ""], ["Rogers", "Simon", ""], ["Kaski", "Samuel", ""]]}, {"id": "1403.4897", "submitter": "Sudhakar Prasad", "authors": "Sudhakar Prasad", "title": "Asymptotics of Bayesian Error Probability and Rotating-PSF-Based Source\n  Super-Localization in Three Dimensions", "comments": "Submitted to Optics Express, March 17, 2014", "journal-ref": null, "doi": "10.1364/OE.22.016008", "report-no": null, "categories": "physics.optics stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an asymptotic analysis of the minimum probability of error (MPE)\nin inferring the correct hypothesis in a Bayesian multi-hypothesis testing\n(MHT) formalism using many pixels of data that are corrupted by signal\ndependent shot noise, sensor read noise, and background illumination. We\nperform this error analysis for a variety of combined noise and background\nstatistics, including a pseudo-Gaussian distribution that can be employed to\ntreat approximately the photon-counting statistics of signal and background as\nwell as purely Gaussian sensor read-out noise and more general, exponentially\npeaked distributions. We subsequently apply the MPE asymptotics to characterize\nthe minimum conditions needed to localize a point source in three dimensions by\nmeans of a rotating-PSF imager and compare its performance with that of a\nconventional imager in the presence of background and sensor-noise\nfluctuations. In a separate paper, we apply the formalism to the related but\nqualitatively different problem of 2D super-resolution imaging of a closely\nspaced pair of point sources in the plane of best focus.\n", "versions": [{"version": "v1", "created": "Wed, 19 Mar 2014 18:04:21 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Prasad", "Sudhakar", ""]]}, {"id": "1403.4919", "submitter": "Sudhakar Prasad", "authors": "Sudhakar Prasad", "title": "Asymptotics of Bayesian Error Probability and 2D Pair Superresolution", "comments": "Submitted to Optics Express, March 18, 2014", "journal-ref": null, "doi": "10.1364/OE.22.016029", "report-no": null, "categories": "physics.optics stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper employs a recently developed asymptotic Bayesian multi-hypothesis\ntesting (MHT) error analysis to treat the problem of superresolution imaging of\na pair of closely spaced, equally bright point sources. The analysis exploits\nthe notion of the minimum probability of error (MPE) in discriminating between\ntwo competing equi-probable hypotheses, a single point source of a certain\nbrightness at the origin vs. a pair of point sources, each of half the\nbrightness of the single source and located symmetrically about the origin, as\nthe distance between the source pair is changed. For a Gaussian point-spread\nfunction (PSF), the analysis makes predictions on the scaling of the minimum\nsource strength, expressed in units of photon number, required to disambiguate\nthe pair as a function of their separation, in both the signal-dominated and\nbackground-dominated regimes. Certain logarithmic corrections to the quartic\nscaling of the minimum source strength with respect to the degree of\nsuperresolution characterize the signal-dominated regime, while the scaling is\npurely quadratic in the background-dominated regime. For the Gaussian PSF,\ngeneral results for arbitrary strengths of the signal, background, and sensor\nnoise levels are also presented.\n", "versions": [{"version": "v1", "created": "Wed, 19 Mar 2014 19:24:44 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Prasad", "Sudhakar", ""]]}, {"id": "1403.5065", "submitter": "Dario Gasbarra", "authors": "Dario Gasbarra, Jia Liu, Juha Railavo", "title": "Data augmentation in Rician noise model and Bayesian Diffusion Tensor\n  Imaging", "comments": "37 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mapping white matter tracts is an essential step towards understanding brain\nfunction. Diffusion Magnetic Resonance Imaging (dMRI) is the only noninvasive\ntechnique which can detect in vivo anisotropies in the 3-dimensional diffusion\nof water molecules, which correspond to nervous fibers in the living brain. In\nthis process, spectral data from the displacement distribution of water\nmolecules is collected by a magnetic resonance scanner. From the statistical\npoint of view, inverting the Fourier transform from such sparse and noisy\nspectral measurements leads to a non-linear regression problem. Diffusion\ntensor imaging (DTI) is the simplest modeling approach postulating a Gaussian\ndisplacement distribution at each volume element (voxel). Typically the\ninference is based on a linearized log-normal regression model that can fit the\nspectral data at low frequencies. However such approximation fails to fit the\nhigh frequency measurements which contain information about the details of the\ndisplacement distribution but have a low signal to noise ratio. In this paper,\nwe directly work with the Rice noise model and cover the full range of\n$b$-values. Using data augmentation to represent the likelihood, we reduce the\nnon-linear regression problem to the framework of generalized linear models.\nThen we construct a Bayesian hierarchical model in order to perform\nsimultaneously estimation and regularization of the tensor field. Finally the\nBayesian paradigm is implemented by using Markov chain Monte Carlo.\n", "versions": [{"version": "v1", "created": "Thu, 20 Mar 2014 08:37:14 GMT"}], "update_date": "2014-03-21", "authors_parsed": [["Gasbarra", "Dario", ""], ["Liu", "Jia", ""], ["Railavo", "Juha", ""]]}, {"id": "1403.5417", "submitter": "Gianluca Rosso", "authors": "Gianluca Rosso", "title": "Outliers Emphasis on Cluster Analysis - The use of squared Euclidean\n  distance and fuzzy clustering to detect outliers in a dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outlier is the term that indicates in statistics an anomalous observation,\naberrant, clearly distant from others collected observations. The outliers are\nthe subject to animated discussions in various contexts with regard to be or\nnot to be considered in the average evaluations. Outliers can become a precious\nsource of information, on condition that be able to accurately identify the\npresence in the reference datasets. The need to identify the presence of\nclustered outliers in a dataset not previously treated could argue for a fuzzy\nclustering, emphasized by using the quadratic Euclidean distance as similarity\nmeasure. For interesting and useful results, it should be inclined a\npossibilistic clustering approach, where the term 'possibilistic' means, always\nin mathematical rigor, a component of interpretation of values that point out\nanomalous cases. The crisp method does not allow it, the fuzzy method introduce\nit, the possibilistic one use it. This is a very simple paper with divulgative\npurposes, addressed especially to students, but not only.\n", "versions": [{"version": "v1", "created": "Fri, 21 Mar 2014 10:59:49 GMT"}], "update_date": "2014-03-24", "authors_parsed": [["Rosso", "Gianluca", ""]]}, {"id": "1403.5478", "submitter": "Adam Sales", "authors": "Adam C. Sales and Ben B. Hansen", "title": "Limitless Regression Discontinuity", "comments": "Forthcoming in Journal of Educational and Behavioral Statistics", "journal-ref": "Journal of Educational and Behavioral Statistics.\n  2020;45(2):143-174", "doi": "10.3102/1076998619884904", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventionally, regression discontinuity analysis contrasts a univariate\nregression's limits as its independent variable, $R$, approaches a cut-point,\n$c$, from either side. Alternative methods target the average treatment effect\nin a small region around $c$, at the cost of an assumption that treatment\nassignment, $\\mathcal{I}\\left[R<c\\right]$, is ignorable vis a vis potential\noutcomes.\n  Instead, the method presented in this paper assumes Residual Ignorability,\nignorability of treatment assignment vis a vis detrended potential outcomes.\nDetrending is effected not with ordinary least squares but with MM-estimation,\nfollowing a distinct phase of sample decontamination. The method's inferences\nacknowledge uncertainty in both of these adjustments, despite its applicability\nwhether $R$ is discrete or continuous; it is uniquely robust to leading\nvalidity threats facing regression discontinuity designs.\n", "versions": [{"version": "v1", "created": "Fri, 21 Mar 2014 14:43:25 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2015 16:53:14 GMT"}, {"version": "v3", "created": "Wed, 24 Aug 2016 20:27:21 GMT"}, {"version": "v4", "created": "Wed, 21 Jun 2017 19:58:19 GMT"}, {"version": "v5", "created": "Tue, 15 May 2018 18:04:09 GMT"}, {"version": "v6", "created": "Fri, 29 Mar 2019 00:55:20 GMT"}, {"version": "v7", "created": "Fri, 4 Oct 2019 20:07:47 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Sales", "Adam C.", ""], ["Hansen", "Ben B.", ""]]}, {"id": "1403.5481", "submitter": "Chu Mai", "authors": "Bruno Sudret, Chu Mai, Katerina Konakli", "title": "Assessment of the lognormality assumption of seismic fragility curves\n  using non-parametric representations", "comments": "17 pages, 10 figures, submitted to Structural Safety journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fragility curves are commonly used in civil engineering to estimate the\nvulnerability of structures to earthquakes. The probability of failure\nassociated with a failure criterion (e.g. the maximal inter-storey drift ratio\nbeing greater than a prescribed threshold) is represented as a function of the\nintensity of the earthquake ground motion (e.g. peak ground acceleration or\nspectral acceleration). The classical approach consists in assuming a lognormal\nshape of the fragility curves. In this paper, we introduce two non-parametric\napproaches to establish the fragility curves without making any assumption,\nnamely the conditional Monte Carlo simulation and the kernel density\nestimation. As an illustration, we compute the fragility curves of a 3-storey\nsteel structure, accounting for the nonlinear behavior of the system. The\ncurves obtained by the proposed approaches are compared with each other and\nwith those obtained using the classical lognormal assumption.\n", "versions": [{"version": "v1", "created": "Fri, 21 Mar 2014 14:55:47 GMT"}, {"version": "v2", "created": "Mon, 4 May 2015 09:36:06 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Sudret", "Bruno", ""], ["Mai", "Chu", ""], ["Konakli", "Katerina", ""]]}, {"id": "1403.5534", "submitter": "Allen Downey", "authors": "Allen B. Downey", "title": "Religious affiliation, education and Internet use", "comments": "12 pages, 1 figure, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using data from the General Social Survey, we measure the effect of education\nand Internet use on religious affiliation. We find that Internet use is\nassociated with decreased probability of religious affiliation; for moderate\nuse (2 or more hours per week) the odds ratio is 0.82 (CI 0.69--0.98, p=0.01).\nFor heavier use (7 or more hours per week) the odds ratio is 0.58 (CI\n0.41--0.81, p<0.001). In the 2010 U.S. population, Internet use could account\nfor 5.1 million people with no religious affiliation, or 20% of the observed\ndecrease in affiliation relative to the 1980s. Increases in college graduation\nbetween the 1980s and 2000s could account for an additional 5% of the decrease.\n", "versions": [{"version": "v1", "created": "Fri, 21 Mar 2014 18:20:49 GMT"}], "update_date": "2014-03-24", "authors_parsed": [["Downey", "Allen B.", ""]]}, {"id": "1403.5537", "submitter": "Alexandre Janon", "authors": "Yohann De Castro (LM-Orsay), Alexandre Janon (LM-Orsay, - M\\'ethodes\n  d'Analyse Stochastique des Codes et Traitements Num\\'eriques)", "title": "Randomized pick-freeze for sparse Sobol indices estimation in high\n  dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article investigates a new procedure to estimate the influence of each\nvariable of a given function defined on a high-dimensional space. More\nprecisely, we are concerned with describing a function of a large number $p$ of\nparameters that depends only on a small number $s$ of them. Our proposed method\nis an unconstrained $\\ell_{1}$-minimization based on the Sobol's method. We\nprove that, with only $\\mathcal O(s\\log p)$ evaluations of $f$, one can find\nwhich are the relevant parameters.\n", "versions": [{"version": "v1", "created": "Fri, 21 Mar 2014 18:41:09 GMT"}], "update_date": "2014-03-24", "authors_parsed": [["De Castro", "Yohann", "", "LM-Orsay"], ["Janon", "Alexandre", "", "LM-Orsay, - M\u00e9thodes\n  d'Analyse Stochastique des Codes et Traitements Num\u00e9riques"]]}, {"id": "1403.5539", "submitter": "Mathilde Grandjacques", "authors": "Mathilde Grandjacques (G2ELab), Alexandre Janon (LM-Orsay, GdR\n  MASCOT-NUM), Benoit Delinchant (G2ELab), Olivier Adrot (G-SCOP\\_GCSP)", "title": "Pick and freeze estimation of sensitivity indices for models with\n  dependent and dynamic input processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses sensitivity analysis for dynamic models, linking\ndependent inputs to observed outputs. The usual method to estimate Sobol\nindices are based on the independence of input variables. We present a method\nto overpass this constraint when inputs are Gaussian processes of high\ndimension in a time related framework. Our proposition leads to a\ngeneralization of Sobol indices when inputs are both dependant and dynamic. The\nmethod of estimation is a modification of the Pick and Freeze simulation\nscheme. First we study the general Gaussian cases and secondly we detail the\ncase of stationary models. We then apply the results to an example of heat\nexchanges inside a building.\n", "versions": [{"version": "v1", "created": "Fri, 21 Mar 2014 18:48:07 GMT"}, {"version": "v2", "created": "Sun, 13 Sep 2015 06:23:53 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Grandjacques", "Mathilde", "", "G2ELab"], ["Janon", "Alexandre", "", "LM-Orsay, GdR\n  MASCOT-NUM"], ["Delinchant", "Benoit", "", "G2ELab"], ["Adrot", "Olivier", "", "G-SCOP\\_GCSP"]]}, {"id": "1403.5833", "submitter": "Salil Mehta", "authors": "Salil Mehta", "title": "Sophisticated gamblers ruin and survival chances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST q-fin.RM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note explores the mathematical theory to solve modern gamblers ruin\nproblems. We establish a ruin framework and solve for the probability of\nbankruptcy. We also show how this relates to the expected time to bankruptcy\nand review the risk neutral probabilities associated an adjustment to\nasymmetrical views.\n", "versions": [{"version": "v1", "created": "Mon, 24 Mar 2014 02:07:38 GMT"}], "update_date": "2014-03-25", "authors_parsed": [["Mehta", "Salil", ""]]}, {"id": "1403.5841", "submitter": "Danang Qoyyimi", "authors": "Danang Teguh Qoyyimi and Ricardas Zitikis", "title": "Measuring the lack of monotonicity in functions", "comments": null, "journal-ref": "Mathematical Scientist. Dec2014, Vol. 39 Issue 2, p107-117. 11p", "doi": null, "report-no": null, "categories": "stat.AP math.FA math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Problems in econometrics, insurance, reliability engineering, and statistics\nquite often rely on the assumption that certain functions are non-decreasing.\nTo satisfy this requirement, researchers frequently model the underlying\nphenomena using parametric and semi-parametric families of functions, thus\neffectively specifying the required shapes of the functions. To tackle these\nproblems in a non-parametric way, in this paper we suggest indices for\nmeasuring the lack of monotonicity in functions. We investigate properties of\nthe indices and also offer a convenient computational technique for practical\nuse.\n", "versions": [{"version": "v1", "created": "Mon, 24 Mar 2014 03:29:06 GMT"}], "update_date": "2015-02-26", "authors_parsed": [["Qoyyimi", "Danang Teguh", ""], ["Zitikis", "Ricardas", ""]]}, {"id": "1403.5877", "submitter": "Anastasios Kyrillidis", "authors": "Anastasios Kyrillidis and Anastasios Zouzias", "title": "Non-uniform Feature Sampling for Decision Tree Ensembles", "comments": "7 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the effectiveness of non-uniform randomized feature selection in\ndecision tree classification. We experimentally evaluate two feature selection\nmethodologies, based on information extracted from the provided dataset: $(i)$\n\\emph{leverage scores-based} and $(ii)$ \\emph{norm-based} feature selection.\nExperimental evaluation of the proposed feature selection techniques indicate\nthat such approaches might be more effective compared to naive uniform feature\nselection and moreover having comparable performance to the random forest\nalgorithm [3]\n", "versions": [{"version": "v1", "created": "Mon, 24 Mar 2014 08:26:19 GMT"}], "update_date": "2014-03-25", "authors_parsed": [["Kyrillidis", "Anastasios", ""], ["Zouzias", "Anastasios", ""]]}, {"id": "1403.5977", "submitter": "Frederic Pascal", "authors": "Yacine Chitour and Romain Couillet and Frederic Pascal", "title": "On the convergence of Maronna's $M$-estimators of scatter", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2014.2367547", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, {we propose an alternative proof for the uniqueness} of\nMaronna's $M$-estimator of scatter (Maronna, 1976) for $N$ vector observations\n$\\mathbf y_1,...,\\mathbf y_N\\in\\mathbb R^m$ under a mild constraint of linear\nindependence of any subset of $m$ of these vectors. This entails in particular\nalmost sure uniqueness for random vectors $\\mathbf y_i$ with a density as long\nas $N>m$. {This approach allows to establish further relations that demonstrate\nthat a properly normalized Tyler's $M$-estimator of scatter (Tyler, 1987) can\nbe considered as a limit of Maronna's $M$-estimator. More precisely, the\ncontribution is to show that each $M$-estimator converges towards a particular\nTyler's $M$-estimator.} These results find important implications in recent\nworks on the large dimensional (random matrix) regime of robust $M$-estimation.\n", "versions": [{"version": "v1", "created": "Mon, 24 Mar 2014 14:50:09 GMT"}, {"version": "v2", "created": "Mon, 22 Sep 2014 16:42:17 GMT"}, {"version": "v3", "created": "Mon, 27 Oct 2014 12:09:46 GMT"}, {"version": "v4", "created": "Tue, 4 Nov 2014 10:36:51 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Chitour", "Yacine", ""], ["Couillet", "Romain", ""], ["Pascal", "Frederic", ""]]}, {"id": "1403.5997", "submitter": "Niko Br\\\"ummer", "authors": "Niko Br\\\"ummer and Albert Swart", "title": "Bayesian calibration for forensic evidence reporting", "comments": "accepted for Interspeech 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a Bayesian solution for the problem in forensic speaker\nrecognition, where there may be very little background material for estimating\nscore calibration parameters. We work within the Bayesian paradigm of evidence\nreporting and develop a principled probabilistic treatment of the problem,\nwhich results in a Bayesian likelihood-ratio as the vehicle for reporting\nweight of evidence. We show in contrast, that reporting a likelihood-ratio\ndistribution does not solve this problem. Our solution is experimentally\nexercised on a simulated forensic scenario, using NIST SRE'12 scores, which\ndemonstrates a clear advantage for the proposed method compared to the\ntraditional plugin calibration recipe.\n", "versions": [{"version": "v1", "created": "Mon, 24 Mar 2014 15:25:59 GMT"}, {"version": "v2", "created": "Tue, 25 Mar 2014 07:27:21 GMT"}, {"version": "v3", "created": "Tue, 10 Jun 2014 08:18:06 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Br\u00fcmmer", "Niko", ""], ["Swart", "Albert", ""]]}, {"id": "1403.6008", "submitter": "Niko Br\\\"ummer", "authors": "Niko Br\\\"ummer and Edward de Villiers", "title": "What is the `relevant population' in Bayesian forensic inference?", "comments": "Technical report, AGNITIO Research, South Africa", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In works discussing the Bayesian paradigm for presenting forensic evidence in\ncourt, the concept of a `relevant population' is often mentioned, without a\nclear definition of what is meant, and without recommendations of how to select\nsuch populations. This note is to try to better understand this concept. Our\nanalysis is intended to be general enough to be applicable to different\nforensic technologies and we shall consider both DNA profiling and speaker\nrecognition as examples.\n", "versions": [{"version": "v1", "created": "Mon, 24 Mar 2014 15:40:41 GMT"}], "update_date": "2014-03-25", "authors_parsed": [["Br\u00fcmmer", "Niko", ""], ["de Villiers", "Edward", ""]]}, {"id": "1403.6453", "submitter": "Seth Zimmerman", "authors": "Seth Zimmerman", "title": "Detecting Deficiencies: An Optimal Group Testing Algorithm", "comments": "Slightly edited 3/22/15", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of group testing to locate all instances of disease in a large\npopulation of blood samples was first considered seventy years ago. Since then,\nseveral methods have been used to approximate the minimum expected number of\ntests. The procedure presented here, in contrast to all previous ones, takes a\nconstructive rather than a top-down approach. As far as could be verified, it\noffers the first proven solution to the problem of finding the minimum expected\nnumber of tests with a predetermined procedure. Computer results indicate that\nthe algorithm has a Fibonacci-based pattern.\n", "versions": [{"version": "v1", "created": "Tue, 25 Mar 2014 19:03:52 GMT"}, {"version": "v2", "created": "Sun, 22 Mar 2015 20:52:13 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Zimmerman", "Seth", ""]]}, {"id": "1403.6623", "submitter": "Florian Frommlet", "authors": "Erich Dolejsi, Bernhard Bodenstorfer, Florian Frommlet", "title": "Analyzing genome-wide association studies with an FDR controlling\n  modification of the Bayesian information criterion", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0103322", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prevailing method of analyzing GWAS data is still to test each marker\nindividually, although from a statistical point of view it is quite obvious\nthat in case of complex traits such single marker tests are not ideal. Recently\nseveral model selection approaches for GWAS have been suggested, most of them\nbased on LASSO-type procedures. Here we will discuss an alternative model\nselection approach which is based on a modification of the Bayesian Information\nCriterion (mBIC2) which was previously shown to have certain asymptotic\noptimality properties in terms of minimizing the misclassification error.\nHeuristic search strategies are introduced which attempt to find the model\nwhich minimizes mBIC2, and which are efficient enough to allow the analysis of\nGWAS data.\n  Our approach is implemented in a software package called MOSGWA. Its\nperformance in case control GWAS is compared with the two algorithms HLASSO and\nGWASelect, as well as with single marker tests, where we performed a simulation\nstudy based on real SNP data from the POPRES sample. Our results show that\nMOSGWA performs slightly better than HLASSO, whereas according to our\nsimulations GWASelect does not control the type I error when used to\nautomatically determine the number of important SNPs. We also reanalyze the\nGWAS data from the Wellcome Trust Case-Control Consortium (WTCCC) and compare\nthe findings of the different procedures.\n", "versions": [{"version": "v1", "created": "Wed, 26 Mar 2014 10:45:44 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Dolejsi", "Erich", ""], ["Bodenstorfer", "Bernhard", ""], ["Frommlet", "Florian", ""]]}, {"id": "1403.7118", "submitter": "Benjamin Hofner", "authors": "Benjamin Hofner and Thomas Kneib and Torsten Hothorn", "title": "A Unified Framework of Constrained Regression", "comments": "This is a preliminary version of the manuscript. The final\n  publication is available at\n  http://link.springer.com/article/10.1007/s11222-014-9520-y", "journal-ref": null, "doi": "10.1007/s11222-014-9520-y", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized additive models (GAMs) play an important role in modeling and\nunderstanding complex relationships in modern applied statistics. They allow\nfor flexible, data-driven estimation of covariate effects. Yet researchers\noften have a priori knowledge of certain effects, which might be monotonic or\nperiodic (cyclic) or should fulfill boundary conditions. We propose a unified\nframework to incorporate these constraints for both univariate and bivariate\neffect estimates and for varying coefficients. As the framework is based on\ncomponent-wise boosting methods, variables can be selected intrinsically, and\neffects can be estimated for a wide range of different distributional\nassumptions. Bootstrap confidence intervals for the effect estimates are\nderived to assess the models. We present three case studies from environmental\nsciences to illustrate the proposed seamless modeling framework. All discussed\nconstrained effect estimates are implemented in the comprehensive R package\nmboost for model-based boosting.\n", "versions": [{"version": "v1", "created": "Thu, 27 Mar 2014 16:28:00 GMT"}, {"version": "v2", "created": "Tue, 30 Sep 2014 18:15:49 GMT"}, {"version": "v3", "created": "Wed, 1 Oct 2014 07:51:05 GMT"}, {"version": "v4", "created": "Fri, 7 Nov 2014 10:18:12 GMT"}], "update_date": "2014-11-10", "authors_parsed": [["Hofner", "Benjamin", ""], ["Kneib", "Thomas", ""], ["Hothorn", "Torsten", ""]]}, {"id": "1403.7274", "submitter": "William Fithian", "authors": "William Fithian and Jane Elith and Trevor Hastie and David A. Keith", "title": "Bias Correction in Species Distribution Models: Pooling Survey and\n  Collection Data for Multiple Species", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Presence-only records may provide data on the distributions of rare species,\nbut commonly suffer from large, unknown biases due to their typically haphazard\ncollection schemes. Presence-absence or count data collected in systematic,\nplanned surveys are more reliable but typically less abundant.\n  We proposed a probabilistic model to allow for joint analysis of\npresence-only and survey data to exploit their complementary strengths. Our\nmethod pools presence-only and presence-absence data for many species and\nmaximizes a joint likelihood, simultaneously estimating and adjusting for the\nsampling bias affecting the presence-only data. By assuming that the sampling\nbias is the same for all species, we can borrow strength across species to\nefficiently estimate the bias and improve our inference from presence-only\ndata.\n  We evaluate our model's performance on data for 36 eucalypt species in\nsoutheastern Australia. We find that presence-only records exhibit a strong\nsampling bias toward the coast and toward Sydney, the largest city. Our\ndata-pooling technique substantially improves the out-of-sample predictive\nperformance of our model when the amount of available presence-absence data for\na given species is scarce.\n  If we have only presence-only data and no presence-absence data for a given\nspecies, but both types of data for several other species that suffer from the\nsame spatial sampling bias, then our method can obtain an unbiased estimate of\nthe first species' geographic range.\n", "versions": [{"version": "v1", "created": "Fri, 28 Mar 2014 03:26:48 GMT"}, {"version": "v2", "created": "Thu, 7 Aug 2014 19:24:17 GMT"}], "update_date": "2014-08-08", "authors_parsed": [["Fithian", "William", ""], ["Elith", "Jane", ""], ["Hastie", "Trevor", ""], ["Keith", "David A.", ""]]}, {"id": "1403.7548", "submitter": "Alexander Wakim", "authors": "Alexander Wakim and Jimmy Jin", "title": "Functional Data Analysis of Aging Curves in Sports", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that athletic and physical condition is affected by age.\nPlotting an individual athlete's performance against age creates a graph\ncommonly called the player's aging curve. Despite the obvious interest to\ncoaches and managers, the analysis of aging curves so far has used fairly\nrudimentary techniques. In this paper, we introduce functional data analysis\n(FDA) to the study of aging curves in sports and argue that it is both more\ngeneral and more flexible compared to the methods that have previously been\nused. We also illustrate the rich analysis that is possible by analyzing data\nfor NBA and MLB players. In the analysis of MLB data, we use functional\nprincipal components analysis (fPCA) to perform functional hypothesis testing\nand show differences in aging curves between potential power hitters and\npotential non-power hitters. The analysis of aging curves in NBA players\nillustrates the use of the PACE method. We show that there are three distinct\naging patterns among NBA players and that player scoring ability differs across\nthe patterns. We also show that aging pattern is independent of position.\n", "versions": [{"version": "v1", "created": "Fri, 28 Mar 2014 21:41:48 GMT"}, {"version": "v2", "created": "Tue, 1 Apr 2014 02:21:14 GMT"}], "update_date": "2014-04-02", "authors_parsed": [["Wakim", "Alexander", ""], ["Jin", "Jimmy", ""]]}, {"id": "1403.7642", "submitter": "Andrew Karl", "authors": "Andrew T. Karl", "title": "The Sensitivity of College Football Rankings to Several Modeling Choices", "comments": null, "journal-ref": "Journal of Quantitative Analysis in Sports (2012), Volume 8, Issue\n  3", "doi": "10.1515/1559-0410.1471", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a multiple-membership generalized linear mixed model for\nranking college football teams using only their win/loss records. The model\nresults in an intractable, high-dimensional integral due to the random effects\nstructure and nonlinear link function. We use recent data sets to explore the\neffect of the choice of integral approximation and other modeling assumptions\non the rankings. Varying the modeling assumptions sometimes leads to changes in\nthe team rankings that could affect bowl assignments.\n", "versions": [{"version": "v1", "created": "Sat, 29 Mar 2014 15:54:39 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Karl", "Andrew T.", ""]]}, {"id": "1403.7644", "submitter": "Andrew Karl", "authors": "Andrew T. Karl, Yan Yang and Sharon L. Lohr", "title": "Efficient Maximum Likelihood Estimation of Multiple Membership Linear\n  Mixed Models, with an Application to Educational Value-Added Assessments", "comments": null, "journal-ref": "Computational Statistics & Data Analysis, Volume 59, March 2013,\n  Pages 13-27", "doi": "10.1016/j.csda.2012.10.004", "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generalized persistence (GP) model, developed in the context of\nestimating ``value added'' by individual teachers to their students' current\nand future test scores, is one of the most flexible value-added models in the\nliterature. Although developed in the educational setting, the GP model can\npotentially be applied to any structure where each sequential response of a\nlower-level unit may be associated with a different higher-level unit, and the\neffects of the higher-level units may persist over time. The flexibility of the\nGP model, however, and its multiple membership random effects structure lead to\ncomputational challenges that have limited the model's availability. We develop\nan EM algorithm to compute maximum likelihood estimates efficiently for the GP\nmodel, making use of the sparse structure of the random effects and error\ncovariance matrices. The algorithm is implemented in the package GPvam in R\nstatistical software. We give examples of the computations and illustrate the\ngains in computational efficiency achieved by our estimation procedure.\n", "versions": [{"version": "v1", "created": "Sat, 29 Mar 2014 16:22:59 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Karl", "Andrew T.", ""], ["Yang", "Yan", ""], ["Lohr", "Sharon L.", ""]]}, {"id": "1403.7971", "submitter": "Dominique Haughton", "authors": "Dominique Haughton (SAMM, GREMAQ), Guangying Hua, Danny Jin, John Lin,\n  Qizhi Wei, Changan Zhang", "title": "Optimization of the marketing mix in the health care industry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes data mining techniques to model the return on investment\nfrom various types of promotional spending to market a drug and then uses the\nmodel to draw conclusions on how the pharmaceutical industry might go about\nallocating marketing expenditures in a more efficient manner, potentially\nreducing costs to the consumer\n", "versions": [{"version": "v1", "created": "Mon, 31 Mar 2014 12:35:47 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Haughton", "Dominique", "", "SAMM, GREMAQ"], ["Hua", "Guangying", ""], ["Jin", "Danny", ""], ["Lin", "John", ""], ["Wei", "Qizhi", ""], ["Zhang", "Changan", ""]]}, {"id": "1403.7972", "submitter": "Dominique Haughton", "authors": "Dominique Haughton (SAMM, GREMAQ), Guangying Hua, Danny Jin, John Lin,\n  Qizhi Wei, Changan Zhang", "title": "Imputing unknown competitor marketing activity with a Hidden Markov\n  Chain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate on a case study with two competing products at a bank how one\ncan use a Hidden Markov Chain (HMC) to estimate missing information on a\ncompetitor's marketing activity. The idea is that given time series with sales\nvolumes for products A and B and marketing expenditures for product A, as well\nas suitable predictors of sales for products A and B, we can infer at each\npoint in time whether it is likely or not that marketing activities took place\nfor product B. The method is successful in identifying the presence or absence\nof marketing activity for product B about 84% of the time. We allude to the\nissue of whether, if one can infer marketing activity about product B from\nknowledge of marketing activity for product A and of sales volumes of both\nproducts, the reverse might be possible and one might be able to impute\nmarketing activity for product A from knowledge of that of product B. This\nleads to a concept of symmetric imputation of competing marketing activity. The\nexposition in this paper aims to be accessible and relevant to practitioners.\n", "versions": [{"version": "v1", "created": "Mon, 31 Mar 2014 12:36:27 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Haughton", "Dominique", "", "SAMM, GREMAQ"], ["Hua", "Guangying", ""], ["Jin", "Danny", ""], ["Lin", "John", ""], ["Wei", "Qizhi", ""], ["Zhang", "Changan", ""]]}, {"id": "1403.8039", "submitter": "Sachin Malik", "authors": "Rajesh Singh and Sachin Malik", "title": "A New Estimator For Population Mean Using Two Auxiliary Variables in\n  Stratified random Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we suggest an estimator using two auxiliary variables in\nstratified random sampling. The propose estimator has an improvement over mean\nper unit estimator as well as some other considered estimators. Expressions for\nbias and MSE of the estimator are derived up to first degree of approximation.\nMoreover, these theoretical findings are supported by a numerical example with\noriginal data. Key words: Study variable, auxiliary variable, stratified random\nsampling, bias and mean squared error.\n", "versions": [{"version": "v1", "created": "Mon, 31 Mar 2014 15:10:01 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Singh", "Rajesh", ""], ["Malik", "Sachin", ""]]}]