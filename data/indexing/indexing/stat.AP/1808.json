[{"id": "1808.00003", "submitter": "Ashot A. Akopian", "authors": "A.A. Akopian", "title": "Review on the astronomical estimators of number of flare stars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP astro-ph.SR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Review is devoted to estimators (introduced in astronomy by famous astronomer\nAmbartsumian and his followers) that are used to estimating the unknown number\nof flare stars and other randomly flashing objects (stars with X-ray flares,\nsolar type stars with superflares). Some important astronomical applications of\nthem are presented. The development of these methods in astronomy have\nproceeded regardless of development of analogous methods of mathematical\nstatistics. Brief history of this development and parallels with similar\nstatistical methods is presented.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 08:03:59 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2019 15:35:57 GMT"}, {"version": "v3", "created": "Tue, 5 Feb 2019 08:59:11 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Akopian", "A. A.", ""]]}, {"id": "1808.00036", "submitter": "Seyed Mostafa Kia", "authors": "Seyed Mostafa Kia, Christian F. Beckmann, Andre F. Marquand", "title": "Scalable Multi-Task Gaussian Process Tensor Regression for Normative\n  Modeling of Structured Variation in Neuroimaging Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most brain disorders are very heterogeneous in terms of their underlying\nbiology and developing analysis methods to model such heterogeneity is a major\nchallenge. A promising approach is to use probabilistic regression methods to\nestimate normative models of brain function using (f)MRI data then use these to\nmap variation across individuals in clinical populations (e.g., via anomaly\ndetection). To fully capture individual differences, it is crucial to\nstatistically model the patterns of correlation across different brain regions\nand individuals. However, this is very challenging for neuroimaging data\nbecause of high-dimensionality and highly structured patterns of correlation\nacross multiple axes. Here, we propose a general and flexible multi-task\nlearning framework to address this problem. Our model uses a tensor-variate\nGaussian process in a Bayesian mixed-effects model and makes use of Kronecker\nalgebra and a low-rank approximation to scale efficiently to multi-way\nneuroimaging data at the whole brain level. On a publicly available clinical\nfMRI dataset, we show that our computationally affordable approach\nsubstantially improves detection sensitivity over both a mass-univariate\nnormative model and a classifier that --unlike our approach-- has full access\nto the clinical labels.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 19:26:59 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 16:30:17 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Kia", "Seyed Mostafa", ""], ["Beckmann", "Christian F.", ""], ["Marquand", "Andre F.", ""]]}, {"id": "1808.00114", "submitter": "Min Liu", "authors": "Nanyu Chen, Min Liu, Ya Xu", "title": "Automatic Detection and Diagnosis of Biased Online Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have seen a massive growth of online experiments at LinkedIn, and in\nindustry at large. It is now more important than ever to create an intelligent\nA/B platform that can truly democratize A/B testing by allowing everyone to\nmake quality decisions, regardless of their skillset. With the tremendous\nknowledge base created around experimentation, we are able to mine through\nhistorical data, and discover the most common causes for biased experiments. In\nthis paper, we share four of such common causes, and how we build into our A/B\ntesting platform the automatic detection and diagnosis of such root causes.\nThese root causes range from design-imposed bias, self-selection bias, novelty\neffect and trigger-day effect. We will discuss in detail what each bias is and\nthe scalable algorithm we developed to detect the bias. Surfacing up the\nexistence and root cause of bias automatically for every experiment is an\nimportant milestone towards intelligent A/B testing.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 23:50:57 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Chen", "Nanyu", ""], ["Liu", "Min", ""], ["Xu", "Ya", ""]]}, {"id": "1808.00142", "submitter": "John Malik", "authors": "John Malik, Yu-Lun Lo, Hau-tieng Wu", "title": "Sleep-wake classification via quantifying heart rate variability by\n  convolutional neural network", "comments": null, "journal-ref": null, "doi": "10.1088/1361-6579/aad5a9", "report-no": null, "categories": "stat.AP physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fluctuations in heart rate are intimately tied to changes in the\nphysiological state of the organism. We examine and exploit this relationship\nby classifying a human subject's wake/sleep status using his instantaneous\nheart rate (IHR) series. We use a convolutional neural network (CNN) to build\nfeatures from the IHR series extracted from a whole-night electrocardiogram\n(ECG) and predict every 30 seconds whether the subject is awake or asleep. Our\ntraining database consists of 56 normal subjects, and we consider three\ndifferent databases for validation; one is private, and two are public with\ndifferent races and apnea severities. On our private database of 27 subjects,\nour accuracy, sensitivity, specificity, and AUC values for predicting the wake\nstage are 83.1%, 52.4%, 89.4%, and 0.83, respectively. Validation performance\nis similar on our two public databases. When we use the photoplethysmography\ninstead of the ECG to obtain the IHR series, the performance is also\ncomparable. A robustness check is carried out to confirm the obtained\nperformance statistics. This result advocates for an effective and scalable\nmethod for recognizing changes in physiological state using non-invasive heart\nrate monitoring. The CNN model adaptively quantifies IHR fluctuation as well as\nits location in time and is suitable for differentiating between the wake and\nsleep stages.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 02:28:59 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Malik", "John", ""], ["Lo", "Yu-Lun", ""], ["Wu", "Hau-tieng", ""]]}, {"id": "1808.00382", "submitter": "Allen Riddell", "authors": "Allen Riddell, Michael Betancourt", "title": "Reassembling the English novel, 1789-1919", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.SI stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The absence of an exhaustive bibliography of novels published in the British\nIsles and Ireland during the 19th century blocks several lines of research in\nsociologically-inclined literary history and book history. Without a detailed\naccount of novelistic production, it is difficult to characterize, for example,\nthe population of individuals who pursued careers as novelists. This paper\ncontributes to efforts to develop such an account by estimating yearly rates of\nnew novel publication in the British Isles and Ireland between 1789 and 1919.\nThis period witnessed, in aggregate, the publication of between 40,000 and\n63,000 previously unpublished novels. The number of new novels published each\nyear counts as essential information for researchers interested in\nunderstanding the development of the text industry between 1789 and 1919.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 15:41:51 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 17:10:02 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Riddell", "Allen", ""], ["Betancourt", "Michael", ""]]}, {"id": "1808.00436", "submitter": "Gianluca Mastrantonio", "authors": "Gianluca Mastrantonio, Clara Grazian, Sara Mancinelli, Enrico Bibbona", "title": "New formulation of the Logistic-Gaussian process to analyze trajectory\n  tracking data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improved communication systems, shrinking battery sizes and the price drop of\ntracking devices have led to an increasing availability of trajectory tracking\ndata. These data are often analyzed to understand animal behavior.\n  In this work, we propose a new model for interpreting the animal movent as a\nmixture of characteristic patterns, that we interpret as different behaviors.\nThe probability that the animal is behaving according to a specific pattern, at\neach time instant, is non-parametrically estimated using the Logistic-Gaussian\nprocess. Owing to a new formalization and the way we specify the\ncoregionalization matrix of the associated multivariate Gaussian process, our\nmodel is invariant with respect to the choice of the reference element and of\nthe ordering of the probability vector components. We fit the model under a\nBayesian framework, and show that the Markov chain Monte Carlo algorithm we\npropose is straightforward to implement.\n  We perform a simulation study with the aim of showing the ability of the\nestimation procedure to retrieve the model parameters. We also test the\nperformance of the information criterion we used to select the number of\nbehaviors. The model is then applied to a real dataset where a wolf has been\nobserved before and after procreation. The results are easy to interpret, and\nclear differences emerge in the two phases.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 17:33:52 GMT"}, {"version": "v2", "created": "Sat, 18 Aug 2018 21:30:14 GMT"}, {"version": "v3", "created": "Wed, 11 Sep 2019 10:51:48 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Mastrantonio", "Gianluca", ""], ["Grazian", "Clara", ""], ["Mancinelli", "Sara", ""], ["Bibbona", "Enrico", ""]]}, {"id": "1808.00615", "submitter": "Archie Chapman", "authors": "Thomas Power, Gregor Verbi\\v{c}, Archie C. Chapman", "title": "A Nonparametric Bayesian Methodology for Synthesizing Residential Solar\n  Generation and Demand Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The uptake of behind-the-meter distributed energy resources in low-voltage\ndistribution networks has reached a level where network issues have started to\nemerge, which requires new tools for operation and planning. In this paper, we\npropose a methodology for synthesizing stochastic demand and generation\nprofiles for unobserved customers with rooftop PV, called prosumers. The\nproposed model bridges the gap between the limited available empirical data,\nand the large amount of high-quality, stochastic demand and generation data\nrequired for probabilistic analysis. The approach employs clustering analysis\nand a Dirichlet-categorical hierarchical model of the features of unobserved\nprosumers. Based on the data of clusters of prosumers, Markov chain models of\ndemand and generation profiles are constructed from empirical data, and\nsynthetic demand profiles are subsequently sampled from these. The sampled\ntraces are cross-validated and show a good statistical fit to the observed\ndata. Two case studies are considered to confirm the validity of the proposed\nmethodology. The first studies the impact of behavioral differences on the\nsynthetic demand profiles, while the second looks at the impact of varying\nsolar generation penetration on demand profiles.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 01:09:24 GMT"}, {"version": "v2", "created": "Tue, 18 Dec 2018 05:23:25 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Power", "Thomas", ""], ["Verbi\u010d", "Gregor", ""], ["Chapman", "Archie C.", ""]]}, {"id": "1808.00958", "submitter": "Bruno Tuffin", "authors": "Ahmed Kamoun (IMT Atlantique), Patrick Maill\\'e (RSM), Bruno Tuffin\n  (DIONYSOS)", "title": "Evaluating search engines and defining a consensus implementation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different search engines provide different outputs for the same keyword. This\nmay be due to different definitions of relevance, and/or to different\nknowledge/anticipation of users' preferences, but rankings are also suspected\nto be biased towards own content, which may prejudicial to other content\nproviders. In this paper, we make some initial steps toward a rigorous\ncomparison and analysis of search engines, by proposing a definition for a\nconsensual relevance of a page with respect to a keyword, from a set of search\nengines. More specifically, we look at the results of several search engines\nfor a sample of keywords, and define for each keyword the visibility of a page\nbased on its ranking over all search engines. This allows to define a score of\nthe search engine for a keyword, and then its average score over all keywords.\nBased on the pages visibility, we can also define the consensus search engine\nas the one showing the most visible results for each keyword. We have\nimplemented this model and present an analysis of the results.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 11:28:28 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Kamoun", "Ahmed", "", "IMT Atlantique"], ["Maill\u00e9", "Patrick", "", "RSM"], ["Tuffin", "Bruno", "", "DIONYSOS"]]}, {"id": "1808.01384", "submitter": "Pantelis - Zenon Hadjipantelis", "authors": "Pantelis Z. Hadjipantelis, Kyunghee Han, Jane-Ling Wang, Seungmi Yang,\n  Richard M. Martin, Michael S. Kramer, Emily Oken, Hans-Georg M\\\"uller", "title": "Associating Growth in Infancy and Cognitive Performance in Early\n  Childhood: A functional data analysis approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physical growth traits can be naturally represented by continuous functions.\nIn a large dataset of infancy growth patterns, we develop a practical approach\nto infer statistical associations between growth-trajectories and IQ\nperformance in early childhood. The main objective of this study is to show how\nto assess physical growth curves and detect if particular infancy growth\npatterns are associated with differences in IQ (Full-scale WASI scores) in\nlater ages using a semi-parametric functional response model. Additionally, we\ninvestigate the association between different growth measurements in terms of\ntheir cross-correlation with each other, their correlation with later IQ, as\nwell as their time-varying dynamics. This analysis framework can easily\nincorporate or learn population information in a non-parametric way, rendering\nthe existence of prior population charts partially redundant.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 23:00:35 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Hadjipantelis", "Pantelis Z.", ""], ["Han", "Kyunghee", ""], ["Wang", "Jane-Ling", ""], ["Yang", "Seungmi", ""], ["Martin", "Richard M.", ""], ["Kramer", "Michael S.", ""], ["Oken", "Emily", ""], ["M\u00fcller", "Hans-Georg", ""]]}, {"id": "1808.01514", "submitter": "Jonathan Auerbach", "authors": "Jonathan Auerbach and Phyllis Wan", "title": "Forecasting the Urban Skyline with Extreme Value Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The world's urban population is expected to grow fifty percent by the year\n2050 and exceed six billion. The major challenges confronting cities, such as\nsustainability, safety, and equality, will depend on the infrastructure\ndeveloped to accommodate the increase. Urban planners have long debated the\nconsequences of vertical expansion---the concentration of residents by\nconstructing tall buildings---over horizontal expansion---the dispersal of\nresidents by extending urban boundaries. Yet relatively little work has\npredicted the vertical expansion of cities and quantified the likelihood and\ntherefore urgency of these consequences.\n  We regard tall buildings as random exceedances over a threshold and use\nextreme value theory to forecast the skyscrapers that will dominate the urban\nskyline in 2050 if present trends continue. We predict forty-one thousand\nskyscrapers will surpass 150 meters and 40 floors, an increase of eight percent\na year, far outpacing the expected urban population growth of two percent a\nyear. The typical tall skyscraper will not be noticeably taller, and the\ntallest will likely exceed one thousand meters but not one mile. If a mile-high\nskyscraper is constructed, it will hold fewer occupants than many of the\nmile-highs currently designed. We predict roughly three-quarters the number of\nfloors of the Mile-High Tower, two-thirds of Next Tokyo's Sky Mile Tower, and\nhalf the floors of Frank Lloyd Wright's The Illinois---three prominent plans\nfor a mile-high skyscraper. However, the relationship between floor and height\nwill vary across cities.\n", "versions": [{"version": "v1", "created": "Sat, 4 Aug 2018 17:49:49 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 17:26:40 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Auerbach", "Jonathan", ""], ["Wan", "Phyllis", ""]]}, {"id": "1808.01693", "submitter": "Francesca Matano", "authors": "Francesca Matano and Val\\'erie Ventura", "title": "Computationally efficient model selection for joint spikes and waveforms\n  decoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent paradigm for decoding behavioral variables or stimuli from neuron\nensembles relies on joint models for electrode spike trains and their\nwaveforms, which, in principle, is more efficient than decoding from electrode\nspike trains alone or from sorted neuron spike trains. In this paper, we decode\nthe velocity of arm reaches of a rhesus macaque monkey to show that including\nwaveform features indiscriminately in a joint decoding model can contribute\nmore noise and bias than useful information about the kinematics, and thus\ndegrade decoding performance. We also show that selecting which waveform\nfeatures should enter the model to lower the prediction risk can boost decoding\nperformance substantially. For the data analyzed here, a stepwise search for a\nlow risk electrode spikes and waveforms joint model yielded a low risk Bayesian\nmodel that is 30% more efficient than the corresponding risk minimized Bayesian\nmodel based on electrode spike trains alone. The joint model was also\ncomparably efficient to decoding from a risk minimized model based only on\nsorted neuron spike trains and hash, confirming previous results that one can\ndo away with the problematic spike sorting step in decoding applications. We\nwere able to search for low risk joint models through a large model space\nthanks to a short cut formula, which accelerates large matrix inversions in\nstepwise searches for models based on Gaussian linear observation equations.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 22:17:57 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Matano", "Francesca", ""], ["Ventura", "Val\u00e9rie", ""]]}, {"id": "1808.01770", "submitter": "Vivien Goepp", "authors": "Vivien Goepp (MAP5 - UMR 8145), Olivier Bouaziz (MAP5 - UMR 8145),\n  Gr\\'egory Nuel (LPSM UMR 8001)", "title": "Spline Regression with Automatic Knot Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a new method for automatically selecting knots in\nspline regression. The approach consists in setting a large number of initial\nknots and fitting the spline regression through a penalized likelihood\nprocedure called adaptive ridge. The proposed method is similar to penalized\nspline regression methods (e.g. P-splines), with the noticeable difference that\nthe output is a sparse spline regression with a small number of knots. We show\nthat our method called A-spline, for adaptive splines yields sparse regression\nmodels with high interpretability, while having similar predictive performance\nsimilar to penalized spline regression methods. A-spline is applied both to\nsimulated and real dataset. A fast and publicly available implementation in R\nis provided along with this paper.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 08:33:57 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Goepp", "Vivien", "", "MAP5 - UMR 8145"], ["Bouaziz", "Olivier", "", "MAP5 - UMR 8145"], ["Nuel", "Gr\u00e9gory", "", "LPSM UMR 8001"]]}, {"id": "1808.02190", "submitter": "Yikai Wang", "authors": "Yikai Wang, Xuefei Hu, Howard Chang, Lance Waller, Jessica Belle, Yang\n  Liu", "title": "A Bayesian Downscaler Model to Estimate Daily PM2.5 levels in the\n  Continental US", "comments": "14 pages, 6 figures", "journal-ref": "Int. J. Environ. Res. Public Health 2018, 15(9), 1999", "doi": "10.3390/ijerph15091999", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been growing interest in extending the coverage of ground PM2.5\nmonitoring networks based on satellite remote sensing data. With broad spatial\nand temporal coverage, satellite based monitoring network has a strong\npotential to complement the ground monitor system in terms of the\nspatial-temporal availability of the air quality data. However, most existing\ncalibration models focused on a relatively small spatial domain and cannot be\ngeneralized to national-wise study. In this paper, we proposed a statistically\nreliable and interpretable national modeling framework based on Bayesian\ndownscaling methods with the application to the calibration of the daily ground\nPM2.5 concentrations across the Continental U.S. using satellite-retrieved\naerosol optical depth (AOD) and other ancillary predictors in 2011. Our\napproach flexibly models the PM2.5 versus AOD and the potential related\ngeographical factors varying across the climate regions and yields spatial and\ntemporal specific parameters to enhance the model interpretability. Moreover,\nour model accurately predicted the national PM2.5 with a R2 at 70% and\ngenerates reliable annual and seasonal PM2.5 concentration maps with its SD.\nOverall, this modeling framework can be applied to the national scale PM2.5\nexposure assessments and also quantify the prediction errors.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 03:08:25 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Wang", "Yikai", ""], ["Hu", "Xuefei", ""], ["Chang", "Howard", ""], ["Waller", "Lance", ""], ["Belle", "Jessica", ""], ["Liu", "Yang", ""]]}, {"id": "1808.02195", "submitter": "Milad Rafiee Vahid", "authors": "Milad R. Vahid, Bernard Hanzon, Raimund J. Ober", "title": "Fisher information matrix for single molecules with stochastic\n  trajectories", "comments": null, "journal-ref": "SIAM Journal on Imaging Sciences, 2020, Vol. 13, No. 1 : pp.\n  234-264", "doi": "10.1137/19M1242562", "report-no": null, "categories": "q-bio.QM physics.bio-ph stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tracking of objects in cellular environments has become a vital tool in\nmolecular cell biology. A particularly important example is single molecule\ntracking which enables the study of the motion of a molecule in cellular\nenvironments and provides quantitative information on the behavior of\nindividual molecules in cellular environments, which were not available before\nthrough bulk studies. Here, we consider a dynamical system where the motion of\nan object is modeled by stochastic differential equations (SDEs), and\nmeasurements are the detected photons emitted by the moving fluorescently\nlabeled object, which occur at discrete time points, corresponding to the\narrival times of a Poisson process, in contrast to uniform time points which\nhave been commonly used in similar dynamical systems. The measurements are\ndistributed according to optical diffraction theory, and therefore, they would\nbe modeled by different distributions, e.g., a Born and Wolf profile for an\nout-of-focus molecule. For some special circumstances, Gaussian image models\nhave been proposed. In this paper, we introduce a stochastic framework in which\nwe calculate the maximum likelihood estimates of the biophysical parameters of\nthe molecular interactions, e.g., diffusion and drift coefficients. More\nimportantly, we develop a general framework to calculate the Cram\\'er-Rao lower\nbound (CRLB), given by the inverse of the Fisher information matrix, for the\nestimation of unknown parameters and use it as a benchmark in the evaluation of\nthe standard deviation of the estimates. There exists no established method,\neven for Gaussian measurements, to systematically calculate the CRLB for the\ngeneral motion model that we consider in this paper. We apply the developed\nmethodology to simulated data of a molecule with linear trajectories and show\nthat the standard deviation of the estimates matches well with the square root\nof the CRLB.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 03:32:16 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 07:50:16 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Vahid", "Milad R.", ""], ["Hanzon", "Bernard", ""], ["Ober", "Raimund J.", ""]]}, {"id": "1808.02214", "submitter": "Yiying Zhang", "authors": "Xiaoyu Zhang and Yiying Zhang and Rui Fang", "title": "Allocations of Cold Standbys to Series and Parallel Systems with\n  Dependent Components", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of industrial engineering, cold-standby redundancies\nallocation strategy is usually adopted to improve the reliability of coherent\nsystems. This paper investigates optimal allocation strategies of cold standbys\nfor series and parallel systems comprised of dependent components with\nleft/right tail weakly stochastic arrangement increasing lifetimes. For the\ncase of heterogeneous and independent matched cold standbys, it is proved that\nbetter redundancies should be put in the nodes having weaker [better]\ncomponents for series [parallel] systems. For the case of homogeneous and\nindependent cold standbys, it is shown that more redundancies should be put in\nstandby with weaker [better] components to enhance the reliability of series\n[parallel] systems. The results developed here generalize and extend those\ncorresponding ones in the literature to the case of series and parallel systems\nwith dependent components. Numerical examples are also presented to provide\nguidance for the practical use of our theoretical findings.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 05:16:52 GMT"}, {"version": "v2", "created": "Sun, 24 Mar 2019 02:02:29 GMT"}, {"version": "v3", "created": "Tue, 27 Aug 2019 03:02:43 GMT"}, {"version": "v4", "created": "Fri, 22 Nov 2019 01:20:18 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Zhang", "Xiaoyu", ""], ["Zhang", "Yiying", ""], ["Fang", "Rui", ""]]}, {"id": "1808.02403", "submitter": "Kun Chen", "authors": "Zhe Sun, Wanli Xu, Xiaomei Cong, Gen Li and Kun Chen", "title": "Log-Contrast Regression with Functional Compositional Predictors:\n  Linking Preterm Infant's Gut Microbiome Trajectories to Neurobehavioral\n  Outcome", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The neonatal intensive care unit (NICU) experience is known to be one of the\nmost crucial factors that drive preterm infant's neurodevelopmental and health\noutcomes. It is hypothesized that stressful early life experience of very\npreterm neonate is imprinting gut microbiome by the regulation of the so-called\nbrain-gut axis, and consequently, certain microbiome markers are predictive of\nlater infant neurodevelopment. To investigate, a preterm infant study was\nconducted; infant fecal samples were collected during the infants' first month\nof postnatal age, resulting in functional compositional microbiome data, and\nneurobehavioral outcomes were measured when infants reached 36-38 weeks of\npost-menstrual age. To identify potential microbiome markers and estimate how\nthe trajectories of gut microbiome compositions during early postnatal stage\nimpact later neurobehavioral outcomes of the preterm infants, we innovate a\nsparse log-contrast regression with functional compositional predictors. The\nfunctional simplex structure is strictly preserved, and the functional\ncompositional predictors are allowed to have sparse, smoothly varying, and\naccumulating effects on the outcome through time. Through a pragmatic basis\nexpansion step, the problem boils down to a linearly constrained sparse group\nregression, for which we develop an efficient algorithm and obtain theoretical\nperformance guarantees. Our approach yields insightful results in the preterm\ninfant study. The identified microbiome markers and the estimated time dynamics\nof their impact on the neurobehavioral outcome shed light on the linkage\nbetween stress accumulation in early postnatal stage and neurodevelopmental\nprocess of infants.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 14:39:24 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 02:13:52 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Sun", "Zhe", ""], ["Xu", "Wanli", ""], ["Cong", "Xiaomei", ""], ["Li", "Gen", ""], ["Chen", "Kun", ""]]}, {"id": "1808.02528", "submitter": "Adam Sales", "authors": "Adam C Sales and John F Pane", "title": "Student Log-Data from a Randomized Evaluation of Educational Technology:\n  A Causal Case Study", "comments": "Forthcoming, Journal of Research in Educational Effectiveness", "journal-ref": "Adam C. Sales & John F. Pane (2021) Student Log-Data from a\n  Randomized Evaluation of Educational Technology: A Causal Case Study, Journal\n  of Research on Educational Effectiveness, 14:1, 241-269, DOI:\n  10.1080/19345747.2020.1823538", "doi": "10.1080/19345747.2020.1823538", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Randomized evaluations of educational technology produce log data as a\nbi-product: highly granular data student and teacher usage. These datasets\ncould shed light on causal mechanisms, effect heterogeneity, or optimal use.\nHowever, there are methodological challenges: implementation is not randomized\nand is only defined for the treatment group, and log datasets have a complex\nstructure. This paper discusses three approaches to help surmount these issues.\nOne approach uses data from the treatment group to estimate the effect of usage\non outcomes in an observational study. Another, causal mediation analysis,\nestimates the role of usage in driving the overall effect. Finally, principal\nstratification estimates overall effects for groups of students with the same\n\"potential\" usage. We analyze hint data from an evaluation of the Cognitive\nTutor Algebra I curriculum using these three approaches, with possibly\nconflicting results: the observational study and mediation analysis suggest\nthat hints reduce posttest scores, while principal stratification finds that\ntreatment effects may be correlated with higher rates of hint requests. We\ndiscuss these mixed conclusions and give broader methodological\nrecommendations.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 19:22:30 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 19:03:04 GMT"}, {"version": "v3", "created": "Wed, 9 Sep 2020 20:58:01 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Sales", "Adam C", ""], ["Pane", "John F", ""]]}, {"id": "1808.02707", "submitter": "Italo Romani De Oliveira", "authors": "\\'Italo Romani de Oliveira, Jeffery Musiak", "title": "A Method for Estimating the Probability of Extremely Rare Accidents in\n  Complex Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CE cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the probability of failures or accidents with aerospace systems is\noften necessary when new concepts or designs are introduced, as it is being\ndone for Autonomous Aircraft. If the design is safe, as it is supposed to be,\naccident cases are hard to find. Such analysis needs some variance reduction\ntechnique and several algorithms exist for that, however specific model\nfeatures may cause difficulties in practice, such as the case of system models\nwhere independent agents have to autonomously accomplish missions within finite\ntime, and likely with the presence of human agents. For handling these\nscenarios, this paper presents a novel estimation approach, based on the\ncombination of the well-established variation reduction technique of\nInteracting Particles System (IPS) with the long-standing optimization\nalgorithm denominated DIviding RECTangles (DIRECT). When combined, these two\ntechniques yield statistically significant results for extremely low\nprobabilities. In addition, this novel approach allows the identification of\nintermediate events and simplifies the evaluation of sensitivity of the\nestimated probabilities to certain system parameters.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 10:11:59 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["de Oliveira", "\u00cdtalo Romani", ""], ["Musiak", "Jeffery", ""]]}, {"id": "1808.02763", "submitter": "Rohitash Chandra", "authors": "Jodie Pall, Rohitash Chandra, Danial Azam, Tristan Salles, Jody M.\n  Webster, Richard Scalzo, and Sally Cripps", "title": "Bayesreef: A Bayesian inference framework for modelling reef growth in\n  response to environmental change and biological dynamics", "comments": null, "journal-ref": "Environmental Modelling and Software, 2020", "doi": null, "report-no": null, "categories": "stat.AP cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the impact of environmental processes on vertical reef development\nin geological time is a very challenging task. pyReef-Core is a deterministic\ncarbonate stratigraphic forward model designed to simulate the key biological\nand environmental processes that determine vertical reef accretion and\nassemblage changes in fossil reef drill cores. We present a Bayesian framework\ncalled Bayesreef for the estimation and uncertainty quantification of\nparameters in pyReef-Core that represent environmental conditions affecting the\ngrowth of coral assemblages on geological timescales. We demonstrate the\nexistence of multimodal posterior distributions and investigate the challenges\nof sampling using Markov chain Monte-Carlo (MCMC) methods, which includes\nparallel tempering MCMC. We use synthetic reef-core to investigate fundamental\nissues and then apply the methodology to a selected reef-core from the Great\nBarrier Reef in Australia. The results show that Bayesreef accurately estimates\nand provides uncertainty quantification of the selected parameters that\nrepresent the environment and ecological conditions in pyReef-Core. Bayesreef\nprovides insights into the complex posterior distributions of parameters in\npyReef-Core, which provides the groundwork for future research in this area.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 09:39:15 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 09:28:22 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Pall", "Jodie", ""], ["Chandra", "Rohitash", ""], ["Azam", "Danial", ""], ["Salles", "Tristan", ""], ["Webster", "Jody M.", ""], ["Scalzo", "Richard", ""], ["Cripps", "Sally", ""]]}, {"id": "1808.02766", "submitter": "Daniele Ramazzotti", "authors": "Chris Sauer and Jinghui Dong and Leo Celi and Daniele Ramazzotti", "title": "Improved survival of cancer patients admitted to the ICU between 2002\n  and 2011 at a U.S. teaching hospital", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decades, both critical care and cancer care have improved\nsubstantially. Due to increased cancer-specific survival, we hypothesized that\nboth the number of cancer patients admitted to the ICU and overall survival\nhave increased since the millennium change. MIMIC-III, a freely accessible\ncritical care database of Beth Israel Deaconess Medical Center, Boston, USA was\nused to retrospectively study trends and outcomes of cancer patients admitted\nto the ICU between 2002 and 2011. Multiple logistic regression analysis was\nperformed to adjust for confounders of 28-day and 1-year mortality.\n  Out of 41,468 unique ICU admissions, 1,100 hemato-oncologic, 3,953 oncologic\nand 49 patients with both a hematological and solid malignancy were analyzed.\nHematological patients had higher critical illness scores than non-cancer\npatients, while oncologic patients had similar APACHE-III and SOFA-scores\ncompared to non-cancer patients. In the univariate analysis, cancer was\nstrongly associated with mortality (OR= 2.74, 95%CI: 2.56, 2.94). Over the\n10-year study period, 28-day mortality of cancer patients decreased by 30%.\nThis trend persisted after adjustment for covariates, with cancer patients\nhaving significantly higher mortality (OR=2.63, 95%CI: 2.38, 2.88). Between\n2002 and 2011, both the adjusted odds of 28-day mortality and the adjusted odds\nof 1-year mortality for cancer patients decreased by 6% (95%CI: 4%, 9%). Having\ncancer was the strongest single predictor of 1-year mortality in the\nmultivariate model (OR=4.47, 95%CI: 4.11, 4.84).\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 22:15:20 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Sauer", "Chris", ""], ["Dong", "Jinghui", ""], ["Celi", "Leo", ""], ["Ramazzotti", "Daniele", ""]]}, {"id": "1808.02848", "submitter": "Thomas Peron", "authors": "Thomas Peron, Francisco A. Rodrigues, Luciano da F. Costa", "title": "Pattern Recognition Approach to Violin Shapes of MIMO database", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the landmarks established by the Cremonese school in the 16th century,\nthe history of violin design has been marked by experimentation. While great\neffort has been invested since the early 19th century by the scientific\ncommunity on researching violin acoustics, substantially less attention has\nbeen given to the statistical characterization of how the violin shape evolved\nover time. In this paper we study the morphology of violins retrieved from the\nMusical Instrument Museums Online (MIMO) database -- the largest freely\naccessible platform providing information about instruments held in public\nmuseums. From the violin images, we derive a set of measurements that reflect\nrelevant geometrical features of the instruments. The application of Principal\nComponent Analysis (PCA) uncovered similarities between violin makers and their\nrespective copyists, as well as among luthiers belonging to the same family\nlineage, in the context of historical narrative. Combined with a time-windowed\napproach, thin plate splines visualizations revealed that the average violin\noutline has remained mostly stable over time, not adhering to any particular\ntrends of design across different periods in music history.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 16:21:08 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Peron", "Thomas", ""], ["Rodrigues", "Francisco A.", ""], ["Costa", "Luciano da F.", ""]]}, {"id": "1808.03109", "submitter": "Bettina Drepper", "authors": "Otilia Boldea, Bettina Drepper and Zhuojiong Gan", "title": "Change Point Estimation in Panel Data with Time-Varying Individual\n  Effects", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a method for estimating multiple change points in panel\ndata models with unobserved individual effects via ordinary least-squares\n(OLS). Typically, in this setting, the OLS slope estimators are inconsistent\ndue to the unobserved individual effects bias. As a consequence, existing\nmethods remove the individual effects before change point estimation through\ndata transformations such as first-differencing. We prove that under reasonable\nassumptions, the unobserved individual effects bias has no impact on the\nconsistent estimation of change points. Our simulations show that since our\nmethod does not remove any variation in the dataset before change point\nestimation, it performs better in small samples compared to first-differencing\nmethods. We focus on short panels because they are commonly used in practice,\nand allow for the unobserved individual effects to vary over time. Our method\nis illustrated via two applications: the environmental Kuznets curve and the\nU.S. house price expectations after the financial crisis.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 12:10:14 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Boldea", "Otilia", ""], ["Drepper", "Bettina", ""], ["Gan", "Zhuojiong", ""]]}, {"id": "1808.03141", "submitter": "Emanuele  Giorgi", "authors": "Benjamin Amoah, Emanuele Giorgi and Peter Diggle", "title": "A Geostatistical Framework for Combining Spatially Referenced Disease\n  Prevalence Data from Multiple Diagnostics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple diagnostic tests are often used due to limited resources or because\nthey provide complementary information on the epidemiology of a disease under\ninvestigation. Existing statistical methods to combine prevalence data from\nmultiple diagnostics ignore the potential over-dispersion induced by the\nspatial correlations in the data. To address this issue, we develop a\ngeostatistical framework that allows for joint modelling of data from multiple\ndiagnostics by considering two main classes of inferential problems: (1) to\npredict prevalence for a gold-standard diagnostic using low-cost and\npotentially biased alternative tests; (2) to carry out joint prediction of\nprevalence from multiple tests. We apply the proposed framework to two case\nstudies: mapping Loa loa prevalence in Central and West Africa, using\nmiscroscopy and a questionnaire-based test called RAPLOA; mapping Plasmodium\nfalciparum malaria prevalence in the highlands of Western Kenya using\npolymerase chain reaction and a rapid diagnostic test. We also develop a Monte\nCarlo procedure based on the variogram in order to identify parsimonious\ngeostatistical models that are compatible with the data. Our study highlights\n(i) the importance of accounting for diagnostic-specific residual spatial\nvariation and (ii) the benefits accrued from joint geostatistical modelling so\nas to deliver more reliable and precise inferences on disease prevalence.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 13:21:43 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Amoah", "Benjamin", ""], ["Giorgi", "Emanuele", ""], ["Diggle", "Peter", ""]]}, {"id": "1808.03231", "submitter": "Laura Balzer PhD", "authors": "Laura B. Balzer, Diane V. Havlir, Joshua Schwab, Mark J. Van Der Laan,\n  Maya L. Petersen (for the Search Study Team)", "title": "Statistical Analysis Plan for SEARCH Phase I: Health Outcomes among\n  Adults", "comments": "40 pgs", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document provides the analytic plan for evaluating adult HIV incidence,\nhealth, and implementation outcomes for the first phase of the SEARCH Study.\nLocked: November 27, 2017. Embargoed until July 25, 2018.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 22:02:32 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Balzer", "Laura B.", "", "for the Search Study Team"], ["Havlir", "Diane V.", "", "for the Search Study Team"], ["Schwab", "Joshua", "", "for the Search Study Team"], ["Van Der Laan", "Mark J.", "", "for the Search Study Team"], ["Petersen", "Maya L.", "", "for the Search Study Team"]]}, {"id": "1808.03273", "submitter": "Paul Moore", "authors": "Paul Moore, Terry Lyons, John Gallacher", "title": "Random forest prediction of Alzheimer's disease using pairwise selection\n  from time series data", "comments": "6 pages, 1 figure, 6 tables", "journal-ref": null, "doi": "10.1371/journal.pone.0211558", "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Time-dependent data collected in studies of Alzheimer's disease usually has\nmissing and irregularly sampled data points. For this reason time series\nmethods which assume regular sampling cannot be applied directly to the data\nwithout a pre-processing step. In this paper we use a machine learning method\nto learn the relationship between pairs of data points at different time\nseparations. The input vector comprises a summary of the time series history\nand includes both demographic and non-time varying variables such as genetic\ndata. The dataset used is from the 2017 TADPOLE grand challenge which aims to\npredict the onset of Alzheimer's disease using including demographic, physical\nand cognitive data. The challenge is a three-fold diagnosis classification into\nAD, MCI and control groups, the prediction of ADAS-13 score and the normalised\nventricle volume. While the competition proceeds, forecasting methods may be\ncompared using a leaderboard dataset selected from the Alzheimer's Disease\nNeuroimaging Initiative (ADNI) and with standard metrics for measuring\naccuracy. For diagnosis, we find an mAUC of 0.82, and a classification accuracy\nof 0.73. The results show that the method is effective and comparable with\nother methods.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 14:53:59 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Moore", "Paul", ""], ["Lyons", "Terry", ""], ["Gallacher", "John", ""]]}, {"id": "1808.03318", "submitter": "Jan Mandel", "authors": "James Haley, Angel Farguell Caus, Adam K. Kochanski, Sher Schranz, Jan\n  Mandel", "title": "Data Likelihood of Active Fires Satellite Detection and Applications to\n  Ignition Estimation and Data Assimilation", "comments": "12 pages, 6 figures; VIII International Conference on Forest Fire\n  Research, Coimbra, Portugal, November 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.OH eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data likelihood of fire detection is the probability of the observed\ndetection outcome given the state of the fire spread model. We derive fire\ndetection likelihood of satellite data as a function of the fire arrival time\non the model grid. The data likelihood is constructed by a combination of the\nburn model, the logistic regression of the active fires detections, and the\nGaussian distribution of the geolocation error. The use of the data likelihood\nis then demonstrated by an estimation of the ignition point of a wildland fire\nby the maximization of the likelihood of MODIS and VIIRS data over multiple\npossible ignition points.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 04:41:24 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Haley", "James", ""], ["Caus", "Angel Farguell", ""], ["Kochanski", "Adam K.", ""], ["Schranz", "Sher", ""], ["Mandel", "Jan", ""]]}, {"id": "1808.03364", "submitter": "Matthew Harding", "authors": "Matthew Harding and Carlos Lamarche", "title": "A Panel Quantile Approach to Attrition Bias in Big Data: Evidence from a\n  Randomized Experiment", "comments": "JEL: C21, C23, C25, C55. Keywords: Attrition; Big Data; Quantile\n  regression; Individual Effects; Time-of-Day Pricing", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a quantile regression estimator for panel data models\nwith individual heterogeneity and attrition. The method is motivated by the\nfact that attrition bias is often encountered in Big Data applications. For\nexample, many users sign-up for the latest program but few remain active users\nseveral months later, making the evaluation of such interventions inherently\nvery challenging. Building on earlier work by Hausman and Wise (1979), we\nprovide a simple identification strategy that leads to a two-step estimation\nprocedure. In the first step, the coefficients of interest in the selection\nequation are consistently estimated using parametric or nonparametric methods.\nIn the second step, standard panel quantile methods are employed on a subset of\nweighted observations. The estimator is computationally easy to implement in\nBig Data applications with a large number of subjects. We investigate the\nconditions under which the parameter estimator is asymptotically Gaussian and\nwe carry out a series of Monte Carlo simulations to investigate the finite\nsample properties of the estimator. Lastly, using a simulation exercise, we\napply the method to the evaluation of a recent Time-of-Day electricity pricing\nexperiment inspired by the work of Aigner and Hausman (1980).\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 22:35:27 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Harding", "Matthew", ""], ["Lamarche", "Carlos", ""]]}, {"id": "1808.03374", "submitter": "Jiahao Chen", "authors": "Jiahao Chen and Andreas Noack and Alan Edelman", "title": "Fast computation of the principal components of genotype matrices in\n  Julia", "comments": "15 pages, 6 figures, 3 tables, repository at\n  https://github.com/jiahao/paper-copper-2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CE q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the largest few principal components of a matrix of genetic data is a\ncommon task in genome-wide association studies (GWASs), both for dimensionality\nreduction and for identifying unwanted factors of variation. We describe a\nsimple random matrix model for matrices that arise in GWASs, showing that the\nsingular values have a bulk behavior that obeys a Marchenko-Pastur distributed\nwith a handful of large outliers. We also implement Golub-Kahan-Lanczos (GKL)\nbidiagonalization in the Julia programming language, providing thick restarting\nand a choice between full and partial reorthogonalization strategies to control\nnumerical roundoff. Our implementation of GKL bidiagonalization is up to 36\ntimes faster than software tools used commonly in genomics data analysis for\ncomputing principal components, such as EIGENSOFT and FlashPCA, which use dense\nLAPACK routines and randomized subspace iteration respectively.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 23:47:21 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Chen", "Jiahao", ""], ["Noack", "Andreas", ""], ["Edelman", "Alan", ""]]}, {"id": "1808.03587", "submitter": "Xiaodong Jia", "authors": "Xiaodong Jia, Ming Zhao, Haoshu Cai, Jay Lee", "title": "A simplified convolutional sparse filter for impulsive signature\n  enhancement and its application to the prognostic of rotating machinery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Impulsive signature enhancement (ISE) is an important topic in the monitoring\nof rotating machinery and many different methods have been proposed. Even\nthough, the topic of how to leverage these ISE techniques to improve the data\nquality in terms of prognostics and health management (PHM) still needs to be\ninvestigated. In this work, a systematic view for data quality enhancement is\npresented. The data quality issues for the prognostics and health management\n(PHM) of rotating machinery are identified, and the major steps to enhance data\nquality are organized. Based on this, a novel ISE algorithm is originally\nproposed, the importance of extracting scale invariant features are explained,\nand also related features are proposed for the PHM of rotating machinery. In\norder to demonstrate the effectiveness of the novelties, two experimental\nstudies are presented. The final results indicate that the proposed method can\nbe effectively employed to enhance the data quality for machine failure\ndetection and diagnosis.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 15:26:11 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Jia", "Xiaodong", ""], ["Zhao", "Ming", ""], ["Cai", "Haoshu", ""], ["Lee", "Jay", ""]]}, {"id": "1808.03813", "submitter": "Nicholas Henderson", "authors": "Nicholas C. Henderson and Ravi Varadhan", "title": "Bayesian Bivariate Subgroup Analysis for Risk-Benefit Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subgroup analysis is a frequently used tool for evaluating heterogeneity of\ntreatment effect and heterogeneity in treatment harm across observed baseline\npatient characteristics. While treatment efficacy and adverse event measures\nare often reported separately for each subgroup, analyzing their\nwithin-subgroup joint distribution is critical for better informed patient\ndecision-making. In this paper, we describe Bayesian models for performing a\nsubgroup analysis to compare the joint occurrence of a primary endpoint and an\nadverse event between two treatment arms. Our approaches emphasize estimation\nof heterogeneity in this joint distribution across subgroups, and our\napproaches directly accommodate subgroups with small numbers of observed\nprimary and adverse event combinations. In addition, we describe several ways\nin which our models may be used to generate interpretable summary measures of\nbenefit-risk tradeoffs for each subgroup. The methods described here are\nillustrated throughout using a large cardiovascular trial (N = 9,361)\ninvestigating the efficacy of an intervention for reducing systolic blood\npressure to a lower-than-usual target.\n", "versions": [{"version": "v1", "created": "Sat, 11 Aug 2018 14:44:14 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Henderson", "Nicholas C.", ""], ["Varadhan", "Ravi", ""]]}, {"id": "1808.03827", "submitter": "Willie Boag", "authors": "Willie Boag, Harini Suresh, Leo Anthony Celi, Peter Szolovits, Marzyeh\n  Ghassemi", "title": "Racial Disparities and Mistrust in End-of-Life Care", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are established racial disparities in healthcare, including during\nend-of-life care, when poor communication and trust can lead to suboptimal\noutcomes for patients and their families. In this work, we find that racial\ndisparities which have been reported in existing literature are also present in\nthe MIMIC-III database. We hypothesize that one underlying cause of this\ndisparity is due to mistrust between patient and caregivers, and we develop\nmultiple possible trust metric proxies (using coded interpersonal variables and\nclinical notes) to measure this phenomenon more directly. These metrics show\neven stronger disparities in end-of-life care than race does, and they also\ntend to demonstrate statistically significant higher levels of mistrust for\nblack patients than white ones. Finally, we demonstrate that these metrics\nimprove performance on three clinical tasks: in-hospital mortality, discharge\nagainst medical advice (AMA) and modified care status (e.g., DNR, DNI, etc.).\n", "versions": [{"version": "v1", "created": "Sat, 11 Aug 2018 16:13:23 GMT"}, {"version": "v2", "created": "Thu, 16 Aug 2018 00:04:38 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Boag", "Willie", ""], ["Suresh", "Harini", ""], ["Celi", "Leo Anthony", ""], ["Szolovits", "Peter", ""], ["Ghassemi", "Marzyeh", ""]]}, {"id": "1808.03831", "submitter": "Zheng Chen", "authors": "Dong Han, Yawen Hou, Zheng Chen", "title": "Sample size determination in superiority or non-inferiority clinical\n  trials with time-to-event data under exponential, Weibull and Gompertz\n  distributions", "comments": null, "journal-ref": "Communications in Statistics - Simulation and Computation, 2021", "doi": "10.1080/03610918.2021.1879137", "report-no": "LSSP-2019-0881", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To examine the effect of exponential, Weibull and Gompertz distributions on\nsample size determination for superiority trials (STs) or non-inferiority\ntrials (NTs) with time-to-event data, we present two sample size formulas for\nSTs or NTs based on Weibull and Gompertz distributions, respectively. The\nformulas are compared with the current exponential formula to examine their\nperformance. The simulation results show that the sample size formula based on\nthe Weibull distribution is the most robust among the three formulas in STs or\nNTs. We suggest that recognizing the appropriate distribution in advance is\nbeneficial for proper project planning and that assuming a Weibull distributed\nsurvival time is most advantageous in STs or NTs.\n", "versions": [{"version": "v1", "created": "Sat, 11 Aug 2018 16:32:34 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Han", "Dong", ""], ["Hou", "Yawen", ""], ["Chen", "Zheng", ""]]}, {"id": "1808.03934", "submitter": "Sameer Deshpande", "authors": "Sameer K. Deshpande, Raiden B. Hasegawa, Jordan Weiss, Dylan S. Small", "title": "Protocol for an observational study on the effects of playing football\n  in adolescence on mental health in early adulthood", "comments": "Updated tables summarizing the matches constructed", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  More than 1 million students play high school American football annually, but\nmany health professionals have recently questioned its safety or called for its\nban. These concerns have been partially driven by reports of chronic traumatic\nencephalopathy (CTE), increased risks of neurodegenerative disease, and\nassociations between concussion history and later-life cognitive impairment and\ndepression among retired professional football players.\n  A recent observational study of a cohort of men who graduated from a\nWisconsin high school in 1957 found no statistically significant harmful\neffects of playing high school football on a range of cognitive, psychological,\nand socio-economic outcomes measured at ages 35, 54, 65, and 72. Unfortunately,\nthese findings may not generalize to younger populations, thanks to changes and\nimprovements in football helmet technology and training techniques. In\nparticular, these changes may have led to increased perceptions of safety but\nultimately more dangerous styles of play, characterized by the frequent\nsub-concussive impacts thought to be associated with later-life neurological\ndecline.\n  In this work, we replicate the methodology of that earlier matched\nobservational study using data from the National Longitudinal Study of\nAdolescent to Adult Health (Add Health). These include adolescent and family\nco-morbidities, academic experience, self-reported levels of general health and\nphysical activity, and the score on the Add Health Picture Vocabulary Test. Our\nprimary outcome is the CES-D score measured in 2008 when subjects were aged 24\n-- 34 and settling into early adulthood. We also examine several secondary\noutcomes related to physical and psychological health, including suicidality.\nOur results can provide insight into the natural history of potential\nfootball-related decline and dysfunction.\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2018 12:46:40 GMT"}, {"version": "v2", "created": "Fri, 9 Nov 2018 20:11:48 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Deshpande", "Sameer K.", ""], ["Hasegawa", "Raiden B.", ""], ["Weiss", "Jordan", ""], ["Small", "Dylan S.", ""]]}, {"id": "1808.04278", "submitter": "Peter Rousseeuw", "authors": "Ana Helena Tavares, Jakob Raymaekers, Peter J. Rousseeuw, Paula Brito,\n  Vera Afreixo", "title": "Clustering genomic words in human DNA using peaks and trends of\n  distributions", "comments": null, "journal-ref": "Advances in Data Analysis and Classification, 2020", "doi": "10.1007/s11634-019-00362-x", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we seek clusters of genomic words in human DNA by studying their\ninter-word lag distributions. Due to the particularly spiked nature of these\nhistograms, a clustering procedure is proposed that first decomposes each\ndistribution into a baseline and a peak distribution. An outlier-robust fitting\nmethod is used to estimate the baseline distribution (the `trend'), and a\nsparse vector of detrended data captures the peak structure. A simulation study\ndemonstrates the effectiveness of the clustering procedure in grouping\ndistributions with similar peak behavior and/or baseline features. The\nprocedure is applied to investigate similarities between the distribution\npatterns of genomic words of lengths 3 and 5 in the human genome. These\nexperiments demonstrate the potential of the new method for identifying words\nwith similar distance patterns.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 14:55:09 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Tavares", "Ana Helena", ""], ["Raymaekers", "Jakob", ""], ["Rousseeuw", "Peter J.", ""], ["Brito", "Paula", ""], ["Afreixo", "Vera", ""]]}, {"id": "1808.04312", "submitter": "Anne Presanis", "authors": "Daniela De Angelis, Anne M. Presanis", "title": "Analysing Multiple Epidemic Data Sources", "comments": "This manuscript is a preprint of a Chapter to appear in the \"Handbook\n  of Infectious Disease Data Analysis\", Held, L., Hens, N., O'Neill, P.D. and\n  Wallinga, J. (Eds.). Chapman & Hall/CRC, 2018. Please use the book for\n  possible citations", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evidence-based knowledge of infectious disease burden, including prevalence,\nincidence, severity and transmission, in different population strata and\nlocations, and possibly in real time, is crucial to the planning and evaluation\nof public health policies. Direct observation of a disease process is rarely\npossible. However, latent characteristics of an epidemic and its evolution can\noften be inferred from the synthesis of indirect information from various\nroutine data sources, as well as expert opinion. The simultaneous synthesis of\nmultiple data sources, often conveniently carried out in a Bayesian framework,\nposes a number of statistical and computational challenges: the heterogeneity\nin type, relevance and granularity of the data, together with selection and\ninformative observation biases, lead to complex probabilistic models that are\ndifficult to build and fit, and challenging to criticize. Using motivating case\nstudies of influenza, this chapter illustrates the cycle of model development\nand criticism in the context of Bayesian evidence synthesis, highlighting the\nchallenges of complex model building, computationally efficient inference, and\nconflicting evidence.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 16:12:59 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["De Angelis", "Daniela", ""], ["Presanis", "Anne M.", ""]]}, {"id": "1808.04351", "submitter": "Sujith Mangalathu", "authors": "Sujith Mangalathu, Jong-Su Jeon, and Jiqing Jiang", "title": "Skew Adjustment Factors for Fragilities of California Box-Girder Bridges\n  Subjected to Near-Fault and Far-Field Ground Motions", "comments": "5 Figure, 8 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Past reconnaissance studies revealed that bridges close to active faults are\nmore susceptible to damage and more than 60% of the bridges in California are\nskewed. To assess the combined effect of near-fault ground motions and\nskewness, this paper evaluates the seismic vulnerability of skewed concrete\nbox-girder bridges in California subjected to near-fault and far-field ground\nmotions. The relative risk of skewness and fault-location on the bridges is\nevaluated by developing fragility curves of bridge components and system\naccounting for the material, geometric, and structural uncertainties. It is\nnoted that the skewness and bridge site close to active faults make bridges\nmore vulnerable, and the existing modification factor in HAZUS cannot capture\nthe variation in the median value of the fragilities appropriately. A new set\nof fragility adjustment factors for skewness coupled with the effect of fault\nlocation is suggested in this paper.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 01:32:52 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Mangalathu", "Sujith", ""], ["Jeon", "Jong-Su", ""], ["Jiang", "Jiqing", ""]]}, {"id": "1808.04360", "submitter": "Yang Liu", "authors": "Yang Liu, Sebastien Blandin, Samitha Samaranayake", "title": "Stochastic on-time arrival problem in transit networks", "comments": "29 pages; 12 figures. This manuscript version is made available under\n  the CC-BY-NC-ND 4.0 license\n  https://creativecommons.org/licenses/by-nc-nd/4.0/", "journal-ref": null, "doi": "10.1016/j.trb.2018.11.013", "report-no": null, "categories": "cs.DS cs.SY math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article considers the stochastic on-time arrival problem in transit\nnetworks where both the travel time and the waiting time for transit services\nare stochastic. A specific challenge of this problem is the combinatorial\nsolution space due to the unknown ordering of transit line arrivals. We propose\na network structure appropriate to the online decision-making of a passenger,\nincluding boarding, waiting and transferring. In this framework, we design a\ndynamic programming algorithm that is pseudo-polynomial in the number of\ntransit stations and travel time budget, and exponential in the number of\ntransit lines at a station, which is a small number in practice. To reduce the\nsearch space, we propose a definition of transit line dominance, and techniques\nto identify dominance, which decrease the computation time by up to 90% in\nnumerical experiments. Extensive numerical experiments are conducted on both a\nsynthetic network and the Chicago transit network.\n", "versions": [{"version": "v1", "created": "Sat, 11 Aug 2018 02:45:21 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 19:28:16 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Liu", "Yang", ""], ["Blandin", "Sebastien", ""], ["Samaranayake", "Samitha", ""]]}, {"id": "1808.04408", "submitter": "S. Stanley Young", "authors": "S. Stanley Young, Warren Kindzierski", "title": "Combined background information for meta-analysis evaluation", "comments": "23 pages. Multiple figures. Multiple tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive numbers of meta-analysis studies are being published. A Google\nScholar search of \"systematic review and meta-analysis\" returns about 452k hits\nsince 2014. The search was done on Jan 14, 2019. There is a need to have some\nway to judge the reliability of a positive claim made in a meta-analysis that\nuses observational studies. Our idea is to examine the quality of the\nobservational studies used in the meta-analysis and to examine the\nheterogeneity of those studies. We provide background information and examples:\na listing of negative studies, a simulation of p-value plots, and multiple\nexamples of p-value plots.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 19:09:42 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2018 21:09:13 GMT"}, {"version": "v3", "created": "Mon, 14 Jan 2019 20:07:31 GMT"}, {"version": "v4", "created": "Wed, 16 Jan 2019 03:25:31 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Young", "S. Stanley", ""], ["Kindzierski", "Warren", ""]]}, {"id": "1808.04416", "submitter": "Matias Cattaneo", "authors": "Matias D. Cattaneo, Luke Keele, Rocio Titiunik, Gonzalo Vazquez-Bare", "title": "Extrapolating Treatment Effects in Multi-Cutoff Regression Discontinuity\n  Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In non-experimental settings, the Regression Discontinuity (RD) design is one\nof the most credible identification strategies for program evaluation and\ncausal inference. However, RD treatment effect estimands are necessarily local,\nmaking statistical methods for the extrapolation of these effects a key area\nfor development. We introduce a new method for extrapolation of RD effects that\nrelies on the presence of multiple cutoffs, and is therefore design-based. Our\napproach employs an easy-to-interpret identifying assumption that mimics the\nidea of \"common trends\" in difference-in-differences designs. We illustrate our\nmethods with data on a subsidized loan program on post-education attendance in\nColombia, and offer new evidence on program effects for students with test\nscores away from the cutoff that determined program eligibility.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 19:38:11 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2019 20:26:56 GMT"}, {"version": "v3", "created": "Wed, 1 Apr 2020 16:14:13 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Cattaneo", "Matias D.", ""], ["Keele", "Luke", ""], ["Titiunik", "Rocio", ""], ["Vazquez-Bare", "Gonzalo", ""]]}, {"id": "1808.04511", "submitter": "Juan Sosa", "authors": "Juan Sosa and Abel Rodriguez", "title": "A Record Linkage Model Incorporating Relational Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a novel Bayesian approach for linking multiple\nsocial networks in order to discover the same real world person having\ndifferent accounts across networks. In particular, we develop a latent model\nthat allow us to jointly characterize the network and linkage structures\nrelying in both relational and profile data. In contrast to other existing\napproaches in the machine learning literature, our Bayesian implementation\nnaturally provides uncertainty quantification via posterior probabilities for\nthe linkage structure itself or any function of it. Our findings clearly\nsuggest that our methodology can produce accurate point estimates of the\nlinkage structure even in the absence of profile information, and also, in an\nidentity resolution setting, our results confirm that including relational data\ninto the matching process improves the linkage accuracy. We illustrate our\nmethodology using real data from popular social networks such as Twitter,\nFacebook, and YouTube.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 03:00:29 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Sosa", "Juan", ""], ["Rodriguez", "Abel", ""]]}, {"id": "1808.04582", "submitter": "Christian Damgaard", "authors": "Christian Damgaard", "title": "The joint distribution of pin-point plant cover data: a reparametrized\n  Dirichlet -- multinomial distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  A reparametrized Dirichlet-multinomial distribution is introduced, and the\ncovariance matrix, as well as, the algorithm for calculating the PDF for n\nspecies are provided. The distribution is suited for modelling the joint\ndistribution of pin-point cover data of spatially aggregated plant species, and\nthe parametrization ensures that the degree of spatially aggregation is\nmodelled by a parameter and the mean cover of the species is modelled by the\nother parameters. This last property is convenient for using the distribution\nin Bayesian hierarchical models where the mean cover of the different species\ntypically is modelled as latent variables.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 08:26:54 GMT"}, {"version": "v2", "created": "Tue, 28 Aug 2018 08:31:57 GMT"}, {"version": "v3", "created": "Sat, 8 Dec 2018 12:32:07 GMT"}, {"version": "v4", "created": "Mon, 6 May 2019 09:50:42 GMT"}, {"version": "v5", "created": "Fri, 17 Jan 2020 08:38:17 GMT"}, {"version": "v6", "created": "Tue, 18 Feb 2020 13:31:18 GMT"}, {"version": "v7", "created": "Tue, 3 Mar 2020 09:18:51 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Damgaard", "Christian", ""]]}, {"id": "1808.04679", "submitter": "Li-Fang Cheng", "authors": "Li-Fang Cheng, Niranjani Prasad, Barbara E Engelhardt", "title": "An Optimal Policy for Patient Laboratory Tests in Intensive Care Units", "comments": "The first two authors contributed equally to this work. Preprint of\n  an article submitted for consideration in Pacific Symposium on Biocomputing\n  copyright 2018 [copyright World Scientific Publishing Company]\n  [https://psb.stanford.edu/]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Laboratory testing is an integral tool in the management of patient care in\nhospitals, particularly in intensive care units (ICUs). There exists an\ninherent trade-off in the selection and timing of lab tests between\nconsiderations of the expected utility in clinical decision-making of a given\ntest at a specific time, and the associated cost or risk it poses to the\npatient. In this work, we introduce a framework that learns policies for\nordering lab tests which optimizes for this trade-off. Our approach uses batch\noff-policy reinforcement learning with a composite reward function based on\nclinical imperatives, applied to data that include examples of clinicians\nordering labs for patients. To this end, we develop and extend principles of\nPareto optimality to improve the selection of actions based on multiple reward\nfunction components while respecting typical procedural considerations and\nprioritization of clinical goals in the ICU. Our experiments show that we can\nestimate a policy that reduces the frequency of lab tests and optimizes timing\nto minimize information redundancy. We also find that the estimated policies\ntypically suggest ordering lab tests well ahead of critical onsets--such as\nmechanical ventilation or dialysis--that depend on the lab results. We evaluate\nour approach by quantifying how these policies may initiate earlier onset of\ntreatment.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 13:34:06 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Cheng", "Li-Fang", ""], ["Prasad", "Niranjani", ""], ["Engelhardt", "Barbara E", ""]]}, {"id": "1808.04698", "submitter": "Mike West", "authors": "Lindsay R. Berry, Paul Helman and Mike West", "title": "Probabilistic forecasting of heterogeneous consumer transaction-sales\n  time series", "comments": "23 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new Bayesian methodology for consumer sales forecasting. With a\nfocus on multi-step ahead forecasting of daily sales of many supermarket items,\nwe adapt dynamic count mixture models to forecast individual customer\ntransactions, and introduce novel dynamic binary cascade models for predicting\ncounts of items per transaction. These transactions-sales models can\nincorporate time-varying trend, seasonal, price, promotion, random effects and\nother outlet-specific predictors for individual items. Sequential Bayesian\nanalysis involves fast, parallel filtering on sets of decoupled items and is\nadaptable across items that may exhibit widely varying characteristics. A\nmulti-scale approach enables information sharing across items with related\npatterns over time to improve prediction while maintaining scalability to many\nitems. A motivating case study in many-item, multi-period, multi-step ahead\nsupermarket sales forecasting provides examples that demonstrate improved\nforecast accuracy in multiple metrics, and illustrates the benefits of full\nprobabilistic models for forecast accuracy evaluation and comparison.\n  Keywords: Bayesian forecasting; decouple/recouple; dynamic binary cascade;\nforecast calibration; intermittent demand; multi-scale forecasting; predicting\nrare events; sales per transaction; supermarket sales forecasting\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 14:03:03 GMT"}, {"version": "v2", "created": "Mon, 20 Aug 2018 16:56:54 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Berry", "Lindsay R.", ""], ["Helman", "Paul", ""], ["West", "Mike", ""]]}, {"id": "1808.04753", "submitter": "Si Cheng", "authors": "Si Cheng (1), Daniel J. Eck (2), Forrest W. Crawford (3,4,5 and 6)\n  ((1) Department of Biostatistics, University of Washington, (2) Department of\n  Statistics, University of Illinois Urbana-Champaign, (3) Department of\n  Biostatistics, Yale School of Public Health, (4) Department of Statistics &\n  Data Science, Yale University, (5) Department of Ecology & Evolutionary\n  Biology, Yale University, (6) Yale School of Management)", "title": "Estimating the size of a hidden finite set: large-sample behavior of\n  estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A finite set is \"hidden\" if its elements are not directly enumerable or if\nits size cannot be ascertained via a deterministic query. In public health,\nepidemiology, demography, ecology and intelligence analysis, researchers have\ndeveloped a wide variety of indirect statistical approaches, under different\nmodels for sampling and observation, for estimating the size of a hidden set.\nSome methods make use of random sampling with known or estimable sampling\nprobabilities, and others make structural assumptions about relationships (e.g.\nordering or network information) between the elements that comprise the hidden\nset. In this review, we describe models and methods for learning about the size\nof a hidden finite set, with special attention to asymptotic properties of\nestimators. We study the properties of these methods under two asymptotic\nregimes, \"infill\" in which the number of fixed-size samples increases, but the\npopulation size remains constant, and \"outfill\" in which the sample size and\npopulation size grow together. Statistical properties under these two regimes\ncan be dramatically different.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 15:31:56 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 23:20:16 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Cheng", "Si", "", "3,4,5 and 6"], ["Eck", "Daniel J.", "", "3,4,5 and 6"], ["Crawford", "Forrest W.", "", "3,4,5 and 6"]]}, {"id": "1808.04765", "submitter": "Garyfallos Konstantinoudis Mr", "authors": "Garyfallos Konstantinoudis, Dominic Schuhmacher, H{\\aa}vard Rue, Ben\n  Spycher", "title": "Discrete versus continuous domain models for disease mapping", "comments": "28 pages, 4 figures, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main goal of disease mapping is to estimate disease risk and identify\nhigh-risk areas. Such analyses are hampered by the limited geographical\nresolution of the available data. Typically the available data are counts per\nspatial unit and the common approach is the Besag--York--Molli{\\'e} (BYM)\nmodel. When precise geocodes are available, it is more natural to use\nLog-Gaussian Cox processes (LGCPs). In a simulation study mimicking childhood\nleukaemia incidence using actual residential locations of all children in the\ncanton of Z\\\"urich, Switzerland, we compare the ability of these models to\nrecover risk surfaces and identify high-risk areas. We then apply both\napproaches to actual data on childhood leukaemia incidence in the canton of\nZ\\\"urich during 1985-2015. We found that LGCPs outperform BYM models in almost\nall scenarios considered. Our findings suggest that there are important gains\nto be made from the use of LGCPs in spatial epidemiology.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 15:51:43 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2019 10:20:57 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Konstantinoudis", "Garyfallos", ""], ["Schuhmacher", "Dominic", ""], ["Rue", "H\u00e5vard", ""], ["Spycher", "Ben", ""]]}, {"id": "1808.04871", "submitter": "Daniel Daly-Grafstein", "authors": "Daniel Daly-Grafstein and Luke Bornn", "title": "Rao-Blackwellizing Field Goal Percentage", "comments": "23 pages, 6 figures, LaTeX, updated section 2 on Rao-Blackwell\n  derivation, section 4.1 on Empirical Bayes shrinkage, added rank comparison,\n  updated Figures 1,3,4,5", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shooting skill in the NBA is typically measured by field goal percentage\n(FG%) - the number of makes out of the total number of shots. Even more\nadvanced metrics like true shooting percentage are calculated by counting each\nplayer's 2-point, 3-point, and free throw makes and misses, ignoring the\nspatiotemporal data now available (Kubatko et al. 2007). In this paper we aim\nto better characterize player shooting skill by introducing a new estimator\nbased on post-shot release shot-make probabilities. Via the Rao-Blackwell\ntheorem, we propose a shot-make probability model that conditions probability\nestimates on shot trajectory information, thereby reducing the variance of the\nnew estimator relative to standard FG%. We obtain shooting information by using\noptical tracking data to estimate three factors for each shot: entry angle,\nshot depth, and left-right accuracy. Next we use these factors to model\nshot-make probabilities for all shots in the 2014-15 season, and use these\nprobabilities to produce a Rao-Blackwellized FG% estimator (RB-FG%) for each\nplayer. We demonstrate that RB-FG% is better than raw FG% at predicting 3-point\nshooting and true-shooting percentages. Overall, we find that conditioning\nshot-make probabilities on spatial trajectory information stabilizes inference\nof FG%, creating the potential to estimate shooting statistics earlier in a\nseason than was previously possible.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 19:39:22 GMT"}, {"version": "v2", "created": "Thu, 16 Aug 2018 06:04:13 GMT"}, {"version": "v3", "created": "Mon, 29 Oct 2018 17:34:34 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Daly-Grafstein", "Daniel", ""], ["Bornn", "Luke", ""]]}, {"id": "1808.04880", "submitter": "Alexander New", "authors": "Alexander New and Kristin P. Bennett", "title": "A Precision Environment-Wide Association Study of Hypertension via\n  Supervised Cadre Models", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem in precision health of grouping people into\nsubpopulations based on their degree of vulnerability to a risk factor. These\nsubpopulations cannot be discovered with traditional clustering techniques\nbecause their quality is evaluated with a supervised metric: the ease of\nmodeling a response variable over observations within them. Instead, we apply\nthe supervised cadre model (SCM), which does use this metric. We extend the SCM\nformalism so that it may be applied to multivariate regression and binary\nclassification problems. We also develop a way to use conditional entropy to\nassess the confidence in the process by which a subject is assigned their\ncadre. Using the SCM, we generalize the environment-wide association study\n(EWAS) workflow to be able to model heterogeneity in population risk. In our\nEWAS, we consider more than two hundred environmental exposure factors and find\ntheir association with diastolic blood pressure, systolic blood pressure, and\nhypertension. This requires adapting the SCM to be applicable to data generated\nby a complex survey design. After correcting for false positives, we found 25\nexposure variables that had a significant association with at least one of our\nresponse variables. Eight of these were significant for a discovered\nsubpopulation but not for the overall population. Some of these associations\nhave been identified by previous researchers, while others appear to be novel.\nWe examine several discovered subpopulations in detail, and we find that they\nare interpretable and that they suggest further research questions.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 20:08:33 GMT"}, {"version": "v2", "created": "Sun, 9 Dec 2018 20:25:47 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["New", "Alexander", ""], ["Bennett", "Kristin P.", ""]]}, {"id": "1808.04904", "submitter": "Yuxiang Xie", "authors": "Yuxiang Xie and Nanyu Chen and Xiaolin Shi", "title": "False Discovery Rate Controlled Heterogeneous Treatment Effect Detection\n  for Online Controlled Experiments", "comments": null, "journal-ref": "Yuxiang Xie, Nanyu Chen, and Xiaolin Shi. 2018. KDD '18\n  Proceedings of the 24th ACM SIGKDD International Conference on Knowledge\n  Discovery & Data Mining Pages 876-885", "doi": "10.1145/3219819.3219860", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online controlled experiments (a.k.a. A/B testing) have been used as the\nmantra for data-driven decision making on feature changing and product shipping\nin many Internet companies. However, it is still a great challenge to\nsystematically measure how every code or feature change impacts millions of\nusers with great heterogeneity (e.g. countries, ages, devices). The most\ncommonly used A/B testing framework in many companies is based on Average\nTreatment Effect (ATE), which cannot detect the heterogeneity of treatment\neffect on users with different characteristics. In this paper, we propose\nstatistical methods that can systematically and accurately identify\nHeterogeneous Treatment Effect (HTE) of any user cohort of interest (e.g.\nmobile device type, country), and determine which factors (e.g. age, gender) of\nusers contribute to the heterogeneity of the treatment effect in an A/B test.\nBy applying these methods on both simulation data and real-world\nexperimentation data, we show how they work robustly with controlled low False\nDiscover Rate (FDR), and at the same time, provides us with useful insights\nabout the heterogeneity of identified user groups. We have deployed a toolkit\nbased on these methods, and have used it to measure the Heterogeneous Treatment\nEffect of many A/B tests at Snap.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 21:40:17 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Xie", "Yuxiang", ""], ["Chen", "Nanyu", ""], ["Shi", "Xiaolin", ""]]}, {"id": "1808.05276", "submitter": "Renzhi Jing", "authors": "Renzhi Jing and Ning Lin", "title": "Tropical Cyclone Intensity Evolution Modeled as a Dependent Hidden\n  Markov Process", "comments": null, "journal-ref": null, "doi": "10.1175/JCLI-D-19-0027.1", "report-no": null, "categories": "stat.AP physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A hidden Markov model is developed to simulate tropical cyclone intensity\nevolution dependent on the surrounding large-scale environment. The model\nconsiders three unobserved (hidden) discrete states of intensification and\nassociates each state with a probability distribution of intensity change. The\nstorm's transit from one state to another is described as a Markov chain. Both\nthe intensity change and state transit components of the model are dependent on\nenvironmental variables including potential intensity, vertical wind shear,\nrelative humidity, and ocean feedback. This Markov environment-dependent\nhurricane intensity model (MeHiM) is used to simulate the evolution of storm\nintensity along the storm track over the ocean, and a simple decay model is\nadded to estimate the intensity change when the storm moves over land. Data for\nthe North Atlantic (NA) basin from 1979-2014 (555 storms) are used for model\ndevelopment and evaluation. Probability distributions of 6-h and 24-h intensity\nchange, lifetime maximum intensity, and landfall intensity based on model\nsimulations and observations compare well. Although the MeHiM is still limited\nin fully describing rapid intensification, it shows a significant improvement\nover previous statistical models (e.g., linear, nonlinear, and finite mixture\nmodels).\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 19:58:22 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Jing", "Renzhi", ""], ["Lin", "Ning", ""]]}, {"id": "1808.05289", "submitter": "Jie Yang", "authors": "Liyuan Jiang, Shuang Zhou, Keren Li, Fangfang Wang and Jie Yang", "title": "A New Nonparametric Estimate of the Risk-Neutral Density with\n  Applications to Variance Swaps", "comments": "19 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new nonparametric approach for estimating the risk-neutral\ndensity of asset prices and reformulate its estimation into a\ndouble-constrained optimization problem. We evaluate our approach using the\nS\\&P 500 market option prices from 1996 to 2015. A comprehensive\ncross-validation study shows that our approach outperforms the existing\nnonparametric quartic B-spline and cubic spline methods, as well as the\nparametric method based on the Normal Inverse Gaussian distribution. As an\napplication, we use the proposed density estimator to price long-term variance\nswaps, and the model-implied prices match reasonably well with those of the\nvariance future downloaded from the CBOE website.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 21:32:25 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2019 03:09:35 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Jiang", "Liyuan", ""], ["Zhou", "Shuang", ""], ["Li", "Keren", ""], ["Wang", "Fangfang", ""], ["Yang", "Jie", ""]]}, {"id": "1808.05298", "submitter": "Erin Peterson", "authors": "Erin E Peterson, Edgar Santos-Fern\\'andez, Carla Chen, Sam Clifford,\n  Julie Vercelloni, Alan Pearse, Ross Brown, Bryce Christensen, Allan James,\n  Ken Anthony, Jennifer Loder, Manuel Gonz\\'alez-Rivero, Chris Roelfsema,\n  M.Julian Caley, Tomasz Bednarz, and Kerrie Mengersen", "title": "Monitoring through many eyes: Integrating disparate datasets to improve\n  monitoring of the Great Barrier Reef", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous organisations collect data in the Great Barrier Reef (GBR), but they\nare rarely analysed together due to different program objectives, methods, and\ndata quality. We developed a weighted spatiotemporal Bayesian model and used it\nto integrate image based hard coral data collected by professional and citizen\nscientists, who captured and or classified underwater images. We used the model\nto predict coral cover across the GBR with estimates of uncertainty; thus\nfilling gaps in space and time where no data exist. Additional data increased\nthe models predictive ability by 43 percent, but did not affect model\ninferences about pressures (e.g. bleaching and cyclone damage). Thus, effective\nintegration of professional and high-volume citizen data could enhance the\ncapacity and cost efficiency of monitoring programs. This general approach is\nequally viable for other variables collected in the marine environment or other\necosystems; opening up new opportunities to integrate data and provide pathways\nfor community engagement and stewardship.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 22:39:20 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 02:56:53 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Peterson", "Erin E", ""], ["Santos-Fern\u00e1ndez", "Edgar", ""], ["Chen", "Carla", ""], ["Clifford", "Sam", ""], ["Vercelloni", "Julie", ""], ["Pearse", "Alan", ""], ["Brown", "Ross", ""], ["Christensen", "Bryce", ""], ["James", "Allan", ""], ["Anthony", "Ken", ""], ["Loder", "Jennifer", ""], ["Gonz\u00e1lez-Rivero", "Manuel", ""], ["Roelfsema", "Chris", ""], ["Caley", "M. Julian", ""], ["Bednarz", "Tomasz", ""], ["Mengersen", "Kerrie", ""]]}, {"id": "1808.05593", "submitter": "Forrest Crawford", "authors": "Daniel J. Eck, Olga Morozova, Forrest W. Crawford", "title": "Randomization for the susceptibility effect of an infectious disease\n  intervention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST q-bio.PE stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized trials of infectious disease interventions, such as vaccines,\noften focus on groups of connected or potentially interacting individuals. When\nthe pathogen of interest is transmissible between study subjects, interference\nmay occur: individual infection outcomes may depend on treatments received by\nothers. Epidemiologists have defined the primary causal effect of interest --\ncalled the \"susceptibility effect\" -- as a contrast in infection risk under\ntreatment versus no treatment, while holding exposure to infectiousness\nconstant. A related quantity -- the \"direct effect\" -- is defined as an\nunconditional contrast between the infection risk under treatment versus no\ntreatment. The purpose of this paper is to show that under a widely recommended\nrandomization design, the direct effect may fail to recover the sign of the\ntrue susceptibility effect of the intervention in a randomized trial when\noutcomes are contagious. The analytical approach uses structural features of\ninfectious disease transmission to define the susceptibility effect. A new\nprobabilistic coupling argument reveals stochastic dominance relations between\npotential infection outcomes under different treatment allocations. The results\nsuggest that estimating the direct effect under randomization may provide\nmisleading inferences about the effect of an intervention -- such as a vaccine\n-- when outcomes are contagious.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 17:25:07 GMT"}, {"version": "v2", "created": "Sat, 8 Sep 2018 17:55:27 GMT"}, {"version": "v3", "created": "Mon, 9 Dec 2019 15:59:31 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Eck", "Daniel J.", ""], ["Morozova", "Olga", ""], ["Crawford", "Forrest W.", ""]]}, {"id": "1808.05766", "submitter": "Yongshuai Jiang", "authors": "Yongshuai Jiang, Jing Xu, Simeng Hu, Di Liu, Linna Zhao and Xu Zhou", "title": "The Function Transformation Omics - Funomics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.OT stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  There are no two identical leaves in the world, so how to find effective\nmarkers or features to distinguish them is an important issue. Function\ntransformation, such as f(x,y) and f(x,y,z), can transform two, three, or\nmultiple input/observation variables (in biology, it generally refers to the\nobserved/measured value of biomarkers, biological characteristics, or other\nindicators) into a new output variable (new characteristics or indicators).\nThis provided us a chance to re-cognize objective things or relationships\nbeyond the original measurements. For example, Body Mass Index, which transform\nweight and high into a new indicator BMI=x/y^2 (where x is weight and y is\nhigh), is commonly used in to gauge obesity. Here, we proposed a new system,\nFunomics (Function Transformation Omics), for understanding the world in a\ndifferent perspective. Funome can be understood as a set of math functions\nconsist of basic elementary functions (such as power functions and exponential\nfunctions) and basic mathematical operations (such as addition, subtraction).\nBy scanning the whole Funome, researchers can identify some special functions\n(called handsome functions) which can generate the novel important output\nvariable (characteristics or indicators). We also start \"the Funome project\" to\ndevelop novel methods, function library and analysis software for Funome\nstudies. The Funome project will accelerate the discovery of new useful\nindicators or characteristics, will improve the utilization efficiency of\ndirectly measured data, and will enhance our ability to understand the world.\nThe analysis tools and data resources about the Funome project can be found\ngradually at http://www.funome.com.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 05:57:42 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Jiang", "Yongshuai", ""], ["Xu", "Jing", ""], ["Hu", "Simeng", ""], ["Liu", "Di", ""], ["Zhao", "Linna", ""], ["Zhou", "Xu", ""]]}, {"id": "1808.05865", "submitter": "Paul Moore", "authors": "P.J. Moore, J. Gallacher, T.J. Lyons", "title": "Using path signatures to predict a diagnosis of Alzheimer's disease", "comments": "5 pages, 3 figures. arXiv admin note: text overlap with\n  arXiv:1808.03273", "journal-ref": null, "doi": "10.1371/journal.pone.0222212", "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The path signature is a means of feature generation that can encode nonlinear\ninteractions in the data as well as the usual linear features. It can\ndistinguish the ordering of time-sequenced changes: for example whether or not\nthe hippocampus shrinks fast, then slowly or the converse. It provides\ninterpretable features and its output is a fixed length vector irrespective of\nthe number of input points so it can encode longitudinal data of varying length\nand with missing data points. In this paper we demonstrate the path signature\nin providing features to distinguish a set of people with Alzheimer's disease\nfrom a matched set of healthy individuals. The data used are volume\nmeasurements of the whole brain, ventricles and hippocampus from the\nAlzheimer's Disease Neuroimaging Initiative (ADNI). The path signature method\nis shown to be a useful tool for the processing of sequential data which is\nbecoming increasingly available as monitoring technologies are applied.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 15:41:58 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Moore", "P. J.", ""], ["Gallacher", "J.", ""], ["Lyons", "T. J.", ""]]}, {"id": "1808.05945", "submitter": "Behram Wali", "authors": "Behram Wali, David Greene, Asad Khattak, Jun Liu", "title": "Analyzing within Garage Fuel Economy Gaps to Support Vehicle Purchasing\n  Decisions - A Copula-Based Modeling & Forecasting Approach", "comments": null, "journal-ref": "Transportation Research Part D: Transport and Environment, 63,\n  186-208 (2018)", "doi": "10.1016/j.trd.2018.04.023", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key purpose of the U.S. government fuel economy ratings is to provide\nprecise and unbiased fuel economy estimates to assist consumers in their\nvehicle purchase decisions. For the official fuel economy ratings to be useful,\nthe numbers must be relatively reliable. This study focuses on quantifying the\nvariations of on-road fuel economy relative to official government ratings\n(fuel economy gap) and seeks proper characterizations for the degree of\nstochastic dependence between the fuel economy gaps of pairs of vehicles.By\nusing unique data reported by customers of the U.S. government website\nwww.fueleconomy.gov, the study presents an innovative copula-based\njoint-modeling and forecasting framework for exploring the complex stochastic\ndependencies (both nonlinear and non-normal) between the fuel economy gaps of\nvehicles reported by the same person. While the EPA label estimates are similar\nto the average numbers reported by website customers, significant, non-linear\nvariation exists in the fuel economy gaps for the two vehicles across the\nsample. In particular, a positive dependence, characterized by Student-t\ncopula, is observed between the fuel economy gaps of the two vehicles with\nsignificant dependencies in the tails of the bivariate distribution; a pair in\nwhich one vehicle achieves better (worse) fuel economy is likely to contain a\nsecond vehicle getting better (worse) fuel economy as well. However, the\nresults also suggest that the strength of overall association is weak (Kendall\nTau = 0.28). This implies a lack of compelling agreement between fuel economy\ngaps which could weaken consumers confidence in making relative comparisons\namong vehicles.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 22:38:51 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Wali", "Behram", ""], ["Greene", "David", ""], ["Khattak", "Asad", ""], ["Liu", "Jun", ""]]}, {"id": "1808.05977", "submitter": "Pablo Giuliani", "authors": "Shuang Zhou, P. Giuliani, J. Piekarewicz, Anirban Bhattacharya, and\n  Debdeep Pati", "title": "Revisiting the proton-radius problem using constrained Gaussian\n  processes", "comments": null, "journal-ref": "Phys. Rev. C 99, 055202 (2019)", "doi": "10.1103/PhysRevC.99.055202", "report-no": null, "categories": "nucl-th nucl-ex stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: The \"proton radius puzzle\" refers to an eight-year old problem\nthat highlights major inconsistencies in the extraction of the charge radius of\nthe proton from muonic Lamb-shift experiments as compared against experiments\nusing elastic electron scattering. For the latter, the determination of the\ncharge radius involves an extrapolation of the experimental form factor to zero\nmomentum transfer.\n  Purpose: To estimate the proton radius by introducing a novel non-parametric\napproach to model the electric form factor of the proton.\n  Methods: Within a Bayesian paradigm, we develop a model flexible enough to\nfit the data without any parametric assumptions on the form factor. The\nBayesian estimation is guided by imposing only two physical constraints on the\nform factor: (a) its value at zero momentum transfer (normalization) and (b)\nits overall shape, assumed to be a monotonically decreasing function of the\nmomentum transfer. Variants of these assumptions are explored to assess the\nimpact of these constraints.\n  Results: So far our results are inconclusive in regard to the proton puzzle,\nas they depend on both, the assumed constrains and the range of experimental\ndata used. For example, if only low momentum-transfer data is used, adopting\nonly the normalization constraint provides a value compatible with the smaller\nmuonic result, while imposing only the shape constraint favors the larger\nelectronic value.\n  Conclusions: We have presented a novel technique to estimate the proton\nradius from electron scattering data based on a non-parametric Gaussian\nprocess. We have shown the impact of the physical constraints imposed on the\nform factor and of the range of experimental data used. In this regard, we are\nhopeful that as this technique is refined and with the anticipated new results\nfrom the PRad experiment, we will get closer to resolve of the puzzle.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 19:55:53 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Zhou", "Shuang", ""], ["Giuliani", "P.", ""], ["Piekarewicz", "J.", ""], ["Bhattacharya", "Anirban", ""], ["Pati", "Debdeep", ""]]}, {"id": "1808.06021", "submitter": "Amir Karami", "authors": "Amir Karami and Matthew Collins", "title": "What do the US West Coast Public Libraries Post on Twitter?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Twitter has provided a great opportunity for public libraries to disseminate\ninformation for a variety of purposes. Twitter data have been applied in\ndifferent domains such as health, politics, and history. There are thousands of\npublic libraries in the US, but no study has yet investigated the content of\ntheir social media posts like tweets to find their interests. Moreover,\ntraditional content analysis of Twitter content is not an efficient task for\nexploring thousands of tweets. Therefore, there is a need for automatic methods\nto overcome the limitations of manual methods. This paper proposes a\ncomputational approach to collecting and analyzing using Twitter Application\nProgramming Interfaces (API) and investigates more than 138,000 tweets from 48\nUS west coast libraries using topic modeling. We found 20 topics and assigned\nthem to five categories including public relations, book, event, training, and\nsocial good. Our results show that the US west coast libraries are more\ninterested in using Twitter for public relations and book-related events. This\nresearch has both practical and theoretical applications for libraries as well\nas other organizations to explore social media actives of their customer and\nthemselves.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 23:50:01 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2018 15:11:22 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Karami", "Amir", ""], ["Collins", "Matthew", ""]]}, {"id": "1808.06022", "submitter": "Amir Karami", "authors": "Amir Karami, Frank Webb, Vanessa L. Kitzie", "title": "Characterizing Transgender Health Issues in Twitter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although there are millions of transgender people in the world, a lack of\ninformation exists about their health issues. This issue has consequences for\nthe medical field, which only has a nascent understanding of how to identify\nand meet this population's health-related needs. Social media sites like\nTwitter provide new opportunities for transgender people to overcome these\nbarriers by sharing their personal health experiences. Our research employs a\ncomputational framework to collect tweets from self-identified transgender\nusers, detect those that are health-related, and identify their information\nneeds. This framework is significant because it provides a macro-scale\nperspective on an issue that lacks investigation at national or demographic\nlevels. Our findings identified 54 distinct health-related topics that we\ngrouped into 7 broader categories. Further, we found both linguistic and\ntopical differences in the health-related information shared by transgender men\n(TM) as com-pared to transgender women (TW). These findings can help inform\nmedical and policy-based strategies for health interventions within transgender\ncommunities. Also, our proposed approach can inform the development of\ncomputational strategies to identify the health-related information needs of\nother marginalized populations.\n", "versions": [{"version": "v1", "created": "Sat, 18 Aug 2018 00:00:19 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2018 15:08:24 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Karami", "Amir", ""], ["Webb", "Frank", ""], ["Kitzie", "Vanessa L.", ""]]}, {"id": "1808.06109", "submitter": "Shaoyang Ning", "authors": "Yang Li, Shaoyang Ning, Sarah E. Calvo, Vamsi K. Mootha, Jun S. Liu", "title": "Bayesian Hidden Markov Tree Models for Clustering Genes with Shared\n  Evolutionary History", "comments": "34 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determination of functions for poorly characterized genes is crucial for\nunderstanding biological processes and studying human diseases. Functionally\nassociated genes are often gained and lost together through evolution.\nTherefore identifying co-evolution of genes can predict functional gene-gene\nassociations. We describe here the full statistical model and computational\nstrategies underlying the original algorithm, CLustering by Inferred Models of\nEvolution (CLIME 1.0) recently reported by us [Li et al., 2014]. CLIME 1.0\nemploys a mixture of tree-structured hidden Markov models for gene evolution\nprocess, and a Bayesian model-based clustering algorithm to detect gene modules\nwith shared evolutionary histories (termed evolutionary conserved modules, or\nECMs). A Dirichlet process prior was adopted for estimating the number of gene\nclusters and a Gibbs sampler was developed for posterior sampling. We further\ndeveloped an extended version, CLIME 1.1, to incorporate the uncertainty on the\nevolutionary tree structure. By simulation studies and benchmarks on real data\nsets, we show that CLIME 1.0 and CLIME 1.1 outperform traditional methods that\nuse simple metrics (e.g., the Hamming distance or Pearson correlation) to\nmeasure co-evolution between pairs of genes.\n", "versions": [{"version": "v1", "created": "Sat, 18 Aug 2018 18:16:58 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Li", "Yang", ""], ["Ning", "Shaoyang", ""], ["Calvo", "Sarah E.", ""], ["Mootha", "Vamsi K.", ""], ["Liu", "Jun S.", ""]]}, {"id": "1808.06241", "submitter": "Ilya Safro", "authors": "Saroj Kumar Dash and Ilya Safro and Ravisutha Sakrepatna\n  Srinivasamurthy", "title": "Spatio-temporal prediction of crimes using network analytic approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is quite evident that majority of the population lives in urban area today\nthan in any time of the human history. This trend seems to increase in coming\nyears. A study [5] says that nearly 80.7% of total population in USA stays in\nurban area. By 2030 nearly 60% of the population in the world will live in or\nmove to cities. With the increase in urban population, it is important to keep\nan eye on criminal activities. By doing so, governments can enforce intelligent\npolicing systems and hence many government agencies and local authorities have\nmade the crime data publicly available. In this paper, we analyze Chicago city\ncrime data fused with other social information sources using network analytic\ntechniques to predict criminal activity for the next year. We observe that as\nwe add more layers of data which represent different aspects of the society,\nthe quality of prediction is improved. Our prediction models not just predict\ntotal number of crimes for the whole Chicago city, rather they predict number\nof crimes for all types of crimes and for different regions in City of Chicago.\n", "versions": [{"version": "v1", "created": "Sun, 19 Aug 2018 18:38:58 GMT"}, {"version": "v2", "created": "Tue, 30 Oct 2018 05:02:43 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Dash", "Saroj Kumar", ""], ["Safro", "Ilya", ""], ["Srinivasamurthy", "Ravisutha Sakrepatna", ""]]}, {"id": "1808.06310", "submitter": "Daniel Nevo", "authors": "Daniel Nevo, Judith J. Lok, Donna Spiegelman", "title": "Analysis of \"Learn-As-You-Go\" (LAGO) Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In learn-as-you-go (LAGO) adaptive studies, the intervention is a complex\npackage consisting of multiple components, and is adapted in stages during the\nstudy based on past outcome data. This design formalizes standard practice, and\ndesires for practice, in public health intervention studies. An effective\nintervention package is sought, while minimizing intervention package cost.\nWhen analyzing data from a learn-as-you-go study, the interventions in later\nstages depend upon the outcomes in the previous stages, violating standard\nstatistical theory. We develop methods for estimating the intervention effects\nin a LAGO study. We prove consistency and asymptotic normality using a novel\ncoupling argument, ensuring the validity of the test for the hypothesis of no\noverall intervention effect. We develop a confidence set for the optimal\nintervention package and confidence bands for the success probabilities under\nalternative package compositions. We illustrate our methods in the BetterBirth\nStudy, which aimed to improve maternal and neonatal outcomes among 157,689\nbirths in Uttar Pradesh, India through a complex, multi-component intervention\npackage.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 05:29:53 GMT"}, {"version": "v2", "created": "Wed, 23 Jan 2019 08:43:31 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Nevo", "Daniel", ""], ["Lok", "Judith J.", ""], ["Spiegelman", "Donna", ""]]}, {"id": "1808.06316", "submitter": "Saisai Ma", "authors": "Saisai Ma, Jiuyong Li, Lin Liu, Thuc Duy Le", "title": "Discovering Context Specific Causal Relationships", "comments": "This paper has been accepted by Intelligent Data Analysis", "journal-ref": "Intelligent Data Analysis 23(4), 2019", "doi": null, "report-no": null, "categories": "cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing need of personalised decision making, such as\npersonalised medicine and online recommendations, a growing attention has been\npaid to the discovery of the context and heterogeneity of causal relationships.\nMost existing methods, however, assume a known cause (e.g. a new drug) and\nfocus on identifying from data the contexts of heterogeneous effects of the\ncause (e.g. patient groups with different responses to the new drug). There is\nno approach to efficiently detecting directly from observational data context\nspecific causal relationships, i.e. discovering the causes and their contexts\nsimultaneously. In this paper, by taking the advantages of highly efficient\ndecision tree induction and the well established causal inference framework, we\npropose the Tree based Context Causal rule discovery (TCC) method, for\nefficient exploration of context specific causal relationships from data.\nExperiments with both synthetic and real world data sets show that TCC can\neffectively discover context specific causal rules from the data.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 06:03:47 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Ma", "Saisai", ""], ["Li", "Jiuyong", ""], ["Liu", "Lin", ""], ["Le", "Thuc Duy", ""]]}, {"id": "1808.06367", "submitter": "Clement Abi Nader", "authors": "Clement Abi Nader, Nicholas Ayache, Philippe Robert, and Marco Lorenzi\n  (for the Alzheimer's Disease Neuroimaging Initiative)", "title": "Alzheimer's Disease Modelling and Staging through Independent Gaussian\n  Process Analysis of Spatio-Temporal Brain Changes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alzheimer's disease (AD) is characterized by complex and largely unknown\nprogression dynamics affecting the brain's morphology. Although the disease\nevolution spans decades, to date we cannot rely on long-term data to model the\npathological progression, since most of the available measures are on a\nshort-term scale. It is therefore difficult to understand and quantify the\ntemporal progression patterns affecting the brain regions across the AD\nevolution. In this work, we tackle this problem by presenting a generative\nmodel based on probabilistic matrix factorization across temporal and spatial\nsources. The proposed method addresses the problem of disease progression\nmodelling by introducing clinically-inspired statistical priors. To promote\nsmoothness in time and model plausible pathological evolutions, the temporal\nsources are defined as monotonic and independent Gaussian Processes. We also\nestimate an individual time-shift parameter for each patient to automatically\nposition him/her along the sources time-axis. To encode the spatial continuity\nof the brain sub-structures, the spatial sources are modeled as Gaussian random\nfields. We test our algorithm on grey matter maps extracted from brain\nstructural images. The experiments highlight differential temporal progression\npatterns mapping brain regions key to the AD pathology, and reveal a\ndisease-specific time scale associated with the decline of volumetric\nbiomarkers across clinical stages.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 09:54:53 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Nader", "Clement Abi", "", "for the Alzheimer's Disease Neuroimaging Initiative"], ["Ayache", "Nicholas", "", "for the Alzheimer's Disease Neuroimaging Initiative"], ["Robert", "Philippe", "", "for the Alzheimer's Disease Neuroimaging Initiative"], ["Lorenzi", "Marco", "", "for the Alzheimer's Disease Neuroimaging Initiative"]]}, {"id": "1808.06440", "submitter": "Tony Wong", "authors": "Tony E. Wong", "title": "An Integration and Assessment of Covariates of Nonstationary Storm Surge\n  Statistical Behavior by Bayesian Model Averaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Projections of storm surge return levels are a basic requirement for\neffective management of coastal risks. A common approach to estimate hazards\nposed by extreme sea levels is to use a statistical model, which may use a time\nseries of a climate variable as a covariate to modulate the statistical model\nand account for potentially nonstationary storm surge behavior. Previous work\nusing nonstationary statistical approaches, however, has demonstrated the\nimportance of accounting for the many inherent modeling uncertainties.\nAdditionally, previous assessments of coastal flood hazard using statistical\nmodeling have typically relied on a single climate covariate, which likely\nleaves out important processes and leads to potential biases. Here, I employ\nupon a recently developed approach to integrate stationary and nonstationary\nstatistical models, and examine the effects of choice of covariate time series\non projected flood hazard. Furthermore, I expand upon this approach by\ndeveloping a nonstationary storm surge statistical model that makes use of\nmultiple covariate time series: global mean temperature, sea level, North\nAtlantic Oscillation index and time. I show that a storm surge model that\naccounts for additional processes raises the projected 100-year storm surge\nreturn level by up to 23 centimeters relative to a stationary model or one that\nemploys a single covariate time series. I find that the total marginal model\nlikelihood associated with each set of nonstationary models given by the\ncandidate covariates, as well as a stationary model, is about 20%. These\nresults shed light on how best to account for potential nonstationary coastal\nsurge behavior, and incorporate more processes into surge projections. By\nincluding a wider range of physical process information and considering\nnonstationary behavior, these methods will better enable modeling efforts to\ninform coastal risk management.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 13:25:12 GMT"}, {"version": "v2", "created": "Sat, 25 Aug 2018 23:25:20 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Wong", "Tony E.", ""]]}, {"id": "1808.06610", "submitter": "Remco Dijkman", "authors": "Rodrigo Goncalves, Rui J. de Almeida, Remco M. Dijkman", "title": "Predicting Stochastic Travel Times based on High-Volume Floating Car\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transportation planning depends on predictions of the travel times between\nloading and unloading locations. While accurate techniques exist for making\ndeterministic predictions of travel times based on real-world data, making\nstochastic predictions remains an open issue. This paper aims to fill this gap\nby showing how floating car data from TomTom can be used to make stochastic\npredictions of travel times. It also shows how these predictions are affected\nby choices that can be made with respect to the level of aggregation of the\ndata in space and time, and by choices regarding the dependence between travel\ntimes on different parts of the route.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 13:34:34 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Goncalves", "Rodrigo", ""], ["de Almeida", "Rui J.", ""], ["Dijkman", "Remco M.", ""]]}, {"id": "1808.06718", "submitter": "Edward Frees", "authors": "Edward Frees (for the Actuarial Community)", "title": "Loss Data Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Loss Data Analytics is an interactive, online, freely available text. The\nidea behind the name Loss Data Analytics is to integrate classical loss data\nmodels from applied probability with modern analytic tools. In particular, we\nseek to recognize that big data (including social media and usage based\ninsurance) are here and high speed computation is readily available.\n  The online version contains many interactive objects (quizzes, computer\ndemonstrations, interactive graphs, video, and the like) to promote deeper\nlearning. A subset of the book is available for offline reading in pdf and EPUB\nformats. The online text will be available in multiple languages to promote\naccess to a worldwide audience.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 23:11:41 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Frees", "Edward", "", "for the Actuarial Community"]]}, {"id": "1808.06798", "submitter": "Veronica Vinciotti Dr", "authors": "Elisa Tosetti and Veronica Vinciotti", "title": "A computationally efficient correlated mixed Probit for credit risk\n  modelling", "comments": null, "journal-ref": null, "doi": "10.1111/rssc.12352", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed Probit models are widely applied in many fields where prediction of a\nbinary response is of interest. Typically, the random effects are assumed to be\nindependent but this is seldom the case for many real applications. In the\ncredit risk application considered in this paper, random effects are present at\nthe level of industrial sectors and they are expected to be correlated due to\ninter-firm credit links inducing dependencies in the firms' risk to default.\nUnfortunately, existing inferential procedures for correlated mixed Probit\nmodels are computationally very intensive already for a moderate number of\neffects. Borrowing from the literature on large network inference, we propose\nan efficient Expectation-Maximization algorithm for unconstrained and penalised\nlikelihood estimation and derive the asymptotic standard errors of the\nestimates. An extensive simulation study shows that the proposed approach\nenjoys substantial computational gains relative to standard Monte Carlo\napproaches, while still providing accurate parameter estimates. Using data on\nnearly 64,000 accounts for small and medium-sized enterprises in the United\nKingdom in 2013 across 14 industrial sectors, we find that accounting for\nnetwork effects via a correlated mixed Probit model increases significantly the\ndefault prediction power of the model compared to conventional default\nprediction models, making efficient inferential procedures for these models\nparticularly useful in this field.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 08:41:25 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Tosetti", "Elisa", ""], ["Vinciotti", "Veronica", ""]]}, {"id": "1808.06863", "submitter": "Berthold-Georg Englert", "authors": "Yanwu Gu, Weijun Li, Michael Evans, and Berthold-Georg Englert", "title": "Very strong evidence in favor of quantum mechanics and against local\n  hidden variables from a Bayesian analysis", "comments": "18 pages, 5 figures, 17 tables; v2 is final", "journal-ref": "Phys. Rev. A 99, 022112 (2019)", "doi": "10.1103/PhysRevA.99.022112", "report-no": null, "categories": "quant-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The data of four recent experiments --- conducted in Delft, Vienna, Boulder,\nand Munich with the aim of refuting nonquantum hidden-variables alternatives to\nthe quantum-mechanical description --- are evaluated from a Bayesian\nperspective of what constitutes evidence in statistical data. We find that each\nof the experiments provides strong, or very strong, evidence in favor of\nquantum mechanics and against the nonquantum alternatives. This Bayesian\nanalysis supplements the previous non-Bayesian ones, which refuted the\nalternatives on the basis of small p-values, but could not support quantum\nmechanics.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 12:14:11 GMT"}, {"version": "v2", "created": "Tue, 22 Jan 2019 08:48:33 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Gu", "Yanwu", ""], ["Li", "Weijun", ""], ["Evans", "Michael", ""], ["Englert", "Berthold-Georg", ""]]}, {"id": "1808.06884", "submitter": "Donghui Yan", "authors": "Donghui Yan and Gary E. Davis", "title": "The Turtleback Diagram for Conditional Probability", "comments": "23 pages, 14 figures", "journal-ref": "The Open Journal of Statistics, Vol 8(4), 684-705, 2018", "doi": "10.4236/ojs.2018.84045", "report-no": null, "categories": "stat.AP cs.GR stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We elaborate on an alternative representation of conditional probability to\nthe usual tree diagram. We term the representation `turtleback diagram' for its\nresemblance to the pattern on turtle shells. Adopting the set theoretic view of\nevents and the sample space, the turtleback diagram uses elements from Venn\ndiagrams---set intersection, complement and partition---for conditioning, with\nthe additional notion that the area of a set indicates probability whereas the\nratio of areas for conditional probability. Once parts of the diagram are drawn\nand properly labeled, the calculation of conditional probability involves only\nsimple arithmetic on the area of relevant sets. We discuss turtleback diagrams\nin relation to other visual representations of conditional probability, and\ndetail several scenarios in which turtleback diagrams prove useful. By the\nequivalence of recursive space partition and the tree, the turtleback diagram\nis seen to be equally expressive as the tree diagram for representing abstract\nconcepts. We also provide empirical data on the use of turtleback diagrams with\nundergraduate students in elementary statistics or probability courses.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 13:05:13 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Yan", "Donghui", ""], ["Davis", "Gary E.", ""]]}, {"id": "1808.06999", "submitter": "Behram Wali", "authors": "Behram Wali, Asad Khattak, Aemal Khattak", "title": "A Heterogeneity Based Case-Control Analysis of Motorcyclist Injury\n  Crashes: Evidence from Motorcycle Crash Causation Study", "comments": null, "journal-ref": "Accident Analysis & Prevention, 119, 202-214 (2018)", "doi": "10.1016/j.aap.2018.07.024", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main objective of this study is to quantify how different\npolicy-sensitive factors are associated with risk of motorcycle injury crashes,\nwhile controlling for rider-specific, psycho-physiological, and other\nobserved/unobserved factors. The analysis utilizes data from a matched\ncase-control design collected through the FHWA Motorcycle Crash Causation\nStudy. In particular, 351 cases (motorcyclists involved in injury crashes) are\nanalyzed vis-a-vis similarly-at-risk 702 matched controls (motorcyclists not\ninvolved in crashes). The paper presents a novel heterogeneity based\nstatistical analysis that accounts for the possibility of both within and\nbetween matched case-control variations. Overall, the correlations between key\nrisk factors and injury crash propensity exhibit significant observed and\nunobserved heterogeneity. The results of best-fit random parameters logit model\nwith heterogeneity-in-means show that riders with partial helmet coverage have\na significantly lower risk of injury crash involvement. Lack of motorcycle\nrider conspicuity captured by dark (red) upper body clothing is associated with\nsignificantly higher injury crash risk (odds ratio 3.87). Importantly, a rider\nmotorcycle-oriented lower clothing significantly lowers the odds of injury\ncrash involvement. Formal motorcycle driving training in recent years was\nassociated with lower injury crash propensity. Finally, riders with less sleep\nprior to crash/interview exhibited 1.97 times higher odds of crash involvement.\nMethodologically, the correlations of several rider, exposure, apparel, and\nriding history related factors with crash risk do not only vary in magnitude\nbut in direction as well. The study results indicate the need to develop\nappropriate countermeasures, such as refresher motorcycle training courses,\nprevention of sleep-deprived/fatigued riding, and riding under the influence of\nalcohol/drugs.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 00:13:51 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Wali", "Behram", ""], ["Khattak", "Asad", ""], ["Khattak", "Aemal", ""]]}, {"id": "1808.07138", "submitter": "Jorge Lopez Mr.", "authors": "Jorge H. Lopez", "title": "Geometrical effects on mobility", "comments": "7 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we analyze the effect of randomly deleting streets of a\nsynthetic city on the statistics of displacements. Our city is constituted\ninitially by a set of streets that form a regular tessellation of the euclidean\nplane. Therefore we will have three types of cities, formed by squares,\ntriangles or hexagons. We studied the complementary cumulative distribution\nfunction for displacements (CCDF). For the whole set of streets the CCDF is a\nstretched exponential, and as streets are deleted this function becomes a\nlinear function and then two clear different exponentials. This behavior is\nqualitatively the same for all the tessellations. Most of this functions has\nbeen reported in the literature when studying the displacements of individuals\nbased on cell data trajectories and GPS information. However, in the light of\nthis work, the appearance of different functions for displacements CCDF can be\nattributed to the connectivity of the underlying street network. It is\nremarkably that for some proportion of streets we got a linear function for\nsuch function, and as far as we know this behavior has not been reported nor\nconsidered. Therefore, it is advisable to analyze experimental in the light of\nconnectivity of the street network to make correlations with the present work.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 21:31:25 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Lopez", "Jorge H.", ""]]}, {"id": "1808.07221", "submitter": "Kaspar Rufibach", "authors": "Ulrich Beyer, David Dejardin, Matthias Meller, Kaspar Rufibach, Hans\n  Ulrich Burger", "title": "A multistate model for early decision making in oncology", "comments": null, "journal-ref": "Biom. J., 62(3), 550-567, 2020", "doi": "10.1002/bimj.201800250", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of oncology drugs progresses through multiple phases, where\nafter each phase a decision is made about whether to move a molecule forward.\nEarly phase efficacy decisions are often made on the basis of single arm\nstudies based on RECIST tumor response as endpoint. This decision rules are\nimplicitly assuming some form of surrogacy between tumor response and long-term\nendpoints like progression-free survival (PFS) or overall survival (OS). The\nsurrogacy is most often assessed as weak, but sufficient to allow a rapid\ndecision making as early phase studies lack the survival follow up and number\nof patients to properly assess PFS or OS. With the emergence of therapies with\nnew mechanisms of action, for which the link between RECIST tumor response and\nlong-term endpoints is either not accessible yet because not enough data is\navailable to perform a meta-regression, or the link is weaker than with\nclassical chemotherapies, tumor response based rules may not be optimal. In\nthis paper, we explore the use of a multistate model for decision making based\non single-arm early phase trials. The multistate model allows to account for\nmore information than the simple RECIST response status, namely, the time to\nget to response, the duration of response, the PFS time and time to death. We\npropose to base the decision on efficacy on the OS hazard ratio (HR), predicted\nfrom a multistate model based on early phase data with limited survival\nfollow-up, combined with historical control data. Using three case studies and\nsimulations, we illustrate the feasibility of the estimation of the OS HR using\na multistate model based on limited data from early phase studies. We argue\nthat, in the presence of limited follow up and small sample size, and on\nassumptions within the multistate model, the OS prediction is acceptable and\nmay lead to better decisions for continuing the development of a drug.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 05:15:53 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Beyer", "Ulrich", ""], ["Dejardin", "David", ""], ["Meller", "Matthias", ""], ["Rufibach", "Kaspar", ""], ["Burger", "Hans Ulrich", ""]]}, {"id": "1808.07265", "submitter": "Mohamed Laib", "authors": "Luciano Telesca, Mohamed Laib, Fabian Guignard, Dasaraden Mauree,\n  Mikhail Kanevski", "title": "Linearity versus non-linearity in high frequency multilevel wind time\n  series measured in urban areas", "comments": null, "journal-ref": null, "doi": "10.1016/j.chaos.2019.02.002", "report-no": null, "categories": "stat.AP physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, high frequency wind time series measured at different heights\nfrom the ground (from 5.5 to 25.5 meters) in an urban area were investigated.\nThe spectrum of each series is characterized by a power-law behaviour at low\nfrequency range, with a mean spectral exponent of about 1.5, which is rather\nconsistent with the Kolmogorov spectrum of atmospheric turbulence. The\ndetrended fluctuation analysis was applied on the magnitude and sign series of\nthe increments of wind speed, in order to get information about the linear and\nnonlinear dynamics of the time series. Both the sign series and magnitude\nseries are characterized by two timescale ranges; in particular the scaling\nexponent of the magnitude series in the high timescale range seems to be\nrelated with the height of the sensor. This study aims to understand better\nhigh frequency wind speed in urban areas and to disclose the underlying\nmechanism governing the wind fluctuations at different heights.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 08:11:33 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Telesca", "Luciano", ""], ["Laib", "Mohamed", ""], ["Guignard", "Fabian", ""], ["Mauree", "Dasaraden", ""], ["Kanevski", "Mikhail", ""]]}, {"id": "1808.07347", "submitter": "Jingxing Wang", "authors": "Jingxing Wang, Abdullah Alshelahi, Mingdi You, Eunshin Byon, Romesh\n  Saigal", "title": "Integrative Density Forecast and Uncertainty Quantification of Wind\n  Power Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The volatile nature of wind power generation creates challenges in achieving\nsecure power grid operations. It is, therefore, necessary to make accurate wind\npower prediction and its uncertainty quantification. Wind power forecasting\nusually depends on wind speed prediction and the wind-to-power conversion\nprocess. However, most current wind power prediction models only consider\nportions of the uncertainty. This paper develops an integrative framework for\npredicting wind power density, considering uncertainties arising from both wind\nspeed prediction and the wind-to-power conversion process. Specifically, we\nmodel wind speed using the inhomogeneous Geometric Brownian Motion and convert\nthe wind speed prediction density into the wind power density in a closed-form.\nThe resulting wind power density allows quantifying prediction uncertainties\nthrough prediction intervals. To forecast the power output, we minimize the\nexpected prediction cost with (unequal) penalties on the overestimation and\nunderestimation. We show the predictive power of the proposed approach using\ndata from multiple operating wind farms located at different sites.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 13:27:23 GMT"}, {"version": "v2", "created": "Sun, 27 Sep 2020 17:25:38 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Wang", "Jingxing", ""], ["Alshelahi", "Abdullah", ""], ["You", "Mingdi", ""], ["Byon", "Eunshin", ""], ["Saigal", "Romesh", ""]]}, {"id": "1808.07377", "submitter": "Raymundo Arroyave", "authors": "Pejman Honarmandi, Alex Solomou, Raymundo Arroyave, Dimitris Lagoudas", "title": "Parametric Analysis of a Phenomenological Constitutive Model for\n  Thermally Induced Phase Transformation in Ni-Ti Shape Memory Alloys", "comments": "26 pages, 5 figures, 4 tables", "journal-ref": null, "doi": "10.1088/1361-651X/ab0040", "report-no": null, "categories": "stat.AP cond-mat.mtrl-sci", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this work, a thermo-mechanical model that predicts the actuation response\nof shape memory alloys is probabilistically calibrated against three\nexperimental data sets simultaneously. Before calibration, a design of\nexperiments (DOE) has been performed in order to identify the parameters most\ninfluential on the actuation response of the system and thus reduce the\ndimensionality of the problem. Subsequently, uncertainty quantification (UQ) of\nthe influential parameters was carried out through Bayesian Markov Chain Monte\nCarlo (MCMC). The assessed uncertainties in the model parameters were then\npropagated to the transformation strain-temperature hysteresis curves (the\nmodel output) using first an approximate approach based on the\nvariance-covariance matrix of the MCMC-calibrated model parameters and then an\nexplicit propagation of uncertainty through MCMC-based sampling. Results show\ngood agreement between model and experimental hysteresis loops after\nprobabilistic MCMC calibration such that the experimental data are situated\nwithin 95% Bayesian confidence intervals. The application of the MCMC-based\nUQ/UP approach in decision making for experimental design has also been shown\nby comparing the information that can be gained from running replicas around a\nsingle new experimental condition versus running experiments in different\nregions of the experimental space.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 23:39:03 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Honarmandi", "Pejman", ""], ["Solomou", "Alex", ""], ["Arroyave", "Raymundo", ""], ["Lagoudas", "Dimitris", ""]]}, {"id": "1808.07662", "submitter": "Sarit Agami", "authors": "Sarit Agami, David M. Zucker and Donna Spiegelman", "title": "Estimation in the Cox Survival Regression Model with Covariate\n  Measurement Error and a Changepoint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Cox regression model is a popular model for analyzing the relationship\nbetween a covariate and a survival endpoint. The standard Cox model assumes a\nconstant covariate effect across the entire covariate domain. However, in many\nepidemiological and other applications, the covariate of main interest is\nsubject to a threshold effect: a change in the slope at a certain point within\nthe covariate domain. Often, the covariate of interest is subject to some\ndegree of measurement error. In this paper, we study measurement error\ncorrection in the case where the threshold is known. Several bias correction\nmethods are examined: two versions of regression calibration (RC1 and RC2, the\nlatter of which is new), two methods based on the induced relative risk under a\nrare event assumption (RR1 and RR2, the latter of which is new), a maximum\npseudo-partial likelihood estimator (MPPLE), and simulation-extrapolation\n(SIMEX). We develop the theory, present simulations comparing the methods, and\nillustrate their use on data concerning the relationship between chronic air\npollution exposure to particulate matter PM10 and fatal myocardial infarction\n(Nurses Health Study (NHS)), and on data concerning the effect of a subject's\nlong-term underlying systolic blood pressure level on the risk of\ncardiovascular disease death (Framingham Heart Study (FHS)). The simulations\nindicate that the best methods are RR2 and MPPLE.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 08:23:19 GMT"}, {"version": "v2", "created": "Fri, 30 Aug 2019 04:50:54 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Agami", "Sarit", ""], ["Zucker", "David M.", ""], ["Spiegelman", "Donna", ""]]}, {"id": "1808.07667", "submitter": "Petra Friederichs", "authors": "Florian Kapp and Petra Friederichs and Sebastian Brune and Michael\n  Weniger", "title": "Spatial verification of high-resolution ensemble precipitation forecasts\n  using local wavelet spectra", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The purpose of this study is to perform verification of the structural\ncharacteristics of high-resolution spatial forecasts without relying on an\nobject identification algorithm. To this end, a wavelet approach developed for\nimage texture analysis is applied to an ensemble of high-resolution\nquantitative precipitation forecasts. The forecasts are verified against\nestimates from a high-resolution regional reanalysis with a similar model\nversion. The wavelet approach estimates an averaged wavelet spectrum for each\nspatial field of the ensemble forecasts and the reanalysis, thereby removing\nall information on the localization of precipitation and investigating solely\nthe overall structure of forecasts and reanalysis. In order to assess skill\nusing a multivariate score, an additional reduction of dimensionality is\nneeded. This is performed using singular vectors from a linear discriminant\nanalysis as it favors data compression in the direction where the ensemble is\nmost discriminating. We discuss implications of this strategy, show that the\naveraged wavelet spectra give valuable information on forecast performance. The\nskill difference between a so-called perfect forecast using for verification a\nmember of the ensemble, and the non-perfect forecast using the reanalysis\npoints to significant deficiencies of the forecast ensemble. Overall, the\ndiscriminating power solely based on global spectral information is remarkable,\nand the COSMO-DE-EPS is a quite good forecast ensemble with respect to the\nreanalysis.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 08:42:32 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Kapp", "Florian", ""], ["Friederichs", "Petra", ""], ["Brune", "Sebastian", ""], ["Weniger", "Michael", ""]]}, {"id": "1808.07795", "submitter": "Geoffrey Wodtke", "authors": "Geoffrey T. Wodtke, Zahide Alaca, and Xiang Zhou", "title": "Regression-with-residuals Estimation of Marginal Effects: A Method of\n  Adjusting for Treatment-induced Confounders that may also be Moderators", "comments": "68 pages, 6 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Treatment-induced confounders complicate analyses of time-varying treatment\neffects and causal mediation. Conditioning on these variables naively to\nestimate marginal effects may inappropriately block causal pathways and may\ninduce spurious associations between treatment and the outcome, leading to\nbias. Although several methods for estimating marginal effects avoid these\ncomplications, including inverse-probability-of-treatment-weighted (IPTW)\nestimation of marginal structural models (MSMs) as well as g- and\nregression-with-residuals (RWR) estimation of highly constrained structural\nnested mean models (SNMMs), each suffers from a set of nontrivial limitations.\nSpecifically, IPTW estimation is inefficient, is difficult to use with\ncontinuous treatments or mediators, and may suffer from finite-sample bias,\nwhile g- and RWR estimation of highly constrained SNMMs for marginal effects\nare premised on the unrealistic assumption that there is no effect moderation.\nIn this study, we adapt the method of RWR to estimate marginal effects with a\nset of moderately constrained SNMMs that accommodate several types of\ntreatment-by-confounder and/or mediator-by-confounder interaction, thereby\nrelaxing the assumption of no effect moderation. Through a series of simulation\nexperiments and empirical examples, we show that this approach outperforms IPTW\nestimation of MSMs as well as both g- and RWR estimation of highly constrained\nSNMMs in which effect moderation is assumed away.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 15:14:18 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Wodtke", "Geoffrey T.", ""], ["Alaca", "Zahide", ""], ["Zhou", "Xiang", ""]]}, {"id": "1808.07804", "submitter": "Bradly Stadie", "authors": "S\\\"oren R. K\\\"unzel, Bradly C. Stadie, Nikita Vemuri, Varsha\n  Ramakrishnan, Jasjeet S. Sekhon, Pieter Abbeel", "title": "Transfer Learning for Estimating Causal Effects using Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop new algorithms for estimating heterogeneous treatment effects,\ncombining recent developments in transfer learning for neural networks with\ninsights from the causal inference literature. By taking advantage of transfer\nlearning, we are able to efficiently use different data sources that are\nrelated to the same underlying causal mechanisms. We compare our algorithms\nwith those in the extant literature using extensive simulation studies based on\nlarge-scale voter persuasion experiments and the MNIST database. Our methods\ncan perform an order of magnitude better than existing benchmarks while using a\nfraction of the data.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 15:27:14 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["K\u00fcnzel", "S\u00f6ren R.", ""], ["Stadie", "Bradly C.", ""], ["Vemuri", "Nikita", ""], ["Ramakrishnan", "Varsha", ""], ["Sekhon", "Jasjeet S.", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1808.07843", "submitter": "Johannes Keller", "authors": "Johannes Keller, Harrie-Jan Hendricks Franssen, Gabriele Marquart", "title": "Comparing seven variants of the Ensemble Kalman Filter: How many\n  synthetic experiments are needed?", "comments": null, "journal-ref": "Water Resources Research, 2018", "doi": "10.1029/2018WR023374", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Ensemble Kalman Filter (EnKF) is a popular estimation technique in the\ngeosciences. It is used as a numerical tool for state vector prognosis and\nparameter estimation. The EnKF can, for example, help to evaluate the\ngeothermal potential of an aquifer. In such applications, the EnKF is often\nused with small or medium ensemble sizes. It is therefore of interest to\ncharacterize the EnKF behavior for these ensemble sizes. For seven ensemble\nsizes (50, 70, 100, 250, 500, 1000, 2000) and seven EnKF-variants (Damped,\nIterative, Local, Hybrid, Dual, Normal Score and Classical EnKF), we computed\n1000 synthetic parameter estimation experiments for two set-ups: a 2D tracer\ntransport problem and a 2D flow problem with one injection well. For each\nmodel, the only difference among synthetic experiments was the generated set of\nrandom permeability fields. The 1000 synthetic experiments allow to calculate\nthe pdf of the RMSE of the characterization of the permeability field.\nComparing mean RMSEs for different EnKF-variants, ensemble sizes and\nflow/transport set-ups suggests that multiple synthetic experiments are needed\nfor a solid performance comparison. In this work, 10 synthetic experiments were\nneeded to correctly distinguish RMSE differences between EnKF-variants smaller\nthan 10%. For detecting RMSE differences smaller than 2%, 100 synthetic\nexperiments were needed for ensemble sizes 50, 70, 100 and 250. The overall\nranking of the EnKF-variants is strongly dependent on the physical model set-up\nand the ensemble size.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 17:05:11 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Keller", "Johannes", ""], ["Franssen", "Harrie-Jan Hendricks", ""], ["Marquart", "Gabriele", ""]]}, {"id": "1808.07861", "submitter": "Dalia Ghanem", "authors": "Xiaomeng Cui, Dalia Ghanem and Todd Kuffner", "title": "On model selection criteria for climate change impact studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Climate change impact studies inform policymakers on the estimated damages of\nfuture climate change on economic, health and other outcomes. In most studies,\nan annual outcome variable is observed, e.g. agricultural yield, annual\nmortality or gross domestic product, along with a higher-frequency regressor,\ne.g. daily temperature. While applied researchers tend to consider multiple\nmodels to characterize the relationship between the outcome and the\nhigh-frequency regressor, to inform policy a choice between the damage\nfunctions implied by the different models has to be made. This paper formalizes\nthe model selection problem in this empirical setting and provides conditions\nfor the consistency of Monte Carlo Cross-validation and generalized information\ncriteria. A simulation study illustrates the theoretical results and points to\nthe relevance of the signal-to-noise ratio for the finite-sample behavior of\nthe model selection criteria. Two empirical applications with starkly different\nsignal-to-noise ratios illustrate the practical implications of the formal\nanalysis on model selection criteria provided in this paper.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 17:48:00 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 17:17:57 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Cui", "Xiaomeng", ""], ["Ghanem", "Dalia", ""], ["Kuffner", "Todd", ""]]}, {"id": "1808.07989", "submitter": "Charles Onu", "authors": "Charles C. Onu, Lara J. Kanbar, Wissam Shalish, Karen A. Brown,\n  Guilherme M. Sant'Anna, Robert E. Kearney, Doina Precup", "title": "A Semi-Markov Chain Approach to Modeling Respiratory Patterns Prior to\n  Extubation in Preterm Infants", "comments": "Published in: 2017 39th Annual International Conference of the IEEE\n  Engineering in Medicine and Biology Society (EMBC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After birth, extremely preterm infants often require specialized respiratory\nmanagement in the form of invasive mechanical ventilation (IMV). Protracted IMV\nis associated with detrimental outcomes and morbidities. Premature extubation,\non the other hand, would necessitate reintubation which is risky, technically\nchallenging and could further lead to lung injury or disease. We present an\napproach to modeling respiratory patterns of infants who succeeded extubation\nand those who required reintubation which relies on Markov models. We compare\nthe use of traditional Markov chains to semi-Markov models which emphasize\ncross-pattern transitions and timing information, and to multi-chain Markov\nmodels which can concisely represent non-stationarity in respiratory behavior\nover time. The models we developed expose specific, unique similarities as well\nas vital differences between the two populations.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 03:04:11 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Onu", "Charles C.", ""], ["Kanbar", "Lara J.", ""], ["Shalish", "Wissam", ""], ["Brown", "Karen A.", ""], ["Sant'Anna", "Guilherme M.", ""], ["Kearney", "Robert E.", ""], ["Precup", "Doina", ""]]}, {"id": "1808.08086", "submitter": "Alberto Sorrentino", "authors": "Gianvittorio Luria, Dunja Duran, Elisa Visani, Sara Sommariva, Fabio\n  Rotondi, Davide Rossi Sebastiano, Ferruccio Panzica, Michele Piana, Alberto\n  Sorrentino", "title": "Bayesian Multi--Dipole Modeling in the Frequency Domain", "comments": null, "journal-ref": "Journal of Neuroscience Methods Volume 312, 15 January 2019, Pages\n  27-36", "doi": "10.1016/j.jneumeth.2018.11.007", "report-no": null, "categories": "q-bio.QM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Magneto- and Electro-encephalography record the electromagnetic\nfield generated by neural currents with high temporal frequency and good\nspatial resolution, and are therefore well suited for source localization in\nthe time and in the frequency domain. In particular, localization of the\ngenerators of neural oscillations is very important in the study of cognitive\nprocesses in the healthy and in the pathological brain.\n  New method: We introduce the use of a Bayesian multi-dipole localization\nmethod in the frequency domain. Given the Fourier Transform of the data at one\nor multiple frequencies and/or trials, the algorithm approximates numerically\nthe posterior distribution with Monte Carlo techniques.\n  Results: We use synthetic data to show that the proposed method behaves well\nunder a wide range of experimental conditions, including low signal-to-noise\nratios and correlated sources. We use dipole clusters to mimic the effect of\nextended sources. In addition, we test the algorithm on real MEG data to\nconfirm its feasibility.\n  Comparison with existing method(s): Throughout the whole study, DICS (Dynamic\nImaging of Coherent Sources) is used systematically as a benchmark. The two\nmethods provide similar general pictures; the posterior distributions of the\nBayesian approach contain much richer information at the price of a higher\ncomputational cost.\n  Conclusions: The Bayesian method described in this paper represents a\nreliable approach for localization of multiple dipoles in the frequency domain.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 10:55:07 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2018 14:21:54 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Luria", "Gianvittorio", ""], ["Duran", "Dunja", ""], ["Visani", "Elisa", ""], ["Sommariva", "Sara", ""], ["Rotondi", "Fabio", ""], ["Sebastiano", "Davide Rossi", ""], ["Panzica", "Ferruccio", ""], ["Piana", "Michele", ""], ["Sorrentino", "Alberto", ""]]}, {"id": "1808.08142", "submitter": "Marta Blangiardo", "authors": "Marta Blangiardo, Monica Pirani, Lauren Kanapka, Anna Hansell, Gary\n  Fuller", "title": "A hierarchical modelling approach to assess multi pollutant effects in\n  time-series studies", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0212565", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When assessing the short term effect of air pollution on health outcomes, it\nis common practice to consider one pollutant at a time, due to their high\ncorrelation. Multi pollutant methods have been recently proposed, mainly\nconsisting of collapsing the different pollutants into air quality indexes or\nclustering the pollutants and then evaluating the effect of each cluster on the\nhealth outcome. A major drawback of such approaches is that it is not possible\nto evaluate the health impact of each pollutant. In this paper we propose the\nuse of the Bayesian hierarchical framework to deal with multi pollutant\nconcentration in a two-component model: a pollutant model is specified to\nestimate the `true' concentration values for if your each pollutant and then\nsuch concentration is linked to the health outcomes in a time series\nperspective. Through a simulation study we evaluate the model performance and\nwe apply the modelling framework to investigate the effect of six pollutants on\ncardiovascular mortality in Greater London in 2011-2012.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 14:03:35 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Blangiardo", "Marta", ""], ["Pirani", "Monica", ""], ["Kanapka", "Lauren", ""], ["Hansell", "Anna", ""], ["Fuller", "Gary", ""]]}, {"id": "1808.08199", "submitter": "Yili Hong", "authors": "Chris Gotwalt, Li Xu, Yili Hong and William Q. Meeker", "title": "Applications of the Fractional-Random-Weight Bootstrap", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bootstrap, based on resampling, has, for several decades, been a widely\nused method for computing confidence intervals for applications where no exact\nmethod is available and when sample sizes are not large enough to be able to\nrely on easy-to-compute large-sample approximate methods, such a Wald\n(normal-approximation) confidence intervals. Simulation based bootstrap\nintervals have been proven useful in that their actual coverage probabilities\nare close to the nominal confidence level in small samples. Small samples\nanalytical approximations such as the Wald method, however, tend to have\ncoverage probabilities that greatly exceed the nominal confidence level. There\nare, however, many applications where the resampling bootstrap method cannot be\nused. These include situations where the data are heavily censored, logistic\nregression when the success response is a rare event or where there is\ninsufficient mixing of successes and failures across the explanatory\nvariable(s), and designed experiments where the number of parameters is close\nto the number of observations. The thing that these three situations have in\ncommon is that there may be a substantial proportion of the resamples where is\nnot possible to estimate all of the parameters in the model. This paper reviews\nthe fractional-random-weight bootstrap method and demonstrates how it can be\nused to avoid these problems and construct confidence intervals. For the\nexamples, it is seen that the fractional-random-weight bootstrap method is easy\nto use and has advantages over the resampling method in many challenging\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 16:29:21 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Gotwalt", "Chris", ""], ["Xu", "Li", ""], ["Hong", "Yili", ""], ["Meeker", "William Q.", ""]]}, {"id": "1808.08229", "submitter": "Sarit Agami", "authors": "Sarit Agami, David M. Zucker and Donna Spiegelman", "title": "Cox Model with Covariate Measurement Error and Unknown Changepoint", "comments": "26 pages. arXiv admin note: text overlap with arXiv:1808.07662", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard Cox model in survival analysis assumes that the covariate effect\nis constant across the entire covariate domain. However, in many applications,\nthere is interest in considering the possibility that the covariate of main\ninterest is subject to a threshold effect: a change in the slope at a certain\npoint within the covariate domain. Often, the value of this threshold is\nunknown and need to be estimated. In addition, often, the covariate of interest\nis not measured exactly, but rather is subject to some degree of measurement\nerror. In this paper, we discuss estimation of the model parameters under an\nindependent additive error model where the covariate of interesting is measured\nwith error and the potential threshold value in this covariate is unknown. As\nin earlier work which discussed the case of konwn threshold, we study the\nperformance of several bias correction methods: two versions of regression\ncalibration (RC1 and RC2), two versions of the fitting a model for the induced\nrelative risk (RR1 and RR2), maximum pseudo-partial likelihood estimator\n(MPPLE) and simulation-extrapolation (SIMEX). These correction methods are\ncompared with the naive estimator. We develop the relevant theory, present a\nsimulation study comparing the several correction methods, and illustrate the\nuse of the bias correction methods in data from the Nurses Health Study (NHS)\nconcerning the relationship between chronic air pollution exposure to\nparticulate matter of diameter 10 $\\mu$m or less (PM$_{10}$). The simulation\nresults suggest that the best overall choice of bias correction method is\neither the RR2 method or the MPPLE method.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 08:06:08 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Agami", "Sarit", ""], ["Zucker", "David M.", ""], ["Spiegelman", "Donna", ""]]}, {"id": "1808.08286", "submitter": "Michele Scipioni", "authors": "Michele Scipioni, Stefano Pedemonte, Maria Filomena Santarelli and\n  Luigi Landini", "title": "Probabilistic Graphical Modeling approach to dynamic PET direct\n  parametric map estimation and image reconstruction", "comments": "15 pages (main manuscript + supplementary material); submitted for\n  peer-review to IEEE Transactions of Medical Imaging (IEEE-TMI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.data-an physics.med-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of dynamic emission tomography, the conventional processing\npipeline consists of independent image reconstruction of single time frames,\nfollowed by the application of a suitable kinetic model to time activity curves\n(TACs) at the voxel or region-of-interest level. The relatively new field of 4D\nPET direct reconstruction, by contrast, seeks to move beyond this scheme and\nincorporate information from multiple time frames within the reconstruction\ntask. Existing 4D direct models are based on a deterministic description of\nvoxels' TACs, captured by the chosen kinetic model, considering the photon\ncounting process the only source of uncertainty. In this work, we introduce a\nnew probabilistic modeling strategy based on the key assumption that activity\ntime course would be subject to uncertainty even if the parameters of the\nunderlying dynamic process were known. This leads to a hierarchical Bayesian\nmodel, which we formulate using the formalism of Probabilistic Graphical\nModeling (PGM). The inference of the joint probability density function arising\nfrom PGM is addressed using a new gradient-based iterative algorithm, which\npresents several advantages compared to existing direct methods: it is flexible\nto an arbitrary choice of linear and nonlinear kinetic model; it enables the\ninclusion of arbitrary (sub)differentiable priors for parametric maps; it is\nsimpler to implement and suitable to integration in computing frameworks for\nmachine learning. Computer simulations and an application to real patient scan\nshowed how the proposed approach allows us to weight the importance of the\nkinetic model, providing a bridge between indirect and deterministic direct\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 19:44:30 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Scipioni", "Michele", ""], ["Pedemonte", "Stefano", ""], ["Santarelli", "Maria Filomena", ""], ["Landini", "Luigi", ""]]}, {"id": "1808.08299", "submitter": "Charles Onu", "authors": "Charles C. Onu", "title": "Harnessing Infant Cry for swift, cost-effective Diagnosis of Perinatal\n  Asphyxia in low-resource settings", "comments": "Presented at 2014 IEEE Canada International Humanitarian Technology\n  Conference - (IHTC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perinatal Asphyxia is one of the top three causes of infant mortality in\ndeveloping countries, resulting to the death of about 1.2 million newborns\nevery year. At its early stages, the presence of asphyxia cannot be\nconclusively determined visually or via physical examination, but by medical\ndiagnosis. In resource-poor settings, where skilled attendance at birth is a\nluxury, most cases only get detected when the damaging consequences begin to\nmanifest or worse still, after death of the affected infant. In this project,\nwe explored the approach of machine learning in developing a low-cost\ndiagnostic solution. We designed a support vector machine-based pattern\nrecognition system that models patterns in the cries of known asphyxiating\ninfants (and normal infants) and then uses the developed model for\nclassification of `new' infants as having asphyxia or not. Our prototype has\nbeen tested in a laboratory setting to give prediction accuracy of up to\n88.85%. If higher accuracies can be obtained, this research may be a key\ncontributor to the 4th Millennium Development Goal (MDG) of reducing mortality\nin under-five children.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 20:25:01 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Onu", "Charles C.", ""]]}, {"id": "1808.08657", "submitter": "Chaitanya Poolla", "authors": "Chaitanya Poolla, Abraham K. Ishihara", "title": "Localized solar power prediction based on weather data from local\n  history and global forecasts", "comments": "5 pages, 4 figures, 3 tables, 45th IEEE Photovoltaic Specialists\n  Conference (PVSC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent interest in net-zero sustainability for commercial buildings,\nintegration of photovoltaic (PV) assets becomes even more important. This\nintegration remains a challenge due to high solar variability and uncertainty\nin the prediction of PV output. Most existing methods predict PV output using\neither local power/weather history or global weather forecasts, thereby\nignoring either the impending global phenomena or the relevant local\ncharacteristics, respectively. This work proposes to leverage weather data from\nboth local weather history and global forecasts based on time series modeling\nwith exogenous inputs. The proposed model results in eighteen hour ahead\nforecasts with a mean accuracy of $\\approx$ 80\\% and uses data from the\nNational Ocean and Atmospheric Administration's (NOAA) High-Resolution Rapid\nRefresh (HRRR) model.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 01:33:19 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Poolla", "Chaitanya", ""], ["Ishihara", "Abraham K.", ""]]}, {"id": "1808.08776", "submitter": "David Horn", "authors": "D. Horn", "title": "Field Formulation of Parzen Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Parzen window density is a well-known technique, associating Gaussian\nkernels with data points. It is a very useful tool in data exploration, with\nparticular importance for clustering schemes and image analysis. This method is\npresented here within a formalism containing scalar fields, such as the density\nfunction and its potential, and their corresponding gradients. The potential is\nderived from the density through the dependence of the latter on the common\nscale parameter of all Gaussian kernels. The loci of extrema of the density and\npotential scalar fields are points of interest which obey a variation condition\non a novel indicator function. They serve as focal points of clustering methods\ndepending on maximization of the density, or minimization of the potential,\naccordingly. The mixed inter-dependencies of the different fields in d-dim\ndata-space and 1-d scale-space, are discussed. They lead to a Schr\\H{o}dinger\nequation in d-dim, and to a diffusion equation in (d+1)-dim\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 10:45:25 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Horn", "D.", ""]]}, {"id": "1808.08847", "submitter": "Mohamed Laib", "authors": "Luciano Telesca, Fabian Guignard, Mohamed Laib, Mikhail Kanevski", "title": "Analysis of temporal properties of wind extremes", "comments": null, "journal-ref": null, "doi": "10.1016/j.renene.2019.06.089", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 10-minute average wind speed series recorded at 132 stations distributed\nrather homogeneously in the territory of Switzerland are investigated. Wind\nextremes are defined on the base of run theory: fixing a percentile-based\nthreshold of the wind speed distribution, a wind extreme is defined as a\nsequence of consecutive wind values (or duration of the extreme) above the\nthreshold. This definition allows to analyse the sequence of extremes as a\ntemporal point process marked by the duration of the extremes. The average\nprobability density function of the duration of the extremes of the wind speed\nmeasured in Switzerland does not depend on the percentile-based threshold and\ndecrease with the increase of the extreme duration. The time-clustering\nbehaviour of the sequences of the wind extremes was analysed by using the\nglobal and local coefficient of variation and the Allan Factor. The wind\nextremes are globally time-clustered, although they tend to behave as a Poisson\nprocess with the increase of the minimum extreme duration. Locally, the wind\nextremes tend to be clustered for any percentile-based threshold for stations\nlocated above about 2,000 m a.s.l. By using the Allan Factor, it was revealed\nthat wind extremes tend to be clustered even at lower timescales especially for\nthe higher stations.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 13:54:17 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Telesca", "Luciano", ""], ["Guignard", "Fabian", ""], ["Laib", "Mohamed", ""], ["Kanevski", "Mikhail", ""]]}, {"id": "1808.08982", "submitter": "Chenglong Ye", "authors": "Chenglong Ye, Lin Zhang, Mingxuan Han, Yanjia Yu, Bingxin Zhao, Yuhong\n  Yang", "title": "Combining Predictions of Auto Insurance Claims", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at achieving better performance of prediction by combining\ncandidate predictions, with the focus on the highly-skewed auto insurance claim\ncost data. We analyze a version of the Kangaroo Auto Insurance company data,\nand incorporate different combining methods under five measurements of\nprediction accuracy. The results show: 1) When there exists an outstanding\nprediction among the candidate predictions, the phenomenon of the 'forecast\ncombination puzzle' may not exist. The simple average method may perform much\nworse than the more sophisticated combining methods; 2) The choice of the\nprediction accuracy measure is crucial in defining the best candidate\nprediction for 'low frequency and high severity' datasets. For example, Mean\nSquared Error (MSE) does not distinguish well different combining methods, the\nMSE values of which are very close to each other; 3) The performances of\ndifferent combining methods can be drastically different. 4) Overall, the\ncombining approach is valuable to improve the prediction accuracy for insurance\nclaim cost.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 18:10:24 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Ye", "Chenglong", ""], ["Zhang", "Lin", ""], ["Han", "Mingxuan", ""], ["Yu", "Yanjia", ""], ["Zhao", "Bingxin", ""], ["Yang", "Yuhong", ""]]}, {"id": "1808.09126", "submitter": "Hao Xu", "authors": "Hao Xu, Matthew J. Bechle, Meng Wang, Adam A. Szpiro, Sverre Vedal,\n  Yuqi Bai, Julian D. Marshall", "title": "National PM2.5 and NO2 Exposure Models for China Based on Land Use\n  Regression, Satellite Measurements, and Universal Kriging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outdoor air pollution is a major killer worldwide and the fourth largest\ncontributor to the burden of disease in China. China is the most populous\ncountry in the world and also has the largest number of air pollution deaths\nper year, yet the spatial resolution of existing national air pollution\nestimates for China is generally relatively low. We address this knowledge gap\nby developing and evaluating national empirical models for China incorporating\nland-use regression (LUR), satellite measurements, and universal kriging (UK).\nWe test the resulting models in several ways, including (1) comparing models\ndeveloped using forward stepwise regression vs. partial least squares (PLS)\nregression, (2) comparing models developed with and without satellite\nmeasurements, and with and without UK, and (3) 10-fold cross-validation (CV),\nleave-one-province-out(LOPO) CV, and leave-one-city-out(LOCO) CV. Satellite\ndata and kriging are complementary in making predictions more accurate: kriging\nimproved the models in well-sampled areas; satellite data substantially\nimproved performance at locations far away from monitors. Stepwise forward\nselection performs similarly to PLS in 10-fold CV, but better than PLS in\nLOPO-CV. Our best models employ forward selection and UK, with 10-fold CV R2 of\n0.89 (for both 2014 and 2015) for PM2.5 and of 0.73 (year-2014) and 0.78\n(year-2015) for NO2. Population-weighted concentrations during 2014-2015\ndecreased for PM2.5 (58.7 {\\mu}g/m3 to 52.3 {\\mu}g/m3) and NO2 (29.6 {\\mu}g/m3\nto 26.8 {\\mu}g/m3). We produced the first high resolution national LUR models\nfor annual-average concentrations in China. Models were applied on 1 km grid to\nsupport future research. In 2015, more than 80% of the Chinese population lived\nin areas that exceed the Chinese national PM2.5 standard, 35 {\\mu}g/m3. Results\nhere will be publicly available and may be useful for environmental health\nresearch.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 05:36:17 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Xu", "Hao", ""], ["Bechle", "Matthew J.", ""], ["Wang", "Meng", ""], ["Szpiro", "Adam A.", ""], ["Vedal", "Sverre", ""], ["Bai", "Yuqi", ""], ["Marshall", "Julian D.", ""]]}, {"id": "1808.09169", "submitter": "Huw Llewelyn Dr", "authors": "Huw Llewelyn", "title": "Diagnostic tests can predict efficacy without randomization to\n  intervention and control by solving simultaneous equations based on different\n  test results: Required when assessing the effectiveness of test, trace and\n  isolation", "comments": "22 pages, 6 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The efficacy of an intervention can be assessed by randomising patients to\ndifferent diagnostic tests instead of to an intervention and control. This is\nbased on a familiar view that treatment will be more effective if given to\npatients based on the results of appropriate diagnostic tests (i.e. the correct\ndiagnosis) than if it given to those based on the results of inappropriate\ntests (i.e. the wrong diagnosis). The tests must have different characteristics\ne.g. sensitivities with respect to the outcome. This principle is applied by\nallocating an intervention to an individual if a test result is positive (or on\none side of a threshold) and allocating to a control to an individual if the\nresult is negative (or on the other side of the threshold). This can also be\ndone with different dichotomizing thresholds for one test. The frequencies of\nthe outcome in those with each of the four resulting observations are then used\nto calculate the risk reduction by solving a pair of simultaneous equations.\nThis assumes that the risk reduction and the overall frequency of the outcome\nis the same in both groups created by randomization. The calculations are\nillustrated by using data from a randomized controlled trial that assessed the\nefficacy of the angiotensin receptor blocker in lowering the risk of diabetic\nnephropathy in patients conditional on different urinary albumin excretion\nrates. They are also illustrated with simulated data based on a suggested\nmethodology for assessing the effectiveness of test, trace and isolation to\nreduce transmission of the SARS-Cov-2 virus using RT-PCR and LFD tests. This\napproach also allows the sensitivity and specificity of these tests with\nrespect of the outcome of viral spreading to be determined irrespective of the\nefficacy of isolation.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 08:40:18 GMT"}, {"version": "v10", "created": "Thu, 22 Apr 2021 10:09:11 GMT"}, {"version": "v11", "created": "Tue, 11 May 2021 23:25:02 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2019 10:51:47 GMT"}, {"version": "v3", "created": "Mon, 9 Sep 2019 09:19:02 GMT"}, {"version": "v4", "created": "Wed, 12 Feb 2020 23:59:27 GMT"}, {"version": "v5", "created": "Thu, 27 Feb 2020 09:20:56 GMT"}, {"version": "v6", "created": "Tue, 26 Jan 2021 23:52:11 GMT"}, {"version": "v7", "created": "Thu, 28 Jan 2021 07:28:14 GMT"}, {"version": "v8", "created": "Wed, 3 Feb 2021 22:46:53 GMT"}, {"version": "v9", "created": "Sat, 17 Apr 2021 22:43:05 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Llewelyn", "Huw", ""]]}, {"id": "1808.09254", "submitter": "Martin Jullum PhD", "authors": "Martin Jullum, Thordis Thorarinsdottir and Fabian E. Bachl", "title": "Estimating seal pup production in the Greenland Sea using Bayesian\n  hierarchical modeling", "comments": null, "journal-ref": null, "doi": "10.1111/rssc.12397", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Greenland Sea is an important breeding ground for harp and hooded seals.\nEstimates of the annual seal pup production are critical factors in the\nabundance estimation needed for management of the species. These estimates are\nusually based on counts from aerial photographic surveys. However, only a minor\npart of the whelping region can be photographed, due to its large extent. To\nestimate the total seal pup production, we propose a Bayesian hierarchical\nmodeling approach motivated by viewing the seal pup appearances as a\nrealization of a log-Gaussian Cox process using covariate information from\nsatellite imagery as a proxy for ice thickness. For inference, we utilize the\nstochastic partial differential equation (SPDE) module of the integrated nested\nLaplace approximation (INLA) framework. In a case study using survey data from\n2012, we compare our results with existing methodology in a comprehensive\ncross-validation study. The results of the study indicate that our method\nimproves local estimation performance, and that the increased prediction\nuncertainty of our method is required to obtain calibrated count predictions.\nThis suggests that the sampling density of the survey design may not be\nsufficient to obtain reliable estimates of the seal pup production.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 12:39:31 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 19:56:25 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Jullum", "Martin", ""], ["Thorarinsdottir", "Thordis", ""], ["Bachl", "Fabian E.", ""]]}, {"id": "1808.09322", "submitter": "James Salter", "authors": "James M. Salter, Daniel B. Williamson, Lauren J. Gregoire, and Tamsin\n  L. Edwards", "title": "Quantifying spatio-temporal boundary condition uncertainty for the North\n  American deglaciation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ice sheet models are used to study the deglaciation of North America at the\nend of the last ice age (past 21,000 years), so that we might understand\nwhether and how existing ice sheets may reduce or disappear under climate\nchange. Though ice sheet models have a few parameters controlling physical\nbehaviour of the ice mass, they also require boundary conditions for climate\n(spatio-temporal fields of temperature and precipitation, typically on regular\ngrids and at monthly intervals). The behaviour of the ice sheet is highly\nsensitive to these fields, and there is relatively little data from geological\nrecords to constrain them as the land was covered with ice. We develop a\nmethodology for generating a range of plausible boundary conditions, using a\nlow-dimensional basis representation of the spatio-temporal input. We derive\nthis basis by combining key patterns, extracted from a small ensemble of\nclimate model simulations of the deglaciation, with sparse spatio-temporal\nobservations. By jointly varying the ice sheet parameters and basis vector\ncoefficients, we run ensembles of the Glimmer ice sheet model that\nsimultaneously explore both climate and ice sheet model uncertainties. We use\nthese to calibrate the ice sheet physics and boundary conditions for Glimmer,\nby ruling out regions of the joint coefficient and parameter space via history\nmatching. We use binary ice/no ice observations from reconstructions of past\nice sheet margin position to constrain this space by introducing a novel metric\nfor history matching to binary data.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 14:24:43 GMT"}, {"version": "v2", "created": "Fri, 22 Mar 2019 12:20:47 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Salter", "James M.", ""], ["Williamson", "Daniel B.", ""], ["Gregoire", "Lauren J.", ""], ["Edwards", "Tamsin L.", ""]]}, {"id": "1808.09448", "submitter": "Vladimir Pastukhov", "authors": "Dragi Anevski and Vladimir Pastukhov", "title": "Estimating the distribution and thinning parameters of a homogeneous\n  multimode Poisson process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose estimators of the distribution of events of\ndifferent kinds in a multimode Poisson process. We give the explicit solution\nfor the maximum likelihood estimator, and derive its strong consistency and\nasymptotic normality. We also provide an order restricted estimator and derive\nits consistency and asymptotic distribution. We discuss the application of the\nestimator to the detection of neutrons in a novel detector being developed at\nthe European Spallation Source in Lund, Sweden. The inference problem gives\nrise to Sylvester-Ramanujan system of equations.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 14:32:58 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Anevski", "Dragi", ""], ["Pastukhov", "Vladimir", ""]]}, {"id": "1808.09831", "submitter": "Vanesa Jorda", "authors": "Vanesa Jorda, Jos\\'e Mar\\'ia Sarabia, Markus J\\\"antti", "title": "Estimation of income inequality from grouped data", "comments": "37 pages, 3 figures and 14 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grouped data in form of income shares have been conventionally used to\nestimate income inequality due to the lack of availability of individual\nrecords. Most prior research on economic inequality relies on lower bounds of\ninequality measures in order to avoid the need to impose a parametric\nfunctional form to describe the income distribution. These estimates neglect\nincome differences within shares, introducing, therefore, a potential source of\nmeasurement error. The aim of this paper is to explore a nuanced alternative to\nestimate income inequality, which leads to a reliable representation of the\nincome distribution within shares. We examine the performance of the\ngeneralized beta distribution of the second kind and related models to estimate\ndifferent inequality measures and compare the accuracy of these estimates with\nthe nonparametric lower bound in more than 5000 datasets covering 182 countries\nover the period 1867-2015. We deploy two different econometric strategies to\nestimate the parametric distributions, non-linear least squares and generalised\nmethod of moments, both implemented in R and conveniently available in the\npackage GB2group. Despite its popularity, the nonparametric approach is\noutperformed even the simplest two-parameter models. Our results confirm the\nexcellent performance of the GB2 distribution to represent income data for a\nheterogeneous sample of countries, which provides highly reliable estimates of\nseveral inequality measures. This strong result and the access to an easy tool\nto implement the estimation of this family of distributions, we believe, will\nincentivize its use, thus contributing to the development of reliable estimates\nof inequality trends.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 13:53:29 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Jorda", "Vanesa", ""], ["Sarabia", "Jos\u00e9 Mar\u00eda", ""], ["J\u00e4ntti", "Markus", ""]]}, {"id": "1808.09842", "submitter": "Divine Wanduku Dr.", "authors": "Divine Wanduku", "title": "Analyzing the qualitative properties of white noise on a family of\n  infectious disease models in a highly random environment", "comments": null, "journal-ref": "In: Dutta H., Peters J. (eds) Applied Mathematical Analysis:\n  Theory, Methods, and Applications. Studies in Systems, Decision and Control,\n  vol 177(2020). Springer, Cham", "doi": "10.1007/978-3-319-99918-0_17", "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A class of stochastic vector-borne infectious disease models is derived and\nstudied. The class type is determined by a general nonlinear incidence rate of\nthe disease. The disease spreads in a highly random environment with\nvariability from the disease transmission and natural death rates. Other\nsources of variability include the random delays of disease incubation inside\nthe vector and the human being, and also the random delay due to the period of\neffective acquired immunity against the disease. The basic reproduction number\nand other threshold conditions for disease eradication are computed. The\nqualitative behaviors of the disease dynamics are examined under the different\nsources of variability in the system. A technique to classify the different\nlevels of the intensities of the noises in the system is presented, and used to\ninvestigate the qualitative behaviors of the disease dynamics in the\ninfection-free steady state population under the different intensity levels of\nthe white noises in the system. Moreover, the possibility of population\nextinction, whenever the intensities of the white noises in the system are high\nis examined. Numerical simulation results are presented to elucidate the\ntheoretical results.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 14:13:44 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Wanduku", "Divine", ""]]}, {"id": "1808.09897", "submitter": "Nathan VanHoudnos", "authors": "Carson D. Sestili and William S. Snavely and Nathan M. VanHoudnos", "title": "Towards security defect prediction with AI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we investigate the limits of the current state of the art AI\nsystem for detecting buffer overflows and compare it with current static\nanalysis tools. To do so, we developed a code generator, s-bAbI, capable of\nproducing an arbitrarily large number of code samples of controlled complexity.\nWe found that the static analysis engines we examined have good precision, but\npoor recall on this dataset, except for a sound static analyzer that has good\nprecision and recall. We found that the state of the art AI system, a memory\nnetwork modeled after Choi et al. [1], can achieve similar performance to the\nstatic analysis engines, but requires an exhaustive amount of training data in\norder to do so. Our work points towards future approaches that may solve these\nproblems; namely, using representations of code that can capture appropriate\nscope information and using deep learning methods that are able to perform\narithmetic operations.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 15:57:27 GMT"}, {"version": "v2", "created": "Wed, 12 Sep 2018 16:40:29 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Sestili", "Carson D.", ""], ["Snavely", "William S.", ""], ["VanHoudnos", "Nathan M.", ""]]}, {"id": "1808.10026", "submitter": "Andr\\'es Felipe L\\'opez-Lopera", "authors": "Andr\\'es F. L\\'opez-Lopera, Nicolas Durrande and Mauricio A. Alvarez", "title": "Physically-Inspired Gaussian Process Models for Post-Transcriptional\n  Regulation in Drosophila", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The regulatory process of Drosophila is thoroughly studied for understanding\na great variety of biological principles. While pattern-forming gene networks\nare analysed in the transcription step, post-transcriptional events (e.g.\ntranslation, protein processing) play an important role in establishing protein\nexpression patterns and levels. Since the post-transcriptional regulation of\nDrosophila depends on spatiotemporal interactions between mRNAs and gap\nproteins, proper physically-inspired stochastic models are required to study\nthe link between both quantities. Previous research attempts have shown that\nusing Gaussian processes (GPs) and differential equations lead to promising\npredictions when analysing regulatory networks. Here we aim at further\ninvestigating two types of physically-inspired GP models based on a\nreaction-diffusion equation where the main difference lies in where the prior\nis placed. While one of them has been studied previously using protein data\nonly, the other is novel and yields a simple approach requiring only the\ndifferentiation of kernel functions. In contrast to other stochastic\nframeworks, discretising the spatial space is not required here. Both GP models\nare tested under different conditions depending on the availability of gap gene\nmRNA expression data. Finally, their performances are assessed on a\nhigh-resolution dataset describing the blastoderm stage of the early embryo of\nDrosophila melanogaster\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 20:02:41 GMT"}, {"version": "v2", "created": "Sat, 6 Apr 2019 07:49:26 GMT"}, {"version": "v3", "created": "Tue, 21 May 2019 07:29:15 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["L\u00f3pez-Lopera", "Andr\u00e9s F.", ""], ["Durrande", "Nicolas", ""], ["Alvarez", "Mauricio A.", ""]]}, {"id": "1808.10132", "submitter": "Belinda Spratt", "authors": "Belinda Spratt and Erhan Kozan", "title": "Reducing post-surgery recovery bed occupancy with a probabilistic\n  forecast model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Operations Research approaches to surgical scheduling are becoming\nincreasingly popular in both theory and practice. Often these models neglect\nstochasticity in order to reduce the computational complexity of the problem.\nWe wish to provide practitioners and hospital administrative staff with a\nstart-of-day probabilistic forecast for the occupancy of post-surgery recovery\nspaces. The model minimises the maximum expected occupancy of the recovery\nunit, thus levelling the workload of hospital staff, and reducing the\nlikelihood of bed shortages and bottlenecks. We show that a Poisson binomial\nrandom variable models the number of patients in the recovery when\nparameterised by the surgical case sequence. A mixed integer nonlinear\nprogramming model for the surgical case sequencing problem reduces the maximum\nexpected occupancy in post-surgery recovery spaces. Simulated Annealing\nproduces good solutions in short amounts of computational time. We evaluate the\nmethodology with a full year of historical data. The solution techniques reduce\nmaximum expected recovery occupancy by 18% on average. This alleviates a large\namount of stress on staff in the postsurgery recovery spaces, reduces the\nlikelihood of bottlenecks, and improves the quality of care provided to\npatients.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 06:11:22 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 06:24:12 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Spratt", "Belinda", ""], ["Kozan", "Erhan", ""]]}, {"id": "1808.10173", "submitter": "Henk van Elst", "authors": "Henk van Elst", "title": "An Introduction to Inductive Statistical Inference: from Parameter\n  Estimation to Decision-Making", "comments": "161 pages, 22 *.eps figures, LaTeX2e, hyperlinked references. First\n  thorough revision, extended list of references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  These lecture notes aim at a post-Bachelor audience with a background at an\nintroductory level in Applied Mathematics and Applied Statistics. They discuss\nthe logic and methodology of the Bayes-Laplace approach to inductive\nstatistical inference that places common sense and the guiding lines of the\nscientific method at the heart of systematic analyses of quantitative-empirical\ndata. Following an exposition of exactly solvable cases of single- and\ntwo-parameter estimation problems, the main focus is laid on Markov Chain Monte\nCarlo (MCMC) simulations on the basis of Hamiltonian Monte Carlo sampling of\nposterior joint probability distributions for regression parameters occurring\nin generalised linear models for a univariate outcome variable. The modelling\nof fixed effects as well as of correlated varying effects via multi-level\nmodels in non-centred parametrisation is considered. The simulation of\nposterior predictive distributions is outlined. The assessment of a model's\nrelative out-of-sample posterior predictive accuracy with information\nentropy-based criteria WAIC and LOOIC and model comparison with Bayes factors\nare addressed. Concluding, a conceptual link to the behavioural subjective\nexpected utility representation of a single decision-maker's choice behaviour\nin static one-shot decision problems is established. Vectorised codes for MCMC\nsimulations of multi-dimensional posterior joint probability distributions with\nthe Stan probabilistic programming language implemented in the statistical\nsoftware R are provided. The lecture notes are fully hyperlinked. They direct\nthe reader to original scientific research papers, online resources on\ninductive statistical inference, and to pertinent biographical information.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 08:19:47 GMT"}, {"version": "v2", "created": "Sun, 30 Aug 2020 09:48:53 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["van Elst", "Henk", ""]]}, {"id": "1808.10511", "submitter": "MD Zadid Khan", "authors": "MD Zadid Khan, Sakib Mahmud Khan, Mashrur Chowdhury, Kakan Dey", "title": "Development and Evaluation of Recurrent Neural Network based Models for\n  Hourly Traffic Volume and AADT Prediction", "comments": "26 pages, 17 figures, 8275 words, Submitted to Transportation\n  Research Record (TRR) Journal, Currently under review", "journal-ref": "Transportation Research Record, Vol 2673, Issue 7, 2019", "doi": "10.1177/0361198119849059", "report-no": "0361198119849059", "categories": "stat.AP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prediction of high-resolution hourly traffic volumes of a given roadway\nis essential for transportation planning. Traditionally, Automatic Traffic\nRecorders (ATR) are used to collect this hourly volume data. These large\ndatasets are time series data characterized by long-term temporal dependencies\nand missing values. Regarding the temporal dependencies, all roadways are\ncharacterized by seasonal variations that can be weekly, monthly or yearly,\ndepending on the cause of the variation. Regarding the missing data in a\ntime-series sequence, traditional time series forecasting models perform poorly\nunder the influence of seasonal variations. To address this limitation, robust,\nRecurrent Neural Network (RNN) based, multi-step ahead forecasting models are\ndeveloped for time-series in this study. The simple RNN, the Gated Recurrent\nUnit (GRU) and the Long Short-Term Memory (LSTM) units are used to develop the\nmodel and evaluate its performance. Two approaches are used to address the\nmissing value issue: masking and imputation, in conjunction with the RNN\nmodels. Six different imputation algorithms are then used to identify the best\nmodel. The analysis indicates that the LSTM model performs better than simple\nRNN and GRU models, and imputation performs better than masking to predict\nfuture traffic volume. Based on analysis using 92 ATRs, the LSTM-Median model\nis deemed the best model in all scenarios for hourly traffic volume and AADT\nprediction, with an average RMSE of 274 and MAPE of 18.91% for hourly traffic\nvolume prediction and average RMSE of 824 and MAPE of 2.10% for AADT\nprediction.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 16:07:33 GMT"}, {"version": "v2", "created": "Mon, 10 Sep 2018 18:12:57 GMT"}, {"version": "v3", "created": "Mon, 26 Nov 2018 04:36:04 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Khan", "MD Zadid", ""], ["Khan", "Sakib Mahmud", ""], ["Chowdhury", "Mashrur", ""], ["Dey", "Kakan", ""]]}, {"id": "1808.10563", "submitter": "Yunpeng Zhao", "authors": "Charles Weko and Yunpeng Zhao", "title": "Penalized Component Hub Models", "comments": null, "journal-ref": "Social Networks (2017)", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social network analysis presupposes that observed social behavior is\ninfluenced by an unobserved network. Traditional approaches to inferring the\nlatent network use pairwise descriptive statistics that rely on a variety of\nmeasures of co-occurrence. While these techniques have proven useful in a wide\nrange of applications, the literature does not describe the generating\nmechanism of the observed data from the network.\n  In a previous article, the authors presented a technique which used a finite\nmixture model as the connection between the unobserved network and the observed\nsocial behavior. This model assumed that each group was the result of a star\ngraph on a subset of the population. Thus, each group was the result of a\nleader who selected members of the population to be in the group. They called\nthese hub models.\n  This approach treats the network values as parameters of a model. However,\nthis leads to a general challenge in estimating parameters which must be\naddressed. For small datasets there can be far more parameters to estimate than\nthere are observations. Under these conditions, the estimated network can be\nunstable.\n  In this article, we propose a solution which penalizes the number of nodes\nwhich can exert a leadership role. We implement this as a pseudo-Expectation\nMaximization algorithm.\n  We demonstrate this technique through a series of simulations which show that\nwhen the number of leaders is sparse, parameter estimation is improved.\nFurther, we apply this technique to a dataset of animal behavior and an example\nof recommender systems.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 01:17:45 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Weko", "Charles", ""], ["Zhao", "Yunpeng", ""]]}, {"id": "1808.10577", "submitter": "Yongnam Kim", "authors": "Yongnam Kim", "title": "Apples to Oranges: Causal Effects of Answer Changing in Multiple-Choice\n  Exams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whether examinees' answer changing behavior while taking multiple-choice\nexams is beneficial or harmful is a long-standing puzzle in the educational and\npsychological measurement literature. Formalizing the problem using the\npotential outcomes framework, this article shows that the traditional method of\ncomparing the proportions of \"wrong to right\" and \"right to wrong\" answer\nchanging patterns--a method that has recently been criticized by van der\nLinden, Jeon, and Ferrara (2011)--indeed correctly identify the sign of the\naverage answer changing effect, but only for those examinees who actually\nchanged their initial responses. This subgroup effect is referred to as the\naverage treatment effect on the treated (ATT) and generally differs from the\naverage treatment effect on the untreated (ATU), that is, those who did not\nchange their initial responses. Analyzing two real data sets, including van der\nLinden et al.'s (2011) controversial data, this article finds that the ATT of\nanswer changing is positive while the ATU of answer changing is negative,\ntherefore, the debate on answer changing effects can be easily resolved. The\narticle also shows that answer changing and answer reviewing are two distinct\ntreatments and knowing answer changing effects is not informative for\npredicting answer reviewing effects.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 02:52:41 GMT"}, {"version": "v2", "created": "Thu, 6 Dec 2018 19:46:52 GMT"}, {"version": "v3", "created": "Tue, 12 Mar 2019 18:20:27 GMT"}, {"version": "v4", "created": "Mon, 14 Oct 2019 20:11:03 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Kim", "Yongnam", ""]]}, {"id": "1808.10618", "submitter": "Jens Rauch", "authors": "Jens Rauch, Jens H\\\"users, Birgit Babitsch and Ursula H\\\"ubner", "title": "Understanding the Characteristics of Frequent Users of Emergency\n  Departments: What Role Do Medical Conditions Play?", "comments": "63rd Annual Conference of the German Association for Medical\n  Informatics, Biometry and Epidemiology (GMDS)", "journal-ref": null, "doi": "10.3233/978-1-61499-896-9-175", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frequent users of emergency departments (ED) pose a significant challenge to\nhospital emergency services. Despite a wealth of studies in this field, it is\nhardly understood, what medical conditions lead to frequent attendance. We\nexamine (1) what ambulatory care sensitive conditions (ACSC) are linked to\nfrequent use, (2) how frequent users can be clustered into subgroups with\nrespect to their diagnoses, acuity and admittance, and (3) whether frequent use\nis related to higher acuity or admission rate. We identified several ACSC that\nhighly increase the risk for heavy ED use, extracted four major diagnose\nsubgroups and found no significant effect neither for acuity nor admission\nrate. Our study indicates that especially patients in need of (nursing) care\nform subgroups of frequent users, which implies that quality of care services\nmight be crucial for tackling frequent use. Hospitals are advised to regularly\nanalyze their ED data in the EHR to better align resources.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 07:44:51 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Rauch", "Jens", ""], ["H\u00fcsers", "Jens", ""], ["Babitsch", "Birgit", ""], ["H\u00fcbner", "Ursula", ""]]}]