[{"id": "1003.0182", "submitter": "Richard D. Gill", "authors": "Richard D. Gill, Niels Keiding", "title": "Product-limit estimators of the gap time distribution of a renewal\n  process under different sampling patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric estimation of the gap time distribution in a simple renewal\nprocess may be considered a problem in survival analysis under particular\nsampling frames corresponding to how the renewal process is observed. This note\ndescribes several such situations where simple product limit estimators, though\ninefficient, may still be useful.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2010 14:44:05 GMT"}], "update_date": "2010-03-02", "authors_parsed": [["Gill", "Richard D.", ""], ["Keiding", "Niels", ""]]}, {"id": "1003.0275", "submitter": "Denis Belomestny", "authors": "Denis Belomestny", "title": "Statistical inference for time-changed L\\'{e}vy processes via composite\n  characteristic function estimation", "comments": "Published in at http://dx.doi.org/10.1214/11-AOS901 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2011, Vol. 39, No. 4, 2205-2242", "doi": "10.1214/11-AOS901", "report-no": "IMS-AOS-AOS901", "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, the problem of semi-parametric inference on the parameters\nof a multidimensional L\\'{e}vy process $L_t$ with independent components based\non the low-frequency observations of the corresponding time-changed L\\'{e}vy\nprocess $L_{\\mathcal{T}(t)}$, where $\\mathcal{T}$ is a nonnegative,\nnondecreasing real-valued process independent of $L_t$, is studied. We show\nthat this problem is closely related to the problem of composite function\nestimation that has recently gotten much attention in statistical literature.\nUnder suitable identifiability conditions, we propose a consistent estimate for\nthe L\\'{e}vy density of $L_t$ and derive the uniform as well as the pointwise\nconvergence rates of the estimate proposed. Moreover, we prove that the rates\nobtained are optimal in a minimax sense over suitable classes of time-changed\nL\\'{e}vy models. Finally, we present a simulation study showing the performance\nof our estimation algorithm in the case of time-changed Normal Inverse Gaussian\n(NIG) L\\'{e}vy processes.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2010 08:45:30 GMT"}, {"version": "v2", "created": "Wed, 29 Dec 2010 14:14:12 GMT"}, {"version": "v3", "created": "Mon, 30 Jan 2012 14:32:55 GMT"}], "update_date": "2012-01-31", "authors_parsed": [["Belomestny", "Denis", ""]]}, {"id": "1003.0747", "submitter": "Pierre Neuvial", "authors": "Pierre Neuvial (LPMA, SG)", "title": "Asymptotic Results on Adaptive False Discovery Rate Controlling\n  Procedures Based on Kernel Estimators", "comments": null, "journal-ref": "Journal of Machine Learning Research 14 (2013) 1423-1459", "doi": null, "report-no": null, "categories": "math.ST physics.data-an q-bio.QM stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The False Discovery Rate (FDR) is a commonly used type I error rate in\nmultiple testing problems. It is defined as the expected False Discovery\nProportion (FDP), that is, the expected fraction of false positives among\nrejected hypotheses. When the hypotheses are independent, the\nBenjamini-Hochberg procedure achieves FDR control at any pre-specified level.\nBy construction, FDR control offers no guarantee in terms of power, or type II\nerror. A number of alternative procedures have been developed, including\nplug-in procedures that aim at gaining power by incorporating an estimate of\nthe proportion of true null hypotheses. In this paper, we study the asymptotic\nbehavior of a class of plug-in procedures based on kernel estimators of the\ndensity of the $p$-values, as the number $m$ of tested hypotheses grows to\ninfinity. In a setting where the hypotheses tested are independent, we prove\nthat these procedures are asymptotically more powerful in two respects: (i) a\ntighter asymptotic FDR control for any target FDR level and (ii) a broader\nrange of target levels yielding positive asymptotic power. We also show that\nthis increased asymptotic power comes at the price of slower, non-parametric\nconvergence rates for the FDP. These rates are of the form $m^{-k/(2k+1)}$,\nwhere $k$ is determined by the regularity of the density of the $p$-value\ndistribution, or, equivalently, of the test statistics distribution. These\nresults are applied to one- and two-sided tests statistics for Gaussian and\nLaplace location models, and for the Student model.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2010 08:17:28 GMT"}, {"version": "v2", "created": "Sat, 20 Apr 2013 08:47:22 GMT"}], "update_date": "2013-10-04", "authors_parsed": [["Neuvial", "Pierre", "", "LPMA, SG"]]}, {"id": "1003.0996", "submitter": "Ada Lau", "authors": "Ada Lau, Patrick McSharry", "title": "Approaches for multi-step density forecasts with application to\n  aggregated wind power", "comments": "Corrected version includes updated equation (18). Published in at\n  http://dx.doi.org/10.1214/09-AOAS320 the Annals of Applied Statistics\n  (http://www.imstat.org/aoas/) by the Institute of Mathematical Statistics\n  (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2010, Vol. 4, No. 3, 1311-1341", "doi": "10.1214/09-AOAS320", "report-no": "IMS-AOAS-AOAS320", "categories": "stat.AP physics.ao-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generation of multi-step density forecasts for non-Gaussian data mostly\nrelies on Monte Carlo simulations which are computationally intensive. Using\naggregated wind power in Ireland, we study two approaches of multi-step density\nforecasts which can be obtained from simple iterations so that intensive\ncomputations are avoided. In the first approach, we apply a logistic\ntransformation to normalize the data approximately and describe the transformed\ndata using ARIMA--GARCH models so that multi-step forecasts can be iterated\neasily. In the second approach, we describe the forecast densities by truncated\nnormal distributions which are governed by two parameters, namely, the\nconditional mean and conditional variance. We apply exponential smoothing\nmethods to forecast the two parameters simultaneously. Since the underlying\nmodel of exponential smoothing is Gaussian, we are able to obtain multi-step\nforecasts of the parameters by simple iterations and thus generate forecast\ndensities as truncated normal distributions. We generate forecasts for wind\npower from 15 minutes to 24 hours ahead. Results show that the first approach\ngenerates superior forecasts and slightly outperforms the second approach under\nvarious proper scores. Nevertheless, the second approach is computationally\nmore efficient and gives more robust results under different lengths of\ntraining data. It also provides an attractive alternative approach since one is\nallowed to choose a particular parametric density for the forecasts, and is\nvaluable when there are no obvious transformations to normalize the data.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2010 09:37:37 GMT"}, {"version": "v2", "created": "Fri, 12 Nov 2010 14:07:13 GMT"}, {"version": "v3", "created": "Mon, 6 Dec 2010 12:27:38 GMT"}, {"version": "v4", "created": "Mon, 10 Jan 2011 08:04:51 GMT"}], "update_date": "2011-01-11", "authors_parsed": [["Lau", "Ada", ""], ["McSharry", "Patrick", ""]]}, {"id": "1003.1018", "submitter": "Matjaz Perc", "authors": "Matjaz Perc", "title": "Zipf's law and log-normal distributions in measures of scientific output\n  across fields and institutions: 40 years of Slovenia's research as an example", "comments": "8 pages, 3 figures; accepted for publication in Journal of\n  Informetrics [supplementary material available at\n  http://www.matjazperc.com/sicris/stats.html]", "journal-ref": "Journal of Informetrics 4 (2010) 358-364", "doi": "10.1016/j.joi.2010.03.001", "report-no": null, "categories": "physics.data-an cs.DB stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Slovenia's Current Research Information System (SICRIS) currently hosts\n86,443 publications with citation data from 8,359 researchers working on the\nwhole plethora of social and natural sciences from 1970 till present. Using\nthese data, we show that the citation distributions derived from individual\npublications have Zipfian properties in that they can be fitted by a power law\n$P(x) \\sim x^{-\\alpha}$, with $\\alpha$ between 2.4 and 3.1 depending on the\ninstitution and field of research. Distributions of indexes that quantify the\nsuccess of researchers rather than individual publications, on the other hand,\ncannot be associated with a power law. We find that for Egghe's g-index and\nHirsch's h-index the log-normal form $P(x) \\sim \\exp[-a\\ln x -b(\\ln x)^2]$\napplies best, with $a$ and $b$ depending moderately on the underlying set of\nresearchers. In special cases, particularly for institutions with a strongly\nhierarchical constitution and research fields with high self-citation rates,\nexponential distributions can be observed as well. Both indexes yield\ndistributions with equivalent statistical properties, which is a strong\nindicator for their consistency and logical connectedness. At the same time,\ndifferences in the assessment of citation histories of individual researchers\nstrengthen their importance for properly evaluating the quality and impact of\nscientific output.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2010 11:48:28 GMT"}], "update_date": "2010-06-08", "authors_parsed": [["Perc", "Matjaz", ""]]}, {"id": "1003.1325", "submitter": "Mayra Ivanoff Lora", "authors": "Mayra Ivanoff Lora, Julio M Singer", "title": "Beta-binomial/gamma-Poisson regression models for repeated counts with\n  random parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Beta-binomial/Poisson models have been used by many authors to model\nmultivariate count data. Lora and Singer (Statistics in Medicine, 2008)\nextended such models to accommodate repeated multivariate count data with\noverdipersion in the binomial component. To overcome some of the limitations of\nthat model, we consider a beta-binomial/gamma-Poisson alternative that also\nallows for both overdispersion and different covariances between the Poisson\ncounts. We obtain maximum likelihood estimates for the parameters using a\nNewton-Raphson algorithm and compare both models in a practical example.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2010 18:39:47 GMT"}], "update_date": "2010-03-08", "authors_parsed": [["Lora", "Mayra Ivanoff", ""], ["Singer", "Julio M", ""]]}, {"id": "1003.1727", "submitter": "Alexandre B. Simas", "authors": "Wagner Barreto-Souza and Alexandre B. Simas", "title": "The exp-$G$ family of probability distributions", "comments": "A much improved version of the pioneering manuscript \"A new family of\n  distributions based on the trucanted exponential distribution\" presented at\n  the 18 SINAPE (Simp\\'osio Nacional de Probabilidade e Estat\\'istica), 2008,\n  S\\~ao Paulo, Brazil.", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a new method to add a parameter to a family of\ndistributions. The additional parameter is completely studied and a full\ndescription of its behaviour in the distribution is given. We obtain several\nmathematical properties of the new class of distributions such as\nKullback-Leibler divergence, Shannon entropy, moments, order statistics,\nestimation of the parameters and inference for large sample. Further, we showed\nthat the new distribution have the reference distribution as special case, and\nthat the usual inference procedures also hold in this case. Furthermore, we\napplied our method to yield three-parameter extensions of the Weibull and beta\ndistributions. To motivate the use of our class of distributions, we present a\nsuccessful application to fatigue life data.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2010 21:09:18 GMT"}], "update_date": "2010-03-10", "authors_parsed": [["Barreto-Souza", "Wagner", ""], ["Simas", "Alexandre B.", ""]]}, {"id": "1003.2253", "submitter": "Anton Westveld", "authors": "Anton H. Westveld and Peter D. Hoff", "title": "A Statistical View of Learning in the Centipede Game", "comments": null, "journal-ref": "Stat 2 (1), 242-254. 2013", "doi": "10.1002/sta4.32", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we evaluate the statistical evidence that a population of\nstudents learn about the sub-game perfect Nash equilibrium of the centipede\ngame via repeated play of the game. This is done by formulating a model in\nwhich a player's error in assessing the utility of decisions changes as they\ngain experience with the game. We first estimate parameters in a statistical\nmodel where the probabilities of choices of the players are given by a Quantal\nResponse Equilibrium (QRE) (McKelvey and Palfrey, 1995, 1996, 1998), but are\nallowed to change with repeated play. This model gives a better fit to the data\nthan similar models previously considered. However, substantial correlation of\noutcomes of games having a common player suggests that a statistical model that\ncaptures within-subject correlation is more appropriate. Thus we then estimate\nparameters in a model which allows for within-player correlation of decisions\nand rates of learning. Through out the paper we also consider and compare the\nuse of randomization tests and posterior predictive tests in the context of\nexploratory and confirmatory data analyses.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2010 04:36:05 GMT"}, {"version": "v2", "created": "Wed, 8 Sep 2010 01:52:56 GMT"}], "update_date": "2013-11-20", "authors_parsed": [["Westveld", "Anton H.", ""], ["Hoff", "Peter D.", ""]]}, {"id": "1003.2307", "submitter": "Stefan J. Wijnholds", "authors": "Stefan J. Wijnholds and Alle-Jan van der Veen", "title": "Fundamental Imaging Limits of Radio Telescope Arrays", "comments": "12 pages, 8 figures", "journal-ref": "IEEE Journal of Selected Topics in Signal Processing, vol. 2, no.\n  5, pp613-623, October 2008", "doi": "10.1109/JSTSP.2008.2004216", "report-no": null, "categories": "astro-ph.IM physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fidelity of radio astronomical images is generally assessed by practical\nexperience, i.e. using rules of thumb, although some aspects and cases have\nbeen treated rigorously. In this paper we present a mathematical framework\ncapable of describing the fundamental limits of radio astronomical imaging\nproblems. Although the data model assumes a single snapshot observation, i.e.\nvariations in time and frequency are not considered, this framework is\nsufficiently general to allow extension to synthesis observations. Using tools\nfrom statistical signal processing and linear algebra, we discuss the\ntractability of the imaging and deconvolution problem, the redistribution of\nnoise in the map by the imaging and deconvolution process, the covariance of\nthe image values due to propagation of calibration errors and thermal noise and\nthe upper limit on the number of sources tractable by self calibration. The\ncombination of covariance of the image values and the number of tractable\nsources determines the effective noise floor achievable in the imaging process.\nThe effective noise provides a better figure of merit than dynamic range since\nit includes the spatial variations of the noise. Our results provide handles\nfor improving the imaging performance by design of the array.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2010 11:28:43 GMT"}], "update_date": "2010-03-12", "authors_parsed": [["Wijnholds", "Stefan J.", ""], ["van der Veen", "Alle-Jan", ""]]}, {"id": "1003.2321", "submitter": "Hideaki Aoyama", "authors": "Hideaki Aoyama, Yoshi Fujiwara, and Mauro Gallegati", "title": "Micro-Macro Relation of Production - The Double Scaling Law for\n  Statistical Physics of Economy -", "comments": "5 pages with 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that an economic system populated by multiple agents generates an\nequilibrium distribution in the form of multiple scaling laws of conditional\nPDFs, which are sufficient for characterizing the probability distribution. The\nexistence of the double scaling law is demonstrated empirically for the sales\nand the labor of one million Japanese firms. Theoretical study of the scaling\nlaws suggests lognormal joint distributions of sales and labor and a scaling\nlaw for labor productivity, both of which are confirmed empirically. This\nframework offers characterization of the equilibrium distribution with a small\nnumber of scaling indices, which determine macroscopic quantities, thus setting\nthe stage for an equivalence with statistical physics, bridging micro- and\nmacro-economics.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2010 12:33:44 GMT"}], "update_date": "2010-03-12", "authors_parsed": [["Aoyama", "Hideaki", ""], ["Fujiwara", "Yoshi", ""], ["Gallegati", "Mauro", ""]]}, {"id": "1003.2469", "submitter": "Daniel  Romero", "authors": "Daniel M. Romero and Jon Kleinberg", "title": "The Directed Closure Process in Hybrid Social-Information Networks, with\n  an Analysis of Link Formation on Twitter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CY physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has often been taken as a working assumption that directed links in\ninformation networks are frequently formed by \"short-cutting\" a two-step path\nbetween the source and the destination -- a kind of implicit \"link copying\"\nanalogous to the process of triadic closure in social networks. Despite the\nrole of this assumption in theoretical models such as preferential attachment,\nit has received very little direct empirical investigation. Here we develop a\nformalization and methodology for studying this type of directed closure\nprocess, and we provide evidence for its important role in the formation of\nlinks on Twitter. We then analyze a sequence of models designed to capture the\nstructural phenomena related to directed closure that we observe in the Twitter\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2010 03:53:22 GMT"}], "update_date": "2010-03-15", "authors_parsed": [["Romero", "Daniel M.", ""], ["Kleinberg", "Jon", ""]]}, {"id": "1003.2497", "submitter": "Stefan J. Wijnholds", "authors": "Stefan J. Wijnholds and Alle-Jan van der Veen", "title": "Self-Calibration of Radio Astronomical Arrays With Non-Diagonal Noise\n  Covariance Matrix", "comments": "5 pages, 5 figures, conference paper", "journal-ref": "Proceedings of the 17th European Signal Processing Conference\n  (EuSiPCo), Glasgow (United Kingdom), 24-28 August 2009", "doi": null, "report-no": null, "categories": "astro-ph.IM physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The radio astronomy community is currently building a number of phased array\ntelescopes. The calibration of these telescopes is hampered by the fact that\ncovariances of signals from closely spaced antennas are sensitive to noise\ncoupling and to variations in sky brightness on large spatial scales. These\neffects are difficult and computationally expensive to model. We propose to\nmodel them phenomenologically using a non-diagonal noise covariance matrix. The\nparameters can be estimated using a weighted alternating least squares (WALS)\nalgorithm iterating between the calibration parameters and the additive\nnuisance parameters. We demonstrate the effectiveness of our method using data\nfrom the low frequency array (LOFAR) prototype station.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2010 09:03:02 GMT"}], "update_date": "2010-03-15", "authors_parsed": [["Wijnholds", "Stefan J.", ""], ["van der Veen", "Alle-Jan", ""]]}, {"id": "1003.2934", "submitter": "Maarten Ambaum", "authors": "Maarten H. P. Ambaum", "title": "Significance Tests in Climate Science", "comments": "9 pages", "journal-ref": "Journal of Climate, vol. 23, 5927-5932 (2010)", "doi": "10.1175/2010JCLI3746.1", "report-no": null, "categories": "physics.ao-ph physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large fraction of papers in the climate literature includes erroneous uses\nof significance tests. A Bayesian analysis is presented to highlight the\nmeaning of significance tests and why typical misuse occurs. It is concluded\nthat a significance test very rarely provides useful quantitative information.\nThe significance statistic is not a quantitative measure of how confident we\ncan be of the 'reality' of a given result.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2010 14:59:38 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Ambaum", "Maarten H. P.", ""]]}, {"id": "1003.3362", "submitter": "Ge Wang", "authors": "Ge Wang, Jiansheng Yang", "title": "Axiomatic Quantification of Co-authors' Relative Contributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.DL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decades, the competition for academic resources has gradually\nintensified, and worsened with the current financial crisis. To optimize the\nresource allocation, individualized assessment of research results is being\nactively studied but the current indices, such as the number of papers, the\nnumber of citations, the h-factor and its variants have limitations, especially\ntheir inability of determining co-authors' credit shares fairly. Here we\nestablish an axiomatic system and quantify co-authors' relative contributions.\nOur methodology avoids subjective assignment of co-authors' credits using the\ninflated, fractional or harmonic methods, and provides a quantitative tool for\nscientific management such as funding and tenure decisions.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2010 13:10:18 GMT"}], "update_date": "2010-03-18", "authors_parsed": [["Wang", "Ge", ""], ["Yang", "Jiansheng", ""]]}, {"id": "1003.3984", "submitter": "Javier Turek Mr.", "authors": "Javier Turek, Irad Yavneh, Matan Protter, Michael Elad", "title": "On MMSE and MAP Denoising Under Sparse Representation Modeling Over a\n  Unitary Dictionary", "comments": "29 pages, 10 figures", "journal-ref": null, "doi": "10.1109/TSP.2011.2151190", "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among the many ways to model signals, a recent approach that draws\nconsiderable attention is sparse representation modeling. In this model, the\nsignal is assumed to be generated as a random linear combination of a few atoms\nfrom a pre-specified dictionary. In this work we analyze two Bayesian denoising\nalgorithms -- the Maximum-Aposteriori Probability (MAP) and the\nMinimum-Mean-Squared-Error (MMSE) estimators, under the assumption that the\ndictionary is unitary. It is well known that both these estimators lead to a\nscalar shrinkage on the transformed coefficients, albeit with a different\nresponse curve. In this work we start by deriving closed-form expressions for\nthese shrinkage curves and then analyze their performance. Upper bounds on the\nMAP and the MMSE estimation errors are derived. We tie these to the error\nobtained by a so-called oracle estimator, where the support is given,\nestablishing a worst-case gain-factor between the MAP/MMSE estimation errors\nand the oracle's performance. These denoising algorithms are demonstrated on\nsynthetic signals and on true data (images).\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2010 08:39:51 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Turek", "Javier", ""], ["Yavneh", "Irad", ""], ["Protter", "Matan", ""], ["Elad", "Michael", ""]]}, {"id": "1003.3985", "submitter": "Raja Giryes", "authors": "Raja Giryes, Michael Elad, Yonina C Eldar", "title": "The Projected GSURE for Automatic Parameter Tuning in Iterative\n  Shrinkage Methods", "comments": "20 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear inverse problems are very common in signal and image processing. Many\nalgorithms that aim at solving such problems include unknown parameters that\nneed tuning. In this work we focus on optimally selecting such parameters in\niterative shrinkage methods for image deblurring and image zooming. Our work\nuses the projected Generalized Stein Unbiased Risk Estimator (GSURE) for\ndetermining the threshold value lambda and the iterations number K in these\nalgorithms. The proposed parameter selection is shown to handle any degradation\noperator, including ill-posed and even rectangular ones. This is achieved by\nusing GSURE on the projected expected error. We further propose an efficient\ngreedy parameter setting scheme, that tunes the parameter while iterating\nwithout impairing the resulting deblurring performance. Finally, we provide\nextensive comparisons to conventional methods for parameter selection, showing\nthe superiority of the use of the projected GSURE.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2010 08:43:52 GMT"}], "update_date": "2010-03-23", "authors_parsed": [["Giryes", "Raja", ""], ["Elad", "Michael", ""], ["Eldar", "Yonina C", ""]]}, {"id": "1003.4466", "submitter": "Marie Kratz", "authors": "Armelle Guillou, Marie Kratz and Yann Le Strat", "title": "An Extreme Value Theory approach for the early detection of time\n  clusters with application to the surveillance of Salmonella", "comments": "21 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to generate a warning system for the early detection of\ntime clusters applied to public health surveillance data. This new method\nrelies on the evaluation of a return period associated to any new count of a\nparticular infection reported to a surveillance system. The method is applied\nto Salmonella surveillance in France and compared to the model developed by\nFarrington et al.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2010 17:06:16 GMT"}], "update_date": "2010-03-24", "authors_parsed": [["Guillou", "Armelle", ""], ["Kratz", "Marie", ""], ["Strat", "Yann Le", ""]]}, {"id": "1003.5535", "submitter": "Max Little", "authors": "Max A. Little and Nick S. Jones", "title": "Sparse bayesian step-filtering for high-throughput analysis of molecular\n  machine dynamics", "comments": "4 pages, link to code available from author's website.", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nature has evolved many molecular machines such as kinesin, myosin, and the\nrotary flagellar motor powered by an ion current from the mitochondria. Direct\nobservation of the step-like motion of these machines with time series from\nnovel experimental assays has recently become possible. These time series are\ncorrupted by molecular and experimental noise that requires removal, but\nclassical signal processing is of limited use for recovering such step-like\ndynamics. This paper reports simple, novel Bayesian filters that are robust to\nstep-like dynamics in noise, and introduce an L1-regularized, global filter\nwhose sparse solution can be rapidly obtained by standard convex optimization\nmethods. We show these techniques outperforming classical filters on simulated\ntime series in terms of their ability to accurately recover the underlying step\ndynamics. To show the techniques in action, we extract step-like speed\ntransitions from Rhodobacter sphaeroides flagellar motor time series. Code\nimplementing these algorithms available from\nhttp://www.eng.ox.ac.uk/samp/members/max/software/.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2010 13:22:37 GMT"}], "update_date": "2010-03-30", "authors_parsed": [["Little", "Max A.", ""], ["Jones", "Nick S.", ""]]}, {"id": "1003.5539", "submitter": "Gyemin Lee", "authors": "Gyemin Lee (1) and William Finn (2) and Clayton Scott (1,3) ((1)\n  Department of Electrical Engineering and Computer Science, University of\n  Michigan, (2) Department of Pathology, University of Michigan (3) Department\n  of Statistics, University of Michigan)", "title": "Statistical File Matching of Flow Cytometry Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flow cytometry is a technology that rapidly measures antigen-based markers\nassociated to cells in a cell population. Although analysis of flow cytometry\ndata has traditionally considered one or two markers at a time, there has been\nincreasing interest in multidimensional analysis. However, flow cytometers are\nlimited in the number of markers they can jointly observe, which is typically a\nfraction of the number of markers of interest. For this reason, practitioners\noften perform multiple assays based on different, overlapping combinations of\nmarkers. In this paper, we address the challenge of imputing the high\ndimensional jointly distributed values of marker attributes based on\noverlapping marginal observations. We show that simple nearest neighbor based\nimputation can lead to spurious subpopulations in the imputed data, and\nintroduce an alternative approach based on nearest neighbor imputation\nrestricted to a cell's subpopulation. This requires us to perform clustering\nwith missing data, which we address with a mixture model approach and novel EM\nalgorithm. Since mixture model fitting may be ill-posed, we also develop\ntechniques to initialize the EM algorithm using domain knowledge. We\ndemonstrate our approach on real flow cytometry data.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2010 18:15:26 GMT"}], "update_date": "2010-03-30", "authors_parsed": [["Lee", "Gyemin", ""], ["Finn", "William", ""], ["Scott", "Clayton", ""]]}]