[{"id": "1901.00052", "submitter": "Ehsan Najafi", "authors": "Ehsan Najafi, Reza Khanbilvardi", "title": "Clustering and Trend Analysis of Global Extreme Droughts from 1900 to\n  2014", "comments": "12 pages, 1 table, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drought is one of the most devastating environmental disasters. Analyzing\nhistorical changes in climate extremes is critical for mitigating its adverse\nimpacts in the future. In the present study, the spatial and temporal\ncharacteristics of the global severe droughts using Palmer Drought Intensity\nIndex (PDSI) from 1900 to 2014 are explored. K-means clustering is implemented\nto partition the extreme negative PDSI values. The global extreme droughts\nmagnitude around the world from 1950 to 1980 were less intense compared to the\nother decades. In 2012, the largest areas around the world, especially Canada,\nexperienced their most severe historical droughts. Results show that the most\nrecent extreme droughts occurred in some regions such as the North of Canada,\ncentral regions of the US, Southwest of Europe and Southeast Asia. We found\nthat after 1980, the spatial extent of the regions that experienced extreme\ndrought have increased substantially.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 21:52:46 GMT"}, {"version": "v2", "created": "Sun, 10 Feb 2019 14:31:10 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Najafi", "Ehsan", ""], ["Khanbilvardi", "Reza", ""]]}, {"id": "1901.00089", "submitter": "Graham Weinberg", "authors": "Graham V. Weinberg", "title": "Approximation of the Cell Under Test in Sliding Window Detection\n  Processes", "comments": null, "journal-ref": "Progress In Electromagnetics Research Letters, Vol. 84, 75-81,\n  2019", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of sliding window detection detection processes requires careful\nconsideration of the cell under test, which is an amplitude squared measurement\nof the signal plus clutter in the complex domain. Some authors have suggested\nthat there is sufficient merit in the approximation of the cell under test by a\ndistributional model similar to that assumed for the clutter distribution.\nUnder such an assumption, the development of Neyman-Pearson detectors is\nfacilitated. The purpose of the current paper is to demonstrate, in a modern\nmaritime surveillance radar context, that such an approximation is invalid.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jan 2019 04:25:23 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Weinberg", "Graham V.", ""]]}, {"id": "1901.00103", "submitter": "Elham Azimi", "authors": "Elham Azimi, Bud Griffis", "title": "Statistical Analysis Of NYC Buildings And Wind Damages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this study is to determine the types of existing buildings\nthat are at risk of falling debris based on height, age, construction\nclassification, construction methods and materials and occupancy. This study\nfocuses on elements that could become debris under high wind action and present\na hazard to pedestrians, vehicles, and nearby structures. This study evaluated\nthe particular building elements that might become Wind Generated Debris (WGD).\nThis was accomplished by inspecting 500 buildings located in Manhattan that\nexperienced wind-related incidents. The results illustrate that the building\nelements most likely to produce WGD are windows, followed by exterior fixtures,\nroof elements, stairs/sidewalk shed, and balcony elements, respectively.\nConsequently, FISP inspectors should pay particular attention to these\nelements, which have higher probabilities in causing incidents.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jan 2019 06:13:58 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Azimi", "Elham", ""], ["Griffis", "Bud", ""]]}, {"id": "1901.00226", "submitter": "Alexandra Suvorikova", "authors": "Alexey Kroshnin, Vladimir Spokoiny and Alexandra Suvorikova", "title": "Statistical inference for Bures-Wasserstein barycenters", "comments": "37 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce the concept of Bures-Wasserstein barycenter $Q_*$,\nthat is essentially a Fr\\'echet mean of some distribution $\\mathbb{P}$\nsupported on a subspace of positive semi-definite Hermitian operators\n$\\mathbb{H}_{+}(d)$. We allow a barycenter to be restricted to some affine\nsubspace of $\\mathbb{H}_{+}(d)$ and provide conditions ensuring its existence\nand uniqueness. We also investigate convergence and concentration properties of\nan empirical counterpart of $Q_*$ in both Frobenius norm and Bures-Wasserstein\ndistance, and explain, how obtained results are connected to optimal\ntransportation theory and can be applied to statistical inference in quantum\nmechanics.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 00:58:31 GMT"}, {"version": "v2", "created": "Mon, 11 Feb 2019 12:16:42 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Kroshnin", "Alexey", ""], ["Spokoiny", "Vladimir", ""], ["Suvorikova", "Alexandra", ""]]}, {"id": "1901.00250", "submitter": "Graham Weinberg", "authors": "Graham V. Weinberg", "title": "Extension of the Geometric Mean Constant False Alarm Rate Detector to\n  Multiple Pulses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of sliding window detection processes, based upon a single\ncell under test, and operating in clutter modelled by a Pareto distribution,\nhas been examined extensively. This includes the construction of decision rules\nwith the complete constant false alarm rate property. However, the case where\nthere are multiple pulses available has only been examined in the partial\nconstant false alarm rate scenario. This paper outlines in the latter case how\nthe probability of false alarm can be produced, for a geometric mean detector,\nusing properties of gamma distributions. The extension of this result, to the\nfull constant false alarm rate detector case, is then presented.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 03:22:00 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Weinberg", "Graham V.", ""]]}, {"id": "1901.00336", "submitter": "Ross Towe", "authors": "Ross Towe, Jonathan Tawn, Emma Eastoe, Rob Lamb", "title": "Modelling the clustering of extreme events for short-term risk\n  assessment", "comments": null, "journal-ref": null, "doi": "10.1007/s13253-019-00376-0", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Having reliable estimates of the occurrence rates of extreme events is highly\nimportant for insurance companies, government agencies and the general public.\nThe rarity of an extreme event is typically expressed through its return\nperiod, i.e., the expected waiting time between events of the observed size if\nthe extreme events of the processes are independent and identically\ndistributed. A major limitation with this measure is when an unexpectedly high\nnumber of events occur within the next few months immediately after a\n\\textit{T} year event, with \\textit{T} large. Such events undermine the trust\nin the quality of these risk estimates. The clustering of apparently\nindependent extreme events can occur as a result of local non-stationarity of\nthe process, which can be explained by covariates or random effects. We show\nhow accounting for these covariates and random effects provides more accurate\nestimates of return levels and aids short-term risk assessment through the use\nof a new risk measure, which provides evidence of risk which is complementary\nto the return period.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 12:21:50 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 15:25:53 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Towe", "Ross", ""], ["Tawn", "Jonathan", ""], ["Eastoe", "Emma", ""], ["Lamb", "Rob", ""]]}, {"id": "1901.00402", "submitter": "Andrew Elliott", "authors": "Andrew Elliott, Mihai Cucuringu, Milton Martinez Luaces, Paul Reidy,\n  Gesine Reinert", "title": "Anomaly Detection in Networks with Application to Financial Transaction\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI physics.soc-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper is motivated by the task of detecting anomalies in networks of\nfinancial transactions, with accounts as nodes and a directed weighted edge\nbetween two nodes denoting a money transfer. The weight of the edge is the\ntransaction amount. Examples of anomalies in networks include long paths of\nlarge transaction amounts, rings of large payments, and cliques of accounts.\nThere are many methods available which detect such specific structures in\nnetworks. Here we introduce a method which is able to detect previously\nunspecified anomalies in networks. The method is based on a combination of\nfeatures from network comparison and spectral analysis as well as local\nstatistics, yielding 140 main features. We then use a simple feature sum\nmethod, as well as a random forest method, in order to classify nodes as normal\nor anomalous. We test the method first on synthetic networks which we\ngenerated, and second on a set of synthetic networks which were generated\nwithout the methods team having access to the ground truth. The first set of\nsynthetic networks was split in a training set of 70 percent of the networks,\nand a test set of 30 percent of the networks. The resulting classifier was then\napplied to the second set of synthetic networks. We compare our method with\nOddball, a widely used method for anomaly detection in networks, as well as to\nrandom classification. While Oddball outperforms random classification, both\nour feature sum method and our random forest method outperform Oddball. On the\ntest set, the random forest outperforms feature sum, whereas on the second\nsynthetic data set, initially feature sum tends to pick up more anomalies than\nrandom forest, with this behaviour reversing for lower-scoring anomalies. In\nall cases, the top 2 percent of flagged anomalies contained on average over 90\npercent of the planted anomalies.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 14:51:35 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 18:03:06 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Elliott", "Andrew", ""], ["Cucuringu", "Mihai", ""], ["Luaces", "Milton Martinez", ""], ["Reidy", "Paul", ""], ["Reinert", "Gesine", ""]]}, {"id": "1901.00699", "submitter": "Spencer Wheatley Dr.", "authors": "Spencer Wheatley, Annette Hofmann and Didier Sornette", "title": "Data breaches in the catastrophe framework & beyond", "comments": "Previous version presented at Symposium on Insurance and Emerging\n  Risks, St. John's University, March 10th, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Development of sustainable insurance for cyber risks, with associated\nbenefits, inter alia requires reduction of ambiguity of the risk. Considering\ncyber risk, and data breaches in particular, as a man-made catastrophe\nclarifies the actuarial need for multiple levels of analysis - going beyond\nclaims-driven loss statistics alone to include exposure, hazard, breach size,\nand so on - and necessitating specific advances in scope, quality, and\nstandards of both data and models. The prominent human element, as well as\ndynamic, networked, and multi-type nature, of cyber risk makes it perhaps\nuniquely challenging. Complementary top-down statistical, and bottom-up\nanalytical approaches are discussed. Focusing on data breach severity, measured\nin private information items ('ids') extracted, we exploit relatively mature\nopen data for U.S. data breaches. We show that this extremely heavy-tailed risk\nis worsening for external attacker ('hack') events - both in frequency and\nseverity. Writing in Q2-2018, the median predicted number of ids breached in\nthe U.S. due to hacking, for the last 6 months of 2018, is 0.5 billion. But\nwith a 5% chance that the figure exceeds 7 billion - doubling the historical\ntotal. 'Fortunately' the total breach in that period turned out to be near the\nmedian.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 12:38:32 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 17:16:47 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Wheatley", "Spencer", ""], ["Hofmann", "Annette", ""], ["Sornette", "Didier", ""]]}, {"id": "1901.00795", "submitter": "Francisco Javier Delgado-Vences", "authors": "Francisco Delgado-Vences, Arelly Ornelas", "title": "Modelling Italian mortality rates with a geometric-type fractional\n  Ornstein-Uhlenbeck process", "comments": "19 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to model mortality hazard rates for human population using the\nexponential of the solution of a stochastic differential equation (SDE). The\nnoise in the SDE is a fractional Brownian motion. We will use the well-known\nfractional Ornstein-Uhlenbeck process. Using the Hurst parameter we showed that\nmortality rates exhibit long-term memory. The proposed model is a\ngeneralization of the model introduced by [6], where they used an SDE driven\nwith a Brownian motion. We tested our model with the Italian population between\nthe years 1950 to 2004.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 15:58:11 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Delgado-Vences", "Francisco", ""], ["Ornelas", "Arelly", ""]]}, {"id": "1901.00834", "submitter": "Marcus Cordi", "authors": "Marcus Cordi, Damien Challet, Serge Kassibrakis", "title": "The market nanostructure origin of asset price time reversal asymmetry", "comments": "19 pages, 10 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a framework to infer lead-lag networks between the states of\nelements of complex systems, determined at different timescales. As such\nnetworks encode the causal structure of a system, infering lead-lag networks\nfor many pairs of timescales provides a global picture of the mutual influence\nbetween timescales. We apply our method to two trader-resolved FX data sets and\ndocument strong and complex asymmetric influence of timescales on the structure\nof lead-lag networks. Expectedly, this asymmetry extends to trader activity:\nfor institutional clients in our dataset, past activity on timescales longer\nthan 3 hours is more correlated with future activity at shorter timescales than\nthe opposite (Zumbach effect), while a reverse Zumbach effect is found for past\ntimescales shorter than 3 hours; retail clients have a totally different, and\nmuch more intricate, structure of asymmetric timescale influence. The causality\nstructures are clearly caused by markedly different behaviors of the two types\nof traders. Hence, market nanostructure, i.e., market dynamics at the\nindividual trader level, provides an unprecedented insight into the causality\nstructure of financial markets, which is much more complex than previously\nthought.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 18:09:08 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 11:35:02 GMT"}, {"version": "v3", "created": "Tue, 7 Apr 2020 09:21:40 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Cordi", "Marcus", ""], ["Challet", "Damien", ""], ["Kassibrakis", "Serge", ""]]}, {"id": "1901.00883", "submitter": "Wenying Ji", "authors": "Yitong Li, Wenying Ji, and Simaan M. AbouRizk", "title": "Enhanced Welding Operator Quality Performance Measurement: Work\n  Experience-Integrated Bayesian Prior Determination", "comments": "8 pages, 5 figures, 2 tables, i3CE 2019", "journal-ref": null, "doi": "10.1061/9780784482438.076", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measurement of operator quality performance has been challenging in the\nconstruction fabrication industry. Among various causes, the learning effect is\na significant factor, which needs to be incorporated in achieving a reliable\noperator quality performance analysis. This research aims to enhance a\npreviously developed operator quality performance measurement approach by\nincorporating the learning effect (i.e., work experience). To achieve this\ngoal, the Plateau learning model is selected to quantitatively represent the\nrelationship between quality performance and work experience through a\nbeta-binomial regression approach. Based on this relationship, an informative\nprior determination approach, which incorporates operator work experience\ninformation, is developed to enhance the previous Bayesian-based operator\nquality performance measurement. Academically, this research provides a\nsystematic approach to derive Bayesian informative priors through integrating\nmulti-source information. Practically, the proposed approach reliably measures\noperator quality performance in fabrication quality control processes.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 19:17:31 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Li", "Yitong", ""], ["Ji", "Wenying", ""], ["AbouRizk", "Simaan M.", ""]]}, {"id": "1901.01296", "submitter": "Graham Weinberg", "authors": "Graham V. Weinberg", "title": "Compensating for Interference in Sliding Window Detection Processes\n  using a Bayesian Paradigm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sliding window detectors are non-coherent decision processes, designed in an\nattempt to control the probability of false alarm, for application to radar\ntarget detection. In earlier low resolution radar systems it was possible to\nspecify such detectors quite easily, due to the Gaussian nature of clutter\nreturns, in an X-band maritime surveillance radar context. As radar resolution\nimproved with corresponding developments in modern technology, it became\ndifficult to construct sliding window detectors with the constant false alarm\nrate property. However, over the last eight years this situation has been\nrectified, due to improved understanding of the way in which such detectors\nshould be constructed. This paper examines the Bayesian approach to the\nconstruction of such detectors. In particular, the design of sliding window\ndetectors, with the constant false alarm rate property, with the capacity to\nmanage interfering targets, will be outlined.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 20:04:41 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Weinberg", "Graham V.", ""]]}, {"id": "1901.01308", "submitter": "Kaspar Rufibach", "authors": "Kaspar Rufibach and Dominik Heinzmann and Annabelle Monnet", "title": "Integrating Phase 2 into Phase 3 based on an Intermediate Endpoint While\n  Accounting for a Cure Proportion -- with an Application to the Design of a\n  Clinical Trial in Acute Myeloid Leukemia", "comments": "23 pages, 3 figures, 3 tables. All code is available on github:\n  https://github.com/numbersman77/integratePhase2.git", "journal-ref": "Pharm. Stat., 19, 44-58, 2020", "doi": "10.1002/pst.1969", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a trial with primary endpoint overall survival for a molecule with\ncurative potential, statistical methods that rely on the proportional hazards\nassumption may underestimate the power and the time to final analysis. We show\nhow a cure proportion model can be used to get the necessary number of events\nand appropriate timing via simulation. If Phase 1 results for the new drug are\nexceptional and/or the medical need in the target population is high, a Phase 3\ntrial might be initiated after Phase 1. Building in a futility interim analysis\ninto such a pivotal trial may mitigate the uncertainty of moving directly to\nPhase 3. However, if cure is possible, overall survival might not be mature\nenough at the interim to support a futility decision. We propose to base this\ndecision on an intermediate endpoint that is sufficiently associated with\nsurvival. Planning for such an interim can be interpreted as making a\nrandomized Phase 2 trial a part of the pivotal trial: if stopped at the\ninterim, the trial data would be analyzed and a decision on a subsequent Phase\n3 trial would be made. If the trial continues at the interim then the Phase 3\ntrial is already underway. To select a futility boundary, a mechanistic\nsimulation model that connects the intermediate endpoint and survival is\nproposed. We illustrate how this approach was used to design a pivotal\nrandomized trial in acute myeloid leukemia, discuss historical data that\ninformed the simulation model, and operational challenges when implementing it.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 20:54:12 GMT"}, {"version": "v2", "created": "Thu, 13 Jun 2019 07:06:10 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Rufibach", "Kaspar", ""], ["Heinzmann", "Dominik", ""], ["Monnet", "Annabelle", ""]]}, {"id": "1901.01330", "submitter": "Owais Gilani", "authors": "Owais Gilani, Lisa A. McKay, Timothy G. Gregoire, Yongtao Guan, Brian\n  P. Leaderer, Theodore R. Holford", "title": "Spatiotemporal Calibration of Atmospheric Nitrogen Dioxide Concentration\n  Estimates From an Air Quality Model for Connecticut", "comments": "23 pages, 8 figures, supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A spatiotemporal calibration and resolution refinement model was fitted to\ncalibrate nitrogen dioxide (NO$_2$) concentration estimates from the Community\nMultiscale Air Quality (CMAQ) model, using two sources of observed data on\nNO$_2$ that differed in their spatial and temporal resolutions. To refine the\nspatial resolution of the CMAQ model estimates, we leveraged information using\nadditional local covariates including total traffic volume within 2 km,\npopulation density, elevation, and land use characteristics. Predictions from\nthis model greatly improved the bias in the CMAQ estimates, as observed by the\nmuch lower mean squared error (MSE) at the NO$_2$ monitor sites. The final\nmodel was used to predict the daily concentration of ambient NO$_2$ over the\nentire state of Connecticut on a grid with pixels of size 300 x 300 m. A\ncomparison of the prediction map with a similar map for the CMAQ estimates\nshowed marked improvement in the spatial resolution. The effect of local\ncovariates was evident in the finer spatial resolution map, where the\ncontribution of traffic on major highways to ambient NO$_2$ concentration\nstands out. An animation was also provided to show the change in the\nconcentration of ambient NO$_2$ over space and time for 1994 and 1995.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 22:25:47 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Gilani", "Owais", ""], ["McKay", "Lisa A.", ""], ["Gregoire", "Timothy G.", ""], ["Guan", "Yongtao", ""], ["Leaderer", "Brian P.", ""], ["Holford", "Theodore R.", ""]]}, {"id": "1901.01338", "submitter": "Owais Gilani", "authors": "Owais Gilani, Simon Urbanek, Michael J. Kane", "title": "Distributions of Human Exposure to Ozone During Commuting Hours in\n  Connecticut using the Cellular Device Network", "comments": null, "journal-ref": null, "doi": "10.1007/s13253-019-00378-y", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Epidemiologic studies have established associations between various air\npollutants and adverse health outcomes for adults and children. Due to high\ncosts of monitoring air pollutant concentrations for subjects enrolled in a\nstudy, statisticians predict exposure concentrations from spatial models that\nare developed using concentrations monitored at a few sites. In the absence of\ndetailed information on when and where subjects move during the study window,\nresearchers typically assume that the subjects spend their entire day at home,\nschool or work. This assumption can potentially lead to large exposure\nassignment bias. In this study, we aim to determine the distribution of the\nexposure assignment bias for an air pollutant (ozone) when subjects are assumed\nto be static as compared to accounting for individual mobility. To achieve this\ngoal, we use cell-phone mobility data on approximately 400,000 users in the\nstate of Connecticut during a week in July, 2016, in conjunction with an ozone\npollution model, and compare individual ozone exposure assuming static versus\nmobile scenarios. Our results show that exposure models not taking mobility\ninto account often provide poor estimates of individuals commuting into and out\nof urban areas: the average 8-hour maximum difference between these estimates\ncan exceed 80 parts per billion (ppb). However, for most of the population, the\ndifference in exposure assignment between the two models is small, thereby\nvalidating many current epidemiologic studies focusing on exposure to ozone.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 22:58:49 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 18:57:23 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Gilani", "Owais", ""], ["Urbanek", "Simon", ""], ["Kane", "Michael J.", ""]]}, {"id": "1901.01559", "submitter": "Arturo Erdely", "authors": "Arturo Erdely", "title": "A copula based approach for electoral quick counts", "comments": "12 pages, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An electoral quick count is a statistical procedure whose main objective is\nto obtain a relatively small but representative sample of all the polling\nstations in a certain election, and to measure the uncertainty about the final\nresult before the total count of votes. A stratified sampling design is\ncommonly preferred to reduce estimation variability. The present work shows\nthat dependence among strata and among candidates should be taken into\nconsideration for statistical inferences therein, and a copula based model is\nproposed and applied to Mexico's 2006, 2012, and 2018 presidential elections\ndata.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jan 2019 15:58:11 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Erdely", "Arturo", ""]]}, {"id": "1901.01869", "submitter": "Luke Keele", "authors": "Luke J. Keele, Dylan S. Small, Jesse Y. Hsu, Colin B. Fogarty", "title": "Patterns of Effects and Sensitivity Analysis for\n  Differences-in-Differences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applied analysts often use the differences-in-differences (DID) method to\nestimate the causal effect of policy interventions with observational data. The\nmethod is widely used, as the required before and after comparison of a treated\nand control group is commonly encountered in practice. DID removes bias from\nunobserved time-invariant confounders. While DID removes bias from\ntime-invariant confounders, bias from time-varying confounders may be present.\nHence, like any observational comparison, DID studies remain susceptible to\nbias from hidden confounders. Here, we develop a method of sensitivity analysis\nthat allows investigators to quantify the amount of bias necessary to change a\nstudy's conclusions. Our method operates within a matched design that removes\nbias from observed baseline covariates. We develop methods for both binary and\ncontinuous outcomes. We then apply our methods to two different empirical\nexamples from the social sciences. In the first application, we study the\neffect of changes to disability payments in Germany. In the second, we\nre-examine whether election day registration increased turnout in Wisconsin.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 15:07:06 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2019 17:12:50 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Keele", "Luke J.", ""], ["Small", "Dylan S.", ""], ["Hsu", "Jesse Y.", ""], ["Fogarty", "Colin B.", ""]]}, {"id": "1901.01985", "submitter": "Ming Dong", "authors": "Ming Dong", "title": "Combining Unsupervised and Supervised Learning for Asset Class Failure\n  Prediction in Power Systems", "comments": "8 pages, 3 figures", "journal-ref": "IEEE Trans. on Power Systems, 2019", "doi": "10.1109/TPWRS.2019.2920915", "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In power systems, an asset class is a group of power equipment that has the\nsame function and shares similar electrical or mechanical characteristics.\nPredicting failures for different asset classes is critical for electric\nutilities towards developing cost-effective asset management strategies.\nPreviously, physical age based Weibull distribution has been widely used to\nfailure prediction. However, this mathematical model cannot incorporate asset\ncondition data such as inspection or testing results. As a result, the\nprediction cannot be very specific and accurate for individual assets. To solve\nthis important problem, this paper proposes a novel and comprehensive\ndata-driven approach based on asset condition data: K-means clustering as an\nunsupervised learning method is used to analyze the inner structure of\nhistorical asset condition data and produce the asset conditional ages;\nlogistic regression as a supervised learning method takes in both asset\nphysical ages and conditional ages to classify and predict asset statuses.\nFurthermore, an index called average aging rate is defined to quantify, track\nand estimate the relationship between asset physical age and conditional age.\nThis approach was applied to an urban distribution system in West Canada to\npredict medium-voltage cable failures. Case studies and comparison with\nstandard Weibull distribution are provided. The proposed approach demonstrates\nsuperior performance and practicality for predicting asset class failures in\npower systems.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jan 2019 03:44:20 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2020 04:27:19 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Dong", "Ming", ""]]}, {"id": "1901.02152", "submitter": "Fan Li", "authors": "Fan Li and Fan Li", "title": "Double-Robust Estimation in Difference-in-Differences with an\n  Application to Traffic Safety Evaluation", "comments": "31 pages, 1 figure, 5 tables; the paper is published in Observational\n  Studies", "journal-ref": "Observational Studies. 5, 1-20 (2019)", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Difference-in-differences (DID) is a widely used approach for drawing causal\ninference from observational panel data. Two common estimation strategies for\nDID are outcome regression and propensity score weighting. In this paper,\nmotivated by a real application in traffic safety research, we propose a new\ndouble-robust DID estimator that hybridizes regression and propensity score\nweighting. We particularly focus on the case of discrete outcomes. We show that\nthe proposed double-robust estimator possesses the desirable large-sample\nrobustness property. We conduct a simulation study to examine its finite-sample\nperformance and compare with alternative methods. Our empirical results from a\nPennsylvania Department of Transportation data suggest that rumble strips are\nmarginally effective in reducing vehicle crashes.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 04:39:04 GMT"}, {"version": "v2", "created": "Mon, 18 Mar 2019 22:01:51 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Li", "Fan", ""], ["Li", "Fan", ""]]}, {"id": "1901.02157", "submitter": "Siyang Tao", "authors": "Nariankadu D. Shyamalkumar and Siyang Tao", "title": "On Tail Dependence Matrices -- The Realization Problem for Parametric\n  Families", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among bivariate tail dependence measures, the tail dependence coefficient has\nemerged as the popular choice. Akin to the correlation matrix, a multivariate\ndependence measure is constructed using these bivariate measures, and this is\nreferred to in the literature as the tail dependence matrix (TDM). While the\nproblem of determining whether a given ${d\\times d}$ matrix is a correlation\nmatrix is of the order $O(d^3)$ in complexity, determining if a matrix is a TDM\n(the realization problem) is believed to be non-polynomial in complexity. Using\na linear programming (LP) formulation, we show that the combinatorial structure\nof the constraints is related to the intractable max-cut problem in a weighted\ngraph. This connection provides an avenue for constructing parametric classes\nadmitting a polynomial in $d$ algorithm for determining membership in its\nconstraint polytope. The complexity of the general realization problem is\njustifiably of much theoretical interest. Since in practice one resorts to\nlower dimensional parametrization of the TDMs, we posit that it is rather the\ncomplexity of the realization problem restricted to parametric classes of TDMs,\nand algorithms for it, that are more practically relevant. In this paper, we\nshow how the inherent symmetry and sparsity in a parametrization can be\nexploited to achieve a significant reduction in the LP formulation, which can\nlead to polynomial complexity of such realization problems - some\nparametrizations even resulting in the constraint polytope being independent of\n$d$. We also explore the use of a probabilistic viewpoint on TDMs to derive the\nconstraint polytopes.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 05:12:45 GMT"}, {"version": "v2", "created": "Thu, 1 Aug 2019 04:47:44 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Shyamalkumar", "Nariankadu D.", ""], ["Tao", "Siyang", ""]]}, {"id": "1901.02248", "submitter": "Han Lin Shang", "authors": "Fearghal Kearney and Han Lin Shang", "title": "Uncovering predictability in the evolution of the WTI oil futures curve", "comments": "28 pages, 4 figures, to appear in European Financial Management", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately forecasting the price of oil, the world's most actively traded\ncommodity, is of great importance to both academics and practitioners. We\ncontribute by proposing a functional time series based method to model and\nforecast oil futures. Our approach boasts a number of theoretical and practical\nadvantages including effectively exploiting underlying process dynamics missed\nby classical discrete approaches. We evaluate the finite-sample performance\nagainst established benchmarks using a model confidence set test. A realistic\nout-of-sample exercise provides strong support for the adoption of our approach\nwith it residing in the superior set of models in all considered instances.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 11:02:01 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Kearney", "Fearghal", ""], ["Shang", "Han Lin", ""]]}, {"id": "1901.02471", "submitter": "Alexis Akira Toda", "authors": "Alexis Akira Toda and Yulong Wang", "title": "Efficient Minimum Distance Estimation of Pareto Exponent from Top Income\n  Shares", "comments": null, "journal-ref": null, "doi": "10.1002/jae.2788", "report-no": null, "categories": "math.ST econ.GN q-fin.EC stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient estimation method for the income Pareto exponent when\nonly certain top income shares are observable. Our estimator is based on the\nasymptotic theory of weighted sums of order statistics and the efficient\nminimum distance estimator. Simulations show that our estimator has excellent\nfinite sample properties. We apply our estimation method to U.S. top income\nshare data and find that the Pareto exponent has been stable at around 1.5\nsince 1985, suggesting that the rise in inequality during the last three\ndecades is mainly driven by redistribution between the rich and poor, not among\nthe rich.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 19:06:05 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2020 16:31:51 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Toda", "Alexis Akira", ""], ["Wang", "Yulong", ""]]}, {"id": "1901.02630", "submitter": "Daniel Dinsdale", "authors": "Daniel Dinsdale and Matias Salibian-Barrera", "title": "Modelling ocean temperatures from bio-probes under preferential sampling", "comments": "38 pages, 12 figures, accepted for publication in the Annals of\n  Applied Statistics", "journal-ref": null, "doi": "10.1214/18-AOAS1217", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last 25 years there has been an important increase in the amount of\ndata collected from animal-mounted sensors (bio-probes), which are often used\nto study the animals' behaviour or environment. We focus here on an example of\nthe latter, where the interest is in sea surface temperature (SST), and\nmeasurements are taken from sensors mounted on Elephant Seals in the Southern\nIndian ocean. We show that standard geostatistical models may not be reliable\nfor this type of data, due to the possibility that the regions visited by the\nanimals may depend on the SST. This phenomenon is known in the literature as\npreferential sampling, and, if ignored, it may affect the resulting spatial\npredictions and parameter estimates. Research on this topic has been mostly\nrestricted to stationary sampling locations such as monitoring sites. The main\ncontribution of this manuscript is to extend this methodology to observations\nobtained by devices that move through the region of interest, as is the case\nwith the tagged seals. More specifically, we propose a flexible framework for\ninference on preferentially sampled fields, where the process that generates\nthe sampling locations is stochastic and moving over time through a\n2-dimensional space. Our simulation studies confirm that predictions obtained\nfrom the preferential sampling model are more reliable when this phenomenon is\npresent, and they compare very well to the standard ones when there is no\npreferential sampling. Finally, we note that the conclusions of our analysis of\nthe SST data can change considerably when we incorporate preferential sampling\nin the model.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 08:15:44 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Dinsdale", "Daniel", ""], ["Salibian-Barrera", "Matias", ""]]}, {"id": "1901.02791", "submitter": "Oliver Stoner", "authors": "Oliver Stoner, Gavin Shaddick, Theo Economou, Sophie Gumy, Jessica\n  Lewis, Itzel Lucio, Giulia Ruggeri and Heather Adair-Rohani", "title": "Global Household Energy Model: A Multivariate Hierarchical Approach to\n  Estimating Trends in the Use of Polluting and Clean Fuels for Cooking", "comments": "24 Pages, 14 Figures", "journal-ref": null, "doi": "10.1111/rssc.12428", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2017 an estimated 3 billion people used polluting fuels and technologies\nas their primary cooking solution, with 3.8 million deaths annually attributed\nto household exposure to the resulting fine particulate matter air pollution.\nCurrently, health burdens are calculated using aggregations of fuel types, e.g.\nsolid fuels, as country-level estimates of the use of specific fuel types, e.g.\nwood and charcoal, are unavailable. To expand the knowledge base about impacts\nof household air pollution on health, we develop and implement a Bayesian\nhierarchical model, based on Generalized Dirichlet Multinomial distributions,\nthat jointly estimates non-linear trends in the use of eight key fuel types,\novercoming several data-specific challenges including missing or combined fuel\nuse values. We assess model fit using within-sample predictive analysis and an\nout-of-sample prediction experiment to evaluate the model's forecasting\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 15:59:15 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2019 18:23:52 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Stoner", "Oliver", ""], ["Shaddick", "Gavin", ""], ["Economou", "Theo", ""], ["Gumy", "Sophie", ""], ["Lewis", "Jessica", ""], ["Lucio", "Itzel", ""], ["Ruggeri", "Giulia", ""], ["Adair-Rohani", "Heather", ""]]}, {"id": "1901.02830", "submitter": "Yongnam Kim", "authors": "Yongnam Kim", "title": "Partial Identification of Answer Reviewing Effects in Multiple-Choice\n  Exams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Does reviewing previous answers during multiple-choice exams help examinees\nincrease their final score? This article formalizes the question using a\nrigorous causal framework, the potential outcomes framework. Viewing examinees'\nreviewing status as a treatment and their final score as an outcome, the\narticle first explains the challenges of identifying the causal effect of\nanswer reviewing in regular exam-taking settings. In addition to the\nincapability of randomizing the treatment selection (reviewing status) and the\nlack of other information to make this selection process ignorable, the\ntreatment variable itself is not fully known to researchers. Looking at\nexaminees' answer sheet data, it is unclear whether an examinee who did not\nchange his or her answer on a specific item reviewed it but retained the\ninitial answer (treatment condition) or chose not to review it (control\ncondition). Despite such challenges, however, the article develops partial\nidentification strategies and shows that the sign of the answer reviewing\neffect can be reasonably inferred. By analyzing a statewide math assessment\ndata set, the article finds that reviewing initial answers is generally\nbeneficial for examinees.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 17:01:27 GMT"}, {"version": "v2", "created": "Mon, 22 Apr 2019 20:31:22 GMT"}, {"version": "v3", "created": "Tue, 10 Sep 2019 19:58:59 GMT"}, {"version": "v4", "created": "Mon, 14 Oct 2019 19:12:09 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Kim", "Yongnam", ""]]}, {"id": "1901.02936", "submitter": "Ruijun Ma", "authors": "Ruijun Ma and Lee H. Dicker", "title": "The Mahalanobis kernel for heritability estimation in genome-wide\n  association studies: fixed-effects and random-effects methods", "comments": "21 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear mixed models (LMMs) are widely used for heritability estimation in\ngenome-wide association studies (GWAS). In standard approaches to heritability\nestimation with LMMs, a genetic relationship matrix (GRM) must be specified. In\nGWAS, the GRM is frequently a correlation matrix estimated from the study\npopulation's genotypes, which corresponds to a normalized Euclidean distance\nkernel. In this paper, we show that reliance on the Euclidean distance kernel\ncontributes to several unresolved modeling inconsistencies in heritability\nestimation for GWAS. These inconsistencies can cause biased heritability\nestimates in the presence of linkage disequilibrium (LD), depending on the\ndistribution of causal variants. We show that these biases can be resolved (at\nleast at the modeling level) if one adopts a Mahalanobis distance-based GRM for\nLMM analysis. Additionally, we propose a new definition of partitioned\nheritability -- the heritability attributable to a subset of genes or single\nnucleotide polymorphisms (SNPs) -- using the Mahalanobis GRM, and show that it\ninherits many of the nice consistency properties identified in our original\nanalysis. Partitioned heritability is a relatively new area for GWAS analysis,\nwhere inconsistency issues related to LD have previously been known to be\nespecially pernicious.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 21:18:52 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Ma", "Ruijun", ""], ["Dicker", "Lee H.", ""]]}, {"id": "1901.02991", "submitter": "Jason Poulos", "authors": "Kellie Ottoboni and Jason Poulos", "title": "Estimating population average treatment effects from experiments with\n  noncompliance", "comments": "Forthcoming, Journal of Causal Inference", "journal-ref": "Journal of Causal Inference, 8(1), 108-130 (2020)", "doi": "10.1515/jci-2018-0035", "report-no": null, "categories": "stat.ME econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized control trials (RCTs) are the gold standard for estimating causal\neffects, but often use samples that are non-representative of the actual\npopulation of interest. We propose a reweighting method for estimating\npopulation average treatment effects in settings with noncompliance.\nSimulations show the proposed compliance-adjusted population estimator\noutperforms its unadjusted counterpart when compliance is relatively low and\ncan be predicted by observed covariates. We apply the method to evaluate the\neffect of Medicaid coverage on health care use for a target population of\nadults who may benefit from expansions to the Medicaid program. We draw RCT\ndata from the Oregon Health Insurance Experiment, where less than one-third of\nthose randomly selected to receive Medicaid benefits actually enrolled.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 01:35:38 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 20:22:25 GMT"}, {"version": "v3", "created": "Sat, 8 Aug 2020 14:42:38 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Ottoboni", "Kellie", ""], ["Poulos", "Jason", ""]]}, {"id": "1901.03090", "submitter": "Johannes Bracher", "authors": "Johannes Bracher and Leonhard Held", "title": "Endemic-epidemic models with discrete-time serial interval distributions\n  for infectious disease prediction", "comments": "A previous version of this paper had the title \"Multivariate\n  endemic-epidemic models with higher-order lags and an application to outbreak\n  detection\" (see v1)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate count time series models are an important tool for the analysis\nand prediction of infectious disease spread. We consider the endemic-epidemic\nframework, an autoregressive model class for infectious disease surveillance\ncounts, and replace the default autoregression on counts from the previous time\nperiod with more flexible weighting schemes inspired by discrete-time serial\ninterval distributions. We employ three different parametric formulations, each\nwith an additional unknown weighting parameter estimated via a profile\nlikelihood approach, and compare them to an unrestricted nonparametric\napproach. The new methods are illustrated in a univariate analysis of dengue\nfever incidence in San Juan, Puerto Rico, and a spatio-temporal study of viral\ngastroenteritis in the twelve districts of Berlin. We assess the predictive\nperformance of the suggested models and several reference models at various\nforecast horizons. In both applications, the performance of the\nendemic-epidemic models is considerably improved by the proposed weighting\nschemes.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 10:46:29 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2019 08:17:11 GMT"}, {"version": "v3", "created": "Fri, 13 Mar 2020 15:48:38 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Bracher", "Johannes", ""], ["Held", "Leonhard", ""]]}, {"id": "1901.03151", "submitter": "Anna Maria Barlow", "authors": "Anna Maria Barlow, Chris Sherlock and Jonathan Tawn", "title": "Inference for extreme values under threshold-based stopping rules", "comments": null, "journal-ref": "J. R. Stat. Soc. C, 69: 765-789 (2020)", "doi": "10.1111/rssc.12420", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a propensity for an extreme value analyses to be conducted as a\nconsequence of the occurrence of a large flooding event. This timing of the\nanalysis introduces bias and poor coverage probabilities into the associated\nrisk assessments and leads subsequently to inefficient flood protection\nschemes. We explore these problems through studying stochastic stopping\ncriteria and propose new likelihood-based inferences that mitigate against\nthese difficulties. Our methods are illustrated through the analysis of the\nriver Lune, following it experiencing the UK's largest ever measured flow event\nin 2015. We show that without accounting for this stopping feature there would\nbe substantial over-design in response to the event.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 13:37:14 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 14:48:55 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Barlow", "Anna Maria", ""], ["Sherlock", "Chris", ""], ["Tawn", "Jonathan", ""]]}, {"id": "1901.03289", "submitter": "Kaveh Bakhsh Kelarestaghi", "authors": "Md Rauful Islam, Kaveh Bakhsh Kelarestaghi, Alireza Ermagun, and\n  Snehanshu Banerjee", "title": "Gender Differences in Injury Severity Risk of Single-Vehicle Crashes in\n  Virginia: A Nested Logit Analysis of Heterogeneity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores gender differences in injury severity risk using a\ncomprehensive crash dataset including the driver, vehicle, environment, and\nroadway characteristics. For the purpose of this study, only single vehicle\ncrashes that occurred in the Commonwealth of Virginia, collected in a five year\nperiod from 2011 to 2015, were used. This study contributes to the literature\nof crash analysis by incorporating an extensive dataset with normalized\nattributes. The dataset used for model estimation integrated from two different\ndata sources. These data sources include Virginia Traffic Records Electronic\nData System (TREDS) to incorporate crash information and Virginia Base Mapping\nProgram to incorporate roadway characteristics. A two level nested logit model\nis developed for each gender, in order to relax the Independence of Irrelevant\nAlternatives (IIA) assumption. Several crash determinants including but not\nlimited to, drivers characteristics, drivers behavior, vehicle condition,\nweather conditions, roadway geometry and surface conditions were found to have\nan impact on both male and female drivers. This study is an assessment of\ninfluence of different factors on crash severity segmented by gender. A few\ninteresting results include; (i) female fatality risk exceeds male fatality\nrisk when driving under the influence of alcohol, (ii) speeding contributes to\nlower fatality risk among female drivers compared to their male counterparts,\n(iii) poor vehicle conditions have no impact on injury risk to female drivers\nwhile having an increased injury risk among male drivers, and (iv) while\ndriving in a work zone area increases the risk of property damage crashes, the\nimpact is higher for male drivers compared to female drivers.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 17:40:38 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Islam", "Md Rauful", ""], ["Kelarestaghi", "Kaveh Bakhsh", ""], ["Ermagun", "Alireza", ""], ["Banerjee", "Snehanshu", ""]]}, {"id": "1901.03328", "submitter": "Caifa Zhou", "authors": "Caifa Zhou and Andreas Wieser", "title": "Modified Jaccard Index Analysis and Adaptive Feature Selection for\n  Location Fingerprinting with Limited Computational Complexity", "comments": "15 pagers, 10 figures, 10 tables, revised version for publishing to\n  TLBS. arXiv admin note: text overlap with arXiv:1711.07812", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach for fingerprinting-based positioning which reduces the\ndata requirements and computational complexity of the online positioning stage.\nIt is based on a segmentation of the entire region of interest into subregions,\nidentification of candidate subregions during the online-stage, and position\nestimation using a preselected subset of relevant features. The subregion\nselection uses a modified Jaccard index which quantifies the similarity between\nthe features observed by the user and those available within the reference\nfingerprint map. The adaptive feature selection is achieved using an adaptive\nforward-backward greedy search which determines a subset of features for each\nsubregion, relevant with respect to a given fingerprinting-based positioning\nmethod. In an empirical study using signals of opportunity for fingerprinting\nthe proposed subregion and feature selection reduce the processing time during\nthe online-stage by a factor of about 10 while the positioning accuracy does\nnot deteriorate significantly. In fact, in one of the two study cases the 90th\npercentile of the circular error increased by 7.5% while in the other study\ncase we even found a reduction of the corresponding circular error by 30%.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 14:31:25 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Zhou", "Caifa", ""], ["Wieser", "Andreas", ""]]}, {"id": "1901.03423", "submitter": "Eric Jay Daza", "authors": "Eric Jay Daza", "title": "Person as Population: A Longitudinal View of Single-Subject Causal\n  Inference for Analyzing Self-Tracked Health Data", "comments": "18 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-subject health data are becoming increasingly available thanks to\nadvances in self-tracking technology (e.g., wearable devices, mobile apps,\nsensors, implants). Many users and health caregivers seek to use such\nobservational time series data to recommend changing health practices in order\nto achieve desired health outcomes. However, there are few available causal\ninference approaches that are flexible enough to analyze such idiographic data.\nWe develop a recently introduced causal-analysis framework based on n-of-1\nrandomized trials, and implement a flexible random-forests g-formula approach\nto estimating a recurring individualized effect called the \"average period\ntreatment effect\". In the process, we argue that our approach essentially\nresembles that of a longitudinal study by partitioning a single time series\ninto periods taking on binary treatment levels. We analyze six years of the\nauthor's own self-tracked physical activity and weight data to demonstrate our\napproach, and compare the results of our analysis to one that does not properly\naccount for confounding.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 22:39:21 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2019 06:30:26 GMT"}, {"version": "v3", "created": "Tue, 22 Jan 2019 18:35:31 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Daza", "Eric Jay", ""]]}, {"id": "1901.03437", "submitter": "Graham Weinberg", "authors": "Graham V. Weinberg", "title": "Multipulse Order Statistic Constant False Alarm Rate Detector in Pareto\n  Background", "comments": "arXiv admin note: text overlap with arXiv:1901.00250", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent study, the extension of sliding window detectors from the single\nto multipulse case has been considered. This short note continues the analysis\nof such detectors, and specifies an order statistic variation. The probability\nof false alarm is derived in a useful compact mathematical expression.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 00:13:49 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Weinberg", "Graham V.", ""]]}, {"id": "1901.03531", "submitter": "Chieh-Hsi Wu", "authors": "Chieh-Hsi Wu and Chris C. Holmes", "title": "Supervised variable selection in randomised controlled trials prior to\n  exploration of treatment effect heterogeneity: an example from severe malaria", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploration of treatment effect heterogeneity (TEH) is an increasingly\nimportant aspect of modern statistical analysis for stratified medicine in\nrandomised controlled trials (RCTs) as we start to gather more information on\ntrial participants and wish to maximise the opportunities for learning from\ndata. However, the analyst should refrain from including a large number of\nvariables in a treatment interaction discovery stage. Because doing so can\nsignificantly dilute the power to detect any true outcome-predictive\ninteractions between treatments and covariates. Current guidance is limited and\nmainly relies on the use of unsupervised learning methods, such as hierarchical\nclustering or principal components analysis, to reduce the dimension of the\nvariable space prior to interaction tests. In this article we show that\noutcome-driven dimension reduction, i.e. supervised variable selection, can\nmaintain power without inflating the type-I error or false-positive rate. We\nprovide theoretical and applied results to support our approach. The applied\nresults are obtained from illustrating our framework on the dataset from an RCT\nin severe malaria. We also pay particular attention to the internal risk model\napproach for TEH discovery, which we show is a particular case of our method\nand we point to improvements over current implementation.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 09:58:20 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Wu", "Chieh-Hsi", ""], ["Holmes", "Chris C.", ""]]}, {"id": "1901.03677", "submitter": "Reid Priedhorsky", "authors": "Reid Priedhorsky (1), Ashlynn R. Daughton (1 and 2), Martha Barnard\n  (3), Fiona O'Connell (3), Dave Osthus (1) ((1) Los Alamos National\n  Laboratory, (2) University of Colorado Boulder, (3) Minnetonka Public\n  Schools)", "title": "Estimating influenza incidence using search query deceptiveness and\n  generalized ridge regression", "comments": "27 pages, 8 figures", "journal-ref": null, "doi": "10.1371/journal.pcbi.1007165", "report-no": "LA-UR 18-24467", "categories": "q-bio.PE cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Seasonal influenza is a sometimes surprisingly impactful disease, causing\nthousands of deaths per year along with much additional morbidity. Timely\nknowledge of the outbreak state is valuable for managing an effective response.\nThe current state of the art is to gather this knowledge using in-person\npatient contact. While accurate, this is time-consuming and expensive. This has\nmotivated inquiry into new approaches using internet activity traces, based on\nthe theory that lay observations of health status lead to informative features\nin internet data.\n  These approaches risk being deceived by activity traces having a\ncoincidental, rather than informative, relationship to disease incidence; to\nour knowledge, this risk has not yet been quantitatively explored. We evaluated\nboth simulated and real activity traces of varying deceptiveness for influenza\nincidence estimation using linear regression.\n  We found that deceptiveness knowledge does reduce error in such estimates,\nthat it may help automatically-selected features perform as well or better than\nfeatures that require human curation, and that a semantic distance measure\nderived from the Wikipedia article category tree serves as a useful proxy for\ndeceptiveness. This suggests that disease incidence estimation models should\nincorporate not only data about how internet features map to incidence but also\nadditional data to estimate feature deceptiveness. By doing so, we may gain one\nmore step along the path to accurate, reliable disease incidence estimation\nusing internet data. This capability would improve public health by decreasing\nthe cost and increasing the timeliness of such estimates.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 18:04:42 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Priedhorsky", "Reid", "", "1 and 2"], ["Daughton", "Ashlynn R.", "", "1 and 2"], ["Barnard", "Martha", ""], ["O'Connell", "Fiona", ""], ["Osthus", "Dave", ""]]}, {"id": "1901.03939", "submitter": "Yaqiong Wang", "authors": "Yaqiong Wang, Minya Xu, Hui Huang, Songxi Chen", "title": "Bias detection of $PM_{2.5}$ monitor readings using hidden dynamic\n  geostatistical calibration model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and reliable data stream plays an important role in air quality\nassessment. Air pollution data collected from monitoring networks, however,\ncould be biased due to instrumental error or other interventions, which covers\nup the real pollution status and misleads policy making. In this study,\nmotivated by the needs for objective bias detection, we propose a hidden\ndynamic geostatistical calibration (HDGC) model to automatically identify\nmonitoring stations with constantly biased readings. The HDGC model is a\ntwo-level hierarchical model whose parameters are estimated through an\nefficient Expectation-Maximization algorithm. Effectiveness of the proposed\nmodel is demonstrated by a simple numerical study. Our method is applied to\nhourly $PM_{2.5}$ data from 36 stations in Hebei province, China, over the\nperiod from March 2014 to February 2017. Significantly abnormal readings are\ndetected from stations in two cities.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jan 2019 06:36:10 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Wang", "Yaqiong", ""], ["Xu", "Minya", ""], ["Huang", "Hui", ""], ["Chen", "Songxi", ""]]}, {"id": "1901.04380", "submitter": "Hadrien Lorenzo", "authors": "Hadrien Lorenzo, J\\'er\\^ome Saracco, Rodolphe Thi\\'ebaut", "title": "Supervised Learning for Multi-Block Incomplete Data", "comments": "44 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the supervised high dimensional settings with a large number of variables\nand a low number of individuals, one objective is to select the relevant\nvariables and thus to reduce the dimension. That subspace selection is often\nmanaged with supervised tools. However, some data can be missing, compromising\nthe validity of the sub-space selection. We propose a Partial Least Square\n(PLS) based method, called Multi-block Data-Driven sparse PLS mdd-sPLS,\nallowing jointly variable selection and subspace estimation while training and\ntesting missing data imputation through a new algorithm called Koh-Lanta. This\nmethod was challenged through simulations against existing methods such as mean\nimputation, nipals, softImpute and imputeMFA. In the context of supervised\nanalysis of high dimensional data, the proposed method shows the lowest\nprediction error of the response variables. So far this is the only method\ncombining data imputation and response variable prediction. The superiority of\nthe supervised multi-block mdd-sPLS method increases with the intra-block and\ninter-block correlations. The application to a real data-set from a rVSV-ZEBOV\nEbola vaccine trial revealed interesting and biologically relevant results. The\nmethod is implemented in a R-package available on the CRAN and a Python-package\navailable on pypi.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 16:25:41 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Lorenzo", "Hadrien", ""], ["Saracco", "J\u00e9r\u00f4me", ""], ["Thi\u00e9baut", "Rodolphe", ""]]}, {"id": "1901.04531", "submitter": "Nandi Leslie", "authors": "Nandi O. Leslie, Richard E. Harang, Lawrence P. Knachel, and Alexander\n  Kott", "title": "Statistical Models for the Number of Successful Cyber Intrusions", "comments": null, "journal-ref": "The Journal of Defense Modeling and Simulation, 15(1), 49-63", "doi": "10.1177/1548512917715342", "report-no": null, "categories": "cs.CR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose several generalized linear models (GLMs) to predict the number of\nsuccessful cyber intrusions (or \"intrusions\") into an organization's computer\nnetwork, where the rate at which intrusions occur is a function of the\nfollowing observable characteristics of the organization: (i) domain name\nserver (DNS) traffic classified by their top-level domains (TLDs); (ii) the\nnumber of network security policy violations; and (iii) a set of predictors\nthat we collectively call \"cyber footprint\" that is comprised of the number of\nhosts on the organization's network, the organization's similarity to\neducational institution behavior (SEIB), and its number of records on\nscholar.google.com (ROSG). In addition, we evaluate the number of intrusions to\ndetermine whether these events follow a Poisson or negative binomial (NB)\nprobability distribution. We reveal that the NB GLM provides the best fit model\nfor the observed count data, number of intrusions per organization, because the\nNB model allows the variance of the count data to exceed the mean. We also show\nthat there are restricted and simpler NB regression models that omit selected\npredictors and improve the goodness-of-fit of the NB GLM for the observed data.\nWith our model simulations, we identify certain TLDs in the DNS traffic as\nhaving significant impact on the number of intrusions. In addition, we use the\nmodels and regression results to conclude that the number of network security\npolicy violations are consistently predictive of the number of intrusions.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 19:35:57 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Leslie", "Nandi O.", ""], ["Harang", "Richard E.", ""], ["Knachel", "Lawrence P.", ""], ["Kott", "Alexander", ""]]}, {"id": "1901.04592", "submitter": "Chandan Singh", "authors": "W. James Murdoch, Chandan Singh, Karl Kumbier, Reza Abbasi-Asl, Bin Yu", "title": "Interpretable machine learning: definitions, methods, and applications", "comments": "11 pages", "journal-ref": "Published in PNAS 2019", "doi": "10.1073/pnas.1900654116", "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine-learning models have demonstrated great success in learning complex\npatterns that enable them to make predictions about unobserved data. In\naddition to using models for prediction, the ability to interpret what a model\nhas learned is receiving an increasing amount of attention. However, this\nincreased focus has led to considerable confusion about the notion of\ninterpretability. In particular, it is unclear how the wide array of proposed\ninterpretation methods are related, and what common concepts can be used to\nevaluate them.\n  We aim to address these concerns by defining interpretability in the context\nof machine learning and introducing the Predictive, Descriptive, Relevant (PDR)\nframework for discussing interpretations. The PDR framework provides three\noverarching desiderata for evaluation: predictive accuracy, descriptive\naccuracy and relevancy, with relevancy judged relative to a human audience.\nMoreover, to help manage the deluge of interpretation methods, we introduce a\ncategorization of existing techniques into model-based and post-hoc categories,\nwith sub-groups including sparsity, modularity and simulatability. To\ndemonstrate how practitioners can use the PDR framework to evaluate and\nunderstand interpretations, we provide numerous real-world examples. These\nexamples highlight the often under-appreciated role played by human audiences\nin discussions of interpretability. Finally, based on our framework, we discuss\nlimitations of existing methods and directions for future work. We hope that\nthis work will provide a common vocabulary that will make it easier for both\npractitioners and researchers to discuss and choose from the full range of\ninterpretation methods.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 22:35:26 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Murdoch", "W. James", ""], ["Singh", "Chandan", ""], ["Kumbier", "Karl", ""], ["Abbasi-Asl", "Reza", ""], ["Yu", "Bin", ""]]}, {"id": "1901.04597", "submitter": "Katie Mollan", "authors": "Katie R. Mollan, Ilana M. Trumble, Sarah A. Reifeis, Orlando Ferrer,\n  Camden P. Bay, Pedro L. Baldoni, and Michael G. Hudgens", "title": "Exact Power of the Rank-Sum Test for a Continuous Variable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate power calculations are essential in small studies containing\nexpensive experimental units or high-stakes exposures. Herein, exact power of\nthe Wilcoxon Mann-Whitney rank-sum test of a continuous variable is formulated\nusing a Monte Carlo approach and defining P(X < Y) = p as a measure of effect\nsize, where X and Y denote random observations from two distributions\nhypothesized to be equal under the null. Effect size p fosters productive\ncommunications because researchers understand p = 0.5 is analogous to a fair\ncoin toss, and p near 0 or 1 represents a large effect. This approach is\nfeasible even without background data. Simulations were conducted comparing the\nexact power approach to existing approaches by Rosner & Glynn (2009), Shieh et\nal. (2006), Noether (1987), and O'Brien-Castelloe (2006). Approximations by\nNoether and O'Brien-Castelloe are shown to be inaccurate for small sample\nsizes. The Rosner & Glynn and Shieh et al. approaches performed well in many\nsmall sample scenarios, though both are restricted to location-shift\nalternatives and neither approach is theoretically justified for small samples.\nThe exact method is recommended and available in the R package wmwpow.\n  KEYWORDS: Mann-Whitney test, Monte Carlo simulation, non-parametric, power\nanalysis, Wilcoxon rank-sum test\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 22:47:18 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Mollan", "Katie R.", ""], ["Trumble", "Ilana M.", ""], ["Reifeis", "Sarah A.", ""], ["Ferrer", "Orlando", ""], ["Bay", "Camden P.", ""], ["Baldoni", "Pedro L.", ""], ["Hudgens", "Michael G.", ""]]}, {"id": "1901.04610", "submitter": "Greg Salvesen", "authors": "Greg Salvesen", "title": "Six-Day Footraces in the Post-Pedestrianism Era", "comments": "12 pages, 8 figures, accepted to the Journal of Quantitative Analysis\n  in Sports", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a six-day footrace, competitors accumulate as much distance as possible on\nfoot over 144 consecutive hours by circumambulating a loop course. Now an\nobscure event on the fringe of ultra running and contested by amateurs, six-day\nraces and the associated sport of pedestrianism used to be a lucrative\nprofessional athletic endeavor. Indeed, pedestrianism was the most popular\nspectator sport in America c. 1874-c. 1881. We analyzed data from 277 six-day\nraces spanning 37 years in the post-pedestrianism era (1981-2018). Men\noutnumber women 3:1 in six-day race participation. The men's (women's) six-day\nworld record is 644.2 (549.1) miles and the top 4% achieve 500 (450) miles.\nAdopting the forecasting model of Godsey (2012), we predict a 53% (21%)\nprobability that the men's (women's) world record will be broken within the\nnext decade.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 00:05:18 GMT"}, {"version": "v2", "created": "Wed, 16 Jan 2019 17:24:22 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Salvesen", "Greg", ""]]}, {"id": "1901.04695", "submitter": "Hugo Lewi Hammer Dr.", "authors": "Hugo Lewi Hammer", "title": "Statistical models for short and long term forecasts of snow depth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasting of future snow depths is useful for many applications like road\nsafety, winter sport activities, avalanche risk assessment and hydrology.\nMotivated by the lack of statistical forecasts models for snow depth, in this\npaper we present a set of models to fill this gap. First, we present a model to\ndo short term forecasts when we assume that reliable weather forecasts of air\ntemperature and precipitation are available. The covariates are included\nnonlinearly into the model following basic physical principles of snowfall,\nsnow aging and melting. Due to the large set of observations with snow depth\nequal to zero, we use a zero-inflated gamma regression model, which is commonly\nused to similar applications like precipitation. We also do long term forecasts\nof snow depth and much further than traditional weather forecasts for\ntemperature and precipitation. The long-term forecasts are based on fitting\nmodels to historic time series of precipitation, temperature and snow depth. We\nfit the models to data from three locations in Norway with different climatic\nproperties. Forecasting five days into the future, the results showed that,\ngiven reliable weather forecasts of temperature and precipitation, the forecast\nerrors in absolute value was between 3 and 7 cm for different locations in\nNorway. Forecasting three weeks into the future, the forecast errors were\nbetween 7 and 16 cm.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 07:44:40 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Hammer", "Hugo Lewi", ""]]}, {"id": "1901.04779", "submitter": "Shovanur Haque", "authors": "Shovanur Haque, Kerrie Mengersen and Steven Stern", "title": "Assessing the accuracy of record linkages with Markov chain based Monte\n  Carlo simulation approach", "comments": "33 pages, 10 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Record linkage is the process of finding matches and linking records from\ndifferent data sources so that the linked records belong to the same entity.\nThere is an increasing number of applications of record linkage in statistical,\nhealth, government and business organisations to link administrative, survey,\npopulation census and other files to create a complete set of information for\nmore complete and comprehensive analysis. To make valid inferences using a\nlinked file, it is increasingly becoming important to assess the linking\nmethod. It is also important to find techniques to improve the linking process\nto achieve higher accuracy. This motivates to develop a method for assessing\nlinking process and help decide which linking method is likely to be more\naccurate for a linking task. This paper proposes a Markov Chain based Monte\nCarlo simulation approach, MaCSim for assessing a linking method and\nillustrates the utility of the approach using a realistic synthetic dataset\nreceived from the Australian Bureau of Statistics to avoid privacy issues\nassociated with using real personal information. A linking method applied by\nMaCSim is also defined. To assess the defined linking method, correct re-link\nproportions for each record are calculated using our developed simulation\napproach. The accuracy is determined for a number of simulated datasets. The\nanalyses indicated promising performance of the proposed method MaCSim of the\nassessment of accuracy of the linkages. The computational aspects of the\nmethodology are also investigated to assess its feasibility for practical use.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 11:56:54 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 12:28:24 GMT"}, {"version": "v3", "created": "Tue, 29 Sep 2020 09:45:12 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Haque", "Shovanur", ""], ["Mengersen", "Kerrie", ""], ["Stern", "Steven", ""]]}, {"id": "1901.04836", "submitter": "George Hassan-Coring", "authors": "George Hassan-Coring", "title": "Inferring Causality in Agent-Based Simulations - Literature Review", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.10905.67684", "report-no": null, "categories": "cs.MA stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Complex systems have interested researchers across a broad range of fields\nfor many years and as computing has become more accesible and feasible, it is\nnow possible to simulate aspects of these systems. A major point of research is\nhow emergent behaviour arises and the underlying causes of it. This paper aims\nto discuss and compare different methods of identifying causal links between\nagents in such systems in order to gain further understanding of the structure.\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2018 01:27:48 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Hassan-Coring", "George", ""]]}, {"id": "1901.04869", "submitter": "Cord A. M\\\"uller", "authors": "Cord A. M\\\"uller", "title": "Optimal acceptance sampling for modules F and F1 of the European\n  Measuring Instruments Directive", "comments": "accepted by J. Appl. Stat. (2019)", "journal-ref": null, "doi": "10.1080/02664763.2019.1588235", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acceptance sampling plans offered by ISO 2859-1 are far from optimal under\nthe conditions for statistical verification in modules F and F1 as prescribed\nby Annex II of the Measuring Instruments Directive (MID) 2014/32/EU, resulting\nin sample sizes that are larger than necessary. An optimised single-sampling\nscheme is derived, both for large lots using the binomial distribution and for\nfinite-sized lots using the exact hypergeometric distribution, resulting in\nsmaller sample sizes that are economically more efficient while offering the\nfull statistical protection required by the MID.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 14:55:39 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 11:16:11 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["M\u00fcller", "Cord A.", ""]]}, {"id": "1901.04884", "submitter": "Michal Valko", "authors": "Jean-Bastien Grill and Michal Valko and R\\'emi Munos", "title": "Optimistic optimization of a Brownian", "comments": "10 pages, 2 figures", "journal-ref": "Neural Information Processing Systems (NeurIPS 2018)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of optimizing a Brownian motion. We consider a\n(random) realization $W$ of a Brownian motion with input space in $[0,1]$.\nGiven $W$, our goal is to return an $\\epsilon$-approximation of its maximum\nusing the smallest possible number of function evaluations, the sample\ncomplexity of the algorithm. We provide an algorithm with sample complexity of\norder $\\log^2(1/\\epsilon)$. This improves over previous results of Al-Mharmah\nand Calvin (1996) and Calvin et al. (2017) which provided only polynomial\nrates. Our algorithm is adaptive---each query depends on previous values---and\nis an instance of the optimism-in-the-face-of-uncertainty principle.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 15:37:11 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Grill", "Jean-Bastien", ""], ["Valko", "Michal", ""], ["Munos", "R\u00e9mi", ""]]}, {"id": "1901.04913", "submitter": "Hien Nguyen", "authors": "Jessica J. Bagnall and Andrew T. Jones and Natalie Karavarsamis and\n  Hien D. Nguyen", "title": "The fully-visible Boltzmann machine and the Senate of the 45th\n  Australian Parliament in 2016", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After the 2016 double dissolution election, the 45th Australian Parliament\nwas formed. At the time of its swearing in, the Senate of the 45th Australian\nParliament consisted of nine political parties, the largest number in the\nhistory of the Australian Parliament. Due to the breadth of the political\nspectrum that the Senate represented, the situation presented an interesting\nopportunity for the study of political interactions in the Australian context.\nUsing publicly available Senate voting data in 2016, we quantitatively analyzed\ntwo aspects of the Senate. Firstly, we analyzed the degree to which each of the\nnon-government parties of the Senate are pro- or anti-government. Secondly, we\nanalyzed the degree to which the votes of each of the non-government Senate\nparties are in concordance or discordance with one another. We utilized the\nfully-visible Boltzmann machine (FVBM) model in order to conduct these\nanalyses. The FVBM is an artificial neural network that can be viewed as a\nmultivariate generalization of the Bernoulli distribution. Via a maximum\npseudolikelihood estimation approach, we conducted parameter estimation and\nconstructed hypotheses test that revealed the interaction structures within the\nAustralian Senate. The conclusions that we drew are well-supported by external\nsources of information.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 10:34:52 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Bagnall", "Jessica J.", ""], ["Jones", "Andrew T.", ""], ["Karavarsamis", "Natalie", ""], ["Nguyen", "Hien D.", ""]]}, {"id": "1901.04915", "submitter": "Denis Mongin", "authors": "Denis Mongin, Adriana Uribe, Julien Gateau, Baris Gencer, Boris\n  Cheval, St\\'ephane Cullati, Delphine S. Courvoisier", "title": "Dynamical analysis in a self-regulated system undergoing multiple\n  excitations: first order differential equation approach", "comments": null, "journal-ref": "Multivariate Behavioral Research, 2020", "doi": "10.1080/00273171.2020.1754155", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This article proposes a dynamical system modeling approach for the analysis\nof longitudinal data of self-regulated systems experiencing multiple\nexcitations. The aim of such an approach is to focus on the evolution of a\nsignal (e.g., heart rate) before, during, and after excitations taking the\nsystem out of its equilibrium (e.g., physical effort during cardiac stress\ntesting). Dynamical modeling can be applied to a broad range of outcomes such\nas physiological processes in medicine and psychosocial processes in social\nsciences, and it allows to extract simple characteristics of the signal\nstudied. The model we propose is based on a first order linear differential\nequation defined by three main parameters corresponding to the initial\nequilibrium value, the dynamic characteristic time, and the reaction to the\nexcitation. In this paper, several estimation procedures for this model are\nconsidered and tested in a simulation study, that clarifies under which\nconditions accurate estimates are provided. Finally, applications of this model\nare illustrated using cardiology data recorded during effort tests.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 10:33:44 GMT"}, {"version": "v2", "created": "Sun, 1 Mar 2020 21:40:25 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Mongin", "Denis", ""], ["Uribe", "Adriana", ""], ["Gateau", "Julien", ""], ["Gencer", "Baris", ""], ["Cheval", "Boris", ""], ["Cullati", "St\u00e9phane", ""], ["Courvoisier", "Delphine S.", ""]]}, {"id": "1901.04916", "submitter": "Eben Kenah", "authors": "Yushuf Sharker and Eben Kenah", "title": "Pairwise accelerated failure time models for infectious disease\n  transmission with external sources of infection", "comments": "24 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pairwise survival analysis handles dependent happenings in infectious disease\ntransmission data by analyzing failure times in ordered pairs of individuals.\nThe contact interval in the pair $ij$ is the time from the onset of\ninfectiousness in $i$ to infectious contact from $i$ to $j$, where an\ninfectious contact is sufficient to infect $j$ if he or she is susceptible. The\ncontact interval distribution determines transmission probabilities and the\ninfectiousness profile of infected individuals. Many important questions in\ninfectious disease epidemiology involve the effects of covariates (e.g., age or\nvaccination status) on transmission. Here, we generalize earlier pairwise\nmethods in two ways: First, we introduce an accelerated failure time model that\nallows the contact interval rate parameter to depend on infectiousness\ncovariates for $i$, susceptibility covariates for $j$, and pairwise covariates.\nSecond, we show how internal infections (caused by individuals under\nobservation) and external infections (caused environmental or community\nsources) can be handled simultaneously. In simulations, we show that these\nmethods produce valid point and interval estimates and that accounting for\nexternal infections is critical to consistent estimation. Finally, we use these\nmethods to analyze household surveillance data from Los Angeles County during\nthe 2009 influenza A(H1N1) pandemic.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 02:30:22 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2019 19:15:59 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Sharker", "Yushuf", ""], ["Kenah", "Eben", ""]]}, {"id": "1901.04927", "submitter": "Clement Atzberger", "authors": "Chrisgone Adede, Robert Oboko, Peter Wagacha and Clement Atzberger", "title": "A mixed model approach to drought prediction using artificial neural\n  networks: Case of an operational drought monitoring environment", "comments": "18 pages, 13 figures and 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Droughts, with their increasing frequency of occurrence, continue to\nnegatively affect livelihoods and elements at risk. For example, the 2011 in\ndrought in east Africa has caused massive losses document to have cost the\nKenyan economy over $12bn. With the foregoing, the demand for ex-ante drought\nmonitoring systems is ever-increasing. The study uses 10 precipitation and\nvegetation variables that are lagged over 1, 2 and 3-month time-steps to\npredict drought situations. In the model space search for the most predictive\nartificial neural network (ANN) model, as opposed to the traditional greedy\nsearch for the most predictive variables, we use the General Additive Model\n(GAM) approach. Together with a set of assumptions, we thereby reduce the\ncardinality of the space of models. Even though we build a total of 102 GAM\nmodels, only 21 have R2 greater than 0.7 and are thus subjected to the ANN\nprocess. The ANN process itself uses the brute-force approach that\nautomatically partitions the training data into 10 sub-samples, builds the ANN\nmodels in these samples and evaluates their performance using multiple metrics.\nThe results show the superiority of 1-month lag of the variables as compared to\nlonger time lags of 2 and 3 months. The champion ANN model recorded an R2 of\n0.78 in model testing using the out-of-sample data. This illustrates its\nability to be a good predictor of drought situations 1-month ahead.\nInvestigated as a classifier, the champion has a modest accuracy of 66% and a\nmulti-class area under the ROC curve (AUROC) of 89.99%\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 20:16:56 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Adede", "Chrisgone", ""], ["Oboko", "Robert", ""], ["Wagacha", "Peter", ""], ["Atzberger", "Clement", ""]]}, {"id": "1901.04935", "submitter": "Prabuchandran Krithivasan Jayachandran", "authors": "Prabuchandran K.J., Nitin Singh, Pankaj Dayama, Vinayaka Pandit", "title": "Change Point Detection for Compositional Multivariate Data", "comments": "9 pages,4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change point detection algorithms have numerous applications in fields of\nscientific and economic importance. We consider the problem of change point\ndetection on compositional multivariate data (each sample is a probability mass\nfunction), which is a practically important sub-class of general multivariate\ndata. While the problem of change-point detection is well studied in univariate\nsetting, and there are few viable implementations for a general multivariate\ndata, the existing methods do not perform well on compositional data. In this\npaper, we propose a parametric approach for change point detection in\ncompositional data. Moreover, using simple transformations on data, we extend\nour approach to handle any general multivariate data. Experimentally, we show\nthat our method performs significantly better on compositional data and is\ncompetitive on general data compared to the available state of the art\nimplementations.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 08:10:41 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["J.", "Prabuchandran K.", ""], ["Singh", "Nitin", ""], ["Dayama", "Pankaj", ""], ["Pandit", "Vinayaka", ""]]}, {"id": "1901.04993", "submitter": "Xinli Yu T", "authors": "Xinli Yu, Zheng Chen, Wei-Shih Yang, Xiaohua Hu, Erjia Yan", "title": "Large-Scale Joint Topic, Sentiment & User Preference Analysis for Online\n  Reviews", "comments": null, "journal-ref": null, "doi": "10.1109/BigData.2017.8258000", "report-no": null, "categories": "cs.IR cs.LG stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper presents a non-trivial reconstruction of a previous joint\ntopic-sentiment-preference review model TSPRA with stick-breaking\nrepresentation under the framework of variational inference (VI) and stochastic\nvariational inference (SVI). TSPRA is a Gibbs Sampling based model that solves\ntopics, word sentiments and user preferences altogether and has been shown to\nachieve good performance, but for large data set it can only learn from a\nrelatively small sample. We develop the variational models vTSPRA and svTSPRA\nto improve the time use, and our new approach is capable of processing millions\nof reviews. We rebuild the generative process, improve the rating regression,\nsolve and present the coordinate-ascent updates of variational parameters, and\nshow the time complexity of each iteration is theoretically linear to the\ncorpus size, and the experiments on Amazon data sets show it converges faster\nthan TSPRA and attains better results given the same amount of time. In\naddition, we tune svTSPRA into an online algorithm ovTSPRA that can monitor\noscillations of sentiment and preference overtime. Some interesting\nfluctuations are captured and possible explanations are provided. The results\ngive strong visual evidence that user preference is better treated as an\nindependent factor from sentiment.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 20:33:20 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Yu", "Xinli", ""], ["Chen", "Zheng", ""], ["Yang", "Wei-Shih", ""], ["Hu", "Xiaohua", ""], ["Yan", "Erjia", ""]]}, {"id": "1901.05070", "submitter": "Han Qiu", "authors": "Han Qiu", "title": "An Inattention Model for Traveler Behavior with e-Coupons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.TH stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we consider traveler coupon redemption behavior from the\nperspective of an urban mobility service. Assuming traveler behavior is in\naccordance with the principle of utility maximization, we first formulate a\nbaseline dynamical model for traveler's expected future trip sequence under the\nframework of Markov decision processes and from which we derive approximations\nof the optimal coupon redemption policy. However, we find that this baseline\nmodel cannot explain perfectly observed coupon redemption behavior of traveler\nfor a car-sharing service. To resolve this deviation from utility-maximizing\nbehavior, we suggest a hypothesis that travelers may not be aware of all\ncoupons available to them. Based on this hypothesis, we formulate an\ninattention model on unawareness, which is complementary to the existing models\nof inattention, and incorporate it into the baseline model. Estimation results\nshow that the proposed model better explains the coupon redemption dataset than\nthe baseline model. We also conduct a simulation experiment to quantify the\nnegative impact of unawareness on coupons' promotional effects. These results\ncan be used by mobility service operators to design effective coupon\ndistribution schemes in practice.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 13:10:11 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Qiu", "Han", ""]]}, {"id": "1901.05086", "submitter": "Aaron Clauset", "authors": "Aaron Clauset", "title": "On the frequency and severity of interstate wars", "comments": "8 pages, 5 figures; book chapter adaptation of A. Clauset, \"Trends\n  and fluctuations in the severity of interstate wars.\" Science Advances 4,\n  eaao3580 (2018), with additional historical and statistical context and\n  discussion", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lewis Fry Richardson argued that the frequency and severity of deadly\nconflicts of all kinds, from homicides to interstate wars and everything in\nbetween, followed universal statistical patterns: their frequency followed a\nsimple Poisson arrival process and their severity followed a simple power-law\ndistribution. Although his methods and data in the mid-20th century were\nneither rigorous nor comprehensive, his insights about violent conflicts have\nendured. In this chapter, using modern statistical methods and data, we show\nthat Richardson's original claims appear largely correct, with a few caveats.\nThese facts place important constraints on our understanding of the underlying\nmechanisms that produce individual wars and periods of peace, and shed light on\nthe persistent debate about trends in conflict.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 23:22:16 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Clauset", "Aaron", ""]]}, {"id": "1901.05124", "submitter": "Mohsen Sadatsafavi", "authors": "Mohsen Sadatsafavi, Mohammad Mansournia, Paul Gustafson", "title": "A threshold-free summary index for quantifying the capacity of\n  covariates to yield efficient treatment rules", "comments": "27 pages, 2 figures, 2 tables", "journal-ref": null, "doi": "10.1002/sim.8481", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The focus of this paper is on quantifying the capacity of covariates in\ndevising efficient treatment rules when data from a randomized trial are\navailable. Conventional one-variable-at-a-time subgroup analysis based on\nstatistical hypothesis testing of covariate-by-treatment interaction is\nill-suited for this purpose. The application of decision theory results in\ntreatment rules that compare the expected benefit of treatment given the\npatient's covariates against a treatment threshold. However, determining\ntreatment threshold is often context-specific, and any given threshold might\nseem arbitrary at the reporting stages of a clinical trial. We propose a\nthreshold-free metric that quantifies the capacity of a set of covariates\ntowards finding individuals who will benefit the most from treatment. The\nconstruct of the proposed metric is comparing the expected outcomes with and\nwithout knowledge of covariates when one of a two randomly selected patients\nare to be treated. We show that the resulting index can also be expressed in\nterms of integrated treatment benefit as a function of covariates over the\nentire range of treatment thresholds. We also propose a semi-parametric\nestimation method suitable for out-of-sample validation and adjustment for\noptimism. We use data from a clinical trial of preventive antibiotic therapy\nfor reducing exacerbation rate in Chronic Obstructive Pulmonary Disease to\ndemonstrate the calculations in a step-by-step fashion. The proposed index has\nintuitive and theoretically sound interpretation and can be estimated with\nrelative ease for a wide class of regression models. Beyond the conceptual\ndevelopments presented in this work, various aspects of estimation and\ninference for such metrics need to be pursued in future research.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 03:19:00 GMT"}, {"version": "v2", "created": "Wed, 6 Feb 2019 21:03:56 GMT"}, {"version": "v3", "created": "Fri, 23 Aug 2019 02:14:30 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Sadatsafavi", "Mohsen", ""], ["Mansournia", "Mohammad", ""], ["Gustafson", "Paul", ""]]}, {"id": "1901.05191", "submitter": "Massimiliano Russo", "authors": "Massimiliano Russo, Burton H. Singer and David B. Dunson", "title": "Multivariate mixed membership modeling: Inferring domain-specific risk\n  profiles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Characterizing the shared memberships of individuals in a classification\nscheme poses severe interpretability issues, even when using a moderate number\nof classes (say 4). Mixed membership models quantify this phenomenon, but they\ntypically focus on goodness-of-fit more than on interpretable inference. To\nachieve a good numerical fit, these models may in fact require many extreme\nprofiles, making the results difficult to interpret. We introduce a new class\nof multivariate mixed membership models that, when variables can be partitioned\ninto subject-matter based domains, can provide a good fit to the data using\nfewer profiles than standard formulations. The proposed model explicitly\naccounts for the blocks of variables corresponding to the distinct domains\nalong with a cross-domain correlation structure, which provides new information\nabout shared membership of individuals in a complex classification scheme. We\nspecify a multivariate logistic normal distribution for the membership vectors,\nwhich allows easy introduction of auxiliary information leveraging a latent\nmultivariate logistic regression. A Bayesian approach to inference, relying on\nP\\'olya gamma data augmentation, facilitates efficient posterior computation\nvia Markov Chain Monte Carlo. We apply this methodology to a spatially explicit\nstudy of malaria risk over time on the Brazilian Amazon frontier.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 09:32:42 GMT"}, {"version": "v2", "created": "Sun, 28 Apr 2019 15:31:07 GMT"}, {"version": "v3", "created": "Thu, 31 Dec 2020 17:57:04 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Russo", "Massimiliano", ""], ["Singer", "Burton H.", ""], ["Dunson", "David B.", ""]]}, {"id": "1901.05356", "submitter": "Christine Anderson-Cook", "authors": "Christine M. Anderson-Cook, Kary L. Myers, Lu Lu, Michael L. Fugate,\n  Kevin R. Quinlan, Norma Pawley", "title": "How to Host a Data Competition: Statistical Advice for Design and\n  Analysis of a Data Competition", "comments": "36 pages", "journal-ref": null, "doi": "10.1002/sam.11404", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data competitions rely on real-time leaderboards to rank competitor entries\nand stimulate algorithm improvement. While such competitions have become quite\npopular and prevalent, particularly in supervised learning formats, their\nimplementations by the host are highly variable. Without careful planning, a\nsupervised learning competition is vulnerable to overfitting, where the winning\nsolutions are so closely tuned to the particular set of provided data that they\ncannot generalize to the underlying problem of interest to the host. This paper\noutlines some important considerations for strategically designing relevant and\ninformative data sets to maximize the learning outcome from hosting a\ncompetition based on our experience. It also describes a post-competition\nanalysis that enables robust and efficient assessment of the strengths and\nweaknesses of solutions from different competitors, as well as greater\nunderstanding of the regions of the input space that are well-solved. The\npost-competition analysis, which complements the leaderboard, uses exploratory\ndata analysis and generalized linear models (GLMs). The GLMs not only expand\nthe range of results we can explore, they also provide more detailed analysis\nof individual sub-questions including similarities and differences between\nalgorithms across different types of scenarios, universally easy or hard\nregions of the input space, and different learning objectives. When coupled\nwith a strategically planned data generation approach, the methods provide\nricher and more informative summaries to enhance the interpretation of results\nbeyond just the rankings on the leaderboard. The methods are illustrated with a\nrecently completed competition to evaluate algorithms capable of detecting,\nidentifying, and locating radioactive materials in an urban environment.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 15:56:19 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Anderson-Cook", "Christine M.", ""], ["Myers", "Kary L.", ""], ["Lu", "Lu", ""], ["Fugate", "Michael L.", ""], ["Quinlan", "Kevin R.", ""], ["Pawley", "Norma", ""]]}, {"id": "1901.05369", "submitter": "Kaushik Jana Dr.", "authors": "Kaushik Jana and Debasis Sengupta", "title": "Improving linear quantile regression for replicated data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with improvement of linear quantile regression, when there\nare a few distinct values of the covariates but many replicates. On can improve\nasymptotic efficiency of the estimated regression coefficients by using\nsuitable weights in quantile regression, or simply by using weighted least\nsquares regression on the conditional sample quantiles. The asymptotic\nvariances of the unweighted and weighted estimators coincide only in some\nrestrictive special cases, e.g., when the density of the conditional response\nhas identical values at the quantile of interest over the support of the\ncovariate. The dominance of the weighted estimators is demonstrated in a\nsimulation study, and through the analysis of a data set on tropical cyclones.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 16:22:59 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2020 11:24:53 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Jana", "Kaushik", ""], ["Sengupta", "Debasis", ""]]}, {"id": "1901.05566", "submitter": "Majdi Radaideh", "authors": "Majdi I. Radaideh, Mohammad I. Radaideh", "title": "Application of Stochastic and Deterministic Techniques for Uncertainty\n  Quantification and Sensitivity Analysis of Energy Systems", "comments": "References list is updated, the paper is converted to a more\n  professional and easy to read template, the technical content stays the same", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensitivity analysis (SA) and uncertainty quantification (UQ) are used to\nassess and improve engineering models. In this study, various methods of SA and\nUQ are described and applied in theoretical and practical examples for use in\nenergy system analysis. This paper includes local SA (one-at-a-time linear\nperturbation), global SA (Morris screening), variance decomposition (Sobol\nindices), and regression-based SA. For UQ, stochastic methods (Monte Carlo\nsampling) and deterministic methods (using SA profiles) are used. Simple test\nproblems are included to demonstrate the described methods where input\nparameter interactions, linear correlation, model nonlinearity, local\nsensitivity, output uncertainty, and variance contribution are explored.\nPractical applications of analyzing the efficiency and power output uncertainty\nof a molten carbonate fuel cell (MCFC) are conducted. Using different methods,\nthe uncertainty in the MCFC responses is about 10%. Both SA and UQ methods\nagree on the importance ranking of the fuel cell operating temperature and\ncathode activation energy as the most influential parameters. Both parameters\ncontribute to more than 90% of the maximum power and efficiency variance. The\nmethods applied in this paper can be used to achieve a comprehensive\nmathematical understanding of a particular energy model, which can lead to\nbetter performance.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 00:00:27 GMT"}, {"version": "v2", "created": "Fri, 21 Jun 2019 19:57:12 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Radaideh", "Majdi I.", ""], ["Radaideh", "Mohammad I.", ""]]}, {"id": "1901.05722", "submitter": "Gunther Schauberger", "authors": "Andreas Groll and Jonas Heiner and Gunther Schauberger and J\\\"orn\n  Uhrmeister", "title": "Prediction of the 2019 IHF World Men's Handball Championship - An\n  underdispersed sparse count data regression model", "comments": "arXiv admin note: substantial text overlap with arXiv:1806.03208", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we compare several different modeling approaches for count data\napplied to the scores of handball matches with regard to their predictive\nperformances based on all matches from the four previous IHF World Men's\nHandball Championships 2011 - 2017: (underdispersed) Poisson regression models,\nGaussian response models and negative binomial models. All models are based on\nthe teams' covariate information. Within this comparison, the Gaussian response\nmodel turns out to be the best-performing prediction method on the training\ndata and is, therefore, chosen as the final model. Based on its estimates, the\nIHF World Men's Handball Championship 2019 is simulated repeatedly and winning\nprobabilities are obtained for all teams. The model clearly favors Denmark\nbefore France. Additionally, we provide survival probabilities for all teams\nand at all tournament stages as well as probabilities for all teams to qualify\nfor the main round.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 10:41:16 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Groll", "Andreas", ""], ["Heiner", "Jonas", ""], ["Schauberger", "Gunther", ""], ["Uhrmeister", "J\u00f6rn", ""]]}, {"id": "1901.05885", "submitter": "Hamidreza Ghasemi Damavandi", "authors": "Hamidreza Ghasemi Damavandi, Reepal Shah", "title": "A Learning Framework for An Accurate Prediction of Rainfall Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present work is aimed to examine the potential of advanced machine\nlearning strategies to predict the monthly rainfall (precipitation) for the\nIndus Basin, using climatological variables such as air temperature,\ngeo-potential height, relative humidity and elevation. In this work, the focus\nis on thirteen geographical locations, called index points, within the basin.\nArguably, not all of the hydrological components are relevant to the\nprecipitation rate, and therefore, need to be filtered out, leading to a\nlower-dimensional feature space. Towards this goal, we adopted the gradient\nboosting method to extract the most contributive features for precipitation\nrate prediction. Five state-of-the-art machine learning methods have then been\ntrained where pearson correlation coefficient and mean absolute error have been\nreported as the prediction performance criteria. The Random Forest regression\nmodel outperformed the other regression models achieving the maximum pearson\ncorrelation coefficient and minimum mean absolute error for most of the index\npoints. Our results suggest the relative humidity (for pressure levels of 300\nmb and 150 mb, respectively), the u-direction wind (for pressure level of 700\nmb), air temperature (for pressure levels of 150 mb and 10 mb, respectively) as\nthe top five influencing features for accurate forecasting the precipitation\nrate.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 16:48:56 GMT"}], "update_date": "2019-01-27", "authors_parsed": [["Damavandi", "Hamidreza Ghasemi", ""], ["Shah", "Reepal", ""]]}, {"id": "1901.06016", "submitter": "Piyush Tagade", "authors": "Saket Mishra and Piyush Tagade", "title": "Learning formation energy of inorganic compounds using matrix variate\n  deep Gaussian process", "comments": "On further analysis, authors found the usage of Gegenbauer polynomial\n  combined with radial distribution function and angular distribution function\n  inappropriate. Authors withdraw the paper due to erroneous use of\n  fingerprinting", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future advancement of engineering applications is dependent on design of\nnovel materials with desired properties. Enormous size of known chemical space\nnecessitates use of automated high throughput screening to search the desired\nmaterial. The high throughput screening uses quantum chemistry calculations to\npredict material properties, however, computational complexity of these\ncalculations often imposes prohibitively high cost on the search for desired\nmaterial. This critical bottleneck is resolved by using deep machine learning\nto emulate the quantum computations. However, the deep learning algorithms\nrequire a large training dataset to ensure an acceptable generalization, which\nis often unavailable a-priory. In this paper, we propose a deep Gaussian\nprocess based approach to develop an emulator for quantum calculations. We\nfurther propose a novel molecular descriptor that enables implementation of the\nproposed approach. As demonstrated in this paper, the proposed approach can be\nimplemented using a small dataset. We demonstrate efficacy of our approach for\nprediction of formation energy of inorganic molecules.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2018 17:08:42 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 15:21:44 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Mishra", "Saket", ""], ["Tagade", "Piyush", ""]]}, {"id": "1901.06242", "submitter": "Charith Perera", "authors": "Yuchao Zhou, Suparna De, Gideon Ewa, Charith Perera, Klaus Moessner", "title": "Data-driven Air Quality Characterisation for Urban Environments: a Case\n  Study", "comments": "IEEE ACCESS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The economic and social impact of poor air quality in towns and cities is\nincreasingly being recognised, together with the need for effective ways of\ncreating awareness of real-time air quality levels and their impact on human\nhealth. With local authority maintained monitoring stations being\ngeographically sparse and the resultant datasets also featuring missing labels,\ncomputational data-driven mechanisms are needed to address the data sparsity\nchallenge. In this paper, we propose a machine learning-based method to\naccurately predict the Air Quality Index (AQI), using environmental monitoring\ndata together with meteorological measurements. To do so, we develop an air\nquality estimation framework that implements a neural network that is enhanced\nwith a novel Non-linear Autoregressive neural network with exogenous input\n(NARX), especially designed for time series prediction. The framework is\napplied to a case study featuring different monitoring sites in London, with\ncomparisons against other standard machine-learning based predictive algorithms\nshowing the feasibility and robust performance of the proposed method for\ndifferent kinds of areas within an urban region.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 13:40:03 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Zhou", "Yuchao", ""], ["De", "Suparna", ""], ["Ewa", "Gideon", ""], ["Perera", "Charith", ""], ["Moessner", "Klaus", ""]]}, {"id": "1901.06462", "submitter": "Jingchen Hu", "authors": "Jingchen Hu, Terrance D. Savitsky", "title": "Bayesian Pseudo Posterior Synthesis for Data Privacy Protection", "comments": "This is to replace arXiv:1908.07639", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical agencies utilize models to synthesize respondent-level data for\nrelease to the general public as an alternative to the actual data records. A\nBayesian model synthesizer encodes privacy protection by employing a\nhierarchical prior construction that induces smoothing of the real data\ndistribution. Synthetic respondent-level data records are often preferred to\nsummary data tables due to the many possible uses by researchers and data\nanalysts. Agencies balance a trade-off between utility of the synthetic data\nversus disclosure risks and hold a specific target threshold for disclosure\nrisk before releasing synthetic datasets. We introduce a pseudo posterior\nlikelihood that exponentiates each contribution by an observation\nrecord-indexed weight in (0, 1), defined to be inversely proportional to the\ndisclosure risk for that record in the synthetic data. Our use of a vector of\nweights allows more precise downweighting of high risk records in a fashion\nthat better preserves utility as compared with using a scalar weight. We\nillustrate our method with a simulation study and an application to the\nConsumer Expenditure Survey of the U.S. Bureau of Labor Statistics. We\ndemonstrate how the frequentist consistency and uncertainty quantification are\naffected by the inverse risk-weighting.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jan 2019 03:29:28 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 16:02:25 GMT"}, {"version": "v3", "created": "Fri, 15 May 2020 21:03:42 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Hu", "Jingchen", ""], ["Savitsky", "Terrance D.", ""]]}, {"id": "1901.06766", "submitter": "Shuguan Yang", "authors": "Shuguan Yang, Sean Qian", "title": "Understanding and predicting travel time with spatio-temporal features\n  of network traffic flow, weather and incidents", "comments": null, "journal-ref": "IEEE Intelligent Transportation Systems Magazine, 2019, Vol. 3,\n  221-234", "doi": "10.1109/MITS.2019.2919615", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Travel time on a route varies substantially by time of day and from day to\nday. It is critical to understand to what extent this variation is correlated\nwith various factors, such as weather, incidents, events or travel demand level\nin the context of dynamic networks. This helps a better decision making for\ninfrastructure planning and real-time traffic operation. We propose a\ndata-driven approach to understand and predict highway travel time using\nspatio-temporal features of those factors, all of which are acquired from\nmultiple data sources. The prediction model holistically selects the most\nrelated features from a high-dimensional feature space by correlation analysis,\nprinciple component analysis and LASSO. We test and compare the performance of\nseveral regression models in predicting travel time 30 min in advance via two\ncase studies: (1) a 6-mile highway corridor of I-270N in D.C. region, and (2) a\n2.3-mile corridor of I-376E in Pittsburgh region. We found that some\nbottlenecks scattered in the network can imply congestion on those corridors at\nleast 30 minutes in advance, including those on the alternative route to the\ncorridors of study. In addition, real-time travel time is statistically related\nto incidents on some specific locations, morning/afternoon travel demand,\nvisibility, precipitation, wind speed/gust and the weather type. All those\nspatio-temporal information together help improve prediction accuracy,\ncomparing to using only speed data. In both case studies, random forest shows\nthe most promise, reaching a root-mean-squared error of 16.6\\% and 17.0\\%\nrespectively in afternoon peak hours for the entire year of 2014.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 01:58:53 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 03:53:56 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Yang", "Shuguan", ""], ["Qian", "Sean", ""]]}, {"id": "1901.06889", "submitter": "Daniel J. Schad", "authors": "Daniel J. Schad, Shravan Vasishth", "title": "The posterior probability of a null hypothesis given a statistically\n  significant result", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When researchers carry out a null hypothesis significance test, it is\ntempting to assume that a statistically significant result lowers Prob(H0), the\nprobability of the null hypothesis being true. Technically, such a statement is\nmeaningless for various reasons: e.g., the null hypothesis does not have a\nprobability associated with it. However, it is possible to relax certain\nassumptions to compute the posterior probability Prob(H0) under repeated\nsampling. We show in a step-by-step guide that the intuitively appealing\nbelief, that Prob(H0) is low when significant results have been obtained under\nrepeated sampling, is in general incorrect and depends greatly on: (a) the\nprior probability of the null being true; (b) Type I error, and (c) Type II\nerror. Through step-by-step simulations using open-source code in the R System\nof Statistical Computing, we show that uncertainty about the null hypothesis\nbeing true often remains high despite a significant result. To help the reader\ndevelop intuitions about this common misconception, we provide a Shiny app\n(https://danielschad.shinyapps.io/probnull/). We expect that this tutorial will\nbe widely useful for students and researchers in many fields of psychological\nscience and beyond, and will help researchers better understand and judge\nresults from null hypothesis significance tests.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 11:45:59 GMT"}, {"version": "v2", "created": "Sat, 23 Mar 2019 12:46:10 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Schad", "Daniel J.", ""], ["Vasishth", "Shravan", ""]]}, {"id": "1901.07396", "submitter": "Vahid Tadayon", "authors": "Vahid Tadayon", "title": "Bayesian Prediction of Nitrate Concentration Using a Gaussian\n  Log-Gaussian Spatial Model with Measurement Error in Explanatory Variables", "comments": "This article has been removed by arXiv administrators due to\n  falsified authorship", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article has been removed by arXiv administrators due to falsified\nauthorship.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 07:03:31 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Tadayon", "Vahid", ""]]}, {"id": "1901.07504", "submitter": "Yaoyuan Vincent Tan", "authors": "Yaoyuan Vincent Tan and Jason Roy", "title": "Bayesian additive regression trees and the General BART model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian additive regression trees (BART) is a flexible prediction\nmodel/machine learning approach that has gained widespread popularity in recent\nyears. As BART becomes more mainstream, there is an increased need for a paper\nthat walks readers through the details of BART, from what it is to why it\nworks. This tutorial is aimed at providing such a resource. In addition to\nexplaining the different components of BART using simple examples, we also\ndiscuss a framework, the General BART model, that unifies some of the recent\nBART extensions, including semiparametric models, correlated outcomes,\nstatistical matching problems in surveys, and models with weaker distributional\nassumptions. By showing how these models fit into a single framework, we hope\nto demonstrate a simple way of applying BART to research problems that go\nbeyond the original independent continuous or binary outcomes framework.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 18:27:40 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Tan", "Yaoyuan Vincent", ""], ["Roy", "Jason", ""]]}, {"id": "1901.07607", "submitter": "Xilei Zhao", "authors": "Xiang Yan, Xilei Zhao, Yuan Han, Pascal Van Hentenryck, Tawanna\n  Dillahunt", "title": "Mobility-on-demand versus fixed-route transit systems: an evaluation of\n  traveler preferences in low-income communities", "comments": "27 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emerging transportation technologies, such as ride-hailing and autonomous\nvehicles, are disrupting the transportation sector and transforming public\ntransit. Some transit observers envision future public transit to be integrated\ntransit systems with fixed-route services running along major corridors and\non-demand ridesharing services covering lower-density areas. A switch from a\nconventional fixed-route service model to this kind of integrated\nmobility-on-demand transit system, however, may elicit varied responses from\nlocal residents. This paper evaluates traveler preferences for a proposed\nintegrated mobility-on-demand transit system versus the existing fixed-route\nsystem, with a particular focus on disadvantaged travelers. We conducted a\nsurvey in two low-resource communities in the United States, namely, Detroit\nand Ypsilanti, Michigan. A majority of survey respondents preferred a\nmobility-on-demand transit system over a fixed-route one. Based on ordered\nlogit model outputs, we found a stronger preference for mobility-on-demand\ntransit among males, college graduates, individuals who have never heard of or\nused ride-hailing before, and individuals who currently receive inferior\ntransit services. By contrast, preferences varied little by age, income, race,\nor disability status. The most important benefit of a mobility-on-demand\ntransit system perceived by the survey respondents is enhanced transit\naccessibility to different destinations, whereas their major concerns include\nthe need to actively request rides, possible transit-fare increases, and\npotential technological failures. Addressing the concerns of female riders and\naccommodating the needs of less technology-proficient individuals should be\nmajor priorities for transit agencies that are considering mobility-on-demand\ninitiatives.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 20:26:58 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Yan", "Xiang", ""], ["Zhao", "Xilei", ""], ["Han", "Yuan", ""], ["Van Hentenryck", "Pascal", ""], ["Dillahunt", "Tawanna", ""]]}, {"id": "1901.07903", "submitter": "Jerome Stenger", "authors": "Jerome Stenger (EDF R&D PRISME, GdR MASCOT-NUM, IMT), Fabrice Gamboa\n  (IMT), Merlin Keller (EDF R&D PRISME), Bertrand Iooss (EDF R&D PRISME, IMT,\n  GdR MASCOT-NUM)", "title": "Optimal Uncertainty Quantification of a risk measurement from a\n  thermal-hydraulic code using Canonical Moments", "comments": "arXiv admin note: substantial text overlap with arXiv:1811.12788", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an industrial computer code related to nuclear safety. A major topic\nof interest is to assess the uncertainties tainting the results of a computer\nsimulation. In this work we gain robustness on the quantification of a risk\nmeasurement by accounting for all sources of uncertainties tainting the inputs\nof a computer code. To that extent, we evaluate the maximum quantile over a\nclass of distributions defined only by constraints on their moments. Two\noptions are available when dealing with such complex optimization problems: one\ncan either optimize under constraints; or preferably, one should reformulate\nthe objective function. We identify a well suited parameterization to compute\nthe optimal quantile based on the theory of canonical moments. It allows an\neffective, free of constraints, optimization.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 10:22:59 GMT"}, {"version": "v2", "created": "Wed, 28 Aug 2019 14:08:00 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Stenger", "Jerome", "", "EDF R&D PRISME, GdR MASCOT-NUM, IMT"], ["Gamboa", "Fabrice", "", "IMT"], ["Keller", "Merlin", "", "EDF R&D PRISME"], ["Iooss", "Bertrand", "", "EDF R&D PRISME, IMT,\n  GdR MASCOT-NUM"]]}, {"id": "1901.07994", "submitter": "Seyed MohammadReza Hosseini", "authors": "Seyed MohammadReza Hosseini, Afshin Isazadeh, Ali Noroozi, Mohammad\n  Ali Sebt", "title": "Uncertainty Principle in Distributed MIMO Radars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radar uncertainty principle indicates that there is an inherent invariance in\nthe product of the time-delay and Doppler-shift measurement accuracy and\nresolution which can be tuned by the waveform at transmitter. In this paper,\nbased on the radar uncertainty principle, a conceptual waveform design is\nproposed for a distributed multiple-input multiple-output (MIMO) radar system\nin order to improve the Cramer-Rao lower bound (CRLB) of the target position\nand velocity. To this end, a non-convex band constrained optimization problem\nis formulated, and a local and the global solution to the problem are obtained\nby sequential quadratic programming (SQP) and particle swarm algorithms,\nrespectively. Numerical results are also included to illustrate the\neffectiveness of the proposed mechanism on the CRLB of the target position and\nvelocity. By numerical results, it is also concluded that the global solution\nto the optimization problem is obtained at a vertex of the bounding box.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 16:45:59 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Hosseini", "Seyed MohammadReza", ""], ["Isazadeh", "Afshin", ""], ["Noroozi", "Ali", ""], ["Sebt", "Mohammad Ali", ""]]}, {"id": "1901.08105", "submitter": "German Rosati Dr.", "authors": "Antonio V\\'azquez Brust and Tom\\'as Olego and Germ\\'an Rosati", "title": "Construcci\\'on de un Mapa de Vulnerabilidad Sanitaria en Argentina a\n  partir de datos p\\'ublicos", "comments": "28 pages, in Spanish. 9 figures, Working Paper in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This document is intended to present in detail the processing criteria and\nthe analysis techniques used for the production of the Vulnerability Map\nSanitary based on the use of public and open data sources. The paper makes use\nof statistical analysis techniques (MCA, PCA, etc.) and machine learning\n(autoencoders) for the processing and analysis of information. The final\nproduct is a map at the census track level that seeks to quantify the\npopulation's access to basic health benefits.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 04:59:50 GMT"}, {"version": "v2", "created": "Fri, 12 Apr 2019 02:26:31 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Brust", "Antonio V\u00e1zquez", ""], ["Olego", "Tom\u00e1s", ""], ["Rosati", "Germ\u00e1n", ""]]}, {"id": "1901.08117", "submitter": "Cecilia Balocchi", "authors": "Cecilia Balocchi and Shane T. Jensen", "title": "Spatial Modeling of Trends in Crime over Time in Philadelphia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the relationship between change in crime over time and the\ngeography of urban areas is an important problem for urban planning. Accurate\nestimation of changing crime rates throughout a city would aid law enforcement\nas well as enable studies of the association between crime and the built\nenvironment. Bayesian modeling is a promising direction since areal data\nrequire principled sharing of information to address spatial autocorrelation\nbetween proximal neighborhoods. We develop several Bayesian approaches to\nspatial sharing of information between neighborhoods while modeling trends in\ncrime counts over time. We apply our methodology to estimate changes in crime\nthroughout Philadelphia over the 2006-15 period, while also incorporating\nspatially-varying economic and demographic predictors. We find that the local\nshrinkage imposed by a conditional autoregressive model has substantial\nbenefits in terms of out-of-sample predictive accuracy of crime. We also\nexplore the possibility of spatial discontinuities between neighborhoods that\ncould represent natural barriers or aspects of the built environment.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 20:10:59 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 19:41:41 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Balocchi", "Cecilia", ""], ["Jensen", "Shane T.", ""]]}, {"id": "1901.08196", "submitter": "Liyan Xie", "authors": "Liyan Xie, Yao Xie, George V. Moustakides", "title": "Asynchronous Multi-Sensor Change-Point Detection for Seismic Tremors", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the sequential change-point detection for asynchronous\nmulti-sensors, where each sensor observe a signal (due to change-point) at\ndifferent times. We propose an asynchronous Subspace-CUSUM procedure based on\njointly estimating the unknown signal waveform and the unknown relative delays\nbetween the sensors. Using the estimated delays, we can align signals and use\nthe subspace to combine the multiple sensor observations. We derive the optimal\ndrift parameter for the proposed procedure, and characterize the relationship\nbetween the expected detection delay, average run length (of false alarms), and\nthe energy of the time-varying signal. We demonstrate the good performance of\nthe proposed procedure using simulation and real data. We also demonstrate that\nthe proposed procedure outperforms the well-known `one-shot procedure' in\ndetecting weak and asynchronous signals.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 02:03:34 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Xie", "Liyan", ""], ["Xie", "Yao", ""], ["Moustakides", "George V.", ""]]}, {"id": "1901.08434", "submitter": "George Moustakides", "authors": "George V. Moustakides", "title": "Detecting Changes in Hidden Markov Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of sequential detection of a change in the\nstatistical behavior of a hidden Markov model. By adopting a worst-case\nanalysis with respect to the time of change and by taking into account the data\nthat can be accessed by the change-imposing mechanism we offer alternative\nformulations of the problem. For each formulation we derive the optimum\nShewhart test that maximizes the worst-case detection probability while\nguaranteeing infrequent false alarms.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 14:41:14 GMT"}, {"version": "v2", "created": "Sat, 26 Jan 2019 15:02:20 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Moustakides", "George V.", ""]]}, {"id": "1901.08497", "submitter": "Georgios Giasemidis Dr", "authors": "Georgios Giasemidis and Stephen Haben", "title": "Modelling the Demand and Uncertainty of Low Voltage Networks and the\n  Effect of non-Domestic Consumers", "comments": "32 pages, 16 figures, 5 tables", "journal-ref": "Sustainable Energy, Grids and Networks, Volume 16, December 2018,\n  Pages 341-350", "doi": "10.1016/j.segan.2018.10.002", "report-no": null, "categories": "stat.AP physics.data-an physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing use and spread of low carbon technologies are expected to\ncause new patterns in electric demand and set novel challenges to a\ndistribution network operator (DNO). In this study, we build upon a recently\nintroduced method, called \"buddying\", which simulates low voltage (LV) networks\nof both residential and non-domestic (e.g. shops, offices, schools, hospitals,\netc.) customers through optimization (via a genetic algorithm) of demands based\non limited monitored and customer data. The algorithm assigns a limited but\ndiverse number of monitored households (the \"buddies\") to the unmonitored\ncustomers on a network. We study and compare two algorithms, one where\nsubstation monitoring data is available and a second where no substation\ninformation is used. Despite the roll out of monitoring equipment at domestic\nproperties and/or substations, less data is available for commercial customers.\nThis study focuses on substations with commercial customers most of which have\nno monitored `buddy', in which case a profile must be created. Due to the\nvolatile nature of the low voltage networks, uncertainty bounds are crucial for\noperational purposes. We introduce and demonstrate two techniques for modelling\nthe confidence bounds on the modelled LV networks. The first method uses\nprobabilistic forecast methods based on substation monitoring; the second only\nuses a simple bootstrap of the sample of monitored customers but has the\nadvantage of not requiring monitoring at the substation. These modelling tools,\nbuddying and uncertainty bounds, can give further insight to a DNO to better\nplan and manage the network when limited information is available.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 16:51:58 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Giasemidis", "Georgios", ""], ["Haben", "Stephen", ""]]}, {"id": "1901.08874", "submitter": "Thordis Thorarinsdottir", "authors": "Ola Haug, Thordis L Thorarinsdottir, Sigrunn H S{\\o}rbye and Christian\n  L E Franzke", "title": "Spatial trend analysis of gridded temperature data at varying spatial\n  scales", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical assessments of trends in gridded temperature data perform\nindependent evaluations across the grid, thus, ignoring spatial correlations in\nthe trend estimates. In particular, this affects assessments of trend\nsignificance as evaluation of the collective significance of individual tests\nis commonly neglected. In this article we build a space-time hierarchical\nBayesian model for temperature anomalies where the trend coefficient is modeled\nby a latent Gaussian random field. This enables us to calculate simultaneous\ncredible regions for joint significance assessments. In a case study, we assess\nsummer season trends in 65 years of gridded temperature data over Europe. We\nfind that while spatial smoothing generally results in larger regions where the\nnull hypothesis of no trend is rejected, this is not the case for all\nsub-regions.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 14:05:47 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Haug", "Ola", ""], ["Thorarinsdottir", "Thordis L", ""], ["S\u00f8rbye", "Sigrunn H", ""], ["Franzke", "Christian L E", ""]]}, {"id": "1901.08941", "submitter": "David Darmon", "authors": "David Darmon, William Rand, Michelle Girvan", "title": "Computational landscape of user behavior on social media", "comments": null, "journal-ref": "Physical Review E 98.6 (2018): 062306", "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing abundance of 'digital footprints' left by human\ninteractions in online environments, e.g., social media and app use, the\nability to model complex human behavior has become increasingly possible. Many\napproaches have been proposed, however, most previous model frameworks are\nfairly restrictive. We introduce a new social modeling approach that enables\nthe creation of models directly from data with minimal a priori restrictions on\nthe model class. In particular, we infer the minimally complex, maximally\npredictive representation of an individual's behavior when viewed in isolation\nand as driven by a social input. We then apply this framework to a\nheterogeneous catalog of human behavior collected from fifteen thousand users\non the microblogging platform Twitter. The models allow us to describe how a\nuser processes their past behavior and their social inputs. Despite the\ndiversity of observed user behavior, most models inferred fall into a small\nsubclass of all possible finite-state processes. Thus, our work demonstrates\nthat user behavior, while quite complex, belies simple underlying computational\nstructures.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 15:59:41 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Darmon", "David", ""], ["Rand", "William", ""], ["Girvan", "Michelle", ""]]}, {"id": "1901.09145", "submitter": "Md Al Masum Bhuiyan", "authors": "Maria C Mariani, Md Al Masum Bhuiyan, Osei K Tweneboah, Hector\n  Gonzalez-Huizar, Ionut Florescu", "title": "Volatility Models Applied to Geophysics and High Frequency Financial\n  Market Data", "comments": "30 Pages, 23 figures", "journal-ref": "Physica A: Statistical Mechanics and its Applications, Volume 503,\n  1 August 2018, Pages 304-321", "doi": "10.1016/j.physa.2018.02.167", "report-no": null, "categories": "stat.AP econ.EM q-fin.ST", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This work is devoted to the study of modeling geophysical and financial time\nseries. A class of volatility models with time-varying parameters is presented\nto forecast the volatility of time series in a stationary environment. The\nmodeling of stationary time series with consistent properties facilitates\nprediction with much certainty. Using the GARCH and stochastic volatility\nmodel, we forecast one-step-ahead suggested volatility with +/- 2 standard\nprediction errors, which is enacted via Maximum Likelihood Estimation. We\ncompare the stochastic volatility model relying on the filtering technique as\nused in the conditional volatility with the GARCH model. We conclude that the\nstochastic volatility is a better forecasting tool than GARCH (1, 1), since it\nis less conditioned by autoregressive past information.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jan 2019 02:35:54 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Mariani", "Maria C", ""], ["Bhuiyan", "Md Al Masum", ""], ["Tweneboah", "Osei K", ""], ["Gonzalez-Huizar", "Hector", ""], ["Florescu", "Ionut", ""]]}, {"id": "1901.09214", "submitter": "Pedro Ramos", "authors": "Francisco Louzada, Pedro Luiz Ramos, Hayala C. C. Souza, Gleici da\n  Silva Castro Perdona", "title": "On the unified zero-inflated cure-rate survival models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a unified version for survival models that includes\nzero-inflation and cure rate proportions, and allows different distributions\nfor the unknown competitive causes. Our model has as particular cases several\nusual cure rate survival models.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jan 2019 13:47:19 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 15:21:54 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Louzada", "Francisco", ""], ["Ramos", "Pedro Luiz", ""], ["Souza", "Hayala C. C.", ""], ["Perdona", "Gleici da Silva Castro", ""]]}, {"id": "1901.09249", "submitter": "Paul McNicholas", "authors": "Tyler Roick, Dimitris Karlis and Paul D. McNicholas", "title": "Clustering Discrete-Valued Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a need for the development of models that are able to account for\ndiscreteness in data, along with its time series properties and correlation.\nOur focus falls on INteger-valued AutoRegressive (INAR) type models. The INAR\ntype models can be used in conjunction with existing model-based clustering\ntechniques to cluster discrete-valued time series data. With the use of a\nfinite mixture model, several existing techniques such as the selection of the\nnumber of clusters, estimation using expectation-maximization and model\nselection are applicable. The proposed model is then demonstrated on real data\nto illustrate its clustering applications.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jan 2019 17:36:51 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 20:10:12 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Roick", "Tyler", ""], ["Karlis", "Dimitris", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1901.09299", "submitter": "Saviz Saei", "authors": "Saviz Saei, Mohsen Mohammadi, Mahsa Fekriseri, Kouroush Jenab,", "title": "A computational method for estimating Burr XII parameters with complete\n  and multiple censored data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flexibility in shape and scale of Burr XII distribution can make close\napproximation of numerous well-known probability density functions. Due to\nthese capabilities, the usages of Burr XII distribution are applied in risk\nanalysis, lifetime data analysis and process capability estimation. In this\npaper the Cross-Entropy (CE) method is further developed in terms of Maximum\nLikelihood Estimation (MLE) to estimate the parameters of Burr XII distribution\nfor the complete data or in the presence of multiple censoring. A simulation\nstudy is conducted to evaluate the performance of the MLE by means of CE method\nfor different parameter settings and sample sizes. The results are compared to\nother existing methods in both uncensored and censored situations.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jan 2019 01:18:16 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Saei", "Saviz", ""], ["Mohammadi", "Mohsen", ""], ["Fekriseri", "Mahsa", ""], ["Jenab", "Kouroush", ""]]}, {"id": "1901.09317", "submitter": "Donghui Yan", "authors": "Donghui Yan, Congcong Li, Na Cong, Le Yu, Peng Gong", "title": "A Structured Approach to the Analysis of Remote Sensing Images", "comments": "27 pages, 13 figures", "journal-ref": null, "doi": "10.1080/01431161.2019.1607611", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of studies for the analysis of remote sensing images has been\ngrowing exponentially in the last decades. Many studies, however, only report\nresults---in the form of certain performance metrics---by a few selected\nalgorithms on a training and testing sample. While this often yields valuable\ninsights, it tells little about some important aspects. For example, one might\nbe interested in understanding the nature of a study by the interaction of\nalgorithm, features, and the sample as these collectively contribute to the\noutcome; among these three, which would be a more productive direction in\nimproving a study; how to assess the sample quality or the value of a set of\nfeatures etc. With a focus on land-use classification, we advocate the use of a\nstructured analysis. The output of a study is viewed as the result of the\ninterplay among three input dimensions: feature, sample, and algorithm.\nSimilarly, another dimension, the error, can be decomposed into error along\neach input dimension. Such a structural decomposition of the inputs or error\ncould help better understand the nature of the problem and potentially suggest\ndirections for improvement. We use the analysis of a remote sensing image at a\nstudy site in Guangzhou, China, to demonstrate how such a structured analysis\ncould be carried out and what insights it generates. The structured analysis\ncould be applied to a new study, or as a diagnosis to an existing one. We\nexpect this will inform practice in the analysis of remote sensing images, and\nhelp advance the state-of-the-art of land-use classification.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jan 2019 04:52:26 GMT"}, {"version": "v2", "created": "Sat, 4 Jan 2020 22:08:15 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Yan", "Donghui", ""], ["Li", "Congcong", ""], ["Cong", "Na", ""], ["Yu", "Le", ""], ["Gong", "Peng", ""]]}, {"id": "1901.09475", "submitter": "Eric Strobl", "authors": "Eric V. Strobl", "title": "Causal Discovery with a Mixture of DAGs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal processes in biomedicine may contain cycles, evolve over time or\ndiffer between populations. However, many graphical models cannot accommodate\nthese conditions. We propose to model causation using a mixture of directed\ncyclic graphs (DAGs), where the joint distribution in a population follows a\nDAG at any single point in time but potentially different DAGs across time. We\nalso introduce an algorithm called Causal Inference over Mixtures that uses\nlongitudinal data to infer a graph summarizing the causal relations generated\nfrom a mixture of DAGs. Experiments demonstrate improved performance compared\nto prior approaches.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 00:57:20 GMT"}, {"version": "v2", "created": "Sat, 5 Sep 2020 14:58:23 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Strobl", "Eric V.", ""]]}, {"id": "1901.09562", "submitter": "Bertrand Cloez", "authors": "B Cloez (MISTEA), T Daufresne (UMR Eco\\&Sols), M Kerioui (SPHERE), B\n  Fontez (MISTEA)", "title": "Galton-Watson process and bayesian inference: A turnkey method for the\n  viability study of small populations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  1 Sharp prediction of extinction times is needed in biodiversity monitoring\nand conservation management. 2 The Galton-Watson process is a classical\nstochastic model for describing population dynamics. Its evolution is like the\nmatrix population model where offspring numbers are random. Extinction\nprobability, extinction time, abundance are well known and given by explicit\nformulas. In contrast with the deterministic model, it can be applied to small\npopulations. 3 Parameters of this model can be estimated through the Bayesian\ninference framework. This enables to consider non-arbitrary scenarios. 4 We\nshow how coupling Bayesian inference with the Galton-Watson model provides\nseveral features: i) a flexible modelling approach with easily understandable\nparameters ii) compatibility with the classical matrix population model (Leslie\ntype model) iii) A non-computational approach which then leads to more\ninformation with less computing iv) a non-arbitrary choice for scenarios,\nparameters... It can be seen to go one step further than the classical matrix\npopulation model for the viability problem. 5 To illustrate these features, we\nprovide analysis details for two examples whose one of which is a real life\nexample.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 09:14:44 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Cloez", "B", "", "MISTEA"], ["Daufresne", "T", "", "UMR Eco\\&Sols"], ["Kerioui", "M", "", "SPHERE"], ["Fontez", "B", "", "MISTEA"]]}, {"id": "1901.09587", "submitter": "Santosh Kumar", "authors": "Ayana Sarkar, Santosh Kumar", "title": "Bures-Hall Ensemble: Spectral Densities and Average Entropies", "comments": "Published version; 4 figures, 2 tables", "journal-ref": "Journal of Physics A: Mathematical and Theoretical, Volume 52,\n  Page - 295203, Year 2019", "doi": "10.1088/1751-8121/ab2675", "report-no": null, "categories": "math-ph math.MP quant-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an ensemble of random density matrices distributed according to\nthe Bures measure. The corresponding joint probability density of eigenvalues\nis described by the fixed trace Bures-Hall ensemble of random matrices which,\nin turn, is related to its unrestricted trace counterpart via a Laplace\ntransform. We investigate the spectral statistics of both these ensembles and,\nin particular, focus on the level density, for which we obtain exact\nclosed-form results involving Pfaffians. In the fixed trace case, the level\ndensity expression is used to obtain an exact result for the average\nHavrda-Charv\\'at-Tsallis (HCT) entropy as a finite sum. Averages of von Neumann\nentropy, linear entropy and purity follow by considering appropriate limits in\nthe average HCT expression. Based on exact evaluations of the average von\nNeumann entropy and the average purity, we also conjecture very simple formulae\nfor these, which are similar to those in the Hilbert-Schmidt ensemble.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 10:38:13 GMT"}, {"version": "v2", "created": "Thu, 11 Jul 2019 04:37:29 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Sarkar", "Ayana", ""], ["Kumar", "Santosh", ""]]}, {"id": "1901.09659", "submitter": "Nandana Sengupta", "authors": "Nandana Sengupta, Nati Srebro and James Evans", "title": "Simple Surveys: Response Retrieval Inspired by Recommendation Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade, the use of simple rating and comparison surveys has\nproliferated on social and digital media platforms to fuel recommendations.\nThese simple surveys and their extrapolation with machine learning algorithms\nshed light on user preferences over large and growing pools of items, such as\nmovies, songs and ads. Social scientists have a long history of measuring\nperceptions, preferences and opinions, often over smaller, discrete item sets\nwith exhaustive rating or ranking surveys. This paper introduces simple surveys\nfor social science application. We ran experiments to compare the predictive\naccuracy of both individual and aggregate comparative assessments using four\ntypes of simple surveys: pairwise comparisons and ratings on 2, 5 and\ncontinuous point scales in three distinct contexts: perceived Safety of Google\nStreetview Images, Likeability of Artwork, and Hilarity of Animal GIFs. Across\ncontexts, we find that continuous scale ratings best predict individual\nassessments but consume the most time and cognitive effort. Binary choice\nsurveys are quick and perform best to predict aggregate assessments, useful for\ncollective decision tasks, but poorly predict personalized preferences, for\nwhich they are currently used by Netflix to recommend movies. Pairwise\ncomparisons, by contrast, perform well to predict personal assessments, but\npoorly predict aggregate assessments despite being widely used to crowdsource\nideas and collective preferences. We demonstrate how findings from these\nsurveys can be visualized in a low-dimensional space that reveals distinct\nrespondent interpretations of questions asked in each context. We conclude by\nreflecting on differences between sparse, incomplete simple surveys and their\ntraditional survey counterparts in terms of efficiency, information elicited\nand settings in which knowing less about more may be critical for social\nscience.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 05:18:11 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Sengupta", "Nandana", ""], ["Srebro", "Nati", ""], ["Evans", "James", ""]]}, {"id": "1901.09692", "submitter": "Fu-Chun Yeh", "authors": "Fu-Chun Yeh", "title": "Forecasting mortality using Google trend", "comments": "9 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the motility model for the developed country, which United\nState possesses the largest economy in the world and thus serves as an ideal\nrepresentation, is investigated. Early surveillance of the causes of death is\ncritical which can allow the preparation of preventive steps against critical\ndisease such as dengue fever. Studies reported that some search queries,\nespecially those diseases related terms on Google Trends are essential. To this\nend, we include either main cause of death or the extended or the more general\nterminologies from Google Trends to decode the mortality related terms using\nthe Wiener Cascade Model. Using time series and Wavelet scalogram of search\nterms, the patterns of search queries are categorized into different levels of\nperiodicity. The results include the decoding trend, the features importance,\nand the accuracy of the decoding patterns. Three scenarios regard predictors\ninclude the use of all 19 features, the top ten most periodic predictors, or\nthe ten predictors with highest weighting. All search queries spans from\nDecember 2013 - December 2018. The results show that search terms with both\nhigher weight and annual periodic pattern contribute more in forecasting the\nword die; however, only predictors with higher weight are valuable to forecast\nthe word death.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 11:09:54 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2019 00:05:40 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Yeh", "Fu-Chun", ""]]}, {"id": "1901.09729", "submitter": "Micha{\\l} Narajewski", "authors": "Micha{\\l} Narajewski and Florian Ziel", "title": "Estimation and simulation of the transaction arrival process in intraday\n  electricity markets", "comments": null, "journal-ref": "Energies 2019, 12(23), 4518", "doi": "10.3390/en12234518", "report-no": null, "categories": "econ.GN q-fin.EC q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the novel problem of the estimation of transaction arrival\nprocesses in the intraday electricity markets. We model the inter-arrivals\nusing multiple time-varying parametric densities based on the generalized F\ndistribution estimated by maximum likelihood. We analyse both the in-sample\ncharacteristics and the probabilistic forecasting performance. In a rolling\nwindow forecasting study, we simulate many trajectories to evaluate the\nforecasts and gain significant insights into the model fit. The prediction\naccuracy is evaluated by a functional version of the MAE (mean absolute error),\nRMSE (root mean squared error) and CRPS (continuous ranked probability score)\nfor the simulated count processes. This paper fills the gap in the literature\nregarding the intensity estimation of transaction arrivals and is a major\ncontribution to the topic, yet leaves much of the field for further\ndevelopment. The study presented in this paper is conducted based on the German\nIntraday Continuous electricity market data, but this method can be easily\napplied to any other continuous intraday electricity market. For the German\nmarket, a specific generalized gamma distribution setup explains the overall\nbehaviour significantly best, especially as the tail behaviour of the process\nis well covered.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 15:17:45 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 11:05:10 GMT"}, {"version": "v3", "created": "Wed, 27 Nov 2019 13:54:36 GMT"}, {"version": "v4", "created": "Mon, 2 Dec 2019 09:31:39 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Narajewski", "Micha\u0142", ""], ["Ziel", "Florian", ""]]}, {"id": "1901.09775", "submitter": "Christian Winkler", "authors": "Christian Winkler, Katharina Linden, Andreas Mayr, Thomas Schultz,\n  Thomas Welchowski, Johannes Breuer, Ulrike Herberg", "title": "RefCurv: A Software for the Construction of Pediatric Reference Curves", "comments": "You can find more information about the software (tutorials, link to\n  the source code, etc.) on https://refcurv.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In medicine, reference curves serve as an important tool for everyday\nclinical practice. Pediatricians assess the growth process of children with the\nhelp of percentile curves serving as norm references. The mathematical methods\nfor the construction of these reference curves are sophisticated and often\nrequire technical knowledge beyond the scope of physicians. An easy-to-use\nsoftware for life scientists and physicians is missing. As a consequence, most\nmedical publications do not document the construction properly. This project\naims to develop a software that enables non-technical users to apply modern\nstatistical methods to create and analyze reference curves. In this paper, we\npresent RefCurv, a software that facilitates the construction of reference\ncurves. The software comprises functionalities to select and visualize data.\nUsers can fit models to the data and graphically present them as percentile\ncurves. Furthermore, the software provides features to highlight possible\noutliers, perform model selection, and analyze the sensitivity. RefCurv is an\nopen-source software with a graphical user interface (GUI) written in Python.\nIt uses R and the gamlss add-on package (Rigby and Stasinopoulos (2005)) as the\nunderlying statistical engine. In summary, RefCurv is the first software based\non the gamlss package, which enables practitioners to construct and analyze\nreference curves in a user-friendly GUI. In broader terms, the software brings\ntogether the fields of statistical learning and medical application.\nConsequently, RefCurv can help to establish the construction of reference\ncurves in other medical fields.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 16:25:09 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Winkler", "Christian", ""], ["Linden", "Katharina", ""], ["Mayr", "Andreas", ""], ["Schultz", "Thomas", ""], ["Welchowski", "Thomas", ""], ["Breuer", "Johannes", ""], ["Herberg", "Ulrike", ""]]}, {"id": "1901.10225", "submitter": "Sally Paganin", "authors": "Sally Paganin, Amy H. Herring, Andrew F. Olshan, David B. Dunson", "title": "Centered Partition Process: Informative Priors for Clustering", "comments": null, "journal-ref": null, "doi": "10.1214/20-BA1197", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a very rich literature proposing Bayesian approaches for clustering\nstarting with a prior probability distribution on partitions. Most approaches\nassume exchangeability, leading to simple representations in terms of\nExchangeable Partition Probability Functions (EPPF). Gibbs-type priors\nencompass a broad class of such cases, including Dirichlet and Pitman-Yor\nprocesses. Even though there have been some proposals to relax the\nexchangeability assumption, allowing covariate-dependence and partial\nexchangeability, limited consideration has been given on how to include\nconcrete prior knowledge on the partition. For example, we are motivated by an\nepidemiological application, in which we wish to cluster birth defects into\ngroups and we have prior knowledge of an initial clustering provided by\nexperts. As a general approach for including such prior knowledge, we propose a\nCentered Partition (CP) process that modifies the EPPF to favor partitions\nclose to an initial one. Some properties of the CP prior are described, a\ngeneral algorithm for posterior computation is developed, and we illustrate the\nmethodology through simulation examples and an application to the motivating\nepidemiology study of birth defects.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 11:18:19 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Paganin", "Sally", ""], ["Herring", "Amy H.", ""], ["Olshan", "Andrew F.", ""], ["Dunson", "David B.", ""]]}, {"id": "1901.10257", "submitter": "Earo Wang", "authors": "Earo Wang, Dianne Cook, Rob J Hyndman", "title": "A new tidy data structure to support exploration and modeling of\n  temporal data", "comments": "Revision on Section 4 and 5", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining temporal data for information is often inhibited by a multitude of\nformats: irregular or multiple time intervals, point events that need\naggregating, multiple observational units or repeated measurements on multiple\nindividuals, and heterogeneous data types. On the other hand, the software\nsupporting time series modeling and forecasting, makes strict assumptions on\nthe data to be provided, typically requiring a matrix of numeric data with\nimplicit time indexes. Going from raw data to model-ready data is painful. This\nwork presents a cohesive and conceptual framework for organizing and\nmanipulating temporal data, which in turn flows into visualization, modeling\nand forecasting routines. Tidy data principles are extended to temporal data\nby: (1) mapping the semantics of a dataset into its physical layout; (2)\nincluding an explicitly declared index variable representing time; (3)\nincorporating a \"key\" comprising single or multiple variables to uniquely\nidentify units over time. This tidy data representation most naturally supports\nthinking of operations on the data as building blocks, forming part of a \"data\npipeline\" in time-based contexts. A sound data pipeline facilitates a fluent\nworkflow for analyzing temporal data. The infrastructure of tidy temporal data\nhas been implemented in the R package \"tsibble\".\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 12:48:59 GMT"}, {"version": "v2", "created": "Wed, 13 Feb 2019 05:32:47 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Wang", "Earo", ""], ["Cook", "Dianne", ""], ["Hyndman", "Rob J", ""]]}, {"id": "1901.10377", "submitter": "Guillermo Abramson", "authors": "Fernando E. Vargas, Jos\\'e L. Lanata, Guillermo Abramson, Marcelo N.\n  Kuperman and Danae Fiore", "title": "Digging the topology of rock art in Northwestern Patagonia", "comments": null, "journal-ref": "Journal of Complex Networks, cnz033 (2019)", "doi": "10.1093/comnet/cnz033", "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a study on the rock art of Northern Patagonia based on network\nanalysis and communities detection. We unveil a significant aggregation of\narchaeological sites, linked by common rock art motifs that turn out to be\nconsistent with their geographical distribution and archaeological background\nof hunter-gatherer stages of regional peopling and land use. This exploratory\nstudy will allow us to approach more accurately some social strategies of\nvisual communication entailed by rock art motif distribution, in space and\ntime.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 21:05:34 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 20:13:29 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Vargas", "Fernando E.", ""], ["Lanata", "Jos\u00e9 L.", ""], ["Abramson", "Guillermo", ""], ["Kuperman", "Marcelo N.", ""], ["Fiore", "Danae", ""]]}, {"id": "1901.10383", "submitter": "Zulfiqar Ali Z.ali", "authors": "Zulfiqar Ali, Ijaz Hussain, Muhammad Faisal, Ibrahim M. Almanjahie,\n  Muhammad Ismail, Maqsood Ahmad, and Ishfaq Ahmad", "title": "A New Weighting Scheme in Weighted Markov Model for Predicting the\n  Probability of Drought Episodes", "comments": null, "journal-ref": "Advances in Meteorology,2018", "doi": "10.1155/2018/8954656", "report-no": null, "categories": "stat.AP physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drought is a complex stochastic natural hazard caused by prolonged shortage\nof rainfall. Several environmental factors are involved in determining drought\nclasses at the specific monitoring station. Therefore, efficient sequence\nprocessing techniques are required to explore and predict the periodic\ninformation about the various episodes of drought classes. In this study, we\nproposed a new weighting scheme to predict the probability of various drought\nclasses under Weighted Markov Chain (WMC) model. We provide a standardized\nscheme of weights for ordinal sequences of drought classifications by\nnormalizing squared weighted Cohen Kappa. Illustrations of the proposed scheme\nare given by including temporal ordinal data on drought classes determined by\nthe standardized precipitation temperature index (SPTI). Experimental results\nshow that the proposed weighting scheme for WMC model is sufficiently flexible\nto address actual changes in drought classifications by restructuring the\ntransient behavior of a Markov chain. In summary, this paper proposes a new\nweighting scheme to improve the accuracy of the WMC, specifically in the field\nof hydrology.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 20:41:22 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Ali", "Zulfiqar", ""], ["Hussain", "Ijaz", ""], ["Faisal", "Muhammad", ""], ["Almanjahie", "Ibrahim M.", ""], ["Ismail", "Muhammad", ""], ["Ahmad", "Maqsood", ""], ["Ahmad", "Ishfaq", ""]]}, {"id": "1901.10399", "submitter": "Prajamitra Bhuyan Dr.", "authors": "Phalguni Nanda, Prajamitra Bhuyan and Anup Dewanji", "title": "Optimal Replacement Policy under Cumulative Damage Model and Strength\n  Degradation with Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-life scenarios, system failure depends on dynamic\nstress-strength interference, where strength degrades and stress accumulates\nconcurrently over time. In this paper, we consider the problem of finding an\noptimal replacement strategy that balances the cost of replacement with the\ncost of failure and results in a minimum expected cost per unit time under\ncumulative damage model with strength degradation. The existing recommendations\nare applicable only under restricted distributional assumptions and/or with\nfixed strength. As theoretical evaluation of the expected cost per unit time\nturns out to be very complicated, a simulation-based algorithm is proposed to\nevaluate the expected cost rate and find the optimal replacement strategy. The\nproposed method is easy to implement having wider domain of application. For\nillustration, the proposed method is applied to real case studies on mailbox\nand cell-phone battery experiments.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 21:39:33 GMT"}, {"version": "v2", "created": "Mon, 12 Aug 2019 08:18:27 GMT"}, {"version": "v3", "created": "Mon, 2 Dec 2019 15:03:30 GMT"}, {"version": "v4", "created": "Fri, 27 Mar 2020 09:51:45 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Nanda", "Phalguni", ""], ["Bhuyan", "Prajamitra", ""], ["Dewanji", "Anup", ""]]}, {"id": "1901.10400", "submitter": "Cameron Carlin", "authors": "Nicole Fronda, Jessica Asencio, Cameron Carlin, David Ledbetter,\n  Melissa Aczon, Randall Wetzel, Barry Markovitz", "title": "Predicting Individual Responses to Vasoactive Medications in Children\n  with Septic Shock", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Predict individual septic children's personalized physiologic\nresponses to vasoactive titrations by training a Recurrent Neural Network (RNN)\nusing EMR data.\n  Materials and Methods: This study retrospectively analyzed EMR of patients\nadmitted to a pediatric ICU from 2009 to 2017. Data included charted time\nseries vitals, labs, drugs, and interventions of children with septic shock\ntreated with dopamine, epinephrine, or norepinephrine. A RNN was trained to\npredict responses in heart rate (HR), systolic blood pressure (SBP), diastolic\nblood pressure (DBP) and mean arterial pressure (MAP) to 8,640 titrations\nduring 652 septic episodes and evaluated on a holdout set of 3,883 titrations\nduring 254 episodes. A linear regression model using titration data as its sole\ninput was also developed and compared to the RNN model. Evaluation methods\nincluded the correlation coefficient between actual physiologic responses and\nRNN predictions, mean absolute error (MAE), and area under the receiver\noperating characteristic curve (AUC).\n  Results: The actual physiologic responses displayed significant variability\nand were more accurately predicted by the RNN model than by titration alone\n(r=0.20 vs r=0.05, p<0.01). The RNN showed MAE and AUC improvements over the\nlinear model. The RNN's MAEs associated with dopamine and epinephrine were 1-3%\nlower than the linear regression model MAE for HR, SBP, DBP, and MAP. Across\nall vitals vasoactives, the RNN achieved 1-19% AUC improvement over the linear\nmodel.\n  Conclusion: This initial attempt in pediatric critical care to predict\nindividual physiologic responses to vasoactive dose changes in children with\nseptic shock demonstrated an RNN model showed some improvement over a linear\nmodel. While not yet clinically applicable, further development may assist\nclinical administration of vasoactive medications in children with septic\nshock.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 23:43:04 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Fronda", "Nicole", ""], ["Asencio", "Jessica", ""], ["Carlin", "Cameron", ""], ["Ledbetter", "David", ""], ["Aczon", "Melissa", ""], ["Wetzel", "Randall", ""], ["Markovitz", "Barry", ""]]}, {"id": "1901.10552", "submitter": "Revathi Anil Kumar", "authors": "Revathi Anil Kumar and Mark Chamness", "title": "Stochastic Estimated Risk for Storage Capacity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Managing data storage growth is of crucial importance to businesses. Poor\npractices can lead to large data and financial losses. Access to storage\ninformation along with timely action, or capacity forecasting, are essential to\navoid these losses. In addition, ensuring high accuracy of capacity forecast\nestimates along with ease of interpretability plays an important role for any\ncustomer facing tool. In this paper, we introduce Stochastic Estimated Risk\n(SER), a tool developed at Nutanix that has been in production. SER shifts the\nfocus from forecasting a single estimate for date of attaining full capacity to\npredicting the risk associated with running out of storage capacity. Using a\nBrownian motion with drift model, SER estimates the probability that a system\nwill run out of capacity within a specific time frame. Our results showed that\na probabilistic approach is more accurate and credible, for systems with\nnon-linear patterns, compared to a regression or ensemble forecasting models.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 22:41:07 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Kumar", "Revathi Anil", ""], ["Chamness", "Mark", ""]]}, {"id": "1901.10566", "submitter": "Sherri Rose", "authors": "Anna Zink and Sherri Rose", "title": "Fair Regression for Health Care Spending", "comments": "30 pages, 3 figures", "journal-ref": "Biometrics (2020)", "doi": "10.1111/biom.13206", "report-no": null, "categories": "stat.AP cs.CY stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distribution of health care payments to insurance plans has substantial\nconsequences for social policy. Risk adjustment formulas predict spending in\nhealth insurance markets in order to provide fair benefits and health care\ncoverage for all enrollees, regardless of their health status. Unfortunately,\ncurrent risk adjustment formulas are known to underpredict spending for\nspecific groups of enrollees leading to undercompensated payments to health\ninsurers. This incentivizes insurers to design their plans such that\nindividuals in undercompensated groups will be less likely to enroll, impacting\naccess to health care for these groups. To improve risk adjustment formulas for\nundercompensated groups, we expand on concepts from the statistics, computer\nscience, and health economics literature to develop new fair regression methods\nfor continuous outcomes by building fairness considerations directly into the\nobjective function. We additionally propose a novel measure of fairness while\nasserting that a suite of metrics is necessary in order to evaluate risk\nadjustment formulas more fully. Our data application using the IBM MarketScan\nResearch Databases and simulation studies demonstrate that these new fair\nregression methods may lead to massive improvements in group fairness (e.g.,\n98%) with only small reductions in overall fit (e.g., 4%).\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 04:06:50 GMT"}, {"version": "v2", "created": "Sat, 13 Jul 2019 06:11:28 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Zink", "Anna", ""], ["Rose", "Sherri", ""]]}, {"id": "1901.10589", "submitter": "Triet Le", "authors": "Triet M. Le", "title": "A Robust Time Series Model with Outliers and Missing Entries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of robustly learning the correlation function\nfor a univariate time series with the presence of noise, outliers and missing\nentries. The outliers or anomalies considered here are sparse and rare events\nthat deviate from normality which is depicted by a correlation function and an\nuncertainty condition. This general formulation is applied to univariate time\nseries of event counts (or non-negative time series) where the correlation is a\nlog-linear function with the uncertainty condition following the Poisson\ndistribution. Approximations to the sparsity constraint, such as $\\ell^r, 0<\nr\\le 1$, are used to obtain robustness in the presence of outliers. The\n$\\ell^r$ constraint is also applied to the correlation function to reduce the\nnumber of active coefficients. This task also helps bypassing the model\nselection procedure. Simulated results are presented to validate the model.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 22:19:12 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Le", "Triet M.", ""]]}, {"id": "1901.10771", "submitter": "Takashi Shinzato", "authors": "Takashi Shinzato", "title": "Minimal Investment Risk with Cost and Return Constraints: A Replica\n  Analysis", "comments": "14 pages, 2 figures", "journal-ref": null, "doi": "10.7566/JPSJ.88.064804", "report-no": null, "categories": "q-fin.PM cond-mat.dis-nn cond-mat.soft q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous studies into the budget constraint of portfolio optimization\nproblems based on statistical mechanical informatics have not considered that\nthe purchase cost per unit of each asset is distinct. Moreover, the fact that\nthe optimal investment allocation differs depending on the size of investable\nfunds has also been neglected. In this paper, we approach the problem of\ninvestment risk minimization using replica analysis. This problem imposes cost\nand return constraints. We also derive the macroscopic theory indicated by the\noptimal solution and confirm the validity of our proposed method through\nnumerical experiments.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 11:36:23 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Shinzato", "Takashi", ""]]}, {"id": "1901.10782", "submitter": "Michele Nguyen", "authors": "Michele Nguyen, Rosalind E. Howes, Tim C.D. Lucas, Katherine E.\n  Battle, Ewan Cameron, Harry S. Gibson, Jennifer Rozier, Suzanne Keddie, Emma\n  Collins, Rohan Arambepola, Su Yun Kang, Chantal Hendriks, Anita Nandi, Susan\n  F. Rumisha, Samir Bhatt, Sedera A. Mioramalala, Mauricette Andriamananjara\n  Nambinisoa, Fanjasoa Rakotomanana, Peter W. Gething, Daniel J. Weiss", "title": "Mapping malaria seasonality: a case study from Madagascar", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many malaria-endemic areas experience seasonal fluctuations in case incidence\nas Anopheles mosquito and Plasmodium parasite life cycles respond to changing\nenvironmental conditions. While most existing maps of malaria seasonality use\nfixed thresholds of rainfall, temperature, and/or vegetation indices to\nidentify suitable transmission months, we develop a statistical modelling\nframework for characterising the seasonal patterns derived directly from case\ndata.\n  The procedure involves a spatiotemporal regression model for estimating the\nmonthly proportions of total annual cases and an algorithm to identify\noperationally relevant characteristics such as the transmission start and peak\nmonths. A seasonality index combines the monthly proportion estimates and\nexisting estimates of annual case incidence to provide a summary of \"how\nseasonal\" locations are relative to their surroundings. An advancement upon\npast seasonality mapping endeavours is the presentation of the uncertainty\nassociated with each map, which will enable policymakers to make more\nstatistically sound decisions. The methodology is illustrated using health\nfacility data from Madagascar.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 12:20:52 GMT"}, {"version": "v2", "created": "Fri, 17 May 2019 15:04:41 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Nguyen", "Michele", ""], ["Howes", "Rosalind E.", ""], ["Lucas", "Tim C. D.", ""], ["Battle", "Katherine E.", ""], ["Cameron", "Ewan", ""], ["Gibson", "Harry S.", ""], ["Rozier", "Jennifer", ""], ["Keddie", "Suzanne", ""], ["Collins", "Emma", ""], ["Arambepola", "Rohan", ""], ["Kang", "Su Yun", ""], ["Hendriks", "Chantal", ""], ["Nandi", "Anita", ""], ["Rumisha", "Susan F.", ""], ["Bhatt", "Samir", ""], ["Mioramalala", "Sedera A.", ""], ["Nambinisoa", "Mauricette Andriamananjara", ""], ["Rakotomanana", "Fanjasoa", ""], ["Gething", "Peter W.", ""], ["Weiss", "Daniel J.", ""]]}, {"id": "1901.10867", "submitter": "Mouloud Belbahri", "authors": "Mouloud Belbahri, Alejandro Murua, Olivier Gandouet, Vahid Partovi Nia", "title": "Uplift Regression: The R Package tools4uplift", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uplift modeling aims at predicting the causal effect of an action such as a\nmedical treatment or a marketing campaign on a particular individual, by taking\ninto consideration the response to a treatment. The treatment group contains\nindividuals who are subject to an action; a control group serves for\ncomparison. Uplift modeling is used to order the individuals with respect to\nthe value of a causal effect, e.g., positive, neutral, or negative. Though\nthere are some computational methods available for uplift modeling, most of\nthem exclude statistical regression models. The R Package tools4uplift intends\nto fill this gap. This package comprises tools for: i) quantization, ii)\nvisualization, iii) feature selection, and iv) model validation.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 14:50:19 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Belbahri", "Mouloud", ""], ["Murua", "Alejandro", ""], ["Gandouet", "Olivier", ""], ["Nia", "Vahid Partovi", ""]]}, {"id": "1901.10960", "submitter": "Erwan Koch", "authors": "Erwan Koch, Jonathan Koh, Anthony C. Davison, Chiara Lepore, Michael\n  K. Tippett", "title": "Trends in the extremes of environments associated with severe US\n  thunderstorms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Severe thunderstorms can have devastating impacts. Concurrently high values\nof convective available potential energy (CAPE) and storm relative helicity\n(SRH) are known to be conducive to severe weather, so high values of\nPROD=$\\sqrt{\\mathrm{CAPE}} \\times$SRH have been used to indicate high risk of\nsevere thunderstorms. We consider the extreme values of these three variables\nfor a large area of the contiguous US over the period 1979-2015, and use\nextreme-value theory and a multiple testing procedure to show that there is a\nsignificant time trend in the extremes for PROD maxima in April, May and\nAugust, for CAPE maxima in April, May and June, and for maxima of SRH in April\nand May. These observed increases in CAPE are also relevant for rainfall\nextremes and are expected in a warmer climate, but have not previously been\nreported. Moreover, we show that the El Ni\\~no-Southern Oscillation explains\nvariation in the extremes of PROD and SRH in February. Our results suggest that\nthe risk from severe thunderstorms in April and May is increasing in parts of\nthe US where it was already high, and that the risk from storms in February\ntends to be higher over the main part of the region during La Ni\\~na years. Our\nresults differ from those obtained in earlier studies using extreme-value\ntechniques to analyze a quantity similar to PROD.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 17:19:37 GMT"}, {"version": "v2", "created": "Wed, 13 Feb 2019 12:17:11 GMT"}, {"version": "v3", "created": "Wed, 30 Oct 2019 14:47:46 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Koch", "Erwan", ""], ["Koh", "Jonathan", ""], ["Davison", "Anthony C.", ""], ["Lepore", "Chiara", ""], ["Tippett", "Michael K.", ""]]}, {"id": "1901.10984", "submitter": "Massimo Ostilli", "authors": "Lucas Nicolao and Massimo Ostilli", "title": "Critical states in Political Trends. How much reliable is a poll on\n  Twitter? A study by means of the Potts Model", "comments": "14 pages, 14 figures", "journal-ref": "Physica A 533, 121920 (2019)", "doi": "10.1016/j.physa.2019.121920", "report-no": null, "categories": "cond-mat.stat-mech cond-mat.dis-nn stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Twitter data related to political trends have tentatively\nbeen used to make predictions (poll) about several electoral events. Given $q$\ncandidates for an election and a time-series of Twitts (short messages), one\ncan extract the $q$ mean trends and the $q(q+1)/2$ Twitt-to-Twitt correlations,\nand look for the statistical models that reproduce these data. On the base of\nseveral electoral events and assuming a stationary regime, we find out the\nfollowing: i) the maximization of the entropy singles out a microscopic model\n(single-Twitt-level) that coincides with a $q$-state Potts model having\nsuitable couplings and external fields to be determined via an inverse problem\nfrom the two sets of data; ii) correlations decay as $1/N_{eff}$, where\n$N_{eff}$ is a small fraction of the mean number of Twitts; iii) the simplest\nstatistical models that reproduce these correlations are the multinomial\ndistribution (MD), characterized by $q$ external fields, and the mean-field\nPotts model (MFP), characterized by one coupling; iv) remarkably, this coupling\nturns out to be always close to its critical value. This results in a MD or MFP\nmodel scenario that discriminates between cases in which polls are reliable and\nnot reliable, respectively. More precisely, predictions based on polls should\nbe avoided whenever the data maps to a MFP because anomalous large fluctuations\n(if $q=2$) or sudden jumps (if $q\\geq 3$) in the trends might take place as a\nresult of a second-order or a first-order phase transition of the MFP,\nrespectively.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 18:25:48 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Nicolao", "Lucas", ""], ["Ostilli", "Massimo", ""]]}, {"id": "1901.11052", "submitter": "Andrey Gorshenin", "authors": "Victor Korolev, Andrey Gorshenin", "title": "Improved mathematical models of statistical regularities in\n  precipitation", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents improved mathematical models and methods for statistical\nregularities in the behavior of some important characteristics of\nprecipitation: duration of a wet period, maximum daily and total precipitation\nvolumes within a such period. The asymptotic approximations are deduced using\nlimit theorems for statistics constructed from samples with random sizes having\nthe generalized negative binomial (GNB) distribution. It demonstrates excellent\nconcordance with the empirical distribution of the duration of wet periods\nmeasured in days. The asymptotic distribution of the maximum daily\nprecipitation volume within a wet period turns out to be a tempered scale\nmixture of the gamma distribution with the scale factor having the Weibull\ndistribution, whereas the asymptotic approximation to the total precipitation\nvolume for a wet period turns out to be the generalized gamma (GG)\ndistribution. Two approaches to the definition of abnormally extremal\nprecipitation are presented. The first approach is based on an excess of a\ncertain quantile of the asymptotic distribution of the maximum daily\nprecipitation. The second approach is based on the GG model for the total\nprecipitation volume. The corresponding statistical test is compared with a\npreviously proposed one based on tha classical gamma distribution using real\nprecipitation data.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 19:17:33 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Korolev", "Victor", ""], ["Gorshenin", "Andrey", ""]]}, {"id": "1901.11172", "submitter": "Eric Lock", "authors": "Eric F. Lock and Dipankar Bandyopadhyay", "title": "Bayesian nonparametric multiway regression for clustered binomial data", "comments": "20 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a Bayesian nonparametric regression model for data with multiway\n(tensor) structure, motivated by an application to periodontal disease (PD)\ndata. Our outcome is the number of diseased sites measured over four different\ntooth types for each subject, with subject-specific covariates available as\npredictors. The outcomes are not well-characterized by simple parametric\nmodels, so we use a nonparametric approach with a binomial likelihood wherein\nthe latent probabilities are drawn from a mixture with an arbitrary number of\ncomponents, analogous to a Dirichlet Process (DP). We use a flexible probit\nstick-breaking formulation for the component weights that allows for covariate\ndependence and clustering structure in the outcomes. The parameter space for\nthis model is large and multiway: patients $\\times$ tooth types $\\times$\ncovariates $\\times$ components. We reduce its effective dimensionality, and\naccount for the multiway structure, via low-rank assumptions. We illustrate how\nthis can improve performance, and simplify interpretation, while still\nproviding sufficient flexibility. We describe a general and efficient Gibbs\nsampling algorithm for posterior computation. The resulting fit to the PD data\noutperforms competitors, and is interpretable and well-calibrated. An\ninteractive visual of the predictive model is available at\nhttp://ericfrazerlock.com/toothdata/ToothDisplay.html , and the code is\navailable at https://github.com/lockEF/NonparametricMultiway .\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 02:05:07 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Lock", "Eric F.", ""], ["Bandyopadhyay", "Dipankar", ""]]}, {"id": "1901.11389", "submitter": "Nooshin Yousefi", "authors": "Stamatis Tsianikas, Jian Zhou, Nooshin Yousefi, David W. Coit", "title": "Battery selection for optimal grid-outage resilient photovoltaic and\n  battery systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The first and most important purpose of the current research work is to\ninvestigate the effects that different battery types have on the optimal\nconfiguration of photovoltaic (PV) and battery systems, from both economic and\nresilience perspectives. Many industry reports, as well as research papers,\nhave already highlighted the crucial role that storage systems have in the\ncoming years in the electricity sector, especially when combined with renewable\nenergy systems (RES). Given the high cost of storage technologies, there is an\nurgent need for optimizing such integrated energy systems. In this paper, a\nsimulation-based method is adopted and improved, in order to compare different\nbattery types based on their characteristics by considering projected trends in\nthe future. After the introduction of four different battery types, i.e.\nlead-acid, sodium sulphur, vanadium redox and lithium-ion, the mathematical\nmodel for the optimization problem is presented, along with the required\nexplanations. Subsequently, a case study is described and the numerical\nassumptions are defined. Therein, our specific focus addresses the different\nvalues that the four battery types possess in three critical parameters, i.e.\nbattery cost, efficiency and depth of discharge (DoD). Finally, results and\ndiscussion are provided in an illustrative and informative way. This model\nprovides a useful guide for relevant future work in the area, and also serves\nas a baseline for more comprehensive methodologies regarding optimal sizing of\nphotovoltaic and battery systems.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 19:48:31 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Tsianikas", "Stamatis", ""], ["Zhou", "Jian", ""], ["Yousefi", "Nooshin", ""], ["Coit", "David W.", ""]]}, {"id": "1901.11395", "submitter": "Christopher Kovach", "authors": "Christopher K. Kovach and Matthew A. Howard III", "title": "Decomposition of Higher-Order Spectra for Blind Multiple-Input\n  Deconvolution, Pattern Identification and Separation", "comments": "published, open access version", "journal-ref": null, "doi": "10.1016/j.sigpro.2019.07.007", "report-no": null, "categories": "eess.SP q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Like the ordinary power spectrum, higher-order spectra (HOS) describe signal\nproperties that are invariant under translations in time. Unlike the power\nspectrum, HOS retain phase information from which details of the signal\nwaveform can be recovered. Here we consider the problem of identifying multiple\nunknown transient waveforms which recur within an ensemble of records at\nmutually random delays. We develop a new technique for recovering filters from\nHOS whose performance in waveform detection approaches that of an optimal\nmatched filter, requiring no prior information about the waveforms. Unlike\nprevious techniques of signal identification through HOS, the method applies\nequally well to signals with deterministic and non-deterministic HOS. In the\nnon-deterministic case, it yields an additive decomposition, introducing a new\napproach to the separation of component processes within non-Gaussian signals\nhaving non-deterministic higher moments. We show a close relationship to\nminimum-entropy blind deconvolution (MED), which the present technique improves\nupon by avoiding the need for numerical optimization, while requiring only\nnumerically stable operations of time shift, element-wise multiplication and\naveraging, making it particularly suited for real-time applications. The\napplication of HOS decomposition to real-world signals is demonstrated with\nblind denoising, detection and classification of normal and abnormal heartbeats\nin electrocardiograms.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 20:08:18 GMT"}, {"version": "v2", "created": "Sun, 25 Aug 2019 16:23:53 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Kovach", "Christopher K.", ""], ["Howard", "Matthew A.", "III"]]}]