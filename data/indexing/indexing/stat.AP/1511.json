[{"id": "1511.00148", "submitter": "Indre Zliobaite", "authors": "Indre Zliobaite", "title": "A survey on measuring indirect discrimination in machine learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, many decisions are made using predictive models built on historical\ndata.Predictive models may systematically discriminate groups of people even if\nthe computing process is fair and well-intentioned. Discrimination-aware data\nmining studies how to make predictive models free from discrimination, when\nhistorical data, on which they are built, may be biased, incomplete, or even\ncontain past discriminatory decisions. Discrimination refers to disadvantageous\ntreatment of a person based on belonging to a category rather than on\nindividual merit. In this survey we review and organize various discrimination\nmeasures that have been used for measuring discrimination in data, as well as\nin evaluating performance of discrimination-aware predictive models. We also\ndiscuss related measures from other disciplines, which have not been used for\nmeasuring discrimination, but potentially could be suitable for this purpose.\nWe computationally analyze properties of selected measures. We also review and\ndiscuss measuring procedures, and present recommendations for practitioners.\nThe primary target audience is data mining, machine learning, pattern\nrecognition, statistical modeling researchers developing new methods for\nnon-discriminatory predictive modeling. In addition, practitioners and policy\nmakers would use the survey for diagnosing potential discrimination by\npredictive models.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2015 16:04:12 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Zliobaite", "Indre", ""]]}, {"id": "1511.00154", "submitter": "Christos Merkatas", "authors": "Christos Merkatas, Konstantinos Kaloudis, Spyridon J. Hatjispyros", "title": "A Bayesian Nonparametric approach to Reconstruction and Prediction of\n  Random Dynamical Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian nonparametric mixture model for the reconstruction and\nprediction from observed time series data, of discretized stochastic dynamical\nsystems, based on Markov Chain Monte Carlo methods (MCMC). Our results can be\nused by researchers in physical modeling interested in a fast and accurate\nestimation of low dimensional stochastic models when the size of the observed\ntime series is small and the noise process (perhaps) is non-Gaussian. The\ninference procedure is demonstrated specifically in the case of polynomial maps\nof arbitrary degree and when a Geometric Stick Breaking mixture process prior\nover the space of densities, is applied to the additive errors. Our method is\nparsimonious compared to Bayesian nonparametric techniques based on Dirichlet\nprocess mixtures, flexible and general. Simulations based on synthetic time\nseries are presented.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2015 17:26:19 GMT"}, {"version": "v2", "created": "Sat, 30 Sep 2017 08:15:36 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Merkatas", "Christos", ""], ["Kaloudis", "Konstantinos", ""], ["Hatjispyros", "Spyridon J.", ""]]}, {"id": "1511.00297", "submitter": "Timothy Randolph", "authors": "Timothy W. Randolph, Sen Zhao, Wade Copeland, Meredith Hullar, Ali\n  Shojaie", "title": "Kernel-Penalized Regression for Analysis of Microbiome Data", "comments": "Revision to the organization of the material and changes to the\n  simulations", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of human microbiome data is often based on dimension-reduced\ngraphical displays and clustering derived from vectors of microbial abundances\nin each sample. Common to these ordination methods is the use of biologically\nmotivated definitions of similarity. Principal coordinate analysis, in\nparticular, is often performed using ecologically defined distances, allowing\nanalyses to incorporate context-dependent, non-Euclidean structure. Here we\ndescribe how to take a step beyond ordination plots and incorporate this\nstructure into high-dimensional penalized regression models. Within this\nframework, the estimate of a regression coefficient vector is obtained via the\njoint eigen properties of multiple similarity matrices, or kernels. This allows\nfor multivariate regression models to incorporate both a matrix of microbial\nabundances and, for instance, a matrix of phylogenetically-informed\nsimilarities between the abundance profiles. Further, we show how this\nregression framework can be used to address the compositional nature of\nmultivariate predictors comprised of relative abundances; that is, vectors\nwhose entries sum to a constant. We illustrate this regression framework with\nseveral simulations using data from two recent studies on the gut and vaginal\nmicrobiome. We conclude with an application to our own data, where we also\nincorporate a significance test for the estimated coefficients that represent\nassociations between microbial abundance and a response.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2015 19:17:43 GMT"}, {"version": "v2", "created": "Mon, 9 Jan 2017 22:28:16 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Randolph", "Timothy W.", ""], ["Zhao", "Sen", ""], ["Copeland", "Wade", ""], ["Hullar", "Meredith", ""], ["Shojaie", "Ali", ""]]}, {"id": "1511.00298", "submitter": "Andr\\'e Beauducel", "authors": "Andr\\'e Beauducel", "title": "A Schmid-Leiman based transformation resulting in perfect\n  inter-correlations of three types of factor score predictors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factor score predictors are to be computed when the individual scores on the\nfactors are of interest. Conditions for a perfect inter-correlation of the\nregression/best linear factor score predictor, the best linear conditionally\nunbiased predictor, and the determinant best linear correlation-preserving\npredictor are presented. When these three types of factor score predictors are\nperfectly correlated for corresponding factors, the factor score predictors\ncomputed from one method will have the virtues of the factor score predictors\ncomputed from the other methods. A Schmid-Leiman based transformation for which\nthe three types of factor score predictors are perfectly correlated for\ncorresponding orthogonal factors is proposed.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2015 19:18:04 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Beauducel", "Andr\u00e9", ""]]}, {"id": "1511.01214", "submitter": "Giri Gopalan", "authors": "Giri Gopalan", "title": "Quantification of observed prior and likelihood information in\n  parametric Bayesian modeling", "comments": "Abbreviated and edited conference version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two data-dependent information metrics are developed to quantify the\ninformation of the prior and likelihood functions within a parametric Bayesian\nmodel, one of which is closely related to the reference priors from Berger,\nBernardo, and Sun, and information measure introduced by Lindley. A combination\nof theoretical, empirical, and computational support provides evidence that\nthese information-theoretic metrics may be useful diagnostic tools when\nperforming a Bayesian analysis.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 06:07:57 GMT"}, {"version": "v10", "created": "Fri, 24 Mar 2017 12:39:47 GMT"}, {"version": "v11", "created": "Mon, 27 Mar 2017 17:46:05 GMT"}, {"version": "v12", "created": "Sun, 18 Jun 2017 20:18:45 GMT"}, {"version": "v13", "created": "Thu, 7 Sep 2017 00:56:38 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2015 18:08:01 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2015 20:53:12 GMT"}, {"version": "v4", "created": "Sat, 12 Dec 2015 16:42:04 GMT"}, {"version": "v5", "created": "Fri, 18 Dec 2015 20:11:08 GMT"}, {"version": "v6", "created": "Thu, 7 Jan 2016 21:10:04 GMT"}, {"version": "v7", "created": "Mon, 7 Mar 2016 05:37:01 GMT"}, {"version": "v8", "created": "Sat, 25 Jun 2016 05:17:53 GMT"}, {"version": "v9", "created": "Mon, 12 Dec 2016 01:43:46 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Gopalan", "Giri", ""]]}, {"id": "1511.01480", "submitter": "Maurizio Naldi", "authors": "Maurizio Naldi", "title": "Approximation of the truncated Zeta distribution and Zipf's law", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zipf's law appears in many application areas but does not have a closed form\nexpression, which may make its use cumbersome. Since it coincides with the\ntruncated version of the Zeta distribution, in this paper we propose three\napproximate closed form expressions for the truncated Zeta distribution, which\nmay be employed for Zipf's law as well. The three approximations are based on\nthe replacement of the sum occurring in Zipf's law with an integral, and are\nnamed respectively the integral approximation, the average integral\napproximation, and the trapezoidal approximation. While the first one is shown\nto be of little use, the trapezoidal approximation exhibits an error which is\ntypically lower than 1\\%, but is as low as 0.1\\% for the range of values of the\nZipf parameter below 1.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 19:30:27 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Naldi", "Maurizio", ""]]}, {"id": "1511.01536", "submitter": "Tyler Bonnell Ph.D.", "authors": "Tyler R. Bonnell, S. Peter Henzi, Louise Barrett", "title": "Sparse movement data can reveal social influences on individual travel\n  decisions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The monitoring of animal movement patterns provides insights into animals\ndecision-making behaviour. It is generally assumed that high-resolution data\nare needed to extract meaningful behavioural patterns, which potentially limits\nthe application of this approach. Obtaining high-resolution movement data\ncontinues to be an economic and technical challenge, particularly for animals\nthat live in social groups. Here, we test whether accurate movement behaviour\ncan be extracted from data that possesses increasingly lower temporal\nresolution. To do so, we use a modified version of force matching, in which\nsimulated forces acting on a focal animal are compared to observed movement\ndata. We show that useful information can be extracted from sparse data. We\napply this approach to a sparse movement dataset collected on the adult members\nof a troop of baboons in the DeHoop Nature Reserve, South Africa. We use these\ndata to test the hypothesis that individuals are sensitive to isolation from\nthe group as a whole or, alternatively, whether they are sensitive to the\nlocation of specific individuals within the group. Using data from a focal\nanimal, our data provide support for both hypothesis, with stronger support for\nthe latter. Although the focal animal was found to be sensitive to the group,\nthis occurred only on a small number of occasions when the group as a whole was\nhighly clustered as a single entity away from the focal animal. We suggest that\nspecific social interactions may thus drive overall group cohesion. Given that\nsparse movement data is informative about individual movement behaviour, we\nsuggest that both high (~seconds) and relatively low (~minutes) resolution\ndatasets are valuable for the study of how individuals react to and manipulate\ntheir local social and ecological environments.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 22:21:26 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Bonnell", "Tyler R.", ""], ["Henzi", "S. Peter", ""], ["Barrett", "Louise", ""]]}, {"id": "1511.01634", "submitter": "Saeid Haghighatshoar", "authors": "Saeid Haghighatshoar and Giuseppe Caire", "title": "An Active-Sensing Approach to Channel Vector Subspace Estimation in\n  mm-Wave Massive MIMO Systems", "comments": "7 pages, 2 figures, Submitted to IEEE International Conference on\n  Communications (ICC 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Millimeter-wave (mm-Wave) cellular systems are a promising option for a very\nhigh data rate communication because of the large bandwidth available at\nmm-Wave frequencies. Due to the large path-loss exponent in the mm-Wave range\nof the spectrum, directional beamforming with a large antenna gain is necessary\nat the transmitter, the receiver or both for capturing sufficient signal power.\nThis in turn implies that fast and robust channel estimation plays a central\nrole in systems performance since without a reliable estimate of the channel\nstate the received signal-to-noise ratio (SNR) would be much lower than the\nminimum necessary for a reliable communication.\n  In this paper, we mainly focus on single-antenna users and a multi-antenna\nbase-station. We propose an adaptive sampling scheme to speed up the user's\nsignal subspace estimation. In our scheme, the beamforming vector for taking\nevery new sample is adaptively selected based on all the previous beamforming\nvectors and the resulting output observations. We apply the theory of optimal\ndesign of experiments in statistics to design an adaptive algorithm for\nestimating the signal subspace of each user. The resulting subspace estimates\nfor different users can be exploited to efficiently communicate to the users\nand to manage the interference. We cast our proposed algorithm as\nlow-complexity optimization problems, and illustrate its efficiency via\nnumerical simulations.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 07:25:04 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Haghighatshoar", "Saeid", ""], ["Caire", "Giuseppe", ""]]}, {"id": "1511.01639", "submitter": "Lasse Holmstr\\\"{o}m", "authors": "Lasse Holmstr\\\"om, Liisa Ilvonen, Heikki Sepp\\\"a, Siim Veski", "title": "A Bayesian spatiotemporal model for reconstructing climate from multiple\n  pollen records", "comments": "Published at http://dx.doi.org/10.1214/15-AOAS832 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 3, 1194-1225", "doi": "10.1214/15-AOAS832", "report-no": "IMS-AOAS-AOAS832", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Holocene (the last 12,000 years) temperature variation, including the\ntransition out of the last Ice Age to a warmer climate, is reconstructed at\nmultiple locations in southern Finland, Sweden and Estonia based on pollen\nfossil data from lake sediment cores. A novel Bayesian statistical approach is\nproposed that allows the reconstructed temperature histories to interact\nthrough shared environmental response parameters and spatial dependence. The\nprior distribution for past temperatures is partially based on numerical\nclimate simulation. The features in the reconstructions are consistent with the\nquantitative climate reconstructions based on more commonly used reconstruction\ntechniques. The results suggest that the novel spatio-temporal approach can\nprovide quantitative reconstructions that are smoother, less uncertain and\ngenerally more realistic than the site-specific individual reconstructions.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 07:45:18 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Holmstr\u00f6m", "Lasse", ""], ["Ilvonen", "Liisa", ""], ["Sepp\u00e4", "Heikki", ""], ["Veski", "Siim", ""]]}, {"id": "1511.01641", "submitter": "Luke B. Smith", "authors": "Luke B. Smith, Montserrat Fuentes, Penny Gordon-Larsen, Brian J. Reich", "title": "Quantile regression for mixed models with an application to examine\n  blood pressure trends in China", "comments": "Published at http://dx.doi.org/10.1214/15-AOAS841 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 3, 1226-1246", "doi": "10.1214/15-AOAS841", "report-no": "IMS-AOAS-AOAS841", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiometabolic diseases have substantially increased in China in the past 20\nyears and blood pressure is a primary modifiable risk factor. Using data from\nthe China Health and Nutrition Survey, we examine blood pressure trends in\nChina from 1991 to 2009, with a concentration on age cohorts and urbanicity.\nVery large values of blood pressure are of interest, so we model the\nconditional quantile functions of systolic and diastolic blood pressure. This\nallows the covariate effects in the middle of the distribution to vary from\nthose in the upper tail, the focal point of our analysis. We join the\ndistributions of systolic and diastolic blood pressure using a copula, which\npermits the relationships between the covariates and the two responses to share\ninformation and enables probabilistic statements about systolic and diastolic\nblood pressure jointly. Our copula maintains the marginal distributions of the\ngroup quantile effects while accounting for within-subject dependence, enabling\ninference at the population and subject levels. Our population-level regression\neffects change across quantile level, year and blood pressure type, providing a\nrich environment for inference. To our knowledge, this is the first quantile\nfunction model to explicitly model within-subject autocorrelation and is the\nfirst quantile function approach that simultaneously models multivariate\nconditional response. We find that the association between high blood pressure\nand living in an urban area has evolved from positive to negative, with the\nstrongest changes occurring in the upper tail. The increase in urbanization\nover the last twenty years coupled with the transition from the positive\nassociation between urbanization and blood pressure in earlier years to a more\nuniform association with urbanization suggests increasing blood pressure over\ntime throughout China, even in less urbanized areas. Our methods are available\nin the R package BSquare.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 07:50:14 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Smith", "Luke B.", ""], ["Fuentes", "Montserrat", ""], ["Gordon-Larsen", "Penny", ""], ["Reich", "Brian J.", ""]]}, {"id": "1511.01644", "submitter": "Benjamin Letham", "authors": "Benjamin Letham, Cynthia Rudin, Tyler H. McCormick, David Madigan", "title": "Interpretable classifiers using rules and Bayesian analysis: Building a\n  better stroke prediction model", "comments": "Published at http://dx.doi.org/10.1214/15-AOAS848 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 3, 1350-1371", "doi": "10.1214/15-AOAS848", "report-no": "IMS-AOAS-AOAS848", "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to produce predictive models that are not only accurate, but are also\ninterpretable to human experts. Our models are decision lists, which consist of\na series of if...then... statements (e.g., if high blood pressure, then stroke)\nthat discretize a high-dimensional, multivariate feature space into a series of\nsimple, readily interpretable decision statements. We introduce a generative\nmodel called Bayesian Rule Lists that yields a posterior distribution over\npossible decision lists. It employs a novel prior structure to encourage\nsparsity. Our experiments show that Bayesian Rule Lists has predictive accuracy\non par with the current top algorithms for prediction in machine learning. Our\nmethod is motivated by recent developments in personalized medicine, and can be\nused to produce highly accurate and interpretable medical scoring systems. We\ndemonstrate this by producing an alternative to the CHADS$_2$ score, actively\nused in clinical practice for estimating the risk of stroke in patients that\nhave atrial fibrillation. Our model is as interpretable as CHADS$_2$, but more\naccurate.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 08:01:05 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Letham", "Benjamin", ""], ["Rudin", "Cynthia", ""], ["McCormick", "Tyler H.", ""], ["Madigan", "David", ""]]}, {"id": "1511.01654", "submitter": "Jukka Ranta", "authors": "Jukka Ranta, Roland Lindqvist, Ingrid Hansson, Pirkko Tuominen,\n  Maarten Nauta", "title": "A Bayesian approach to the evaluation of risk-based microbiological\n  criteria for \\uppercaseCampylobacter in broiler meat", "comments": "Published at http://dx.doi.org/10.1214/15-AOAS845 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 3, 1415-1432", "doi": "10.1214/15-AOAS845", "report-no": "IMS-AOAS-AOAS845", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shifting from traditional hazard-based food safety management toward\nrisk-based management requires statistical methods for evaluating intermediate\ntargets in food production, such as microbiological criteria (MC), in terms of\ntheir effects on human risk of illness. A fully risk-based evaluation of MC\ninvolves several uncertainties that are related to both the underlying\nQuantitative Microbiological Risk Assessment (QMRA) model and the\nproduction-specific sample data on the prevalence and concentrations of\nmicrobes in production batches. We used Bayesian modeling for statistical\ninference and evidence synthesis of two sample data sets. Thus, parameter\nuncertainty was represented by a joint posterior distribution, which we then\nused to predict the risk and to evaluate the criteria for acceptance of\nproduction batches. We also applied the Bayesian model to compare alternative\ncriteria, accounting for the statistical uncertainty of parameters, conditional\non the data sets. Comparison of the posterior mean relative risk,\n$E(\\mathit{RR}|\\mathrm{data})=E(P(\\mathrm{illness}|\\mathrm{criterion is\nmet})/P(\\mathrm{illness})|\\mathrm{data})$, and relative posterior risk,\n$\\mathit{RPR}=P(\\mathrm{illness}|\\mathrm{data, criterion is\nmet})/P(\\mathrm{illness}|\\mathrm{data})$, showed very similar results, but\ncomputing is more efficient for RPR. Based on the sample data, together with\nthe QMRA model, one could achieve a relative risk of 0.4 by insisting that the\ndefault criterion be fulfilled for acceptance of each batch.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 08:54:13 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Ranta", "Jukka", ""], ["Lindqvist", "Roland", ""], ["Hansson", "Ingrid", ""], ["Tuominen", "Pirkko", ""], ["Nauta", "Maarten", ""]]}, {"id": "1511.01690", "submitter": "Viven Visaya", "authors": "Maria Vivien Visaya, David Sherwell, Charles Kimpolo, and Mark\n  Collinson", "title": "Exploratory Analysis of Multivariate Longitudinal Child Education Data", "comments": "27 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyse binary multivariate longitudinal data of a population of\nhouseholds from a rural district in South Africa. Using a 2-dimensional\ngraphical representation of longitudinal data, each household's data is\ntransformed into a time-evolving geometric orbit. Orbits communicate complete\ninformation of change in the data over time and provide insights into the\ndynamics of both a household's and the population's evolution. The outcome of\ninterest is child educational default, where defaulting is defined as having\nfailed more than three years of schooling. A visual analysis of the impact on\neducational default of three household factors, namely the presence of a\nbiological mother, the age of the household head (minor- or adult- headed\nhousehold) and the death of an adult, is presented. In both the non-defaulting\nand defaulting households, dynamics is mainly described by the temporary in-\nand out-migration of biological mothers. We find that the presence of mother is\nmore likely in the non-defaulting households. Owing to insufficient events\ninvolving change in the age of the household head and adult deaths, we have no\nconclusion regarding their effect. Orbits offer easily interpreted information\nof clusters, patterns of change, and the density of state transitions of\nhousehold orbits.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 10:39:36 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Visaya", "Maria Vivien", ""], ["Sherwell", "David", ""], ["Kimpolo", "Charles", ""], ["Collinson", "Mark", ""]]}, {"id": "1511.01855", "submitter": "Francisco Javier Rubio", "authors": "Francisco J. Rubio and Yili Hong", "title": "Survival and lifetime data analysis with a flexible class of\n  distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a general class of continuous univariate distributions with\npositive support obtained by transforming the class of two-piece distributions.\nWe show that this class of distributions is very flexible, easy to implement,\nand contains members that can capture different tail behaviours and shapes,\nproducing also a variety of hazard functions. The proposed distributions\nrepresent a flexible alternative to the classical choices such as the\nlog-normal, Gamma, and Weibull distributions. We investigate empirically the\ninferential properties of the proposed models through an extensive simulation\nstudy. We present some applications using real data in the contexts of\ntime-to-event and accelerated failure time models. In the second kind of\napplications, we explore the use of these models in the estimation of the\ndistribution of the individual remaining life.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 19:13:08 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Rubio", "Francisco J.", ""], ["Hong", "Yili", ""]]}, {"id": "1511.01863", "submitter": "Anders Eklund", "authors": "Anders Eklund, Thomas Nichols, Hans Knutsson", "title": "Can parametric statistical methods be trusted for fMRI based group\n  studies?", "comments": null, "journal-ref": "PNAS (2016), vol. 113 no. 28, 7900 - 7905", "doi": "10.1073/pnas.1602413113", "report-no": null, "categories": "stat.AP math.ST stat.CO stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The most widely used task fMRI analyses use parametric methods that depend on\na variety of assumptions. While individual aspects of these fMRI models have\nbeen evaluated, they have not been evaluated in a comprehensive manner with\nempirical data. In this work, a total of 2 million random task fMRI group\nanalyses have been performed using resting state fMRI data, to compute\nempirical familywise error rates for the software packages SPM, FSL and AFNI,\nas well as a standard non-parametric permutation method. While there is some\nvariation, for a nominal familywise error rate of 5% the parametric statistical\nmethods are shown to be conservative for voxel-wise inference and invalid for\ncluster-wise inference; in particular, cluster size inference with a cluster\ndefining threshold of p = 0.01 generates familywise error rates up to 60%. We\nconduct a number of follow up analyses and investigations that suggest the\ncause of the invalid cluster inferences is spatial auto correlation functions\nthat do not follow the assumed Gaussian shape. By comparison, the\nnon-parametric permutation test, which is based on a small number of\nassumptions, is found to produce valid results for voxel as well as cluster\nwise inference. Using real task data, we compare the results between one\nparametric method and the permutation test, and find stark differences in the\nconclusions drawn between the two using cluster inference. These findings speak\nto the need of validating the statistical methods being used in the\nneuroimaging field.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 19:34:57 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Eklund", "Anders", ""], ["Nichols", "Thomas", ""], ["Knutsson", "Hans", ""]]}, {"id": "1511.01864", "submitter": "Hannes Matuschek", "authors": "Hannes Matuschek and Reinhold Kliegl and Shravan Vasishth and Harald\n  Baayen and Douglas Bates", "title": "Balancing Type I Error and Power in Linear Mixed Models", "comments": null, "journal-ref": "Journal of Memory and Language, 2017, 94, 305-315", "doi": "10.1016/j.jml.2017.01.001", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear mixed-effects models have increasingly replaced mixed-model analyses\nof variance for statistical inference in factorial psycholinguistic\nexperiments. Although LMMs have many advantages over ANOVA, like ANOVAs,\nsetting them up for data analysis also requires some care. One simple option,\nwhen numerically possible, is to fit the full variance-covariance structure of\nrandom effects (the maximal model; Barr et al. 2013), presumably to keep Type I\nerror down to the nominal alpha in the presence of random effects. Although it\nis true that fitting a model with only random intercepts may lead to higher\nType I error, fitting a maximal model also has a cost: it can lead to a\nsignificant loss of power. We demonstrate this with simulations and suggest\nthat for typical psychological and psycholinguistic data, higher power is\nachieved without inflating Type I error rate if a model selection criterion is\nused to select a random effect structure that is supported by the data.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 19:35:56 GMT"}, {"version": "v2", "created": "Tue, 12 Jul 2016 14:23:28 GMT"}, {"version": "v3", "created": "Mon, 2 Jan 2017 13:13:55 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Matuschek", "Hannes", ""], ["Kliegl", "Reinhold", ""], ["Vasishth", "Shravan", ""], ["Baayen", "Harald", ""], ["Bates", "Douglas", ""]]}, {"id": "1511.02001", "submitter": "Michael Scheuerer", "authors": "Michael Scheuerer, David M\\\"oller", "title": "Probabilistic wind speed forecasting on a grid based on ensemble model\n  output statistics", "comments": "Published at http://dx.doi.org/10.1214/15-AOAS843 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 3, 1328-1349", "doi": "10.1214/15-AOAS843", "report-no": "IMS-AOAS-AOAS843", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic forecasts of wind speed are important for a wide range of\napplications, ranging from operational decision making in connection with wind\npower generation to storm warnings, ship routing and aviation. We present a\nstatistical method that provides locally calibrated, probabilistic wind speed\nforecasts at any desired place within the forecast domain based on the output\nof a numerical weather prediction (NWP) model. Three approaches for wind speed\npost-processing are proposed, which use either truncated normal, gamma or\ntruncated logistic distributions to make probabilistic predictions about future\nobservations conditional on the forecasts of an ensemble prediction system\n(EPS). In order to provide probabilistic forecasts on a grid, predictive\ndistributions that were calibrated with local wind speed observations need to\nbe interpolated. We study several interpolation schemes that combine\ngeostatistical methods with local information on annual mean wind speeds, and\nevaluate the proposed methodology with surface wind speed forecasts over\nGermany from the COSMO-DE (Consortium for Small-scale Modelling) ensemble\nprediction system.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2015 08:11:04 GMT"}], "update_date": "2016-08-06", "authors_parsed": [["Scheuerer", "Michael", ""], ["M\u00f6ller", "David", ""]]}, {"id": "1511.02345", "submitter": "Rafael Z\\'arate-Mi\\~nano", "authors": "Rafael Z\\'arate Mi\\~nano, Federico Milano", "title": "Construction of SDE-based wind speed models with exponential\n  autocorrelation", "comments": "17 pages, 25 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a systematic method to build wind speed models based on\nstochastic differential equations (SDEs). The resulting models produce\nstochastic processes with a given probability distribution and exponential\ndecaying autocorrelation function. The only information needed to build the\nmodels is the probability density function of the wind speed and its\nautocorrelation coefficient. Unlike other methods previously proposed in the\nliterature, the proposed method leads to models able to reproduce an exact\nexponential autocorrelation even if the probability distribution is not\nGaussian. A sufficient condition for the property above is provided. The paper\nincludes the explicit formulation of SDE-based wind speed models obtained from\nseveral probability distributions used in the literature to describe different\nwind speed behaviors.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2015 11:38:46 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Mi\u00f1ano", "Rafael Z\u00e1rate", ""], ["Milano", "Federico", ""]]}, {"id": "1511.02362", "submitter": "Grzegorz A Rempala", "authors": "Mark G. Burch, Karly A. Jacobsen, Joseph H. Tien, Grzegorz A. Rempala", "title": "Network-Based Analysis of a Small Ebola Outbreak", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for estimating epidemic parameters in network-based\nstochastic epidemic models when the total number of infections is assumed to be\nsmall. We illustrate the method by reanalyzing the data from the 2014\nDemocratic Republic of the Congo (DRC) Ebola outbreak described in Maganga et\nal. (2014).\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2015 14:50:55 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Burch", "Mark G.", ""], ["Jacobsen", "Karly A.", ""], ["Tien", "Joseph H.", ""], ["Rempala", "Grzegorz A.", ""]]}, {"id": "1511.02644", "submitter": "Matteo Fasiolo", "authors": "Matteo Fasiolo and Simon N. Wood", "title": "Approximate methods for dynamic ecological models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document is due to appear as a chapter of the forthcoming Handbook of\nApproximate Bayesian Computation (ABC) by S. Sisson, L. Fan, and M. Beaumont.\nHere we describe some of the circumstances under which statistical ecologists\nmight benefit from using methods that base statistical inference on a set of\nsummary statistics, rather than on the full data. We focus particularly on one\nsuch approach, Synthetic Likelihood, and we show how this method represents an\nalternative to particle filters, for the purpose of fitting State Space Models\nof ecological interest. As an example application, we consider the\nprey-predator model of Turchin and Ellner (2000), and we use it to analyse the\nobserved population dynamics of Fennoscandian voles.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 11:45:38 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Fasiolo", "Matteo", ""], ["Wood", "Simon N.", ""]]}, {"id": "1511.02778", "submitter": "Willy Rodr\\'iguez", "authors": "Olivier Mazet, Willy Rodr\\'iguez, Simona Grusea, Simon Boitard and\n  Loun\\`es Chikhi", "title": "On the importance of being structured: instantaneous coalescence rates\n  and a re-evaluation of human evolution", "comments": "46 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most species are structured and influenced by processes that either increased\nor reduced gene flow between populations. However, most population genetic\ninference methods ignore population structure and reconstruct a history\ncharacterized by population size changes under the assumption that species\nbehave as panmictic units. This is potentially problematic since population\nstructure can generate spurious signals of population size change. Moreover,\nwhen the model assumed for demographic inference is misspecified, genomic data\nwill likely increase the precision of misleading if not meaningless parameters.\nIn a context of model uncertainty (panmixia \\textit{versus} structure) genomic\ndata may thus not necessarily lead to improved statistical inference.\n  We consider two haploid genomes and develop a theory which explains why any\ndemographic model (with or without population size changes) will necessarily be\ninterpreted as a series of changes in population size by inference methods\nignoring structure. We introduce a new parameter, the IICR (inverse\ninstantaneous coalescence rate), and show that it is equivalent to a population\nsize only in panmictic models, and mostly misleading for structured models. We\nargue that this general issue affects all population genetics methods ignoring\npopulation structure. We take the PSMC method as an example and show that it\ninfers population size changes that never took place. We apply our approach to\nhuman genomic data and find a reduction in gene flow at the start of the\nPleistocene, a major increase throughout the Middle-Pleistocene, and an abrupt\ndisconnection preceding the emergence of modern humans.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 17:36:52 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Mazet", "Olivier", ""], ["Rodr\u00edguez", "Willy", ""], ["Grusea", "Simona", ""], ["Boitard", "Simon", ""], ["Chikhi", "Loun\u00e8s", ""]]}, {"id": "1511.02930", "submitter": "Vishesh Karwa", "authors": "Vishesh Karwa and Pavel N. Krivitsky and Aleksandra B. Slavkovi\\'c", "title": "Sharing Social Network Data: Differentially Private Estimation of\n  Exponential-Family Random Graph Models", "comments": "Updated, 39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.CR cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by a real-life problem of sharing social network data that contain\nsensitive personal information, we propose a novel approach to release and\nanalyze synthetic graphs in order to protect privacy of individual\nrelationships captured by the social network while maintaining the validity of\nstatistical results. A case study using a version of the Enron e-mail corpus\ndataset demonstrates the application and usefulness of the proposed techniques\nin solving the challenging problem of maintaining privacy \\emph{and} supporting\nopen access to network data to ensure reproducibility of existing studies and\ndiscovering new scientific insights that can be obtained by analyzing such\ndata. We use a simple yet effective randomized response mechanism to generate\nsynthetic networks under $\\epsilon$-edge differential privacy, and then use\nlikelihood based inference for missing data and Markov chain Monte Carlo\ntechniques to fit exponential-family random graph models to the generated\nsynthetic networks.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 23:36:30 GMT"}, {"version": "v2", "created": "Fri, 23 Sep 2016 16:48:20 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Karwa", "Vishesh", ""], ["Krivitsky", "Pavel N.", ""], ["Slavkovi\u0107", "Aleksandra B.", ""]]}, {"id": "1511.03000", "submitter": "Pavel Krupskii", "authors": "Pavel Krupskii, Raphael Huser, Marc G. Genton", "title": "Factor Copula Models for Replicated Spatial Data", "comments": "34 pages, 3 tables and 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new copula model that can be used with replicated spatial data.\nUnlike the multivariate normal copula, the proposed copula is based on the\nassumption that a common factor exists and affects the joint dependence of all\nmeasurements of the process. Moreover, the proposed copula can model tail\ndependence and tail asymmetry. The model is parameterized in terms of a\ncovariance function that may be chosen from the many models proposed in the\nliterature, such as the Matern model. For some choice of common factors, the\njoint copula density is given in closed form and therefore likelihood\nestimation is very fast. In the general case, one-dimensional numerical\nintegration is needed to calculate the likelihood, but estimation is still\nreasonably fast even with large data sets. We use simulation studies to show\nthe wide range of dependence structures that can be generated by the proposed\nmodel with different choices of common factors. We apply the proposed model to\nspatial temperature data and compare its performance with some popular\ngeostatistics models.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 06:33:43 GMT"}, {"version": "v2", "created": "Sun, 10 Jul 2016 19:28:13 GMT"}, {"version": "v3", "created": "Wed, 7 Dec 2016 17:29:06 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Krupskii", "Pavel", ""], ["Huser", "Raphael", ""], ["Genton", "Marc G.", ""]]}, {"id": "1511.03120", "submitter": "Harald Baayen", "authors": "Harald Baayen and Shravan Vasishth and Douglas Bates and Reinhold\n  Kliegl", "title": "The cave of Shadows. Addressing the human factor with generalized\n  additive mixed models", "comments": "45 pages, 18 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized additive mixed models are introduced as an extension of the\ngeneralized linear mixed model which makes it possible to deal with temporal\nautocorrelational structure in experimental data. This autocorrelational\nstructure is likely to be a consequence of learning, fatigue, or the ebb and\nflow of attention within an experiment (the `human factor'). Unlike molecules\nor plots of barley, subjects in psycholinguistic experiments are intelligent\nbeings that depend for their survival on constant adaptation to their\nenvironment, including the environment of an experiment. Three data sets\nillustrate that the human factor may interact with predictors of interest, both\nfactorial and metric. We also show that, especially within the framework of the\ngeneralized additive model, in the nonlinear world, fitting maximally complex\nmodels that take every possible contingency into account is ill-advised as a\nmodeling strategy. Alternative modeling strategies are discussed for both\nconfirmatory and exploratory data analysis.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 14:25:04 GMT"}, {"version": "v2", "created": "Mon, 14 Nov 2016 21:03:49 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Baayen", "Harald", ""], ["Vasishth", "Shravan", ""], ["Bates", "Douglas", ""], ["Kliegl", "Reinhold", ""]]}, {"id": "1511.03330", "submitter": "Leontine Alkema", "authors": "Leontine Alkema, Sanqian Zhang, Doris Chou, Alison Gemmill, Ann-Beth\n  Moller, Doris Ma Fat, Lale Say, Colin Mathers and Daniel Hogan", "title": "A Bayesian approach to the global estimation of maternal mortality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The maternal mortality ratio (MMR) is defined as the number of maternal\ndeaths in a population per 100,000 live births. Country-specific MMR estimates\nare published on a regular basis by the United Nations Maternal Mortality\nEstimation Inter-agency Group (UN MMEIG) to track progress in reducing maternal\ndeaths and to evaluate regional and national performance related to Millennium\nDevelopment Goal (MDG) 5, which calls for a 75% reduction in the MMR between\n1990 and 2015.\n  Until 2014, the UN MMEIG used a multilevel regression model for producing\nestimates for countries without sufficient data from vital registration\nsystems. While this model worked well in the past to assess MMR levels for\ncountries with limited data, it was deemed unsatisfactory for final MDG 5\nreporting for countries where longer time series of observations had become\navailable because by construction, estimated trends in the MMR were\ncovariate-driven only and did not necessarily track data-driven trends.\n  We developed a Bayesian maternal mortality estimation model, which extends\nupon the UN MMEIG multilevel regression model. The new model assesses\ndata-driven trends through the inclusion of an ARIMA time series model that\ncaptures accelerations and decelerations in the rate of change in the MMR.\nVarying reporting and data quality issues are accounted for in source-specific\ndata models. The revised model provides data-driven estimates of MMR levels and\ntrends and will be used for MDG 5 reporting for all countries.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 22:57:49 GMT"}], "update_date": "2015-11-12", "authors_parsed": [["Alkema", "Leontine", ""], ["Zhang", "Sanqian", ""], ["Chou", "Doris", ""], ["Gemmill", "Alison", ""], ["Moller", "Ann-Beth", ""], ["Fat", "Doris Ma", ""], ["Say", "Lale", ""], ["Mathers", "Colin", ""], ["Hogan", "Daniel", ""]]}, {"id": "1511.03395", "submitter": "Ben Letham", "authors": "Benjamin Letham, Portia A. Letham, Cynthia Rudin, Edward P. Browne", "title": "Prediction uncertainty and optimal experimental design for learning\n  dynamical systems", "comments": null, "journal-ref": "Chaos 26, 063110 (2016)", "doi": "10.1063/1.4953795", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamical systems are frequently used to model biological systems. When these\nmodels are fit to data it is necessary to ascertain the uncertainty in the\nmodel fit. Here we present prediction deviation, a new metric of uncertainty\nthat determines the extent to which observed data have constrained the model's\npredictions. This is accomplished by solving an optimization problem that\nsearches for a pair of models that each provide a good fit for the observed\ndata, yet have maximally different predictions. We develop a method for\nestimating a priori the impact that additional experiments would have on the\nprediction deviation, allowing the experimenter to design a set of experiments\nthat would most reduce uncertainty. We use prediction deviation to assess\nuncertainty in a model of interferon-alpha inhibition of viral infection, and\nto select a sequence of experiments that reduces this uncertainty. Finally we\nprove a theoretical result which shows that prediction deviation provides\nbounds on the trajectories of the underlying true model. These results show\nthat prediction deviation is a meaningful metric of uncertainty that can be\nused for optimal experimental design.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 06:10:08 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2016 15:22:54 GMT"}, {"version": "v3", "created": "Tue, 21 Jun 2016 03:28:15 GMT"}, {"version": "v4", "created": "Thu, 26 Jan 2017 04:47:45 GMT"}, {"version": "v5", "created": "Tue, 6 Jun 2017 21:15:03 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Letham", "Benjamin", ""], ["Letham", "Portia A.", ""], ["Rudin", "Cynthia", ""], ["Browne", "Edward P.", ""]]}, {"id": "1511.03467", "submitter": "Jake Carson", "authors": "Jake Carson, Michel Crucifix, Simon Preston, Richard D. Wilkinson", "title": "Bayesian model selection for the glacial-interglacial cycle", "comments": null, "journal-ref": "Journal of the Royal Statistical Society: Series C (Applied\n  Statistics), 67(1):25-54, 2018", "doi": "10.1111/rssc.12222", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A prevailing viewpoint in palaeoclimate science is that a single\npalaeoclimate record contains insufficient information to discriminate between\nmost competing explanatory models. Results we present here suggest the\ncontrary. Using SMC^2 combined with novel Brownian bridge type proposals for\nthe state trajectories, we show that even with relatively short time series it\nis possible to estimate Bayes factors to sufficient accuracy to be able to\nselect between competing models. The results show that Monte Carlo methodology\nand computer power have now advanced to the point where a full Bayesian\nanalysis for a wide class of conceptual climate models is now possible. The\nresults also highlight a problem with estimating the chronology of the climate\nrecord prior to further statistical analysis, a practice which is common in\npalaeoclimate science. Using two datasets based on the same record but with\ndifferent estimated chronologies results in conflicting conclusions about the\nimportance of the orbital forcing on the glacial cycle, and about the internal\ndynamics generating the glacial cycle, even though the difference between the\ntwo estimated chronologies is consistent with dating uncertainty. This\nhighlights a need for chronology estimation and other inferential questions to\nbe addressed in a joint statistical procedure.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 11:51:36 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Carson", "Jake", ""], ["Crucifix", "Michel", ""], ["Preston", "Simon", ""], ["Wilkinson", "Richard D.", ""]]}, {"id": "1511.03475", "submitter": "Richard Wilkinson", "authors": "Philip B. Holden, Neil R. Edwards, James Hensman, Richard D. Wilkinson", "title": "ABC for climate: dealing with expensive simulators", "comments": "To appear in the forthcoming Handbook of Approximate Bayesian\n  Computation (ABC) by S. Sisson, L. Fan, and M. Beaumont", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is due to appear as a chapter of the forthcoming Handbook of\nApproximate Bayesian Computation (ABC) by S. Sisson, L. Fan, and M. Beaumont.\nWe describe the challenge of calibrating climate simulators, and discuss the\ndifferences in emphasis in climate science compared to many of the more\ntraditional ABC application areas. The primary difficulty is how to do\ninference with a computationally expensive simulator which we can only afford\nto run a small number of times, and we describe how Gaussian process emulators\nare used as surrogate models in this case. We introduce the idea of history\nmatching, which is a non-probabilistic calibration method, which divides the\nparameter space into (not im)plausible and implausible regions. History\nmatching can be shown to be a special case of ABC, but with a greater emphasis\non defining realistic simulator discrepancy bounds, and using these to define\ntolerances and metrics. We describe a design approach for choosing parameter\nvalues at which to run the simulator, and illustrate the approach on a toy\nclimate model, showing that with careful design we can find the plausible\nregion with a very small number of model evaluations. Finally, we describe how\ncalibrated GENIE-1 (an earth system model of intermediate complexity)\npredictions have been used, and why it is important to accurately characterise\nparametric uncertainty.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 12:26:35 GMT"}], "update_date": "2015-11-12", "authors_parsed": [["Holden", "Philip B.", ""], ["Edwards", "Neil R.", ""], ["Hensman", "James", ""], ["Wilkinson", "Richard D.", ""]]}, {"id": "1511.03726", "submitter": "Camilo Lamus", "authors": "Camilo Lamus, Matti S. Hamalainen, Emery N. Brown, and Patrick L.\n  Purdon", "title": "An Analysis of How Spatiotemporal Dynamic Models of Brain Activity Could\n  Improve MEG/EEG Inverse Solutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MEG and EEG are noninvasive functional neuroimaging techniques that provide\nrecordings of brain activity with high temporal resolution, and thus provide a\nunique window to study fast time-scale neural dynamics in humans. However, the\naccuracy of brain activity estimates resulting from these data is limited\nmainly because 1) the number of sensors is much smaller than the number of\nsources, and 2) the low sensitivity of the recording device to deep or radially\noriented sources. These factors limit the number of sources that can be\nrecovered and bias estimates to superficial cortical areas, resulting in the\nneed to include a priori information about the source activity. The question of\nhow to specify this information and how it might lead to improved solutions\nremains a critical open problem. In this paper we show that the incorporation\nof knowledge about the brain's underlying connectivity and spatiotemporal\ndynamics could dramatically improve inverse solutions. To do this, we develop\nthe concept of the \\textit{dynamic lead field mapping}, which expresses how\ninformation about source activity at a given time is mapped not only to the\nimmediate measurement, but to a time series of measurements. With this mapping\nwe show that the number of source parameters that can be recovered could\nincrease by up to a factor of ${\\sim20}$, and that such improvement is\nprimarily represented by deep cortical areas. Our result implies that future\ndevelopments in MEG/EEG analysis that model spatialtemporal dynamics have the\npotential to dramatically increase source resolution.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 23:12:06 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Lamus", "Camilo", ""], ["Hamalainen", "Matti S.", ""], ["Brown", "Emery N.", ""], ["Purdon", "Patrick L.", ""]]}, {"id": "1511.03876", "submitter": "Victor Blanco", "authors": "V\\'ictor Blanco and Jos\\'e M. P\\'erez-S\\'anchez", "title": "On the aggregation of experts' information in Bonus-Malus systems", "comments": "8 Tables; 3 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present in this paper a new premium computation principle based on the use\nof prior information from multiple sources for computing the premium charged to\na policyholder. Under this framework, based on the use of Ordered Weighted\nAveraging (OWA) operators, we propose alternative collective and Bayes premiums\nand describe some approaches to compute them. Several examples illustrates the\nnew framework for premium computation.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 12:38:50 GMT"}, {"version": "v2", "created": "Wed, 9 Nov 2016 17:05:15 GMT"}], "update_date": "2016-11-10", "authors_parsed": [["Blanco", "V\u00edctor", ""], ["P\u00e9rez-S\u00e1nchez", "Jos\u00e9 M.", ""]]}, {"id": "1511.04351", "submitter": "Scott Bruce", "authors": "Scott Bruce", "title": "A Scalable Framework for NBA Player and Team Comparisons Using Player\n  Tracking Data", "comments": "18 pages including figures and appendices", "journal-ref": "Journal of Sports Analytics 2 (2016) 107-119", "doi": "10.3233/JSA-160022", "report-no": null, "categories": "stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The release of NBA player tracking data greatly enhances the granularity and\ndimensionality of basketball statistics used to evaluate and compare player\nperformance. However, the high dimensionality of this new data source can be\ntroublesome as it demands more computational resources and reduces the ability\nto easily interpret findings. Therefore, we must find a way to reduce the\ndimensionality of the data while retaining the ability to differentiate and\ncompare player performance.\n  In this paper, Principal Component Analysis (PCA) is used to identify four\nprincipal components that account for 68% of the variation in player tracking\ndata from the 2013-2014 regular season and intuitive interpretations of these\nnew dimensions are developed by examining the statistics that influence them\nthe most. In this new high variance, low dimensional space, you can easily\ncompare statistical profiles across any or all of the principal component\ndimensions to evaluate characteristics that make certain players and teams\nsimilar or unique. A simple measure of similarity between two player or team\nstatistical profiles based on the four principal component scores is also\nconstructed. The Statistical Diversity Index (SDI) allows for quick and\nintuitive comparisons using the entirety of the player tracking data. As new\nstatistics emerge, this framework is scalable as it can incorporate existing\nand new data sources by reconstructing the principal component dimensions and\nSDI for improved comparisons. Using principal component scores and SDI, several\nuse cases are presented for improved personnel management.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 16:40:28 GMT"}, {"version": "v2", "created": "Sun, 10 Jan 2016 06:01:34 GMT"}], "update_date": "2016-11-02", "authors_parsed": [["Bruce", "Scott", ""]]}, {"id": "1511.04656", "submitter": "Xiao Li", "authors": "Xiao Li and Jinzhu Jia and Yuan Yao", "title": "Mixed and missing data: a unified treatment with latent graphical models", "comments": "11 pages, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to learn latent graphical models when data have mixed variables\nand missing values. This model could be used for further data analysis,\nincluding regression, classification, ranking etc. It also could be used for\nimputing missing values. We specify a latent Gaussian model for the data, where\nthe categorical variables are generated by discretizing an unobserved variable\nand the latent variables are multivariate Gaussian. The observed data consists\nof two parts: observed Gaussian variables and observed categorical variables,\nwhere the latter part is considered as partially missing Gaussian variables. We\nuse the Expectation-Maximization algorithm to fit the model. To prevent\noverfitting we use sparse inverse covariance estimation to obtain sparse\nestimate of the latent covariance matrix, equivalently, the graphical model.\nThe fitted model then could be used for problems including re- gression,\nclassification and ranking. Such an approach is applied to a medical data set\nwhere our method outperforms the state-of-the-art methods. Simulation studies\nand real data results suggest that our proposed model performs better than\nrandom forest in terms of prediction error when the model is correctly\nspecified, and is a better imputation method than hot deck imputation even if\nthe model is not correctly specified.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2015 04:01:44 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Li", "Xiao", ""], ["Jia", "Jinzhu", ""], ["Yao", "Yuan", ""]]}, {"id": "1511.04780", "submitter": "Sebastian Weichwald", "authors": "Sebastian Weichwald, Timm Meyer, Ozan \\\"Ozdenizci, Bernhard\n  Sch\\\"olkopf, Tonio Ball, Moritz Grosse-Wentrup", "title": "Causal interpretation rules for encoding and decoding models in\n  neuroimaging", "comments": "accepted manuscript", "journal-ref": "NeuroImage, 110:48-59, 2015", "doi": "10.1016/j.neuroimage.2015.01.036", "report-no": null, "categories": "stat.ML cs.LG q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal terminology is often introduced in the interpretation of encoding and\ndecoding models trained on neuroimaging data. In this article, we investigate\nwhich causal statements are warranted and which ones are not supported by\nempirical evidence. We argue that the distinction between encoding and decoding\nmodels is not sufficient for this purpose: relevant features in encoding and\ndecoding models carry a different meaning in stimulus- and in response-based\nexperimental paradigms. We show that only encoding models in the stimulus-based\nsetting support unambiguous causal interpretations. By combining encoding and\ndecoding models trained on the same data, however, we obtain insights into\ncausal relations beyond those that are implied by each individual model type.\nWe illustrate the empirical relevance of our theoretical findings on EEG data\nrecorded during a visuo-motor learning task.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2015 23:16:10 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Weichwald", "Sebastian", ""], ["Meyer", "Timm", ""], ["\u00d6zdenizci", "Ozan", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Ball", "Tonio", ""], ["Grosse-Wentrup", "Moritz", ""]]}, {"id": "1511.04812", "submitter": "Nicholas Reich", "authors": "Nicholas G. Reich, Stephen A. Lauer, Krzysztof Sakrejda, Sopon\n  Iamsirithaworn, Soawapak Hinjoy, Paphanij Suangtho, Suthanun Suthachana,\n  Hannah E. Clapham, Henrik Salje, Derek A. T. Cummings, Justin Lessler", "title": "Infrastructure and methods for real-time predictions of the 2014 dengue\n  fever season in Thailand", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Epidemics of communicable diseases place a huge burden on public health\ninfrastructures across the world. Producing accurate and actionable forecasts\nof infectious disease incidence at short and long time scales will improve\npublic health response to outbreaks. However, scientists and public health\nofficials face many obstacles in trying to create accurate and actionable\nreal-time forecasts of infectious disease incidence. Dengue is a mosquito-borne\nvirus that annually infects over 400 million people worldwide. We developed a\nreal-time forecasting model for dengue hemorrhagic fever in the 77 provinces of\nThailand. We created an operational and computational infrastructure that\ngenerated multi-step predictions of dengue incidence in Thai provinces every\ntwo weeks throughout 2014. These predictions show mixed performance across\nprovinces, out-performing na\\\"ive seasonal models in over half of provinces at\na 1.5 month horizon. Additionally, to assess the degree to which delays in case\nreporting make long-range prediction a challenging task, we compared the\nperformance of our real-time predictions with predictions made with fully\nreported data. This paper provides valuable lessons for the implementation of\nreal-time predictions in the context of public health decision making.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 03:40:48 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Reich", "Nicholas G.", ""], ["Lauer", "Stephen A.", ""], ["Sakrejda", "Krzysztof", ""], ["Iamsirithaworn", "Sopon", ""], ["Hinjoy", "Soawapak", ""], ["Suangtho", "Paphanij", ""], ["Suthachana", "Suthanun", ""], ["Clapham", "Hannah E.", ""], ["Salje", "Henrik", ""], ["Cummings", "Derek A. T.", ""], ["Lessler", "Justin", ""]]}, {"id": "1511.04970", "submitter": "Bruno Gon\\c{c}alves", "authors": "Bruno Gon\\c{c}alves and David S\\'anchez", "title": "Learning about Spanish dialects through Twitter", "comments": "16 pages, 5 figures, 1 table", "journal-ref": "RILI, XVI 2 (28), 65-75 (2016)", "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.CY physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper maps the large-scale variation of the Spanish language by\nemploying a corpus based on geographically tagged Twitter messages. Lexical\ndialects are extracted from an analysis of variants of tens of concepts. The\nresulting maps show linguistic variation on an unprecedented scale across the\nglobe. We discuss the properties of the main dialects within a machine learning\napproach and find that varieties spoken in urban areas have an international\ncharacter in contrast to country areas where dialects show a more regional\nuniformity.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 14:29:38 GMT"}, {"version": "v2", "created": "Mon, 6 Feb 2017 00:51:34 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Gon\u00e7alves", "Bruno", ""], ["S\u00e1nchez", "David", ""]]}, {"id": "1511.05056", "submitter": "Camilo Lamus", "authors": "Camilo Lamus, Matti S. H\\\"am\\\"al\\\"ainen, Simona Temereanca, Emery N.\n  Brown, and Patrick L. Purdon", "title": "A Spatiotemporal Dynamic Solution to the MEG Inverse Problem: An\n  Empirical Bayes Approach", "comments": null, "journal-ref": "NeuroImage, vol. 63, no. 2, pp. 894-909. 2012", "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MEG/EEG are non-invasive imaging techniques that record brain activity with\nhigh temporal resolution. However, estimation of brain source currents from\nsurface recordings requires solving an ill-posed inverse problem. Converging\nlines of evidence in neuroscience, from neuronal network models to\nresting-state imaging and neurophysiology, suggest that cortical activation is\na distributed spatiotemporal dynamic process, supported by both local and\nlong-distance neuroanatomic connections. Because spatiotemporal dynamics of\nthis kind are central to brain physiology, inverse solutions could be improved\nby incorporating models of these dynamics. In this article, we present a model\nfor cortical activity based on nearest-neighbor autoregression that\nincorporates local spatiotemporal interactions between distributed sources in a\nmanner consistent with neurophysiology and neuroanatomy. We develop a dynamic\nMaximum a Posteriori Expectation-Maximization (dMAP-EM) source localization\nalgorithm for estimation of cortical sources and model parameters based on the\nKalman Filter, the Fixed Interval Smoother, and the EM algorithms. We apply the\ndMAP-EM algorithm to simulated experiments as well as to human experimental\ndata. Furthermore, we derive expressions to relate our dynamic estimation\nformulas to those of standard static models, and show how dynamic methods\noptimally assimilate past and future data. Our results establish the\nfeasibility of spatiotemporal dynamic estimation in large-scale distributed\nsource spaces with several thousand source locations and hundreds of sensors,\nwith resulting inverse solutions that provide substantial performance\nimprovements over static methods.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 17:45:05 GMT"}, {"version": "v2", "created": "Thu, 7 Apr 2016 00:59:37 GMT"}, {"version": "v3", "created": "Sun, 19 Jun 2016 00:02:53 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Lamus", "Camilo", ""], ["H\u00e4m\u00e4l\u00e4inen", "Matti S.", ""], ["Temereanca", "Simona", ""], ["Brown", "Emery N.", ""], ["Purdon", "Patrick L.", ""]]}, {"id": "1511.05185", "submitter": "Jack O'Brien", "authors": "John D. O'Brien and Kathryn Lin and Scott MacEachern", "title": "Mixture model of pottery distributions from Lake Chad Basin\n  archaeological sites reveals ancient segregation patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a new statistical approach to analyzing an extremely common\narchaeological data type -- potsherds -- that infers the structure of cultural\nrelationships across a set of excavations. This method, applied to data from a\nset of complex, culturally heterogeneous sites around the Mandara mountains in\nthe Lake Chad Basin, articulates currently understood cultural succession into\nthe Iron Age. We show how the approach can be integrated with radiocarbon dates\nto provide detailed portraits of cultural dynamics and deposition patterns\nwithin single excavations that, in this context, indicate historical\nethnolinguistic segregation patterns. We conclude with a discussion of the many\npossible model extensions using other archaeological data types.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 21:28:35 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["O'Brien", "John D.", ""], ["Lin", "Kathryn", ""], ["MacEachern", "Scott", ""]]}, {"id": "1511.05309", "submitter": "Yehezkel Resheff", "authors": "Yehezkel S. Resheff, Daphna Weinshall", "title": "Optimized Linear Imputation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Often in real-world datasets, especially in high dimensional data, some\nfeature values are missing. Since most data analysis and statistical methods do\nnot handle gracefully missing values, the first step in the analysis requires\nthe imputation of missing values. Indeed, there has been a long standing\ninterest in methods for the imputation of missing values as a pre-processing\nstep. One recent and effective approach, the IRMI stepwise regression\nimputation method, uses a linear regression model for each real-valued feature\non the basis of all other features in the dataset. However, the proposed\niterative formulation lacks convergence guarantee. Here we propose a closely\nrelated method, stated as a single optimization problem and a block\ncoordinate-descent solution which is guaranteed to converge to a local minimum.\nExperiments show results on both synthetic and benchmark datasets, which are\ncomparable to the results of the IRMI method whenever it converges. However,\nwhile in the set of experiments described here IRMI often does not converge,\nthe performance of our methods is shown to be markedly superior in comparison\nwith other methods.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 08:26:40 GMT"}, {"version": "v2", "created": "Mon, 25 Jul 2016 17:46:18 GMT"}, {"version": "v3", "created": "Wed, 7 Dec 2016 13:28:52 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Resheff", "Yehezkel S.", ""], ["Weinshall", "Daphna", ""]]}, {"id": "1511.05356", "submitter": "Estela Bee Dagum", "authors": "Estela Bee Dagum, Silvia Bianconcini", "title": "A new set of asymmetric filters for tracking the short-term trend in\n  real-time", "comments": "Published at http://dx.doi.org/10.1214/15-AOAS856 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 3, 1433-1458", "doi": "10.1214/15-AOAS856", "report-no": "IMS-AOAS-AOAS856", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For assessing in real time the short-term trend of major economic indicators,\nofficial statistical agencies generally rely on asymmetric filters that were\ndeveloped by Musgrave in 1964. However, the use of the latter introduces\nrevisions as new observations are added to the series and, from a policy-making\nviewpoint, they are too slow in detecting true turning points. In this paper,\nwe use a reproducing kernel methodology to derive asymmetric filters that\nconverge quickly and monotonically to the corresponding symmetric one. We show\ntheoretically that proposed criteria for time-varying bandwidth selection\nproduce real-time trend-cycle filters to be preferred to the Musgrave filters\nfrom the viewpoint of revisions and time delay to detect true turning points.\nWe use a set of leading, coincident and lagging indicators of the US economy to\nillustrate the potential gains statistical agencies could have by also using\nour methods in their practice.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 11:44:10 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Dagum", "Estela Bee", ""], ["Bianconcini", "Silvia", ""]]}, {"id": "1511.05360", "submitter": "J. R. Lockwood", "authors": "J. R. Lockwood, Terrance D. Savitsky, Daniel F. McCaffrey", "title": "Inferring constructs of effective teaching from classroom observations:\n  An application of Bayesian exploratory factor analysis without restrictions", "comments": "Published at http://dx.doi.org/10.1214/15-AOAS833 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 3, 1484-1509", "doi": "10.1214/15-AOAS833", "report-no": "IMS-AOAS-AOAS833", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ratings of teachers' instructional practices using standardized classroom\nobservation instruments are increasingly being used for both research and\nteacher accountability. There are multiple instruments in use, each attempting\nto evaluate many dimensions of teaching and classroom activities, and little is\nknown about what underlying teaching quality attributes are being measured. We\nuse data from multiple instruments collected from 458 middle school mathematics\nand English language arts teachers to inform research and practice on teacher\nperformance measurement by modeling latent constructs of high-quality teaching.\nWe make inferences about these constructs using a novel approach to Bayesian\nexploratory factor analysis (EFA) that, unlike commonly used approaches for\nidentifying factor loadings in Bayesian EFA, is invariant to how the data\ndimensions are ordered. Applying this approach to ratings of lessons reveals\ntwo distinct teaching constructs in both mathematics and English language arts:\n(1) quality of instructional practices; and (2) quality of teacher management\nof classrooms. We demonstrate the relationships of these constructs to other\nindicators of teaching quality, including teacher content knowledge and student\nperformance on standardized tests.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 11:52:34 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Lockwood", "J. R.", ""], ["Savitsky", "Terrance D.", ""], ["McCaffrey", "Daniel F.", ""]]}, {"id": "1511.05367", "submitter": "Thomas A. Murray", "authors": "Thomas A. Murray, Brian P. Hobbs, Bradley P. Carlin", "title": "Combining nonexchangeable functional or survival data sources in\n  oncology using generalized mixture commensurate priors", "comments": "Published at http://dx.doi.org/10.1214/15-AOAS840 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 3, 1549-1570", "doi": "10.1214/15-AOAS840", "report-no": "IMS-AOAS-AOAS840", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional approaches to statistical inference preclude structures that\nfacilitate incorporation of supplemental information acquired from similar\ncircumstances. For example, the analysis of data obtained using perfusion\ncomputed tomography to characterize functional imaging biomarkers in cancerous\nregions of the liver can benefit from partially informative data collected\nconcurrently in noncancerous regions. This paper presents a hierarchical model\nstructure that leverages all available information about a curve, using\npenalized splines, while accommodating important between-source features. Our\nproposed methods flexibly borrow strength from the supplemental data to a\ndegree that reflects the commensurability of the supplemental curve with the\nprimary curve. We investigate our method's properties for nonparametric\nregression via simulation, and apply it to a set of liver cancer data. We also\napply our method for a semiparametric hazard model to data from a clinical\ntrial that compares time to disease progression for three colorectal cancer\ntreatments, while supplementing inference with information from a previous\ntrial that tested the current standard of care.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 12:05:07 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Murray", "Thomas A.", ""], ["Hobbs", "Brian P.", ""], ["Carlin", "Bradley P.", ""]]}, {"id": "1511.05369", "submitter": "Irina Ostrovnaya", "authors": "Irina Ostrovnaya, Venkatraman E. Seshan, Colin B. Begg", "title": "Using somatic mutation data to test tumors for clonal relatedness", "comments": "Published at http://dx.doi.org/10.1214/15-AOAS836 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 3, 1533-1548", "doi": "10.1214/15-AOAS836", "report-no": "IMS-AOAS-AOAS836", "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge for cancer pathologists is to determine whether a new tumor\nin a patient with cancer is a metastasis or an independent occurrence of the\ndisease. In recent years numerous studies have evaluated pairs of tumor\nspecimens to examine the similarity of the somatic characteristics of the\ntumors and to test for clonal relatedness. As the landscape of mutation testing\nhas evolved, a number of statistical methods for determining clonality have\ndeveloped, notably for comparing losses of heterozygosity at candidate markers,\nand for comparing copy number profiles. Increasingly tumors are being evaluated\nfor point mutations in panels of candidate genes using gene sequencing\ntechnologies. Comparison of the mutational profiles of pairs of tumors presents\nunusual methodological challenges: mutations at some loci are much more common\nthan others; knowledge of the marginal mutation probabilities is scanty for\nmost loci at which mutations might occur; the sample space of potential\nmutational profiles is vast. We examine this problem and propose a test for\nclonal relatedness of a pair of tumors from a single patient. Using\nsimulations, its properties are shown to be promising. The method is\nillustrated using several examples from the literature.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 12:07:32 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Ostrovnaya", "Irina", ""], ["Seshan", "Venkatraman E.", ""], ["Begg", "Colin B.", ""]]}, {"id": "1511.05372", "submitter": "Zhao-Hua Lu", "authors": "Zhao-Hua Lu, Sy-Miin Chow, Andrew Sherwood, Hongtu Zhu", "title": "Bayesian analysis of ambulatory blood pressure dynamics with application\n  to irregularly spaced sparse data", "comments": "Published at http://dx.doi.org/10.1214/15-AOAS846 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 3, 1601-1620", "doi": "10.1214/15-AOAS846", "report-no": "IMS-AOAS-AOAS846", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ambulatory cardiovascular (CV) measurements provide valuable insights into\nindividuals' health conditions in \"real-life,\" everyday settings. Current\nmethods of modeling ambulatory CV data do not consider the dynamic\ncharacteristics of the full data set and their relationships with covariates\nsuch as caffeine use and stress. We propose a stochastic differential equation\n(SDE) in the form of a dual nonlinear Ornstein--Uhlenbeck (OU) model with\nperson-specific covariates to capture the morning surge and nighttime dipping\ndynamics of ambulatory CV data. To circumvent the data analytic constraint that\nempirical measurements are typically collected at irregular and much larger\ntime intervals than those evaluated in simulation studies of SDEs, we adopt a\nBayesian approach with a regularized Brownian Bridge sampler (RBBS) and an\nefficient multiresolution (MR) algorithm to fit the proposed SDE. The MR\nalgorithm can produce more efficient MCMC samples that is crucial for valid\nparameter estimation and inference. Using this model and algorithm to data from\nthe Duke Behavioral Investigation of Hypertension Study, results indicate that\nage, caffeine intake, gender and race have effects on distinct dynamic\ncharacteristics of the participants' CV trajectories.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 12:11:15 GMT"}, {"version": "v2", "created": "Tue, 10 Jan 2017 12:38:56 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Lu", "Zhao-Hua", ""], ["Chow", "Sy-Miin", ""], ["Sherwood", "Andrew", ""], ["Zhu", "Hongtu", ""]]}, {"id": "1511.05375", "submitter": "Thierry Chekouo", "authors": "Thierry Chekouo, Alejandro Murua, Wolfgang Raffelsberger", "title": "The Gibbs-plaid biclustering model", "comments": "Published at http://dx.doi.org/10.1214/15-AOAS854 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 3, 1643-1670", "doi": "10.1214/15-AOAS854", "report-no": "IMS-AOAS-AOAS854", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and develop a Bayesian plaid model for biclustering that accounts\nfor the prior dependency between genes (and/or conditions) through a stochastic\nrelational graph. This work is motivated by the need for improved understanding\nof the molecular mechanisms of human diseases for which effective drugs are\nlacking, and based on the extensive raw data available through gene expression\nprofiling. We model the prior dependency information from biological knowledge\ngathered from gene ontologies. Our model, the Gibbs-plaid model, assumes that\nthe relational graph is governed by a Gibbs random field. To estimate the\nposterior distribution of the bicluster membership labels, we develop a\nstochastic algorithm that is partly based on the Wang-Landau flat-histogram\nalgorithm. We apply our method to a gene expression database created from the\nstudy of retinal detachment, with the aim of confirming known or finding novel\nsubnetworks of proteins associated with this disorder.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 12:20:14 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Chekouo", "Thierry", ""], ["Murua", "Alejandro", ""], ["Raffelsberger", "Wolfgang", ""]]}, {"id": "1511.05397", "submitter": "Forrest Crawford", "authors": "Forrest W. Crawford and Peter M. Aronow and Li Zeng and Jianghong Li", "title": "Identification of homophily and preferential recruitment in\n  respondent-driven sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Respondent-driven sampling (RDS) is a link-tracing procedure for surveying\nhidden or hard-to-reach populations in which subjects recruit other subjects\nvia their social network. There is significant research interest in detecting\nclustering or dependence of epidemiological traits in networks, but researchers\ndisagree about whether data from RDS studies can reveal it. Two distinct\nmechanisms account for dependence in traits of recruiters and recruitees in an\nRDS study: homophily, the tendency for individuals to share social ties with\nothers exhibiting similar characteristics, and preferential recruitment, in\nwhich recruiters do not recruit uniformly at random from their available\nalters. The different effects of network homophily and preferential recruitment\nin RDS studies have been a source of confusion in methodological research on\nRDS, and in empirical studies of the social context of health risk in hidden\npopulations. In this paper, we give rigorous definitions of homophily and\npreferential recruitment and show that neither can be measured precisely in\ngeneral RDS studies. We derive nonparametric identification regions for\nhomophily and preferential recruitment and show that these parameters are not\npoint identified unless the network takes a degenerate form. The results\nindicate that claims of homophily or recruitment bias measured from empirical\nRDS studies may not be credible. We apply our identification results to a study\ninvolving both a network census and RDS on a population of injection drug users\nin Hartford, CT.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 13:41:02 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Crawford", "Forrest W.", ""], ["Aronow", "Peter M.", ""], ["Zeng", "Li", ""], ["Li", "Jianghong", ""]]}, {"id": "1511.05478", "submitter": "Hilde Wilkinson-Herbots Dr", "authors": "Hilde Wilkinson-Herbots", "title": "A fast method to estimate speciation parameters in a model of isolation\n  with an initial period of gene flow and to test alternative evolutionary\n  scenarios", "comments": "16 pages, 5 figures, 3 tables, ancillary file: computer code in R", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a model of \"isolation with an initial period of migration\" (IIM),\nwhere an ancestral population instantaneously split into two descendant\npopulations which exchanged migrants symmetrically at a constant rate for a\nperiod of time but which are now completely isolated from each other. A method\nof Maximum Likelihood estimation of the parameters of the model is implemented,\nfor data consisting of the number of nucleotide differences between two DNA\nsequences at each of a large number of independent loci, using the explicit\nanalytical expressions for the likelihood obtained in Wilkinson-Herbots (2012).\nThe method is demonstrated on a large set of DNA sequence data from two species\nof Drosophila, as well as on simulated data. The method is extremely fast,\nreturning parameter estimates in less than 1 minute for a data set consisting\nof the numbers of differences between pairs of sequences from 10,000s of loci,\nor in a small fraction of a second if all loci are trimmed to the same\nestimated mutation rate. It is also illustrated how the maximized likelihood\ncan be used to quickly distinguish between competing models describing\nalternative evolutionary scenarios, either by comparing AIC scores or by means\nof likelihood ratio tests. The present implementation is for a simple version\nof the model, but various extensions are possible and are briefly discussed.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 17:28:18 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Wilkinson-Herbots", "Hilde", ""]]}, {"id": "1511.05600", "submitter": "Joonhwi Joo", "authors": "Ali Hortacsu and Joonhwi Joo", "title": "Semiparametric Estimation of a CES Demand System with Observed and\n  Unobserved Product Characteristics", "comments": "48 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a characteristics based demand estimation framework for the\nMarshallian demand system obtained by solving a budget-constrained constant\nelasticity of substitution (CES) utility maximization problem. From our\nMarshallian CES demand system, we derive the same market share equation of\nBerry (1994); Berry, Levinsohn, and Pakes (1995)'s characteristics based logit\ndemand system. Our CES demand estimation framework can accommodate zero\npredicted and observed market shares by conceptually separating the\nwhether-to-buy decision and how-much-to-buy decision. Furthermore, the\nestimator we suggest allows a tractable semiparametric estimation strategy that\nis flexible regarding the distribution of unobservable product characteristics.\nWe apply our framework to scanner data on cola sales, where we show estimated\ndemand curves can be upward sloping if zero market shares are not accommodated\nproperly.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 22:01:15 GMT"}, {"version": "v2", "created": "Thu, 31 Mar 2016 03:35:28 GMT"}, {"version": "v3", "created": "Sat, 25 Jun 2016 23:17:43 GMT"}, {"version": "v4", "created": "Wed, 11 Jan 2017 19:44:30 GMT"}, {"version": "v5", "created": "Wed, 5 Apr 2017 16:28:37 GMT"}, {"version": "v6", "created": "Wed, 12 Apr 2017 18:27:45 GMT"}, {"version": "v7", "created": "Fri, 19 Jan 2018 01:25:38 GMT"}, {"version": "v8", "created": "Sun, 17 Jun 2018 17:39:40 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Hortacsu", "Ali", ""], ["Joo", "Joonhwi", ""]]}, {"id": "1511.05614", "submitter": "Ryan Dew", "authors": "Ryan Dew and Asim Ansari", "title": "Model-based Dashboards for Customer Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automating the customer analytics process is crucial for companies that\nmanage distinct customer bases. In such data-rich and dynamic environments,\nvisualization plays a key role in understanding events of interest. These ideas\nhave led to the popularity of analytics dashboards, yet academic research has\npaid scant attention to these managerial needs. We develop a probabilistic,\nnonparametric framework for understanding and predicting individual-level\nspending using Gaussian process priors over latent functions that describe\ncustomer spending along calendar time, interpurchase time, and customer\nlifetime dimensions. These curves form a dashboard that provides a visual\nmodel-based representation of purchasing dynamics that is easily\ncomprehensible. The model flexibly and automatically captures the form and\nduration of the impact of events that influence spend propensity, even when\nsuch events are unknown a-priori. We illustrate the use of our Gaussian Process\nPropensity Model (GPPM) on data from two popular mobile games. We show that the\nGPPM generalizes hazard and buy-till-you-die models by incorporating calendar\ntime dynamics while simultaneously accounting for recency and lifetime effects.\nIt therefore provides insights about spending propensity beyond those available\nfrom these models. Finally, we show that the GPPM outperforms these benchmarks\nboth in fitting and forecasting real and simulated spend data.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 23:19:00 GMT"}, {"version": "v2", "created": "Sat, 28 Nov 2015 22:23:37 GMT"}, {"version": "v3", "created": "Tue, 1 Mar 2016 21:27:58 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Dew", "Ryan", ""], ["Ansari", "Asim", ""]]}, {"id": "1511.05698", "submitter": "Abderrahim Halimi", "authors": "Abderrahim Halimi, Paul Honeine, Jose Bioucas-Dias", "title": "Hyperspectral Unmixing in Presence of Endmember Variability,\n  Nonlinearity or Mismodelling Effects", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2016.2590324", "report-no": null, "categories": "physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents three hyperspectral mixture models jointly with Bayesian\nalgorithms for supervised hyperspectral unmixing. Based on the residual\ncomponent analysis model, the proposed general formulation assumes the linear\nmodel to be corrupted by an additive term whose expression can be adapted to\naccount for nonlinearities (NL), endmember variability (EV), or mismodelling\neffects (ME). The NL effect is introduced by considering a polynomial\nexpression that is related to bilinear models. The proposed new formulation of\nEV accounts for shape and scale endmember changes while enforcing a smooth\nspectral/spatial variation. The ME formulation takes into account the effect of\noutliers and copes with some types of EV and NL. The known constraints on the\nparameter of each observation model are modeled via suitable priors. The\nposterior distribution associated with each Bayesian model is optimized using a\ncoordinate descent algorithm which allows the computation of the maximum a\nposteriori estimator of the unknown model parameters. The proposed mixture and\nBayesian models and their estimation algorithms are validated on both synthetic\nand real images showing competitive results regarding the quality of the\ninferences and the computational complexity when compared to the\nstate-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 08:50:21 GMT"}, {"version": "v2", "created": "Mon, 18 Jul 2016 21:01:56 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Halimi", "Abderrahim", ""], ["Honeine", "Paul", ""], ["Bioucas-Dias", "Jose", ""]]}, {"id": "1511.05728", "submitter": "Manfred te Grotenhuis Dr", "authors": "Manfred te Grotenhuis and Paula Thijs", "title": "Dummy variables and their interactions in regression analysis: examples\n  from research on body mass index", "comments": "7448 words, 3 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is especially written for students and demonstrates the correct\nuse of nominal and ordinal scaled variables in regression analysis by means of\nso-called dummy variables. We start out with examples of body mass index (BMI)\ndifferences between males and females, and between low, middle, and high\neducated people. We extend our examples with several explanatory (dummy)\nvariables and the interactions between dummy variables. Readers learn how to\nuse dummy variables and their interactions and how to interpret the statistical\nresults. We included data, SPSS syntax, and additional information on a website\n(http://www.ru.nl/sociology/mt/bmi/downloads/) that goes with this text. No\nmathematical knowledge is required.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 10:52:07 GMT"}, {"version": "v2", "created": "Sat, 21 Nov 2015 19:08:32 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Grotenhuis", "Manfred te", ""], ["Thijs", "Paula", ""]]}, {"id": "1511.05837", "submitter": "Stylianos Kampakis", "authors": "Stylianos Kampakis, William Thomas", "title": "Using Machine Learning to Predict the Outcome of English County twenty\n  over Cricket Matches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cricket betting is a multi-billion dollar market. Therefore, there is a\nstrong incentive for models that can predict the outcomes of games and beat the\nodds provided by bookers. The aim of this study was to investigate to what\ndegree it is possible to predict the outcome of cricket matches. The target\ncompetition was the English twenty over county cricket cup. The original\nfeatures alongside engineered features gave rise to more than 500 team and\nplayer statistics. The models were optimized firstly with team features only\nand then both team and player features. The performance of the models was\ntested over individual seasons from 2009 to 2014 having been trained over\nprevious season data in each case. The optimal model was a simple prediction\nmethod combined with complex hierarchical features and was shown to\nsignificantly outperform a gambling industry benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 15:39:18 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Kampakis", "Stylianos", ""], ["Thomas", "William", ""]]}, {"id": "1511.05877", "submitter": "Zied Ben Bouallegue", "authors": "Zied Ben Bouallegue and Tobias Heppelmann and Susanne E. Theis and\n  Pierre Pinson", "title": "Generation of scenarios from calibrated ensemble forecasts with a dual\n  ensemble copula coupling approach", "comments": null, "journal-ref": null, "doi": "10.1175/MWR-D-15-0403.1", "report-no": null, "categories": "stat.AP physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic forecasts in the form of ensemble of scenarios are required for\ncomplex decision making processes. Ensemble forecasting systems provide such\nproducts but the spatio-temporal structures of the forecast uncertainty is lost\nwhen statistical calibration of the ensemble forecasts is applied for each lead\ntime and location independently. Non-parametric approaches allow the\nreconstruction of spatio-temporal joint probability distributions at a low\ncomputational cost. For example, the ensemble copula coupling (ECC) method\nrebuilds the multivariate aspect of the forecast from the original ensemble\nforecasts. Based on the assumption of error stationarity, parametric methods\naim to fully describe the forecast dependence structures. In this study, the\nconcept of ECC is combined with past data statistics in order to account for\nthe autocorrelation of the forecast error. The new approach, called d-ECC, is\napplied to wind forecasts from the high resolution ensemble system COSMO-DE-EPS\nrun operationally at the German weather service. Scenarios generated by ECC and\nd-ECC are compared and assessed in the form of time series by means of\nmultivariate verification tools and in a product oriented framework.\nVerification results over a 3 month period show that the innovative method\nd-ECC outperforms or performs as well as ECC in all investigated aspects.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 16:58:08 GMT"}, {"version": "v2", "created": "Fri, 3 Jun 2016 09:10:56 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Bouallegue", "Zied Ben", ""], ["Heppelmann", "Tobias", ""], ["Theis", "Susanne E.", ""], ["Pinson", "Pierre", ""]]}, {"id": "1511.05924", "submitter": "Shuo Chen", "authors": "Shuo Chen, Chengsheng Jiang, Lance Waller", "title": "Automatic Region-wise Spatially Varying Coefficient Regression Model: an\n  Application to National Cardiovascular Disease Mortality and Air Pollution\n  Association Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by analyzing a national data base of annual air pollution and\ncardiovascular disease mortality rate for 3100 counties in the U.S. (areal\ndata), we develop a novel statistical framework to automatically detect\nspatially varying region-wise associations between air pollution exposures and\nhealth outcomes. The automatic region-wise spatially varying coefficient model\nconsists three parts: we first compute the similarity matrix between the\nexposure-health outcome associations of all spatial units, then segment the\nwhole map into a set of disjoint regions based on the adjacency matrix with\nconstraints that all spatial units within a region are contiguous and have\nsimilar association, and lastly estimate the region specific associations\nbetween exposure and health outcome. We implement the framework by using\nregression and spectral graph techniques. We develop goodness of fit criteria\nfor model assessment and model selection. The simulation study confirms the\nsatisfactory performance of our model. We further employ our method to\ninvestigate the association between airborne particulate matter smaller than\n2.5 microns (PM 2.5) and cardiovascular disease mortality. The results\nsuccessfully identify regions with distinct associations between the mortality\nrate and PM 2.5 that may provide insightful guidance for environmental health\nresearch.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 20:09:26 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Chen", "Shuo", ""], ["Jiang", "Chengsheng", ""], ["Waller", "Lance", ""]]}, {"id": "1511.06013", "submitter": "Roger Hoerl", "authors": "Roger W. Hoerl and Ronald D. Snee", "title": "Statistical Engineering: An Idea Whose Time Has Come?", "comments": "30 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several authors, including the American Statistician (ASA), have noted the\nchallenges facing statisticians when attacking large, complex, unstructured\nproblems, as opposed to well-defined textbook problems. Clearly, the standard\nparadigm of selecting the one \"correct\" statistical method for such problems is\nnot sufficient; a new paradigm is needed. Statistical engineering has been\nproposed as a discipline that can provide a viable paradigm to attack such\nproblems, used in conjunction with sound statistical science. Of course, in\norder to develop as a true discipline, statistical engineering needs a\nwell-developed theory, not just a formal definition and successful case\nstudies. This article documents and disseminates the current state of the\nunderlying theory of statistical engineering. Our purpose is to provide a\nvehicle for applied statisticians to further enhance the practice of\nstatistics, and for academics so interested to continue development of the\nunderlying theory of statistical engineering.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 22:50:37 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Hoerl", "Roger W.", ""], ["Snee", "Ronald D.", ""]]}, {"id": "1511.06028", "submitter": "Michal Koles\\'ar", "authors": "Timothy B. Armstrong and Michal Koles\\'ar", "title": "Optimal inference in a class of regression models", "comments": "39 pages plus supplementary materials", "journal-ref": "Econometrica, Volume 86, Issue 2, March 2018, Pages 655-683", "doi": "10.3982/ECTA14434", "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of constructing confidence intervals (CIs) for a\nlinear functional of a regression function, such as its value at a point, the\nregression discontinuity parameter, or a regression coefficient in a linear or\npartly linear regression. Our main assumption is that the regression function\nis known to lie in a convex function class, which covers most smoothness and/or\nshape assumptions used in econometrics. We derive finite-sample optimal CIs and\nsharp efficiency bounds under normal errors with known variance. We show that\nthese results translate to uniform (over the function class) asymptotic results\nwhen the error distribution is not known. When the function class is\ncentrosymmetric, these efficiency bounds imply that minimax CIs are close to\nefficient at smooth regression functions. This implies, in particular, that it\nis impossible to form CIs that are tighter using data-dependent tuning\nparameters, and maintain coverage over the whole function class. We specialize\nour results to inference on the regression discontinuity parameter, and\nillustrate them in simulations and an empirical application.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 00:03:37 GMT"}, {"version": "v2", "created": "Thu, 26 May 2016 20:07:14 GMT"}, {"version": "v3", "created": "Mon, 22 May 2017 16:07:56 GMT"}, {"version": "v4", "created": "Wed, 22 Nov 2017 05:57:54 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Armstrong", "Timothy B.", ""], ["Koles\u00e1r", "Michal", ""]]}, {"id": "1511.06063", "submitter": "Nirav Bhatt", "authors": "P Satya Jayadev, Aravind Rajeswaran, Nirav P Bhatt, Ramkrishna\n  Pasumarthy", "title": "A Novel Approach for Phase Identification in Smart Grids Using Graph\n  Theory and Principal Component Analysis", "comments": "Accepted for the presentation at ACC 16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consumers with low demand, like households, are generally supplied\nsingle-phase power by connecting their service mains to one of the phases of a\ndistribution transformer. The distribution companies face the problem of\nkeeping a record of consumer connectivity to a phase due to uninformed changes\nthat happen. The exact phase connectivity information is important for the\nefficient operation and control of distribution system. We propose a new data\ndriven approach to the problem based on Principal Component Analysis (PCA) and\nits Graph Theoretic interpretations, using energy measurements in equally timed\nshort intervals, generated from smart meters. We propose an algorithm for\ninferring phase connectivity from noisy measurements. The algorithm is\ndemonstrated using simulated data for phase connectivities in distribution\nnetworks.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 05:39:16 GMT"}, {"version": "v2", "created": "Tue, 7 Jun 2016 14:31:29 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Jayadev", "P Satya", ""], ["Rajeswaran", "Aravind", ""], ["Bhatt", "Nirav P", ""], ["Pasumarthy", "Ramkrishna", ""]]}, {"id": "1511.06262", "submitter": "Giuseppe Jurman", "authors": "Giuseppe Jurman", "title": "Seasonal Linear Predictivity in National Football Championships", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the results of sport matches and competitions is an arising\nresearch field, benefiting from the growing amount of available data and the\nnovel data analytics techniques. Excellent forecasts can be achieved by\nadvanced machine learning methods applied to detailed historical data,\nespecially in very popular sports such as football (soccer). Here we show that,\ndespite the large number of confounding factors, the results of a football team\nin longer competitions (e.g., a national league) follow a basically linear\ntrend useful for predictive purposes, too. In support of this claim, we present\na set of experiments of linear regression on a database collecting the yearly\nresults of 707 teams playing in 22 divisions from 11 countries, in 20 football\nseasons.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 17:14:28 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Jurman", "Giuseppe", ""]]}, {"id": "1511.06417", "submitter": "Behnaz Pirzamanbein", "authors": "Behnaz Pirzamanbein (1 and 2), Johan Lindstr\\\"om (1), Anneli Poska (3\n  and 4) and Marie-Jos\\'e Gaillard (5) ((1) Centre for Mathematical Sciences,\n  Lund University, Sweden, (2) Centre for Environmental and Climate Research,\n  Lund University, Sweden, (3) Department of Physical Geography and Ecosystems\n  Analysis, Lund University, Sweden, (4) Institute of Geology, Tallinn\n  University of Technology, Estonia, (5) Department of Biology and\n  Environmental Sciences, Linnaeus University, Sweden)", "title": "Modelling Spatial Compositional Data: Reconstructions of past land cover\n  and uncertainties", "comments": null, "journal-ref": null, "doi": "10.1016/j.spasta.2018.03.005", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we construct a hierarchical model for spatial compositional\ndata, which is used to reconstruct past land-cover compositions (in terms of\nconiferous forest, broadleaved forest, and unforested/open land) for five time\nperiods during the past $6\\,000$ years over Europe. The model consists of a\nGaussian Markov Random Field (GMRF) with Dirichlet observations. A block\nupdated Markov chain Monte Carlo (MCMC), including an adaptive Metropolis\nadjusted Langevin step, is used to estimate model parameters. The sparse\nprecision matrix in the GMRF provides computational advantages leading to a\nfast MCMC algorithm. Reconstructions are obtained by combining pollen-based\nestimates of vegetation cover at a limited number of locations with scenarios\nof past deforestation and output from a dynamic vegetation model. To evaluate\nuncertainties in the predictions a novel way of constructing joint confidence\nregions for the entire composition at each prediction location is proposed. The\nhierarchical model's ability to reconstruct past land cover is evaluated\nthrough cross validation for all time periods, and by comparing reconstructions\nfor the recent past to a present day European forest map. The evaluation\nresults are promising and the model is able to capture known structures in past\nland-cover compositions.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 22:09:15 GMT"}, {"version": "v2", "created": "Fri, 24 Feb 2017 09:30:15 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Pirzamanbein", "Behnaz", "", "1 and 2"], ["Lindstr\u00f6m", "Johan", "", "3\n  and 4"], ["Poska", "Anneli", "", "3\n  and 4"], ["Gaillard", "Marie-Jos\u00e9", ""]]}, {"id": "1511.06462", "submitter": "Ilya Soloveychik", "authors": "Ilya Soloveychik and Ami Wiesel", "title": "Joint Inverse Covariances Estimation with Mutual Linear Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of joint estimation of structured inverse covariance\nmatrices. We perform the estimation using groups of measurements with different\ncovariances of the same unknown structure. Assuming the inverse covariances to\nspan a low dimensional linear subspace in the space of symmetric matrices, our\naim is to determine this structure. It is then utilized to improve the\nestimation of the inverse covariances. We propose a novel optimization\nalgorithm discovering and exploiting the underlying structure and provide its\nefficient implementation. Numerical simulations are presented to illustrate the\nperformance benefits of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 00:18:12 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Soloveychik", "Ilya", ""], ["Wiesel", "Ami", ""]]}, {"id": "1511.06546", "submitter": "Antti Honkela", "authors": "Aravind Sankar, Brandon Malone, Sion Bayliss, Ben Pascoe, Guillaume\n  M\\'eric, Matthew D. Hitchings, Samuel K. Sheppard, Edward J. Feil, Jukka\n  Corander and Antti Honkela", "title": "Bayesian identification of bacterial strains from sequencing data", "comments": "16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN q-bio.QM stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Rapidly assaying the diversity of a bacterial species present in a sample\nobtained from a hospital patient or an evironmental source has become possible\nafter recent technological advances in DNA sequencing. For several applications\nit is important to accurately identify the presence and estimate relative\nabundances of the target organisms from short sequence reads obtained from a\nsample. This task is particularly challenging when the set of interest includes\nvery closely related organisms, such as different strains of pathogenic\nbacteria, which can vary considerably in terms of virulence, resistance and\nspread. Using advanced Bayesian statistical modelling and computation\ntechniques we introduce a novel pipeline for bacterial identification that is\nshown to outperform the currently leading pipeline for this purpose. Our\napproach enables fast and accurate sequence-based identification of bacterial\nstrains while using only modest computational resources. Hence it provides a\nuseful tool for a wide spectrum of applications, including rapid clinical\ndiagnostics to distinguish among closely related strains causing nosocomial\ninfections. The software implementation is available at\nhttps://github.com/PROBIC/BIB\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 10:12:06 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2016 17:16:45 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Sankar", "Aravind", ""], ["Malone", "Brandon", ""], ["Bayliss", "Sion", ""], ["Pascoe", "Ben", ""], ["M\u00e9ric", "Guillaume", ""], ["Hitchings", "Matthew D.", ""], ["Sheppard", "Samuel K.", ""], ["Feil", "Edward J.", ""], ["Corander", "Jukka", ""], ["Honkela", "Antti", ""]]}, {"id": "1511.06639", "submitter": "Pierre-Olivier Amblard", "authors": "Augusto Zebadua, Pierre-Olivier Amblard, Eric Moisan and Olivier .J.J.\n  Michel", "title": "Compressed and quantized correlation estimators", "comments": "submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In passive monitoring using sensor networks, low energy supplies drastically\nconstrain sensors in terms of calculation and communication abilities.\nDesigning processing algorithms at the sensor level that take into account\nthese constraints is an important problem in this context. We study here the\nestimation of correlation functions between sensors using compressed\nacquisition and one-bit-quantization. The estimation is achieved directly using\ncompressed samples, without considering any reconstruction of the signals. We\nshow that if the signals of interest are far from white noise, estimation of\nthe correlation using $M$ compressed samples out of $N\\geq M$ can be more\nadvantageous than estimation of the correlation using $M$ consecutive samples.\nThe analysis consists of studying the asymptotic performance of the estimators\nat a fixed compression rate. We provide the analysis when the compression is\nrealized by a random projection matrix composed of independent and identically\ndistributed entries. The framework includes widely used random projection\nmatrices, such as Gaussian and Bernoulli matrices, and it also includes very\nsparse matrices. However, it does not include subsampling without replacement,\nfor which a separate analysis is provided. When considering\none-bit-quantization as well, the theoretical analysis is not tractable.\nHowever, empirical evidence allows the conclusion that in practical situations,\ncompressed and quantized estimators behave sufficiently correctly to be useful\nin, for example, time-delay estimation and model estimation.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 15:29:35 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Zebadua", "Augusto", ""], ["Amblard", "Pierre-Olivier", ""], ["Moisan", "Eric", ""], ["Michel", "Olivier . J. J.", ""]]}, {"id": "1511.06663", "submitter": "Cecilia Damon", "authors": "Luca Talenti, Margaux Luck, Anastasia Yartseva, Nicolas Argy, Sandrine\n  Houz\\'e and Cecilia Damon", "title": "L1 logistic regression as a feature selection step for training stable\n  classification trees for the prediction of severity criteria in imported\n  malaria", "comments": "18 pages, 10 figures, ICLR, computational science - Learning,\n  Imported Malaria, L1 logistic regression, Decision tree", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate classification methods using explanatory and predictive models\nare necessary for characterizing subgroups of patients according to their risk\nprofiles. Popular methods include logistic regression and classification trees\nwith performances that vary according to the nature and the characteristics of\nthe dataset. In the context of imported malaria, we aimed at classifying\nseverity criteria based on a heterogeneous patient population. We investigated\nthese approaches by implementing two different strategies: L1 logistic\nregression (L1LR) that models a single global solution and classification trees\nthat model multiple local solutions corresponding to discriminant subregions of\nthe feature space. For each strategy, we built a standard model, and a sparser\nversion of it. As an alternative to pruning, we explore a promising approach\nthat first constrains the tree model with an L1LR-based feature selection, an\napproach we called L1LR-Tree. The objective is to decrease its vulnerability to\nsmall data variations by removing variables corresponding to unstable local\nphenomena. Our study is twofold: i) from a methodological perspective comparing\nthe performances and the stability of the three previous methods, i.e L1LR,\nclassification trees and L1LR-Tree, for the classification of severe forms of\nimported malaria, and ii) from an applied perspective improving the actual\nclassification of severe forms of imported malaria by identifying more\npersonalized profiles predictive of several clinical criteria based on\nvariables dismissed for the clinical definition of the disease. The main\nmethodological results show that the combined method L1LR-Tree builds sparse\nand stable models that significantly predicts the different severity criteria\nand outperforms all the other methods in terms of accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 16:12:59 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Talenti", "Luca", ""], ["Luck", "Margaux", ""], ["Yartseva", "Anastasia", ""], ["Argy", "Nicolas", ""], ["Houz\u00e9", "Sandrine", ""], ["Damon", "Cecilia", ""]]}, {"id": "1511.06730", "submitter": "Fl\\'avio Gon\\c{c}alves", "authors": "Vin\\'icius Diniz Mayrink and Fl\\'avio Bambirra Gon\\c{c}alves", "title": "A Bayesian hidden Markov mixture model to detect overexpressed\n  chromosome regions", "comments": "22 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we propose a hidden Markov mixture model for the analysis of\ngene expression measurements mapped to chromosome locations. These expression\nvalues represent preprocessed light intensities observed in each probe of\nAffymetrix oligonucleotide arrays. Here, the algorithm BLAT is used to align\nthousands of probe sequences to each chromosome. The main goal is to identify\ngenome regions associated with high expression values which define clusters\ncomposed by consecutive observations. The proposed model assumes a mixture\ndistribution in which one of the components (the one with the highest expected\nvalue) is supposed to accommodate the overexpressed clusters. The model takes\nadvantage of the serial structure of the data and uses the distance information\nbetween neighbours to infer about the existence of a Markov dependence. This\ndependence is crucially important in the detection of overexpressed regions. We\npropose and discuss a Markov chain Monte Carlo algorithm to fit the model.\nFinally, the proposed methodology is used to analyse five data sets\nrepresenting three types of cancer (breast, ovarian and brain).\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 19:23:11 GMT"}, {"version": "v2", "created": "Sat, 23 Jul 2016 20:43:07 GMT"}, {"version": "v3", "created": "Mon, 26 Sep 2016 13:56:31 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Mayrink", "Vin\u00edcius Diniz", ""], ["Gon\u00e7alves", "Fl\u00e1vio Bambirra", ""]]}, {"id": "1511.06798", "submitter": "Luke Miratrix", "authors": "Luke Miratrix and Robin Ackerman", "title": "Conducting sparse feature selection on arbitrarily long phrases in text\n  corpora with a focus on interpretability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general framework for topic-specific summarization of large text\ncorpora, and illustrate how it can be used for analysis in two quite different\ncontexts: an OSHA database of fatality and catastrophe reports (to facilitate\nsurveillance for patterns in circumstances leading to injury or death) and\nlegal decisions on workers' compensation claims (to explore relevant case law).\nOur summarization framework, built on sparse classification methods, is a\ncompromise between simple word frequency based methods currently in wide use,\nand more heavyweight, model-intensive methods such as Latent Dirichlet\nAllocation (LDA). For a particular topic of interest (e.g., mental health\ndisability, or chemical reactions), we regress a labeling of documents onto the\nhigh-dimensional counts of all the other words and phrases in the documents.\nThe resulting small set of phrases found as predictive are then harvested as\nthe summary. Using a branch-and-bound approach, this method can be extended to\nallow for phrases of arbitrary length, which allows for potentially rich\nsummarization. We discuss how focus on the purpose of the summaries can inform\nchoices of regularization parameters and model constraints. We evaluate this\ntool by comparing computational time and summary statistics of the resulting\nword lists to three other methods in the literature. We also present a new R\npackage, textreg. Overall, we argue that sparse methods have much to offer text\nanalysis, and is a branch of research that should be considered further in this\ncontext.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 23:39:48 GMT"}, {"version": "v2", "created": "Sat, 23 Jul 2016 00:18:19 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Miratrix", "Luke", ""], ["Ackerman", "Robin", ""]]}, {"id": "1511.06849", "submitter": "Mark Rowley", "authors": "M. Rowley, H. Garmo, M. Van Hemelrijck, W. Wulaningsih, B. Grundmark,\n  B. Zethelius, N. Hammar, G. Walldius, M. Inoue, L. Holmberg and A. C. C.\n  Coolen", "title": "Practical survival analysis tools for heterogeneous cohorts and\n  informative censoring", "comments": "23 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In heterogeneous cohorts and those where censoring by non-primary risks is\ninformative many conventional survival analysis methods are not applicable; the\nproportional hazards assumption is usually violated at population level and the\nobserved crude hazard rates are no longer estimators of what they would have\nbeen in the absence of other risks. In this paper, we develop a fully Bayesian\nsurvival analysis to determine the probabilistically optimal description of a\nheterogeneous cohort and we propose a novel means of recovering hazard rates\nand survival functions `decontaminated' of the effects of any competing risks.\nMost competing risks studies implicitly assume that risk correlations are\ninduced by cohort or disease heterogeneity that is not captured by covariates.\nWe additionally assume that proportional hazards hold at the level of\nindividuals, for all risks, leading to a generic statistical description that\nallows us to decontaminate the effects of informative censoring, and from which\nCox regression, frailty and random effects models, and latent class models can\nall be recovered as special cases. Synthetic data confirm that our approach can\nmap a cohort's substructure, and remove heterogeneity-induced false\nprotectivity and false aetiology effects. Application to survival data from the\nULSAM cohort leads to plausible alternative explanations for previous\ncounter-intuitive inferences to prostate cancer. The importance of managing\ncardiovascular disease as a comorbidity in women diagnosed with breast cancer\nis suggested on application to survival data from the AMORIS study.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2015 07:30:56 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Rowley", "M.", ""], ["Garmo", "H.", ""], ["Van Hemelrijck", "M.", ""], ["Wulaningsih", "W.", ""], ["Grundmark", "B.", ""], ["Zethelius", "B.", ""], ["Hammar", "N.", ""], ["Walldius", "G.", ""], ["Inoue", "M.", ""], ["Holmberg", "L.", ""], ["Coolen", "A. C. C.", ""]]}, {"id": "1511.06896", "submitter": "Cristina Mollica", "authors": "Cristina Mollica and Lea Petrella", "title": "Bayesian binary quantile regression for the analysis of Bachelor-Master\n  transition", "comments": "24 pages, 7 figures and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multi-cycle organization of modern university systems stimulates the\ninterest in studying the progression to higher level degree courses during the\nacademic career. In particular, after the achievement of the first level\nqualification (Bachelor degree), students have to decide whether to continue\ntheir university studies, by enrolling in a second level (Master) programme, or\nto conclude their training experience. In this work we propose a binary\nquantile regression approach to analyze the Bachelor-Master transition\nphenomenon with the adoption of the Bayesian inferential perspective. In\naddition to the traditional predictors of academic outcomes, such as the\npersonal characteristics and the field of study, different aspects of the\nstudent's performance are considered. Moreover, a new contextual variable,\nindicating the type of university regulations, is taken into account in the\nmodel specification. The utility of the Bayesian binary quantile regression to\ncharacterize the non-continuation decision after the first cycle studies is\nillustrated with an application to administrative data of Bachelor graduates at\nthe School of Economics of Sapienza University of Rome and compared with a more\nconventional logistic regression approach.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2015 16:25:27 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2015 08:33:52 GMT"}, {"version": "v3", "created": "Fri, 4 Dec 2015 11:48:22 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Mollica", "Cristina", ""], ["Petrella", "Lea", ""]]}, {"id": "1511.07102", "submitter": "Jessica Ford", "authors": "Jessica H Ford, Toby A Patterson, Mark V Bravington", "title": "Efficient MCMC implementation of multi-state mark-recapture models", "comments": "23 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inherent differences in behaviour of individual animal movement can introduce\nbias into estimates of population parameters derived from mark-recapture data.\nAdditionally, quantifying individual heterogeneity is of considerable interest\nin it's own right as numerous studies have shown how heterogeneity can drive\npopulation dynamics. In this paper we incorporate multiple measures of\nindividual heterogeneity into a multi-state mark-recapture model, using a\nBeta-Binomial Gibbs sampler using MCMC estimation. We also present a novel\nIndependent Metropolis-Hastings sampler which allows for efficient updating of\nthe hyper-parameters which cannot be updated using Gibbs sampling. We tested\nthe model using simulation studies and applied the model to mark-resight data\nof North Atlantic humpback whales observed in the Stellwagen Bank National\nMarine Sanctuary where heterogeneity is present in both sighting probability\nand site preference. Simulation studies show asymptotic convergence of the\nposterior distribution for each of the hyper-parameters to true parameter\nvalues. In application to humpback whales individual heterogeneity is evident\nin sighting probability and propensity to use the marine sanctuary.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 03:32:08 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Ford", "Jessica H", ""], ["Patterson", "Toby A", ""], ["Bravington", "Mark V", ""]]}, {"id": "1511.07103", "submitter": "Jessica Ford", "authors": "Jessica H Ford, Toby A Patterson, Mark V Bravington", "title": "Modelling latent individual heterogeneity in mark-recapture data with\n  Dirichlet process priors", "comments": "18 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The natural subgroups often seen in mark-recapture studies and the complexity\nof real mark-recapture data means that parametric and discrete style models can\nbe insufficient. Non-parametric models avoid these often restrictive\nassumptions. We consider the non-parametric Dirichlet process for modelling\nlatent individual heterogeneity in probability of observation and the\nprobability of remaining in or out of a marine sanctuary. Simulation studies\ndemonstrated accurate estimation of multiple groups of latent individual\nheterogeneity. Simulations were also used to identify the limits of the\nDirichlet process. The ability of the Dirichlet process to pick up unimodal\nheterogeneity was explored in order to avoid potential spurious multimodality.\nIn application to a subset of the data from the North Atlantic humpback whales\nwe were able to estimate annual population-level variation in usage of the\nmarine sanctuary and three measures of individual-level variation. With the\nDirichlet process prior we were able to detect multimodality in each parameter.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 03:32:17 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Ford", "Jessica H", ""], ["Patterson", "Toby A", ""], ["Bravington", "Mark V", ""]]}, {"id": "1511.07281", "submitter": "Andr\\'es Felipe L\\'opez-Lopera", "authors": "Andr\\'es F. L\\'opez-Lopera and Mauricio A. \\'Alvarez and \\'Avaro A.\n  Orozco", "title": "Sparse Linear Models applied to Power Quality Disturbance Classification", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-52277-7_63", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power quality (PQ) analysis describes the non-pure electric signals that are\nusually present in electric power systems. The automatic recognition of PQ\ndisturbances can be seen as a pattern recognition problem, in which different\ntypes of waveform distortion are differentiated based on their features.\nSimilar to other quasi-stationary signals, PQ disturbances can be decomposed\ninto time-frequency dependent components by using time-frequency or time-scale\ntransforms, also known as dictionaries. These dictionaries are used in the\nfeature extraction step in pattern recognition systems. Short-time Fourier,\nWavelets and Stockwell transforms are some of the most common dictionaries used\nin the PQ community, aiming to achieve a better signal representation. To the\nbest of our knowledge, previous works about PQ disturbance classification have\nbeen restricted to the use of one among several available dictionaries. Taking\nadvantage of the theory behind sparse linear models (SLM), we introduce a\nsparse method for PQ representation, starting from overcomplete dictionaries.\nIn particular, we apply Group Lasso. We employ different types of\ntime-frequency (or time-scale) dictionaries to characterize the PQ\ndisturbances, and evaluate their performance under different pattern\nrecognition algorithms. We show that the SLM reduce the PQ classification\ncomplexity promoting sparse basis selection, and improving the classification\naccuracy.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 15:44:16 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["L\u00f3pez-Lopera", "Andr\u00e9s F.", ""], ["\u00c1lvarez", "Mauricio A.", ""], ["Orozco", "\u00c1varo A.", ""]]}, {"id": "1511.07504", "submitter": "Enrique del Castillo", "authors": "Enrique del Castillo, Alessia Beretta, Quirico Semeraro", "title": "Analysis and Optimal Targets Setup of a Multihead Weighing Machine", "comments": "21 pages, 5 figures", "journal-ref": "European Journal of Operational Research, Volume 259, Issue 1, 16\n  May 2017, Pages 384-393", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multihead weighing machines (MWMs) are ubiquitous in industry for fast and\naccurate packaging of a wide variety of foods and vegetables, small hardware\nitems and office supplies. A MWM consists of a system of multiple hoppers that\nare filled with product which when discharged through a funnel fills a package\nto a desired weight. Operating this machine requires first to specify the\nproduct weight targets or setpoints that each hopper should contain on average\nin each cycle, which do not need to be identical. The selection of these\nsetpoints has a major impact on the performance of a MWM. Each cycle, the\nmachine fills a package running a built-in knapsack algorithm that opens --or\nleaves shut-- different combinations of hoppers releasing their content such\nthat the total weight of each package is near to its target, minimizing the\namount of product ``given away\". In this paper, we address the practical open\nproblem for industry of how to determine the setpoint weights for each of the\nhoppers given a desired total package weight for a widely used type of MWM. An\norder statistic formulation based on a characterization of near-optimal\nsolutions is presented. This is shown to be computationally intractable, and a\nfaster heuristic that utilizes a lower bound approximation of the expected\nsmallest order statistic is proposed instead. The setup solutions obtained with\nthe proposed methods can result in substantial savings for MWM users.\nAlternatively, the analysis presented could be used by management to justify\nthe acquisition of new MWM machines.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 23:13:54 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["del Castillo", "Enrique", ""], ["Beretta", "Alessia", ""], ["Semeraro", "Quirico", ""]]}, {"id": "1511.07696", "submitter": "Sukhdev Singh", "authors": "Sukhdev Singh and Yogesh Mani Tripathi", "title": "Acceptance sampling plans for inverse Weibull distribution based on\n  truncated life test", "comments": "14 pages, 8 tables, 1 real data set", "journal-ref": null, "doi": "10.1007/s41872-017-0022-8", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we develop double acceptance sampling plan and group\nacceptance sampling plan for an inverse Weibull distribution based on a\ntruncated life test. We consider the median lifetime of the test units as a\nquality parameter and obtain the design parameters such as sample size and\nacceptance number. These plans are obtained under the consumer's risk and the\nproducer's risk simultaneously involved at a certain confidence level. We\npresent a simulation study to support the proposed methods and a comparison\nbetween single and double acceptance sampling plans is made. A real data set is\nalso analyzed to illustrate the implementation of the proposed sampling plans.\nFurther, the situation under which the proposed samplings plans can also be\nused for other percentiles points is discussed. Finally a conclusion is\npresented.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 13:31:51 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Singh", "Sukhdev", ""], ["Tripathi", "Yogesh Mani", ""]]}, {"id": "1511.07821", "submitter": "Tetsuya Takaishi", "authors": "Ting Ting Chen, Tetsuya Takaishi", "title": "Box-Cox transformation of firm size data in statistical analysis", "comments": "4 pages, 9 figures", "journal-ref": "Journal of Physics: Conference Series 490 (2014) 012182", "doi": "10.1088/1742-6596/490/1/012182", "report-no": null, "categories": "stat.AP q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Firm size data usually do not show the normality that is often assumed in\nstatistical analysis such as regression analysis. In this study we focus on two\nfirm size data: the number of employees and sale. Those data deviate\nconsiderably from a normal distribution. To improve the normality of those data\nwe transform them by the Box-Cox transformation with appropriate parameters.\nThe Box-Cox transformation parameters are determined so that the transformed\ndata best show the kurtosis of a normal distribution. It is found that the two\nfirm size data transformed by the Box-Cox transformation show strong linearity.\nThis indicates that the number of employees and sale have the similar property\nas a firm size indicator. The Box-Cox parameters obtained for the firm size\ndata are found to be very close to zero. In this case the Box-Cox\ntransformations are approximately a log-transformation. This suggests that the\nfirm size data we used are approximately log-normal distributions.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 15:56:27 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Chen", "Ting Ting", ""], ["Takaishi", "Tetsuya", ""]]}, {"id": "1511.07839", "submitter": "Stephen Reid", "authors": "Stephen Reid, Jonathan Taylor, Robert Tibshirani", "title": "A general framework for estimation and inference from clusters of\n  features", "comments": "27 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applied statistical problems often come with pre-specified groupings to\npredictors. It is natural to test for the presence of simultaneous group-wide\nsignal for groups in isolation, or for multiple groups together. Classical\ntests for the presence of such signals rely either on tests for the omission of\nthe entire block of variables (the classical F-test) or on the creation of an\nunsupervised prototype for the group (either a group centroid or first\nprincipal component) and subsequent t-tests on these prototypes.\n  In this paper, we propose test statistics that aim for power improvements\nover these classical approaches. In particular, we first create group\nprototypes, with reference to the response, hopefully improving on the\nunsupervised prototypes, and then testing with likelihood ratio statistics\nincorporating only these prototypes. We propose a (potentially) novel model,\ncalled the \"prototype model\", which naturally models the two-step\nprototype-then-test procedure. Furthermore, we introduce an inferential schema\ndetailing the unique considerations for different combinations of prototype\nformation and univariate/multivariate testing models. The prototype model also\nsuggests new applications to estimation and prediction.\n  Prototype formation often relies on variable selection, which invalidates\nclassical Gaussian test theory. We use recent advances in selective inference\nto account for selection in the prototyping step and retain test validity.\nSimulation experiments suggest that our testing procedure enjoys more power\nthan do classical approaches.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 19:30:36 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Reid", "Stephen", ""], ["Taylor", "Jonathan", ""], ["Tibshirani", "Robert", ""]]}, {"id": "1511.08272", "submitter": "Suyan Tian", "authors": "Suyan Tian, Chi Wang, Howard H. Chang", "title": "Feature selection for longitudinal microarray data by adapting a pathway\n  analysis method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Introduction: Feature selection and gene set analysis are of increasing\ninterest in bioinformatics. While these two approaches have been developed for\ndifferent purposes, we describe how some gene set analysis methods can be used\nto conduct feature selection. Here we adapt the gene set analysis method,\nsignificance analysis of microarray gene set reduction (SAMGSR), for feature\nselection, and propose two extensions-simple SAMGSR and two-level SAMGSR to\nidentify relevant features for longitudinal microarray data.\n  Results and Discussion: When applied to a real-world application, both simple\nand two-level SAMGSR work comparably well. Using simulated data, we further\ndemonstrate that both SAMGSR extensions have the ability to identify the true\nrelevant genes. If the relevant genes are not highly correlated with the\nirrelevant ones, the final models given by the two SAMGSR extensions are\nparsimonious as well.\n  Conclusions: By adapting SAMGSR for feature selection and applying the\nproposed algorithms on a longitudinal gene expression dataset, we demonstrate\nthat a gene set analysis method can be used for the purpose of feature\nselection. We believe this work paves the way for more research to bridge\nfeature selection and gene set analysis with the development of novel\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 02:31:33 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Tian", "Suyan", ""], ["Wang", "Chi", ""], ["Chang", "Howard H.", ""]]}, {"id": "1511.08445", "submitter": "Giulia Paci", "authors": "Giulia Paci, Giampaolo Cristadoro, Barbara Monti, Marco Lenci, Mirko\n  Degli Esposti, Gastone C. Castellani and Daniel Remondini", "title": "Characterization of DNA methylation as a function of biological\n  complexity via dinucleotide inter-distances", "comments": "13 pages, 5 figures. To be published in the Philosophical\n  Transactions A theme issue \"DNA as information\"", "journal-ref": null, "doi": "10.1098/rsta.2015.0227", "report-no": null, "categories": "q-bio.QM physics.bio-ph q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We perform a statistical study of the distances between successive\noccurrencies of a given dinucleotide in the DNA sequence for a number of\norganisms of different complexity. Our analysis highlights peculiar features of\nthe dinucleotide CG distribution in mammalian DNA, pointing towards a\nconnection with the role of such dinucleotide in DNA methylation. While the CG\ndistributions of mammals exhibit exponential tails with comparable parameters,\nthe picture for the other organisms studied (e.g., fish, insects, bacteria and\nviruses) is more heterogeneous, possibly because in these organisms DNA\nmethylation has different functional roles. Our analysis suggests that the\ndistribution of the distances between dinucleotides CG provides useful insights\nin characterizing and classifying organisms in terms of methylation\nfunctionalities.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 16:30:39 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Paci", "Giulia", ""], ["Cristadoro", "Giampaolo", ""], ["Monti", "Barbara", ""], ["Lenci", "Marco", ""], ["Esposti", "Mirko Degli", ""], ["Castellani", "Gastone C.", ""], ["Remondini", "Daniel", ""]]}, {"id": "1511.08670", "submitter": "Marco Cox", "authors": "Marco Cox and Bert de Vries", "title": "A Bayesian binary classification approach to pure tone audiometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pure tone hearing threshold is usually estimated from responses to\nstimuli at a set of standard frequencies. This paper describes a probabilistic\napproach to the estimation problem in which the hearing threshold is modelled\nas a smooth continuous function of frequency using a Gaussian process. This\nallows sampling at any frequency and reduces the number of required\nmeasurements. The Gaussian process is combined with a probabilistic response\nmodel to account for uncertainty in the responses. The resulting full model can\nbe interpreted as a two-dimensional binary classifier for stimuli, and provides\nuncertainty bands on the estimated threshold curve. The optimal next stimulus\nis determined based on an information theoretic criterion. This leads to a\nrobust adaptive estimation method that can be applied to fully automate the\nhearing threshold estimation process.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2015 13:44:08 GMT"}, {"version": "v2", "created": "Wed, 16 Mar 2016 10:57:08 GMT"}], "update_date": "2016-03-17", "authors_parsed": [["Cox", "Marco", ""], ["de Vries", "Bert", ""]]}, {"id": "1511.08775", "submitter": "Daniel W. Heck", "authors": "Daniel W. Heck and Eric-Jan Wagenmakers", "title": "Adjusted Priors for Bayes Factors Involving Reparameterized Order\n  Constraints", "comments": null, "journal-ref": "Journal of Mathematical Psychology 73 (2016) 110-116", "doi": "10.1016/j.jmp.2016.05.004", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many psychological theories that are instantiated as statistical models imply\norder constraints on the model parameters. To fit and test such restrictions,\norder constraints of the form $\\theta_i \\leq \\theta_j$ can be reparameterized\nwith auxiliary parameters $\\eta\\in [0,1]$ to replace the original parameters by\n$\\theta_i = \\eta\\cdot\\theta_j$. This approach is especially common in\nmultinomial processing tree (MPT) modeling because the reparameterized, less\ncomplex model also belongs to the MPT class. Here, we discuss the importance of\nadjusting the prior distributions for the auxiliary parameters of a\nreparameterized model. This adjustment is important for computing the Bayes\nfactor, a model selection criterion that measures the evidence in favor of an\norder constraint by trading off model fit and complexity. We show that uniform\npriors for the auxiliary parameters result in a Bayes factor that differs from\nthe one that is obtained using a multivariate uniform prior on the\norder-constrained original parameters. As a remedy, we derive the adjusted\npriors for the auxiliary parameters of the reparameterized model. The practical\nrelevance of the problem is underscored with a concrete example using the\nmulti-trial pair-clustering model.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2015 19:51:00 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2016 15:24:48 GMT"}, {"version": "v3", "created": "Sat, 7 May 2016 14:04:03 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Heck", "Daniel W.", ""], ["Wagenmakers", "Eric-Jan", ""]]}, {"id": "1511.09416", "submitter": "Julie Bessac", "authors": "Julie Bessac and Emil Mihai Constantinescu and Mihai Anitescu", "title": "Stochastic simulation of predictive space-time scenarios of wind speed\n  using observations and physical models", "comments": null, "journal-ref": null, "doi": null, "report-no": "ANL/MCS-P5432-1015", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a statistical space-time model for predicting atmospheric wind\nspeed based on deterministic numerical weather predictions and historical\nmeasurements. We consider a Gaussian multivariate space-time framework that\ncombines multiple sources of past physical model outputs and measurements along\nwith model predictions in order to produce a probabilistic wind speed forecast\nwithin the prediction window. We illustrate this strategy on a ground wind\nspeed forecast for several months in 2012 for a region near the Great Lakes in\nthe United States. The results show that the prediction is improved in the\nmean-squared sense relative to the numerical forecasts as well as in\nprobabilistic scores. Moreover, the samples are shown to produce realistic wind\nscenarios based on the sample spectrum.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 18:19:02 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2016 23:12:51 GMT"}, {"version": "v3", "created": "Thu, 20 Oct 2016 17:05:32 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Bessac", "Julie", ""], ["Constantinescu", "Emil Mihai", ""], ["Anitescu", "Mihai", ""]]}]