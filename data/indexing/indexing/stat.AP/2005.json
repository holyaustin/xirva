[{"id": "2005.00039", "submitter": "Ulrike von Luxburg", "authors": "Sascha Meyen, Dorothee M. B. Sigg, Ulrike von Luxburg, Volker H. Franz", "title": "Group Decisions based on Confidence Weighted Majority Voting", "comments": null, "journal-ref": "Cognitive Research: Principles and Implications 6, 18 (2021)", "doi": "10.1186/s41235-021-00279-0", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: It has repeatedly been reported that when making decisions under\nuncertainty, groups outperform individuals. In a lab setting, real groups are\noften replaced by simulated groups: Instead of performing an actual group\ndiscussion, individual responses are aggregated by a numerical computation.\nWhile studies typically use unweighted majority voting (MV) for this\naggregation, the theoretically optimal method is confidence weighted majority\nvoting (CWMV) -- if confidence ratings for the individual responses are\navailable. However, it is not entirely clear how well the theoretically derived\nCWMV method predicts real group decisions and confidences. Therefore, we\ncompared simulated group responses using CWMV and MV to real group responses.\n  Results: Simulated group decisions based on CWMV matched the accuracy of real\ngroup decisions very well, while simulated group decisions based on MV showed\nlower accuracy. Also, CWMV well predicted the confidence that groups put into\ntheir group decisions. Yet, individuals and real groups showed a bias towards\nunder-confidence while CWMV does not.\n  Conclusion: Our results highlight the importance of taking into account\nindividual confidences when investigating group decisions: We found that real\ngroups can aggregate individual confidences such that they match the optimal\naggregation given by CWMV. This implies that research using simulated group\ndecisions should use CWMV and not MV.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 18:19:29 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 07:48:22 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Meyen", "Sascha", ""], ["Sigg", "Dorothee M. B.", ""], ["von Luxburg", "Ulrike", ""], ["Franz", "Volker H.", ""]]}, {"id": "2005.00072", "submitter": "Anish Agarwal", "authors": "Anish Agarwal, Abdullah Alomar, Arnab Sarker, Devavrat Shah, Dennis\n  Shen, Cindy Yang", "title": "Two Burning Questions on COVID-19: Did shutting down the economy help?\n  Can we (partially) reopen the economy without risking the second wave?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As we reach the apex of the COVID-19 pandemic, the most pressing question\nfacing us is: can we even partially reopen the economy without risking a second\nwave? We first need to understand if shutting down the economy helped. And if\nit did, is it possible to achieve similar gains in the war against the pandemic\nwhile partially opening up the economy? To do so, it is critical to understand\nthe effects of the various interventions that can be put into place and their\ncorresponding health and economic implications. Since many interventions exist,\nthe key challenge facing policy makers is understanding the potential\ntrade-offs between them, and choosing the particular set of interventions that\nworks best for their circumstance. In this memo, we provide an overview of\nSynthetic Interventions (a natural generalization of Synthetic Control), a\ndata-driven and statistically principled method to perform what-if scenario\nplanning, i.e., for policy makers to understand the trade-offs between\ndifferent interventions before having to actually enact them. In essence, the\nmethod leverages information from different interventions that have already\nbeen enacted across the world and fits it to a policy maker's setting of\ninterest, e.g., to estimate the effect of mobility-restricting interventions on\nthe U.S., we use daily death data from countries that enforced severe mobility\nrestrictions to create a \"synthetic low mobility U.S.\" and predict the\ncounterfactual trajectory of the U.S. if it had indeed applied a similar\nintervention. Using Synthetic Interventions, we find that lifting severe\nmobility restrictions and only retaining moderate mobility restrictions (at\nretail and transit locations), seems to effectively flatten the curve. We hope\nthis provides guidance on weighing the trade-offs between the safety of the\npopulation, strain on the healthcare system, and impact on the economy.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 19:39:34 GMT"}, {"version": "v2", "created": "Sun, 10 May 2020 15:52:22 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Agarwal", "Anish", ""], ["Alomar", "Abdullah", ""], ["Sarker", "Arnab", ""], ["Shah", "Devavrat", ""], ["Shen", "Dennis", ""], ["Yang", "Cindy", ""]]}, {"id": "2005.00137", "submitter": "Marina Toger", "authors": "Marina Toger, Ian Shuttleworth, John \\\"Osth", "title": "How average is average? Temporal patterns in human behaviour as measured\n  by mobile phone data -- or why chose Thursdays", "comments": "10 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Mobile phone data -- with file sizes scaling into terabytes -- easily\noverwhelm the computational capacity available to some researchers. Moreover,\nfor ethical reasons, data access is often granted only to particular subsets,\nrestricting analyses to cover single days, weeks, or geographical areas.\nConsequently, it is frequently impossible to set a particular analysis or event\nin its context and know how typical it is, compared to other days, weeks or\nmonths. This is important for academic referees questioning research on mobile\nphone data and for the analysts in deciding how to sample, how much data to\nprocess, and which events are anomalous. All these issues require an\nunderstanding of variability in Big Data to answer the question of how average\nis average? This paper provides a method, using a large mobile phone dataset,\nto answer these basic but necessary questions. We show that file size is a\nrobust proxy for the activity level of phone users by profiling the temporal\nvariability of the data at an hourly, daily and monthly level. We then apply\ntime-series analysis to isolate temporal periodicity. Finally, we discuss\nconfidence limits to anomalous events in the data. We recommend an analytical\napproach to mobile phone data selection which suggests that ideally data should\nbe sampled across days, across working weeks, and across the year, to obtain a\nrepresentative average. However, where this is impossible, the temporal\nvariability is such that specific weekdays' data can provide a fair picture of\nother days in their general structure.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 23:06:15 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Toger", "Marina", ""], ["Shuttleworth", "Ian", ""], ["\u00d6sth", "John", ""]]}, {"id": "2005.00401", "submitter": "Ian Jonsen", "authors": "Ian D. Jonsen, Toby A. Patterson, Daniel P. Costa, Philip D. Doherty,\n  Brendan J. Godley, W. James Grecian, Christophe Guinet, Xavier Hoenner, Sarah\n  S. Kienle, Patrick W. Robison, Stephen C. Votier, Matthew J. Witt, Mark A.\n  Hindell, Robert G. Harcourt, Clive R. McMahon", "title": "A continuous-time state-space model for rapid quality-control of Argos\n  locations from animal-borne tags", "comments": "25 pages, 10 figures", "journal-ref": "Mov Ecol 8, 31 (2020)", "doi": "10.1186/s40462-020-00217-7", "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-space models are important tools for quality control of error-prone\nanimal movement data. The near real-time (within 24 h) capability of the Argos\nsatellite system aids dynamic ocean management of human activities by informing\nwhen animals enter intensive use zones. This capability also facilitates use of\nocean observations from animal-borne sensors in operational ocean forecasting\nmodels. Such near real-time data provision requires rapid, reliable quality\ncontrol to deal with error-prone Argos locations. We formulate a\ncontinuous-time state-space model for the three types of Argos location data\n(Least-Squares, Kalman filter, and Kalman smoother), accounting for irregular\ntiming of observations. Our model is deliberately simple to ensure speed and\nreliability for automated, near real-time quality control of Argos data. We\nvalidate the model by fitting to Argos data collected from 61 individuals\nacross 7 marine vertebrates and compare model-estimated locations to GPS\nlocations. Estimation accuracy varied among species with median Root Mean\nSquared Errors usually < 5 km and decreased with increasing data sampling rate\nand precision of Argos locations. Including a model parameter to inflate Argos\nerror ellipse sizes resulted in more accurate location estimates. In some\ncases, the model appreciably improved the accuracy of the Argos Kalman smoother\nlocations, which should not be possible if the smoother uses all available\ninformation. Our model provides quality-controlled locations from Argos\nLeast-Squares or Kalman filter data with slightly better accuracy than Argos\nKalman smoother data that are only available via reprocessing. Simplicity and\nease of use make the model suitable both for automated quality control of near\nreal-time Argos data and for manual use by researchers working with historical\nArgos data.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 14:28:28 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Jonsen", "Ian D.", ""], ["Patterson", "Toby A.", ""], ["Costa", "Daniel P.", ""], ["Doherty", "Philip D.", ""], ["Godley", "Brendan J.", ""], ["Grecian", "W. James", ""], ["Guinet", "Christophe", ""], ["Hoenner", "Xavier", ""], ["Kienle", "Sarah S.", ""], ["Robison", "Patrick W.", ""], ["Votier", "Stephen C.", ""], ["Witt", "Matthew J.", ""], ["Hindell", "Mark A.", ""], ["Harcourt", "Robert G.", ""], ["McMahon", "Clive R.", ""]]}, {"id": "2005.00469", "submitter": "Sophie Achard", "authors": "F\\'elix Renard, Christian Heinrich, Marine Bouthillon, Maleka Schenck,\n  Francis Schneider, St\\'ephane Kremer, Sophie Achard", "title": "Manifold learning for brain connectivity", "comments": "submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human brain connectome studies aim at extracting and analyzing relevant\nfeatures associated to pathologies of interest. Usually this consists in\nmodeling the brain connectome as a graph and in using graph metrics as\nfeatures. A fine brain description requires graph metrics computation at the\nnode level. Given the relatively reduced number of patients in standard\ncohorts, such data analysis problems fall in the high-dimension low sample size\nframework. In this context, our goal is to provide a machine learning technique\nthat exhibits flexibility, gives the investigator grip on the features and\ncovariates, allows visualization and exploration, and yields insight into the\ndata and the biological phenomena at stake. The retained approach is dimension\nreduction in a manifold learning methodology, the originality lying in that one\n(or several) reduced variables be chosen by the investigator. The proposed\nmethod is illustrated on two studies, the first one addressing comatose\npatients, the second one addressing young versus elderly population comparison.\nThe method sheds light on the graph metrics and underlying neurobiological\nphenomena.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 16:06:29 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Renard", "F\u00e9lix", ""], ["Heinrich", "Christian", ""], ["Bouthillon", "Marine", ""], ["Schenck", "Maleka", ""], ["Schneider", "Francis", ""], ["Kremer", "St\u00e9phane", ""], ["Achard", "Sophie", ""]]}, {"id": "2005.00501", "submitter": "Roger Silva Ph.d", "authors": "Marina M. de Queiroz, Rosangela H. Loschi and Roger W. C. Silva", "title": "Multivariate Log-Skewed Distributions with normal kernel and their\n  Applications", "comments": "20 pages", "journal-ref": "Statistics (Berlin), 2016", "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce two classes of multivariate log skewed distributions with normal\nkernel: the log canonical fundamental skew-normal (log-CFUSN) and the log\nunified skew-normal (log-SUN). We also discuss some properties of the log-CFUSN\nfamily of distributions. These new classes of log-skewed distributions include\nthe log-normal and multivariate log-skew normal families as particular cases.\nWe discuss some issues related to Bayesian inference in the log-CFUSN family of\ndistributions, mainly we focus on how to model the prior uncertainty about the\nskewing parameter. Based on the stochastic representation of the log-CFUSN\nfamily, we propose a data augmentation strategy for sampling from the posterior\ndistributions. This proposed family is used to analyze the US national monthly\nprecipitation data. We conclude that a high dimensional skewing function lead\nto a better model fit.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 17:16:16 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["de Queiroz", "Marina M.", ""], ["Loschi", "Rosangela H.", ""], ["Silva", "Roger W. C.", ""]]}, {"id": "2005.00563", "submitter": "Khandker Nurul Habib", "authors": "Khandker Nurul Habib, Wafic El-Assi, Tian Lin", "title": "How Large is too Large? A Review of the Issues related to Sample Size\n  Requirements of Regional Household Travel Surveys with a Case Study on the\n  Greater Toronto and Hamilton Area (GTHA)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a review of sample size issues related to regional\nhousehold travel surveys. A review of current practices reveals that different\nperspectives and, as a result, different practices exist in Canada, US, and\nabroad on sample size. The paper uses data from the Transportation Tomorrow\nSurvey (TTS) - a household travel survey conducted every five years in the\nGreater Toronto and Hamilton Area (GTHA) - for a set of empirical\ninvestigations that asses the adequacy of household travel survey samples. The\nempirical investigations reveal that even with a 5% sample size, a full\nrepresentation of the population and its corresponding travel behaviour may be\ndifficult (at the 95% confidence level). Therefore, based on the results of the\nempirical investigations and the literature review, the paper proposes a\nflexible framework for household travel survey sample size determination,\nespecially for Canadian municipalities.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 18:27:48 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Habib", "Khandker Nurul", ""], ["El-Assi", "Wafic", ""], ["Lin", "Tian", ""]]}, {"id": "2005.00564", "submitter": "David Robertson", "authors": "David S. Robertson, Kim May Lee, Boryana C. Lopez-Kolkovska and Sofia\n  S. Villar", "title": "Response-adaptive randomization in clinical trials: from myths to\n  practical considerations", "comments": "Major update in response to reviewers' comments", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Response-adaptive randomization (RAR) is part of a wider class of\ndata-dependent sampling algorithms, for which clinical trials are used as a\nmotivating application. In that context, patient allocation to treatments is\ndetermined by randomization probabilities that are altered based on the accrued\nresponse data in order to achieve experimental goals. RAR has received abundant\ntheoretical attention from the biostatistical literature since the 1930's and\nhas been the subject of numerous debates. In the last decade, it has received\nrenewed consideration from the applied and methodological communities, driven\nby successful practical examples and its widespread use in machine learning.\nPapers on the subject can give different views on its usefulness, and\nreconciling these may be difficult. This work aims to address this gap by\nproviding a unified, broad and up-to-date review of methodological and\npractical issues to consider when debating the use of RAR in clinical trials.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 18:34:19 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2020 13:54:30 GMT"}, {"version": "v3", "created": "Fri, 11 Jun 2021 17:29:11 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Robertson", "David S.", ""], ["Lee", "Kim May", ""], ["Lopez-Kolkovska", "Boryana C.", ""], ["Villar", "Sofia S.", ""]]}, {"id": "2005.00592", "submitter": "Mogens Graf Plessen", "authors": "Mogens Graf Plessen", "title": "Integrated Time Series Summarization and Prediction Algorithm and its\n  Application to COVID-19 Data Mining", "comments": "10 pages double column, 8 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a simple method to extract from a set of multiple related\ntime series a compressed representation for each time series based on\nstatistics for the entire set of all time series. This is achieved by a\nhierarchical algorithm that first generates an alphabet of shapelets based on\nthe segmentation of centroids for clustered data, before labels of these\nshapelets are assigned to the segmentation of each single time series via\nnearest neighbor search using unconstrained dynamic time warping as distance\nmeasure to deal with non-uniform time series lenghts. Thereby, a sequence of\nlabels is assigned for each time series. Completion of the last label sequence\npermits prediction of individual time series. Proposed method is evaluated on\ntwo global COVID-19 datasets, first, for the number of daily net cases (daily\nnew infections minus daily recoveries), and, second, for the number of daily\ndeaths attributed to COVID-19 as of April 27, 2020. The first dataset involves\n249 time series for different countries, each of length 96. The second dataset\ninvolves 264 time series, each of length 96. Based on detected anomalies in\navailable data a decentralized exit strategy from lockdowns is advocated.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 20:16:39 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Plessen", "Mogens Graf", ""]]}, {"id": "2005.00597", "submitter": "Irina Gaynanova", "authors": "Benjamin Risk and Irina Gaynanova", "title": "Simultaneous Non-Gaussian Component Analysis (SING) for Data Integration\n  in Neuroimaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As advances in technology allow the acquisition of complementary information,\nit is increasingly common for scientific studies to collect multiple datasets.\nLarge-scale neuroimaging studies often include multiple modalities (e.g., task\nfunctional MRI, resting-state fMRI, diffusion MRI, and/or structural MRI), with\nthe aim to understand the relationships between datasets. In this study, we\nseek to understand whether regions of the brain activated in a working memory\ntask relate to resting-state correlations. In neuroimaging, a popular approach\nuses principal component analysis for dimension reduction prior to canonical\ncorrelation analysis with joint independent component analysis, but this may\ndiscard biological features with low variance and/or spuriously associate\nstructure unique to a dataset with joint structure. We introduce Simultaneous\nNon-Gaussian component analysis (SING) in which dimension reduction and feature\nextraction are achieved simultaneously, and shared information is captured via\nsubject scores. We apply our method to a working memory task and resting-state\ncorrelations from the Human Connectome Project. We find joint structure as\nevident from joint scores whose loadings highlight resting-state correlations\ninvolving regions associated with working memory. Moreover, some of the subject\nscores are related to fluid intelligence.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 20:35:00 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 00:54:51 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Risk", "Benjamin", ""], ["Gaynanova", "Irina", ""]]}, {"id": "2005.00662", "submitter": "Se Yoon Lee", "authors": "Se Yoon Lee, Bowen Lei, and Bani K. Mallick", "title": "Estimation of COVID-19 spread curves integrating global data and\n  borrowing information", "comments": null, "journal-ref": "PLOS ONE 15 (2020) 1- 17", "doi": "10.1371/journal.pone.0236860", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, novel coronavirus disease 2019 (COVID-19) is a big threat to\nglobal health. The rapid spread of the virus has created pandemic, and\ncountries all over the world are struggling with a surge in COVID-19 infected\ncases. There are no drugs or other therapeutics approved by the US Food and\nDrug Administration to prevent or treat COVID-19: information on the disease is\nvery limited and scattered even if it exists. This motivates the use of data\nintegration, combining data from diverse sources and eliciting useful\ninformation with a unified view of them. In this paper, we propose a Bayesian\nhierarchical model that integrates global data for real-time prediction of\ninfection trajectory for multiple countries. Because the proposed model takes\nadvantage of borrowing information across multiple countries, it outperforms an\nexisting individual country-based model. As fully Bayesian way has been\nadopted, the model provides a powerful predictive tool endowed with uncertainty\nquantification. Additionally, a joint variable selection technique has been\nintegrated into the proposed modeling scheme, which aimed to identify possible\ncountry-level risk factors for severe disease due to COVID-19.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 00:13:48 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 18:09:04 GMT"}, {"version": "v3", "created": "Sat, 16 May 2020 03:12:43 GMT"}, {"version": "v4", "created": "Tue, 7 Jul 2020 20:01:36 GMT"}, {"version": "v5", "created": "Fri, 10 Jul 2020 17:26:20 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Lee", "Se Yoon", ""], ["Lei", "Bowen", ""], ["Mallick", "Bani K.", ""]]}, {"id": "2005.00667", "submitter": "Chenfeng Xiong", "authors": "Chenfeng Xiong, Songhua Hu, Mofeng Yang, Hannah N Younes, Weiyu Luo,\n  Sepehr Ghader, Lei Zhang", "title": "Data-Driven Modeling Reveals the Impact of Stay-at-Home Orders on Human\n  Mobility during the COVID-19 Pandemic in the U.S", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One approach to delay the spread of the novel coronavirus (COVID-19) is to\nreduce human travel by imposing travel restriction policies. It is yet unclear\nhow effective those policies are on suppressing the mobility trend due to the\nlack of ground truth and large-scale dataset describing human mobility during\nthe pandemic. This study uses real-world location-based service data collected\nfrom anonymized mobile devices to uncover mobility changes during COVID-19 and\nunder the 'Stay-at-home' state orders in the U.S. The study measures human\nmobility with two important metrics: daily average number of trips per person\nand daily average person-miles traveled. The data-driven analysis and modeling\nattribute less than 5% of the reduction in the number of trips and person-miles\ntraveled to the effect of the policy. The models developed in the study exhibit\nhigh prediction accuracy and can be applied to inform epidemics modeling with\nempirically verified mobility trends and to support time-sensitive\ndecision-making processes.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 00:37:54 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 01:36:41 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Xiong", "Chenfeng", ""], ["Hu", "Songhua", ""], ["Yang", "Mofeng", ""], ["Younes", "Hannah N", ""], ["Luo", "Weiyu", ""], ["Ghader", "Sepehr", ""], ["Zhang", "Lei", ""]]}, {"id": "2005.00869", "submitter": "Philip Pavlik Jr.", "authors": "Philip I. Pavlik, Jr., Luke G. Eglington, and Leigh M.\n  Harrell-Williams", "title": "Logistic Knowledge Tracing: A Constrained Framework for Learner Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive learning technology solutions often use a learner model to trace\nlearning and make pedagogical decisions. The present research introduces a\nformalized methodology for specifying learner models, Logistic Knowledge\nTracing (LKT), that consolidates many extant learner modeling methods. The\nstrength of LKT is the specification of a symbolic notation system for\nalternative logistic regression models that is powerful enough to specify many\nextant models in the literature and many new models. To demonstrate the\ngenerality of LKT, we fit 12 models, some variants of well-known models and\nsome newly devised, to 6 learning technology datasets. The results indicated\nthat no single learner model was best in all cases, further justifying a broad\napproach that considers multiple learner model features and the learning\ncontext. The models presented here avoid student-level fixed parameters to\nincrease generalizability. We also introduce features to stand in for these\nintercepts. We argue that to be maximally applicable, a learner model needs to\nadapt to student differences, rather than needing to be pre-parameterized with\nthe level of each student's ability.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 15:59:24 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 21:31:48 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Pavlik,", "Philip I.", "Jr."], ["Eglington", "Luke G.", ""], ["Harrell-Williams", "Leigh M.", ""]]}, {"id": "2005.00931", "submitter": "Esam Mahdi", "authors": "Esam Mahdi", "title": "portes: An R Package for Portmanteau Tests in Time Series Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we introduce the R package portes with extensive\nillustrative applications. The asymptotic distributions and the Monte Carlo\nprocedures of the most popular univariate and multivariate portmanteau test\nstatistics, including a new generalized variance statistic, for time series\nmodels using the powerful parallel computing framework facility, are\nimplemented in this package. The proposed package has a general mechanism where\nthe user can compute the test statistic for the diagnostic checking of any time\nseries models. This package is also useful for simulating univariate and\nmultivariate seasonal and nonseasonal ARIMA/VARIMA time series with finite and\ninfinite variances, testing for stationarity and invertibility, and estimating\nparameters from stable distributions.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 22:16:28 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Mahdi", "Esam", ""]]}, {"id": "2005.00947", "submitter": "Rui Sun", "authors": "David Simchi-Levi, Rui Sun, Huanan Zhang", "title": "Online Learning and Optimization for Revenue Management Problems with\n  Add-on Discounts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study in this paper a revenue management problem with add-on discounts.\nThe problem is motivated by the practice in the video game industry, where a\nretailer offers discounts on selected supportive products (e.g. video games) to\ncustomers who have also purchased the core products (e.g. video game consoles).\nWe formulate this problem as an optimization problem to determine the prices of\ndifferent products and the selection of products with add-on discounts. To\novercome the computational challenge of this optimization problem, we propose\nan efficient FPTAS algorithm that can solve the problem approximately to any\ndesired accuracy. Moreover, we consider the revenue management problem in the\nsetting where the retailer has no prior knowledge of the demand functions of\ndifferent products. To resolve this problem, we propose a UCB-based learning\nalgorithm that uses the FPTAS optimization algorithm as a subroutine. We show\nthat our learning algorithm can converge to the optimal algorithm that has\naccess to the true demand functions, and we prove that the convergence rate is\ntight up to a certain logarithmic term. In addition, we conduct numerical\nexperiments with the real-world transaction data we collect from a popular\nvideo gaming brand's online store on Tmall.com. The experiment results\nillustrate our learning algorithm's robust performance and fast convergence in\nvarious scenarios. We also compare our algorithm with the optimal policy that\ndoes not use any add-on discount, and the results show the advantages of using\nthe add-on discount strategy in practice.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 23:54:17 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Simchi-Levi", "David", ""], ["Sun", "Rui", ""], ["Zhang", "Huanan", ""]]}, {"id": "2005.00952", "submitter": "Michael Dumelle", "authors": "Michael Dumelle and Jay M. Ver Hoef and Claudio Fuentes and Alix\n  Gitelman", "title": "A Linear Mixed Model Formulation for Spatio-Temporal Random Processes\n  with Computational Advances for the Separable and Product-Sum Covariances", "comments": "43 pages (including an Appendix) and 8 figures", "journal-ref": "Spatial Staistics, Volume 43, 2021", "doi": "10.1016/j.spasta.2021.100510", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe spatio-temporal random processes using linear mixed models. We\nshow how many commonly used models can be viewed as special cases of this\ngeneral framework and pay close attention to models with separable or\nproduct-sum covariances. The proposed linear mixed model formulation\nfacilitates the implementation of a novel algorithm using Stegle\neigendecompositions, a recursive application of the Sherman-Morrison-Woodbury\nformula, and Helmert-Wolf blocking to efficiently invert separable and\nproduct-sum covariance matrices, even when every spatial location is not\nobserved at every time point. We show our algorithm provides noticeable\nimprovements over the standard Cholesky decomposition approach. Via\nsimulations, we assess the performance of the separable and product-sum\ncovariances and identify scenarios where separable covariances are noticeably\ninferior to product-sum covariances. We also compare likelihood-based and\nsemivariogram-based estimation and discuss benefits and drawbacks of both. We\nuse the proposed approach to analyze daily maximum temperature data in Oregon,\nUSA, during the 2019 summer. We end by offering guidelines for choosing among\nthese covariances and estimation methods based on properties of observed data.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 00:11:43 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Dumelle", "Michael", ""], ["Hoef", "Jay M. Ver", ""], ["Fuentes", "Claudio", ""], ["Gitelman", "Alix", ""]]}, {"id": "2005.00971", "submitter": "Esam Mahdi", "authors": "Esam Mahdi", "title": "A Powerful Portmanteau Test for Detecting Nonlinearity in Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new portmanteau test statistic is proposed for detecting nonlinearity in\ntime series data. In this paper, we elaborate on the Toeplitz autocorrelation\nmatrix to the autocorrelation and cross-correlation of residuals and squared\nresiduals block matrix. We derive a new portmanteau test statistic using the\nlog of the determinant of the mth autocorrelations and cross-correlations block\nmatrix. The asymptotic distribution of the proposed test statistic is derived\nas a linear combination of chi-squared distributions and can be approximated by\na gamma distribution. This test is applied to identify the linearity and\nnonlinearity dependency of some stationary time series models. It is shown that\nthe convergence of the new test to its asymptotic distribution is reasonable\nwith higher power than other tests in many situations. We demonstrate the\nefficiency of the proposed test by investigating linear and nonlinear effects\nin Vodafone Qatar and Nikkei-300 daily returns.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 02:56:08 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Mahdi", "Esam", ""]]}, {"id": "2005.01058", "submitter": "Emmanuel Caron", "authors": "Emmanuel Caron, J\\'er\\^ome Dedecker, Bertrand Michel", "title": "Gaussian linear model selection in a dependent context", "comments": "30 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the nonparametric linear model, when the error\nprocess is a dependent Gaussian process. We focus on the estimation of the mean\nvector via a model selection approach. We first give the general theoretical\nform of the penalty function, ensuring that the penalized estimator among a\ncollection of models satisfies an oracle inequality. Then we derive a penalty\nshape involving the spectral radius of the covariance matrix of the errors,\nwhich can be chosen proportional to the dimension when the error process is\nstationary and short range dependent. However, this penalty can be too rough in\nsome cases, in particular when the error process is long range dependent. In a\nsecond part, we focus on the fixed-design regression model assuming that the\nerror process is a stationary Gaussian process. We propose a model selection\nprocedure in order to estimate the mean function via piecewise polynomials on a\nregular partition, when the error process is either short range dependent, long\nrange dependent or anti-persistent. We present different kinds of penalties,\ndepending on the memory of the process. For each case, an adaptive estimator is\nbuilt, and the rates of convergence are computed. Thanks to several sets of\nsimulations, we study the performance of these different penalties for all\ntypes of errors (short memory, long memory and anti-persistent errors).\nFinally, we give an application of our method to the well-known Nile data,\nwhich clearly shows that the type of dependence of the error process must be\ntaken into account.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 11:33:06 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Caron", "Emmanuel", ""], ["Dedecker", "J\u00e9r\u00f4me", ""], ["Michel", "Bertrand", ""]]}, {"id": "2005.01167", "submitter": "Oliver Johnson", "authors": "Patrick Garland and Dave Babbitt and Maksym Bondarenko and Alessandro\n  Sorichetta and Andrew J. Tatem and Oliver Johnson", "title": "The COVID-19 pandemic as experienced by the individual", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ongoing COVID-19 pandemic has progressed with varying degrees of\nintensity in individual countries, suggesting it is important to analyse\nfactors that vary between them. We study measures of `population-weighted\ndensity', which capture density as perceived by a randomly chosen individual.\nThese measures of population density can significantly explain variation in the\ninitial rate of spread of COVID-19 between countries within Europe. However,\nsuch measures do not explain differences on a global scale, particularly when\nconsidering countries in East Asia, or looking later into the epidemics.\nTherefore, to control for country-level differences in response to COVID-19 we\nconsider the cross-cultural measure of individualism proposed by Hofstede. This\nscore can significantly explain variation in the size of epidemics across\nEurope, North America, and East Asia. Using both our measure of\npopulation-weighted density and the Hofstede score we can significantly explain\nhalf the variation in the current size of epidemics across Europe and North\nAmerica. By controlling for country-level responses to the virus and population\ndensity, our analysis of the global incidence of COVID-19 can help focus\nattention on epidemic control measures that are effective for individual\ncountries.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 18:56:39 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2020 10:52:29 GMT"}, {"version": "v3", "created": "Tue, 29 Sep 2020 11:45:05 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Garland", "Patrick", ""], ["Babbitt", "Dave", ""], ["Bondarenko", "Maksym", ""], ["Sorichetta", "Alessandro", ""], ["Tatem", "Andrew J.", ""], ["Johnson", "Oliver", ""]]}, {"id": "2005.01169", "submitter": "Liangliang Zhang", "authors": "Liangliang Zhang, Yushu Shi, Kim-Anh Do, Christine B. Peterson and\n  Robert R. Jenq", "title": "ProgPermute: Progressive permutation for a dynamic representation of the\n  robustness of microbiome discoveries", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identification of features is a critical task in microbiome studies that is\ncomplicated by the fact that microbial data are high dimensional and\nheterogeneous. Masked by the complexity of the data, the problem of separating\nsignals from noise becomes challenging and troublesome. For instance, when\nperforming differential abundance tests, multiple testing adjustments tend to\nbe overconservative, as the probability of a type I error (false positive)\nincreases dramatically with the large numbers of hypotheses. Moreover, the\ngrouping effect of interest can be obscured by heterogeneity. These factors can\nincorrectly lead to the conclusion that there are no differences in the\nmicrobiome compositions. We translate and represent the problem of identifying\ndifferential features as a dynamic layout of separating the signal from its\nrandom background. We propose progressive permutation as a method to achieve\nthis process and show converging patterns. More specifically, we progressively\npermute the grouping factor labels of the microbiome samples and perform\nmultiple differential abundance tests in each scenario. We then compare the\nsignal strength of the top features from the original data with their\nperformance in permutations, and observe an apparent decreasing trend if these\ntop features are true positives identified from the data. We have developed\nthis into a user-friendly RShiny tool and R package, which consist of functions\nthat can convey the overall association between the microbiome and the grouping\nfactor, rank the robustness of the discovered microbes, and list the\ndiscoveries, their effect sizes, and individual abundances.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 19:25:57 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 14:45:47 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Zhang", "Liangliang", ""], ["Shi", "Yushu", ""], ["Do", "Kim-Anh", ""], ["Peterson", "Christine B.", ""], ["Jenq", "Robert R.", ""]]}, {"id": "2005.01171", "submitter": "Keerati Suibkitwanchai", "authors": "Keerati Suibkitwanchai, Adam M. Sykulski, Guillermo Perez Algorta,\n  Daniel Waller and Catherine Walshe", "title": "Nonparametric Time Series Summary Statistics for High-Frequency\n  Accelerometry Data from Individuals with Advanced Dementia", "comments": null, "journal-ref": "PLoS ONE 15(9): e0239368 (2020)", "doi": "10.1371/journal.pone.0239368", "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accelerometry data has been widely used to measure activity and the circadian\nrhythm of individuals across the health sciences, in particular with people\nwith advanced dementia. Modern accelerometers can record continuous\nobservations on a single individual for several days at a sampling frequency of\nthe order of one hertz. Such rich and lengthy data sets provide new\nopportunities for statistical insight, but also pose challenges in selecting\nfrom a wide range of possible summary statistics, and how the calculation of\nsuch statistics should be optimally tuned and implemented. In this paper, we\nbuild on existing approaches, as well as propose new summary statistics, and\ndetail how these should be implemented with high frequency accelerometry data.\nWe test and validate our methods on an observed data set from 26 recordings\nfrom individuals with advanced dementia and 14 recordings from individuals\nwithout dementia. We study four metrics: Interdaily stability (IS), intradaily\nvariability (IV), the scaling exponent from detrended fluctuation analysis\n(DFA), and a novel nonparametric estimator which we call the proportion of\nvariance (PoV), which calculates the strength of the circadian rhythm using\nspectral density estimation. We perform a detailed analysis indicating how the\ntime series should be optimally subsampled to calculate IV, and recommend a\nsubsampling rate of approximately 5 minutes for the dataset that has been\nstudied. In addition, we propose the use of the DFA scaling exponent separately\nfor daytime and nighttime, to further separate effects between individuals. We\ncompare the relationships between all these methods and show that they\neffectively capture different features of the time series.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 19:32:31 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 21:54:19 GMT"}, {"version": "v3", "created": "Wed, 30 Sep 2020 00:13:35 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Suibkitwanchai", "Keerati", ""], ["Sykulski", "Adam M.", ""], ["Algorta", "Guillermo Perez", ""], ["Waller", "Daniel", ""], ["Walshe", "Catherine", ""]]}, {"id": "2005.01365", "submitter": "Micha{\\l} Narajewski", "authors": "Micha{\\l} Narajewski and Florian Ziel", "title": "Ensemble Forecasting for Intraday Electricity Prices: Simulating\n  Trajectories", "comments": "accepted for publication in Applied Energy", "journal-ref": "Applied Energy 2020, 279", "doi": "10.1016/j.apenergy.2020.115801", "report-no": null, "categories": "q-fin.ST cs.LG cs.SY econ.EM eess.SY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies concerning the point electricity price forecasting have shown\nevidence that the hourly German Intraday Continuous Market is weak-form\nefficient. Therefore, we take a novel, advanced approach to the problem. A\nprobabilistic forecasting of the hourly intraday electricity prices is\nperformed by simulating trajectories in every trading window to receive a\nrealistic ensemble to allow for more efficient intraday trading and redispatch.\nA generalized additive model is fitted to the price differences with the\nassumption that they follow a zero-inflated distribution, precisely a mixture\nof the Dirac and the Student's t-distributions. Moreover, the mixing term is\nestimated using a high-dimensional logistic regression with lasso penalty. We\nmodel the expected value and volatility of the series using i.a. autoregressive\nand no-trade effects or load, wind and solar generation forecasts and\naccounting for the non-linearities in e.g. time to maturity. Both the in-sample\ncharacteristics and forecasting performance are analysed using a rolling window\nforecasting study. Multiple versions of the model are compared to several\nbenchmark models and evaluated using probabilistic forecasting measures and\nsignificance tests. The study aims to forecast the price distribution in the\nGerman Intraday Continuous Market in the last 3 hours of trading, but the\napproach allows for application to other continuous markets, especially in\nEurope. The results prove superiority of the mixture model over the benchmarks\ngaining the most from the modelling of the volatility. They also indicate that\nthe introduction of XBID reduced the market volatility.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 10:21:20 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 11:46:30 GMT"}, {"version": "v3", "created": "Sat, 29 Aug 2020 13:12:31 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Narajewski", "Micha\u0142", ""], ["Ziel", "Florian", ""]]}, {"id": "2005.01379", "submitter": "Gaetano Romano", "authors": "Gaetano Romano, Guillem Rigaill, Vincent Runge, Paul Fearnhead", "title": "Detecting Abrupt Changes in the Presence of Local Fluctuations and\n  Autocorrelated Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whilst there are a plethora of algorithms for detecting changes in mean in\nunivariate time-series, almost all struggle in real applications where there is\nautocorrelated noise or where the mean fluctuates locally between the abrupt\nchanges that one wishes to detect. In these cases, default implementations,\nwhich are often based on assumptions of a constant mean between changes and\nindependent noise, can lead to substantial over-estimation of the number of\nchanges. We propose a principled approach to detect such abrupt changes that\nmodels local fluctuations as a random walk process and autocorrelated noise via\nan AR(1) process. We then estimate the number and location of changepoints by\nminimising a penalised cost based on this model. We develop a novel and\nefficient dynamic programming algorithm, DeCAFS, that can solve this\nminimisation problem; despite the additional challenge of dependence across\nsegments, due to the autocorrelated noise, which makes existing algorithms\ninapplicable. Theory and empirical results show that our approach has greater\npower at detecting abrupt changes than existing approaches. We apply our method\nto measuring gene expression levels in bacteria.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 10:51:19 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Romano", "Gaetano", ""], ["Rigaill", "Guillem", ""], ["Runge", "Vincent", ""], ["Fearnhead", "Paul", ""]]}, {"id": "2005.01423", "submitter": "Dawei Chen", "authors": "Dawei Chen, Jiangtao Wang, Wenjie Ruan, Qiang Ni, and Sumi Helal", "title": "Enabling Cost-Effective Population Health Monitoring By Exploiting\n  Spatiotemporal Correlation: An Empirical Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because of its important role in health policy-shaping, population health\nmonitoring (PHM) is considered a fundamental block for public health services.\nHowever, traditional public health data collection approaches, such as\nclinic-visit-based data integration or health surveys, could be very costly and\ntime-consuming. To address this challenge, this paper proposes a cost-effective\napproach called Compressive Population Health (CPH), where a subset of a given\narea is selected in terms of regions within the area for data collection in the\ntraditional way, while leveraging inherent spatial correlations of neighboring\nregions to perform data inference for the rest of the area. By alternating\nselected regions longitudinally, this approach can validate and correct\npreviously assessed spatial correlations. To verify whether the idea of CPH is\nfeasible, we conduct an in-depth study based on spatiotemporal morbidity rates\nof chronic diseases in more than 500 regions around London for over ten years.\nWe introduce our CPH approach and present three extensive analytical studies.\nThe first confirms that significant spatiotemporal correlations do exist. In\nthe second study, by deploying multiple state-of-the-art data recovery\nalgorithms, we verify that these spatiotemporal correlations can be leveraged\nto do data inference accurately using only a small number of samples. Finally,\nwe compare different methods for region selection for traditional data\ncollection and show how such methods can further reduce the overall cost while\nmaintaining high PHM quality.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 19:30:39 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Chen", "Dawei", ""], ["Wang", "Jiangtao", ""], ["Ruan", "Wenjie", ""], ["Ni", "Qiang", ""], ["Helal", "Sumi", ""]]}, {"id": "2005.01457", "submitter": "Hisashi Noma", "authors": "Hisashi Noma, Tomohiro Shinozaki, Katsuhiro Iba, Satoshi Teramukai and\n  Toshi A. Furukawa", "title": "Confidence intervals of prediction accuracy measures for multivariable\n  prediction models based on the bootstrap-based optimism correction methods", "comments": null, "journal-ref": "Stat Med. 2021", "doi": "10.1002/sim.9148", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In assessing prediction accuracy of multivariable prediction models, optimism\ncorrections are essential for preventing biased results. However, in most\npublished papers of clinical prediction models, the point estimates of the\nprediction accuracy measures are corrected by adequate bootstrap-based\ncorrection methods, but their confidence intervals are not corrected, e.g., the\nDeLong's confidence interval is usually used for assessing the C-statistic.\nThese naive methods do not adjust for the optimism bias and do not account for\nstatistical variability in the estimation of parameters in the prediction\nmodels. Therefore, their coverage probabilities of the true value of the\nprediction accuracy measure can be seriously below the nominal level (e.g.,\n95%). In this article, we provide two generic bootstrap methods, namely (1)\nlocation-shifted bootstrap confidence intervals and (2) two-stage bootstrap\nconfidence intervals, that can be generally applied to the bootstrap-based\noptimism correction methods, i.e., the Harrell's bias correction, 0.632, and\n0.632+ methods. In addition, they can be widely applied to various methods for\nprediction model development involving modern shrinkage methods such as the\nridge and lasso regressions. Through numerical evaluations by simulations, the\nproposed confidence intervals showed favourable coverage performances. Besides,\nthe current standard practices based on the optimism-uncorrected methods showed\nserious undercoverage properties. To avoid erroneous results, the\noptimism-uncorrected confidence intervals should not be used in practice, and\nthe adjusted methods are recommended instead. We also developed the R package\npredboot for implementing these methods (https://github.com/nomahi/predboot).\nThe effectiveness of the proposed methods are illustrated via applications to\nthe GUSTO-I clinical trial.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 13:15:54 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 11:33:59 GMT"}, {"version": "v3", "created": "Fri, 8 May 2020 08:40:44 GMT"}, {"version": "v4", "created": "Wed, 30 Jun 2021 10:18:57 GMT"}, {"version": "v5", "created": "Sun, 25 Jul 2021 12:56:27 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Noma", "Hisashi", ""], ["Shinozaki", "Tomohiro", ""], ["Iba", "Katsuhiro", ""], ["Teramukai", "Satoshi", ""], ["Furukawa", "Toshi A.", ""]]}, {"id": "2005.01481", "submitter": "Luis Machado Dr", "authors": "Paula Lopes, Pedro Campos, Luis Meira-Machado", "title": "Survival Analysis of Organizational Networks -- An Exploratory Study", "comments": "13 pages, 5 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Organizations interact with the environment and with other organizations, and\nthese interactions constitute an important way of learning and evolution. To\novercome the problems that they face during their existence, organizations must\ncertainly adopt survival strategies, both individually and in group. The aim of\nthis study is to evaluate the effect of a set of prognostic factors\n(organizational, size, collaborate strategies, etc.) in the survival of\norganizational networks. Statistical methods for time to event data were used\nto analyze the data. We have used the Kaplan-Meier product-limit method to\ncompute and plot estimates of survival, while hypothesis tests were used to\ncompare survival times across several groups. Regression models were used to\nstudy the effect of continuous predictors as well as to test multiple\npredictors at once. Since violations of the proportional hazards were found for\nseveral predictors, accelerated failure time models were used to study the\neffect of explanatory variables on network survival.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 13:38:24 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Lopes", "Paula", ""], ["Campos", "Pedro", ""], ["Meira-Machado", "Luis", ""]]}, {"id": "2005.01517", "submitter": "Mikko Kuronen", "authors": "Mikko Kuronen, Mari Myllym\\\"aki, Adam Loavenbruck, Aila S\\\"arkk\\\"a", "title": "Point process models for sweat gland activation observed with noise", "comments": "27 pages, 12 figures", "journal-ref": "Statistics in Medicine. 2021", "doi": "10.1002/sim.8891", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of the paper is to construct spatial models for the activation of\nsweat glands for healthy subjects and subjects suffering from peripheral\nneuropathy by using videos of sweating recorded from the subjects. The sweat\npatterns are regarded as realizations of spatial point processes and two point\nprocess models for the sweat gland activation and two methods for inference are\nproposed. Several image analysis steps are needed to extract the point patterns\nfrom the videos and some incorrectly identified sweat gland locations may be\npresent in the data. To take into account the errors we either include an error\nterm in the point process model or use an estimation procedure that is robust\nwith respect to the errors.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 14:31:36 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Kuronen", "Mikko", ""], ["Myllym\u00e4ki", "Mari", ""], ["Loavenbruck", "Adam", ""], ["S\u00e4rkk\u00e4", "Aila", ""]]}, {"id": "2005.01549", "submitter": "Kieran Campbell", "authors": "Allen W Zhang and Kieran R Campbell", "title": "Computational modelling in single-cell cancer genomics: methods and\n  future directions", "comments": "Review article; 10 pages, 1 figure, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-cell technologies have revolutionized biomedical research by enabling\nscalable measurement of the genome, transcriptome, and proteome of multiple\nsystems at single-cell resolution. Now widely applied to cancer models, these\nassays offer new insights into tumour heterogeneity, which underlies cancer\ninitiation, progression, and relapse. However, the large quantities of\nhigh-dimensional, noisy data produced by single-cell assays can complicate data\nanalysis, obscuring biological signals with technical artefacts. In this review\narticle, we outline the major challenges in analyzing single-cell cancer\ngenomics data and survey the current computational tools available to tackle\nthese. We further outline unsolved problems that we consider major\nopportunities for future methods development to help interpret the vast\nquantities of data being generated.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 15:02:57 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Zhang", "Allen W", ""], ["Campbell", "Kieran R", ""]]}, {"id": "2005.01833", "submitter": "Xuefei Lu", "authors": "Emanuele Borgonovo and Xuefei Lu", "title": "Is Time to Intervention in the COVID-19 Outbreak Really Important? A\n  Global Sensitivity Analysis Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Italy has been one of the first countries timewise strongly impacted by the\nCOVID-19 pandemic. The adoption of social distancing and heavy lockdown\nmeasures is posing a heavy burden on the population and the economy. The timing\nof the measures has crucial policy-making implications. Using publicly\navailable data for the pandemic progression in Italy, we quantitatively assess\nthe effect of the intervention time on the pandemic expansion, with a\nmethodology that combines a generalized\nsusceptible-exposed-infectious-recovered (SEIR) model together with statistical\nlearning methods. The modeling shows that the lockdown has strongly deviated\nthe pandemic trajectory in Italy. However, the difference between the forecasts\nand real data up to 20 April 2020 can be explained only by the existence of a\ntime lag between the actual issuance date and the full effect of the measures.\nTo understand the relative importance of intervention with respect to other\nfactors, a thorough uncertainty quantification of the model predictions is\nperformed. Global sensitivity indices show that the the time of intervention is\n4 times more relevant than quarantine, and eight times more important than\nintrinsic features of the pandemic such as protection and infection rates. The\nrelevance of their interactions is also quantified and studied.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 20:40:32 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Borgonovo", "Emanuele", ""], ["Lu", "Xuefei", ""]]}, {"id": "2005.01860", "submitter": "Kristian Agas{\\o}ster Haaga", "authors": "Kristian Agas{\\o}ster Haaga, David Diego, Jo Brendryen, Bjarte\n  Hannisdal", "title": "A simple test for causality in complex systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP nlin.CD stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We provide a new solution to the long-standing problem of inferring causality\nfrom observations without modeling the unknown mechanisms. We show that the\nevolution of any dynamical system is related to a predictive asymmetry that\nquantifies causal connections from limited observations. A built-in\nsignificance criterion obviates surrogate testing and drastically improves\ncomputational efficiency. We validate our test on numerous synthetic systems\nexhibiting behavior commonly occurring in nature, from linear and nonlinear\nstochastic processes to systems exhibiting nonlinear deterministic chaos, and\non real-world data with known ground truths. Applied to the controversial\nproblem of glacial-interglacial sea level and CO$_{2}$ evolving in lock-step,\nour test uncovers empirical evidence for CO$_{2}$ as a driver of sea level over\nthe last 800 thousand years. Our findings are relevant to any discipline where\ntime series are used to study natural systems.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 21:41:16 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Haaga", "Kristian Agas\u00f8ster", ""], ["Diego", "David", ""], ["Brendryen", "Jo", ""], ["Hannisdal", "Bjarte", ""]]}, {"id": "2005.01873", "submitter": "Dylan Small", "authors": "Dylan S. Small, Dan Firth, Luke Keele, Matthew Huber, Molly\n  Passarella, Scott Lorch, Heather Burris", "title": "Protocol for a Study of the Effect of Surface Mining in Central\n  Appalachia on Adverse Birth Outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surface mining has become a major method of coal mining in Central Appalachia\nalongside the traditional underground mining. Concerns have been raised about\nthe health effects of this surface mining, particularly mountaintop removal\nmining where coal is mined upon steep mountaintops by removing the mountaintop\nthrough clearcutting forests and explosives. We have designed a matched\nobservational study to assess the effects of surface mining in Central\nAppalachia on adverse birth outcomes. This protocol describes for the study the\nbackground and motivation, the sample selection and the analysis plan.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 22:27:07 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Small", "Dylan S.", ""], ["Firth", "Dan", ""], ["Keele", "Luke", ""], ["Huber", "Matthew", ""], ["Passarella", "Molly", ""], ["Lorch", "Scott", ""], ["Burris", "Heather", ""]]}, {"id": "2005.01962", "submitter": "Mikko Kuronen", "authors": "Mikko Kuronen, Aila S\\\"arkk\\\"a, Matti Vihola, Mari Myllym\\\"aki", "title": "Hierarchical log Gaussian Cox process for regeneration in uneven-aged\n  forests", "comments": "28 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a hierarchical log Gaussian Cox process (LGCP) for point patterns,\nwhere a set of points x affects another set of points y but not vice versa. We\nuse the model to investigate the effect of large trees to the locations of\nseedlings. In the model, every point in x has a parametric influence kernel or\nsignal, which together form an influence field. Conditionally on the\nparameters, the influence field acts as a spatial covariate in the intensity of\nthe model, and the intensity itself is a non-linear function of the parameters.\nPoints outside the observation window may affect the influence field inside the\nwindow. We propose an edge correction to account for this missing data. The\nparameters of the model are estimated in a Bayesian framework using Markov\nchain Monte Carlo (MCMC) where a Laplace approximation is used for the Gaussian\nfield of the LGCP model. The proposed model is used to analyze the effect of\nlarge trees on the success of regeneration in uneven-aged forest stands in\nFinland.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 06:18:15 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 08:15:49 GMT"}, {"version": "v3", "created": "Wed, 7 Jul 2021 11:41:16 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Kuronen", "Mikko", ""], ["S\u00e4rkk\u00e4", "Aila", ""], ["Vihola", "Matti", ""], ["Myllym\u00e4ki", "Mari", ""]]}, {"id": "2005.02001", "submitter": "Michael Spence", "authors": "Michael A. Spence, Khatija Alliji, Hayley J. Bannister, Nicola D.\n  Walker and Angela Muench", "title": "Fish should not be in isolation: Calculating maximum sustainable yield\n  using an ensemble model", "comments": "39 pages 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many jurisdictions have a legal requirement to manage fish stocks to maximum\nsustainable yield (MSY). Generally, MSY is calculated on a single-species\nbasis, however in reality, the yield of one species depends, not only on its\nown fishing level, but that of other species. We show that bold assumptions\nabout the effect of interacting species on MSY are made when managing on a\nsingle-species basis, often leading to inconsistent and conflicting advice,\ndemonstrating the requirement of a multispecies MSY (MMSY). Although there are\nseveral definitions of MMSY, there is no consensus. Furthermore, calculating a\nMMSY can be difficult as there are many models, of varying complexity, each\nwith their own strengths and weaknesses, and the value if MMSY can be sensitive\nto the model used. Here, we use an ensemble model to combine different\nmultispecies models, exploiting their individual strengths and quantifying\ntheir uncertainties and discrepancies, to calculate a more robust MMSY. We\ndemonstrate this by calculating a MMSY for nine species in the North Sea. We\nfound that it would be impossible to fish at single-species MSY and that MMSY\nled to higher yields and revenues than current levels.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 08:29:51 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Spence", "Michael A.", ""], ["Alliji", "Khatija", ""], ["Bannister", "Hayley J.", ""], ["Walker", "Nicola D.", ""], ["Muench", "Angela", ""]]}, {"id": "2005.02090", "submitter": "Simon Wood", "authors": "Simon N. Wood", "title": "Inferring UK COVID-19 fatal infection trajectories from daily mortality\n  data: were infections already in decline before the UK lockdowns?", "comments": "Gives the location of the replication code and corrects an accidental\n  deletion in the first line of the conclusions", "journal-ref": "Biometrics 2021", "doi": "10.1111/biom.13462", "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of new infections per day is a key quantity for effective epidemic\nmanagement. It can be estimated relatively directly by testing of random\npopulation samples. Without such direct epidemiological measurement, other\napproaches are required to infer whether the number of new cases is likely to\nbe increasing or decreasing: for example, estimating the pathogen effective\nreproduction number, R, using data gathered from the clinical response to the\ndisease. For Covid-19 (SARS-CoV-2) such R estimation is heavily dependent on\nmodelling assumptions, because the available clinical case data are\nopportunistic observational data subject to severe temporal confounding. Given\nthis difficulty it is useful to retrospectively reconstruct the time course of\ninfections from the least compromised available data, using minimal prior\nassumptions. A Bayesian inverse problem approach applied to UK data on first\nwave Covid-19 deaths and the disease duration distribution suggests that fatal\ninfections were in decline before full UK lockdown (24 March 2020), and that\nfatal infections in Sweden started to decline only a day or two later. An\nanalysis of UK data using the model of Flaxman et al. (2020, Nature 584) gives\nthe same result under relaxation of its prior assumptions on R, suggesting an\nenhanced role for non pharmaceutical interventions (NPIs) short of full lock\ndown in the UK context. Similar patterns appear to have occurred in the\nsubsequent two lockdowns. Estimates from the main UK Covid statistical\nsurveillance surveys, available since original publication, support these\nresults. Replication code for the paper is available in the supporting\ninformation of doi/10.1111/biom.13462.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 12:06:43 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 12:25:23 GMT"}, {"version": "v3", "created": "Sat, 4 Jul 2020 15:10:05 GMT"}, {"version": "v4", "created": "Sat, 8 Aug 2020 14:17:48 GMT"}, {"version": "v5", "created": "Thu, 17 Sep 2020 20:19:54 GMT"}, {"version": "v6", "created": "Thu, 18 Mar 2021 07:56:06 GMT"}, {"version": "v7", "created": "Tue, 25 May 2021 10:23:18 GMT"}, {"version": "v8", "created": "Thu, 17 Jun 2021 08:36:16 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Wood", "Simon N.", ""]]}, {"id": "2005.02280", "submitter": "L\u00c3\u00a1szl\u00c3\u00b3 Csat\u00c3\u00b3", "authors": "L\\'aszl\\'o Csat\\'o", "title": "Coronavirus and sports leagues: obtaining a fair ranking when the season\n  cannot resume", "comments": "16 pages, 3 tables", "journal-ref": "IMA Journal of Management Mathematics, forthcoming, 2021", "doi": "10.1093/imaman/dpab020", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many sports leagues are played in a tightly scheduled round-robin format,\nleaving a limited time window to postpone matches. If the season cannot resume\ndue to an external shock such as the coronavirus pandemic in 2020, the ranking\nof the teams becomes non-trivial: it is necessary to account for schedule\nimbalances and possibly for the different number of matches played. First in\nthe literature, we identify a set of desired axioms for ranking in these\nincomplete tournaments. It is verified that the generalised row sum, a\nparametric family of scoring rules, satisfies all of them. In particular, the\nwell-established least squares method maximizes the influence of the strength\nof opponents on the ranking. Our approach is applied for six major premier\nEuropean soccer competitions, where the rankings are found to be robust\nconcerning the weight of adjustment for the strength of the opponents. Some\nmethodologically simpler alternative policies are also discussed.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 15:25:52 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 14:26:23 GMT"}, {"version": "v3", "created": "Tue, 25 Aug 2020 14:15:09 GMT"}, {"version": "v4", "created": "Sun, 23 May 2021 09:55:34 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Csat\u00f3", "L\u00e1szl\u00f3", ""]]}, {"id": "2005.02282", "submitter": "Gabriel Calvo", "authors": "Gabriel Calvo, Carmen Armero, Maria Grazia Pennino and Luigi Spezia", "title": "Bayesian longitudinal models for exploring European sardine fishing in\n  the Mediterranean Sea", "comments": "16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Mediterranean Sea, catches are dominated by small pelagic fish,\nrepresenting nearly the 49\\% of the total harvest. Among them, the European\nsardine (Sardina pilchardus) is one of the most commercially important species\nshowing high over-exploitation rates in recent last years. In this study we\nanalysed the European sardine landings in the Mediterranean Sea from 1970 to\n2014. We made use of Bayesian longitudinal linear mixed models in order to\nassess differences in the temporal evolution of fishing between and within\ncountries. Furthermore, we modelled the subsequent joint evolution of artisanal\nand industrial fisheries. Overall results confirmed that Mediterranean fishery\ntime series were highly diverse along their dynamics and this heterogeneity was\npersistent throughout the time. In addition, results highlighted a positive\nrelationship between the two types of fishing.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 15:35:33 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Calvo", "Gabriel", ""], ["Armero", "Carmen", ""], ["Pennino", "Maria Grazia", ""], ["Spezia", "Luigi", ""]]}, {"id": "2005.02488", "submitter": "Tim Verdonck", "authors": "Sebastiaan H\\\"oppner and Bart Baesens and Wouter Verbeke and Tim\n  Verdonck", "title": "Instance-Dependent Cost-Sensitive Learning for Detecting Transfer Fraud", "comments": "24 pages, 4 figures, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Card transaction fraud is a growing problem affecting card holders worldwide.\nFinancial institutions increasingly rely upon data-driven methods for\ndeveloping fraud detection systems, which are able to automatically detect and\nblock fraudulent transactions. From a machine learning perspective, the task of\ndetecting fraudulent transactions is a binary classification problem.\nClassification models are commonly trained and evaluated in terms of\nstatistical performance measures, such as likelihood and AUC, respectively.\nThese measures, however, do not take into account the actual business\nobjective, which is to minimize the financial losses due to fraud. Fraud\ndetection is to be acknowledged as an instance-dependent cost-sensitive\nclassification problem, where the costs due to misclassification vary between\ninstances, and requiring adapted approaches for learning a classification\nmodel. In this article, an instance-dependent threshold is derived, based on\nthe instance-dependent cost matrix for transfer fraud detection, that allows\nfor making the optimal cost-based decision for each transaction. Two novel\nclassifiers are presented, based on lasso-regularized logistic regression and\ngradient tree boosting, which directly minimize the proposed instance-dependent\ncost measure when learning a classification model. The proposed methods are\nimplemented in the R packages cslogit and csboost, and compared against\nstate-of-the-art methods on a publicly available data set from the machine\nlearning competition website Kaggle and a proprietary card transaction data\nset. The results of the experiments highlight the potential of reducing fraud\nlosses by adopting the proposed methods.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 20:53:07 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["H\u00f6ppner", "Sebastiaan", ""], ["Baesens", "Bart", ""], ["Verbeke", "Wouter", ""], ["Verdonck", "Tim", ""]]}, {"id": "2005.02509", "submitter": "Antonio Linero", "authors": "Piyali Basak, Antonio R. Linero, Debajyoti SInha, and Stuart Lipsitz", "title": "Semiparametric analysis of clustered interval-censored survival data\n  using Soft Bayesian Additive Regression Trees (SBART)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Popular parametric and semiparametric hazards regression models for clustered\nsurvival data are inappropriate and inadequate when the unknown effects of\ndifferent covariates and clustering are complex. This calls for a flexible\nmodeling framework to yield efficient survival prediction. Moreover, for some\nsurvival studies involving time to occurrence of some asymptomatic events,\nsurvival times are typically interval censored between consecutive clinical\ninspections. In this article, we propose a robust semiparametric model for\nclustered interval-censored survival data under a paradigm of Bayesian ensemble\nlearning, called Soft Bayesian Additive Regression Trees or SBART (Linero and\nYang, 2018), which combines multiple sparse (soft) decision trees to attain\nexcellent predictive accuracy. We develop a novel semiparametric hazards\nregression model by modeling the hazard function as a product of a parametric\nbaseline hazard function and a nonparametric component that uses SBART to\nincorporate clustering, unknown functional forms of the main effects, and\ninteraction effects of various covariates. In addition to being applicable for\nleft-censored, right-censored, and interval-censored survival data, our\nmethodology is implemented using a data augmentation scheme which allows for\nexisting Bayesian backfitting algorithms to be used. We illustrate the\npractical implementation and advantages of our method via simulation studies\nand an analysis of a prostate cancer surgery study where dependence on the\nexperience and skill level of the physicians leads to clustering of survival\ntimes. We conclude by discussing our method's applicability in studies\ninvolving high dimensional data with complex underlying associations.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 21:34:52 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 20:25:58 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Basak", "Piyali", ""], ["Linero", "Antonio R.", ""], ["SInha", "Debajyoti", ""], ["Lipsitz", "Stuart", ""]]}, {"id": "2005.02535", "submitter": "Philippe Goulet Coulombe", "authors": "Philippe Goulet Coulombe and Maximilian G\\\"obel", "title": "Arctic Amplification of Anthropogenic Forcing: A Vector Autoregressive\n  Analysis", "comments": "(last) minor revisions", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On September 15th 2020, Arctic sea ice extent (SIE) ranked second-to-lowest\nin history and keeps trending downward. The understanding of how feedback loops\namplify the effects of external CO2 forcing is still limited. We propose the\nVARCTIC, which is a Vector Autoregression (VAR) designed to capture and\nextrapolate Arctic feedback loops. VARs are dynamic simultaneous systems of\nequations, routinely estimated to predict and understand the interactions of\nmultiple macroeconomic time series. The VARCTIC is a parsimonious compromise\nbetween full-blown climate models and purely statistical approaches that\nusually offer little explanation of the underlying mechanism. Our completely\nunconditional forecast has SIE hitting 0 in September by the 2060's. Impulse\nresponse functions reveal that anthropogenic CO2 emission shocks have an\nunusually durable effect on SIE -- a property shared by no other shock. We find\nAlbedo- and Thickness-based feedbacks to be the main amplification channels\nthrough which CO2 anomalies impact SIE in the short/medium run. Further,\nconditional forecast analyses reveal that the future path of SIE crucially\ndepends on the evolution of CO2 emissions, with outcomes ranging from\nrecovering SIE to it reaching 0 in the 2050's. Finally, Albedo and Thickness\nfeedbacks are shown to play an important role in accelerating the speed at\nwhich predicted SIE is heading towards 0.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 23:34:24 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 02:20:02 GMT"}, {"version": "v3", "created": "Mon, 18 Jan 2021 05:05:05 GMT"}, {"version": "v4", "created": "Tue, 9 Mar 2021 16:52:09 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Coulombe", "Philippe Goulet", ""], ["G\u00f6bel", "Maximilian", ""]]}, {"id": "2005.02814", "submitter": "Abhinandan Dalal", "authors": "Abhinandan Dalal, Diganta Mukherjee and Subhrajyoty Roy", "title": "The Information Content of Taster's Valuation in Tea Auctions of India", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tea auctions across India occur as an ascending open auction, conducted\nonline. Before the auction, a sample of the tea lot is sent to potential\nbidders and a group of tea tasters. The seller's reserve price is a\nconfidential function of the tea taster's valuation, which also possibly acts\nas a signal to the bidders.\n  In this paper, we work with the dataset from a single tea auction house, J\nThomas, of tea dust category, on 49 weeks in the time span of 2018-2019, with\nthe following objectives in mind:\n  $\\bullet$ Objective classification of the various categories of tea dust (25)\ninto a more manageable, and robust classification of the tea dust, based on\nsource and grades.\n  $\\bullet$ Predict which tea lots would be sold in the auction market, and a\nmodel for the final price conditioned on sale.\n  $\\bullet$ To study the distribution of price and ratio of the sold tea\nauction lots.\n  $\\bullet$ Make a detailed analysis of the information obtained from the tea\ntaster's valuation and its impact on the final auction price.\n  The model used has shown various promising results on cross-validation. The\nimportance of valuation is firmly established through analysis of causal\nrelationship between the valuation and the actual price. The authors hope that\nthis study of the properties and the detailed analysis of the role played by\nthe various factors, would be significant in the decision making process for\nthe players of the auction game, pave the way to remove the manual interference\nin an attempt to automate the auction procedure, and improve tea quality in\nmarkets.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 16:39:47 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Dalal", "Abhinandan", ""], ["Mukherjee", "Diganta", ""], ["Roy", "Subhrajyoty", ""]]}, {"id": "2005.02817", "submitter": "Saswata Sahoo Dr", "authors": "Saswata Sahoo and Souradip Chakraborty", "title": "Graph Spectral Feature Learning for Mixed Data of Categorical and\n  Numerical Type", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature learning in the presence of a mixed type of variables, numerical and\ncategorical types, is an important issue for related modeling problems. For\nsimple neighborhood queries under mixed data space, standard practice is to\nconsider numerical and categorical variables separately and combining them\nbased on some suitable distance functions. Alternatives, such as Kernel\nlearning or Principal Component do not explicitly consider the inter-dependence\nstructure among the mixed type of variables. In this work, we propose a novel\nstrategy to explicitly model the probabilistic dependence structure among the\nmixed type of variables by an undirected graph. Spectral decomposition of the\ngraph Laplacian provides the desired feature transformation. The Eigen spectrum\nof the transformed feature space shows increased separability and more\nprominent clusterability among the observations. The main novelty of our paper\nlies in capturing interactions of the mixed feature type in an unsupervised\nframework using a graphical model. We numerically validate the implications of\nthe feature learning strategy\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 13:36:59 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Sahoo", "Saswata", ""], ["Chakraborty", "Souradip", ""]]}, {"id": "2005.02931", "submitter": "Pierre G. B. Moutounet-Cartan", "authors": "Pierre G. B. Moutounet-Cartan", "title": "A Bernoulli Mixture Model to Understand and Predict Children\n  Longitudinal Wheezing Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this research, we estimate that around $27.99(\\pm2.15)\\%$ of the\npopulation has experienced wheezing before turning 1 in the United Kingdom.\nFurthermore, the Bernoulli Mixture Model classification is found to work best\nwith $K=4$ clusters in order to better balance the separability of the clusters\nwith their explanatory nature, based on a cohort of $N=1184$. The probability\nof the group of parents in the $j$th cluster to say that their children have\nwheezed during the $i$th age is assumed $P_{ij} \\sim \\text{Beta}(1/2, 1/2)$,\nthe probabilities of assignment to each cluster is $R \\sim\n\\text{Dirichlet}_K(\\alpha)$, the assignment of the $n$th patient to each\ncluster is $Z_n\\ |\\ R \\sim \\text{Categorical}(R)$, and the $n$th patient\nwheezed during the $i$th age is $X_{in}\\ |\\ P_{ij}, Z_n \\sim\n\\text{Bernoulli}(P_{i,Z_n})$; where $i\\in\\{1,\\dots,6\\}$, $j\\in\\{1,\\dots,K\\}$,\nand $n\\in\\{1,\\dots, N\\}$. The classification is then performed through the E-M\noptimization algorithm. We found that this clustering method groups efficiently\nthe patients with late-childhood wheezing, persistent wheezing, early-childhood\nwheezing, and none or sporadic wheezing. Furthermore, we found that this method\nis not dependent on the data-set, and can include data-sets with missing\nentries.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 16:09:45 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Moutounet-Cartan", "Pierre G. B.", ""]]}, {"id": "2005.02952", "submitter": "Mickael Albertus", "authors": "Mickael Albertus", "title": "Exponential increase of the power of the independence and homogeneity\n  chi-square tests with auxiliary information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is an extension of the work about the exponential increase of the\npower of two non-parametric tests: the $ Z $-test and the chi-square\ngoodness-of-fit test. Subject to having auxiliary information, it is possible\nto improve exponentially relative to the size of the sample the power of the\nfamous chi-square tests of independence and homogeneity. Improving the power of\nthese statistical tests by using auxiliary information makes it possible either\nto reduce the probability of accepting the null hypothesis under the\nalternative hypothesis, or to reduce the size of the sample necessary to reach\na predefined power. The suggested method is computational and some simple\nstatistical applications are presented to illustrate these results. The\nframework of this work is non-parametric, so it can be applied to any kind of\ndata and any area using statistics.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 16:47:46 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Albertus", "Mickael", ""]]}, {"id": "2005.03243", "submitter": "Md Nazmul Islam", "authors": "Md Nazmul Islam", "title": "Classification of pediatric pneumonia using chest X-rays by functional\n  regression", "comments": "26 pages, 9 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP eess.IV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  An accurate and prompt diagnosis of pediatric pneumonia is imperative for\nsuccessful treatment intervention. One approach to diagnose pneumonia cases is\nusing radiographic data. In this article, we propose a novel parsimonious\nscalar-on-image classification model adopting the ideas of functional data\nanalysis. Our main idea is to treat images as functional measurements and\nexploit underlying covariance structures to select basis functions; these bases\nare then used in approximating both image profiles and corresponding regression\ncoefficient. We re-express the regression model into a standard generalized\nlinear model where the functional principal component scores are treated as\ncovariates. We apply the method to (1) classify pneumonia against healthy and\nviral against bacterial pneumonia patients, and (2) test the null effect about\nthe association between images and responses. Extensive simulation studies show\nexcellent numerical performance in terms of classification, hypothesis testing,\nand efficient computation.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 04:24:26 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Islam", "Md Nazmul", ""]]}, {"id": "2005.03465", "submitter": "Tai-Yu Ma", "authors": "Tai-Yu Ma, Joseph Y.J. Chow, Sylvain Klein, Ziyi Ma", "title": "A stochastic user-operator assignment game for microtransit service\n  evaluation: A case study of Kussbus in Luxembourg", "comments": "arXiv admin note: substantial text overlap with arXiv:1912.01984", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.CE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a stochastic variant of the stable matching model from\nRasulkhani and Chow [1] which allows microtransit operators to evaluate their\noperation policy and resource allocations. The proposed model takes into\naccount the stochastic nature of users' travel utility perception, resulting in\na probabilistic stable operation cost allocation outcome to design ticket price\nand ridership forecasting. We applied the model for the operation policy\nevaluation of a microtransit service in Luxembourg and its border area. The\nmethodology for the model parameters estimation and calibration is developed.\nThe results provide useful insights for the operator and the government to\nimprove the ridership of the service.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 06:57:35 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Ma", "Tai-Yu", ""], ["Chow", "Joseph Y. J.", ""], ["Klein", "Sylvain", ""], ["Ma", "Ziyi", ""]]}, {"id": "2005.03500", "submitter": "Benjamin Avanzi", "authors": "Benjamin Avanzi and Gregory Clive Taylor and Phuong Anh Vu and Bernard\n  Wong", "title": "On unbalanced data and common shock models in stochastic loss reserving", "comments": null, "journal-ref": "Ann. actuar. sci. 15 (2021) 173-203", "doi": "10.1017/S1748499520000196", "report-no": null, "categories": "q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Introducing common shocks is a popular dependence modelling approach, with\nsome recent applications in loss reserving. The main advantage of this approach\nis the ability to capture structural dependence coming from known\nrelationships. In addition, it helps with the parsimonious construction of\ncorrelation matrices of large dimensions. However, complications arise in the\npresence of \"unbalanced data\", that is, when (expected) magnitude of\nobservations over a single triangle, or between triangles, can vary\nsubstantially. Specifically, if a single common shock is applied to all of\nthese cells, it can contribute insignificantly to the larger values and/or\nswamp the smaller ones, unless careful adjustments are made. This problem is\nfurther complicated in applications involving negative claim amounts. In this\npaper, we address this problem in the loss reserving context using a common\nshock Tweedie approach for unbalanced data. We show that the solution not only\nprovides a much better balance of the common shock proportions relative to the\nunbalanced data, but it is also parsimonious. Finally, the common shock Tweedie\nmodel also provides distributional tractability.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 14:03:34 GMT"}, {"version": "v2", "created": "Sun, 17 May 2020 09:01:40 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Avanzi", "Benjamin", ""], ["Taylor", "Gregory Clive", ""], ["Vu", "Phuong Anh", ""], ["Wong", "Bernard", ""]]}, {"id": "2005.03540", "submitter": "Micha\\\"el Zamo", "authors": "Micha\\\"el Zamo, Liliane Bel, Olivier Mestre", "title": "Sequential Aggregation of Probabilistic Forecasts -- Applicaton to Wind\n  Speed Ensemble Forecasts", "comments": "38 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In the field of numerical weather prediction (NWP), the probabilistic\ndistribution of the future state of the atmosphere is sampled with\nMonte-Carlo-like simulations, called ensembles. These ensembles have\ndeficiencies (such as conditional biases) that can be corrected thanks to\nstatistical post-processing methods. Several ensembles exist and may be\ncorrected with different statistiscal methods. A further step is to combine\nthese raw or post-processed ensembles. The theory of prediction with expert\nadvice allows us to build combination algorithms with theoretical guarantees on\nthe forecast performance. This article adapts this theory to the case of\nprobabilistic forecasts issued as step-wise cumulative distribution functions\n(CDF). The theory is applied to wind speed forecasting, by combining several\nraw or post-processed ensembles, considered as CDFs. The second goal of this\nstudy is to explore the use of two forecast performance criteria: the Continous\nranked probability score (CRPS) and the Jolliffe-Primo test. Comparing the\nresults obtained with both criteria leads to reconsidering the usual way to\nbuild skillful probabilistic forecasts, based on the minimization of the CRPS.\nMinimizing the CRPS does not necessarily produce reliable forecasts according\nto the Jolliffe-Primo test. The Jolliffe-Primo test generally selects reliable\nforecasts, but could lead to issuing suboptimal forecasts in terms of CRPS. It\nis proposed to use both criterion to achieve reliable and skillful\nprobabilistic forecasts.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 15:07:43 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Zamo", "Micha\u00ebl", ""], ["Bel", "Liliane", ""], ["Mestre", "Olivier", ""]]}, {"id": "2005.03604", "submitter": "Rohan Arambepola", "authors": "Rohan Arambepola, Tim C D Lucas, Anita K Nandi, Peter W Gething and\n  Ewan Cameron", "title": "A simulation study of disaggregation regression for spatial disease\n  mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disaggregation regression has become an important tool in spatial disease\nmapping for making fine-scale predictions of disease risk from aggregated\nresponse data. By including high resolution covariate information and modelling\nthe data generating process on a fine scale, it is hoped that these models can\naccurately learn the relationships between covariates and response at a fine\nspatial scale. However, validating these high resolution predictions can be a\nchallenge, as often there is no data observed at this spatial scale. In this\nstudy, disaggregation regression was performed on simulated data in various\nsettings and the resulting fine-scale predictions are compared to the simulated\nground truth. Performance was investigated with varying numbers of data points,\nsizes of aggregated areas and levels of model misspecification. The\neffectiveness of cross validation on the aggregate level as a measure of\nfine-scale predictive performance was also investigated. Predictive performance\nimproved as the number of observations increased and as the size of the\naggregated areas decreased. When the model was well-specified, fine-scale\npredictions were accurate even with small numbers of observations and large\naggregated areas. Under model misspecification predictive performance was\nsignificantly worse for large aggregated areas but remained high when response\ndata was aggregated over smaller regions. Cross-validation correlation on the\naggregate level was a moderately good predictor of fine-scale predictive\nperformance. While the simulations are unlikely to capture the nuances of\nreal-life response data, this study gives insight into the effectiveness of\ndisaggregation regression in different contexts.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 16:56:36 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Arambepola", "Rohan", ""], ["Lucas", "Tim C D", ""], ["Nandi", "Anita K", ""], ["Gething", "Peter W", ""], ["Cameron", "Ewan", ""]]}, {"id": "2005.03625", "submitter": "John R.J. Thompson", "authors": "John R.J. Thompson, Longlong Feng, R. Mark Reesor, Chuck Grace", "title": "Know Your Clients' behaviours: a cluster analysis of financial\n  transactions", "comments": "38 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Canada, financial advisors and dealers are required by provincial\nsecurities commissions and self-regulatory organizations--charged with direct\nregulation over investment dealers and mutual fund dealers--to respectively\ncollect and maintain Know Your Client (KYC) information, such as their age or\nrisk tolerance, for investor accounts. With this information, investors, under\ntheir advisor's guidance, make decisions on their investments which are\npresumed to be beneficial to their investment goals. Our unique dataset is\nprovided by a financial investment dealer with over 50,000 accounts for over\n23,000 clients. We use a modified behavioural finance recency, frequency,\nmonetary model for engineering features that quantify investor behaviours, and\nmachine learning clustering algorithms to find groups of investors that behave\nsimilarly. We show that the KYC information collected does not explain client\nbehaviours, whereas trade and transaction frequency and volume are most\ninformative. We believe the results shown herein encourage financial regulators\nand advisors to use more advanced metrics to better understand and predict\ninvestor behaviours.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 17:22:40 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 14:29:13 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Thompson", "John R. J.", ""], ["Feng", "Longlong", ""], ["Reesor", "R. Mark", ""], ["Grace", "Chuck", ""]]}, {"id": "2005.03651", "submitter": "Alessio Notari", "authors": "Alessio Notari, Giorgio Torrieri", "title": "COVID-19 transmission risk factors", "comments": "42 pages, 31 figures, 32 tables. Principal Component Analysis added\n  in v2", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM physics.soc-ph q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze risk factors correlated with the initial transmission growth rate\nof the recent COVID-19 pandemic in different countries. The number of cases\nfollows in its early stages an almost exponential expansion; we chose as a\nstarting point in each country the first day $d_i$ with 30 cases and we fitted\nfor 12 days, capturing thus the early exponential growth. We looked then for\nlinear correlations of the exponents $\\alpha$ with other variables, for a\nsample of 126 countries. We find a positive correlation, {\\it i.e. faster\nspread of COVID-19}, with high confidence level with the following variables,\nwith respective $p$-value: low Temperature ($4\\cdot10^{-7}$), high ratio of old\nvs.~working-age people ($3\\cdot10^{-6}$), life expectancy ($8\\cdot10^{-6}$),\nnumber of international tourists ($1\\cdot10^{-5}$), earlier epidemic starting\ndate $d_i$ ($2\\cdot10^{-5}$), high level of physical contact in greeting habits\n($6 \\cdot 10^{-5}$), lung cancer prevalence ($6 \\cdot 10^{-5}$), obesity in\nmales ($1 \\cdot 10^{-4}$), share of population in urban areas\n($2\\cdot10^{-4}$), cancer prevalence ($3 \\cdot 10^{-4}$), alcohol consumption\n($0.0019$), daily smoking prevalence ($0.0036$), UV index ($0.004$, 73\ncountries). We also find a correlation with low Vitamin D levels\n($0.002-0.006$, smaller sample, $\\sim 50$ countries, to be confirmed on a\nlarger sample). There is highly significant correlation also with blood types:\npositive correlation with types RH- ($3\\cdot10^{-5}$) and A+ ($3\\cdot10^{-3}$),\nnegative correlation with B+ ($2\\cdot10^{-4}$). Several of the above variables\nare intercorrelated and likely to have common interpretations. We performed a\nPrincipal Component Analysis, in order to find their significant independent\nlinear combinations. We also analyzed a possible bias: countries with low\nGDP-per capita might have less testing and we discuss correlation with the\nabove variables.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 17:57:58 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 22:16:08 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Notari", "Alessio", ""], ["Torrieri", "Giorgio", ""]]}, {"id": "2005.03658", "submitter": "Mark Risser", "authors": "Mark Risser", "title": "Nonstationary Bayesian modeling for a large data set of derived surface\n  temperature return values", "comments": "arXiv admin note: substantial text overlap with arXiv:1910.14101", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heat waves resulting from prolonged extreme temperatures pose a significant\nrisk to human health globally. Given the limitations of observations of extreme\ntemperature, climate models are often used to characterize extreme temperature\nglobally, from which one can derive quantities like return values to summarize\nthe magnitude of a low probability event for an arbitrary geographic location.\nHowever, while these derived quantities are useful on their own, it is also\noften important to apply a spatial statistical model to such data in order to,\ne.g., understand how the spatial dependence properties of the return values\nvary over space and emulate the climate model for generating additional spatial\nfields with corresponding statistical properties. For these objectives, when\nmodeling global data it is critical to use a nonstationary covariance function.\nFurthermore, given that the output of modern global climate models can be on\nthe order of $\\mathcal{O}(10^4)$, it is important to utilize approximate\nGaussian process methods to enable inference. In this paper, we demonstrate the\napplication of methodology introduced in Risser and Turek (2020) to conduct a\nnonstationary and fully Bayesian analysis of a large data set of 20-year return\nvalues derived from an ensemble of global climate model runs with over 50,000\nspatial locations. This analysis uses the freely available BayesNSGP software\npackage for R.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 21:34:33 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Risser", "Mark", ""]]}, {"id": "2005.03698", "submitter": "Dirk Tasche", "authors": "Dirk Tasche", "title": "Proving prediction prudence", "comments": "23 pages, some typos corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how to perform tests on samples of pairs of observations and\npredictions in order to assess whether or not the predictions are prudent.\nPrudence requires that that the mean of the difference of the\nobservation-prediction pairs can be shown to be significantly negative. For\nsafe conclusions, we suggest testing both unweighted (or equally weighted) and\nweighted means and explicitly taking into account the randomness of individual\npairs. The test methods presented are mainly specified as bootstrap and normal\napproximation algorithms. The tests are general but can be applied in\nparticular in the area of credit risk, both for regulatory and accounting\npurposes.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 18:47:12 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 17:19:01 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Tasche", "Dirk", ""]]}, {"id": "2005.03699", "submitter": "Adam Samara", "authors": "Adam Samara, Felix Rempe and Simone G\\\"ottlich", "title": "Modelling arterial travel time distribution using copulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of travel time distribution (TTD) is critical for reliable\nroute guidance and provides theoretical bases and technical support for\nadvanced traffic management and control. The state-of-the art procedure for\nestimating arterial TTD commonly assumes that the path travel time follows a\ncertain distribution without considering segment correlation. However, this\napproach is usually unrealistic as travel times on successive segments may be\ndependent. In this study, copula functions are used to model arterial TTD as\ncopulas are able to incorporate for segment correlation. First, segment\ncorrelation is empirically investigated using day-to-day GPS data provided by\nBMW Group for one major urban arterial in Munich, Germany. Segment TTDs are\nestimated using a finite Gaussian Mixture Model (GMM). Next, several copula\nmodels are introduced, namely Gaussian, Student-t, Clayton, and Gumbel, to\nmodel the dependent structure between segment TTDs. The parameters of each\ncopula model are obtained by Maximum Log Likelihood Estimation. Then, path TTDs\ncomprised of consecutive segment TTDs are estimated based on the copula models.\nThe scalability of the model is evaluated by investigating the performance for\nan increasing number of aggregated links. The best fitting copula is determined\nin terms of goodness-of-fit test. The results demonstrate the advantage of the\nproposed copula model for an increasing number of aggregated segments, compared\nto the convolution without incorporating segment correlations.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 18:48:16 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Samara", "Adam", ""], ["Rempe", "Felix", ""], ["G\u00f6ttlich", "Simone", ""]]}, {"id": "2005.03855", "submitter": "Kota Ogasawara", "authors": "Kota Ogasawara", "title": "Prenatal Selection: Scarring Effects of Pandemic Influenza", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study uses the 1918-1920 influenza pandemic in Japan with newly\ndigitized and complete census records on births, infant deaths, and sex ratios\nduring childhood to analyze mortality selection in utero and its persistency in\nthe gender imbalance. I find that fetal exposure to pandemic influenza during\nthe first trimester of the pregnancy period decreases the proportion of males\nat birth during this period. The mechanism suggests that the decline in male\nbirths owing to pandemic influenza has not led to the positive selection into\nbirths, but it has been associated with the deterioration of fetal and infant\nhealth. This result supports a wide range of previous literature on the\nlong-run adverse effects of pandemic influenza, and it is consistent with a\nrecent argument that the postnatal selection mechanism owing to the\nsocioeconomic status of parents is negligible (Almond 2006; Beach et al. 2018).\nAnalyses using population censuses provide evidence suggesting that postnatal\ninfluenza exposure has long-term impacts on the sex ratio of children aged\n5-12, thereby implying potential disturbance effects on the future marriage and\nlabor markets.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 04:56:01 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 13:46:11 GMT"}, {"version": "v3", "created": "Wed, 31 Mar 2021 02:02:21 GMT"}, {"version": "v4", "created": "Tue, 25 May 2021 07:17:47 GMT"}, {"version": "v5", "created": "Wed, 21 Jul 2021 13:01:02 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Ogasawara", "Kota", ""]]}, {"id": "2005.04034", "submitter": "Michel Besserve", "authors": "Shervin Safavi, Nikos K. Logothetis and Michel Besserve", "title": "From univariate to multivariate coupling between continuous signals and\n  point processes: a mathematical framework", "comments": "50 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series datasets often contain heterogeneous signals, composed of both\ncontinuously changing quantities and discretely occurring events. The coupling\nbetween these measurements may provide insights into key underlying mechanisms\nof the systems under study. To better extract this information, we investigate\nthe asymptotic statistical properties of coupling measures between continuous\nsignals and point processes. We first introduce martingale stochastic\nintegration theory as a mathematical model for a family of statistical\nquantities that include the Phase Locking Value, a classical coupling measure\nto characterize complex dynamics. Based on the martingale Central Limit\nTheorem, we can then derive the asymptotic Gaussian distribution of estimates\nof such coupling measure, that can be exploited for statistical testing.\nSecond, based on multivariate extensions of this result and Random Matrix\nTheory, we establish a principled way to analyze the low rank coupling between\na large number of point processes and continuous signals. For a null hypothesis\nof no coupling, we establish sufficient conditions for the empirical\ndistribution of squared singular values of the matrix to converge, as the\nnumber of measured signals increases, to the well-known Marchenko-Pastur (MP)\nlaw, and the largest squared singular value converges to the upper end of the\nMPs support. This justifies a simple thresholding approach to assess the\nsignificance of multivariate coupling. Finally, we illustrate with simulations\nthe relevance of our univariate and multivariate results in the context of\nneural time series, addressing how to reliably quantify the interplay between\nmulti channel Local Field Potential signals and the spiking activity of a large\npopulation of neurons.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 13:31:29 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Safavi", "Shervin", ""], ["Logothetis", "Nikos K.", ""], ["Besserve", "Michel", ""]]}, {"id": "2005.04139", "submitter": "Nanwei Wang", "authors": "Nanwei Wang, Laurent Briollais, Helene Massam", "title": "The scalable Birth-Death MCMC Algorithm for Mixed Graphical Model\n  Learning with Application to Genomic Data Integration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in biological research have seen the emergence of\nhigh-throughput technologies with numerous applications that allow the study of\nbiological mechanisms at an unprecedented depth and scale. A large amount of\ngenomic data is now distributed through consortia like The Cancer Genome Atlas\n(TCGA), where specific types of biological information on specific type of\ntissue or cell are available. In cancer research, the challenge is now to\nperform integrative analyses of high-dimensional multi-omic data with the goal\nto better understand genomic processes that correlate with cancer outcomes,\ne.g. elucidate gene networks that discriminate a specific cancer subgroups\n(cancer sub-typing) or discovering gene networks that overlap across different\ncancer types (pan-cancer studies). In this paper, we propose a novel mixed\ngraphical model approach to analyze multi-omic data of different types\n(continuous, discrete and count) and perform model selection by extending the\nBirth-Death MCMC (BDMCMC) algorithm initially proposed by\n\\citet{stephens2000bayesian} and later developed by\n\\cite{mohammadi2015bayesian}. We compare the performance of our method to the\nLASSO method and the standard BDMCMC method using simulations and find that our\nmethod is superior in terms of both computational efficiency and the accuracy\nof the model selection results. Finally, an application to the TCGA breast\ncancer data shows that integrating genomic information at different levels\n(mutation and expression data) leads to better subtyping of breast cancers.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 16:34:58 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Wang", "Nanwei", ""], ["Briollais", "Laurent", ""], ["Massam", "Helene", ""]]}, {"id": "2005.04176", "submitter": "Caroline Wang", "authors": "Caroline Wang, Bin Han, Bhrij Patel, Feroze Mohideen, Cynthia Rudin", "title": "In Pursuit of Interpretable, Fair and Accurate Machine Learning for\n  Criminal Recidivism Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, academics and investigative journalists have criticized\ncertain commercial risk assessments for their black-box nature and failure to\nsatisfy competing notions of fairness. Since then, the field of interpretable\nmachine learning has created simple yet effective algorithms, while the field\nof fair machine learning has proposed various mathematical definitions of\nfairness. However, studies from these fields are largely independent, despite\nthe fact that many applications of machine learning to social issues require\nboth fairness and interpretability. We explore the intersection by revisiting\nthe recidivism prediction problem using state-of-the-art tools from\ninterpretable machine learning, and assessing the models for performance,\ninterpretability, and fairness. Unlike previous works, we compare against two\nexisting risk assessments (COMPAS and the Arnold Public Safety Assessment) and\ntrain models that output probabilities rather than binary predictions. We\npresent multiple models that beat these risk assessments in performance, and\nprovide a fairness analysis of these models. Our results imply that machine\nlearning models should be trained separately for separate locations, and\nupdated over time.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 17:16:31 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Wang", "Caroline", ""], ["Han", "Bin", ""], ["Patel", "Bhrij", ""], ["Mohideen", "Feroze", ""], ["Rudin", "Cynthia", ""]]}, {"id": "2005.04261", "submitter": "Burak K\\\"ursad G\\\"unhan", "authors": "Burak K\\\"ursad G\\\"unhan, Paul Meyvisch, Tim Friede", "title": "Shrinkage estimation for dose-response modeling in phase II trials with\n  multiple schedules", "comments": null, "journal-ref": "Statistics in Biopharmaceutical Research, 2020", "doi": "10.1080/19466315.2020.1850519", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, phase II trials with multiple schedules (frequency of\nadministrations) have become more popular, for instance in the development of\ntreatments for atopic dermatitis. If the relationship of the dose and response\nis described by a parametric model, a simplistic approach is to pool doses from\ndifferent schedules. However, this approach ignores the potential heterogeneity\nin dose-response curves between schedules. A more reasonable approach is the\npartial pooling, i.e. certain parameters of the dose-response curves are\nshared, while others are allowed to vary. Rather than using schedule-specific\nfixed-effects, we propose a Bayesian hierarchical model with random-effects to\nmodel the between-schedule heterogeneity with regard to certain parameters.\nSchedule-specific dose-response relationships can then be estimated using\nshrinkage estimation. Considering Emax models, the proposed method displayed\ndesirable performance in terms of the mean absolute error and the coverage\nprobabilities for the dose-response curve compared to the complete pooling.\nFurthermore, it outperformed the partial pooling with schedule-specific\nfixed-effects by producing lower mean absolute error and shorter credible\nintervals. The methods are illustrated using simulations and a phase II trial\nexample in atopic dermatitis. A publicly available R package, \\texttt{ModStan},\nis developed to automate the implementation of the proposed method\n(\\href{https://github.com/gunhanb/ModStan}{https://github.com/gunhanb/ModStan}).\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 19:09:19 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["G\u00fcnhan", "Burak K\u00fcrsad", ""], ["Meyvisch", "Paul", ""], ["Friede", "Tim", ""]]}, {"id": "2005.04500", "submitter": "Joann Jasiak", "authors": "Christian Gourieroux and Joann Jasiak", "title": "Time Varying Markov Process with Partially Observed Aggregate Data; An\n  Application to Coronavirus", "comments": "31 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major difficulty in the analysis of propagation of the coronavirus is that\nmany infected individuals show no symptoms of Covid-19. This implies a lack of\ninformation on the total counts of infected individuals and of recovered and\nimmunized individuals. In this paper, we consider parametric time varying\nMarkov processes of Coronavirus propagation and show how to estimate the model\nparameters and approximate the unobserved counts from daily numbers of infected\nand detected individuals and total daily death counts. This model-based\napproach is illustrated in an application to French data.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 19:34:21 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Gourieroux", "Christian", ""], ["Jasiak", "Joann", ""]]}, {"id": "2005.04522", "submitter": "Jens Kley-Holsteg", "authors": "Jens Kley-Holsteg and Florian Ziel", "title": "Probabilistic Multi-Step-Ahead Short-Term Water Demand Forecasting with\n  Lasso", "comments": "accepted for publication in ASCE's Journal of Water Resources\n  Planning and Management. Kley-Holsteg, J., Ziel, F.. Forthcoming.\n  \"Probabilistic multi-step-ahead short-term water demand forecasting with\n  lasso.\" Journal of Water Resources Planning and Management. DOI:\n  10.1061/(ASCE)WR.1943-5452.0001268", "journal-ref": null, "doi": "10.1061/(ASCE)WR.1943-5452.0001268", "report-no": null, "categories": "stat.AP econ.EM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Water demand is a highly important variable for operational control and\ndecision making. Hence, the development of accurate forecasts is a valuable\nfield of research to further improve the efficiency of water utilities.\nFocusing on probabilistic multi-step-ahead forecasting, a time series model is\nintroduced, to capture typical autoregressive, calendar and seasonal effects,\nto account for time-varying variance, and to quantify the uncertainty and\npath-dependency of the water demand process. To deal with the high complexity\nof the water demand process a high-dimensional feature space is applied, which\nis efficiently tuned by an automatic shrinkage and selection operator (lasso).\nIt allows to obtain an accurate, simple interpretable and fast computable\nforecasting model, which is well suited for real-time applications. The\ncomplete probabilistic forecasting framework allows not only for simulating the\nmean and the marginal properties, but also the correlation structure between\nhours within the forecasting horizon. For practitioners, complete probabilistic\nmulti-step-ahead forecasts are of considerable relevance as they provide\nadditional information about the expected aggregated or cumulative water\ndemand, so that a statement can be made about the probability with which a\nwater storage capacity can guarantee the supply over a certain period of time.\nThis information allows to better control storage capacities and to better\nensure the smooth operation of pumps. To appropriately evaluate the forecasting\nperformance of the considered models, the energy score (ES) as a strictly\nproper multidimensional evaluation criterion, is introduced. The methodology is\napplied to the hourly water demand data of a German water supplier.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 22:26:09 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Kley-Holsteg", "Jens", ""], ["Ziel", "Florian", ""]]}, {"id": "2005.04557", "submitter": "Youzhi Liang", "authors": "Xiaoyu Wu, Zeyu Bai, Jianguo Jia, Youzhi Liang", "title": "A Multi-Variate Triple-Regression Forecasting Algorithm for Long-Term\n  Customized Allergy Season Prediction", "comments": "4 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel multi-variate algorithm using a\ntriple-regression methodology to predict the airborne-pollen allergy season\nthat can be customized for each patient in the long term. To improve the\nprediction accuracy, we first perform a pre-processing to integrate the\nhistorical data of pollen concentration and various inferential signals from\nother covariates such as the meteorological data. We then propose a novel\nalgorithm which encompasses three-stage regressions: in Stage 1, a regression\nmodel to predict the start/end date of a airborne-pollen allergy season is\ntrained from a feature matrix extracted from 12 time series of the covariates\nwith a rolling window; in Stage 2, a regression model to predict the\ncorresponding uncertainty is trained based on the feature matrix and the\nprediction result from Stage 1; in Stage 3, a weighted linear regression model\nis built upon prediction results from Stage 1 and 2. It is observed and proved\nthat Stage 3 contributes to the improved forecasting accuracy and the reduced\nuncertainty of the multi-variate triple-regression algorithm. Based on\ndifferent allergy sensitivity level, the triggering concentration of the pollen\n- the definition of the allergy season can be customized individually. In our\nbacktesting, a mean absolute error (MAE) of 4.7 days was achieved using the\nalgorithm. We conclude that this algorithm could be applicable in both generic\nand long-term forecasting problems.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 02:42:12 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 18:58:37 GMT"}, {"version": "v3", "created": "Fri, 11 Dec 2020 01:23:50 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Wu", "Xiaoyu", ""], ["Bai", "Zeyu", ""], ["Jia", "Jianguo", ""], ["Liang", "Youzhi", ""]]}, {"id": "2005.04721", "submitter": "Geoffrey Johnson", "authors": "Geoffrey S Johnson", "title": "Decision Making in Drug Development via Confidence Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In clinical drug development a typical phase three power calculation for a\nGo/No-Go decision is performed by replacing unknown population-level quantities\nin the power function with what is observed from a literature review or what is\nobserved in phase two. Many authors and practitioners view this as an assumed\nvalue of power and offer the Bayesian quantity probability of success or\nassurance as an alternative. The claim is by averaging over a prior or\nposterior distribution, probability of success transcends power by capturing\nthe uncertainty around the unknown true treatment effect and any other\npopulation-level parameters. We use confidence distributions to frame both the\nprobability of success calculation and the typical power calculation as merely\nproducing two different point estimates of power. We demonstrate that Go/No-Go\ndecisions based on either point estimate of power do not adequately quantify\nand control the risk involved, and instead we argue for Go/No-Go decisions that\nutilize inference on power for better risk management and decision making. This\ninference on power can be derived and displayed using confidence distributions.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 17:25:05 GMT"}, {"version": "v10", "created": "Wed, 10 Mar 2021 19:46:14 GMT"}, {"version": "v11", "created": "Mon, 7 Jun 2021 18:22:09 GMT"}, {"version": "v12", "created": "Thu, 15 Jul 2021 17:03:43 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 03:50:23 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2020 12:33:07 GMT"}, {"version": "v4", "created": "Mon, 3 Aug 2020 19:06:31 GMT"}, {"version": "v5", "created": "Thu, 6 Aug 2020 13:12:07 GMT"}, {"version": "v6", "created": "Mon, 10 Aug 2020 22:35:41 GMT"}, {"version": "v7", "created": "Tue, 13 Oct 2020 14:25:49 GMT"}, {"version": "v8", "created": "Sat, 2 Jan 2021 00:05:34 GMT"}, {"version": "v9", "created": "Fri, 19 Feb 2021 01:13:16 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Johnson", "Geoffrey S", ""]]}, {"id": "2005.04879", "submitter": "Ming Bo Cai", "authors": "Ming Bo Cai, Michael Shvartsman, Anqi Wu, Hejia Zhang, Xia Zhu", "title": "Incorporating structured assumptions with probabilistic graphical models\n  in fMRI data analysis", "comments": "update with the version accepted by Neuropsychologia", "journal-ref": "Neuropsychologia, 107500 (2020)", "doi": "10.1016/j.neuropsychologia.2020.107500", "report-no": null, "categories": "stat.AP cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the wide adoption of functional magnetic resonance imaging (fMRI) by\ncognitive neuroscience researchers, large volumes of brain imaging data have\nbeen accumulated in recent years. Aggregating these data to derive scientific\ninsights often faces the challenge that fMRI data are high-dimensional,\nheterogeneous across people, and noisy. These challenges demand the development\nof computational tools that are tailored both for the neuroscience questions\nand for the properties of the data. We review a few recently developed\nalgorithms in various domains of fMRI research: fMRI in naturalistic tasks,\nanalyzing full-brain functional connectivity, pattern classification, inferring\nrepresentational similarity and modeling structured residuals. These algorithms\nall tackle the challenges in fMRI similarly: they start by making clear\nstatements of assumptions about neural data and existing domain knowledge,\nincorporating those assumptions and domain knowledge into probabilistic\ngraphical models, and using those models to estimate properties of interest or\nlatent structures in the data. Such approaches can avoid erroneous findings,\nreduce the impact of noise, better utilize known properties of the data, and\nbetter aggregate data across groups of subjects. With these successful cases,\nwe advocate wider adoption of explicit model construction in cognitive\nneuroscience. Although we focus on fMRI, the principle illustrated here is\ngenerally applicable to brain data of other modalities.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 06:32:54 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 00:44:14 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Cai", "Ming Bo", ""], ["Shvartsman", "Michael", ""], ["Wu", "Anqi", ""], ["Zhang", "Hejia", ""], ["Zhu", "Xia", ""]]}, {"id": "2005.04914", "submitter": "Jie Wu", "authors": "J. Wu, Z. Zheng, Y. Li, Y. Zhang", "title": "Scalable Interpretable Learning for Multi-Response Error-in-Variables\n  Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Corrupted data sets containing noisy or missing observations are prevalent in\nvarious contemporary applications such as economics, finance and\nbioinformatics. Despite the recent methodological and algorithmic advances in\nhigh-dimensional multi-response regression, how to achieve scalable and\ninterpretable estimation under contaminated covariates is unclear. In this\npaper, we develop a new methodology called convex conditioned sequential sparse\nlearning (COSS) for error-in-variables multi-response regression under both\nadditive measurement errors and random missing data. It combines the strengths\nof the recently developed sequential sparse factor regression and the nearest\npositive semi-definite matrix projection, thus enjoying stepwise convexity and\nscalability in large-scale association analyses. Comprehensive theoretical\nguarantees are provided and we demonstrate the effectiveness of the proposed\nmethodology through numerical studies.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 08:17:45 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Wu", "J.", ""], ["Zheng", "Z.", ""], ["Li", "Y.", ""], ["Zhang", "Y.", ""]]}, {"id": "2005.05009", "submitter": "Jean-Fran\\c{c}ois Coeurjolly", "authors": "Jean-Fran\\c{c}ois Coeurjolly", "title": "Digit analysis for Covid-19 reported data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The coronavirus which appeared in December 2019 in Wuhan has spread out\nworldwide and caused the death of more than 280,000 people (as of May, 11\n2020). Since February 2020, doubts were raised about the numbers of confirmed\ncases and deaths reported by the Chinese government. In this paper, we examine\ndata available from China at the city and provincial levels and we compare them\nwith Canadian provincial data, US state data and French regional data. We\nconsider cumulative and daily numbers of confirmed cases and deaths and examine\nthese numbers through the lens of their first two digits and in particular we\nmeasure departures of these first two digits to the Newcomb-Benford\ndistribution, often used to detect frauds. Our finding is that there is no\nevidence that cumulative and daily numbers of confirmed cases and deaths for\nall these countries have different first or second digit distributions. We also\nshow that the Newcomb-Benford distribution cannot be rejected for these data.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 11:42:56 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Coeurjolly", "Jean-Fran\u00e7ois", ""]]}, {"id": "2005.05272", "submitter": "Rapha\\\"el d'Andrimont", "authors": "Rapha\\\"el d'Andrimont, Momchil Yordanov, Laura Martinez-Sanchez,\n  Beatrice Eiselt, Alessandra Palmieri, Paolo Dominici, Javier Gallego, Hannes\n  Isaak Reuter, Christian Joebges, Guido Lemoine, Marijn van der Velde", "title": "Harmonised LUCAS in-situ data and photos on land cover and use from 5\n  tri-annual surveys in the European Union", "comments": null, "journal-ref": "Sci Data 7, 352 (2020)", "doi": "10.1038/s41597-020-00675-z", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accurately characterizing land surface changes with Earth Observation\nrequires geo-localized ground truth. In the European Union (EU), a tri-annual\nsurveyed sample of land cover and land use has been collected since 2006 under\nthe Land Use/Cover Area frame Survey (LUCAS). A total of 1,351,293 observations\nat 651,780 unique locations for 117 variables along with 5.4 million photos\nwere collected during five LUCAS surveys. Until now, these data have never been\nharmonised into one database, limiting full exploitation of the information.\nThis paper describes the LUCAS point sampling/surveying methodology, including\ncollection of standard variables such as land cover, environmental parameters,\nand full resolution landscape and point photos, and then describes the\nharmonisation process. The resulting harmonised database is the most\ncomprehensive in-situ dataset on land cover and use in the EU. The database is\nvaluable for geo-spatial and statistical analysis of land use and land cover\nchange. Furthermore, its potential to provide multi-temporal in-situ data will\nbe enhanced by recent computational advances such as deep learning.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 17:16:25 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["d'Andrimont", "Rapha\u00ebl", ""], ["Yordanov", "Momchil", ""], ["Martinez-Sanchez", "Laura", ""], ["Eiselt", "Beatrice", ""], ["Palmieri", "Alessandra", ""], ["Dominici", "Paolo", ""], ["Gallego", "Javier", ""], ["Reuter", "Hannes Isaak", ""], ["Joebges", "Christian", ""], ["Lemoine", "Guido", ""], ["van der Velde", "Marijn", ""]]}, {"id": "2005.05286", "submitter": "Benjamin Guedj", "authors": "Florent Dewez and Benjamin Guedj and Vincent Vandewalle", "title": "From industry-wide parameters to aircraft-centric on-flight inference:\n  improving aeronautics performance prediction with machine learning", "comments": "Published in Data-Centric Engineering", "journal-ref": "Data-Centric Engineering 2020", "doi": "10.1017/dce.2020.12", "report-no": null, "categories": "stat.AP cs.LG stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Aircraft performance models play a key role in airline operations, especially\nin planning a fuel-efficient flight. In practice, manufacturers provide\nguidelines which are slightly modified throughout the aircraft life cycle via\nthe tuning of a single factor, enabling better fuel predictions. However this\nhas limitations, in particular they do not reflect the evolution of each\nfeature impacting the aircraft performance. Our goal here is to overcome this\nlimitation. The key contribution of the present article is to foster the use of\nmachine learning to leverage the massive amounts of data continuously recorded\nduring flights performed by an aircraft and provide models reflecting its\nactual and individual performance. We illustrate our approach by focusing on\nthe estimation of the drag and lift coefficients from recorded flight data. As\nthese coefficients are not directly recorded, we resort to aerodynamics\napproximations. As a safety check, we provide bounds to assess the accuracy of\nboth the aerodynamics approximation and the statistical performance of our\napproach. We provide numerical results on a collection of machine learning\nalgorithms. We report excellent accuracy on real-life data and exhibit\nempirical evidence to support our modelling, in coherence with aerodynamics\nprinciples.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 17:40:17 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2020 16:42:36 GMT"}, {"version": "v3", "created": "Thu, 4 Feb 2021 10:22:43 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Dewez", "Florent", ""], ["Guedj", "Benjamin", ""], ["Vandewalle", "Vincent", ""]]}, {"id": "2005.05380", "submitter": "Junbo Zhao", "authors": "Junbo Zhao, Marcos Netto, Zhenyu Huang, Samson Shenglong Yu, Antonio\n  Gomez-Exposito, Shaobu Wang, Innocent Kamwa, Shahrokh Akhlaghi, Lamine Mili,\n  Vladimir Terzija, A. P. Sakis Meliopoulos, Bikash Pal, Abhinav Kumar Singh,\n  Ali Abur, Tianshu Bi, Alireza Rouhani", "title": "Roles of Dynamic State Estimation in Power System Modeling, Monitoring\n  and Operation", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": "10.1109/TPWRS.2020.3028047", "report-no": null, "categories": "eess.SP stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Power system dynamic state estimation (DSE) remains an active research area.\nThis is driven by the absence of accurate models, the increasing availability\nof fast-sampled, time-synchronized measurements, and the advances in the\ncapability, scalability, and affordability of computing and communications.\nThis paper discusses the advantages of DSE as compared to static state\nestimation, and the implementation differences between the two, including the\nmeasurement configuration, modeling framework and support software features.\nThe important roles of DSE are discussed from modeling, monitoring and\noperation aspects for today's synchronous machine dominated systems and the\nfuture power electronics-interfaced generation systems. Several examples are\npresented to demonstrate the benefits of DSE on enhancing the operational\nrobustness and resilience of 21st century power system through time critical\napplications. Future research directions are identified and discussed, paving\nthe way for developing the next generation of energy management systems.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 18:53:44 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Zhao", "Junbo", ""], ["Netto", "Marcos", ""], ["Huang", "Zhenyu", ""], ["Yu", "Samson Shenglong", ""], ["Gomez-Exposito", "Antonio", ""], ["Wang", "Shaobu", ""], ["Kamwa", "Innocent", ""], ["Akhlaghi", "Shahrokh", ""], ["Mili", "Lamine", ""], ["Terzija", "Vladimir", ""], ["Meliopoulos", "A. P. Sakis", ""], ["Pal", "Bikash", ""], ["Singh", "Abhinav Kumar", ""], ["Abur", "Ali", ""], ["Bi", "Tianshu", ""], ["Rouhani", "Alireza", ""]]}, {"id": "2005.05464", "submitter": "Marcos Prates O", "authors": "Douglas R. M. Azevedo and Marcos O. Prates and Michael R. Willig", "title": "Non-Separable Spatio-temporal Models via Transformed Gaussian Markov\n  Random Fields", "comments": "15 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models that capture the spatial and temporal dynamics are applicable in many\nscience fields. Non-separable spatio-temporal models were introduced in the\nliterature to capture these features. However, these models are generally\ncomplicated in construction and interpretation. We introduce a class of\nnon-separable Transformed Gaussian Markov Random Fields (TGMRF) in which the\ndependence structure is flexible and facilitates simple interpretations\nconcerning spatial, temporal and spatio-temporal parameters. Moreover, TGMRF\nmodels have the advantage of allowing specialists to define any desired\nmarginal distribution in model construction without suffering from\nspatio-temporal confounding. Consequently, the use of spatio-temporal models\nunder the TGMRF framework leads to a new class of general models, such as\nspatio-temporal Gamma random fields, that can be directly used to model Poisson\nintensity for space-time data. The proposed model was applied to identify\nimportant environmental characteristics that affect variation in the abundance\nof Nenia tridens, a dominant species of snail in a well-studied tropical\necosystem, and to characterize its spatial and temporal trends, which are\nparticularly critical during the Anthropocene, an epoch of time characterized\nby human-induced environmental change associated with climate and land use.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 22:07:35 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Azevedo", "Douglas R. M.", ""], ["Prates", "Marcos O.", ""], ["Willig", "Michael R.", ""]]}, {"id": "2005.05628", "submitter": "Claire Boyer", "authors": "Pascaline Descloux (UNIGE), Claire Boyer (LPSM UMR 8001), Julie Josse\n  (CMAP), Aude Sportisse (LPSM (UMR\\_8001)), Sylvain Sardy", "title": "Robust Lasso-Zero for sparse corruption and model selection with missing\n  covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Robust Lasso-Zero, an extension of the Lasso-Zero methodology\n[Descloux and Sardy, 2018], initially introduced for sparse linear models, to\nthe sparse corruptions problem. We give theoretical guarantees on the sign\nrecovery of the parameters for a slightly simplified version of the estimator,\ncalled Thresholded Justice Pursuit. The use of Robust Lasso-Zero is showcased\nfor variable selection with missing values in the covariates. In addition to\nnot requiring the specification of a model for the covariates, nor estimating\ntheir covariance matrix or the noise variance, the method has the great\nadvantage of handling missing not-at random values without specifying a\nparametric model. Numerical experiments and a medical application underline the\nrelevance of Robust Lasso-Zero in such a context with few available\ncompetitors. The method is easy to use and implemented in the R library lass0.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 09:05:46 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Descloux", "Pascaline", "", "UNIGE"], ["Boyer", "Claire", "", "LPSM UMR 8001"], ["Josse", "Julie", "", "CMAP"], ["Sportisse", "Aude", "", "LPSM"], ["Sardy", "Sylvain", ""]]}, {"id": "2005.05665", "submitter": "Miriam Bertola", "authors": "Miriam Bertola, Alberto Viglione and G\\\"unter Bl\\\"oschl", "title": "Informed attribution of flood changes to decadal variation of\n  atmospheric, catchment and river drivers in Upper Austria", "comments": "Published in Journal of Hydrology", "journal-ref": "Journal of Hydrology 577 (2019) 123919", "doi": "10.1016/j.jhydrol.2019.123919", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Flood changes may be attributed to drivers of change that belong to three\nmain classes: atmospheric, catchment and river system drivers. In this work, we\npropose a data-based attribution approach for selecting which driver best\nrelates to variations in time of the flood frequency curve. The flood peaks are\nassumed to follow a Gumbel distribution, whose location parameter changes in\ntime as a function of the decadal variations of one of the following\nalternative covariates: annual and extreme precipitation for different\ndurations, an agricultural land-use intensification index, and reservoir\nconstruction in the catchment, quantified by an index. The parameters of this\nattribution model are estimated by Bayesian inference. Prior information on one\nof these parameters, the elasticity of flood peaks to the respective driver, is\ntaken from the existing literature to increase the robustness of the method to\nspurious correlations between flood and covariate time series. Therefore, the\nattribution model is informed in two ways: by the use of covariates,\nrepresenting the drivers of change, and by the priors, representing the\nhydrological understanding of how these covariates influence floods. The\nWatanabe-Akaike information criterion is used to compare models involving\nalternative covariates. We apply the approach to 96 catchments in Upper\nAustria, where positive flood peak trends have been observed in the past 50\nyears. Results show that, in Upper Austria, one or seven day extreme\nprecipitation is usually a better covariate for variations of the flood\nfrequency curve than precipitation at longer time scales. Agricultural land-use\nintensification rarely is the best covariate, and the reservoir index never is,\nsuggesting that catchment and river drivers are less important than atmospheric\nones.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 10:20:22 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Bertola", "Miriam", ""], ["Viglione", "Alberto", ""], ["Bl\u00f6schl", "G\u00fcnter", ""]]}, {"id": "2005.05783", "submitter": "Xinlian Yu", "authors": "Xinlian Yu, Tien Mai, Jing Ding-Mastera, Song Gao and Emma Frejinger", "title": "Modeling Route Choice with Real-Time Information: Comparing the\n  Recursive and Non-Recursive Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.SY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the routing policy choice problems in a stochastic time-dependent\n(STD) network. A routing policy is defined as a decision rule applied at the\nend of each link that maps the realized traffic condition to the decision on\nthe link to take next. Two types of routing policy choice models are formulated\nwith perfect online information (POI): recursive logit model and non-recursive\nlogit model. In the non-recursive model, a choice set of routing policies\nbetween an origin-destination (OD) pair is generated, and a probabilistic\nchoice is modeled at the origin, while the choice of the next link at each link\nis a deterministic execution of the chosen routing policy. In the recursive\nmodel, the probabilistic choice of the next link is modeled at each link,\nfollowing the framework of dynamic discrete choice models. The two models are\nfurther compared in terms of computational efficiency in estimation and\nprediction, and flexibility in systematic utility specification and modeling\ncorrelation.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 18:23:46 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 20:23:23 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Yu", "Xinlian", ""], ["Mai", "Tien", ""], ["Ding-Mastera", "Jing", ""], ["Gao", "Song", ""], ["Frejinger", "Emma", ""]]}, {"id": "2005.05808", "submitter": "Eva Cantoni", "authors": "Setareh Ranjbar, Eva Cantoni, Val\\'erie Chavez-Demoulin, Giampiero\n  Marra, Rosalba Radice, Katia Jaton-Ogay", "title": "Modelling the Extremes of Seasonal Viruses and Hospital Congestion: The\n  Example of Flu in a Swiss Hospital", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Viruses causing flu or milder coronavirus colds are often referred to as\n\"seasonal viruses\" as they tend to subside in warmer months. In other words,\nmeteorological conditions tend to impact the activity of viruses, and this\ninformation can be exploited for the operational management of hospitals. In\nthis study, we use three years of daily data from one of the biggest hospitals\nin Switzerland and focus on modelling the extremes of hospital visits from\npatients showing flu-like symptoms and the number of positive cases of flu. We\npropose employing a discrete Generalized Pareto distribution for the number of\npositive and negative cases, and a Generalized Pareto distribution for the odds\nof positive cases. Our modelling framework allows for the parameters of these\ndistributions to be linked to covariate effects, and for outlying observations\nto be dealt with via a robust estimation approach. Because meteorological\nconditions may vary over time, we use meteorological and not calendar\nvariations to explain hospital charge extremes, and our empirical findings\nhighlight their significance. We propose a measure of hospital congestion and a\nrelated tool to estimate the resulting CaRe (Charge-at-Risk-estimation) under\ndifferent meteorological conditions. The relevant numerical computations can be\neasily carried out using the freely available GJRM R package. The introduced\napproach could be applied to several types of seasonal disease data such as\nthose derived from the new virus SARS-CoV-2 and its COVID-19 disease which is\nat the moment wreaking havoc worldwide. The empirical effectiveness of the\nproposed method is assessed through a simulation study.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 14:21:02 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Ranjbar", "Setareh", ""], ["Cantoni", "Eva", ""], ["Chavez-Demoulin", "Val\u00e9rie", ""], ["Marra", "Giampiero", ""], ["Radice", "Rosalba", ""], ["Jaton-Ogay", "Katia", ""]]}, {"id": "2005.05883", "submitter": "Adam Chang", "authors": "Adam Chang", "title": "Networks in a World Unknown: Public WhatsApp Groups in the Venezuelan\n  Refugee Crisis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By early March 2020, five million Venezuelans had fled their home country\nafter its complete economic and institutional collapse, and over 1.6 million\nhave migrated to Colombia. Migrants struggle to start their lives over in\nColombia, having arrived with few economic resources, and often no legal\ndocumentation, in cities with little to offer them. Venezuelan migrants,\nhowever, rely heavily on mobile phones and social media networks as lifelines\nfor information, opportunities, and resources -- making WhatsApp both a\ncritical tool for migrants' settlement and integration, as well as an\ninvaluable source of data through which we can better understand migrant\nexperiences. This thesis explores the dynamics of public WhatsApp groups used\nby Venezuelan migrants to Colombia, and what they can tell us about how\nmigrants use and share information. We center our research on information\nspread and trust, especially as they intersect with concentration and\ngeographic heterogeneity within groups. We analyze messages and memberships\nbroadly, then explore interaction within groups, fake news and economic scams,\nand effects of the coronavirus pandemic. Our results have a range of policy\nimplications, from reflections on Colombia's decision to shut its borders\namidst the coronavirus pandemic, to understandings of how aid organizations can\neffectively share information over social media channels.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 05:46:51 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Chang", "Adam", ""]]}, {"id": "2005.05952", "submitter": "Danilo Alvares", "authors": "Danilo Alvares, Elena L\\'azaro, Virgilio G\\'omez-Rubio, and Carmen\n  Armero", "title": "Bayesian survival analysis with BUGS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Survival analysis is one of the most important fields of statistics in\nmedicine and the biological sciences. In addition, the computational advances\nin the last decades have favoured the use of Bayesian methods in this context,\nproviding a flexible and powerful alternative to the traditional frequentist\napproach. The objective of this paper is to summarise some of the most popular\nBayesian survival models, such as accelerated failure time, proportional\nhazards, mixture cure, competing risks, frailty, and joint models of\nlongitudinal and survival data. Moreover, an implementation of each presented\nmodel is provided using a BUGS syntax that can be run with JAGS from the R\nprogramming language. Reference to other Bayesian R-packages are also\ndiscussed.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 17:52:46 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 14:54:11 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Alvares", "Danilo", ""], ["L\u00e1zaro", "Elena", ""], ["G\u00f3mez-Rubio", "Virgilio", ""], ["Armero", "Carmen", ""]]}, {"id": "2005.06095", "submitter": "Arun Kuchibhotla", "authors": "Arun Kumar Kuchibhotla", "title": "Exchangeability, Conformal Prediction, and Rank Tests", "comments": "Added reference to Shi et al. (2020, JASA). Corrected Theorem 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conformal prediction has been a very popular method of distribution-free\npredictive inference in recent years in machine learning and statistics. Its\npopularity stems from the fact that it works as a wrapper around any prediction\nalgorithm such as neural networks or random forests. Exchangeability is at the\ncore of the validity of conformal prediction. The concept of exchangeability is\nalso at the core of rank tests widely known in nonparametric statistics. In\nthis paper, we review the concept of exchangeability and discuss the\nimplications for conformal prediction and rank tests. We provide a low-level\nintroduction to these topics, and discuss the similarities between conformal\nprediction and rank tests.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 00:39:45 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 16:12:48 GMT"}, {"version": "v3", "created": "Fri, 4 Jun 2021 01:35:40 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Kuchibhotla", "Arun Kumar", ""]]}, {"id": "2005.06180", "submitter": "Marco Piangerelli", "authors": "Andrea De Simone and Marco Piangerelli", "title": "The impact of undetected cases on tracking epidemics: the case of\n  COVID-19", "comments": "23 Pages, 10 Figures", "journal-ref": "Chaos, Solitons & Fractals, Volume 140, 2020, 110167", "doi": "10.1016/j.chaos.2020.110167", "report-no": null, "categories": "q-bio.PE q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key indicators used in tracking the evolution of an infectious\ndisease isthe reproduction number. This quantity is usually computed using the\nreportednumber of cases, but ignoring that many more individuals may be\ninfected (e.g.asymptomatics). We propose a statistical procedure to quantify\nthe impact of un-detected infectious cases on the determination of the\neffective reproduction number. Our approach is stochastic, data-driven and not\nrelying on any compartmentalmodel. It is applied to the COVID-19 case in eight\ndifferent countries and all Italianregions, showing that the effect of\nundetected cases leads to estimates of the effective reproduction numbers\nlarger than those obtained only with the reported cases by factors ranging from\ntwo to ten. Our findings urge caution about deciding when and how to relax\ncontainment measures based on the value of the reproduction number.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 06:49:46 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["De Simone", "Andrea", ""], ["Piangerelli", "Marco", ""]]}, {"id": "2005.06613", "submitter": "Charlie Kirkwood", "authors": "Charlie Kirkwood, Theo Economou, Henry Odbert, Nicolas Pugeault", "title": "A framework for probabilistic weather forecast post-processing across\n  models and lead times using machine learning", "comments": "17 pages, 9 figures, to be published in Philosophical Transactions of\n  the Royal Society A", "journal-ref": null, "doi": "10.1098/rsta.2020.0099", "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasting the weather is an increasingly data intensive exercise. Numerical\nWeather Prediction (NWP) models are becoming more complex, with higher\nresolutions, and there are increasing numbers of different models in operation.\nWhile the forecasting skill of NWP models continues to improve, the number and\ncomplexity of these models poses a new challenge for the operational\nmeteorologist: how should the information from all available models, each with\ntheir own unique biases and limitations, be combined in order to provide\nstakeholders with well-calibrated probabilistic forecasts to use in decision\nmaking? In this paper, we use a road surface temperature example to demonstrate\na three-stage framework that uses machine learning to bridge the gap between\nsets of separate forecasts from NWP models and the 'ideal' forecast for\ndecision support: probabilities of future weather outcomes. First, we use\nQuantile Regression Forests to learn the error profile of each numerical model,\nand use these to apply empirically-derived probability distributions to\nforecasts. Second, we combine these probabilistic forecasts using quantile\naveraging. Third, we interpolate between the aggregate quantiles in order to\ngenerate a full predictive distribution, which we demonstrate has properties\nsuitable for decision support. Our results suggest that this approach provides\nan effective and operationally viable framework for the cohesive\npost-processing of weather forecasts across multiple models and lead times to\nproduce a well-calibrated probabilistic output.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 16:46:02 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 09:45:25 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Kirkwood", "Charlie", ""], ["Economou", "Theo", ""], ["Odbert", "Henry", ""], ["Pugeault", "Nicolas", ""]]}, {"id": "2005.06617", "submitter": "Matthew Aldridge", "authors": "Matthew Aldridge", "title": "Conservative two-stage group testing", "comments": "17 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by applications in testing for COVID-19, we consider a variant of\ntwo-stage group testing we call \"conservative\" two-stage testing, where every\nitem declared to be defective must be definitively confirmed by being tested by\nitself in the second stage. We study this in the linear regime where the\nprevalence is fixed while the number of items is large. We study various\nnonadaptive test designs for the first stage, and derive a new lower bound for\nthe total number of tests required. We find that a first-stage design with\nconstant tests per item and constant items per test due to Broder and Kumar\n(arXiv:2004.01684) is extremely close to optimal. Simulations back up the\ntheoretical results.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 22:14:49 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Aldridge", "Matthew", ""]]}, {"id": "2005.06630", "submitter": "Ahmed Allam", "authors": "Ahmed Allam, Matthias Dittberner, Anna Sintsova, Dominique Brodbeck,\n  Michael Krauthammer", "title": "Patient Similarity Analysis with Longitudinal Health Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY q-bio.QM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Healthcare professionals have long envisioned using the enormous processing\npowers of computers to discover new facts and medical knowledge locked inside\nelectronic health records. These vast medical archives contain time-resolved\ninformation about medical visits, tests and procedures, as well as outcomes,\nwhich together form individual patient journeys. By assessing the similarities\namong these journeys, it is possible to uncover clusters of common disease\ntrajectories with shared health outcomes. The assignment of patient journeys to\nspecific clusters may in turn serve as the basis for personalized outcome\nprediction and treatment selection. This procedure is a non-trivial\ncomputational problem, as it requires the comparison of patient data with\nmulti-dimensional and multi-modal features that are captured at different times\nand resolutions. In this review, we provide a comprehensive overview of the\ntools and methods that are used in patient similarity analysis with\nlongitudinal data and discuss its potential for improving clinical decision\nmaking.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 07:06:02 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Allam", "Ahmed", ""], ["Dittberner", "Matthias", ""], ["Sintsova", "Anna", ""], ["Brodbeck", "Dominique", ""], ["Krauthammer", "Michael", ""]]}, {"id": "2005.06769", "submitter": "Christoph Rothe", "authors": "Christoph Rothe", "title": "Combining Population and Study Data for Inference on Event Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note considers the problem of conducting statistical inference on the\nshare of individuals in some subgroup of a population that experience some\nevent. The specific complication is that the size of the subgroup needs to be\nestimated, whereas the number of individuals that experience the event is\nknown. The problem is motivated by the recent study of Streeck et al. (2020),\nwho estimate the infection fatality rate (IFR) of SARS-CoV-2 infection in a\nGerman town that experienced a super-spreading event in mid-February 2020. In\ntheir case the subgroup of interest is comprised of all infected individuals,\nand the event is death caused by the infection. We clarify issues with the\nprecise definition of the target parameter in this context, and propose\nconfidence intervals (CIs) based on classical statistical principles that\nresult in good coverage properties.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 07:30:53 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Rothe", "Christoph", ""]]}, {"id": "2005.06776", "submitter": "Bastien Mallein", "authors": "Vincent Brault, Bastien Mallein and Jean-Francois Rupprecht", "title": "Group testing as a strategy for the epidemiologic monitoring of COVID-19", "comments": "21 pages, 14 figures", "journal-ref": null, "doi": "10.1371/journal.pcbi.1008726", "report-no": null, "categories": "q-bio.QM physics.soc-ph stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sample pooling consists in combining samples from multiple individuals into a\nsingle pool that is then tested using a unique test-kit. A positive test means\nthat at least one individual within the pool is infected. Here, we propose an\nanalysis and applications of sample pooling to the epidemiologic monitoring of\nCOVID-19. We first introduce a model of the RT-qPCR process used to test for\nthe presence of virus in a sample and construct a statistical model for the\nviral load in a typical infected individual inspired by the clinical data from\nJones et. al. (2020). We then propose a method for the measure of the\nprevalence in a population, based on group testing, taking into account the\nincreased number of false negatives associated with this method. Finally, we\npresent an application of sample pooling for the prevention of epidemic\noutbreak in closed connected communities (e.g. nursing homes).\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 07:47:35 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 14:09:44 GMT"}, {"version": "v3", "created": "Thu, 25 Jun 2020 14:16:14 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Brault", "Vincent", ""], ["Mallein", "Bastien", ""], ["Rupprecht", "Jean-Francois", ""]]}, {"id": "2005.06830", "submitter": "Teemu H\\\"ark\\\"onen", "authors": "Teemu H\\\"ark\\\"onen, Lassi Roininen, Matthew T. Moores, and Erik M.\n  Vartiainen", "title": "Bayesian quantification for coherent anti-Stokes Raman scattering\n  spectroscopy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian statistical model for analyzing coherent anti-Stokes\nRaman scattering (CARS) spectra. Our quantitative analysis includes statistical\nestimation of constituent line-shape parameters, underlying Raman signal,\nerror-corrected CARS spectrum, and the measured CARS spectrum. As such, this\nwork enables extensive uncertainty quantification in the context of CARS\nspectroscopy. Furthermore, we present an unsupervised method for improving\nspectral resolution of Raman-like spectra requiring little to no \\textit{a\npriori} information. Finally, the recently-proposed wavelet prism method for\ncorrecting the experimental artefacts in CARS is enhanced by using\ninterpolation techniques for wavelets. The method is validated using CARS\nspectra of adenosine mono-, di-, and triphosphate in water, as well as,\nequimolar aqueous solutions of D-fructose, D-glucose, and their disaccharide\ncombination sucrose.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 09:20:41 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["H\u00e4rk\u00f6nen", "Teemu", ""], ["Roininen", "Lassi", ""], ["Moores", "Matthew T.", ""], ["Vartiainen", "Erik M.", ""]]}, {"id": "2005.06851", "submitter": "Michael Pfarrhofer", "authors": "Florian Huber and Michael Pfarrhofer", "title": "Dynamic shrinkage in time-varying parameter stochastic volatility in\n  mean models", "comments": "JEL: C11, C32, C53, E31; Keywords: state-space models, inflation\n  forecasting, inflation uncertainty, real time data, replication", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Successful forecasting models strike a balance between parsimony and\nflexibility. This is often achieved by employing suitable shrinkage priors that\npenalize model complexity but also reward model fit. In this note, we modify\nthe stochastic volatility in mean (SVM) model proposed in Chan (2017) by\nintroducing state-of-the-art shrinkage techniques that allow for time-variation\nin the degree of shrinkage. Using a real-time inflation forecast exercise, we\nshow that employing more flexible prior distributions on several key parameters\nslightly improves forecast performance for the United States (US), the United\nKingdom (UK) and the Euro Area (EA). Comparing in-sample results reveals that\nour proposed model yields qualitatively similar insights to the original\nversion of the model.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 10:10:09 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Huber", "Florian", ""], ["Pfarrhofer", "Michael", ""]]}, {"id": "2005.07062", "submitter": "Christian Schroeder de Witt", "authors": "Christian Schroeder de Witt, Bradley Gram-Hansen, Nantas Nardelli,\n  Andrew Gambardella, Rob Zinkov, Puneet Dokania, N. Siddharth, Ana Belen\n  Espinosa-Gonzalez, Ara Darzi, Philip Torr, At{\\i}l{\\i}m G\\\"une\\c{s} Baydin", "title": "Simulation-Based Inference for Global Health Decisions", "comments": null, "journal-ref": "ICML Workshop on Machine Learning for Global Health,\n  Thirty-Seventh International Conference on Machine Learning (ICML 2020)", "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic has highlighted the importance of in-silico\nepidemiological modelling in predicting the dynamics of infectious diseases to\ninform health policy and decision makers about suitable prevention and\ncontainment strategies. Work in this setting involves solving challenging\ninference and control problems in individual-based models of ever increasing\ncomplexity. Here we discuss recent breakthroughs in machine learning,\nspecifically in simulation-based inference, and explore its potential as a\nnovel venue for model calibration to support the design and evaluation of\npublic health interventions. To further stimulate research, we are developing\nsoftware interfaces that turn two cornerstone COVID-19 and malaria epidemiology\nmodels COVID-sim, (https://github.com/mrc-ide/covid-sim/) and OpenMalaria\n(https://github.com/SwissTPH/openmalaria) into probabilistic programs, enabling\nefficient interpretable Bayesian inference within those simulators.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 15:29:45 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["de Witt", "Christian Schroeder", ""], ["Gram-Hansen", "Bradley", ""], ["Nardelli", "Nantas", ""], ["Gambardella", "Andrew", ""], ["Zinkov", "Rob", ""], ["Dokania", "Puneet", ""], ["Siddharth", "N.", ""], ["Espinosa-Gonzalez", "Ana Belen", ""], ["Darzi", "Ara", ""], ["Torr", "Philip", ""], ["Baydin", "At\u0131l\u0131m G\u00fcne\u015f", ""]]}, {"id": "2005.07142", "submitter": "Michail Katsoulis", "authors": "Michail Katsoulis, Christina Bamia", "title": "Moving from two- to multi-way interactions among binary risk factors on\n  the additive scale", "comments": "14 pages (main text), 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many studies have focused on investigating deviations from additive\ninteraction of two dichotomous risk factors on a binary outcome. There is,\nhowever, a gap in the literature with respect to interactions on the additive\nscale of >2 risk factors. In this paper, we present an approach for examining\ndeviations from additive interaction among three on more binary exposures. The\nrelative excess risk due to interaction (RERI) is used as measure of additive\ninteraction. First, we concentrate on three risk factors - we propose to\ndecompose the total RERI to: the RERI owned to the joint presence of all 3 risk\nfactors and the RERI of any two risk factors, given that the third is absent.\nWe then extend this approach, to >3 binary risk factors. For illustration, we\nuse a sample from data from the Greek EPIC cohort and we investigate the\nassociation with overall mortality of Mediterranean diet, body mass index\n(BMI), and, smoking. Our formulae enable better interpretability of any\nevidence for deviations from additivity owned to more than two risk factors and\nprovide simple ways of communicating such results from a public health\nperspective by attributing any excess relative risk to specific combinations of\nthese factors.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 17:01:37 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Katsoulis", "Michail", ""], ["Bamia", "Christina", ""]]}, {"id": "2005.07180", "submitter": "Julius von K\\\"ugelgen", "authors": "Julius von K\\\"ugelgen, Luigi Gresele, Bernhard Sch\\\"olkopf", "title": "Simpson's paradox in Covid-19 case fatality rates: a mediation analysis\n  of age-related causal effects", "comments": "Journal version with full Appendix. The first two authors contributed\n  equally to this work", "journal-ref": "IEEE Transactions on Artificial Intelligence, vol. 2, no. 1, pp.\n  18-27, Feb. 2021", "doi": "10.1109/TAI.2021.3073088", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We point out an instantiation of Simpson's paradox in Covid-19 case fatality\nrates (CFRs): comparing a large-scale study from China (17 Feb) with early\nreports from Italy (9 Mar), we find that CFRs are lower in Italy for every age\ngroup, but higher overall. This phenomenon is explained by a stark difference\nin case demographic between the two countries. Using this as a motivating\nexample, we introduce basic concepts from mediation analysis and show how these\ncan be used to quantify different direct and indirect effects when assuming a\ncoarse-grained causal graph involving country, age, and case fatality. We\ncurate an age-stratified CFR dataset with >750k cases and conduct a case study,\ninvestigating total, direct, and indirect (age-mediated) causal effects between\ndifferent countries and at different points in time. This allows us to separate\nage-related effects from others unrelated to age and facilitates a more\ntransparent comparison of CFRs across countries at different stages of the\nCovid-19 pandemic. Using longitudinal data from Italy, we discover a sign\nreversal of the direct causal effect in mid-March which temporally aligns with\nthe reported collapse of the healthcare system in parts of the country.\nMoreover, we find that direct and indirect effects across 132 pairs of\ncountries are only weakly correlated, suggesting that a country's policy and\ncase demographic may be largely unrelated. We point out limitations and\nextensions for future work, and, finally, discuss the role of causal reasoning\nin the broader context of using AI to combat the Covid-19 pandemic.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 17:52:10 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 18:26:14 GMT"}, {"version": "v3", "created": "Wed, 23 Jun 2021 11:40:40 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["von K\u00fcgelgen", "Julius", ""], ["Gresele", "Luigi", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "2005.07271", "submitter": "Stefano M. Iacus", "authors": "Krzysztof Bartoszek and Emanuele Guidotti and Stefano Maria Iacus and\n  Marcin Okr\\'oj", "title": "Are official confirmed cases and fatalities counts good enough to study\n  the COVID-19 pandemic dynamics? A critical assessment through the case of\n  Italy", "comments": "updated references", "journal-ref": "Nonlinear Dynamics, 101, 1951-1979 (2020)", "doi": "10.1007/s11071-020-05761-w", "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the COVID-19 outbreak is developing the two most frequently reported\nstatistics seem to be the raw confirmed case and case fatalities counts.\nFocusing on Italy, one of the hardest hit countries, we look at how these two\nvalues could be put in perspective to reflect the dynamics of the virus spread.\nIn particular, we find that merely considering the confirmed case counts would\nbe very misleading. The number of daily tests grows, while the daily fraction\nof confirmed cases to total tests has a change point. It (depending on region)\ngenerally increases with strong fluctuations till (around, depending on region)\n15th-22nd March and then decreases linearly after. Combined with the increasing\ntrend of daily performed tests, the raw confirmed case counts are not\nrepresentative of the situation and are confounded with the sampling effort.\nThis we observe when regressing on time the logged fraction of positive tests\nand for comparison the logged raw confirmed count. Hence, calibrating model\nparameters for this virus's dynamics should not be done based only on confirmed\ncase counts (without rescaling by the number of tests), but take also\nfatalities and hospitalization count under consideration as variables not prone\nto be distorted by testing efforts. Furthermore, reporting statistics on the\nnational level does not say much about the dynamics of the disease, which are\ntaking place at the regional level. These findings are based on the official\ndata of total death counts up to 15th April 2020 released by ISTAT and up to\n10th May 2020 for the number of cases. In this work we do not fit models but we\nrather investigate whether this task is possible at all. This work also informs\nabout a new tool to collect and harmonize official statistics coming from\ndifferent sources in the form of a package for the R statistical environment\nand presents the COVID-19 Data Hub.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 21:18:08 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 09:35:29 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Bartoszek", "Krzysztof", ""], ["Guidotti", "Emanuele", ""], ["Iacus", "Stefano Maria", ""], ["Okr\u00f3j", "Marcin", ""]]}, {"id": "2005.07334", "submitter": "Juan Doblas", "authors": "Juan Doblas", "title": "Optimizing SAR data processing and thresholding for forest change\n  detection: an application for early deforestation warnings on eastern\n  Amazonia", "comments": "To be published on the XX SBSR (2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The present work proposes a prototype for an operational method for early\ndeforestation detection of cloudy tropical rainforests. The proposed\nmethodology makes use of Sentinel-1 SAR data processed into the Google Earth\nEngine platform for flag the areas where the probability of recent\ndeforestation is high. The evaluation of the results over a region on the\nEastern Amazon basin showed that copolarized data (VV band) offers the best\nresults in terms of producer's accuracy (95,4% for a 5% significance, 88,9% for\n1% significance), while crosspolarized data (VH band) offered excellent results\nin terms of user's accuracy (86% for a 5% significance, 100% for 1%\nsignificance).\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 03:12:04 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Doblas", "Juan", ""]]}, {"id": "2005.07450", "submitter": "Frank Werner", "authors": "Gytis Kulaitis, Axel Munk, Frank Werner", "title": "What is resolution? A statistical minimax testing perspective on\n  super-resolution microscopy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a general rule of thumb the resolution of a light microscope (i.e. the\nability to discern objects) is predominantly described by the full width at\nhalf maximum (FWHM) of its point spread function (psf)---the diameter of the\nblurring density at half of its maximum. Classical wave optics suggests a\nlinear relationship between FWHM and resolution also manifested in the well\nknown Abbe and Rayleigh criteria, dating back to the end of 19th century.\nHowever, during the last two decades conventional light microscopy has\nundergone a shift from microscopic scales to nanoscales. This increase in\nresolution comes with the need to incorporate the random nature of observations\n(light photons) and challenges the classical view of discernability, as we\nargue in this paper. Instead, we suggest a statistical description of\nresolution obtained from such random data. Our notion of discernability is\nbased on statistical testing whether one or two objects with the same total\nintensity are present. For Poisson measurements we get linear dependence of the\n(minimax) detection boundary on the FWHM, whereas for a homogeneous Gaussian\nmodel the dependence of resolution is nonlinear. Hence, at small physical\nscales modeling by homogeneous gaussians is inadequate, although often\nimplicitly assumed in many reconstruction algorithms. In contrast, the Poisson\nmodel and its variance stabilized Gaussian approximation seem to provide a\nstatistically sound description of resolution at the nanoscale. Our theory is\nalso applicable to other imaging setups, such as telescopes.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 09:59:15 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 07:27:47 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Kulaitis", "Gytis", ""], ["Munk", "Axel", ""], ["Werner", "Frank", ""]]}, {"id": "2005.07452", "submitter": "Marc Schneble", "authors": "Marc Schneble, Giacomo De Nicola, G\\\"oran Kauermann and Ursula Berger", "title": "Nowcasting fatal COVID-19 infections on a regional level in Germany", "comments": "22 pages, 9 Figures", "journal-ref": null, "doi": "10.1002/bimj.202000143", "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyse the temporal and regional structure in mortality rates related to\nCOVID-19 infections. We relate the fatality date of each deceased patient to\nthe corresponding day of registration of the infection, leading to a nowcasting\nmodel which allows us to estimate the number of present-day infections that\nwill, at a later date, prove to be fatal. The numbers are broken down to the\ndistrict level in Germany. Given that death counts generally provide more\nreliable information on the spread of the disease compared to infection counts,\nwhich inevitably depend on testing strategy and capacity, the proposed model\nand the presented results allow to obtain reliable insight into the current\nstate of the pandemic in Germany.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 10:01:51 GMT"}, {"version": "v2", "created": "Sat, 21 Nov 2020 09:35:22 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Schneble", "Marc", ""], ["De Nicola", "Giacomo", ""], ["Kauermann", "G\u00f6ran", ""], ["Berger", "Ursula", ""]]}, {"id": "2005.07468", "submitter": "Sabyasachi Mukhopadhyay", "authors": "Sabyasachi Mukhopadhyay, Hans-Peter Piepho, Sourabh Bhattacharya,\n  Holly T. Dublin and Joseph O. Ogutu", "title": "Hierarchical Bayesian state-space modeling of age- and sex-structured\n  wildlife population dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biodiversity is declining at alarming rates worldwide, including for large\nwild mammals. It is therefore imperative to develop effective population\nconservation and recovery strategies. Population dynamics models can provide\ninsights into processes driving declines of particular populations of a species\nand their relative importance. We develop an integrated Bayesian state-space\npopulation dynamics model for wildlife populations and illustrate it using a\ntopi population inhabiting the Masai Mara Ecosystem in Kenya. The model is\ngeneral and integrates ground demographic survey with aerial survey monitoring\ndata. It incorporates population age- and sex-structure and life-history traits\nand relates birth rates, age-specific survival rates and sex ratio with\nmeteorological covariates, prior population density, environmental seasonality\nand predation risk. The model runs on a monthly time step, enabling accurate\ncharacterization of reproductive seasonality, phenology, synchrony and\nprolificacy of births and juvenile recruitment. Model performance is evaluated\nusing balanced bootstrap sampling and comparing predictions with aerial\npopulation size estimates. The model is implemented using MCMC methods and\nreproduces several well-known features of the Mara topi population, including\nstriking and persistent population decline, seasonality of births and juvenile\nrecruitment. It can be readily adapted for other wildlife species and extended\nto incorporate several additional useful features.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 10:55:06 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Mukhopadhyay", "Sabyasachi", ""], ["Piepho", "Hans-Peter", ""], ["Bhattacharya", "Sourabh", ""], ["Dublin", "Holly T.", ""], ["Ogutu", "Joseph O.", ""]]}, {"id": "2005.07567", "submitter": "Guangcun Shan Prof.", "authors": "Lu Han, G.C. Shan, H.Y. Wang, S.Q. Gao, W.X. Zhou", "title": "Accelerating drug repurposing for COVID-19 via modeling drug mechanism\n  of action with large scale gene-expression profiles", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The novel coronavirus disease, named COVID-19, emerged in China in December\n2019, and has rapidly spread around the world. It is clearly urgent to fight\nCOVID-19 at global scale. The development of methods for identifying drug uses\nbased on phenotypic data can improve the efficiency of drug development.\nHowever, there are still many difficulties in identifying drug applications\nbased on cell picture data. This work reported one state-of-the-art machine\nlearning method to identify drug uses based on the cell image features of 1024\ndrugs generated in the LINCS program. Because the multi-dimensional features of\nthe image are affected by non-experimental factors, the characteristics of\nsimilar drugs vary greatly, and the current sample number is not enough to use\ndeep learning and other methods are used for learning optimization. As a\nconsequence, this study is based on the supervised ITML algorithm to convert\nthe characteristics of drugs. The results show that the characteristics of ITML\nconversion are more conducive to the recognition of drug functions. The\nanalysis of feature conversion shows that different features play important\nroles in identifying different drug functions. For the current COVID-19,\nChloroquine and Hydroxychloroquine achieve antiviral effects by inhibiting\nendocytosis, etc., and were classified to the same community. And Clomiphene in\nthe same community inibited the entry of Ebola Virus, indicated a similar MoAs\nthat could be reflected by cell image.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 14:28:56 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Han", "Lu", ""], ["Shan", "G. C.", ""], ["Wang", "H. Y.", ""], ["Gao", "S. Q.", ""], ["Zhou", "W. X.", ""]]}, {"id": "2005.07593", "submitter": "Aristides Moustakas", "authors": "Aristides Moustakas and Orestis Davlias", "title": "Minimal effect of prescribed burning on fire spread rate and intensity\n  in savanna ecosystems", "comments": "Journal paper published in: Stochastic Environmental Research & Risk\n  Assessment", "journal-ref": null, "doi": "10.1007/s00477-021-01977-3.", "report-no": null, "categories": "q-bio.PE q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fire has been an integral part of the Earth for millennia. Several recent\nwildfires have exhibited an unprecedented spatial and temporal extent and their\ncontrol is beyond national firefighting capabilities. Prescribed or controlled\nburning treatments are debated as a potential measure for ameliorating the\nspread and intensity of wildfires. Machine learning analysis using random\nforests was performed in a spatio-temporal data set comprising a large number\nof savanna fires across 22 years. Results indicate that fire return interval\nwas not an important predictor of fire spread rate or fire intensity, having a\nfeature importance of 3.5%, among eight other predictor variables. Manipulating\nburn seasonality showed a feature importance of 6% or less regarding fire\nspread rate or fire intensity. While manipulated fire return interval and\nseasonality moderated both fire spread rate and intensity, their overall\neffects were low in comparison with meteorological (hydrological and climatic)\nvariables. The variables with the highest feature importance regarding fire\nspread rate resulted in fuel moisture with 21%, relative humidity with 15%,\nwind speed with 14%, and last years rainfall with 14%. The variables with the\nhighest feature importance regarding fire intensity included fuel load with\n21.5%, fuel moisture with 16.5%, relative humidity with 12.5%, air temperature\nwith 12.5%, and rainfall with 12.5%. Predicting fire spread rate and intensity\nhas been a poor endeavour thus far and we show that more data of the variables\nalready monitored would not result in higher predictive accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 15:17:06 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 09:50:56 GMT"}, {"version": "v3", "created": "Fri, 29 Jan 2021 12:34:20 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Moustakas", "Aristides", ""], ["Davlias", "Orestis", ""]]}, {"id": "2005.07742", "submitter": "Daniel Eck", "authors": "Charles Young, David Dalpiaz, Daniel J. Eck", "title": "SEAM methodology for context-rich player matchup evaluations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop the SEAM (synthetic estimated average matchup) method for\ndescribing batter versus pitcher matchups in baseball, both numerically and\nvisually. We first estimate the distribution of balls put into play by a batter\nfacing a pitcher, called the spray chart distribution. This distribution is\nconditional on batter and pitcher characteristics. These characteristics are a\nbetter expression of talent than any conventional statistics. Many individual\nmatchups have a sample size that is too small to be reliable. Synthetic\nversions of the batter and pitcher under consideration are constructed in order\nto alleviate these concerns. Weights governing how much influence these\nsynthetic players have on the overall spray chart distribution are constructed\nto minimize expected mean square error. We then provide novel performance\nmetrics that are calculated as expectations taken with respect to the spray\nchart distribution. These performance metrics provide a context rich approach\nto player evaluation. Our main contribution is a Shiny app that allows users to\nevaluate any batter-pitcher matchup that has occurred or could have occurred in\nthe last five years. One can access this app at\n\\url{https://seam.stat.illinois.edu/app/}. This interactive tool has utility\nfor anyone interested in baseball as well as team executives and players.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 18:58:46 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Young", "Charles", ""], ["Dalpiaz", "David", ""], ["Eck", "Daniel J.", ""]]}, {"id": "2005.07823", "submitter": "Xiaowei Yue", "authors": "Yinhua Liu, Wenzheng Zhao, Rui Sun, Xiaowei Yue", "title": "Optimal Path Planning for Automated Dimensional Inspection of Free-Form\n  Surfaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural dimensional inspection is vital for the process monitoring,\nquality control, and fault diagnosis in the mass production of auto bodies.\nComparing with the non-contact measurement, the high-precision five-axis\nmeasuring machine with the touch-trigger probe is a preferred choice for data\ncollection. It can assist manufacturers in making accurate inspection quickly.\nAs the increase of free-form surfaces and diverse surface orientations in auto\nbody design, existing inspection approaches cannot capture some new critical\nfeatures in the curvature of auto bodies in an efficient way. Therefore, we\nneed to develop new path planning methods for automated dimensional inspection\nof free-form surfaces. This paper proposes an optimal path planning system for\nautomated programming of measuring point inspection by incorporating probe\nrotations and effective collision detection. Specifically, the methodological\ncontributions include: (i) a dynamic searching volume-based algorithm is\ndeveloped to detect potential collisions in the local path between measurement\npoints; (ii) a local path generation method is proposed with the integration of\nthe probe trajectory and the stylus rotation. Then, the inspection time matrix\nis proposed to quantify the measuring time of diverse local paths; (iii) an\noptimization approach of the global inspection path for all critical points on\nthe auto body is developed to minimize the total inspection time. A case study\nhas been conducted on an auto body to verify the performance of the proposed\nmethod. Results show that the collision-free path for the free-form auto body\ncould be generated automatically with off-line programming, and the proposed\nmethod produces about 40% fewer dummy points and needs 32% less movement time\nin the auto body inspection process.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 23:22:23 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Liu", "Yinhua", ""], ["Zhao", "Wenzheng", ""], ["Sun", "Rui", ""], ["Yue", "Xiaowei", ""]]}, {"id": "2005.07880", "submitter": "Kevin Smith", "authors": "Kevin D. Smith, Saber Jafarpour, Ananthram Swami, and Francesco Bullo", "title": "Topology Inference with Multivariate Cumulants: The M\\\"obius Inference\n  Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.NI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many tasks regarding the monitoring, management, and design of communication\nnetworks rely on knowledge of the routing topology. However, the standard\napproach to topology mapping--namely, active probing with traceroutes--relies\non cooperation from increasingly non-cooperative routers, leading to missing\ninformation. Network tomography, which uses end-to-end measurements of additive\nlink metrics (like delays or log packet loss rates) across monitor paths, is a\npossible remedy. Network tomography does not require that routers cooperate\nwith traceroute probes, and it has already been used to infer the structure of\nmulticast trees. This paper goes a step further. We provide a tomographic\nmethod to infer the underlying routing topology of an arbitrary set of monitor\npaths using the joint distribution of end-to-end measurements, without making\nany assumptions on routing behavior. Our approach, called the M\\\"obius\nInference Algorithm (MIA), uses cumulants of this distribution to quantify\nhigh-order interactions among monitor paths, and it applies M\\\"obius inversion\nto \"disentangle\" these interactions. In addition to MIA, we provide a more\npractical variant called Sparse M\\\"obius Inference, which uses various sparsity\nheuristics to reduce the number and order of cumulants required to be\nestimated. We show the viability of our approach using synthetic case studies\nbased on real-world ISP topologies.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 05:56:11 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 17:47:06 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Smith", "Kevin D.", ""], ["Jafarpour", "Saber", ""], ["Swami", "Ananthram", ""], ["Bullo", "Francesco", ""]]}, {"id": "2005.07882", "submitter": "Chandan Singh", "authors": "Nick Altieri, Rebecca L. Barter, James Duncan, Raaz Dwivedi, Karl\n  Kumbier, Xiao Li, Robert Netzorg, Briton Park, Chandan Singh, Yan Shuo Tan,\n  Tiffany Tang, Yu Wang, Chao Zhang, Bin Yu", "title": "Curating a COVID-19 data repository and forecasting county-level death\n  counts in the United States", "comments": "Authors ordered alphabetically. All authors contributed significantly\n  to this work. All collected data, modeling code, forecasts, and\n  visualizations are updated daily and available at\n  \\url{https://github.com/Yu-Group/covid19-severity-prediction}", "journal-ref": "Published in Harvard Data Science Review, 2020", "doi": "10.1162/99608f92.1d4e0dae", "report-no": null, "categories": "stat.AP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the COVID-19 outbreak evolves, accurate forecasting continues to play an\nextremely important role in informing policy decisions. In this paper, we\npresent our continuous curation of a large data repository containing COVID-19\ninformation from a range of sources. We use this data to develop predictions\nand corresponding prediction intervals for the short-term trajectory of\nCOVID-19 cumulative death counts at the county-level in the United States up to\ntwo weeks ahead. Using data from January 22 to June 20, 2020, we develop and\ncombine multiple forecasts using ensembling techniques, resulting in an\nensemble we refer to as Combined Linear and Exponential Predictors (CLEP). Our\nindividual predictors include county-specific exponential and linear\npredictors, a shared exponential predictor that pools data together across\ncounties, an expanded shared exponential predictor that uses data from\nneighboring counties, and a demographics-based shared exponential predictor. We\nuse prediction errors from the past five days to assess the uncertainty of our\ndeath predictions, resulting in generally-applicable prediction intervals,\nMaximum (absolute) Error Prediction Intervals (MEPI). MEPI achieves a coverage\nrate of more than 94% when averaged across counties for predicting cumulative\nrecorded death counts two weeks in the future. Our forecasts are currently\nbeing used by the non-profit organization, Response4Life, to determine the\nmedical supply need for individual hospitals and have directly contributed to\nthe distribution of medical supplies across the country. We hope that our\nforecasts and data repository at https://covidseverity.com can help guide\nnecessary county-specific decision-making and help counties prepare for their\ncontinued fight against COVID-19.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 06:00:28 GMT"}, {"version": "v2", "created": "Sun, 9 Aug 2020 20:54:26 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Altieri", "Nick", ""], ["Barter", "Rebecca L.", ""], ["Duncan", "James", ""], ["Dwivedi", "Raaz", ""], ["Kumbier", "Karl", ""], ["Li", "Xiao", ""], ["Netzorg", "Robert", ""], ["Park", "Briton", ""], ["Singh", "Chandan", ""], ["Tan", "Yan Shuo", ""], ["Tang", "Tiffany", ""], ["Wang", "Yu", ""], ["Zhang", "Chao", ""], ["Yu", "Bin", ""]]}, {"id": "2005.07999", "submitter": "Frederico Fetter", "authors": "Frederico Fetter, Daniel Gamermann, Carolina Brito", "title": "On the stability of the Brazilian presidential regime: a statistical\n  analysis", "comments": null, "journal-ref": null, "doi": "10.1016/j.physa.2021.125832", "report-no": null, "categories": "physics.soc-ph cond-mat.stat-mech stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brazil's presidential system is characterized by the existence of many\npolitical parties that are elected for the Chamber of Deputies and unite in\nlegislative coalitions to form a majority. Since the re-democratization in\n1985, Brazil has had 8 direct presidential elections, among which there were\ntwo impeachments of the elected president. In this work we characterize the\nstability of the presidential regime and the periods of rupture analysing the\nvotes that took place in the Chamber of Deputies from 1991 to 2019. We start by\nmeasuring the cohesion of the parties and the congress in the votes,\nquantifying the agreement between the votes of congressmen and observe that\nthere is a stronger polarization among congressmen during legislative periods\nwhere there was no impeachment, referred to here as stable legislative periods.\nUsing clustering algorithms, we are able to associate these polarized groups\nobserved during the stable periods with the opposition to the government and\ngovernment base. To characterize the impeachment of Dilma Roussef that happened\nin 2016 we analyze how the agreement between congressmen and the government\nevolved over time and identified, using cluster algorithms, that all the\nparties belonging to the majority coalition of the president, except her own\nparty and another one, migrated to the opposition just before the impeachment.\nOur analyses allow us to identify some differences between stable presidential\nperiods and Legislative terms with an impeachment.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 14:19:36 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Fetter", "Frederico", ""], ["Gamermann", "Daniel", ""], ["Brito", "Carolina", ""]]}, {"id": "2005.08005", "submitter": "Stefan Feuerriegel", "authors": "Christof Naumzik, Stefan Feuerriegel", "title": "Forecasting electricity prices with machine learning: Predictor\n  sensitivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Trading on electricity markets occurs such that the price settlement\ntakes place before delivery, often day-ahead. In practice, these prices are\nhighly volatile as they largely depend upon a range of variables such as\nelectricity demand and the feed-in from renewable energy sources. Hence,\naccurate forecasts are demanded.\n  Approach: This paper aims at comparing different predictors stemming from\nsupply-side (solar and wind power generation), demand-side, fuel-related and\neconomic influences. For this reason, we implement a broad range of non-linear\nmodels from machine learning and draw upon the information-fusion-based\nsensitivity analysis.\n  Findings: We disentangle the respective relevance of each predictor. We show\nthat external predictors altogether decrease root mean squared errors by up to\n21.96%. A Diebold-Mariano test statistically proves that the forecasting\naccuracy of the proposed machine learning models is superior.\n  Originality: The benefit of adding further predictors has only recently\nreceived traction; however, little is known about how the individual variables\ncontribute to improving forecasts in machine learning.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 14:45:13 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Naumzik", "Christof", ""], ["Feuerriegel", "Stefan", ""]]}, {"id": "2005.08318", "submitter": "Amir Weiss", "authors": "Amir Weiss", "title": "Blind Direction-of-Arrival Estimation in Acoustic Vector-Sensor Arrays\n  via Tensor Decomposition and Kullback-Leibler Divergence Covariance Fitting", "comments": "Page 5, eq. (19) corrected + some minor text corrections", "journal-ref": null, "doi": "10.1109/TSP.2020.3043814", "report-no": null, "categories": "eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A blind Direction-of-Arrivals (DOAs) estimate of narrowband signals for\nAcoustic Vector-Sensor (AVS) arrays is proposed. Building upon the special\nstructure of the signal measured by an AVS, we show that the covariance matrix\nof all the received signals from the array admits a natural low-rank 4-way\ntensor representation. Thus, rather than estimating the DOAs directly from the\nraw data, our estimate arises from the unique parametric Canonical Polyadic\nDecomposition (CPD) of the observations' Second-Order Statistics (SOSs) tensor.\nBy exploiting results from fundamental statistics and the recently re-emerging\ntensor theory, we derive a consistent blind CPD-based DOAs estimate without\nprior assumptions on the array configuration. We show that this estimate is a\nsolution to an equivalent approximate joint diagonalization problem, and\npropose an ad-hoc iterative solution. Additionally, we derive the Cram\\'er-Rao\nlower bound for Gaussian signals, and use it to derive the iterative Fisher\nscoring algorithm for the computation of the Maximum Likelihood Estimate (MLE)\nin this particular signal model. We then show that the MLE for the Gaussian\nmodel can in fact be used to obtain improved DOAs estimates for non-Gaussian\nsignals as well (under mild conditions), which are optimal under the\nKullback-Leibler divergence covariance fitting criterion, harnessing additional\ninformation encapsulated in the SOSs. Our analytical results are corroborated\nby simulation experiments in various scenarios, which also demonstrate the\nconsiderable improved accuracy w.r.t. a competing state-of-the-art blind\nestimate for AVS arrays, reducing the resulting root mean squared error by up\nto more than an order of magnitude.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 17:48:06 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 15:59:19 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Weiss", "Amir", ""]]}, {"id": "2005.08380", "submitter": "Andy Lau", "authors": "Andy M. Lau, Jurgen Claesen, Kjetil Hansen, Argyris Politis", "title": "Deuteros 2.0: Peptide-level significance testing of data from hydrogen\n  deuterium exchange mass spectrometry", "comments": "Application note with 3 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Summary: Hydrogen deuterium exchange mass spectrometry (HDX-MS) is becoming\nincreasing routine for monitoring changes in the structural dynamics of\nproteins. Differential HDX-MS allows comparison of individual protein states,\nsuch as in the absence or presence of a ligand. This can be used to attribute\nchanges in conformation to binding events, allowing the mapping of entire\ncon-formational networks. As such, the number of necessary cross-state\ncomparisons quickly increas-es as additional states are introduced to the\nsystem of study. There are currently very few software packages available that\noffer quick and informative comparison of HDX-MS datasets and even few-er which\noffer statistical analysis and advanced visualization. Following the feedback\nfrom our origi-nal software Deuteros, we present Deuteros 2.0 which has been\nredesigned from the ground up to fulfil a greater role in the HDX-MS analysis\npipeline. Deuteros 2.0 features a repertoire of facilities for back exchange\ncorrection, data summarization, peptide-level statistical analysis and advanced\ndata plotting features. Availability: Deuteros 2.0 can be downloaded from\nhttps://github.com/andymlau/Deuteros_2.0 under the Apache 2.0 license.\nInstallation of Deuteros 2.0 requires the MATLAB Runtime Library available free\nof charge from MathWorks\n(https://www.mathworks.com/products/compiler/matlab-runtime.html) and is\navailable for both Windows and Mac operating systems.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 22:01:10 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Lau", "Andy M.", ""], ["Claesen", "Jurgen", ""], ["Hansen", "Kjetil", ""], ["Politis", "Argyris", ""]]}, {"id": "2005.08459", "submitter": "Harlan Campbell", "authors": "Harlan Campbell, Perry de Valpine, Lauren Maxwell, Valentijn MT de\n  Jong, Thomas Debray, Thomas J\\\"anisch, Paul Gustafson", "title": "Bayesian adjustment for preferential testing in estimating the COVID-19\n  infection fatality rate", "comments": "53 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key challenge in estimating the infection fatality rate (IFR) -- and its\nrelation with various factors of interest -- is determining the total number of\ncases. The total number of cases is not known because not everyone is tested,\nbut also, more importantly, because tested individuals are not representative\nof the population at large. We refer to the phenomenon whereby infected\nindividuals are more likely to be tested than non-infected individuals, as\n\"preferential testing.\" An open question is whether or not it is possible to\nreliably estimate the IFR without any specific knowledge about the degree to\nwhich the data are biased by preferential testing. In this paper we take a\npartial identifiability approach, formulating clearly where deliberate prior\nassumptions can be made and presenting a Bayesian model which pools information\nfrom different samples. When the model is fit to European data obtained from\nseroprevalence studies and national official COVID-19 statistics, we estimate\nthe overall COVID-19 IFR for Europe to be 0.53%, 95% C.I. = [0.39%, 0.69%].\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 05:00:46 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2020 20:52:44 GMT"}, {"version": "v3", "created": "Sat, 29 Aug 2020 19:36:18 GMT"}, {"version": "v4", "created": "Wed, 10 Mar 2021 18:56:36 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Campbell", "Harlan", ""], ["de Valpine", "Perry", ""], ["Maxwell", "Lauren", ""], ["de Jong", "Valentijn MT", ""], ["Debray", "Thomas", ""], ["J\u00e4nisch", "Thomas", ""], ["Gustafson", "Paul", ""]]}, {"id": "2005.08583", "submitter": "Ben Moews", "authors": "Ben Moews, Morgan A. Schmitz, Andrew J. Lawler, Joe Zuntz, Alex I.\n  Malz, Rafael S. de Souza, Ricardo Vilalta, Alberto Krone-Martins, Emille E.\n  O. Ishida (for the COIN Collaboration)", "title": "Ridges in the Dark Energy Survey for cosmic trough identification", "comments": "12 pages, 5 figures, preprint submitted to MNRAS", "journal-ref": null, "doi": "10.1093/mnras/staa3204", "report-no": null, "categories": "astro-ph.CO astro-ph.IM stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cosmic voids and their corresponding redshift-aggregated projections of mass\ndensities, known as troughs, play an important role in our attempt to model the\nlarge-scale structure of the Universe. Understanding these structures leads to\ntests comparing the standard model with alternative cosmologies, constraints on\nthe dark energy equation of state, and provides evidence to differentiate among\ngravitational theories. In this paper, we extend the subspace-constrained mean\nshift algorithm, a recently introduced method to estimate density ridges, and\napply it to 2D weak-lensing mass density maps from the Dark Energy Survey Y1\ndata release to identify curvilinear filamentary structures. We compare the\nobtained ridges with previous approaches to extract trough structure in the\nsame data, and apply curvelets as an alternative wavelet-based method to\nconstrain densities. We then invoke the Wasserstein distance between noisy and\nnoiseless simulations to validate the denoising capabilities of our method. Our\nresults demonstrate the viability of ridge estimation as a precursor for\ndenoising weak lensing quantities to recover the large-scale structure, paving\nthe way for a more versatile and effective search for troughs.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 10:48:55 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Moews", "Ben", "", "for the COIN Collaboration"], ["Schmitz", "Morgan A.", "", "for the COIN Collaboration"], ["Lawler", "Andrew J.", "", "for the COIN Collaboration"], ["Zuntz", "Joe", "", "for the COIN Collaboration"], ["Malz", "Alex I.", "", "for the COIN Collaboration"], ["de Souza", "Rafael S.", "", "for the COIN Collaboration"], ["Vilalta", "Ricardo", "", "for the COIN Collaboration"], ["Krone-Martins", "Alberto", "", "for the COIN Collaboration"], ["Ishida", "Emille E. O.", "", "for the COIN Collaboration"]]}, {"id": "2005.08652", "submitter": "Abhinav Prakash", "authors": "Yu Ding, Nitesh Kumar, Abhinav Prakash, Adaiyibo E. Kio, Xin Liu, Lei\n  Liu, Qingchang Li", "title": "A Case Study of Space-time Performance Comparison of Wind Turbines on a\n  Wind Farm", "comments": "31 pages, 9 figures", "journal-ref": "Renewable Energy, 171, 735-746 (2021)", "doi": "10.1016/j.renene.2021.02.136", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an academia-industry joint case study, which was\nconducted to quantify and compare multi-year changes in power production\nperformance of multiple turbines scattered over a mid-size wind farm. This\nanalysis is referred to as a space-time performance comparison. One key aspect\nin power performance analysis is to have the wind and environmental inputs\ncontrolled for. This research employs, in a sequential fashion, two principal\nmodeling components to exercise tight control of multiple input conditions -- a\ncovariate matching method, followed by a Gaussian process model-based\nfunctional comparison. The analysis method is applied to a wind farm that\nhouses 66 turbines on a moderately complex terrain. The power production and\nenvironmental data span nearly four years, during which period the turbines\nhave gone through multiple technical upgrades. The space-time analysis presents\na quantitative and global picture showing how turbines differ relative to each\nother as well as how each of them changes over time.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 12:36:02 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2020 21:18:16 GMT"}, {"version": "v3", "created": "Fri, 19 Feb 2021 01:33:05 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Ding", "Yu", ""], ["Kumar", "Nitesh", ""], ["Prakash", "Abhinav", ""], ["Kio", "Adaiyibo E.", ""], ["Liu", "Xin", ""], ["Liu", "Lei", ""], ["Li", "Qingchang", ""]]}, {"id": "2005.08665", "submitter": "Shixiang Zhu", "authors": "Shixiang Zhu, Ruyi Ding, Minghe Zhang, Pascal Van Hentenryck, Yao Xie", "title": "Spatio-Temporal Point Processes with Attention for Traffic Congestion\n  Event Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel framework for modeling traffic congestion events over road\nnetworks. Using multi-modal data by combining count data from traffic sensors\nwith police reports that report traffic incidents, we aim to capture two types\nof triggering effect for congestion events. Current traffic congestion at one\nlocation may cause future congestion over the road network, and traffic\nincidents may cause spread traffic congestion. To model the non-homogeneous\ntemporal dependence of the event on the past, we use a novel attention-based\nmechanism based on neural networks embedding for point processes. To\nincorporate the directional spatial dependence induced by the road network, we\nadapt the \"tail-up\" model from the context of spatial statistics to the traffic\nnetwork setting. We demonstrate our approach's superior performance compared to\nthe state-of-the-art methods for both synthetic and real data.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 04:22:18 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 19:54:08 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Zhu", "Shixiang", ""], ["Ding", "Ruyi", ""], ["Zhang", "Minghe", ""], ["Van Hentenryck", "Pascal", ""], ["Xie", "Yao", ""]]}, {"id": "2005.08837", "submitter": "Ahmed Alaa", "authors": "Zhaozhi Qian, Ahmed M. Alaa, Mihaela van der Schaar", "title": "When and How to Lift the Lockdown? Global COVID-19 Scenario Analysis and\n  Policy Assessment using Compartmental Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The coronavirus disease 2019 (COVID-19) global pandemic has led many\ncountries to impose unprecedented lockdown measures in order to slow down the\noutbreak. Questions on whether governments have acted promptly enough, and\nwhether lockdown measures can be lifted soon have since been central in public\ndiscourse. Data-driven models that predict COVID-19 fatalities under different\nlockdown policy scenarios are essential for addressing these questions and\ninforming governments on future policy directions. To this end, this paper\ndevelops a Bayesian model for predicting the effects of COVID-19 lockdown\npolicies in a global context -- we treat each country as a distinct data point,\nand exploit variations of policies across countries to learn country-specific\npolicy effects. Our model utilizes a two-layer Gaussian process (GP) prior --\nthe lower layer uses a compartmental SEIR (Susceptible, Exposed, Infected,\nRecovered) model as a prior mean function with \"country-and-policy-specific\"\nparameters that capture fatality curves under \"counterfactual\" policies within\neach country, whereas the upper layer is shared across all countries, and\nlearns lower-layer SEIR parameters as a function of a country's features and\nits policy indicators. Our model combines the solid mechanistic foundations of\nSEIR models (Bayesian priors) with the flexible data-driven modeling and\ngradient-based optimization routines of machine learning (Bayesian posteriors)\n-- i.e., the entire model is trained end-to-end via stochastic variational\ninference. We compare the projections of COVID-19 fatalities by our model with\nother models listed by the Center for Disease Control (CDC), and provide\nscenario analyses for various lockdown and reopening strategies highlighting\ntheir impact on COVID-19 fatalities.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 18:21:50 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2020 16:55:22 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Qian", "Zhaozhi", ""], ["Alaa", "Ahmed M.", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "2005.09018", "submitter": "Claudio Heinrich", "authors": "Claudio Heinrich", "title": "On the number of bins in a rank histogram", "comments": null, "journal-ref": null, "doi": "10.1002/qj.3932", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rank histograms are popular tools for assessing the reliability of\nmeteorological ensemble forecast systems. A reliable forecast system leads to a\nuniform rank histogram, and deviations from uniformity can indicate\nmiscalibrations. However, the ability to identify such deviations by visual\ninspection of rank histogram plots crucially depends on the number of bins\nchosen for the histogram. If too few bins are chosen, the rank histogram is\nlikely to miss miscalibrations; if too many are chosen, even perfectly\ncalibrated forecast systems can yield rank histograms that do not appear\nuniform. In this paper we address this trade-off and propose a method for\nchoosing the number of bins for a rank histogram. The goal of our method is to\nselect a number of bins such that the intuitive decision whether a histogram is\nuniform or not is as close as possible to a formal statistical test. Our\nresults indicate that it is often appropriate to choose fewer bins than the\nusual choice of ensemble size plus one, especially when the number of\nobservations available for verification is small.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 18:26:47 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 08:12:28 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Heinrich", "Claudio", ""]]}, {"id": "2005.09024", "submitter": "Erin Schliep", "authors": "Erin M. Schliep, Toryn L.J. Schafer, Matthew Hawkey", "title": "Distributed lag models to identify the cumulative effects of training\n  and recovery in athletes using multivariate ordinal wellness data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subjective wellness data can provide important information on the well-being\nof athletes and be used to maximize player performance and detect and prevent\nagainst injury. Wellness data, which are often ordinal and multivariate,\ninclude metrics relating to the physical, mental, and emotional status of the\nathlete. Training and recovery can have significant short- and long-term\neffects on athlete wellness, and these effects can vary across individual. We\ndevelop a joint multivariate latent factor model for ordinal response data to\ninvestigate the effects of training and recovery on athlete wellness. We use a\nlatent factor distributed lag model to capture the cumulative effects of\ntraining and recovery through time. Current efforts using subjective wellness\ndata have averaged over these metrics to create a univariate summary of\nwellness, however this approach can mask important information in the data. Our\nmultivariate model leverages each ordinal variable and can be used to identify\nthe relative importance of each in monitoring athlete wellness. The model is\napplied to athlete daily wellness, training, and recovery data collected across\ntwo Major League Soccer seasons.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 18:45:20 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Schliep", "Erin M.", ""], ["Schafer", "Toryn L. J.", ""], ["Hawkey", "Matthew", ""]]}, {"id": "2005.09043", "submitter": "Naz Gul Ms", "authors": "Naz Gul, Nosheen Faiz, Dan Brawn, Rafal Kulakowski, Zardad Khan and\n  Berthold Lausen", "title": "Optimal survival trees ensemble", "comments": "The paper is 24 pages long with 27 figures and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have adopted an approach of selecting accurate and diverse\ntrees based on individual or collective performance within an ensemble for\nclassification and regression problems. This work follows in the wake of these\ninvestigations and considers the possibility of growing a forest of optimal\nsurvival trees. Initially, a large set of survival trees are grown using the\nmethod of random survival forest. The grown trees are then ranked from smallest\nto highest value of their prediction error using out-of-bag observations for\neach respective survival tree. The top ranked survival trees are then assessed\nfor their collective performance as an ensemble. This ensemble is initiated\nwith the survival tree which stands first in rank, then further trees are\ntested one by one by adding them to the ensemble in order of rank. A survival\ntree is selected for the resultant ensemble if the performance improves after\nan assessment using independent training data. This ensemble is called an\noptimal trees ensemble (OSTE). The proposed method is assessed using 17\nbenchmark datasets and the results are compared with those of random survival\nforest, conditional inference forest, bagging and a non tree based method, the\nCox proportional hazard model. In addition to improve predictive performance,\nthe proposed method reduces the number of survival trees in the ensemble as\ncompared to the other tree based methods. The method is implemented in an R\npackage called \"OSTE\".\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 19:28:16 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Gul", "Naz", ""], ["Faiz", "Nosheen", ""], ["Brawn", "Dan", ""], ["Kulakowski", "Rafal", ""], ["Khan", "Zardad", ""], ["Lausen", "Berthold", ""]]}, {"id": "2005.09130", "submitter": "Shuxi Zeng", "authors": "Shuxi Zeng, Fan Li and Peng Ding", "title": "Is being an only child harmful to psychological health?: Evidence from\n  an instrumental variable analysis of China's One-Child Policy", "comments": "33 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper evaluates the effects of being an only child in a family on\npsychological health, leveraging data on the One-Child Policy in China. We use\nan instrumental variable approach to address the potential unmeasured\nconfounding between the fertility decision and psychological health, where the\ninstrumental variable is an index on the intensity of the implementation of the\nOne-Child Policy. We establish an analytical link between the local\ninstrumental variable approach and principal stratification to accommodate the\ncontinuous instrumental variable. Within the principal stratification\nframework, we postulate a Bayesian hierarchical model to infer various causal\nestimands of policy interest while adjusting for the clustering data structure.\nWe apply the method to the data from the China Family Panel Studies and find\nsmall but statistically significant negative effects of being an only child on\nself-reported psychological health for some subpopulations. Our analysis\nreveals treatment effect heterogeneity with respect to both observed and\nunobserved characteristics. In particular, urban males suffer the most from\nbeing only children, and the negative effect has larger magnitude if the\nfamilies were more resistant to the One-Child Policy. We also conduct\nsensitivity analysis to assess the key instrumental variable assumption.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 23:13:29 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 06:48:49 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Zeng", "Shuxi", ""], ["Li", "Fan", ""], ["Ding", "Peng", ""]]}, {"id": "2005.09145", "submitter": "Yunyi Zhang", "authors": "Yunyi Zhang, Dimitris N. Politis", "title": "Bootstrap prediction intervals with asymptotic conditional validity and\n  unconditional guarantees", "comments": "27 pages and 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It can be argued that optimal prediction should take into account all\navailable data. Therefore, to evaluate a prediction interval's performance one\nshould employ conditional coverage probability, conditioning on all available\nobservations. Focusing on a linear model, we derive the asymptotic distribution\nof the difference between the conditional coverage probability of a nominal\nprediction interval and the conditional coverage probability of a prediction\ninterval obtained via a residual-based bootstrap. Applying this result, we show\nthat a prediction interval generated by the residual-based bootstrap has\napproximately 50% probability to yield conditional under-coverage. We then\ndevelop a new bootstrap algorithm that generates a prediction interval that\nasymptotically controls both the conditional coverage probability as well as\nthe possibility of conditional under-coverage. We complement the asymptotic\nresults with several finite-sample simulations.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 00:20:10 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2021 17:51:50 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Zhang", "Yunyi", ""], ["Politis", "Dimitris N.", ""]]}, {"id": "2005.09152", "submitter": "Anqi Dong", "authors": "Anqi Dong, Amirhossein Taghvaei, Tryphon T. Georgiou", "title": "Lasso formulation of the shortest path problem", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS math.ST stat.AP stat.CO stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The shortest path problem is formulated as an $l_1$-regularized regression\nproblem, known as lasso. Based on this formulation, a connection is established\nbetween Dijkstra's shortest path algorithm and the least angle regression\n(LARS) for the lasso problem. Specifically, the solution path of the lasso\nproblem, obtained by varying the regularization parameter from infinity to zero\n(the regularization path), corresponds to shortest path trees that appear in\nthe bi-directional Dijkstra algorithm. Although Dijkstra's algorithm and the\nLARS formulation provide exact solutions, they become impractical when the size\nof the graph is exceedingly large. To overcome this issue, the alternating\ndirection method of multipliers (ADMM) is proposed to solve the lasso\nformulation. The resulting algorithm produces good and fast approximations of\nthe shortest path by sacrificing exactness that may not be absolutely essential\nin many applications. Numerical experiments are provided to illustrate the\nperformance of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 01:16:01 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 18:04:43 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Dong", "Anqi", ""], ["Taghvaei", "Amirhossein", ""], ["Georgiou", "Tryphon T.", ""]]}, {"id": "2005.09210", "submitter": "Kyle Messier", "authors": "Kyle P Messier and Matthias Katzfuss", "title": "Scalable penalized spatiotemporal land-use regression for ground-level\n  nitrogen dioxide", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nitrogen dioxide (NO$_2$) is a primary constituent of traffic-related air\npollution and has well established harmful environmental and human-health\nimpacts. Knowledge of the spatiotemporal distribution of NO$_2$ is critical for\nexposure and risk assessment. A common approach for assessing air pollution\nexposure is linear regression involving spatially referenced covariates, known\nas land-use regression (LUR). We develop a scalable approach for simultaneous\nvariable selection and estimation of LUR models with spatiotemporally\ncorrelated errors, by combining a general-Vecchia Gaussian process\napproximation with a penalty on the LUR coefficients. In comparisons to\nexisting methods using simulated data, our approach resulted in higher\nmodel-selection specificity and sensitivity and in better prediction in terms\nof calibration and sharpness, for a wide range of relevant settings. In our\nspatiotemporal analysis of daily, US-wide, ground-level NO$_2$ data, our\napproach was more accurate, and produced a sparser and more interpretable\nmodel. Our daily predictions elucidate spatiotemporal patterns of NO$_2$\nconcentrations across the United States, including significant variations\nbetween cities and intra-urban variation. Thus, our predictions will be useful\nfor epidemiological and risk-assessment studies seeking daily, national-scale\npredictions, and they can be used in acute-outcome health-risk assessments.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 04:33:10 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 20:26:44 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Messier", "Kyle P", ""], ["Katzfuss", "Matthias", ""]]}, {"id": "2005.09365", "submitter": "Peter Green", "authors": "Peter J. Green, Julia Mortera", "title": "Inference about complex relationships using peak height data from DNA\n  mixtures", "comments": "29 pages, 12 figures, 20 tables; V2 has different casework examples,\n  and general minor edits; V3 has general edits following review, including\n  lengthier exposition; V4 has further explanation, and a supplementary\n  appendix on related software; V5 corrects typo, updates references and has\n  new version of Fig 5", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In both criminal cases and civil cases there is an increasing demand for the\nanalysis of DNA mixtures involving relationships. The goal might be, for\nexample, to identify the contributors to a DNA mixture where the donors may be\nrelated, or to infer the relationship between individuals based on a mixture.\n  This paper introduces an approach to modelling and computation for DNA\nmixtures involving contributors with arbitrarily complex relationships. It\nbuilds on an extension of Jacquard's condensed coefficients of identity, to\nspecify and compute with joint relationships, not only pairwise ones, including\nthe possibility of inbreeding. The methodology developed is applied to two\ncasework examples involving a missing person, and simulation studies of\nperformance, in which the ability of the methodology to recover complex\nrelationship information from synthetic data with known `true' family structure\nis examined.\n  The methods used to analyse the examples are implemented in the new KinMix R\npackage, that extends the DNAmixtures package to allow for modelling DNA\nmixtures with related contributors. KinMix inherits from DNAmixtures the\ncapacity to deal with mixtures with many contributors, in a time- and\nspace-efficient way.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 11:17:13 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2020 18:29:59 GMT"}, {"version": "v3", "created": "Tue, 10 Nov 2020 17:58:04 GMT"}, {"version": "v4", "created": "Tue, 9 Feb 2021 17:13:01 GMT"}, {"version": "v5", "created": "Wed, 21 Apr 2021 14:52:14 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Green", "Peter J.", ""], ["Mortera", "Julia", ""]]}, {"id": "2005.09381", "submitter": "Pablo Dorta-Gonzalez", "authors": "Pablo Dorta-Gonz\\'alez, Rafael Su\\'arez-Vega, Mar\\'ia Isabel\n  Dorta-Gonz\\'alez", "title": "Open Access effect on uncitedness: A large-scale study controlling by\n  discipline, source type and visibility", "comments": "33 pages, 10 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There are many factors that affect the probability of being uncited during\nthe first years after publication. In this study, we analyze three of these\nfactors for journals, conference proceedings and book series: the field (in 316\nsubject categories of the Scopus database), the access modality (open access\nvs. paywalled), and the visibility of the source (through the percentile of the\naverage impact in the subject category). We quantify the effect of these\nfactors on the probability of being uncited. This probability is measured\nthrough the percentage of uncited documents in the serial sources of the Scopus\ndatabase at about two years after publication. As a main result, we do not find\nany strong correlation between open access and uncitedness. Within the group of\nmost cited journals (Q1 and top 10%), open access journals generally have\nsomewhat lower uncited rates. However, in the intermediate quartiles (Q2 and\nQ3) almost no differences are observed, while for Q4 the uncited rate is again\nsomewhat lower in the case of the OA group. This is important because it\nprovides new evidence in the debate about open access citation advantage.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 12:14:31 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Dorta-Gonz\u00e1lez", "Pablo", ""], ["Su\u00e1rez-Vega", "Rafael", ""], ["Dorta-Gonz\u00e1lez", "Mar\u00eda Isabel", ""]]}, {"id": "2005.09396", "submitter": "Giacomo De Nicola", "authors": "Giacomo De Nicola, Benjamin Sischka and G\\\"oran Kauermann", "title": "Mixture Models and Networks -- Overview of Stochastic Blockmodelling", "comments": "23 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture models are probabilistic models aimed at uncovering and representing\nlatent subgroups within a population. In the realm of network data analysis,\nthe latent subgroups of nodes are typically identified by their connectivity\nbehaviour, with nodes behaving similarly belonging to the same community. In\nthis context, mixture modelling is pursued through stochastic blockmodelling.\nWe consider stochastic blockmodels and some of their variants and extensions\nfrom a mixture modelling perspective. We also survey some of the main classes\nof estimation methods available, and propose an alternative approach. In\naddition to the discussion of inferential properties and estimating procedures,\nwe focus on the application of the models to several real-world network\ndatasets, showcasing the advantages and pitfalls of different approaches.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 12:42:21 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 17:19:28 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["De Nicola", "Giacomo", ""], ["Sischka", "Benjamin", ""], ["Kauermann", "G\u00f6ran", ""]]}, {"id": "2005.09616", "submitter": "Raunak Shrestha", "authors": "Ratna K. Shrestha, Raunak Shrestha", "title": "Group segmentation and heterogeneity in the choice of cooking fuels in\n  post-earthquake Nepal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Segmenting population into subgroups with higher intergroup, but lower\nintragroup, heterogeneity can be useful in enhancing the effectiveness of many\nsocio-economic policy interventions; yet it has received little attention in\npromoting clean cooking. Here, we use PERMANOVA, a distance-based multivariate\nanalysis, to identify the factor that captures the highest intergroup\nheterogeneity in the choice of cooking fuels. Applying this approach to the\npost-earthquake data on 747,137 households from Nepal, we find that ethnicity\nexplains 39.12% of variation in fuel choice, followed by income (26.30%),\neducation (12.62%), and location (4.05%). This finding indicates that\nethnicity, rather than income or other factors, as a basis of policy\ninterventions may be more effective in promoting clean cooking. We also find\nthat, among the ethnic groups in Nepal, the most marginalized Chepang/Thami\ncommunity exhibits the lowest intragroup diversity (Shannon index = 0.101)\nwhile Newars the highest (0.667). This information on intra-ethnic diversity in\nfuel choice can have important policy implications for reducing ethnic gap in\nclean cooking.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 23:17:17 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Shrestha", "Ratna K.", ""], ["Shrestha", "Raunak", ""]]}, {"id": "2005.09738", "submitter": "Yun Li", "authors": "Yun Li, Douglas E. Schaubel, Kevin He", "title": "Matching methods for obtaining survival functions to estimate the effect\n  of a time-dependent treatment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In observational studies of survival time featuring a binary time-dependent\ntreatment, the hazard ratio (an instantaneous measure) is often used to\nrepresent the treatment effect. However, investigators are often more\ninterested in the difference in survival functions. We propose semiparametric\nmethods to estimate the causal effect of treatment among the treated with\nrespect to survival probability. The objective is to compare post-treatment\nsurvival with the survival function that would have been observed in the\nabsence of treatment. For each patient, we compute a prognostic score (based on\nthe pre-treatment death hazard) and a propensity score (based on the treatment\nhazard). Each treated patient is then matched with an alive, uncensored and\nnot-yet-treated patient with similar prognostic and/or propensity scores. The\nexperience of each treated and matched patient is weighted using a variant of\nInverse Probability of Censoring Weighting to account for the impact of\ncensoring. We propose estimators of the treatment-specific survival functions\n(and their difference), computed through weighted Nelson-Aalen estimators.\nClosed-form variance estimators are proposed which take into consideration the\npotential replication of subjects across matched sets. The proposed methods are\nevaluated through simulation, then applied to estimate the effect of kidney\ntransplantation on survival among end-stage renal disease patients using data\nfrom a national organ failure registry.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 20:22:54 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Li", "Yun", ""], ["Schaubel", "Douglas E.", ""], ["He", "Kevin", ""]]}, {"id": "2005.09780", "submitter": "Yun Li", "authors": "Yun Li, Yoonseok Lee, Friedrich K Port and Bruce M Robinson", "title": "The Impact of Unmeasured Within- and Between-Cluster Confounding on the\n  Bias of Effect Estimators from Fixed Effect, Mixed effect and Instrumental\n  Variable Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instrumental variable methods are popular choices in combating unmeasured\nconfounding to obtain less biased effect estimates. However, we demonstrate\nthat alternative methods may give less biased estimates depending on the nature\nof unmeasured confounding. Treatment preferences of clusters (e.g., physician\npractices) are the most f6requently used instruments in instrumental variable\nanalyses (IVA). These preference-based IVAs are usually conducted on data\nclustered by region, hospital/facility, or physician, where unmeasured\nconfounding often occurs within or between clusters. We aim to quantify the\nimpact of unmeasured confounding on the bias of effect estimators in IVA, as\nwell as alternative methods including ordinary least squares regression, linear\nmixed models (LMM) and fixed effect models (FE) to study the effect of a\ncontinuous exposure (e.g., treatment dose). We derive bias formulae of\nestimators from these four methods in the presence of unmeasured within- and/or\nbetween-cluster confounders. We show that IVAs can provide consistent estimates\nwhen unmeasured within-cluster confounding exists, but not when between-cluster\nconfounding exists. On the other hand, FEs and LMMs can provide consistent\nestimates when unmeasured between-cluster confounding exits, but not for\nwithin-cluster confounding. Whether IVAs are advantageous in reducing bias over\nFEs and LMMs depends on the extent of unmeasured within-cluster confounding\nrelative to between-cluster confounding. Furthermore, the impact of unmeasured\nbetween-cluster confounding on IVA estimates is larger than the impact of\nunmeasured within-cluster confounding on FE and LMM estimates. We illustrate\nthese methods through data applications. Our findings provide guidance for\nchoosing appropriate methods to combat the dominant types of unmeasured\nconfounders and help interpret statistical results in the context of unmeasured\nconfounding\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 22:45:28 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Li", "Yun", ""], ["Lee", "Yoonseok", ""], ["Port", "Friedrich K", ""], ["Robinson", "Bruce M", ""]]}, {"id": "2005.09955", "submitter": "Muhammad Adnan Dr.", "authors": "Shiraz Ahmed, Muhammad Adnan, Davy Janssens, Geert Wets", "title": "A Route to School Informational Intervention for Air Pollution Exposure\n  Reduction", "comments": "Post-Print (Author accepted Version)", "journal-ref": "Sustainable Cities and Society, vol 53, 101965 (2020)", "doi": "10.1016/j.scs.2019.101965", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Walking and cycling are promoted to encourage sustainable travel behavior\namong children and adults. School children during their travel episode\nto-and-from school are disproportionately exposed to air pollution due to\nmultiple reasons such as proximity to high traffic roads and peak volumes. This\npaper presents a route to school informational intervention that was developed\nincorporating approaches and methods suggested in the literature for effective\nbehavioral interventions. The intervention was implemented using escorting\nparents/guardians (N=104) of school children of Antwerp, Belgium to adopt\nschool routes with least exposure to pollutants. Collected data and its\nanalysis revealed that 60% participants (N= 62) could benefit themselves by\nadopting the suggested cleanest routes to school, of whom a significant\nproportion of participants (i.e. 34%, N= 35) have a difference of average NO2\nconcentration between the alternative and current route of around 10{\\mu}g/m3.\nThis information about alternatives routes with their potential benefits was\npresented to each participant via defined study protocols. 18 Based on the\nfeedback of participants that could potentially adopt suggested alternatives,\n77% (N=48) have switched their routes. These results indicated that\nintervention was effective, and it can bring higher benefits when implemented\non a wider scale.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 10:47:37 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Ahmed", "Shiraz", ""], ["Adnan", "Muhammad", ""], ["Janssens", "Davy", ""], ["Wets", "Geert", ""]]}, {"id": "2005.09981", "submitter": "Daisuke Murakami", "authors": "Daisuke Murakami, Daniel A. Griffith", "title": "Balancing spatial and non-spatial variation in varying coefficient\n  modeling: a remedy for spurious correlation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study discusses the importance of balancing spatial and non-spatial\nvariation in spatial regression modeling. Unlike spatially varying coefficients\n(SVC) modeling, which is popular in spatial statistics, non-spatially varying\ncoefficients (NVC) modeling has largely been unexplored in spatial fields.\nNevertheless, as we will explain, consideration of non-spatial variation is\nneeded not only to improve model accuracy but also to reduce spurious\ncorrelation among varying coefficients, which is a major problem in SVC\nmodeling. We consider a Moran eigenvector approach modeling spatially and\nnon-spatially varying coefficients (S&NVC). A Monte Carlo simulation experiment\ncomparing our S&NVC model with existing SVC models suggests both modeling\naccuracy and computational efficiency for our approach. Beyond that, somewhat\nsurprisingly, our approach identifies true and spurious correlations among\ncoefficients nearly perfectly, even when usual SVC models suffer from severe\nspurious correlations. It implies that S&NVC model should be used even when the\nanalysis purpose is modeling SVCs. Finally, our S&NVC model is employed to\nanalyze a residential land price dataset. Its results suggest existence of both\nspatial and non-spatial variation in regression coefficients in practice. The\nS&NVC model is now implemented in the R package spmoran.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 11:53:10 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 09:50:35 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Murakami", "Daisuke", ""], ["Griffith", "Daniel A.", ""]]}, {"id": "2005.10011", "submitter": "Kevin Wilson Dr", "authors": "Sara Graziadio and Kevin J. Wilson", "title": "Uncertainty representation for early phase clinical test evaluations: a\n  case study", "comments": "22 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In early clinical test evaluations the potential benefits of the introduction\nof a new technology into the healthcare system are assessed in the challenging\nsituation of limited available empirical data. The aim of these evaluations is\nto provide additional evidence for the decision maker, who is typically a\nfunder or the company developing the test, to evaluate which technologies\nshould progress to the next stage of evaluation. In this paper we consider the\nevaluation of a diagnostic test for patients suffering from Chronic Obstructive\nPulmonary Disease (COPD). We describe the use of graphical models, prior\nelicitation and uncertainty analysis to provide the required evidence to allow\nthe test to progress to the next stage of evaluation. We specifically discuss\ninferring an influence diagram from a care pathway and conducting an\nelicitation exercise to allow specification of prior distributions over all\nmodel parameters. We describe the uncertainty analysis, via Monte Carlo\nsimulation, which allowed us to demonstrate that the potential value of the\ntest was robust to uncertainties. This paper provides a case study illustrating\nhow a careful Bayesian analysis can be used to enhance early clinical test\nevaluations.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 12:50:58 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Graziadio", "Sara", ""], ["Wilson", "Kevin J.", ""]]}, {"id": "2005.10092", "submitter": "Matteo Fasiolo", "authors": "Christian Capezza, Biagio Palumbo, Yannig Goude, Simon N. Wood and\n  Matteo Fasiolo", "title": "Additive stacking for disaggregate electricity demand forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future grid management systems will coordinate distributed production and\nstorage resources to manage, in a cost effective fashion, the increased load\nand variability brought by the electrification of transportation and by a\nhigher share of weather dependent production. Electricity demand forecasts at a\nlow level of aggregation will be key inputs for such systems. We focus on\nforecasting demand at the individual household level, which is more challenging\nthan forecasting aggregate demand, due to the lower signal-to-noise ratio and\nto the heterogeneity of consumption patterns across households. We propose a\nnew ensemble method for probabilistic forecasting, which borrows strength\nacross the households while accommodating their individual idiosyncrasies. In\nparticular, we develop a set of models or 'experts' which capture different\ndemand dynamics and we fit each of them to the data from each household. Then\nwe construct an aggregation of experts where the ensemble weights are estimated\non the whole data set, the main innovation being that we let the weights vary\nwith the covariates by adopting an additive model structure. In particular, the\nproposed aggregation method is an extension of regression stacking (Breiman,\n1996) where the mixture weights are modelled using linear combinations of\nparametric, smooth or random effects. The methods for building and fitting\nadditive stacking models are implemented by the gamFactory R package, available\nat https://github.com/mfasiolo/gamFactory.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 14:54:22 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Capezza", "Christian", ""], ["Palumbo", "Biagio", ""], ["Goude", "Yannig", ""], ["Wood", "Simon N.", ""], ["Fasiolo", "Matteo", ""]]}, {"id": "2005.10123", "submitter": "Andrew Holbrook", "authors": "Andrew J. Holbrook, Charles E. Loeffler, Seth R. Flaxman, Marc A.\n  Suchard", "title": "Scalable Bayesian inference for self-excitatory stochastic processes\n  applied to big American gunfire data", "comments": "Submitted to Statistics and Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Hawkes process and its extensions effectively model self-excitatory\nphenomena including earthquakes, viral pandemics, financial transactions,\nneural spike trains and the spread of memes through social networks. The\nusefulness of these stochastic process models within a host of economic sectors\nand scientific disciplines is undercut by the processes' computational burden:\ncomplexity of likelihood evaluations grows quadratically in the number of\nobservations for both the temporal and spatiotemporal Hawkes processes. We show\nthat, with care, one may parallelize these calculations using both central and\ngraphics processing unit implementations to achieve over 100-fold speedups over\nsingle-core processing. Using a simple adaptive Metropolis-Hastings scheme, we\napply our high-performance computing framework to a Bayesian analysis of big\ngunshot data generated in Washington D.C. between the years of 2006 and 2019,\nthereby extending a past analysis of the same data from under 10,000 to over\n85,000 observations. To encourage wide-spread use, we provide hpHawkes, an\nopen-source R package, and discuss high-level implementation and program design\nfor leveraging aspects of computational hardware that become necessary in a big\ndata setting.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 20:14:42 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Holbrook", "Andrew J.", ""], ["Loeffler", "Charles E.", ""], ["Flaxman", "Seth R.", ""], ["Suchard", "Marc A.", ""]]}, {"id": "2005.10125", "submitter": "Mariflor Vega-Carrasco", "authors": "Mariflor Vega-Carrasco, Jason O'sullivan, Rosie Prior, Ioanna\n  Manolopoulou, Mirco Musolesi", "title": "Modelling Grocery Retail Topic Distributions: Evaluation,\n  Interpretability and Stability", "comments": "20 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CL stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the shopping motivations behind market baskets has high\ncommercial value in the grocery retail industry. Analyzing shopping\ntransactions demands techniques that can cope with the volume and\ndimensionality of grocery transactional data while keeping interpretable\noutcomes. Latent Dirichlet Allocation (LDA) provides a suitable framework to\nprocess grocery transactions and to discover a broad representation of\ncustomers' shopping motivations. However, summarizing the posterior\ndistribution of an LDA model is challenging, while individual LDA draws may not\nbe coherent and cannot capture topic uncertainty. Moreover, the evaluation of\nLDA models is dominated by model-fit measures which may not adequately capture\nthe qualitative aspects such as interpretability and stability of topics.\n  In this paper, we introduce clustering methodology that post-processes\nposterior LDA draws to summarise the entire posterior distribution and identify\nsemantic modes represented as recurrent topics. Our approach is an alternative\nto standard label-switching techniques and provides a single posterior summary\nset of topics, as well as associated measures of uncertainty. Furthermore, we\nestablish a more holistic definition for model evaluation, which assesses topic\nmodels based not only on their likelihood but also on their coherence,\ndistinctiveness and stability. By means of a survey, we set thresholds for the\ninterpretation of topic coherence and topic similarity in the domain of grocery\nretail data. We demonstrate that the selection of recurrent topics through our\nclustering methodology not only improves model likelihood but also outperforms\nthe qualitative aspects of LDA such as interpretability and stability. We\nillustrate our methods on an example from a large UK supermarket chain.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 21:23:36 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 15:29:50 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Vega-Carrasco", "Mariflor", ""], ["O'sullivan", "Jason", ""], ["Prior", "Rosie", ""], ["Manolopoulou", "Ioanna", ""], ["Musolesi", "Mirco", ""]]}, {"id": "2005.10173", "submitter": "Yolanda Larriba", "authors": "Cristina Rueda, Yolanda Larriba and Adri\\'an Lamela", "title": "The hidden waves in the ECG uncovered: a sound automated interpretation\n  method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel approach for analysing cardiac rhythm data is presented in this\npaper. Heartbeats are decomposed into the five fundamental $P$, $Q$, $R$, $S$\nand $T$ waves plus an error term to account for artefacts in the data which\nprovides a meaningful, physical interpretation of the heart's electric system.\nThe morphology of each wave is concisely described using four parameters that\nallow to all the different patterns in heartbeats be characterized and thus\ndifferentiated\n  This multi-purpose approach solves such questions as the extraction of\ninterpretable features, the detection of the fiducial marks of the fundamental\nwaves, or the generation of synthetic data and the denoising of signals. Yet,\nthe greatest benefit from this new discovery will be the automatic diagnosis of\nheart anomalies as well as other clinical uses with great advantages compared\nto the rigid, vulnerable and black box machine learning procedures, widely used\nin medical devices.\n  The paper shows the enormous potential of the method in practice;\nspecifically, the capability to discriminate subjects, characterize\nmorphologies and detect the fiducial marks (reference points) are validated\nnumerically using simulated and real data, thus proving that it outperforms its\ncompetitors.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 16:30:33 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 12:25:20 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Rueda", "Cristina", ""], ["Larriba", "Yolanda", ""], ["Lamela", "Adri\u00e1n", ""]]}, {"id": "2005.10237", "submitter": "J\\\"org Stoye", "authors": "J\\\"org Stoye", "title": "A Critical Assessment of Some Recent Work on COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I tentatively re-analyze data from two well-publicized studies on COVID-19,\nnamely the Charit\\'{e} \"viral load in children\" and the Bonn \"seroprevalence in\nHeinsberg/Gangelt\" study, from information available in the preprints. The\nstudies have the following in common:\n  - They received worldwide attention and arguably had policy impact.\n  - The thrusts of their findings align with the respective lead authors'\n(different) public stances on appropriate response to COVID-19.\n  - Tentatively, my reading of the Gangelt study neutralizes its thrust, and my\nreading of the Charit\\'{e} study reverses it.\n  The exercise may aid in placing these studies in the literature. With all\ncaveats that apply to n=2 quickfire analyses based off preprints, one also\nwonders whether it illustrates inadvertent effects of \"researcher degrees of\nfreedom.\"\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 17:54:25 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 02:21:49 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Stoye", "J\u00f6rg", ""]]}, {"id": "2005.10307", "submitter": "William Artman", "authors": "William J. Artman, Ashkan Ertefaie, Kevin G. Lynch, James R. McKay,\n  Brent A. Johnson", "title": "Adjusting for Partial Compliance in SMARTs: a Bayesian Semiparametric\n  Approach", "comments": "31 pages, 8 figures, 13 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cyclical and heterogeneous nature of many substance use disorders\nhighlights the need to adapt the type or the dose of treatment to accommodate\nthe specific and changing needs of individuals. The Adaptive Treatment for\nAlcohol and Cocaine Dependence study (ENGAGE) is a multi-stage randomized trial\nthat aimed to provide longitudinal data for constructing treatment strategies\nto improve patients' engagement in therapy. However, the high rate of\nnoncompliance and lack of analytic tools to account for noncompliance have\nimpeded researchers from using the data to achieve the main goal of the trial.\nWe overcome this issue by defining our target parameter as the mean outcome\nunder different treatment strategies for given potential compliance strata and\npropose a Bayesian semiparametric model to estimate this quantity. While it\nadds substantial complexities to the analysis, one important feature of our\nwork is that we consider partial rather than binary compliance classes which is\nmore relevant in longitudinal studies. We assess the performance of our method\nthrough comprehensive simulation studies. We illustrate its application on the\nENGAGE study and demonstrate that the optimal treatment strategy depends on\ncompliance strata.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 18:38:44 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Artman", "William J.", ""], ["Ertefaie", "Ashkan", ""], ["Lynch", "Kevin G.", ""], ["McKay", "James R.", ""], ["Johnson", "Brent A.", ""]]}, {"id": "2005.10335", "submitter": "Stefano Cabras", "authors": "Stefano Cabras", "title": "A Bayesian - Deep Learning model for estimating Covid-19 evolution in\n  Spain", "comments": "Related to: https://github.com/scabras/covid19-bayes-dl", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a semi-parametric approach to estimate Covid-19\n(SARS-CoV-2) evolution in Spain. Considering the sequences of 14 days\ncumulative incidence of all Spanish regions, it combines modern Deep Learning\n(DL) techniques for analyzing sequences with the usual Bayesian Poisson-Gamma\nmodel for counts. DL model provides a suitable description of observed\nsequences but no reliable uncertainty quantification around it can be obtained.\nTo overcome this we use the prediction from DL as an expert elicitation of the\nexpected number of counts along with their uncertainty and thus obtaining the\nposterior predictive distribution of counts in an orthodox Bayesian analysis\nusing the well known Poisson-Gamma model. The overall resulting model allows us\nto either predict the future evolution of the sequences on all regions, as well\nas, estimating the consequences of eventual scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 19:57:12 GMT"}, {"version": "v2", "created": "Sat, 6 Mar 2021 17:27:38 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Cabras", "Stefano", ""]]}, {"id": "2005.10454", "submitter": "Curtis Murray", "authors": "Curtis Murray, Lewis Mitchell, Jonathan Tuke, Mark Mackay", "title": "Symptom extraction from the narratives of personal experiences with\n  COVID-19 on Reddit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media discussion of COVID-19 provides a rich source of information\ninto how the virus affects people's lives that is qualitatively different from\ntraditional public health datasets. In particular, when individuals self-report\ntheir experiences over the course of the virus on social media, it can allow\nfor identification of the emotions each stage of symptoms engenders in the\npatient. Posts to the Reddit forum r/COVID19Positive contain first-hand\naccounts from COVID-19 positive patients, giving insight into personal\nstruggles with the virus. These posts often feature a temporal structure\nindicating the number of days after developing symptoms the text refers to.\nUsing topic modelling and sentiment analysis, we quantify the change in\ndiscussion of COVID-19 throughout individuals' experiences for the first 14\ndays since symptom onset. Discourse on early symptoms such as fever, cough, and\nsore throat was concentrated towards the beginning of the posts, while language\nindicating breathing issues peaked around ten days. Some conversation around\ncritical cases was also identified and appeared at a roughly constant rate. We\nidentified two clear clusters of positive and negative emotions associated with\nthe evolution of these symptoms and mapped their relationships. Our results\nprovide a perspective on the patient experience of COVID-19 that complements\nother medical data streams and can potentially reveal when mental health issues\nmight appear.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 03:54:51 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Murray", "Curtis", ""], ["Mitchell", "Lewis", ""], ["Tuke", "Jonathan", ""], ["Mackay", "Mark", ""]]}, {"id": "2005.10698", "submitter": "Robin Hirt", "authors": "Robin Hirt, Niklas K\\\"uhl, Yusuf Peker, Gerhard Satzger", "title": "How to Learn from Others: Transfer Machine Learning with Additive\n  Regression Models to Improve Sales Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a variety of business situations, the introduction or improvement of\nmachine learning approaches is impaired as these cannot draw on existing\nanalytical models. However, in many cases similar problems may have already\nbeen solved elsewhere-but the accumulated analytical knowledge cannot be tapped\nto solve a new problem, e.g., because of privacy barriers. For the particular\npurpose of sales forecasting for similar entities, we propose a transfer\nmachine learning approach based on additive regression models that lets new\nentities benefit from models of existing entities. We evaluate the approach on\na rich, multi-year dataset of multiple restaurant branches. We differentiate\nthe options to simply transfer models from one branch to another (\"zero shot\")\nor to transfer and adapt them. We analyze feasibility and performance against\nseveral forecasting benchmarks. The results show the potential of the approach\nto exploit the collectively available analytical knowledge. Thus, we contribute\nan approach that is generalizable beyond sales forecasting and the specific use\ncase in particular. In addition, we demonstrate its feasibility for a typical\nuse case as well as the potential for improving forecasting quality. These\nresults should inform academia, as they help to leverage knowledge across\nvarious entities, and have immediate practical application in industry.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 15:44:37 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Hirt", "Robin", ""], ["K\u00fchl", "Niklas", ""], ["Peker", "Yusuf", ""], ["Satzger", "Gerhard", ""]]}, {"id": "2005.10879", "submitter": "Steven Smith", "authors": "Steven T. Smith, Edward K. Kao, Erika D. Mackin, Danelle C. Shah, Olga\n  Simek, Donald B. Rubin", "title": "Automatic Detection of Influential Actors in Disinformation Networks", "comments": "Proc. Natl. Acad. Sciences U.S.A. Vol. 118, No. 4, e2011216118", "journal-ref": null, "doi": "10.1073/pnas.2011216118", "report-no": null, "categories": "cs.SI cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The weaponization of digital communications and social media to conduct\ndisinformation campaigns at immense scale, speed, and reach presents new\nchallenges to identify and counter hostile influence operations (IOs). This\npaper presents an end-to-end framework to automate detection of disinformation\nnarratives, networks, and influential actors. The framework integrates natural\nlanguage processing, machine learning, graph analytics, and a novel network\ncausal inference approach to quantify the impact of individual actors in\nspreading IO narratives. We demonstrate its capability on real-world hostile IO\ncampaigns with Twitter datasets collected during the 2017 French presidential\nelections, and known IO accounts disclosed by Twitter over a broad range of IO\ncampaigns (May 2007 to February 2020), over 50,000 accounts, 17 countries, and\ndifferent account types including both trolls and bots. Our system detects IO\naccounts with 96% precision, 79% recall, and 96% area-under-the-PR-curve, maps\nout salient network communities, and discovers high-impact accounts that escape\nthe lens of traditional impact statistics based on activity counts and network\ncentrality. Results are corroborated with independent sources of known IO\naccounts from U.S. Congressional reports, investigative journalism, and IO\ndatasets provided by Twitter.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 20:15:51 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 01:10:13 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2021 22:15:57 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Smith", "Steven T.", ""], ["Kao", "Edward K.", ""], ["Mackin", "Erika D.", ""], ["Shah", "Danelle C.", ""], ["Simek", "Olga", ""], ["Rubin", "Donald B.", ""]]}, {"id": "2005.11161", "submitter": "Yusuke Sakumoto", "authors": "Yusuke Sakumoto and Hiroyuki Ohsaki", "title": "Graph Degree Heterogeneity Facilitates Random Walker Meetings", "comments": "11 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various graph algorithms have been developed with multiple random walks, the\nmovement of several independent random walkers on a graph. Designing an\nefficient graph algorithm based on multiple random walks requires investigating\nmultiple random walks theoretically to attain a deep understanding of their\ncharacteristics. The first meeting time is one of the important metrics for\nmultiple random walks. The first meeting time on a graph is defined by the time\nit takes for multiple random walkers to meet at the same node in a graph. This\ntime is closely related to the rendezvous problem, a fundamental problem in\ncomputer science. The first meeting time of multiple random walks has been\nanalyzed previously, but many of these analyses have focused on regular graphs.\nIn this paper, we analyze the first meeting time of multiple random walks in\narbitrary graphs and clarify the effects of graph structures on expected\nvalues. First, we derive the spectral formula of the expected first meeting\ntime on the basis of spectral graph theory. Then, we examine the principal\ncomponent of the expected first meeting time using the derived spectral\nformula. The clarified principal component reveals that (a)the expected first\nmeeting time is almost dominated by $n/(1+d_{\\rm std}^2/d_{\\rm avg}^2)$ and\n(b)the expected first meeting time is independent of the starting nodes of\nrandom walkers, where $n$ is the number of nodes of the graph. $d_{\\rm avg}$\nand $d_{\\rm std}$ are the average and the standard deviation of weighted node\ndegrees, respectively. The characteristic(a) is useful for understanding the\neffect of the graph structure on the first meeting time. According to the\nrevealed effect of graph structures, the variance of the coefficient $d_{\\rm\nstd}/d_{\\rm avg}$(degree heterogeneity) for weighted degrees facilitates the\nmeeting of random walkers.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 13:04:15 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 10:48:25 GMT"}, {"version": "v3", "created": "Wed, 10 Jun 2020 10:25:31 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Sakumoto", "Yusuke", ""], ["Ohsaki", "Hiroyuki", ""]]}, {"id": "2005.11168", "submitter": "Sebastian Baumbach", "authors": "Sebastian Baumbach and Florian Sachs and Sheraz Ahmed and Andreas\n  Dengel", "title": "QuIS: The Question of Intelligent Site Selection", "comments": "arXiv admin note: text overlap with arXiv:1608.01212", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Site selection is one of the most important decisions to be made by\ncompanies. Such a decision depends on various factors of sites, including\nsocio-economic, geographical, ecological, as well as specific requirements of\ncompanies. The existing approaches for site selection are manual, subjective,\nand not scalable. The paper presents the new approach QuIS for site selection,\nwhich is automatic, scalable, and more effective than existing state-of-the-art\nmethods. It impartially finds suitables site based on analyzing decisive data\nof all location factors in both time and space. Another highlight of the\nproposed method is that the recommendations are supported by explanations,\ni.e., why something was suggested. To assess the effectiveness of the presented\nmethod, a case study on site selection of supermarkets in Germany is performed\nusing real data of more than 200 location factors for 11.162 sites. Evaluation\nresults show that there is a big coverage (86.4 %) between the sites of\nexisting supermarkets selected by economists and the sites recommended by the\npresented method. In addition, the method also recommends many sites (328)\nwhere it is benefial to open a new supermarket. Furthermore, new decisive\nlocation factors are revealed, which have an impact on the existence of\nsupermarkets.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 09:04:19 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Baumbach", "Sebastian", ""], ["Sachs", "Florian", ""], ["Ahmed", "Sheraz", ""], ["Dengel", "Andreas", ""]]}, {"id": "2005.11233", "submitter": "Maciej Ber\\k{e}sewicz", "authors": "Jacek Bia{\\l}ek, Maciej Ber\\k{e}sewicz", "title": "Scanner data in inflation measurement: from raw data to price indices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.GN q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scanner data offer new opportunities for CPI or HICP calculation. They can be\nobtained from a~wide variety of~retailers (supermarkets, home electronics,\nInternet shops, etc.) and provide information at the level of~the barcode. One\nof~advantages of~using scanner data is the fact that they contain complete\ntransaction information, i.e. prices and quantities for every sold item. To use\nscanner data, it must be carefully processed. After clearing data and unifying\nproduct names, products should be carefully classified (e.g. into COICOP 5 or\nbelow), matched, filtered and aggregated. These procedures often require\ncreating new IT or writing custom scripts (R, Python, Mathematica, SAS,\nothers). One of~new challenges connected with scanner data is the appropriate\nchoice of~the index formula. In this article we present a~proposal for the\nimplementation of~individual stages of~handling scanner data. We also point out\npotential problems during scanner data processing and their solutions. Finally,\nwe compare a~large number of~price index methods based on real scanner datasets\nand we verify their sensitivity on adopted data filtering and aggregating\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 15:22:40 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Bia\u0142ek", "Jacek", ""], ["Ber\u0119sewicz", "Maciej", ""]]}, {"id": "2005.11242", "submitter": "Antonio Quintero-Rincon", "authors": "Antonio Quintero-Rinc\\'on, Carlos D'Giano, Hadj Batatia", "title": "Mu-suppression detection in motor imagery electroencephalographic\n  signals using the generalized extreme value distribution", "comments": "10 pages, 6 Figures, 4 tables", "journal-ref": "IEEE 2020", "doi": "10.1109/IJCNN48605.2020.9206862", "report-no": "978-1-7281-6926-2", "categories": "eess.SP stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper deals with the detection of mu-suppression from\nelectroencephalographic (EEG) signals in brain-computer interface (BCI). For\nthis purpose, an efficient algorithm is proposed based on a statistical model\nand a linear classifier. Precisely, the generalized extreme value distribution\n(GEV) is proposed to represent the power spectrum density of the EEG signal in\nthe central motor cortex. The associated three parameters are estimated using\nthe maximum likelihood method. Based on these parameters, a simple and\nefficient linear classifier was designed to classify three types of events:\nimagery, movement, and resting. Preliminary results show that the proposed\nstatistical model can be used in order to detect precisely the mu-suppression\nand distinguish different EEG events, with very good classification accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 15:51:25 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 20:14:39 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Quintero-Rinc\u00f3n", "Antonio", ""], ["D'Giano", "Carlos", ""], ["Batatia", "Hadj", ""]]}, {"id": "2005.11278", "submitter": "Martin Huber", "authors": "Martin Huber and Henrika Langen", "title": "The Impact of Response Measures on COVID-19-Related Hospitalization and\n  Death Rates in Germany and Switzerland", "comments": "Evaluation of lockdown measures in Germany and Switzerland aimed at\n  containing the COVID-19 epidemic", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We assess the impact of COVID-19 response measures implemented in Germany and\nSwitzerland on cumulative COVID-19-related hospitalization and death rates. Our\nanalysis exploits the fact that the epidemic was more advanced in some regions\nthan in others when certain lockdown measures came into force, based on\nmeasuring health outcomes relative to the region-specific start of the epidemic\nand comparing outcomes across regions with earlier and later start dates. When\nestimating the effect of the relative timing of measures, we control for\nregional characteristics and initial epidemic trends by linear regression\n(Germany and Switzerland), doubly robust estimation (Germany), or synthetic\ncontrols (Switzerland). We find for both countries that a relatively later\nexposure to the measures entails higher cumulative hospitalization and death\nrates on region-specific days after the outbreak of the epidemic, suggesting\nthat an earlier imposition of measures is more effective than a later one. For\nGermany, we also evaluate curfews (as introduced in a subset of states) based\non cross-regional variation. We do not find any effects of curfews on top of\nthe federally imposed contact restriction that banned groups of more than 2\nindividuals. Finally, an analysis of mobility patterns in Switzerland shows an\nimmediate behavioral effect of the lockdown in terms of reduced mobility.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 13:02:43 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 18:19:45 GMT"}, {"version": "v3", "created": "Fri, 19 Jun 2020 06:24:54 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Huber", "Martin", ""], ["Langen", "Henrika", ""]]}, {"id": "2005.11510", "submitter": "Ionas Erb", "authors": "Ionas Erb and Nihat Ay", "title": "The Information-Geometric Perspective of Compositional Data Analysis", "comments": "22 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information geometry uses the formal tools of differential geometry to\ndescribe the space of probability distributions as a Riemannian manifold with\nan additional dual structure. The formal equivalence of compositional data with\ndiscrete probability distributions makes it possible to apply the same\ndescription to the sample space of Compositional Data Analysis (CoDA). The\nlatter has been formally described as a Euclidean space with an orthonormal\nbasis featuring components that are suitable combinations of the original\nparts. In contrast to the Euclidean metric, the information-geometric\ndescription singles out the Fisher information metric as the only one keeping\nthe manifold's geometric structure invariant under equivalent representations\nof the underlying random variables. Well-known concepts that are valid in\nEuclidean coordinates, e.g., the Pythogorean theorem, are generalized by\ninformation geometry to corresponding notions that hold for more general\ncoordinates. In briefly reviewing Euclidean CoDA and, in more detail, the\ninformation-geometric approach, we show how the latter justifies the use of\ndistance measures and divergences that so far have received little attention in\nCoDA as they do not fit the Euclidean geometry favored by current thinking. We\nalso show how entropy and relative entropy can describe amalgamations in a\nsimple way, while Aitchison distance requires the use of geometric means to\nobtain more succinct relationships. We proceed to prove the information\nmonotonicity property for Aitchison distance. We close with some thoughts about\nnew directions in CoDA where the rich structure that is provided by information\ngeometry could be exploited.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 10:36:50 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 22:40:32 GMT"}, {"version": "v3", "created": "Tue, 27 Apr 2021 09:14:11 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Erb", "Ionas", ""], ["Ay", "Nihat", ""]]}, {"id": "2005.11738", "submitter": "Rico Krueger", "authors": "Rico Krueger, Prateek Bansal, Prasad Buddhavarapu", "title": "A New Spatial Count Data Model with Bayesian Additive Regression Trees\n  for Accident Hot Spot Identification", "comments": null, "journal-ref": "Accident Analysis & Prevention Volume 144, September 2020, 105623", "doi": "10.1016/j.aap.2020.105623", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification of accident hot spots is a central task of road safety\nmanagement. Bayesian count data models have emerged as the workhorse method for\nproducing probabilistic rankings of hazardous sites in road networks.\nTypically, these methods assume simple linear link function specifications,\nwhich, however, limit the predictive power of a model. Furthermore, extensive\nspecification searches are precluded by complex model structures arising from\nthe need to account for unobserved heterogeneity and spatial correlations.\nModern machine learning (ML) methods offer ways to automate the specification\nof the link function. However, these methods do not capture estimation\nuncertainty, and it is also difficult to incorporate spatial correlations. In\nlight of these gaps in the literature, this paper proposes a new spatial\nnegative binomial model, which uses Bayesian additive regression trees to\nendogenously select the specification of the link function. Posterior inference\nin the proposed model is made feasible with the help of the Polya-Gamma data\naugmentation technique. We test the performance of this new model on a crash\ncount data set from a metropolitan highway network. The empirical results show\nthat the proposed model performs at least as well as a baseline spatial count\ndata model with random parameters in terms of goodness of fit and site ranking\nability.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 13:00:59 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Krueger", "Rico", ""], ["Bansal", "Prateek", ""], ["Buddhavarapu", "Prasad", ""]]}, {"id": "2005.11774", "submitter": "Irina Blazhievska", "authors": "Irina Blazhievska", "title": "Cumulant methods in the estimation of response functions in\n  time-invariant linear systems", "comments": "17 pictures, 202 pages, in Ukrainian, Dissertation, Kyev Polytechnic\n  Institute (2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thesis is devoted to the application of cumulant analysis in the estimation\nof impulse response functions for continuous time-invariant linear systems,\nincluding systems with inner noises. The main assumption of the work is the\nsecond-order integration of the impulse response function. Our study deals with\ncumulant analysis of sample cross-correlograms between stationary Gaussian\nstochastic processes. An important role was played by integral representations\nfor the higher-order cumulants of these second-order statistics. Using the\ndiagram formula, all representations are reduced to the finite sums of\nintegrals involving cyclic products of kernels. In the work we proved the\nconvergence to zero of the corresponding integrals. Then, since the Gaussian\ndistribution is uniquelly determined by its cumulants and also all higher-order\ncumulants of the estimators tend to zero, we establish the asymptotic normality\nof the integral-type cross-correlogram estimators.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 15:22:25 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Blazhievska", "Irina", ""]]}, {"id": "2005.11785", "submitter": "Saverio Ranciati", "authors": "Saverio Ranciati, Alberto Roverato, Alessandra Luati", "title": "Fused graphical lasso for brain networks with symmetries", "comments": "27 pages, 3 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuroimaging is the growing area of neuroscience devoted to produce data with\nthe goal of capturing processes and dynamics of the human brain. We consider\nthe problem of inferring the brain connectivity network from time dependent\nfunctional magnetic resonance imaging (fMRI) scans. To this aim we propose the\nsymmetric graphical lasso, a penalized likelihood method with a fused type\npenalty function that takes into explicit account the natural symmetrical\nstructure of the brain. Symmetric graphical lasso allows one to learn\nsimultaneously both the network structure and a set of symmetries across the\ntwo hemispheres. We implement an alternating directions method of multipliers\nalgorithm to solve the corresponding convex optimization problem. Furthermore,\nwe apply our methods to estimate the brain networks of two subjects, one\nhealthy and the other affected by a mental disorder, and to compare them with\nrespect to their symmetric structure. The method applies once the temporal\ndependence characterising fMRI data has been accounted for and we compare the\nimpact on the analysis of different detrending techniques on the estimated\nbrain networks.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 15:45:52 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 14:26:46 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Ranciati", "Saverio", ""], ["Roverato", "Alberto", ""], ["Luati", "Alessandra", ""]]}, {"id": "2005.11856", "submitter": "Joseph Paul Cohen", "authors": "Joseph Paul Cohen and Lan Dao and Paul Morrison and Karsten Roth and\n  Yoshua Bengio and Beiyi Shen and Almas Abbasi and Mahsa Hoshmand-Kochi and\n  Marzyeh Ghassemi and Haifang Li and Tim Q Duong", "title": "Predicting COVID-19 Pneumonia Severity on Chest X-ray with Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.LG q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: The need to streamline patient management for COVID-19 has become\nmore pressing than ever. Chest X-rays provide a non-invasive (potentially\nbedside) tool to monitor the progression of the disease. In this study, we\npresent a severity score prediction model for COVID-19 pneumonia for frontal\nchest X-ray images. Such a tool can gauge severity of COVID-19 lung infections\n(and pneumonia in general) that can be used for escalation or de-escalation of\ncare as well as monitoring treatment efficacy, especially in the ICU.\n  Methods: Images from a public COVID-19 database were scored retrospectively\nby three blinded experts in terms of the extent of lung involvement as well as\nthe degree of opacity. A neural network model that was pre-trained on large\n(non-COVID-19) chest X-ray datasets is used to construct features for COVID-19\nimages which are predictive for our task.\n  Results: This study finds that training a regression model on a subset of the\noutputs from an this pre-trained chest X-ray model predicts our geographic\nextent score (range 0-8) with 1.14 mean absolute error (MAE) and our lung\nopacity score (range 0-6) with 0.78 MAE.\n  Conclusions: These results indicate that our model's ability to gauge\nseverity of COVID-19 lung infections could be used for escalation or\nde-escalation of care as well as monitoring treatment efficacy, especially in\nthe intensive care unit (ICU). A proper clinical trial is needed to evaluate\nefficacy. To enable this we make our code, labels, and data available online at\nhttps://github.com/mlmed/torchxrayvision/tree/master/scripts/covid-severity and\nhttps://github.com/ieee8023/covid-chestxray-dataset\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 23:13:16 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2020 16:40:48 GMT"}, {"version": "v3", "created": "Tue, 30 Jun 2020 17:09:53 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Cohen", "Joseph Paul", ""], ["Dao", "Lan", ""], ["Morrison", "Paul", ""], ["Roth", "Karsten", ""], ["Bengio", "Yoshua", ""], ["Shen", "Beiyi", ""], ["Abbasi", "Almas", ""], ["Hoshmand-Kochi", "Mahsa", ""], ["Ghassemi", "Marzyeh", ""], ["Li", "Haifang", ""], ["Duong", "Tim Q", ""]]}, {"id": "2005.11911", "submitter": "Zeyi Wang", "authors": "Zeyi Wang, Eric Bridgeford, Shangsi Wang, Joshua T. Vogelstein, Brian\n  Caffo", "title": "Statistical Analysis of Data Repeatability Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of modern data collection and processing techniques has seen the\nsize, scale, and complexity of data grow exponentially. A seminal step in\nleveraging these rich datasets for downstream inference is understanding the\ncharacteristics of the data which are repeatable -- the aspects of the data\nthat are able to be identified under a duplicated analysis. Conflictingly, the\nutility of traditional repeatability measures, such as the intraclass\ncorrelation coefficient, under these settings is limited. In recent work, novel\ndata repeatability measures have been introduced in the context where a set of\nsubjects are measured twice or more, including: fingerprinting, rank sums, and\ngeneralizations of the intraclass correlation coefficient. However, the\nrelationships between, and the best practices among these measures remains\nlargely unknown. In this manuscript, we formalize a novel repeatability\nmeasure, discriminability. We show that it is deterministically linked with the\ncorrelation coefficient under univariate random effect models, and has desired\nproperty of optimal accuracy for inferential tasks using multivariate\nmeasurements. Additionally, we overview and systematically compare\nrepeatability statistics using both theoretical results and simulations. We\nshow that the rank sum statistic is deterministically linked to a consistent\nestimator of discriminability. The power of permutation tests derived from\nthese measures are compared numerically under Gaussian and non-Gaussian\nsettings, with and without simulated batch effects. Motivated by both\ntheoretical and empirical results, we provide methodological recommendations\nfor each benchmark setting to serve as a resource for future analyses. We\nbelieve these recommendations will play an important role towards improving\nrepeatability in fields such as functional magnetic resonance imaging,\ngenomics, pharmacology, and more.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 03:47:09 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 19:38:04 GMT"}, {"version": "v3", "created": "Thu, 20 Aug 2020 08:57:51 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Wang", "Zeyi", ""], ["Bridgeford", "Eric", ""], ["Wang", "Shangsi", ""], ["Vogelstein", "Joshua T.", ""], ["Caffo", "Brian", ""]]}, {"id": "2005.11993", "submitter": "Aimee Taylor", "authors": "Aimee R Taylor and James A Watson and Caroline O Buckee", "title": "Pixelate to communicate: visualising uncertainty in maps of disease risk\n  and other spatial continua", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maps have long been been used to visualise estimates of spatial variables, in\nparticular disease burden and risk. Predictions made using a geostatistical\nmodel have uncertainty that typically varies spatially. However, this\nuncertainty is difficult to map with the estimate itself and is often not\nincluded as a result, thereby generating a potentially misleading sense of\ncertainty about disease burden or other important variables. To remedy this, we\npropose simultaneously visualising predictions and their associated uncertainty\nwithin a single map by varying pixel size. We illustrate our approach using\nexamples of malaria incidence, but the method could be applied to predictions\nof any spatial continua with associated uncertainty.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 09:22:47 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Taylor", "Aimee R", ""], ["Watson", "James A", ""], ["Buckee", "Caroline O", ""]]}, {"id": "2005.12055", "submitter": "Seyed Mostafa Kia", "authors": "Seyed Mostafa Kia, Hester Huijsdens, Richard Dinga, Thomas Wolfers,\n  Maarten Mennes, Ole A. Andreassen, Lars T. Westlye, Christian F. Beckmann,\n  Andre F. Marquand", "title": "Hierarchical Bayesian Regression for Multi-Site Normative Modeling of\n  Neuroimaging Data", "comments": "To be published in MICCAI 2020 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical neuroimaging has recently witnessed explosive growth in data\navailability which brings studying heterogeneity in clinical cohorts to the\nspotlight. Normative modeling is an emerging statistical tool for achieving\nthis objective. However, its application remains technically challenging due to\ndifficulties in properly dealing with nuisance variation, for example due to\nvariability in image acquisition devices. Here, in a fully probabilistic\nframework, we propose an application of hierarchical Bayesian regression (HBR)\nfor multi-site normative modeling. Our experimental results confirm the\nsuperiority of HBR in deriving more accurate normative ranges on large\nmulti-site neuroimaging data compared to widely used methods. This provides the\npossibility i) to learn the normative range of structural and functional brain\nmeasures on large multi-site data; ii) to recalibrate and reuse the learned\nmodel on local small data; therefore, HBR closes the technical loop for\napplying normative modeling as a medical tool for the diagnosis and prognosis\nof mental disorders.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 11:55:19 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Kia", "Seyed Mostafa", ""], ["Huijsdens", "Hester", ""], ["Dinga", "Richard", ""], ["Wolfers", "Thomas", ""], ["Mennes", "Maarten", ""], ["Andreassen", "Ole A.", ""], ["Westlye", "Lars T.", ""], ["Beckmann", "Christian F.", ""], ["Marquand", "Andre F.", ""]]}, {"id": "2005.12093", "submitter": "Paul Doukhan M.", "authors": "Paul Doukhan and Naushad Mamode Khan and Michael H. Neumann", "title": "Mixing properties of Skellam-GARCH processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider integer-valued GARCH processes, where the count variable\nconditioned on past values of the count and state variables follows a so-called\nSkellam distribution. Using arguments for contractive Markov chains we prove\nthat the process has a unique stationary regime. Furthermore, we show\nasymptotic regularity ($\\beta$-mixing) with geometrically decaying coefficients\nfor the count process. These probabilistic results are complemented by a\nstatistical analysis, a few simulations as well as an application to recent\nCOVID-19 data.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 13:16:30 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 09:51:26 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Doukhan", "Paul", ""], ["Khan", "Naushad Mamode", ""], ["Neumann", "Michael H.", ""]]}, {"id": "2005.12102", "submitter": "Maria Michela Dickson", "authors": "Riccardo Gianluigi Serio, Maria Michela Dickson, Diego Giuliani,\n  Giuseppe Espa", "title": "Green production as a factor of survival for innovative startups.\n  Evidence from Italy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many studies have analyzed empirically the determinants of survival for\ninnovative startup companies using data about the characteristics of\nentrepreneurs and management or focusing on firm- and industry-specific\nvariables. However, no attempts have been made so far to assess the role of the\nenvironmental sustainability of the production process. Based on data\ndescribing the characteristics of the Italian innovative startups in the period\n2009-2018, this article studies the differences in survival between green and\nnon-green companies. We show that, while controlling for other confounding\nfactors, startups characterized by a green production process tend to survive\nlonger than their counterparts. In particular, we estimate that a green\ninnovative startup is more than twice as likely to survive than a non-green\none. This evidence may support the idea that environment sustainability can\nhelp economic development.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 13:36:52 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Serio", "Riccardo Gianluigi", ""], ["Dickson", "Maria Michela", ""], ["Giuliani", "Diego", ""], ["Espa", "Giuseppe", ""]]}, {"id": "2005.12170", "submitter": "Alessandra Micheletti", "authors": "Luisa Ferrari, Giuseppe Gerardi, Giancarlo Manzi, Alessandra\n  Micheletti, Federica Nicolussi, Elia Biganzoli, Silvia Salini", "title": "Modelling provincial Covid-19 epidemic data in Italy using an adjusted\n  time-dependent SIRD model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop a predictive model for the spread of COVID-19\ninfection at a provincial (i.e. EU NUTS-3) level in Italy by using official\ndata from the Italian Ministry of Health integrated with data extracted from\ndaily official press conferences of regional authorities and from local\nnewspaper websites. This integration is mainly concerned with COVID-19 cause\nspecific death data which are not available at NUTS-3 level from open official\ndata data channels. An adjusted time-dependent SIRD model is used to predict\nthe behavior of the epidemic, specifically the number of susceptible, infected,\ndeceased and recovered people. Predictive model performance is evaluated using\ncomparison with real data.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 15:33:00 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 14:11:20 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Ferrari", "Luisa", ""], ["Gerardi", "Giuseppe", ""], ["Manzi", "Giancarlo", ""], ["Micheletti", "Alessandra", ""], ["Nicolussi", "Federica", ""], ["Biganzoli", "Elia", ""], ["Salini", "Silvia", ""]]}, {"id": "2005.12197", "submitter": "Christopher Small", "authors": "C. Small", "title": "Spatiotemporal Network Evolution of Anthropogenic Night Light 1992-2015", "comments": "25 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph physics.geo-ph stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Satellite imaging of night light provides a global record of lighted\ndevelopment from 1992 to present. Not all settlements can be detected with\nnight light, but the continuum of built environments where a rapidly growing\nmajority of the population lives generally can. Segmenting a continuous field\ninto discrete spatially contiguous subsets of pixels produces a spatial network\nin which each contiguous segment represents a distinct network component.\nRepresenting the continuum of lighted development as spatial networks of\nvarying spatial connectivity allows the generative conditions for power law\nnetwork structure to be applied in a spatial context to provide a general\nexplanation for similar scaling observed in settlements and other land cover\ntypes. This study introduces a novel methodology to combine complementary\nsources of satellite-derived night light observations to quantify the evolution\nof the global spatial network structure of lighted development from 1992 to\n2015. Area-perimeter distributions of network components show multifractal\nscaling with larger components becoming increasingly tortuous. Rank-size\ndistributions of network components are linear and well described by power laws\nwith exponents within 0.08 of -1 for all 27 subsets of geography, year and\ndegree of connectivity - indicating robust scaling properties. Area\ndistributions of luminance within network components show an abrupt transition\nfrom continuously low-skewed for smaller components to discontinuous, nearly\nuniform luminance distributions for larger components, suggesting a fundamental\nchange in network structure despite a continuum of size and shape. These\nresults suggest that city size scaling, observed inconsistently for\nadministratively-defined city populations, is more consistent for\nphysically-defined settlement network area and can be explained more simply by\na spatial network growth process.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 16:11:51 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Small", "C.", ""]]}, {"id": "2005.12376", "submitter": "Sebasti\\'an Contreras", "authors": "Sebasti\\'an Contreras, Juan Pablo Biron-Lattes, H. Andr\\'es\n  Villavicencio, David Medina-Ortiz, Nyna Llanovarced-Kawles, \\'Alvaro\n  Olivera-Nappa", "title": "Statistically-based methodology for revealing real contagion trends and\n  correcting delay-induced errors in the assessment of COVID-19 pandemic", "comments": null, "journal-ref": "Chaos Solitons Fractals 139 (2020) 110087", "doi": "10.1016/j.chaos.2020.110087", "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  COVID-19 pandemic has reshaped our world in a timescale much shorter than\nwhat we can understand. Particularities of SARS-CoV-2, such as its persistence\nin surfaces and the lack of a curative treatment or vaccine against COVID-19,\nhave pushed authorities to apply restrictive policies to control its spreading.\nAs data drove most of the decisions made in this global contingency, their\nquality is a critical variable for decision-making actors, and therefore should\nbe carefully curated. In this work, we analyze the sources of error in\ntypically reported epidemiological variables and usual tests used for\ndiagnosis, and their impact on our understanding of COVID-19 spreading\ndynamics. We address the existence of different delays in the report of new\ncases, induced by the incubation time of the virus and testing-diagnosis time\ngaps, and other error sources related to the sensitivity/specificity of the\ntests used to diagnose COVID-19. Using a statistically-based algorithm, we\nperform a temporal reclassification of cases to avoid delay-induced errors,\nbuilding up new epidemiologic curves centered in the day where the contagion\neffectively occurred. We also statistically enhance the robustness behind the\ndischarge/recovery clinical criteria in the absence of a direct test, which is\ntypically the case of non-first world countries, where the limited testing\ncapabilities are fully dedicated to the evaluation of new cases. Finally, we\napplied our methodology to assess the evolution of the pandemic in Chile\nthrough the Effective Reproduction Number $R_t$, identifying different moments\nin which data was misleading governmental actions. In doing so, we aim to raise\npublic awareness of the need for proper data reporting and processing protocols\nfor epidemiological modelling and predictions.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 20:15:37 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 08:52:12 GMT"}, {"version": "v3", "created": "Wed, 24 Jun 2020 19:13:17 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Contreras", "Sebasti\u00e1n", ""], ["Biron-Lattes", "Juan Pablo", ""], ["Villavicencio", "H. Andr\u00e9s", ""], ["Medina-Ortiz", "David", ""], ["Llanovarced-Kawles", "Nyna", ""], ["Olivera-Nappa", "\u00c1lvaro", ""]]}, {"id": "2005.12427", "submitter": "Jonas B\\'eal", "authors": "Jonas B\\'eal, Aur\\'elien Latouche", "title": "Causal inference with multiple versions of treatment and application to\n  personalized medicine", "comments": "Around 12 pages, with 5 figures Associated GitHub repository:\n  https://github.com/JonasBeal/Causal_Precision_Medicine", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The development of high-throughput sequencing and targeted therapies has led\nto the emergence of personalized medicine: a patient's molecular profile or the\npresence of a specific biomarker of drug response will correspond to a\ntreatment recommendation made either by a physician or by a treatment\nassignment algorithm. The growing number of such algorithms raises the question\nof how to quantify their clinical impact knowing that a personalized medicine\nstrategy will inherently include different versions of treatment.\n  We thus specify an appropriate causal framework with multiple versions of\ntreatment to define the causal effects of interest for precision medicine\nstrategies and estimate them emulating clinical trials with observational data.\nTherefore, we determine whether the treatment assignment algorithm is more\nefficient than different control arms: gold standard treatment, observed\ntreatments or random assignment of targeted treatments.\n  Causal estimates of the precision medicine effects are first evaluated on\nsimulated data and they demonstrate a lower biases and variances compared with\nnaive estimation of the difference in expected outcome between treatment arms.\nThe various simulations scenarios also point out the different bias sources\ndepending on the clinical situation (heterogeneity of response, assignment of\nobserved treatments etc.). A RShiny interactive application is also provided to\nfurther explore other user-defined scenarios. The method is then applied to\ndata from patient-derived xenografts (PDX): each patient tumour is implanted in\nseveral immunodeficient cloned mice later treated with different drugs, thus\nproviding access to all corresponding drug sensitivities for all patients.\nAccess to these unique pre-clinical data emulating counterfactual outcomes\nallows to validate the reliability of causal estimates obtained with the\nproposed method.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 22:08:10 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["B\u00e9al", "Jonas", ""], ["Latouche", "Aur\u00e9lien", ""]]}, {"id": "2005.12455", "submitter": "Ali Eshragh", "authors": "Ali Eshragh, Saed Alizamir, Peter Howley and Elizabeth Stojanovski", "title": "Modeling the Dynamics of the COVID-19 Population in Australia: A\n  Probabilistic Analysis", "comments": "25 pages, 7 figures, 3 tables", "journal-ref": null, "doi": "10.1371/journal.pone.0240153", "report-no": null, "categories": "stat.AP physics.soc-ph q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The novel Corona Virus COVID-19 arrived on Australian shores around 25\nJanuary 2020. This paper presents a novel method of dynamically modeling and\nforecasting the COVID-19 pandemic in Australia with a high degree of accuracy\nand in a timely manner using limited data; a valuable resource that can be used\nto guide government decision-making on societal restrictions on a daily and/or\nweekly basis. The \"partially-observable stochastic process\" used in this study\npredicts not only the future actual values with extremely low error, but also\nthe percentage of unobserved COVID-19 cases in the population. The model can\nfurther assist policy makers to assess the effectiveness of several possible\nalternative scenarios in their decision-making processes.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 00:36:05 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Eshragh", "Ali", ""], ["Alizamir", "Saed", ""], ["Howley", "Peter", ""], ["Stojanovski", "Elizabeth", ""]]}, {"id": "2005.12641", "submitter": "Hidetoshi Matsui", "authors": "Hidetoshi Matsui", "title": "Varying-coefficient functional additive models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the varying coefficient functional linear model to the nonlinear\nmodel and propose a varying coefficient functional additive model. The proposed\nmethod can represent the relationship between functional predictors and a\nscalar response where the response depends on an exogenous variable.It captures\nthe nonlinear structure between variables and also provides interpretable\nrelationship of them. The model is estimated through basis expansions and\npenalized likelihood method, and then the tuning parameters included at the\nestimation procedure are selected by a model selection criterion. Simulation\nstudies are provided to show the effectiveness of the proposed method. We also\napply it to the analysis of crop yield data and then investigate how and when\nthe environmental factor relates to the amount of the crop yield.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 11:44:38 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Matsui", "Hidetoshi", ""]]}, {"id": "2005.12647", "submitter": "Christian Truden", "authors": "Konstantin Posch, Christian Truden, Philipp Hungerl\\\"ander, J\\\"urgen\n  Pilz", "title": "A Bayesian Approach for Predicting Food and Beverage Sales in Staff\n  Canteens and Restaurants", "comments": null, "journal-ref": null, "doi": "10.1016/j.ijforecast.2021.06.001", "report-no": null, "categories": "stat.AP cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate demand forecasting is one of the key aspects for successfully\nmanaging restaurants and staff canteens. In particular, properly predicting\nfuture sales of menu items allows a precise ordering of food stock. From an\nenvironmental point of view, this ensures maintaining a low level of\npre-consumer food waste, while from the managerial point of view, this is\ncritical to guarantee the profitability of the restaurant. Hence, we are\ninterested in predicting future values of the daily sold quantities of given\nmenu items. The corresponding time series show multiple strong seasonalities,\ntrend changes, data gaps, and outliers. We propose a forecasting approach that\nis solely based on the data retrieved from Point of Sales systems and allows\nfor a straightforward human interpretation. Therefore, we propose two\ngeneralized additive models for predicting the future sales. In an extensive\nevaluation, we consider two data sets collected at a casual restaurant and a\nlarge staff canteen consisting of multiple time series, that cover a period of\n20 months, respectively. We show that the proposed models fit the features of\nthe considered restaurant data. Moreover, we compare the predictive performance\nof our method against the performance of other well-established forecasting\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 12:05:06 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 16:07:44 GMT"}, {"version": "v3", "created": "Mon, 10 May 2021 13:06:55 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Posch", "Konstantin", ""], ["Truden", "Christian", ""], ["Hungerl\u00e4nder", "Philipp", ""], ["Pilz", "J\u00fcrgen", ""]]}, {"id": "2005.12731", "submitter": "Daryl DeFord", "authors": "Daryl DeFord, Moon Duchin, and Justin Solomon", "title": "A Computational Approach to Measuring Vote Elasticity and\n  Competitiveness", "comments": "38 pages, 8 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent wave of attention to partisan gerrymandering has come with a push\nto refine or replace the laws that govern political redistricting around the\ncountry. A common element in several states' reform efforts has been the\ninclusion of competitiveness metrics, or scores that evaluate a districting\nplan based on the extent to which district-level outcomes are in play or are\nlikely to be closely contested.\n  In this paper, we examine several classes of competitiveness metrics\nmotivated by recent reform proposals and then evaluate their potential outcomes\nacross large ensembles of districting plans at the Congressional and state\nSenate levels. This is part of a growing literature using MCMC techniques from\napplied statistics to situate plans and criteria in the context of valid\nredistricting alternatives. Our empirical analysis focuses on five\nstates---Utah, Georgia, Wisconsin, Virginia, and Massachusetts---chosen to\nrepresent a range of partisan attributes. We highlight situation-specific\ndifficulties in creating good competitiveness metrics and show that optimizing\ncompetitiveness can produce unintended consequences on other partisan metrics.\nThese results demonstrate the importance of (1) avoiding writing detailed\nmetric constraints into long-lasting constitutional reform and (2) carrying out\ncareful mathematical modeling on real geo-electoral data in each redistricting\ncycle.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 14:01:31 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["DeFord", "Daryl", ""], ["Duchin", "Moon", ""], ["Solomon", "Justin", ""]]}, {"id": "2005.12777", "submitter": "Edward Boone", "authors": "Ryad Ghanam, Edward L. Boone, Abdel-Salam G. Abdel-Salam", "title": "SEIRD Model for Qatar Covid-19 Outbreak: A Case Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Covid-19 outbreak of 2020 has required many governments to develop\nmathematical-statistical models of the outbreak for policy and planning\npurposes. This work provides a tutorial on building a compartmental model using\nSusceptibles, Exposed, Infected, Recovered and Deaths status through time. A\nBayesian Framework is utilized to perform both parameter estimation and\npredictions. This model uses interventions to quantify the impact of various\ngovernment attempts to slow the spread of the virus. Predictions are also made\nto determine when the peak Active Infections will occur.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 14:57:14 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Ghanam", "Ryad", ""], ["Boone", "Edward L.", ""], ["Abdel-Salam", "Abdel-Salam G.", ""]]}, {"id": "2005.12783", "submitter": "Antonio Fern\\'andez Anta", "authors": "Oluwasegun Ojo, Augusto Garc\\'ia-Agundez, Benjamin Girault, Harold\n  Hern\\'andez, Elisa Cabana, Amanda Garc\\'ia-Garc\\'ia, Payman Arabshahi, Carlos\n  Baquero, Paolo Casari, Ednaldo Jos\\'e Ferreira, Davide Frey, Chryssis\n  Georgiou, Mathieu Goessens, Anna Ishchenko, Ernesto Jim\\'enez, Oleksiy\n  Kebkal, Rosa Lillo, Raquel Menezes, Nicolas Nicolaou, Antonio Ortega, Paul\n  Patras, Julian C Roberts, Efstathios Stavrakis, Yuichi Tanaka, Antonio\n  Fern\\'andez Anta", "title": "CoronaSurveys: Using Surveys with Indirect Reporting to Estimate the\n  Incidence and Evolution of Epidemics", "comments": "Presented at The KDD Workshop on Humanitarian Mapping, San Diego,\n  California USA, August 24, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CY stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The world is suffering from a pandemic called COVID-19, caused by the\nSARS-CoV-2 virus. National governments have problems evaluating the reach of\nthe epidemic, due to having limited resources and tests at their disposal. This\nproblem is especially acute in low and middle-income countries (LMICs). Hence,\nany simple, cheap and flexible means of evaluating the incidence and evolution\nof the epidemic in a given country with a reasonable level of accuracy is\nuseful. In this paper, we propose a technique based on (anonymous) surveys in\nwhich participants report on the health status of their contacts. This indirect\nreporting technique, known in the literature as network scale-up method,\npreserves the privacy of the participants and their contacts, and collects\ninformation from a larger fraction of the population (as compared to individual\nsurveys). This technique has been deployed in the CoronaSurveys project, which\nhas been collecting reports for the COVID-19 pandemic for more than two months.\nResults obtained by CoronaSurveys show the power and flexibility of the\napproach, suggesting that it could be an inexpensive and powerful tool for\nLMICs.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 11:58:23 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 12:18:50 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Ojo", "Oluwasegun", ""], ["Garc\u00eda-Agundez", "Augusto", ""], ["Girault", "Benjamin", ""], ["Hern\u00e1ndez", "Harold", ""], ["Cabana", "Elisa", ""], ["Garc\u00eda-Garc\u00eda", "Amanda", ""], ["Arabshahi", "Payman", ""], ["Baquero", "Carlos", ""], ["Casari", "Paolo", ""], ["Ferreira", "Ednaldo Jos\u00e9", ""], ["Frey", "Davide", ""], ["Georgiou", "Chryssis", ""], ["Goessens", "Mathieu", ""], ["Ishchenko", "Anna", ""], ["Jim\u00e9nez", "Ernesto", ""], ["Kebkal", "Oleksiy", ""], ["Lillo", "Rosa", ""], ["Menezes", "Raquel", ""], ["Nicolaou", "Nicolas", ""], ["Ortega", "Antonio", ""], ["Patras", "Paul", ""], ["Roberts", "Julian C", ""], ["Stavrakis", "Efstathios", ""], ["Tanaka", "Yuichi", ""], ["Anta", "Antonio Fern\u00e1ndez", ""]]}, {"id": "2005.12784", "submitter": "Tenglong Li", "authors": "Tenglong Li and Kenneth A. Frank", "title": "The probability of a robust inference for internal validity and its\n  applications in regression models", "comments": "arXiv admin note: substantial text overlap with arXiv:1906.08726", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The internal validity of observational study is often subject to debate. In\nthis study, we define the unobserved sample based on the counterfactuals and\nformalize its relationship with the null hypothesis statistical testing (NHST)\nfor regression models. The probability of a robust inference for internal\nvalidity, i.e., the PIV, is the probability of rejecting the null hypothesis\nagain based on the ideal sample which is defined as the combination of the\nobserved and unobserved samples, provided the same null hypothesis has already\nbeen rejected for the observed sample. When the unconfoundedness assumption is\ndubious, one can bound the PIV of an inference based on bounded belief about\nthe mean counterfactual outcomes, which is often needed in this case.\nEssentially, the PIV is statistical power of the NHST that is thought to be\nbuilt on the ideal sample. We summarize the process of evaluating internal\nvalidity with the PIV into a six-step procedure and illustrate it with an\nempirical example (i.e., Hong and Raudenbush (2005)).\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 03:29:19 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Li", "Tenglong", ""], ["Frank", "Kenneth A.", ""]]}, {"id": "2005.12840", "submitter": "Michelangelo Misuraca", "authors": "Michelangelo Misuraca, Alessia Forciniti, Germana Scepi, Maria Spano", "title": "Sentiment Analysis for Education with R: packages, methods and practical\n  applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Sentiment Analysis (SA) refers to a family of techniques at the crossroads of\nstatistics, natural language processing, and computational linguistics. The\nprimary goal is to detect the semantic orientation of individual opinions and\ncomments expressed in written texts. There are several practical applications\nof SA in several domains. In an educational context, the use of this approach\nallows processing students' feedback, aiming at monitoring the teaching\neffectiveness of instructors and enhancing the learning experience. This paper\nwants to review the different R packages that can be used to carry on SA,\ncomparing the implemented methods, discussing their characteristics, and\nshowing how they perform by considering a simple example.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 14:10:43 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Misuraca", "Michelangelo", ""], ["Forciniti", "Alessia", ""], ["Scepi", "Germana", ""], ["Spano", "Maria", ""]]}, {"id": "2005.12853", "submitter": "Stephanie Kovalchik", "authors": "Stephanie Kovalchik, Martin Ingram, Kokum Weeratunga, Cagatay Goncu", "title": "Space-Time VON CRAMM: Evaluating Decision-Making in Tennis with\n  Variational generatiON of Complete Resolution Arcs via Mixture Modeling", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sports tracking data are the high-resolution spatiotemporal observations of a\ncompetitive event. The growing collection of these data in professional sport\nallows us to address a fundamental problem of modern sport: how to attribute\nvalue to individual actions? Taking advantage of the smoothness of ball and\nplayer movement in tennis, we present a functional data framework for\nestimating expected shot value (ESV) in continuous time. Our approach is a\nthree-step recipe: 1) a generative model for a full-resolution functional\nrepresentation of ball and player trajectories using an infinite Bayesian\nGaussian mixture model (GMM), 2) conditioning of the GMM on observed positional\ndata, and 3) the prediction of shot outcomes given the functional encoding of a\nshot event. From the ESV we derive three metrics of central interest: value\nadded with shot taking (VAST), Shot IQ, and value added with court coverage\n(VACC), which respectively attribute value to shot execution, shot selection\nand movement around the court. We rate player performance at the 2019 US Open\non these advanced metrics and show how each adds a novel perspective to\nperformance evaluation in tennis that goes beyond simple counts of outcomes by\nquantitatively assessing the decisions players make throughout a point.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 04:14:41 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Kovalchik", "Stephanie", ""], ["Ingram", "Martin", ""], ["Weeratunga", "Kokum", ""], ["Goncu", "Cagatay", ""]]}, {"id": "2005.12857", "submitter": "Christian Molkenthin", "authors": "Christian Molkenthin (1), Christian Donner (2), Sebastian Reich (1),\n  Gert Z\\\"oller (1), Sebastian Hainzl (3), Matthias Holschneider (1) and\n  Manfred Opper (4)", "title": "GP-ETAS: Semiparametric Bayesian inference for the spatio-temporal\n  Epidemic Type Aftershock Sequence model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.geo-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The spatio-temporal Epidemic Type Aftershock Sequence (ETAS) model is widely\nused to describe the self-exciting nature of earthquake occurrences. While\ntraditional inference methods provide only point estimates of the model\nparameters, we aim at a full Bayesian treatment of model inference, allowing\nnaturally to incorporate prior knowledge and uncertainty quantification of the\nresulting estimates. Therefore, we introduce a highly flexible, non-parametric\nrepresentation for the spatially varying ETAS background intensity through a\nGaussian process (GP) prior. Combined with classical triggering functions this\nresults in a new model formulation, namely the GP-ETAS model. We enable\ntractable and efficient Gibbs sampling by deriving an augmented form of the\nGP-ETAS inference problem. This novel sampling approach allows us to assess the\nposterior model variables conditioned on observed earthquake catalogues, i.e.,\nthe spatial background intensity and the parameters of the triggering function.\nEmpirical results on two synthetic data sets indicate that GP-ETAS outperforms\nstandard models and thus demonstrate the predictive power for observed\nearthquake catalogues including uncertainty quantification for the estimated\nparameters. Finally, a case study for the l'Aquila region, Italy, with the\ndevastating event on 6 April 2009, is presented.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 16:39:22 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Molkenthin", "Christian", ""], ["Donner", "Christian", ""], ["Reich", "Sebastian", ""], ["Z\u00f6ller", "Gert", ""], ["Hainzl", "Sebastian", ""], ["Holschneider", "Matthias", ""], ["Opper", "Manfred", ""]]}, {"id": "2005.12879", "submitter": "Nicholas Randolph", "authors": "E. Benjamin Randall, Nicholas Z. Randolph, Alen Alexanderian, Mette S.\n  Olufsen", "title": "Global sensitivity analysis informed model reduction and selection\n  applied to a Valsalva maneuver model", "comments": null, "journal-ref": null, "doi": "10.1016/j.jtbi.2021.110759", "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we develop a methodology for model reduction and selection\ninformed by global sensitivity analysis (GSA) methods. We apply these\ntechniques to a control model that takes systolic blood pressure and thoracic\ntissue pressure data as inputs and predicts heart rate in response to the\nValsalva maneuver (VM). The study compares four GSA methods based on Sobol'\nindices (SIs) quantifying the parameter influence on the difference between the\nmodel output and the heart rate data. The GSA methods include standard scalar\nSIs determining the average parameter influence over the time interval studied\nand three time-varying methods analyzing how parameter influence changes over\ntime. The time-varying methods include a new technique, termed limited-memory\nSIs, predicting parameter influence using a moving window approach. Using the\nlimited-memory SIs, we perform model reduction and selection to analyze the\nnecessity of modeling both the aortic and carotid baroreceptor regions in\nresponse to the VM. We compare the original model to three systematically\nreduced models including (i) the aortic and carotid regions, (ii) the aortic\nregion only, and (iii) the carotid region only. Model selection is done\nquantitatively using the Akaike and Bayesian Information Criteria and\nqualitatively by comparing the neurological predictions. Results show that it\nis necessary to incorporate both the aortic and carotid regions to model the\nVM.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 17:18:34 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 21:47:40 GMT"}, {"version": "v3", "created": "Fri, 14 May 2021 15:48:22 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Randall", "E. Benjamin", ""], ["Randolph", "Nicholas Z.", ""], ["Alexanderian", "Alen", ""], ["Olufsen", "Mette S.", ""]]}, {"id": "2005.12881", "submitter": "Johannes Bracher", "authors": "Johannes Bracher, Evan L. Ray, Tilmann Gneiting and Nicholas G. Reich", "title": "Evaluating epidemic forecasts in an interval format", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pcbi.1008618", "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For practical reasons, many forecasts of case, hospitalization and death\ncounts in the context of the current COVID-19 pandemic are issued in the form\nof central predictive intervals at various levels. This is also the case for\nthe forecasts collected in the COVID-19 Forecast Hub\n(https://covid19forecasthub.org/). Forecast evaluation metrics like the\nlogarithmic score, which has been applied in several infectious disease\nforecasting challenges, are then not available as they require full predictive\ndistributions. This article provides an overview of how established methods for\nthe evaluation of quantile and interval forecasts can be applied to epidemic\nforecasts in this format. Specifically, we discuss the computation and\ninterpretation of the weighted interval score, which is a proper score that\napproximates the continuous ranked probability score. It can be interpreted as\na generalization of the absolute error to probabilistic forecasts and allows\nfor a decomposition into a measure of sharpness and penalties for over- and\nunderprediction.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 17:19:26 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2020 14:37:14 GMT"}, {"version": "v3", "created": "Fri, 8 Jan 2021 20:25:15 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Bracher", "Johannes", ""], ["Ray", "Evan L.", ""], ["Gneiting", "Tilmann", ""], ["Reich", "Nicholas G.", ""]]}, {"id": "2005.13005", "submitter": "Roberto Baviera", "authors": "Roberto Baviera and Giuseppe Messuti", "title": "Daily Middle-Term Probabilistic Forecasting of Power Consumption in\n  North-East England", "comments": "26 pages, 6 figures. arXiv admin note: text overlap with\n  arXiv:2006.16388", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic forecasting of power consumption in a middle-term horizon\n(months to a year) is a main challenge in the energy sector. It plays a key\nrole in planning future generation plants and transmission grid. We propose a\nnew model that incorporates trend and seasonality features as in traditional\ntime-series analysis and weather conditions as explicative variables in a\nparsimonious machine learning approach, known as Gaussian Process. Applying to\na daily power consumption dataset in North East England provided by one of the\nlargest energy suppliers, we obtain promising results in Out-of-Sample density\nforecasts up to one year, even using a small dataset, with only a two-year\nIn-Sample data. In order to verify the quality of the achieved power\nconsumption probabilistic forecast we consider measures that are common in the\nenergy sector as pinball loss and Winkler score and backtesting conditional and\nunconditional tests, standard in the banking sector after the introduction of\nBasel II Accords.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 20:05:10 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 07:42:59 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Baviera", "Roberto", ""], ["Messuti", "Giuseppe", ""]]}, {"id": "2005.13011", "submitter": "Sara Algeri", "authors": "Sara Algeri and Xiangyu Zhang", "title": "Exhaustive goodness-of-fit via smoothed inference and graphics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical tests of goodness-of-fit aim to validate the conformity of a\npostulated model to the data under study. Given their inferential nature, they\ncan be considered a crucial step in confirmatory data analysis. In their\nstandard formulation, however, they do not allow exploring how the hypothesized\nmodel deviates from the truth nor do they provide any insight into how the\nrejected model could be improved to better fit the data. The main goal of this\nwork is to establish a comprehensive framework for goodness-of-fit which\nnaturally integrates modeling, estimation, inference, and graphics. Modeling\nand estimation focus on a novel formulation of smooth tests that easily extends\nto arbitrary distributions, either continuous or discrete. Inference and\nadequate post-selection adjustments are performed via a specially designed\nsmoothed bootstrap and the results are summarized via an exhaustive graphical\ntool called CD-plot.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 20:14:17 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 18:21:35 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Algeri", "Sara", ""], ["Zhang", "Xiangyu", ""]]}, {"id": "2005.13107", "submitter": "Zichao Wang", "authors": "Zichao Wang, Yi Gu, Andrew Lan, Richard Baraniuk", "title": "VarFA: A Variational Factor Analysis Framework For Efficient Bayesian\n  Learning Analytics", "comments": "edm 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CY cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose VarFA, a variational inference factor analysis framework that\nextends existing factor analysis models for educational data mining to\nefficiently output uncertainty estimation in the model's estimated factors.\nSuch uncertainty information is useful, for example, for an adaptive testing\nscenario, where additional tests can be administered if the model is not quite\ncertain about a students' skill level estimation. Traditional Bayesian\ninference methods that produce such uncertainty information are computationally\nexpensive and do not scale to large data sets. VarFA utilizes variational\ninference which makes it possible to efficiently perform Bayesian inference\neven on very large data sets. We use the sparse factor analysis model as a case\nstudy and demonstrate the efficacy of VarFA on both synthetic and real data\nsets. VarFA is also very general and can be applied to a wide array of factor\nanalysis models.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 01:03:07 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 20:46:06 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Wang", "Zichao", ""], ["Gu", "Yi", ""], ["Lan", "Andrew", ""], ["Baraniuk", "Richard", ""]]}, {"id": "2005.13199", "submitter": "Riko Kelter", "authors": "Riko Kelter", "title": "Bayesian model selection in the $\\mathcal{M}$-open setting --\n  Approximate posterior inference and probability-proportional-to-size\n  subsampling for efficient large-scale leave-one-out cross-validation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparison of competing statistical models is an essential part of\npsychological research. From a Bayesian perspective, various approaches to\nmodel comparison and selection have been proposed in the literature. However,\nthe applicability of these approaches strongly depends on the assumptions about\nthe model space $\\mathcal{M}$, the so-called model view. Furthermore,\ntraditional methods like leave-one-out cross-validation (LOO-CV) estimate the\nexpected log predictive density (ELPD) of a model to investigate how the model\ngeneralises out-of-sample, which quickly becomes computationally inefficient\nwhen sample size becomes large. Here, we provide a tutorial on approximate\nPareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO),\na computationally efficient method for Bayesian model comparison. First, we\ndiscuss several model views and the available Bayesian model comparison methods\nin each. We then use Bayesian logistic regression as a running example how to\napply the method in practice, and show that it outperforms other methods like\nLOO-CV or information criteria in terms of computational effort while providing\nsimilarly accurate ELPD estimates. In a second step, we show how even\nlarge-scale models can be compared efficiently by using posterior\napproximations in combination with probability-proportional-to-size\nsubsampling. We show how to compare competing models based on the ELPD\nestimates provided, and how to conduct posterior predictive checks to safeguard\nagainst overconfidence in one of the models under consideration. We conclude\nthat the method is attractive for mathematical psychologists who aim at\ncomparing several competing statistical models, which are possibly\nhigh-dimensional and in the big-data regime.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 06:57:27 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Kelter", "Riko", ""]]}, {"id": "2005.13404", "submitter": "Benjamin Laufer", "authors": "Benjamin Laufer", "title": "Compounding Injustice: History and Prediction in Carceral\n  Decision-Making", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Risk assessment algorithms in criminal justice put people's lives at the\ndiscretion of a simple statistical tool. This thesis explores how algorithmic\ndecision-making in criminal policy can exhibit feedback effects, where\ndisadvantage accumulates among those deemed 'high risk' by the state. Evidence\nfrom Philadelphia suggests that risk - and, by extension, criminality - is not\nfundamental or in any way exogenous to political decision-making. A close look\nat the geographical and demographic properties of risk calls into question the\ncurrent practice of prediction in criminal policy. Using court docket summaries\nfrom Philadelphia, we find evidence of a criminogenic effect of incarceration,\neven controlling for existing determinants of 'criminal risk'. With evidence\nthat criminal treatment can influence future criminal convictions, we explore\nthe theoretical implications of compounding effects in repeated carceral\ndecisions.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 14:51:50 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Laufer", "Benjamin", ""]]}, {"id": "2005.13416", "submitter": "L\\'aszl\\'o Csat\\'o", "authors": "L\\'aszl\\'o Csat\\'o and D\\'ora Gr\\'eta Petr\\'oczy", "title": "Bibliometric indices as a measure of long-term competitive balance in\n  knockout tournaments", "comments": "23 pages, 9 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.GT econ.GN q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We argue for the application of bibliometric indices to quantify long-term\nuncertainty of outcome in sports. The Euclidean index is proposed to reward\nquality over quantity, while the rectangle index can be an appropriate measure\nof core performance. Their differences are highlighted through an axiomatic\nanalysis and several examples. Our approach also requires a weighting scheme to\ncompare different achievements. The methodology is illustrated by studying the\nknockout stage of the UEFA Champions League in the 16 seasons played between\n2003 and 2019: club and country performances as well as three types of\ncompetitive balance are considered. Measuring competition at the level of\nnational associations is a novelty. All results are remarkably robust\nconcerning the bibliometric index and the assigned weights. Inequality has not\nincreased among the elite clubs and between the national associations, however,\nit has changed within some countries. Since the performances of national\nassociations are more stable than the results of individual clubs, it would be\nbetter to build the seeding in the UEFA Champions League group stage upon\nassociation coefficients adjusted for league finishing positions rather than\nclub coefficients.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 15:20:54 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2020 10:41:55 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Csat\u00f3", "L\u00e1szl\u00f3", ""], ["Petr\u00f3czy", "D\u00f3ra Gr\u00e9ta", ""]]}, {"id": "2005.13417", "submitter": "Tim Janke", "authors": "Tim Janke and Florian Steinke", "title": "Probabilistic multivariate electricity price forecasting using implicit\n  generative ensemble post-processing", "comments": "To be presented at the 16th International Conference on Probabilistic\n  Methods Applied to Power Systems 2020 (PMAPS 2020)", "journal-ref": null, "doi": "10.1109/PMAPS47429.2020.9183687", "report-no": null, "categories": "stat.AP econ.EM q-fin.RM q-fin.ST stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reliable estimation of forecast uncertainties is crucial for\nrisk-sensitive optimal decision making. In this paper, we propose implicit\ngenerative ensemble post-processing, a novel framework for multivariate\nprobabilistic electricity price forecasting. We use a likelihood-free implicit\ngenerative model based on an ensemble of point forecasting models to generate\nmultivariate electricity price scenarios with a coherent dependency structure\nas a representation of the joint predictive distribution. Our ensemble\npost-processing method outperforms well-established model combination\nbenchmarks. This is demonstrated on a data set from the German day-ahead\nmarket. As our method works on top of an ensemble of domain-specific expert\nmodels, it can readily be deployed to other forecasting tasks.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 15:22:10 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Janke", "Tim", ""], ["Steinke", "Florian", ""]]}, {"id": "2005.13463", "submitter": "Akbir M Khan Mr", "authors": "Akbir Khan", "title": "Latent Racial Bias -- Evaluating Racism in Police Stop-and-Searches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the latent racial bias, a metric and method to\nevaluate the racial bias within specific events. For the purpose of this paper\nwe explore the British Home Office dataset of stop-and-search incidents. We\nexplore the racial bias in the choice of targets, using a number of statistical\nmodels such as graphical probabilistic and TrueSkill Ranking. Firstly, we\npropose a probabilistic graphical models for modelling racial bias within\nstop-and-searches and explore varying priors. Secondly using our inference\nmethods, we produce a set of probability distributions for different\nracial/ethnic groups based on said model and data. Finally, we produce a set of\nexamples of applications of this model, predicting biases not only for stops\nbut also in the reactive response by law officers.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 20:39:37 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Khan", "Akbir", ""]]}, {"id": "2005.13490", "submitter": "Claudia Neves", "authors": "Evandro Konzen, Claudia Neves, Philip Jonathan", "title": "Modelling non-stationary extremes of storm severity: a tale of two\n  approaches", "comments": "Keywords: Circular statistics, extreme quantile estimation, kernel\n  smoothing, parametric, semi-parametric, significant wave height, threshold\n  selection", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models for extreme values accommodating non-stationarity have been amply\nstudied and evaluated from a parametric perspective. Whilst these models are\nflexible, in the sense that many parametrizations can be explored, they assume\nan asymptotic distribution as the proper fit to observations from the tail.\nThis paper provides a holistic approach to the modelling of non-stationary\nextreme events by iterating between parametric and semi-parametric approaches,\nthus providing an automatic procedure to estimate a moving threshold with\nrespect to a periodic covariate in circular data. By exploiting advantages and\nmitigating pitfalls of each approach, a unified framework is provided as the\nbackbone for estimating extreme quantiles, including that of the $T$-year level\nand finite right endpoint, which seeks to optimize bias-variance trade-off. To\nthis end, two tuning parameters related to the spread of peaks over threshold\nare introduced. We provide guidance for applying the methodology to the\ndirectional modelling of hindcast storm peak significant wave heights recorded\nin the North Sea. Although the theoretical underpinning for adaptation of\nwell-known estimators in statistics of extremes to circular data is given in\nsome detail, the derivation of their asymptotic properties lays beyond the\nscope of this paper. A bootstrap technique is implemented for obtaining\ndirection-driven confidence bounds in such a way as to account for the relevant\nboundary restrictions with minimal sensitivity to initial point. This provides\na template for other applications where the analysis of directional extremes is\nof importance.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 16:53:20 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Konzen", "Evandro", ""], ["Neves", "Claudia", ""], ["Jonathan", "Philip", ""]]}, {"id": "2005.13650", "submitter": "Daniel Fraiman", "authors": "In\\'es Armend\\'ariz, Pablo A. Ferrari, Daniel Fraiman, Jos\\'e M.\n  Mart\\'inez, Silvina Ponce Dawson", "title": "Group testing with nested pools", "comments": "23 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST physics.data-an q-bio.QM stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to identify the infected individuals of a population, their samples\nare divided in equally sized groups called pools and a single laboratory test\nis applied to each pool. Individuals whose samples belong to pools that test\nnegative are declared healthy, while each pool that tests positive is divided\ninto smaller, equally sized pools which are tested in the next stage. This\nscheme is called adaptive, because the composition of the pools at each stage\ndepends on results from previous stages, and nested because each pool is a\nsubset of a pool of the previous stage. Is the infection probability $p$ is not\nsmaller than $1-3^{-1/3}$ it is best to test each sample (no pooling). If\n$p<1-3^{-1/3}$, we compute the mean $D_k(m,p)$ and the variance of the number\nof tests per individual as a function of the pool sizes $m=(m_1,\\dots,m_k)$ in\nthe first $k$ stages; in the $(k+1)$-th stage all remaining samples are tested.\nThe case $k=1$ was proposed by Dorfman in his seminal paper in 1943. The goal\nis to minimize $D_k(m,p)$, which is called the cost associated to~$m$. We show\nthat for $p\\in (0, 1-3^{-1/3})$ the optimal choice is one of four possible\nschemes, which are explicitly described. For $p>2^{-51}$ we show overwhelming\nnumerical evidence that the best choice is $(3^k\\text{ or\n}3^{k-1}4,3^{k-1},\\dots,3^2,3 )$, with a precise description of the range of\n$p$'s where each holds. We then focus on schemes of the type $(3^k,\\dots,3)$,\nand estimate that the cost of the best scheme of this type for $p$, determined\nby the choice of $k=k_3(p)$, is of order $O\\big(p\\log(1/p)\\big)$. This is the\nsame order as that of the cost of the optimal scheme, and the difference of\nthese costs is explicitly bounded. As an example, for $p=0.02$ the optimal\nchoice is $k=3$, $m=(27,9,3)$, with cost $0.20$; that is, the mean number of\ntests required to screen 100 individuals is 20.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 21:00:31 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2020 23:43:00 GMT"}, {"version": "v3", "created": "Thu, 19 Nov 2020 22:17:46 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Armend\u00e1riz", "In\u00e9s", ""], ["Ferrari", "Pablo A.", ""], ["Fraiman", "Daniel", ""], ["Mart\u00ednez", "Jos\u00e9 M.", ""], ["Dawson", "Silvina Ponce", ""]]}, {"id": "2005.13739", "submitter": "Tong Chen", "authors": "Tong Chen, Thomas Lumley", "title": "Optimal multi-wave sampling for regression modelling in two-phase\n  designs", "comments": null, "journal-ref": null, "doi": "10.1002/sim.8760", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-phase designs involve measuring extra variables on a subset of the cohort\nwhere some variables are already measured. The goal of two-phase designs is to\nchoose a subsample of individuals from the cohort and analyse that subsample\nefficiently. It is of interest to obtain an optimal design that gives the most\nefficient estimates of regression parameters. In this paper, we propose a\nmulti-wave sampling design to approximate the optimal design for design-based\nestimators. Influences functions are used to compute the optimal sampling\nallocations. We propose to use informative priors on regression parameters to\nderive the wave-1 sampling probabilities because any pre-specified sampling\nprobabilities may be far from optimal and decrease efficiency. Generalised\nraking is used in statistical analysis. We show that a two-wave sampling with\nreasonable informative priors will end up with higher precision for the\nparameter of interest and be close to the underlying optimal design.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 02:09:05 GMT"}, {"version": "v2", "created": "Sun, 23 Aug 2020 01:39:45 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Chen", "Tong", ""], ["Lumley", "Thomas", ""]]}, {"id": "2005.13979", "submitter": "Tim Friede", "authors": "Cornelia Ursula Kunz, Silke J\\\"orgens, Frank Bretz, Nigel Stallard,\n  Kelly Van Lancker, Dong Xi, Sarah Zohar, Christoph Gerlinger, Tim Friede", "title": "Clinical trials impacted by the COVID-19 pandemic: Adaptive designs to\n  the rescue?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Very recently the new pathogen severe acute respiratory syndrome coronavirus\n2 (SARS-CoV-2) was identified and the coronavirus disease 2019 (COVID-19)\ndeclared a pandemic by the World Health Organization. The pandemic has a number\nof consequences for the ongoing clinical trials in non-COVID-19 conditions.\nMotivated by four currently ongoing clinical trials in a variety of disease\nareas we illustrate the challenges faced by the pandemic and sketch out\npossible solutions including adaptive designs. Guidance is provided on (i)\nwhere blinded adaptations can help; (ii) how to achieve type I error rate\ncontrol, if required; (iii) how to deal with potential treatment effect\nheterogeneity; (iv) how to utilize early readouts; and (v) how to utilize\nBayesian techniques. In more detail approaches to resizing a trial affected by\nthe pandemic are developed including considerations to stop a trial early, the\nuse of group-sequential designs or sample size adjustment. All methods\nconsidered are implemented in a freely available R shiny app. Furthermore,\nregulatory and operational issues including the role of data monitoring\ncommittees are discussed.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 13:32:08 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Kunz", "Cornelia Ursula", ""], ["J\u00f6rgens", "Silke", ""], ["Bretz", "Frank", ""], ["Stallard", "Nigel", ""], ["Van Lancker", "Kelly", ""], ["Xi", "Dong", ""], ["Zohar", "Sarah", ""], ["Gerlinger", "Christoph", ""], ["Friede", "Tim", ""]]}, {"id": "2005.14057", "submitter": "Andrii Babii", "authors": "Andrii Babii and Eric Ghysels and Jonas Striaukas", "title": "Machine Learning Time Series Regressions with an Application to\n  Nowcasting", "comments": "Portions of this work previously appeared as arXiv:1912.06307v1 which\n  has been split into two articles", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces structured machine learning regressions for\nhigh-dimensional time series data potentially sampled at different frequencies.\nThe sparse-group LASSO estimator can take advantage of such time series data\nstructures and outperforms the unstructured LASSO. We establish oracle\ninequalities for the sparse-group LASSO estimator within a framework that\nallows for the mixing processes and recognizes that the financial and the\nmacroeconomic data may have heavier than exponential tails. An empirical\napplication to nowcasting US GDP growth indicates that the estimator performs\nfavorably compared to other alternatives and that text data can be a useful\naddition to more traditional numerical data.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 14:42:58 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 00:50:12 GMT"}, {"version": "v3", "created": "Wed, 2 Dec 2020 19:53:54 GMT"}, {"version": "v4", "created": "Sat, 12 Dec 2020 18:30:09 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Babii", "Andrii", ""], ["Ghysels", "Eric", ""], ["Striaukas", "Jonas", ""]]}, {"id": "2005.14083", "submitter": "Parker Holzer", "authors": "Parker Holzer, Jessi Cisewski-Kehe, Debra Fischer, Lily Zhao", "title": "A Hermite-Gaussian Based Radial Velocity Estimation Method", "comments": "48 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.EP astro-ph.IM stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As the first successful technique used to detect exoplanets orbiting distant\nstars, the Radial Velocity Method aims to detect a periodic Doppler shift in a\nstar's spectrum. We introduce a new, mathematically rigorous, approach to\ndetect such a signal that accounts for functional relationships of neighboring\nwavelengths, minimizes the role of wavelength interpolation, accounts for\nheteroskedastic noise, and easily allows for statistical inference. Using\nHermite-Gaussian functions, we show that the problem of detecting a Doppler\nshift in the spectrum can be reduced to linear regression in many settings. A\nsimulation study demonstrates that the proposed method is able to accurately\nestimate an individual spectrum's radial velocity with precision below 0.3 m/s.\nFurthermore, the new method outperforms the traditional Cross-Correlation\nFunction approach by reducing the root mean squared error up to 15 cm/s. The\nproposed method is also demonstrated on a new set of observations from the\nEXtreme PREcision Spectrometer (EXPRES) for the star 51 Pegasi, and\nsuccessfully recovers estimates that agree well with previous studies of this\nplanetary system. Data and Python3 code associated with this work can be found\nat https://github.com/parkerholzer/hgrv_method. The method is also implemented\nin the open source R package rvmethod.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 15:25:42 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Holzer", "Parker", ""], ["Cisewski-Kehe", "Jessi", ""], ["Fischer", "Debra", ""], ["Zhao", "Lily", ""]]}, {"id": "2005.14168", "submitter": "Paul Schrimpf", "authors": "Victor Chernozhukov, Hiroyuki Kasaha, Paul Schrimpf", "title": "Causal Impact of Masks, Policies, Behavior on Early Covid-19 Pandemic in\n  the U.S", "comments": null, "journal-ref": "Journal of Econometrics (2020)", "doi": "10.1016/j.jeconom.2020.09.003", "report-no": null, "categories": "econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper evaluates the dynamic impact of various policies adopted by US\nstates on the growth rates of confirmed Covid-19 cases and deaths as well as\nsocial distancing behavior measured by Google Mobility Reports, where we take\ninto consideration people's voluntarily behavioral response to new information\nof transmission risks. Our analysis finds that both policies and information on\ntransmission risks are important determinants of Covid-19 cases and deaths and\nshows that a change in policies explains a large fraction of observed changes\nin social distancing behavior. Our counterfactual experiments suggest that\nnationally mandating face masks for employees on April 1st could have reduced\nthe growth rate of cases and deaths by more than 10 percentage points in late\nApril, and could have led to as much as 17 to 55 percent less deaths nationally\nby the end of May, which roughly translates into 17 to 55 thousand saved lives.\nOur estimates imply that removing non-essential business closures (while\nmaintaining school closures, restrictions on movie theaters and restaurants)\ncould have led to -20 to 60 percent more cases and deaths by the end of May. We\nalso find that, without stay-at-home orders, cases would have been larger by 25\nto 170 percent, which implies that 0.5 to 3.4 million more Americans could have\nbeen infected if stay-at-home orders had not been implemented. Finally, not\nhaving implemented any policies could have led to at least a 7 fold increase\nwith an uninformative upper bound in cases (and deaths) by the end of May in\nthe US, with considerable uncertainty over the effects of school closures,\nwhich had little cross-sectional variation.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 17:32:31 GMT"}, {"version": "v2", "created": "Sat, 30 May 2020 04:05:42 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2020 04:09:58 GMT"}, {"version": "v4", "created": "Mon, 19 Oct 2020 22:59:19 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Chernozhukov", "Victor", ""], ["Kasaha", "Hiroyuki", ""], ["Schrimpf", "Paul", ""]]}, {"id": "2005.14181", "submitter": "Hugo Carvalho", "authors": "Hugo Tremonte de Carvalho, Fl\\'avio Rainho \\'Avila, Luiz Wagner\n  Pereira Biscainho", "title": "Bayesian Restoration of Audio Degraded by Low-Frequency Pulses Modeled\n  via Gaussian Process", "comments": "14 pages, 7 figures, 4 tables. Submitted to IEEE Journal of Selected\n  Topics in Signal Processing - Special Issue \"Reconstruction of audio from\n  incomplete or highly degraded observations\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.SD eess.SP stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common defect found when reproducing old vinyl and gramophone recordings\nwith mechanical devices are the long pulses with significant low-frequency\ncontent caused by the interaction of the arm-needle system with deep scratches\nor even breakages on the media surface. Previous approaches to their\nsuppression on digital counterparts of the recordings depend on a prior\nestimation of the pulse location, usually performed via heuristic methods. This\npaper proposes a novel Bayesian approach capable of jointly estimating the\npulse location; interpolating the almost annihilated signal underlying the\nstrong discontinuity that initiates the pulse; and also estimating the long\npulse tail by a simple Gaussian Process, allowing its suppression from the\ncorrupted signal. The posterior distribution for the model parameters as well\nfor the pulse is explored via Markov-Chain Monte Carlo (MCMC) algorithms.\nControlled experiments indicate that the proposed method, while requiring\nsignificantly less user intervention, achieves perceptual results similar to\nthose of previous approaches and performs well when dealing with naturally\ndegraded signals.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 17:52:26 GMT"}, {"version": "v2", "created": "Sat, 26 Sep 2020 14:11:51 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["de Carvalho", "Hugo Tremonte", ""], ["\u00c1vila", "Fl\u00e1vio Rainho", ""], ["Biscainho", "Luiz Wagner Pereira", ""]]}, {"id": "2005.14186", "submitter": "Stephane Gaubert", "authors": "St\\'ephane Gaubert, Marianne Akian, Xavier Allamigeon, Marin Boyet,\n  Baptiste Colin, Th\\'eotime Grohens, Laurent Massouli\\'e, David P. Parsons,\n  Fr\\'ed\\'eric Adnet, \\'Erick Chanzy, Laurent Goix, Fr\\'ed\\'eric Lapostolle,\n  \\'Eric Lecarpentier, Christophe Leroy, Thomas Loeb, Jean-S\\'ebastien Marx,\n  Caroline T\\'elion, Laurent Tr\\'eluyer and Pierre Carli", "title": "Understanding and monitoring the evolution of the Covid-19 epidemic from\n  medical emergency calls: the example of the Paris area", "comments": "Changes v1->v2: Section 7 expanded. Changes v2->v3: bibliography\n  expanded; minor improvements and corrections", "journal-ref": "Comptes Rendus -- Math\\'ematique, Volume 358, issue 7 (2020), p.\n  843-875", "doi": "10.5802/crmath.99", "report-no": null, "categories": "stat.AP cs.SI math.DS physics.soc-ph q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We portray the evolution of the Covid-19 epidemic during the crisis of\nMarch-April 2020 in the Paris area, by analyzing the medical emergency calls\nreceived by the EMS of the four central departments of this area (Centre 15 of\nSAMU 75, 92, 93 and 94). Our study reveals strong dissimilarities between these\ndepartments. We show that the logarithm of each epidemic observable can be\napproximated by a piecewise linear function of time. This allows us to\ndistinguish the different phases of the epidemic, and to identify the delay\nbetween sanitary measures and their influence on the load of EMS. This also\nleads to an algorithm, allowing one to detect epidemic resurgences. We rely on\na transport PDE epidemiological model, and we use methods from Perron-Frobenius\ntheory and tropical geometry.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 17:56:14 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 07:23:44 GMT"}, {"version": "v3", "created": "Mon, 20 Jul 2020 16:48:42 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Gaubert", "St\u00e9phane", ""], ["Akian", "Marianne", ""], ["Allamigeon", "Xavier", ""], ["Boyet", "Marin", ""], ["Colin", "Baptiste", ""], ["Grohens", "Th\u00e9otime", ""], ["Massouli\u00e9", "Laurent", ""], ["Parsons", "David P.", ""], ["Adnet", "Fr\u00e9d\u00e9ric", ""], ["Chanzy", "\u00c9rick", ""], ["Goix", "Laurent", ""], ["Lapostolle", "Fr\u00e9d\u00e9ric", ""], ["Lecarpentier", "\u00c9ric", ""], ["Leroy", "Christophe", ""], ["Loeb", "Thomas", ""], ["Marx", "Jean-S\u00e9bastien", ""], ["T\u00e9lion", "Caroline", ""], ["Tr\u00e9luyer", "Laurent", ""], ["Carli", "Pierre", ""]]}, {"id": "2005.14230", "submitter": "Nathaniel Bastian PhD", "authors": "Marc Chal\\'e, Nathaniel D. Bastian, Jeffery Weir", "title": "Algorithm Selection Framework for Cyber Attack Detection", "comments": "6 pages, 7 figures, 1 table, accepted to WiseML '20", "journal-ref": null, "doi": "10.1145/3395352.3402623", "report-no": null, "categories": "cs.CR cs.AI cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of cyber threats against both wired and wireless computer systems\nand other components of the Internet of Things continues to increase annually.\nIn this work, an algorithm selection framework is employed on the NSL-KDD data\nset and a novel paradigm of machine learning taxonomy is presented. The\nframework uses a combination of user input and meta-features to select the best\nalgorithm to detect cyber attacks on a network. Performance is compared between\na rule-of-thumb strategy and a meta-learning strategy. The framework removes\nthe conjecture of the common trial-and-error algorithm selection method. The\nframework recommends five algorithms from the taxonomy. Both strategies\nrecommend a high-performing algorithm, though not the best performing. The work\ndemonstrates the close connectedness between algorithm selection and the\ntaxonomy for which it is premised.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 18:49:29 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Chal\u00e9", "Marc", ""], ["Bastian", "Nathaniel D.", ""], ["Weir", "Jeffery", ""]]}, {"id": "2005.14263", "submitter": "Jonne Pohjankukka Dr.", "authors": "Jonne Pohjankukka, Tapio Pahikkala, Paavo Nevalainen, Jukka Heikkonen", "title": "Estimating the Prediction Performance of Spatial Models via Spatial\n  k-Fold Cross Validation", "comments": "18 pages, 12 figures, 1 table", "journal-ref": "International Journal of Geographical Information Science, Volume\n  31, 2017, Issue 10, pages 2001-2019", "doi": "10.1080/13658816.2017.1346255", "report-no": null, "categories": "stat.AP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning one often assumes the data are independent when\nevaluating model performance. However, this rarely holds in practise.\nGeographic information data sets are an example where the data points have\nstronger dependencies among each other the closer they are geographically. This\nphenomenon known as spatial autocorrelation (SAC) causes the standard cross\nvalidation (CV) methods to produce optimistically biased prediction performance\nestimates for spatial models, which can result in increased costs and accidents\nin practical applications. To overcome this problem we propose a modified\nversion of the CV method called spatial k-fold cross validation (SKCV), which\nprovides a useful estimate for model prediction performance without optimistic\nbias due to SAC. We test SKCV with three real world cases involving open\nnatural data showing that the estimates produced by the ordinary CV are up to\n40% more optimistic than those of SKCV. Both regression and classification\ncases are considered in our experiments. In addition, we will show how the SKCV\nmethod can be applied as a criterion for selecting data sampling density for\nnew research area.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 19:55:18 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Pohjankukka", "Jonne", ""], ["Pahikkala", "Tapio", ""], ["Nevalainen", "Paavo", ""], ["Heikkonen", "Jukka", ""]]}, {"id": "2005.14303", "submitter": "Trevor Hefley", "authors": "Trevor Hefley", "title": "Model selection for ecological community data using tree shrinkage\n  priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers and managers model ecological communities to infer the biotic and\nabiotic variables that shape species' ranges, habitat use, and co-occurrence\nwhich, in turn, are used to support management decisions and test ecological\ntheories. Recently, species distribution models were developed for and applied\nto data from ecological communities. Model development and selection for\necological community data is difficult because a high level of complexity is\ndesired and achieved by including numerous parameters, which can degrade\npredictive accuracy and be challenging to interpret and communicate. Like other\nstatistical models, multi-species distribution models can be overparameterized.\nRegularization is a technique that optimizes predictive accuracy by shrinking\nor eliminating model parameters. For Bayesian models, the prior distribution\nautomatically regularizes parameters. We propose a tree shrinkage prior for\nBayesian multi-species distributions models that performs regularization and\nreduces the number of regression coefficients associated with predictor\nvariables. Using this prior, the number of regression coefficients in\nmulti-species distributions models is reduced by estimation of unique\nregression coefficients for a smaller number of guilds rather than a larger\nnumber of species. We demonstrated our tree shrinkage prior using examples of\npresence-absence data for six species of aquatic vegetation and relative\nabundance data for 15 species of fish. Our results show that the tree shrinkage\nprior can increase the predictive accuracy of multi-species distribution models\nand enable researchers to infer the number and species composition of guilds\nfrom ecological community data.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 21:26:14 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Hefley", "Trevor", ""]]}, {"id": "2005.14316", "submitter": "Trevor Hefley", "authors": "Trevor J. Hefley, W. Alice Boyle, Narmadha M. Mohankumar", "title": "Accounting for location uncertainty in distance sampling data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ecologists use distance sampling to estimate the abundance of plants and\nanimals while correcting for undetected individuals. By design, data collection\nis simplified by requiring only the distances from a transect to the detected\nindividuals be recorded. Compared to traditional design-based methods that\nrequire restrictive assumption and limit the use of distance sampling data,\nmodel-based approaches enable broader applications such as spatial prediction,\ninferring species-habitat relationships, unbiased estimation from\npreferentially sampled transects, and integration into multi-type data models.\nUnfortunately, model-based approaches require the exact location of each\ndetected individual in order to incorporate environmental and habitat\ncharacteristics as predictor variables. We modified model-based methods for\ndistance sampling data by including a probability distribution that accounts\nfor location uncertainty generated when only the distances are recorded. We\ntested and demonstrated our method using a simulation experiment and by\nmodeling the habitat use of Dickcissels (Spiza americana) using distance\nsampling data collected from the Konza Prairie in Kansas, USA. Our results\nshowed that ignoring location uncertainty can result in biased coefficient\nestimates and predictions. However, accounting for location uncertainty\nremedies the issue and results in reliable inference and prediction. Like other\ntypes of measurement error, hierarchical models can accommodate the data\ncollection process thereby enabling reliable inference. Our approach is a\nsignificant advancement for the analysis of distance sampling data because it\nremedies the deleterious effects of location uncertainty and requires only\ndistances be recorded. In turn, this enables historical distance sampling data\nsets to be compatible with modern data collection and modeling practices.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 22:06:23 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Hefley", "Trevor J.", ""], ["Boyle", "W. Alice", ""], ["Mohankumar", "Narmadha M.", ""]]}, {"id": "2005.14361", "submitter": "Pablo Olivares", "authors": "Konrad Gajewski and Sebastian Ferrando and Pablo Olivares", "title": "Pricing Energy Contracts under Regime Switching Time-Changed models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The shortcomings of the popular Black-Scholes-Merton (BSM) model have led to\nmodels which could more accurately model the behavior of the underlying assets\nin energy markets, particularly in electricity and future oil prices. In this\npaper we consider a class of regime switching time-changed Levy processes,\nwhich builds upon the BSM model by incorporating jumps through a random clock,\nas well as randomly varying parameters according to a two-state continuous-time\nMarkov chain. We implement pricing methods based on expansions of the\ncharacteristic function as in \\cite{Fourier}. Finally, we estimate the\nparameters of the model by incorporating historic energy data and option quotes\nusing a variety of methods.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 01:18:26 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Gajewski", "Konrad", ""], ["Ferrando", "Sebastian", ""], ["Olivares", "Pablo", ""]]}, {"id": "2005.14409", "submitter": "Ben Marafino", "authors": "Ben J. Marafino, Alejandro Schuler, Vincent X. Liu, Gabriel J.\n  Escobar, Mike Baiocchi", "title": "A Causal Machine Learning Framework for Predicting Preventable Hospital\n  Readmissions", "comments": "51 pages; 5 figures and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical predictive algorithms are increasingly being used to form the basis\nfor optimal treatment policies--that is, to enable interventions to be targeted\nto the patients who will presumably benefit most. Despite taking advantage of\nrecent advances in supervised machine learning, these algorithms remain, in a\nsense, blunt instruments--often being developed and deployed without a full\naccounting of the causal aspects of the prediction problems they are intended\nto solve. Indeed, in many settings, including among patients at risk of\nreadmission, the riskiest patients may derive less benefit from a preventative\nintervention compared to those at lower risk. Moreover, targeting an\nintervention to a population, rather than limiting it to a small group of\nhigh-risk patients, may lead to far greater overall utility if the patients\nwith the most modifiable (or preventable) outcomes across the population could\nbe identified. Based on these insights, we introduce a causal machine learning\nframework that decouples this prediction problem into causal and predictive\nparts, which clearly delineates the complementary roles of causal inference and\nprediction in this problem. We estimate treatment effects using causal forests,\nand characterize treatment effect heterogeneity across levels of predicted risk\nusing these estimates. Furthermore, we show how these effect estimates could be\nused in concert with the modeled \"payoffs\" associated with successful\nprevention of individual readmissions to maximize overall utility. Based on\ndata taken from before and after the implementation of a readmissions\nprevention intervention at Kaiser Permanente Northern California, our results\nsuggest that nearly four times as many readmissions could be prevented annually\nwith this approach compared to targeting this intervention using predicted\nrisk.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 06:16:23 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2020 21:12:26 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Marafino", "Ben J.", ""], ["Schuler", "Alejandro", ""], ["Liu", "Vincent X.", ""], ["Escobar", "Gabriel J.", ""], ["Baiocchi", "Mike", ""]]}, {"id": "2005.14511", "submitter": "Navid Alemi Koohbanani", "authors": "Navid Alemi Koohbanani, Mostafa Jahanifar, Neda Zamani Tajadin, and\n  Nasir Rajpoot", "title": "NuClick: A Deep Learning Framework for Interactive Segmentation of\n  Microscopy Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object segmentation is an important step in the workflow of computational\npathology. Deep learning based models generally require large amount of labeled\ndata for precise and reliable prediction. However, collecting labeled data is\nexpensive because it often requires expert knowledge, particularly in medical\nimaging domain where labels are the result of a time-consuming analysis made by\none or more human experts. As nuclei, cells and glands are fundamental objects\nfor downstream analysis in computational pathology/cytology, in this paper we\npropose a simple CNN-based approach to speed up collecting annotations for\nthese objects which requires minimum interaction from the annotator. We show\nthat for nuclei and cells in histology and cytology images, one click inside\neach object is enough for NuClick to yield a precise annotation. For\nmulticellular structures such as glands, we propose a novel approach to provide\nthe NuClick with a squiggle as a guiding signal, enabling it to segment the\nglandular boundaries. These supervisory signals are fed to the network as\nauxiliary inputs along with RGB channels. With detailed experiments, we show\nthat NuClick is adaptable to the object scale, robust against variations in the\nuser input, adaptable to new domains, and delivers reliable annotations. An\ninstance segmentation model trained on masks generated by NuClick achieved the\nfirst rank in LYON19 challenge. As exemplar outputs of our framework, we are\nreleasing two datasets: 1) a dataset of lymphocyte annotations within IHC\nimages, and 2) a dataset of segmented WBCs in blood smear images.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 11:51:27 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 15:27:41 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Koohbanani", "Navid Alemi", ""], ["Jahanifar", "Mostafa", ""], ["Tajadin", "Neda Zamani", ""], ["Rajpoot", "Nasir", ""]]}, {"id": "2005.14621", "submitter": "Ibrahim Alabdulmohsin", "authors": "Ibrahim Alabdulmohsin", "title": "Fair Classification via Unconstrained Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Achieving the Bayes optimal binary classification rule subject to group\nfairness constraints is known to be reducible, in some cases, to learning a\ngroup-wise thresholding rule over the Bayes regressor. In this paper, we extend\nthis result by proving that, in a broader setting, the Bayes optimal fair\nlearning rule remains a group-wise thresholding rule over the Bayes regressor\nbut with a (possible) randomization at the thresholds. This provides a stronger\njustification to the post-processing approach in fair classification, in which\n(1) a predictor is learned first, after which (2) its output is adjusted to\nremove bias. We show how the post-processing rule in this two-stage approach\ncan be learned quite efficiently by solving an unconstrained optimization\nproblem. The proposed algorithm can be applied to any black-box machine\nlearning model, such as deep neural networks, random forests and support vector\nmachines. In addition, it can accommodate many fairness criteria that have been\npreviously proposed in the literature, such as equalized odds and statistical\nparity. We prove that the algorithm is Bayes consistent and motivate it,\nfurthermore, via an impossibility result that quantifies the tradeoff between\naccuracy and fairness across multiple demographic groups. Finally, we conclude\nby validating the algorithm on the Adult benchmark dataset.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 11:29:05 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Alabdulmohsin", "Ibrahim", ""]]}, {"id": "2005.14624", "submitter": "Yongming Qu", "authors": "Yongming Qu, Junxiang Luo, Stephen J. Ruberg", "title": "Implementation of Tripartite Estimands Using Adherence Causal Estimators\n  Under the Causal Inference Framework", "comments": "25 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intercurrent events (ICEs) and missing values are inevitable in clinical\ntrials of any size and duration, making it difficult to assess the treatment\neffect for all patients in randomized clinical trials. Defining the appropriate\nestimand that is relevant to the clinical research question is the first step\nin analyzing data. The tripartite estimands, which evaluate the treatment\ndifferences in the proportion of patients with ICEs due to adverse events, the\nproportion of patients with ICEs due to lack of efficacy, and the primary\nefficacy outcome for those who can adhere to study treatment under the causal\ninference framework, are of interest to many stakeholders in understanding the\ntotality of treatment effects. In this manuscript, we discuss the details of\nhow to estimate tripartite estimands based on a causal inference framework and\nhow to interpret tripartite estimates through a phase 3 clinical study\nevaluating a basal insulin treatment for patients with type 1 diabetes.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 15:30:53 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Qu", "Yongming", ""], ["Luo", "Junxiang", ""], ["Ruberg", "Stephen J.", ""]]}, {"id": "2005.14670", "submitter": "Florian Ziel", "authors": "Florian Ziel", "title": "The energy distance for ensemble and scenario reduction", "comments": "Accepted for publication in Philosophical Transactions A", "journal-ref": null, "doi": "10.1098/rsta.2019.0431", "report-no": null, "categories": "stat.ML cs.LG math.OC q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scenario reduction techniques are widely applied for solving sophisticated\ndynamic and stochastic programs, especially in energy and power systems, but\nalso used in probabilistic forecasting, clustering and estimating generative\nadversarial networks (GANs). We propose a new method for ensemble and scenario\nreduction based on the energy distance which is a special case of the maximum\nmean discrepancy (MMD). We discuss the choice of energy distance in detail,\nespecially in comparison to the popular Wasserstein distance which is\ndominating the scenario reduction literature. The energy distance is a metric\nbetween probability measures that allows for powerful tests for equality of\narbitrary multivariate distributions or independence. Thanks to the latter, it\nis a suitable candidate for ensemble and scenario reduction problems. The\ntheoretical properties and considered examples indicate clearly that the\nreduced scenario sets tend to exhibit better statistical properties for the\nenergy distance than a corresponding reduction with respect to the Wasserstein\ndistance. We show applications to a Bernoulli random walk and two real data\nbased examples for electricity demand profiles and day-ahead electricity\nprices.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 16:52:23 GMT"}, {"version": "v2", "created": "Sat, 3 Oct 2020 18:46:10 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Ziel", "Florian", ""]]}, {"id": "2005.14700", "submitter": "Omar El Housni", "authors": "Omar El Housni, Mika Sumida, Paat Rusmevichientong, Huseyin Topaloglu,\n  Serhan Ziya", "title": "Can Testing Ease Social Distancing Measures? Future Evolution of\n  COVID-19 in NYC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \"New York State on Pause\" executive order came into effect on March 22\nwith the goal of ensuring adequate social distancing to alleviate the spread of\nCOVID-19. Pause will remain effective in New York City in some form until early\nJune. We use a compartmentalized model to study the effects of testing capacity\nand social distancing measures on the evolution of the pandemic in the\n\"post-Pause\" period in the City. We find that testing capacity must increase\ndramatically if it is to counterbalance even relatively small relaxations in\nsocial distancing measures in the immediate post-Pause period. In particular,\nif the City performs 20,000 tests per day and relaxes the social distancing\nmeasures to the pre-Pause norms, then the total number of deaths by the end of\nSeptember can reach 250,000. By keeping the social distancing measures to\nsomewhere halfway between the pre- and in-Pause norms and performing 100,000\ntests per day, the total number of deaths by the end of September can be kept\nat around 27,000. Going back to the pre-Pause social distancing norms quickly\nmust be accompanied by an exorbitant testing capacity, if one is to suppress\nexcessive deaths. If the City is to go back to the \"pre-Pause\" social\ndistancing norms in the immediate post-Pause period and keep the total number\nof deaths by the end of September at around 35,000, then it should be\nperforming 500,000 tests per day. Our findings have important implications on\nthe magnitude of the testing capacity the City needs as it relaxes the social\ndistancing measures to reopen its economy.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 22:08:34 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Housni", "Omar El", ""], ["Sumida", "Mika", ""], ["Rusmevichientong", "Paat", ""], ["Topaloglu", "Huseyin", ""], ["Ziya", "Serhan", ""]]}]