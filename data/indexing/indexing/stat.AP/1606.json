[{"id": "1606.00252", "submitter": "Lingxue Zhu", "authors": "Lingxue Zhu, Jing Lei, Bernie Devlin, Kathryn Roeder", "title": "Testing High Dimensional Covariance Matrices, with Application to\n  Detecting Schizophrenia Risk Genes", "comments": "25 pages, 5 figures, 3 tables", "journal-ref": "Ann. Appl. Stat. 11 (2017), no. 3, 1810--1831", "doi": "10.1214/17-AOAS1062", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientists routinely compare gene expression levels in cases versus controls\nin part to determine genes associated with a disease. Similarly, detecting\ncase-control differences in co-expression among genes can be critical to\nunderstanding complex human diseases; however statistical methods have been\nlimited by the high dimensional nature of this problem. In this paper, we\nconstruct a sparse-Leading-Eigenvalue-Driven (sLED) test for comparing two\nhigh-dimensional covariance matrices. By focusing on the spectrum of the\ndifferential matrix, sLED provides a novel perspective that accommodates what\nwe assume to be common, namely sparse and weak signals in gene expression data,\nand it is closely related with Sparse Principal Component Analysis. We prove\nthat sLED achieves full power asymptotically under mild assumptions, and\nsimulation studies verify that it outperforms other existing procedures under\nmany biologically plausible scenarios. Applying sLED to the largest\ngene-expression dataset obtained from post-mortem brain tissue from\nSchizophrenia patients and controls, we provide a novel list of genes\nimplicated in Schizophrenia and reveal intriguing patterns in gene\nco-expression change for Schizophrenia subjects. We also illustrate that sLED\ncan be generalized to compare other gene-gene \"relationship\" matrices that are\nof practical interest, such as the weighted adjacency matrices.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 12:30:19 GMT"}, {"version": "v2", "created": "Mon, 26 Sep 2016 06:11:53 GMT"}, {"version": "v3", "created": "Tue, 22 Nov 2016 04:17:42 GMT"}, {"version": "v4", "created": "Wed, 7 Dec 2016 19:17:08 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Zhu", "Lingxue", ""], ["Lei", "Jing", ""], ["Devlin", "Bernie", ""], ["Roeder", "Kathryn", ""]]}, {"id": "1606.00361", "submitter": "Jos\\'e M. P\\'erez-S\\'anchez", "authors": "J.M. P\\'erez-S\\'anchez and E. G\\'omez-D\\'eniz", "title": "Simulating Posterior Distributions for Zero-Inflated Automobile\n  Insurance Data", "comments": "17 pages, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized linear models (GLMs) using a regression procedure to fit\nrelationships between predictor and target variables are widely used in\nautomobile insurance data. Here, in the process of ratemaking and in order to\ncompute the premiums to be charged to the policy--holders it is crucial to\ndetect the relevant variables which affect to the value of the premium since in\nthis case the insurer could eventually fix more precisely the premiums. We\npropose here a methodology with a different perspective. Instead of the\nexponential family we pay attention to the Power Series Distributions and\ndevelop a Bayesian methodology using sampling--based methods in order to detect\nrelevant variables in automobile insurance data set. This model, as the GLMs,\nallows to incorporate the presence of an excessive number of zero counts and\noverdispersion phenomena (variance larger than the mean). Following this\nspirit, in this paper we present a novel and flexible zero--inflated Bayesian\nregression model. This model includes other familiar models such as the\nzero--inflated Poisson and zero--inflated geometric models, as special cases. A\nBayesian estimation method is developed as an alternative to traditionally used\nmaximum likelihood based methods to analyze such data. For a real data\ncollected from 2004 to 2005 in an Australian insurance company an example is\nprovided by using Markov Chain Monte Carlo method which is developed in WinBUGS\npackage. The results show that the new Bayesian method performs the previous\nmodels.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 10:50:40 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["P\u00e9rez-S\u00e1nchez", "J. M.", ""], ["G\u00f3mez-D\u00e9niz", "E.", ""]]}, {"id": "1606.00475", "submitter": "Daniel Mirman", "authors": "Daniel Mirman, Jon-Frederick Landrigan, Spiro Kokolis, Sean Verillo,\n  and Casey Ferrara", "title": "Permutation-based cluster size correction for voxel-based lesion-symptom\n  mapping", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Voxel-based lesion-symptom mapping (VLSM) is a major method for studying\nbrain-behavior relationships that leverages modern neuroimaging analysis\ntechniques to build on the classic approach of examining the relationship\nbetween location of brain damage and cognitive deficits. Testing an association\nbetween deficit severity and lesion status in each voxel involves very many\nindividual tests and requires statistical correction for multiple comparisons.\nSeveral strategies have been adapted from analysis of functional neuroimaging\ndata, though VLSM faces a more difficult trade-off between avoiding false\npositives and statistical power (missing true effects). One such strategy is\nusing permutation to non-parametrically determine a null distribution of\ncluster sizes, which is then used to establish a minimum cluster size\nthreshold. This strategy is intuitively appealing because it respects the\nnecessary spatial contiguity of stroke lesions and connects with the typical\ncluster-based interpretation of VLSM results. We evaluated this strategy for\ndetecting true lesion-symptom relations using simulated deficit scores based on\npercent damage to defined brain regions (BA 45 and BA 39) in a sample of 124\nindividuals with left hemisphere stroke. Even under the most conservative\nsettings tested here, the region identified by VLSM with cluster size\ncorrection systematically extended well beyond the true region. As a result,\nthis strategy appears to be effective for ruling out situations with no true\nlesion-symptom relations, but the spatial contiguity of stroke lesions may\ncause identified lesion-symptom relations to extend beyond their true regions.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 21:28:53 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Mirman", "Daniel", ""], ["Landrigan", "Jon-Frederick", ""], ["Kokolis", "Spiro", ""], ["Verillo", "Sean", ""], ["Ferrara", "Casey", ""]]}, {"id": "1606.00546", "submitter": "Florian Ziel", "authors": "Florian Ziel, Carsten Croonenbroeck, Daniel Ambach", "title": "Forecasting wind power - Modeling periodic and non-linear effects under\n  conditional heteroscedasticity", "comments": null, "journal-ref": "Applied Energy, 177 (2016) 285-297", "doi": "10.1016/j.apenergy.2016.05.111", "report-no": null, "categories": "stat.AP stat.CO stat.ML stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we present an approach that enables joint wind speed and wind\npower forecasts for a wind park. We combine a multivariate seasonal time\nvarying threshold autoregressive moving average (TVARMA) model with a power\nthreshold generalized autoregressive conditional heteroscedastic (power-TGARCH)\nmodel. The modeling framework incorporates diurnal and annual periodicity\nmodeling by periodic B-splines, conditional heteroscedasticity and a complex\nautoregressive structure with non-linear impacts. In contrast to usually\ntime-consuming estimation approaches as likelihood estimation, we apply a\nhigh-dimensional shrinkage technique. We utilize an iteratively re-weighted\nleast absolute shrinkage and selection operator (lasso) technique. It allows\nfor conditional heteroscedasticity, provides fast computing times and\nguarantees a parsimonious and regularized specification, even though the\nparameter space may be vast. We are able to show that our approach provides\naccurate forecasts of wind power at a turbine-specific level for forecasting\nhorizons of up to 48 h (short- to medium-term forecasts).\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 06:01:14 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Ziel", "Florian", ""], ["Croonenbroeck", "Carsten", ""], ["Ambach", "Daniel", ""]]}, {"id": "1606.00656", "submitter": "Gergo Barta", "authors": "Gergo Barta, Gabor Nagy, Gabor Simon, Gyozo Papp", "title": "Forecasting Framework for Open Access Time Series in Energy", "comments": "6 pages, 6 figures, IEEE Energycon 2016, Leuven, Belgium", "journal-ref": null, "doi": "10.1109/ENERGYCON.2016.7514015", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a framework for automated forecasting of\nenergy-related time series using open access data from European Network of\nTransmission System Operators for Electricity (ENTSO-E). The framework provides\nforecasts for various European countries using publicly available historical\ndata only. Our solution was benchmarked using the actual load data and the\ncountry provided estimates (where available). We conclude that the proposed\nsystem can produce timely forecasts with comparable prediction accuracy in a\nnumber of cases. We also investigate the probabilistic case of forecasting -\nthat is, providing a probability distribution rather than a simple point\nforecast - and incorporate it into a web based API that provides quick and easy\naccess to reliable forecasts.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 12:59:07 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Barta", "Gergo", ""], ["Nagy", "Gabor", ""], ["Simon", "Gabor", ""], ["Papp", "Gyozo", ""]]}, {"id": "1606.00753", "submitter": "Daniel Barkoczi", "authors": "Daniel Barkoczi and Mirta Galesic", "title": "Social learning strategies modify the effect of network structure on\n  group performance", "comments": null, "journal-ref": "Nat. Commun. 7, 13109 (2016)", "doi": "10.1038/ncomms13109", "report-no": null, "categories": "cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The structure of communication networks is an important determinant of the\ncapacity of teams, organizations and societies to solve policy, business and\nscience problems. Yet, previous studies reached contradictory results about the\nrelationship between network structure and performance, finding support for the\nsuperiority of both well-connected efficient and poorly connected inefficient\nnetwork structures. Here we argue that understanding how communication networks\naffect group performance requires taking into consideration the social learning\nstrategies of individual team members. We show that efficient networks\noutperform inefficient networks when individuals rely on conformity by copying\nthe most frequent solution among their contacts. However, inefficient networks\nare superior when individuals follow the best member by copying the group\nmember with the highest payoff. In addition, groups relying on conformity based\non a small sample of others excel at complex tasks, while groups following the\nbest member achieve greatest performance for simple tasks. Our findings\nreconcile contradictory results in the literature and have broad implications\nfor the study of social learning across disciplines.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 16:30:52 GMT"}, {"version": "v2", "created": "Fri, 21 Oct 2016 15:28:31 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Barkoczi", "Daniel", ""], ["Galesic", "Mirta", ""]]}, {"id": "1606.00919", "submitter": "Jack Raymond", "authors": "Jack Raymond, Sheir Yarkoni, Evgeny Andriyash", "title": "Global warming: Temperature estimation in annealers", "comments": "28 pages, 14 figures ; supplementary materials 18 pages, 5 figures\n  [v4: updated supplementary material file correcting broken references]", "journal-ref": "Frontiers in ICT 3 , 23 (2016)", "doi": "10.3389/fict.2016.00023", "report-no": null, "categories": "quant-ph cond-mat.stat-mech stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sampling from a Boltzmann distribution is NP-hard and so requires heuristic\napproaches. Quantum annealing is one promising candidate. The failure of\nannealing dynamics to equilibrate on practical time scales is a well understood\nlimitation, but does not always prevent a heuristically useful distribution\nfrom being generated. In this paper we evaluate several methods for determining\na useful operational temperature range for annealers. We show that, even where\ndistributions deviate from the Boltzmann distribution due to ergodicity\nbreaking, these estimates can be useful. We introduce the concepts of local and\nglobal temperatures that are captured by different estimation methods. We argue\nthat for practical application it often makes sense to analyze annealers that\nare subject to post-processing in order to isolate the macroscopic distribution\ndeviations that are a practical barrier to their application.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 22:05:29 GMT"}, {"version": "v2", "created": "Sat, 13 Aug 2016 17:57:32 GMT"}, {"version": "v3", "created": "Tue, 16 Aug 2016 15:19:36 GMT"}, {"version": "v4", "created": "Fri, 25 Aug 2017 00:39:07 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Raymond", "Jack", ""], ["Yarkoni", "Sheir", ""], ["Andriyash", "Evgeny", ""]]}, {"id": "1606.00980", "submitter": "Per Sid\\'en", "authors": "Per Sid\\'en, Anders Eklund, David Bolin and Mattias Villani", "title": "Fast Bayesian whole-brain fMRI analysis with spatial 3D priors", "comments": null, "journal-ref": "NeuroImage (2017), vol. 146, 211-225", "doi": "10.1016/j.neuroimage.2016.11.040", "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial whole-brain Bayesian modeling of task-related functional magnetic\nresonance imaging (fMRI) is a great computational challenge. Most of the\ncurrently proposed methods therefore do inference in subregions of the brain\nseparately or do approximate inference without comparison to the true posterior\ndistribution. A popular such method, which is now the standard method for\nBayesian single subject analysis in the SPM software, is introduced in Penny et\nal. (2005b). The method processes the data slice-by-slice and uses an\napproximate variational Bayes (VB) estimation algorithm that enforces posterior\nindependence between activity coefficients in different voxels. We introduce a\nfast and practical Markov chain Monte Carlo (MCMC) scheme for exact inference\nin the same model, both slice-wise and for the whole brain using a 3D prior on\nactivity coefficients. The algorithm exploits sparsity and uses modern\ntechniques for efficient sampling from high-dimensional Gaussian distributions,\nleading to speed-ups without which MCMC would not be a practical option. Using\nMCMC, we are for the first time able to evaluate the approximate VB posterior\nagainst the exact MCMC posterior, and show that VB can lead to spurious\nactivation. In addition, we develop an improved VB method that drops the\nassumption of independent voxels a posteriori. This algorithm is shown to be\nmuch faster than both MCMC and the original VB for large datasets, with\nnegligible error compared to the MCMC posterior.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 06:40:19 GMT"}, {"version": "v2", "created": "Thu, 29 Sep 2016 08:40:58 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Sid\u00e9n", "Per", ""], ["Eklund", "Anders", ""], ["Bolin", "David", ""], ["Villani", "Mattias", ""]]}, {"id": "1606.01200", "submitter": "Michal Koles\\'ar", "authors": "Timothy B. Armstrong and Michal Koles\\'ar", "title": "Simple and Honest Confidence Intervals in Nonparametric Regression", "comments": "46 pages, plus a 54-page supplemental appendix", "journal-ref": "Quantitative Economics, Volume 11, Issue 1, January 2020, pages\n  1-39", "doi": "10.3982/QE1199", "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of constructing honest confidence intervals (CIs) for\na scalar parameter of interest, such as the regression discontinuity parameter,\nin nonparametric regression based on kernel or local polynomial estimators. To\nensure that our CIs are honest, we use critical values that take into account\nthe possible bias of the estimator upon which the CIs are based. We show that\nthis approach leads to CIs that are more efficient than conventional CIs that\nachieve coverage by undersmoothing or subtracting an estimate of the bias. We\ngive sharp efficiency bounds of using different kernels, and derive the optimal\nbandwidth for constructing honest CIs. We show that using the bandwidth that\nminimizes the maximum mean-squared error results in CIs that are nearly\nefficient and that in this case, the critical value depends only on the rate of\nconvergence. For the common case in which the rate of convergence is\n$n^{-2/5}$, the appropriate critical value for 95% CIs is 2.18, rather than the\nusual 1.96 critical value. We illustrate our results in a Monte Carlo analysis\nand an empirical application.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 17:47:30 GMT"}, {"version": "v2", "created": "Wed, 5 Oct 2016 20:00:05 GMT"}, {"version": "v3", "created": "Mon, 19 Mar 2018 00:51:02 GMT"}, {"version": "v4", "created": "Wed, 29 Aug 2018 22:05:31 GMT"}, {"version": "v5", "created": "Thu, 6 Jun 2019 16:27:20 GMT"}, {"version": "v6", "created": "Wed, 28 Aug 2019 21:15:21 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Armstrong", "Timothy B.", ""], ["Koles\u00e1r", "Michal", ""]]}, {"id": "1606.01473", "submitter": "Katelyn Gao", "authors": "Katelyn Gao", "title": "Confidence Intervals for Algorithmic Leveraging in Linear Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The age of big data has produced data sets that are computationally expensive\nto analyze and store. Algorithmic leveraging proposes that we sample\nobservations from the original data set to generate a representative data set\nand then perform analysis on the representative data set. In this paper, we\npresent efficient algorithms for constructing finite sample confidence\nintervals for each algorithmic leveraging estimated regression coefficient,\nwith asymptotic coverage guarantees. In simulations, we confirm empirically\nthat the confidence intervals have the desired coverage probabilities, while\nbootstrap confidence intervals may not.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jun 2016 07:40:08 GMT"}, {"version": "v2", "created": "Wed, 13 Jul 2016 07:04:33 GMT"}, {"version": "v3", "created": "Sun, 10 Sep 2017 06:25:37 GMT"}, {"version": "v4", "created": "Sat, 10 Mar 2018 23:21:32 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Gao", "Katelyn", ""]]}, {"id": "1606.01634", "submitter": "Lev B Klebanov", "authors": "Lev B. Klebanov, Zeev Volkovich", "title": "Number of clusters, deconvolution and classical problem of moments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the paper there is given a connection between one special case of cluster\nanalysis, deconvolution problem, and classical moment problem. Namely, the\nmethods used there are applied to solve deconvolution problem for the case of\none known distribution and another one concentrated in unknown finite number of\npoints. These results can be applied to estimate a number of clusters for the\ncase of scale or location mixture of identical distributions.\n  keywords: number of clusters, deconvolution problem, classical moment\nproblem, scale and location mixtures.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 06:58:57 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Klebanov", "Lev B.", ""], ["Volkovich", "Zeev", ""]]}, {"id": "1606.01666", "submitter": "Claudia K\\\"ollmann", "authors": "Claudia K\\\"ollmann and Katja Ickstadt and Roland Fried", "title": "Beyond unimodal regression: modelling multimodality with piecewise\n  unimodal regression or deconvolution models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shape constraints enable us to reflect prior knowledge in regression\nsettings. A unimodality constraint, for example, can describe the frequent case\nof a first increasing and then decreasing intensity. Yet, data shapes often\nexhibit multiple modes. Therefore, we go beyond unimodal regression and propose\nmodelling multimodality with piecewise unimodal regression or with\ndeconvolution models based on unimodal peak shapes. Usefulness of unimodal\nregression and its multimodal extensions is demonstrated within three\napplications areas: marine biology, astroparticle physics and breath gas\nanalysis. Despite this diversity, valuable results are obtained in each\napplication. This encourages the use of these methods in other areas as well.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 09:18:05 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["K\u00f6llmann", "Claudia", ""], ["Ickstadt", "Katja", ""], ["Fried", "Roland", ""]]}, {"id": "1606.01746", "submitter": "Amelia Sim\\'o", "authors": "Sonia Barahona, Ximo Gual-Arnau, Maria Victoria Ib\\'a\\~nez and Amelia\n  Sim\\'o", "title": "Unsupervised classification of children's bodies using currents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object classification according to their shape and size is of key importance\nin many scientific fields. This work focuses on the case where the size and\nshape of an object is characterized by a current}. A current is a mathematical\nobject which has been proved relevant to the modeling of geometrical data, like\nsubmanifolds, through integration of vector fields along them. As a consequence\nof the choice of a vector-valued Reproducing Kernel Hilbert Space (RKHS) as a\ntest space for integrating manifolds, it is possible to consider that shapes\nare embedded in this Hilbert Space. A vector-valued RKHS is a Hilbert space of\nvector fields; therefore, it is possible to compute a mean of shapes, or to\ncalculate a distance between two manifolds. This embedding enables us to\nconsider size-and-shape classification algorithms.\n  These algorithms are applied to a 3D database obtained from an anthropometric\nsurvey of the Spanish child population with a potential application to online\nsales of children's wear.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 13:52:24 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Barahona", "Sonia", ""], ["Gual-Arnau", "Ximo", ""], ["Ib\u00e1\u00f1ez", "Maria Victoria", ""], ["Sim\u00f3", "Amelia", ""]]}, {"id": "1606.01797", "submitter": "Ra\\'ul Torres D\\'iaz", "authors": "Ra\\'ul Torres, Carlo De Michele, Henry Laniado and Rosa E. Lillo", "title": "Directional Multivariate Extremes in Environmental Phenomena", "comments": "Article with supplementary material in the appendix", "journal-ref": "Environmetrics, Volume 28, Issue 2 March 2017 e2428", "doi": "10.1002/env.2428", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several environmental phenomena can be described by different correlated\nvariables that must be considered jointly in order to be more representative of\nthe nature of these phenomena. For such events, identification of extremes is\ninappropriate if it is based on marginal analysis. Extremes have usually been\nlinked to the notion of quantile, which is an important tool to analyze risk in\nthe univariate setting. We propose to identify multivariate extremes and\nanalyze environmental phenomena in terms of the directional multivariate\nquantile, which allows us to analyze the data considering all the variables\nimplied in the phenomena, as well as look at the data in interesting directions\nthat can better describe an environmental catastrophe. Since there are many\nreferences in the literature that propose extremes detection based on copula\nmodels, we also generalize the copula method by introducing the directional\napproach. Advantages and disadvantages of the non-parametric proposal that we\nintroduce and the copula methods are provided in the paper. We show with\nsimulated and real data sets how by considering the first principal component\ndirection we can improve the visualization of extremes. Finally, two cases of\nstudy are analyzed: a synthetic case of flood risk at a dam (a 3-variable\ncase), and a real case study of sea storms (a 5-variable case).\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 15:51:25 GMT"}, {"version": "v2", "created": "Fri, 10 Jun 2016 15:10:06 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Torres", "Ra\u00fal", ""], ["De Michele", "Carlo", ""], ["Laniado", "Henry", ""], ["Lillo", "Rosa E.", ""]]}, {"id": "1606.01836", "submitter": "Tauhid Zaman", "authors": "Carter Mundell, Juan Pablo Vielma, and Tauhid Zaman", "title": "Predicting Performance Under Stressful Conditions Using Galvanic Skin\n  Response", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid growth of the availability of wearable biosensors has created the\nopportunity for using biological signals to measure worker performance. An\nimportant question is how to use such signals to not just measure, but actually\npredict worker performance on a task under stressful and potentially high risk\nconditions. Here we show that the biological signal known as galvanic skin\nresponse (GSR) allows such a prediction. We conduct an experiment where\nsubjects answer arithmetic questions under low and high stress conditions while\nhaving their GSR monitored using a wearable biosensor. Using only the GSR\nmeasured under low stress conditions, we are able to predict which subjects\nwill perform well under high stress conditions, achieving an area under the\ncurve (AUC) of 0.76. If we try to make similar predictions without using any\nbiometric signals, the AUC barely exceeds 0.50. Our result suggests that\nperformance in high stress conditions can be predicted using signals obtained\nfrom wearable biosensors in low stress conditions.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 17:22:16 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Mundell", "Carter", ""], ["Vielma", "Juan Pablo", ""], ["Zaman", "Tauhid", ""]]}, {"id": "1606.01855", "submitter": "Aaron Schein", "authors": "Aaron Schein, Mingyuan Zhou, David M. Blei, Hanna Wallach", "title": "Bayesian Poisson Tucker Decomposition for Learning the Structure of\n  International Relations", "comments": "To appear in Proceedings of the 33rd International Conference on\n  Machine Learning (ICML 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Bayesian Poisson Tucker decomposition (BPTD) for modeling\ncountry--country interaction event data. These data consist of interaction\nevents of the form \"country $i$ took action $a$ toward country $j$ at time\n$t$.\" BPTD discovers overlapping country--community memberships, including the\nnumber of latent communities. In addition, it discovers directed\ncommunity--community interaction networks that are specific to \"topics\" of\naction types and temporal \"regimes.\" We show that BPTD yields an efficient MCMC\ninference algorithm and achieves better predictive performance than related\nmodels. We also demonstrate that it discovers interpretable latent structure\nthat agrees with our knowledge of international relations.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 18:34:56 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Schein", "Aaron", ""], ["Zhou", "Mingyuan", ""], ["Blei", "David M.", ""], ["Wallach", "Hanna", ""]]}, {"id": "1606.02074", "submitter": "Andrey Kormilitzin", "authors": "A. B. Kormilitzin, K. E. A. Saunders, P. J. Harrison, J. R. Geddes, T.\n  J. Lyons", "title": "Application of the Signature Method to Pattern Recognition in the CEQUEL\n  Clinical Trial", "comments": "16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classification procedure of streaming data usually requires various ad\nhoc methods or particular heuristic models. We explore a novel non-parametric\nand systematic approach to analysis of heterogeneous sequential data. We\ndemonstrate an application of this method to classification of the delays in\nresponding to the prompts, from subjects with bipolar disorder collected during\na clinical trial, using both synthetic and real examples. We show how this\nmethod can provide a natural and systematic way to extract characteristic\nfeatures from sequential data.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 09:36:29 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Kormilitzin", "A. B.", ""], ["Saunders", "K. E. A.", ""], ["Harrison", "P. J.", ""], ["Geddes", "J. R.", ""], ["Lyons", "T. J.", ""]]}, {"id": "1606.02101", "submitter": "Keiichi Fukaya", "authors": "Keiichi Fukaya, J. Andrew Royle, Takehiro Okuda, Masahiro Nakaoka,\n  Takashi Noda", "title": "A multistate dynamic site occupancy model for spatially aggregated\n  sessile communities", "comments": "14 pages, 3 figures", "journal-ref": "Methods in Ecology and Evolution 8 (2017) 757-767", "doi": "10.1111/2041-210X.12690", "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov community models have been applied to sessile organisms because such\nmodels facilitate estimation of transition probabilities by tracking species\noccupancy at many fixed observation points over multiple periods of time.\nEstimation of transition probabilities of sessile communities seems easy in\nprinciple but may still be difficult in practice because resampling error\n(i.e., a failure to resample exactly the same location at fixed points) may\ncause significant estimation bias. Previous studies have developed novel\nanalytical methods to correct for this estimation bias. However, they did not\nconsider the local structure of community composition induced by the aggregated\ndistribution of organisms that is typically observed in sessile assemblages and\nis very likely to affect observations. In this study, we developed a multistate\ndynamic site occupancy model to estimate transition probabilities that accounts\nfor resampling errors associated with local community structure. The model\napplies a nonparametric multivariate kernel smoothing methodology to the latent\noccupancy component to estimate the local state composition near each\nobservation point, which is assumed to determine the probability distribution\nof data conditional on the occurrence of resampling error. By using computer\nsimulations, we confirmed that an observation process that depends on local\ncommunity structure may bias inferences about transition probabilities. By\napplying the proposed model to a real dataset of intertidal sessile\ncommunities, we also showed that estimates of transition probabilities and of\nthe properties of community dynamics may differ considerably when spatial\ndependence is taken into account. Our approach can even accommodate an\nanisotropic spatial correlation of species composition, and may serve as a\nbasis for inferring complex nonlinear ecological dynamics.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 11:36:43 GMT"}, {"version": "v2", "created": "Tue, 11 Oct 2016 11:12:26 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Fukaya", "Keiichi", ""], ["Royle", "J. Andrew", ""], ["Okuda", "Takehiro", ""], ["Nakaoka", "Masahiro", ""], ["Noda", "Takashi", ""]]}, {"id": "1606.02186", "submitter": "Ah Yeon Park", "authors": "Ah Yeon Park, John A. D. Aston and Frederic Ferraty", "title": "Stable and predictive functional domain selection with application to\n  brain images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by increasing trends of relating brain images to a clinical outcome\nof interest, we propose a functional domain selection (FuDoS) method that\neffectively selects subregions of the brain associated with the outcome. View\neach individual's brain as a 3D functional object, the statistical aim is to\ndistinguish the region where a regression coefficient $\\beta(t)=0$ from\n$\\beta(t)\\neq0$, where $t$ denotes spatial location. FuDoS is composed of two\nstages of estimation. We first segment the brain into several small parts based\non the correlation structure. Then, potential subsets are built using the\nobtained segments and their predictive performance are evaluated to select the\nbest subset, augmented by a stability selection criterion. We conduct extensive\nsimulations both for 1D and 3D functional data, and evaluate its effectiveness\nin selecting the true subregion. We also investigate predictive ability of the\nselected stable regions. To find the brain regions related to cognitive\nability, FuDoS is applied to the ADNI's PET data. Due to the induced\nsparseness, the results naturally provide more interpretable information about\nthe relations between the regions and the outcome. Moreover, the selected\nregions from our analysis show high associations with the expected anatomical\nbrain areas known to have memory-related functions.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 15:38:38 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Park", "Ah Yeon", ""], ["Aston", "John A. D.", ""], ["Ferraty", "Frederic", ""]]}, {"id": "1606.02231", "submitter": "Diego Fernandez Slezak", "authors": "Facundo Carrillo and Natalia Mota and Mauro Copelli and Sidarta\n  Ribeiro and Mariano Sigman and Guillermo Cecchi and Diego Fernandez Slezak", "title": "Emotional Intensity analysis in Bipolar subjects", "comments": "Presented at MLINI-2015 workshop, 2015 (arXiv:cs/0101200)", "journal-ref": null, "doi": null, "report-no": "MLINI/2015/19", "categories": "cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The massive availability of digital repositories of human thought opens\nradical novel way of studying the human mind. Natural language processing tools\nand computational models have evolved such that many mental conditions are\npredicted by analysing speech. Transcription of interviews and discourses are\nanalyzed using syntactic, grammatical or sentiment analysis to infer the mental\nstate. Here we set to investigate if classification of Bipolar and control\nsubjects is possible. We develop the Emotion Intensity Index based on the\nDictionary of Affect, and find that subjects categories are distinguishable.\nUsing classical classification techniques we get more than 75\\% of labeling\nperformance. These results sumed to previous studies show that current\nautomated speech analysis is capable of identifying altered mental states\ntowards a quantitative psychiatry.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 17:44:44 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Carrillo", "Facundo", ""], ["Mota", "Natalia", ""], ["Copelli", "Mauro", ""], ["Ribeiro", "Sidarta", ""], ["Sigman", "Mariano", ""], ["Cecchi", "Guillermo", ""], ["Slezak", "Diego Fernandez", ""]]}, {"id": "1606.02381", "submitter": "Tsuyoshi Kunihama", "authors": "Tsuyoshi Kunihama, Carolyn T. Halpern, Amy H. Herring", "title": "Nonparametric Bayes models for mixed-scale longitudinal surveys", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling and computation for multivariate longitudinal surveys have proven\nchallenging, particularly when data are not all continuous and Gaussian but\ncontain discrete measurements. In many social science surveys, study\nparticipants are selected via complex survey designs such as stratified random\nsampling, leading to discrepancies between the sample and population, which are\nfurther compounded by missing data and loss to follow up. Survey weights are\ntypically constructed to address these issues, but it is not clear how to\ninclude them in models. Motivated by data on sexual development, we propose a\nnovel nonparametric approach for mixed-scale longitudinal data in surveys. In\nthe proposed approach, the mixed-scale multivariate response is expressed\nthrough an underlying continuous variable with dynamic latent factors inducing\ntime-varying associations. Bias from the survey design is adjusted for in\nposterior computation relying on a Markov chain Monte Carlo algorithm. The\napproach is assessed in simulation studies, and applied to the National\nLongitudinal Study of Adolescent to Adult Health.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 02:45:13 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Kunihama", "Tsuyoshi", ""], ["Halpern", "Carolyn T.", ""], ["Herring", "Amy H.", ""]]}, {"id": "1606.02536", "submitter": "Keiichi Fukaya", "authors": "Keiichi Fukaya, Ai Kawamori, Yutaka Osada, Masumi Kitazawa, Makio\n  Ishiguro", "title": "The forecasting of menstruation based on a state-space modeling of basal\n  body temperature time series", "comments": "16 pages, 4 figures", "journal-ref": "Statistics in Medicine 36 (2017) 3361-3379", "doi": "10.1002/sim.7345", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Women's basal body temperature (BBT) follows a periodic pattern that is\nassociated with the events in their menstrual cycle. Although daily BBT time\nseries contain potentially useful information for estimating the underlying\nmenstrual phase and for predicting the length of current menstrual cycle, few\nmodels have been constructed for BBT time series. Here, we propose a\nstate-space model that includes menstrual phase as a latent state variable to\nexplain fluctuations in BBT and menstrual cycle length. Conditional\ndistributions for the menstrual phase were obtained by using sequential\nBayesian filtering techniques. A predictive distribution for the upcoming onset\nof menstruation was then derived based on the conditional distributions and the\nmodel, leading to a novel statistical framework that provided a sequentially\nupdated prediction of the day of onset of menstruation. We applied this\nframework to a real dataset comprising women's self-reported BBT and days of\nmenstruation, comparing the prediction accuracy of our proposed method with\nthat of conventional calendar calculation. We found that our proposed method\nprovided a better prediction of the day of onset of menstruation. Potential\nextensions of this framework may provide the basis of modeling and predicting\nother events that are associated with the menstrual cycle.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 12:56:42 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Fukaya", "Keiichi", ""], ["Kawamori", "Ai", ""], ["Osada", "Yutaka", ""], ["Kitazawa", "Masumi", ""], ["Ishiguro", "Makio", ""]]}, {"id": "1606.02690", "submitter": "Sandra E. Safo", "authors": "Sandra E. Safo, Shuzhao Li, Qi Long", "title": "Integrative analysis of transcriptomic and metabolomic data via sparse\n  canonical correlation analysis with incorporation of biological information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrative analyses of different high dimensional data types are becoming\nincreasingly popular. Similarly, incorporating prior functional relationships\namong variables in data analysis has been a topic of increasing interest as it\nhelps elucidate underlying mechanisms among complex diseases. In this paper,\nthe goal is to assess association between transcriptomic and metabolomic data\nfrom a Predictive Health Institute (PHI) study including healthy adults at high\nrisk of developing cardiovascular diseases. To this end, we develop statistical\nmethods for identifying sparse structure in canonical correlation analysis\n(CCA) with incorporation of biological/structural information. Our proposed\nmethods use prior network structural information among genes and among\nmetabolites to guide selection of relevant genes and metabolites in sparse CCA,\nproviding insight on the molecular underpinning of cardiovascular disease. Our\nsimulations demonstrate that the structured sparse CCA methods outperform\nseveral existing sparse CCA methods in selecting relevant genes and metabolites\nwhen structural information is informative and are robust to mis-specified\nstructural information. Our analysis of the PHI study reveals that a number of\ngenes and metabolic pathways including some known to be associated with\ncardiovascular diseases are enriched in the subset of genes and metabolites\nselected by our proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 19:03:39 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Safo", "Sandra E.", ""], ["Li", "Shuzhao", ""], ["Long", "Qi", ""]]}, {"id": "1606.03245", "submitter": "Deniz Yenigun", "authors": "Deniz Yenigun, Gunes Ertan, Michael Siciliano", "title": "Omission and Commission Errors in Network Cognition and Network\n  Estimation using ROC Curve", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cognitive Social Structure (CSS) network studies collect relational data on\nrespondents' direct ties and their perception of ties among all other\nindividuals in the network. When reporting their perception networks,\nrespondents commit two types of errors, namely, omission (false negatives) and\ncommission (false positives) errors. We first assess the relationship between\nthese two error types, and their contributions on the overall respondent\naccuracy. Next we propose a method for estimating networks based on perceptions\nof a random sample of respondents from a bounded social network, which utilizes\nthe Receiving Operator Characteristic (ROC) curve for balancing the tradeoffs\nbetween omission and commission errors. A comparative numerical study shows\nthat the proposed estimation method performs well. This new method can be\neasily integrated to organization studies that use randomized surveys to study\nmultiple organizations. The burgeoning field of multilevel analysis of\ninter-organizational networks can also immensely benefit from this approach.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 09:31:52 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["Yenigun", "Deniz", ""], ["Ertan", "Gunes", ""], ["Siciliano", "Michael", ""]]}, {"id": "1606.03295", "submitter": "Niels Olsen", "authors": "Niels Lundtorp Olsen, Bo Markussen, Lars Lau Rak\\^et", "title": "Simultaneous inference for misaligned multivariate functional data", "comments": "44 pages in total including tables and figures. Additional 9 pages of\n  supplementary material and references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider inference for misaligned multivariate functional data that\nrepresents the same underlying curve, but where the functional samples have\nsystematic differences in shape. In this paper we introduce a new class of\ngenerally applicable models where warping effects are modeled through nonlinear\ntransformation of latent Gaussian variables and systematic shape differences\nare modeled by Gaussian processes. To model cross-covariance between sample\ncoordinates we introduce a class of low-dimensional cross-covariance structures\nsuitable for modeling multivariate functional data. We present a method for\ndoing maximum-likelihood estimation in the models and apply the method to three\ndata sets. The first data set is from a motion tracking system where the\nspatial positions of a large number of body-markers are tracked in\nthree-dimensions over time. The second data set consists of height and weight\nmeasurements for Danish boys. The third data set consists of three-dimensional\nspatial hand paths from a controlled obstacle-avoidance experiment. We use the\ndeveloped method to estimate the cross-covariance structure, and use a\nclassification setup to demonstrate that the method outperforms\nstate-of-the-art methods for handling misaligned curve data.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 12:41:36 GMT"}, {"version": "v2", "created": "Thu, 4 May 2017 12:24:58 GMT"}, {"version": "v3", "created": "Fri, 8 Dec 2017 12:55:55 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Olsen", "Niels Lundtorp", ""], ["Markussen", "Bo", ""], ["Rak\u00eat", "Lars Lau", ""]]}, {"id": "1606.03376", "submitter": "Gabriela Cybis", "authors": "Gabriela Bettella Cybis, Marcio Valk, Silvia Regina Costa Lopes", "title": "Clustering and Classification of Genetic Data Through U-Statistics", "comments": "27 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genetic data are frequently categorical and have complex dependence\nstructures that are not always well understood. For this reason, clustering and\nclassification based on genetic data, while highly relevant, are challenging\nstatistical problems. Here we consider a highly versatile U-statistics based\napproach built on dissimilarities between pairs of data points for\nnonparametric clustering. In this work we propose statistical tests to assess\ngroup homogeneity taking into account the multiple testing issues, and a\nclustering algorithm based on dissimilarities within and between groups that\nhighly speeds up the homogeneity test. We also propose a test to verify\nclassification significance of a sample in one of two groups. A Monte Carlo\nsimulation study is presented to evaluate power of the classification test,\nconsidering different group sizes and degree of separation. Size and power of\nthe homogeneity test are also analyzed through simulations that compare it to\ncompeting methods. Finally, the methodology is applied to three different\ngenetic datasets: global human genetic diversity, breast tumor gene expression\nand Dengue virus serotypes. These applications showcase this statistical\nframework's ability to answer diverse biological questions while adapting to\nthe specificities of the different datatypes.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 15:57:20 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["Cybis", "Gabriela Bettella", ""], ["Valk", "Marcio", ""], ["Lopes", "Silvia Regina Costa", ""]]}, {"id": "1606.04086", "submitter": "Michal Koles\\'ar", "authors": "Michal Koles\\'ar and Christoph Rothe", "title": "Inference in Regression Discontinuity Designs with a Discrete Running\n  Variable", "comments": "47 pages plus supplemental materials", "journal-ref": "American Economic Review, vol. 108, no. 8, August 2018 (pp.\n  2277-2304)", "doi": "10.1257/aer.20160945", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider inference in regression discontinuity designs when the running\nvariable only takes a moderate number of distinct values. In particular, we\nstudy the common practice of using confidence intervals (CIs) based on standard\nerrors that are clustered by the running variable as a means to make inference\nrobust to model misspecification (Lee and Card, 2008). We derive theoretical\nresults and present simulation and empirical evidence showing that these CIs do\nnot guard against model misspecification, and that they have poor coverage\nproperties. We therefore recommend against using these CIs in practice. We\ninstead propose two alternative CIs with guaranteed coverage properties under\neasily interpretable restrictions on the conditional expectation function.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 19:55:31 GMT"}, {"version": "v2", "created": "Wed, 22 Jun 2016 02:48:18 GMT"}, {"version": "v3", "created": "Wed, 31 May 2017 18:10:35 GMT"}, {"version": "v4", "created": "Sat, 18 Nov 2017 23:14:51 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Koles\u00e1r", "Michal", ""], ["Rothe", "Christoph", ""]]}, {"id": "1606.04146", "submitter": "Hyunseung Kang", "authors": "Hyunseung Kang, Laura Peck, and Luke Keele", "title": "Inference for Instrumental Variables: A Randomization Inference Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method of instrumental variables (IV) provides a framework to study\ncausal effects in both randomized experiments with noncompliance and in\nobservational studies where natural circumstances produce as-if random nudges\nto accept treatment. Traditionally, inference for IV relied on asymptotic\napproximations of the distribution of the Wald estimator or two-stage least\nsquares, often with structural modeling assumptions and/or moment conditions.\nIn this paper, we utilize the randomization inference approach to IV inference.\nFirst, we outline the exact method, which uses the randomized assignment of\ntreatment in experiments as a basis for inference, but lacks a closed-form\nsolution and may be computationally infeasible in many applications. We then\nprovide an alternative to the exact method, the almost exact method, which is\ncomputationally feasible but retains the advantages of the exact method. We\nalso review asymptotic methods of inference, including those associated with\ntwo-stage least squares, and analytically compare them to randomization\ninference methods. We also perform additional comparisons using a set of\nsimulations. We conclude with three different applications from the social\nsciences.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 21:27:27 GMT"}, {"version": "v2", "created": "Tue, 6 Feb 2018 18:12:01 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Kang", "Hyunseung", ""], ["Peck", "Laura", ""], ["Keele", "Luke", ""]]}, {"id": "1606.04153", "submitter": "Carlos Pineda", "authors": "Jos\\'e A. Morales, Sergio S\\'anchez, Jorge Flores, Carlos Pineda,\n  Carlos Gershenson, Germinal Cocho, Jer\\'onimo Zizumbo, Gerardo I\\~niguez", "title": "Universal temporal features of rankings in competitive sports and games", "comments": null, "journal-ref": "EPJ Data Science 5:33 (2016)", "doi": "10.1140/epjds/s13688-016-0096-y", "report-no": null, "categories": "physics.soc-ph nlin.AO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many complex phenomena, from the selection of traits in biological systems to\nhierarchy formation in social and economic entities, show signs of competition\nand heterogeneous performance in the temporal evolution of their components,\nwhich may eventually lead to stratified structures such as the wealth\ndistribution worldwide. However, it is still unclear whether the road to\nhierarchical complexity is determined by the particularities of each phenomena,\nor if there are universal mechanisms of stratification common to many systems.\nHuman sports and games, with their (varied but simplified) rules of competition\nand measures of performance, serve as an ideal test bed to look for universal\nfeatures of hierarchy formation. With this goal in mind, we analyse here the\nbehaviour of players and team rankings over time for several sports and games.\nEven though, for a given time, the distribution of performance ranks varies\nacross activities, we find statistical regularities in the dynamics of ranks.\nSpecifically the rank diversity, a measure of the number of elements occupying\na given rank over a length of time, has the same functional form in sports and\ngames as in languages, another system where competition is determined by the\nuse or disuse of grammatical structures. Our results support the notion that\nhierarchical phenomena may be driven by the same underlying mechanisms of rank\nformation, regardless of the nature of their components. Moreover, such\nregularities can in principle be used to predict lifetimes of rank occupancy,\nthus increasing our ability to forecast stratification in the presence of\ncompetition.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 22:02:13 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Morales", "Jos\u00e9 A.", ""], ["S\u00e1nchez", "Sergio", ""], ["Flores", "Jorge", ""], ["Pineda", "Carlos", ""], ["Gershenson", "Carlos", ""], ["Cocho", "Germinal", ""], ["Zizumbo", "Jer\u00f3nimo", ""], ["I\u00f1iguez", "Gerardo", ""]]}, {"id": "1606.04203", "submitter": "Shang Li", "authors": "Shang Li and Xiaodong Wang", "title": "Order-2 Asymptotic Optimality of the Fully Distributed Sequential\n  Hypothesis Test", "comments": "36 pages", "journal-ref": "IEEE Transactions on Information Theory, vol. 64, no. 4, pp.\n  2742-2758, April 2018", "doi": "10.1109/TIT.2018.2806961", "report-no": null, "categories": "stat.AP cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work analyzes the asymptotic performances of fully distributed\nsequential hypothesis testing procedures as the type-I and type-II error rates\napproach zero, in the context of a sensor network without a fusion center. In\nparticular, the sensor network is defined by an undirected graph, where each\nsensor can observe samples over time, access the information from the adjacent\nsensors, and perform the sequential test based on its own decision statistic.\nDifferent from most literature, the sampling process and the information\nexchange process in our framework take place simultaneously (or, at least in\ncomparable time-scales), thus cannot be decoupled from one another. Two\nmessage-passing schemes are considered, based on which the distributed\nsequential probability ratio test (DSPRT) is carried out respectively. The\nfirst scheme features the dissemination of the raw samples. Although the sample\npropagation based DSPRT is shown to yield the asymptotically optimal\nperformance at each sensor, it incurs excessive inter-sensor communication\noverhead due to the exchange of raw samples with index information. The second\nscheme adopts the consensus algorithm, where the local decision statistic is\nexchanged between sensors instead of the raw samples, thus significantly\nlowering the communication requirement compared to the first scheme. In\nparticular, the decision statistic for DSPRT at each sensor is updated by the\nweighted average of the decision statistics in the neighbourhood at every\nmessage-passing step. We show that, under certain regularity conditions, the\nconsensus algorithm based DSPRT also yields the order-2 asymptotically optimal\nperformance at all sensors.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 05:00:33 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Li", "Shang", ""], ["Wang", "Xiaodong", ""]]}, {"id": "1606.04564", "submitter": "Andrew Zammit-Mangion", "authors": "Andrew Zammit-Mangion, Noel Cressie, Anita L. Ganesan", "title": "Non-Gaussian bivariate modelling with application to atmospheric\n  trace-gas inversion", "comments": "45 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Atmospheric trace-gas inversion is the procedure by which the sources and\nsinks of a trace gas are identified from observations of its mole fraction at\nisolated locations in space and time. This is inherently a spatio-temporal\nbivariate inversion problem, since the mole-fraction field evolves in space and\ntime and the flux is also spatio-temporally distributed. Further, the bivariate\nmodel is likely to be non-Gaussian since the flux field is rarely Gaussian.\nHere, we use conditioning to construct a non-Gaussian bivariate model, and we\ndescribe some of its properties through auto- and cross-cumulant functions. A\nbivariate non-Gaussian, specifically trans-Gaussian, model is then achieved\nthrough the use of Box--Cox transformations, and we facilitate Bayesian\ninference by approximating the likelihood in a hierarchical framework.\nTrace-gas inversion, especially at high spatial resolution, is frequently\nhighly sensitive to prior specification. Therefore, unlike conventional\napproaches, we assimilate trace-gas inventory information with the\nobservational data at the parameter layer, thus shifting prior sensitivity from\nthe inventory itself to its spatial characteristics (e.g., its spatial length\nscale). We demonstrate the approach in controlled-experiment studies of methane\ninversion, using fluxes extracted from inventories of the UK and Ireland and of\nNorthern Australia.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 20:52:42 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Zammit-Mangion", "Andrew", ""], ["Cressie", "Noel", ""], ["Ganesan", "Anita L.", ""]]}, {"id": "1606.04636", "submitter": "Agnieszka Werpachowska", "authors": "Agnieszka Werpachowska (Averisera Ltd) and Roman Werpachowski", "title": "Microsimulations of demographic changes in England and Wales under\n  different EU referendum scenarios", "comments": "13 pages, to appear in the International Journal of Microsimulations", "journal-ref": "International Journal of Microsimulation, 10(2), 2017", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We perform stochastic microsimulations of the dynamics of England and Wales\npopulation after the British referendum on EU membership, considering different\npossible outcomes. Employing available survey data, we model the demographics\nof the region over the next generation, as shaped by births, deaths and\ninternational migration. The migration patterns between England and Wales and\nthe remaining EU countries are modified according to the possible scenarios of\ntheir future relations. We find that Brexit will accelerate the overall\npopulation ageing and the deepening imbalance between workers and retirees but\nreduce the population growth and the fraction of women of reproductive age. In\nthe alternative scenarios of remaining in the EU these effects will be\npartially forestalled by the influx of immigrants from current and prospective\nEU countries and their children. In all considered scenarios the native British\npopulation declines. Our study demonstrates that microsimulations can be a\nuseful tool for designing and evaluating the country's policies in the advent\nof fundamental transformations.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 04:08:26 GMT"}, {"version": "v2", "created": "Mon, 20 Jun 2016 17:29:54 GMT"}, {"version": "v3", "created": "Fri, 24 Jun 2016 18:57:34 GMT"}, {"version": "v4", "created": "Thu, 6 Apr 2017 12:47:01 GMT"}, {"version": "v5", "created": "Thu, 8 Jun 2017 17:12:35 GMT"}, {"version": "v6", "created": "Fri, 8 Sep 2017 21:08:25 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Werpachowska", "Agnieszka", "", "Averisera Ltd"], ["Werpachowski", "Roman", ""]]}, {"id": "1606.04896", "submitter": "Elias Chaibub Neto", "authors": "Elias Chaibub Neto", "title": "Using instrumental variables to disentangle treatment and placebo\n  effects in blinded and unblinded randomized clinical trials influenced by\n  unmeasured confounders", "comments": "29 pages, 13 figures. Version 2 includes randomization confidence\n  intervals and more references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical trials traditionally employ blinding as a design mechanism to reduce\nthe influence of placebo effects. In practice, however, it can be difficult or\nimpossible to blind study participants and unblinded trials are common in\nmedical research. Here we show how instrumental variables can be used to\nquantify and disentangle treatment and placebo effects in randomized clinical\ntrials comparing control and active treatments in the presence of confounders.\nThe key idea is to use randomization to separately manipulate treatment\nassignment and psychological encouragement messages that increase the\nparticipants' desire for improved symptoms. The proposed approach is able to\nimprove the estimation of treatment effects in blinded studies and, most\nimportantly, opens the doors to account for placebo effects in unblinded\ntrials.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 18:36:46 GMT"}, {"version": "v2", "created": "Tue, 21 Jun 2016 19:39:54 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Neto", "Elias Chaibub", ""]]}, {"id": "1606.05067", "submitter": "Han Lin Shang", "authors": "Han Lin Shang", "title": "Mortality and life expectancy forecasting for a group of populations in\n  developed countries: A multilevel functional data method", "comments": "61 pages, 6 figures, 13 tables", "journal-ref": "The Annals of Applied Statistics 2016, 10(3), 1639-1672", "doi": "10.1214/16-AOAS953", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multilevel functional data method is adapted for forecasting age-specific\nmortality for two or more populations in developed countries with high-quality\nvital registration systems. It uses multilevel functional principal component\nanalysis of aggregate and population-specific data to extract the common trend\nand population-specific residual trend among populations. If the forecasts of\npopulation-specific residual trends do not show a long-term trend, then\nconvergence in forecasts may be achieved. This method is first applied to age-\nand sex-specific data for the United Kingdom, and its forecast accuracy is then\nfurther compared with several existing methods, including independent\nfunctional data and product-ratio methods, through a multi-country comparison.\nThe proposed method is also demonstrated by age-, sex- and state-specific data\nin Australia, where the convergence in forecasts can possibly be achieved by\nsex and state. For forecasting age-specific mortality, the multilevel\nfunctional data method is more accurate than the other coherent methods\nconsidered. For forecasting female life expectancy at birth, the multilevel\nfunctional data method is outperformed by the Bayesian method of \\cite{RLG14}.\nFor forecasting male life expectancy at birth, the multilevel functional data\nmethod performs better than the Bayesian methods in terms of point forecasts,\nbut less well in terms of interval forecasts. Supplementary materials for this\narticle are available online.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 07:00:49 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Shang", "Han Lin", ""]]}, {"id": "1606.05107", "submitter": "Isobel Claire Gormley Dr.", "authors": "Damien McParland, Catherine M. Phillips, Lorraine Brennan, Helen M.\n  Roche and Isobel Claire Gormley", "title": "Clustering high dimensional mixed data to uncover sub-phenotypes:joint\n  analysis of phenotypic and genotypic data", "comments": "36 pages, 5 figures and 4 tables", "journal-ref": "Statistics in Medicine (2017) 36(28), 4548-4569", "doi": "10.1002/sim.7371", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The LIPGENE-SU.VI.MAX study, like many others, recorded high dimensional\ncontinuous phenotypic data and categorical genotypic data. LIPGENE-SU.VI.MAX\nfocuses on the need to account for both phenotypic and genetic factors when\nstudying the metabolic syndrome (MetS), a complex disorder that can lead to\nhigher risk of type 2 diabetes and cardiovascular disease. Interest lies in\nclustering the LIPGENE-SU.VI.MAX participants into homogeneous groups or\nsub-phenotypes, by jointly considering their phenotypic and genotypic data, and\nin determining which variables are discriminatory.\n  A novel latent variable model which elegantly accommodates high dimensional,\nmixed data is developed to cluster LIPGENE-SU.VI.MAX participants using a\nBayesian finite mixture model. A computationally efficient variable selection\nalgorithm is incorporated, estimation is via a Gibbs sampling algorithm and an\napproximate BIC-MCMC criterion is developed to select the optimal model.\n  Two clusters or sub-phenotypes (`healthy' and `at risk') are uncovered. A\nsmall subset of variables is deemed discriminatory which notably includes\nphenotypic and genotypic variables, highlighting the need to jointly consider\nboth factors. Further, seven years after the LIPGENE-SU.VI.MAX data were\ncollected, participants underwent further analysis to diagnose presence or\nabsence of the MetS. The two uncovered sub-phenotypes strongly correspond to\nthe seven year follow up disease classification, highlighting the role of\nphenotypic and genotypic factors in the MetS, and emphasising the potential\nutility of the clustering approach in early screening. Additionally, the\nability of the proposed approach to define the uncertainty in sub-phenotype\nmembership at the participant level is synonymous with the concepts of\nprecision medicine and nutrition.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 09:23:37 GMT"}, {"version": "v2", "created": "Thu, 24 May 2018 14:26:08 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["McParland", "Damien", ""], ["Phillips", "Catherine M.", ""], ["Brennan", "Lorraine", ""], ["Roche", "Helen M.", ""], ["Gormley", "Isobel Claire", ""]]}, {"id": "1606.05156", "submitter": "Joao Vieira", "authors": "Joao Vieira, Fredrik Rusek, Ove Edfors, Steffen Malkowsky, Liang Liu,\n  Fredrik Tufvesson", "title": "Reciprocity Calibration for Massive MIMO: Proposal, Modeling and\n  Validation", "comments": "Submitted to IEEE Transactions on Wireless Communications,\n  21/Feb/2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a mutual coupling based calibration method for\ntime-division-duplex massive MIMO systems, which enables downlink precoding\nbased on uplink channel estimates. The entire calibration procedure is carried\nout solely at the base station (BS) side by sounding all BS antenna pairs. An\nExpectation-Maximization (EM) algorithm is derived, which processes the\nmeasured channels in order to estimate calibration coefficients. The EM\nalgorithm outperforms current state-of-the-art narrow-band calibration schemes\nin a mean squared error (MSE) and sum-rate capacity sense. Like its\npredecessors, the EM algorithm is general in the sense that it is not only\nsuitable to calibrate a co-located massive MIMO BS, but also very suitable for\ncalibrating multiple BSs in distributed MIMO systems.\n  The proposed method is validated with experimental evidence obtained from a\nmassive MIMO testbed. In addition, we address the estimated narrow-band\ncalibration coefficients as a stochastic process across frequency, and study\nthe subspace of this process based on measurement data. With the insights of\nthis study, we propose an estimator which exploits the structure of the process\nin order to reduce the calibration error across frequency. A model for the\ncalibration error is also proposed based on the asymptotic properties of the\nestimator, and is validated with measurement results.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 12:13:27 GMT"}, {"version": "v2", "created": "Thu, 17 Nov 2016 14:06:39 GMT"}, {"version": "v3", "created": "Tue, 21 Feb 2017 16:11:23 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Vieira", "Joao", ""], ["Rusek", "Fredrik", ""], ["Edfors", "Ove", ""], ["Malkowsky", "Steffen", ""], ["Liu", "Liang", ""], ["Tufvesson", "Fredrik", ""]]}, {"id": "1606.05196", "submitter": "Fabo  Feng", "authors": "Fabo Feng and M. Tuomi and H. R. A. Jones and R. P. Butler and S. Vogt", "title": "A Goldilocks principle for modeling radial velocity noise", "comments": "14 pages, 6 figures, accepted for publication in MNRAS", "journal-ref": null, "doi": "10.1093/mnras/stw1478", "report-no": null, "categories": "astro-ph.EP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The doppler measurements of stars are diluted and distorted by stellar\nactivity noise. Different choices of noise models and statistical methods have\nled to much controversy in the confirmation of exoplanet candidates obtained\nthrough analysing radial velocity data. To quantify the limitation of various\nmodels and methods, we compare different noise models and signal detection\ncriteria for various simulated and real data sets in the Bayesian framework.\nAccording to our analyses, the white noise model tend to interpret noise as\nsignal, leading to false positives. On the other hand, the red noise models are\nlikely to interprete signal as noise, resulting in false negatives. We find\nthat the Bayesian information criterion combined with a Bayes factor threshold\nof 150 can efficiently rule out false positives and confirm true detections. We\nfurther propose a Goldilocks principle aimed at modeling radial velocity noise\nto avoid too many false positives and too many false negatives. We propose that\nthe noise model with RHK-dependent jitter is used in combination with the\nmoving average model to detect planetary signals for M dwarfs. Our work may\nalso shed light on the noise modeling for hotter stars, and provide a valid\napproach for finding similar principles in other disciplines.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 14:22:10 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Feng", "Fabo", ""], ["Tuomi", "M.", ""], ["Jones", "H. R. A.", ""], ["Butler", "R. P.", ""], ["Vogt", "S.", ""]]}, {"id": "1606.05363", "submitter": "Zhengyi Zhou", "authors": "Zhengyi Zhou", "title": "Predicting Ambulance Demand: Challenges and Methods", "comments": "presented at 2016 ICML Workshop on #Data4Good: Machine Learning in\n  Social Good Applications, New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting ambulance demand accurately at a fine resolution in time and space\n(e.g., every hour and 1 km$^2$) is critical for staff / fleet management and\ndynamic deployment. There are several challenges: though the dataset is\ntypically large-scale, demand per time period and locality is almost always\nzero. The demand arises from complex urban geography and exhibits complex\nspatio-temporal patterns, both of which need to captured and exploited. To\naddress these challenges, we propose three methods based on Gaussian mixture\nmodels, kernel density estimation, and kernel warping. These methods provide\nspatio-temporal predictions for Toronto and Melbourne that are significantly\nmore accurate than the current industry practice.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 20:25:47 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Zhou", "Zhengyi", ""]]}, {"id": "1606.05382", "submitter": "Arin Chaudhuri", "authors": "Arin Chaudhuri, Deovrat Kakde, Maria Jahja, Wei Xiao, Hansi Jiang,\n  Seunghyun Kong, Sergiy Peredriy", "title": "Sampling Method for Fast Training of Support Vector Data Description", "comments": null, "journal-ref": null, "doi": "10.1109/RAM.2018.8463127", "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support Vector Data Description (SVDD) is a popular outlier detection\ntechnique which constructs a flexible description of the input data. SVDD\ncomputation time is high for large training datasets which limits its use in\nbig-data process-monitoring applications. We propose a new iterative\nsampling-based method for SVDD training. The method incrementally learns the\ntraining data description at each iteration by computing SVDD on an independent\nrandom sample selected with replacement from the training data set. The\nexperimental results indicate that the proposed method is extremely fast and\nprovides a good data description .\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 23:18:23 GMT"}, {"version": "v2", "created": "Fri, 24 Jun 2016 17:30:57 GMT"}, {"version": "v3", "created": "Sun, 25 Sep 2016 22:15:38 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Chaudhuri", "Arin", ""], ["Kakde", "Deovrat", ""], ["Jahja", "Maria", ""], ["Xiao", "Wei", ""], ["Jiang", "Hansi", ""], ["Kong", "Seunghyun", ""], ["Peredriy", "Sergiy", ""]]}, {"id": "1606.05619", "submitter": "Paramita Saha-Chaudhuri", "authors": "Paramita Saha-Chaudhuri and Clarice Weinberg", "title": "Data Privacy and Specimen Pooling: Using an old tool for New Challenges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Background: In the context of ongoing debate over data confidentiality versus\nshared use of research data, as raised following the new EU General Data\nProtection Regulation, we seek to find alternate techniques that can balance\nthese two issues. In particular, we demonstrate that an existing epidemiologic\ntool, specimen pooling, can be adapted as a privacy-preserving method to enable\ndata analysis while maintaining data confidentiality. Specimen pooling is a\ncost-effective tool in studying the effect of an expensive-to-measure exposure\non a disease outcome, for both unmatched and matched case-control designs. We\npropose the technique in a new context to analyze confidential data and\ndemonstrate that it can be successfully used to estimate OR of covariates based\non aggregate data when individual patient data cannot be shared.\n  Methods: We demonstrate the application of specimen pooling based on\naggregate covariate level and show that aggregated covariate levels can be used\nin a conditional logistic regression model to estimate individual-level odds\nratio parameters of interest. We then show how to adapt the technique as a\nprivacy-preserving method to analyze data from a matched case-control design. A\nsimilar approach can be applied for an unmatched design and unconditional\nlogistic regression.\n  Results: The parameter estimates from the standard conditional logistic\nregression are compared to those based on aggregated data. The parameter\nestimates are similar and have similar standard errors and confidence interval\ncoverage.\n  Conclusions: Pooling can be used effectively to analyze confidential data\narising from distributed data networks and will be extremely useful in\npharmacoepidemiology.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 18:44:07 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Saha-Chaudhuri", "Paramita", ""], ["Weinberg", "Clarice", ""]]}, {"id": "1606.05656", "submitter": "Leopoldo Catania", "authors": "Leopoldo Catania and Nima Nonejad", "title": "Dynamic Model Averaging for Practitioners in Economics and Finance: The\n  eDMA Package", "comments": "21 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.CE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Raftery, Karny, and Ettler (2010) introduce an estimation technique, which\nthey refer to as Dynamic Model Averaging (DMA). In their application, DMA is\nused to predict the output strip thickness for a cold rolling mill, where the\noutput is measured with a time delay. Recently, DMA has also shown to be useful\nin macroeconomic and financial applications. In this paper, we present the eDMA\npackage for DMA estimation implemented in R. The eDMA package is especially\nsuited for practitioners in economics and finance, where typically a large\nnumber of predictors are available. Our implementation is up to 133 times\nfaster then a standard implementation using a single-core CPU. Thus, with the\nhelp of this package, practitioners are able to perform DMA on a standard PC\nwithout resorting to large clusters, which are not easily available to all\nresearchers. We demonstrate the usefulness of this package through simulation\nexperiments and an empirical application using quarterly U.S. inflation data.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 20:00:56 GMT"}, {"version": "v2", "created": "Mon, 25 Jul 2016 14:30:52 GMT"}, {"version": "v3", "created": "Mon, 16 Oct 2017 19:34:04 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Catania", "Leopoldo", ""], ["Nonejad", "Nima", ""]]}, {"id": "1606.05658", "submitter": "Trevor Hefley", "authors": "Trevor J. Hefley, Kristin M. Broms, Brian M. Brost, Frances E.\n  Buderman, Shannon L. Kay, Henry R. Scharf, John R. Tipton, Perry J. Williams,\n  Mevin B. Hooten", "title": "The basis function approach for modeling autocorrelation in ecological\n  data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing ecological data often requires modeling the autocorrelation created\nby spatial and temporal processes. Many of the statistical methods used to\naccount for autocorrelation can be viewed as regression models that include\nbasis functions. Understanding the concept of basis functions enables\necologists to modify commonly used ecological models to account for\nautocorrelation, which can improve inference and predictive accuracy.\nUnderstanding the properties of basis functions is essential for evaluating the\nfit of spatial or time-series models, detecting a hidden form of\nmulticollinearity, and analyzing large data sets. We present important concepts\nand properties related to basis functions and illustrate several tools and\ntechniques ecologists can use when modeling autocorrelation in ecological data.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 20:04:46 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Hefley", "Trevor J.", ""], ["Broms", "Kristin M.", ""], ["Brost", "Brian M.", ""], ["Buderman", "Frances E.", ""], ["Kay", "Shannon L.", ""], ["Scharf", "Henry R.", ""], ["Tipton", "John R.", ""], ["Williams", "Perry J.", ""], ["Hooten", "Mevin B.", ""]]}, {"id": "1606.05771", "submitter": "Sacha Epskamp", "authors": "Sacha Epskamp", "title": "Brief Report on Estimating Regularized Gaussian Networks from Continuous\n  and Ordinal Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent literature, the Gaussian Graphical model (GGM; Lauritzen, 1996),a\nnetwork of partial correlation coefficients, has been used to capture potential\ndynamic relationships between observed variables. The GGM can be estimated\nusing regularization in combination with model selection using the extended\nBayesian Information Criterion (Foygel and Drton, 2010). I term this\nmethodology GeLasso, and asses its performance using a plausible psychological\nnetwork structure with both continuous and ordinal datasets.Simulation results\nindicate that GeLasso works well as an out-of-the-box method to estimate\nnetwork structures.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jun 2016 16:02:48 GMT"}, {"version": "v2", "created": "Fri, 22 Sep 2017 04:43:41 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Epskamp", "Sacha", ""]]}, {"id": "1606.05778", "submitter": "Kota Mori", "authors": "Kota Mori", "title": "Estimating the Handicap Effect in the Go Game: A Regression\n  Discontinuity Design Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides an estimate for the handicap effect in the go game, a\nboard game widely played in Asia and other parts of the world. The estimation\nutilizes a unique handicap assignment rule of the game, where the amount of\nhandicaps changes discontinuously with the players' strengths. A dataset\nsuitable for this estimation strategy is collected from game archives of an\nonline platform. The result implies that an additional handicap typically\nchanges the game odds by about 30 percent points, while the impact varies\nacross the handicap level.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jun 2016 16:41:27 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Mori", "Kota", ""]]}, {"id": "1606.05900", "submitter": "Timothy Brathwaite", "authors": "Timothy Brathwaite, Joan L. Walker", "title": "Asymmetric, Closed-Form, Finite-Parameter Models of Multinomial Choice", "comments": "47 pages, 7 figures, 9 tables", "journal-ref": null, "doi": "10.1016/j.jocm.2018.01.002", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In transportation, the number of observations associated with one discrete\noutcome is often greatly different from the number of observations associated\nwith another discrete outcome. This situation is known as class-imbalance. In\nstatistics, one hypothesized explanation for class imbalance is the existence\nof data generating processes that are characterized by asymmetric (as opposed\nto typically symmetric) probability functions. Despite being a valid hypothesis\nfor class-imbalanced choice situations, few simple models exist for testing\nthis explanation in transportation settings---settings that are inherently\nmultinomial. Our paper fills this gap. As such, it should be of interest to\ntransportation scholars and practitioners alike.\n  Overall, we addressed the following questions: \"how can one construct\nasymmetric, closed-form, finite-parameter models of multinomial choice\" and\n\"how do such models compare against commonly used symmetric models?\" To do so,\nwe (1) introduced a new class of closed-form, finite-parameter, multinomial\nchoice models that we call \"logit-type models,\" (2) introduced a procedure for\nusing our logit-type models to extend existing binary choice models to the\nmultinomial setting, and (3) introduced a procedure for creating new binary\nchoice models (both symmetric and asymmetric). Together, our contributions\nallow us to create new asymmetric, multinomial choice models by creating\nmultinomial extensions of asymmetric, binary choice models that already exist\nor that we create ourselves. We demonstrated our methods by developing four new\nasymmetric, multinomial choice models. We found that most of our asymmetric\nmodels dominated the multinomial logit (MNL) model in terms of in-sample and\nout-of-sample log-likelihoods. Moreover, on our two empirical applications, we\nalso found practical differences between the MNL model and our new asymmetric\nmodels.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jun 2016 18:51:09 GMT"}, {"version": "v2", "created": "Fri, 9 Feb 2018 04:53:51 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Brathwaite", "Timothy", ""], ["Walker", "Joan L.", ""]]}, {"id": "1606.05907", "submitter": "Kevin Coakley", "authors": "Kevin J Coakley and Jifeng Qu", "title": "Spectral model selection in the electronic measurement of the Boltzmann\n  constant by Johnson noise thermometry", "comments": "25 pages, 8 figures. New version has a modified abstract, added text\n  in Introduction, new entries in Table 5, additional references, and a new\n  title. The main technical results are unchanged", "journal-ref": null, "doi": "10.1088/1681-7575/aa5d21", "report-no": null, "categories": "stat.AP physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the electronic measurement of the Boltzmann constant based on Johnson\nnoise thermometry, the ratio of the power spectral densities of thermal noise\nacross a resistor at the triple point of water, and pseudo-random noise\nsynthetically generated by a quantum-accurate voltage-noise source is constant\nto within 1 part in a billion for frequencies up to 1 GHz. Given this ratio,\nand the values of other known or measured parameters, one can determine the\nBoltzmann constant. Due, in part, to mismatch between transmission lines, the\nexperimental ratio spectrum varies with frequency. We model this spectrum as an\neven polynomial function of frequency where the constant term in the polynomial\ndetermines the Boltzmann constant. When determining this constant (offset) from\nexperimental data, the assumed complexity of the ratio spectrum model and the\nmaximum frequency analyzed (fitting bandwidth) dramatically affects results. We\nselect the complexity of the model by cross-validation. For each of many\nfitting bandwidths, we determine the component of uncertainty of the offset\nterm that accounts for random and systematic effects associated with imperfect\nknowledge of model complexity. We select the fitting bandwidth that minimizes\nthis uncertainty. In the most recent measurement of the Boltzmann constant,\nresults were determined, in part, by application of an earlier version of the\nmethod described here. Here, we extend the earlier analysis by considering a\nbroader range of fitting bandwidths and quantify an additional component of\nuncertainty that accounts for imperfect performance of our fitting bandwidth\nselection method. For idealized simulated data our method correctly selects the\ntrue complexity of the ratio spectrum model for all cases considered. A new\nanalysis of data from the recent experiment yields evidence for a temporal\ntrend in the offset parameters.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jun 2016 20:45:49 GMT"}, {"version": "v2", "created": "Thu, 2 Feb 2017 21:59:38 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Coakley", "Kevin J", ""], ["Qu", "Jifeng", ""]]}, {"id": "1606.06017", "submitter": "Catherine Matias", "authors": "Ana Arribas-Gil, Catherine Matias (LPMA)", "title": "A time warping approach to multiple sequence alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach for multiple sequence alignment (MSA) derived from the\ndynamic time warping viewpoint and recent techniques of curve synchronization\ndeveloped in the context of functional data analysis. Starting from pairwise\nalignments of all the sequences (viewed as paths in a certain space), we\nconstruct a median path that represents the MSA we are looking for. We\nestablish a proof of concept that our method could be an interesting ingredient\nto include into refined MSA techniques. We present a simple synthetic\nexperiment as well as the study of a benchmark dataset, together with\ncomparisons with 2 widely used MSA softwares.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 08:56:38 GMT"}, {"version": "v2", "created": "Fri, 6 Jan 2017 15:39:07 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Arribas-Gil", "Ana", "", "LPMA"], ["Matias", "Catherine", "", "LPMA"]]}, {"id": "1606.06115", "submitter": "Andrzej Jarynowski", "authors": "Andrzej Buda, Andrzej Jarynowski", "title": "Diffusion paths between product life-cycles in European phonographic\n  markets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.SI nlin.AO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have investigated the product life-cycles of almost 17 000 hit singles\nperformed on the 12 biggest national phonographic markets in Europe including:\nAustria, Belgium, France, Germany, Ireland, Italy, Netherlands, Norway, Spain,\nSweden, Switzerland and the United Kingdom. We have considered weekly singles\ncharts from the last 50 years (1966-2015) in each country. We analyze the\nspread of hit singles popularity (chart topping) as an epidemiological process\nperformed on various European countries. Popular hit singles are contagious\nfrom one country to another. Thus, we consider time delays between the initial\nhit single release and reaching the highest position on consecutive national\nsingles charts. We create directed network of countries representing\ntransitions of hit singles popularity between countries. It is obtained by\nsimulating the most likely paths and picking up the most frequent links. A\ncountry of initial hit single release is considered as a source of infection.\nOur algorithm builds up spanning trees by attaching new nodes. The probability\nof attachment depends on:\n  1) new nodes immunity\n  2) infectivity of previous nodes from the tree. Thus we obtain network of\npopularity spread with a hub the UK, a bridge the Netherlands and outliers\nItaly and Spain. We have found a characteristic topology of hit singles\npopularity spread. The positive correlation between this network and geographic\nor cultural grid-map of Europe is also observed. However, the network of\npopularity spread has some typical properties of complex networks.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 13:45:28 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Buda", "Andrzej", ""], ["Jarynowski", "Andrzej", ""]]}, {"id": "1606.06279", "submitter": "Luca Pappalardo", "authors": "Luca Pappalardo and Maarten Vanhoof and Lorenzo Gabrielli and Zbigniew\n  Smoreda and Dino Pedreschi and Fosca Giannotti", "title": "An analytical framework to nowcast well-being using mobile phone data", "comments": null, "journal-ref": null, "doi": "10.1007/s41060-016-0013-2", "report-no": null, "categories": "cs.CY cs.SI physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An intriguing open question is whether measurements made on Big Data\nrecording human activities can yield us high-fidelity proxies of socio-economic\ndevelopment and well-being. Can we monitor and predict the socio-economic\ndevelopment of a territory just by observing the behavior of its inhabitants\nthrough the lens of Big Data? In this paper, we design a data-driven analytical\nframework that uses mobility measures and social measures extracted from mobile\nphone data to estimate indicators for socio-economic development and\nwell-being. We discover that the diversity of mobility, defined in terms of\nentropy of the individual users' trajectories, exhibits (i) significant\ncorrelation with two different socio-economic indicators and (ii) the highest\nimportance in predictive models built to predict the socio-economic indicators.\nOur analytical framework opens an interesting perspective to study human\nbehavior through the lens of Big Data by means of new statistical indicators\nthat quantify and possibly \"nowcast\" the well-being and the socio-economic\ndevelopment of a territory.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 23:15:23 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Pappalardo", "Luca", ""], ["Vanhoof", "Maarten", ""], ["Gabrielli", "Lorenzo", ""], ["Smoreda", "Zbigniew", ""], ["Pedreschi", "Dino", ""], ["Giannotti", "Fosca", ""]]}, {"id": "1606.06284", "submitter": "Amanda Mejia", "authors": "Amanda F. Mejia, Mary Beth Nebel, Anita D. Barber, Ann S. Choe and\n  Martin A. Lindquist", "title": "Effects of Scan Length and Shrinkage on Reliability of Resting-State\n  Functional Connectivity in the Human Connectome Project", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we use data from the Human Connectome Project (N=461) to\ninvestigate the effect of scan length on reliability of resting-state\nfunctional connectivity (rsFC) estimates produced from resting-state functional\nmagnetic resonance imaging (rsfMRI). Additionally, we study the benefits of\nempirical Bayes shrinkage, in which subject-level estimates borrow strength\nfrom the population average by trading a small increase in bias for a greater\nreduction in variance. For each subject, we compute raw and shrinkage estimates\nof rsFC between 300 regions identified through independent components analysis\n(ICA) based on rsfMRI scans varying from 3 to 30 minutes in length. The time\ncourse for each region is determined using dual regression, and rsFC is\nestimated as the Pearson correlation between each pair of time courses.\nShrinkage estimates for each subject are computed as a weighted average between\nthe raw subject-level estimate and the population average estimate, where the\nweight is determined for each connection by the relationship of within-subject\nvariance to between-subject variance. We find that shrinkage estimates exhibit\ngreater reliability than raw estimates for most connections, with 30-40%\nimprovement using scans less than 10 minutes in length and 10-20% improvement\nusing scans of 20-30 minutes. We also observe significant spatial variability\nin reliability of both raw and shrinkage estimates, with connections within the\ndefault mode and motor networks exhibiting the greatest reliability and\nbetween-network connections exhibiting the poorest reliability. We conclude\nthat the scan length required for reliable estimation of rsFC depends on the\nspecific connections of interest, and shrinkage can be used to increase\nreliability of rsFC, even when produced from long, high-quality rsfMRI scans.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jun 2016 15:34:11 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Mejia", "Amanda F.", ""], ["Nebel", "Mary Beth", ""], ["Barber", "Anita D.", ""], ["Choe", "Ann S.", ""], ["Lindquist", "Martin A.", ""]]}, {"id": "1606.06323", "submitter": "Barbara Han", "authors": "Barbara A. Han, Laura Yang", "title": "Predicting Novel Tick Vectors of Zoonotic Disease", "comments": "Presented at 2016 ICML Workshop on #Data4Good: Machine Learning in\n  Social Good Applications, New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the resurgence of tick-borne diseases such as Lyme disease and the\nemergence of new pathogens such as Powassan virus, understanding what\ndistinguishes vector from non-vector species, and predicting undiscovered tick\nvectors is an important step towards mitigating human disease risk. We apply\ngeneralized boosted regression to interrogate over 90 features for over 240\nspecies of Ixodes ticks. Our model predicted vector status with ~97% accuracy\nand implicated 14 tick species whose intrinsic trait profiles confer high\nprobabilities (~80%) that they are capable of transmitting infections from\nanimal hosts to humans. Distinguishing characteristics of zoonotic tick vectors\ninclude several anatomical structures that facilitate efficient host seeking\nand blood-feeding from a wide variety of host species. Boosted regression\nanalysis produced both actionable predictions to guide ongoing surveillance as\nwell as testable hypotheses about the biological underpinnings of vectorial\ncapacity across tick species.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 20:33:07 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Han", "Barbara A.", ""], ["Yang", "Laura", ""]]}, {"id": "1606.06328", "submitter": "Ian Barnett", "authors": "Ian Barnett and Jukka-Pekka Onnela", "title": "Inferring Mobility Measures from GPS Traces with Missing Data", "comments": "33 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With increasing availability of smartphones with GPS capabilities,\nlarge-scale studies relating individual-level mobility patterns to a wide\nvariety of patient-centered outcomes, from mood disorders to surgical recovery,\nare becoming a reality. Similar past studies have been small in scale and have\nprovided wearable GPS devices to subjects. These devices typically collect\nmobility traces continuously without significant gaps in the data, and\nconsequently the problem of data missingness has been safely ignored.\nLeveraging subjects' own smartphones makes it possible to scale up and extend\nthe duration of these types of studies, but at the same time introduces a\nsubstantial challenge: to preserve a smartphone's battery, GPS can be active\nonly for a small portion of the time, frequently less than $10\\%$, leading to a\ntremendous missing data problem. We introduce a principled statistical\napproach, based on weighted resampling of the observed data, to impute the\nmissing mobility traces, which we then summarize using different mobility\nmeasures. We compare the strengths of our approach to linear interpolation, a\npopular approach for dealing with missing data, both analytically and through\nsimulation of missingness for empirical data. We conclude that our imputation\napproach better mirrors human mobility both theoretically and over a sample of\nGPS mobility traces from 182 individuals in the Geolife data set, where,\nrelative to linear interpolation, imputation resulted in a 10-fold reduction in\nthe error averaged across all mobility features.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 20:55:05 GMT"}, {"version": "v2", "created": "Thu, 19 Apr 2018 14:36:10 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Barnett", "Ian", ""], ["Onnela", "Jukka-Pekka", ""]]}, {"id": "1606.06423", "submitter": "Mehdi Korki", "authors": "Hadi Zayyani and Mehdi Korki", "title": "Non-Coherent Direction of Arrival Estimation via Frequency Estimation", "comments": "4 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This letter investigates the non-coherent Direction of Arrival (DOA)\nestimation problem dealing with the DOA estimation from magnitude only\nmeasurements of the array output. The magnitude squared of the array output is\nexpanded as a superposition of some harmonics. Hence, a frequency estimation\napproach is used to find some nonlinear relations between DOAs, which results\nin inherent ambiguities. To solve the nonlinear equations and resolve the\nambiguities, we assume a high amplitude reference target at low angles to\nestimate the true DOA's with no ambiguities. However, the proposed algorithm\nrequires a large number of antenna array elements to accurately estimate the\nDOA's. To overcome this drawback, and to enhance the estimation accuracy, we\nsuggest two variants of the algorithm. One is to add virtual elements in the\narray and the second is to integrate multiple snapshots. Simulation results\nshow that the proposed frequency estimation-based algorithm outperforms the\nnon-coherent GESPAR algorithm in the low signal to noise ratio (SNR) regime and\nit is two orders of magnitude faster than the non-coherent GESPAR algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 04:54:55 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Zayyani", "Hadi", ""], ["Korki", "Mehdi", ""]]}, {"id": "1606.06521", "submitter": "Elvira Di Nardo Prof.", "authors": "E. Di Nardo, R. Simone", "title": "CUB models: a preliminary fuzzy approach to heterogeneity", "comments": "10 pages, invited contribution at SIS2016 (Salerno, Italy), in\n  SIS2016 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In line with the increasing attention paid to deal with uncertainty in\nordinal data models, we propose to combine Fuzzy models with \\cub models within\nquestionnaire analysis. In particular, the focus will be on \\cub models'\nuncertainty parameter and its interpretation as a preliminary measure of\nheterogeneity, by introducing membership, non-membership and uncertainty\nfunctions in the more general framework of Intuitionistic Fuzzy Sets. Our\nproposal is discussed on the basis of the Evaluation of Orientation Services\nsurvey collected at University of Naples Federico II.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 11:27:15 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Di Nardo", "E.", ""], ["Simone", "R.", ""]]}, {"id": "1606.06562", "submitter": "Giovanni Parmigiani", "authors": "Travis Gerke, Svitlana Tyekucheva, Lorelei Mucci, Giovanni Parmigiani", "title": "Logistic push: a regression framework for partial AUC optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The area under the receiver operating characteristic curve (AUC) is often\nused to evaluate the performance of clinical prediction models. Recently, a\nmore refined strategy has been proposed to examine a partial area under the\ncurve (pAUC), which can account for differing costs associated with false\nnegative versus false positive results. Such consideration can substantially\nincrease the clinical utility of prediction models depending on the clinical\nquestion. Properties of the pAUC estimator create significant challenges for\npAUC-optimal marker selection and model building. As such, current approaches\ntowards these aims can be complex and computationally intensive. We present a\nsimpler method based on weighted logistic regressions. We refer to our strategy\nas logistic push, due to shared heuristics with the ranking algorithm P-norm\npush. Logistic push is particularly useful in the high-dimensional setting,\nwhere fast and broadly available algorithms for fitting penalized regressions\ncan be used for both marker selection and model fitting.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 13:31:38 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Gerke", "Travis", ""], ["Tyekucheva", "Svitlana", ""], ["Mucci", "Lorelei", ""], ["Parmigiani", "Giovanni", ""]]}, {"id": "1606.06591", "submitter": "Nikolai Gagunashvili", "authors": "Nikolai Gagunashvili", "title": "Tests for Comparing Weighted Histograms. Review and Improvements", "comments": "25 pages, 5 figures", "journal-ref": "Eur. Phys. J. Plus (2017) 132: 196", "doi": "10.1140/epjp/i2017-11481-1", "report-no": null, "categories": "physics.data-an astro-ph.IM hep-ex nucl-ex stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Histograms with weighted entries are used to estimate probability density\nfunctions. Computer simulation is the main application of this type of\nhistograms. A review on chi-square tests for comparing weighted histograms is\npresented in this paper. Improvements to these tests that have a size closer to\nits nominal value are proposed. Numerical examples are presented for evaluation\nand demonstration of various applications of the tests.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 14:25:26 GMT"}, {"version": "v2", "created": "Sat, 10 Sep 2016 18:39:01 GMT"}, {"version": "v3", "created": "Fri, 24 Mar 2017 15:47:26 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Gagunashvili", "Nikolai", ""]]}, {"id": "1606.06633", "submitter": "Gaby Schneider", "authors": "Stefan Albert, Michael Messer, Julia Schiemann, Jochen Roeper, Gaby\n  Schneider", "title": "Multi-scale detection of variance changes in renewal processes in the\n  presence of rate change points", "comments": "file updated with final accepted version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-stationarity of the rate or variance of events is a well-known problem in\nthe description and analysis of time series of events, such as neuronal spike\ntrains. A multiple filter test (MFT) for rate homogeneity has been proposed\nearlier that detects change points on multiple time scales simultaneously. It\nis based on a filtered derivative approach, and the rejection threshold derives\nfrom a Gaussian limit process $L$ which is independent of the point process\nparameters.\n  Here we extend the MFT to variance homogeneity of life times. When the rate\nis constant, the MFT extends directly to the null hypothesis of constant\nvariance. In the presence of rate change points, we propose to incorporate\nestimates of these in the test for variance homogeneity, using an adaptation of\nthe test statistic. The resulting limit process shows slight deviations from\n$L$ that depend on unknown process parameters. However, these deviations are\nsmall and do not considerably change the properties of the statistical test.\nThis allows practical application, e.g.~to neuronal spike trains, which\nindicates various profiles of rate and variance change points.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 15:56:59 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2018 12:21:51 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Albert", "Stefan", ""], ["Messer", "Michael", ""], ["Schiemann", "Julia", ""], ["Roeper", "Jochen", ""], ["Schneider", "Gaby", ""]]}, {"id": "1606.06758", "submitter": "Florent Leclercq", "authors": "Florent Leclercq, Guilhem Lavaux, Jens Jasche, Benjamin Wandelt", "title": "Comparing cosmic web classifiers using information theory", "comments": "20 pages, 8 figures, 6 tables. Matches JCAP published version. Public\n  data available from the first author's website (currently\n  http://icg.port.ac.uk/~leclercq/)", "journal-ref": "JCAP08 (2016) 027", "doi": "10.1088/1475-7516/2016/08/027", "report-no": null, "categories": "astro-ph.CO astro-ph.IM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a decision scheme for optimally choosing a classifier, which\nsegments the cosmic web into different structure types (voids, sheets,\nfilaments, and clusters). Our framework, based on information theory, accounts\nfor the design aims of different classes of possible applications: (i)\nparameter inference, (ii) model selection, and (iii) prediction of new\nobservations. As an illustration, we use cosmographic maps of web-types in the\nSloan Digital Sky Survey to assess the relative performance of the classifiers\nT-web, DIVA and ORIGAMI for: (i) analyzing the morphology of the cosmic web,\n(ii) discriminating dark energy models, and (iii) predicting galaxy colors. Our\nstudy substantiates a data-supported connection between cosmic web analysis and\ninformation theory, and paves the path towards principled design of analysis\nprocedures for the next generation of galaxy surveys. We have made the cosmic\nweb maps, galaxy catalog, and analysis scripts used in this work publicly\navailable.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 20:31:53 GMT"}, {"version": "v2", "created": "Fri, 19 Aug 2016 15:03:52 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Leclercq", "Florent", ""], ["Lavaux", "Guilhem", ""], ["Jasche", "Jens", ""], ["Wandelt", "Benjamin", ""]]}, {"id": "1606.07153", "submitter": "Ryan Giordano", "authors": "Ryan Giordano, Tamara Broderick, Rachael Meager, Jonathan Huggins,\n  Michael Jordan", "title": "Fast robustness quantification with variational Bayes", "comments": "presented at 2016 ICML Workshop on #Data4Good: Machine Learning in\n  Social Good Applications, New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian hierarchical models are increasing popular in economics. When using\nhierarchical models, it is useful not only to calculate posterior expectations,\nbut also to measure the robustness of these expectations to reasonable\nalternative prior choices. We use variational Bayes and linear response methods\nto provide fast, accurate posterior means and robustness measures with an\napplication to measuring the effectiveness of microcredit in the developing\nworld.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 01:19:17 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Giordano", "Ryan", ""], ["Broderick", "Tamara", ""], ["Meager", "Rachael", ""], ["Huggins", "Jonathan", ""], ["Jordan", "Michael", ""]]}, {"id": "1606.07228", "submitter": "Yannick Vandendijck", "authors": "Yannick Vandendijck, Christel Faes, Niel Hens", "title": "Prevalence and trend estimation from observational data with highly\n  variable post-stratification weights", "comments": "Published at http://dx.doi.org/10.1214/15-AOAS874 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2016, Vol. 10, No. 1, 94-117", "doi": "10.1214/15-AOAS874", "report-no": "IMS-AOAS-AOAS874", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In observational surveys, post-stratification is used to reduce bias\nresulting from differences between the survey population and the population\nunder investigation. However, this can lead to inflated post-stratification\nweights and, therefore, appropriate methods are required to obtain less\nvariable estimates. Proposed methods include collapsing post-strata, trimming\npost-stratification weights, generalized regression estimators (GREG) and\nweight smoothing models, the latter defined by random-effects models that\ninduce shrinkage across post-stratum means. Here, we first describe the\nweight-smoothing model for prevalence estimation from binary survey outcomes in\nobservational surveys. Second, we propose an extension of this method for trend\nestimation. And, third, a method is provided such that the GREG can be used for\nprevalence and trend estimation for observational surveys. Variance estimates\nof all methods are described. A simulation study is performed to compare the\nproposed methods with other established methods. The performance of the\nnonparametric GREG is consistent over all simulation conditions and therefore\nserves as a valuable solution for prevalence and trend estimation from\nobservational surveys. The method is applied to the estimation of the\nprevalence and incidence trend of influenza-like illness using the 2010/2011\nGreat Influenza Survey in Flanders, Belgium.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 08:47:39 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Vandendijck", "Yannick", ""], ["Faes", "Christel", ""], ["Hens", "Niel", ""]]}, {"id": "1606.07300", "submitter": "Nirmal Baran Chakrabarti", "authors": "N B Chakrabarti", "title": "A Note on a Sum of Lognormals", "comments": "16 pages, 6 figures, to be submitted to a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note considers the applicability of Gauss-Hermite quadrature and direct\nnumerical quadrature for computation of moment generating function (mgf) and\nthe derivatives. A preprocessing using the asymptotic technique is employed\nwhile computing the characteristic function (chf) using Gauss Hermite\nquadrature while this is optional for mgf. The mgf of the low and high\namplitude regions of a single lognormal variable and the derivatives is\nexamined and attention is drawn to the effect of variance. The problem of\ninversion of the mgf/chf of a sum of lognormals to obtain the CDF/pdf is\nconsidered with special reference to methods related to Post Widder technique,\nGaussian quadrature and the Fourier series method. The method based on the\ncomplex exponential integral which makes use of the derivative of the cumulant\nis an alternative. Segmentation of the mgf/chf on the basis of the derivative\nstructure which indicates activity rate is shown to be useful.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 12:59:56 GMT"}, {"version": "v2", "created": "Fri, 23 Dec 2016 11:16:51 GMT"}], "update_date": "2016-12-26", "authors_parsed": [["Chakrabarti", "N B", ""]]}, {"id": "1606.07369", "submitter": "David Dooling", "authors": "David Dooling, Angela Kim, Barbara McAneny, Jennifer Webster", "title": "Personalized Prognostic Models for Oncology: A Machine Learning Approach", "comments": "28 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have applied a little-known data transformation to subsets of the\nSurveillance, Epidemiology, and End Results (SEER) publically available data of\nthe National Cancer Institute (NCI) to make it suitable input to standard\nmachine learning classifiers. This transformation properly treats the\nright-censored data in the SEER data and the resulting Random Forest and\nMulti-Layer Perceptron models predict full survival curves. Treating the 6, 12,\nand 60 months points of the resulting survival curves as 3 binary classifiers,\nthe 18 resulting classifiers have AUC values ranging from .765 to .885. Further\nevidence that the models have generalized well from the training data is\nprovided by the extremely high levels of agreement between the random forest\nand neural network models predictions on the 6, 12, and 60 month binary\nclassifiers.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 15:55:22 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Dooling", "David", ""], ["Kim", "Angela", ""], ["McAneny", "Barbara", ""], ["Webster", "Jennifer", ""]]}, {"id": "1606.07667", "submitter": "Egil Ferkingstad", "authors": "Egil Ferkingstad, Oli Pall Geirsson, Birgir Hrafnkelsson, Olafur\n  Birgir Davidsson and Sigurdur Magnus Gardarsson", "title": "A Bayesian hierarchical model for monthly maxima of instantaneous flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a comprehensive Bayesian hierarchical model for monthly maxima of\ninstantaneous flow in river catchments. The Gumbel distribution is used as the\nprobabilistic model for the observations, which are assumed to come from\nseveral catchments. Our suggested latent model is Gaussian and designed for\nmonthly maxima, making better use of the data than the standard approach using\nannual maxima. At the latent level, linear mixed models are used for both the\nlocation and scale parameters of the Gumbel distribution, accounting for\nseasonal dependence and covariates from the catchments. The specification of\nprior distributions makes use of penalised complexity (PC) priors, to ensure\nrobust inference for the latent parameters. The main idea behind the PC priors\nis to shrink toward a base model, thus avoiding overfitting. PC priors also\nprovide a convenient framework for prior elicitation based on simple notions of\nscale. Prior distributions for regression coefficients are also elicited based\non hydrological and meteorological knowledge. Posterior inference was done\nusing the MCMC split sampler, an efficient Gibbs blocking scheme tailored to\nlatent Gaussian models. The proposed model was applied to observed data from\neight river catchments in Iceland. A cross-validation study demonstrates good\npredictive performance.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 12:50:11 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Ferkingstad", "Egil", ""], ["Geirsson", "Oli Pall", ""], ["Hrafnkelsson", "Birgir", ""], ["Davidsson", "Olafur Birgir", ""], ["Gardarsson", "Sigurdur Magnus", ""]]}, {"id": "1606.07855", "submitter": "Weisi Deng Weisi Deng", "authors": "Weisi Deng, Yuting Ji, and Lang Tong", "title": "Probabilistic Forecasting and Simulation of Electricity Markets via\n  Online Dictionary Learning", "comments": "8 pages, 6 figures, Hawaii International Conference on System\n  Sciences 2017 (HICSS-50)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of probabilistic forecasting and online simulation of real-time\nelectricity market with stochastic generation and demand is considered. By\nexploiting the parametric structure of the direct current optimal power flow, a\nnew technique based on online dictionary learning (ODL) is proposed. The ODL\napproach incorporates real-time measurements and historical traces to produce\nforecasts of joint and marginal probability distributions of future locational\nmarginal prices, power flows, and dispatch levels, conditional on the system\nstate at the time of forecasting. Compared with standard Monte Carlo simulation\ntechniques, the ODL approach offers several orders of magnitude improvement in\ncomputation time, making it feasible for online forecasting of market\noperations. Numerical simulations on large and moderate size power systems\nillustrate its performance and complexity features and its potential as a tool\nfor system operators.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jun 2016 00:23:56 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Deng", "Weisi", ""], ["Ji", "Yuting", ""], ["Tong", "Lang", ""]]}, {"id": "1606.07986", "submitter": "Ephraim Hanks", "authors": "Ephraim M. Hanks and David A. Hughes", "title": "Flexible discrete space models of animal movement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Movement drives the spread of infectious disease, gene flow, and other\ncritical ecological processes. To study these processes we need models for\nmovement that capture complex behavior that changes over time and space in\nresponse to biotic and abiotic factors. Penalized likelihood approaches, such\nas penalized semiparametric spline expansions and LASSO regression, allow\ninference on complex models without overfitting. Continuous-time Markov chains\n(CTMCs) have been recently introduced as a flexible discrete-space model for\nanimal movement. Modeling with CTMCs involves discretizing an animal's path to\nthe resolution of a raster grid. The resulting stochastic process model can\neasily incorporate environmental and other covariates, represented as raster\nlayers, that affect directional bias and overall movement rate. We introduce a\nweighted likelihood approach that allows for modeling movement using CTMCs,\nwith path uncertainty due to missing data modeled by imputing continuous-time\npaths between telemetry locations. The framework we introduce allows for\ninference on CTMC movement models using existing software for fitting Poisson\nregression models, including penalized versions of Poisson regression. The\nresult is a flexible, powerful, and accessible framework for modeling a wide\nrange of animal movement behavior.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jun 2016 02:10:08 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Hanks", "Ephraim M.", ""], ["Hughes", "David A.", ""]]}, {"id": "1606.08098", "submitter": "Armeen Taeb", "authors": "Armeen Taeb, John T. Reager, Michael Turmon, Venkat Chandrasekaran", "title": "California Reservoir Drought Sensitivity and Exhaustion Risk Using\n  Statistical Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ongoing California drought has highlighted the potential vulnerability of\nstate water management infrastructure to multi-year dry intervals. Due to the\nhigh complexity of the network, dynamic storage changes across the California\nreservoir system have been difficult to model using either conventional\nstatistical or physical approaches. Here, we analyze the interactions of\nmonthly volumes in a network of 55 large California reservoirs, over a period\nof 136 months from 2004 to 2015, and we develop a latent-variable graphical\nmodel of their joint fluctuations. We achieve reliable and tractable modeling\nof the system because the model structure allows unique recovery of the\nbest-in-class model via convex optimization with control of the number of free\nparameters. We extract a statewide `latent' influencing factor which turns out\nto be highly correlated with both the Palmer Drought Severity Index (PDSI,\n$\\rho \\approx 0.86$) and hydroelectric production ($\\rho \\approx 0.71$).\nFurther, the model allows us to determine system health measures such as\nexhaustion probability and per-reservoir drought sensitivity. We find that as\nPDSI approaches -6, there is a probability greater than 50\\% of simultaneous\nexhaustion of multiple large reservoirs.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jun 2016 23:33:06 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Taeb", "Armeen", ""], ["Reager", "John T.", ""], ["Turmon", "Michael", ""], ["Chandrasekaran", "Venkat", ""]]}, {"id": "1606.08136", "submitter": "Dimitrios Berberidis", "authors": "Dimitris Berberidis and Georgios B. Giannakis", "title": "Data Sketching for Large-Scale Kalman Filtering", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2017.2691662", "report-no": null, "categories": "stat.AP cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an age of exponentially increasing data generation, performing inference\ntasks by utilizing the available information in its entirety is not always an\naffordable option. The present paper puts forth approaches to render tracking\nof large-scale dynamic processes via a Kalman filter affordable, by processing\na reduced number of data. Three distinct methods are introduced for reducing\nthe number of data involved in the correction step of the filter. Towards this\ngoal, the first two methods employ random projections and innovation-based\ncensoring to effect dimensionality reduction and measurement selection\nrespectively. The third method achieves reduced complexity by leveraging\nsequential processing of observations and selecting a few informative updates\nbased on an information-theoretic metric. Simulations on synthetic data,\ncompare the proposed methods with competing alternatives, and corroborate their\nefficacy in terms of estimation accuracy over complexity reduction. Finally,\nmonitoring large networks is considered as an application domain, with the\nproposed methods tested on Kronecker graphs to evaluate their efficiency in\ntracking traffic matrices and time-varying link costs.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 06:30:31 GMT"}, {"version": "v2", "created": "Fri, 6 Jan 2017 21:54:50 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Berberidis", "Dimitris", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1606.08199", "submitter": "Guillaume Flandin", "authors": "Guillaume Flandin and Karl J. Friston", "title": "Analysis of family-wise error rates in statistical parametric mapping\n  using random field theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical report revisits the analysis of family-wise error rates in\nstatistical parametric mapping - using random field theory - reported in\n(Eklund et al., 2015). Contrary to the understandable spin that these sorts of\nanalyses attract, a review of their results suggests that they endorse the use\nof parametric assumptions - and random field theory - in the analysis of\nfunctional neuroimaging data. We briefly rehearse the advantages parametric\nanalyses offer over nonparametric alternatives and then unpack the implications\nof (Eklund et al., 2015) for parametric procedures.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 10:28:12 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Flandin", "Guillaume", ""], ["Friston", "Karl J.", ""]]}, {"id": "1606.08291", "submitter": "Mike West", "authors": "Lutz F. Gruber and Mike West", "title": "Bayesian forecasting and scalable multivariate volatility analysis using\n  simultaneous graphical dynamic models", "comments": "28 pages, 9 figures, 7 tables", "journal-ref": "Econometrics and Statistics, 2017, Volume 3, pages 3-22", "doi": "10.1016/j.ecosta.2017.03.003", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently introduced class of simultaneous graphical dynamic linear models\n(SGDLMs) defines an ability to scale on-line Bayesian analysis and forecasting\nto higher-dimensional time series. This paper advances the methodology of\nSGDLMs, developing and embedding a novel, adaptive method of simultaneous\npredictor selection in forward filtering for on-line learning and forecasting.\nThe advances include developments in Bayesian computation for scalability, and\na case study in exploring the resulting potential for improved short-term\nforecasting of large-scale volatility matrices. A case study concerns financial\nforecasting and portfolio optimization with a 400-dimensional series of daily\nstock prices. Analysis shows that the SGDLM forecasts volatilities and\nco-volatilities well, making it ideally suited to contributing to quantitative\ninvestment strategies to improve portfolio returns. We also identify\nperformance metrics linked to the sequential Bayesian filtering analysis that\nturn out to define a leading indicator of increased financial market stresses,\ncomparable to but leading the standard St. Louis Fed Financial Stress Index\n(STLFSI) measure. Parallel computation using GPU implementations substantially\nadvance the ability to fit and use these models.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 14:40:23 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Gruber", "Lutz F.", ""], ["West", "Mike", ""]]}, {"id": "1606.08292", "submitter": "Mike West", "authors": "Jouchi Nakajima and Mike West", "title": "Dynamics and sparsity in latent threshold factor models: A study in\n  multivariate EEG signal processing", "comments": "27 pages, 13 figures, link to external web site for supplementary\n  animated figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss Bayesian analysis of multivariate time series with dynamic factor\nmodels that exploit time-adaptive sparsity in model parametrizations via the\nlatent threshold approach. One central focus is on the transfer responses of\nmultiple interrelated series to underlying, dynamic latent factor processes.\nStructured priors on model hyper-parameters are key to the efficacy of dynamic\nlatent thresholding, and MCMC-based computation enables model fitting and\nanalysis. A detailed case study of electroencephalographic (EEG) data from\nexperimental psychiatry highlights the use of latent threshold extensions of\ntime-varying vector autoregressive and factor models. This study explores a\nclass of dynamic transfer response factor models, extending prior Bayesian\nmodeling of multiple EEG series and highlighting the practical utility of the\nlatent thresholding concept in multivariate, non-stationary time series\nanalysis.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 14:40:51 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Nakajima", "Jouchi", ""], ["West", "Mike", ""]]}, {"id": "1606.08350", "submitter": "Ba Tuong Vo Prof", "authors": "Ba Ngu Vo, Ba Tuong Vo, Hung Gia Hoang", "title": "An Efficient Implementation of the Generalized Labeled Multi-Bernoulli\n  Filter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an efficient implementation of the generalized labeled\nmulti-Bernoulli (GLMB) filter by combining the prediction and update into a\nsingle step. In contrast to an earlier implementation that involves separate\ntruncations in the prediction and update steps, the proposed implementation\nrequires only one truncation procedure for each iteration. Furthermore, we\npropose an efficient algorithm for truncating the GLMB filtering density based\non Gibbs sampling. The resulting implementation has a linear complexity in the\nnumber of measurements and quadratic in the number of hypothesized objects.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 16:24:03 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2017 09:54:15 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Vo", "Ba Ngu", ""], ["Vo", "Ba Tuong", ""], ["Hoang", "Hung Gia", ""]]}, {"id": "1606.08644", "submitter": "Lionel Barnett", "authors": "Lionel Barnett and Anil K. Seth (Sackler Centre for Consciousness\n  Science, School of Engineering and Informatics, University of Sussex, UK)", "title": "Detectability of Granger causality for subsampled continuous-time\n  neurophysiological processes", "comments": "In review, Journal of Neuroscience Methods", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Granger causality is well established within the neurosciences for inference\nof directed functional connectivity from neurophysiological data. These data\nusually consist of time series which subsample a continuous-time\nbiophysiological process. While it is well-known that subsampling can lead to\nimputation of spurious causal connections where none exist, here we address the\nequally important issue of the effects of subsampling on the ability to\nreliably detect causal connections which do exist.\n  Neurophysiological processes typically feature signal propagation delays on\nmultiple time scales; accordingly, we base our analysis on a distributed-lag,\ncontinuous-time stochastic model, and consider Granger causality in continuous\ntime at finite prediction horizons. Via exact analytical solutions, we identify\nrelationships among sampling frequency, underlying causal time scales and\ndetectability of causalities. Our analysis reveals complex interactions between\nthe time scale(s) of neural signal propagation and sampling frequency: we\ndemonstrate that Granger causality decays exponentially as the sample time\ninterval increases beyond causal delay times, identify detectability \"black\nspots\" and \"sweet spots\", and show that subsampling may sometimes improve\ndetectability. We also demonstrate that the invariance of Granger causality\nunder causal, invertible filtering fails at finite prediction horizons. We\ndiscuss the implications of our results for inference of Granger causality at\nthe neural level from various neurophysiological recording modes, and emphasise\nthat sampling rates for causal analysis of neurophysiological time series\nshould be informed by domain-specific time scales.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 10:40:35 GMT"}, {"version": "v2", "created": "Wed, 7 Sep 2016 14:53:47 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Barnett", "Lionel", "", "Sackler Centre for Consciousness\n  Science, School of Engineering and Informatics, University of Sussex, UK"], ["Seth", "Anil K.", "", "Sackler Centre for Consciousness\n  Science, School of Engineering and Informatics, University of Sussex, UK"]]}, {"id": "1606.08698", "submitter": "Guillem Collell", "authors": "Guillem Collell, Drazen Prelec, Kaustubh Patil", "title": "Reviving Threshold-Moving: a Simple Plug-in Bagging Ensemble for Binary\n  and Multiclass Imbalanced Data", "comments": "Typo in the proof fixed. TP/(P+N)=P(y=1) replaced by P/(P+N)=P(y=1)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Class imbalance presents a major hurdle in the application of data mining\nmethods. A common practice to deal with it is to create ensembles of\nclassifiers that learn from resampled balanced data. For example, bagged\ndecision trees combined with random undersampling (RUS) or the synthetic\nminority oversampling technique (SMOTE). However, most of the resampling\nmethods entail asymmetric changes to the examples of different classes, which\nin turn can introduce its own biases in the model. Furthermore, those methods\nrequire a performance measure to be specified a priori before learning. An\nalternative is to use a so-called threshold-moving method that a posteriori\nchanges the decision threshold of a model to counteract the imbalance, thus has\na potential to adapt to the performance measure of interest. Surprisingly,\nlittle attention has been paid to the potential of combining bagging ensemble\nwith threshold-moving. In this paper, we present probability thresholding\nbagging (PT-bagging), a versatile plug-in method that fills this gap. Contrary\nto usual rebalancing practice, our method preserves the natural class\ndistribution of the data resulting in well calibrated posterior probabilities.\nWe also extend the proposed method to handle multiclass data. The method is\nvalidated on binary and multiclass benchmark data sets. We perform analyses\nthat provide insights into the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 13:49:30 GMT"}, {"version": "v2", "created": "Sun, 3 Jul 2016 18:53:14 GMT"}, {"version": "v3", "created": "Tue, 20 Jun 2017 08:09:37 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Collell", "Guillem", ""], ["Prelec", "Drazen", ""], ["Patil", "Kaustubh", ""]]}, {"id": "1606.08908", "submitter": "Mark Risser", "authors": "Mark D. Risser and Daithi A. Stone and Christopher J. Paciorek and\n  Michael F. Wehner and Oliver Angelil", "title": "Quantifying the effect of interannual ocean variability on the\n  attribution of extreme climate events to human influence", "comments": null, "journal-ref": null, "doi": "10.1007/s00382-016-3492-x", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the climate change research community has become highly\ninterested in describing the anthropogenic influence on extreme weather events,\ncommonly termed \"event attribution.\" Limitations in the observational record\nand in computational resources motivate the use of uncoupled,\natmosphere/land-only climate models with prescribed ocean conditions run over a\nshort period, leading up to and including an event of interest. In this\napproach, large ensembles of high-resolution simulations can be generated under\nfactual observed conditions and counterfactual conditions that might have been\nobserved in the absence of human interference; these can be used to estimate\nthe change in probability of the given event due to anthropogenic influence.\nHowever, using a prescribed ocean state ignores the possibility that estimates\nof attributable risk might be a function of the ocean state. Thus, the\nuncertainty in attributable risk is likely underestimated, implying an\nover-confidence in anthropogenic influence.\n  In this work, we estimate the year-to-year variability in calculations of the\nanthropogenic contribution to extreme weather based on large ensembles of\natmospheric model simulations. Our results both quantify the magnitude of\nyear-to-year variability and categorize the degree to which conclusions of\nattributable risk are qualitatively affected. The methodology is illustrated by\nexploring extreme temperature and precipitation events for the northwest coast\nof South America and northern-central Siberia; we also provides results for\nregions around the globe. While it remains preferable to perform a full\nmulti-year analysis, the results presented here can serve as an indication of\nwhere and when attribution researchers should be concerned about the use of\natmosphere-only simulations.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 22:39:14 GMT"}, {"version": "v2", "created": "Wed, 28 Sep 2016 22:00:18 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Risser", "Mark D.", ""], ["Stone", "Daithi A.", ""], ["Paciorek", "Christopher J.", ""], ["Wehner", "Michael F.", ""], ["Angelil", "Oliver", ""]]}, {"id": "1606.09004", "submitter": "Sarah Friedrich", "authors": "Arne Bathke, Sarah Friedrich, Frank Konietschke, Markus Pauly,\n  Wolfgang Staffen, Nicolas Strobl, and Yvonne H\\\"oller", "title": "Using EEG, SPECT, and Multivariate Resampling Methods to Differentiate\n  Between Alzheimer's and other Cognitive Impairments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The incidence of Alzheimer's disease (AD) and other forms of dementia is\nincreasing in most western countries. For a precise and early diagnosis,\nseveral examination modalities exist, among them single-photon emission\ncomputed tomography (SPECT) and the electroencephalogram (EEG). The latter is\nhighly available, free of radiation hazards, and non-invasive. Thus, its\ndiagnostic utility regarding different stages of dementia is of great interest\nin neurological research, along with the question of whether its utility\ndepends on age or sex of the person being examined. However, SPECT or EEG\nmeasurements are intrinsically multivariate, and there has been a shortage of\nsufficiently general inferential techniques for the analysis of multivariate\ndata in factorial designs when neither multivariate normality nor equality of\ncovariance matrices across groups should be assumed. We adapt an asymptotic\nmodel based (parametric) bootstrap approach to this situation, demonstrate its\nability, and use it for a truly multivariate analysis of the EEG and SPECT\nmeasurements, taking into account demographic factors such as age and sex.\nThese multivariate results are supplemented by marginal effects bootstrap\ninference whose theoretical properties can be derived analogously to the\nmultivariate methods. Both inference approaches can have advantages in\nparticular situations, as illustrated in the data analysis.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 08:52:23 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Bathke", "Arne", ""], ["Friedrich", "Sarah", ""], ["Konietschke", "Frank", ""], ["Pauly", "Markus", ""], ["Staffen", "Wolfgang", ""], ["Strobl", "Nicolas", ""], ["H\u00f6ller", "Yvonne", ""]]}, {"id": "1606.09184", "submitter": "Peter Schulam", "authors": "Peter Schulam and Raman Arora", "title": "Disease Trajectory Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical researchers are coming to appreciate that many diseases are in fact\ncomplex, heterogeneous syndromes composed of subpopulations that express\ndifferent variants of a related complication. Time series data extracted from\nindividual electronic health records (EHR) offer an exciting new way to study\nsubtle differences in the way these diseases progress over time. In this paper,\nwe focus on answering two questions that can be asked using these databases of\ntime series. First, we want to understand whether there are individuals with\nsimilar disease trajectories and whether there are a small number of degrees of\nfreedom that account for differences in trajectories across the population.\nSecond, we want to understand how important clinical outcomes are associated\nwith disease trajectories. To answer these questions, we propose the Disease\nTrajectory Map (DTM), a novel probabilistic model that learns low-dimensional\nrepresentations of sparse and irregularly sampled time series. We propose a\nstochastic variational inference algorithm for learning the DTM that allows the\nmodel to scale to large modern medical datasets. To demonstrate the DTM, we\nanalyze data collected on patients with the complex autoimmune disease,\nscleroderma. We find that DTM learns meaningful representations of disease\ntrajectories and that the representations are significantly associated with\nimportant clinical outcomes.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 17:06:45 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Schulam", "Peter", ""], ["Arora", "Raman", ""]]}]