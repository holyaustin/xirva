[{"id": "2101.00059", "submitter": "Hong Zhang", "authors": "Hong Zhang, Qing Li, Devan V. Mehrotra and Judong Shen", "title": "CauchyCP: a powerful test under non-proportional hazards using Cauchy\n  combination of change-point Cox regressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.GN stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Non-proportional hazards data are routinely encountered in randomized\nclinical trials. In such cases, classic Cox proportional hazards model can\nsuffer from severe power loss, with difficulty in interpretation of the\nestimated hazard ratio since the treatment effect varies over time. We propose\nCauchyCP, an omnibus test of change-point Cox regression models, to overcome\nboth challenges while detecting signals of non-proportional hazards patterns.\nExtensive simulation studies demonstrate that, compared to existing treatment\ncomparison tests under non-proportional hazards, the proposed CauchyCP test 1)\ncontrols the type I error better at small $\\alpha$ levels ($< 0.01$); 2)\nincreases the power of detecting time-varying effects; and 3) is more\ncomputationally efficient. The superior performance of CauchyCP is further\nillustrated using retrospective analyses of two randomized clinical trial\ndatasets and a pharmacogenetic biomarker study dataset. The R package\n$\\textit{CauchyCP}$ is publicly available on CRAN.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 20:26:06 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Zhang", "Hong", ""], ["Li", "Qing", ""], ["Mehrotra", "Devan V.", ""], ["Shen", "Judong", ""]]}, {"id": "2101.00356", "submitter": "Roel Ceballos", "authors": "Merlito Villa, Roel F. Ceballos", "title": "Analysis and Forecasting of Fire incidence in Davao City", "comments": null, "journal-ref": "Recoletos Multidisciplinary Research Journal, Vol 8 No 2 (2020)", "doi": "10.32871/rmrj2008.02", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fire incidence is a big problem for every local government unit in the\nPhilippines. The two most detrimental effects of fire incidence are economic\nloss and loss of life. To mitigate these losses, proper planning and\nimplementation of control measures must be done. An essential aspect of\nplanning and control measures is prediction of possible fire incidences. This\nstudy is conducted to analyze the historical data to create a forecasting model\nfor the fire incidence in Davao City. Results of the analyses show that fire\nincidence has no trend or seasonality, and occurrences of fire are neither\nconsistently increasing nor decreasing over time. Furthermore, the absence of\nseasonality in the data indicate that surge of fire incidence may occur at any\ntime of the year. Therefore, fire prevention activities should be done all year\nround and not just during fire prevention month.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 02:51:22 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 00:03:19 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Villa", "Merlito", ""], ["Ceballos", "Roel F.", ""]]}, {"id": "2101.00357", "submitter": "Asim Dey", "authors": "Asim K. Dey and Kumer P. Das", "title": "How do mobility restrictions and social distancing during COVID-19\n  affect the crude oil price?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop an air mobility index and use the newly developed Apple's driving\ntrend index to evaluate the impact of COVID-19 on the crude oil price. We use\nquantile regression and stationary and non-stationary extreme value models to\nstudy the impact. We find that both the \\textit{air mobility index} and\n\\textit{driving trend index} significantly influence lower and upper quantiles\nas well as the median of the WTI crude oil price. The extreme value model\nsuggests that an event like COVID-19 may push oil prices to a negative\nterritory again as the air mobility decreases drastically during such\npandemics.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 02:53:48 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Dey", "Asim K.", ""], ["Das", "Kumer P.", ""]]}, {"id": "2101.00405", "submitter": "Michele Garetto", "authors": "Michele Garetto and Emilio Leonardi and Giovanni Luca Torrisi", "title": "A time-modulated Hawkes process to model the spread of COVID-19 and the\n  impact of countermeasures", "comments": "13 colored figures", "journal-ref": "Annual Reviews in Control, 2021", "doi": "10.1016/j.arcontrol.2021.02.002", "report-no": null, "categories": "q-bio.PE math.PR physics.soc-ph stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Motivated by the recent outbreak of coronavirus (COVID-19), we propose a\nstochastic model of epidemic temporal growth and mitigation based on a\ntime-modulated Hawkes process. The model is sufficiently rich to incorporate\nspecific characteristics of the novel coronavirus, to capture the impact of\nundetected, asymptomatic and super-diffusive individuals, and especially to\ntake into account time-varying counter-measures and detection efforts. Yet, it\nis simple enough to allow scalable and efficient computation of the temporal\nevolution of the epidemic, and exploration of what-if scenarios. Compared to\ntraditional compartmental models, our approach allows a more faithful\ndescription of virus specific features, such as distributions for the time\nspent in stages, which is crucial when the time-scale of control (e.g.,\nmobility restrictions) is comparable to the lifetime of a single infection. We\napply the model to the first and second wave of COVID-19 in Italy, shedding\nlight into several effects related to mobility restrictions introduced by the\ngovernment, and to the effectiveness of contact tracing and mass testing\nperformed by the national health service.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 08:53:32 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Garetto", "Michele", ""], ["Leonardi", "Emilio", ""], ["Torrisi", "Giovanni Luca", ""]]}, {"id": "2101.00427", "submitter": "Ludwig Hothorn", "authors": "Ludwig A. Hothorn", "title": "A statistical method for estimating the no-observed-adverse-event-level", "comments": "4 data examples, with raw data and R-code", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In toxicological risk assessment the benchmark dose (BMD) is recommended\ninstead of the no-observed-adverse effect-level (NOAEL). Still a simple test\nprocedure to estimate NOAEL is proposed here, explaining its advantages and\ndisadvantages. Versatile applicability is illustrated using four different data\nexamples of selected in vivo toxicity bioassays.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 11:25:27 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Hothorn", "Ludwig A.", ""]]}, {"id": "2101.00457", "submitter": "Eiji Konaka", "authors": "Eiji Konaka", "title": "Home advantage of European major football leagues under COVID-19\n  pandemic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since March 2020, the environment surrounding football has changed\ndramatically because of the COVID-19 pandemic. After a few months' break,\nre-scheduled matches were held behind closed doors without spectators. The main\nobjective of this study is a quantitative evaluation of ``crowd effects'' on\nhome advantage, using the results of these closed matches. The proposed\nanalysis uses pairwise comparison method to reduce the effects caused by the\nunbalanced schedule. The following conclusions were drawn from the statistical\nhypothesis tests conducted in this study: In four major European leagues, the\nhome advantage is reduced in closed matches compared to than in the normal\nsituation, i.e., with spectators. The reduction amounts among leagues were\ndifferent. For example, in Germany, the home advantage was negative during the\nclosed-match period. On the other hand,in England,statistically significant\ndifferences in home advantage were not observed between closed matches and\nnormal situation.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 14:37:14 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 17:31:17 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Konaka", "Eiji", ""]]}, {"id": "2101.00466", "submitter": "Lijun Sun Mr", "authors": "Zhanhong Cheng, Martin Trepanier, Lijun Sun", "title": "Real-time forecasting of metro origin-destination matrices with\n  high-order weighted dynamic mode decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasting the short-term ridership among origin-destination pairs (OD\nmatrix) of a metro system is crucial in real-time metro operation. However,\nthis problem is notoriously difficult due to the high-dimensional, sparse,\nnoisy, and skewed nature of OD matrices. This paper proposes a High-order\nWeighted Dynamic Mode Decomposition (HW-DMD) model for short-term metro OD\nmatrices forecasting. DMD uses Singular Value Decomposition (SVD) to extract\nlow-rank approximation from OD data, and a high-order vector autoregression\nmodel is estimated on the reduced space for forecasting. To address a practical\nissue that metro OD matrices cannot be observed in real-time, we use the\nboarding demand to replace the unavailable OD matrices. Particularly, we\nconsider the time-evolving feature of metro systems and improve the forecast by\nexponentially reducing the weights for old data. Moreover, we develop a\ntailored online update algorithm for HW-DMD to update the model coefficients\ndaily without storing historical data or retraining. Experiments on data from a\nlarge-scale metro system show the proposed HW-DMD is robust to the noisy and\nsparse data and significantly outperforms baseline models in forecasting both\nOD matrices and boarding flow. The online update algorithm also shows\nconsistent accuracy over a long time when maintaining an HW-DMD model at low\ncosts.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 15:24:39 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Cheng", "Zhanhong", ""], ["Trepanier", "Martin", ""], ["Sun", "Lijun", ""]]}, {"id": "2101.00484", "submitter": "Fan Li", "authors": "Fan Li, Hengshi Yu, Paul J. Rathouz, Elizabeth L. Turner, John S.\n  Preisser", "title": "Marginal modeling of cluster-period means and intraclass correlations in\n  stepped wedge designs with binary outcomes", "comments": "28 pages, 2 figures, 3 tables", "journal-ref": "Biostatistics (2021)", "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stepped wedge cluster randomized trials (SW-CRTs) with binary outcomes are\nincreasingly used in prevention and implementation studies. Marginal models\nrepresent a flexible tool for analyzing SW-CRTs with population-averaged\ninterpretations, but the joint estimation of the mean and intraclass\ncorrelation coefficients (ICCs) can be computationally intensive due to large\ncluster-period sizes. Motivated by the need for marginal inference in SW-CRTs,\nwe propose a simple and efficient estimating equations approach to analyze\ncluster-period means. We show that the quasi-score for the marginal mean\ndefined from individual-level observations can be reformulated as the\nquasi-score for the same marginal mean defined from the cluster-period means.\nAn additional mapping of the individual-level ICCs into correlations for the\ncluster-period means further provides a rigorous justification for the\ncluster-period approach. The proposed approach addresses a long-recognized\ncomputational burden associated with estimating equations defined based on\nindividual-level observations, and enables fast point and interval estimation\nof the intervention effect and correlations. We further propose matrix-adjusted\nestimating equations to improve the finite-sample inference for ICCs. By\nproviding a valid approach to estimate ICCs within the class of generalized\nlinear models for correlated binary outcomes, this article operationalizes key\nrecommendations from the CONSORT extension to SW-CRTs, including the reporting\nof ICCs.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 17:40:01 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Li", "Fan", ""], ["Yu", "Hengshi", ""], ["Rathouz", "Paul J.", ""], ["Turner", "Elizabeth L.", ""], ["Preisser", "John S.", ""]]}, {"id": "2101.00592", "submitter": "Weijian Luo", "authors": "Weijian Luo and Mai Wo", "title": "Binary Outcome Copula Regression Model with Sampling Gradient Fitting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Use copula to model dependency of variable extends multivariate gaussian\nassumption. In this paper we first empirically studied copula regression model\nwith continous response. Both simulation study and real data study are given.\nSecondly we give a novel copula regression model with binary outcome, and we\npropose a score gradient estimation algorithms to fit the model. Both\nsimulation study and real data study are given for our model and fitting\nalgorithm.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 09:35:44 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Luo", "Weijian", ""], ["Wo", "Mai", ""]]}, {"id": "2101.00598", "submitter": "Sanket Kamthe", "authors": "Sanket Kamthe, Samuel Assefa, Marc Deisenroth", "title": "Copula Flows for Synthetic Data Generation", "comments": "Working paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The ability to generate high-fidelity synthetic data is crucial when\navailable (real) data is limited or where privacy and data protection standards\nallow only for limited use of the given data, e.g., in medical and financial\ndata-sets. Current state-of-the-art methods for synthetic data generation are\nbased on generative models, such as Generative Adversarial Networks (GANs).\nEven though GANs have achieved remarkable results in synthetic data generation,\nthey are often challenging to interpret.Furthermore, GAN-based methods can\nsuffer when used with mixed real and categorical variables.Moreover, loss\nfunction (discriminator loss) design itself is problem specific, i.e., the\ngenerative model may not be useful for tasks it was not explicitly trained for.\nIn this paper, we propose to use a probabilistic model as a synthetic data\ngenerator. Learning the probabilistic model for the data is equivalent to\nestimating the density of the data. Based on the copula theory, we divide the\ndensity estimation task into two parts, i.e., estimating univariate marginals\nand estimating the multivariate copula density over the univariate marginals.\nWe use normalising flows to learn both the copula density and univariate\nmarginals. We benchmark our method on both simulated and real data-sets in\nterms of density estimation as well as the ability to generate high-fidelity\nsynthetic data\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 10:06:23 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Kamthe", "Sanket", ""], ["Assefa", "Samuel", ""], ["Deisenroth", "Marc", ""]]}, {"id": "2101.00661", "submitter": "David R\\\"ugamer", "authors": "Cornelius Fritz, Emilio Dorigatti, David R\\\"ugamer", "title": "Combining Graph Neural Networks and Spatio-temporal Disease Models to\n  Predict COVID-19 Cases in Germany", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During 2020, the infection rate of COVID-19 has been investigated by many\nscholars from different research fields. In this context, reliable and\ninterpretable forecasts of disease incidents are a vital tool for policymakers\nto manage healthcare resources. Several experts have called for the necessity\nto account for human mobility to explain the spread of COVID-19. Existing\napproaches are often applying standard models of the respective research field.\nThis habit, however, often comes along with certain restrictions. For instance,\nmost statistical or epidemiological models cannot directly incorporate\nunstructured data sources, including relational data that may encode human\nmobility. In contrast, machine learning approaches may yield better predictions\nby exploiting these data structures, yet lack intuitive interpretability as\nthey are often categorized as black-box models. We propose a trade-off between\nboth research directions and present a multimodal learning approach that\ncombines the advantages of statistical regression and machine learning models\nfor predicting local COVID-19 cases in Germany. This novel approach enables the\nuse of a richer collection of data types, including mobility flows and\ncolocation probabilities, and yields the lowest MSE scores throughout our\nobservational period in our benchmark study. The results corroborate the\nnecessity of including mobility data and showcase the flexibility and\ninterpretability of our approach.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 16:39:00 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Fritz", "Cornelius", ""], ["Dorigatti", "Emilio", ""], ["R\u00fcgamer", "David", ""]]}, {"id": "2101.00827", "submitter": "Yanfei Kang", "authors": "Xixi Li, Fotios Petropoulos, Yanfei Kang", "title": "Improving forecasting by subsampling seasonal time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series forecasting plays an increasingly important role in modern\nbusiness decisions. In today's data-rich environment, people often aim to\nchoose the optimal forecasting model for their data. However, identifying the\noptimal model requires professional knowledge and experience, making accurate\nforecasting a challenging task. To mitigate the importance of model selection,\nwe propose a simple and reliable algorithm to improve the forecasting\nperformance. Specifically, we construct multiple time series with different\nsub-seasons from the original time series. These derived series highlight\ndifferent sub-seasonal patterns of the original series, making it possible for\nthe forecasting methods to capture diverse patterns and components of the data.\nSubsequently, we make forecasts for these multiple series separately with\nclassical statistical models (ETS or ARIMA). Finally, the forecasts of these\nmultiple series are combined with equal weights. We evaluate our approach on\nwidely-used forecasting competition data sets (M1, M3, and M4) in terms of both\npoint forecasts and prediction intervals. We observe performance improvements\ncompared with the benchmarks. Our approach is particularly suitable and robust\nfor the data with higher frequency. To demonstrate the practical value of our\nproposition, we showcase the performance improvements from our approach on\nhourly load data that exhibit multiple seasonality.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 08:30:41 GMT"}, {"version": "v2", "created": "Sat, 10 Jul 2021 10:01:02 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Li", "Xixi", ""], ["Petropoulos", "Fotios", ""], ["Kang", "Yanfei", ""]]}, {"id": "2101.00913", "submitter": "Taha Yasseri", "authors": "Menno Schellekens and Taha Yasseri", "title": "Credit Crunch: The Role of Household Lending Capacity in the Dutch\n  Housing Boom and Bust 1995-2018", "comments": "under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.GN", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  What causes house prices to rise and fall? Economists identify household\naccess to credit as a crucial factor. \"Loan-to-Value\" and \"Debt-to-GDP\" ratios\nare the standard measures for credit access. However, these measures fail to\nexplain the depth of the Dutch housing bust after the 2009 Financial Crisis.\nThis work is the first to model household lending capacity based on the\nformulas that Dutch banks use in the mortgage application process. We compare\nthe ability of regression models to forecast housing prices when different\nmeasures of credit access are utilised. We show that our measure of household\nlending capacity is a forward-looking, highly predictive variable that\noutperforms `Loan-to-Value' and debt ratios in forecasting the Dutch crisis.\nSharp declines in lending capacity foreshadow the market deceleration.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 12:10:07 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Schellekens", "Menno", ""], ["Yasseri", "Taha", ""]]}, {"id": "2101.00967", "submitter": "Lynn Wahab", "authors": "Lynn Wahab, Ezzat Chebaro, Jad Ismail, Amir Nasrelddine, Ali El-Zein", "title": "A Predictive Model for Geographic Distributions of Mangroves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Climate change is an impending disaster which is of pressing concern more and\nmore every year. Countless efforts have been made to study the long-term\neffects of climate change on agriculture, land resources, and biodiversity.\nStudies involving marine life, however, are less prevalent in the literature.\nOur research studies the available data on the population of mangroves (groups\nof shrubs or small trees living in saline coastal intertidal zones) and their\ncorrelations to climate change variables, specifically, temperature, heat\ncontent, various sea levels, and sea salinity. Mangroves are especially\nrelevant to oceanic ecosystems because of their protective nature towards other\nmarine life, as well as their high absorption rate of carbon dioxide, and their\nability to withstand varying levels of salinity of our coasts. The change in\nglobal distribution was studied based on global distributions of the previous\nyear, as well as ocean heat content, salinity, temperature, halosteric sea\nlevel, thermosteric sea level, and total steric sea level. The best performing\npredictive model was a support vector regressor, which yielded a correlation\ncoefficient of 0.9998.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 22:52:50 GMT"}, {"version": "v2", "created": "Sun, 10 Jan 2021 18:42:49 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Wahab", "Lynn", ""], ["Chebaro", "Ezzat", ""], ["Ismail", "Jad", ""], ["Nasrelddine", "Amir", ""], ["El-Zein", "Ali", ""]]}, {"id": "2101.01093", "submitter": "Yusuke Narita", "authors": "Atila Abdulkadiroglu, Joshua D. Angrist, Yusuke Narita, and Parag\n  Pathak", "title": "Breaking Ties: Regression Discontinuity Design Meets Market Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many schools in large urban districts have more applicants than seats.\nCentralized school assignment algorithms ration seats at over-subscribed\nschools using randomly assigned lottery numbers, non-lottery tie-breakers like\ntest scores, or both. The New York City public high school match illustrates\nthe latter, using test scores and other criteria to rank applicants at\n``screened'' schools, combined with lottery tie-breaking at unscreened\n``lottery'' schools. We show how to identify causal effects of school\nattendance in such settings. Our approach generalizes regression discontinuity\nmethods to allow for multiple treatments and multiple running variables, some\nof which are randomly assigned. The key to this generalization is a local\npropensity score that quantifies the school assignment probabilities induced by\nlottery and non-lottery tie-breakers. The local propensity score is applied in\nan empirical assessment of the predictive value of New York City's school\nreport cards. Schools that receive a high grade indeed improve SAT math scores\nand increase graduation rates, though by much less than OLS estimates suggest.\nSelection bias in OLS estimates is egregious for screened schools.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 09:00:13 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Abdulkadiroglu", "Atila", ""], ["Angrist", "Joshua D.", ""], ["Narita", "Yusuke", ""], ["Pathak", "Parag", ""]]}, {"id": "2101.01270", "submitter": "Taha Yasseri", "authors": "Annika Kreil, Andranik Tumasjan, Taha Yasseri, Isabell Welpe", "title": "What drives passion? An empirical examination on the impact of\n  personality trait interactions and job environments on work passion", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Passionate employees are essential for organisational success as they foster\nhigher performance and exhibit lower turnover or absenteeism. While a large\nbody of research has investigated the consequences of passion, we know only\nlittle about its antecedents. Integrating trait interaction theory with trait\nactivation theory, this paper examines how personality traits, i.e.\nconscien-tiousness, agreeableness, and neuroticism impact passion at work\nacross different job situa-tions. Passion has been conceptualized as a\ntwo-dimensional construct, consisting of harmoni-ous work passion (HWP) and\nobsessive work passion (OWP). Our study is based on a sample of N = 824\nparticipants from the myPersonality project. We find a positive relationship\nbetween neuroticism and OWP in enterprising environments. Further, we find a\nthree-way interaction between conscientiousness, agreeableness, and\nenterprising environment in predicting OWP. Our findings imply that the impact\nof personality configurations on different forms of passion is contingent on\nthe job environment. Moreover, in line with self-regulation theory, the results\nreveal agreeableness as a \"cool influencer\" and neuroticism as a \"hot\ninfluencer\" of the relation-ship between conscientiousness and work passion. We\nderive practical implications for organi-sations on how to foster work passion,\nparticularly HWP, in organisations.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 22:56:57 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2021 18:59:37 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Kreil", "Annika", ""], ["Tumasjan", "Andranik", ""], ["Yasseri", "Taha", ""], ["Welpe", "Isabell", ""]]}, {"id": "2101.01295", "submitter": "Jonathan Fintzi", "authors": "Jonathan Fintzi and Dean Follmann", "title": "Assessing Vaccine Durability in Randomized Trials Following Placebo\n  Crossover", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized vaccine trials are used to assess vaccine efficacy and to\ncharacterize the durability of vaccine induced protection. If efficacy is\ndemonstrated, the treatment of placebo volunteers becomes an issue. For\nCOVID-19 vaccine trials, there is broad consensus that placebo volunteers\nshould be offered a vaccine once efficacy has been established. This will\nlikely lead to most placebo volunteers crossing over to the vaccine arm, thus\ncomplicating the assessment of long term durability. We show how to analyze\ndurability following placebo crossover and demonstrate that the vaccine\nefficacy profile that would be observed in a placebo controlled trial is\nrecoverable in a trial with placebo crossover. This result holds no matter when\nthe crossover occurs and with no assumptions about the form of the efficacy\nprofile. We only require that the vaccine efficacy profile applies to the newly\nvaccinated irrespective of the timing of vaccination. We develop different\nmethods to estimate efficacy within the context of a proportional hazards\nregression model and explore via simulation the implications of placebo\ncrossover for estimation of vaccine efficacy under different efficacy dynamics\nand study designs. We apply our methods to simulated COVID-19 vaccine trials\nwith durable and waning vaccine efficacy and a total follow-up of two years.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 00:29:51 GMT"}, {"version": "v2", "created": "Sat, 20 Feb 2021 03:37:16 GMT"}, {"version": "v3", "created": "Tue, 9 Mar 2021 00:02:17 GMT"}, {"version": "v4", "created": "Mon, 5 Apr 2021 13:21:28 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Fintzi", "Jonathan", ""], ["Follmann", "Dean", ""]]}, {"id": "2101.01501", "submitter": "Krzysztof Bartoszek", "authors": "Krzysztof Bartoszek and Albin V\\\"asterlund", "title": "\"Old Techniques for New Times\": the RMaCzek package for producing\n  Czekanowski's Diagrams", "comments": null, "journal-ref": "Biometrical Letters 57(2):89-118 (2020)", "doi": "10.2478/bile-2020-0008", "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the MaCzek Visual Basic program we provide an R package, RMaCzek,\nthat produces Czekanowski's diagram. Our package permits any seriation and\ndistance method the user provides. In this paper we focus on the OLO and\nQAP_2SUM methods from the seriation package. We illustrate the possibilities of\nour package with three anthropological studies, one socio-economical one and a\nphylogenetically motivated simulation study.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 13:33:12 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Bartoszek", "Krzysztof", ""], ["V\u00e4sterlund", "Albin", ""]]}, {"id": "2101.01521", "submitter": "Aidan Hughes", "authors": "Aidan J. Hughes, Robert J. Barthorpe, N. Dervilis, Charles R. Farrar\n  and Keith Worden", "title": "A probabilistic risk-based decision framework for structural health\n  monitoring", "comments": null, "journal-ref": "Mechanical Systems and Signal Processing, Volume 150, 2021,\n  107339, ISSN 0888-3270", "doi": "10.1016/j.ymssp.2020.107339", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Obtaining the ability to make informed decisions regarding the operation and\nmaintenance of structures, provides a major incentive for the implementation of\nstructural health monitoring (SHM) systems. Probabilistic risk assessment (PRA)\nis an established methodology that allows engineers to make risk-informed\ndecisions regarding the design and operation of safety-critical and high-value\nassets in industries such as nuclear and aerospace. The current paper aims to\nformulate a risk-based decision framework for structural health monitoring that\ncombines elements of PRA with the existing SHM paradigm. As an apt tool for\nreasoning and decision-making under uncertainty, probabilistic graphical models\nserve as the foundation of the framework. The framework involves modelling\nfailure modes of structures as Bayesian network representations of fault trees\nand then assigning costs or utilities to the failure events. The fault trees\nallow for information to pass from probabilistic classifiers to influence\ndiagram representations of decision processes whilst also providing nodes\nwithin the graphical model that may be queried to obtain marginal probability\ndistributions over local damage states within a structure. Optimal courses of\naction for structures are selected by determining the strategies that maximise\nexpected utility. The risk-based framework is demonstrated on a realistic\ntruss-like structure and supported by experimental data. Finally, a discussion\nof the risk-based approach is made and further challenges pertaining to\ndecision-making processes in the context of SHM are identified.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 14:10:34 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Hughes", "Aidan J.", ""], ["Barthorpe", "Robert J.", ""], ["Dervilis", "N.", ""], ["Farrar", "Charles R.", ""], ["Worden", "Keith", ""]]}, {"id": "2101.01531", "submitter": "Huthaifa I. Ashqar", "authors": "Lee Whieldon and Huthaifa Ashqar", "title": "Predicting Residential Property Value in Catonsville, Maryland: A\n  Comparison of Multiple Regression Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.GN q-fin.EC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Predicting Residential Property Value in Catonsville, Maryland: A Comparison\nof Multiple Regression Techniques\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 20:27:35 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Whieldon", "Lee", ""], ["Ashqar", "Huthaifa", ""]]}, {"id": "2101.01532", "submitter": "Shuo Wang", "authors": "Xian Yang, Shuo Wang, Yuting Xing, Ling Li, Richard Yi Da Xu, Karl J.\n  Friston and Yike Guo", "title": "Revealing the Transmission Dynamics of COVID-19: A Bayesian Framework\n  for $R_t$ Estimation", "comments": "Xian Yang and Shuo Wang contribute equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.bio-ph physics.soc-ph q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In epidemiological modelling, the instantaneous reproduction number, $R_t$,\nis important to understand the transmission dynamics of infectious diseases.\nCurrent $R_t$ estimates often suffer from problems such as lagging, averaging\nand uncertainties demoting the usefulness of $R_t$. To address these problems,\nwe propose a new method in the framework of sequential Bayesian inference where\na Data Assimilation approach is taken for $R_t$ estimation, resulting in the\nstate-of-the-art 'DAR$_t$' system for $R_t$ estimation. With DAR$_t$, the\nproblem of time misalignment caused by lagging observations is tackled by\nincorporating observation delays into the joint inference of infections and\n$R_t$; the drawback of averaging is improved by instantaneous updating upon new\nobservations and a model selection mechanism capturing abrupt changes caused by\ninterventions; the uncertainty is quantified and reduced by employing Bayesian\nsmoothing. We validate the performance of DAR$_t$ through simulations and\ndemonstrate its power in revealing the transmission dynamics of COVID-19.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 07:38:35 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Yang", "Xian", ""], ["Wang", "Shuo", ""], ["Xing", "Yuting", ""], ["Li", "Ling", ""], ["Da Xu", "Richard Yi", ""], ["Friston", "Karl J.", ""], ["Guo", "Yike", ""]]}, {"id": "2101.01624", "submitter": "Marco Mingione", "authors": "Pierfrancesco Alaimo Di Loro and Marco Mingione and Jonah Lipsitt and\n  Christina M. Batteate and Michael Jerrett and Sudipto Banerjee", "title": "Bayesian hierarchical modeling and analysis for physical activity\n  trajectories using actigraph data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Rapid developments in streaming data technologies are continuing to generate\nincreased interest in monitoring human activity. Wearable devices, such as\nwrist-worn sensors that monitor gross motor activity (actigraphy), have become\nprevalent. An actigraph unit continually records the activity level of an\nindividual, producing a very large amount of data at a high-resolution that can\nbe immediately downloaded and analyzed. While this kind of \\textit{big data}\nincludes both spatial and temporal information, the variation in such data\nseems to be more appropriately modeled by considering stochastic evolution\nthrough time while accounting for spatial information separately. We propose a\ncomprehensive Bayesian hierarchical modeling and inferential framework for\nactigraphy data reckoning with the massive sizes of such databases while\nattempting to offer full inference. Building upon recent developments in this\nfield, we construct Nearest Neighbour Gaussian Processes (NNGPs) for actigraphy\ndata to compute at large temporal scales. More specifically, we construct a\ntemporal NNGP and we focus on the optimized implementation of the collapsed\nalgorithm in this specific context. This approach permits improved model\nscaling while also offering full inference. We test and validate our methods on\nsimulated data and subsequently apply and verify their predictive ability on an\noriginal dataset concerning a health study conducted by the Fielding School of\nPublic Health of the University of California, Los Angeles.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 16:15:24 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 11:20:07 GMT"}, {"version": "v3", "created": "Tue, 8 Jun 2021 16:21:12 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Di Loro", "Pierfrancesco Alaimo", ""], ["Mingione", "Marco", ""], ["Lipsitt", "Jonah", ""], ["Batteate", "Christina M.", ""], ["Jerrett", "Michael", ""], ["Banerjee", "Sudipto", ""]]}, {"id": "2101.01662", "submitter": "Luca Pappalardo", "authors": "Luca Pappalardo, Alessio Rossi, Giuseppe Pontillo, Michela Natilli,\n  Paolo Cintia", "title": "Explaining the difference between men's and women's football", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Women's football is gaining supporters and practitioners worldwide, raising\nquestions about what the differences are with men's football. While the two\nsports are often compared based on the players' physical attributes, we analyze\nthe spatio-temporal events during matches in the last World Cups to compare\nmale and female teams based on their technical performance. We train an\nartificial intelligence model to recognize if a team is male or female based on\nvariables that describe a match's playing intensity, accuracy, and performance\nquality. Our model accurately distinguishes between men's and women's football,\nrevealing crucial technical differences, which we investigate through the\nextraction of explanations from the classifier's decisions. The differences\nbetween men's and women's football are rooted in play accuracy, the recovery\ntime of ball possession, and the players' performance quality. Our methodology\nmay help journalists and fans understand what makes women's football a distinct\nsport and coaches design tactics tailored to female teams.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 17:25:53 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Pappalardo", "Luca", ""], ["Rossi", "Alessio", ""], ["Pontillo", "Giuseppe", ""], ["Natilli", "Michela", ""], ["Cintia", "Paolo", ""]]}, {"id": "2101.01703", "submitter": "Subhabrata Majumdar", "authors": "Subhabrata Majumdar, Cheryl Flynn, Ritwik Mitra", "title": "Evaluating Fairness in the Presence of Spatial Autocorrelation", "comments": "Extended abstract presented in SDSS-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spite of considerable practical importance, current algorithmic fairness\nliterature lacks in technical methods to account for underlying geographic\ndependency while evaluating or mitigating fairness issues for spatial data. We\ninitiate the study of spatial fairness in this paper, taking the first step\ntowards formalizing this line of quantitative methods. Fairness considerations\nfor spatial data often get confounded by the underlying spatial\nautocorrelation. We propose hypothesis testing methodology to detect the\npresence and strength of this effect, then mitigate it using a spatial\nfiltering-based approach -- in order to enable application of existing bias\ndetection metrics. We evaluate our proposed methodology through numerical\nexperiments on real and synthetic datasets, demonstrating that in presence of\nseveral types of confounding effects due to the underlying spatial structure\nour testing methods perform well in maintaining low type-II errors and nominal\ntype-I errors.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 18:47:12 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 15:09:39 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Majumdar", "Subhabrata", ""], ["Flynn", "Cheryl", ""], ["Mitra", "Ritwik", ""]]}, {"id": "2101.01835", "submitter": "Blanca Vazquez", "authors": "Blanca Vazquez, Gibran Fuentes, Fabian Garcia, Gabriela Borrayo, Juan\n  Prohias", "title": "Risk markers by sex and age group for in-hospital mortality in patients\n  with STEMI or NSTEMI: an approach based on machine learning", "comments": "20 pages, 6 figures, submitted to BMC Medical Informatics and\n  Decision Making", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning (ML) has demonstrated promising results in the\nidentification of clinical markers for Acute Coronary Syndrome (ACS) from\nelectronic health records (EHR). In the past, the ACS was perceived as a health\nproblem mainly for men and women were under-represented in clinical trials,\nwhich led to both sexes receiving the same clinical attention. Although some\napproaches have emphasized the importance of distinguishing markers, these\ndistinctions remain unclear. This study aims at exploiting ML methods for\nidentifying in-hospital mortality markers by sex and age-group for patients\nwith ST-elevation myocardial infarction (STEMI) and the Non-ST-elevation\nmyocardial infarction (NSTEMI) from EHR. From the MIMIC-III database, we\nextracted 1,299 patients with STEMI and 2,820 patients with NSTEMI. We trained\nand validated mortality prediction models with different hyperparameters,\nclinical sets, and ML methods. Using the best performing model and a\ngame-theoretic approach to interpret predictions, we identified risk markers\nfor patients with STEMI and NSTEMI separately. The models based on Extreme\nGradient Boosting achieved the highest performance: AUC=0.92 (95\\%\nCI:0.87-0.98) for STEMI and AUC=0.87 (95\\% CI:0.80-0.93) for NSTEMI. For STEMI,\nthe top markers for both sexes are the presence of hyponatremia, and metabolic\nacidosis. More specific markers for women are acute kidney failure, and age>75\nyears, while for men are chronic kidney failure, and age>70 years. In contrast,\nfor NSTEMI, the top markers for both sexes are advanced age, and intubation\nprocedures. The specific markers for women are low creatinine levels and age>60\nyears, whilst, for men are damage to the left atrium and age>70 years. We\nconsider that distinguishing markers for sexes could lead to more appropriate\ntreatment strategies, thus improving clinical outcomes.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 00:56:54 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Vazquez", "Blanca", ""], ["Fuentes", "Gibran", ""], ["Garcia", "Fabian", ""], ["Borrayo", "Gabriela", ""], ["Prohias", "Juan", ""]]}, {"id": "2101.01837", "submitter": "Jun-Ichi Takeshita", "authors": "Jun-ichi Takeshita, Akinobu Toyoda, Hidenori Tani, Yasunori Endo,\n  Sadaaki Miyamoto", "title": "Classification of chemical compounds based on the correlation between\n  \\textit{in vitro} gene expression profiles", "comments": "13pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Toxicity evaluation of chemical compounds has traditionally relied on animal\nexperiments;however, the demand for non-animal-based prediction methods for\ntoxicology of compounds is increasing worldwide. Our aim was to provide a\nclassification method for compounds based on \\textit{in vitro} gene expression\nprofiles. The \\textit{in vitro} gene expression data analyzed in the present\nstudy was obtained from our previous study. The data concerned nine compounds\ntypically employed in chemical management.We used agglomerative hierarchical\nclustering to classify the compounds;however, there was a statistical\ndifficulty to be overcome.We needed to properly extract RNAs for clustering\nfrom more than 30,000 RNAs. In order to overcome this difficulty, we introduced\na combinatorial optimization problem with respect to both gene expression\nlevels and the correlation between gene expression profiles. Then, the\nsimulated annealing algorithm was used to obtain a good solution for the\nproblem. As a result, the nine compounds were divided into two groups using\n1,000 extracted RNAs. Our proposed methodology enables read-across, one of the\nframeworks for predicting toxicology, based on \\textit{in vitro} gene\nexpression profiles.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 01:07:35 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Takeshita", "Jun-ichi", ""], ["Toyoda", "Akinobu", ""], ["Tani", "Hidenori", ""], ["Endo", "Yasunori", ""], ["Miyamoto", "Sadaaki", ""]]}, {"id": "2101.01885", "submitter": "Peter Attia", "authors": "Peter M. Attia, Kristen A. Severson, Jeremy D. Witmer", "title": "Statistical learning for accurate and interpretable battery lifetime\n  prediction", "comments": "Submitted to the Journal of the Electrochemical Society", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.mtrl-sci stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data-driven methods for battery lifetime prediction are attracting increasing\nattention for applications in which the degradation mechanisms are poorly\nunderstood and suitable training sets are available. However, while advanced\nmachine learning and deep learning methods promise high performance with\nminimal data preprocessing, simpler linear models with engineered features\noften achieve comparable performance, especially for small training sets, while\nalso providing physical and statistical interpretability. In this work, we use\na previously published dataset to develop simple, accurate, and interpretable\ndata-driven models for battery lifetime prediction. We first present the\n\"capacity matrix\" concept as a compact representation of battery\nelectrochemical cycling data, along with a series of feature representations.\nWe then create a number of univariate and multivariate models, many of which\nachieve comparable performance to the highest-performing models previously\npublished for this dataset. These models also provide insights into the\ndegradation of these cells. Our approaches can be used both to quickly train\nmodels for a new dataset and to benchmark the performance of more advanced\nmachine learning methods.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 06:05:24 GMT"}, {"version": "v2", "created": "Sat, 24 Apr 2021 19:53:56 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Attia", "Peter M.", ""], ["Severson", "Kristen A.", ""], ["Witmer", "Jeremy D.", ""]]}, {"id": "2101.01992", "submitter": "Manh Cuong Ng\\^o", "authors": "Manh Cuong Ng\\^o, Raghavendra Selvan, Outi Tervo, Mads Peter\n  Heide-J{\\o}rgensen and Susanne Ditlevsen", "title": "Detection of foraging behavior from accelerometer data using U-Net type\n  convolutional networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Narwhal is one of the most mysterious marine mammals, due to its isolated\nhabitat in the Arctic region. Tagging is a technology that has the potential to\nexplore the activities of this species, where behavioral information can be\ncollected from instrumented individuals. This includes accelerometer data,\ndiving and acoustic data as well as GPS positioning. An essential element in\nunderstanding the ecological role of toothed whales is to characterize their\nfeeding behavior and estimate the amount of food consumption. Buzzes are sounds\nemitted by toothed whales that are related directly to the foraging behaviors.\nIt is therefore of interest to measure or estimate the rate of buzzing to\nestimate prey intake. The main goal of this paper is to find a way to detect\nprey capture attempts directly from accelerometer data, and thus be able to\nestimate food consumption without the need for the more demanding acoustic\ndata. We develop 3 automated buzz detection methods based on accelerometer and\ndepth data solely. We use a dataset from 5 narwhals instrumented in East\nGreenland in 2018 to train, validate and test a logistic regression model and\nthe machine learning algorithms random forest and deep learning, using the\nbuzzes detected from acoustic data as the ground truth. The deep learning\nalgorithm performed best among the tested methods. We conclude that reliable\nbuzz detectors can be derived from high-frequency-sampling, back-mounted\naccelerometer tags, thus providing an alternative tool for studies of foraging\necology of marine mammals in their natural environments. We also compare buzz\ndetection with certain movement patterns, such as sudden changes in\nacceleration (jerks), found in other marine mammal species for estimating prey\ncapture. We find that narwhals do not seem to make big jerks when foraging and\nconclude that their hunting patterns in that respect differ from other marine\nmammals.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 12:29:06 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Ng\u00f4", "Manh Cuong", ""], ["Selvan", "Raghavendra", ""], ["Tervo", "Outi", ""], ["Heide-J\u00f8rgensen", "Mads Peter", ""], ["Ditlevsen", "Susanne", ""]]}, {"id": "2101.02094", "submitter": "Maciej Skorski", "authors": "Maciej Skorski", "title": "Bernstein-Type Bounds for Beta Distribution", "comments": "minor corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work establishes Bernstein-type closed-form concentration inequalities\nfor the beta distribution, with optimal variance proxy. For skewed\ndistributions, these bounds are demonstrated to be more accurate then\nsub-gaussian and sub-gamma inequalities from prior works.\n  The approach builds on the recursion obtained from a hyper-geometric\nrepresentation of the central moments.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 15:36:29 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 18:58:22 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Skorski", "Maciej", ""]]}, {"id": "2101.02104", "submitter": "Edward Wheatcroft", "authors": "Edward Wheatcroft and Ewelina Sienkiewicz", "title": "A Probabilistic Model for Predicting Shot Success in Football", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Football forecasting models traditionally rate teams on past match results,\nthat is based on the number of goals scored. Goals, however, involve a high\nelement of chance and thus past results often do not reflect the performances\nof the teams. In recent years, it has become increasingly clear that accounting\nfor other match events such as shots at goal can provide a better indication of\nthe relative strengths of two teams than the number of goals scored. Forecast\nmodels based on this information have been shown to be successful in\noutperforming those based purely on match results. A notable weakness, however,\nis that this approach does not take into account differences in the probability\nof shot success among teams. A team that is more likely to score from a shot\nwill need fewer shots to win a match, on average. In this paper, we propose a\nsimple parametric model to predict the probability of a team scoring, given it\nhas taken a shot at goal. We show that the resulting forecasts are able to\noutperform a model assuming an equal probability of shot success among all\nteams. We then show that the model can be combined with predictions of the\nnumber of shots achieved by each team, and can increase the skill of forecasts\nof both the match outcome and of whether the total number of goals in a match\nwill exceed 2.5. We assess the performance of the forecasts alongside two\nbetting strategies and find mixed evidence for improved performance.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 15:51:59 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Wheatcroft", "Edward", ""], ["Sienkiewicz", "Ewelina", ""]]}, {"id": "2101.02110", "submitter": "Thierry Roncalli", "authors": "Thierry Roncalli, Fatma Karray-Meziou, Fran\\c{c}ois Pan, Margaux\n  Regnault", "title": "Liquidity Stress Testing in Asset Management -- Part 1. Modeling the\n  Liability Liquidity Risk", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.14893.72165", "report-no": null, "categories": "q-fin.RM q-fin.CP stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This article is part of a comprehensive research project on liquidity risk in\nasset management, which can be divided into three dimensions. The first\ndimension covers liability liquidity risk (or funding liquidity) modeling, the\nsecond dimension focuses on asset liquidity risk (or market liquidity)\nmodeling, and the third dimension considers asset-liability liquidity risk\nmanagement (or asset-liability matching). The purpose of this research is to\npropose a methodological and practical framework in order to perform liquidity\nstress testing programs, which comply with regulatory guidelines (ESMA, 2019)\nand are useful for fund managers. The review of the academic literature and\nprofessional research studies shows that there is a lack of standardized and\nanalytical models. The aim of this research project is then to fill the gap\nwith the goal to develop mathematical and statistical approaches, and provide\nappropriate answers.\n  In this first part that focuses on liability liquidity risk modeling, we\npropose several statistical models for estimating redemption shocks. The\nhistorical approach must be complemented by an analytical approach based on\nzero-inflated models if we want to understand the true parameters that\ninfluence the redemption shocks. Moreover, we must also distinguish aggregate\npopulation models and individual-based models if we want to develop behavioral\napproaches. Once these different statistical models are calibrated, the second\nbig issue is the risk measure to assess normal and stressed redemption shocks.\nFinally, the last issue is to develop a factor model that can translate stress\nscenarios on market risk factors into stress scenarios on fund liabilities.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 16:08:27 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Roncalli", "Thierry", ""], ["Karray-Meziou", "Fatma", ""], ["Pan", "Fran\u00e7ois", ""], ["Regnault", "Margaux", ""]]}, {"id": "2101.02113", "submitter": "Francesca Tang", "authors": "Francesca Tang, Yang Feng, Hamza Chiheb, Jianqing Fan", "title": "The Interplay of Demographic Variables and Social Distancing Scores in\n  Deep Prediction of U.S. COVID-19 Cases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the severity of the COVID-19 outbreak, we characterize the nature of the\ngrowth trajectories of counties in the United States using a novel combination\nof spectral clustering and the correlation matrix. As the U.S. and the rest of\nthe world are experiencing a severe second wave of infections, the importance\nof assigning growth membership to counties and understanding the determinants\nof the growth are increasingly evident. Subsequently, we select the demographic\nfeatures that are most statistically significant in distinguishing the\ncommunities. Lastly, we effectively predict the future growth of a given county\nwith an LSTM using three social distancing scores. This comprehensive study\ncaptures the nature of counties' growth in cases at a very micro-level using\ngrowth communities, demographic factors, and social distancing performance to\nhelp government agencies utilize known information to make appropriate\ndecisions regarding which potential counties to target resources and funding\nto.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 16:12:29 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Tang", "Francesca", ""], ["Feng", "Yang", ""], ["Chiheb", "Hamza", ""], ["Fan", "Jianqing", ""]]}, {"id": "2101.02280", "submitter": "Linda Sun", "authors": "Linda Z. Sun, Cai (Iris) Wu, Xiaoyun (Nicole) Li, Cong Chen, Emmett V.\n  Schmidt", "title": "Independent Action Models and Prediction of Combination Treatment\n  Effects for Response Rate, Duration of Response and Tumor Size Change in\n  Oncology Drug Development", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  An unprecedented number of new cancer targets are in development, and most\nare being developed in combination therapies. Early oncology development is\nstrategically challenged in choosing the best combinations to move forward to\nlate stage development. The most common early endpoints to be assessed in such\ndecision-making include objective response rate, duration of response and tumor\nsize change. In this paper, using independent-drug-action and\nBliss-drug-independence concepts as a foundation, we introduce simple models to\npredict combination therapy efficacy for duration of response and tumor size\nchange. These models complement previous publications using the independent\naction models (Palmer 2017, Schmidt 2020) to predict progression-free survival\nand objective response rate and serve as new predictive models to understand\ndrug combinations for early endpoints. The models can be applied to predict the\ncombination treatment effect for early endpoints given monotherapy data, or to\nestimate the possible effect of one monotherapy in the combination if data are\navailable from the combination therapy and the other monotherapy. Such\nquantitative work facilitates efficient oncology drug development.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 21:47:02 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Sun", "Linda Z.", "", "Iris"], ["Cai", "", "", "Iris"], ["Wu", "", "", "Nicole"], ["Xiaoyun", "", "", "Nicole"], ["Li", "", ""], ["Chen", "Cong", ""], ["Schmidt", "Emmett V.", ""]]}, {"id": "2101.02288", "submitter": "Masoud Ataei", "authors": "Masoud Ataei, Shengyuan Chen, Zijiang Yang, M.Reza Peyghami", "title": "Theory and Applications of Financial Chaos Index", "comments": null, "journal-ref": "PHYSA 126160 2021", "doi": "10.1016/j.physa.2021.126160", "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop a new stock market index that captures the chaos existing in the\nmarket by measuring the mutual changes of asset prices. This new index relies\non a tensor-based embedding of the stock market information, which in turn\nfrees it from the restrictive value- or capitalization-weighting assumptions\nthat commonly underlie other various popular indexes. We show that our index is\na robust estimator of the market volatility which enables us to characterize\nthe market by performing the task of segmentation with a high degree of\nreliability. In addition, we analyze the dynamics and kinematics of the\nrealized market volatility as compared to the implied volatility by introducing\na time-dependent dynamical system model. Our computational results which\npertain to the time period from January 1990 to December 2019 imply that there\nexist a bidirectional causal relation between the processes underlying the\nrealized and implied volatility of the stock market within the given time\nperiod, where it is shown that the later has a stronger causal effect on the\nformer as compared to the opposite. This result connotes that the implied\nvolatility of the market plays a key role in characterization of the market's\nrealized volatility.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 18:01:13 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Ataei", "Masoud", ""], ["Chen", "Shengyuan", ""], ["Yang", "Zijiang", ""], ["Peyghami", "M. Reza", ""]]}, {"id": "2101.02304", "submitter": "Samuel W.K. Wong", "authors": "Shiyu He and Samuel W.K. Wong", "title": "Statistical challenges in the analysis of sequence and structure data\n  for the COVID-19 spike protein", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the major target of many vaccines and neutralizing antibodies against\nSARS-CoV-2, the spike (S) protein is observed to mutate over time. In this\npaper, we present statistical approaches to tackle some challenges associated\nwith the analysis of S-protein data. We build a Bayesian hierarchical model to\nstudy the temporal and spatial evolution of S-protein sequences, after grouping\nthe sequences into representative clusters. We then apply sampling methods to\ninvestigate possible changes to the S-protein's 3-D structure as a result of\ncommonly observed mutations. While the increasing spread of D614G variants has\nbeen noted in other research, our results also show that the co-occurring\nmutations of D614G together with S477N or A222V may spread even more rapidly,\nas quantified by our model estimates.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 23:53:28 GMT"}, {"version": "v2", "created": "Sun, 31 Jan 2021 01:46:20 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["He", "Shiyu", ""], ["Wong", "Samuel W. K.", ""]]}, {"id": "2101.02305", "submitter": "Maryam Motamedi", "authors": "Maryam Motamedi, Na Li, Douglas G. Down and Nancy M. Heddle", "title": "Demand Forecasting for Platelet Usage: from Univariate Time Series to\n  Multivariate Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Platelet products are both expensive and have very short shelf lives. As\nusage rates for platelets are highly variable, the effective management of\nplatelet demand and supply is very important yet challenging. The primary goal\nof this paper is to present an efficient forecasting model for platelet demand\nat Canadian Blood Services (CBS). To accomplish this goal, four different\ndemand forecasting methods, ARIMA (Auto Regressive Moving Average), Prophet,\nlasso regression (least absolute shrinkage and selection operator) and LSTM\n(Long Short-Term Memory) networks are utilized and evaluated. We use a large\nclinical dataset for a centralized blood distribution centre for four hospitals\nin Hamilton, Ontario, spanning from 2010 to 2018 and consisting of daily\nplatelet transfusions along with information such as the product\nspecifications, the recipients' characteristics, and the recipients' laboratory\ntest results. This study is the first to utilize different methods from\nstatistical time series models to data-driven regression and a machine learning\ntechnique for platelet transfusion using clinical predictors and with different\namounts of data. We find that the multivariate approaches have the highest\naccuracy in general, however, if sufficient data are available, a simpler time\nseries approach such as ARIMA appears to be sufficient. We also comment on the\napproach to choose clinical indicators (inputs) for the multivariate models.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 23:54:10 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Motamedi", "Maryam", ""], ["Li", "Na", ""], ["Down", "Douglas G.", ""], ["Heddle", "Nancy M.", ""]]}, {"id": "2101.02379", "submitter": "Xueqi Zhao", "authors": "Xueqi Zhao and Enrique del Castillo", "title": "A Registration-free approach for Statistical Process Control of 3D\n  scanned objects via FEM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work in on-line Statistical Process Control (SPC) of manufactured\n3-dimensional (3-D) objects has been proposed based on the estimation of the\nspectrum of the Laplace-Beltrami (LB) operator, a differential operator that\nencodes the geometrical features of a manifold and is widely used in Machine\nLearning (i.e., Manifold Learning). The resulting spectra are an intrinsic\ngeometrical feature of each part, and thus can be compared between parts\navoiding the part to part registration (or \"part localization\") pre-processing\nor the need for equal size meshes, characteristics which are required in\nprevious approaches for SPC of 3D parts. The recent spectral SPC methods,\nhowever, are limited to monitoring surface data from objects such that the\nscanned meshes have no boundaries, holes, or missing portions. In this paper,\nwe extend spectral methods by first considering a more accurate and general\nestimator of the LB spectrum that is obtained by application of Finite Element\nMethods (FEM) to the solution of Helmholtz's equation with boundaries. It is\nshown how the new spectral FEM approach, while it retains the advantages of not\nrequiring part localization/registration or equal size datasets scanned from\neach part, it provides more accurate spectrum estimates, which results in\nfaster detection of out of control conditions than earlier methods, can be\napplied to both mesh or volumetric (solid) scans, and furthermore, it is shown\nhow it can be applied to partial scans that result in open meshes (surface or\nvolumetric) with boundaries, increasing the practical applicability of the\nmethods. The present work brings SPC methods closer to contemporary research in\nComputer Graphics and Manifold Learning. MATLAB code that reproduces the\nexamples of this paper is provided in the supplementary materials.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 05:30:45 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Zhao", "Xueqi", ""], ["del Castillo", "Enrique", ""]]}, {"id": "2101.02521", "submitter": "Joel Persson", "authors": "Joel Persson, Jurriaan F. Parie and Stefan Feuerriegel", "title": "Monitoring the COVID-19 epidemic with nationwide telecommunication data", "comments": "118 pages, 39 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In response to the novel coronavirus disease (COVID-19), governments have\nintroduced severe policy measures with substantial effects on human behavior.\nHere, we perform a large-scale, spatio-temporal analysis of human mobility\nduring the COVID-19 epidemic. We derive human mobility from anonymized,\naggregated telecommunication data in a nationwide setting (Switzerland;\nFebruary 10 - April 26, 2020), consisting of ~1.5 billion trips. In comparison\nto the same time period from 2019, human movement in Switzerland dropped by\n49.1%. The strongest reduction is linked to bans on gatherings of more than 5\npeople, which is estimated to have decreased mobility by 24.9%, followed by\nvenue closures (stores, restaurants, and bars) and school closures. As such,\nhuman mobility at a given day predicts reported cases 7-13 days ahead. A 1%\nreduction in human mobility predicts a 0.88-1.11% reduction in daily reported\nCOVID-19 cases. When managing epidemics, monitoring human mobility via\ntelecommunication data can support public decision-makers in two ways. First,\nit helps in assessing policy impact; second, it provides a scalable tool for\nnear real-time epidemic surveillance, thereby enabling evidence-based policies.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 12:45:44 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2021 11:07:07 GMT"}, {"version": "v3", "created": "Wed, 13 Jan 2021 16:43:16 GMT"}, {"version": "v4", "created": "Sun, 11 Apr 2021 15:27:48 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Persson", "Joel", ""], ["Parie", "Jurriaan F.", ""], ["Feuerriegel", "Stefan", ""]]}, {"id": "2101.02530", "submitter": "Alexander Neergaard Olesen", "authors": "Alexander Neergaard Olesen, Poul Jennum, Emmanuel Mignot and Helge B.\n  D. Sorensen", "title": "MSED: a multi-modal sleep event detection model for clinical sleep\n  analysis", "comments": "20 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.SP stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Study objective: Clinical sleep analysis require manual analysis of sleep\npatterns for correct diagnosis of sleep disorders. Several studies show\nsignificant variability in scoring discrete sleep events. We wished to\ninvestigate, whether an automatic method could be used for detection of\narousals (Ar), leg movements (LM) and sleep disordered breathing (SDB) events,\nand if the joint detection of these events performed better than having three\nseparate models.\n  Methods: We designed a single deep neural network architecture to jointly\ndetect sleep events in a polysomnogram. We trained the model on 1653 recordings\nof individuals, and tested the optimized model on 1000 separate recordings. The\nperformance of the model was quantified by F1, precision, and recall scores,\nand by correlating index values to clinical values using Pearson's correlation\ncoefficient.\n  Results: F1 scores for the optimized model was 0.70, 0.63, and 0.62 for Ar,\nLM, and SDB, respectively. The performance was higher, when detecting events\njointly compared to corresponding single-event models. Index values computed\nfrom detected events correlated well with manual annotations ($r^2$ = 0.73,\n$r^2$ = 0.77, $r^2$ = 0.78, respectively).\n  Conclusion: Detecting arousals, leg movements and sleep disordered breathing\nevents jointly is possible, and the computed index values correlates well with\nhuman annotations.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 13:08:44 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Olesen", "Alexander Neergaard", ""], ["Jennum", "Poul", ""], ["Mignot", "Emmanuel", ""], ["Sorensen", "Helge B. D.", ""]]}, {"id": "2101.02567", "submitter": "Roberto Galeazzi", "authors": "Pegah Barkhordari, Roberto Galeazzi, and Mogens Blanke", "title": "Monitoring of Railpad Long-term Condition in Turnouts Using Extreme\n  Value Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.SY stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The railpad is a key element in railway infrastructures that plays an\nessential role in the train-track dynamics. Presence of worn or defective\nrailpads along railway track may lead to large wheel/rail interaction forces,\nand a high rate of deterioration for track components. Despite the importance\nof railpad, the track infrastructure managers use no inspection tool for\nmonitoring in-service railpads over time. In this paper, a novel data-driven\nmonitoring tool for long-term performance analysis of in-service railpads is\ndeveloped based on train-induced vibration data collected by a track-side\nmeasurement system. The monitoring tool consists of a method for track\nresonance frequencies estimation, a temperature-frequency model for describing\nrailpad behavior with respect to ambient temperature, and a generalized\nlikelihood ratio test based on the generalized extreme value distribution for\ndetecting changes in the railpad status over time. To evaluate the performance\nof the proposed monitoring system, the status of railpads at four different\nlocations along a railway turnout is monitored over a period of 18 months. It\nis shown that the monitoring system can successfully detect changes in railpad\nproperties over the considered period.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 14:45:49 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Barkhordari", "Pegah", ""], ["Galeazzi", "Roberto", ""], ["Blanke", "Mogens", ""]]}, {"id": "2101.02580", "submitter": "Souvik Banerjee", "authors": "Gajendra K. Vishwakarma, Atanu Bhattacharjee, Souvik Banerjee", "title": "Handling Missingness Value on Jointly Measured Time-Course and\n  Time-to-event Data", "comments": "20 pages, 2 figures, 6 tables. Communications in Statistics -\n  Simulation and Computation (2020)", "journal-ref": null, "doi": "10.1080/03610918.2020.1851711", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Joint modeling technique is a recent advancement in effectively analyzing the\nlongitudinal history of patients with the occurrence of an event of interest\nattached to it. This procedure is successfully implemented in biomarker studies\nto examine parents with the occurrence of tumor. One of the typical problem\nthat influences the necessary inference is the presence of missing values in\nthe longitudinal responses as well as in covariates. The occurrence of\nmissingness is very common due to the dropout of patients from the study. This\narticle presents an effective and detailed way to handle the missing values in\nthe covariates and response variable. This study discusses the effect of\ndifferent multiple imputation techniques on the inferences of joint modeling\nimplemented on imputed datasets. A simulation study is carried out to replicate\nthe complex data structures and conveniently perform our analysis to show its\nefficacy in terms of parameter estimation. This analysis is further illustrated\nwith the longitudinal and survival outcomes of biomarkers' study by assessing\nproper codes in R programming language.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 15:10:59 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Vishwakarma", "Gajendra K.", ""], ["Bhattacharjee", "Atanu", ""], ["Banerjee", "Souvik", ""]]}, {"id": "2101.02630", "submitter": "Alejandro Carderera", "authors": "Alejandro Carderera and Sebastian Pokutta and Christof Sch\\\"utte and\n  Martin Weiser", "title": "CINDy: Conditional gradient-based Identification of Non-linear Dynamics\n  -- Noise-robust recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Governing equations are essential to the study of nonlinear dynamics, often\nenabling the prediction of previously unseen behaviors as well as the inclusion\ninto control strategies. The discovery of governing equations from data thus\nhas the potential to transform data-rich fields where well-established\ndynamical models remain unknown. This work contributes to the recent trend in\ndata-driven sparse identification of nonlinear dynamics of finding the best\nsparse fit to observational data in a large library of potential nonlinear\nmodels. We propose an efficient first-order Conditional Gradient algorithm for\nsolving the underlying optimization problem. In comparison to the most\nprominent alternative algorithms, the new algorithm shows significantly\nimproved performance on several essential issues like sparsity-induction,\nstructure-preservation, noise robustness, and sample efficiency. We demonstrate\nthese advantages on several dynamics from the field of synchronization,\nparticle dynamics, and enzyme chemistry.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 17:05:02 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 14:44:25 GMT"}, {"version": "v3", "created": "Wed, 28 Apr 2021 18:47:56 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Carderera", "Alejandro", ""], ["Pokutta", "Sebastian", ""], ["Sch\u00fctte", "Christof", ""], ["Weiser", "Martin", ""]]}, {"id": "2101.02924", "submitter": "Tim Prangemeier", "authors": "Tim Prangemeier, Christian Wildner, Maleen Hanst, and Heinz Koeppl", "title": "Maximizing Information Gain for the Characterization of Biomolecular\n  Circuits", "comments": "NanoCom 2018, accepted", "journal-ref": null, "doi": "10.1145/3233188.3233217", "report-no": null, "categories": "q-bio.MN cs.SY eess.SY physics.ins-det q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitatively predictive models of biomolecular circuits are important tools\nfor the design of synthetic biology and molecular communication circuits. The\ninformation content of typical time-lapse single-cell data for the inference of\nkinetic parameters is not only limited by measurement uncertainty and intrinsic\nstochasticity, but also by the employed perturbations. Novel microfluidic\ndevices enable the synthesis of temporal chemical concentration profiles. The\ninformativeness of a perturbation can be quantified based on mutual\ninformation. We propose an approximate method to perform optimal experimental\ndesign of such perturbation profiles. To estimate the mutual information we\nperform a multivariate log-normal approximation of the joint distribution over\nparameters and observations and scan the design space using Metropolis-Hastings\nsampling. The method is demonstrated by finding optimal perturbation sequences\nfor synthetic case studies on a gene expression model with varying reporter\ncharacteristics.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 09:13:46 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Prangemeier", "Tim", ""], ["Wildner", "Christian", ""], ["Hanst", "Maleen", ""], ["Koeppl", "Heinz", ""]]}, {"id": "2101.02938", "submitter": "Shiyuan He", "authors": "Shiyuan He, Zhenfeng Lin, Wenlong Yuan, Lucas M. Macri, Jianhua Z.\n  Huang", "title": "Simultaneous inference of periods and period-luminosity relations for\n  Mira variable stars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP astro-ph.GA astro-ph.SR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Period--Luminosity relation (PLR) of Mira variable stars is an important\ntool to determine astronomical distances. The common approach of estimating the\nPLR is a two-step procedure that first estimates the Mira periods and then runs\na linear regression of magnitude on log period. When the light curves are\nsparse and noisy, the accuracy of period estimation decreases and can suffer\nfrom aliasing effects. Some methods improve accuracy by incorporating complex\nmodel structures at the expense of significant computational costs. Another\ndrawback of existing methods is that they only provide point estimation without\nproper estimation of uncertainty. To overcome these challenges, we develop a\nhierarchical Bayesian model that simultaneously models the quasi-periodic\nvariations for a collection of Mira light curves while estimating their common\nPLR. By borrowing strengths through the PLR, our method automatically reduces\nthe aliasing effect, improves the accuracy of period estimation, and is capable\nof characterizing the estimation uncertainty. We develop a scalable stochastic\nvariational inference algorithm for computation that can effectively deal with\nthe multimodal posterior of period. The effectiveness of the proposed method is\ndemonstrated through simulations, and an application to observations of Miras\nin the Local Group galaxy M33. Without using ad-hoc period correction tricks,\nour method achieves a distance estimate of M33 that is consistent with\npublished work. Our method also shows superior robustness to downsampling of\nthe light curves.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 10:06:49 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["He", "Shiyuan", ""], ["Lin", "Zhenfeng", ""], ["Yuan", "Wenlong", ""], ["Macri", "Lucas M.", ""], ["Huang", "Jianhua Z.", ""]]}, {"id": "2101.03098", "submitter": "Berkay Gulcan", "authors": "Berkay Gulcan and Sandra D. Eksioglu and Yongjia Song and Mohammad\n  Roni and Qiushi Chen", "title": "Optimization Models for Integrated Biorefinery Operations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Variations of physical and chemical characteristics of biomass lead to an\nuneven flow of biomass in a biorefinery, which reduces equipment utilization\nand increases operational costs. Uncertainty of biomass supply and high\nprocessing costs increase the risk of investing in the US's cellulosic biofuel\nindustry. We propose a stochastic programming model to streamline processes\nwithin a biorefinery. A chance constraint models system's reliability\nrequirement that the reactor is operating at a high utilization rate given\nuncertain biomass moisture content, particle size distribution, and equipment\nfailure. The model identifies operating conditions of equipment and inventory\nlevel to maintain a continuous flow of biomass to the reactor. The Sample\nAverage Approximation method approximates the chance constraint and a bisection\nsearch-based heuristic solves this approximation. A case study is developed\nusing real-life data collected at Idaho National Laboratory's pilot biomass\nprocessing facility. An extensive computational analysis indicates that\nsequencing of biomass bales based on moisture level, increasing storage\ncapacity, and managing particle size distribution increase utilization of the\nreactor and reduce operational costs.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 16:54:09 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Gulcan", "Berkay", ""], ["Eksioglu", "Sandra D.", ""], ["Song", "Yongjia", ""], ["Roni", "Mohammad", ""], ["Chen", "Qiushi", ""]]}, {"id": "2101.03133", "submitter": "Quan-Lin Li", "authors": "Quan-Lin Li, Chengliang Wang, Yiming Xu, Chi Zhang, Yanxia Chang,\n  Xiaole Wu, Zhen-Ping Fan, Zhi-Guo Liu", "title": "Infections Forecasting and Intervention Effect Evaluation for COVID-19\n  via a Data-Driven Markov Process and Heterogeneous Simulation", "comments": "29 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Coronavirus Disease 2019 (COVID-19) pandemic has caused tremendous amount\nof deaths and a devastating impact on the economic development all over the\nworld. Thus, it is paramount to control its further transmission, for which\npurpose it is necessary to find the mechanism of its transmission process and\nevaluate the effect of different control strategies. To deal with these issues,\nwe describe the transmission of COVID-19 as an explosive Markov process with\nfour parameters. The state transitions of the proposed Markov process can\nclearly disclose the terrible explosion and complex heterogeneity of COVID-19.\nBased on this, we further propose a simulation approach with heterogeneous\ninfections. Experimentations show that our approach can closely track the real\ntransmission process of COVID-19, disclose its transmission mechanism, and\nforecast the transmission under different non-drug intervention strategies.\nMore importantly, our approach can helpfully develop effective strategies for\ncontrolling COVID-19 and appropriately compare their control effect in\ndifferent countries/cities.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 17:37:24 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Li", "Quan-Lin", ""], ["Wang", "Chengliang", ""], ["Xu", "Yiming", ""], ["Zhang", "Chi", ""], ["Chang", "Yanxia", ""], ["Wu", "Xiaole", ""], ["Fan", "Zhen-Ping", ""], ["Liu", "Zhi-Guo", ""]]}, {"id": "2101.03137", "submitter": "Gurcan Comert", "authors": "Samuel Darko, Gurcan Comert, Noelle A Mware, Faith Kibuye", "title": "Evaluation of Sustainable Green Materials: Pinecone in Permeable\n  Adsorptive Barriers for Remediation of Groundwater Contaminated by $Pb^{2+}$\n  and Methylene Blue", "comments": "14 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.chem-ph physics.geo-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We report herein, the potential of raw pinecone powder (PCP) and pinecone\nbiochar (PCBC) as alternatives to activated carbon used in Permeable Adsorptive\nBarriers (PABs) for the in situ remediation of polluted groundwater. A\nconstructed lab-scale unconfined aquifer ($38\\times30\\times17$ $cm$) fitted\nwith PCP and PCBC PABs ($21\\times3\\times20$ $cm$), was evaluated for the\nremoval of $Pb^{2+}$ ions in a continuous flow setup. Results indicate that\nafter $3600$ minutes, PCP was able to reduce $Pb^{2+}$ ions from a Co=$50$\n$mg/L$ to $7.94$ $mg/L$ for the first run and $19.4$ $mg/L$ for a second run,\nrespectively. Comparatively, PCBC reached $6.5$ $mg/L$ for the first run and\n$8.94$ for the second run. It was confirmed that adsorption was best described\nby the first-order kinetic model with $R^2$ values above $0.95$. Maximum\nadsorption capacity values were found to be $1.00$, $0.63$, $1.08$, and $0.85$\nmg/g for each scenario respectively. In addition, nonlinear regression models\nof exponential and Gaussian Processes are fit to explain remediation by time\nfor $Pb^{2+}$ and Methylene Blue. Gaussian Processes are able to better explain\nthe variation of pollution removal compared to simpler exponential models. When\nregressed against true removal percentages all models are able to provide\n$R^2>0.99$.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 02:05:37 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Darko", "Samuel", ""], ["Comert", "Gurcan", ""], ["Mware", "Noelle A", ""], ["Kibuye", "Faith", ""]]}, {"id": "2101.03254", "submitter": "Xuxue Sun", "authors": "Xuxue Sun, Nan Kong, Nazmus Sakib, Chao Meng, Kathryn Hyer, Hongdao\n  Meng, Chris Masterson, Mingyang Li", "title": "A Latent Survival Analysis Enabled Simulation Platform For Nursing Home\n  Staffing Strategy Evaluation", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nursing homes are critical facilities for caring frail older adults with\nround-the-clock formal care and personal assistance. To ensure quality care for\nnursing home residents, adequate staffing level is of great importance. Current\nnursing home staffing practice is mainly based on experience and regulation.\nThe objective of this paper is to investigate the viability of experience-based\nand regulation-based strategies, as well as alternative staffing strategies to\nminimize labor costs subject to heterogeneous service demand of nursing home\nresidents under various scenarios of census. We propose a data-driven analysis\nframework to model heterogeneous service demand of nursing home residents and\nfurther identify appropriate staffing strategies by combing survival model and\ncomputer simulation techniques as well as domain knowledge. Specifically, in\nthe analysis, we develop an agent-based simulation tool consisting of four main\nmodules, namely individual length of stay predictor, individual daily staff\ntime generator, facility level staffing strategy evaluator, and graphical user\ninterface. We use real nursing home data to validate the proposed model, and\ndemonstrate that the identified staffing strategy significantly reduces the\ntotal labor cost of certified nursing assistants compared to the benchmark\nstrategies. Additionally, the proposed length of stay predictive model that\nconsiders multiple discharge dispositions exhibits superior accuracy and offers\nbetter staffing decisions than those without the consideration. Further, we\nconstruct different census scenarios of nursing home residents to demonstrate\nthe capability of the proposed framework in helping adjust staffing decisions\nof nursing home administrators in various realistic settings.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 23:32:45 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 13:02:41 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Sun", "Xuxue", ""], ["Kong", "Nan", ""], ["Sakib", "Nazmus", ""], ["Meng", "Chao", ""], ["Hyer", "Kathryn", ""], ["Meng", "Hongdao", ""], ["Masterson", "Chris", ""], ["Li", "Mingyang", ""]]}, {"id": "2101.03268", "submitter": "Evan Sidrow", "authors": "Evan Sidrow, Nancy Heckman, Sarah M.E. Fortune, Andrew W. Trites, Ian\n  Murphy, Marie Auger-M\\'eth\\'e", "title": "Modelling multi-scale state-switching functional data with hidden Markov\n  models", "comments": "23 pages, 8 figures, 2 tables. Supplementary material appended to\n  submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data sets comprised of sequences of curves sampled at high frequencies in\ntime are increasingly common in practice, but they can exhibit complicated\ndependence structures that cannot be modelled using common methods of\nFunctional Data Analysis (FDA). We detail a hierarchical approach which treats\nthe curves as observations from a hidden Markov model (HMM). The distribution\nof each curve is then defined by another fine-scale model which may involve\nauto-regression and require data transformations using moving-window summary\nstatistics or Fourier analysis. This approach is broadly applicable to\nsequences of curves exhibiting intricate dependence structures. As a case\nstudy, we use this framework to model the fine-scale kinematic movement of a\nnorthern resident killer whale (Orcinus orca) off the coast of British\nColumbia, Canada. Through simulations, we show that our model produces more\ninterpretable state estimation and more accurate parameter estimates compared\nto existing methods.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 01:23:18 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Sidrow", "Evan", ""], ["Heckman", "Nancy", ""], ["Fortune", "Sarah M. E.", ""], ["Trites", "Andrew W.", ""], ["Murphy", "Ian", ""], ["Auger-M\u00e9th\u00e9", "Marie", ""]]}, {"id": "2101.03328", "submitter": "Sadegh Movahed", "authors": "H. Masoomy, B. Askari, M. N. Najafi and S. M. S. Movahed", "title": "Persistent Homology of Weighted Visibility Graph from Fractional\n  Gaussian Noise", "comments": "17 pages, 13 figures, Comments Welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cs.CG math.AT stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we utilize persistent homology technique to examine the\ntopological properties of the visibility graph constructed from fractional\nGaussian noise (fGn). We develop the weighted natural visibility graph\nalgorithm and the standard network in addition to the global properties in the\ncontext of topology, will be examined. Our results demonstrate that the\ndistribution of {\\it eigenvector} and {\\it betweenness centralities} behave as\npower-law decay. The scaling exponent of {\\it eigenvector centrality} and the\nmoment of {\\it eigenvalue} distribution, $M_{n}$, for $n\\ge1$ reveal the\ndependency on the Hurst exponent, $H$, containing the sample size effect. We\nalso focus on persistent homology of $k$-dimensional topological holes\nincorporating the filtration of simplicial complexes of associated graph. The\ndimension of homology group represented by {\\it Betti numbers} demonstrates a\nstrong dependency on the Hurst exponent. More precisely, the scaling exponent\nof the number of $k$-dimensional topological \\textit{holes} appearing and\ndisappearing at a given threshold, depends on $H$ which is almost not affected\nby finite sample size. We show that the distribution function of\n\\textit{lifetime} for $k$-dimensional topological holes decay exponentially and\ncorresponding slope is an increasing function versus $H$ and more\ninterestingly, the sample size effect is completely disappeared in this\nquantity. The persistence entropy logarithmically grows with the size of\nvisibility graph of system with almost $H$-dependent prefactors.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 09:57:24 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Masoomy", "H.", ""], ["Askari", "B.", ""], ["Najafi", "M. N.", ""], ["Movahed", "S. M. S.", ""]]}, {"id": "2101.03454", "submitter": "M\\'arcio Diniz", "authors": "M\\'arcio A. Diniz, Gillian Gresham, Sungjin Kim, Michael Luu, N. Lynn\n  Henry, Mourad Tighiouart, Greg Yothers, Patricia A. Ganz and Andr\\'e Rogatko", "title": "Visualizing adverse events in clinical trials using correspondence\n  analysis with R-package visae", "comments": "21 pages, 2 figures and 14 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to apply stacked CA using contribution biplots as a tool to\nexplore differences in AE data among treatments in clinical trials. We defined\nfive levels of refinement for the analysis based on data derived from the\nCommon Terminology Criteria for Adverse Events (CTCAE) grades, domains, terms\nand their combinations. In addition, we developed a Shiny app built in an\nR-package, publicly available on Comprehensive R Archive Network (CRAN), to\ninteractively investigate CA configurations. Data from two randomized\ncontrolled trials (RCT) were used to illustrate the proposed methods: NSABP\nR-04, a neoadjuvant rectal 2x2 factorial trial comparing radiation therapy with\neither capecitabine (Cape) or 5-fluorouracil (5-FU) alone with or without\noxaliplatin (Oxa), and NSABP B-35, a double-blind RCT comparing tamoxifen to\nanastrozole in postmenopausal women with hormone-positive ductal carcinoma in\nsitu. In the R04 trial (n=1308), CA biplots displayed the discrepancies between\nsingle agent treatments and their combinations with Oxa at all levels of AE\nclasses, such that these discrepancies were responsible for the largest portion\nof the explained variability among treatments. In addition, an interaction\neffect when adding Oxa to Cape/5-FU was identified when the distance between\nCape+Oxa and 5-FU+Oxa was observed to be larger than the distance between 5-FU\nand Cape, with Cape+Oxa and 5-FU+Oxa in different quadrants of the CA biplots.\nIn the B35 trial (n=3009), CA biplots showed different patterns for\nnon-adherent Anastrozole and Tamoxifen compared with their adherent\ncounterparts. CA with contribution biplot is an effective tool that can be used\nto summarize AE data in a two-dimensional display while minimizing the loss of\ninformation and interpretation.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 01:58:22 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 18:33:55 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Diniz", "M\u00e1rcio A.", ""], ["Gresham", "Gillian", ""], ["Kim", "Sungjin", ""], ["Luu", "Michael", ""], ["Henry", "N. Lynn", ""], ["Tighiouart", "Mourad", ""], ["Yothers", "Greg", ""], ["Ganz", "Patricia A.", ""], ["Rogatko", "Andr\u00e9", ""]]}, {"id": "2101.03491", "submitter": "Narumasa Tsutsumida", "authors": "Joseph Emile Honour Percival, Narumasa Tsutsumida, Daisuke Murakami,\n  Takahiro Yoshida, Tomoki Nakaya", "title": "gwpcorMapper: an interactive mapping tool for exploring geographically\n  weighted correlation and partial correlation in high-dimensional geospatial\n  datasets", "comments": "18 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Exploratory spatial data analysis (ESDA) plays a key role in research that\nincludes geographic data. In ESDA, analysts often want to be able to visualize\nobservations and local relationships on a map. However, software dedicated to\nvisualizing local spatial relations be-tween multiple variables in high\ndimensional datasets remains undeveloped. This paper introduces gwpcorMapper, a\nnewly developed software application for mapping geographically weighted\ncorrelation and partial correlation in large multivariate datasets.\ngwpcorMap-per facilitates ESDA by giving researchers the ability to interact\nwith map components that describe local correlative relationships. We built\ngwpcorMapper using the R Shiny framework. The software inherits its core\nalgorithm from GWpcor, an R library for calculating the geographically weighted\ncorrelation and partial correlation statistics. We demonstrate the application\nof gwpcorMapper by using it to explore census data in order to find meaningful\nrelationships that describe the work-life environment in the 23 special wards\nof Tokyo, Japan. We show that gwpcorMapper is useful in both variable selection\nand parameter tuning for geographically weighted statistics. gwpcorMapper\nhighlights that there are strong statistically clear local variations in the\nrelationship between the number of commuters and the total number of hours\nworked when considering the total population in each district across the 23\nspecial wards of Tokyo. Our application demonstrates that the ESDA process with\nhigh-dimensional geospatial data using gwpcorMapper has applications across\nmultiple fields.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 07:16:29 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Percival", "Joseph Emile Honour", ""], ["Tsutsumida", "Narumasa", ""], ["Murakami", "Daisuke", ""], ["Yoshida", "Takahiro", ""], ["Nakaya", "Tomoki", ""]]}, {"id": "2101.03536", "submitter": "Soumita Modak Ph.D.", "authors": "Soumita Modak", "title": "Distinction of groups of gamma-ray bursts in the BATSE catalog through\n  fuzzy clustering", "comments": "26 pages; 6 figures", "journal-ref": "Astronomy and Computing, Year: 2021, Volume 34, Article id 100441,\n  7 pages", "doi": "10.1016/j.ascom.2020.100441", "report-no": null, "categories": "stat.AP astro-ph.HE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In search for the possible astrophysical sources behind origination of the\ndiverse gamma-ray bursts, cluster analyses are performed to find homogeneous\ngroups, which discover an intermediate group other than the conventional short\nand long bursts. However, very recently, few studies indicate a possibility of\nthe existence of more than three (namely five) groups. Therefore, in this\npaper, fuzzy clustering is conducted on the gamma-ray bursts from the final\n'Burst and Transient Source Experiment' catalog to cross-check the significance\nof these new groups. Meticulous study on individual bursts based on their\nmemberships in the fuzzy clusters confirms the previously well-known three\ngroups against the newly found five.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 12:43:01 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Modak", "Soumita", ""]]}, {"id": "2101.03579", "submitter": "Michele Peruzzi", "authors": "Michele Peruzzi, Sudipto Banerjee, David B. Dunson, Andrew O. Finley", "title": "Grid-Parametrize-Split (GriPS) for Improved Scalable Inference in\n  Spatial Big Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Rapid advancements in spatial technologies including Geographic Information\nSystems (GIS) and remote sensing have generated massive amounts of spatially\nreferenced data in a variety of scientific and data-driven industrial\napplications. These advancements have led to a substantial, and still\nexpanding, literature on the modeling and analysis of spatially oriented big\ndata. In particular, Bayesian inferences for high-dimensional spatial processes\nare being sought in a variety of remote-sensing applications including, but not\nlimited to, modeling next generation Light Detection and Ranging (LiDAR)\nsystems and other remotely sensed data. Massively scalable spatial processes,\nin particular Gaussian processes (GPs), are being explored extensively for the\nincreasingly encountered big data settings. Recent developments include GPs\nconstructed from sparse Directed Acyclic Graphs (DAGs) with a limited number of\nneighbors (parents) to characterize dependence across the spatial domain. The\nDAG can be used to devise fast algorithms for posterior sampling of the latent\nprocess, but these may exhibit pathological behavior in estimating covariance\nparameters. While these issues are mitigated by considering marginalized\nsamplers that exploit the underlying sparse precision matrix, these algorithms\nare slower, less flexible, and oblivious of structure in the data. The current\narticle introduces the Grid-Parametrize-Split (GriPS) approach for conducting\nBayesian inference in spatially oriented big data settings by a combination of\ncareful model construction and algorithm design to effectuate substantial\nimprovements in MCMC convergence. We demonstrate the effectiveness of our\nproposed methods through simulation experiments and subsequently undertake the\nmodeling of LiDAR outcomes and production of their predictive maps using G-LiHT\nand other remotely sensed variables.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 16:51:42 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Peruzzi", "Michele", ""], ["Banerjee", "Sudipto", ""], ["Dunson", "David B.", ""], ["Finley", "Andrew O.", ""]]}, {"id": "2101.03622", "submitter": "F\\'abio Silveira", "authors": "F\\'abio V. J. Silveira, Frank Gomes-Silva, C\\'icero C. R. Brito, Jader\n  S. Jale, Felipe R. S. Gusm\\~ao, S\\'ilvio F. A. Xavier-J\\'unior, Jo\\~ao S.\n  Rocha", "title": "Modelling wind speed with a univariate probability distribution\n  depending on two baseline functions", "comments": "22 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Characterizing the wind speed distribution properly is essential for the\nsatisfactory production of potential energy in wind farms, being the mixture\nmodels usually employed in the description of such data. However, some mixture\nmodels commonly have the undesirable property of non-identifiability. In this\nwork, we present an alternative distribution which is able to fit the wind\nspeed data adequately. The new model, called Normal-Weibull-Weibull, is\nidentifiable and its cumulative distribution function is written as a\ncomposition of two baseline functions. We discuss structural properties of the\nclass that generates the proposed model, such as the linear representation of\nthe probability density function, moments and moment generating function. We\nperform a Monte Carlo simulation study to investigate the behavior of the\nmaximum likelihood estimates of the parameters. Finally, we present\napplications of the new distribution for modelling wind speed data measured in\nfive different cities of the Northeastern Region of Brazil.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 20:19:54 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Silveira", "F\u00e1bio V. J.", ""], ["Gomes-Silva", "Frank", ""], ["Brito", "C\u00edcero C. R.", ""], ["Jale", "Jader S.", ""], ["Gusm\u00e3o", "Felipe R. S.", ""], ["Xavier-J\u00fanior", "S\u00edlvio F. A.", ""], ["Rocha", "Jo\u00e3o S.", ""]]}, {"id": "2101.03671", "submitter": "Xuxue Sun", "authors": "Xuxue Sun, Wenjun Cai, Qiong Zhang, Mingyang Li", "title": "A Degradation Performance Model With Mixed-type Covariates and Latent\n  Heterogeneity", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Successful modeling of degradation performance data is essential for accurate\nreliability assessment and failure predictions of highly reliable product\nunits. The degradation performance measurements over time are highly\nheterogeneous. Such heterogeneity can be partially attributed to external\nfactors, such as accelerated/environmental conditions, and can also be\nattributed to internal factors, such as material microstructure characteristics\nof product units. The latent heterogeneity due to the unobserved/unknown\nfactors shared within each product unit may also exists and need to be\nconsidered as well. Existing degradation models often fail to consider (i) the\ninfluence of both external accelerated/environmental conditions and internal\nmaterial information, (ii) the influence of unobserved/unknown factors within\neach unit. In this work, we propose a generic degradation performance modeling\nframework with mixed-type covariates and latent heterogeneity to account for\nboth influences of observed internal and external factors as well as unobserved\nfactors. Effective estimation algorithm is also developed to jointly quantify\nthe influences of mixed-type covariates and individual latent heterogeneity,\nand also to examine the potential interaction between mixed-type covariates.\nFunctional data analysis and data augmentation techniques are employed to\naddress a series of estimation issues. A real case study is further provided to\ndemonstrate the superior performance of the proposed approach over several\nalternative modeling approaches. Besides, the proposed degradation performance\nmodeling framework also provides interpretable findings.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 02:29:05 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Sun", "Xuxue", ""], ["Cai", "Wenjun", ""], ["Zhang", "Qiong", ""], ["Li", "Mingyang", ""]]}, {"id": "2101.03725", "submitter": "Billy Pik Lik Lau", "authors": "Billy Pik Lik Lau, Benny Kai Kiat Ng, Chau Yuen, Bige Tuncer, Keng Hua\n  Chong", "title": "The Study of Urban Residential's Public Space Activeness using\n  Space-centric Approach", "comments": "Accepted at IEEE Internet of Things Journal 2021", "journal-ref": null, "doi": "10.1109/JIOT.2021.3051343", "report-no": null, "categories": "stat.AP cs.OH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advancement of the Internet of Things (IoT) and communication\nplatform, large scale sensor deployment can be easily implemented in an urban\ncity to collect various information. To date, there are only a handful of\nresearch studies about understanding the usage of urban public spaces.\nLeveraging IoT, various sensors have been deployed in an urban residential area\nto monitor and study public space utilization patterns. In this paper, we\npropose a data processing system to generate space-centric insights about the\nutilization of an urban residential region of multiple points of interest\n(PoIs) that consists of 190,000m$^2$ real estate. We identify the activeness of\neach PoI based on the spectral clustering, and then study their corresponding\nstatic features, which are composed of transportation, commercial facilities,\npopulation density, along with other characteristics. Through the heuristic\nfeatures inferring, the residential density and commercial facilities are the\nmost significant factors affecting public place utilization.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 06:47:38 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 02:11:02 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Lau", "Billy Pik Lik", ""], ["Ng", "Benny Kai Kiat", ""], ["Yuen", "Chau", ""], ["Tuncer", "Bige", ""], ["Chong", "Keng Hua", ""]]}, {"id": "2101.03944", "submitter": "Rajarshi Banerjee", "authors": "Haonan Wu, Rajarshi Banerjee, Indhumathi Venkatachalam, Daniel\n  Percy-Hughes and Praveen Chougale", "title": "Impact of Interventional Policies Including Vaccine on Covid-19\n  Propagation and Socio-Economic Factors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A novel coronavirus disease has emerged (later named COVID-19) and caused the\nworld to enter a new reality, with many direct and indirect factors influencing\nit. Some are human-controllable (e.g. interventional policies, mobility and the\nvaccine); some are not (e.g. the weather). We have sought to test how a change\nin these human-controllable factors might influence two measures: the number of\ndaily cases against economic impact. If applied at the right level and with\nup-to-date data to measure, policymakers would be able to make targeted\ninterventions and measure their cost. This study aims to provide a predictive\nanalytics framework to model, predict and simulate COVID-19 propagation and the\nsocio-economic impact of interventions intended to reduce the spread of the\ndisease such as policy and/or vaccine. It allows policymakers, government\nrepresentatives and business leaders to make better-informed decisions about\nthe potential effect of various interventions with forward-looking views via\nscenario planning. We have leveraged a recently launched open-source COVID-19\nbig data platform and used published research to find potentially relevant\nvariables (features) and leveraged in-depth data quality checks and analytics\nfor feature selection and predictions. An advanced machine learning pipeline\nhas been developed armed with a self-evolving model, deployed on a modern\nmachine learning architecture. It has high accuracy for trend prediction\n(back-tested with r-squared) and is augmented with interpretability for deeper\ninsights.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 15:08:07 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Wu", "Haonan", ""], ["Banerjee", "Rajarshi", ""], ["Venkatachalam", "Indhumathi", ""], ["Percy-Hughes", "Daniel", ""], ["Chougale", "Praveen", ""]]}, {"id": "2101.04081", "submitter": "Apostolos Gkatzionis", "authors": "Apostolos Gkatzionis, Stephen Burgess and Paul J. Newcombe", "title": "Statistical Methods for cis-Mendelian Randomization", "comments": "35 pages (29 main text + 6 supplement), 3 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.GN stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mendelian randomization is the use of genetic variants to assess the\nexistence of a causal relationship between a risk factor and an outcome of\ninterest. In this paper we focus on Mendelian randomization analyses with many\ncorrelated variants from a single gene region, and particularly on\ncis-Mendelian randomization studies which uses protein expression as a risk\nfactor. Such studies must rely on a small, curated set of variants from the\nstudied region; using all variants in the region requires inverting an\nill-conditioned genetic correlation matrix and results in numerically unstable\ncausal effect estimates. We review methods for variable selection and causal\neffect estimation in cis-Mendelian randomization, ranging from stepwise pruning\nand conditional analysis to principal components analysis, factor analysis and\nBayesian variable selection. In a simulation study, we show that the various\nmethods have a comparable performance in analyses with large sample sizes and\nstrong genetic instruments. However, when weak instrument bias is suspected,\nfactor analysis and Bayesian variable selection produce more reliable inference\nthan simple pruning approaches, which are often used in practice. We conclude\nby examining two case studies, assessing the effects of LDL-cholesterol and\nserum testosterone on coronary heart disease risk using variants in the HMGCR\nand SHBG gene regions respectively.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 18:23:04 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Gkatzionis", "Apostolos", ""], ["Burgess", "Stephen", ""], ["Newcombe", "Paul J.", ""]]}, {"id": "2101.04263", "submitter": "Shengchun Kong", "authors": "Shengchun Kong, Dominik Heinzmann, Sabine Lauer, Tian Lu", "title": "Weighted Approach for Estimating Effects in Principal Strata with\n  Missing Data for a Categorical Post-Baseline Variable in Randomized\n  Controlled Trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research was motivated by studying anti-drug antibody (ADA) formation\nand its potential impact on long-term benefit of a biologic treatment in a\nrandomized controlled trial, in which ADA status was not only unobserved in the\ncontrol arm but also in a subset of patients from the experimental treatment\narm. Recent literature considers the principal stratum estimand strategy to\nestimate treatment effect in groups of patients defined by an intercurrent\nstatus, i.e. in groups defined by a post-randomization variable only observed\nin one arm and potentially associated with the outcome. However, status\ninformation might be missing even for a non-negligible number of patients in\nthe experimental arm. For this setting, a novel weighted principal stratum\napproach is presented: Data from patients with missing intercurrent event\nstatus were re-weighted based on baseline covariates and additional\nlongitudinal information. A theoretical justification of the proposed approach\nis provided for different types of outcomes, and assumptions allowing for\ncausal conclusions on treatment effect are specified and investigated.\nSimulations demonstrated that the proposed method yielded valid inference and\nwas robust against certain violations of assumptions. The method was shown to\nperform well in a clinical study with ADA status as an intercurrent event.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 02:24:37 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Kong", "Shengchun", ""], ["Heinzmann", "Dominik", ""], ["Lauer", "Sabine", ""], ["Lu", "Tian", ""]]}, {"id": "2101.04334", "submitter": "Tong Shen", "authors": "Shuhao Jiao, Tong Shen, Zhaoxia Yu, Hernando Ombao", "title": "Change-point detection using spectral PCA for multivariate time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a two-stage approach Spec PC-CP to identify change points in\nmultivariate time series. In the first stage, we obtain a low-dimensional\nsummary of the high-dimensional time series by Spectral Principal Component\nAnalysis (Spec-PCA). In the second stage, we apply cumulative sum-type test on\nthe Spectral PCA component using a binary segmentation algorithm. Compared with\nexisting approaches, the proposed method is able to capture the lead-lag\nrelationship in time series. Our simulations demonstrate that the Spec PC-CP\nmethod performs significantly better than competing methods for detecting\nchange points in high-dimensional time series. The results on epileptic seizure\nEEG data and stock data also indicate that our new method can efficiently\n{detect} change points corresponding to the onset of the underlying events.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 07:32:39 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Jiao", "Shuhao", ""], ["Shen", "Tong", ""], ["Yu", "Zhaoxia", ""], ["Ombao", "Hernando", ""]]}, {"id": "2101.04426", "submitter": "Mirko Signorelli", "authors": "Mirko Signorelli, Pietro Spitali, Cristina Al-Khalili Szigyarto, The\n  MARK-MD Consortium, Roula Tsonaka", "title": "Penalized regression calibration: a method for the prediction of\n  survival outcomes using complex longitudinal and high-dimensional data", "comments": "Minor changes from version 1 (typos in legends of figures,\n  acknowledgements, etc.)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Longitudinal and high-dimensional measurements have become increasingly\ncommon in biomedical research. However, methods to predict survival outcomes\nusing covariates that are both longitudinal and high-dimensional are currently\nmissing. In this article we propose penalized regression calibration (PRC), a\nmethod that can be employed to predict survival in such situations.\n  PRC comprises three modelling steps: first, the trajectories described by the\nlongitudinal predictors are flexibly modelled through the specification of\nmultivariate latent process mixed models. Second, subject-specific summaries of\nthe longitudinal trajectories are derived from the fitted mixed effects models.\nThird, the time to event outcome is predicted using the subject-specific\nsummaries as covariates in a penalized Cox model.\n  To ensure a proper internal validation of the fitted PRC models, we\nfurthermore develop a cluster bootstrap optimism correction procedure (CBOCP)\nthat allows to correct for the optimistic bias of apparent measures of\npredictiveness.\n  After studying the behaviour of PRC via simulations, we conclude by\nillustrating an application of PRC to data from an observational study that\ninvolved patients affected by Duchenne muscular dystrophy (DMD), where the goal\nis predict time to loss of ambulation using longitudinal blood biomarkers.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 11:56:18 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 16:49:15 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Signorelli", "Mirko", ""], ["Spitali", "Pietro", ""], ["Szigyarto", "Cristina Al-Khalili", ""], ["Consortium", "The MARK-MD", ""], ["Tsonaka", "Roula", ""]]}, {"id": "2101.04428", "submitter": "Suhan Shetty", "authors": "Suhan Shetty, Jo\\~ao Silv\\'erio, and Sylvain Calinon", "title": "Ergodic Exploration using Tensor Train: Applications in Insertion Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.SY eess.SY math.DS math.OC stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In robotics, ergodic control extends the tracking principle by specifying a\nprobability distribution over an area to cover instead of a trajectory to\ntrack. The original problem is formulated as a spectral multiscale coverage\nproblem, typically requiring the spatial distribution to be decomposed as\nFourier series. This approach does not scale well to control problems requiring\nexploration in search space of more than 2 dimensions. To address this issue,\nwe propose the use of tensor trains, a recent low-rank tensor decomposition\ntechnique from the field of multilinear algebra. The proposed solution is\nefficient, both computationally and storage-wise, hence making it suitable for\nits online implementation in robotic systems. The approach is applied to a\npeg-in-hole insertion task requiring full 6D end-effector poses, implemented\nwith a 7-axis Franka Emika Panda robot. In this experiment, ergodic exploration\nallows the task to be achieved without requiring the use of force/torque\nsensors.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 11:58:32 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 19:08:16 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Shetty", "Suhan", ""], ["Silv\u00e9rio", "Jo\u00e3o", ""], ["Calinon", "Sylvain", ""]]}, {"id": "2101.04774", "submitter": "Peter Strong", "authors": "Peter Strong, Aditi Shenvi, Xuewen Yu, K.Nadia Papamichail, Henry P\n  Wynn, Jim Q Smith", "title": "Building A Bayesian Decision Support System for Evaluating COVID-19\n  Countermeasure Strategies", "comments": "31 pages including supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Decision making in the face of a disaster requires the consideration of\nseveral complex factors. In such cases, Bayesian multi-criteria decision\nanalysis provides a framework for decision making. In this paper, we present\nhow to construct a multi-attribute decision support system for choosing between\ncountermeasure strategies, such as lockdowns, designed to mitigate the effects\nof COVID-19. Such an analysis can evaluate both the short term and long term\nefficacy of various candidate countermeasures. The expected utility scores of a\ncountermeasure strategy capture the expected impact of the policies on health\noutcomes and other measures of population well-being. The broad methodologies\nwe use here have been established for some time. However, this application has\nmany novel elements to it: the pervasive uncertainty of the science; the\nnecessary dynamic shifts between regimes within each candidate suite of\ncountermeasures; and the fast moving stochastic development of the underlying\nthreat all present new challenges to this domain. Our methodology is\nillustrated by demonstrating in a simplified example how the efficacy of\nvarious strategies can be formally compared through balancing impacts of\ncountermeasures, not only on the short term (e.g. COVID-19 deaths) but the\nmedium to long term effects on the population (e.g increased poverty).\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 22:02:50 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 11:29:56 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Strong", "Peter", ""], ["Shenvi", "Aditi", ""], ["Yu", "Xuewen", ""], ["Papamichail", "K. Nadia", ""], ["Wynn", "Henry P", ""], ["Smith", "Jim Q", ""]]}, {"id": "2101.05009", "submitter": "Alexander Marx", "authors": "Alexander Marx and Lincen Yang and Matthijs van Leeuwen", "title": "Estimating Conditional Mutual Information for Discrete-Continuous\n  Mixtures using Multi-Dimensional Adaptive Histograms", "comments": "Extended version including supplementary material for main paper\n  which is (will be) published in: Proceedings of the SIAM International\n  Conference on Data Mining (SDM'21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating conditional mutual information (CMI) is an essential yet\nchallenging step in many machine learning and data mining tasks. Estimating CMI\nfrom data that contains both discrete and continuous variables, or even\ndiscrete-continuous mixture variables, is a particularly hard problem. In this\npaper, we show that CMI for such mixture variables, defined based on the\nRadon-Nikodym derivate, can be written as a sum of entropies, just like CMI for\npurely discrete or continuous data. Further, we show that CMI can be\nconsistently estimated for discrete-continuous mixture variables by learning an\nadaptive histogram model. In practice, we estimate such a model by iteratively\ndiscretizing the continuous data points in the mixture variables. To evaluate\nthe performance of our estimator, we benchmark it against state-of-the-art CMI\nestimators as well as evaluate it in a causal discovery setting.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 11:21:25 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Marx", "Alexander", ""], ["Yang", "Lincen", ""], ["van Leeuwen", "Matthijs", ""]]}, {"id": "2101.05240", "submitter": "Monica Alexander", "authors": "Monica Alexander, Michael Y.C. Chong, Marija Pejcinovska", "title": "Estimating causes of maternal death in data-sparse contexts", "comments": "This article has been removed by arXiv administrators as the\n  submitter did not have the authority to grant the license at the time of\n  submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding the underlying causes of maternal death across all regions of\nthe world is essential to inform policies and resource allocation to reduce the\nmortality burden. However, in many countries of the world there exists very\nlittle data on the causes of maternal death, and data that do exist do not\ncapture the entire population of risk. In this paper we present a Bayesian\nhierarchical multinomial model to estimate maternal cause of death\ndistributions globally, regionally and for all countries worldwide. The\nframework combines data from various sources to inform estimates, including\ndata from civil registration and vital systems, smaller-scale surveys and\nstudies, and high-quality data from confidential enquiries and surveillance\nsystems. The framework accounts of varying data quality and coverage, and\nallows for situations where one or more causes of death are missing. We\nillustrate the results of the model on three case study countries that have\ndifferent data availability situations: Canada, Nigeria and the United States.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 18:02:50 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 16:36:36 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Alexander", "Monica", ""], ["Chong", "Michael Y. C.", ""], ["Pejcinovska", "Marija", ""]]}, {"id": "2101.05350", "submitter": "Chih-Li Sung", "authors": "Chih-Li Sung", "title": "Estimating functional parameters for understanding the impact of weather\n  and government interventions on COVID-19 outbreak", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the coronavirus disease 2019 (COVID-19) has shown profound effects on\npublic health and the economy worldwide, it becomes crucial to assess the\nimpact on the virus transmission and develop effective strategies to address\nthe challenge. A new statistical model derived from the SIR epidemic model with\nfunctional parameters is proposed to understand the impact of weather and\ngovernment interventions on the virus spread and also provide the forecasts of\nCOVID-19 infections among eight metropolitan areas in the United States. The\nmodel uses Bayesian inference with Gaussian process priors to study the\nfunctional parameters nonparametrically, and sensitivity analysis is adopted to\ninvestigate the main and interaction effects of these factors. This analysis\nreveals several important results including the potential interaction effects\nbetween weather and government interventions, which shed new light on the\neffective strategies for policymakers to mitigate the COVID-19 outbreak.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 21:22:03 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Sung", "Chih-Li", ""]]}, {"id": "2101.05389", "submitter": "Panpan Zhang", "authors": "Yelie Yuan and Jun Yan and Panpan Zhang", "title": "Assortativity measures for weighted and directed networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assortativity measures the tendency of a vertex in a network being connected\nby other vertexes with respect to some vertex-specific features. Classical\nassortativity coefficients are defined for unweighted and undirected networks\nwith respect to vertex degree. We propose a class of assortativity coefficients\nthat capture the assortative characteristics and structure of weighted and\ndirected networks more precisely. The vertex-to-vertex strength correlation is\nused as an example, but the proposed measure can be applied to any pair of\nvertex-specific features. The effectiveness of the proposed measure is assessed\nthrough extensive simulations based on prevalent random network models in\ncomparison with existing assortativity measures. In application World\nInput-Ouput Networks,the new measures reveal interesting insights that would\nnot be obtained by using existing ones. An implementation is publicly available\nin a R package \"wdnet\".\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 23:27:28 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Yuan", "Yelie", ""], ["Yan", "Jun", ""], ["Zhang", "Panpan", ""]]}, {"id": "2101.05636", "submitter": "Pablo Dorta-Gonzalez", "authors": "Pablo Dorta-Gonz\\'alez, Sara M. Gonz\\'alez-Betancor, Mar\\'ia Isabel\n  Dorta-Gonz\\'alez", "title": "To what extent is researchers' data-sharing motivated by formal\n  mechanisms of recognition and credit?", "comments": "26 pages, 4 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data sharing by researchers is a centerpiece of Open Science principles and\nscientific progress. For a sample of 6019 researchers, we analyze the\nextent/frequency of their data sharing. Specifically, the relationship with the\nfollowing four variables: how much they value data citations, the extent to\nwhich their data-sharing activities are formally recognized, their perceptions\nof whether sufficient credit is awarded for data sharing, and the reported\nextent to which data citations motivate their data sharing. In addition, we\nanalyze the extent to which researchers have reused openly accessible data, as\nwell as how data sharing varies by professional age-cohort, and its\nrelationship to the value they place on data citations. Furthermore, we\nconsider most of the explanatory variables simultaneously by estimating a\nmultiple linear regression that predicts the extent/frequency of their data\nsharing. We use the dataset of the State of Open Data Survey 2019 by Springer\nNature and Digital Science. Results do allow us to conclude that a desire for\nrecognition/credit is a major incentive for data sharing. Thus, the possibility\nof receiving data citations is highly valued when sharing data, especially\namong younger researchers, irrespective of the frequency with which it is\npracticed. Finally, the practice of data sharing was found to be more prevalent\nat late research career stages, despite this being when citations are less\nvalued and have a lower motivational impact. This could be due to the fact that\nlater-career researchers may benefit less from keeping their data private.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 14:45:33 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Dorta-Gonz\u00e1lez", "Pablo", ""], ["Gonz\u00e1lez-Betancor", "Sara M.", ""], ["Dorta-Gonz\u00e1lez", "Mar\u00eda Isabel", ""]]}, {"id": "2101.05677", "submitter": "Ashwin Misra", "authors": "Ashwin Misra, Ankit Mittal, Vihaan Misra and Deepanshu Pandey", "title": "Improving non-deterministic uncertainty modelling in Industry 4.0\n  scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT cs.AI stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The latest Industrial revolution has helped industries in achieving very high\nrates of productivity and efficiency. It has introduced data aggregation and\ncyber-physical systems to optimize planning and scheduling. Although,\nuncertainty in the environment and the imprecise nature of human operators are\nnot accurately considered for into the decision making process. This leads to\ndelays in consignments and imprecise budget estimations. This widespread\npractice in the industrial models is flawed and requires rectification. Various\nother articles have approached to solve this problem through stochastic or\nfuzzy set model methods. This paper presents a comprehensive method to\nlogically and realistically quantify the non-deterministic uncertainty through\nprobabilistic uncertainty modelling. This method is applicable on virtually all\nIndustrial data sets, as the model is self adjusting and uses\nepsilon-contamination to cater to limited or incomplete data sets. The results\nare numerically validated through an Industrial data set in Flanders, Belgium.\nThe data driven results achieved through this robust scheduling method\nillustrate the improvement in performance.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 23:17:55 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Misra", "Ashwin", ""], ["Mittal", "Ankit", ""], ["Misra", "Vihaan", ""], ["Pandey", "Deepanshu", ""]]}, {"id": "2101.05769", "submitter": "Marc Vidal", "authors": "Marc Vidal, Mattia Rosso, Ana M. Aguilera", "title": "Bi-Smoothed Functional Independent Component Analysis for EEG Artifact\n  Removal", "comments": "9 pages, 3 figures, 1 table", "journal-ref": "Mathematics 9, no. 11: 1243 (2021)", "doi": "10.3390/math9111243", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivated by mapping adverse artifactual events caused by body movements in\nelectroencephalographic (EEG) signals, we present a functional independent\ncomponent analysis based on the spectral decomposition of the kurtosis operator\nof a smoothed principal component expansion. A discrete roughness penalty is\nintroduced in the orthonormality constraint of the covariance eigenfunctions in\norder to obtain the smoothed basis for the proposed independent component\nmodel. To select the tuning parameters, a cross-validation method that\nincorporates shrinkage is used to enhance the performance on functional\nrepresentations with large basis dimension. This method provides an estimation\nstrategy to determine the penalty parameter and the optimal number of\ncomponents. Our independent component approach is applied to real EEG data to\nestimate genuine brain potentials from a contaminated signal. As a result, it\nis possible to control high-frequency remnants of neural origin overlapping\nartifactual sources to optimize their removal from the signal. An R package\nimplementing our methods is available at CRAN.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 18:20:20 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 08:53:45 GMT"}, {"version": "v3", "created": "Thu, 27 May 2021 17:58:52 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Vidal", "Marc", ""], ["Rosso", "Mattia", ""], ["Aguilera", "Ana M.", ""]]}, {"id": "2101.05808", "submitter": "Adam Spannaus", "authors": "Adam Spannaus, Kody J. H. Law, Piotr Luszczek, Farzana Nasrin, Cassie\n  Putman Micucci, Peter K. Liaw, Louis J. Santodonato, David J. Keffer,\n  Vasileios Maroulas", "title": "Materials Fingerprinting Classification", "comments": null, "journal-ref": null, "doi": "10.1016/j.cpc.2021.108019", "report-no": null, "categories": "cond-mat.mtrl-sci cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Significant progress in many classes of materials could be made with the\navailability of experimentally-derived large datasets composed of atomic\nidentities and three-dimensional coordinates. Methods for visualizing the local\natomic structure, such as atom probe tomography (APT), which routinely generate\ndatasets comprised of millions of atoms, are an important step in realizing\nthis goal. However, state-of-the-art APT instruments generate noisy and sparse\ndatasets that provide information about elemental type, but obscure atomic\nstructures, thus limiting their subsequent value for materials discovery. The\napplication of a materials fingerprinting process, a machine learning algorithm\ncoupled with topological data analysis, provides an avenue by which\nhere-to-fore unprecedented structural information can be extracted from an APT\ndataset. As a proof of concept, the material fingerprint is applied to\nhigh-entropy alloy APT datasets containing body-centered cubic (BCC) and\nface-centered cubic (FCC) crystal structures. A local atomic configuration\ncentered on an arbitrary atom is assigned a topological descriptor, with which\nit can be characterized as a BCC or FCC lattice with near perfect accuracy,\ndespite the inherent noise in the dataset. This successful identification of a\nfingerprint is a crucial first step in the development of algorithms which can\nextract more nuanced information, such as chemical ordering, from existing\ndatasets of complex materials.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 20:32:38 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Spannaus", "Adam", ""], ["Law", "Kody J. H.", ""], ["Luszczek", "Piotr", ""], ["Nasrin", "Farzana", ""], ["Micucci", "Cassie Putman", ""], ["Liaw", "Peter K.", ""], ["Santodonato", "Louis J.", ""], ["Keffer", "David J.", ""], ["Maroulas", "Vasileios", ""]]}, {"id": "2101.05928", "submitter": "Mingao Yuan", "authors": "Mingao Yuan and Qian Wen", "title": "A practical test for a planted community in heterogeneous networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the fundamental task in graph data mining is to find a planted\ncommunity(dense subgraph), which has wide application in biology, finance, spam\ndetection and so on. For a real network data, the existence of a dense subgraph\nis generally unknown. Statistical tests have been devised to testing the\nexistence of dense subgraph in a homogeneous random graph. However, many\nnetworks present extreme heterogeneity, that is, the degrees of nodes or\nvertexes don't concentrate on a typical value. The existing tests designed for\nhomogeneous random graph are not straightforwardly applicable to the\nheterogeneous case. Recently, scan test was proposed for detecting a dense\nsubgraph in heterogeneous(inhomogeneous) graph(\\cite{BCHV19}). However, the\ncomputational complexity of the scan test is generally not polynomial in the\ngraph size, which makes the test impractical for large or moderate networks. In\nthis paper, we propose a polynomial-time test that has the standard normal\ndistribution as the null limiting distribution. The power of the test is\ntheoretically investigated and we evaluate the performance of the test by\nsimulation and real data example.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 01:34:14 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Yuan", "Mingao", ""], ["Wen", "Qian", ""]]}, {"id": "2101.06188", "submitter": "Jingchen Hu", "authors": "Jingchen Hu, Terrance D. Savitsky, Matthew R. Williams", "title": "Private Tabular Survey Data Products through Synthetic Microdata\n  Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose three synthetic microdata approaches to generate private tabular\nsurvey data products for public release. We adapt a disclosure risk\nbased-weighted pseudo posterior mechanism to survey data with a focus on\nproducing tabular products under a formal privacy guarantee. Two of our\napproaches synthesize the observed sample distribution of the outcome and\nsurvey weights, jointly, such that both quantities together possess a\nprobabilistic differential privacy guarantee. The privacy-protected outcome and\nsampling weights are used to construct tabular cell estimates and associated\nstandard errors to correct for survey sampling bias. The third approach\nsynthesizes the population distribution from the observed sample under a pseudo\nposterior construction that treats survey sampling weights as fixed to correct\nthe sample likelihood to approximate that for the population. Each by-record\nsampling weight in the pseudo posterior is, in turn, multiplied by the\nassociated privacy, risk-based weight for that record to create a composite\npseudo posterior mechanism that both corrects for survey bias and provides a\nprivacy guarantee for the observed sample. Through a simulation study and a\nreal data application to the Survey of Doctorate Recipients public use file, we\ndemonstrate that our three microdata synthesis approaches to construct tabular\nproducts provide superior utility preservation as compared to the\nadditive-noise approach of the Laplace Mechanism. Moreover, all our approaches\nallow the release of microdata to the public, enabling additional analyses at\nno extra privacy cost.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 15:56:49 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Hu", "Jingchen", ""], ["Savitsky", "Terrance D.", ""], ["Williams", "Matthew R.", ""]]}, {"id": "2101.06211", "submitter": "Prateek Bansal", "authors": "Thijs Dekker and Prateek Bansal", "title": "A Bayesian perspective on sampling of alternatives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we apply a Bayesian perspective to sampling of alternatives\nfor multinomial logit (MNL) and mixed multinomial logit (MMNL) models. We find\nthree theoretical results -- i) McFadden's correction factor under the uniform\nsampling protocol can be transferred to the Bayesian context in MNL; ii) the\nuniform sampling protocol minimises the loss in information on the parameters\nof interest (i.e. the kernel of the posterior density) and thereby has\ndesirable small sample properties in MNL; and iii) our theoretical results\nextend to Bayesian MMNL models using data augmentation. Notably, sampling of\nalternatives in Bayesian MMNL models does not require the inclusion of the\nadditional correction factor, as identified by Guevara and Ben-Akiva (2013a) in\nclassical settings. Accordingly, due to desirable small and large sample\nproperties, uniform sampling is the recommended sampling protocol in MNL and\nMMNL, irrespective of the estimation framework selected.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 16:59:02 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Dekker", "Thijs", ""], ["Bansal", "Prateek", ""]]}, {"id": "2101.06415", "submitter": "Guangxing Wang", "authors": "Guangxing Wang, Sisheng Liu, Fang Han and Chongzhi Di", "title": "Robust Functional Principal Component Analysis via Functional Pairwise\n  Spatial Signs", "comments": "23 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Functional principal component analysis (FPCA) has been widely used to\ncapture major modes of variation and reduce dimensions in functional data\nanalysis. However, standard FPCA based on the sample covariance estimator does\nnot work well in the presence of outliers. To address this challenge, a new\nrobust functional principal component analysis approach based on the functional\npairwise spatial sign (PASS) operator, termed PASS FPCA, is introduced where we\npropose estimation procedures for both eigenfunctions and eigenvalues with and\nwithout measurement error. Compared to existing robust FPCA methods, the\nproposed one requires weaker distributional assumptions to conserve the\neigenspace of the covariance function. In particular, a class of distributions\ncalled the weakly functional coordinate symmetric (weakly FCS) is introduced\nthat allows for severe asymmetry and is strictly larger than the functional\nelliptical distribution class, the latter of which has been well used in the\nrobust statistics literature. The robustness of the PASS FPCA is demonstrated\nvia simulation studies and analyses of accelerometry data from a large-scale\nepidemiological study of physical activity on older women that partly motivates\nthis work.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 09:24:25 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Wang", "Guangxing", ""], ["Liu", "Sisheng", ""], ["Han", "Fang", ""], ["Di", "Chongzhi", ""]]}, {"id": "2101.06458", "submitter": "Gian Maria Campedelli", "authors": "Gian Maria Campedelli and Maria Rita D'Orsogna", "title": "Temporal Clustering of Disorder Events During the COVID-19 Pandemic", "comments": "37 pages, 16 figures", "journal-ref": "PLOS ONE, 16(4), e0250433 (2021)", "doi": "10.1371/journal.pone.0250433", "report-no": null, "categories": "physics.soc-ph cs.LG econ.GN q-fin.EC stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The COVID-19 pandemic has unleashed multiple public health, socio-economic,\nand institutional crises. Measures taken to slow the spread of the virus have\nfostered significant strain between authorities and citizens, leading to waves\nof social unrest and anti-government demonstrations. We study the temporal\nnature of pandemic-related disorder events as tallied by the \"COVID-19 Disorder\nTracker\" initiative by focusing on the three countries with the largest number\nof incidents, India, Israel, and Mexico. By fitting Poisson and Hawkes\nprocesses to the stream of data, we find that disorder events are\ninter-dependent and self-excite in all three countries. Geographic clustering\nconfirms these features at the subnational level, indicating that nationwide\ndisorders emerge as the convergence of meso-scale patterns of self-excitation.\nConsiderable diversity is observed among countries when computing correlations\nof events between subnational clusters; these are discussed in the context of\nspecific political, societal and geographic characteristics. Israel, the most\nterritorially compact and where large scale protests were coordinated in\nresponse to government lockdowns, displays the largest reactivity and the\nshortest period of influence following an event, as well as the strongest\nnationwide synchrony. In Mexico, where complete lockdown orders were never\nmandated, reactivity and nationwide synchrony are lowest. Our work highlights\nthe need for authorities to promote local information campaigns to ensure that\nlivelihoods and virus containment policies are not perceived as mutually\nexclusive.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 15:34:42 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 09:58:09 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Campedelli", "Gian Maria", ""], ["D'Orsogna", "Maria Rita", ""]]}, {"id": "2101.06631", "submitter": "Yuling Yao", "authors": "Yuling Yao, Rajib Mozumder, Benjamin Bostick, Brian Mailloux, Charles\n  F. Harvey, Andrew Gelman, Alexander van Geen", "title": "Making the most of imprecise measurements: Changing patterns of arsenic\n  concentrations in shallow wells of Bangladesh from laboratory and field data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Millions of people in Bangladesh drink well water contaminated with arsenic.\nDespite the severity of this heath crisis, little is known about the extent to\nwhich groundwater arsenic concentrations change over time: Are concentrations\ngenerally rising, or is arsenic being flushed out of aquifers? Are spatially\npatterns of high and low concentrations across wells homogenizing over time, or\nare these spatial gradients becoming more pronounced? To address these\nquestions, we analyze a large set of arsenic concentrations that were sampled\nwithin a 25 km$^2$ area of Bangladesh over time. We compare two blanket survey\ncollected in 2000/2001 and 2012/2013 from the same villages but relying on a\nlargely different set of wells. The early set consists of 4574 accurate\nlaboratory measurements, but the later set poses a challenge for analysis\nbecause it is composed of 8229 less accurate categorical measurements conducted\nin the field with a kit. We construct a Bayesian model that jointly calibrates\nthe measurement errors, applies spatial smoothing, and describes the\nspatiotemporal dynamic with a diffusion-like process model. Our statistical\nanalysis reveals that arsenic concentrations change over time and that their\nmean dropped from 110 to 96 $\\mu$g/L over 12 years, although one quarter of\nindividual wells are inferred to see an increase. The largest decreases\noccurred at the wells with locally high concentrations where the estimated\nLaplacian indicated that the arsenic surface was strongly concave. However,\nwell with initially low concentrations were unlikely to be contaminated by\nnearby high concentration wells over a decade. We validate the model using a\nposterior predictive check on an external subset of laboratory measurements\nfrom the same 271 wells in the same study area available for 2000, 2014, and\n2015.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 09:35:55 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Yao", "Yuling", ""], ["Mozumder", "Rajib", ""], ["Bostick", "Benjamin", ""], ["Mailloux", "Brian", ""], ["Harvey", "Charles F.", ""], ["Gelman", "Andrew", ""], ["van Geen", "Alexander", ""]]}, {"id": "2101.06717", "submitter": "S\\'andor Baran", "authors": "Benedikt Schulz, Mehrez El Ayari, Sebastian Lerch and S\\'andor Baran", "title": "Post-processing numerical weather prediction ensembles for probabilistic\n  solar irradiance forecasting", "comments": "32 pages, 16 figures, 1 table", "journal-ref": "Solar Energy 220 (2021), 1016-1031", "doi": "10.1016/j.solener.2021.03.023", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In order to enable the transition towards renewable energy sources,\nprobabilistic energy forecasting is of critical importance for incorporating\nvolatile power sources such as solar energy into the electrical grid. Solar\nenergy forecasting methods often aim to provide probabilistic predictions of\nsolar irradiance. In particular, many hybrid approaches combine physical\ninformation from numerical weather prediction models with statistical methods.\nEven though the physical models can provide useful information at intra-day and\nday-ahead forecast horizons, ensemble weather forecasts from multiple model\nruns are often not calibrated and show systematic biases. We propose a\npost-processing model for ensemble weather predictions of solar irradiance at\ntemporal resolutions between 30 minutes and 6 hours. The proposed models\nprovide probabilistic forecasts in the form of a censored logistic probability\ndistribution for lead times up to 5 days and are evaluated in two case studies\ncovering distinct physical models, geographical regions, temporal resolutions,\nand types of solar irradiance. We find that post-processing consistently and\nsignificantly improves the forecast performance of the ensemble predictions for\nlead times up to at least 48 hours and is well able to correct the systematic\nlack of calibration.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 17:10:20 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 12:13:55 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Schulz", "Benedikt", ""], ["Ayari", "Mehrez El", ""], ["Lerch", "Sebastian", ""], ["Baran", "S\u00e1ndor", ""]]}, {"id": "2101.06755", "submitter": "Daniel Eck", "authors": "Ellen S. Fireman, Zachary S. Donnini, Daniel J. Eck, Michael B.\n  Weissman", "title": "Are in-person lectures beneficial for all students? A Study of a Large\n  Statistics Class", "comments": "Supplementary materials are available upon request", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over 1000 students over four semesters were given the option of taking an\nintroductory statistics class either by in-person attendance in lectures,\naugmented by online recorded lectures, or by taking the same class without the\nin-person lectures. The all-online students did slightly better on\ncomputer-graded exams. The causal effect of choosing only online lectures was\nestimated by adjusting for potential confounders using four methods. The four\nnearly identical point estimates remained positive but were small and not\nstatistically significant. No statistically significant differences were found\nin preliminary comparisons of effects on females/males, U.S./non-U.S. citizens,\nfreshmen/non-freshman, and lower-scoring/higher-scoring math ACT groups.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 19:16:42 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 14:00:47 GMT"}, {"version": "v3", "created": "Sun, 25 Jul 2021 16:03:10 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Fireman", "Ellen S.", ""], ["Donnini", "Zachary S.", ""], ["Eck", "Daniel J.", ""], ["Weissman", "Michael B.", ""]]}, {"id": "2101.06813", "submitter": "Zhengchun Liu", "authors": "Jiali Wang, Zhengchun Liu, Ian Foster, Won Chang, Rajkumar Kettimuthu,\n  Rao Kotamarthi", "title": "Fast and accurate learned multiresolution dynamical downscaling for\n  precipitation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study develops a neural network-based approach for emulating\nhigh-resolution modeled precipitation data with comparable statistical\nproperties but at greatly reduced computational cost. The key idea is to use\ncombination of low- and high- resolution simulations to train a neural network\nto map from the former to the latter. Specifically, we define two types of\nCNNs, one that stacks variables directly and one that encodes each variable\nbefore stacking, and we train each CNN type both with a conventional loss\nfunction, such as mean square error (MSE), and with a conditional generative\nadversarial network (CGAN), for a total of four CNN variants. We compare the\nfour new CNN-derived high-resolution precipitation results with precipitation\ngenerated from original high resolution simulations, a bilinear interpolater\nand the state-of-the-art CNN-based super-resolution (SR) technique. Results\nshow that the SR technique produces results similar to those of the bilinear\ninterpolator with smoother spatial and temporal distributions and smaller data\nvariabilities and extremes than the original high resolution simulations. While\nthe new CNNs trained by MSE generate better results over some regions than the\ninterpolator and SR technique do, their predictions are still not as close as\nthe original high resolution simulations. The CNNs trained by CGAN generate\nmore realistic and physically reasonable results, better capturing not only\ndata variability in time and space but also extremes such as intense and\nlong-lasting storms. The new proposed CNN-based downscaling approach can\ndownscale precipitation from 50~km to 12~km in 14~min for 30~years once the\nnetwork is trained (training takes 4~hours using 1~GPU), while the conventional\ndynamical downscaling would take 1~month using 600 CPU cores to generate\nsimulations at the resolution of 12~km over contiguous United States.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 00:25:04 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Wang", "Jiali", ""], ["Liu", "Zhengchun", ""], ["Foster", "Ian", ""], ["Chang", "Won", ""], ["Kettimuthu", "Rajkumar", ""], ["Kotamarthi", "Rao", ""]]}, {"id": "2101.06967", "submitter": "Alexandre Thiery", "authors": "Atin Ghosh and Alexandre H. Thiery", "title": "On Data-Augmentation and Consistency-Based Semi-Supervised Learning", "comments": null, "journal-ref": "ICLR 2021", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently proposed consistency-based Semi-Supervised Learning (SSL) methods\nsuch as the $\\Pi$-model, temporal ensembling, the mean teacher, or the virtual\nadversarial training, have advanced the state of the art in several SSL tasks.\nThese methods can typically reach performances that are comparable to their\nfully supervised counterparts while using only a fraction of labelled examples.\nDespite these methodological advances, the understanding of these methods is\nstill relatively limited. In this text, we analyse (variations of) the\n$\\Pi$-model in settings where analytically tractable results can be obtained.\nWe establish links with Manifold Tangent Classifiers and demonstrate that the\nquality of the perturbations is key to obtaining reasonable SSL performances.\nImportantly, we propose a simple extension of the Hidden Manifold Model that\nnaturally incorporates data-augmentation schemes and offers a framework for\nunderstanding and experimenting with SSL methods.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 10:12:31 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Ghosh", "Atin", ""], ["Thiery", "Alexandre H.", ""]]}, {"id": "2101.07009", "submitter": "Ali Salloum", "authors": "Ali Salloum, Ted Hsuan Yun Chen, Mikko Kivel\\\"a", "title": "Separating Controversy from Noise: Comparison and Normalization of\n  Structural Polarization Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Quantifying the amount of polarization is crucial for understanding and\nstudying political polarization in political and social systems. Several\nmethods are used commonly to measure polarization in social networks by purely\ninspecting their structure. We analyse eight of such methods and show that all\nof them yield high polarization scores even for random networks with similar\ndensity and degree distributions to typical real-world networks. Further, some\nof the methods are sensitive to degree distributions and relative sizes of the\npolarized groups. We propose normalization to the existing scores and a minimal\nset of tests that a score should pass in order for it to be suitable for\nseparating polarized networks from random noise. The performance of the scores\nincreased by 38%-220% after normalization in a classification task of 203\nnetworks. Further, we find that the choice of method is not as important as\nnormalization, after which most of the methods have better performance than the\nbest-performing method before normalization. This work opens up the possibility\nto critically assess and compare the features and performance of structural\npolarization methods.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 11:18:32 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Salloum", "Ali", ""], ["Chen", "Ted Hsuan Yun", ""], ["Kivel\u00e4", "Mikko", ""]]}, {"id": "2101.07029", "submitter": "Felix Bestehorn", "authors": "Felix Bestehorn and Maike Bestehorn and Christian Kirches", "title": "A deterministic matching method for exact matchings to compare the\n  outcome of different interventions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Statistical matching methods are widely used in the social and health\nsciences to estimate causal effects using observational data. Often the\nobjective is to find comparable groups with similar covariate distributions in\na dataset, with the aim to reduce bias in a random experiment. We aim to\ndevelop a foundation for deterministic methods which provide results with low\nbias, while retaining interpretability. The proposed method matches on the\ncovariates and calculates all possible maximal exact matchesfor a given dataset\nwithout adding numerical errors. Notable advantages of our method over existing\nmatching algorithms are that all available information for exact matches is\nused, no additional bias is introduced, it can be combined with other matching\nmethods for inexact matching to reduce pruning and that the result is\ncalculated in a fast and deterministic way. For a given dataset the result is\ntherefore provably unique for exact matches in the mathematical sense. We\nprovide proofs, instructions for implementation as well as a numerical example\ncalculated for comparison on a complete survey.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 12:12:55 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Bestehorn", "Felix", ""], ["Bestehorn", "Maike", ""], ["Kirches", "Christian", ""]]}, {"id": "2101.07167", "submitter": "Jordan Richards", "authors": "Jordan Richards and Jennifer L. Wadsworth", "title": "Spatial deformation for non-stationary extremal dependence", "comments": "41 pages, 10 figures", "journal-ref": "Environmetrics, e2671 (2021)", "doi": "10.1002/env.2671", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modelling the extremal dependence structure of spatial data is considerably\neasier if that structure is stationary. However, for data observed over large\nor complicated domains, non-stationarity will often prevail. Current methods\nfor modelling non-stationarity in extremal dependence rely on models that are\neither computationally difficult to fit or require prior knowledge of\ncovariates. Sampson and Guttorp (1992) proposed a simple technique for handling\nnon-stationarity in spatial dependence by smoothly mapping the sampling\nlocations of the process from the original geographical space to a latent space\nwhere stationarity can be reasonably assumed. We present an extension of this\nmethod to a spatial extremes framework by considering least squares\nminimisation of pairwise theoretical and empirical extremal dependence\nmeasures. Along with some practical advice on applying these deformations, we\nprovide a detailed simulation study in which we propose three spatial processes\nwith varying degrees of non-stationarity in their extremal and central\ndependence structures. The methodology is applied to Australian summer\ntemperature extremes and UK precipitation to illustrate its efficacy compared\nto a naive modelling approach.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 17:16:47 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Richards", "Jordan", ""], ["Wadsworth", "Jennifer L.", ""]]}, {"id": "2101.07197", "submitter": "Ted Hsuan Yun Chen", "authors": "Christian S. Schmid, Ted Hsuan Yun Chen, Bruce A. Desmarais", "title": "Generative Dynamics of Supreme Court Citations: Analysis with a New\n  Statistical Network Model", "comments": "34 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.DL physics.soc-ph", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The significance and influence of US Supreme Court majority opinions derive\nin large part from opinions' roles as precedents for future opinions. A growing\nbody of literature seeks to understand what drives the use of opinions as\nprecedents through the study of Supreme Court case citation patterns. We raise\ntwo limitations of existing work on Supreme Court citations. First, dyadic\ncitations are typically aggregated to the case level before they are analyzed.\nSecond, citations are treated as if they arise independently. We present a\nmethodology for studying citations between Supreme Court opinions at the dyadic\nlevel, as a network, that overcomes these limitations. This methodology -- the\ncitation exponential random graph model, for which we provide user-friendly\nsoftware -- enables researchers to account for the effects of case\ncharacteristics and complex forms of network dependence in citation formation.\nWe then analyze a network that includes all Supreme Court cases decided between\n1950 and 2015. We find evidence for dependence processes, including\nreciprocity, transitivity, and popularity. The dependence effects are as\nsubstantively and statistically significant as the effects of exogenous\ncovariates, indicating that models of Supreme Court citation should incorporate\nboth the effects of case characteristics and the structure of past citations.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 17:43:57 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Schmid", "Christian S.", ""], ["Chen", "Ted Hsuan Yun", ""], ["Desmarais", "Bruce A.", ""]]}, {"id": "2101.07329", "submitter": "Lucy D'Agostino McGowan", "authors": "Lucy D'Agostino McGowan, Kyra H. Grantz, and Eleanor Murray", "title": "Quantifying Uncertainty in Infectious Disease Mechanistic Models", "comments": "American Journal of Epidemiology, 2021", "journal-ref": null, "doi": "10.1093/aje/kwab013", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This primer describes the statistical uncertainty in mechanistic models and\nprovides R code to quantify it. We begin with an overview of mechanistic models\nfor infectious disease, and then describe the sources of statistical\nuncertainty in the context of a case study on SARS-CoV-2. We describe the\nstatistical uncertainty as belonging to three categories: data uncertainty,\nstochastic uncertainty, and structural uncertainty. We demonstrate how to\naccount for each of these via statistical uncertainty measures and sensitivity\nanalyses broadly, as well as in a specific case study on estimating the basic\nreproductive number, $R_0$, for SARS-CoV-2.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 21:12:33 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 20:02:59 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["McGowan", "Lucy D'Agostino", ""], ["Grantz", "Kyra H.", ""], ["Murray", "Eleanor", ""]]}, {"id": "2101.07357", "submitter": "David Lim", "authors": "David K. Lim, Naim U. Rashid, Junier B. Oliva, Joseph G. Ibrahim", "title": "Handling Non-ignorably Missing Features in Electronic Health Records\n  Data Using Importance-Weighted Autoencoders", "comments": "37 pages, 3 figures, 3 tables, under review (Journal of the American\n  Statistical Association)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Electronic Health Records (EHRs) are commonly used to investigate\nrelationships between patient health information and outcomes. Deep learning\nmethods are emerging as powerful tools to learn such relationships, given the\ncharacteristic high dimension and large sample size of EHR datasets. The\nPhysionet 2012 Challenge involves an EHR dataset pertaining to 12,000 ICU\npatients, where researchers investigated the relationships between clinical\nmeasurements, and in-hospital mortality. However, the prevalence and complexity\nof missing data in the Physionet data present significant challenges for the\napplication of deep learning methods, such as Variational Autoencoders (VAEs).\nAlthough a rich literature exists regarding the treatment of missing data in\ntraditional statistical models, it is unclear how this extends to deep learning\narchitectures. To address these issues, we propose a novel extension of VAEs\ncalled Importance-Weighted Autoencoders (IWAEs) to flexibly handle Missing Not\nAt Random (MNAR) patterns in the Physionet data. Our proposed method models the\nmissingness mechanism using an embedded neural network, eliminating the need to\nspecify the exact form of the missingness mechanism a priori. We show that the\nuse of our method leads to more realistic imputed values relative to the\nstate-of-the-art, as well as significant differences in fitted downstream\nmodels for mortality.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 22:53:29 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 20:05:41 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Lim", "David K.", ""], ["Rashid", "Naim U.", ""], ["Oliva", "Junier B.", ""], ["Ibrahim", "Joseph G.", ""]]}, {"id": "2101.07374", "submitter": "Kaiqiong Zhao", "authors": "Kaiqiong Zhao, Karim Oualkacha, Lajmi Lakhal-Chaieb, Aur\\'elie Labbe,\n  Kathleen Klein, Sasha Bernatsky, Marie Hudson, In\\'es Colmegna, Celia M.T.\n  Greenwood", "title": "Detecting differentially methylated regions in bisulfite sequencing data\n  using quasi-binomial mixed models with smooth covariate effect estimates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying disease-associated changes in DNA methylation can help to gain a\nbetter understanding of disease etiology. Bisulfite sequencing technology\nallows the generation of methylation profiles at single base of DNA. We\npreviously developed a method for estimating smooth covariate effects and\nidentifying differentially methylated regions (DMRs) from bisulfite sequencing\ndata, which copes with experimental errors and variable read depths; this\nmethod utilizes the binomial distribution to characterize the variability in\nthe methylated counts. However, bisulfite sequencing data frequently include\nlow-count integers and can exhibit over or under dispersion relative to the\nbinomial distribution. We present a substantial improvement to our previous\nwork by proposing a quasi-likelihood-based regional testing approach which\naccounts for multiplicative and additive sources of dispersion. We demonstrate\nthe theoretical properties of the resulting tests, as well as their marginal\nand conditional interpretations. Simulations show that the proposed method\nprovides correct inference for smooth covariate effects and captures the major\nmethylation patterns with excellent power.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 23:29:27 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Zhao", "Kaiqiong", ""], ["Oualkacha", "Karim", ""], ["Lakhal-Chaieb", "Lajmi", ""], ["Labbe", "Aur\u00e9lie", ""], ["Klein", "Kathleen", ""], ["Bernatsky", "Sasha", ""], ["Hudson", "Marie", ""], ["Colmegna", "In\u00e9s", ""], ["Greenwood", "Celia M. T.", ""]]}, {"id": "2101.07392", "submitter": "Ellicott Matthay", "authors": "Ellicott C. Matthay, Erin Hagan, Laura M. Gottlieb, May Lynn Tan,\n  David Vlahov, Nancy Adler, M. Maria Glymour", "title": "Powering population health research: Considerations for plausible and\n  actionable effect sizes", "comments": "24 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Evidence for Action (E4A), a signature program of the Robert Wood Johnson\nFoundation, funds investigator-initiated research on the impacts of social\nprograms and policies on population health and health inequities. Across\nthousands of letters of intent and full proposals E4A has received since 2015,\none of the most common methodological challenges faced by applicants is\nselecting realistic effect sizes to inform power and sample size calculations.\nE4A prioritizes health studies that are both (1) adequately powered to detect\neffect sizes that may reasonably be expected for the given intervention and (2)\nlikely to achieve intervention effects sizes that, if demonstrated, correspond\nto actionable evidence for population health stakeholders. However, little\nguidance exists to inform the selection of effect sizes for population health\nresearch proposals. We draw on examples of five rigorously evaluated population\nhealth interventions. These examples illustrate considerations for selecting\nrealistic and actionable effect sizes as inputs to power and sample size\ncalculations for research proposals to study population health interventions.\nWe show that plausible effects sizes for population health inteventions may be\nsmaller than commonly cited guidelines suggest. Effect sizes achieved with\npopulation health interventions depend on the characteristics of the\nintervention, the target population, and the outcomes studied. Population\nhealth impact depends on the proportion of the population receiving the\nintervention. When adequately powered, even studies of interventions with small\neffect sizes can offer valuable evidence to inform population health if such\ninterventions can be implemented broadly. Demonstrating the effectiveness of\nsuch interventions, however, requires large sample sizes.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 00:56:30 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Matthay", "Ellicott C.", ""], ["Hagan", "Erin", ""], ["Gottlieb", "Laura M.", ""], ["Tan", "May Lynn", ""], ["Vlahov", "David", ""], ["Adler", "Nancy", ""], ["Glymour", "M. Maria", ""]]}, {"id": "2101.07398", "submitter": "Jacob Spertus", "authors": "Jacob V Spertus", "title": "Optimal sampling and assay for soil organic carbon estimation", "comments": "30 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The world needs around 150 Pg of negative carbon emissions to mitigate\nclimate change. Global soils may provide a stable, sizeable reservoir to help\nachieve this goal by sequestering atmospheric carbon dioxide as soil organic\ncarbon (SOC). In turn, SOC can support healthy soils and provide a multitude of\necosystem benefits. To support SOC sequestration, researchers and policy makers\nmust be able to precisely measure the amount of SOC in a given plot of land.\nSOC measurement is typically accomplished by taking soil cores selected at\nrandom from the plot under study, mixing (compositing) some of them together,\nand analyzing (assaying) the composited samples in a laboratory. Compositing\nreduces assay costs, which can be substantial. Taking samples is also costly.\nGiven uncertainties and costs in both sampling and assay along with a desired\nestimation precision, there is an optimal composite size that will minimize the\nbudget required to achieve that precision. Conversely, given a fixed budget,\nthere is a composite size that minimizes uncertainty. In this paper, we\ndescribe and formalize sampling and assay for SOC and derive the optima for\nthree commonly used assay methods: dry combustion in an elemental analyzer,\nloss-on-ignition, and mid-infrared spectroscopy. We demonstrate the utility of\nthis approach using data from a soil survey conducted in California. We give\nrecommendations for practice and provide software to implement our framework.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 01:23:37 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 20:12:30 GMT"}, {"version": "v3", "created": "Wed, 24 Feb 2021 03:20:02 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Spertus", "Jacob V", ""]]}, {"id": "2101.07408", "submitter": "Matthew Davidow", "authors": "Matthew Davidow, Cory Merow, Judy Che-Castaldo, Toryn Schafer,\n  Marie-Christine Duker, Derek Corcoran, David Matteson", "title": "Clustering Future Scenarios Based on Predicted Range Maps", "comments": "19 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Predictions of biodiversity trajectories under climate change are crucial in\norder to act effectively in maintaining the diversity of species. In many\necological applications, future predictions are made under various global\nwarming scenarios as described by a range of different climate models. The\noutputs of these various predictions call for a reliable interpretation. We\npropose a interpretable and flexible two step methodology to measure the\nsimilarity between predicted species range maps and cluster the future scenario\npredictions utilizing a spectral clustering technique. We find that clustering\nbased on ecological impact (predicted species range maps) is mainly driven by\nthe amount of warming. We contrast this with clustering based only on predicted\nclimate features, which is driven mainly by climate models. The differences\nbetween these clusterings illustrate that it is crucial to incorporate\necological information to understand the relevant differences between climate\nmodels. The findings of this work can be used to better synthesize forecasts of\nbiodiversity loss under the wide spectrum of results that emerge when\nconsidering potential future biodiversity loss.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 01:48:08 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Davidow", "Matthew", ""], ["Merow", "Cory", ""], ["Che-Castaldo", "Judy", ""], ["Schafer", "Toryn", ""], ["Duker", "Marie-Christine", ""], ["Corcoran", "Derek", ""], ["Matteson", "David", ""]]}, {"id": "2101.07426", "submitter": "Vincent Tan", "authors": "Eugene T. Y. Ang, Milashini Nambiar, Yong Sheng Soh, Vincent Y. F. Tan", "title": "An Interpretable Intensive Care Unit Mortality Risk Calculator", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mortality risk is a major concern to patients have just been discharged from\nthe intensive care unit (ICU). Many studies have been directed to construct\nmachine learning models to predict such risk. Although these models are highly\naccurate, they are less amenable to interpretation and clinicians are typically\nunable to gain further insights into the patients' health conditions and the\nunderlying factors that influence their mortality risk. In this paper, we use\npatients' profiles extracted from the MIMIC-III clinical database to construct\nrisk calculators based on different machine learning techniques such as\nlogistic regression, decision trees, random forests and multilayer perceptrons.\nWe perform an extensive benchmarking study that compares the most salient\nfeatures as predicted by various methods. We observe a high degree of agreement\nacross the considered machine learning methods; in particular, the cardiac\nsurgery recovery unit, age, and blood urea nitrogen levels are commonly\npredicted to be the most salient features for determining patients' mortality\nrisks. Our work has the potential for clinicians to interpret risk predictions.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 02:51:44 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Ang", "Eugene T. Y.", ""], ["Nambiar", "Milashini", ""], ["Soh", "Yong Sheng", ""], ["Tan", "Vincent Y. F.", ""]]}, {"id": "2101.07456", "submitter": "Ali Rafei", "authors": "Ali Rafei, Carol A. C. Flannagan, Brady T. West and Michael R. Elliott", "title": "Robust Bayesian Inference for Big Data: Combining Sensor-based Records\n  with Traditional Survey Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Big Data often presents as massive non-probability samples. Not only is the\nselection mechanism often unknown, but larger data volume amplifies the\nrelative contribution of selection bias to total error. Existing bias\nadjustment approaches assume that the conditional mean structures have been\ncorrectly specified for the selection indicator or key substantive measures. In\nthe presence of a reference probability sample, these methods rely on a\npseudo-likelihood method to account for the sampling weights of the reference\nsample, which is parametric in nature. Under a Bayesian framework, handling the\nsampling weights is an even bigger hurdle. To further protect against model\nmisspecification, we expand the idea of double robustness such that more\nflexible non-parametric methods as well as Bayesian models can be used for\nprediction. In particular, we employ Bayesian additive regression trees, which\nnot only capture non-linear associations automatically but permit direct\nquantification of the uncertainty of point estimates through its posterior\npredictive draws. We apply our method to sensor-based naturalistic driving data\nfrom the second Strategic Highway Research Program using the 2017 National\nHousehold Travel Survey as a benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 04:20:15 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Rafei", "Ali", ""], ["Flannagan", "Carol A. C.", ""], ["West", "Brady T.", ""], ["Elliott", "Michael R.", ""]]}, {"id": "2101.07526", "submitter": "Ragnhild Laursen", "authors": "Ragnhild Laursen and Asger Hobolth", "title": "A sampling algorithm to compute the set of feasible solutions for\n  non-negative matrix factorization with an arbitrary rank", "comments": "18 pages, 8 figures, 1 algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Non-negative Matrix Factorization (NMF) is a useful method to extract\nfeatures from multivariate data, but an important and sometimes neglected\nconcern is that NMF can result in non-unique solutions. Often, there exist a\nSet of Feasible Solutions (SFS), which makes it more difficult to interpret the\nfactorization. This problem is especially ignored in cancer genomics, where NMF\nis used to infer information about the mutational processes present in the\nevolution of cancer. In this paper the extent of non-uniqueness is investigated\nfor two mutational counts data, and a new sampling algorithm, that can find the\nSFS, is introduced. Our sampling algorithm is easy to implement and applies to\nan arbitrary rank of NMF. This is in contrast to state of the art, where the\nNMF rank must be smaller than or equal to four. For lower ranks we show that\nour algorithm performs similarly to the polygon inflation algorithm that is\ndeveloped in relations to chemometrics. Furthermore, we show how the size of\nthe SFS can have a high influence on the appearing variability of a solution.\nOur sampling algorithm is implemented in an R package \\textbf{SFS}\n(\\url{https://github.com/ragnhildlaursen/SFS}).\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 09:27:45 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Laursen", "Ragnhild", ""], ["Hobolth", "Asger", ""]]}, {"id": "2101.07532", "submitter": "Maria Thurow", "authors": "Maria Thurow, Florian Dumpert, Burim Ramosaj, Markus Pauly", "title": "Goodness (of fit) of Imputation Accuracy: The GoodImpact Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In statistical survey analysis, (partial) non-responders are integral\nelements during data acquisition. Treating missing values during data\npreparation and data analysis is therefore a non-trivial underpinning. Focusing\non different data sets from the Federal Statistical Office of Germany\n(DESTATIS), we investigate various imputation methods regarding their\nimputation accuracy. Since the latter is not uniquely determined in theory and\npractice, we study different measures for assessing imputation accuracy: Beyond\nthe most common measures, the normalized-root mean squared error (NRMSE) and\nthe proportion of false classification (PFC), we put a special focus on\n(distribution) distance- and association measures for assessing imputation\naccuracy. The aim is to deliver guidelines for correctly assessing\ndistributional accuracy after imputation. Our empirical findings indicate a\ndiscrepancy between the NRMSE resp. PFC and distance measures. While the latter\nmeasure distributional similarities, NRMSE and PFC focus on data\nreproducibility. We realize that a low NRMSE or PFC seem not to imply lower\ndistributional discrepancies. Although several measures for assessing\ndistributional discrepancies exist, our results indicate that not all of them\nare suitable for evaluating imputation-induced differences.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 09:35:15 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Thurow", "Maria", ""], ["Dumpert", "Florian", ""], ["Ramosaj", "Burim", ""], ["Pauly", "Markus", ""]]}, {"id": "2101.07575", "submitter": "Chanseok Park", "authors": "Chanseok Park and Min Wang", "title": "A note on the g and h control charts", "comments": "17 pages, 1 figure, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this note, we revisit the $g$ and $h$ control charts that are commonly\nused for monitoring the number of conforming cases between the two consecutive\nappearances of nonconformities. It is known that the process parameter of these\ncharts is usually unknown and estimated by using the maximum likelihood\nestimator and the minimum variance unbiased estimator. However, the minimum\nvariance unbiased estimator in the control charts has been inappropriately used\nin the quality engineering literature. This observation motivates us to provide\nthe correct minimum variance unbiased estimator and investigate theoretical and\nempirical biases of these estimators under consideration. Given that these\ncharts are developed based on the underlying assumption that samples from the\nprocess should be balanced, which is often not satisfied in many practical\napplications, we propose a method for constructing these charts with unbalanced\nsamples.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 11:47:51 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 04:41:52 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Park", "Chanseok", ""], ["Wang", "Min", ""]]}, {"id": "2101.07661", "submitter": "Martin Huber", "authors": "Simon Berset, Martin Huber, Mark Schelker", "title": "The fiscal response to revenue shocks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the impact of fiscal revenue shocks on local fiscal policy. We focus\non the very volatile revenues from the immovable property gains tax in the\ncanton of Zurich, Switzerland, and analyze fiscal behavior following large and\nrare positive and negative revenue shocks. We apply causal machine learning\nstrategies and implement the post-double-selection LASSO estimator to identify\nthe causal effect of revenue shocks on public finances. We show that local\npolicymakers overall predominantly smooth fiscal shocks. However, we also find\nsome patterns consistent with fiscal conservatism, where positive shocks are\nsmoothed, while negative ones are mitigated by spending cuts.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 14:51:28 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Berset", "Simon", ""], ["Huber", "Martin", ""], ["Schelker", "Mark", ""]]}, {"id": "2101.07683", "submitter": "Kui Yang", "authors": "Kui Yang, Wenjing Zhao, Constantinos Antoniou", "title": "Utilizing Import Vector Machines to Identify Dangerous Pro-active\n  Traffic Conditions", "comments": "6 pages, 3 figures, 2020 IEEE 23rd International Conference on\n  Intelligent Transportation Systems (ITSC)", "journal-ref": "In 2020 IEEE 23rd International Conference on Intelligent\n  Transportation Systems (ITSC) (pp. 1-6). IEEE", "doi": "10.1109/ITSC45102.2020.9294284", "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic accidents have been a severe issue in metropolises with the\ndevelopment of traffic flow. This paper explores the theory and application of\na recently developed machine learning technique, namely Import Vector Machines\n(IVMs), in real-time crash risk analysis, which is a hot topic to reduce\ntraffic accidents. Historical crash data and corresponding traffic data from\nShanghai Urban Expressway System were employed and matched. Traffic conditions\nare labelled as dangerous (i.e. probably leading to a crash) and safe (i.e. a\nnormal traffic condition) based on 5-minute measurements of average speed,\nvolume and occupancy. The IVM algorithm is trained to build the classifier and\nits performance is compared to the popular and successfully applied technique\nof Support Vector Machines (SVMs). The main findings indicate that IVMs could\nsuccessfully be employed in real-time identification of dangerous pro-active\ntraffic conditions. Furthermore, similar to the \"support points\" of the SVM,\nthe IVM model uses only a fraction of the training data to index kernel basis\nfunctions, typically a much smaller fraction than the SVM, and its\nclassification rates are similar to those of SVMs. This gives the IVM a\ncomputational advantage over the SVM, especially when the size of the training\ndata set is large.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 15:22:23 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Yang", "Kui", ""], ["Zhao", "Wenjing", ""], ["Antoniou", "Constantinos", ""]]}, {"id": "2101.07695", "submitter": "Stefano M. Iacus", "authors": "Tiziana Carpi, Airo Hino, Stefano Maria Iacus, Giuseppe Porro", "title": "Twitter Subjective Well-Being Indicator During COVID-19 Pandemic: A\n  Cross-Country Comparative Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN cs.CL q-fin.EC stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This study analyzes the impact of the COVID-19 pandemic on the subjective\nwell-being as measured through Twitter data indicators for Japan and Italy. It\nturns out that, overall, the subjective well-being dropped by 11.7% for Italy\nand 8.3% for Japan in the first nine months of 2020 compared to the last two\nmonths of 2019 and even more compared to the historical mean of the indexes.\nThrough a data science approach we try to identify the possible causes of this\ndrop down by considering several explanatory variables including, climate and\nair quality data, number of COVID-19 cases and deaths, Facebook Covid and flu\nsymptoms global survey, Google Trends data and coronavirus-related searches,\nGoogle mobility data, policy intervention measures, economic variables and\ntheir Google Trends proxies, as well as health and stress proxy variables based\non big data. We show that a simple static regression model is not able to\ncapture the complexity of well-being and therefore we propose a dynamic elastic\nnet approach to show how different group of factors may impact the well-being\nin different periods, even over a short time length, and showing further\ncountry-specific aspects. Finally, a structural equation modeling analysis\ntries to address the causal relationships among the COVID-19 factors and\nsubjective well-being showing that, overall, prolonged mobility\nrestrictions,flu and Covid-like symptoms, economic uncertainty, social\ndistancing and news about the pandemic have negative effects on the subjective\nwell-being.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 15:51:53 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Carpi", "Tiziana", ""], ["Hino", "Airo", ""], ["Iacus", "Stefano Maria", ""], ["Porro", "Giuseppe", ""]]}, {"id": "2101.07710", "submitter": "Mohammad Fayaz", "authors": "Mohammad Fayaz, Alireza Abadi, Soheila Khodakarim", "title": "The effect of Hybrid Principal Components Analysis on the Signal\n  Compression Functional Regression: With EEG-fMRI Application", "comments": "It has 11 pages with 3 tables and 3 figures. It presented at \"The\n  13th International Conference of the ERCIM WG on Computational and\n  Methodological Statistics (CMStatistics 2020) (Virtual), 19-21 December 2020\"\n  (http://www.cmstatistics.org/CMStatistics2020/index.php). We plan to publish\n  it in statistical journals, especially in the conference's recommended\n  journals", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.OT", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Objective: In some situations that exist both scalar and functional data,\ncalled mixed and hybrid data, the hybrid PCA (HPCA) was introduced. Among the\nregression models for the hybrid data, we can count covariate-adjusted HPCA,\nthe Semi-functional partial linear regression, function-on-function (FOF)\nregression with signal compression, and functional additive regression, models.\nIn this article, we study the effects of HPCA decomposition of hybrid data on\nthe prediction accuracy of the FOF regression with signal compressions. Method:\nWe stated a two-step procedure for incorporating the HPCA in the functional\nregressions. The first step is reconstructing the data based on the HPCAs and\nthe second step is merging data on the other dimensions and calculate the\npoint-wise average of the desired functional dimension. We also choose the\nnumber of HPCA based on Mean Squared Perdition Error (MSPE). Result: In the two\nsimulations, we show that the regression models with the first HPCA have the\nbest accuracy prediction and model fit summaries among no HPCA and all HPCAs\nwith a training/testing approach. Finally, we applied our methodology to the\nEEG-fMRI dataset. Conclusions: We conclude that our methodology improves the\nprediction of the experiments with the EEG datasets. And we recommend that\ninstead of using the functional PCA on the desired dimension, reconstruct the\ndata with HPCA and average it on the other two dimensions for functional\nregression models.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 16:26:47 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Fayaz", "Mohammad", ""], ["Abadi", "Alireza", ""], ["Khodakarim", "Soheila", ""]]}, {"id": "2101.07771", "submitter": "Grace Deng", "authors": "Judy P. Che-Castaldo, R\\'emi Cousin, Stefani Daryanto, Grace Deng,\n  Mei-Ling E. Feng, Rajesh K. Gupta, Dezhi Hong, Ryan M. McGranaghan, Olukunle\n  O. Owolabi, Tianyi Qu, Wei Ren, Toryn L. J. Schafer, Ashutosh Sharma,\n  Chaopeng Shen, Mila Getmansky Sherman, Deborah A. Sunter, Lan Wang, David S.\n  Matteson", "title": "Critical Risk Indicators (CRIs) for the electric power grid: A survey\n  and discussion of interconnected effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The electric power grid is a critical societal resource connecting multiple\ninfrastructural domains such as agriculture, transportation, and manufacturing.\nThe electrical grid as an infrastructure is shaped by human activity and public\npolicy in terms of demand and supply requirements. Further, the grid is subject\nto changes and stresses due to solar weather, climate, hydrology, and ecology.\nThe emerging interconnected and complex network dependencies make such\ninteractions increasingly dynamic causing potentially large swings, thus\npresenting new challenges to manage the coupled human-natural system. This\npaper provides a survey of models and methods that seek to explore the\nsignificant interconnected impact of the electric power grid and interdependent\ndomains. We also provide relevant critical risk indicators (CRIs) across\ndiverse domains that may influence electric power grid risks, including\nclimate, ecology, hydrology, finance, space weather, and agriculture. We\ndiscuss the convergence of indicators from individual domains to explore\npossible systemic risk, i.e., holistic risk arising from cross-domains\ninterconnections. Our study provides an important first step towards\ndata-driven analysis and predictive modeling of risks in the coupled\ninterconnected systems. Further, we propose a compositional approach to risk\nassessment that incorporates diverse domain expertise and information, data\nscience, and computer science to identify domain-specific CRIs and their union\nin systemic risk indicators.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 18:36:50 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 02:46:35 GMT"}, {"version": "v3", "created": "Fri, 4 Jun 2021 20:16:24 GMT"}, {"version": "v4", "created": "Wed, 9 Jun 2021 21:57:10 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Che-Castaldo", "Judy P.", ""], ["Cousin", "R\u00e9mi", ""], ["Daryanto", "Stefani", ""], ["Deng", "Grace", ""], ["Feng", "Mei-Ling E.", ""], ["Gupta", "Rajesh K.", ""], ["Hong", "Dezhi", ""], ["McGranaghan", "Ryan M.", ""], ["Owolabi", "Olukunle O.", ""], ["Qu", "Tianyi", ""], ["Ren", "Wei", ""], ["Schafer", "Toryn L. J.", ""], ["Sharma", "Ashutosh", ""], ["Shen", "Chaopeng", ""], ["Sherman", "Mila Getmansky", ""], ["Sunter", "Deborah A.", ""], ["Wang", "Lan", ""], ["Matteson", "David S.", ""]]}, {"id": "2101.07919", "submitter": "Jonas Krampe", "authors": "Alexander Braumann, Jonas Krampe, Jens-Peter Kreiss and Efstathios\n  Paparoditis", "title": "Estimation of the Distribution of the Individual Reproduction Number:\n  The Case of the COVID-19 Pandemic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of estimating the distribution of the individual\nreproduction number governing the COVID-19 pandemic. Under the assumption that\nthis random variable follows a Negative Binomial distribution, we focus on\nconstructing estimators of the parameters of this distribution using reported\ninfection data and taking into account issues like under-reporting or the time\nbehavior of the infection and of the reporting processes. To this end, we\nextract information from regionally dissaggregated data reported by German\nhealth authorities, in order to estimate not only the mean but also the\nvariance of the distribution of the individual reproduction number. In contrast\nto the mean, the latter parameter also depends on the unknown under-reporting\nrate of the pandemic. The estimates obtained allow not only for a better\nunderstanding of the time-varying behavior of the expected value of the\nindividual reproduction number but also of its dispersion, for the construction\nof bootstrap confidence intervals and for a discussion of the implications of\ndifferent policy interventions. Our methodological investigations are\naccompanied by an empirical study of the development of the COVID-19 pandemic\nin Germany, which shows a strong overdispersion of the individual reproduction\nnumber.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 01:17:56 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Braumann", "Alexander", ""], ["Krampe", "Jonas", ""], ["Kreiss", "Jens-Peter", ""], ["Paparoditis", "Efstathios", ""]]}, {"id": "2101.07934", "submitter": "Xinyue Qi", "authors": "Xinyue Qi, Shouhao Zhou, Yucai Wang, Michael L. Wang, Chan Shen", "title": "Bayesian Meta-analysis of Rare Events with Non-ignorable Missing Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-analysis is a powerful tool for drug safety assessment by synthesizing\ntreatment-related toxicological findings from independent clinical trials.\nHowever, published clinical studies may or may not report all adverse events\n(AEs) if the observed number of AEs were fewer than a pre-specified\nstudy-dependent cutoff. Subsequently, with censored information ignored, the\nestimated incidence rate of AEs could be significantly biased. To address this\nnon-ignorable missing data problem in meta-analysis, we propose a Bayesian\nmultilevel regression model to accommodate the censored rare event data. The\nperformance of the proposed Bayesian model of censored data compared to other\nexisting methods is demonstrated through simulation studies under various\ncensoring scenarios. Finally, the proposed approach is illustrated using data\nfrom a recent meta-analysis of 125 clinical trials involving PD-1/PD-L1\ninhibitors with respect to their toxicity profiles.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 02:30:56 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Qi", "Xinyue", ""], ["Zhou", "Shouhao", ""], ["Wang", "Yucai", ""], ["Wang", "Michael L.", ""], ["Shen", "Chan", ""]]}, {"id": "2101.08049", "submitter": "Luka \\v{Z}nidari\\v{c}", "authors": "Luka \\v{Z}nidari\\v{c}, Gjorgji Nusev, Bertrand Morel, Julie Mougin,\n  {\\DJ}ani Juri\\v{c}i\\'c and Pavle Bo\\v{s}koski", "title": "Evaluating uncertainties in electrochemical impedance spectra of solid\n  oxide fuel cells", "comments": "28 pages, 18 figures. Submitted to: Applied Energy", "journal-ref": null, "doi": "10.1016/j.apenergy.2021.117101", "report-no": null, "categories": "stat.CO cs.LG stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Electrochemical impedance spectroscopy (EIS) is a widely used tool for\ncharacterization of fuel cells and other electrochemical conversion systems.\nWhen applied to the on-line monitoring in the context of in-field applications,\nthe disturbances, drifts and sensor noise may cause severe distortions in the\nevaluated spectra, especially in the low-frequency part. Failure to ignore the\nrandom effects can result in misinterpreted spectra and, consequently, in\nmisleading diagnostic reasoning. This fact has not been often addressed in the\nresearch so far. In this paper, we propose an approach to the quantification of\nthe spectral uncertainty, which relies on evaluating the uncertainty of the\nequivalent circuit model (ECM). We apply the computationally efficient\nvariational Bayes (VB) method and compare the quality of the results with those\nobtained with the Markov chain Monte Carlo (MCMC) algorithm. Namely, MCMC\nalgorithm returns accurate distributions of the estimated model parameters,\nwhile VB approach provides the approximate distributions. By using simulated\nand real data we show that approximate results provided by VB approach,\nalthough slightly over-optimistic, are still close to the more realistic MCMC\nestimates. A great advantage of the VB method for online monitoring is low\ncomputational load, which is several orders of magnitude lower compared to\nMCMC. The performance of VB algorithm is demonstrated on a case of ECM\nparameters estimation in a 6 cell solid oxide fuel cell (SOFC) stack. The\ncomplete numerical implementation for recreating the results can be found at\nhttps://repo.ijs.si/lznidaric/variational-bayes-supplementary-material.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 10:07:32 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 12:22:32 GMT"}, {"version": "v3", "created": "Wed, 5 May 2021 11:35:23 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["\u017dnidari\u010d", "Luka", ""], ["Nusev", "Gjorgji", ""], ["Morel", "Bertrand", ""], ["Mougin", "Julie", ""], ["Juri\u010di\u0107", "\u0110ani", ""], ["Bo\u0161koski", "Pavle", ""]]}, {"id": "2101.08083", "submitter": "Marouane Il Idrissi", "authors": "Marouane Il Idrissi, Vincent Chabridon, Bertrand Iooss", "title": "Developments and applications of Shapley effects to reliability-oriented\n  sensitivity analysis with correlated inputs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliability-oriented sensitivity analysis methods have been developed for\nunderstanding the influence of model inputs relative to events which\ncharacterize the failure of a system (e.g., a threshold exceedance of the model\noutput). In this field, the target sensitivity analysis focuses primarily on\ncapturing the influence of the inputs on the occurrence of such a critical\nevent. This paper proposes new target sensitivity indices, based on the Shapley\nvalues and called \"target Shapley effects\", allowing for interpretable\nsensitivity measures under dependent inputs. Two algorithms (one based on Monte\nCarlo sampling, and a given-data algorithm based on a nearest-neighbors\nprocedure) are proposed for the estimation of these target Shapley effects\nbased on the $\\ell^2$ norm. Additionally, the behavior of these target Shapley\neffects are theoretically and empirically studied through various toy-cases.\nFinally, the application of these new indices in two real-world use-cases (a\nriver flood model and a COVID-19 epidemiological model) is discussed.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 11:39:24 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 12:01:07 GMT"}, {"version": "v3", "created": "Wed, 19 May 2021 08:43:25 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Idrissi", "Marouane Il", ""], ["Chabridon", "Vincent", ""], ["Iooss", "Bertrand", ""]]}, {"id": "2101.08175", "submitter": "Silvia Montagna", "authors": "Patric Dolmeta, Raffaele Argiento, Silvia Montagna", "title": "Bayesian GARCH Modeling of Functional Sports Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The use of statistical methods in sport analytics has gained a rapidly\ngrowing interest over the last decade, and nowadays is common practice. In\nparticular, the interest in understanding and predicting an athlete's\nperformance throughout his/her career is motivated by the need to evaluate the\nefficacy of training programs, anticipate fatigue to prevent injuries and\ndetect unexpected of disproportionate increases in performance that might be\nindicative of doping. Moreover, fast evolving data gathering technologies\nrequire up to date modelling techniques that adapt to the distinctive features\nof sports data. In this work, we propose a hierarchical Bayesian model for\ndescribing and predicting the evolution of performance over time for shot put\nathletes. To account for seasonality and heterogeneity in recorded results, we\nrely both on a smooth functional contribution and on a linear mixed effect\nmodel with heteroskedastic errors to represent the athlete-specific\ntrajectories. The resulting model provides an accurate description of the\nperformance trajectories and helps specifying both the intra- and\ninter-seasonal variability of measurements. Further, the model allows for the\nprediction of athletes' performance in future seasons. We apply our model to an\nextensive real world data set on performance data of professional shot put\nathletes recorded at elite competitions.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 15:16:10 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Dolmeta", "Patric", ""], ["Argiento", "Raffaele", ""], ["Montagna", "Silvia", ""]]}, {"id": "2101.08198", "submitter": "Philip Sansom", "authors": "Philip G. Sansom, Donald Cummins, Stephan Siegert and David B.\n  Stephenson", "title": "Towards reliable projections of global mean surface temperature", "comments": "21 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantifying the risk of global warming exceeding critical targets such as 2.0\nK requires reliable projections of uncertainty as well as best estimates of\nGlobal Mean Surface Temperature (GMST). However, uncertainty bands on GMST\nprojections are often calculated heuristically and have several potential\nshortcomings. In particular, the uncertainty bands shown in IPCC plume\nprojections of GMST are based on the distribution of GMST anomalies from\nclimate model runs and so are strongly determined by model characteristics with\nlittle influence from observations of the real-world. Physically motivated\ntime-series approaches are proposed based on fitting energy balance models\n(EBMs) to climate model outputs and observations in order to constrain future\nprojections. It is shown that EBMs fitted to one forcing scenario will not\nproduce reliable projections when different forcing scenarios are applied. The\nerrors in the EBM projections can be interpreted as arising due to a\ndiscrepancy in the effective forcing felt by the model. A simple time-series\napproach to correcting the projections is proposed based on learning the\nevolution of the forcing discrepancy so that it can be projected into the\nfuture. These approaches give reliable projections of GMST when tested in a\nperfect model setting, and when applied to observations lead to well\nconstrained projections with lower mean warming and narrower projection bands\nthan previous estimates. Despite the reduced uncertainty, the lower warming\nleads to a greatly reduced probability of exceeding the 2.0 K warming target.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 16:12:03 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Sansom", "Philip G.", ""], ["Cummins", "Donald", ""], ["Siegert", "Stephan", ""], ["Stephenson", "David B.", ""]]}, {"id": "2101.08334", "submitter": "Giovanna Menardi", "authors": "Giovanna Menardi and Domenico De Stefano", "title": "Density-based clustering of social networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The idea underlying the modal formulation of density-based clustering is to\nassociate groups with the regions around the modes of the probability density\nfunction underlying the data. This correspondence between clusters and dense\nregions in the sample space is here exploited to discuss an extension of this\napproach to the analysis of social networks. Such extension seems particularly\nappealing: conceptually, the notion of high-density cluster fits well the one\nof community in a network, regarded to as a collection of individuals with\ndense local ties in its neighbourhood. The lack of a probabilistic notion of\ndensity in networks is turned into a major strength of the proposed method,\nwhere node-wise measures that quantify the role and position of actors may be\nused to derive different community configurations. The approach allows for the\nidentification of a hierarchical structure of clusters, which may catch\ndifferent degrees of resolution of the clustering structure. This feature well\nfits the nature of social networks, disentangling a different involvement of\nindividuals in social aggregations.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 21:50:41 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Menardi", "Giovanna", ""], ["De Stefano", "Domenico", ""]]}, {"id": "2101.08345", "submitter": "Giovanna Menardi", "authors": "Giovanna Menardi", "title": "Nonparametric clustering for image segmentation", "comments": null, "journal-ref": "Statistical Analysis and Data Mining, 13(1), 83-97 (2020)", "doi": "10.1002/sam.11444", "report-no": null, "categories": "cs.CV eess.IV stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image segmentation aims at identifying regions of interest within an image,\nby grouping pixels according to their properties. This task resembles the\nstatistical one of clustering, yet many standard clustering methods fail to\nmeet the basic requirements of image segmentation: segment shapes are often\nbiased toward predetermined shapes and their number is rarely determined\nautomatically. Nonparametric clustering is, in principle, free from these\nlimitations and turns out to be particularly suitable for the task of image\nsegmentation. This is also witnessed by several operational analogies, as, for\ninstance, the resort to topological data analysis and spatial tessellation in\nboth the frameworks. We discuss the application of nonparametric clustering to\nimage segmentation and provide an algorithm specific for this task. Pixel\nsimilarity is evaluated in terms of density of the color representation and the\nadjacency structure of the pixels is exploited to introduce a simple, yet\neffective method to identify image segments as disconnected high-density\nregions. The proposed method works both to segment an image and to detect its\nboundaries and can be seen as a generalization to color images of the class of\nthresholding methods.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 22:27:44 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Menardi", "Giovanna", ""]]}, {"id": "2101.08551", "submitter": "Robert Verschuren", "authors": "Robert Matthijs Verschuren", "title": "Customer Price Sensitivities in Competitive Automobile Insurance Markets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Insurers are increasingly adopting more demand-based strategies to\nincorporate the indirect effect of premium changes on their policyholders'\nwillingness to stay. However, since in practice both insurers' renewal premia\nand customers' responses to these premia typically depend on the customer's\nlevel of risk, it remains challenging in these strategies to determine how to\nproperly control for this confounding. We therefore consider a causal inference\napproach in this paper to account for customer price sensitivities and to\ndeduce optimal, multi-period profit maximizing premium renewal offers. More\nspecifically, we extend the discrete treatment framework of Guelman and\nGuill\\'en (2014) by Extreme Gradient Boosting, or XGBoost, and by multiple\nimputation to better account for the uncertainty in the counterfactual\nresponses. We additionally introduce the continuous treatment framework with\nXGBoost to the insurance literature to allow identification of the exact\noptimal renewal offers and account for any competition in the market by\nincluding competitor offers. The application of the two treatment frameworks to\na Dutch automobile insurance portfolio suggests that a policy's competitiveness\nin the market is crucial for a customer's price sensitivity and that XGBoost is\nmore appropriate to describe this than the traditional logistic regression.\nMoreover, an efficient frontier of both frameworks indicates that substantially\nmore profit can be gained on the portfolio than realized, also already with\nless churn and in particular if we allow for continuous rate changes. A\nmulti-period renewal optimization confirms these findings and demonstrates that\nthe competitiveness enables temporal feedback of previous rate changes on\nfuture demand.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 11:07:20 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Verschuren", "Robert Matthijs", ""]]}, {"id": "2101.08573", "submitter": "Tobias Braun Mr", "authors": "Tobias Braun, Matthias Waechter, Joachim Peinke, Thomas Guhr", "title": "Correlated power time series of individual wind turbines: A data driven\n  model approach", "comments": "20 pages, 8 figures", "journal-ref": "J. Renewable Sustainable Energy12, 023301 (2020)", "doi": "10.1063/1.5139039", "report-no": null, "categories": "stat.AP physics.app-ph physics.data-an", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Wind farms can be regarded as complex systems that are, on the one hand,\ncoupled to the nonlinear, stochastic characteristics of weather and, on the\nother hand, strongly influenced by supervisory control mechanisms. One crucial\nproblem in this context today is the predictability of wind energy as an\nintermittent renewable resource with additional non-stationary nature. In this\ncontext, we analyze the power time series measured in an offshore wind farm for\na total period of one year with a time resolution of 10 min. Applying detrended\nfluctuation analysis, we characterize the autocorrelation of power time series\nand find a Hurst exponent in the persistent regime with cross-over behavior. To\nenrich the modeling perspective of complex large wind energy systems, we\ndevelop a stochastic reduced-form model ofpower time series. The observed\ntransitions between two dominating power generation phases are reflected by a\nbistable deterministic component, while correlated stochastic fluctuations\naccount for the identified persistence. The model succeeds to qualitatively\nreproduce several empirical characteristics such as the autocorrelation\nfunction and the bimodal probability density function.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 12:23:17 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Braun", "Tobias", ""], ["Waechter", "Matthias", ""], ["Peinke", "Joachim", ""], ["Guhr", "Thomas", ""]]}, {"id": "2101.08590", "submitter": "Kara Rudolph", "authors": "Kara E. Rudolph and Ivan Diaz", "title": "When the ends don't justify the means: Learning a treatment strategy to\n  prevent harmful indirect effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is a growing literature on finding so-called optimal treatment rules,\nwhich are rules by which to assign treatment to individuals based on an\nindividual's characteristics, such that a desired outcome is maximized. A\nrelated goal entails identifying individuals who are predicted to have a\nharmful indirect effect (the effect of treatment on an outcome through\nmediators) even in the presence of an overall beneficial effect of the\ntreatment on the outcome. In some cases, the likelihood of a harmful indirect\neffect may outweigh a likely beneficial overall effect, and would be reason to\ncaution against treatment for indicated individuals. We build on both the\ncurrent mediation and optimal treatment rule literature to propose a method of\nidentifying a subgroup for which the treatment effect through the mediator is\nharmful. Our approach is nonparametric, incorporates post-treatment variables\nthat may confound the mediator-outcome relationship, and does not make\nrestrictions on the distribution of baseline covariates, mediating variables\n(considered jointly), or outcomes. We apply the proposed approach to identify a\nsubgroup of boys in the Moving to Opportunity housing voucher experiment who\nare predicted to have harmful indirect effects, though the average total effect\nis beneficial.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 13:05:16 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Rudolph", "Kara E.", ""], ["Diaz", "Ivan", ""]]}, {"id": "2101.08765", "submitter": "Shulei Wang", "authors": "Shulei Wang", "title": "Robust Differential Abundance Test in Compositional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST q-bio.QM stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential abundance tests in compositional data are essential and\nfundamental tasks in various biomedical applications, such as single-cell, bulk\nRNA-seq, and microbiome data analysis. However, despite the recent developments\nin these fields, differential abundance analysis in compositional data remains\na complicated and unsolved statistical problem, because of the compositional\nconstraint and prevalent zero counts in the dataset. This study introduces a\nnew differential abundance test, the robust differential abundance (RDB) test,\nto address these challenges. Compared with existing methods, the RDB test 1) is\nsimple and computationally efficient, 2) is robust to prevalent zero counts in\ncompositional datasets, 3) can take the data's compositional nature into\naccount, and 4) has a theoretical guarantee of controlling false discoveries in\na general setting. Furthermore, in the presence of observed covariates, the RDB\ntest can work with the covariate balancing techniques to remove the potential\nconfounding effects and draw reliable conclusions. Finally, we apply the new\ntest to several numerical examples using simulated and real datasets to\ndemonstrate its practical merits.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 18:37:24 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 19:27:04 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Wang", "Shulei", ""]]}, {"id": "2101.08841", "submitter": "Robert Miller", "authors": "Florian Rupprecht, S\\\"oren Enge, Kornelius Schmidt, Wei Gao, Clemens\n  Kirschbaum, Robert Miller", "title": "Automating LC-MS/MS mass chromatogram quantification. Wavelet transform\n  based peak detection and automated estimation of peak boundaries and\n  signal-to-noise ratio using signal processing methods", "comments": "20 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  While there are many different methods for peak detection, no automatic\nmethods for marking peak boundaries to calculate area under the curve (AUC) and\nsignal-to-noise ratio (SNR) estimation exist. An algorithm for the automation\nof liquid chromatography tandem mass spectrometry (LC-MS/MS) mass chromatogram\nquantification was developed and validated. Continuous wavelet transformation\nand other digital signal processing methods were used in a multi-step procedure\nto calculate concentrations of six different analytes. To evaluate the\nperformance of the algorithm, the results of the manual quantification of 446\nhair samples with 6 different steroid hormones by two experts were compared to\nthe algorithm results. The proposed approach of automating mass chromatogram\nquantification is reliable and valid. The algorithm returns less nondetectables\nthan human raters. Based on signal to noise ratio, human non-detectables could\nbe correctly classified with a diagnostic performance of AUC = 0.95. The\nalgorithm presented here allows fast, automated, reliable, and valid\ncomputational peak detection and quantification in LC- MS/MS.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 20:25:34 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Rupprecht", "Florian", ""], ["Enge", "S\u00f6ren", ""], ["Schmidt", "Kornelius", ""], ["Gao", "Wei", ""], ["Kirschbaum", "Clemens", ""], ["Miller", "Robert", ""]]}, {"id": "2101.08872", "submitter": "Andrea Arnold", "authors": "Anna Fitzpatrick, Molly Folino, Andrea Arnold", "title": "Fourier Series-Based Approximation of Time-Varying Parameters Using the\n  Ensemble Kalman Filter", "comments": "13 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we propose a Fourier series-based approximation method using\nensemble Kalman filtering to estimate time-varying parameters in deterministic\ndynamical systems. We demonstrate the capability of this approach in estimating\nboth sinusoidal and polynomial forcing parameters in a mass-spring system.\nResults emphasize the importance of the choice of frequencies in the\napproximation model terms on the corresponding time-varying parameter\nestimates.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 22:26:03 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Fitzpatrick", "Anna", ""], ["Folino", "Molly", ""], ["Arnold", "Andrea", ""]]}, {"id": "2101.09022", "submitter": "Pamela M. Chiroque-Solano", "authors": "Pamela M. Chiroque-Solano and Fernando A. S. Moura", "title": "A heavy-tailed and overdispersed collective risk model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Insurance data can be asymmetric with heavy tails, causing inadequate\nadjustments of the usually applied models. To deal with this issue,\nhierarchical models for collective risk with heavy-tails of the claims\ndistributions that take also into account overdispersion of the number of\nclaims are proposed. In particular, the distribution of the logarithm of the\naggregate value of claims is assumed to follow a Student-t distribution.\nAdditionally, to incorporate possible overdispersion, the number of claims is\nmodeled as having a negative binomial distribution. Bayesian decision theory is\ninvoked to calculate the fair premium based on the modified absolute deviation\nutility. An application to a health insurance dataset is presented together\nwith some diagnostic measures to identify excess variability. The variability\nmeasures are analyzed using the marginal posterior predictive distribution of\nthe premiums according to some competitive models. Finally, a simulation study\nis carried out to assess the predictive capability of the model and the\nadequacy of the Bayesian estimation procedure.\n  Keywords: Continuous ranked probability score (CRPS); decision theory;\ninsurance data; marginal posterior predictive; tail value at risk; value at\nrisk.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 09:43:19 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 12:01:38 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Chiroque-Solano", "Pamela M.", ""], ["Moura", "Fernando A. S.", ""]]}, {"id": "2101.09109", "submitter": "Hisashi Kobayashi", "authors": "Hisashi Kobayashi", "title": "Stochastic Modeling of an Infectious Disease Part III-A: Analysis of\n  Time-Nonhomogeneous Models", "comments": "35 figures. An earlier version was presented at ITC-32 as a keynote\n  on September 23, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend our BDI (birth-death-immigration) process based stochastic model of\nan infectious disease to time-nonhomogeneous cases. First, we discuss the\ndeterministic model, and derive the expected value of the infection process.\nThen as an application we consider that a government issues a decree to its\ncitizens to curtail their activities that may incur further infections and show\nhow the public's tardy response may further increase infections and prolong the\nepidemic much longer than one might think. We seek to solve a partial\ndifferential equation for the probability generating function. We find,\nhowever, that an exact solution is obtainable only for the BD process, i.e., no\narrivals of the infected from outside. The coefficient of variation for the\nnonhomogeneous BD process is found to be well over unity. This result implies\nthat the variations among different sample paths will be as large as in the\nnegative binomial distribution with r<1, which was found in Part I for the\nhomogeneous BDI model. In the final section, we illustrate, using our running\nexample, how much information we can derive from the time dependent PMF\n(probability mass function) P_k(t)=Pr[I(t)=k]. We present graphical plots of\nthe PMF at various t's, and cross-sections of this function at various k's. A\nmesh plot of the function over the (k, t) plane summarizes the above numerous\nplots. The results of this paper reinforce our earlier claim (see Abstract of\nPart II) that it would be a futile effort to attempt to identify all possible\nreasons why environments of similar situations differ so much in their epidemic\npatterns. Mere \"luck\" plays a more significant role than most of us may\nbelieve. We should be prepared for a worse possible scenario, which only a\nstochastic model can provide with probabilistic qualification. An empirical\nvalidation of the above results will be given in Part III-B.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 13:57:44 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Kobayashi", "Hisashi", ""]]}, {"id": "2101.09147", "submitter": "Jesus Fuentes", "authors": "Jes\\'us Fuentes and Octavio Obreg\\'on", "title": "A superstatistical formulation of complexity measures", "comments": "17 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math-ph math.MP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is discussed how the superstatistical formulation of effective Boltzmann\nfactors can be related to the concept of Kolmogorov complexity, generating an\ninfinite set of complexity measures (CMs) for quantifying information. At this\nlevel, the information is treated according to its background, which means that\nthe CM depends on the inherent attributes of the information scenario. While\nthe basic Boltzmann factor directly produces the standard complexity measure\n(SCM), it succeeds in the description of large-scale scenarios where the data\ncomponents are not interrelated with themselves, thus adopting the behaviour of\na gas. What happens in scenarios in which the presence of sources and sinks of\ninformation cannot be neglected, needs of a CM other than the one produced by\nthe ordinary Boltzmann factor. We introduce a set of flexible CMs, without free\nparameters, that converge asymptotically to the Kolmogorov complexity, but also\nquantify the information in scenarios with a reasonable small density of\nstates. We prove that these CMs are obtained from a generalised relative\nentropy and we suggest why such measures are the only compatible\ngeneralisations of the SCM.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 23:19:14 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Fuentes", "Jes\u00fas", ""], ["Obreg\u00f3n", "Octavio", ""]]}, {"id": "2101.09297", "submitter": "Keith Zirkle", "authors": "Keith W. Zirkle, Marie-Abele Bind, Jenise L. Swall, and David C.\n  Wheeler", "title": "Addressing Spatially Structured Interference in Causal Analysis Using\n  Propensity Scores", "comments": "37 pages, 7 figures, being submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Environmental epidemiologists are increasingly interested in establishing\ncausality between exposures and health outcomes. A popular model for causal\ninference is the Rubin Causal Model (RCM), which typically seeks to estimate\nthe average difference in study units' potential outcomes. An important\nassumption under RCM is no interference; that is, the potential outcomes of one\nunit are not affected by the exposure status of other units. The no\ninterference assumption is violated if we expect spillover or diffusion of\nexposure effects based on units' proximity to other units and several other\ncausal estimands arise. Air pollution epidemiology typically violates this\nassumption when we expect upwind events to affect downwind or nearby locations.\nThis paper adapts causal assumptions from social network research to address\ninterference and allow estimation of both direct and spillover causal effects.\nWe use propensity score-based methods to estimate these effects when\nconsidering the effects of the Environmental Protection Agency's 2005\nnonattainment designations for particulate matter with aerodynamic diameter\nless than 2.5 micrograms per cubic meter (PM2.5) on lung cancer incidence using\ncounty-level data obtained from the Surveillance, Epidemiology, and End Results\n(SEER) Program. We compare these methods in a rigorous simulation study that\nconsiders both spatially autocorrelated variables, interference, and missing\nconfounders. We find that pruning and matching based on the propensity score\nproduces the highest probability coverage of the true causal effects and lower\nmean squared error. When applied to the research question, we found protective\ndirect and spillover causal effects.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 19:12:53 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Zirkle", "Keith W.", ""], ["Bind", "Marie-Abele", ""], ["Swall", "Jenise L.", ""], ["Wheeler", "David C.", ""]]}, {"id": "2101.09351", "submitter": "Miguel N\\'u\\~nez Peir\\'o", "authors": "Miguel N\\'u\\~nez-Peir\\'o, Carmen Sanchez-Guevara Sanchez, F. Javier\n  Neila Gonzalez", "title": "Hourly evolution of intra-urban temperature variability across the local\n  climate zones. The case of Madrid", "comments": "7 figures, 8 tables, 1 appendix", "journal-ref": null, "doi": "10.1016/j.uclim.2021.100921", "report-no": null, "categories": "physics.ao-ph physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Field measurement campaigns have grown exponentially in recent years,\nstemming from the need for reliable data to validate urban climate models and\nobtain a better understanding of urban climate features. Also contributing to\nthis growth is the Local Climate Zone (LCZ) scheme, firstly developed to\nenhance the accuracy in the contextualisation of urban measurements, and lately\nused for characterising urban areas. Due to its relative novelty, researchers\nare still investigating the potential of LCZs and its indicators for urban\ntemperature variability detection. In this respect, the present study\nintroduces the results of an extensive monitoring campaign carried out in the\ncity of Madrid over a two-year period (2016-2018). The aim of this work is to\nfurther examine the relationships between LCZs and air temperature differences,\nwith emphasis on their hourly and seasonal evolution. A graphical and\nstatistical analysis to identify temperature variability trends for each LCZ is\nperformed. Results support the existing evidence suggesting a high level of\neffectiveness in capturing the heat island (UHI) profile of different urban\nareas, while underperforming when it comes to capturing diurnal temperature\nvariability. The incorporation of indicators that explain the daytime\ntemperature variation phenomenon into the LCZ scheme is therefore recommended,\nwarranting further research.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 09:41:02 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["N\u00fa\u00f1ez-Peir\u00f3", "Miguel", ""], ["Sanchez", "Carmen Sanchez-Guevara", ""], ["Gonzalez", "F. Javier Neila", ""]]}, {"id": "2101.09352", "submitter": "Raphael Huser", "authors": "Matheus B. Guerrero, Rapha\\\"el Huser and Hernando Ombao", "title": "Conex-Connect: Learning Patterns in Extremal Brain Connectivity From\n  Multi-Channel EEG Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Epilepsy is a chronic neurological disorder affecting more than 50 million\npeople globally. An epileptic seizure acts like a temporary shock to the\nneuronal system, disrupting normal electrical activity in the brain. Epilepsy\nis frequently diagnosed with electroencephalograms (EEGs). Current methods\nstudy the time-varying spectra and coherence but do not directly model changes\nin extreme behavior. Thus, we propose a new approach to characterize brain\nconnectivity based on the joint tail behavior of the EEGs. Our proposed method,\nthe conditional extremal dependence for brain connectivity (Conex-Connect), is\na pioneering approach that links the association between extreme values of\nhigher oscillations at a reference channel with the other brain network\nchannels. Using the Conex-Connect method, we discover changes in the extremal\ndependence driven by the activity at the foci of the epileptic seizure. Our\nmodel-based approach reveals that, pre-seizure, the dependence is notably\nstable for all channels when conditioning on extreme values of the focal\nseizure area. Post-seizure, by contrast, the dependence between channels is\nweaker, and dependence patterns are more \"chaotic\". Moreover, in terms of\nspectral decomposition, we find that high values of the high-frequency\nGamma-band are the most relevant features to explain the conditional extremal\ndependence of brain connectivity.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 18:53:05 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Guerrero", "Matheus B.", ""], ["Huser", "Rapha\u00ebl", ""], ["Ombao", "Hernando", ""]]}, {"id": "2101.09382", "submitter": "Krzysztof Szajowski", "authors": "Krzysztof J. Szajowski and Kinga W{\\l}odarczyk", "title": "A measure of the importance of roads based on topography and traffic\n  intensity", "comments": "35 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.GR math.ST stat.AP stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mathematical models of street traffic allowing assessment of the importance\nof their individual segments for the functionality of the street system is\nconsidering. Based on methods of cooperative games and the reliability theory\nthe suitable measure is constructed. The main goal is to analyze methods for\nassessing the importance (rank) of road fragments, including their functions. A\nrelevance of these elements for effective accessibility for the entire system\nwill be considered.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 23:53:49 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Szajowski", "Krzysztof J.", ""], ["W\u0142odarczyk", "Kinga", ""]]}, {"id": "2101.09395", "submitter": "Xiaodong Wang", "authors": "Xiaodong Wang and Fushing Hsieh", "title": "Unraveling S&P500 stock volatility and networks -- An\n  encoding-and-decoding approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Volatility of financial stock is referring to the degree of uncertainty or\nrisk embedded within a stock's dynamics. Such risk has been received huge\namounts of attention from diverse financial researchers. By following the\nconcept of regime-switching model, we proposed a non-parametric approach, named\nencoding-and-decoding, to discover multiple volatility states embedded within a\ndiscrete time series of stock returns. The encoding is performed across the\nentire span of temporal time points for relatively extreme events with respect\nto a chosen quantile-based threshold. As such the return time series is\ntransformed into Bernoulli-variable processes. In the decoding phase, we\ncomputationally seek for locations of change points via estimations based on a\nnew searching algorithm conjunction to the Bayesian information criterion\napplied on the observed collection of recurrence times upon the binary process.\nBesides the independence required for building the Geometric distributional\nlikelihood function, the proposed approach can functionally partition the\nentire return time series into a collection of homogeneous segments without any\nassumptions of dynamic structure and underlying distributions. In the numerical\nexperiments, our approach is found favorably compared with Viterbi's under\nHidden Markov Model (HMM) settings. In the real data applications, volatility\ndynamics of every single stock of S&P500 are computed and revealed. Then, a\nnon-linear dependency of any stock-pair is derived by measuring through\nconcurrent volatility states. Finally, various networks dealing with distinct\nfinancial implications are consequently established to represent different\naspects of global connectivity among all stocks in S&P500.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 01:30:19 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 23:46:07 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Wang", "Xiaodong", ""], ["Hsieh", "Fushing", ""]]}, {"id": "2101.09418", "submitter": "Xinyue Chang", "authors": "Xinyue Chang, Zhengyuan Zhu, Xiongtao Dai and Jonathan Hobbs", "title": "A Geospatial Functional Model For OCO-2 Data with Application on\n  Imputation and Land Fraction Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data from NASA's Orbiting Carbon Observatory-2 (OCO-2) satellite is essential\nto many carbon management strategies. A retrieval algorithm is used to estimate\nCO2 concentration using the radiance data measured by OCO-2. However, due to\nfactors such as cloud cover and cosmic rays, the spatial coverage of the\nretrieval algorithm is limited in some areas of critical importance for carbon\ncycle science. Mixed land/water pixels along the coastline are also not used in\nthe retrieval processing due to the lack of valid ancillary variables including\nland fraction. We propose an approach to model spatial spectral data to solve\nthese two problems by radiance imputation and land fraction estimation. The\nspectral observations are modeled as spatially indexed functional data with\nfootprint-specific parameters and are reduced to much lower dimensions by\nfunctional principal component analysis. The principal component scores are\nmodeled as random fields to account for the spatial dependence, and the missing\nspectral observations are imputed by kriging the principal component scores.\nThe proposed method is shown to impute spectral radiance with high accuracy for\nobservations over the Pacific Ocean. An unmixing approach based on this model\nprovides much more accurate land fraction estimates in our validation study\nalong Greece coastlines.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 05:09:33 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Chang", "Xinyue", ""], ["Zhu", "Zhengyuan", ""], ["Dai", "Xiongtao", ""], ["Hobbs", "Jonathan", ""]]}, {"id": "2101.09587", "submitter": "Wenyi Wang", "authors": "Zeya Wang, Veera Baladandayuthapan, Ahmed O. Kaseb, Hesham M. Amin,\n  Manal M. Hassan, Wenyi Wang, Jeffrey S. Morris", "title": "Bayesian Edge Regression in Undirected Graphical Models to Characterize\n  Interpatient Heterogeneity in Cancer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical models are commonly used to discover associations within gene or\nprotein networks for complex diseases such as cancer. Most existing methods\nestimate a single graph for a population, while in many cases, researchers are\ninterested in characterizing the heterogeneity of individual networks across\nsubjects with respect to subject-level covariates. Examples include assessments\nof how the network varies with patient-specific prognostic scores or\ncomparisons of tumor and normal graphs while accounting for tumor purity as a\ncontinuous predictor. In this paper, we propose a novel edge regression model\nfor undirected graphs, which estimates conditional dependencies as a function\nof subject-level covariates. Bayesian shrinkage algorithms are used to induce\nsparsity in the underlying graphical models. We assess our model performance\nthrough simulation studies focused on comparing tumor and normal graphs while\nadjusting for tumor purity and a case study assessing how blood protein\nnetworks in hepatocellular carcinoma patients vary with severity of disease,\nmeasured by HepatoScore, a novel biomarker signature measuring disease\nseverity.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 21:24:53 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Wang", "Zeya", ""], ["Baladandayuthapan", "Veera", ""], ["Kaseb", "Ahmed O.", ""], ["Amin", "Hesham M.", ""], ["Hassan", "Manal M.", ""], ["Wang", "Wenyi", ""], ["Morris", "Jeffrey S.", ""]]}, {"id": "2101.09689", "submitter": "Ni Ding Dr", "authors": "Ni Ding and Yucheng Liu and Farhad Farokhi", "title": "A Linear Reduction Method for Local Differential Privacy and Log-lift", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of publishing data $X$ while protecting\ncorrelated sensitive information $S$. We propose a linear method to generate\nthe sanitized data $Y$ with the same alphabet $\\mathcal{Y} = \\mathcal{X}$ that\nattains local differential privacy (LDP) and log-lift at the same time. It is\nrevealed that both LDP and log-lift are inversely proportional to the\nstatistical distance between conditional probability $P_{Y|S}(x|s)$ and\nmarginal probability $P_{Y}(x)$: the closer the two probabilities are, the more\nprivate $Y$ is. Specifying $P_{Y|S}(x|s)$ that linearly reduces this distance\n$|P_{Y|S}(x|s) - P_Y(x)| = (1-\\alpha)|P_{X|S}(x|s) - P_X(x)|,\\forall s,x$ for\nsome $\\alpha \\in (0,1]$, we study the problem of how to generate $Y$ from the\noriginal data $S$ and $X$. The Markov randomization/sanitization scheme\n$P_{Y|X}(x|x') = P_{Y|S,X}(x|s,x')$ is obtained by solving linear equations.\nThe optimal non-Markov sanitization, the transition probability\n$P_{Y|S,X}(x|s,x')$ that depends on $S$, can be determined by maximizing the\ndata utility subject to linear equality constraints. We compute the solution\nfor two linear utility function: the expected distance and total variance\ndistance. It is shown that the non-Markov randomization significantly improves\ndata utility and the marginal probability $P_X(x)$ remains the same after the\nlinear sanitization method: $P_Y(x) = P_X(x), \\forall x \\in \\mathcal{X}$.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 11:18:22 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 10:35:28 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Ding", "Ni", ""], ["Liu", "Yucheng", ""], ["Farokhi", "Farhad", ""]]}, {"id": "2101.09942", "submitter": "Tingnan Gong", "authors": "Tingnan Gong, Yu Chen and Weiping Zhang", "title": "An Environmentally-Adaptive Hawkes Process with An Application to\n  COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We proposed a new generalized model based on the classical Hawkes process\nwith environmental multipliers, which is called an environmentally-adaptive\nHawkes (EAH) model. Compared to the classical self-exciting Hawkes process, the\nEAH model exhibits more flexibility in a macro environmentally temporal sense,\nand can model more complex processes by using dynamic branching matrix. We\ndemonstrate the well-definedness of this EAH model. A more specified version of\nthis new model is applied to model COVID-19 pandemic data through an efficient\nEM-like algorithm. Consequently, the proposed model consistently outperforms\nthe classical Hawkes process.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 08:10:11 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Gong", "Tingnan", ""], ["Chen", "Yu", ""], ["Zhang", "Weiping", ""]]}, {"id": "2101.09960", "submitter": "Guilhem Cassan", "authors": "Guilhem Cassan and Milan Van Steenvoort", "title": "Political Regime and COVID 19 death rate: efficient, biasing or simply\n  different autocracies ?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The difference in COVID 19 death rates across political regimes has caught a\nlot of attention. The \"efficient autocracy\" view suggests that autocracies may\nbe more efficient at putting in place policies that contain COVID 19 spread. On\nthe other hand, the \"biasing autocracy\" view underlines that autocracies may be\nunder reporting their COVID 19 data. We use fixed effect panel regression\nmethods to discriminate between the two sides of the debate. Our results show\nthat a third view may in fact be prevailing: once pre-determined\ncharacteristics of countries are accounted for, COVID 19 death rates equalize\nacross political regimes. The difference in death rate across political regime\nseems therefore to be primarily due to omitted variable bias.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 08:58:21 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Cassan", "Guilhem", ""], ["Van Steenvoort", "Milan", ""]]}, {"id": "2101.10005", "submitter": "Yasin Memari", "authors": "Yasin Memari", "title": "Low incidence rate of COVID-19 undermines confidence in estimation of\n  the vaccine efficacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowing the true effect size of clinical interventions in randomised clinical\ntrials is key to informing the public health policies. Vaccine efficacy is\ndefined in terms of the relative risk or the ratio of two disease risks.\nHowever, only approximate methods are available for estimating the variance of\nthe relative risk. In this article, we show using a probabilistic model that\nuncertainty in the efficacy rate could be underestimated when the disease risk\nis low. Factoring in the baseline rate of the disease, we estimate broader\nconfidence intervals for the efficacy rates of the vaccines recently developed\nfor COVID-19. We propose new confidence intervals for the relative risk. We\nfurther show that sample sizes required for phase 3 efficacy trials are\nroutinely underestimated and propose a new method for sample size calculation\nwhere the efficacy is of interest. We also discuss the deleterious effects of\nclassification bias which is particularly relevant at low disease prevalence.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 10:45:42 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 14:51:26 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Memari", "Yasin", ""]]}, {"id": "2101.10103", "submitter": "Arnald Puy", "authors": "Arnald Puy, Samuele Lo Piano, Andrea Saltelli, Simon A. Levin", "title": "sensobol: an R package to compute variance-based sensitivity indices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The R package \"sensobol\" provides several functions to conduct variance-based\nuncertainty and sensitivity analysis, from the estimation of sensitivity\nindices to the visual representation of the results. It implements several\nstate-of-the-art first and total-order estimators and allows the computation of\nup to third-order effects, as well as of the approximation error, in a swift\nand user-friendly way. Its flexibility makes it also appropriate for models\nwith either a scalar or a multivariate output. We illustrate its functionality\nby conducting a variance-based sensitivity analysis of three classic models:\nthe Sobol' (1998) G function, the logistic population growth model of Verhulst\n(1845), and the spruce budworm and forest model of Ludwig, Jones and Holling\n(1976).\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 12:36:39 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 14:09:38 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Puy", "Arnald", ""], ["Piano", "Samuele Lo", ""], ["Saltelli", "Andrea", ""], ["Levin", "Simon A.", ""]]}, {"id": "2101.10266", "submitter": "Rohan Sukumaran", "authors": "Rohan Sukumaran, Parth Patwa, T V Sethuraman, Sheshank Shankar,\n  Rishank Kanaparti, Joseph Bae, Yash Mathur, Abhishek Singh, Ayush Chopra,\n  Myungsun Kang, Priya Ramaswamy and Ramesh Raskar", "title": "COVID-19 Outbreak Prediction and Analysis using Self Reported Symptoms", "comments": "15 pages, 16 Figures - Latest version on the Journal of Behavioural\n  Data Science - https://isdsa.org/_media/jbds/v1n1/v1n1p8.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is crucial for policymakers to understand the community prevalence of\nCOVID-19 so combative resources can be effectively allocated and prioritized\nduring the COVID-19 pandemic. Traditionally, community prevalence has been\nassessed through diagnostic and antibody testing data. However, despite the\nincreasing availability of COVID-19 testing, the required level has not been\nmet in most parts of the globe, introducing a need for an alternative method\nfor communities to determine disease prevalence. This is further complicated by\nthe observation that COVID-19 prevalence and spread varies across different\nspatial, temporal, and demographics. In this study, we understand trends in the\nspread of COVID-19 by utilizing the results of self-reported COVID-19 symptoms\nsurveys as an alternative to COVID-19 testing reports. This allows us to assess\ncommunity disease prevalence, even in areas with low COVID-19 testing ability.\nUsing individually reported symptom data from various populations, our method\npredicts the likely percentage of the population that tested positive for\nCOVID-19. We do so with a Mean Absolute Error (MAE) of 1.14 and Mean Relative\nError (MRE) of 60.40\\% with 95\\% confidence interval as (60.12, 60.67). This\nimplies that our model predicts +/- 1140 cases than the original in a\npopulation of 1 million. In addition, we forecast the location-wise percentage\nof the population testing positive for the next 30 days using self-reported\nsymptoms data from previous days. The MAE for this method is as low as 0.15\n(MRE of 23.61\\% with 95\\% confidence interval as (23.6, 13.7)) for New York. We\npresent an analysis of these results, exposing various clinical attributes of\ninterest across different demographics. Lastly, we qualitatively analyze how\nvarious policy enactments (testing, curfew) affect the prevalence of COVID-19\nin a community.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 00:37:24 GMT"}, {"version": "v2", "created": "Sat, 19 Jun 2021 17:07:03 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Sukumaran", "Rohan", ""], ["Patwa", "Parth", ""], ["Sethuraman", "T V", ""], ["Shankar", "Sheshank", ""], ["Kanaparti", "Rishank", ""], ["Bae", "Joseph", ""], ["Mathur", "Yash", ""], ["Singh", "Abhishek", ""], ["Chopra", "Ayush", ""], ["Kang", "Myungsun", ""], ["Ramaswamy", "Priya", ""], ["Raskar", "Ramesh", ""]]}, {"id": "2101.10359", "submitter": "Francis Diebold", "authors": "Francis X. Diebold and Maximilian Gobel", "title": "Real-Time Fixed-Target Statistical Prediction of Arctic Sea Ice Extent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple statistical approach for fixed-target forecasting of\nArctic sea ice extent, and we provide a case study of its real-time performance\nfor target date September 2020. The real-time forecasting begins in early June\nand proceeds through late September. We visually detail the evolution of the\nstatistically-optimal point, interval, and density forecasts as time passes,\nnew information arrives, and the end of September approaches. Among other\nthings, our visualizations may provide useful windows for assessing the\nagreement between dynamical climate models and observational data.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 19:11:24 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 12:50:49 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Diebold", "Francis X.", ""], ["Gobel", "Maximilian", ""]]}, {"id": "2101.10383", "submitter": "Francisco Corona", "authors": "Francisco Corona, Graciela Gonz\\'alez-Far\\'ias and Jes\\'us\n  L\\'opez-P\\'erez", "title": "A nowcasting approach to generate timely estimates of Mexican economic\n  activity: An application to the period of COVID-19", "comments": "29 pages, 8 figures, 1 table and 1 annex", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a new approach based on dynamic factor models\n(DFMs) to perform nowcasts for the percentage annual variation of the Mexican\nGlobal Economic Activity Indicator (IGAE in Spanish). The procedure consists of\nthe following steps: i) build a timely and correlated database by using\neconomic and financial time series and real-time variables such as social\nmobility and significant topics extracted by Google Trends; ii) estimate the\ncommon factors using the two-step methodology of Doz et al. (2011); iii) use\nthe common factors in univariate time-series models for test data; and iv)\naccording to the best results obtained in the previous step, combine the\nstatistically equal better nowcasts (Diebold-Mariano test) to generate the\ncurrent nowcasts. We obtain timely and accurate nowcasts for the IGAE,\nincluding those for the current phase of drastic drops in the economy related\nto COVID-19 sanitary measures. Additionally, the approach allows us to\ndisentangle the key variables in the DFM by estimating the confidence interval\nfor both the factor loadings and the factor estimates. This approach can be\nused in official statistics to obtain preliminary estimates for IGAE up to 50\ndays before the official results.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 20:10:34 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Corona", "Francisco", ""], ["Gonz\u00e1lez-Far\u00edas", "Graciela", ""], ["L\u00f3pez-P\u00e9rez", "Jes\u00fas", ""]]}, {"id": "2101.10466", "submitter": "Andrew Spieker", "authors": "Nicholas Illenberger, Nandita Mitra, Andrew J. Spieker", "title": "A regression framework for a probabilistic measure of cost-effectiveness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To make informed health policy decisions regarding a treatment, we must\nconsider both its cost and its clinical effectiveness. In past work, we\nintroduced the net benefit separation (NBS) as a novel measure of\ncost-effectiveness. The NBS is a probabilistic measure that characterizes the\nextent to which a treated patient will be more likely to experience benefit as\ncompared to an untreated patient. Due to variation in treatment response across\npatients, uncovering factors that influence cost-effectiveness can assist\npolicy makers in population-level decisions regarding resource allocation. In\nthis paper, we introduce a regression framework for NBS in order to estimate\ncovariate-specific NBS and find determinants of variation in NBS. Our approach\nis able to accommodate informative cost censoring through inverse probability\nweighting techniques, and addresses confounding through a semiparametric\nstandardization procedure. Through simulations, we show that NBS regression\nperforms well in a variety of common scenarios. We apply our proposed\nregression procedure to a realistic simulated data set as an illustration of\nhow our approach could be used to investigate the association between cancer\nstage, comorbidities and cost-effectiveness when comparing adjuvant radiation\ntherapy and chemotherapy in post-hysterectomy endometrial cancer patients.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 23:01:29 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Illenberger", "Nicholas", ""], ["Mitra", "Nandita", ""], ["Spieker", "Andrew J.", ""]]}, {"id": "2101.10515", "submitter": "Rui Zhang", "authors": "Rui Zhang, Junting Chen, Yao Xie, Alexander Shapiro, Urbashi Mitra", "title": "Testing Rank of Incomplete Unimodal Matrices", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2021.3070524", "report-no": null, "categories": "stat.AP math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several statistics-based detectors, based on unimodal matrix models, for\ndetermining the number of sources in a field are designed. A new variance ratio\nstatistic is proposed, and its asymptotic distribution is analyzed. The\nvariance ratio detector is shown to outperform the alternatives. It is shown\nthat further improvements are achievable via optimally selected rotations.\nNumerical experiments demonstrate the performance gains of our detection\nmethods over the baseline approach.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 01:52:52 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Zhang", "Rui", ""], ["Chen", "Junting", ""], ["Xie", "Yao", ""], ["Shapiro", "Alexander", ""], ["Mitra", "Urbashi", ""]]}, {"id": "2101.10589", "submitter": "Mehul S. Raval", "authors": "Snehal Rajput, Rupal Agravat, Mohendra Roy, Mehul S Raval", "title": "Glioblastoma Multiforme Patient Survival Prediction", "comments": "10 pages, 9 figures", "journal-ref": "2021 International Conference on Medical Imaging and\n  Computer-Aided Diagnosis (MICAD 2021)", "doi": null, "report-no": null, "categories": "eess.IV cs.CV stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Glioblastoma Multiforme is a very aggressive type of brain tumor. Due to\nspatial and temporal intra-tissue inhomogeneity, location and the extent of the\ncancer tissue, it is difficult to detect and dissect the tumor regions. In this\npaper, we propose survival prognosis models using four regressors operating on\nhandcrafted image-based and radiomics features. We hypothesize that the\nradiomics shape features have the highest correlation with survival prediction.\nThe proposed approaches were assessed on the Brain Tumor Segmentation\n(BraTS-2020) challenge dataset. The highest accuracy of image features with\nrandom forest regressor approach was 51.5\\% for the training and 51.7\\% for the\nvalidation dataset. The gradient boosting regressor with shape features gave an\naccuracy of 91.5\\% and 62.1\\% on training and validation datasets respectively.\nIt is better than the BraTS 2020 survival prediction challenge winners on the\ntraining and validation datasets. Our work shows that handcrafted features\nexhibit a strong correlation with survival prediction. The consensus based\nregressor with gradient boosting and radiomics shape features is the best\ncombination for survival prediction.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 06:47:14 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Rajput", "Snehal", ""], ["Agravat", "Rupal", ""], ["Roy", "Mohendra", ""], ["Raval", "Mehul S", ""]]}, {"id": "2101.10597", "submitter": "Christophe Ley", "authors": "Hans Van Eetvelde and Lars Magnus Hvattum and Christophe Ley", "title": "The Probabilistic Final Standing Calculator: a fair stochastic tool to\n  handle abruptly stopped football seasons", "comments": "4 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic has left its marks in the sports world, forcing the\nfull-stop of all sports-related activities in the first half of 2020. Football\nleagues were suddenly stopped and each country was hesitating between a\nrelaunch of the competition and a premature ending. Some opted for the latter\noption, and took as the final standing of the season the ranking from the\nmoment the competition got interrupted. This decision has been perceived as\nunfair, especially by those teams who had remaining matches against easier\nopponents. In this paper, we introduce a tool to calculate in a fairer way the\nfinal standings of domestic leagues that have to stop prematurely: our\nProbabilistic Final Standing Calculator (PFSC). It is based on a stochastic\nmodel taking into account the results of the matches played and simulating the\nremaining matches, yielding the probabilities for the various possible final\nrankings. We have compared our PFSC with state-of-the-art prediction models,\nusing previous seasons which we pretend to stop at different points in time. We\nillustrate our PFSC by showing how a probabilistic ranking of the French Ligue\n1 in the stopped 2019-2020 season could have led to alternative, potentially\nfairer, decisions on the final standing.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 07:14:40 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Van Eetvelde", "Hans", ""], ["Hvattum", "Lars Magnus", ""], ["Ley", "Christophe", ""]]}, {"id": "2101.10640", "submitter": "Paul Platzer", "authors": "Paul Platzer, Pascal Yiou (ESTIMR), Philippe Naveau (ESTIMR),\n  Jean-Fran\\c{c}ois Filipot, Maxime Thiebaut, Pierre Tandeo (IMT Atlantique -\n  SC)", "title": "Probability distributions for analog-to-target distances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some properties of chaotic dynamical systems can be probed through features\nof recurrences, also called analogs. In practice, analogs are nearest\nneighbours of the state of a system, taken from a large database called the\ncatalog. Analogs have been used in many atmospheric applications including\nforecasts, downscaling, predictability estimation, and attribution of extreme\nevents. The distances of the analogs to the target state condition the\nperformances of analog applications. These distances can be viewed as random\nvariables, and their probability distributions can be related to the catalog\nsize and properties of the system at stake. A few studies have focused on the\nfirst moments of return time statistics for the best analog, fixing an\nobjective of maximum distance from this analog to the target state. However,\nfor practical use and to reduce estimation variance, applications usually\nrequire not just one, but many analogs. In this paper, we evaluate from a\ntheoretical standpoint and with numerical experiments the probability\ndistributions of the $K$-best analog-to-target distances. We show that\ndimensionality plays a role on the size of the catalog needed to find good\nanalogs, and also on the relative means and variances of the $K$-best analogs.\nOur results are based on recently developed tools from dynamical systems\ntheory. These findings are illustrated with numerical simulations of a\nwell-known chaotic dynamical system and on 10m-wind reanalysis data in\nnorth-west France. A practical application of our derivations for the purpose\nof objective-based dimension reduction is shown using the same reanalysis data.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 09:10:12 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Platzer", "Paul", "", "ESTIMR"], ["Yiou", "Pascal", "", "ESTIMR"], ["Naveau", "Philippe", "", "ESTIMR"], ["Filipot", "Jean-Fran\u00e7ois", "", "IMT Atlantique -\n  SC"], ["Thiebaut", "Maxime", "", "IMT Atlantique -\n  SC"], ["Tandeo", "Pierre", "", "IMT Atlantique -\n  SC"]]}, {"id": "2101.10645", "submitter": "Marco Di Stefano", "authors": "Stefano Franzini, Marco Di Stefano and Cristian Micheletti", "title": "essHi-C: Essential component analysis of Hi-C matrices", "comments": "14 pages, 4 figures. This is the Authors' Original Version of the\n  article, which has been accepted for publication in Bioinformatics published\n  by Oxford University Press", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN q-bio.BM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Hi-C matrices are cornerstones for qualitative and quantitative\nstudies of genome folding, from its territorial organization to compartments\nand topological domains. The high dynamic range of genomic distances probed in\nHi-C assays reflects in an inherent stochastic background of the interactions\nmatrices, which inevitably convolve the features of interest with largely\naspecific ones. Results: Here we introduce a discuss essHi-C, a method to\nisolate the specific, or essential component of Hi-C matrices from the\naspecific portion of the spectrum that is compatible with random matrices.\nSystematic comparisons show that essHi-C improves the clarity of the\ninteraction patterns, enhances the robustness against sequencing depth, allows\nthe unsupervised clustering of experiments in different cell lines and recovers\nthe cell-cycle phasing of single-cells based on Hi-C data. Thus, essHi-C\nprovides means for isolating significant biological and physical features from\nHi-C matrices.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 09:17:29 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Franzini", "Stefano", ""], ["Di Stefano", "Marco", ""], ["Micheletti", "Cristian", ""]]}, {"id": "2101.10719", "submitter": "Pedro Cadahia Delgado", "authors": "Pedro Cadah\\'ia and Jose Manuel Bravo Caro", "title": "Short-term prediction of Time Series based on bounding techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper it is reconsidered the prediction problem in time series\nframework by using a new non-parametric approach. Through this reconsideration,\nthe prediction is obtained by a weighted sum of past observed data. These\nweights are obtained by solving a constrained linear optimization problem that\nminimizes an outer bound of the prediction error. The innovation is to consider\nboth deterministic and stochastic assumptions in order to obtain the upper\nbound of the prediction error, a tuning parameter is used to balance these\ndeterministic-stochastic assumptions in order to improve the predictor\nperformance. A benchmark is included to illustrate that the proposed predictor\ncan obtain suitable results in a prediction scheme, and can be an interesting\nalternative method to the classical non-parametric methods. Besides, it is\nshown how this model can outperform the preexisting ones in a short term\nforecast.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 11:27:36 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Cadah\u00eda", "Pedro", ""], ["Caro", "Jose Manuel Bravo", ""]]}, {"id": "2101.10853", "submitter": "Tsuyoshi Kato", "authors": "HongYuan Cao and Tsuyoshi Kato", "title": "Asymmetric Tobit analysis for correlation estimation from censored data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Contamination of water resources with pathogenic microorganisms excreted in\nhuman feces is a worldwide public health concern. Surveillance of fecal\ncontamination is commonly performed by routine monitoring for a single type or\na few types of microorganism(s). To design a feasible routine for periodic\nmonitoring and to control risks of exposure to pathogens, reliable statistical\nalgorithms for inferring correlations between concentrations of microorganisms\nin water need to be established. Moreover, because pathogens are often present\nin low concentrations, some contaminations are likely to be under a detection\nlimit. This yields a pairwise left-censored dataset and complicates computation\nof correlation coefficients. Errors of correlation estimation can be smaller if\nundetected values are imputed better. To obtain better imputations, we utilize\nside information and develop a new technique, the \\emph{asymmetric Tobit model}\nwhich is an extension of the Tobit model so that domain knowledge can be\nexploited effectively when fitting the model to a censored dataset. The\nempirical results demonstrate that imputation with domain knowledge is\neffective for this task.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 12:52:44 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Cao", "HongYuan", ""], ["Kato", "Tsuyoshi", ""]]}, {"id": "2101.10880", "submitter": "Richard Samworth", "authors": "Thomas B. Berrett and Richard J. Samworth", "title": "USP: an independence test that improves on Pearson's chi-squared and the\n  $G$-test", "comments": "27 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the $U$-Statistic Permutation (USP) test of independence in the\ncontext of discrete data displayed in a contingency table. Either Pearson's\nchi-squared test of independence, or the $G$-test, are typically used for this\ntask, but we argue that these tests have serious deficiencies, both in terms of\ntheir inability to control the size of the test, and their power properties. By\ncontrast, the USP test is guaranteed to control the size of the test at the\nnominal level for all sample sizes, has no issues with small (or zero) cell\ncounts, and is able to detect distributions that violate independence in only a\nminimal way. The test statistic is derived from a $U$-statistic estimator of a\nnatural population measure of dependence, and we prove that this is the unique\nminimum variance unbiased estimator of this population quantity. The practical\nutility of the USP test is demonstrated on both simulated data, where its power\ncan be dramatically greater than those of Pearson's test and the $G$-test, and\non real data. The USP test is implemented in the R package USP.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 15:42:44 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Berrett", "Thomas B.", ""], ["Samworth", "Richard J.", ""]]}, {"id": "2101.10896", "submitter": "Emiliano Valdez", "authors": "Shuang Yin, Guojun Gan, Emiliano A. Valdez, Jeyaraj Vadiveloo", "title": "Applications of Clustering with Mixed Type Data in Life Insurance", "comments": "25 pages, 6 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Death benefits are generally the largest cash flow item that affects\nfinancial statements of life insurers where some still do not have a systematic\nprocess to track and monitor death claims experience. In this article, we\nexplore data clustering to examine and understand how actual death claims\ndiffer from expected, an early stage of developing a monitoring system crucial\nfor risk management. We extend the $k$-prototypes clustering algorithm to draw\ninference from a life insurance dataset using only the insured's\ncharacteristics and policy information without regard to known mortality. This\nclustering has the feature to efficiently handle categorical, numerical, and\nspatial attributes. Using gap statistics, the optimal clusters obtained from\nthe algorithm are then used to compare actual to expected death claims\nexperience of the life insurance portfolio. Our empirical data contains\nobservations, during 2014, of approximately 1.14 million policies with a total\ninsured amount of over 650 billion dollars. For this portfolio, the algorithm\nproduced three natural clusters, with each cluster having a lower actual to\nexpected death claims but with differing variability. The analytical results\nprovide management a process to identify policyholders' attributes that\ndominate significant mortality deviations, and thereby enhance decision making\nfor taking necessary actions.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 16:04:36 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Yin", "Shuang", ""], ["Gan", "Guojun", ""], ["Valdez", "Emiliano A.", ""], ["Vadiveloo", "Jeyaraj", ""]]}, {"id": "2101.10998", "submitter": "Cong Shen", "authors": "Hyun-Suk Lee, Cong Shen, William Zame, Jang-Won Lee, Mihaela van der\n  Schaar", "title": "SDF-Bayes: Cautious Optimism in Safe Dose-Finding Clinical Trials with\n  Drug Combinations and Heterogeneous Patient Groups", "comments": "Accepted to AISTATS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phase I clinical trials are designed to test the safety (non-toxicity) of\ndrugs and find the maximum tolerated dose (MTD). This task becomes\nsignificantly more challenging when multiple-drug dose-combinations (DC) are\ninvolved, due to the inherent conflict between the exponentially increasing DC\ncandidates and the limited patient budget. This paper proposes a novel Bayesian\ndesign, SDF-Bayes, for finding the MTD for drug combinations in the presence of\nsafety constraints. Rather than the conventional principle of escalating or\nde-escalating the current dose of one drug (perhaps alternating between drugs),\nSDF-Bayes proceeds by cautious optimism: it chooses the next DC that, on the\nbasis of current information, is most likely to be the MTD (optimism), subject\nto the constraint that it only chooses DCs that have a high probability of\nbeing safe (caution). We also propose an extension, SDF-Bayes-AR, that accounts\nfor patient heterogeneity and enables heterogeneous patient recruitment.\nExtensive experiments based on both synthetic and real-world datasets\ndemonstrate the advantages of SDF-Bayes over state of the art DC trial designs\nin terms of accuracy and safety.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 18:59:26 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Lee", "Hyun-Suk", ""], ["Shen", "Cong", ""], ["Zame", "William", ""], ["Lee", "Jang-Won", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "2101.11179", "submitter": "Minghe Zhang", "authors": "Minghe Zhang, Chen Xu, Andy Sun, Feng Qiu, Yao Xie", "title": "Solar Radiation Anomaly Events Modeling Using Spatial-Temporal Mutually\n  Interactive Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling and predicting solar events, in particular, the solar ramping event\nis critical for improving situational awareness for solar power generation\nsystems. Solar ramping events are significantly impacted by weather conditions\nsuch as temperature, humidity, and cloud density. Discovering the correlation\nbetween different locations and times is a highly challenging task since the\nsystem is complex and noisy. We propose a novel method to model and predict\nramping events from spatial-temporal sequential solar radiation data based on a\nspatio-temporal interactive Bernoulli process. We demonstrate the good\nperformance of our approach on real solar radiation datasets.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 03:02:39 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Zhang", "Minghe", ""], ["Xu", "Chen", ""], ["Sun", "Andy", ""], ["Qiu", "Feng", ""], ["Xie", "Yao", ""]]}, {"id": "2101.11182", "submitter": "Jon Steingrimsson", "authors": "Jon A. Steingrimsson, Constantine Gatsonis, Issa J. Dahabreh", "title": "Transporting a prediction model for use in a new target population", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider methods for transporting a prediction model and assessing its\nperformance for use in a new target population, when outcome and covariate\ninformation for model development is available from a simple random sample from\nthe source population, but only covariate information is available on a simple\nrandom sample from the target population. We discuss how to tailor the\nprediction model for use in the target population, how to assess model\nperformance in the target population (e.g., by estimating the target population\nmean squared error), and how to perform model and tuning parameter selection in\nthe context of the target population. We provide identifiability results for\nthe target population mean squared error of a potentially misspecified\nprediction model under a sampling design where the source study and the target\npopulation samples are obtained separately. We also introduce the concept of\nprediction error modifiers that can be used to reason about the need for\ntailoring measures of model performance to the target population and provide an\nillustration of the methods using simulated data.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 03:11:08 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 15:15:19 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Steingrimsson", "Jon A.", ""], ["Gatsonis", "Constantine", ""], ["Dahabreh", "Issa J.", ""]]}, {"id": "2101.11190", "submitter": "Xiao Liu", "authors": "Reza Iranzad, Xiao Liu, W. Art Chaovalitwongse, Daniel S. Hippe,\n  Shouyi Wang, Jie Han, Phawis Thammasorn, Chunyan Duan, Jing Zeng, Stephen R.\n  Bowen", "title": "Boost-S: Gradient Boosted Trees for Spatial Data and Its Application to\n  FDG-PET Imaging Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Boosting Trees are one of the most successful statistical learning approaches\nthat involve sequentially growing an ensemble of simple regression trees (i.e.,\n\"weak learners\"). However, gradient boosted trees are not yet available for\nspatially correlated data. This paper proposes a new gradient Boosted Trees\nalgorithm for Spatial Data (Boost-S) with covariate information. Boost-S\nintegrates the spatial correlation structure into the classical framework of\ngradient boosted trees. Each tree is grown by solving a regularized\noptimization problem, where the objective function involves two penalty terms\non tree complexity and takes into account the underlying spatial correlation. A\ncomputationally-efficient algorithm is proposed to obtain the ensemble trees.\nThe proposed Boost-S is applied to the spatially-correlated FDG-PET\n(fluorodeoxyglucose-positron emission tomography) imaging data collected during\ncancer chemoradiotherapy. Our numerical investigations successfully demonstrate\nthe advantages of the proposed Boost-S over existing approaches for this\nparticular application.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 04:08:27 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 20:39:53 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Iranzad", "Reza", ""], ["Liu", "Xiao", ""], ["Chaovalitwongse", "W. Art", ""], ["Hippe", "Daniel S.", ""], ["Wang", "Shouyi", ""], ["Han", "Jie", ""], ["Thammasorn", "Phawis", ""], ["Duan", "Chunyan", ""], ["Zeng", "Jing", ""], ["Bowen", "Stephen R.", ""]]}, {"id": "2101.11202", "submitter": "Cong Xu", "authors": "Cong Xu, Hans Moritz G\\\"unther, Vinay L. Kashyap, Thomas C. M. Lee,\n  Andreas Zezas", "title": "Change point detection and image segmentation for time series of\n  astrophysical images", "comments": "22 pages, 10 figures", "journal-ref": null, "doi": "10.3847/1538-3881/abe0b6", "report-no": null, "categories": "astro-ph.IM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many astrophysical phenomena are time-varying, in the sense that their\nintensity, energy spectrum, and/or the spatial distribution of the emission\nsuddenly change. This paper develops a method for modeling a time series of\nimages. Under the assumption that the arrival times of the photons follow a\nPoisson process, the data are binned into 4D grids of voxels (time, energy\nband, and x-y coordinates), and viewed as a time series of non-homogeneous\nPoisson images. The method assumes that at each time point, the corresponding\nmulti-band image stack is an unknown 3D piecewise constant function including\nPoisson noise. It also assumes that all image stacks between any two adjacent\nchange points (in time domain) share the same unknown piecewise constant\nfunction. The proposed method is designed to estimate the number and the\nlocations of all the change points (in time domain), as well as all the unknown\npiecewise constant functions between any pairs of the change points. The method\napplies the minimum description length (MDL) principle to perform this task. A\npractical algorithm is also developed to solve the corresponding complicated\noptimization problem. Simulation experiments and applications to real datasets\nshow that the proposed method enjoys very promising empirical properties.\nApplications to two real datasets, the XMM observation of a flaring star and an\nemerging solar coronal loop, illustrate the usage of the proposed method and\nthe scientific insight gained from it.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 04:37:53 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Xu", "Cong", ""], ["G\u00fcnther", "Hans Moritz", ""], ["Kashyap", "Vinay L.", ""], ["Lee", "Thomas C. M.", ""], ["Zezas", "Andreas", ""]]}, {"id": "2101.11208", "submitter": "Fotis Kopsaftopoulos", "authors": "Ahmad Amer, Fotis Kopsaftopoulos", "title": "Statistical guided-waves-based SHM via stochastic non-parametric time\n  series models", "comments": "37 pages, 21 figures", "journal-ref": null, "doi": "10.1177/14759217211024527", "report-no": null, "categories": "eess.SP stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Damage detection in active-sensing, guided-waves-based Structural Health\nMonitoring (SHM) has evolved through multiple eras of development during the\npast decades. Nevertheless, there still exists a number of challenges facing\nthe current state-of-the-art approaches, both in the industry as well as in\nresearch and development, including low damage sensitivity, lack of robustness\nto uncertainties, need for user-defined thresholds, and non-uniform response\nacross a sensor network. In this work, a novel statistical framework is\nproposed for active-sensing SHM based on the use of ultrasonic guided waves.\nThis framework is based on stochastic non-parametric time series models and\ntheir corresponding statistical properties in order to readily provide healthy\nconfidence bounds and enable accurate and robust damage detection via the use\nof appropriate statistical decision making tests. Three such methods and\ncorresponding statistical quantities (test statistics) along with decision\nmaking schemes are formulated and experimentally assessed via the use of three\ncoupons with different levels of complexity: an Al plate with a growing notch,\na Carbon fiber-reinforced plastic (CFRP) plate with added weights to simulate\nlocal damages, and the CFRP panel used in the Open Guided Waves project [1],\nall fitted with piezoelectric transducers and a pitch-catch configuration. The\nperformance of the proposed methods is compared to that of state-of-the-art\ntime-domain damage indices (DIs). The results demonstrate the increased\nsensitivity and robustness of the proposed methods, with better tracking\ncapability of damage evolution compared to conventional approaches, even for\ndamage-non-intersecting actuator-sensor paths. Overall, the proposed\nstatistical methods exhibit greater damage sensitivity across different\ncomponents, with enhanced robustness to uncertainty, as well as user-friendly\napplication.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 05:13:56 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 20:25:43 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Amer", "Ahmad", ""], ["Kopsaftopoulos", "Fotis", ""]]}, {"id": "2101.11446", "submitter": "Zhi Xua Lian", "authors": "Z.X.Lian", "title": "A study on information behavior of scholars for article keywords\n  selection", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This project takes the factors of keyword selection behavior as the research\nobject. Qualitative analysis methods such as interview and grounded theory were\nused to construct causal influence path model. Combined with computer\nsimulation technology such as multi-agent simulation experiment method was used\nto study the factors of keyword selection from two dimensions of individual to\ngroup. The research was carried out according to the path of factor analysis at\nindividual level macro situation simulation optimization of scientific research\ndata management. Based on the aforementioned review of existing researches and\nexplanations of keywords selection, this study adopts a qualitative research\ndesign to expand the explanation, and macro simulation based on the results of\nqualitative research. There are two steps in this study, one is do interview\nwith authors and then design macro simulation according the deductive and\nqualitative content analysis results.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 14:25:36 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Lian", "Z. X.", ""]]}, {"id": "2101.11533", "submitter": "Timothy Park", "authors": "Timothy Park, Franz J. Kiraly and Stephen J. Bourne", "title": "Periodic seismicity detection without declustering", "comments": "55 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Any periodic variations of earthquake occurrence rates in response to small,\nknown, periodic stress variations provide important opportunities to learn\nabout the earthquake nucleation process. Yet, reliable detection of earthquake\nperiodicity is complicated by the presence of earthquake clustering due to\naftershocks and foreshocks. Existing methods for detecting periodicity in an\nearthquake catalogue typically require the prior removal of these clustered\nevents. Declustering is a highly uncertain process, so declustering methods are\ninherently non-unique. Incorrect declustering may remove some independent\nevents, or fail to remove some aftershocks or foreshocks, or both. These two\ntypes of error could respectively lead to false negative or false positive\nreporting of periodic seismicity. To overcome these limitations, we propose a\nnew method for detecting earthquake periodicity that does not require\ndeclustering. Our approach is to modify the existing Schuster Spectrum Test\n(SST) by adapting a test statistic for periodic seismicity to account for the\npresence of clustered earthquakes within the catalogue without requiring their\nidentification and removal.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 16:37:30 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Park", "Timothy", ""], ["Kiraly", "Franz J.", ""], ["Bourne", "Stephen J.", ""]]}, {"id": "2101.11583", "submitter": "Sally Paganin", "authors": "Sally Paganin, Christopher J. Paciorek, Claudia Wehrhahn, Abel\n  Rodriguez, Sophia Rabe-Hesketh, Perry de Valpine", "title": "Computational methods for Bayesian semiparametric Item Response Theory\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Item response theory (IRT) models are widely used to obtain interpretable\ninference when analyzing data from questionnaires, scaling binary responses\ninto continuous constructs. Typically, these models rely on a normality\nassumption for the latent trait characterizing individuals in the population\nunder study. However, this assumption can be unrealistic and lead to biased\nresults. We relax the normality assumption by considering a flexible Dirichlet\nProcess mixture model as a nonparametric prior on the distribution of the\nindividual latent traits. Although this approach has been considered in the\nliterature before, there is a lack of comprehensive studies of such models or\ngeneral software tools. To fill this gap, we show how the NIMBLE framework for\nhierarchical statistical modeling enables the use of flexible priors on the\nlatent trait distribution, specifically illustrating the use of Dirichlet\nProcess mixtures in two-parameter logistic (2PL) IRT models. We study how\ndifferent sets of constraints can lead to model identifiability and give\nguidance on eliciting prior distributions. Using both simulated and real-world\ndata, we conduct an in-depth study of Markov chain Monte Carlo posterior\nsampling efficiency for several sampling strategies. We conclude that having\naccess to semiparametric models can be broadly useful, as it allows inference\non the entire underlying ability distribution and its functionals, with NIMBLE\nbeing a flexible framework for estimation of such models.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 18:17:24 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Paganin", "Sally", ""], ["Paciorek", "Christopher J.", ""], ["Wehrhahn", "Claudia", ""], ["Rodriguez", "Abel", ""], ["Rabe-Hesketh", "Sophia", ""], ["de Valpine", "Perry", ""]]}, {"id": "2101.11881", "submitter": "Rohitash Chandra", "authors": "Rohitash Chandra, Ayush Jain, Divyanshu Singh Chauhan", "title": "Deep learning via LSTM models for COVID-19 infection forecasting in\n  India", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We have entered an era of a pandemic that has shaken the world with major\nimpact to medical systems, economics and agriculture. Prominent computational\nand mathematical models have been unreliable due to the complexity of the\nspread of infections. Moreover, lack of data collection and reporting makes any\nsuch modelling attempts unreliable. Hence we need to re-look at the situation\nwith the latest data sources and most comprehensive forecasting models. Deep\nlearning models such as recurrent neural networks are well suited for modelling\ntemporal sequences. In this paper, prominent recurrent neural networks, in\nparticular \\textit{long short term memory} (LSTMs) networks, bidirectional\nLSTM, and encoder-decoder LSTM models for multi-step (short-term) forecasting\nthe spread of COVID-infections among selected states in India. We select states\nwith COVID-19 hotpots in terms of the rate of infections and compare with\nstates where infections have been contained or reached their peak and provide\ntwo months ahead forecast that shows that cases will slowly decline. Our\nresults show that long-term forecasts are promising which motivates the\napplication of the method in other countries or areas. We note that although we\nmade some progress in forecasting, the challenges in modelling remain due to\ndata and difficulty in capturing factors such as population density, travel\nlogistics, and social aspects such culture and lifestyle.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 09:19:10 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Chandra", "Rohitash", ""], ["Jain", "Ayush", ""], ["Chauhan", "Divyanshu Singh", ""]]}, {"id": "2101.11907", "submitter": "Simon Brant Mr.", "authors": "Simon Boge Brant, Ingrid Hob{\\ae}k Haff", "title": "The fraud loss for selecting the model complexity in fraud detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In fraud detection applications, the investigator is typically limited to\ncontrolling a restricted number k of cases. The most efficient manner of\nallocating the resources is then to try selecting the k cases with the highest\nprobability of being fraudulent. The prediction model used for this purpose\nmust normally be regularized to avoid overfitting and consequently bad\nprediction performance. A new loss function, denoted the fraud loss, is\nproposed for selecting the model complexity via a tuning parameter. A\nsimulation study is performed to find the optimal settings for validation.\nFurther, the performance of the proposed procedure is compared to the most\nrelevant competing procedure, based on the area under the receiver operating\ncharacteristic curve (AUC), in a set of simulations, as well as on a VAT fraud\ndataset. In most cases, choosing the complexity of the model according to the\nfraud loss, gave a better than, or comparable performance to the AUC in terms\nof the fraud loss.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 10:18:44 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Brant", "Simon Boge", ""], ["Haff", "Ingrid Hob\u00e6k", ""]]}, {"id": "2101.11991", "submitter": "Kwangmin Lee", "authors": "Kwangmin Lee, Seongil Jo, and Jaeyong Lee", "title": "Seroprevalence of SARS-CoV-2 antibodies in South Korea", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In $2020$, Korea Disease Control and Prevention Agency reported three rounds\nof surveys on seroprevalence of severe acute respiratory syndrome coronavirus 2\n(SARS-CoV-2) antibodies in South Korea. We analyze the seroprevalence surveys\nusing a Bayesian method with an informative prior distribution on the\nseroprevalence parameter, and the sensitivity and specificity of the diagnostic\ntest. We construct the informative prior using the posterior distribution\nobtained from the clinical evaluation data based on the plaque reduction\nneutralization test. The constraint of the seroprevalence parameter induced\nfrom the known confirmed coronavirus 2019 cases can be imposed naturally in the\nproposed Bayesian model. We also prove that the confidence interval of the\nseroprevalence parameter based on the Rao's test can be the empty set, while\nthe Bayesian method renders a reasonable interval estimator. As of the $30$th\nof October $2020$, the $95\\%$ credible interval of the estimated SARS-CoV-2\npositive population does not exceed $307,448$, approximately $0.6\\%$ of the\nKorean population.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 13:31:59 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Lee", "Kwangmin", ""], ["Jo", "Seongil", ""], ["Lee", "Jaeyong", ""]]}, {"id": "2101.12267", "submitter": "Joshua Williams", "authors": "Joshua Williams and J. Zico Kolter", "title": "A Bayesian Model of Cash Bail Decisions", "comments": "Accepted at 2021 ACM Conference on Fairness, Accountability, and\n  Transparency (ACM FAccT 2021) Associated data can be found at\n  `https://github.com/jnwilliams/padockets'", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of cash bail as a mechanism for detaining defendants pre-trial is an\noften-criticized system that many have argued violates the presumption of\n\"innocent until proven guilty.\" Many studies have sought to understand both the\nlong-term effects of cash bail's use and the disparate rate of cash bail\nassignments along demographic lines (race, gender, etc). However, such work is\noften susceptible to problems of infra-marginality -- that the data we observe\ncan only describe average outcomes, and not the outcomes associated with the\nmarginal decision.\n  In this work, we address this problem by creating a hierarchical Bayesian\nmodel of cash bail assignments. Specifically, our approach models cash bail\ndecisions as a probabilistic process whereby judges balance the relative costs\nof assigning cash bail with the cost of defendants potentially skipping court\ndates, and where these skip probabilities are estimated based upon features of\nthe individual case. We then use Monte Carlo inference to sample the\ndistribution over these costs for different magistrates and across different\nraces. We fit this model to a data set we have collected of over 50,000 court\ncases in the Allegheny and Philadelphia counties in Pennsylvania. Our analysis\nof 50 separate judges shows that they are uniformly more likely to assign cash\nbail to black defendants than to white defendants, even given identical\nlikelihood of skipping a court appearance. This analysis raises further\nquestions about the equity of the practice of cash bail, irrespective of its\nunderlying legal justification.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 20:45:11 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Williams", "Joshua", ""], ["Kolter", "J. Zico", ""]]}, {"id": "2101.12285", "submitter": "Louis Jensen", "authors": "Louis G. Jensen, David J. Williamson, Ute Hahn", "title": "Semiparametric point process modelling of blinking artifacts in PALM", "comments": "27 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photoactivated localization microscopy (PALM) is a powerful imaging technique\nfor characterization of protein organization in biological cells. Due to the\nstochastic blinking of fluorescent probes, and camera discretization effects,\neach protein gives rise to a cluster of artificial observations. These blinking\nartifacts are an obstacle for qualitative analysis of PALM data, and tools for\ntheir correction are in high demand. We develop the Independent Blinking\nCluster point process (IBCpp) family of models, and present results on the mark\ncorrelation function. We then construct the PALM-IBCpp - a semiparametric IBCpp\ntailored for PALM data. We describe a procedure for estimation of parameters,\nwhich can be used without parametric assumptions on the spatial organization of\nproteins. The parameters include the kinetic rates that control blinking, and\nas such can be used to correct subsequent data analysis. The method is\ndemonstrated on real data, and in a simulation study, where blinking artifacts\nwere precisely quantified in a range of realistic settings.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 21:31:56 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Jensen", "Louis G.", ""], ["Williamson", "David J.", ""], ["Hahn", "Ute", ""]]}, {"id": "2101.12318", "submitter": "Molly Offer-Westort", "authors": "Molly Offer-Westort, Drew Dimmery", "title": "Experimentation for Homogenous Policy Change", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When the Stable Unit Treatment Value Assumption (SUTVA) is violated and there\nis interference among units, there is not a uniquely defined Average Treatment\nEffect (ATE), and alternative estimands may be of interest, among them average\nunit-level differences in outcomes under different homogeneous treatment\npolicies. We term this target the Homogeneous Assignment Average Treatment\nEffect (HAATE). We consider approaches to experimental design with multiple\ntreatment conditions under partial interference and, given the estimand of\ninterest, we show that difference-in-means estimators may perform better than\ncorrectly specified regression models in finite samples on root mean squared\nerror (RMSE). With errors correlated at the cluster level, we demonstrate that\ntwo-stage randomization procedures with intra-cluster correlation of treatment\nstrictly between zero and one may dominate one-stage randomization designs on\nthe same metric. Simulations demonstrate performance of this approach; an\napplication to online experiments at Facebook is discussed.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 23:19:31 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Offer-Westort", "Molly", ""], ["Dimmery", "Drew", ""]]}, {"id": "2101.12326", "submitter": "Lina Montoya", "authors": "Lina Montoya, Mark van der Laan, Alexander Luedtke, Jennifer Skeem,\n  Jeremy Coyle, Maya Petersen", "title": "The Optimal Dynamic Treatment Rule SuperLearner: Considerations,\n  Performance, and Application", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The optimal dynamic treatment rule (ODTR) framework offers an approach for\nunderstanding which kinds of patients respond best to specific treatments -- in\nother words, treatment effect heterogeneity. Recently, there has been a\nproliferation of methods for estimating the ODTR. One such method is an\nextension of the SuperLearner algorithm -- an ensemble method to optimally\ncombine candidate algorithms extensively used in prediction problems -- to\nODTRs. Following the \"causal roadmap,\" we causally and statistically define the\nODTR and provide an introduction to estimating it using the ODTR SuperLearner.\nAdditionally, we highlight practical choices when implementing the algorithm,\nincluding choice of candidate algorithms, metalearners to combine the\ncandidates, and risk functions to select the best combination of algorithms.\nUsing simulations, we illustrate how estimating the ODTR using this\nSuperLearner approach can uncover treatment effect heterogeneity more\neffectively than traditional approaches based on fitting a parametric\nregression of the outcome on the treatment, covariates and treatment-covariate\ninteractions. We investigate the implications of choices in implementing an\nODTR SuperLearner at various sample sizes. Our results show the advantages of:\n(1) including a combination of both flexible machine learning algorithms and\nsimple parametric estimators in the library of candidate algorithms; (2) using\nan ensemble metalearner to combine candidates rather than selecting only the\nbest-performing candidate; (3) using the mean outcome under the rule as a risk\nfunction. Finally, we apply the ODTR SuperLearner to the \"Interventions\" study,\nan ongoing randomized controlled trial, to identify which justice-involved\nadults with mental illness benefit most from cognitive behavioral therapy (CBT)\nto reduce criminal re-offending.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 00:05:22 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 01:27:25 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Montoya", "Lina", ""], ["van der Laan", "Mark", ""], ["Luedtke", "Alexander", ""], ["Skeem", "Jennifer", ""], ["Coyle", "Jeremy", ""], ["Petersen", "Maya", ""]]}, {"id": "2101.12333", "submitter": "Lina Montoya", "authors": "Lina Montoya, Jennifer Skeem, Mark van der Laan, Maya Petersen", "title": "Performance and Application of Estimators for the Value of an Optimal\n  Dynamic Treatment Rule", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given an (optimal) dynamic treatment rule, it may be of interest to evaluate\nthat rule -- that is, to ask the causal question: what is the expected outcome\nhad every subject received treatment according to that rule? In this paper, we\nstudy the performance of estimators that approximate the true value of: 1) an\n$a$ $priori$ known dynamic treatment rule 2) the true, unknown optimal dynamic\ntreatment rule (ODTR); 3) an estimated ODTR, a so-called \"data-adaptive\nparameter,\" whose true value depends on the sample. Using simulations of\npoint-treatment data, we specifically investigate: 1) the impact of\nincreasingly data-adaptive estimation of nuisance parameters and/or of the ODTR\non performance; 2) the potential for improved efficiency and bias reduction\nthrough the use of semiparametric efficient estimators; and, 3) the importance\nof sample splitting based on CV-TMLE for accurate inference. In the simulations\nconsidered, there was very little cost and many benefits to using the\ncross-validated targeted maximum likelihood estimator (CV-TMLE) to estimate the\nvalue of the true and estimated ODTR; importantly, and in contrast to non\ncross-validated estimators, the performance of CV-TMLE was maintained even when\nhighly data-adaptive algorithms were used to estimate both nuisance parameters\nand the ODTR. In addition, we apply these estimators for the value of the rule\nto the \"Interventions\" Study, an ongoing randomized controlled trial, to\nidentify whether assigning cognitive behavioral therapy (CBT) to criminal\njustice-involved adults with mental illness using an ODTR significantly reduces\nthe probability of recidivism, compared to assigning CBT in a\nnon-individualized way.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 00:57:04 GMT"}, {"version": "v2", "created": "Sat, 6 Feb 2021 20:38:28 GMT"}, {"version": "v3", "created": "Tue, 6 Jul 2021 01:31:08 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Montoya", "Lina", ""], ["Skeem", "Jennifer", ""], ["van der Laan", "Mark", ""], ["Petersen", "Maya", ""]]}, {"id": "2101.12348", "submitter": "John Hoffman", "authors": "John Hoffman, Jacob Vanderplas, Joel Hartman, Gaspar Bakos", "title": "A Fast Template Periodogram for Detecting Non-sinusoidal Fixed-shape\n  Signals in Irregularly Sampled Time Series", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Astrophysical time series often contain periodic signals. The large and\ngrowing volume of time series data from photometric surveys demands\ncomputationally efficient methods for detecting and characterizing such\nsignals. The most efficient algorithms available for this purpose are those\nthat exploit the $\\mathcal{O}(N\\log N)$ scaling of the Fast Fourier Transform\n(FFT). However, these methods are not optimal for non-sinusoidal signal shapes.\nTemplate fits (or periodic matched filters) optimize sensitivity for a priori\nknown signal shapes but at a significant computational cost. Current\nimplementations of template periodograms scale as $\\mathcal{O}(N_f N_{obs})$,\nwhere $N_f$ is the number of trial frequencies and $N_{obs}$ is the number of\nlightcurve observations, and due to non-convexity, they do not guarantee the\nbest fit at each trial frequency, which can lead to spurious results. In this\nwork, we present a non-linear extension of the Lomb-Scargle periodogram to\nobtain a template-fitting algorithm that is both accurate (globally optimal\nsolutions are obtained except in pathological cases) and computationally\nefficient (scaling as $\\mathcal{O}(N_f\\log N_f)$ for a given template). The\nnon-linear optimization of the template fit at each frequency is recast as a\npolynomial zero-finding problem, where the coefficients of the polynomial can\nbe computed efficiently with the non-equispaced fast Fourier transform. We show\nthat our method, which uses truncated Fourier series to approximate templates,\nis an order of magnitude faster than existing algorithms for small problems\n($N\\lesssim 10$ observations) and 2 orders of magnitude faster for long\nbase-line time series with $N_{obs} \\gtrsim 10^4$ observations. An open-source\nimplementation of the fast template periodogram is available at\nhttps://www.github.com/PrincetonUniversity/FastTemplatePeriodogram.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 01:39:29 GMT"}, {"version": "v2", "created": "Sun, 7 Feb 2021 14:19:11 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Hoffman", "John", ""], ["Vanderplas", "Jacob", ""], ["Hartman", "Joel", ""], ["Bakos", "Gaspar", ""]]}, {"id": "2101.12451", "submitter": "Eileen Zhang", "authors": "Eileen Zhang", "title": "A Study on the Association between Maternal Childhood Trauma Exposure\n  and Placental-fetal Stress Physiology during Pregnancy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It has been found that the effect of childhood trauma (CT) exposure may pass\non to the next generation. Scientists have hypothesized that the association\nbetween CT exposure and placental-fetal stress physiology is the mechanism. A\nstudy was conducted to examine the hypothesis. To examine the association\nbetween CT exposure and placental corticotrophin-releasing hormone (pCRH),\nlinear mixed effect model and hierarchical Bayesian linear model were\nconstructed. In Bayesian inference, by providing conditionally conjugate\npriors, Gibbs sampler was used to draw MCMC samples. Piecewise linear mixed\neffect model was conducted in order to adjust to the dramatic change of pCRH at\naround week 20 into pregnancy. Pearson residual, QQ, ACF and trace plots were\nused to justify the model adequacy. Likelihood ratio test and DIC were utilized\nto model selection. The association between CT exposure and pCRH during\npregnancy is obvious. The effect of CT exposure on pCRH varies dramatically\nover gestational age. Women with one childhood trauma would experience 11.9%\nhigher in pCRH towards the end of pregnancy than those without childhood\ntrauma. The increase rate of pCRH after week 20 is almost four-fold larger than\nthat before week 20. Frequentist and Bayesian inference produce similar\nresults. The findings support the hypothesis that the effect of CT exposure on\npCRH over GA exists. The effect changes dramatically at around week 20 into\npregnancy.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 07:50:26 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Zhang", "Eileen", ""]]}, {"id": "2101.12492", "submitter": "Mingao Yuan", "authors": "Mingao Yuan and Qian Wen", "title": "A Practical Two-Sample Test for Weighted Random Graphs", "comments": "to appear in Journal of Applied Statistics", "journal-ref": "Journal of Applied Statistics, 2021", "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Network (graph) data analysis is a popular research topic in statistics and\nmachine learning. In application, one is frequently confronted with graph\ntwo-sample hypothesis testing where the goal is to test the difference between\ntwo graph populations. Several statistical tests have been devised for this\npurpose in the context of binary graphs. However, many of the practical\nnetworks are weighted and existing procedures can't be directly applied to\nweighted graphs. In this paper, we study the weighted graph two-sample\nhypothesis testing problem and propose a practical test statistic. We prove\nthat the proposed test statistic converges in distribution to the standard\nnormal distribution under the null hypothesis and analyze its power\ntheoretically. The simulation study shows that the proposed test has\nsatisfactory performance and it substantially outperforms the existing\ncounterpart in the binary graph case. A real data application is provided to\nillustrate the method.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 09:59:45 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Yuan", "Mingao", ""], ["Wen", "Qian", ""]]}, {"id": "2101.12499", "submitter": "Alessandro Casa", "authors": "Alessandro Casa, Tom F. O'Callaghan and Thomas Brendan Murphy", "title": "Parsimonious Bayesian Factor Analysis for modelling latent structures in\n  spectroscopy data", "comments": "23 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years animal diet has been receiving increased attention, in\nparticular examining the impact of pasture-based feeding strategies on the\nquality of milk and dairy products, in line with the increased prevalence of\ngrass-fed dairy products appearing on market shelves. To date, there are\nlimited testing methods available for the verification of grass-fed dairy\ntherefore these products are susceptible to food fraud and adulteration. Hence\nstatistical tools studying potential differences among milk samples coming from\nanimals on different feeding systems are required, thus providing increased\nsecurity around the authenticity of the products. Infrared spectroscopy\ntechniques are widely used to collect data on milk samples and to predict milk\nrelated traits. While these data are routinely used to predict the composition\nof the macro components of milk, each spectrum provides a reservoir of\nunharnessed information about the sample. The interpretation of these data\npresents a number of challenges due to their high-dimensionality and the\nrelationships amongst the spectral variables. In this work we propose a\nmodification of the standard factor analysis to induce a parsimonious summary\nof spectroscopic data. The procedure maps the observations into a\nlow-dimensional latent space while simultaneously clustering observed\nvariables. The method indicates possible redundancies in the data and it helps\ndisentangle the complex relationships among the wavelengths. A flexible\nBayesian estimation procedure is proposed for model fitting, providing\nreasonable values for the number of latent factors and clusters. The method is\napplied on milk mid-infrared spectroscopy data from dairy cows on different\npasture and non-pasture based diets, providing accurate modelling of the data\ncorrelation, the clustering of variables and information on differences between\nmilk samples from cows on different diets.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 10:06:15 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Casa", "Alessandro", ""], ["O'Callaghan", "Tom F.", ""], ["Murphy", "Thomas Brendan", ""]]}, {"id": "2101.12642", "submitter": "Indranil Sahoo", "authors": "Edward L. Boone, Abdel-Salam G. Abdel-Salam, Indranil Sahoo, Ryad\n  Ghanam, Xi Chen and Aiman Hanif", "title": "Monitoring SEIRD model parameters using MEWMA for the COVID-19 pandemic\n  with application to the State of Qatar", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  During the current COVID-19 pandemic, decision makers are tasked with\nimplementing and evaluating strategies for both treatment and disease\nprevention. In order to make effective decisions, they need to simultaneously\nmonitor various attributes of the pandemic such as transmission rate and\ninfection rate for disease prevention, recovery rate which indicates treatment\neffectiveness as well as the mortality rate and others. This work presents a\ntechnique for monitoring the pandemic by employing an Susceptible, Exposed,\nInfected, Recovered Death model regularly estimated by an augmented particle\nMarkov chain Monte Carlo scheme in which the posterior distribution samples are\nmonitored via Multivariate Exponentially Weighted Average process monitoring.\nThis is illustrated on the COVID-19 data for the State of Qatar.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 15:34:38 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Boone", "Edward L.", ""], ["Abdel-Salam", "Abdel-Salam G.", ""], ["Sahoo", "Indranil", ""], ["Ghanam", "Ryad", ""], ["Chen", "Xi", ""], ["Hanif", "Aiman", ""]]}, {"id": "2101.12686", "submitter": "Gertraud Malsiner-Walli", "authors": "Bettina Gr\\\"un, Gertraud Malsiner-Walli, Sylvia Fr\\\"uhwirth-Schnatter", "title": "How many data clusters are in the Galaxy data set? Bayesian cluster\n  analysis in action", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In model-based clustering, the Galaxy data set is often used as a benchmark\ndata set to study the performance of different modeling approaches. Aitkin\n(2001) compares maximum likelihood and Bayesian analyses of the Galaxy data set\nand expresses reservations about the Bayesian approach due to the fact that the\nprior assumptions imposed remain rather obscure while playing a major role in\nthe results obtained and conclusions drawn.\n  The aim of the paper is to address Aitkin's concerns about the Bayesian\napproach by shedding light on how the specified priors impact on the number of\nestimated clusters. We perform a sensitivity analysis of different prior\nspecifications for the mixtures of finite mixture model, i.e., the mixture\nmodel where a prior on the number of components is included. We use an\nextensive set of different prior specifications in a full factorial design and\nassess their impact on the estimated number of clusters for the Galaxy data\nset. Results highlight the interaction effects of the prior specifications and\nprovide insights into which prior specifications are recommended to obtain a\nsparse clustering solution.\n  A clear understanding of the impact of the prior specifications removes\nrestraints preventing the use of Bayesian methods due to the complexity of\nselecting suitable priors. Also, the regularizing properties of the priors may\nbe intentionally exploited to obtain a suitable clustering solution meeting\nprior expectations and needs of the application.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 17:14:39 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Gr\u00fcn", "Bettina", ""], ["Malsiner-Walli", "Gertraud", ""], ["Fr\u00fchwirth-Schnatter", "Sylvia", ""]]}, {"id": "2101.12693", "submitter": "Xiaochun Meng", "authors": "Carol Alexander, Michael Coulon, Yang Han, Xiaochun Meng", "title": "Evaluating the Discrimination Ability of Proper Multivariate Scoring\n  Rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proper scoring rules are commonly applied to quantify the accuracy of\ndistribution forecasts. Given an observation they assign a scalar score to each\ndistribution forecast, with the the lowest expected score attributed to the\ntrue distribution. The energy and variogram scores are two rules that have\nrecently gained some popularity in multivariate settings because their\ncomputation does not require a forecast to have parametric density function and\nso they are broadly applicable. Here we conduct a simulation study to compare\nthe discrimination ability between the energy score and three variogram scores.\nCompared with other studies, our simulation design is more realistic because it\nis supported by a historical data set containing commodity prices, currencies\nand interest rates, and our data generating processes include a diverse\nselection of models with different marginal distributions, dependence\nstructure, and calibration windows. This facilitates a comprehensive comparison\nof the performance of proper scoring rules in different settings. To compare\nthe scores we use three metrics: the mean relative score, error rate and a\ngeneralised discrimination heuristic. Overall, we find that the variogram score\nwith parameter p=0.5 outperforms the energy score and the other two variogram\nscores.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 17:23:33 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Alexander", "Carol", ""], ["Coulon", "Michael", ""], ["Han", "Yang", ""], ["Meng", "Xiaochun", ""]]}]