[{"id": "1309.0063", "submitter": "Yoshiaki Itoh", "authors": "Sumie Ueda, Kumi Makino, Yoshiaki Itoh, and Takashi Tsuchiya", "title": "Logistic Growth for the Nuzi Cuneiform Tablets: Analyzing Family\n  Networks in Ancient Mesopotamia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We reconstruct the year of publication of each cuneiform tablet of the Nuzi\nsociety in ancient Mesopotamia. The tablets, are on land transaction, marriage,\nloan, slavery contracts etc. The number of tablets seem to increase by logistic\ngrowth until saturation. It may show the dynamics of concentration of lands or\nother properties into few powerful families in a period of about twenty years.\nWe reconstruct family trees and social networks of Nuzi and estimate the\npublication years of cuneiform tablets consistently with the trees and\nnetworks, formulating least squares problems with linear inequality\nconstraints.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2013 04:47:37 GMT"}], "update_date": "2013-09-03", "authors_parsed": [["Ueda", "Sumie", ""], ["Makino", "Kumi", ""], ["Itoh", "Yoshiaki", ""], ["Tsuchiya", "Takashi", ""]]}, {"id": "1309.0159", "submitter": "Jan Mandel", "authors": "Martin Vejmelka, Adam K. Kochanski, and Jan Mandel", "title": "Data assimilation of fuel moisture in WRF-SFIRE", "comments": "4th Fire Behavior and Fuels Conference Raleigh, North Carolina, USA,\n  February 18 - 22, 2013", "journal-ref": "Proceedings of 4th Fire Behavior and Fuels Conference 2013, Wade,\n  D. D. and Fox, R. L., eds., pp. 122-137, International Association of\n  Wildland Fire, 2014", "doi": null, "report-no": "UCD CCM Report 314", "categories": "physics.ao-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fuel moisture is a major influence on the behavior of wildland fires and an\nimportant underlying factor in fire risk. We present a method to assimilate\nspatially sparse fuel moisture observations from remote automatic weather\nstations (RAWS) into the moisture model in WRF-SFIRE. WRF-SFIRE is a coupled\natmospheric and fire behavior model which simulates the evolution of fuel\nmoisture in idealized fuel species based on atmospheric state. The proposed\nmethod uses a modified trend surface model to estimate the fuel moisture field\nand its uncertainty based on currently available observations. At each grid\npoint of WRF-SFIRE, this information is combined with the model forecast using\na nonlinear Kalman filter, leading to an updated estimate of fuel moisture. We\ndemonstrate the effectiveness of the method with tests in two real-world\nsituations: a region in Southern California, where two large Santa Ana fires\noccurred recently, and on a domain enclosing Colorado.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2013 21:30:19 GMT"}], "update_date": "2014-11-05", "authors_parsed": [["Vejmelka", "Martin", ""], ["Kochanski", "Adam K.", ""], ["Mandel", "Jan", ""]]}, {"id": "1309.0675", "submitter": "Soumendu Sundar Mukherjee", "authors": "Sourabh Banerjee, Ayanendranath Basu, Sourabh Bhattacharya, Smarajit\n  Bose, Dalia Chakrabarty and Soumendu Sundar Mukherjee", "title": "Minimum Distance Estimation of Milky Way Model Parameters and Related\n  Inference", "comments": "25 pages, 10 Figures. This version incorporates the suggestions made\n  by the referees. To appear in SIAM/ASA Journal on Uncertainty Quantification", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP astro-ph.GA astro-ph.SR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to estimate the location of the Sun in the disk of the\nMilky Way using a method based on the Hellinger distance and construct\nconfidence sets on our estimate of the unknown location using a bootstrap based\nmethod. Assuming the Galactic disk to be two-dimensional, the sought solar\nlocation then reduces to the radial distance separating the Sun from the\nGalactic center and the angular separation of the Galactic center to Sun line,\nfrom a pre-fixed line on the disk. On astronomical scales, the unknown solar\nlocation is equivalent to the location of us earthlings who observe the\nvelocities of a sample of stars in the neighborhood of the Sun. This unknown\nlocation is estimated by undertaking pairwise comparisons of the estimated\ndensity of the observed set of velocities of the sampled stars, with densities\nestimated using synthetic stellar velocity data sets generated at chosen\nlocations in the Milky Way disk according to four base astrophysical models.\nThe \"match\" between the pair of estimated densities is parameterized by the\naffinity measure based on the familiar Hellinger distance. We perform a novel\ncross-validation procedure to establish a desirable \"consistency\" property of\nthe proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2013 13:42:46 GMT"}, {"version": "v2", "created": "Fri, 15 Aug 2014 20:45:38 GMT"}], "update_date": "2014-08-19", "authors_parsed": [["Banerjee", "Sourabh", ""], ["Basu", "Ayanendranath", ""], ["Bhattacharya", "Sourabh", ""], ["Bose", "Smarajit", ""], ["Chakrabarty", "Dalia", ""], ["Mukherjee", "Soumendu Sundar", ""]]}, {"id": "1309.0781", "submitter": "Nick Williams", "authors": "Nick Williams", "title": "An Exploratory Data Survey of Drug Name Incidence and Prevalence From\n  the FDA's Adverse Event Reporting System, 2004 to 2012Q2", "comments": "14 Figures 19 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drug Names, Population Level Surveillance and the FDA's Adverse Event\nReporting System: An Exploratory Data Survey of Drug Name Incidence and\nPrevalence, 2004-2012Q2 Purpose: To count and monitor the drug names reported\nin the publicly available version of the Federal Adverse Event Reporting System\n(FAERS) from 2004 to 2012Q2 in a maximized sensitivity relational model.\nMethods: Data mining and data modeling was conducted and event based summary\nstatistics with plots were created from over nine continuous years of\ncontinuous FAERS data. Results: This FAERS model contains 344,452 individual\ndrug names and 432,541,994 count references which occurred across 4,148,761\nhuman subjects in the 34 quarter study period. Plots for the top 100 scoring\ndrug name references are reported by year and quarter; the top 100 drug names\ncontain 143,384,240 references or 33% of all drug name references over 34\nquarters of continuous FAERS data. Conclusions: While FAERS contains many drugs\nand adverse event reports, its data pertains to very few of them. Drug name\nincidence lends timely and effective surveillance of large populations of\nAverse Event Reports and does not require the cause of the AE, nor its validity\nto be known to detect a mass poisoning.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2013 01:09:23 GMT"}], "update_date": "2013-09-04", "authors_parsed": [["Williams", "Nick", ""]]}, {"id": "1309.0837", "submitter": "Xiaoquan Wen", "authors": "Xiaoquan Wen", "title": "Bayesian Model Selection in Complex Linear Systems, as Illustrated in\n  Genetic Association Studies", "comments": null, "journal-ref": "Biometrics 2014 Mar; 70(1): 73-83", "doi": "10.1111/biom.12112", "report-no": null, "categories": "stat.ME q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by examples from genetic association studies, this paper considers\nthe model selection problem in a general complex linear model system and in a\nBayesian framework. We discuss formulating model selection problems and\nincorporating context-dependent {\\it a priori} information through different\nlevels of prior specifications. We also derive analytic Bayes factors and their\napproximations to facilitate model selection and discuss their theoretical and\ncomputational properties. We demonstrate our Bayesian approach based on an\nimplemented Markov Chain Monte Carlo (MCMC) algorithm in simulations and a real\ndata application of mapping tissue-specific eQTLs. Our novel results on Bayes\nfactors provide a general framework to perform efficient model comparisons in\ncomplex linear model systems.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2013 20:35:07 GMT"}], "update_date": "2014-03-14", "authors_parsed": [["Wen", "Xiaoquan", ""]]}, {"id": "1309.1192", "submitter": "Karl Broman", "authors": "Karl W. Broman", "title": "Fourteen years of R/qtl: Just barely sustainable", "comments": "Previously submission to First Workshop on Sustainable Software for\n  Science: Practice and Experiences (WSSSPE),\n  http://wssspe.researchcomputing.org.uk; revised for submission to the Journal\n  of Open Research Software, http://openresearchsoftware.metajnl.com/", "journal-ref": "Broman, K.W. 2014. Fourteen Years of R/qtl: Just Barely\n  Sustainable. Journal of Open Research Software 2(1):e11", "doi": "10.5334/jors.at", "report-no": null, "categories": "stat.CO stat.AP", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  R/qtl is an R package for mapping quantitative trait loci (genetic loci that\ncontribute to variation in quantitative traits) in experimental crosses. Its\ndevelopment began in 2000. There have been 38 software releases since 2001. The\nlatest release contains 35k lines of R code and 24k lines of C code, plus 15k\nlines of code for the documentation. Challenges in the development and\nmaintenance of the software are discussed. A key to the success of R/qtl is\nthat it remains a central tool for the chief developer's own research work, and\nso its maintenance is of selfish importance.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2013 21:37:06 GMT"}, {"version": "v2", "created": "Sun, 6 Apr 2014 18:01:18 GMT"}], "update_date": "2014-07-10", "authors_parsed": [["Broman", "Karl W.", ""]]}, {"id": "1309.1412", "submitter": "Stefan Aulbach", "authors": "Stefan Aulbach and Michael Falk", "title": "Testing for a {\\delta}-neighborhood of a generalized Pareto copula", "comments": "32 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multivariate distribution function F is in the max-domain of attraction of\nan extreme value distribution if and only if this is true for the copula\ncorresponding to F and its univariate margins. Aulbach et al. (2012a) have\nshown that a copula satisfies the extreme value condition if and only if the\ncopula is tail equivalent to a generalized Pareto copula (GPC). In this paper\nwe propose a chi-square goodness-of-fit test in arbitrary dimension for testing\nwhether a copula is in a certain neighborhood of a GPC. The test can be applied\nto stochastic processes as well to check whether the corresponding copula\nprocess is close to a generalized Pareto process. Since the p-value of the\nproposed test is highly sensitive to a proper selection of a certain threshold,\nwe also present a graphical tool that makes the decision, whether or not to\nreject the hypothesis, more comfortable.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2013 17:29:33 GMT"}], "update_date": "2013-09-06", "authors_parsed": [["Aulbach", "Stefan", ""], ["Falk", "Michael", ""]]}, {"id": "1309.1602", "submitter": "Leontine Alkema", "authors": "Leontine Alkema, Jin Rou New", "title": "Global estimation of child mortality using a Bayesian B-spline\n  Bias-reduction model", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS768 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 4, 2122-2149", "doi": "10.1214/14-AOAS768", "report-no": "IMS-AOAS-AOAS768", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimates of the under-five mortality rate (U5MR) are used to track progress\nin reducing child mortality and to evaluate countries' performance related to\nMillennium Development Goal 4. However, for the great majority of developing\ncountries without well-functioning vital registration systems, estimating the\nU5MR is challenging due to limited data availability and data quality issues.\nWe describe a Bayesian penalized B-spline regression model for assessing levels\nand trends in the U5MR for all countries in the world, whereby biases in data\nseries are estimated through the inclusion of a multilevel model to improve\nupon the limitations of current methods. B-spline smoothing parameters are also\nestimated through a multilevel model. Improved spline extrapolations are\nobtained through logarithmic pooling of the posterior predictive distribution\nof country-specific changes in spline coefficients with observed changes on the\nglobal level. The proposed model is able to flexibly capture changes in U5MR\nover time, gives point estimates and credible intervals reflecting potential\nbiases in data series and performs reasonably well in out-of-sample validation\nexercises. It has been accepted by the United Nations Inter-agency Group for\nChild Mortality Estimation to generate estimates for all member countries.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2013 11:09:34 GMT"}, {"version": "v2", "created": "Mon, 19 Jan 2015 06:29:41 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["Alkema", "Leontine", ""], ["New", "Jin Rou", ""]]}, {"id": "1309.1799", "submitter": "Yajuan Si", "authors": "Yajuan Si, Natesh S. Pillai, Andrew Gelman", "title": "Bayesian Nonparametric Weighted Sampling Inference", "comments": "Published at http://dx.doi.org/10.1214/14-BA924 in the Bayesian\n  Analysis (http://projecteuclid.org/euclid.ba) by the International Society of\n  Bayesian Analysis (http://bayesian.org/)", "journal-ref": "Bayesian Analysis 2015, Vol. 10, No. 3, 605-625", "doi": "10.1214/14-BA924", "report-no": "VTeX-BA-BA924", "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has historically been a challenge to perform Bayesian inference in a\ndesign-based survey context. The present paper develops a Bayesian model for\nsampling inference in the presence of inverse-probability weights. We use a\nhierarchical approach in which we model the distribution of the weights of the\nnonsampled units in the population and simultaneously include them as\npredictors in a nonparametric Gaussian process regression. We use simulation\nstudies to evaluate the performance of our procedure and compare it to the\nclassical design-based estimator. We apply our method to the Fragile Family and\nChild Wellbeing Study. Our studies find the Bayesian nonparametric finite\npopulation estimator to be more robust than the classical design-based\nestimator without loss in efficiency, which works because we induce\nregularization for small cells and thus this is a way of automatically\nsmoothing the highly variable weights.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2013 01:14:31 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2013 21:52:49 GMT"}, {"version": "v3", "created": "Mon, 13 Oct 2014 21:50:16 GMT"}, {"version": "v4", "created": "Fri, 4 Sep 2015 05:49:15 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Si", "Yajuan", ""], ["Pillai", "Natesh S.", ""], ["Gelman", "Andrew", ""]]}, {"id": "1309.1864", "submitter": "John-Olof Nilsson", "authors": "John-Olof Nilsson and Peter H\\\"andel", "title": "Timing estimation in distributed sensor and control systems with central\n  processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating timing of measurements and actuation in\ndistributed sensor and control systems with central processing. The focus is on\ndirect timing estimation for scenarios where clock synchronization is not\nfeasible or desirable. Models of the timing and central and peripheral time\nstamps are motivated and derived from underlying clock and communication delay\ndefinitions and models. Heuristics for constructing a system time is presented\nand it is outlined how the joint timing and the plant state estimation can be\nhandled. For a simple set of underlying clock and communication delay models,\ninclusion of peripheral unit time stamps is shown to reduce jitter, and it is\nargued that in general it will give significant jitter reduction. Finally, a\nnumerical example is given of a contemporary system design.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2013 13:32:24 GMT"}], "update_date": "2013-09-10", "authors_parsed": [["Nilsson", "John-Olof", ""], ["H\u00e4ndel", "Peter", ""]]}, {"id": "1309.2324", "submitter": "Daniel Manrique-Vallier", "authors": "Daniel Manrique-Vallier", "title": "Longitudinal Mixed Membership trajectory models for disability survey\n  data", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS769 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 4, 2268-2291", "doi": "10.1214/14-AOAS769", "report-no": "IMS-AOAS-AOAS769", "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop methods for analyzing discrete multivariate longitudinal data and\napply them to functional disability data on the U.S. elderly population from\nthe National Long Term Care Survey (NLTCS), 1982-2004. Our models build on a\nMixed Membership framework, in which individuals are allowed multiple\nmembership on a set of extreme profiles characterized by time-dependent\ntrajectories of progression into disability. We also develop an extension that\nallows us to incorporate birth-cohort effects, in order to assess\ninter-generational changes. Applying these methods, we find that most\nindividuals follow trajectories that imply a late onset of disability, and that\nyounger cohorts tend to develop disabilities at a later stage in life compared\nto their elders.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2013 21:10:01 GMT"}, {"version": "v2", "created": "Tue, 29 Jul 2014 15:11:16 GMT"}, {"version": "v3", "created": "Wed, 4 Mar 2015 10:18:07 GMT"}], "update_date": "2015-03-05", "authors_parsed": [["Manrique-Vallier", "Daniel", ""]]}, {"id": "1309.2462", "submitter": "Pablo Echenique-Robba", "authors": "Pablo Echenique-Robba, Mar\\'ia Alejandra Nelo-Baz\\'an, Jos\\'e A.\n  Carrodeguas", "title": "Reducing the standard deviation in multiple-assay experiments where the\n  variation matters but the absolute value does not", "comments": "Supplementary material at http://bit.ly/14I718S", "journal-ref": "PLoS one 8 (2013) e78205", "doi": "10.1371/journal.pone.0078205", "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  You measure the value of a quantity x for a number of systems (cells,\nmolecules, people, chunks of metal, DNA vectors, etc.). You repeat the whole\nset of measures in different occasions or assays, which you try to design as\nequal to one another as possible. Despite the effort, you find that the results\nare too different from one assay to another. As a consequence, some systems'\naverages present standard deviations that are too large to render the results\nstatistically significant. In this work, we present a novel correction method\nof very low mathematical and numerical complexity that can reduce the standard\ndeviation in your results and increase their statistical significance as long\nas two conditions are met: inter-system variations of x matter to you but its\nabsolute value does not, and the different assays display a similar tendency in\nthe values of x; in other words, the results corresponding to different assays\npresent high linear correlation. We demonstrate the improvement that this\nmethod brings about on a real cell biology experiment, but the method can be\napplied to any problem that conforms to the described structure and\nrequirements, in any quantitative scientific field that has to deal with data\nsubject to uncertainty.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2013 11:28:01 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2013 18:27:52 GMT"}, {"version": "v3", "created": "Fri, 1 Nov 2013 10:37:02 GMT"}], "update_date": "2013-11-04", "authors_parsed": [["Echenique-Robba", "Pablo", ""], ["Nelo-Baz\u00e1n", "Mar\u00eda Alejandra", ""], ["Carrodeguas", "Jos\u00e9 A.", ""]]}, {"id": "1309.2627", "submitter": "Xiaoming Lu", "authors": "Lu Xiaoming and Fan Zhaozhi", "title": "Weighted quantile regression for longitudinal data", "comments": "22 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantile regression is a powerful statistical methodology that complements\nthe classical linear regression by examining how covariates influence the\nlocation, scale, and shape of the entire response distribution and offering a\nglobal view of the statistical landscape. In this paper we propose a new\nquantile regression model for longitudinal data. The proposed approach\nincorporates the correlation structure between repeated measures to enhance the\nefficiency of the inference. In order to use the Newton-Raphson iteration\nmethod to obtain convergent estimates, the estimating functions are redefined\nas smoothed functions which are differentiable with respect to regression\nparameters. Our proposed method for quantile regression provides consistent\nestimates with asymptotically normal distributions. Simulation studies are\ncarried out to evaluate the performance of the proposed method. As an\nillustration, the proposed method was applied to a real-life data that contains\nself-reported labor pain for women in two groups.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2013 19:48:21 GMT"}], "update_date": "2013-09-11", "authors_parsed": [["Xiaoming", "Lu", ""], ["Zhaozhi", "Fan", ""]]}, {"id": "1309.2737", "submitter": "Ewan Cameron Dr", "authors": "Ewan Cameron and Tony Pettitt", "title": "On the Evidence for Cosmic Variation of the Fine Structure Constant\n  (II): A Semi-Parametric Bayesian Model Selection Analysis of the Quasar\n  Dataset", "comments": "9 pages, 5 figures, submitted to MNRAS", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the second paper of this series we extend our Bayesian reanalysis of the\nevidence for a cosmic variation of the fine structure constant to the\nsemi-parametric modelling regime. By adopting a mixture of Dirichlet processes\nprior for the unexplained errors in each instrumental subgroup of the benchmark\nquasar dataset we go some way towards freeing our model selection procedure\nfrom the apparent subjectivity of a fixed distributional form. Despite the\ninfinite-dimensional domain of the error hierarchy so constructed we are able\nto demonstrate a recursive scheme for marginal likelihood estimation with\nprior-sensitivity analysis directly analogous to that presented in Paper I,\nthereby allowing the robustness of our posterior Bayes factors to\nhyper-parameter choice and model specification to be readily verified. In the\ncourse of this work we elucidate various similarities between unexplained error\nproblems in the seemingly disparate fields of astronomy and clinical\nmeta-analysis, and we highlight a number of sophisticated techniques for\nhandling such problems made available by past research in the latter. It is our\nhope that the novel approach to semi-parametric model selection demonstrated\nherein may serve as a useful reference for others exploring this potentially\ndifficult class of error model.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2013 06:34:23 GMT"}], "update_date": "2013-09-12", "authors_parsed": [["Cameron", "Ewan", ""], ["Pettitt", "Tony", ""]]}, {"id": "1309.2848", "submitter": "Shabnam Kadir", "authors": "Shabnam N. Kadir, Dan F. M. Goodman, and Kenneth D. Harris", "title": "High-dimensional cluster analysis with the Masked EM Algorithm", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster analysis faces two problems in high dimensions: first, the `curse of\ndimensionality' that can lead to overfitting and poor generalization\nperformance; and second, the sheer time taken for conventional algorithms to\nprocess large amounts of high-dimensional data. In many applications, only a\nsmall subset of features provide information about the cluster membership of\nany one data point, however this informative feature subset may not be the same\nfor all data points. Here we introduce a `Masked EM' algorithm for fitting\nmixture of Gaussians models in such cases. We show that the algorithm performs\nclose to optimally on simulated Gaussian data, and in an application of `spike\nsorting' of high channel-count neuronal recordings.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2013 14:55:50 GMT"}], "update_date": "2013-09-12", "authors_parsed": [["Kadir", "Shabnam N.", ""], ["Goodman", "Dan F. M.", ""], ["Harris", "Kenneth D.", ""]]}, {"id": "1309.3250", "submitter": "Monir Hajiaghayi", "authors": "Monir Hajiaghayi, Bonnie Kirkpatrick, Liangliang Wang, and Alexandre\n  Bouchard-C\\^ot\\'e", "title": "Efficient Continuous-Time Markov Chain Estimation", "comments": "19 pages, 7 figures, 2 tables, 6 Algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems of practical interest rely on Continuous-time Markov\nchains~(CTMCs) defined over combinatorial state spaces, rendering the\ncomputation of transition probabilities, and hence probabilistic inference,\ndifficult or impossible with existing methods. For problems with countably\ninfinite states, where classical methods such as matrix exponentiation are not\napplicable, the main alternative has been particle Markov chain Monte Carlo\nmethods imputing both the holding times and sequences of visited states. We\npropose a particle-based Monte Carlo approach where the holding times are\nmarginalized analytically. We demonstrate that in a range of realistic\ninferential setups, our scheme dramatically reduces the variance of the Monte\nCarlo approximation and yields more accurate parameter posterior approximations\ngiven a fixed computational budget. These experiments are performed on both\nsynthetic and real datasets, drawing from two important examples of CTMCs\nhaving combinatorial state spaces: string-valued mutation models in\nphylogenetics and nucleic acid folding pathways.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2013 19:30:14 GMT"}, {"version": "v2", "created": "Wed, 12 Mar 2014 19:30:34 GMT"}], "update_date": "2014-03-13", "authors_parsed": [["Hajiaghayi", "Monir", ""], ["Kirkpatrick", "Bonnie", ""], ["Wang", "Liangliang", ""], ["Bouchard-C\u00f4t\u00e9", "Alexandre", ""]]}, {"id": "1309.3258", "submitter": "Bogdan  pasaniuc", "authors": "Bogdan Pasaniuc, Noah Zaitlen, Huwenbo Shi, Gaurav Bhatia, Alexander\n  Gusev, Joseph Pickrell, Joel Hirschhorn, David P Strachan, Nick Patterson,\n  Alkes L. Price", "title": "Fast and accurate imputation of summary statistics enhances evidence of\n  functional enrichment", "comments": "32 pages, 4 figures", "journal-ref": null, "doi": "10.1093/bioinformatics/btu416", "report-no": null, "categories": "q-bio.QM q-bio.GN q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imputation using external reference panels is a widely used approach for\nincreasing power in GWAS and meta-analysis. Existing HMM-based imputation\napproaches require individual-level genotypes. Here, we develop a new method\nfor Gaussian imputation from summary association statistics, a type of data\nthat is becoming widely available. In simulations using 1000 Genomes (1000G)\ndata, this method recovers 84% (54%) of the effective sample size for common\n(>5%) and low-frequency (1-5%) variants (increasing to 87% (60%) when summary\nLD information is available from target samples) versus 89% (67%) for HMM-based\nimputation, which cannot be applied to summary statistics. Our approach\naccounts for the limited sample size of the reference panel, a crucial step to\neliminate false-positive associations, and is computationally very fast. As an\nempirical demonstration, we apply our method to 7 case-control phenotypes from\nthe WTCCC data and a study of height in the British 1958 birth cohort (1958BC).\nGaussian imputation from summary statistics recovers 95% (105%) of the\neffective sample size (as quantified by the ratio of $\\chi^2$ association\nstatistics) compared to HMM-based imputation from individual-level genotypes at\nthe 227 (176) published SNPs in the WTCCC (1958BC height) data. In addition,\nfor publicly available summary statistics from large meta-analyses of 4 lipid\ntraits, we publicly release imputed summary statistics at 1000G SNPs, which\ncould not have been obtained using previously published methods, and\ndemonstrate their accuracy by masking subsets of the data. We show that 1000G\nimputation using our approach increases the magnitude and statistical evidence\nof enrichment at genic vs. non-genic loci for these traits, as compared to an\nanalysis without 1000G imputation. Thus, imputation of summary statistics will\nbe a valuable tool in future functional enrichment analyses.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2013 19:44:45 GMT"}], "update_date": "2014-07-03", "authors_parsed": [["Pasaniuc", "Bogdan", ""], ["Zaitlen", "Noah", ""], ["Shi", "Huwenbo", ""], ["Bhatia", "Gaurav", ""], ["Gusev", "Alexander", ""], ["Pickrell", "Joseph", ""], ["Hirschhorn", "Joel", ""], ["Strachan", "David P", ""], ["Patterson", "Nick", ""], ["Price", "Alkes L.", ""]]}, {"id": "1309.3271", "submitter": "Yin-Zhe Ma", "authors": "Yin-Zhe Ma, Aaron Berndsen", "title": "How to combine correlated data sets -- A Bayesian hyperparameter matrix\n  method", "comments": "13 pages, 7 figures. Astronomy and Computing, 2014", "journal-ref": "Astronomy and Computing, 5 (2014) 45-56", "doi": "10.1016/j.ascom.2014.04.005", "report-no": null, "categories": "astro-ph.IM astro-ph.CO stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct a \"hyperparameter matrix\" statistical method for performing the\njoint analyses of multiple correlated astronomical data sets, in which the\nweights of data sets are determined by their own statistical properties. This\nmethod is a generalization of the hyperparameter method constructed by Lahav et\nal. (2000) and Hobson, Bridle, & Lahav (2002) which was designed to combine\nindependent data sets. The advantage of our method is to treat correlations\nbetween multiple data sets and gives appropriate relevant weights of multiple\ndata sets with mutual correlations. We define a new \"element-wise\" product,\nwhich greatly simplifies the likelihood function with hyperparameter matrix. We\nrigorously prove the simplified formula of the joint likelihood and show that\nit recovers the original hyperparameter method in the limit of no covariance\nbetween data sets. We then illustrate the method by applying it to a\ndemonstrative toy model of fitting a straight line to two sets of data. We show\nthat the hyperparameter matrix method can detect unaccounted systematic errors\nor underestimated errors in the data sets. Additionally, the ratio of Bayes'\nfactors provides a distinct indicator of the necessity of including\nhyperparameters. Our example shows that the likelihood we construct for joint\nanalyses of correlated data sets can be widely applied to many astrophysical\nsystems.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2013 20:00:00 GMT"}, {"version": "v2", "created": "Tue, 13 May 2014 20:27:13 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Ma", "Yin-Zhe", ""], ["Berndsen", "Aaron", ""]]}, {"id": "1309.3399", "submitter": "Karol Wawrzyniak K.W.", "authors": "Karol Wawrzyniak and Wojciech Wi\\'slicki", "title": "Grand canonical minority game as a sign predictor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST q-fin.TR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the extended model of Minority game (MG), incorporating\nvariable number of agents and therefore called Grand Canonical, is used for\nprediction. We proved that the best MG-based predictor is constituted by a\ntremendously degenerated system, when only one agent is involved. The\nprediction is the most efficient if the agent is equipped with all strategies\nfrom the Full Strategy Space. Each of these filters is evaluated and, in each\nstep, the best one is chosen. Despite the casual simplicity of the method its\nusefulness is invaluable in many cases including real problems. The significant\npower of the method lies in its ability to fast adaptation if \\lambda-GCMG\nmodification is used. The success rate of prediction is sensitive to the\nproperly set memory length. We considered the feasibility of prediction for the\nMinority and Majority games. These two games are driven by different dynamics\nwhen self-generated time series are considered. Both dynamics tend to be the\nsame when a feedback effect is removed and an exogenous signal is applied.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2013 08:51:04 GMT"}], "update_date": "2013-09-16", "authors_parsed": [["Wawrzyniak", "Karol", ""], ["Wi\u015blicki", "Wojciech", ""]]}, {"id": "1309.3724", "submitter": "Daniel Soudry", "authors": "Daniel Soudry, Suraj Keshri, Patrick Stinson, Min-hwan Oh, Garud\n  Iyengar, Liam Paninski", "title": "A shotgun sampling solution for the common input problem in neural\n  connectivity inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring connectivity in neuronal networks remains a key challenge in\nstatistical neuroscience. The `common input' problem presents the major\nroadblock: it is difficult to reliably distinguish causal connections between\npairs of observed neurons from correlations induced by common input from\nunobserved neurons. Since available recording techniques allow us to sample\nfrom only a small fraction of large networks simultaneously with sufficient\ntemporal resolution, naive connectivity estimators that neglect these common\ninput effects are highly biased. This work proposes a `shotgun' experimental\ndesign, in which we observe multiple sub-networks briefly, in a serial manner.\nThus, while the full network cannot be observed simultaneously at any given\ntime, we may be able to observe most of it during the entire experiment. Using\na generalized linear model for a spiking recurrent neural network, we develop\nscalable approximate Bayesian methods to perform network inference given this\ntype of data, in which only a small fraction of the network is observed in each\ntime bin. We demonstrate in simulation that, using this method: (1) The shotgun\nexperimental design can eliminate the biases induced by common input effects.\n(2) Networks with thousands of neurons, in which only a small fraction of the\nneurons is observed in each time bin, could be quickly and accurately\nestimated. (3) Performance can be improved if we exploit prior information\nabout the probability of having a connection between two neurons, its\ndependence on neuronal cell types (e.g., Dale's law), or its dependence on the\ndistance between neurons.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2013 05:29:53 GMT"}, {"version": "v2", "created": "Thu, 18 Dec 2014 01:12:14 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Soudry", "Daniel", ""], ["Keshri", "Suraj", ""], ["Stinson", "Patrick", ""], ["Oh", "Min-hwan", ""], ["Iyengar", "Garud", ""], ["Paninski", "Liam", ""]]}, {"id": "1309.3729", "submitter": "Didier Fraix-Burnet", "authors": "Tuli De, Didier Fraix-Burnet (IPAG), Asis Kumar Chattopadhyay", "title": "Clustering large number of extragalactic spectra of galaxies and quasars\n  through canopies", "comments": null, "journal-ref": "Communication in Statistics - Theory and Methods (2013) 000", "doi": null, "report-no": null, "categories": "astro-ph.CO astro-ph.IM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster analysis is the distribution of objects into different groups or more\nprecisely the partitioning of a data set into subsets (clusters) so that the\ndata in subsets share some common trait according to some distance measure.\nUnlike classi cation, in clustering one has to rst decide the optimum number of\nclusters and then assign the objects into different clusters. Solution of such\nproblems for a large number of high dimensional data points is quite\ncomplicated and most of the existing algorithms will not perform properly. In\nthe present work a new clustering technique applicable to large data set has\nbeen used to cluster the spectra of 702248 galaxies and quasars having 1540\npoints in wavelength range imposed by the instrument. The proposed technique\nhas successfully discovered ve clusters from this 702248X1540 data matrix.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2013 05:51:37 GMT"}], "update_date": "2013-09-17", "authors_parsed": [["De", "Tuli", "", "IPAG"], ["Fraix-Burnet", "Didier", "", "IPAG"], ["Chattopadhyay", "Asis Kumar", ""]]}, {"id": "1309.4144", "submitter": "Daniel Cervone", "authors": "Daniel Cervone, Natesh S. Pillai, Debdeep Pati, Ross Berbeco, John\n  Henry Lewis", "title": "A location-mixture autoregressive model for online forecasting of lung\n  tumor motion", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS744 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 3, 1341-1371", "doi": "10.1214/14-AOAS744", "report-no": "IMS-AOAS-AOAS744", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lung tumor tracking for radiotherapy requires real-time, multiple-step ahead\nforecasting of a quasi-periodic time series recording instantaneous tumor\nlocations. We introduce a location-mixture autoregressive (LMAR) process that\nadmits multimodal conditional distributions, fast approximate inference using\nthe EM algorithm and accurate multiple-step ahead predictive distributions.\nLMAR outperforms several commonly used methods in terms of out-of-sample\nprediction accuracy using clinical data from lung tumor patients. With its\nsuperior predictive performance and real-time computation, the LMAR model could\nbe effectively implemented for use in current tumor tracking systems.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2013 00:56:42 GMT"}, {"version": "v2", "created": "Sun, 27 Oct 2013 04:40:49 GMT"}, {"version": "v3", "created": "Wed, 9 Apr 2014 18:45:14 GMT"}, {"version": "v4", "created": "Wed, 5 Nov 2014 13:08:36 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Cervone", "Daniel", ""], ["Pillai", "Natesh S.", ""], ["Pati", "Debdeep", ""], ["Berbeco", "Ross", ""], ["Lewis", "John Henry", ""]]}, {"id": "1309.4151", "submitter": "Qiyu Jin", "authors": "Qiyu Jin, Ion Grama and Quansheng Liu", "title": "A Non-Local Means Filter for Removing the Poisson Noise", "comments": "24pages,6figures. arXiv admin note: text overlap with\n  arXiv:1211.6143, arXiv:1201.5968", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new image denoising algorithm to deal with the Poisson noise model is\ngiven, which is based on the idea of Non-Local Mean. By using the \"Oracle\"\nconcept, we establish a theorem to show that the Non-Local Means Filter can\neffectively deal with Poisson noise with some modification. Under the\ntheoretical result, we construct our new algorithm called Non-Local Means\nPoisson Filter and demonstrate in theory that the filter converges at the usual\noptimal rate. The filter is as simple as the classic Non-Local Means and the\nsimulation results show that our filter is very competitive.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2013 02:06:19 GMT"}], "update_date": "2013-09-18", "authors_parsed": [["Jin", "Qiyu", ""], ["Grama", "Ion", ""], ["Liu", "Quansheng", ""]]}, {"id": "1309.4286", "submitter": "Fabio Vandin", "authors": "Fabio Vandin, Alexandra Papoutsaki, Benjamin J. Raphael, Eli Upfal", "title": "Accurate Computation of Survival Statistics in Genome-wide Studies", "comments": "Full version of RECOMB 2013 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key challenge in genomics is to identify genetic variants that distinguish\npatients with different survival time following diagnosis or treatment. While\nthe log-rank test is widely used for this purpose, nearly all implementations\nof the log-rank test rely on an asymptotic approximation that is not\nappropriate in many genomics applications. This is because: the two populations\ndetermined by a genetic variant may have very different sizes; and the\nevaluation of many possible variants demands highly accurate computation of\nvery small p-values. We demonstrate this problem for cancer genomics data where\nthe standard log-rank test leads to many false positive associations between\nsomatic mutations and survival time. We develop and analyze a novel algorithm,\nExact Log-rank Test (ExaLT), that accurately computes the p-value of the\nlog-rank statistic under an exact distribution that is appropriate for any size\npopulations. We demonstrate the advantages of ExaLT on data from published\ncancer genomics studies, finding significant differences from the reported\np-values. We analyze somatic mutations in six cancer types from The Cancer\nGenome Atlas (TCGA), finding mutations with known association to survival as\nwell as several novel associations. In contrast, standard implementations of\nthe log-rank test report dozens-hundreds of likely false positive associations\nas more significant than these known associations.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2013 12:46:07 GMT"}], "update_date": "2013-09-18", "authors_parsed": [["Vandin", "Fabio", ""], ["Papoutsaki", "Alexandra", ""], ["Raphael", "Benjamin J.", ""], ["Upfal", "Eli", ""]]}, {"id": "1309.4513", "submitter": "Bhavya Kailkhura", "authors": "Bhavya Kailkhura and Swastik Brahma and Yunghsiang S. Han and Pramod\n  K. Varshney", "title": "Distributed Detection in Tree Topologies with Byzantines", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2014.2321735", "report-no": null, "categories": "stat.AP cs.CR math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of distributed detection in tree\ntopologies in the presence of Byzantines. The expression for minimum attacking\npower required by the Byzantines to blind the fusion center (FC) is obtained.\nMore specifically, we show that when more than a certain fraction of individual\nnode decisions are falsified, the decision fusion scheme becomes completely\nincapable. We obtain closed form expressions for the optimal attacking\nstrategies that minimize the detection error exponent at the FC. We also look\nat the possible counter-measures from the FC's perspective to protect the\nnetwork from these Byzantines. We formulate the robust topology design problem\nas a bi-level program and provide an efficient algorithm to solve it. We also\nprovide some numerical results to gain insights into the solution.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2013 01:07:18 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Kailkhura", "Bhavya", ""], ["Brahma", "Swastik", ""], ["Han", "Yunghsiang S.", ""], ["Varshney", "Pramod K.", ""]]}, {"id": "1309.4871", "submitter": "Rajesh  Singh", "authors": "Rajesh Singh, Sachin Malik, A. A. Adewara and Florentin Smarandache", "title": "Multivariate ratio estimation with known population proportion of two\n  auxiliary characters for finite population", "comments": "8 pages, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present study, we propose estimators based on geometric and harmonic\nmean for estimating population mean using information on two auxiliary\nattributes in simple random sampling. We have shown that, when we have\nmulti-auxiliary attributes, estimators based on geometric mean and harmonic\nmean are less biased than Olkin (1958), Naik and Gupta (1996) and Singh (1967)\ntype- estimator under certain conditions. However, the MSE of Olkin(1958)\nestimator and geometric and harmonic estimators are same up to the first order\nof approximation.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2013 06:28:30 GMT"}], "update_date": "2013-09-20", "authors_parsed": [["Singh", "Rajesh", ""], ["Malik", "Sachin", ""], ["Adewara", "A. A.", ""], ["Smarandache", "Florentin", ""]]}, {"id": "1309.4975", "submitter": "Enkelejd Hashorva", "authors": "Krzysztof Debicki, Enkelejd Hashorva, Lanpeng Ji", "title": "Gaussian Approximation of Perturbed Chi-Square Risks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show that the conditional distribution of perturbed\nchi-quare risks can be approximated by certain distributions including the\nGaussian ones. Our results are of interest for conditional extreme value models\nand multivariate extremes as shown in three applications.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2013 13:47:04 GMT"}], "update_date": "2013-09-20", "authors_parsed": [["Debicki", "Krzysztof", ""], ["Hashorva", "Enkelejd", ""], ["Ji", "Lanpeng", ""]]}, {"id": "1309.4999", "submitter": "Cyril Voyant", "authors": "Cyril Voyant (SPE), C. Darras (SPE), Marc Muselli (SPE), Christophe\n  Paoli (SPE), Marie Laure Nivet (SPE), Philippe Poggi (SPE)", "title": "Bayesian rules and stochastic models for high accuracy prediction of\n  solar radiation", "comments": "Applied Energy (2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is essential to find solar predictive methods to massively insert\nrenewable energies on the electrical distribution grid. The goal of this study\nis to find the best methodology allowing predicting with high accuracy the\nhourly global radiation. The knowledge of this quantity is essential for the\ngrid manager or the private PV producer in order to anticipate fluctuations\nrelated to clouds occurrences and to stabilize the injected PV power. In this\npaper, we test both methodologies: single and hybrid predictors. In the first\nclass, we include the multi-layer perceptron (MLP), auto-regressive and moving\naverage (ARMA), and persistence models. In the second class, we mix these\npredictors with Bayesian rules to obtain ad-hoc models selections, and Bayesian\naverages of outputs related to single models. If MLP and ARMA are equivalent\n(nRMSE close to 40.5% for the both), this hybridization allows a nRMSE gain\nupper than 14 percentage points compared to the persistence estimation\n(nRMSE=37% versus 51%).\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2013 06:44:33 GMT"}], "update_date": "2013-09-20", "authors_parsed": [["Voyant", "Cyril", "", "SPE"], ["Darras", "C.", "", "SPE"], ["Muselli", "Marc", "", "SPE"], ["Paoli", "Christophe", "", "SPE"], ["Nivet", "Marie Laure", "", "SPE"], ["Poggi", "Philippe", "", "SPE"]]}, {"id": "1309.5056", "submitter": "Anand Bhaskar", "authors": "Anand Bhaskar, Yun S. Song", "title": "Descartes' rule of signs and the identifiability of population\n  demographic models from genomic variation data", "comments": "Published in at http://dx.doi.org/10.1214/14-AOS1264 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2014, Vol. 42, No. 6, 2469-2493", "doi": "10.1214/14-AOS1264", "report-no": "IMS-AOS-AOS1264", "categories": "q-bio.PE math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sample frequency spectrum (SFS) is a widely-used summary statistic of\ngenomic variation in a sample of homologous DNA sequences. It provides a highly\nefficient dimensional reduction of large-scale population genomic data and its\nmathematical dependence on the underlying population demography is well\nunderstood, thus enabling the development of efficient inference algorithms.\nHowever, it has been recently shown that very different population demographies\ncan actually generate the same SFS for arbitrarily large sample sizes. Although\nin principle this nonidentifiability issue poses a thorny challenge to\nstatistical inference, the population size functions involved in the\ncounterexamples are arguably not so biologically realistic. Here, we revisit\nthis problem and examine the identifiability of demographic models under the\nrestriction that the population sizes are piecewise-defined where each piece\nbelongs to some family of biologically-motivated functions. Under this\nassumption, we prove that the expected SFS of a sample uniquely determines the\nunderlying demographic model, provided that the sample is sufficiently large.\nWe obtain a general bound on the sample size sufficient for identifiability;\nthe bound depends on the number of pieces in the demographic model and also on\nthe type of population size function in each piece. In the cases of\npiecewise-constant, piecewise-exponential and piecewise-generalized-exponential\nmodels, which are often assumed in population genomic inferences, we provide\nexplicit formulas for the bounds as simple functions of the number of pieces.\nLastly, we obtain analogous results for the \"folded\" SFS, which is often used\nwhen there is ambiguity as to which allelic type is ancestral. Our results are\nproved using a generalization of Descartes' rule of signs for polynomials to\nthe Laplace transform of piecewise continuous functions.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2013 17:22:24 GMT"}, {"version": "v2", "created": "Mon, 11 Aug 2014 00:09:18 GMT"}, {"version": "v3", "created": "Mon, 1 Dec 2014 10:56:46 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Bhaskar", "Anand", ""], ["Song", "Yun S.", ""]]}, {"id": "1309.5073", "submitter": "Remy Chicheportiche", "authors": "R\\'emy Chicheportiche", "title": "Non-linear dependences in finance", "comments": "PhD Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The thesis is composed of three parts. Part I introduces the mathematical and\nstatistical tools that are relevant for the study of dependences, as well as\nstatistical tests of Goodness-of-fit for empirical probability distributions. I\npropose two extensions of usual tests when dependence is present in the sample\ndata and when observations have a fat-tailed distribution. The financial\ncontent of the thesis starts in Part II. I present there my studies regarding\nthe \"cross-sectional\" dependences among the time series of daily stock returns,\ni.e. the instantaneous forces that link several stocks together and make them\nbehave somewhat collectively rather than purely independently. A calibration of\na new factor model is presented here, together with a comparison to\nmeasurements on real data. Finally, Part III investigates the temporal\ndependences of single time series, using the same tools and measures of\ncorrelation. I propose two contributions to the study of the origin and\ndescription of \"volatility clustering\": one is a generalization of the\nARCH-like feedback construction where the returns are self-exciting, and the\nother one is a more original description of self-dependences in terms of\ncopulas. The latter can be formulated model-free and is not specific to\nfinancial time series. In fact, I also show here how concepts like recurrences,\nrecords, aftershocks and waiting times, that characterize the dynamics in a\ntime series can be written in the unifying framework of the copula.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2013 17:07:49 GMT"}], "update_date": "2013-09-20", "authors_parsed": [["Chicheportiche", "R\u00e9my", ""]]}, {"id": "1309.5109", "submitter": "Ashton Verdery", "authors": "Ashton M. Verdery, Ted Mouw, Shawn Bauldry, Peter J. Mucha", "title": "Network Structure and Biased Variance Estimation in Respondent Driven\n  Sampling", "comments": "56 pages, 5 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores bias in the estimation of sampling variance in Respondent\nDriven Sampling (RDS). Prior methodological work on RDS has focused on its\nproblematic assumptions and the biases and inefficiencies of its estimators of\nthe population mean. Nonetheless, researchers have given only slight attention\nto the topic of estimating sampling variance in RDS, despite the importance of\nvariance estimation for the construction of confidence intervals and hypothesis\ntests. In this paper, we show that the estimators of RDS sampling variance rely\non a critical assumption that the network is First Order Markov (FOM) with\nrespect to the dependent variable of interest. We demonstrate, through\nintuitive examples, mathematical generalizations, and computational experiments\nthat current RDS variance estimators will always underestimate the population\nsampling variance of RDS in empirical networks that do not conform to the FOM\nassumption. Analysis of 215 observed university and school networks from\nFacebook and Add Health indicates that the FOM assumption is violated in every\nempirical network we analyze, and that these violations lead to substantially\nbiased RDS estimators of sampling variance. We propose and test two alternative\nvariance estimators that show some promise for reducing biases, but which also\nillustrate the limits of estimating sampling variance with only partial\ninformation on the underlying population social network.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2013 22:02:23 GMT"}, {"version": "v2", "created": "Sat, 17 May 2014 20:12:23 GMT"}, {"version": "v3", "created": "Fri, 4 Dec 2015 14:39:05 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Verdery", "Ashton M.", ""], ["Mouw", "Ted", ""], ["Bauldry", "Shawn", ""], ["Mucha", "Peter J.", ""]]}, {"id": "1309.5337", "submitter": "Mengjie Chen", "authors": "Mengjie Chen and Haifan Lin and Hongyu Zhao", "title": "Change Point Analysis of Histone Modifications Reveals Epigenetic Blocks\n  Linking to Physical Domains", "comments": "23 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Histone modification is a vital epigenetic mechanism for transcriptional\ncontrol in eukaryotes. High-throughput techniques have enabled whole-genome\nanalysis of histone modifications in recent years. However, most studies assume\none combination of histone modification invariantly translates to one\ntranscriptional output regardless of local chromatin environment. In this study\nwe hypothesize that, the genome is organized into local domains that manifest\nsimilar enrichment pattern of histone modification, which leads to orchestrated\nregulation of expression of genes with relevant bio- logical functions. We\npropose a multivariate Bayesian Change Point (BCP) model to segment the\nDrosophila melanogaster genome into consecutive blocks on the basis of\ncombinatorial patterns of histone marks. By modeling the sparse distribution of\nhistone marks across the chromosome with a zero-inflated Gaussian mixture, our\npartitions capture local BLOCKs that manifest relatively homogeneous enrichment\npattern of histone modifications. We further characterized BLOCKs by their\ntranscription levels, distribution of genes, degree of co-regulation and GO\nenrichment. Our results demonstrate that these BLOCKs, although inferred merely\nfrom histone modifications, reveal strong relevance with physical domains,\nwhich suggest their important roles in chromatin organization and coordinated\ngene regulation.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2013 17:52:18 GMT"}, {"version": "v2", "created": "Fri, 9 May 2014 14:38:44 GMT"}], "update_date": "2014-05-12", "authors_parsed": [["Chen", "Mengjie", ""], ["Lin", "Haifan", ""], ["Zhao", "Hongyu", ""]]}, {"id": "1309.5616", "submitter": "Anat Reiner-Benaim", "authors": "Anat Reiner-Benaim", "title": "Scan statistic tail probability assessment based on process covariance\n  and window size", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A scan statistic is examined for the purpose of testing the existence of a\nglobal peak in a random process with dependent variables of any distribution.\nThe scan statistic tail probability is obtained based on the covariance of the\nmoving sums process, thereby accounting for the spatial nature of the data as\nwell as the size of the searching window. Exact formulas linking this\ncovariance to the window size and the correlation coefficient are developed\nunder general, common and auto covariance structures of the variables in the\noriginal process. The implementation and applicability of the formulas are\ndemonstrated on families of multiple processes of t-statistics. A sensitivity\nanalysis provides further insight into the variant interaction of the tail\nprobability with the influence parameters. An R code for the tail probability\ncomputation is offered within the supplementary material.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2013 16:39:27 GMT"}], "update_date": "2013-09-24", "authors_parsed": [["Reiner-Benaim", "Anat", ""]]}, {"id": "1309.5806", "submitter": "Pierre Blanc", "authors": "Pierre Blanc, R\\'emy Chicheportiche, Jean-Philippe Bouchaud", "title": "The fine structure of volatility feedback II: overnight and intra-day\n  effects", "comments": null, "journal-ref": "Physica A 402 (2014) 58-75", "doi": "10.1016/j.physa.2014.01.047", "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We decompose, within an ARCH framework, the daily volatility of stocks into\novernight and intra-day contributions. We find, as perhaps expected, that the\novernight and intra-day returns behave completely differently. For example,\nwhile past intra-day returns affect equally the future intra-day and overnight\nvolatilities, past overnight returns have a weak effect on future intra-day\nvolatilities (except for the very next one) but impact substantially future\novernight volatilities. The exogenous component of overnight volatilities is\nfound to be close to zero, which means that the lion's share of overnight\nvolatility comes from feedback effects. The residual kurtosis of returns is\nsmall for intra-day returns but infinite for overnight returns. We provide a\nplausible interpretation for these findings, and show that our\nIntra-Day/Overnight model significantly outperforms the standard ARCH framework\nbased on daily returns for Out-of-Sample predictions.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2013 13:59:19 GMT"}, {"version": "v2", "created": "Tue, 20 May 2014 15:46:22 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Blanc", "Pierre", ""], ["Chicheportiche", "R\u00e9my", ""], ["Bouchaud", "Jean-Philippe", ""]]}, {"id": "1309.6111", "submitter": "Alex Lenkoski", "authors": "Anita V. Dyrrdal, Alex Lenkoski, Thordis L. Thorarinsdottir, Frode\n  Stordal", "title": "Bayesian hierarchical modeling of extreme hourly precipitation in Norway", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial maps of extreme precipitation are a critical component of flood\nestimation in hydrological modeling, as well as in the planning and design of\nimportant infrastructure. This is particularly relevant in countries such as\nNorway that have a high density of hydrological power generating facilities and\nare exposed to significant risk of infrastructure damage due to flooding. In\nthis work, we estimate a spatially coherent map of the distribution of extreme\nhourly precipitation in Norway, in terms of return levels, by linking\ngeneralized extreme value (GEV) distributions with latent Gaussian fields in a\nBayesian hierarchical model. Generalized linear models on the parameters of the\nGEV distribution are able to incorporate location-specific geographic and\nmeteorological information and thereby accommodate these effects on extreme\nprecipitation. A Gaussian field on the GEV parameters captures additional\nunexplained spatial heterogeneity and overcomes the sparse grid on which\nobservations are collected. We conduct an extensive analysis of the factors\nthat affect the GEV parameters and show that our combination is able to\nappropriately characterize both the spatial variability of the distribution of\nextreme hourly precipitation in Norway, and the associated uncertainty in these\nestimates.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2013 11:18:55 GMT"}, {"version": "v2", "created": "Mon, 26 May 2014 07:56:10 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Dyrrdal", "Anita V.", ""], ["Lenkoski", "Alex", ""], ["Thorarinsdottir", "Thordis L.", ""], ["Stordal", "Frode", ""]]}, {"id": "1309.6136", "submitter": "Enkelejd Hashorva", "authors": "Enkelejd Hashorva and Zhichao Weng", "title": "Berman's inequality under random scaling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Berman's inequality is the key for establishing asymptotic properties of\nmaxima of Gaussian random sequences and supremum of Gaussian random fields.\nThis contribution shows that, asymptotically an extended version of Berman's\ninequality can be established for randomly scaled Gaussian random vectors. Two\napplications presented in this paper demonstrate the use of Berman's inequality\nunder random scaling.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2013 12:51:42 GMT"}, {"version": "v2", "created": "Wed, 23 Apr 2014 10:30:25 GMT"}], "update_date": "2014-04-24", "authors_parsed": [["Hashorva", "Enkelejd", ""], ["Weng", "Zhichao", ""]]}, {"id": "1309.6158", "submitter": "Giovanni Montana", "authors": "Aaron Sim, Dimosthenis Tsagkrasoulis, Giovanni Montana", "title": "Random Forests on Distance Matrices for Imaging Genetics Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a non-parametric regression methodology, Random Forests on\nDistance Matrices (RFDM), for detecting genetic variants associated to\nquantitative phenotypes representing the human brain's structure or function,\nand obtained using neuroimaging techniques. RFDM, which is an extension of\ndecision forests, requires a distance matrix as response that encodes all\npair-wise phenotypic distances in the random sample. We discuss ways to learn\nsuch distances directly from the data using manifold learning techniques, and\nhow to define such distances when the phenotypes are non-vectorial objects such\nas brain connectivity networks. We also describe an extension of RFDM to detect\nespistatic effects while keeping the computational complexity low. Extensive\nsimulation results and an application to an imaging genetics study of\nAlzheimer's Disease are presented and discussed.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2013 13:58:16 GMT"}], "update_date": "2013-09-25", "authors_parsed": [["Sim", "Aaron", ""], ["Tsagkrasoulis", "Dimosthenis", ""], ["Montana", "Giovanni", ""]]}, {"id": "1309.6178", "submitter": "Till Sabel", "authors": "Till Sabel, Johannes Schmidt-Hieber, Axel Munk", "title": "Spot volatility estimation for high-frequency data: adaptive estimation\n  in practice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop further the spot volatility estimator introduced in Hoffmann, Munk\nand Schmidt-Hieber (2012) from a practical point of view and make it useful for\nthe analysis of high-frequency financial data. In a first part, we adjust the\nestimator substantially in order to achieve good finite sample performance and\nto overcome difficulties arising from violations of the additive microstructure\nnoise model (e.g. jumps, rounding errors). These modifications are justified by\nsimulations. The second part is devoted to investigate the behavior of\nvolatility in response to macroeconomic events. We give evidence that the spot\nvolatility of Euro-BUND futures is considerably higher during press conferences\nof the European Central Bank. As an outlook, we present an estimator for the\nspot covolatility of two different prices.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2013 14:30:20 GMT"}], "update_date": "2013-09-25", "authors_parsed": [["Sabel", "Till", ""], ["Schmidt-Hieber", "Johannes", ""], ["Munk", "Axel", ""]]}, {"id": "1309.6392", "submitter": "Adam Kapelner", "authors": "Alex Goldstein, Adam Kapelner, Justin Bleich and Emil Pitkin", "title": "Peeking Inside the Black Box: Visualizing Statistical Learning with\n  Plots of Individual Conditional Expectation", "comments": "22 pages, 14 figures, 2 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents Individual Conditional Expectation (ICE) plots, a tool\nfor visualizing the model estimated by any supervised learning algorithm.\nClassical partial dependence plots (PDPs) help visualize the average partial\nrelationship between the predicted response and one or more features. In the\npresence of substantial interaction effects, the partial response relationship\ncan be heterogeneous. Thus, an average curve, such as the PDP, can obfuscate\nthe complexity of the modeled relationship. Accordingly, ICE plots refine the\npartial dependence plot by graphing the functional relationship between the\npredicted response and the feature for individual observations. Specifically,\nICE plots highlight the variation in the fitted values across the range of a\ncovariate, suggesting where and to what extent heterogeneities might exist. In\naddition to providing a plotting suite for exploratory analysis, we include a\nvisual test for additive structure in the data generating model. Through\nsimulated examples and real data sets, we demonstrate how ICE plots can shed\nlight on estimated models in ways PDPs cannot. Procedures outlined are\navailable in the R package ICEbox.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2013 03:34:37 GMT"}, {"version": "v2", "created": "Thu, 20 Mar 2014 01:12:11 GMT"}], "update_date": "2014-03-21", "authors_parsed": [["Goldstein", "Alex", ""], ["Kapelner", "Adam", ""], ["Bleich", "Justin", ""], ["Pitkin", "Emil", ""]]}, {"id": "1309.6702", "submitter": "Dominique Guillot", "authors": "Dominique Guillot, Bala Rajaratnam, Julien Emile-Geay", "title": "Statistical paleoclimate reconstructions via Markov random fields", "comments": "Published at http://dx.doi.org/10.1214/14-AOAS794 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 1, 324-352", "doi": "10.1214/14-AOAS794", "report-no": "IMS-AOAS-AOAS794", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding centennial scale climate variability requires data sets that\nare accurate, long, continuous and of broad spatial coverage. Since\ninstrumental measurements are generally only available after 1850, temperature\nfields must be reconstructed using paleoclimate archives, known as proxies.\nVarious climate field reconstructions (CFR) methods have been proposed to\nrelate past temperature to such proxy networks. In this work, we propose a new\nCFR method, called GraphEM, based on Gaussian Markov random fields embedded\nwithin an EM algorithm. Gaussian Markov random fields provide a natural and\nflexible framework for modeling high-dimensional spatial fields. At the same\ntime, they provide the parameter reduction necessary for obtaining precise and\nwell-conditioned estimates of the covariance structure, even in the\nsample-starved setting common in paleoclimate applications. In this paper, we\npropose and compare the performance of different methods to estimate the\ngraphical structure of climate fields, and demonstrate how the GraphEM\nalgorithm can be used to reconstruct past climate variations. The performance\nof GraphEM is compared to the widely used CFR method RegEM with regularization\nvia truncated total least squares, using synthetic data. Our results show that\nGraphEM can yield significant improvements, with uniform gains over space, and\nfar better risk properties. We demonstrate that the spatial structure of\ntemperature fields can be well estimated by graphs where each neighbor is only\nconnected to a few geographically close neighbors, and that the increase in\nperformance is directly related to recovering the underlying sparsity in the\ncovariance of the spatial field. Our work demonstrates how significant\nimprovements can be made in climate reconstruction methods by better modeling\nthe covariance structure of the climate field.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 01:19:20 GMT"}, {"version": "v2", "created": "Tue, 29 Apr 2014 00:02:15 GMT"}, {"version": "v3", "created": "Wed, 22 Oct 2014 05:05:44 GMT"}, {"version": "v4", "created": "Tue, 2 Jun 2015 04:43:58 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Guillot", "Dominique", ""], ["Rajaratnam", "Bala", ""], ["Emile-Geay", "Julien", ""]]}, {"id": "1309.6919", "submitter": "Or Zuk", "authors": "Or Zuk, Amnon Amir, Amit Zeisel, Ohad Shamir and Noam Shental", "title": "Accurate Profiling of Microbial Communities from Massively Parallel\n  Sequencing using Convex Optimization", "comments": "To appear in SPIRE 13", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE q-bio.GN q-bio.QM stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the Microbial Community Reconstruction ({\\bf MCR}) Problem, which\nis fundamental for microbiome analysis. In this problem, the goal is to\nreconstruct the identity and frequency of species comprising a microbial\ncommunity, using short sequence reads from Massively Parallel Sequencing (MPS)\ndata obtained for specified genomic regions. We formulate the problem\nmathematically as a convex optimization problem and provide sufficient\nconditions for identifiability, namely the ability to reconstruct species\nidentity and frequency correctly when the data size (number of reads) grows to\ninfinity. We discuss different metrics for assessing the quality of the\nreconstructed solution, including a novel phylogenetically-aware metric based\non the Mahalanobis distance, and give upper-bounds on the reconstruction error\nfor a finite number of reads under different metrics. We propose a scalable\ndivide-and-conquer algorithm for the problem using convex optimization, which\nenables us to handle large problems (with $\\sim10^6$ species). We show using\nnumerical simulations that for realistic scenarios, where the microbial\ncommunities are sparse, our algorithm gives solutions with high accuracy, both\nin terms of obtaining accurate frequency, and in terms of species phylogenetic\nresolution.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 14:30:13 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Zuk", "Or", ""], ["Amir", "Amnon", ""], ["Zeisel", "Amit", ""], ["Shamir", "Ohad", ""], ["Shental", "Noam", ""]]}, {"id": "1309.7376", "submitter": "Hau-tieng Wu", "authors": "Jin-Ting Zhang, Ming-Yen Cheng, Chi-Jen Tseng, Hau-Tieng Wu", "title": "A New Test for One-Way ANOVA with Functional Data and Application to\n  Ischemic Heart Screening", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and study a new global test, namely the $F_{\\max}$-test, for the\none-way ANOVA problem in functional data analysis. The test statistic is taken\nas the maximum value of the usual pointwise $F$-test statistics over the\ninterval the functional responses are observed. A nonparametric bootstrap\nmethod is employed to approximate the null distribution of the test statistic\nand to obtain an estimated critical value for the test. The asymptotic random\nexpression of the test statistic is derived and the asymptotic power is\nstudied. In particular, under mild conditions, the $F_{\\max}$-test\nasymptotically has the correct level and is root-$n$ consistent in detecting\nlocal alternatives. Via some simulation studies, it is found that in terms of\nboth level accuracy and power, the $F_{\\max}$-test outperforms the Globalized\nPointwise F (GPF) test of \\cite{Zhang_Liang:2013} when the functional data are\nhighly or moderately correlated, and its performance is comparable with the\nlatter otherwise. An application to an ischemic heart real dataset suggests\nthat, after proper manipulation, resting electrocardiogram (ECG) signals can be\nused as an effective tool in clinical ischemic heart screening, without the\nneed of further stress tests as in the current standard procedure.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2013 21:55:10 GMT"}], "update_date": "2013-10-01", "authors_parsed": [["Zhang", "Jin-Ting", ""], ["Cheng", "Ming-Yen", ""], ["Tseng", "Chi-Jen", ""], ["Wu", "Hau-Tieng", ""]]}, {"id": "1309.7501", "submitter": "Abhik Ghosh", "authors": "Abhik Ghosh, Samit Roy, Sujatro Chaklader", "title": "A Model Explaining Correlation Between Observed Values in Contingency\n  Tables", "comments": "Project done as a part of Comprehensive Statistics course in B. Stat.\n  3rd year (2010) in Indian Statistical Institute, Kolkata", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, a model is proposed using Bayesian techniques to account for\nthe high correlation between many observed set of contingency tables. In many\nreal life data this high correlation is encountered. Simulation studies are\nalso given to check the effectiveness of this model.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2013 20:41:35 GMT"}], "update_date": "2013-10-01", "authors_parsed": [["Ghosh", "Abhik", ""], ["Roy", "Samit", ""], ["Chaklader", "Sujatro", ""]]}, {"id": "1309.7721", "submitter": "Adrien Ickowicz", "authors": "Ross Sparks, Adrien Ickowicz", "title": "Spatio-Temporal Disease Surveillance: Forward Selection Scan Statistic", "comments": "19 pages, 4 figures, submitted to Journal of applied statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scan statistic sets the benchmark for spatio-temporal surveillance\nmethods with its popularity. In its simplest form it scans the target area and\ntime to find regions with disease count higher than expected. If the shape and\nsize of the disease outbreaks are known, then to detect it sufficiently early\nthe scan statistic can design its search area to be efficient for this shape\nand size. A plan that is efficient at detecting a range of disease outbreak\nshapes and sizes is important because these vary from one outbreak to the next\nand are generally never known in advance. This paper offers a forward selection\nscan statistic that reduces the computational effort on the usual single window\nscan plan, while still offering greater flexibility in signalling outbreaks of\nvarying shapes. The approach starts by dividing the target geographical regions\ninto a lattice. Secondly it smooths the time series of lattice cell counts\nusing multivariate exponential weighted moving averages. Thirdly, these EWMA\ncell counts are spatially smoothed to reduce spatial noise and leave the\nspatial signal. The fourth step uses forward selection approach to scanning\nmutually exclusive and exhaustive rectangular regions of dynamic dimensions. In\nthe fifth step, it prunes away all insignificant scanned regions where counts\nare not significantly higher than expected. An outbreak is signaled if at least\none region remains after pruning. If all regions are pruned away - including\nthe scan of the target region, then no outbreak is signaled.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2013 04:55:12 GMT"}], "update_date": "2013-10-01", "authors_parsed": [["Sparks", "Ross", ""], ["Ickowicz", "Adrien", ""]]}]