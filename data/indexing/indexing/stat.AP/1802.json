[{"id": "1802.00032", "submitter": "Jiahui Guan", "authors": "Jiahui Guan, Hsieh Fushing", "title": "Coupling geometry on binary bipartite networks: hypotheses testing on\n  pattern geometry and nestedness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Upon a matrix representation of a binary bipartite network, via the\npermutation invariance, a coupling geometry is computed to approximate the\nminimum energy macrostate of a network's system. Such a macrostate is supposed\nto constitute the intrinsic structures of the system, so that the coupling\ngeometry should be taken as information contents, or even the nonparametric\nminimum sufficient statistics of the network data. Then pertinent null and\nalternative hypotheses, such as nestedness, are to be formulated according to\nthe macrostate. That is, any efficient testing statistic needs to be a function\nof this coupling geometry. These conceptual architectures and mechanisms are by\nand large still missing in community ecology literature, and rendered\nmisconceptions prevalent in this research area. Here the algorithmically\ncomputed coupling geometry is shown consisting of deterministic multiscale\nblock patterns, which are framed by two marginal ultrametric trees on row and\ncolumn axes, and stochastic uniform randomness within each block found on the\nfinest scale. Functionally a series of increasingly larger ensembles of matrix\nmimicries is derived by conforming to the multiscale block configurations. Here\nmatrix mimicking is meant to be subject to constraints of row and column sums\nsequences. Based on such a series of ensembles, a profile of distributions\nbecomes a natural device for checking the validity of testing statistics or\nstructural indexes. An energy based index is used for testing whether network\ndata indeed contains structural geometry. A new version block-based nestedness\nindex is also proposed. Its validity is checked and compared with the existing\nones. A computing paradigm, called Data Mechanics, and its application on one\nreal data network are illustrated throughout the developments and discussions\nin this paper.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 19:39:38 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Guan", "Jiahui", ""], ["Fushing", "Hsieh", ""]]}, {"id": "1802.00117", "submitter": "Horacio Samaniego", "authors": "Teodoro Dannemann and Boris Sotomayor-G\\'omez and Horacio Samaniego", "title": "The time geography of segregation during working hours", "comments": "17 pages, 4 figures and 3 tables", "journal-ref": null, "doi": "10.1098/rsos.180749", "report-no": null, "categories": "cs.SI stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Understanding segregation is essential to develop planning tools for building\nmore inclusive cities. Theoretically, segregation at the work place has been\ndescribed as lower compared to residential segregation given the importance of\nskill complementarity among other productive factors shaping the economies of\ncities. This paper tackles segregation during working hours from a dynamical\nperspective by focusing on the movement of urbanites across the city. In\ncontrast to measuring residential patterns of segregation, we used mobile phone\ndata to infer home-work trajectory net- works and apply a community detection\nalgorithm to the example city of Santiago, Chile. We then describe\nqualitatively and quantitatively outlined communities, in terms of their socio\neco- nomic composition. We then evaluate segregation for each of these\ncommunities as the probability that a person from a specific community will\ninteract with a co-worker from the same commu- nity. Finally, we compare these\nresults with simulations where a new work location is set for each real user,\nfollowing the empirical probability distributions of home-work distances and\nangles of direction for each community. Methodologically, this study shows that\nsegregation during working hours for Santiago is unexpectedly high for most of\nthe city with the exception of its central and business district. In fact, the\nonly community that is not statistically segregated corresponds to the downtown\narea of Santiago, described as a zone of encounter and integration across the\ncity.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 01:29:49 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Dannemann", "Teodoro", ""], ["Sotomayor-G\u00f3mez", "Boris", ""], ["Samaniego", "Horacio", ""]]}, {"id": "1802.00228", "submitter": "A. J. van Es", "authors": "Bert van Es", "title": "Strength of forensic evidence for composite hypotheses: An empirical\n  Bayes view with a fixed prior quantile", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the forensic problem of determining the strength of evidence of\na continuously distributed measurement of evidence, in the situation of\ncomposite hypotheses of the prosecutor and the defence concerning a parameter\nof a parametric model, we consider empirical Bayes methods with a prescribed\nquantile value for the prior distribution.\n  Firstly we derive the strength of evidence for nonparametric priors. It turns\nout that we get the by now more or less accepted strength of evidence as the\nratio of two suprema,\n$\\sup_{\\theta\\geq\\theta_0}f(x|\\theta)/\\sup_{\\theta<\\theta_0}f(x|\\theta)$. Here\nthe hypotheses of the prosecutor and defence are given by $H_p: \\theta\\geq\n\\theta_0$ and $H_d:\\theta<\\theta_0$. The evidence is seen as a measurement $x$\nwhich is a realization of a random variable with a density $f(x|\\theta)$.\n  Secondly we consider a similar parametric empirical Bayes method with a\nquantile restriction on the prior where the prior distribution is assumed to be\nnormal. Some interesting strength of evidence functions are derived for this\nsituation.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 10:23:23 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["van Es", "Bert", ""]]}, {"id": "1802.00255", "submitter": "Yuya Yoshikawa", "authors": "Yuya Yoshikawa, Yusaku Imai", "title": "A Nonparametric Delayed Feedback Model for Conversion Rate Prediction", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting conversion rates (CVRs) in display advertising (e.g., predicting\nthe proportion of users who purchase an item (i.e., a conversion) after its\ncorresponding ad is clicked) is important when measuring the effects of ads\nshown to users and to understanding the interests of the users. There is\ngenerally a time delay (i.e., so-called {\\it delayed feedback}) between the ad\nclick and conversion. Owing to the delayed feedback, samples that are converted\nafter an observation period may be treated as negative. To overcome this\ndrawback, CVR prediction assuming that the time delay follows an exponential\ndistribution has been proposed. In practice, however, there is no guarantee\nthat the delay is generated from the exponential distribution, and the best\ndistribution with which to represent the delay depends on the data. In this\npaper, we propose a nonparametric delayed feedback model for CVR prediction\nthat represents the distribution of the time delay without assuming a\nparametric distribution, such as an exponential or Weibull distribution.\nBecause the distribution of the time delay is modeled depending on the content\nof an ad and the features of a user, various shapes of the distribution can be\nrepresented potentially. In experiments, we show that the proposed model can\ncapture the distribution for the time delay on a synthetic dataset, even when\nthe distribution is complicated. Moreover, on a real dataset, we show that the\nproposed model outperforms the existing method that assumes an exponential\ndistribution for the time delay in terms of conversion rate prediction.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 12:01:23 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Yoshikawa", "Yuya", ""], ["Imai", "Yusaku", ""]]}, {"id": "1802.00382", "submitter": "Amitabha Karmakar", "authors": "Amitabha Karmakar", "title": "Classifying medical notes into standard disease codes using Machine\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the automatic classification of patient discharge notes into\nstandard disease labels. We find that Convolutional Neural Networks with\nAttention outperform previous algorithms used in this task, and suggest further\nareas for improvement.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 16:46:00 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Karmakar", "Amitabha", ""]]}, {"id": "1802.00517", "submitter": "Manoel Santos Neto", "authors": "Vera Tomazella, Juv\\^encio S. Nobre, Gustavo H.A. Pereira and Manoel\n  Santos-Neto", "title": "Zero-adjusted Birnbaum-Saunders regression model", "comments": "13 pages 9 figures", "journal-ref": null, "doi": "10.1016/j.spl.2019.01.019", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce the zero-adjusted Birnbaum-Saunders regression\nmodel. This new model generalizes at least seven Birnbaum-Saunders regression\nmodels. The idea of this modeling is mixing a degenerate distribution at zero\nwith a Birnbaum-Saunders distribution. Besides the capacity to account for\nexcess zeros, the zero-adjusted Birnbaum-Saunders distribution additionally\nproduces an attractive modeling structure to right-skewed data. In this model,\nthe mean and precision parameter of the Birnbaum-Saunders distribution and the\nprobability of zeros can be related to linear and/or non-linear predictors\nthrough link functions. We derive a type of residual to perform diagnostic\nanalysis and a perturbation scheme for identifying those observations that\nexert unusual influence on the estimation process. Finally, two applications to\nreal data show the potential of the model.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 23:39:21 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Tomazella", "Vera", ""], ["Nobre", "Juv\u00eancio S.", ""], ["Pereira", "Gustavo H. A.", ""], ["Santos-Neto", "Manoel", ""]]}, {"id": "1802.00828", "submitter": "Deisy Morselli Gysi", "authors": "Deisy Morselli Gysi, Tiago Miranda Fragoso, Volker Buskamp, Eivind\n  Almaas and Katja Nowick", "title": "Comparing multiple networks using the Co-expression Differential Network\n  Analysis (CoDiNA)", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0240523", "report-no": null, "categories": "stat.CO q-bio.MN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biomedical sciences are increasingly recognising the relevance of gene\nco-expression-networks for analysing complex-systems, phenotypes or diseases.\nWhen the goal is investigating complex-phenotypes under varying conditions, it\ncomes naturally to employ comparative network methods. While approaches for\ncomparing two networks exist, this is not the case for multiple networks. Here\nwe present a method for the systematic comparison of an unlimited number of\nnetworks: Co-expression Differential Network Analysis (CoDiNA) for detecting\nlinks and nodes that are common, specific or different to the networks.\nApplying CoDiNA to a neurogenesis study identified genes for neuron\ndifferentiation. Experimentally overexpressing one candidate resulted in\nsignificant disturbance in the underlying neurogenesis' gene regulatory\nnetwork. We compared data from adults and children with active tuberculosis to\ntest for signatures of HIV. We also identified common and distinct network\nfeatures for particular cancer types with CoDiNA. These studies show that\nCoDiNA successfully detects genes associated with the diseases.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 19:53:08 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2018 19:59:58 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Gysi", "Deisy Morselli", ""], ["Fragoso", "Tiago Miranda", ""], ["Buskamp", "Volker", ""], ["Almaas", "Eivind", ""], ["Nowick", "Katja", ""]]}, {"id": "1802.00842", "submitter": "Robert Trangucci", "authors": "Rob Trangucci, Imad Ali, Andrew Gelman, Doug Rivers", "title": "Voting patterns in 2016: Exploration using multilevel regression and\n  poststratification (MRP) on pre-election polls", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyzed 2012 and 2016 YouGov pre-election polls in order to understand\nhow different population groups voted in the 2012 and 2016 elections. We broke\nthe data down by demographics and state. We display our findings with a series\nof graphs and maps. The R code associated with this project is available at\nhttps://github.com/rtrangucci/mrp_2016_election/.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 20:49:44 GMT"}, {"version": "v2", "created": "Thu, 8 Feb 2018 04:13:18 GMT"}, {"version": "v3", "created": "Wed, 14 Mar 2018 13:46:22 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Trangucci", "Rob", ""], ["Ali", "Imad", ""], ["Gelman", "Andrew", ""], ["Rivers", "Doug", ""]]}, {"id": "1802.00994", "submitter": "Eisa Mahmoudi", "authors": "Eisa Mahmoudi, Ameneh Rostami, Rasool Roozegar", "title": "A new integer-valued AR(1) process based on power series thinning\n  operator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the first-order integer-valued autoregressive\n(INAR(1)) model, with Poisson-Lindley innovations based on power series\nthinning operator. Some mathematical features of this process are given and\nestimating the parameters is discussed by three methods; conditional least\nsquares, Yule-Walker equations and conditional maximum likelihood.Then the\nresults are studied for three special cases of power series operators. Finally,\nsome numerical results are presented with a discussion to the obtained results\nand Four real data sets are used to show the potentially of the new process.\n", "versions": [{"version": "v1", "created": "Sat, 3 Feb 2018 16:14:12 GMT"}, {"version": "v2", "created": "Fri, 5 Oct 2018 10:54:26 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Mahmoudi", "Eisa", ""], ["Rostami", "Ameneh", ""], ["Roozegar", "Rasool", ""]]}, {"id": "1802.00998", "submitter": "Ronald Yurko", "authors": "Ronald Yurko, Samuel Ventura, Maksim Horowitz", "title": "nflWAR: A Reproducible Method for Offensive Player Evaluation in\n  Football", "comments": "43 pages, 21 figures, presented at NESSIS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike other major professional sports, American football lacks comprehensive\nstatistical ratings for player evaluation that are both reproducible and easily\ninterpretable in terms of game outcomes. Existing methods for player evaluation\nin football depend heavily on proprietary data, are not reproducible, and lag\nbehind those of other major sports. We present four contributions to the study\nof football statistics in order to address these issues. First, we develop the\nR package nflscrapR to provide easy access to publicly available play-by-play\ndata from the National Football League (NFL) dating back to 2009. Second, we\nintroduce a novel multinomial logistic regression approach for estimating the\nexpected points for each play. Third, we use the expected points as input into\na generalized additive model for estimating the win probability for each play.\nFourth, we introduce our nflWAR framework, using multilevel models to isolate\nthe contributions of individual offensive skill players, and providing\nestimates for their individual wins above replacement (WAR). We estimate the\nuncertainty in each player's WAR through a resampling approach specifically\ndesigned for football, and we present these results for the 2017 NFL season. We\ndiscuss how our reproducible WAR framework, built entirely on publicly\navailable data, can be easily extended to estimate WAR for players at any\nposition, provided that researchers have access to data specifying which\nplayers are on the field during each play. Finally, we discuss the potential\nimplications of this work for NFL teams.\n", "versions": [{"version": "v1", "created": "Sat, 3 Feb 2018 17:06:08 GMT"}, {"version": "v2", "created": "Thu, 12 Jul 2018 03:35:13 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Yurko", "Ronald", ""], ["Ventura", "Samuel", ""], ["Horowitz", "Maksim", ""]]}, {"id": "1802.01053", "submitter": "Evan Rosenman", "authors": "Evan Rosenman, Nitin Viswanathan", "title": "Using Poisson Binomial GLMs to Reveal Voter Preferences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new modeling technique for solving the problem of ecological\ninference, in which individual-level associations are inferred from labeled\ndata available only at the aggregate level. We model aggregate count data as\narising from the Poisson binomial, the distribution of the sum of independent\nbut not identically distributed Bernoulli random variables. We relate\nindividual-level probabilities to individual covariates using both a logistic\nregression and a neural network. A normal approximation is derived via the\nLyapunov Central Limit Theorem, allowing us to efficiently fit these models on\nlarge datasets. We apply this technique to the problem of revealing voter\npreferences in the 2016 presidential election, fitting a model to a sample of\nover four million voters from the highly contested swing state of Pennsylvania.\nWe validate the model at the precinct level via a holdout set, and at the\nindividual level using weak labels, finding that the model is predictive and it\nlearns intuitively reasonable associations.\n", "versions": [{"version": "v1", "created": "Sun, 4 Feb 2018 00:55:46 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Rosenman", "Evan", ""], ["Viswanathan", "Nitin", ""]]}, {"id": "1802.01141", "submitter": "Subhabrata Majumdar", "authors": "Subhabrata Majumdar, Saonli Basu, Matt McGue and Snigdhansu Chatterjee", "title": "Simultaneous Selection of Multiple Important Single Nucleotide\n  Polymorphisms in Familial Genome Wide Association Studies Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a resampling-based fast variable selection technique for selecting\nimportant Single Nucleotide Polymorphisms (SNP) in multi-marker mixed effect\nmodels used in twin studies. Due to computational complexity, current practice\nincludes testing the effect of one SNP at a time, commonly termed as `single\nSNP association analysis'. Joint modeling of genetic variants within a gene or\npathway may have better power to detect the relevant genetic variants, hence we\nadapt our recently proposed framework of $e$-values to address this. In this\npaper, we propose a computationally efficient approach for single SNP detection\nin families while utilizing information on multiple SNPs simultaneously. We\nachieve this through improvements in two aspects. First, unlike other model\nselection techniques, our method only requires training a model with all\npossible predictors. Second, we utilize a fast and scalable bootstrap procedure\nthat only requires Monte-Carlo sampling to obtain bootstrapped copies of the\nestimated vector of coefficients. Using this bootstrap sample, we obtain the\n$e$-value for each SNP, and select SNPs having $e$-values below a threshold. We\nillustrate through numerical studies that our method is more effective in\ndetecting SNPs associated with a trait than either single-marker analysis using\nfamily data or model selection methods that ignore the familial dependency\nstructure. We also use the $e$-values to perform gene-level analysis in nuclear\nfamilies and detect several SNPs that have been implicated to be associated\nwith alcohol consumption.\n", "versions": [{"version": "v1", "created": "Sun, 4 Feb 2018 15:16:31 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 16:16:45 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Majumdar", "Subhabrata", ""], ["Basu", "Saonli", ""], ["McGue", "Matt", ""], ["Chatterjee", "Snigdhansu", ""]]}, {"id": "1802.01429", "submitter": "Jean-Baptiste Camps", "authors": "Jean-Baptiste Camps (CJM (EA 3624))", "title": "Manuscripts in Time and Space: Experiments in Scriptometrics on an Old\n  French Corpus", "comments": "Andrew U. Frank; Christine Ivanovic; Francesco Mambrini; Marco\n  Passarotti; Caroline Sporleder. Corpus-Based Research in the Humanities\n  CRH-2, Jan 2018, Vienna, Austria. Available, online:\n  https://www.oeaw.ac.at/fileadmin/subsites/academiaecorpora/PDF/CRH2.pdf", "journal-ref": "Proceedings of the Second Workshop on Corpus-Based Research in the\n  Humanities CRH-2, 25-26 January 2018, Vienna, Austria, pp.55-64, 2018", "doi": "10.5281/zenodo.1117924", "report-no": null, "categories": "cs.CL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Witnesses of medieval literary texts, preserved in manuscript, are layered\nobjects , being almost exclusively copies of copies. This results in multiple\nand hard to distinguish linguistic strata -- the author's scripta interacting\nwith the scriptae of the various scribes -- in a context where literary written\nlanguage is already a dialectal hybrid. Moreover, no single linguistic\nphenomenon allows to distinguish between different scriptae, and only the\ncombination of multiple characteristics is likely to be significant [9] -- but\nwhich ones? The most common approach is to search for these features in a set\nof previously selected texts, that are supposed to be representative of a given\nscripta. This can induce a circularity, in which texts are used to select\nfeatures that in turn characterise them as belonging to a linguistic area. To\ncounter this issue, this paper offers an unsupervised and corpus-based\napproach, in which clustering methods are applied to an Old French corpus to\nidentify main divisions and groups. Ultimately, scriptometric profiles are\nbuilt for each of them.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 09:07:05 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Camps", "Jean-Baptiste", "", "CJM"]]}, {"id": "1802.01733", "submitter": "Andrzej Jarynowski", "authors": "Andrzej Jarynowski, Damian Marchewka, Andrzej Buda", "title": "Internet - assisted risk assessment of infectious diseases in women\n  sexual and reproductive health", "comments": "accepted in E-methodology", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop open source infection risk calculators for patients and healthcare\nprofessionals as apps for hospital acquired infections (during child-delivery)\nand sexually transmitted infections (like HIV). Advanced versions of ehealth in\nnon-communicable diseases do not apply to epidemiology much. There is, however,\nno infection risk calculator in the Polish Internet so far, despite the\nexistence of data that may be applied to create such a tool.\n  The algorithms involve data from Information Systems (like HIS in hospitals)\nand surveys by applying mathematical modelling, Bayesian inference, logistic\nregressions, covariance analysis and social network analysis. Finally, user may\nfill or import data from Information System to obtain risk assessment and test\ndifferent settings to learn overall risk.\n  The most promising risk calculator is developed for Healthcare-associated\ninfections in modes for patient hospital sanitary inspection. The most extended\nversion for hospital epidemiologists may include many layers of hospital\ninteractions by agent-based modeling. Simplified version of calculator is\ndedicated to patients that require personalized hospitalization history of\npregnancy described by questions represented by quantitative and qualitative\nvariables. Patients receive risk assessment from interactive web application\nwith additional description about modifiable risk factors.\n  We also provide solution for sexually transmitted infections like HIV. The\nresults of calculations with meaningful description and percentage chances are\npresented in real-time to interested users. Finally, user fills the form to\nobtain risk assessment for given settings.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 23:41:29 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Jarynowski", "Andrzej", ""], ["Marchewka", "Damian", ""], ["Buda", "Andrzej", ""]]}, {"id": "1802.01786", "submitter": "Amir Karami", "authors": "Amir Karami, London S. Bennett, Xiaoyun He", "title": "Mining Public Opinion about Economic Issues: Twitter and the U.S.\n  Presidential Election", "comments": null, "journal-ref": null, "doi": "10.4018/IJSDS.2018010102", "report-no": null, "categories": "cs.SI cs.CL cs.IR stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Opinion polls have been the bridge between public opinion and politicians in\nelections. However, developing surveys to disclose people's feedback with\nrespect to economic issues is limited, expensive, and time-consuming. In recent\nyears, social media such as Twitter has enabled people to share their opinions\nregarding elections. Social media has provided a platform for collecting a\nlarge amount of social media data. This paper proposes a computational public\nopinion mining approach to explore the discussion of economic issues in social\nmedia during an election. Current related studies use text mining methods\nindependently for election analysis and election prediction; this research\ncombines two text mining methods: sentiment analysis and topic modeling. The\nproposed approach has effectively been deployed on millions of tweets to\nanalyze economic concerns of people during the 2012 US presidential election.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 03:55:37 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Karami", "Amir", ""], ["Bennett", "London S.", ""], ["He", "Xiaoyun", ""]]}, {"id": "1802.01877", "submitter": "Eleonora Carrozzo", "authors": "R. Arboretti, E. Carrozzo, F. Pesarin, L. Salmaso", "title": "Testing for equivalence: an intersection-union permutation solution", "comments": "21 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of testing for equivalence of two treatments is widely used in\nclinical trials, pharmaceutical experiments,bioequivalence and quality control.\nIt is essentially approached within the intersection-union (IU) principle.\nAccording to this principle the null hypothesis is stated as the set of effects\nlying outside a suitably established interval and the alternative as the set of\neffects lying inside that interval. The solutions provided in the literature\nare mostly based on likelihood techniques, which in turn are rather difficult\nto handle, except for cases lying within the regular exponential family and the\ninvariance principle. The main goal of present paper is to go beyond most of\nthe limitations of likelihood based methods, i.e. to work in a nonparametric\nsetting within the permutation frame. To obtain practical solutions, a new IU\npermutation test is presented and discussed. A simple simulation study for\nevaluating its main properties, and three application examples are also\npresented.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 10:30:50 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Arboretti", "R.", ""], ["Carrozzo", "E.", ""], ["Pesarin", "F.", ""], ["Salmaso", "L.", ""]]}, {"id": "1802.01987", "submitter": "Clement Lee", "authors": "Clement Lee and Darren J Wilkinson", "title": "A hierarchical model of non-homogeneous Poisson processes for Twitter\n  retweets", "comments": "48 pages, 13 figures, 2 tables", "journal-ref": null, "doi": "10.1080/01621459.2019.1585358", "report-no": null, "categories": "stat.AP cs.SI stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a hierarchical model of non-homogeneous Poisson processes (NHPP)\nfor information diffusion on online social media, in particular Twitter\nretweets. The retweets of each original tweet are modelled by a NHPP, for which\nthe intensity function is a product of time-decaying components and another\ncomponent that depends on the follower count of the original tweet author. The\nlatter allows us to explain or predict the ultimate retweet count by a network\ncentrality-related covariate. The inference algorithm enables the Bayes factor\nto be computed, in order to facilitate model selection. Finally, the model is\napplied to the retweet data sets of two hashtags.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 15:04:02 GMT"}, {"version": "v2", "created": "Mon, 8 Oct 2018 16:09:06 GMT"}, {"version": "v3", "created": "Sun, 17 Feb 2019 17:19:28 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Lee", "Clement", ""], ["Wilkinson", "Darren J", ""]]}, {"id": "1802.02099", "submitter": "Saeed Zamanzad Gavidel PhD Candidate", "authors": "Saeed Z. Gavidel, J. L. Rickli", "title": "Triage strategies for agile core sorting in extreme value scenarios", "comments": "13 pages, 7 Figures and 7 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surveys have indicated that the remanufacturing industry is concerned about\nthe necessity of agile and prioritized core sorting due to its potential\nbenefits to optimal core inventory and condition assessment, both at equipment\nand component levels. As such, core sorting holds a pivotal role in generalized\nremanufacturing operations, however, extreme value core arrivals, its\nstochastic nature and resulting sorting issues warrant targeted modelling and\nanalysis. This paper is devoted to triage as an agile and quality-based sorting\nstrategy in extreme value scenarios, which can be utilized as a complementary\ncore sorting strategy. A statistical model of extreme core arrivals is\ndeveloped based on Extreme Value (EV) theory and related Generalized Extreme\nValue (GEV) and Fr\\'echet (Fisher-Tippett type-II) distributions. The model is\napplied to extreme arrivals of valves in an industrial valve repair shop in\norder to formulate extreme arrival sorting strategies. Using a large sample\nsize, distribution parameters are estimated and the stochastic behaviour of the\nextreme valve arrivals is evaluated and verified. A generic analogy between\nmedical triage and remanufacturing triage is discussed, because triage can be\nused to address extreme core arrivals, associated statistical distributions,\nand their effect on core condition assessment in order to enhance management of\nunpredictable consequences of extreme core arrivals.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 17:51:28 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Gavidel", "Saeed Z.", ""], ["Rickli", "J. L.", ""]]}, {"id": "1802.02223", "submitter": "Song-Hwa Kwon", "authors": "Song-Hwa Kwon and Hyeong In Choi and Sung Jin Lee and Nam-Sook Wee", "title": "Seeded Ising Model and Statistical Natures of Human Iris Templates", "comments": "7 pages", "journal-ref": "Phys. Rev. E 98, 032115 (2018)", "doi": "10.1103/PhysRevE.98.032115", "report-no": null, "categories": "stat.AP cs.CV cs.HC physics.data-an q-bio.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a variant of Ising model, called the Seeded Ising Model, to model\nprobabilistic nature of human iris templates. This model is an Ising model in\nwhich the values at certain lattice points are held fixed throughout Ising\nmodel evolution. Using this we show how to reconstruct the full iris template\nfrom partial information, and we show that about 1/6 of the given template is\nneeded to recover almost all information content of the original one in the\nsense that the resulting Hamming distance is well within the range to assert\ncorrectly the identity of the subject. This leads us to propose the concept of\neffective statistical degree of freedom of iris templates and show it is about\n1/6 of the total number of bits. In particular, for a template of $2048$ bits,\nits effective statistical degree of freedom is about $342$ bits, which\ncoincides very well with the degree of freedom computed by the completely\ndifferent method proposed by Daugman.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 02:32:18 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Kwon", "Song-Hwa", ""], ["Choi", "Hyeong In", ""], ["Lee", "Sung Jin", ""], ["Wee", "Nam-Sook", ""]]}, {"id": "1802.02237", "submitter": "Joshua Rehak", "authors": "J. S. Rehak, L. M. Kerby, M. D. DeHart, R. N. Slaybaugh", "title": "Weighted Delta-Tracking in Scattering Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph physics.ins-det stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we expand the weighted delta-tracking routine to include a\ntreatment for scattering. The weighted delta-tracking routine adds survival\nbiasing to normal delta-tracking, improving problem figure of merit. In the\noriginal formulation of this method, only absorption events were considered. We\nhave expanded the method to include scattering and investigated the method's\neffectiveness with two test cases: a pressurized water reactor pin cell and a\nfast reactor pin cell. We compare the figure of merit for calculating infinite\nflux and total cross-section while incrementally changing the amount of\nweighted delta-tracking used. We find that this new WDT routine has strong\npotential to improve the efficiency of fast reactor calculations, and may be\nuseful for light water reactor calculations.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 22:32:10 GMT"}, {"version": "v2", "created": "Thu, 13 Sep 2018 17:02:46 GMT"}, {"version": "v3", "created": "Wed, 5 Dec 2018 17:55:52 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Rehak", "J. S.", ""], ["Kerby", "L. M.", ""], ["DeHart", "M. D.", ""], ["Slaybaugh", "R. N.", ""]]}, {"id": "1802.02286", "submitter": "Meng-Ta Chung", "authors": "Mengta Chung and Matthew S. Johnson", "title": "An MCMC Algorithm for Estimating the Q-matrix in a Bayesian Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this research is to develop an MCMC algorithm for estimating\nthe Q-matrix. Based on the DINA model, the algorithm starts with estimating\ncorrelated attributes. Using a saturated model and a binary decimal conversion,\nthe algorithm transforms possible attribute patterns to a Multinomial\ndistribution. Along with the likelihood of an attribute pattern, a Dirichlet\ndistribution, constructed using Gamma distributions, is used as the prior to\nsample from the posterior. Correlated attributes of examinees are generated\nusing inverse transform sampling. Closed form posteriors for sampling guess and\nslip parameters are found. A distribution for sampling the Q-matrix is derived.\nA relabeling algorithm that accounts for potential label switching is\npresented. A method for simulating data with correlated attributes for the DINA\nmodel is offered. Three simulation studies are conducted to evaluate the\nperformance of the algorithm. An empirical study using the ECPE data is\nperformed. The algorithm is implemented using customized R codes.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 02:14:10 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Chung", "Mengta", ""], ["Johnson", "Matthew S.", ""]]}, {"id": "1802.02299", "submitter": "Rico Krueger", "authors": "Akshay Vij and Rico Krueger", "title": "Random taste heterogeneity in discrete choice models: Flexible\n  nonparametric finite mixture distributions", "comments": null, "journal-ref": "Transportation Research Part B: Methodological, Volume 106,\n  December 2017, Pages 76-101", "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study proposes a mixed logit model with multivariate nonparametric\nfinite mixture distributions. The support of the distribution is specified as a\nhigh-dimensional grid over the coefficient space, with equal or unequal\nintervals between successive points along the same dimension; the location of\neach point on the grid and the probability mass at that point are model\nparameters that need to be estimated. The framework does not require the\nanalyst to specify the shape of the distribution prior to model estimation, but\ncan approximate any multivariate probability distribution function to any\narbitrary degree of accuracy. The grid with unequal intervals, in particular,\noffers greater flexibility than existing multivariate nonparametric\nspecifications, while requiring the estimation of a small number of additional\nparameters. An expectation maximization algorithm is developed for the\nestimation of these models. Multiple synthetic datasets and a case study on\ntravel mode choice behavior are used to demonstrate the value of the model\nframework and estimation algorithm. Compared to extant models that incorporate\nrandom taste heterogeneity through continuous mixture distributions, the\nproposed model provides better out-of-sample predictive ability. Findings\nreveal significant differences in willingness to pay measures between the\nproposed model and extant specifications. The case study further demonstrates\nthe ability of the proposed model to endogenously recover patterns of attribute\nnon-attendance and choice set formation.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 03:54:04 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Vij", "Akshay", ""], ["Krueger", "Rico", ""]]}, {"id": "1802.02423", "submitter": "Ethan C Jackson", "authors": "Ethan C. Jackson, James Alexander Hughes, Mark Daley", "title": "On the Generalizability of Linear and Non-Linear Region of\n  Interest-Based Multivariate Regression Models for fMRI Data", "comments": "Pre-print of paper submitted for review to 2018 IEEE CIBCB", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In contrast to conventional, univariate analysis, various types of\nmultivariate analysis have been applied to functional magnetic resonance\nimaging (fMRI) data. In this paper, we compare two contemporary approaches for\nmultivariate regression on task-based fMRI data: linear regression with ridge\nregularization and non-linear symbolic regression using genetic programming.\nThe data for this project is representative of a contemporary fMRI experimental\ndesign for visual stimuli. Linear and non-linear models were generated for 10\nsubjects, with another 4 withheld for validation. Model quality is evaluated by\ncomparing $R$ scores (Pearson product-moment correlation) in various contexts,\nincluding single run self-fit, within-subject generalization, and\nbetween-subject generalization. Propensity for modelling strategies to overfit\nis estimated using a separate resting state scan. Results suggest that neither\nmethod is objectively or inherently better than the other.\n", "versions": [{"version": "v1", "created": "Sat, 3 Feb 2018 17:39:17 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Jackson", "Ethan C.", ""], ["Hughes", "James Alexander", ""], ["Daley", "Mark", ""]]}, {"id": "1802.02558", "submitter": "Richard Zhao", "authors": "Lucy Xia, Richard Zhao, Yanhui Wu and Xin Tong", "title": "Intentional Control of Type I Error over Unconscious Data Distortion: a\n  Neyman-Pearson Approach to Text Classification", "comments": null, "journal-ref": "Journal of the American Statistical Association, 2020", "doi": "10.1080/01621459.2020.1740711", "report-no": null, "categories": "stat.ME cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the challenges in classifying textual data obtained from\nopen online platforms, which are vulnerable to distortion. Most existing\nclassification methods minimize the overall classification error and may yield\nan undesirably large type I error (relevant textual messages are classified as\nirrelevant), particularly when available data exhibit an asymmetry between\nrelevant and irrelevant information. Data distortion exacerbates this situation\nand often leads to fallacious prediction. To deal with inestimable data\ndistortion, we propose the use of the Neyman-Pearson (NP) classification\nparadigm, which minimizes type II error under a user-specified type I error\nconstraint. Theoretically, we show that the NP oracle is unaffected by data\ndistortion when the class conditional distributions remain the same.\nEmpirically, we study a case of classifying posts about worker strikes obtained\nfrom a leading Chinese microblogging platform, which are frequently prone to\nextensive, unpredictable and inestimable censorship. We demonstrate that, even\nthough the training and test data are susceptible to different distortion and\ntherefore potentially follow different distributions, our proposed NP methods\ncontrol the type I error on test data at the targeted level. The methods and\nimplementation pipeline proposed in our case study are applicable to many other\nproblems involving data distortion.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 18:33:20 GMT"}, {"version": "v2", "created": "Sun, 3 Jun 2018 19:21:05 GMT"}, {"version": "v3", "created": "Wed, 16 Sep 2020 03:49:46 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Xia", "Lucy", ""], ["Zhao", "Richard", ""], ["Wu", "Yanhui", ""], ["Tong", "Xin", ""]]}, {"id": "1802.02730", "submitter": "Guillaume Bouleux", "authors": "Ma\\\"el Dugast, Guillaume Bouleux and Eric Marcon", "title": "Representation and Characterization of Non-Stationary Processes by\n  Dilation Operators and Induced Shape Space Manifolds", "comments": "19 pages, draft paper", "journal-ref": null, "doi": "10.3390/e20090717", "report-no": null, "categories": "math.MG math.DG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have introduce a new vision of stochastic processes through the geometry\ninduced by the dilation. The dilation matrices of a given processes are\nobtained by a composition of rotations matrices, contain the measure\ninformation in a condensed way. Particularly interesting is the fact that the\nobtention of dilation matrices is regardless of the stationarity of the\nunderlying process. When the process is stationary, it coincides with the\nNaimark Dilation and only one rotation matrix is computed, when the process is\nnon-stationary, a set of rotation matrices are computed. In particular, the\nperiodicity of the correlation function that may appear in some classes of\nsignal is transmitted to the set of dilation matrices. These rotation matrices,\nwhich can be arbitrarily close to each other depending on the sampling or the\nrescaling of the signal are seen as a distinctive feature of the signal. In\norder to study this sequence of matrices, and guided by the possibility to\nrescale the signal, the correct geometrical framework to use with the\ndilation's theoretic results is the space of curves on manifolds, that is the\nset of all curve that lies on a base manifold. To give a complete sight about\nthe space of curve, a metric and the derived geodesic equation are provided.\nThe general results are adapted to the more specific case where the base\nmanifold is the Lie group of rotation matrices. The notion of the shape of a\ncurve can be formalized as the set of equivalence classes of curves given by\nthe quotient space of the space of curves and the increasing diffeomorphisms.\nThe metric in the space of curve naturally extent to the space of shapes and\nenable comparison between shapes.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 07:18:50 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Dugast", "Ma\u00ebl", ""], ["Bouleux", "Guillaume", ""], ["Marcon", "Eric", ""]]}, {"id": "1802.02896", "submitter": "Nesreen Ahmed", "authors": "Nesreen K. Ahmed, Ryan Rossi, John Boaz Lee, Theodore L. Willke, Rong\n  Zhou, Xiangnan Kong, Hoda Eldardiry", "title": "Learning Role-based Graph Embeddings", "comments": "StarAI workshop @ IJCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random walks are at the heart of many existing network embedding methods.\nHowever, such algorithms have many limitations that arise from the use of\nrandom walks, e.g., the features resulting from these methods are unable to\ntransfer to new nodes and graphs as they are tied to vertex identity. In this\nwork, we introduce the Role2Vec framework which uses the flexible notion of\nattributed random walks, and serves as a basis for generalizing existing\nmethods such as DeepWalk, node2vec, and many others that leverage random walks.\nOur proposed framework enables these methods to be more widely applicable for\nboth transductive and inductive learning as well as for use on graphs with\nattributes (if available). This is achieved by learning functions that\ngeneralize to new nodes and graphs. We show that our proposed framework is\neffective with an average AUC improvement of 16.55% while requiring on average\n853x less space than existing methods on a variety of graphs.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 00:29:44 GMT"}, {"version": "v2", "created": "Mon, 2 Jul 2018 23:36:25 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Ahmed", "Nesreen K.", ""], ["Rossi", "Ryan", ""], ["Lee", "John Boaz", ""], ["Willke", "Theodore L.", ""], ["Zhou", "Rong", ""], ["Kong", "Xiangnan", ""], ["Eldardiry", "Hoda", ""]]}, {"id": "1802.02972", "submitter": "Rui Marcelino", "authors": "Rui Marcelino, Bruno Natale Pasquarelli and Jaime Sampaio", "title": "Infer\\^encia Baseada em Magnitudes na investiga\\c{c}\\~ao em Ci\\^encias\n  do Esporte. A necessidade de romper com os testes de hip\\'otese nula e os\n  valores de p", "comments": "in Portuguese", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in Sports Sciences is supported often by inferences based on the\ndeclaration of the value of the statistic statistically significant or\nnonsignificant on the bases of a P value derived from a null-hypothesis test.\nTaking into account that studies are manly conducted in sample, the use of null\nhypothesis testing only allows estimating the true values (population) of the\nstatistics used. However, evidence has grown in many areas of knowledge that\nthis approach often leads to confusion and misinterpretation. To overcome this\nlimitation they have recently emerged recommendations to support the\nstatistical analysis with approaches that make use of more intuitive\ninterpretations and more practical, especially based on the magnitudes\n(certainty / uncertainty) of the true values found. With the intent to provide\nalternative solutions to methodological designs recurrently used in research in\nsports sciences, this paper will seek to i) briefly spell out some of the\nweaknesses associated with the null hypothesis tests based in the P value; ii)\nreflect on the implications of the use of practical/clinical significance as\nopposed to statistical significance; iii) submit proposals for use the\ninferences based on the magnitude, particularly in the visualization and\ninterpretation of results; iv) present and discuss the limitations of\nmagnitude-based inference. Thus, this update article discourages, in a\nsustained-based, the use of significance tests based only on the concept of\nnull hypothesis. Alternatively, it is proposed to use methods of inference\nbased on magnitudes as they allow interpretations of the practical/clinical\neffects results obtained. ----- As investiga\\c{c}\\~oes em ci\\^encias do esporte\nsustentam-se frequentemente em infer\\^encias baseadas na declara\\c{c}\\~ao de um\nvalor estatisticamente significativo, ou n\\~ao significativo, com base no valor\nde p que deriva dos testes de hip\\'otese nula. Considerando que os estudos\ns\\~ao iminentemente amostrais, o recurso aos testes de hip\\'otese nula apenas\npossibilita estimar os valores verdadeiros (popula\\c{c}\\~ao) das estat\\'isticas\nutilizadas. Contudo, tem crescido a evid\\^encia, em diversas \\'areas do\nconhecimento, de que esta abordagem origina frequentemente interpreta\\c{c}\\~oes\nconfusas e at\\'e erradas (7). Para ultrapassar esta limita\\c{c}\\~ao t\\^em\nsurgido recentemente recomenda\\c{c}\\~oes no sentido de sustentar as an\\'alises\nestat\\'isticas com abordagens que recorram a interpreta\\c{c}\\~oes mais\nintuitivas e mais pr\\'aticas, baseadas sobretudo nas magnitudes\n(certezas/incertezas) dos valores verdadeiros encontrados. Com o intento de\nfornecer pistas alternativas aos desenhos metodol\\'ogicos recorrentemente\nutilizados na investiga\\c{c}\\~ao em ci\\^encias do esporte, neste trabalho\nprocuraremos i) enunciar sucintamente algumas das fragilidades associadas aos\ntestes de hip\\'otese nula sustentados no valor de p; ii) refletir sobre as\nimplica\\c{c}\\~oes da utiliza\\c{c}\\~ao da signific\\^ancia pr\\'atica/clinica em\noposi\\c{c}\\~ao \\`a signific\\^ancia estat\\'istica; iii) apresentar propostas de\nutiliza\\c{c}\\~ao das t\\'ecnicas de infer\\^encias baseadas na magnitude,\nparticularmente na visualiza\\c{c}\\~ao e interpreta\\c{c}\\~ao dos resultados; iv)\napresentar as principais limita\\c{c}\\~oes do uso das infer\\^encias baseadas em\nmagnitudes. Assim, neste artigo de atualiza\\c{c}\\~ao desencoraja-se, de forma\nsustentada e fundamentada, o uso dos testes de signific\\^ancia baseados apenas\nno conceito de hip\\'otese nula. Em alternativa, prop\\~oe-se a utiliza\\c{c}\\~ao\nde m\\'etodos de infer\\^encias baseados em magnitudes por possibilitarem\ninterpreta\\c{c}\\~oes dos efeitos pr\\'aticos/cl\\'inicos dos resultados obtidos.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 09:14:27 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Marcelino", "Rui", ""], ["Pasquarelli", "Bruno Natale", ""], ["Sampaio", "Jaime", ""]]}, {"id": "1802.03077", "submitter": "Nancy Murray", "authors": "Nancy Murray, Howard H. Chang, Heather Holmes, and Yang Liu", "title": "Combining Satellite Imagery and Numerical Model Simulation to Estimate\n  Ambient Air Pollution: An Ensemble Averaging Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ambient fine particulate matter less than 2.5 $\\mu$m in aerodynamic diameter\n(PM$_{2.5}$) has been linked to various adverse health outcomes and has,\ntherefore, gained interest in public health. However, the sparsity of air\nquality monitors greatly restricts the spatio-temporal coverage of PM$_{2.5}$\nmeasurements, limiting the accuracy of PM$_{2.5}$-related health studies. We\ndevelop a method to combine estimates for PM$_{2.5}$ using satellite-retrieved\naerosol optical depth (AOD) and simulations from the Community Multiscale Air\nQuality (CMAQ) modeling system. While most previous methods utilize AOD or CMAQ\nseparately, we aim to leverage advantages offered by both methods in terms of\nresolution and coverage by using Bayesian model averaging. In an application of\nestimating daily PM$_{2.5}$ in the Southeastern US, the ensemble approach\noutperforms statistical downscalers that use either AOD or CMAQ in\ncross-validation analyses. In addition to PM$_{2.5}$, our approach is also\nhighly applicable for estimating other environmental risks that utilize\ninformation from both satellite imagery and numerical model simulation.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 23:22:30 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Murray", "Nancy", ""], ["Chang", "Howard H.", ""], ["Holmes", "Heather", ""], ["Liu", "Yang", ""]]}, {"id": "1802.03140", "submitter": "Geoff Boeing", "authors": "Jesus M. Barajas, Geoff Boeing, Julie Wartell", "title": "Neighborhood Change, One Pint at a Time: The Impact of Local\n  Characteristics on Craft Breweries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cities have recognized the local impact of small craft breweries, in many\nways altering municipal codes to make it easier to establish breweries and\nmaking them the anchor points of economic development and revitalization.\nNevertheless, we do not know the extent to which these strategies impacted\nchanges at the neighborhood level across the nation. In this chapter, we\nexamine the relationship between growth and locations of craft breweries and\nthe incidence of neighborhood change across the United States. In the first\npart of the chapter, we rely on a unique dataset of geocoded brewery locations\nthat tracks openings and closings from 2004 to the present. Using measures of\nneighborhood change often found in literature on gentrification-related topics,\nwe develop statistical models relying on census tract demographic and\nemployment data to determine the extent to which brewery locations are\nassociated with social and demographic shifts since 2000. The strongest\npredictor of whether a craft brewery opened in 2013 or later in a neighborhood\nwas the presence of a prior brewery. We do not find evidence entirely\nconsistent with the common narrative of a link between gentrification and craft\nbrewing, but we see a link between an influx of lower-to-middle income urban\ncreatives and the introduction of a craft breweries. We advocate for urban\nplanners to recognize the importance of craft breweries in neighborhood\nrevitalization while also protecting residents from potential displacement.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 06:17:57 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Barajas", "Jesus M.", ""], ["Boeing", "Geoff", ""], ["Wartell", "Julie", ""]]}, {"id": "1802.03242", "submitter": "Jason Hilton", "authors": "Jason Hilton, Erengul Dodd, Jonathan J. Forster, Peter W. F. Smith", "title": "Projecting UK Mortality using Bayesian Generalised Additive Models", "comments": "24 pages (including appendix), 14 figures", "journal-ref": null, "doi": "10.1111/rssc.12299", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasts of mortality provide vital information about future populations,\nwith implications for pension and health-care policy as well as for decisions\nmade by private companies about life insurance and annuity pricing. Stochastic\nmortality forecasts allow the uncertainty in mortality predictions to be taken\ninto consideration when making policy decisions and setting product prices.\nLonger lifespans imply that forecasts of mortality at ages 90 and above will\nbecome more important in such calculations.\n  This paper presents a Bayesian approach to the forecasting of mortality that\njointly estimates a Generalised Additive Model (GAM) for mortality for the\nmajority of the age-range and a parametric model for older ages where the data\nare sparser. The GAM allows smooth components to be estimated for age, cohort\nand age-specific improvement rates, together with a non-smoothed period effect.\nForecasts for the United Kingdom are produced using data from the Human\nMortality Database spanning the period 1961-2013. A metric that approximates\npredictive accuracy under Leave-One-Out cross-validation is used to estimate\nweights for the `stacking' of forecasts with different points of transition\nbetween the GAM and parametric elements.\n  Mortality for males and females are estimated separately at first, but a\njoint model allows the asymptotic limit of mortality at old ages to be shared\nbetween sexes, and furthermore provides for forecasts accounting for\ncorrelations in period innovations. The joint and single sex model forecasts\nestimated using data from 1961-2003 are compared against observed data from\n2004-2013 to facilitate model assessment.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 13:05:21 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Hilton", "Jason", ""], ["Dodd", "Erengul", ""], ["Forster", "Jonathan J.", ""], ["Smith", "Peter W. F.", ""]]}, {"id": "1802.03341", "submitter": "David Ellenberger", "authors": "Michael Siebert and David Ellenberger", "title": "Automatic Passenger Counting: Introducing the t-Test Induced Equivalence\n  Test", "comments": "15 pages, 2 figures. Transportation (2019)", "journal-ref": null, "doi": "10.1007/s11116-019-09991-9", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic passenger counting (APC) in public transport has been introduced in\nthe 1970s and has been rapidly emerging in recent years. Still, real-world\napplications continue to face events that are difficult to classify. The\ninduced imprecision needs to be handled as statistical noise and thus methods\nhave been defined to ensure that measurement errors do not exceed certain\nbounds. Various recommendations for such an APC validation have been made to\nestablish criteria that limit the bias and the variability of the measurement\nerrors. In those works, the misinterpretation of non-significance in\nstatistical hypothesis tests for the detection of differences (e.g. Student's\nt-test) proves to be prevalent, although existing methods which were developed\nunder the term equivalence testing in biostatistics (i.e. bioequivalence\ntrials, Schuirmann in J Pharmacokinet Pharmacodyn 15(6):657-680, 1987) would be\nappropriate instead. This heavily affects the calibration and validation\nprocess of APC systems and has been the reason for unexpected results when the\nsample sizes were not suitably chosen: Large sample sizes were assumed to\nimprove the assessment of systematic measurement errors of the devices from a\nuser's perspective as well as from a manufacturer's perspective, but the\nregular t-test fails to achieve that. We introduce a variant of the t-test, the\nrevised t-test, which addresses both type I and type II errors appropriately\nand allows a comprehensible transition from the long-established t-test in a\nwidely used industrial recommendation. This test is appealing, but still it is\nsusceptible to numerical instability. Finally, we analytically reformulate it\nas a numerically stable equivalence test, which is thus easier to use. Our\nresults therefore allow to induce an equivalence test from a t-test and\nincrease the comparability of both tests, especially for decision makers.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 16:47:33 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 15:34:59 GMT"}, {"version": "v3", "created": "Tue, 13 Feb 2018 23:02:57 GMT"}, {"version": "v4", "created": "Fri, 13 Apr 2018 13:16:23 GMT"}, {"version": "v5", "created": "Wed, 19 Jun 2019 13:03:09 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Siebert", "Michael", ""], ["Ellenberger", "David", ""]]}, {"id": "1802.03343", "submitter": "Alessandra Pasquini", "authors": "Alessandra Pasquini, Marco Centra, Guido Pellegrini (MEMOTEF\n  Department Sapienza University of Rome, INAPP, Scienze Sociali ed Eonomiche\n  Department Sapienza University of Rome)", "title": "Long-Term Unemployed hirings: Should targeted or untargeted policies be\n  preferred?", "comments": "30 pages including the appendix, 6 figures, 14 tables, previous\n  versions presented at IWCEE2017 and Astril 2017 Conferences", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To what extent, hiring incentives targeting a specific group of vulnerable\nunemployed (i.e. long term unemployed) are more effective, with respect to\ngeneralised incentives (without a definite target), to increase hirings of the\ntargeted group? Are generalized incentives able to influence hirings of the\nvulnerable group? Do targeted policies have negative side effects too important\nto accept them? Even though there is a huge literature on hiring subsidies,\nthese questions remained unresolved. We tried to answer them, comparing the\nimpact of two similar hiring policies, one oriented towards a target group and\none generalised, implemented on the italian labour market. We used\nadministrative data on job contracts, and counterfactual analysis methods. The\ntargeted policy had a positive and significant impact, while the generalized\npolicy didn't have a significant impact on the vulnerable group. Moreover, we\nconcluded the targeted policy didn't have any indirect negative side effect.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 16:48:15 GMT"}, {"version": "v2", "created": "Wed, 2 May 2018 09:25:57 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Pasquini", "Alessandra", "", "MEMOTEF\n  Department Sapienza University of Rome, INAPP, Scienze Sociali ed Eonomiche\n  Department Sapienza University of Rome"], ["Centra", "Marco", "", "MEMOTEF\n  Department Sapienza University of Rome, INAPP, Scienze Sociali ed Eonomiche\n  Department Sapienza University of Rome"], ["Pellegrini", "Guido", "", "MEMOTEF\n  Department Sapienza University of Rome, INAPP, Scienze Sociali ed Eonomiche\n  Department Sapienza University of Rome"]]}, {"id": "1802.03503", "submitter": "Xing He", "authors": "Zenan Ling, Robert C. Qiu, Xing He, Lei Chu", "title": "A New Approach of Exploiting Self-Adjoint Matrix Polynomials of Large\n  Random Matrices for Anomaly Detection and Fault Location", "comments": "12 pages, 13 figures, submitted to IEEE Trans on Big Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synchronized measurements of a large power grid enable an unprecedented\nopportunity to study the spatialtemporal correlations. Statistical analytics\nfor those massive datasets start with high-dimensional data matrices.\nUncertainty is ubiquitous in a future's power grid. These data matrices are\nrecognized as random matrices. This new point of view is fundamental in our\ntheoretical analysis since true covariance matrices cannot be estimated\naccurately in a high-dimensional regime. As an alternative, we consider\nlarge-dimensional sample covariance matrices in the asymptotic regime to\nreplace the true covariance matrices. The self-adjoint polynomials of\nlarge-dimensional random matrices are studied as statistics for big data\nanalytics. The calculation of the asymptotic spectrum distribution (ASD) for\nsuch a matrix polynomial is understandably challenging. This task is made\npossible by a recent breakthrough in free probability, an active research\nbranch in random matrix theory. This is the very reason why the work of this\npaper is inspired initially. The new approach is interesting in many aspects.\nThe mathematical reason may be most critical. The real-world problems can be\nsolved using this approach, however.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 02:27:09 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Ling", "Zenan", ""], ["Qiu", "Robert C.", ""], ["He", "Xing", ""], ["Chu", "Lei", ""]]}, {"id": "1802.03529", "submitter": "Feihu Xu", "authors": "Feihu Xu, Gal Shulkind, Christos Thrampoulidis, Jeffrey H. Shapiro,\n  Antonio Torralba, Franco N. C. Wong, Gregory W. Wornell", "title": "Revealing hidden scenes by photon-efficient occlusion-based\n  opportunistic active imaging", "comments": "Related theory in arXiv:1711.06297", "journal-ref": "Optics Express 26, 9945 (2018)", "doi": "10.1364/OE.26.009945", "report-no": null, "categories": "stat.AP physics.optics quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to see around corners, i.e., recover details of a hidden scene\nfrom its reflections in the surrounding environment, is of considerable\ninterest in a wide range of applications. However, the diffuse nature of light\nreflected from typical surfaces leads to mixing of spatial information in the\ncollected light, precluding useful scene reconstruction. Here, we employ a\ncomputational imaging technique that opportunistically exploits the presence of\noccluding objects, which obstruct probe-light propagation in the hidden scene,\nto undo the mixing and greatly improve scene recovery. Importantly, our\ntechnique obviates the need for the ultrafast time-of-flight measurements\nemployed by most previous approaches to hidden-scene imaging. Moreover, it does\nso in a photon-efficient manner based on an accurate forward model and a\ncomputational algorithm that, together, respect the physics of three-bounce\nlight propagation and single-photon detection. Using our methodology, we\ndemonstrate reconstruction of hidden-surface reflectivity patterns in a\nmeter-scale environment from non-time-resolved measurements. Ultimately, our\ntechnique represents an instance of a rich and promising new imaging modality\nwith important potential implications for imaging science.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 06:17:28 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Xu", "Feihu", ""], ["Shulkind", "Gal", ""], ["Thrampoulidis", "Christos", ""], ["Shapiro", "Jeffrey H.", ""], ["Torralba", "Antonio", ""], ["Wong", "Franco N. C.", ""], ["Wornell", "Gregory W.", ""]]}, {"id": "1802.03708", "submitter": "Yubo Tao", "authors": "Li Guo, Wolfgang Karl H\\\"ardle, Yubo Tao", "title": "A Time-Varying Network for Cryptocurrencies", "comments": "43 pages, 6 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM q-fin.PM q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cryptocurrencies return cross-predictability and technological similarity\nyield information on risk propagation and market segmentation. To investigate\nthese effects, we build a time-varying network for cryptocurrencies, based on\nthe evolution of return cross-predictability and technological similarities. We\ndevelop a dynamic covariate-assisted spectral clustering method to consistently\nestimate the latent community structure of cryptocurrencies network that\naccounts for both sets of information. We demonstrate that investors can\nachieve better risk diversification by investing in cryptocurrencies from\ndifferent communities. A cross-sectional portfolio that implements an\ninter-crypto momentum trading strategy earns a 1.08% daily return. By\ndissecting the portfolio returns on behavioral factors, we confirm that our\nresults are not driven by behavioral mechanisms.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 08:32:31 GMT"}, {"version": "v2", "created": "Mon, 19 Feb 2018 05:42:37 GMT"}, {"version": "v3", "created": "Mon, 18 Jun 2018 15:40:22 GMT"}, {"version": "v4", "created": "Tue, 9 Apr 2019 12:44:59 GMT"}, {"version": "v5", "created": "Fri, 20 Sep 2019 16:04:37 GMT"}, {"version": "v6", "created": "Sat, 13 Jun 2020 05:09:17 GMT"}, {"version": "v7", "created": "Wed, 21 Jul 2021 12:57:27 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Guo", "Li", ""], ["H\u00e4rdle", "Wolfgang Karl", ""], ["Tao", "Yubo", ""]]}, {"id": "1802.03778", "submitter": "Michelle Norris", "authors": "Michelle Norris", "title": "Sample Design for Audit Populations", "comments": "44 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop several tools for the determination of sample size and design for\nMediCal audits. This audit setting involves a population of claims for\nreimbursement by a healthcare provider which need to be reviewed by an auditor\nto determine the correct amount for each claim. The existing literature\nregarding sample planning for audits is incomplete and often includes\nrestrictive assumptions. To fill these gaps, we exploit the special\nrelationship between the known claim amounts and the unknown post-audit\namounts. We propose a hypergeometric generative process for audit populations\nwhich we use to derive estimators of variances needed for sample size\ndetermination. We further develop a criterion for choosing between simple\nexpansion and ratio estimation and an efficient method for determining exact\noptimal strata breakpoints in populations with repeated values. We also derive\na variance estimator under a more general \"partial error\" model than previous\nresearchers have used. These tools apply more generally to audits where an\noverstated book/claim amount is the primary concern and estimation of the total\ndollar value of the claim errors is the goal. The sample design methods we\ndevelop are illustrated on two simulated audit populations.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 17:43:04 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Norris", "Michelle", ""]]}, {"id": "1802.03839", "submitter": "Casey Kneale", "authors": "Casey Kneale, Steven D. Brown", "title": "Band Target Entropy Minimization and Target Partial Least Squares for\n  Spectral Recovery and Calibration", "comments": null, "journal-ref": null, "doi": "10.1016/j.aca.2018.07.054", "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The resolution and calibration of pure spectra of minority components in\nmeasurements of chemical mixtures without prior knowledge of the mixture is a\nchallenging problem. In this work, a combination of band target entropy\nminimization (BTEM) and target partial least squares (T-PLS) was used to obtain\nestimates for single pure component spectra and to calibrate those estimates in\na true, one-at-a-time fashion. This approach allows for minor components to be\ntargeted and their relative amounts estimated in the presence of other varying\ncomponents in spectral data. The use of T-PLS estimation is an improvement to\nthe BTEM method because it overcomes the need to identify all of the pure\ncomponents prior to estimation. Estimated amounts from this combination were\nfound to be similar to those obtained from a standard method, multivariate\ncurve resolution-alternating least squares (MCR-ALS), on a simple, three\ncomponent mixture dataset. Studies from two experimental datasets demonstrate\nwhere the combination of BTEM and T-PLS could model the pure component spectra\nand obtain concentration profiles of minor components but MCR-ALS could not.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 23:21:38 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2018 16:53:41 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Kneale", "Casey", ""], ["Brown", "Steven D.", ""]]}, {"id": "1802.04170", "submitter": "Simon Olofsson", "authors": "Simon Olofsson, Marc Peter Deisenroth, Ruth Misener", "title": "Design of Experiments for Model Discrimination Hybridising Analytical\n  and Data-Driven Approaches", "comments": null, "journal-ref": "Proc.Mach.Learn.Res. 80 (2018) pp. 3908-3917", "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Healthcare companies must submit pharmaceutical drugs or medical devices to\nregulatory bodies before marketing new technology. Regulatory bodies frequently\nrequire transparent and interpretable computational modelling to justify a new\nhealthcare technology, but researchers may have several competing models for a\nbiological system and too little data to discriminate between the models. In\ndesign of experiments for model discrimination, the goal is to design maximally\ninformative physical experiments in order to discriminate between rival\npredictive models. Prior work has focused either on analytical approaches,\nwhich cannot manage all functions, or on data-driven approaches, which may have\ncomputational difficulties or lack interpretable marginal predictive\ndistributions. We develop a methodology introducing Gaussian process surrogates\nin lieu of the original mechanistic models. We thereby extend existing design\nand model discrimination methods developed for analytical models to cases of\nnon-analytical models in a computationally efficient manner.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 16:34:06 GMT"}, {"version": "v2", "created": "Thu, 31 May 2018 07:39:19 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Olofsson", "Simon", ""], ["Deisenroth", "Marc Peter", ""], ["Misener", "Ruth", ""]]}, {"id": "1802.04233", "submitter": "Diego Mesa", "authors": "Jacek M. Bajor, Diego A. Mesa, Travis J. Osterman, Thomas A. Lasko", "title": "Embedding Complexity In the Data Representation Instead of In the Model:\n  A Case Study Using Heterogeneous Medical Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic Health Records have become popular sources of data for secondary\nresearch, but their use is hampered by the amount of effort it takes to\novercome the sparsity, irregularity, and noise that they contain. Modern\nlearning architectures can remove the need for expert-driven feature\nengineering, but not the need for expert-driven preprocessing to abstract away\nthe inherent messiness of clinical data. This preprocessing effort is often the\ndominant component of a typical clinical prediction project. In this work we\npropose using semantic embedding methods to directly couple the raw, messy\nclinical data to downstream learning architectures with truly minimal\npreprocessing. We examine this step from the perspective of capturing and\nencoding complex data dependencies in the data representation instead of in the\nmodel, which has the nice benefit of allowing downstream processing to be done\nwith fast, lightweight, and simple models accessible to researchers without\nmachine learning expertise. We demonstrate with three typical clinical\nprediction tasks that the highly compressed, embedded data representations\ncapture a large amount of useful complexity, although in some cases the\ncompression is not completely lossless.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 18:31:24 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Bajor", "Jacek M.", ""], ["Mesa", "Diego A.", ""], ["Osterman", "Travis J.", ""], ["Lasko", "Thomas A.", ""]]}, {"id": "1802.04353", "submitter": "Yu Jin", "authors": "Yu Jin, Joseph F. JaJa, Rong Chen, Edward H. Herskovits", "title": "A Data-Driven Approach to Extract Connectivity Structures from Diffusion\n  Tensor Imaging Data", "comments": "Proceedings of 2015 IEEE International Conference on Big Data", "journal-ref": null, "doi": "10.1109/BigData.2015.7363843", "report-no": null, "categories": "cs.CE q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion Tensor Imaging (DTI) is an effective tool for the analysis of\nstructural brain connectivity in normal development and in a broad range of\nbrain disorders. However efforts to derive inherent characteristics of\nstructural brain networks have been hampered by the very high dimensionality of\nthe data, relatively small sample sizes, and the lack of widely acceptable\nconnectivity-based regions of interests (ROIs). Typical approaches have focused\neither on regions defined by standard anatomical atlases that do not\nincorporate anatomical connectivity, or have been based on voxel-wise analysis,\nwhich results in loss of statistical power relative to structure-wise\nconnectivity analysis. In this work, we propose a novel, computationally\nefficient iterative clustering method to generate connectivity-based\nwhole-brain parcellations that converge to a stable parcellation in a few\niterations. Our algorithm is based on a sparse representation of the whole\nbrain connectivity matrix, which reduces the number of edges from around a half\nbillion to a few million while incorporating the necessary spatial constraints.\nWe show that the resulting regions in a sense capture the inherent connectivity\ninformation present in the data, and are stable with respect to initialization\nand the randomization scheme within the algorithm. These parcellations provide\nconsistent structural regions across the subjects of population samples that\nare homogeneous with respect to anatomic connectivity. Our method also derives\nconnectivity structures that can be used to distinguish between population\nsamples with known different structural connectivity. In particular, new\nresults in structural differences for different population samples such as\nFemales vs Males, Normal Controls vs Schizophrenia, and different age groups in\nNormal Controls are also shown.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 20:42:36 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Jin", "Yu", ""], ["JaJa", "Joseph F.", ""], ["Chen", "Rong", ""], ["Herskovits", "Edward H.", ""]]}, {"id": "1802.04447", "submitter": "Yu Jin", "authors": "Yu Jin, Andreas Loukas, Joseph F. JaJa", "title": "Graph Coarsening with Preserved Spectral Properties", "comments": "Submitted to AISTATS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.NA stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale graphs are widely used to represent object relationships in many\nreal world applications. The occurrence of large-scale graphs presents\nsignificant computational challenges to process, analyze, and extract\ninformation. Graph coarsening techniques are commonly used to reduce the\ncomputational load while attempting to maintain the basic structural properties\nof the original graph. As there is no consensus on the specific graph\nproperties preserved by coarse graphs, how to measure the differences between\noriginal and coarse graphs remains a key challenge. In this work, we introduce\na new perspective regarding the graph coarsening based on concepts from\nspectral graph theory. We propose and justify new distance functions that\ncharacterize the differences between original and coarse graphs. We show that\nthe proposed spectral distance naturally captures the structural differences in\nthe graph coarsening process. In addition, we provide efficient graph\ncoarsening algorithms to generate graphs which provably preserve the spectral\nproperties from original graphs. Experiments show that our proposed algorithms\nconsistently achieve better results compared to previous graph coarsening\nmethods on graph classification and block recovery tasks.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 02:57:59 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 06:48:10 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Jin", "Yu", ""], ["Loukas", "Andreas", ""], ["JaJa", "Joseph F.", ""]]}, {"id": "1802.04466", "submitter": "Michael Wojnowicz", "authors": "Vadim Kotov and Michael Wojnowicz", "title": "Towards Generic Deobfuscation of Windows API Calls", "comments": "To be published in the 2018 Network and Distributed Systems Security\n  (NDSS) Symposium via its 2018 Workshop on Binary Analysis Research (BAR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common way to get insight into a malicious program's functionality is to\nlook at which API functions it calls. To complicate the reverse engineering of\ntheir programs, malware authors deploy API obfuscation techniques, hiding them\nfrom analysts' eyes and anti-malware scanners. This problem can be partially\naddressed by using dynamic analysis; that is, by executing a malware sample in\na controlled environment and logging the API calls. However, malware that is\naware of virtual machines and sandboxes might terminate without showing any\nsigns of malicious behavior. In this paper, we introduce a static analysis\ntechnique allowing generic deobfuscation of Windows API calls. The technique\nutilizes symbolic execution and hidden Markov models to predict API names from\nthe arguments passed to the API functions. Our best prediction model can\ncorrectly identify API names with 87.60% accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 05:48:46 GMT"}, {"version": "v2", "created": "Sat, 5 Dec 2020 16:00:43 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Kotov", "Vadim", ""], ["Wojnowicz", "Michael", ""]]}, {"id": "1802.04664", "submitter": "Lovedeep Gondara", "authors": "Lovedeep Gondara, Ke Wang", "title": "Recovering Loss to Followup Information Using Denoising Autoencoders", "comments": "Copyright IEEE 2017, IEEE International Conference on Big Data (Big\n  Data)", "journal-ref": null, "doi": "10.1109/BigData.2017.8258139", "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Loss to followup is a significant issue in healthcare and has serious\nconsequences for a study's validity and cost. Methods available at present for\nrecovering loss to followup information are restricted by their expressive\ncapabilities and struggle to model highly non-linear relations and complex\ninteractions. In this paper we propose a model based on overcomplete denoising\nautoencoders to recover loss to followup information. Designed to work with\nhigh volume data, results on various simulated and real life datasets show our\nmodel is appropriate under varying dataset and loss to followup conditions and\noutperforms the state-of-the-art methods by a wide margin ($\\ge 20\\%$ in some\nscenarios) while preserving the dataset utility for final analysis.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 03:34:03 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Gondara", "Lovedeep", ""], ["Wang", "Ke", ""]]}, {"id": "1802.04755", "submitter": "Daniel Gervini", "authors": "Daniel Gervini, Manoj Khanal", "title": "Exploring patterns of demand in bike sharing systems via replicated\n  point process models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding patterns of demand is fundamental for fleet management of bike\nsharing systems. In this paper we analyze data from the Divvy system of the\ncity of Chicago. We show that the demand of bicycles can be modeled as a\nmultivariate temporal point process, with each dimension corresponding to a\nbike station in the network. The availability of daily replications of the\nprocess allows nonparametric estimation of the intensity functions, even for\nstations with low daily counts, and straightforward estimation of pairwise\ncorrelations between stations. These correlations are then used for clustering,\nrevealing different patterns of bike usage.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 17:34:26 GMT"}, {"version": "v2", "created": "Fri, 6 Jul 2018 23:12:35 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Gervini", "Daniel", ""], ["Khanal", "Manoj", ""]]}, {"id": "1802.04830", "submitter": "Luca Pappalardo", "authors": "Charlotte James and Luca Pappalardo and Alina Sirbu and Filippo Simini", "title": "Prediction of next career moves from scientific profiles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Changing institution is a scientist's key career decision, which plays an\nimportant role in education, scientific productivity, and the generation of\nscientific knowledge. Yet, our understanding of the factors influencing a\nrelocation decision is very limited. In this paper we investigate how the\nscientific profile of a scientist determines their decision to move (i.e.,\nchange institution). To this aim, we describe a scientist's profile by three\nmain aspects: the scientist's recent scientific career, the quality of their\nscientific environment and the structure of their scientific collaboration\nnetwork. We then design and implement a two-stage predictive model: first, we\nuse data mining to predict which researcher will move in the next year on the\nbasis of their scientific profile; second we predict which institution they\nwill choose by using a novel social-gravity model, an adaptation of the\ntraditional gravity model of human mobility. Experiments on a massive dataset\nof scientific publications show that our approach performs well in both the\nstages, resulting in a 85% reduction of the prediction error with respect to\nthe state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 19:37:17 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["James", "Charlotte", ""], ["Pappalardo", "Luca", ""], ["Sirbu", "Alina", ""], ["Simini", "Filippo", ""]]}, {"id": "1802.04843", "submitter": "Kyongche Kang", "authors": "Kyongche Kang, Jinsub Hong, and Hannah Worrall", "title": "Sources of Variance in Two-Photon Microscopy Neuroimaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-photon laser scanning microscopy is widely used in a quickly growing\nfield of neuroscience. It is a fluorescence imaging technique that allows\nimaging of living tissue up to a very high depth to study inherent brain\nstructure and circuitry. Our project deals with examining images from\ntwo-photon calcium imaging, a brain-imaging technique that allows for study of\nneuronal activity in hundreds of neurons and and. As statisticians, we worked\nto apply various methods to better understand the sources of variations that\nare inherent in neuroimages from this imaging technique that are not part of\nthe controlled experiment. Thus, images can be made available for studying the\neffects of physical stimulation on the working brain. Currently there is no\nsystem to examine and prepare such brain images. Thus we worked to develop\nmethods to work towards this end. Our data set had images of a rat's brain in\ntwo states. In the first state the rat is sedated and merely observed and in\nthe other it is repeatedly simulated via electric shocks. We first started by\ncontrolling for the movement of the brain to more accurately observe the\nphysical characteristics of the brain. We analyzed how the variance of the\nbrain images varied between pre and post stimulus by applying Levene's Test.\nFurthermore, we were able to measure how much the images were shifted to see\nthe overall change in movement of the brain due to electrical stimulus.\nTherefore, we were able to visually observe how the brain structure and\nvariance change due to stimulus effects in rat brains.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 20:29:19 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Kang", "Kyongche", ""], ["Hong", "Jinsub", ""], ["Worrall", "Hannah", ""]]}, {"id": "1802.04888", "submitter": "David Colquhoun FRS", "authors": "David Colquhoun", "title": "The false positive risk: a proposal concerning what to do about p values", "comments": "26 pages, 3 Figures", "journal-ref": "he American Statistician Volume 73, 2019 - Issue sup1: Statistical\n  Inference in the 21st Century: A World Beyond p < 0.05", "doi": "10.1080/00031305.2018.1529622", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  It is widely acknowledged that the biomedical literature suffer from a\nsurfeit of false positive results. Part of the reason for this is the\npersistence of the myth that observation of a p value less than 0.05 is\nsufficient justification to claim that you've made a discovery.\n  It is hopeless to expect users to change their reliance on p values unless\nthey are offered an alternative way of judging the reliability of their\nconclusions. If the alternative method is to have a chance of being adopted\nwidely, it will have to be easy to understand and to calculate. One such\nproposal is based on calculation of false positive risk.\n  It is suggested that p values and confidence intervals should continue to be\ngiven, but that they should be supplemented by a single additional number that\nconveys the strength of the evidence better than the p value. This number could\nbe the minimum false positive risk (that calculated on the assumption of a\nprior probability of 0.5, the largest value that can be assumed in the absence\nof hard prior data). Alternatively one could specify the prior probability that\nit would be necessary to believe in order to achieve a false positive risk of,\nsay, 0.05.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 22:58:12 GMT"}, {"version": "v2", "created": "Mon, 30 Apr 2018 23:12:27 GMT"}, {"version": "v3", "created": "Tue, 10 Jul 2018 10:55:29 GMT"}, {"version": "v4", "created": "Sun, 22 Jul 2018 11:01:46 GMT"}, {"version": "v5", "created": "Wed, 3 Oct 2018 10:19:53 GMT"}, {"version": "v6", "created": "Thu, 10 Jan 2019 17:25:04 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Colquhoun", "David", ""]]}, {"id": "1802.04987", "submitter": "Luca Pappalardo", "authors": "Luca Pappalardo and Paolo Cintia and Paolo Ferragina and Emanuele\n  Massucco and Dino Pedreschi and Fosca Giannotti", "title": "PlayeRank: data-driven performance evaluation and player ranking in\n  soccer via a machine learning approach", "comments": null, "journal-ref": "PlayeRank: Data-driven Performance Evaluation and Player Ranking\n  in Soccer via a Machine Learning Approach. ACM Trans. Intell. Syst. Technol.\n  10, 5, Article 59 (September 2019), 27 pages", "doi": "10.1145/3343172", "report-no": null, "categories": "stat.AP cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of evaluating the performance of soccer players is attracting the\ninterest of many companies and the scientific community, thanks to the\navailability of massive data capturing all the events generated during a match\n(e.g., tackles, passes, shots, etc.). Unfortunately, there is no consolidated\nand widely accepted metric for measuring performance quality in all of its\nfacets. In this paper, we design and implement PlayeRank, a data-driven\nframework that offers a principled multi-dimensional and role-aware evaluation\nof the performance of soccer players. We build our framework by deploying a\nmassive dataset of soccer-logs and consisting of millions of match events\npertaining to four seasons of 18 prominent soccer competitions. By comparing\nPlayeRank to known algorithms for performance evaluation in soccer, and by\nexploiting a dataset of players' evaluations made by professional soccer\nscouts, we show that PlayeRank significantly outperforms the competitors. We\nalso explore the ratings produced by {\\sf PlayeRank} and discover interesting\npatterns about the nature of excellent performances and what distinguishes the\ntop players from the others. At the end, we explore some applications of\nPlayeRank -- i.e. searching players and player versatility --- showing its\nflexibility and efficiency, which makes it worth to be used in the design of a\nscalable platform for soccer analytics.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 08:43:43 GMT"}, {"version": "v2", "created": "Fri, 16 Feb 2018 12:22:23 GMT"}, {"version": "v3", "created": "Fri, 25 Jan 2019 13:48:06 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Pappalardo", "Luca", ""], ["Cintia", "Paolo", ""], ["Ferragina", "Paolo", ""], ["Massucco", "Emanuele", ""], ["Pedreschi", "Dino", ""], ["Giannotti", "Fosca", ""]]}, {"id": "1802.05186", "submitter": "Jacob Spertus", "authors": "Jacob Spertus, Marcela Horvitz-Lennon, and Sharon-Lise Normand", "title": "Bayesian Meta-Analysis of Multiple Continuous Treatments: An Application\n  to Antipsychotic Drugs", "comments": "14 Pages, 2 Figures, 2 Tables, 2 Appendix Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling dose-response relationships of drugs is essential to understanding\ntheir effect on patient outcomes under realistic circumstances. While\nintention-to-treat analyses of clinical trials provide the effect of assignment\nto a particular drug and dose, they do not capture observed exposure after\nfactoring in non-adherence and dropout. We develop Bayesian methods to flexibly\nmodel dose-response relationships of binary outcomes with continuous treatment,\nallowing for treatment effect heterogeneity and a non-linear response surface.\nWe use a hierarchical framework for meta-analysis with the explicit goal of\ncombining information from multiple trials while accounting for heterogeneity.\nIn an application, we examine the risk of excessive weight gain for patients\nwith schizophrenia treated with the second generation antipsychotics\npaliperidone, risperidone, or olanzapine in 14 clinical trials. Averaging over\nthe sample population, we found that olanzapine contributed to a 15.6% (95%\nCrI: 6.7, 27.1) excess risk of weight gain at a 500mg cumulative dose.\nPaliperidone conferred a 3.2% (95% CrI: 1.5, 5.2) and risperidone a 14.9% (95%\nCrI: 0.0, 38.7) excess risk at 500mg olanzapine equivalent cumulative doses.\nBlacks had an additional 6.8% (95% CrI: 1.0, 12.4) risk of weight gain over\nnon-blacks at 1000mg olanzapine equivalent cumulative doses of paliperidone.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 16:23:55 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Spertus", "Jacob", ""], ["Horvitz-Lennon", "Marcela", ""], ["Normand", "Sharon-Lise", ""]]}, {"id": "1802.05342", "submitter": "Haraldur Hallgr\\'imsson", "authors": "Haraldur T. Hallgr\\'imsson, Matthew Cieslak, Luca Foschini, Scott T.\n  Grafton, Ambuj K. Singh", "title": "Spatial Coherence of Oriented White Matter Microstructure: Applications\n  to White Matter Regions Associated with Genetic Similarity", "comments": null, "journal-ref": "NeuroImage (2018)", "doi": "10.1016/j.neuroimage.2018.01.050", "report-no": null, "categories": "stat.AP cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to discover differences between populations with respect\nto the spatial coherence of their oriented white matter microstructure in\narbitrarily shaped white matter regions. This method is applied to diffusion\nMRI scans of a subset of the Human Connectome Project dataset: 57 pairs of\nmonozygotic and 52 pairs of dizygotic twins. After controlling for\nmorphological similarity between twins, we identify 3.7% of all white matter as\nbeing associated with genetic similarity (35.1k voxels, $p < 10^{-4}$, false\ndiscovery rate 1.5%), 75% of which spatially clusters into twenty-two\ncontiguous white matter regions. Furthermore, we show that the orientation\nsimilarity within these regions generalizes to a subset of 47 pairs of non-twin\nsiblings, and show that these siblings are on average as similar as dizygotic\ntwins. The regions are located in deep white matter including the superior\nlongitudinal fasciculus, the optic radiations, the middle cerebellar peduncle,\nthe corticospinal tract, and within the anterior temporal lobe, as well as the\ncerebellum, brain stem, and amygdalae.\n  These results extend previous work using undirected fractional anisotrophy\nfor measuring putative heritable influences in white matter. Our\nmultidirectional extension better accounts for crossing fiber connections\nwithin voxels. This bottom up approach has at its basis a novel measurement of\ncoherence within neighboring voxel dyads between subjects, and avoids some of\nthe fundamental ambiguities encountered with tractographic approaches to white\nmatter analysis that estimate global connectivity.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 22:31:14 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Hallgr\u00edmsson", "Haraldur T.", ""], ["Cieslak", "Matthew", ""], ["Foschini", "Luca", ""], ["Grafton", "Scott T.", ""], ["Singh", "Ambuj K.", ""]]}, {"id": "1802.05778", "submitter": "Gregory Matthews", "authors": "Gregory J Matthews, Juliet K. Brophy, Maxwell P. Luetkemeier, Hongie\n  Gu, George K. Thiruvathukal", "title": "A comparison of machine learning techniques for taxonomic classification\n  of teeth from the Family Bovidae", "comments": null, "journal-ref": "Gregory J. Matthews, Juliet K. Brophy, Maxwell Luetkemeier, Hongie\n  Gu & George K. Thiruvathukal (2018) A comparison of machine learning\n  techniques for taxonomic classification of teeth from the Family Bovidae,\n  Journal of Applied Statistics", "doi": "10.1080/02664763.2018.1441381", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study explores the performance of modern, accurate machine learning\nalgorithms on the classification of fossil teeth in the Family Bovidae.\nIsolated bovid teeth are typically the most common fossils found in southern\nAfrica and they often constitute the basis for paleoenvironmental\nreconstructions. Taxonomic identification of fossil bovid teeth, however, is\noften imprecise and subjective. Using modern teeth with known taxons, machine\nlearning algorithms can be trained to classify fossils. Previous work by Brophy\net. al. 2014 uses elliptical Fourier analysis of the form (size and shape) of\nthe outline of the occlusal surface of each tooth as features in a linear\ndiscriminant analysis framework. This manuscript expands on that previous work\nby exploring how different machine learning approaches classify the teeth and\ntesting which technique is best for classification. Five different machine\nlearning techniques including linear discriminant analysis, neural networks,\nnuclear penalized multinomial regression, random forests, and support vector\nmachines were used to estimate these models. Support vector machines and random\nforests perform the best in terms of both log-loss and misclassification rate;\nboth of these methods are improvements over linear discriminant analysis. With\nthe identification and application of these superior methods, bovid teeth can\nbe classified with higher accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 22:10:16 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Matthews", "Gregory J", ""], ["Brophy", "Juliet K.", ""], ["Luetkemeier", "Maxwell P.", ""], ["Gu", "Hongie", ""], ["Thiruvathukal", "George K.", ""]]}, {"id": "1802.05828", "submitter": "Rozhin Eskandarpour", "authors": "Rozhin Eskandarpour, Amin Khodaei, Ali Arab", "title": "Improving Power Grid Resilience Through Predictive Outage Estimation", "comments": null, "journal-ref": "Power Symposium (NAPS), 2017 North American", "doi": "10.1109/NAPS.2017.8107262", "report-no": null, "categories": "cs.SY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, in an attempt to improve power grid resilience, a machine\nlearning model is proposed to predictively estimate the component states in\nresponse to extreme events. The proposed model is based on a multi-dimensional\nSupport Vector Machine (SVM) considering the associated resilience index, i.e.,\nthe infrastructure quality level and the time duration that each component can\nwithstand the event, as well as predicted path and intensity of the upcoming\nextreme event. The outcome of the proposed model is the classified component\nstate data to two categories of outage and operational, which can be further\nused to schedule system resources in a predictive manner with the objective of\nmaximizing its resilience. The proposed model is validated using \\\"A-fold\ncross-validation and model benchmarking techniques. The performance of the\nmodel is tested through numerical simulations and based on a well-defined and\ncommonly-used performance measure.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 04:02:06 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Eskandarpour", "Rozhin", ""], ["Khodaei", "Amin", ""], ["Arab", "Ali", ""]]}, {"id": "1802.06009", "submitter": "Joshua Gardner", "authors": "Josh Gardner, Christopher Brooks", "title": "Dropout Model Evaluation in MOOCs", "comments": null, "journal-ref": "Eighth AAAI Symposium on Educational Advances in Artificial\n  Intelligence (EAAI-18), 2018", "doi": null, "report-no": null, "categories": "stat.AP cs.CY stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of learning analytics needs to adopt a more rigorous approach for\npredictive model evaluation that matches the complex practice of\nmodel-building. In this work, we present a procedure to statistically test\nhypotheses about model performance which goes beyond the state-of-the-practice\nin the community to analyze both algorithms and feature extraction methods from\nraw data. We apply this method to a series of algorithms and feature sets\nderived from a large sample of Massive Open Online Courses (MOOCs). While a\ncomplete comparison of all potential modeling approaches is beyond the scope of\nthis paper, we show that this approach reveals a large gap in dropout\nprediction performance between forum-, assignment-, and clickstream-based\nfeature extraction methods, where the latter is significantly better than the\nformer two, which are in turn indistinguishable from one another. This work has\nmethodological implications for evaluating predictive or AI-based models of\nstudent success, and practical implications for the design and targeting of\nat-risk student models and interventions.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 16:13:39 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Gardner", "Josh", ""], ["Brooks", "Christopher", ""]]}, {"id": "1802.06018", "submitter": "Johannes Riesterer", "authors": "Julian Bruns, Johannes Riesterer, Bowen Wang, Till Riedel, Micheal\n  Beigl", "title": "Automated Quality Assessment of (Citizen) Weather Stations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today we have access to a vast amount of weather, air quality, noise or\nradioactivity data collected by individual around the globe. This volunteered\ngeographic information often contains data of uncertain and of heterogeneous\nquality, in particular when compared to official in-situ measurements. This\nlimits their application, as rigorous, work-intensive data cleaning has to be\nperformed, which reduces the amount of data and cannot be performed in\nreal-time. In this paper, we propose dynamically learning the quality of\nindividual sensors by optimizing a weighted Gaussian process regression using a\ngenetic algorithm. We chose weather stations as our use case as these are the\nmost common VGI measurements. The evaluation is done for the south-west of\nGermany in August 2016 with temperature data from the Wunderground network and\nthe Deutsche Wetter Dienst (DWD), in total 1561 stations. Using a 10-fold\ncross-validation scheme based on the DWD ground truth, we can show significant\nimprovements of the predicted sensor reading. In our experiment we were obtain\na 12.5% improvement on the mean absolute error.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 20:51:33 GMT"}, {"version": "v2", "created": "Tue, 1 May 2018 18:42:22 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Bruns", "Julian", ""], ["Riesterer", "Johannes", ""], ["Wang", "Bowen", ""], ["Riedel", "Till", ""], ["Beigl", "Micheal", ""]]}, {"id": "1802.06100", "submitter": "Thomai Tsiftsi", "authors": "Thomai Tsiftsi and Victor De la Luz", "title": "Extreme Value Analysis of Solar Flare Events", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": "10.1029/2018SW001958", "report-no": null, "categories": "stat.AP astro-ph.SR physics.data-an physics.space-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Space weather events such as solar flares can be harmful for life and\ninfrastructure on earth or in near-earth orbit. In this paper we employ extreme\nvalue theory (EVT) to model extreme solar flare events; EVT offers the\nappropriate tools for the study and estimation of probabilities for\nextrapolation to ranges outside of those that have already been observed. In\nthe past such phenomena have been modelled as following a power law which may\ngives poor estimates of such events due to overestimation. The data used in the\nstudy were X-ray fluxes from NOAA/GOES and the expected return levels for\nCarrington or Halloween like events were calculated with the outcome that the\nexisting data predict similar events happening in 110 and 38 years\nrespectively.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 19:40:56 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Tsiftsi", "Thomai", ""], ["De la Luz", "Victor", ""]]}, {"id": "1802.06310", "submitter": "Karren Yang", "authors": "Karren D. Yang, Abigail Katcoff and Caroline Uhler", "title": "Characterizing and Learning Equivalence Classes of Causal DAGs under\n  Interventions", "comments": "18 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning causal DAGs in the setting where both\nobservational and interventional data is available. This setting is common in\nbiology, where gene regulatory networks can be intervened on using chemical\nreagents or gene deletions. Hauser and B\\\"uhlmann (2012) previously\ncharacterized the identifiability of causal DAGs under perfect interventions,\nwhich eliminate dependencies between targeted variables and their direct\ncauses. In this paper, we extend these identifiability results to general\ninterventions, which may modify the dependencies between targeted variables and\ntheir causes without eliminating them. We define and characterize the\ninterventional Markov equivalence class that can be identified from general\n(not necessarily perfect) intervention experiments. We also propose the first\nprovably consistent algorithm for learning DAGs in this setting and evaluate\nour algorithm on simulated and biological datasets.\n", "versions": [{"version": "v1", "created": "Sat, 17 Feb 2018 23:42:24 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2018 23:27:53 GMT"}, {"version": "v3", "created": "Thu, 7 Feb 2019 17:27:13 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Yang", "Karren D.", ""], ["Katcoff", "Abigail", ""], ["Uhler", "Caroline", ""]]}, {"id": "1802.06517", "submitter": "Ahmed Attia", "authors": "Ahmed Attia, Alen Alexanderian, Arvind K. Saibaba", "title": "Goal-Oriented Optimal Design of Experiments for Large-Scale Bayesian\n  Linear Inverse Problems", "comments": "25 pages, 13 figures", "journal-ref": null, "doi": "10.1088/1361-6420/aad210", "report-no": null, "categories": "cs.CE math.NA math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a framework for goal-oriented optimal design of experiments\n(GOODE) for large-scale Bayesian linear inverse problems governed by PDEs. This\nframework differs from classical Bayesian optimal design of experiments (ODE)\nin the following sense: we seek experimental designs that minimize the\nposterior uncertainty in the experiment end-goal, e.g., a quantity of interest\n(QoI), rather than the estimated parameter itself. This is suitable for\nscenarios in which the solution of an inverse problem is an intermediate step\nand the estimated parameter is then used to compute a QoI. In such problems, a\nGOODE approach has two benefits: the designs can avoid wastage of experimental\nresources by a targeted collection of data, and the resulting design criteria\nare computationally easier to evaluate due to the often low-dimensionality of\nthe QoIs. We present two modified design criteria, A-GOODE and D-GOODE, which\nare natural analogues of classical Bayesian A- and D-optimal criteria. We\nanalyze the connections to other ODE criteria, and provide interpretations for\nthe GOODE criteria by using tools from information theory. Then, we develop an\nefficient gradient-based optimization framework for solving the GOODE\noptimization problems. Additionally, we present comprehensive numerical\nexperiments testing the various aspects of the presented approach. The driving\napplication is the optimal placement of sensors to identify the source of\ncontaminants in a diffusion and transport problem. We enforce sparsity of the\nsensor placements using an $\\ell_1$-norm penalty approach, and propose a\npractical strategy for specifying the associated penalty parameter.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 04:56:59 GMT"}, {"version": "v2", "created": "Mon, 11 Jun 2018 04:40:55 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Attia", "Ahmed", ""], ["Alexanderian", "Alen", ""], ["Saibaba", "Arvind K.", ""]]}, {"id": "1802.06678", "submitter": "Roi Naveiro", "authors": "Roi Naveiro, Sim\\'on Rodr\\'iguez, David R\\'ios Insua", "title": "Large Scale Automated Forecasting for Monitoring Network Safety and\n  Security", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real time large scale streaming data pose major challenges to forecasting, in\nparticular defying the presence of human experts to perform the corresponding\nanalysis. We present here a class of models and methods used to develop an\nautomated, scalable and versatile system for large scale forecasting oriented\ntowards safety and security monitoring. Our system provides short and long term\nforecasts and uses them to detect safety and security issues in relation with\nmultiple internet connected devices well in advance they might take place.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 15:50:02 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2018 16:16:43 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Naveiro", "Roi", ""], ["Rodr\u00edguez", "Sim\u00f3n", ""], ["Insua", "David R\u00edos", ""]]}, {"id": "1802.06710", "submitter": "Kwonsang Lee", "authors": "Kwonsang Lee, Dylan S. Small, Francesca Dominici", "title": "Discovering Effect Modification and Randomization Inference in Air\n  Pollution Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studies have shown that exposure to air pollution, even at low levels,\nsignificantly increases mortality. As regulatory actions are becoming\nprohibitively expensive, robust evidence to guide the development of targeted\ninterventions to reduce air pollution exposure is needed. In this paper, we\nintroduce a novel statistical method that splits the data into two subsamples:\n(a) Using the first subsample, we consider a data-driven search for $\\textit{de\nnovo}$ discovery of subgroups that could have exposure effects that differ from\nthe population mean; and then (b) using the second subsample, we quantify\nevidence of effect modification among the subgroups with nonparametric\nrandomization-based tests. We also develop a sensitivity analysis method to\nassess the robustness of the conclusions to unmeasured confounding bias. Via\nsimulation studies and theoretical arguments, we demonstrate that since we\ndiscover the subgroups in the first subsample, hypothesis testing on the second\nsubsample can focus on theses subgroups only, thus substantially increasing the\nstatistical power of the test. We apply our method to the data of 1,612,414\nMedicare beneficiaries in New England region in the United States for the\nperiod 2000 to 2006. We find that seniors aged between 81-85 with low income\nand seniors aged above 85 have statistically significant higher causal effects\nof exposure to PM$_{2.5}$ on 5-year mortality rate compared to the population\nmean.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 17:11:16 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Lee", "Kwonsang", ""], ["Small", "Dylan S.", ""], ["Dominici", "Francesca", ""]]}, {"id": "1802.06711", "submitter": "Kwonsang Lee", "authors": "Kwonsang Lee, Scott A. Lorch, Dylan S. Small", "title": "Sensitivity analyses for average treatment effects when outcome is\n  censored by death in instrumental variable models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two problems that arise in making causal inferences for non-mortality\noutcomes such as bronchopulmonary dysplasia (BPD) are unmeasured confounding\nand censoring by death, i.e., the outcome is only observed when subjects\nsurvive. In randomized experiments with noncompliance, instrumental variable\nmethods can be used to control for the unmeasured confounding without censoring\nby death. But when there is censoring by death, the average causal treatment\neffect cannot be identified under usual assumptions, but can be studied for a\nspecific subpopulation by using sensitivity analysis with additional\nassumptions. However, in observational studies, evaluation of the local average\ntreatment effect (LATE) in censoring by death problems with unmeasured\nconfounding is not well studied. We develop a novel sensitivity analysis method\nbased on instrumental variable models for studying the LATE. Specifically, we\npresent the identification results under an additional assumption, and propose\na three-step procedure for the LATE estimation. Also, we propose an improved\ntwo-step procedure by simultaneously estimating the instrument propensity score\n(i.e., the probability of instrument given covariates) and the parameters\ninduced by the assumption. We have shown with simulation studies that the\ntwo-step procedure can be more robust and efficient than the three-step\nprocedure. Finally, we apply our sensitivity analysis methods to a study of the\neffect of delivery at high-level neonatal intensive care units on the risk of\nBPD.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 17:13:09 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Lee", "Kwonsang", ""], ["Lorch", "Scott A.", ""], ["Small", "Dylan S.", ""]]}, {"id": "1802.06715", "submitter": "Debasis Kundu Professor", "authors": "Debasis Kundu and Vahid Nekoukhou", "title": "Univariate and Bivariate Geometric Discrete Generalized Exponential\n  Distributions", "comments": "arXiv admin note: text overlap with arXiv:1701.03569", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Marshall and Olkin (1997, Biometrika, 84, 641 - 652) introduced a very\npowerful method to introduce an additional parameter to a class of continuous\ndistribution functions and hence it brings more flexibility to the model. They\nhave demonstrated their method for the exponential and Weibull classes. In the\nsame paper they have briefly indicated regarding its bivariate extension. The\nmain aim of this paper is to introduce the same method, for the first time, to\nthe class of discrete generalized exponential distributions both for the\nunivariate and bivariate cases. We investigate several properties of the\nproposed univariate and bivariate classes. The univariate class has three\nparameters, whereas the bivariate class has five parameters. It is observed\nthat depending on the parameter values the univariate class can be both zero\ninflated as well as heavy tailed. We propose to use EM algorithm to estimate\nthe unknown parameters. Small simulation experiments have been performed to see\nthe effectiveness of the proposed EM algorithm, and a bivariate data set has\nbeen analyzed and it is observed that the proposed models and the EM algorithm\nwork quite well in practice.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 05:58:10 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Kundu", "Debasis", ""], ["Nekoukhou", "Vahid", ""]]}, {"id": "1802.07039", "submitter": "Victor Blanco", "authors": "V\\'ictor Blanco, Rom\\'an Salmer\\'on and Samuel G\\'omez-Haro", "title": "A multicriteria selection system based on player performance. Case\n  study: The Spanish ACB Basketball League", "comments": "17 pages, 6 Tables, 4 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe an approach to rank sport players based on their\nefficiency. Although is extremely useful to analyze the performance of team\ngames there is no unanimity on the use of a single index to perform such a\nranking. We propose a method to summarize the huge amount of information\ncollected at different aspects of a sport team which is almost daily publicly\navailable. The tool will allow agents involved in a player's negotiation to\nshow the strengths (and weaknesses) of the player with respect to other\nplayers. The approach is based on applying a multicriteria outranking\nmethodology using as alternatives the potential players and criteria different\nefficiency indices. A novel automatic parameter tuning approach is detailed\nthat will allow coaches and sports managers to design templates and sports\nstrategies that improve the efficiency of their teams. We report the results\nperformed over the available information on the ACB Basketball League, and we\nshow how it can be easily implemented and interpreted in practice by\ndecision-makers non familiar with the mathematical side of the methodology.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 10:08:46 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Blanco", "V\u00edctor", ""], ["Salmer\u00f3n", "Rom\u00e1n", ""], ["G\u00f3mez-Haro", "Samuel", ""]]}, {"id": "1802.07127", "submitter": "Tom Decroos", "authors": "Tom Decroos, Lotte Bransen, Jan Van Haaren, Jesse Davis", "title": "Actions Speak Louder Than Goals: Valuing Player Actions in Soccer", "comments": "Significant update of the paper. The same core idea, but with a\n  clearer methodology, applied on a different data set, and more extensive\n  experiments. 9 pages + 2 pages appendix. To be published at SIGKDD 2019", "journal-ref": null, "doi": "10.1145/3292500.3330758", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing the impact of the individual actions performed by soccer players\nduring games is a crucial aspect of the player recruitment process.\nUnfortunately, most traditional metrics fall short in addressing this task as\nthey either focus on rare actions like shots and goals alone or fail to account\nfor the context in which the actions occurred. This paper introduces (1) a new\nlanguage for describing individual player actions on the pitch and (2) a\nframework for valuing any type of player action based on its impact on the game\noutcome while accounting for the context in which the action happened. By\naggregating soccer players' action values, their total offensive and defensive\ncontributions to their team can be quantified. We show how our approach\nconsiders relevant contextual information that traditional player evaluation\nmetrics ignore and present a number of use cases related to scouting and\nplaying style characterization in the 2016/2017 and 2017/2018 seasons in\nEurope's top competitions.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 21:28:36 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2019 12:26:32 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Decroos", "Tom", ""], ["Bransen", "Lotte", ""], ["Van Haaren", "Jan", ""], ["Davis", "Jesse", ""]]}, {"id": "1802.07346", "submitter": "Nisar Ahmed", "authors": "Michael Ouimet, David Iglesias, Nisar Ahmed, Sonia Martinez", "title": "Cooperative Robot Localization Using Event-triggered Estimation", "comments": "Revised submission in review with AIAA Journal of Aerospace\n  Information Systems (JAIS), submitted February 17, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.SY eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a novel communication-spare cooperative localization\nalgorithm for a team of mobile unmanned robotic vehicles. Exploiting an\nevent-based estimation paradigm, robots only send measurements to neighbors\nwhen the expected innovation for state estimation is high. Since agents know\nthe event-triggering condition for measurements to be sent, the lack of a\nmeasurement is thus also informative and fused into state estimates. The robots\nuse a Covariance Intersection (CI) mechanism to occasionally synchronize their\nlocal estimates of the full network state. In addition, heuristic balancing\ndynamics on the robots' CI-triggering thresholds ensure that, in large diameter\nnetworks, the local error covariances remains below desired bounds across the\nnetwork. Simulations on both linear and nonlinear dynamics/measurement models\nshow that the event-triggering approach achieves nearly optimal state\nestimation performance in a wide range of operating conditions, even when using\nonly a fraction of the communication cost required by conventional full data\nsharing. The robustness of the proposed approach to lossy communications, as\nwell as the relationship between network topology and CI-based synchronization\nrequirements, are also examined.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 21:47:34 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Ouimet", "Michael", ""], ["Iglesias", "David", ""], ["Ahmed", "Nisar", ""], ["Martinez", "Sonia", ""]]}, {"id": "1802.07350", "submitter": "Shu Wei", "authors": "Shu Wei", "title": "Reimagine Procrastination: Music Preference and Health Habits as Factors\n  on Self-Perceived Procrastination of Young People", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  As the buzzword phenomenon, procrastination holds a continued need for a\ncomprehensive examination of its nature and the associated factors. The\npresented study explores the potential relationship between music taste, life\nstyle and the youngsters' procrastination through quantitative modelling. To\nhandle the big set of survey statistics and the uncertainty caused by the data\nmissingness, the combined methods of factor analysis, multiple imputation (MI)\nand Ordered logit regression are employed. The result reveals that the music\npreference for Hip-hop, AlternativeR and Opera have a significant effect on\nprocrastination. Concerning the living habits, the eating habit and local\nauthority (city/rural) also yield strong connection to the self-perceived\nprocrastination. Implications for this procrastination research is discussed.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 21:53:12 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Wei", "Shu", ""]]}, {"id": "1802.07380", "submitter": "Sean Jewell", "authors": "Sean Jewell, Toby Dylan Hocking, Paul Fearnhead, Daniela Witten", "title": "Fast Nonconvex Deconvolution of Calcium Imaging Data", "comments": "30 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calcium imaging data promises to transform the field of neuroscience by\nmaking it possible to record from large populations of neurons simultaneously.\nHowever, determining the exact moment in time at which a neuron spikes, from a\ncalcium imaging data set, amounts to a non-trivial deconvolution problem which\nis of critical importance for downstream analyses. While a number of\nformulations have been proposed for this task in the recent literature, in this\npaper we focus on a formulation recently proposed in Jewell and Witten (2017)\nwhich has shown initial promising results. However, this proposal is slow to\nrun on fluorescence traces of hundreds of thousands of timesteps.\n  Here we develop a much faster online algorithm for solving the optimization\nproblem of Jewell and Witten (2017) that can be used to deconvolve a\nfluorescence trace of 100,000 timesteps in less than a second. Furthermore,\nthis algorithm overcomes a technical challenge of Jewell and Witten (2017) by\navoiding the occurrence of so-called \"negative\" spikes. We demonstrate that\nthis algorithm has superior performance relative to existing methods for spike\ndeconvolution on calcium imaging datasets that were recently released as part\nof the spikefinder challenge (http://spikefinder.codeneuro.org/).\n  Our C++ implementation, along with R and python wrappers, is publicly\navailable on Github at https://github.com/jewellsean/FastLZeroSpikeInference.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 00:04:42 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Jewell", "Sean", ""], ["Hocking", "Toby Dylan", ""], ["Fearnhead", "Paul", ""], ["Witten", "Daniela", ""]]}, {"id": "1802.07408", "submitter": "Jeremie Houssineau", "authors": "Emmanuel Delande and Jeremie Houssineau and Moriba Jah", "title": "Physics and Human-Based Information Fusion for Improved Resident Space\n  Object Tracking", "comments": null, "journal-ref": null, "doi": "10.1016/j.asr.2018.06.033", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maintaining a catalog of Resident Space Objects (RSOs) can be cast in a\ntypical Bayesian multi-object estimation problem, where the various sources of\nuncertainty in the problem - the orbital mechanics, the kinematic states of the\nidentified objects, the data sources, etc. - are modeled as random variables\nwith associated probability distributions. In the context of Space Situational\nAwareness, however, the information available to a space analyst on many\nuncertain components is scarce, preventing their appropriate modeling with a\nrandom variable and thus their exploitation in a RSO tracking algorithm. A\ntypical example are human-based data sources such as Two-Line Elements (TLEs),\nwhich are publicly available but lack any statistical description of their\naccuracy. In this paper, we propose the first exploitation of uncertain\nvariables in a RSO tracking problem, allowing for a representation of the\nuncertain components reflecting the information available to the space analyst,\nhowever scarce, and nothing more. In particular, we show that a human-based\ndata source and a physics-based data source can be embedded in a unified and\nrigorous Bayesian estimator in order to track a RSO. We illustrate this concept\non a scenario where real TLEs queried from the U.S. Strategic Command are fused\nwith realistically simulated radar observations in order to track a Low-Earth\nOrbit satellite.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 03:03:18 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Delande", "Emmanuel", ""], ["Houssineau", "Jeremie", ""], ["Jah", "Moriba", ""]]}, {"id": "1802.07575", "submitter": "Hossein Mohammadi", "authors": "Hossein Mohammadi, Peter Challenor, Marc Goodfellow", "title": "Emulating dynamic non-linear simulators using Gaussian processes", "comments": null, "journal-ref": "Computational Statistics & Data Analysis 139, 178 - 196 (2019)", "doi": "10.1016/j.csda.2019.05.006", "report-no": null, "categories": "stat.ML math.DS stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dynamic emulation of non-linear deterministic computer codes where the\noutput is a time series, possibly multivariate, is examined. Such computer\nmodels simulate the evolution of some real-world phenomenon over time, for\nexample models of the climate or the functioning of the human brain. The models\nwe are interested in are highly non-linear and exhibit tipping points,\nbifurcations and chaotic behaviour. However, each simulation run could be too\ntime-consuming to perform analyses that require many runs, including\nquantifying the variation in model output with respect to changes in the\ninputs. Therefore, Gaussian process emulators are used to approximate the\noutput of the code. To do this, the flow map of the system under study is\nemulated over a short time period. Then, it is used in an iterative way to\npredict the whole time series. A number of ways are proposed to take into\naccount the uncertainty of inputs to the emulators, after fixed initial\nconditions, and the correlation between them through the time series. The\nmethodology is illustrated with two examples: the highly non-linear dynamical\nsystems described by the Lorenz and Van der Pol equations. In both cases, the\npredictive performance is relatively high and the measure of uncertainty\nprovided by the method reflects the extent of predictability in each system.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 14:07:36 GMT"}, {"version": "v2", "created": "Sat, 3 Mar 2018 13:57:27 GMT"}, {"version": "v3", "created": "Sat, 9 Jun 2018 16:54:16 GMT"}, {"version": "v4", "created": "Mon, 18 Feb 2019 19:32:23 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Mohammadi", "Hossein", ""], ["Challenor", "Peter", ""], ["Goodfellow", "Marc", ""]]}, {"id": "1802.07762", "submitter": "Pierre Masselot", "authors": "Pierre Masselot, Fateh Chebana, Diane B\\'elanger, Andr\\'e St-Hilaire,\n  Belkacem Abdous, Pierre Gosselin, Taha B.M.J. Ouarda", "title": "Aggregating the response in time series regression models, applied to\n  weather-related cardiovascular mortality", "comments": null, "journal-ref": null, "doi": "10.1016/j.scitotenv.2018.02.014", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In environmental epidemiology studies, health response data (e.g.\nhospitalization or mortality) are often noisy because of hospital organization\nand other social factors. The noise in the data can hide the true signal\nrelated to the exposure. The signal can be unveiled by performing a temporal\naggregation on health data and then using it as the response in regression\nanalysis. From aggregated series, a general methodology is introduced to\naccount for the particularities of an aggregated response in a regression\nsetting. This methodology can be used with usually applied regression models in\nweather-related health studies, such as generalized additive models (GAM) and\ndistributed lag nonlinear models (DLNM). In particular, the residuals are\nmodelled using an autoregressive-moving average (ARMA) model to account for the\ntemporal dependence. The proposed methodology is illustrated by modelling the\ninfluence of temperature on cardiovascular mortality in Canada. A comparison\nwith classical DLNMs is provided and several aggregation methods are compared.\nResults show that there is an increase in the fit quality when the response is\naggregated, and that the estimated relationship focuses more on the outcome\nover several days than the classical DLNM. More precisely, among various\ninvestigated aggregation schemes, it was found that an aggregation with an\nasymmetric Epanechnikov kernel is more suited for studying the\ntemperature-mortality relationship.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 19:35:16 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Masselot", "Pierre", ""], ["Chebana", "Fateh", ""], ["B\u00e9langer", "Diane", ""], ["St-Hilaire", "Andr\u00e9", ""], ["Abdous", "Belkacem", ""], ["Gosselin", "Pierre", ""], ["Ouarda", "Taha B. M. J.", ""]]}, {"id": "1802.07807", "submitter": "Samuel Clark", "authors": "Samuel J. Clark", "title": "A Guide to Comparing the Performance of VA Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The literature comparing the performance of algorithms for assigning cause of\ndeath using verbal autopsy data is fractious and does not reach a consensus on\nwhich algorithms perform best, or even how to do the comparison. This\nmanuscript explains the challenges and suggests a way forward. A universal\nchallenge is the lack of standard training and testing data. This limits\nmeaningful comparisons between algorithms, and further, limits the ability of\nany algorithm to classify verbal autopsy deaths by cause in a way that is\nwidely generalizable across regions and through time. Verbal autopsy algorithms\nutilize a variety of information to describe the relationship between verbal\nautopsy symptoms and causes of death - called symptom-cause information (SCI).\nA crowd sourced, public archive of SCI managed by the World Health Organization\n(WHO) is suggested as a way to address the lack of SCI for developing, testing,\nand comparing verbal autopsy coding algorithms, and additionally, as a way to\nensure that algorithm-assigned causes of death are as accurate and comparable\nacross regions and through time as possible.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 21:07:38 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Clark", "Samuel J.", ""]]}, {"id": "1802.08061", "submitter": "Zhijian Wang", "authors": "Nan Zhou, Li Zhang, Shijian Li, Zhijian Wang", "title": "Algorithmic Collusion in Cournot Duopoly Market: Evidence from\n  Experimental Economics", "comments": "22 pages, 7 figures; algorithmic collusion; Cournot duopoly model;\n  experimental economics; game theory; collusion algorithm design; iterated\n  prisoner's dilemma; antitrust; mechanism design", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.GT stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithmic collusion is an emerging concept in current artificial\nintelligence age. Whether algorithmic collusion is a creditable threat remains\nas an argument. In this paper, we propose an algorithm which can extort its\nhuman rival to collude in a Cournot duopoly competing market. In experiments,\nwe show that, the algorithm can successfully extorted its human rival and gets\nhigher profit in long run, meanwhile the human rival will fully collude with\nthe algorithm. As a result, the social welfare declines rapidly and stably.\nBoth in theory and in experiment, our work confirms that, algorithmic collusion\ncan be a creditable threat. In application, we hope, the frameworks, the\nalgorithm design as well as the experiment environment illustrated in this\nwork, can be an incubator or a test bed for researchers and policymakers to\nhandle the emerging algorithmic collusion.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 18:34:27 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Zhou", "Nan", ""], ["Zhang", "Li", ""], ["Li", "Shijian", ""], ["Wang", "Zhijian", ""]]}, {"id": "1802.08161", "submitter": "Augustin Touron", "authors": "Augustin Touron (UP11, EDF R&D)", "title": "Consistency of the maximum likelihood estimator in seasonal hidden\n  Markov models", "comments": "arXiv admin note: text overlap with arXiv:1710.08112", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a variant of hidden Markov models in which the\ntransition probabilities between the states, as well as the emission\ndistributions, are not constant in time but vary in a periodic manner. This\nclass of models, that we will call seasonal hidden Markov models (SHMM) is\nparticularly useful in practice, as many applications involve a seasonal\nbehaviour. However, up to now, there is no theoretical result regarding this\nkind of model. We show that under mild assumptions, SHMM are identifiable: we\ncan identify the transition matrices and the emission distributions from the\njoint distribution of the observations on a period, up to state labelling. We\nalso give sufficient conditions for the strong consistency of the maximum\nlikelihood estimator (MLE). These results are applied to simulated data, using\nthe EM algorithm to compute the MLE. Finally, we show how SHMM can be used in\nreal world applications by applying our model to precipitation data, with\nmixtures of exponential distributions as emission distributions.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 09:14:04 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Touron", "Augustin", "", "UP11, EDF R&D"]]}, {"id": "1802.08194", "submitter": "Arun Chandrasekhar", "authors": "Emily Breza, Arun G. Chandrasekhar, Alireza Tahbaz-Salehi", "title": "Seeing the forest for the trees? An investigation of network knowledge", "comments": "10 Figures and Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper assesses the empirical content of one of the most prevalent\nassumptions in the economics of networks literature, namely the assumption that\ndecision makers have full knowledge about the networks they interact on. Using\nnetwork data from 75 villages, we ask 4,554 individuals to assess whether five\nrandomly chosen pairs of households in their village are linked through\nfinancial, social, and informational relationships. We find that network\nknowledge is low and highly localized, declining steeply with the pair's\nnetwork distance to the respondent. 46% of respondents are not even able to\noffer a guess about the status of a potential link between a given pair of\nindividuals. Even when willing to offer a guess, respondents can only correctly\nidentify the links 37% of the time. We also find that a one-step increase in\nthe social distance to the pair corresponds to a 10pp increase in the\nprobability of misidentifying the link. We then investigate the theoretical\nimplications of this assumption by showing that the predictions of various\nmodels change substantially if agents behave under the more realistic\nassumption of incomplete knowledge about the network. Taken together, our\nresults suggest that the assumption of full network knowledge (i) may serve as\na poor approximation to the real world and (ii) is not innocuous: allowing for\nincomplete network knowledge may have first-order implications for a range of\nqualitative and quantitative results in various contexts.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 17:38:44 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Breza", "Emily", ""], ["Chandrasekhar", "Arun G.", ""], ["Tahbaz-Salehi", "Alireza", ""]]}, {"id": "1802.08238", "submitter": "Yiyang Gu", "authors": "Yiyang Gu", "title": "What are the most important factors that influence the changes in London\n  Real Estate Prices? How to quantify them?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, real estate industry has captured government and public\nattention around the world. The factors influencing the prices of real estate\nare diversified and complex. However, due to the limitations and one-sidedness\nof their respective views, they did not provide enough theoretical basis for\nthe fluctuation of house price and its influential factors. The purpose of this\npaper is to build a housing price model to make the scientific and objective\nanalysis of London's real estate market trends from the year 1996 to 2016 and\nproposes some countermeasures to reasonably control house prices. Specifically,\nthe paper analyzes eight factors which affect the house prices from two\naspects: housing supply and demand and find out the factor which is of vital\nimportance to the increase of housing price per square meter. The problem of a\nhigh level of multicollinearity between them is solved by using principal\ncomponents analysis.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 18:50:51 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Gu", "Yiyang", ""]]}, {"id": "1802.08251", "submitter": "Samuel Blanck", "authors": "Samuel Blanck and Guillemette Marot", "title": "SMAGEXP: a galaxy tool suite for transcriptomics data meta-analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN q-bio.QM stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bakground: With the proliferation of available microarray and high throughput\nsequencing experiments in the public domain, the use of meta-analysis methods\nincreases. In these experiments, where the sample size is often limited,\nmeta-analysis offers the possibility to considerably enhance the statistical\npower and give more accurate results. For those purposes, it combines either\neffect sizes or results of single studies in a appropriate manner. R packages\nmetaMA and metaRNASeq perform meta-analysis on microarray and NGS data,\nrespectively. They are not interchangeable as they rely on statistical modeling\nspecific to each technology.\n  Results: SMAGEXP (Statistical Meta-Analysis for Gene EXPression) integrates\nmetaMA and metaRNAseq packages into Galaxy. We aim to propose a unified way to\ncarry out meta-analysis of gene expression data, while taking care of their\nspecificities. We have developed this tool suite to analyse microarray data\nfrom Gene Expression Omnibus (GEO) database or custom data from affymetrix\nmicroarrays. These data are then combined to carry out meta-analysis using\nmetaMA package. SMAGEXP also offers to combine raw read counts from Next\nGeneration Sequencing (NGS) experiments using DESeq2 and metaRNASeq package. In\nboth cases, key values, independent from the technology type, are reported to\njudge the quality of the meta-analysis. These tools are available on the Galaxy\nmain tool shed. Source code, help and installation instructions are available\non github.\n  Conclusion: The use of Galaxy offers an easy-to-use gene expression\nmeta-analysis tool suite based on the metaMA and metaRNASeq packages.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 13:56:58 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Blanck", "Samuel", ""], ["Marot", "Guillemette", ""]]}, {"id": "1802.08286", "submitter": "Ashkan Zeinalzadeh", "authors": "Ashkan Zeinalzadeh, Donya Ghavidel, and Vijay Gupta", "title": "Reliability and Market Price of Energy in the Presence of Intermittent\n  and Non-Dispatchable Renewable Energies", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The intermittent nature of the renewable energies increases the operation\ncosts of conventional generators. As the share of energy supplied by renewable\nsources increases, these costs also increase. In this paper, we quantify these\ncosts by developing a market clearing price of energy in the presence of\nrenewable energy and congestion constraints. We consider an electricity market\nwhere generators propose their asking price per unit of energy to an\nindependent system operator (ISO). The ISO solve an optimization problem to\ndispatch energy from each generator to minimize the total cost of energy\npurchased on behalf of the consumers.\n  To ensure that the generators are able to meet the load within a desired\nconfidence level, we incorporate the notion of load variance using the\nConditional Value-at-Risk (CVAR) measure in an electricity market and we derive\nthe amount of committed power and market clearing price of energy as a function\nof CVAR. It is shown that a higher penetration of renewable energies may\nincrease the committed power, market clearing price of energy and consumer cost\nof energy due to renewable generation uncertainties. We also obtain an\nupper-bound on the amount that congestion constraints can affect the committed\npower. We present descriptive simulations to illustrate the impact of renewable\nenergy penetration and reliability levels on committed power by the\nnon-renewable generators, difference between the dispatched and committed\npower, market price of energy and profit of renewable and non-renewable\ngenerators.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 19:22:23 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Zeinalzadeh", "Ashkan", ""], ["Ghavidel", "Donya", ""], ["Gupta", "Vijay", ""]]}, {"id": "1802.08308", "submitter": "Qiwei Li", "authors": "Qiwei Li and Xinlei Wang and Faming Liang and Guanghua Xiao", "title": "A Bayesian Mark Interaction Model for Analysis of Tumor Pathology Images", "comments": "53 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advance of imaging technology, digital pathology imaging of tumor\ntissue slides is becoming a routine clinical procedure for cancer diagnosis.\nThis process produces massive imaging data that capture histological details in\nhigh resolution. Recent developments in deep-learning methods have enabled us\nto identify and classify individual cells from digital pathology images at\nlarge scale. The randomly distributed cells can be considered from a marked\npoint process, where each point is defined by its position and cell type.\nReliable statistical approaches to model such marked spatial point patterns can\nprovide new insight into tumor progression and shed light on the biological\nmechanisms of cancer. In this paper, we consider the problem of modeling\nspatial correlations among three commonly seen cells (i.e. lymphocyte, stromal,\nand tumor) observed in tumor pathology images. A novel marking model of marked\npoint processes, with interpretable underlying parameters (some of which are\nclinically meaningful), is proposed in a Bayesian framework. We use Markov\nchain Monte Carlo (MCMC) sampling techniques, combined with the double\nMetropolis-Hastings (DMH) algorithm, to sample from the posterior distribution\nwith an intractable normalizing constant. On the benchmark datasets, we\ndemonstrate how this model-based analysis can lead to sharper inferences than\nordinary exploratory analyses. Lastly, we conduct a case study on the pathology\nimages of 188 lung cancer patients from the National Lung Screening Trial. The\nresults show that the spatial correlation between tumor and stromal cells\npredicts patient prognosis. This statistical methodology not only presents a\nnew model for characterizing spatial correlations in a multi-type spatial point\npattern, but also provides a new perspective for understanding the role of\ncell-cell interactions in cancer progression.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 21:28:12 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Li", "Qiwei", ""], ["Wang", "Xinlei", ""], ["Liang", "Faming", ""], ["Xiao", "Guanghua", ""]]}, {"id": "1802.08561", "submitter": "Kelly Peterson", "authors": "Yuria Utsumi, Ognjen Rudovic, Kelly Peterson, Ricardo Guerrero,\n  Rosalind W. Picard", "title": "Personalized Gaussian Processes for Forecasting of Alzheimer's Disease\n  Assessment Scale-Cognition Sub-Scale (ADAS-Cog13)", "comments": "International Engineering in Medicine and Biology Conference (EMBC)\n  2018 - accepted. 5 pages. arXiv admin note: text overlap with\n  arXiv:1712.00181", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the use of a personalized Gaussian Process model\n(pGP) to predict per-patient changes in ADAS-Cog13 -- a significant predictor\nof Alzheimer's Disease (AD) in the cognitive domain -- using data from each\npatient's previous visits, and testing on future (held-out) data. We start by\nlearning a population-level model using multi-modal data from previously seen\npatients using a base Gaussian Process (GP) regression. The personalized GP\n(pGP) is formed by adapting the base GP sequentially over time to a new\n(target) patient using domain adaptive GPs. We extend this personalized\napproach to predict the values of ADAS-Cog13 over the future 6, 12, 18, and 24\nmonths. We compare this approach to a GP model trained only on past data of the\ntarget patients (tGP), as well as to a new approach that combines pGP with tGP.\nWe find that the new approach, combining pGP with tGP, leads to large\nimprovements in accurately forecasting future ADAS-Cog13 scores.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 01:32:54 GMT"}, {"version": "v2", "created": "Mon, 5 Mar 2018 20:15:35 GMT"}, {"version": "v3", "created": "Mon, 12 Mar 2018 17:36:21 GMT"}, {"version": "v4", "created": "Fri, 4 May 2018 06:09:52 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Utsumi", "Yuria", ""], ["Rudovic", "Ognjen", ""], ["Peterson", "Kelly", ""], ["Guerrero", "Ricardo", ""], ["Picard", "Rosalind W.", ""]]}, {"id": "1802.08616", "submitter": "Adam Sales", "authors": "Anita Israni, Adam C Sales, John F Pane", "title": "Mastery Learning in Practice: A (Mostly) Descriptive Analysis of Log\n  Data from the Cognitive Tutor Algebra I Effectiveness Trial", "comments": "In Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mastery learning, the notion that students learn best if they move on from\nstudying a topic only after having demonstrated mastery, sits at the foundation\nof the theory of intelligent tutoring. This paper is an exploration of how\nmastery learning plays out in practice, based on log data from a large\nrandomized effectiveness trial of the Cognitive Tutor Algebra I (CTAI)\ncurriculum. We find that students frequently progressed from CTAI sections they\nwere working on without demonstrating mastery and worked units out of order.\nMoreover, these behaviors were substantially more common in the second year of\nthe study, in which the CTAI effect was significantly larger. We explore the\nvarious ways students departed from the official CTAI curriculum, focusing on\nheterogeneity between years, states, schools, and students. The paper concludes\nwith an observational study of the effect on post-test scores of teachers\nreassigning students out of their current sections before they mastered the\nrequisite skills, finding that reassignment appears to lowers posttest\nscores--a finding that is fairly resilient to confounding from omitted\ncovariates--but that the effect varies substantially between classrooms.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 16:04:55 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Israni", "Anita", ""], ["Sales", "Adam C", ""], ["Pane", "John F", ""]]}, {"id": "1802.08664", "submitter": "Gavin Whitaker", "authors": "Gavin A. Whitaker, Ricardo Silva, Daniel Edwards", "title": "Modeling goal chances in soccer: a Bayesian inference approach", "comments": "19 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of determining the number of chances a soccer team\ncreates, along with the composite nature of each chance-the players involved\nand the locations on the pitch of the assist and the chance. We propose an\ninterpretable Bayesian inference approach and implement a Poisson model to\ncapture chance occurrences, from which we infer team abilities. We then use a\nGaussian mixture model to capture the areas on the pitch a player makes an\nassist/takes a chance. This approach allows the visualization of differences\nbetween players in the way they approach attacking play (making assists/taking\nchances). We apply the resulting scheme to the 2016/2017 English Premier\nLeague, capturing team abilities to create chances, before highlighting key\nareas where players have most impact.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 18:10:02 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Whitaker", "Gavin A.", ""], ["Silva", "Ricardo", ""], ["Edwards", "Daniel", ""]]}, {"id": "1802.08717", "submitter": "Maciej Mazurowski", "authors": "Maciej A. Mazurowski, Mateusz Buda, Ashirbani Saha, Mustafa R. Bashir", "title": "Deep learning in radiology: an overview of the concepts and a survey of\n  the state of the art", "comments": "27 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is a branch of artificial intelligence where networks of simple\ninterconnected units are used to extract patterns from data in order to solve\ncomplex problems. Deep learning algorithms have shown groundbreaking\nperformance in a variety of sophisticated tasks, especially those related to\nimages. They have often matched or exceeded human performance. Since the\nmedical field of radiology mostly relies on extracting useful information from\nimages, it is a very natural application area for deep learning, and research\nin this area has rapidly grown in recent years. In this article, we review the\nclinical reality of radiology and discuss the opportunities for application of\ndeep learning algorithms. We also introduce basic concepts of deep learning\nincluding convolutional neural networks. Then, we present a survey of the\nresearch in deep learning applied to radiology. We organize the studies by the\ntypes of specific tasks that they attempt to solve and review the broad range\nof utilized deep learning algorithms. Finally, we briefly discuss opportunities\nand challenges for incorporating deep learning in the radiology practice of the\nfuture.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 04:00:55 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Mazurowski", "Maciej A.", ""], ["Buda", "Mateusz", ""], ["Saha", "Ashirbani", ""], ["Bashir", "Mustafa R.", ""]]}, {"id": "1802.08761", "submitter": "Matthew Levine", "authors": "Matthew E. Levine, David J. Albers, Marissa Burgermaster, Patricia G.\n  Davidson, Arlene M. Smaldone, Lena Mamykina", "title": "Behavioral-clinical phenotyping with type 2 diabetes self-monitoring\n  data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: To evaluate unsupervised clustering methods for identifying\nindividual-level behavioral-clinical phenotypes that relate personal biomarkers\nand behavioral traits in type 2 diabetes (T2DM) self-monitoring data. Materials\nand Methods: We used hierarchical clustering (HC) to identify groups of meals\nwith similar nutrition and glycemic impact for 6 individuals with T2DM who\ncollected self-monitoring data. We evaluated clusters on: 1) correspondence to\ngold standards generated by certified diabetes educators (CDEs) for 3\nparticipants; 2) face validity, rated by CDEs, and 3) impact on CDEs' ability\nto identify patterns for another 3 participants. Results: Gold standard (GS)\nincluded 9 patterns across 3 participants. Of these, all 9 were re-discovered\nusing HC: 4 GS patterns were consistent with patterns identified by HC (over\n50% of meals in a cluster followed the pattern); another 5 were included as\nsub-groups in broader clusers. 50% (9/18) of clusters were rated over 3 on\n5-point Likert scale for validity, significance, and being actionable. After\nreviewing clusters, CDEs identified patterns that were more consistent with\ndata (70% reduction in contradictions between patterns and participants'\nrecords). Discussion: Hierarchical clustering of blood glucose and\nmacronutrient consumption appears suitable for discovering behavioral-clinical\nphenotypes in T2DM. Most clusters corresponded to gold standard and were rated\npositively by CDEs for face validity. Cluster visualizations helped CDEs\nidentify more robust patterns in nutrition and glycemic impact, creating new\npossibilities for visual analytic solutions. Conclusion: Machine learning\nmethods can use diabetes self-monitoring data to create personalized\nbehavioral-clinical phenotypes, which may prove useful for delivering\npersonalized medicine.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 23:11:22 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Levine", "Matthew E.", ""], ["Albers", "David J.", ""], ["Burgermaster", "Marissa", ""], ["Davidson", "Patricia G.", ""], ["Smaldone", "Arlene M.", ""], ["Mamykina", "Lena", ""]]}, {"id": "1802.08778", "submitter": "Luis Armona", "authors": "Luis Armona", "title": "Measuring the Demand Effects of Formal and Informal Communication :\n  Evidence from Online Markets for Illicit Drugs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I present evidence that communication between marketplace participants is an\nimportant influence on market demand. I find that consumer demand is\napproximately equally influenced by communication on both formal and informal\nnetworks- namely, product reviews and community forums. In addition, I find\nempirical evidence of a vendor's ability to commit to disclosure dampening the\neffect of communication on demand. I also find that product demand is more\nresponsive to average customer sentiment as the number of messages grows, as\nmay be expected in a Bayesian updating framework.\n", "versions": [{"version": "v1", "created": "Sat, 24 Feb 2018 01:33:27 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Armona", "Luis", ""]]}, {"id": "1802.08848", "submitter": "Leonardo Egidi", "authors": "Leonardo Egidi, Francesco Pauli and Nicola Torelli", "title": "Combining historical data and bookmakers'odds in modelling football\n  scores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modelling football outcomes has gained increasing attention, in large part\ndue to the potential for making substantial profits. Despite the strong\nconnection existing between football models and the bookmakers' betting odds,\nno authors have used the latter for improving the fit and the predictive\naccuracy of these models. We have developed a hierarchical Bayesian Poisson\nmodel in which the scoring rates of the teams are convex combinations of\nparameters estimated from historical data and the additional source of the\nbetting odds. We apply our analysis to a nine-year dataset of the most popular\nEuropean leagues in order to predict match outcomes for their tenth seasons. In\nthis paper, we provide numerical and graphical checks for our model.\n", "versions": [{"version": "v1", "created": "Sat, 24 Feb 2018 14:00:44 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Egidi", "Leonardo", ""], ["Pauli", "Francesco", ""], ["Torelli", "Nicola", ""]]}, {"id": "1802.08910", "submitter": "Vaishnavi Subramanian", "authors": "Vaishnavi Subramanian, Benjamin Chidester, Jian Ma, Minh N. Do", "title": "Correlating Cellular Features with Gene Expression using CCA", "comments": "To appear at IEEE International Symposium on Biomedical Imaging\n  (ISBI) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP eess.IV q-bio.CB q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To understand the biology of cancer, joint analysis of multiple data\nmodalities, including imaging and genomics, is crucial. The involved nature of\ngene-microenvironment interactions necessitates the use of algorithms which\ntreat both data types equally. We propose the use of canonical correlation\nanalysis (CCA) and a sparse variant as a preliminary discovery tool for\nidentifying connections across modalities, specifically between gene expression\nand features describing cell and nucleus shape, texture, and stain intensity in\nhistopathological images. Applied to 615 breast cancer samples from The Cancer\nGenome Atlas, CCA revealed significant correlation of several image features\nwith expression of PAM50 genes, known to be linked to outcome, while Sparse CCA\nrevealed associations with enrichment of pathways implicated in cancer without\nleveraging prior biological understanding. These findings affirm the utility of\nCCA for joint phenotype-genotype analysis of cancer.\n", "versions": [{"version": "v1", "created": "Sat, 24 Feb 2018 20:46:01 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Subramanian", "Vaishnavi", ""], ["Chidester", "Benjamin", ""], ["Ma", "Jian", ""], ["Do", "Minh N.", ""]]}, {"id": "1802.09278", "submitter": "Thordis Thorarinsdottir", "authors": "Thordis L. Thorarinsdottir, Kristoffer H. Hellton, Gunnhildur H.\n  Steinbakk, Lena Schlichting and Kolbj{\\o}rn Engeland", "title": "Bayesian regional flood frequency analysis for large catchments", "comments": null, "journal-ref": null, "doi": "10.1029/2017WR022460", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regional flood frequency analysis is commonly applied in situations where\nthere exists insufficient data at a location for a reliable estimation of flood\nquantiles. We develop a Bayesian hierarchical modeling framework for a regional\nanalysis of data from 203 large catchments in Norway with the generalized\nextreme value (GEV) distribution as the underlying model. Generalized linear\nmodels on the parameters of the GEV distribution are able to incorporate\nlocation-specific geographic and meteorological information and thereby\naccommodate these effects on the flood quantiles. A Bayesian model averaging\ncomponent additionally assesses model uncertainty in the effect of the proposed\ncovariates. The resulting regional model is seen to give substantially better\npredictive performance than the regional model currently used in Norway.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 12:57:47 GMT"}, {"version": "v2", "created": "Fri, 2 Mar 2018 07:52:14 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Thorarinsdottir", "Thordis L.", ""], ["Hellton", "Kristoffer H.", ""], ["Steinbakk", "Gunnhildur H.", ""], ["Schlichting", "Lena", ""], ["Engeland", "Kolbj\u00f8rn", ""]]}, {"id": "1802.09304", "submitter": "Wai Hong Tan", "authors": "Feng Chen and Wai Hong Tan", "title": "Marked Self-Exciting Point Process Modelling of Information Diffusion on\n  Twitter", "comments": "18 pages, 3 figures, accepted for publication in the Annals of\n  Applied Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information diffusion occurs on microblogging platforms like Twitter as\nretweet cascades. When a tweet is posted, it may be retweeted and henceforth\nfurther retweeted, and the retweeting process continues iteratively and\nindefinitely. A natural measure of the popularity of a tweet is the number of\nretweets it generates. Accurate predictions of tweet popularity can assist\nTwitter to rank contents more effectively and facilitate the assessment of\npotential for marketing and campaigning strategies. In this paper, we propose a\nmodel called the Marked Self-Exciting Process with Time-Dependent Excitation\nFunction, or MaSEPTiDE for short, to model the retweeting dynamics and to\npredict the tweet popularity. Our model does not require expensive feature\nengineering but is capable of leveraging the observed dynamics to accurately\npredict the future evolution of retweet cascades. We apply our proposed\nmethodology on a large amount of Twitter data and report substantial\nimprovement in prediction performance over existing approaches in the\nliterature.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 14:01:52 GMT"}, {"version": "v2", "created": "Thu, 19 Apr 2018 07:47:56 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Chen", "Feng", ""], ["Tan", "Wai Hong", ""]]}, {"id": "1802.09509", "submitter": "Paulo Serra", "authors": "Paulo Serra, Michel Mandjes", "title": "Estimation of Local Degree Distributions via Local Weighted Averaging\n  and Monte Carlo Cross-Validation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Owing to their capability of summarising interactions between elements of a\nsystem, networks have become a common type of data in many fields. As networks\ncan be inhomogeneous, in that different regions of the network may exhibit\ndifferent topologies, an important topic concerns their local properties. This\npaper focuses on the estimation of the local degree distribution of a vertex in\nan inhomogeneous network. The contributions are twofold: we propose an\nestimator based on local weighted averaging, and we set up a Monte Carlo\ncross-validation procedure to pick the parameters of this estimator. Under a\nspecific modelling assumption we derive an oracle inequality that shows how the\nmodel parameters affect the precision of the estimator. We illustrate our\nmethod by several numerical experiments, on both real and synthetic data,\nshowing in particular that the approach considerably improves upon the natural,\nempirical estimator.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 18:52:56 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Serra", "Paulo", ""], ["Mandjes", "Michel", ""]]}, {"id": "1802.09631", "submitter": "Thomai Tsiftsi", "authors": "Thomai Tsiftsi, Ian H. Jermyn, Jochen Einbeck", "title": "Bayesian shape modelling of cross-sectional geological data", "comments": "4 pages, 1 figure, In proceedings 29th International Workshop on\n  Statistical Modelling, 14-18 July 2014, Gottingen, Germany. Amsterdam:\n  Statistical Modelling Society, pp. 161-164", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shape information is of great importance in many applications. For example,\nthe oil-bearing capacity of sand bodies, the subterranean remnants of ancient\nrivers, is related to their cross-sectional shapes. The analysis of these\nshapes is therefore of some interest, but current classifications are\nsimplistic and ad hoc. In this paper, we describe the first steps towards a\ncoherent statistical analysis of these shapes by deriving the integrated\nlikelihood for data shapes given class parameters. The result is of interest\nbeyond this particular application.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 22:28:02 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Tsiftsi", "Thomai", ""], ["Jermyn", "Ian H.", ""], ["Einbeck", "Jochen", ""]]}, {"id": "1802.09706", "submitter": "Hau-tieng Wu", "authors": "Hau-Tieng Wu, Jhao-Cheng Wu, Po-Chiun Huang, Ting-Yu Lin, Tsai-Yu\n  Wang, Yuan-Hao Huang, Yu-Lun Lo", "title": "Phenotype-based and Self-learning Inter-individual Sleep Apnea Screening\n  with a Level IV Monitoring System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: We propose a phenotype-based artificial intelligence system that can\nself-learn and is accurate for screening purposes, and test it on a Level IV\nmonitoring system. Methods: Based on the physiological knowledge, we\nhypothesize that the phenotype information will allow us to find subjects from\na well-annotated database that share similar sleep apnea patterns. Therefore,\nfor a new-arriving subject, we can establish a prediction model from the\nexisting database that is adaptive to the subject. We test the proposed\nalgorithm on a database consisting of 62 subjects with the signals recorded\nfrom a Level IV wearable device measuring the thoracic and abdominal movements\nand the SpO2. Results: With the leave-one cross validation, the accuracy of the\nproposed algorithm to screen subjects with an apnea-hypopnea index greater or\nequal to 15 is 93.6%, the positive likelihood ratio is 6.8, and the negative\nlikelihood ratio is 0.03. Conclusion: The results confirm the hypothesis and\nshow that the proposed algorithm has great potential to screen patients with\nSAS.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 03:43:24 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Wu", "Hau-Tieng", ""], ["Wu", "Jhao-Cheng", ""], ["Huang", "Po-Chiun", ""], ["Lin", "Ting-Yu", ""], ["Wang", "Tsai-Yu", ""], ["Huang", "Yuan-Hao", ""], ["Lo", "Yu-Lun", ""]]}, {"id": "1802.09863", "submitter": "Robert Cowell", "authors": "Robert George Cowell", "title": "A unifying framework for the modelling and analysis of STR DNA samples\n  arising in forensic casework", "comments": "172 pages, 57 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new framework for analysing forensic DNA samples using\nprobabilistic genotyping. Specifically it presents a mathematical framework for\nspecifying and combining the steps in producing forensic casework\nelectropherograms of short tandem repeat loci from DNA samples. It is\napplicable to both high and low template DNA samples, that is, samples\ncontaining either high or low amounts DNA. A specific model is developed within\nthe framework, by way of particular modelling assumptions and approximations,\nand its interpretive power presented on examples using simulated data and data\nfrom a publicly available dataset. The framework relies heavily on the use of\nunivariate and multivariate probability generating functions. It is shown that\nthese provide a succinct and elegant mathematical scaffolding to model the key\nsteps in the process. A significant development in this paper is that of new\nnumerical methods for accurately and efficiently evaluating the probability\ndistribution of amplicons arising from the polymerase chain reaction process,\nwhich is modelled as a discrete multi-type branching process. Source code in\nthe scripting languages Python, R and Julia is provided for illustration of\nthese methods. These new developments will be of general interest to persons\nworking outside the province of forensic DNA interpretation that this paper\nfocuses on.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 13:05:16 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Cowell", "Robert George", ""]]}, {"id": "1802.09899", "submitter": "Tommy Liu", "authors": "Tommy Liu", "title": "A Kolmogorov-Smirnov type test for two inter-dependent random variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST physics.data-an stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider $n$ iid random variables, where $\\xi_1, \\ldots, \\xi_n$ are $n$\nrealisations of a random variable $\\xi$ and $\\zeta_1, \\ldots, \\zeta_n$ are $n$\nrealisations of a random variable $\\zeta$. The distribution of each realisation\nof $\\xi$, that is the distribution of \\emph{one} $\\xi_i$, depends on the value\nof the corresponding $\\zeta_i$, that is the probability $P\\left(\\xi_i\\leq\nx\\right)=F(x,\\zeta_i)$. We develop a statistical test to see if the $\\xi_1,\n\\ldots, \\xi_n$ are distributed according to the distribution function\n$F(x,\\zeta_i)$. We call this new statistical test the condition\nKolmogorov-Smirnov test.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 14:28:32 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Liu", "Tommy", ""]]}, {"id": "1802.09992", "submitter": "Yaakov Malinovsky", "authors": "Yaakov Malinovsky", "title": "Follow Up on Detecting Deficiencies: An Optimal Group Testing Algorithm", "comments": "Letter", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent volume of Mathematics Magazine (Vol. 90, No. 3, June 2017) there\nis an interesting article by Seth Zimmerman, titled Detecting Deficiencies: An\nOptimal Group Testing Algorithm. The claim in the summary is contradictory to\nwell-known facts reported in the group- testing literature, which is easily\nverified, beginning with the work by Sobel and Groll (1959), which was cited by\nS. Zimmerman himself. Therefore, I feel compelled to offer a number of comments\nand clarifications. In addition, I have made some correction of mistaken claim\nmade by Zimmerman (2017).\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 16:12:32 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Malinovsky", "Yaakov", ""]]}, {"id": "1802.10003", "submitter": "Paulo Laerte Natti", "authors": "Cainan K. de Oliveira, Henrique G. Menck, Pedro Y. Takito, Eliandro\n  Rodrigues Cirilo, Neyva Maria Lopes Romeiro, \\'Erica R. Takano Natti and\n  Paulo Laerte Natti", "title": "Stock management (Gest\\~ao de estoques)", "comments": "In Portuguese, 17 pages, 12 figures, 7 tables. Conference SEMAT2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.EC math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a great need to stock materials for production, but storing\nmaterials comes at a cost. Lack of organization in the inventory can result in\na very high cost for the final product, in addition to generating other\nproblems in the production chain. In this work we present mathematical and\nstatistical methods applicable to stock management. The stock analysis using\nABC curves serves to identify which are the priority items, the most expensive\nand with the highest turnover (demand), and thus determine, through stock\ncontrol models, the purchase lot size and the periodicity that minimize the\ntotal costs of storing these materials. Using the Economic Order Quantity (EOQ)\nmodel and the (Q,R) model, the inventory costs of a company were minimized. The\ncomparison of the results provided by the models was performed.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 16:42:50 GMT"}, {"version": "v2", "created": "Fri, 1 Jun 2018 20:18:20 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["de Oliveira", "Cainan K.", ""], ["Menck", "Henrique G.", ""], ["Takito", "Pedro Y.", ""], ["Cirilo", "Eliandro Rodrigues", ""], ["Romeiro", "Neyva Maria Lopes", ""], ["Natti", "\u00c9rica R. Takano", ""], ["Natti", "Paulo Laerte", ""]]}, {"id": "1802.10015", "submitter": "Eleni-Rosalina Andrinopoulou", "authors": "Eleni-Rosalina Andrinopoulou, Kazem Nasserinejad, Rhonda Szczesniak\n  and Dimitris Rizopoulos", "title": "Integrating Latent Classes in the Bayesian Shared Parameter Joint Model\n  of Longitudinal and Survival Outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cystic fibrosis is a chronic lung disease which requires frequent patient\nmonitoring to maintain lung function over time and minimize onset of acute\nrespiratory events known as pulmonary exacerbations. From the clinical point of\nview it is important to characterize the association between key biomarkers\nsuch as $FEV_1$ and time-to first exacerbation. Progression of the disease is\nheterogeneous, yielding different sub-groups in the population exhibiting\ndistinct longitudinal profiles. It is desirable to categorize these unobserved\nsub-groups (latent classes) according to their distinctive trajectories.\nAccounting for these latent classes, in other words heterogeneity, will lead to\nimproved estimates of association arising from the joint longitudinal-survival\nmodel.\n  The joint model of longitudinal and survival data constitutes a popular\nframework to analyze such data arising from heterogeneous cohorts. In\nparticular, two paradigms within this framework are the shared parameter joint\nmodels and the joint latent class models. The former paradigm allows one to\nquantify the strength of the association between the longitudinal and survival\noutcomes but does not allow for latent sub-populations. The latter paradigm\nexplicitly postulates the existence of sub-populations but does not directly\nquantify the strength of the association.\n  We propose to integrate latent classes in the shared parameter joint model in\na fully Bayesian approach, which allows us to investigate the association\nbetween $FEV_1$ and time-to first exacerbation within each latent class. We,\nfurthermore, focus on the selection of the optimal number of latent classes.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 16:38:28 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 11:14:51 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Andrinopoulou", "Eleni-Rosalina", ""], ["Nasserinejad", "Kazem", ""], ["Szczesniak", "Rhonda", ""], ["Rizopoulos", "Dimitris", ""]]}, {"id": "1802.10238", "submitter": "Benjamin Shickel", "authors": "Benjamin Shickel, Tyler J. Loftus, Lasith Adhikari, Tezcan\n  Ozrazgat-Baslanti, Azra Bihorac, and Parisa Rashidi", "title": "DeepSOFA: A Continuous Acuity Score for Critically Ill Patients using\n  Clinically Interpretable Deep Learning", "comments": null, "journal-ref": "Scientific Reports (2019) 9:1879", "doi": "10.1038/s41598-019-38491-0", "report-no": null, "categories": "cs.LG cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional methods for assessing illness severity and predicting in-hospital\nmortality among critically ill patients require time-consuming, error-prone\ncalculations using static variable thresholds. These methods do not capitalize\non the emerging availability of streaming electronic health record data or\ncapture time-sensitive individual physiological patterns, a critical task in\nthe intensive care unit. We propose a novel acuity score framework (DeepSOFA)\nthat leverages temporal measurements and interpretable deep learning models to\nassess illness severity at any point during an ICU stay. We compare DeepSOFA\nwith SOFA (Sequential Organ Failure Assessment) baseline models using the same\nmodel inputs and find that at any point during an ICU admission, DeepSOFA\nyields significantly more accurate predictions of in-hospital mortality. A\nDeepSOFA model developed in a public database and validated in a single\ninstitutional cohort had a mean AUC for the entire ICU stay of 0.90 (95% CI\n0.90-0.91) compared with baseline SOFA models with mean AUC 0.79 (95% CI\n0.79-0.80) and 0.85 (95% CI 0.85-0.86). Deep models are well-suited to identify\nICU patients in need of life-saving interventions prior to the occurrence of an\nunexpected adverse event and inform shared decision-making processes among\npatients, providers, and families regarding goals of care and optimal resource\nutilization.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 02:39:02 GMT"}, {"version": "v2", "created": "Thu, 30 Aug 2018 02:04:16 GMT"}, {"version": "v3", "created": "Sun, 23 Dec 2018 01:33:57 GMT"}, {"version": "v4", "created": "Wed, 13 Feb 2019 18:16:07 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Shickel", "Benjamin", ""], ["Loftus", "Tyler J.", ""], ["Adhikari", "Lasith", ""], ["Ozrazgat-Baslanti", "Tezcan", ""], ["Bihorac", "Azra", ""], ["Rashidi", "Parisa", ""]]}, {"id": "1802.10245", "submitter": "Zheng Chen", "authors": "Dong Han, Zheng Chen and Yawen Hou", "title": "Sample size for a non-inferiority clinical trial with time-to-event data\n  in the presence of competing risks", "comments": "24 pages, 1 figure", "journal-ref": "Journal of Biopharmaceutical Statistics, 2018, 28:797-807", "doi": "10.1080/10543406.2017.1399897", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis and planning methods for competing risks model have been\ndescribed in the literatures in recent decades, and non-inferiority clinical\ntrials are helpful in current pharmaceutical practice. Analytical methods for\nnon-inferiority clinical trials in the presence of competing risks were\ninvestigated by Parpia et al., who indicated that the proportional\nsub-distribution hazard model is appropriate in the context of biological\nstudies. However, the analytical methods of competing risks model differ from\nthose appropriate for analyzing non-inferiority clinical trials with a single\noutcome; thus, a corresponding method for planning such trials is necessary. A\nsample size formula for non-inferiority clinical trials in the presence of\ncompeting risks based on the proportional sub-distribution hazard model is\npresented in this paper. The primary endpoint relies on the sub-distribution\nhazard ratio. A total of 120 simulations and an example based on a randomized\ncontrolled trial verified the empirical performance of the presented formula.\nThe results demonstrate that the empirical power of sample size formulas based\non the Weibull distribution for non-inferiority clinical trials with competing\nrisks can reach the targeted power.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 03:11:20 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Han", "Dong", ""], ["Chen", "Zheng", ""], ["Hou", "Yawen", ""]]}, {"id": "1802.10490", "submitter": "Paul Novosad", "authors": "Sam Asher, Paul Novosad, Charlie Rafkin", "title": "Partial Identification of Expectations with Interval Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A conditional expectation function (CEF) can at best be partially identified\nwhen the conditioning variable is interval censored. When the number of bins is\nsmall, existing methods often yield minimally informative bounds. We propose\nthree innovations that make meaningful inference possible in interval data\ncontexts. First, we prove novel nonparametric bounds for contexts where the\ndistribution of the censored variable is known. Second, we show that a class of\nmeasures that describe the conditional mean across a fixed interval of the\nconditioning space can often be bounded tightly even when the CEF itself\ncannot. Third, we show that a constraint on CEF curvature can either tighten\nbounds or can substitute for the monotonicity assumption often made in interval\ndata applications. We derive analytical bounds that use the first two\ninnovations, and develop a numerical method to calculate bounds under the\nthird. We show the performance of the method in simulations and then present\ntwo applications. First, we resolve a known problem in the estimation of\nmortality as a function of education: because individuals with high school or\nless are a smaller and thus more negatively selected group over time, estimates\nof their mortality change are likely to be biased. Our method makes it possible\nto hold education rank bins constant over time, revealing that current\nestimates of rising mortality for less educated women are biased upward in some\ncases by a factor of three. Second, we apply the method to the estimation of\nintergenerational mobility, where researchers frequently use coarsely measured\neducation data in the many contexts where matched parent-child income data are\nunavailable. Conventional measures like the rank-rank correlation may be\nuninformative once interval censoring is taken into account; CEF interval-based\nmeasures of mobility are bounded tightly.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 15:52:05 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Asher", "Sam", ""], ["Novosad", "Paul", ""], ["Rafkin", "Charlie", ""]]}, {"id": "1802.10496", "submitter": "Pamela Shaw", "authors": "Pamela A. Shaw, Veronika Deffner, Ruth H. Keogh, Janet A. Tooze, Kevin\n  W. Dodd, Helmut K\\\"uchenhoff, Victor Kipnis, Laurence S. Freedman (on behalf\n  of the Measurement Error and Misclassification topic group (TG4) of the\n  STRATOS Initiative)", "title": "Epidemiologic analyses with error-prone exposures: Review of current\n  practice and recommendations", "comments": "41 pages, including 4 tables and supplementary material", "journal-ref": "Annals of Epidemiology, 2018, 28(11), 821-828", "doi": "10.1016/j.annepidem.2018.09.001", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Variables in epidemiological observational studies are commonly\nsubject to measurement error and misclassification, but the impact of such\nerrors is frequently not appreciated or ignored. As part of the STRengthening\nAnalytical Thinking for Observational Studies (STRATOS) Initiative, a Task\nGroup on measurement error and misclassification (TG4) seeks to describe the\nscope of this problem and the analysis methods currently in use to address\nmeasurement error. Methods: TG4 conducted a literature survey of four types of\nresearch studies that are typically impacted by exposure measurement error: 1)\ndietary intake cohort studies, 2) dietary intake population surveys, 3)\nphysical activity cohort studies, and 4) air pollution cohort studies. The\nsurvey was conducted to understand current practice for acknowledging and\naddressing measurement error. Results: The survey revealed that while\nresearchers were generally aware that measurement error affected their studies,\nvery few adjusted their analysis for the error. Most articles provided\nincomplete discussion of the potential effects of measurement error on their\nresults. Regression calibration was the most widely used method of adjustment.\nConclusions: Even in areas of epidemiology where measurement error is a known\nproblem, the dominant current practice is to ignore errors in analyses. Methods\nto correct for measurement error are available but require additional data to\ninform the error structure. There is a great need to incorporate such data\ncollection within study designs and improve the analytical approach. Increased\nefforts by investigators, editors and reviewers are also needed to improve\npresentation of research when data are subject to error.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 16:03:10 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Shaw", "Pamela A.", "", "on behalf\n  of the Measurement Error and Misclassification topic group"], ["Deffner", "Veronika", "", "on behalf\n  of the Measurement Error and Misclassification topic group"], ["Keogh", "Ruth H.", "", "on behalf\n  of the Measurement Error and Misclassification topic group"], ["Tooze", "Janet A.", "", "on behalf\n  of the Measurement Error and Misclassification topic group"], ["Dodd", "Kevin W.", "", "on behalf\n  of the Measurement Error and Misclassification topic group"], ["K\u00fcchenhoff", "Helmut", "", "on behalf\n  of the Measurement Error and Misclassification topic group"], ["Kipnis", "Victor", "", "on behalf\n  of the Measurement Error and Misclassification topic group"], ["Freedman", "Laurence S.", "", "on behalf\n  of the Measurement Error and Misclassification topic group"]]}, {"id": "1802.10570", "submitter": "Thomai Tsiftsi", "authors": "Thomai Tsiftsi", "title": "Statistical shape analysis in a Bayesian framework for shapes in two and\n  three dimensions", "comments": "6 pages, 1 figure", "journal-ref": "In proceedings 31st International Workshop on Statistical\n  Modelling, 4-8 July 2016, Rennes, France. Amsterdam: Statistical Modelling\n  Society, pp. 309-314", "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe a novel shape classification method which is\nembedded in the Bayesian paradigm. We discuss the modelling and the resulting\nshape classification algorithm for two and three dimensional data shapes. We\nconclude by evaluating the efficiency and efficacy of the proposed algorithm on\nthe Kimia shape database for the two dimensional case.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 18:20:12 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Tsiftsi", "Thomai", ""]]}]