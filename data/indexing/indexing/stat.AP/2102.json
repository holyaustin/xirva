[{"id": "2102.00447", "submitter": "Anuradha Singh Ms", "authors": "A. Singh, A. Forcina, K. Muniyoor", "title": "Social Mobility in India", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Rapid rise in income inequality in India is a serious concern. While the\nemphasis is on inclusive growth, it seems difficult to tackle the problem\nwithout looking at the intricacies of the problem. Social mobility is one such\nimportant tool which helps in reaching the cause of the problem and focuses on\nbringing long term equality in the country. The purpose of this study is to\nexamine the role of social background and education attainment in generating\noccupation mobility in the country. By applying an extended version of the RC\nassociation model to 68th round (2011-12) of the Employment and Unemployment\nSurvey by the National Sample Survey Office of India, we found that the role of\neducation is not important in generating occupation mobility in India, while\nsocial background plays a critical role in determining one's occupation. This\nstudy successfully highlights the strong intergenerational occupation\nimmobility in the country and also the need to focus on education. In this\nregard, further studies are needed to uncover other crucial factors limiting\nthe growth of individuals in the country.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 13:18:36 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Singh", "A.", ""], ["Forcina", "A.", ""], ["Muniyoor", "K.", ""]]}, {"id": "2102.00461", "submitter": "Mariana Almeida", "authors": "Bruno Jardim and Ricardo Rei and Mariana S. C. Almeida", "title": "Multilingual Email Zoning", "comments": "Accepted at EACL 2021 SRW\n  (https://sites.google.com/view/eaclsrw2021/home); 6 pages with 2 Figures and\n  8 Tables, plus references; Cleverly Multilingual Zoning Corpus available at\n  https://github.com/cleverly-ai/multilingual-email-zoning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.AP stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The segmentation of emails into functional zones (also dubbed email zoning)\nis a relevant preprocessing step for most NLP tasks that deal with emails.\nHowever, despite the multilingual character of emails and their applications,\nprevious literature regarding email zoning corpora and systems was developed\nessentially for English.\n  In this paper, we analyse the existing email zoning corpora and propose a new\nmultilingual benchmark composed of 625 emails in Portuguese, Spanish and\nFrench. Moreover, we introduce OKAPI, the first multilingual email segmentation\nmodel based on a language agnostic sentence encoder. Besides generalizing well\nfor unseen languages, our model is competitive with current English benchmarks,\nand reached new state-of-the-art performances for domain adaptation tasks in\nEnglish.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 14:32:20 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2021 19:37:39 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Jardim", "Bruno", ""], ["Rei", "Ricardo", ""], ["Almeida", "Mariana S. C.", ""]]}, {"id": "2102.00517", "submitter": "Michael Gordon", "authors": "Michael Gordon, Domenico Viganola, Anna Dreber, Magnus Johannesson,\n  Thomas Pfeiffer", "title": "Predicting replicability -- analysis of survey and prediction market\n  data from large-scale forecasting projects", "comments": "19 Pages", "journal-ref": null, "doi": "10.1371/journal.pone.0248780", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The reproducibility of published research has become an important topic in\nscience policy. A number of large-scale replication projects have been\nconducted to gauge the overall reproducibility in specific academic fields.\nHere, we present an analysis of data from four studies which sought to forecast\nthe outcomes of replication projects in the social and behavioural sciences,\nusing human experts who participated in prediction markets and answered\nsurveys. Because the number of findings replicated and predicted in each\nindividual study was small, pooling the data offers an opportunity to evaluate\nhypotheses regarding the performance of prediction markets and surveys at a\nhigher power. In total, peer beliefs were elicited for the replication outcomes\nof 103 published findings. We find there is information within the scientific\ncommunity about the replicability of scientific findings, and that both surveys\nand prediction markets can be used to elicit and aggregate this information.\nOur results show prediction markets can determine the outcomes of direct\nreplications with 73% accuracy (n=103). Both the prediction market prices and\nthe average survey responses are correlated with outcomes (0.581 and 0.564\nrespectively, both p < .001). We also found a significant relationship between\np-values of the original findings and replication outcomes. The dataset is made\navailable through the R package pooledmaRket and can be used to further study\ncommunity beliefs towards replications outcomes as elicited in the surveys and\nprediction markets.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 19:32:06 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Gordon", "Michael", ""], ["Viganola", "Domenico", ""], ["Dreber", "Anna", ""], ["Johannesson", "Magnus", ""], ["Pfeiffer", "Thomas", ""]]}, {"id": "2102.00564", "submitter": "Weiran Cai", "authors": "Weiran Cai, Belgin San-Akca, Jordan Snyder, Grayson Gordon, Zeev Maoz,\n  Raissa M. D'Souza", "title": "Quantifying the Global Support Network for Non-State Armed Groups (NAGs)", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human history has been shaped by armed conflicts. Rather than large-scale\ninterstate wars, low-intensity attacks have been more prevalent in the\npost-World War era. These attacks are often carried out by non-state armed\ngroups (NAGs), which are supported by host states (HSs). We analyze the global\nbipartite network of NAG-HS support and its evolution over the period of\n1945-2010. We find striking parallels to ecological networks such as\nmutualistic and parasitic forms of support, and a nested and modular network\narchitecture. The nestedness emerges from preferential behaviors: highly\nconnected players are more likely to both gain and lose connections.\nLong-persisting major modules are identified, reflecting both regional and\ntrans-regional interests, which show significant turnover in their membership,\ncontrary to the transitory ones. Revealing this architecture further enables\nthe identification of actor's roles and provide insights for effective\nintervention strategies.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 23:59:24 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Cai", "Weiran", ""], ["San-Akca", "Belgin", ""], ["Snyder", "Jordan", ""], ["Gordon", "Grayson", ""], ["Maoz", "Zeev", ""], ["D'Souza", "Raissa M.", ""]]}, {"id": "2102.00574", "submitter": "Sahand Farhoodi", "authors": "Sahand Farhoodi, Uri Eden", "title": "The problem of perfect predictors in statistical spike train models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized Linear Models (GLMs) have been used extensively in statistical\nmodels of spike train data. However, the IRLS algorithm, which is often used to\nfit such models, can fail to converge in situations where response and\nnon-response can be separated by a single predictor or a linear combination of\nmultiple predictors. Such situations are likely to arise in many neural systems\ndue to properties such as refractoriness and incomplete sampling of the signals\nthat influence spiking. In this paper, we describe multiple classes of\napproaches to address this problem: Standard IRLS with a fixed iteration limit,\ncomputing the maximum likelihood solution in the limit, Bayesian estimation,\nregularization, change of basis, and modifying the search parameters. We\ndemonstrate a specific application of each of these methods to spiking data\nfrom rat somatosensory cortex and discuss the advantages and disadvantages of\neach. We also provide an example of a roadmap for selecting a method based on\nthe problem's particular analysis issues and scientific goals.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 00:38:31 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Farhoodi", "Sahand", ""], ["Eden", "Uri", ""]]}, {"id": "2102.00577", "submitter": "Robert Taggart", "authors": "Robert J. Taggart", "title": "Evaluation of point forecasts for extreme events using consistent\n  scoring functions", "comments": "17 pages, 6 figures, 4 tables. Changes in latest version are all\n  minor: Corrected minor typos in Table 4. Replaced $I_0$ with $I_0 \\times I_0$\n  in three locations (pp. 7,8,15). Updated Strategy in Option 1 (p. 4) so that\n  it is optimal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a method for comparing point forecasts in a region of interest,\nsuch as the tails or centre of a variable's range. This method cannot be\nhedged, in contrast to conditionally selecting events to evaluate and then\nusing a scoring function that would have been consistent (or proper) prior to\nevent selection. Our method also gives decompositions of scoring functions that\nare consistent for the mean or a particular quantile or expectile. Each member\nof each decomposition is itself a consistent scoring function that emphasises\nperformance over a selected region of the variable's range. The score of each\nmember of the decomposition has a natural interpretation rooted in optimal\ndecision theory. It is the weighted average of economic regret over user\ndecision thresholds, where the weight emphasises those decision thresholds in\nthe corresponding region of interest.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 00:54:56 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 10:02:36 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Taggart", "Robert J.", ""]]}, {"id": "2102.00687", "submitter": "Kota Ogasawara", "authors": "Kota Ogasawara, Erika Igarashi", "title": "The Impacts of the Gender Imbalance on Marriage and Birth: Evidence from\n  World War II in Japan", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study uses the unprecedented changes in the sex ratio due to the losses\nof men during World War II to identify the impacts of the gender imbalance on\nmarriage market and birth outcomes in Japan. Using newly digitized census-based\nhistorical statistics, we find evidence that men had a stronger bargaining\nposition in the marriage market and intra-household fertility decisions than\nwomen. Under relative male scarcity, while people, especially younger people,\nwere more likely to marry and divorce, widowed women were less likely to\nremarry than widowed men. We also find that women's bargaining position in the\nmarriage market might not have improved throughout the 1950s. Given the\ninstitutional changes in the abortion law after the war, marital fertility and\nstillbirth rates increased in the areas that suffered relative male scarcity.\nOur result on out-of-wedlock births indicates that the theoretical prediction\nof intra-household bargaining is considered to be robust in an economy in which\nmarital fertility is dominant.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 08:03:28 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Ogasawara", "Kota", ""], ["Igarashi", "Erika", ""]]}, {"id": "2102.00884", "submitter": "Zak Varty", "authors": "Zak Varty, Jonathan A. Tawn, Peter M. Atkinson and Stijn Bierman", "title": "Inference for extreme earthquake magnitudes accounting for a\n  time-varying measurement process", "comments": "Submitted to the Annals of Applied Statistics. Main text: 20 pages,\n  11 figures. Supplement: 8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Investment in measuring a process more completely or accurately is only\nuseful if these improvements can be utilised during modelling and inference. We\nconsider how improvements to data quality over time can be incorporated when\nselecting a modelling threshold and in the subsequent inference of an extreme\nvalue analysis. Motivated by earthquake catalogues, we consider variable data\nquality in the form of rounded and incompletely observed data. We develop an\napproach to select a time-varying modelling threshold that makes best use of\nthe available data, accounting for uncertainty in the magnitude model and for\nthe rounding of observations. We show the benefits of the proposed approach on\nsimulated data and apply the method to a catalogue of earthquakes induced by\ngas extraction in the Netherlands. This more than doubles the usable catalogue\nsize and greatly increases the precision of high magnitude quantile estimates.\nThis has important consequences for the design and cost of earthquake defences.\nFor the first time, we find compelling data-driven evidence against the\napplicability of the Gutenberg-Richer law to these earthquakes. Furthermore,\nour approach to automated threshold selection appears to have much potential\nfor generic applications of extreme value methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 14:49:43 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Varty", "Zak", ""], ["Tawn", "Jonathan A.", ""], ["Atkinson", "Peter M.", ""], ["Bierman", "Stijn", ""]]}, {"id": "2102.01135", "submitter": "Kristian Lum", "authors": "Kristian Lum, David B. Dunson, James Johndrow", "title": "Closer than they appear: A Bayesian perspective on individual-level\n  heterogeneity in risk assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Risk assessment instruments are used across the criminal justice system to\nestimate the probability of some future behavior given covariates. The\nestimated probabilities are then used in making decisions at the individual\nlevel. In the past, there has been controversy about whether the probabilities\nderived from group-level calculations can meaningfully be applied to\nindividuals. Using Bayesian hierarchical models applied to a large longitudinal\ndataset from the court system in the state of Kentucky, we analyze variation in\nindividual-level probabilities of failing to appear for court and the extent to\nwhich it is captured by covariates. We find that individuals within the same\nrisk group vary widely in their probability of the outcome. In practice, this\nmeans that allocating individuals to risk groups based on standard approaches\nto risk assessment, in large part, results in creating distinctions among\nindividuals who are not meaningfully different in terms of their likelihood of\nthe outcome. This is because uncertainty about the probability that any\nparticular individual will fail to appear is large relative to the difference\nin average probabilities among any reasonable set of risk groups.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 20:04:41 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Lum", "Kristian", ""], ["Dunson", "David B.", ""], ["Johndrow", "James", ""]]}, {"id": "2102.01141", "submitter": "Huang Huang", "authors": "Huang Huang and Stefano Castruccio and Marc G. Genton", "title": "Forecasting High-Frequency Spatio-Temporal Wind Power with Dimensionally\n  Reduced Echo State Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fast and accurate hourly forecasts of wind speed and power are crucial in\nquantifying and planning the energy budget in the electric grid. Modeling wind\nat a high resolution brings forth considerable challenges given its turbulent\nand highly nonlinear dynamics. In developing countries where wind farms over a\nlarge domain are currently under construction or consideration, this is even\nmore challenging given the necessity of modeling wind over space as well. In\nthis work, we propose a machine learning approach to model the nonlinear hourly\nwind dynamics in Saudi Arabia with a domain-specific choice of knots to reduce\nthe spatial dimensionality. Our results show that for locations highlighted as\nwind abundant by a previous work, our approach results in a 11% improvement in\nthe two-hours-ahead forecasted power against operational standards in the wind\nenergy sector, yielding a saving of nearly one million US dollars over a year\nunder current market prices in Saudi Arabia.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 20:19:50 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Huang", "Huang", ""], ["Castruccio", "Stefano", ""], ["Genton", "Marc G.", ""]]}, {"id": "2102.01144", "submitter": "Han Lin Shang", "authors": "Han Lin Shang", "title": "Double bootstrapping for visualising the distribution of descriptive\n  statistics of functional data", "comments": "22 pages, 9 figures, 1 table, to appear at the Journal of Statistical\n  Computation and Simulation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a double bootstrap procedure for reducing coverage error in the\nconfidence intervals of descriptive statistics for independent and identically\ndistributed functional data. Through a series of Monte Carlo simulations, we\ncompare the finite sample performance of single and double bootstrap procedures\nfor estimating the distribution of descriptive statistics for independent and\nidentically distributed functional data. At the cost of longer computational\ntime, the double bootstrap with the same bootstrap method reduces confidence\nlevel error and provides improved coverage accuracy than the single bootstrap.\nIllustrated by a Canadian weather station data set, the double bootstrap\nprocedure presents a tool for visualising the distribution of the descriptive\nstatistics for the functional data.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 20:22:43 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Shang", "Han Lin", ""]]}, {"id": "2102.01147", "submitter": "Kai Zhang", "authors": "Kai Zhang, Siddharth Karanth, Bela Patel, Robert Murphy, Xiaoqian\n  Jiang", "title": "Real-time Prediction for Mechanical Ventilation in COVID-19 Patients\n  using A Multi-task Gaussian Process Multi-objective Self-attention Network", "comments": "In review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a robust in-time predictor for in-hospital COVID-19 patient's\nprobability of requiring mechanical ventilation. A challenge in the risk\nprediction for COVID-19 patients lies in the great variability and irregular\nsampling of patient's vitals and labs observed in the clinical setting.\nExisting methods have strong limitations in handling time-dependent features'\ncomplex dynamics, either oversimplifying temporal data with summary statistics\nthat lose information or over-engineering features that lead to less robust\noutcomes. We propose a novel in-time risk trajectory predictive model to handle\nthe irregular sampling rate in the data, which follows the dynamics of risk of\nperforming mechanical ventilation for individual patients. The model\nincorporates the Multi-task Gaussian Process using observed values to learn the\nposterior joint multi-variant conditional probability and infer the missing\nvalues on a unified time grid. The temporal imputed data is fed into a\nmulti-objective self-attention network for the prediction task. A novel\npositional encoding layer is proposed and added to the network for producing\nin-time predictions. The positional layer outputs a risk score at each\nuser-defined time point during the entire hospital stay of an inpatient. We\nframe the prediction task into a multi-objective learning framework, and the\nrisk scores at all time points are optimized altogether, which adds robustness\nand consistency to the risk score trajectory prediction. Our experimental\nevaluation on a large database with nationwide in-hospital patients with\nCOVID-19 also demonstrates that it improved the state-of-the-art performance in\nterms of AUC (Area Under the receiver operating characteristic Curve) and AUPRC\n(Area Under the Precision-Recall Curve) performance metrics, especially at\nearly times after hospital admission.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 20:35:22 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Zhang", "Kai", ""], ["Karanth", "Siddharth", ""], ["Patel", "Bela", ""], ["Murphy", "Robert", ""], ["Jiang", "Xiaoqian", ""]]}, {"id": "2102.01599", "submitter": "Emanuele Aliverti", "authors": "Emanuele Aliverti and Stefano Mazzuco and Bruno Scarpa", "title": "Dynamic modeling of mortality via mixtures of skewed distribution\n  functions", "comments": "24 pages, 6 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There has been growing interest on forecasting mortality. In this article, we\npropose a novel dynamic Bayesian approach for modeling and forecasting the\nage-at-death distribution, focusing on a three-components mixture of a Dirac\nmass, a Gaussian distribution and a Skew-Normal distribution. According to the\nspecified model, the age-at-death distribution is characterized via seven\nparameters corresponding to the main aspects of infant, adult and old-age\nmortality. The proposed approach focuses on coherent modeling of multiple\ncountries, and following a Bayesian approach to inference we allow to borrow\ninformation across populations and to shrink parameters towards a common mean\nlevel, implicitly penalizing diverging scenarios. Dynamic modeling across years\nis induced trough an hierarchical dynamic prior distribution that allows to\ncharacterize the temporal evolution of each mortality component and to forecast\nthe age-at-death distribution. Empirical results on multiple countries indicate\nthat the proposed approach outperforms popular methods for forecasting\nmortality, providing interpretable insights on the evolution of mortality.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 16:51:21 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Aliverti", "Emanuele", ""], ["Mazzuco", "Stefano", ""], ["Scarpa", "Bruno", ""]]}, {"id": "2102.01604", "submitter": "Ya\\\"el Balbastre", "authors": "Yael Balbastre, Mikael Brudfors, Michela Azzarito, Christian Lambert,\n  Martina F. Callaghan, John Ashburner", "title": "Model-based multi-parameter mapping", "comments": "20 pages, 6 figures, accepted at Medical Image Analysis", "journal-ref": null, "doi": "10.1016/j.media.2021.102149", "report-no": null, "categories": "cs.CV eess.IV stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Quantitative MR imaging is increasingly favoured for its richer information\ncontent and standardised measures. However, computing quantitative parameter\nmaps, such as those encoding longitudinal relaxation rate (R1), apparent\ntransverse relaxation rate (R2*) or magnetisation-transfer saturation (MTsat),\ninvolves inverting a highly non-linear function. Many methods for deriving\nparameter maps assume perfect measurements and do not consider how noise is\npropagated through the estimation procedure, resulting in needlessly noisy\nmaps. Instead, we propose a probabilistic generative (forward) model of the\nentire dataset, which is formulated and inverted to jointly recover (log)\nparameter maps with a well-defined probabilistic interpretation (e.g., maximum\nlikelihood or maximum a posteriori). The second order optimisation we propose\nfor model fitting achieves rapid and stable convergence thanks to a novel\napproximate Hessian. We demonstrate the utility of our flexible framework in\nthe context of recovering more accurate maps from data acquired using the\npopular multi-parameter mapping protocol. We also show how to incorporate a\njoint total variation prior to further decrease the noise in the maps, noting\nthat the probabilistic formulation allows the uncertainty on the recovered\nparameter maps to be estimated. Our implementation uses a PyTorch backend and\nbenefits from GPU acceleration. It is available at\nhttps://github.com/balbasty/nitorch.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 17:00:11 GMT"}, {"version": "v2", "created": "Thu, 24 Jun 2021 19:02:54 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Balbastre", "Yael", ""], ["Brudfors", "Mikael", ""], ["Azzarito", "Michela", ""], ["Lambert", "Christian", ""], ["Callaghan", "Martina F.", ""], ["Ashburner", "John", ""]]}, {"id": "2102.01612", "submitter": "Dorota M{\\l}ynarczyk", "authors": "Dorota M{\\l}ynarczyk (1), Carmen Armero (2), Virgilio G\\'omez-Rubio\n  (3), Pedro Puig (1 and 4) ((1) Universitat Aut\\`onoma de Barcelona, (2)\n  Universitat de Val\\`encia, (3) Universidad de Castilla-La Mancha, (4) Centre\n  de Recerca Matem\\`atica (CRM))", "title": "Bayesian analysis of population health data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The analysis of population-wide datasets can provide insight on the health\nstatus of large populations so that public health officials can make\ndata-driven decisions. The analysis of such datasets often requires highly\nparameterized models with different types of fixed and randoms effects to\naccount for risk factors, spatial and temporal variations, multilevel effects\nand other sources on uncertainty. To illustrate the potential of Bayesian\nhierarchical models, a dataset of about 500 000 inhabitants released by the\nPolish National Health Fund containing information about ischemic stroke\nincidence for a 2-year period is analyzed using different types of models.\nSpatial logistic regression and survival models are considered for analyzing\nthe individual probabilities of stroke and the times to the occurrence of an\nischemic stroke event. Demographic and socioeconomic variables as well as drug\nprescription information are available at an individual level. Spatial\nvariation is considered by means of region-level random effects.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 17:14:44 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["M\u0142ynarczyk", "Dorota", "", "1 and 4"], ["Armero", "Carmen", "", "1 and 4"], ["G\u00f3mez-Rubio", "Virgilio", "", "1 and 4"], ["Puig", "Pedro", "", "1 and 4"]]}, {"id": "2102.01629", "submitter": "Dario Menasce", "authors": "Gianluca Bonifazi, Luca Lista, Dario Menasce, Mauro Mezzetto, Alberto\n  Oliva, Daniele Pedrini, Roberto Spighi, Antonio Zoccoli", "title": "A statistical analysis of death rates in Italy for the years 2015-2020\n  and a comparison with the casualties reported for the COVID-19 pandemic", "comments": "16 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We analyze the data about casualties in Italy in the period 01/01/2015 to\n30/09/2020 released by the Italian National Institute of Statistics (ISTAT).\nThe data exhibit a clear sinusoidal behavior, whose fit allows for a robust\nsubtraction of the baseline trend of casualties in Italy, with a surplus of\nmortality in correspondence to the flu epidemics in winter and to the hottest\nperiods in summer. While these peaks are symmetric in shape, the peak in\ncoincidence with the COVID-19 pandemics is asymmetric and more pronounced. We\nfit the former with a Gaussian function and the latter with a Gompertz\nfunction, in order to quantify number of casualties, the duration and the\nposition of all causes of excess deaths. The overall quality of the fit to the\ndata turns out to be very good. We discuss the trend of casualties in Italy by\ndifferent classes of ages and for the different genders. We finally compare the\ndata-subtracted casualties as reported by ISTAT with those reported by the\nItalian Department for Civil Protection (DPC) relative to the deaths directly\nattributed to COVID-19, and we discuss the differences.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 17:38:26 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 18:10:26 GMT"}, {"version": "v3", "created": "Mon, 8 Feb 2021 08:15:11 GMT"}, {"version": "v4", "created": "Thu, 11 Mar 2021 09:18:41 GMT"}, {"version": "v5", "created": "Tue, 23 Mar 2021 10:00:46 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Bonifazi", "Gianluca", ""], ["Lista", "Luca", ""], ["Menasce", "Dario", ""], ["Mezzetto", "Mauro", ""], ["Oliva", "Alberto", ""], ["Pedrini", "Daniele", ""], ["Spighi", "Roberto", ""], ["Zoccoli", "Antonio", ""]]}, {"id": "2102.01636", "submitter": "Li Sun", "authors": "Alain Hecq, Li Sun", "title": "Adaptive Random Bandwidth for Inference in CAViaR Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper investigates the size performance of Wald tests for CAViaR models\n(Engle and Manganelli, 2004). We find that the usual estimation strategy on\ntest statistics yields inaccuracies. Indeed, we show that existing density\nestimation methods cannot adapt to the time-variation in the conditional\nprobability densities of CAViaR models. Consequently, we develop a method\ncalled adaptive random bandwidth which can approximate time-varying conditional\nprobability densities robustly for inference testing on CAViaR models based on\nthe asymptotic normality of the model parameter estimator. This proposed method\nalso avoids the problem of choosing an optimal bandwidth in estimating\nprobability densities, and can be extended to multivariate quantile regressions\nstraightforward.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 17:46:02 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Hecq", "Alain", ""], ["Sun", "Li", ""]]}, {"id": "2102.01740", "submitter": "Yili Hong", "authors": "Yili Hong and Jie Min and Caleb B. King and William Q. Meeker", "title": "Reliability Analysis of Artificial Intelligence Systems Using Recurrent\n  Events Data from Autonomous Vehicles", "comments": "30 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence (AI) systems have become increasingly common and the\ntrend will continue. Examples of AI systems include autonomous vehicles (AV),\ncomputer vision, natural language processing, and AI medical experts. To allow\nfor safe and effective deployment of AI systems, the reliability of such\nsystems needs to be assessed. Traditionally, reliability assessment is based on\nreliability test data and the subsequent statistical modeling and analysis. The\navailability of reliability data for AI systems, however, is limited because\nsuch data are typically sensitive and proprietary. The California Department of\nMotor Vehicles (DMV) oversees and regulates an AV testing program, in which\nmany AV manufacturers are conducting AV road tests. Manufacturers participating\nin the program are required to report recurrent disengagement events to\nCalifornia DMV. This information is being made available to the public. In this\npaper, we use recurrent disengagement events as a representation of the\nreliability of the AI system in AV, and propose a statistical framework for\nmodeling and analyzing the recurrent events data from AV driving tests. We use\ntraditional parametric models in software reliability and propose a new\nnonparametric model based on monotonic splines to describe the event process.\nWe develop inference procedures for selecting the best models, quantifying\nuncertainty, and testing heterogeneity in the event process. We then analyze\nthe recurrent events data from four AV manufacturers, and make inferences on\nthe reliability of the AI systems in AV. We also describe how the proposed\nanalysis can be applied to assess the reliability of other AI systems.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 20:25:23 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Hong", "Yili", ""], ["Min", "Jie", ""], ["King", "Caleb B.", ""], ["Meeker", "William Q.", ""]]}, {"id": "2102.01784", "submitter": "Scott Bruce", "authors": "Pramita Bagchi and Scott A. Bruce", "title": "Adaptive Frequency Band Analysis for Functional Time Series", "comments": "33 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The frequency-domain properties of nonstationary functional time series often\ncontain valuable information. These properties are characterized through its\ntime-varying power spectrum. Practitioners seeking low-dimensional summary\nmeasures of the power spectrum often partition frequencies into bands and\ncreate collapsed measures of power within bands. However, standard frequency\nbands have largely been developed through manual inspection of time series data\nand may not adequately summarize power spectra. In this article, we propose a\nframework for adaptive frequency band estimation of nonstationary functional\ntime series that optimally summarizes the time-varying dynamics of the series.\nWe develop a scan statistic and search algorithm to detect changes in the\nfrequency domain. We establish theoretical properties of this framework and\ndevelop a computationally-efficient implementation. The validity of our method\nis also justified through numerous simulation studies and an application to\nanalyzing electroencephalogram data in participants alternating between eyes\nopen and eyes closed conditions.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 22:33:37 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 21:19:25 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Bagchi", "Pramita", ""], ["Bruce", "Scott A.", ""]]}, {"id": "2102.01803", "submitter": "Ganchao Wei", "authors": "Ganchao Wei, Ian H. Stevenson", "title": "Tracking fast and slow changes in synaptic weights from simultaneously\n  observed pre- and postsynaptic spiking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP stat.CO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Synapses change on multiple timescales, ranging from milliseconds to minutes,\ndue to a combination of both short- and long-term plasticity. Here we develop\nan extension of the common Generalized Linear Model to infer both short- and\nlong-term changes in the coupling between a pre- and post-synaptic neuron based\non observed spiking activity. We model short-term synaptic plasticity using\nadditive effects that depend on the presynaptic spike timing, and we model\nlong-term changes in both synaptic weight and baseline firing rate using point\nprocess adaptive smoothing. Using simulations, we first show that this model\ncan accurately recover time-varying synaptic weights 1) for both depressing and\nfacilitating synapses, 2) with a variety of long-term changes (including\nrealistic changes, such as due to STDP), 3) with a range of pre- and\npost-synaptic firing rates, and 4) for both excitatory and inhibitory synapses.\nWe then apply our model to two experimentally recorded putative synaptic\nconnections. We find that simultaneously tracking fast changes in synaptic\nweights, slow changes in synaptic weights, and unexplained variations in\nbaseline firing is essential. Omitting any one of these factors can lead to\nspurious inferences for the others. Altogether, this model provides a flexible\nframework for tracking short- and long-term variation in spike transmission.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 23:54:00 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 02:12:41 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Wei", "Ganchao", ""], ["Stevenson", "Ian H.", ""]]}, {"id": "2102.01821", "submitter": "Ian Frankenburg", "authors": "Ian Frankenburg and Sudipto Banerjee", "title": "A Compartment Model of Human Mobility and Early Covid-19 Dynamics in NYC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we build a mechanistic system to understand the relation\nbetween a reduction in human mobility and Covid-19 spread dynamics within New\nYork City. To this end, we propose a multivariate compartmental model that\njointly models smartphone mobility data and case counts during the first 90\ndays of the epidemic. Parameter calibration is achieved through the formulation\nof a general Bayesian hierarchical model to provide uncertainty quantification\nof resulting estimates. The open-source probabilistic programming language Stan\nis used for the requisite computation. Through sensitivity analysis and\nout-of-sample forecasting, we find our simple and interpretable model provides\nevidence that reductions in human mobility altered case dynamics.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 01:00:00 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 19:59:27 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Frankenburg", "Ian", ""], ["Banerjee", "Sudipto", ""]]}, {"id": "2102.01830", "submitter": "Ricardo Di Pasquale", "authors": "Ricardo Di Pasquale and Javier Marenco", "title": "Machine learning for improving performance in an evolutionary algorithm\n  for minimum path with uncertain costs given by massively simulated scenarios", "comments": "6 pages, 7 figures, IEEE IJCAI DSO 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work we introduce an implementation for which machine learning\ntechniques helped improve the overall performance of an evolutionary algorithm\nfor an optimization problem, namely a variation of robust minimum-cost path in\ngraphs. In this big data optimization problem, a path achieving a good cost in\nmost scenarios from an available set of scenarios (generated by a simulation\nprocess) must be obtained. The most expensive task of our evolutionary\nalgorithm, in terms of computational resources, is the evaluation of candidate\npaths: the fitness function must calculate the cost of the candidate path in\nevery generated scenario. Given the large number of scenarios, this task must\nbe implemented in a distributed environment. We implemented gradient boosting\ndecision trees to classify candidate paths in order to identify good\ncandidates. The cost of the not-so-good candidates is simply forecasted. We\nstudied the training process, gain performance, accuracy, and other variables.\nOur computational experiments show that the computational performance was\nsignificantly improved at the expense of a limited loss of accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 01:38:35 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Di Pasquale", "Ricardo", ""], ["Marenco", "Javier", ""]]}, {"id": "2102.01844", "submitter": "Yanrong Yang", "authors": "Lingyu He, Fei Huang, Jianjie Shi, Yanrong Yang", "title": "Mortality Forecasting using Factor Models: Time-varying or\n  Time-invariant Factor Loadings?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Many existing mortality models follow the framework of classical factor\nmodels, such as the Lee-Carter model and its variants. Latent common factors in\nfactor models are defined as time-related mortality indices (such as $\\kappa_t$\nin the Lee-Carter model). Factor loadings, which capture the linear\nrelationship between age variables and latent common factors (such as $\\beta_x$\nin the Lee-Carter model), are assumed to be time-invariant in the classical\nframework. This assumption is usually too restrictive in reality as mortality\ndatasets typically span a long period of time. Driving forces such as medical\nimprovement of certain diseases, environmental changes and technological\nprogress may significantly influence the relationship of different variables.\nIn this paper, we first develop a factor model with time-varying factor\nloadings (time-varying factor model) as an extension of the classical factor\nmodel for mortality modelling. Two forecasting methods to extrapolate the\nfactor loadings, the local regression method and the naive method, are proposed\nfor the time-varying factor model. From the empirical data analysis, we find\nthat the new model can capture the empirical feature of time-varying factor\nloadings and improve mortality forecasting over different horizons and\ncountries. Further, we propose a novel approach based on change point analysis\nto estimate the optimal `boundary' between short-term and long-term\nforecasting, which is favoured by the local linear regression and naive method,\nrespectively. Additionally, simulation studies are provided to show the\nperformance of the time-varying factor model under various scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 02:28:16 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["He", "Lingyu", ""], ["Huang", "Fei", ""], ["Shi", "Jianjie", ""], ["Yang", "Yanrong", ""]]}, {"id": "2102.01846", "submitter": "Benjamin Williams", "authors": "Benjamin Williams, Will Palmquist, Ryan Elmore", "title": "Simulation-Based Decision Making in the NFL using NFLSimulatoR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we introduce an R software package for simulating plays and\ndrives using play-by-play data from the National Football League. The\nsimulations are generated by sampling play-by-play data from previous football\nseasons.The sampling procedure adds statistical rigor to any decisions or\ninferences arising from examining the simulations. We highlight that the\npackage is particularly useful as a data-driven tool for evaluating potential\nin-game strategies or rule changes within the league. We demonstrate its\nutility by evaluating the oft-debated strategy of $\\textit{going for it}$ on\nfourth down and investigating whether or not teams should pass more than the\ncurrent standard.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 02:44:01 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Williams", "Benjamin", ""], ["Palmquist", "Will", ""], ["Elmore", "Ryan", ""]]}, {"id": "2102.01892", "submitter": "Noel Cressie", "authors": "Noel Cressie", "title": "A few statistical principles for data science", "comments": "19 pages; written for a special issue (festschrift) of the Australian\n  and New Zealand Journal of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In any other circumstance, it might make sense to define the extent of the\nterrain (Data Science) first, and then locate and describe the landmarks\n(Principles). But this data revolution we are experiencing defies a cadastral\nsurvey. Areas are continually being annexed into Data Science. For example,\nbiometrics was traditionally statistics for agriculture in all its forms but\nnow, in Data Science, it means the study of characteristics that can be used to\nidentify an individual. Examples of non-intrusive measurements include height,\nweight, fingerprints, retina scan, voice, photograph/video (facial landmarks\nand facial expressions), and gait. A multivariate analysis of such data would\nbe a complex project for a statistician, but a software engineer might appear\nto have no trouble with it at all. In any applied-statistics project, the\nstatistician worries about uncertainty and quantifies it by modelling data as\nrealisations generated from a probability space. Another approach to\nuncertainty quantification is to find similar data sets, and then use the\nvariability of results between these data sets to capture the uncertainty. Both\napproaches allow 'error bars' to be put on estimates obtained from the original\ndata set, although the interpretations are different. A third approach, that\nconcentrates on giving a single answer and gives up on uncertainty\nquantification, could be considered as Data Engineering, although it has staked\na claim in the Data Science terrain. This article presents a few (actually\nnine) statistical principles for data scientists that have helped me, and\ncontinue to help me, when I work on complex interdisciplinary projects.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 06:28:03 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Cressie", "Noel", ""]]}, {"id": "2102.01935", "submitter": "Wen Wei Loh", "authors": "Wen Wei Loh, Stijn Vansteelandt", "title": "Sensitivity Analysis for Unmeasured Confounding via Effect Extrapolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inferring the causal effect of a non-randomly assigned exposure on an outcome\nrequires adjusting for common causes of the exposure and outcome to avoid\nbiased conclusions. Notwithstanding the efforts investigators routinely make to\nmeasure and adjust for such common causes (or confounders), some confounders\ntypically remain unmeasured, raising the prospect of biased inference in\nobservational studies. Therefore, it is crucial that investigators can\npractically assess their substantive conclusions' relative (in)sensitivity to\npotential unmeasured confounding. In this article, we propose a sensitivity\nanalysis strategy that is informed by the stability of the exposure effect over\ndifferent, well-chosen subsets of the measured confounders. The proposal\nentails first approximating the process for recording confounders to learn\nabout how the effect is potentially affected by varying amounts of unmeasured\nconfounding, then extrapolating to the effect had hypothetical unmeasured\nconfounders been additionally adjusted for. A large set of measured confounders\ncan thus be exploited to provide insight into the likely presence of unmeasured\nconfounding bias, albeit under an assumption about how data on the confounders\nare recorded. The proposal's ability to reveal the true effect and ensure valid\ninference after extrapolation is empirically compared with existing methods\nusing simulation studies. We demonstrate the procedure using two different\npublicly available datasets commonly used for causal inference.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 08:35:46 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Loh", "Wen Wei", ""], ["Vansteelandt", "Stijn", ""]]}, {"id": "2102.01946", "submitter": "Jan Gertheiss", "authors": "Jan Gertheiss, Fabian Scheipl, Tina Lauer, Harald Ehrhardt", "title": "Statistical Inference for Ordinal Predictors in Generalized Linear and\n  Additive Models with Application to Bronchopulmonary Dysplasia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Discrete but ordered covariates are quite common in applied statistics, and\nsome regularized fitting procedures have been proposed for proper handling of\nordinal predictors in statistical modeling. In this study, we show how\nquadratic penalties on adjacent dummy coefficients of ordinal predictors\nproposed in the literature can be incorporated in the framework of generalized\nadditive models, making tools for statistical inference developed there\navailable for ordinal predictors as well. Motivated by an application from\nneonatal medicine, we discuss whether results obtained when constructing\nconfidence intervals and testing significance of smooth terms in generalized\nadditive models are useful with ordinal predictors/penalties as well.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 08:54:10 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Gertheiss", "Jan", ""], ["Scheipl", "Fabian", ""], ["Lauer", "Tina", ""], ["Ehrhardt", "Harald", ""]]}, {"id": "2102.01950", "submitter": "Matthieu Simeoni", "authors": "Matthieu Simeoni and Paul Hurley", "title": "SiML: Sieved Maximum Likelihood for Array Signal Processing", "comments": "5 pages, 2 figures. Published in ICASSP 2021-2021 IEEE International\n  Conference on Acoustics, Speech and Signal Processing (ICASSP), scheduled for\n  6-11 June 2021 in Toronto, Ontario, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Maximum Likelihood (SML) is a popular direction of arrival (DOA)\nestimation technique in array signal processing. It is a parametric method that\njointly estimates signal and instrument noise by maximum likelihood, achieving\nexcellent statistical performance. Some drawbacks are the computational\noverhead as well as the limitation to a point-source data model with fewer\nsources than sensors. In this work, we propose a Sieved Maximum Likelihood\n(SiML) method. It uses a general functional data model, allowing an\nunrestricted number of arbitrarily-shaped sources to be recovered. To this end,\nwe leverage functional analysis tools and express the data in terms of an\ninfinite-dimensional sampling operator acting on a Gaussian random function. We\nshow that SiML is computationally more efficient than traditional SML,\nresilient to noise, and results in much better accuracy than spectral-based\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 09:01:06 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Simeoni", "Matthieu", ""], ["Hurley", "Paul", ""]]}, {"id": "2102.01952", "submitter": "Daniel Dinsdale", "authors": "Will G\\\"urp{\\i}nar-Morgan, Daniel Dinsdale, Joe Gallagher, Aditya\n  Cherukumudi and Patrick Lucey", "title": "You Cannot Do That Ben Stokes: Dynamically Predicting Shot Type in\n  Cricket Using a Personalized Deep Neural Network", "comments": "20 pages, 12 figures, accepted paper for the 2020 MIT Sloan Sports\n  Analytics Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to predict what shot a batsman will attempt given the type of\nball and match situation is both one of the most challenging and strategically\nimportant tasks in cricket.\n  The goal of the batsman is to score as many runs without being dismissed,\nwhilst for bowlers their goal is to stem the flow of runs and ideally to\ndismiss their opponent. Getting the best batsman vs bowler match-up is of\nparamount importance. For example, for the fielding team, the choice of bowler\nagainst the opposition star batsman could be the key difference between winning\nor losing. Therefore, the ability to have a predefined playbook (as in the NFL)\nwhich would allow a team to predict how best to set their fielders given the\ncontext of the game, the batsman they are bowling to and bowlers at their\ndisposal would give them a significant strategic advantage.\n  To this end, we present a personalized deep neural network approach which can\npredict the probabilities of where a specific batsman will hit a specific\nbowler and bowl type, in a specific game-scenario. We demonstrate how our\npersonalized predictions provide vital information to inform the\ndecision-making of coaches and captains, both in terms of pre-match and in-game\ntactical choices, using the 2019 World Cup final between England and New\nZealand as a case study example.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 09:03:59 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["G\u00fcrp\u0131nar-Morgan", "Will", ""], ["Dinsdale", "Daniel", ""], ["Gallagher", "Joe", ""], ["Cherukumudi", "Aditya", ""], ["Lucey", "Patrick", ""]]}, {"id": "2102.02025", "submitter": "Antonio Calcagn\\`i", "authors": "Antonio Calcagn\\`i", "title": "fIRTree: An Item Response Theory modeling of fuzzy rating data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this contribution we describe a novel procedure to represent fuzziness in\nrating scales in terms of fuzzy numbers. Following the rationale of fuzzy\nconversion scale, we adopted a two-step procedure based on a psychometric model\n(i.e., Item Response Theory-based tree) to represent the process of answer\nsurvey questions. This provides a coherent context where fuzzy numbers, and the\nrelated fuzziness, can be interpreted in terms of decision uncertainty that\nusually affects the rater's response process. We reported results from a\nsimulation study and an empirical application to highlight the characteristics\nand properties of the proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 11:59:50 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Calcagn\u00ec", "Antonio", ""]]}, {"id": "2102.02091", "submitter": "Subhankar Dutta", "authors": "Subhankar Dutta, Suchandan Kayal", "title": "Estimation of parameters of the logistic exponential distribution under\n  progressive type-I hybrid censored sample", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper addresses the problem of estimation of the model parameters of the\nlogistic exponential distribution based on progressive type-I hybrid censored\nsample. The maximum likelihood estimates are obtained and computed numerically\nusing Newton-Raphson method. Further, the Bayes estimates are derived under\nsquared error, LINEX and generalized entropy loss functions. Two types\n(independent and bivariate) of prior distributions are considered for the\npurpose of Bayesian estimation. It is seen that the Bayes estimates are not of\nexplicit forms.Thus, Lindley's approximation technique is employed to get\napproximate Bayes estimates. Interval estimates of the parameters based on\nnormal approximate of the maximum likelihood estimates and normal approximation\nof the log-transformed maximum likelihood estimates are constructed. The\nhighest posterior density credible intervals are obtained by using the\nimportance sampling method. Furthermore, numerical computations are reported to\nreview some of the results obtained in the paper. A real life dataset is\nconsidered for the purpose of illustrations.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 14:46:45 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Dutta", "Subhankar", ""], ["Kayal", "Suchandan", ""]]}, {"id": "2102.02111", "submitter": "Sandra Wankm\\\"uller", "authors": "Sandra Wankm\\\"uller", "title": "Neural Transfer Learning with Transformers for Social Science Text\n  Analysis", "comments": "67 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  During the last years, there have been substantial increases in the\nprediction performances of natural language processing models on text-based\nsupervised learning tasks. Especially deep learning models that are based on\nthe Transformer architecture (Vaswani et al., 2017) and are used in a transfer\nlearning setting have contributed to this development. As Transformer-based\nmodels for transfer learning have the potential to achieve higher prediction\naccuracies with relatively few training data instances, they are likely to\nbenefit social scientists that seek to have as accurate as possible text-based\nmeasures but only have limited resources for annotating training data. To\nenable social scientists to leverage these potential benefits for their\nresearch, this paper explains how these methods work, why they might be\nadvantageous, and what their limitations are. Additionally, three\nTransformer-based models for transfer learning, BERT (Devlin et al., 2019),\nRoBERTa (Liu et al., 2019), and the Longformer (Beltagy et al., 2020), are\ncompared to conventional machine learning algorithms on three social science\napplications. Across all evaluated tasks, textual styles, and training data set\nsizes, the conventional models are consistently outperformed by transfer\nlearning with Transformer-based models, thereby demonstrating the potential\nbenefits these models can bring to text-based social science research.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 15:41:20 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Wankm\u00fcller", "Sandra", ""]]}, {"id": "2102.02142", "submitter": "Paul Goldsmith-Pinkham", "authors": "Paul Goldsmith-Pinkham, Maxim Pinkovskiy, and Jacob Wallace", "title": "The Great Equalizer: Medicare and the Geography of Consumer Financial\n  Strain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use a five percent sample of Americans' credit bureau data, combined with\na regression discontinuity approach, to estimate the effect of universal health\ninsurance at age 65-when most Americans become eligible for Medicare-at the\nnational, state, and local level. We find a 30 percent reduction in debt\ncollections-and a two-thirds reduction in the geographic variation in\ncollections-with limited effects on other financial outcomes. The areas that\nexperienced larger reductions in collections debt at age 65 were concentrated\nin the Southern United States, and had higher shares of black residents, people\nwith disabilities, and for-profit hospitals.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 16:57:23 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Goldsmith-Pinkham", "Paul", ""], ["Pinkovskiy", "Maxim", ""], ["Wallace", "Jacob", ""]]}, {"id": "2102.02143", "submitter": "Paulo Hubert", "authors": "Paulo Hubert and Linilson Padovese", "title": "Estimating the radii of air bubbles in water using passive acoustic\n  monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The study of the acoustic emission of underwater gas bubbles is a subject of\nboth theoretical and applied interest, since it finds an important application\nin the development of acoustic monitoring tools for detection and\nquantification of underwater gas leakages. An underlying physical model is\nessential in the study of such emissions, but is not enough: also some\nstatistical procedure must be applied in order to deal with all uncertainties\n(including those caused by background noise). In this paper we take a\nprobabilistic (Bayesian) methodology which is well known in the statistical\nsignal analysis communitiy, and apply it to the problem of estimating the radii\nof air bubbles in water. We introduce the bubblegram, a feature extraction\ntechnique graphically similar to the traditional spectrogram but tailored to\nrespond only to pulse structures that correspond to a given physical model. We\ninvestigate the performance of the bubblegram and our model in general using\nlaboratory generated data.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 16:58:17 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Hubert", "Paulo", ""], ["Padovese", "Linilson", ""]]}, {"id": "2102.02160", "submitter": "Yoo Han", "authors": "Yoo Jeong Han", "title": "An Empirical Study on the Effects of the America Invents Act on Patent\n  Applications Owned by Small Businesses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper evaluates the heterogenous impacts of the America Invents Act of\n2011 (AIA) on patent applications for small and large businesses. Using data\ncollected from the United States Patent and Trademark Office and Google\nPatents, I compare how the probability of successfully overcoming an initial\nrejection is affected by the AIA for small- and large-business applicants,\nrespectively. This comparison is achieved by analyzing the data using a\ndifference-in-differences approach. Results suggest that after the enactment of\nthe AIA, small-business applicants were relatively favored when compared\nagainst large-business applicants. This effect is statistically significant and\nalso practically large.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 17:27:59 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Han", "Yoo Jeong", ""]]}, {"id": "2102.02295", "submitter": "Pavle Bo\\v{s}koski", "authors": "Pavle Bo\\v{s}koski and Matija Perne and Martina Rame\\v{s}a and Biljana\n  Mileva Boshkoska", "title": "Variational Bayes survival analysis for unemployment modelling", "comments": null, "journal-ref": null, "doi": "10.1016/j.knosys.2021.107335", "report-no": null, "categories": "stat.AP cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mathematical modelling of unemployment dynamics attempts to predict the\nprobability of a job seeker finding a job as a function of time. This is\ntypically achieved by using information in unemployment records. These records\nare right censored, making survival analysis a suitable approach for parameter\nestimation. The proposed model uses a deep artificial neural network (ANN) as a\nnon-linear hazard function. Through embedding, high-cardinality categorical\nfeatures are analysed efficiently. The posterior distribution of the ANN\nparameters are estimated using a variational Bayes method. The model is\nevaluated on a time-to-employment data set spanning from 2011 to 2020 provided\nby the Slovenian public employment service. It is used to determine the\nemployment probability over time for each individual on the record. Similar\nmodels could be applied to other questions with multi-dimensional,\nhigh-cardinality categorical data including censored records. Such data is\noften encountered in personal records, for example in medical records.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 21:06:54 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 08:41:08 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Bo\u0161koski", "Pavle", ""], ["Perne", "Matija", ""], ["Rame\u0161a", "Martina", ""], ["Boshkoska", "Biljana Mileva", ""]]}, {"id": "2102.02334", "submitter": "Dirk Douwes-Schultz", "authors": "Dirk Douwes-Schultz and Alexandra M. Schmidt", "title": "A Zero-State Coupled Markov Switching Poisson Model for Spatio-temporal\n  Infectious Disease Counts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatio-temporal counts of infectious disease cases often contain an excess of\nzeros. Existing zero inflated Poisson models applied to such data do not\nadequately capture the switching of the disease between periods of presence and\nabsence overtime. As an alternative, we develop a new zero-state coupled Markov\nswitching Poisson Model, under which the disease switches between periods of\npresence and absence in each area through a series of partially hidden\nnonhomogeneous Markov chains coupled between neighboring locations. When the\ndisease is present, an autoregressive Poisson model generates the cases with a\npossible 0 representing the disease being undetected. Bayesian inference and\nprediction is illustrated using spatio-temporal counts of dengue fever cases in\nRio de Janeiro, Brazil.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 23:29:06 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Douwes-Schultz", "Dirk", ""], ["Schmidt", "Alexandra M.", ""]]}, {"id": "2102.02365", "submitter": "Eld Emanuel Str\\\"om", "authors": "Jonas Kiessling, Emanuel Str\\\"om and Ra\\'ul Tempone", "title": "Wind Field Reconstruction with Adaptive Random Fourier Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate the use of spatial interpolation methods for reconstructing\nthe horizontal near-surface wind field given a sparse set of measurements. In\nparticular, random Fourier features is compared to a set of benchmark methods\nincluding Kriging and Inverse distance weighting. Random Fourier features is a\nlinear model $\\beta(\\pmb x) = \\sum_{k=1}^K \\beta_k e^{i\\omega_k \\pmb x}$\napproximating the velocity field, with frequencies $\\omega_k$ randomly sampled\nand amplitudes $\\beta_k$ trained to minimize a loss function. We include a\nphysically motivated divergence penalty term $|\\nabla \\cdot \\beta(\\pmb x)|^2$,\nas well as a penalty on the Sobolev norm. We derive a bound on the\ngeneralization error and derive a sampling density that minimizes the bound.\nFollowing (arXiv:2007.10683 [math.NA]), we devise an adaptive\nMetropolis-Hastings algorithm for sampling the frequencies of the optimal\ndistribution. In our experiments, our random Fourier features model outperforms\nthe benchmark models.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 01:42:08 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Kiessling", "Jonas", ""], ["Str\u00f6m", "Emanuel", ""], ["Tempone", "Ra\u00fal", ""]]}, {"id": "2102.02409", "submitter": "Runjing Liu", "authors": "Runjing Liu, Jon D. McAuliffe, Jeffrey Regier (for the LSST Dark\n  Energy Science Collaboration)", "title": "Variational Inference for Deblending Crowded Starfields", "comments": "37 pages; 20 figures; 3 tables. Submitted to the Journal of the\n  American Statistical Association", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the image data collected by astronomical surveys, stars and galaxies often\noverlap. Deblending is the task of distinguishing and characterizing individual\nlight sources from survey images. We propose StarNet, a fully Bayesian method\nto deblend sources in astronomical images of crowded star fields. StarNet\nleverages recent advances in variational inference, including amortized\nvariational distributions and the wake-sleep algorithm. Wake-sleep, which\nminimizes forward KL divergence, has significant benefits compared to\ntraditional variational inference, which minimizes a reverse KL divergence. In\nour experiments with SDSS images of the M2 globular cluster, StarNet is\nsubstantially more accurate than two competing methods: Probablistic Cataloging\n(PCAT), a method that uses MCMC for inference, and a software pipeline employed\nby SDSS for deblending (DAOPHOT). In addition, StarNet is as much as $100,000$\ntimes faster than PCAT, exhibiting the scaling characteristics necessary to\nperform fully Bayesian inference on modern astronomical surveys.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 04:36:58 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Liu", "Runjing", "", "for the LSST Dark\n  Energy Science Collaboration"], ["McAuliffe", "Jon D.", "", "for the LSST Dark\n  Energy Science Collaboration"], ["Regier", "Jeffrey", "", "for the LSST Dark\n  Energy Science Collaboration"]]}, {"id": "2102.02642", "submitter": "Benjamin Christoffersen", "authors": "Benjamin Christoffersen, Mark Clements, Keith Humphreys, Hedvig\n  Kjellstr\\\"om", "title": "Asymptotically Exact and Fast Gaussian Copula Models for Imputation of\n  Mixed Data Types", "comments": "20 pages, 1 figures, and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing values with mixed data types is a common problem in a large number of\nmachine learning applications such as processing of surveys and in different\nmedical applications. Recently, Gaussian copula models have been suggested as a\nmeans of performing imputation of missing values using a probabilistic\nframework. While the present Gaussian copula models have shown to yield state\nof the art performance, they have two limitations: they are based on an\napproximation that is fast but may be imprecise and they do not support\nunordered multinomial variables. We address the first limitation using direct\nand arbitrarily precise approximations both for model estimation and imputation\nby using randomized quasi-Monte Carlo procedures. The method we provide has\nlower errors for the estimated model parameters and the imputed values,\ncompared to previously proposed methods. We also extend the previous Gaussian\ncopula models to include unordered multinomial variables in addition to the\npresent support of ordinal, binary, and continuous variables.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 14:42:29 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 10:15:31 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Christoffersen", "Benjamin", ""], ["Clements", "Mark", ""], ["Humphreys", "Keith", ""], ["Kjellstr\u00f6m", "Hedvig", ""]]}, {"id": "2102.02697", "submitter": "Roland Jucknewitz", "authors": "Roland Jucknewitz, Oliver Weidinger, Anja Schramm", "title": "Covid-19 risk factors: Statistical learning from German healthcare\n  claims data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We analyse prior risk factors for severe, critical or fatal courses of\nCovid-19 based on a retrospective cohort using claims data of the AOK Bayern.\nAs our main methodological contribution, we avoid prior grouping and\npre-selection of candidate risk factors. Instead, fine-grained hierarchical\ninformation from medical classification systems for diagnoses, pharmaceuticals\nand procedures are used, resulting in more than 33,000 covariates. Our approach\nhas better predictive ability than well-specified morbidity groups but does not\nneed prior subject-matter knowledge. The methodology and estimated coefficients\nare made available to decision makers to prioritize protective measures towards\nvulnerable subpopulations and to researchers who like to adjust for a large set\nof confounders in studies of individual risk factors.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 15:48:21 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 07:46:49 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Jucknewitz", "Roland", ""], ["Weidinger", "Oliver", ""], ["Schramm", "Anja", ""]]}, {"id": "2102.02731", "submitter": "Marco Benedetti", "authors": "Marco H. Benedetti, Veronica J. Berrocal, Naveen N. Narisetty", "title": "Identifying regions of inhomogeneities in spatial processes via an M-RA\n  and mixture priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Soils have been heralded as a hidden resource that can be leveraged to\nmitigate and address some of the major global environmental challenges.\nSpecifically, the organic carbon stored in soils, called Soil Organic Carbon\n(SOC), can, through proper soil management, help offset fuel emissions,\nincrease food productivity, and improve water quality. As collecting data on\nSOC is costly and time consuming, not much data on SOC is available, although\nunderstanding the spatial variability in SOC is of fundamental importance for\neffective soil management.\n  In this manuscript, we propose a modeling framework that can be used to gain\na better understanding of the dependence structure of a spatial process by\nidentifying regions within a spatial domain where the process displays the same\nspatial correlation range. To achieve this goal, we propose a generalization of\nthe Multi-Resolution Approximation (M-RA) modeling framework of Katzfuss (2017)\noriginally introduced as a strategy to reduce the computational burden\nencountered when analyzing massive spatial datasets.\n  To allow for the possibility that the correlation of a spatial process might\nbe characterized by a different range in different subregions of a spatial\ndomain, we provide the M-RA basis functions weights with a two-component\nmixture prior with one of the mixture components a shrinking prior. We call our\napproach the mixture M-RA. Application of the mixture M-RA model to both\nstationary and non-stationary data shows that the mixture M-RA model can handle\nboth types of data, can correctly establish the type of spatial dependence\nstructure in the data (e.g. stationary vs not), and can identify regions of\nlocal stationarity.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 16:40:51 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Benedetti", "Marco H.", ""], ["Berrocal", "Veronica J.", ""], ["Narisetty", "Naveen N.", ""]]}, {"id": "2102.02752", "submitter": "Lisa Hampson V", "authors": "Lisa V Hampson, Bj\\\"orn Bornkamp, Bj\\\"orn Holzhauer, Joseph Kahn,\n  Markus R Lange, Wen-Lin Luo, Giovanni Della Cioppa, Kelvin Stott, Steffen\n  Ballerstedt", "title": "Improving the assessment of the probability of success in late stage\n  drug development", "comments": "22 pages, 9 figures, 3 tables, 45 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  There are several steps to confirming the safety and efficacy of a new\nmedicine. A sequence of trials, each with its own objectives, is usually\nrequired. Quantitative risk metrics can be useful for informing decisions about\nwhether a medicine should transition from one stage of development to the next.\nTo obtain an estimate of the probability of regulatory approval, pharmaceutical\ncompanies may start with industry-wide success rates and then apply to these\nsubjective adjustments to reflect program-specific information. However, this\napproach lacks transparency and fails to make full use of data from previous\nclinical trials. We describe a quantitative Bayesian approach for calculating\nthe probability of success (PoS) at the end of phase II which incorporates\ninternal clinical data from one or more phase IIb studies, industry-wide\nsuccess rates, and expert opinion or external data if needed. Using an example,\nwe illustrate how PoS can be calculated accounting for differences between the\nphase IIb data and future phase III trials, and discuss how the methods can be\nextended to accommodate accelerated drug development pathways.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 17:29:47 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Hampson", "Lisa V", ""], ["Bornkamp", "Bj\u00f6rn", ""], ["Holzhauer", "Bj\u00f6rn", ""], ["Kahn", "Joseph", ""], ["Lange", "Markus R", ""], ["Luo", "Wen-Lin", ""], ["Della Cioppa", "Giovanni", ""], ["Stott", "Kelvin", ""], ["Ballerstedt", "Steffen", ""]]}, {"id": "2102.02794", "submitter": "Scott Bruce", "authors": "Zeda Li, Scott A. Bruce, and Tian Cai", "title": "Classification of Categorical Time Series Using the Spectral Envelope\n  and Optimal Scalings", "comments": "29 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article introduces a novel approach to the classification of categorical\ntime series under the supervised learning paradigm. To construct meaningful\nfeatures for categorical time series classification, we consider two relevant\nquantities: the spectral envelope and its corresponding set of optimal\nscalings. These quantities characterize oscillatory patterns in a categorical\ntime series as the largest possible power at each frequency, or spectral\nenvelope, obtained by assigning numerical values, or scalings, to categories\nthat optimally emphasize oscillations at each frequency. Our procedure combines\nthese two quantities to produce an interpretable and parsimonious feature-based\nclassifier that can be used to accurately determine group membership for\ncategorical time series. Classification consistency of the proposed method is\ninvestigated, and simulation studies are used to demonstrate accuracy in\nclassifying categorical time series with various underlying group structures.\nFinally, we use the proposed method to explore key differences in oscillatory\npatterns of sleep stage time series for patients with different sleep disorders\nand accurately classify patients accordingly.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 18:29:27 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Li", "Zeda", ""], ["Bruce", "Scott A.", ""], ["Cai", "Tian", ""]]}, {"id": "2102.02852", "submitter": "Bj\\\"orn Holzhauer", "authors": "Bj\\\"orn Holzhauer (1), Lisa V. Hampson (1), John Paul Gosling (2),\n  Bj\\\"orn Bornkamp (1), Joseph Kahn (3), Markus R. Lange (1), Wen-Lin Luo (3),\n  Caterina Brindicci (1), David Lawrence (1), Steffen Ballerstedt (1), Anthony\n  O'Hagan (4) ((1) Novartis Pharma AG, Basel, Switzerland, (2) JBA Risk\n  Management Ltd, Skipton, United Kingdom, (3) Novartis Pharmaceuticals\n  Corporation, East Hanover, USA, (4) The University of Sheffield, School of\n  Mathematics and Statistics, Sheffield, United Kingdom)", "title": "Eliciting judgements about dependent quantities of interest: The SHELF\n  extension and copula methods illustrated using an asthma case study", "comments": "29 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Pharmaceutical companies regularly need to make decisions about drug\ndevelopment programs based on the limited knowledge from early stage clinical\ntrials. In this situation, eliciting the judgements of experts is an attractive\napproach for synthesising evidence on the unknown quantities of interest. When\ncalculating the probability of success for a drug development program, multiple\nquantities of interest - such as the effect of a drug on different endpoints -\nshould not be treated as unrelated.\n  We discuss two approaches for establishing a multivariate distribution for\nseveral related quantities within the SHeffield ELicitation Framework (SHELF).\nThe first approach elicits experts' judgements about a quantity of interest\nconditional on knowledge about another one. For the second approach, we first\nelicit marginal distributions for each quantity of interest. Then, for each\npair of quantities, we elicit the concordance probability that both lie on the\nsame side of their respective elicited medians. This allows us to specify a\ncopula to obtain the joint distribution of the quantities of interest.\n  We show how these approaches were used in an elicitation workshop that was\nperformed to assess the probability of success of the registrational program of\nan asthma drug. The judgements of the experts, which were obtained prior to\ncompletion of the pivotal studies, were well aligned with the final trial\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 19:37:14 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 16:10:53 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Holzhauer", "Bj\u00f6rn", ""], ["Hampson", "Lisa V.", ""], ["Gosling", "John Paul", ""], ["Bornkamp", "Bj\u00f6rn", ""], ["Kahn", "Joseph", ""], ["Lange", "Markus R.", ""], ["Luo", "Wen-Lin", ""], ["Brindicci", "Caterina", ""], ["Lawrence", "David", ""], ["Ballerstedt", "Steffen", ""], ["O'Hagan", "Anthony", ""]]}, {"id": "2102.02999", "submitter": "Won Chang", "authors": "J. Park, W. Chang, B. Choi", "title": "An Interaction Neyman-Scott Point Process Model for Coronavirus\n  Disease-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With rapid transmission, the coronavirus disease 2019 (COVID-19) has led to\nover 2 million deaths worldwide, posing significant societal challenges.\nUnderstanding the spatial patterns of patient visits and detecting the local\nspreading events are crucial to controlling disease outbreaks. We analyze\nhighly detailed COVID-19 contact tracing data collected from Seoul, which\nprovides a unique opportunity to understand the mechanism of patient visit\noccurrence. Analyzing contact tracing data is challenging because patient\nvisits show strong clustering patterns while clusters of events may have\ncomplex interaction behavior. To account for such behaviors, we develop a novel\ninteraction Neyman-Scott process that regards the observed patient visit events\nas offsprings generated from a parent spreading event. Inference for such\nmodels is complicated since the likelihood involves intractable normalizing\nfunctions. To address this issue, we embed an auxiliary variable algorithm into\nour Markov chain Monte Carlo. We fit our model to several simulated and real\ndata examples under different outbreak scenarios and show that our method can\ndescribe spatial patterns of patient visits well. We also provide visualization\ntools that can inform public health interventions for infectious diseases such\nas social distancing.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 05:11:47 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Park", "J.", ""], ["Chang", "W.", ""], ["Choi", "B.", ""]]}, {"id": "2102.03249", "submitter": "Philip White", "authors": "Philip A. White and Henry Frye and Michael F. Christensen and Alan E.\n  Gelfand and John A. Silander Jr", "title": "Spatial Functional Data Modeling of Plant Reflectances", "comments": "20 pages main manuscript, 20 pages supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plant reflectance spectra - the profile of light reflected by leaves across\ndifferent wavelengths - supply the spectral signature for a species at a\nspatial location to enable estimation of functional and taxonomic diversity for\nplants. We consider leaf spectra as \"responses\" to be explained spatially.\nThese spectra/reflectances are functions over a wavelength band that respond to\nthe environment.\n  Our motivating data are gathered for several families from the Cape Floristic\nRegion (CFR) in South Africa and lead us to develop rich novel spatial models\nthat can explain spectra for genera within families. Wavelength responses for\nan individual leaf are viewed as a function of wavelength, leading to\nfunctional data modeling. Local environmental features become covariates. We\nintroduce wavelength - covariate interaction since the response to\nenvironmental regressors may vary with wavelength, so may variance. Formal\nspatial modeling enables prediction of reflectances for genera at unobserved\nlocations with known environmental features. We incorporate spatial dependence,\nwavelength dependence, and space-wavelength interaction (in the spirit of\nspace-time interaction). We implement out-of-sample validation to select a best\nmodel, discovering that the model features listed above are all informative for\nthe functional data analysis. We then supply interpretation of the results\nunder the selected model.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 15:53:24 GMT"}, {"version": "v2", "created": "Sun, 14 Feb 2021 15:56:50 GMT"}, {"version": "v3", "created": "Thu, 25 Mar 2021 04:18:17 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["White", "Philip A.", ""], ["Frye", "Henry", ""], ["Christensen", "Michael F.", ""], ["Gelfand", "Alan E.", ""], ["Silander", "John A.", "Jr"]]}, {"id": "2102.03274", "submitter": "Roy Dong", "authors": "Samir Wadhwa, Roy Dong", "title": "On the Sample Complexity of Causal Discovery and the Value of Domain\n  Expertise", "comments": "Submitted to ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Causal discovery methods seek to identify causal relations between random\nvariables from purely observational data, as opposed to actively collected\nexperimental data where an experimenter intervenes on a subset of correlates.\nOne of the seminal works in this area is the Inferred Causation algorithm,\nwhich guarantees successful causal discovery under the assumption of a\nconditional independence (CI) oracle: an oracle that can states whether two\nrandom variables are conditionally independent given another set of random\nvariables. Practical implementations of this algorithm incorporate statistical\ntests for conditional independence, in place of a CI oracle. In this paper, we\nanalyze the sample complexity of causal discovery algorithms without a CI\noracle: given a certain level of confidence, how many data points are needed\nfor a causal discovery algorithm to identify a causal structure? Furthermore,\nour methods allow us to quantify the value of domain expertise in terms of data\nsamples. Finally, we demonstrate the accuracy of these sample rates with\nnumerical examples, and quantify the benefits of sparsity priors and known\ncausal directions.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 16:26:17 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Wadhwa", "Samir", ""], ["Dong", "Roy", ""]]}, {"id": "2102.03474", "submitter": "Weijian Liu", "authors": "Weijian Liu, Jun Liu, Chengpeng Hao, Yongchan Gao, and Yong-Liang Wang", "title": "Multichannel adaptive signal detection: Basic theory and literature\n  review", "comments": "10 pages, 5 figures. This manuscript is accepted in Science China:\n  Information Sciences", "journal-ref": null, "doi": null, "report-no": "Manuscript No. SCIS-2020-1112.R1", "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multichannel adaptive signal detection jointly uses the test and training\ndata to form an adaptive detector, and then make a decision on whether a target\nexists or not. Remarkably, the resulting adaptive detectors usually possess the\nconstant false alarm rate (CFAR) properties, and hence no additional CFAR\nprocessing is needed. Filtering is not needed as a processing procedure either,\nsince the function of filtering is embedded in the adaptive detector. Moreover,\nadaptive detection usually exhibits better detection performance than the\nfiltering-then-CFAR detection technique. Multichannel adaptive signal detection\nhas been more than 30 years since the first multichannel adaptive detector was\nproposed by Kelly in 1986. However, there are fewer overview articles on this\ntopic. In this paper we give a tutorial overview of multichannel adaptive\nsignal detection, with emphasis on Gaussian background. We present the main\ndeign criteria for adaptive detectors, investigate the relationship between\nadaptive detection and filtering-then-CFAR detection, relationship between\nadaptive detectors and adaptive filters, summarize typical adaptive detectors,\nshow numerical examples, give comprehensive literature review, and discuss some\npossible further research tracks.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 01:54:23 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Liu", "Weijian", ""], ["Liu", "Jun", ""], ["Hao", "Chengpeng", ""], ["Gao", "Yongchan", ""], ["Wang", "Yong-Liang", ""]]}, {"id": "2102.03639", "submitter": "Ranjan Maitra", "authors": "Wei-Chen Chen and Ranjan Maitra", "title": "A Practical Model-based Segmentation Approach for Accurate Activation\n  Detection in Single-Subject functional Magnetic Resonance Imaging Studies", "comments": "20 pages, 9 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional Magnetic Resonance Imaging (fMRI) maps cerebral activation in\nresponse to stimuli but this activation is often difficult to detect,\nespecially in low-signal contexts and single-subject studies. Accurate\nactivation detection can be guided by the fact that very few voxels are, in\nreality, truly activated and that activated voxels are spatially localized, but\nit is challenging to incorporate both these facts. We provide a computationally\nfeasible and methodologically sound model-based approach, implemented in the R\npackage MixfMRI, that bounds the a priori expected proportion of activated\nvoxels while also incorporating spatial context. Results on simulation\nexperiments for different levels of activation detection difficulty are\nuniformly encouraging. The value of the methodology in low-signal and\nsingle-subject fMRI studies is illustrated on a sports imagination experiment.\nConcurrently, we also extend the potential use of fMRI as a clinical tool to,\nfor example, detect awareness and improve treatment in individual patients in\npersistent vegetative state, such as traumatic brain injury survivors.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 18:46:33 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Chen", "Wei-Chen", ""], ["Maitra", "Ranjan", ""]]}, {"id": "2102.03652", "submitter": "Yaakov Malinovsky", "authors": "Yaakov Malinovsky and Paul S. Albert", "title": "Nested Group Testing Procedures for Screening", "comments": "arXiv admin note: text overlap with arXiv:1608.06330", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article reviews a class of adaptive group testing procedures that\noperate under a probabilistic model assumption as follows. Consider a set of\n$N$ items, where item $i$ has the probability $p$ ($p_i$ in the generalized\ngroup testing) to be defective, and the probability $1-p$ to be non-defective\nindependent from the other items. A group test applied to any subset of size\n$n$ is a binary test with two possible outcomes, positive or negative. The\noutcome is negative if all $n$ items are non-defective, whereas the outcome is\npositive if at least one item among the $n$ items is defective. The goal is\ncomplete identification of all $N$ items with the minimum expected number of\ntests.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 19:50:12 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 23:10:14 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Malinovsky", "Yaakov", ""], ["Albert", "Paul S.", ""]]}, {"id": "2102.03711", "submitter": "Kolawole Ogunsina", "authors": "Kolawole Ogunsina, Ilias Bilionis, Daniel DeLaurentis", "title": "Exploratory Data Analysis for Airline Disruption Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Reliable platforms for data collation during airline schedule operations have\nsignificantly increased the quality and quantity of available information for\neffectively managing airline schedule disruptions. To that effect, this paper\napplies macroscopic and microscopic techniques by way of basic statistics and\nmachine learning, respectively, to analyze historical scheduling and operations\ndata from a major airline in the United States. Macroscopic results reveal that\nmajority of irregular operations in airline schedule that occurred over a\none-year period stemmed from disruptions due to flight delays, while\nmicroscopic results validate different modeling assumptions about key drivers\nfor airline disruption management like turnaround as a Gaussian process.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 04:00:10 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 13:19:31 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Ogunsina", "Kolawole", ""], ["Bilionis", "Ilias", ""], ["DeLaurentis", "Daniel", ""]]}, {"id": "2102.03782", "submitter": "Simon Olofsson", "authors": "Simon Olofsson and Eduardo S. Schultz and Adel Mhamdi and Alexander\n  Mitsos and Marc Peter Deisenroth and Ruth Misener", "title": "Design of Dynamic Experiments for Black-Box Model Discrimination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Diverse domains of science and engineering require and use mechanistic\nmathematical models, e.g. systems of differential algebraic equations. Such\nmodels often contain uncertain parameters to be estimated from data. Consider a\ndynamic model discrimination setting where we wish to chose: (i) what is the\nbest mechanistic, time-varying model and (ii) what are the best model parameter\nestimates. These tasks are often termed model\ndiscrimination/selection/validation/verification. Typically, several rival\nmechanistic models can explain data, so we incorporate available data and also\nrun new experiments to gather more data. Design of dynamic experiments for\nmodel discrimination helps optimally collect data. For rival mechanistic models\nwhere we have access to gradient information, we extend existing methods to\nincorporate a wider range of problem uncertainty and show that our proposed\napproach is equivalent to historical approaches when limiting the types of\nconsidered uncertainty. We also consider rival mechanistic models as dynamic\nblack boxes that we can evaluate, e.g. by running legacy code, but where\ngradient or other advanced information is unavailable. We replace these\nblack-box models with Gaussian process surrogate models and thereby extend the\nmodel discrimination setting to additionally incorporate rival black-box model.\nWe also explore the consequences of using Gaussian process surrogates to\napproximate gradient-based methods.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 11:34:39 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Olofsson", "Simon", ""], ["Schultz", "Eduardo S.", ""], ["Mhamdi", "Adel", ""], ["Mitsos", "Alexander", ""], ["Deisenroth", "Marc Peter", ""], ["Misener", "Ruth", ""]]}, {"id": "2102.03894", "submitter": "Henry Linder", "authors": "Henry Linder, Nalini Ravishanker, Ming-Hui Chen, David McIntosh,\n  Stanley Nolan", "title": "Anomaly Detection in Energy Usage Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Energy usage monitoring on higher education campuses is an important step for\nproviding satisfactory service, lowering costs and supporting the move to green\nenergy. We present a collaboration between the Department of Statistics and\nFacilities Operations at an R1 research university to develop statistically\nbased approaches for monitoring monthly energy usage and proportional yearly\nusage for several hundred utility accounts on campus. We compare the\ninterpretability and power of model-free and model-based methods for detection\nof anomalous energy usage patterns in statistically similar groups of accounts.\nOngoing conversation between the academic and operations teams enhances the\npractical utility of the project and enables implementation for the university.\nOur work highlights an application of thoughtful and continuing collaborative\nanalysis using easy-to-understand statistical principles for real-world\ndeployment.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 19:28:47 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Linder", "Henry", ""], ["Ravishanker", "Nalini", ""], ["Chen", "Ming-Hui", ""], ["McIntosh", "David", ""], ["Nolan", "Stanley", ""]]}, {"id": "2102.03895", "submitter": "Jiacheng Zhu", "authors": "Jiacheng Zhu, Aritra Guha, Dat Do, Mengdi Xu, XuanLong Nguyen, Ding\n  Zhao", "title": "Functional optimal transport: map estimation and domain adaptation for\n  functional data", "comments": "23 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a formulation of optimal transport problem for distributions on\nfunction spaces, where the stochastic map between functional domains can be\npartially represented in terms of an (infinite-dimensional) Hilbert-Schmidt\noperator mapping a Hilbert space of functions to another. For numerous machine\nlearning tasks, data can be naturally viewed as samples drawn from spaces of\nfunctions, such as curves and surfaces, in high dimensions. Optimal transport\nfor functional data analysis provides a useful framework of treatment for such\ndomains. In this work, we develop an efficient algorithm for finding the\nstochastic transport map between functional domains and provide theoretical\nguarantees on the existence, uniqueness, and consistency of our estimate for\nthe Hilbert-Schmidt operator. We validate our method on synthetic datasets and\nstudy the geometric properties of the transport map. Experiments on real-world\ndatasets of robot arm trajectories further demonstrate the effectiveness of our\nmethod on applications in domain adaptation.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 19:29:28 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 17:40:37 GMT"}, {"version": "v3", "created": "Fri, 11 Jun 2021 18:58:42 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Zhu", "Jiacheng", ""], ["Guha", "Aritra", ""], ["Do", "Dat", ""], ["Xu", "Mengdi", ""], ["Nguyen", "XuanLong", ""], ["Zhao", "Ding", ""]]}, {"id": "2102.03935", "submitter": "Ramin Bostanabad", "authors": "Nicholas Oune, Ramin Bostanabad", "title": "Latent Map Gaussian Processes for Mixed Variable Metamodeling", "comments": "35 Pages, 7 Figures, 14 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gaussian processes (GPs) are ubiquitously used in sciences and engineering as\nmetamodels. Standard GPs, however, can only handle numerical or quantitative\nvariables. In this paper, we introduce latent map Gaussian processes (LMGPs)\nthat inherit the attractive properties of GPs and are also applicable to mixed\ndata which have both quantitative and qualitative inputs. The core idea behind\nLMGPs is to learn a continuous, low-dimensional latent space or manifold which\nencodes all qualitative inputs. To learn this manifold, we first assign a\nunique prior vector representation to each combination of qualitative inputs.\nWe then use a low-rank linear map to project these priors on a manifold that\ncharacterizes the posterior representations. As the posteriors are\nquantitative, they can be directly used in any standard correlation function\nsuch as the Gaussian or Matern. Hence, the optimal map and the corresponding\nmanifold, along with other hyperparameters of the correlation function, can be\nsystematically learned via maximum likelihood estimation. Through a wide range\nof analytic and real-world examples, we demonstrate the advantages of LMGPs\nover state-of-the-art methods in terms of accuracy and versatility. In\nparticular, we show that LMGPs can handle variable-length inputs, have an\nexplainable neural network interpretation, and provide insights into how\nqualitative inputs affect the response or interact with each other. We also\nemploy LMGPs in Bayesian optimization and illustrate that they can discover\noptimal compound compositions more efficiently than conventional methods that\nconvert compositions to qualitative variables via manual featurization.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 22:21:53 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 19:20:37 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Oune", "Nicholas", ""], ["Bostanabad", "Ramin", ""]]}, {"id": "2102.03975", "submitter": "Shuvayan Banerjee", "authors": "Shuvayan Banerjee, Radhe Srivastava, Ajit Rajwade", "title": "Reconstruction of Sparse Signals under Gaussian Noise and Saturation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most compressed sensing algorithms do not account for the effect of\nsaturation in noisy compressed measurements, though saturation is an important\nconsequence of the limited dynamic range of existing sensors. The few\nalgorithms that handle saturation effects either simply discard saturated\nmeasurements, or impose additional constraints to ensure consistency of the\nestimated signal with the saturated measurements (based on a known saturation\nthreshold) given uniform-bounded noise. In this paper, we instead propose a new\ndata fidelity function which is directly based on ensuring a certain form of\nconsistency between the signal and the saturated measurements, and can be\nexpressed as the negative logarithm of a certain carefully designed likelihood\nfunction. Our estimator works even in the case of Gaussian noise (which is\nunbounded) in the measurements. We prove that our data fidelity function is\nconvex. We moreover, show that it satisfies the condition of Restricted Strong\nConvexity and thereby derive an upper bound on the performance of the\nestimator. We also show that our technique experimentally yields results\nsuperior to the state of the art under a wide variety of experimental settings,\nfor compressive signal recovery from noisy and saturated measurements.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 03:01:46 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Banerjee", "Shuvayan", ""], ["Srivastava", "Radhe", ""], ["Rajwade", "Ajit", ""]]}, {"id": "2102.04004", "submitter": "Andrew Zammit-Mangion", "authors": "Andrew Zammit-Mangion, Michael Bertolacci, Jenny Fisher, Ann Stavert,\n  Matthew L. Rigby, Yi Cao, and Noel Cressie", "title": "WOMBAT: A fully Bayesian global flux-inversion framework", "comments": "46 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  WOMBAT (the WOllongong Methodology for Bayesian Assimilation of Trace-gases)\nis a fully Bayesian hierarchical statistical framework for flux inversion of\ntrace gases from flask, in situ, and remotely sensed data. WOMBAT extends the\nconventional Bayesian-synthesis framework through the consideration of a\ncorrelated error term, the capacity for online bias correction, and the\nprovision of uncertainty quantification on all unknowns that appear in the\nBayesian statistical model. We show, in an observing system simulation\nexperiment (OSSE), that these extensions are crucial when the data are indeed\nbiased and have errors that are correlated. Using the GEOS-Chem atmospheric\ntransport model, we show that WOMBAT is able to obtain posterior means and\nuncertainties on non-fossil-fuel CO$_2$ fluxes from Orbiting Carbon\nObservatory-2 (OCO-2) data that are comparable to those from the Model\nIntercomparison Project (MIP) reported in Crowell et al. (2019, Atmos. Chem.\nPhys., vol. 19). We also find that our predictions of out-of-sample retrievals\nfrom the Total Column Carbon Observing Network are, for the most part, more\naccurate than those made by the MIP participants. Subsequent versions of the\nOCO-2 datasets will be ingested into WOMBAT as they become available.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 05:12:52 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Zammit-Mangion", "Andrew", ""], ["Bertolacci", "Michael", ""], ["Fisher", "Jenny", ""], ["Stavert", "Ann", ""], ["Rigby", "Matthew L.", ""], ["Cao", "Yi", ""], ["Cressie", "Noel", ""]]}, {"id": "2102.04123", "submitter": "Yanrong Yang", "authors": "Lingyu He, Fei Huang, Yanrong Yang", "title": "Data-adaptive Dimension Reduction for US Mortality Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Forecasting accuracy of mortality data is important for the management of\npension funds and pricing of life insurance in actuarial science. Age-specific\nmortality forecasting in the US poses a challenging problem in high dimensional\ntime series analysis. Prior attempts utilize traditional dimension reduction\ntechniques to avoid the curse of dimensionality, and then mortality forecasting\nis achieved through features' forecasting. However, a method of reducing\ndimension pertinent to ideal forecasting is elusive. To address this, we\npropose a novel approach to pursue features that are not only capable of\nrepresenting original data well but also capturing time-serial dependence as\nmost as possible. The proposed method is adaptive for the US mortality data and\nenjoys good statistical performance. As a comparison, our method performs\nbetter than existing approaches, especially in regard to the Lee-Carter Model\nas a benchmark in mortality analysis. Based on forecasting results, we generate\nmore accurate estimates of future life expectancies and prices of life\nannuities, which can have great financial impact on life insurers and social\nsecurities compared with using Lee-Carter Model. Furthermore, various\nsimulations illustrate scenarios under which our method has advantages, as well\nas interpretation of the good performance on mortality data.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 10:58:34 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["He", "Lingyu", ""], ["Huang", "Fei", ""], ["Yang", "Yanrong", ""]]}, {"id": "2102.04124", "submitter": "Wolfgang Karl H\\\"ardle", "authors": "Cathy Yi-Hsuan Chen, Wolfgang Karl H\\\"ardle, Yegor Klochkov", "title": "SONIC: SOcial Network with Influencers and Communities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The integration of social media characteristics into an econometric framework\nrequires modeling a high dimensional dynamic network with dimensions of\nparameter typically much larger than the number of observations. To cope with\nthis problem, we introduce SONIC, a new high-dimensional network model that\nassumes that (1) only few influencers drive the network dynamics; (2) the\ncommunity structure of the network is characterized by homogeneity of response\nto specific influencers, implying their underlying similarity. An estimation\nprocedure is proposed based on a greedy algorithm and LASSO regularization.\nThrough theoretical study and simulations, we show that the matrix parameter\ncan be estimated even when sample size is smaller than the size of the network.\nUsing a novel dataset retrieved from one of leading social media platforms -\nStockTwits and quantifying their opinions via natural language processing, we\nmodel the opinions network dynamics among a select group of users and further\ndetect the latent communities. With a sparsity regularization, we can identify\nimportant nodes in the network.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 11:01:49 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Chen", "Cathy Yi-Hsuan", ""], ["H\u00e4rdle", "Wolfgang Karl", ""], ["Klochkov", "Yegor", ""]]}, {"id": "2102.04210", "submitter": "Rohan Yashraj Gupta", "authors": "Rohan Yashraj Gupta, Satya Sai Mudigonda, Pallav Kumar Baruah, Phani\n  Krishna Kandala", "title": "Implementation of Correlation and Regression Models for Health Insurance\n  Fraud in Covid-19 Environment using Actuarial and Data Science Techniques", "comments": null, "journal-ref": "Int. J. Recent Technol. Eng. vol. 9 no. 3 pp. 699-706 Sep. 2020", "doi": "10.35940/ijrte.C4686.099320", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fraud acts as a major deterrent to a companys growth if uncontrolled. It\nchallenges the fundamental value of Trust in the Insurance business. COVID-19\nbrought additional challenges of increased potential fraud to health insurance\nbusiness. This work describes implementation of existing and enhanced fraud\ndetection methods in the pre-COVID-19 and COVID-19 environments. For this\npurpose, we have developed an innovative enhanced fraud detection framework\nusing actuarial and data science techniques. Triggers specific to COVID-19 are\nidentified in addition to the existing triggers. We have also explored the\nrelationship between insurance fraud and COVID-19. To determine this we\ncalculated Pearson correlation coefficient and fitted logarithmic regression\nmodel between fraud in health insurance and COVID-19 cases. This work uses two\ndatasets: health insurance dataset and Kaggle dataset on COVID-19 cases for the\nsame select geographical location in India. Our experimental results shows\nPearson correlation coefficient of 0.86, which implies that the month on month\nrate of fraudulent cases is highly correlated with month on month rate of\nCOVID-19 cases. The logarithmic regression performed on the data gave the\nr-squared value of 0.91 which indicates that the model is a good fit. This work\naims to provide much needed tools and techniques for health insurance business\nto counter the fraud.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 17:20:41 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Gupta", "Rohan Yashraj", ""], ["Mudigonda", "Satya Sai", ""], ["Baruah", "Pallav Kumar", ""], ["Kandala", "Phani Krishna", ""]]}, {"id": "2102.04233", "submitter": "Nassim Dehouche", "authors": "Nassim Dehouche", "title": "On Some Statistical and Axiomatic Properties of the Injury Severity\n  Score", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Injury Severity Score (ISS) is a standard aggregate indicator of the\noverall severity of multiple injuries to the human body. This score is\ncalculated by summing the squares of the three highest values of the\nAbbreviated Injury Scale (AIS) grades across six body regions of a trauma\nvictim. Despite its widespread usage over the past four decades, little is\nknown in the (mostly medical) literature on the subject about the axiomatic and\nstatistical properties of this quadratic aggregation score. To bridge this gap,\nthe present paper studies the ISS from the perspective of recent advances in\ndecision science. We demonstrate some statistical and axiomatic properties of\nthe ISS as a multicrtieria aggregation procedure. Our study highlights some\nunintended, undesirable properties that stem from arbitrary choices in its\ndesign and that call lead to bias in its use as a patient triage criterion.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 04:04:12 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Dehouche", "Nassim", ""]]}, {"id": "2102.04236", "submitter": "Ger Koole", "authors": "Rik van Leeuwen and Ger Koole", "title": "Demand forecasting in hospitality using smoothed demand curves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Forecasting demand is one of the fundamental components of a successful\nrevenue management system in hospitality. The industry requires understandable\nmodels that contribute to adaptability by a revenue management department to\nmake data-driven decisions. Data analysis and forecasts prove an essential role\nfor the time until the check-in date, which differs per day of week. This paper\naims to provide a new model, which is inspired by cubic smoothing splines,\nresulting in smooth demand curves per rate class over time until the check-in\ndate. This model regulates the error between data points and a smooth curve,\nand is therefore able to capture natural guest behavior. The forecast is\nobtained by solving a linear programming model, which enables the incorporation\nof industry knowledge in the form of constraints. Using data from a major hotel\nchain, a lower error and 13.3% more revenue is obtained.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 08:56:36 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["van Leeuwen", "Rik", ""], ["Koole", "Ger", ""]]}, {"id": "2102.04273", "submitter": "Antonio Calcagn\\`i", "authors": "Antonio Calcagn\\`i, Niccol\\`o Cao, Enrico Rubaltelli, Luigi Lombardi", "title": "A psychometric modeling approach to fuzzy rating data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling fuzziness and imprecision in human rating data is a crucial problem\nin many research areas, including applied statistics, behavioral, social, and\nhealth sciences. Because of the interplay between cognitive, affective, and\ncontextual factors, the process of answering survey questions is a complex\ntask, which can barely be captured by standard (crisp) rating responses. Fuzzy\nrating scales have progressively been adopted to overcome some of the\nlimitations of standard rating scales, including their inability to disentangle\ndecision uncertainty from individual responses. The aim of this article is to\nprovide a novel fuzzy scaling procedure which uses Item Response Theory trees\n(IRTrees) as a psychometric model for the stage-wise latent response process.\nIn so doing, fuzziness of rating data is modeled using the overall rater's\npattern of responses instead of being computed using a single-item based\napproach. This offers a consistent system for interpreting fuzziness in terms\nof individual-based decision uncertainty. A simulation study and two empirical\napplications are adopted to assess the characteristics of the proposed model\nand provide converging results about its effectiveness in modeling fuzziness\nand imprecision in rating data.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 15:13:28 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Calcagn\u00ec", "Antonio", ""], ["Cao", "Niccol\u00f2", ""], ["Rubaltelli", "Enrico", ""], ["Lombardi", "Luigi", ""]]}, {"id": "2102.04296", "submitter": "Jacob Bradley", "authors": "Jacob R. Bradley and Timothy I. Cannings", "title": "Data-driven design of targeted gene panels for estimating immunotherapy\n  biomarkers", "comments": "21 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a novel data-driven framework for the design of targeted gene\npanels for estimating exome-wide biomarkers in cancer immunotherapy. Our first\ngoal is to develop a generative model for the profile of mutation across the\nexome, which allows for gene- and variant type-dependent mutation rates. Based\non this model, we then propose a new procedure for estimating biomarkers such\nas Tumour Mutation Burden and Tumour Indel Burden. Our approach allows the\npractitioner to select a targeted gene panel of a prespecified size, and then\nconstruct an estimator that only depends on the selected genes. Alternatively,\nthe practitioner may apply our method to make predictions based on an existing\ngene panel, or to augment a gene panel to a given size. We demonstrate the\nexcellent performance of our proposal using an annotated mutation dataset from\n1144 Non-Small Cell Lung Cancer patients.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 16:03:05 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 13:48:47 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Bradley", "Jacob R.", ""], ["Cannings", "Timothy I.", ""]]}, {"id": "2102.04298", "submitter": "Luis Enrique Correa Rocha Prof", "authors": "Luis E C Rocha and Jan Ryckebusch and Koen Schoors and Matthew Smith", "title": "The scaling of social interactions across animal species", "comments": "To appear in Scientific Reports", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cond-mat.stat-mech nlin.AO physics.pop-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social animals self-organise to create groups to increase protection against\npredators and productivity. One-to-one interactions are the building blocks of\nthese emergent social structures and may correspond to friendship, grooming,\ncommunication, among other social relations. These structures should be robust\nto failures and provide efficient communication to compensate the costs of\nforming and maintaining the social contacts but the specific purpose of each\nsocial interaction regulates the evolution of the respective social networks.\nWe collate 611 animal social networks and show that the number of social\ncontacts $E$ scales with group size $N$ as a super-linear power-law\n$E=CN^{\\beta}$ for various species of animals, including humans, other mammals\nand non-mammals. We identify that the power-law exponent $\\beta$ varies\naccording to the social function of the interactions as $\\beta = 1+a/4$, with\n$a \\approx {1,2,3,4}$. By fitting a multi-layer model to our data, we observe\nthat the cost to cross social groups also varies according to social function.\nRelatively low costs are observed for physical contact, grooming and group\nmembership which lead to small groups with high and constant social clustering.\nOffline friendship has similar patterns while online friendship shows weak\nsocial structures. The intermediate case of spatial proximity ($\\beta=1.5$ and\nclustering dependency on network size quantitatively similar to friendship)\nsuggests that proximity interactions may be as relevant for the spread of\ninfectious diseases as for social processes like friendship.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 16:03:58 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 14:10:26 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Rocha", "Luis E C", ""], ["Ryckebusch", "Jan", ""], ["Schoors", "Koen", ""], ["Smith", "Matthew", ""]]}, {"id": "2102.04335", "submitter": "Dr. Cesar R. Salas-Guerra", "authors": "Cesar R. Salas-Guerra", "title": "Bipartisan politics and poverty as a risk factor for contagion and\n  mortality from SARS-CoV-2 virus in the United States of America", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In the United States, from the start of the COVID-19 pandemic to December 31,\n2020, 341,199 deaths and more than 19,663,976 infections were recorded. Recent\nliterature establishes that communities with poverty-related health problems,\nsuch as obesity, cardiovascular disease, diabetes, and hypertension, are more\nsusceptible to mortality from SARS-CoV-2 infection. Additionally, controversial\npublic health policies implemented by the nation's political leaders have\nhighlighted the socioeconomic inequalities of minorities. Therefore, through\nmultivariate correlational analysis using machine learning techniques and\nstructural equations, we measure whether social determinants are associated\nwith increased infection and death from COVID-19 disease. The PLS least squares\nregression analysis allowed identifying a significant impact between social\ndeterminants and COVID-19 disease through a predictive value of R2 = .916,\n\\b{eta} = .836, p =. 000 (t-value = 66,137) shows that for each unit of\nincrease in social determinants, COVID-19 disease increases by 91.6%. The\nclustering index used for correlational analysis generated a new data set\ncomprising three groups: C1 Republicans, C2 and C3 Democrats from California,\nNew York, Texas, and Florida. This analysis made it possible to identify the\npoverty variable as the main risk factor related to the high rates of infection\nin Republican states and a high positive correlation between the population not\ninsured with a medical plan and high levels of virus contagion in the states of\ngroup C3. These findings explain the argument that poverty and lack of economic\nsecurity put the public or private health system at risk and calamity.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 15:16:43 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Salas-Guerra", "Cesar R.", ""]]}, {"id": "2102.04342", "submitter": "Jesse Russell", "authors": "Jesse Russell", "title": "The Limits of Computation in Solving Equity Trade-Offs in Machine\n  Learning and Justice System Risk Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper explores how different ideas of racial equity in machine learning,\nin justice settings in particular, can present trade-offs that are difficult to\nsolve computationally. Machine learning is often used in justice settings to\ncreate risk assessments, which are used to determine interventions, resources,\nand punitive actions. Overall aspects and performance of these machine\nlearning-based tools, such as distributions of scores, outcome rates by levels,\nand the frequency of false positives and true positives, can be problematic\nwhen examined by racial group. Models that produce different distributions of\nscores or produce a different relationship between level and outcome are\nproblematic when those scores and levels are directly linked to the restriction\nof individual liberty and to the broader context of racial inequity. While\ncomputation can help highlight these aspects, data and computation are unlikely\nto solve them. This paper explores where values and mission might have to fill\nthe spaces computation leaves.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 16:46:29 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Russell", "Jesse", ""]]}, {"id": "2102.04496", "submitter": "Sanjib Sharma", "authors": "Sanjib Sharma, Ben Seiyon Lee, Robert E. Nicholas, Klaus Keller", "title": "A safety factor approach to designing urban infrastructure for dynamic\n  conditions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Current approaches to design flood-sensitive infrastructure typically assume\na stationary rainfall distribution and neglect many uncertainties. These\nassumptions are inconsistent with observations that suggest intensifying\nextreme precipitation events and the uncertainties surrounding projections of\nthe coupled natural-human systems. Here we show that assuming climate\nstationarity and neglecting deep uncertainties can drastically underestimate\nflood risks and lead to poor infrastructure design choices. We find that\nclimate uncertainty dominates the socioeconomic and engineering uncertainties\nthat impact the hydraulic reliability in stormwater drainage systems. We\nquantify the upfront costs needed to achieve higher hydraulic reliability and\nrobustness against the deep uncertainties surrounding projections of rainfall,\nsurface runoff characteristics, and infrastructure lifetime. Depending on the\nlocation, we find that adding safety factors of 1.4 to 1.7 to the standard\nstormwater pipe design guidance produces robust performance to the considered\ndeep uncertainties.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 19:35:25 GMT"}, {"version": "v2", "created": "Sun, 28 Mar 2021 16:20:38 GMT"}, {"version": "v3", "created": "Tue, 30 Mar 2021 01:26:23 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Sharma", "Sanjib", ""], ["Lee", "Ben Seiyon", ""], ["Nicholas", "Robert E.", ""], ["Keller", "Klaus", ""]]}, {"id": "2102.04544", "submitter": "David Kline", "authors": "David Kline, Ayaz Hyder, Enhao Liu, Michael Rayo, Samuel Malloy,\n  Elisabeth Root", "title": "A Bayesian spatio-temporal nowcasting model for public health\n  decision-making and surveillance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As COVID-19 spread through the United States in 2020, states began to set up\nalert systems to inform policy decisions and serve as risk communication tools\nfor the general public. Many of these systems, like in Ohio, included\nindicators based on an assessment of trends in reported cases. However, when\ncases are indexed by date of disease onset, reporting delays complicate the\ninterpretation of trends. Despite a foundation of statistical literature to\naddress this problem, these methods have not been widely applied in practice.\nIn this paper, we develop a Bayesian spatio-temporal nowcasting model for\nassessing trends in county-level COVID-19 cases in Ohio. We compare the\nperformance of our model to the current approach used in Ohio and the approach\nthat was recommended by the Centers for Disease Control and Prevention. We\ndemonstrate gains in performance while still retaining interpretability using\nour model. In addition, we are able to fully account for uncertainty in both\nthe time series of cases and in the reporting process. While we cannot\neliminate all of the uncertainty in public health surveillance and subsequent\ndecision-making, we must use approaches that embrace these challenges and\ndeliver more accurate and honest assessments to policymakers.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 21:47:24 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Kline", "David", ""], ["Hyder", "Ayaz", ""], ["Liu", "Enhao", ""], ["Rayo", "Michael", ""], ["Malloy", "Samuel", ""], ["Root", "Elisabeth", ""]]}, {"id": "2102.04843", "submitter": "Qinan Wang", "authors": "Qinan Wang, Yaomu Zhou, Xiaofei Chen", "title": "A Vector Autoregression Prediction Model for COVID-19 Outbreak", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Since two people came down a county of north Seattle with positive COVID-19\n(coronavirus-19) in 2019, the current total cases in the United States (U.S.)\nare over 12 million. Predicting the pandemic trend under effective variables is\ncrucial to help find a way to control the epidemic. Based on available\nliterature, we propose a validated Vector Autoregression (VAR) time series\nmodel to predict the positive COVID-19 cases. A real data prediction for U.S.\nis provided based on the U.S. coronavirus data. The key message from our study\nis that the situation of the pandemic will getting worse if there is no\neffective control.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 19:18:37 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Wang", "Qinan", ""], ["Zhou", "Yaomu", ""], ["Chen", "Xiaofei", ""]]}, {"id": "2102.04879", "submitter": "Fotios Petropoulos", "authors": "Spyros Makridakis, Chris Fry, Fotios Petropoulos and Evangelos\n  Spiliotis", "title": "The future of forecasting competitions: Design attributes and principles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Forecasting competitions are the equivalent of laboratory experimentation\nwidely used in physical and life sciences. They provide useful, objective\ninformation to improve the theory and practice of forecasting, advancing the\nfield, expanding its usage and enhancing its value to decision and\npolicymakers. We describe ten design attributes to be considered when\norganizing forecasting competitions, taking into account trade-offs between\noptimal choices and practical concerns like costs, as well as the time and\neffort required to participate in them. Consequently, we map all major past\ncompetitions in respect to their design attributes, identifying similarities\nand differences between them, as well as design gaps, and making suggestions\nabout the principles to be included in future competitions, putting a\nparticular emphasis on learning as much as possible from their implementation\nin order to help improve forecasting accuracy and uncertainty. We discuss that\nthe task of forecasting often presents a multitude of challenges that can be\ndifficult to be captured in a single forecasting contest. To assess the caliber\nof a forecaster, we, therefore, propose that organizers of future competitions\nconsider a multi-contest approach. We suggest the idea of a forecasting\n\"athlon\", where different challenges of varying characteristics take place.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 15:24:07 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 11:10:30 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Makridakis", "Spyros", ""], ["Fry", "Chris", ""], ["Petropoulos", "Fotios", ""], ["Spiliotis", "Evangelos", ""]]}, {"id": "2102.04937", "submitter": "Chihoon Lee", "authors": "Chihoon Lee, Amy R. Ward, Heng-Qing Ye", "title": "Stationary Distribution Convergence of the Offered Waiting Processes in\n  Heavy Traffic under General Patience Time Scaling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a sequence of single server queues with customer abandonment\n(GI/GI/1+GI) under heavy traffic. The patience time distributions vary with the\nsequence, which allows for a wider scope of applications. It is known ([20,\n18]) that the sequence of scaled offered waiting time processes converges\nweakly to a reflecting diffusion process with non-linear drift, as the traffic\nintensity approaches one. In this paper, we further show that the sequence of\nstationary distributions and moments of the offered waiting times, with\ndiffusion scaling, converge to those of the limit diffusion process. This\njustifies the stationary performance of the diffusion limit as a valid\napproximation for the stationary performance of the GI/GI/1+GI queue.\nConsequently, we also derive the approximation for the abandonment probability\nfor the GI/GI/1+GI queue in the stationary state.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 16:48:19 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Lee", "Chihoon", ""], ["Ward", "Amy R.", ""], ["Ye", "Heng-Qing", ""]]}, {"id": "2102.05020", "submitter": "Xianghao Zhan", "authors": "Xianghao Zhan, Yiheng Li, Yuzhe Liu, August G. Domel, Hossein Vahid\n  Alizadeh, Zhou Zhou, Nicholas J. Cecchi, Samuel J. Raymond, Stephen Tiernan,\n  Jesse Ruan, Saeed Barbat, Olivier Gevaert, Michael M. Zeineh, Gerald A.\n  Grant, David B. Camarillo", "title": "Predictive Factors of Kinematics in Traumatic Brain Injury from Head\n  Impacts Based on Statistical Interpretation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.bio-ph cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Brain tissue deformation resulting from head impacts is primarily caused by\nrotation and can lead to traumatic brain injury. To quantify brain injury risk\nbased on measurements of kinematics on the head, finite element (FE) models and\nvarious brain injury criteria based on different factors of these kinematics\nhave been developed, but the contribution of different kinematic factors has\nnot been comprehensively analyzed across different types of head impacts in a\ndata-driven manner. To better design brain injury criteria, the predictive\npower of rotational kinematics factors, which are different in 1) the\nderivative order (angular velocity, angular acceleration, angular jerk), 2) the\ndirection and 3) the power (e.g., square-rooted, squared, cubic) of the angular\nvelocity, were analyzed based on different datasets including laboratory\nimpacts, American football, mixed martial arts (MMA), NHTSA automobile\ncrashworthiness tests and NASCAR crash events. Ordinary least squares\nregressions were built from kinematics factors to the 95\\% maximum principal\nstrain (MPS95), and we compared zero-order correlation coefficients, structure\ncoefficients, commonality analysis, and dominance analysis. The angular\nacceleration, the magnitude, and the first power factors showed the highest\npredictive power for the majority of impacts including laboratory impacts,\nAmerican football impacts, with few exceptions (angular velocity for MMA and\nNASCAR impacts). The predictive power of rotational kinematics in three\ndirections (x: posterior-to-anterior, y: left-to-right, z:\nsuperior-to-inferior) of kinematics varied with different sports and types of\nhead impacts.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 18:37:20 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2021 19:53:49 GMT"}, {"version": "v3", "created": "Thu, 10 Jun 2021 16:15:37 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Zhan", "Xianghao", ""], ["Li", "Yiheng", ""], ["Liu", "Yuzhe", ""], ["Domel", "August G.", ""], ["Alizadeh", "Hossein Vahid", ""], ["Zhou", "Zhou", ""], ["Cecchi", "Nicholas J.", ""], ["Raymond", "Samuel J.", ""], ["Tiernan", "Stephen", ""], ["Ruan", "Jesse", ""], ["Barbat", "Saeed", ""], ["Gevaert", "Olivier", ""], ["Zeineh", "Michael M.", ""], ["Grant", "Gerald A.", ""], ["Camarillo", "David B.", ""]]}, {"id": "2102.05313", "submitter": "Joseph Mikael", "authors": "Carl Remlinger, Joseph Mikael, Romuald Elie", "title": "Conditional and Adversarial Euler-based Generators For Time Series", "comments": "14 page, 9 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce three new generative models for time series. Based on Euler\ndiscretization and Wasserstein metrics, they are able to capture time marginal\ndistributions and temporal dynamics. Two of these methods rely on the\nadaptation of generative adversarial networks (GANs) to time series. Both of\nthem outperform state-of-the-art benchmarks by capturing the underlying\ntemporal structure on synthetic time series. The third algorithm, called\nConditional Euler Generator (CEGEN), minimizes a dedicated distance between the\ntransition probability distributions over all time steps. In the context of Ito\nprocesses, we provide theoretical guarantees that minimizing this criterion\nimplies accurate estimations of the drift and volatility parameters. We\ndemonstrate empirically that CEGEN outperforms state-of-the-art and GAN\ngenerators on both marginal and temporal dynamics metrics. Besides, it\nidentifies accurate correlation structures in high dimension. When few data\npoints are available, we verify the effectiveness of CEGEN, when combined with\ntransfer learning methods on Monte Carlo simulations. Finally, we illustrate\nthe robustness of our method on various real-world datasets.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 08:18:35 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 15:09:04 GMT"}, {"version": "v3", "created": "Tue, 8 Jun 2021 10:45:08 GMT"}, {"version": "v4", "created": "Fri, 11 Jun 2021 07:32:40 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Remlinger", "Carl", ""], ["Mikael", "Joseph", ""], ["Elie", "Romuald", ""]]}, {"id": "2102.05535", "submitter": "Dominic Magirr Dr", "authors": "Dominic Magirr, Jos\\'e L. Jim\\'enez", "title": "Designing group sequential clinical trials when a delayed effect is\n  anticipated: A practical guidance", "comments": "Corrected a typo. Equation (2) should have been an intersection of\n  events, not a union. Same for the subsequent similar equation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A common feature of many recent trials evaluating the effects of\nimmunotherapy on survival is that non-proportional hazards can be anticipated\nat the design stage. This raises the possibility to use a statistical method\ntailored towards testing the purported long-term benefit, rather than applying\nthe more standard log-rank test and/or Cox model. Many such proposals have been\nmade in recent years, but there remains a lack of practical guidance on\nimplementation, particularly in the context of group-sequential designs. In\nthis article, we aim to fill this gap. We discuss how the POPLAR trial, which\ncompared immunotherapy versus chemotherapy in non-small-cell lung cancer, might\nhave been re-designed to be more robust to the presence of a delayed effect. We\nthen provide step-by-step instructions on how to analyse a hypothetical\nrealisation of the trial, based on this new design. Basic theory on weighted\nlog-rank tests and group-sequential methods is covered, and an accompanying R\npackage (including vignette) is provided.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 16:23:30 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 07:46:59 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Magirr", "Dominic", ""], ["Jim\u00e9nez", "Jos\u00e9 L.", ""]]}, {"id": "2102.05591", "submitter": "Sotiris Tegos", "authors": "Sotiris A. Tegos, Dimitrios Tyrovolas, Panagiotis D. Diamantoulakis,\n  and George K. Karagiannidis", "title": "On the Distribution of the Sum of Double-Nakagami-m Random Vectors and\n  Application in Randomly Reconfigurable Surfaces", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT eess.SP math.IT stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reconfigurable intelligent surfaces (RISs) intend to improve significantly\nthe performance of future wireless networks, by controlling the wireless\npropagation medium through elements that can shift the phase of the reflected\nsignals. Although ideally the signals reflected from a RIS are added coherently\nat the receiver, this is very challenging in practice due to the requirement\nfor perfect channel state information (CSI) at the RIS and phase control. To\nfacilitate the performance analysis of more practical RIS-assisted systems,\nfirst, we present novel closed-form expressions for the probability density\nfunction, the cumulative distribution function, the moments, and the\ncharacteristic function of the distribution of the sum of double-Nakagami-m\nrandom vectors, whose amplitudes follow the double-Nakagami-m distribution,\ni.e., the distribution of the product of two random variables following the\nNakagami-m distribution, and phases are circular uniformly distributed. We also\nconsider a special case of this distribution, namely the distribution of the\nsum of Rayleigh-Nakagami-m random vectors. Then, we exploit these expressions\nto investigate the performance of the RIS-assisted composite channel, assuming\nthat the two links undergo Nakagami-m fading and the equivalent phase follows\nthe uniform distribution, which corresponds to the case where CSI is not\navailable at the RIS and leads to a lower bound of the performance of a system\nwith CSI. Closed-form expressions for the outage probability, the average\nreceived signal-to-noise ratio, the ergodic capacity, the bit error\nprobability, the amount of fading, and the channel quality estimation index are\nprovided to evaluate the performance of the considered system. These metrics\nare also derived for the practical special case where one of the two links\nundergoes Rayleigh fading.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 17:41:16 GMT"}, {"version": "v2", "created": "Sat, 29 May 2021 11:38:17 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Tegos", "Sotiris A.", ""], ["Tyrovolas", "Dimitrios", ""], ["Diamantoulakis", "Panagiotis D.", ""], ["Karagiannidis", "George K.", ""]]}, {"id": "2102.05771", "submitter": "Ziv Pollak", "authors": "Ziv Pollak", "title": "Predicting Customer Lifetime Values -- ecommerce use case", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Predicting customer future purchases and lifetime value is a key metrics for\nmanaging marketing campaigns and optimizing marketing spend. This task is\nspecifically challenging when the relationships between the customer and the\nfirm are of a noncontractual nature and therefore the future purchases need to\nbe predicted based mostly on historical purchases. This work compares two\napproaches to predict customer future purchases, first using a\n'buy-till-you-die' statistical model to predict customer behavior and later\nusing a neural network on the same dataset and comparing the results. This\ncomparison will lead to both quantitative and qualitative analysis of those two\nmethods as well as recommendation on how to proceed in different cases and\nopportunities for future research.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 23:17:16 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Pollak", "Ziv", ""]]}, {"id": "2102.05784", "submitter": "Christopher Blier-Wong", "authors": "Christopher Blier-Wong, Jean-Thomas Baillargeon, H\\'el\\`ene Cossette,\n  Luc Lamontagne, Etienne Marceau", "title": "Rethinking Representations in P&C Actuarial Science with Deep Neural\n  Networks", "comments": "27 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Insurance companies gather a growing variety of data for use in the insurance\nprocess, but most traditional ratemaking models are not designed to support\nthem. In particular, many emerging data sources (text, images, sensors) may\ncomplement traditional data to provide better insights to predict the future\nlosses in an insurance contract. This paper presents some of these emerging\ndata sources and presents a unified framework for actuaries to incorporate\nthese in existing ratemaking models. Our approach stems from representation\nlearning, whose goal is to create representations of raw data. A useful\nrepresentation will transform the original data into a dense vector space where\nthe ultimate predictive task is simpler to model. Our paper presents methods to\ntransform non-vectorial data into vectorial representations and provides\nexamples for actuarial science.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 00:10:56 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Blier-Wong", "Christopher", ""], ["Baillargeon", "Jean-Thomas", ""], ["Cossette", "H\u00e9l\u00e8ne", ""], ["Lamontagne", "Luc", ""], ["Marceau", "Etienne", ""]]}, {"id": "2102.05805", "submitter": "Fan Zhou", "authors": "Fan Zhou, Shikai Luo, Xiaohu Qie, Jieping Ye, Hongtu Zhu", "title": "Graph-Based Equilibrium Metrics for Dynamic Supply-Demand Systems with\n  Applications to Ride-sourcing Platforms", "comments": "Accepted by Journal of the American Statistical Association", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  How to dynamically measure the local-to-global spatio-temporal coherence\nbetween demand and supply networks is a fundamental task for ride-sourcing\nplatforms, such as DiDi. Such coherence measurement is critically important for\nthe quantification of the market efficiency and the comparison of different\nplatform policies, such as dispatching. The aim of this paper is to introduce a\ngraph-based equilibrium metric (GEM) to quantify the distance between demand\nand supply networks based on a weighted graph structure. We formulate GEM as\nthe optimal objective value of an unbalanced transport problem, which can be\nefficiently solved by optimizing an equivalent linear programming. We examine\nhow the GEM can help solve three operational tasks of ride-sourcing platforms.\nThe first one is that GEM achieves up to 70.6% reduction in root-mean-square\nerror over the second-best distance measurement for the prediction accuracy.\nThe second one is that the use of GEM for designing order dispatching policy\nincreases answer rate and drivers' revenue for more than 1%, representing a\nhuge improvement in number. The third one is that GEM is to serve as an\nendpoint for comparing different platform policies in AB test.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 01:56:11 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 11:34:19 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Zhou", "Fan", ""], ["Luo", "Shikai", ""], ["Qie", "Xiaohu", ""], ["Ye", "Jieping", ""], ["Zhu", "Hongtu", ""]]}, {"id": "2102.06016", "submitter": "Elizabeth Bismut", "authors": "Elizabeth Bismut and Daniel Straub", "title": "Optimal adaptive inspection and maintenance planning for deteriorating\n  structural systems", "comments": null, "journal-ref": "Reliab.Eng.Syst.Saf. 215 (2021) 107891", "doi": "10.1016/j.ress.2021.107891", "report-no": null, "categories": "eess.SY cs.SY math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimizing inspection and maintenance (I&M) plans for a large deteriorating\nstructure is a computationally challenging task, in particular if one considers\ninterdependences among its components. This is due to the sheer number of\npossible decision alternatives over the lifetime of the structure and the\nuncertainty surrounding the deterioration processes, the structural performance\nand the outcomes of inspection and maintenance actions. To address this\nchallenge, Luque and Straub (2019) proposed a heuristic approach in which I&M\nplans for structural systems are defined through a set of simple decision\nrules. Here, we formalize the optimization of these decision rules and extend\nthe approach to enable adaptive planning. The initially optimal I&M plan is\nsuccessively adapted throughout the service life, based on past inspection and\nmonitoring results. The proposed methodology uses stochastic deterioration\nmodels and accounts for the interdependence among structural components. The\nheuristic-based adaptive planning is illustrated for a structural frame\nsubjected to fatigue.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 08:59:25 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Bismut", "Elizabeth", ""], ["Straub", "Daniel", ""]]}, {"id": "2102.06121", "submitter": "Monica Alexander", "authors": "Monica Alexander and Leontine Alkema", "title": "A Bayesian cohort component projection model to estimate adult\n  populations at the subnational level in data-sparse settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accurate estimates of subnational populations are important for policy\nformulation and monitoring population health indicators. For example, estimates\nof the number of women of reproductive age are important to understand the\npopulation at risk to maternal mortality and unmet need for contraception.\nHowever, in many low-income countries, data on population counts and components\nof population change are limited, and so levels and trends subnationally are\nunclear. We present a Bayesian constrained cohort component model for the\nestimation and projection of subnational populations. The model builds on a\ncohort component projection framework, incorporates census data and estimates\nfrom the United Nation's World Population Prospects, and uses characteristic\nmortality schedules to obtain estimates of population counts and the components\nof population change, including internal migration. The data required as inputs\nto the model are minimal and available across a wide range of countries,\nincluding most low-income countries. The model is applied to estimate and\nproject populations by county in Kenya for 1979-2019, and validated against the\n2019 Kenyan census.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 17:01:13 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 19:22:18 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Alexander", "Monica", ""], ["Alkema", "Leontine", ""]]}, {"id": "2102.06197", "submitter": "Johannes Ernst-Emanuel Buck", "authors": "Ngoc Mai Tran and Johannes Buck and Claudia Kl\\\"uppelberg", "title": "Causal Discovery of a River Network from its Extremes", "comments": "26 pages, 15 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal inference for extremes aims to discover cause and effect relations\nbetween large observed values of random variables. Over the last years, a\nnumber of methods have been proposed for solving the Hidden River Problem, with\nthe Danube data set as benchmark. In this paper, we provide \\QTree, a new and\nsimple algorithm to solve the Hidden River Problem that outperforms existing\nmethods. \\QTree\\ returns a directed graph and achieves almost perfect recovery\non the Danube as well as on new data from the Lower Colorado River. It can\nhandle missing data, has an automated parameter tuning procedure, and runs in\ntime $O(n |V|^2)$, where $n$ is the number of observations and $|V|$ the number\nof nodes in the graph. \\QTree\\ relies on qualitative aspects of the max-linear\nBayesian network model.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 18:57:21 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Tran", "Ngoc Mai", ""], ["Buck", "Johannes", ""], ["Kl\u00fcppelberg", "Claudia", ""]]}, {"id": "2102.06286", "submitter": "Yu Song", "authors": "Yu Song, Madhav V. Chitturi, David A. Noyce", "title": "Automated Vehicle Crash Sequences: Patterns and Potential Uses in Safety\n  Testing", "comments": null, "journal-ref": "Accident Analysis & Prevention, 153, p.106017 (2021)", "doi": "10.1016/j.aap.2021.106017", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With safety being one of the primary motivations for developing automated\nvehicles (AVs), extensive field and simulation tests are being carried out to\nensure AVs can operate safely on roadways. Since 2014, the California DMV has\nbeen collecting AV collision and disengagement reports, which are valuable data\nsources for studying AV crash patterns. In this study, crash sequence data\nextracted from California AV collision reports were used to investigate\npatterns and how they may be used to develop AV test scenarios. Employing\nsequence analysis, this study evaluated 168 AV crashes (with AV in automatic\ndriving mode before disengagement or collision) from 2015 to 2019. Analysis of\nsubsequences showed that the most representative pattern in AV crashes was\n(collision following AV stop) type. Analysis of event transition showed that\ndisengagement, as an event in 24 percent of all studied AV crash sequences, had\na transition probability of 68 percent to an immediate collision. Cluster\nanalysis characterized AV crash sequences into seven groups with distinctive\ncrash dynamic features. Cross-tabulation analysis showed that sequence groups\nwere significantly associated with variables measuring crash outcomes and\ndescribing environmental conditions. Crash sequences are useful for developing\nAV test scenarios. Based on the findings, a scenario-based AV safety testing\nframework was proposed with sequence of events embedded as a core component.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 22:13:39 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Song", "Yu", ""], ["Chitturi", "Madhav V.", ""], ["Noyce", "David A.", ""]]}, {"id": "2102.06381", "submitter": "Panayotis Papoutsis", "authors": "Panayotis Papoutsis (LPSM), Safa Fennia, Constant Bridon, Tarn Duong", "title": "Relaxing door-to-door matching reduces passenger waiting times: a\n  workflow for the analysis of driver GPS traces in a stochastic carpooling\n  service", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Carpooling has the potential to transform itself into a mass transportation\nmode by abandoning its adherence to deterministic passenger-driver matching for\ndoor-to-door journeys, and by adopting instead stochastic matching on a network\nof fixed meeting points. Stochastic matching is where a passenger sends out a\ncarpooling request at a meeting point, and then waits for the arrival of a\nself-selected driver who is already travelling to the requested meeting point.\nCrucially there is no centrally dispatched driver. Moreover, the carpooling is\nassured only between the meeting points, so the onus is on the passengers to\ntravel to/from them by their own means. Thus the success of a stochastic\ncarpooling service relies on the convergence, with minimal perturbation to\ntheir existing travel patterns, to the meeting points which are highly\nfrequented by both passengers and drivers. Due to the innovative nature of\nstochastic carpooling, existing off-the-shelf workflows are largely\ninsufficient for this purpose. To fill the gap in the market, we introduce a\nnovel workflow, comprising of a combination of data science and GIS (Geographic\nInformation Systems), to analyse driver GPS traces. We implement it for an\noperational stochastic carpooling service in south-eastern France, and we\ndemonstrate that relaxing door-to-door matching reduces passenger waiting\ntimes. Our workflow provides additional key operational indicators, namely the\ndriver flow maps, the driver flow temporal profiles and the driver\nparticipation rates.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 07:55:57 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Papoutsis", "Panayotis", "", "LPSM"], ["Fennia", "Safa", ""], ["Bridon", "Constant", ""], ["Duong", "Tarn", ""]]}, {"id": "2102.06739", "submitter": "Francesco Finazzi", "authors": "R\\'emy Bossu, Francesco Finazzi, Robert Steed, Laure Fallou, Istv\\'an\n  Bond\\'ar", "title": "\"Shaking in 5 seconds!\" A Voluntary Smartphone-based Earthquake Early\n  Warning System", "comments": "Second version, 19 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Public earthquake early warning systems have the potential to reduce\nindividual risk by warning people of an incoming tremor but their development\nhas been hampered by costly infrastructure. Furthermore, users' understanding\nof such a service and their reactions to warnings remains poorly studied. The\nsmartphone app of the Earthquake Network initiative turns users' smartphones\ninto motion detectors and provides the first example of purely smartphone-based\nearthquake early warnings, without the need for dedicated seismic station\ninfrastructure and operating in multiple countries. We demonstrate here that\nearly warnings have been emitted in multiple countries even for damaging\nshaking levels and so this offers an alternative in the many regions unlikely\nto be covered by conventional early warning systems in the foreseeable future.\nWe also show that although warnings are understood and appreciated by users,\nnotably to get psychologically prepared, only a fraction take protective\nactions such as \"drop, cover and hold\".\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 19:44:57 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 12:48:30 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Bossu", "R\u00e9my", ""], ["Finazzi", "Francesco", ""], ["Steed", "Robert", ""], ["Fallou", "Laure", ""], ["Bond\u00e1r", "Istv\u00e1n", ""]]}, {"id": "2102.06814", "submitter": "Viet Hung Dao", "authors": "Viet-Hung Dao (1), David Gunawan (2), Minh-Ngoc Tran (3), Robert Kohn\n  (1), Guy E. Hawkins (4), Scott D. Brown (4) ((1) Australian School of\n  Business, University of New South Wales, Sydney, Australia, (2) School of\n  Mathematics and Applied Statistics, University of Wollongong, (3) Discipline\n  of Business Analytics, University of Sydney Business School, (4) School of\n  Psychology, University of Newcastle, Australia)", "title": "Efficient Selection Between Hierarchical Cognitive Models:\n  Cross-validation With Variational Bayes", "comments": "35 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model comparison is the cornerstone of theoretical progress in psychological\nresearch. Common practice overwhelmingly relies on tools that evaluate\ncompeting models by balancing in-sample descriptive adequacy against model\nflexibility, with modern approaches advocating the use of marginal likelihood\nfor hierarchical cognitive models. Cross-validation is another popular approach\nbut its implementation has remained out of reach for cognitive models evaluated\nin a Bayesian hierarchical framework, with the major hurdle being prohibitive\ncomputational cost. To address this issue, we develop novel algorithms that\nmake variational Bayes (VB) inference for hierarchical models feasible and\ncomputationally efficient for complex cognitive models of substantive\ntheoretical interest. It is well known that VB produces good estimates of the\nfirst moments of the parameters which gives good predictive densities\nestimates. We thus develop a novel VB algorithm with Bayesian prediction as a\ntool to perform model comparison by cross-validation, which we refer to as\nCVVB. In particular, the CVVB can be used as a model screening device that\nquickly identifies bad models. We demonstrate the utility of CVVB by revisiting\na classic question in decision making research: what latent components of\nprocessing drive the ubiquitous speed-accuracy tradeoff? We demonstrate that\nCVVB strongly agrees with model comparison via marginal likelihood yet achieves\nthe outcome in much less time. Our approach brings cross-validation within\nreach of theoretically important psychological models, and makes it feasible to\ncompare much larger families of hierarchically specified cognitive models than\nhas previously been possible.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 23:16:37 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Dao", "Viet-Hung", ""], ["Gunawan", "David", ""], ["Tran", "Minh-Ngoc", ""], ["Kohn", "Robert", ""], ["Hawkins", "Guy E.", ""], ["Brown", "Scott D.", ""]]}, {"id": "2102.07566", "submitter": "Antonio Pievatolo", "authors": "Sara Pasquali, Antonio Pievatolo, Antonella Bodini, Fabrizio Ruggeri", "title": "A stochastic SIR model for the analysis of the COVID-19 Italian epidemic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose a stochastic SIR model, specified as a system of stochastic\ndifferential equations, to analyse the data of the Italian COVID-19 epidemic,\ntaking also into account the under-detection of infected and recovered\nindividuals in the population. We find that a correct assessment of the amount\nof under-detection is important to obtain reliable estimates of the critical\nmodel parameters. Moreover, a single SIR model over the whole epidemic period\nis unable to correctly describe the behaviour of the pandemic. Then, the\nadaptation of the model in every time-interval between relevant government\ndecrees that implement contagion mitigation measures, provides short-term\npredictions and a continuously updated assessment of the basic reproduction\nnumber.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 14:09:42 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 07:44:40 GMT"}, {"version": "v3", "created": "Fri, 19 Feb 2021 13:33:06 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Pasquali", "Sara", ""], ["Pievatolo", "Antonio", ""], ["Bodini", "Antonella", ""], ["Ruggeri", "Fabrizio", ""]]}, {"id": "2102.07612", "submitter": "Ath\\'ena\\\"is Gautier", "authors": "Ath\\'ena\\\"is Gautier, David Ginsbourger, Guillaume Pirot", "title": "Goal-oriented adaptive sampling under random field modelling of response\n  probability distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the study of natural and artificial complex systems, responses that are\nnot completely determined by the considered decision variables are commonly\nmodelled probabilistically, resulting in response distributions varying across\ndecision space. We consider cases where the spatial variation of these response\ndistributions does not only concern their mean and/or variance but also other\nfeatures including for instance shape or uni-modality versus multi-modality.\nOur contributions build upon a non-parametric Bayesian approach to modelling\nthe thereby induced fields of probability distributions, and in particular to a\nspatial extension of the logistic Gaussian model.\n  The considered models deliver probabilistic predictions of response\ndistributions at candidate points, allowing for instance to perform\n(approximate) posterior simulations of probability density functions, to\njointly predict multiple moments and other functionals of target distributions,\nas well as to quantify the impact of collecting new samples on the state of\nknowledge of the distribution field of interest. In particular, we introduce\nadaptive sampling strategies leveraging the potential of the considered random\ndistribution field models to guide system evaluations in a goal-oriented way,\nwith a view towards parsimoniously addressing calibration and related problems\nfrom non-linear (stochastic) inversion and global optimisation.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 15:55:23 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 16:42:47 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Gautier", "Ath\u00e9na\u00efs", ""], ["Ginsbourger", "David", ""], ["Pirot", "Guillaume", ""]]}, {"id": "2102.07679", "submitter": "Purvasha Chakravarti Dr.", "authors": "Purvasha Chakravarti, Mikael Kuusela, Jing Lei and Larry Wasserman", "title": "Model-Independent Detection of New Physics Signals Using Interpretable\n  Semi-Supervised Classifier Tests", "comments": "35 pages, 6 figures and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP hep-ph physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central goal in experimental high energy physics is to detect new physics\nsignals that are not explained by known physics. In this paper, we aim to\nsearch for new signals that appear as deviations from known Standard Model\nphysics in high-dimensional particle physics data. To do this, we determine\nwhether there is any statistically significant difference between the\ndistribution of Standard Model background samples and the distribution of the\nexperimental observations, which are a mixture of the background and a\npotential new signal. Traditionally, one also assumes access to a sample from a\nmodel for the hypothesized signal distribution. Here we instead investigate a\nmodel-independent method that does not make any assumptions about the signal\nand uses a semi-supervised classifier to detect the presence of the signal in\nthe experimental data. We construct three test statistics using the classifier:\nan estimated likelihood ratio test (LRT) statistic, a test based on the area\nunder the ROC curve (AUC), and a test based on the misclassification error\n(MCE). Additionally, we propose a method for estimating the signal strength\nparameter and explore active subspace methods to interpret the proposed\nsemi-supervised classifier in order to understand the properties of the\ndetected signal. We investigate the performance of the methods on a data set\nrelated to the search for the Higgs boson at the Large Hadron Collider at CERN.\nWe demonstrate that the semi-supervised tests have power competitive with the\nclassical supervised methods for a well-specified signal, but much higher power\nfor an unexpected signal which might be entirely missed by the supervised\ntests.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 17:11:18 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 17:12:15 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Chakravarti", "Purvasha", ""], ["Kuusela", "Mikael", ""], ["Lei", "Jing", ""], ["Wasserman", "Larry", ""]]}, {"id": "2102.07752", "submitter": "Jalmar Manuel  Farfan Carrasco", "authors": "Lizandra Castilho Fabio, Cristian Villegas, Jalmar M. F. Carrasco,\n  M\\'ario de Castro", "title": "Diagnostic tools for a multivariate negative binomial model for fitting\n  correlated data with overdispersion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We focus on the development of diagnostic tools and an R package called MNB\nfor a multivariate negative binomial (MNB) regression model for detecting\natypical and influential subjects. The MNB model is deduced from a Poisson\nmixed model in which the random intercept follows the generalized log-gamma\n(GLG) distribution. The MNB model for correlated count data leads to an MNB\nregression model that inherits the features of a hierarchical model to\naccommodate the intraclass correlation and the occurrence of overdispersion\nsimultaneously. The asymptotic consistency of the dispersion parameter\nestimator depends on the asymmetry of the GLG distribution. Inferential\nprocedures for the MNB regression model are simple, although it can provide\ninconsistent estimates of the asymptotic variance when the correlation\nstructure is misspecified. We propose the randomized quantile residual for\nchecking the adequacy of the multivariate model, and derive global and local\ninfluence measures from the multivariate model to assess influential subjects.\nFinally, two applications are presented in the data analysis section. The code\nfor installing the MNB package and the code used in the two examples is\nexhibited in the Appendix.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 18:48:11 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Fabio", "Lizandra Castilho", ""], ["Villegas", "Cristian", ""], ["Carrasco", "Jalmar M. F.", ""], ["de Castro", "M\u00e1rio", ""]]}, {"id": "2102.08111", "submitter": "Clara B. Salucci", "authors": "Clara B. Salucci, Azzeddine Bakdi, Ingrid K. Glad, Erik Vanem,\n  Riccardo De Bin", "title": "Simple statistical models and sequential deep learning for Lithium-ion\n  batteries degradation under dynamic conditions: Fractional Polynomials vs\n  Neural Networks", "comments": "42 pages, 7 figures, submitted to Journal of Power Sources", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Longevity and safety of Lithium-ion batteries are facilitated by efficient\nmonitoring and adjustment of the battery operating conditions: hence, it is\ncrucial to implement fast and accurate algorithms for State of Health (SoH)\nmonitoring on the Battery Management System. The task is challenging due to the\ncomplexity and multitude of the factors contributing to the battery capacity\ndegradation, especially because the different degradation processes occur at\nvarious timescales and their interactions play an important role. This paper\nproposes and compares two data-driven approaches: a Long Short-Term Memory\nneural network, from the field of deep learning, and a Multivariable Fractional\nPolynomial regression, from classical statistics. Models from both classes are\ntrained from historical data of one exhausted cell and used to predict the SoH\nof other cells. This work uses data provided by the NASA Ames Prognostics\nCenter of Excellence, characterised by varying loads which simulate dynamic\noperating conditions. Two hypothetical scenarios are considered: one assumes\nthat a recent true capacity measurement is known, the other relies solely on\nthe cell nominal capacity. Both methods are effective, with low prediction\nerrors, and the advantages of one over the other in terms of interpretability\nand complexity are discussed in a critical way.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 12:26:23 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Salucci", "Clara B.", ""], ["Bakdi", "Azzeddine", ""], ["Glad", "Ingrid K.", ""], ["Vanem", "Erik", ""], ["De Bin", "Riccardo", ""]]}, {"id": "2102.08162", "submitter": "Nicolas Pr\\\"ollochs", "authors": "Kirill Solovev, Nicolas Pr\\\"ollochs", "title": "Integrating Floor Plans into Hedonic Models for Rent Price Appraisal", "comments": null, "journal-ref": null, "doi": "10.1145/3442381.3449967", "report-no": null, "categories": "cs.LG econ.GN q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online real estate platforms have become significant marketplaces\nfacilitating users' search for an apartment or a house. Yet it remains\nchallenging to accurately appraise a property's value. Prior works have\nprimarily studied real estate valuation based on hedonic price models that take\nstructured data into account while accompanying unstructured data is typically\nignored. In this study, we investigate to what extent an automated visual\nanalysis of apartment floor plans on online real estate platforms can enhance\nhedonic rent price appraisal. We propose a tailored two-staged deep learning\napproach to learn price-relevant designs of floor plans from historical price\ndata. Subsequently, we integrate the floor plan predictions into hedonic rent\nprice models that account for both structural and locational characteristics of\nan apartment. Our empirical analysis based on a unique dataset of 9174 real\nestate listings suggests that current hedonic models underutilize the available\ndata. We find that (1) the visual design of floor plans has significant\nexplanatory power regarding rent prices - even after controlling for structural\nand locational apartment characteristics, and (2) harnessing floor plans\nresults in an up to 10.56% lower out-of-sample prediction error. We further\nfind that floor plans yield a particularly high gain in prediction performance\nfor older and smaller apartments. Altogether, our empirical findings contribute\nto the existing research body by establishing the link between the visual\ndesign of floor plans and real estate prices. Moreover, our approach has\nimportant implications for online real estate platforms, which can use our\nfindings to enhance user experience in their real estate listings.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 14:05:33 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Solovev", "Kirill", ""], ["Pr\u00f6llochs", "Nicolas", ""]]}, {"id": "2102.08189", "submitter": "Marco Ortu", "authors": "Marco Ortu, Nicola Uras, Claudio Conversano, Giuseppe Destefanis,\n  Silvia Bartolucci", "title": "On Technical Trading and Social Media Indicators in Cryptocurrencies'\n  Price Classification Through Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work aims to analyse the predictability of price movements of\ncryptocurrencies on both hourly and daily data observed from January 2017 to\nJanuary 2021, using deep learning algorithms. For our experiments, we used\nthree sets of features: technical, trading and social media indicators,\nconsidering a restricted model of only technical indicators and an unrestricted\nmodel with technical, trading and social media indicators. We verified whether\nthe consideration of trading and social media indicators, along with the\nclassic technical variables (such as price's returns), leads to a significative\nimprovement in the prediction of cryptocurrencies price's changes. We conducted\nthe study on the two highest cryptocurrencies in volume and value (at the time\nof the study): Bitcoin and Ethereum. We implemented four different machine\nlearning algorithms typically used in time-series classification problems:\nMulti Layers Perceptron (MLP), Convolutional Neural Network (CNN), Long Short\nTerm Memory (LSTM) neural network and Attention Long Short Term Memory (ALSTM).\nWe devised the experiments using the advanced bootstrap technique to consider\nthe variance problem on test samples, which allowed us to evaluate a more\nreliable estimate of the model's performance. Furthermore, the Grid Search\ntechnique was used to find the best hyperparameters values for each implemented\nalgorithm. The study shows that, based on the hourly frequency results, the\nunrestricted model outperforms the restricted one. The addition of the trading\nindicators to the classic technical indicators improves the accuracy of Bitcoin\nand Ethereum price's changes prediction, with an increase of accuracy from a\nrange of 51-55% for the restricted model, to 67-84% for the unrestricted model.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 13:18:36 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 07:00:59 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Ortu", "Marco", ""], ["Uras", "Nicola", ""], ["Conversano", "Claudio", ""], ["Destefanis", "Giuseppe", ""], ["Bartolucci", "Silvia", ""]]}, {"id": "2102.08455", "submitter": "Stefan Bender", "authors": "Stefan Bender, Miriam Sinnhuber, Patrick J. Espy, John P. Burrows", "title": "Mesospheric nitric oxide model from SCIAMACHY data", "comments": "13 pages, 6 figures; published in Atmos. Chem. Phys", "journal-ref": "Atmos. Chem. Phys., 19, 2135--2147, 2019", "doi": "10.5194/acp-19-2135-2019", "report-no": null, "categories": "physics.ao-ph astro-ph.EP physics.space-ph stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an empirical model for nitric oxide NO in the mesosphere\n($\\approx$60--90 km) derived from SCIAMACHY (SCanning Imaging Absorption\nspectroMeter for Atmospheric CHartoghraphY) limb scan data. This work\ncomplements and extends the NOEM (Nitric Oxide Empirical Model; Marsh et al.,\n2004) and SANOMA (SMR Acquired Nitric Oxide Model Atmosphere; Kiviranta et al.,\n2018) empirical models in the lower thermosphere. The regression ansatz builds\non the heritage of studies by Hendrickx et al. (2017) and the superposed epoch\nanalysis by Sinnhuber et al. (2016) which estimate NO production from particle\nprecipitation.\n  Our model relates the daily (longitudinally) averaged NO number densities\nfrom SCIAMACHY (Bender et al., 2017a, b) as a function of geomagnetic latitude\nto the solar Lyman-alpha and the geomagnetic AE (auroral electrojet) indices.\nWe use a non-linear regression model, incorporating a finite and seasonally\nvarying lifetime for the geomagnetically induced NO. We estimate the parameters\nby finding the maximum posterior probability and calculate the parameter\nuncertainties using Markov chain Monte Carlo sampling. In addition to providing\nan estimate of the NO content in the mesosphere, the regression coefficients\nindicate regions where certain processes dominate.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 21:22:57 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Bender", "Stefan", ""], ["Sinnhuber", "Miriam", ""], ["Espy", "Patrick J.", ""], ["Burrows", "John P.", ""]]}, {"id": "2102.08573", "submitter": "Aditya Deshmukh", "authors": "Aditya Deshmukh, Jing Liu and Venugopal V. Veeravalli", "title": "Robust Mean Estimation in High Dimensions via Global Outlier Pursuit", "comments": "arXiv admin note: text overlap with arXiv:2008.09239", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the robust mean estimation problem in high dimensions, where less\nthan half of the datapoints can be arbitrarily corrupted. Motivated by\ncompressive sensing, we formulate the robust mean estimation problem as the\nminimization of the $\\ell_0$-`norm' of an \\emph{outlier indicator vector},\nunder a second moment constraint on the datapoints. We further relax the\n$\\ell_0$-`norm' to the $\\ell_p$-norm ($0<p\\leq 1$) in the objective and prove\nthat the global minima for each of these objectives are order-optimal for the\nrobust mean estimation problem. Then we propose a computationally tractable\niterative $\\ell_p$-minimization and hard thresholding algorithm that outputs an\norder-optimal robust estimate of the population mean. Both synthetic and real\ndata experiments demonstrate that the proposed algorithm outperforms\nstate-of-the-art robust mean estimation methods. The source code will be made\navailable at GitHub.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 04:45:49 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Deshmukh", "Aditya", ""], ["Liu", "Jing", ""], ["Veeravalli", "Venugopal V.", ""]]}, {"id": "2102.08729", "submitter": "Kieran Kalair", "authors": "Kieran Kalair and Colm Connaughton", "title": "Dynamic and interpretable hazard-based models of traffic incident\n  durations", "comments": "38 pages, 15 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding and predicting the duration or \"return-to-normal\" time of\ntraffic incidents is important for system-level management and optimisation of\nroad transportation networks. Increasing real-time availability of multiple\ndata sources characterising the state of urban traffic networks, together with\nadvances in machine learning offer the opportunity for new and improved\napproaches to this problem that go beyond static statistical analyses of\nincident duration. In this paper we consider two such improvements: dynamic\nupdate of incident duration predictions as new information about incidents\nbecomes available and automated interpretation of the factors responsible for\nthese predictions. For our use case, we take one year of incident data and\ntraffic state time-series from the M25 motorway in London. We use it to train\nmodels that predict the probability distribution of incident durations,\nutilising both time-invariant and time-varying features of the data. The latter\nallow predictions to be updated as an incident progresses, and more information\nbecomes available. For dynamic predictions, time-series features are fed into\nthe Match-Net algorithm, a temporal convolutional hitting-time network,\nrecently developed for dynamical survival analysis in clinical applications.\nThe predictions are benchmarked against static regression models for survival\nanalysis and against an established dynamic technique known as landmarking and\nfound to perform favourably by several standard comparison measures. To provide\ninterpretability, we utilise the concept of Shapley values from the domain of\ninterpretable artificial intelligence to rank the features most relevant to the\nmodel predictions at different time horizons. For example, the time of day is\nalways a significantly influential time-invariant feature, whereas the\ntime-series features strongly influence predictions at 5 and 60-minute\nhorizons.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 12:45:51 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Kalair", "Kieran", ""], ["Connaughton", "Colm", ""]]}, {"id": "2102.08803", "submitter": "Till Massing", "authors": "Till Massing, Natalie Reckmann, Jens Klenke, Benjamin Otto, Christoph\n  Hanck, Michael Goedicke", "title": "Effects of Early Warning Emails on Student Performance", "comments": "arXiv admin note: text overlap with arXiv:1906.09864", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We use learning data of an e-assessment platform for an introductory\nmathematical statistics course to predict the probability of passing the final\nexam for each student. Based on these estimated probabilities we sent warning\nemails to students in the next cohort with a low predicted probability to pass.\nWe analyze the effect of this treatment and propose statistical models to\nquantify the effect of the email notification. We detect a small but\nimprecisely estimated effect suggesting effectiveness of such interventions\nonly when administered more intensively.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 15:03:16 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Massing", "Till", ""], ["Reckmann", "Natalie", ""], ["Klenke", "Jens", ""], ["Otto", "Benjamin", ""], ["Hanck", "Christoph", ""], ["Goedicke", "Michael", ""]]}, {"id": "2102.08939", "submitter": "St\\'ephanie Jehan-Besson", "authors": "S. Jehan-Besson and R. Clouard and C. Tilmant and A. de Cesare and A.\n  Lalande and J. Lebenberg and P. Clarysse and L. Sarry and F. Frouin and M.\n  Garreau", "title": "A Mutual Reference Shape for Segmentation Fusion and Evaluation", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.35834.11209", "report-no": null, "categories": "eess.IV stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes the estimation of a mutual shape from a set of different\nsegmentation results using both active contours and information theory. The\nmutual shape is here defined as a consensus shape estimated from a set of\ndifferent segmentations of the same object. In an original manner, such a shape\nis defined as the minimum of a criterion that benefits from both the mutual\ninformation and the joint entropy of the input segmentations. This energy\ncriterion is justified using similarities between information theory quantities\nand area measures, and presented in a continuous variational framework. In\norder to solve this shape optimization problem, shape derivatives are computed\nfor each term of the criterion and interpreted as an evolution equation of an\nactive contour. A mutual shape is then estimated together with the sensitivity\nand specificity of each segmentation. Some synthetic examples allow us to cast\nthe light on the difference between the mutual shape and an average shape. The\napplicability of our framework has also been tested for segmentation evaluation\nand fusion of different types of real images (natural color images, old\nmanuscripts, medical images).\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 18:15:40 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Jehan-Besson", "S.", ""], ["Clouard", "R.", ""], ["Tilmant", "C.", ""], ["de Cesare", "A.", ""], ["Lalande", "A.", ""], ["Lebenberg", "J.", ""], ["Clarysse", "P.", ""], ["Sarry", "L.", ""], ["Frouin", "F.", ""], ["Garreau", "M.", ""]]}, {"id": "2102.09008", "submitter": "Suchit Mehrotra", "authors": "Suchit Mehrotra, Halley Brantley, Peter Onglao, Patricia Bata, Roland\n  Romero, Jacob Westman, Lauren Bangerter, Arnab Maity", "title": "Divide-and-Conquer MCMC for Multivariate Binary Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of large scale medical claims data has the potential to improve\nquality of care by generating insights which can be used to create tailored\nmedical programs. In particular, the multivariate probit model can be used to\ninvestigate the correlation between multiple binary responses of interest in\nsuch data, e.g. the presence of multiple chronic conditions. Bayesian modeling\nis well suited to such analyses because of the automatic uncertainty\nquantification provided by the posterior distribution. A complicating factor is\nthat large medical claims datasets often do not fit in memory, which renders\nthe estimation of the posterior using traditional Markov Chain Monte Carlo\n(MCMC) methods computationally infeasible. To address this challenge, we extend\nexisting divide-and-conquer MCMC algorithms to the multivariate probit model,\ndemonstrating, via simulation, that they should be preferred over mean-field\nvariational inference when the estimation of the latent correlation structure\nbetween binary responses is of primary interest. We apply this algorithm to a\nlarge database of de-identified Medicare Advantage claims from a single large\nUS health insurance provider, where we find medically meaningful groupings of\ncommon chronic conditions and asses the impact of the urban-rural health gap by\nidentifying underutilized provider specialties in rural areas.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 20:02:17 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 20:36:37 GMT"}, {"version": "v3", "created": "Fri, 11 Jun 2021 17:19:34 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Mehrotra", "Suchit", ""], ["Brantley", "Halley", ""], ["Onglao", "Peter", ""], ["Bata", "Patricia", ""], ["Romero", "Roland", ""], ["Westman", "Jacob", ""], ["Bangerter", "Lauren", ""], ["Maity", "Arnab", ""]]}, {"id": "2102.09021", "submitter": "M\\'arton Karsai", "authors": "J\\'ulia Koltai, Orsolya V\\'as\\'arhelyi, Gergely R\\\"ost and M\\'arton\n  Karsai", "title": "Monitoring behavioural responses during pandemic via reconstructed\n  contact matrices from online and representative surveys", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.CY cs.SI stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The unprecedented behavioural responses of societies have been evidently\nshaping the COVID-19 pandemic, yet it is a significant challenge to accurately\nmonitor the continuously changing social mixing patterns in real-time. Contact\nmatrices, usually stratified by age, summarise interaction motifs efficiently,\nbut their collection relies on conventional representative survey techniques,\nwhich are expensive and slow to obtain. Here we report a data collection effort\ninvolving over $2.3\\%$ of the Hungarian population to simultaneously record\ncontact matrices through a longitudinal online and sequence of representative\nphone surveys. To correct non-representative biases characterising the online\ndata, by using census data and the representative samples we develop a\nreconstruction method to provide a scalable, cheap, and flexible way to\ndynamically obtain closer-to-representative contact matrices. Our results\ndemonstrate the potential of combined online-offline data collections to\nunderstand the changing behavioural responses determining the future evolution\nof the outbreak, and inform epidemic models with crucial data.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 20:39:59 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 20:06:28 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Koltai", "J\u00falia", ""], ["V\u00e1s\u00e1rhelyi", "Orsolya", ""], ["R\u00f6st", "Gergely", ""], ["Karsai", "M\u00e1rton", ""]]}, {"id": "2102.09061", "submitter": "Eddy Kwessi", "authors": "Eddy Kwessi, Lloyd Edwards", "title": "Analysis of EEG data using complex geometric structurization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP q-bio.NC stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Electroencephalogram (EEG) is a common tool used to understand brain\nactivities. The data are typically obtained by placing electrodes at the\nsurface of the scalp and recording the oscillations of currents passing through\nthe electrodes. These oscillations can sometimes lead to various\ninterpretations, depending on the subject's health condition, the experiment\ncarried out, the sensitivity of the tools used, human manipulations etc. The\ndata obtained over time can be considered a time series. There is evidence in\nthe literature that epilepsy EEG data may be chaotic. Either way, the embedding\ntheory in dynamical systems suggests that time series from a complex system\ncould be used to reconstruct its phase space under proper conditions. In this\npaper, we propose an analysis of epilepsy electroencephalogram time series data\nbased on a novel approach dubbed complex geometric structurization. Complex\ngeometric structurization stems from the construction of strange attractors\nusing embedding theory from dynamical systems. The complex geometric structures\nare themselves obtained using a geometry tool, namely the $\\alpha$-shapes from\nshape analysis. Initial analyses show a proof of concept in that these complex\nstructures capture the expected changes brain in lobes under consideration.\nFurther, a deeper analysis suggests that these complex structures can be used\nas biomarkers for seizure changes.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 22:49:33 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Kwessi", "Eddy", ""], ["Edwards", "Lloyd", ""]]}, {"id": "2102.09111", "submitter": "Dan Li", "authors": "Dan Li, Dariush Fooladivanda, Sonia Martinez", "title": "Online Optimization and Learning in Uncertain Dynamical Environments\n  with Performance Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.LG cs.SY math.DS math.OC stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework to solve online optimization and learning problems\nin unknown and uncertain dynamical environments. This framework enables us to\nsimultaneously learn the uncertain dynamical environment while making online\ndecisions in a quantifiably robust manner. The main technical approach relies\non the theory of distributional robust optimization that leverages adaptive\nprobabilistic ambiguity sets. However, as defined, the ambiguity set usually\nleads to online intractable problems, and the first part of our work is\ndirected to find reformulations in the form of online convex problems for two\nsub-classes of objective functions. To solve the resulting problems in the\nproposed framework, we further introduce an online version of the Nesterov\naccelerated-gradient algorithm. We determine how the proposed solution system\nachieves a probabilistic regret bound under certain conditions. Two\napplications illustrate the applicability of the proposed framework.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 01:49:06 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Li", "Dan", ""], ["Fooladivanda", "Dariush", ""], ["Martinez", "Sonia", ""]]}, {"id": "2102.09207", "submitter": "Anthony Strittmatter", "authors": "Anthony Strittmatter, Conny Wunsch", "title": "The Gender Pay Gap Revisited with Big Data: Do Methodological Choices\n  Matter?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The vast majority of existing studies that estimate the average unexplained\ngender pay gap use unnecessarily restrictive linear versions of the\nBlinder-Oaxaca decomposition. Using a notably rich and large data set of 1.7\nmillion employees in Switzerland, we investigate how the methodological\nimprovements made possible by such big data affect estimates of the unexplained\ngender pay gap. We study the sensitivity of the estimates with regard to i) the\navailability of observationally comparable men and women, ii) model flexibility\nwhen controlling for wage determinants, and iii) the choice of different\nparametric and semi-parametric estimators, including variants that make use of\nmachine learning methods. We find that these three factors matter greatly.\nBlinder-Oaxaca estimates of the unexplained gender pay gap decline by up to 39%\nwhen we enforce comparability between men and women and use a more flexible\nspecification of the wage equation. Semi-parametric matching yields estimates\nthat when compared with the Blinder-Oaxaca estimates, are up to 50% smaller and\nalso less sensitive to the way wage determinants are included.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 08:12:32 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 08:59:00 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Strittmatter", "Anthony", ""], ["Wunsch", "Conny", ""]]}, {"id": "2102.09288", "submitter": "Soudeep Deb", "authors": "Soudeep Deb", "title": "A mathematical take on the competitive balance of a football league", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Competitive balance in a football league is extremely important from the\nperspective of economic growth of the industry. Many researchers have earlier\nproposed different measures of competitive balance, which are primarily adapted\nfrom the standard economic theory. However, these measures fail to capture the\nfiner nuances of the game. In this work, we discuss a new framework which is\nmore suitable for a football league. First, we present a mathematical proof of\nan ideal situation where a football league becomes perfectly balanced. Next, a\ngoal based index for competitive balance is developed. We present relevant\ntheoretical results and show how the proposed index can be used to formally\ntest for the presence of imbalance. The methods are implemented on the data\nfrom top five European leagues, and it shows that the new approach can better\nexplain the changes in the seasonal competitive balance of the leagues.\nFurther, using appropriate panel data models, we show that the proposed index\nis more suitable to analyze the variability in total revenues of the football\nleagues.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 12:06:14 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Deb", "Soudeep", ""]]}, {"id": "2102.09403", "submitter": "Laura D'Angelo", "authors": "Laura D'Angelo, Antonio Canale, Zhaoxia Yu and Michele Guindani", "title": "Bayesian nonparametric analysis for the detection of spikes in noisy\n  calcium imaging data", "comments": "18 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advancements in miniaturized fluorescence microscopy have made it\npossible to investigate neuronal responses to external stimuli in awake\nbehaving animals through the analysis of intra-cellular calcium signals. An\non-going challenge is deconvoluting the temporal signals to extract the spike\ntrains from the noisy calcium signals' time-series. In this manuscript, we\npropose a nested Bayesian finite mixture specification that allows for the\nestimation of spiking activity and, simultaneously, reconstructing the\ndistributions of the calcium transient spikes' amplitudes under different\nexperimental conditions. The proposed model leverages two nested layers of\nrandom discrete mixture priors to borrow information between experiments and\ndiscover similarities in the distributional patterns of neuronal responses to\ndifferent stimuli. Furthermore, the spikes' intensity values are also clustered\nwithin and between experimental conditions to determine the existence of common\n(recurring) response amplitudes. Simulation studies and the analysis of a data\nset from the Allen Brain Observatory show the effectiveness of the method in\nclustering and detecting neuronal activities.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 14:46:42 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["D'Angelo", "Laura", ""], ["Canale", "Antonio", ""], ["Yu", "Zhaoxia", ""], ["Guindani", "Michele", ""]]}, {"id": "2102.09437", "submitter": "Devin Incerti", "authors": "Devin Incerti, Jeroen P Jansen", "title": "hesim: Health Economic Simulation Modeling and Decision Analysis", "comments": "Minor edits to references, captions, and Section 2.1", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Health economic models simulate the costs and effects of health technologies\nfor use in health technology assessment (HTA) to inform efficient use of scarce\nresources. Models have historically been developed using spreadsheet software\n(e.g., Microsoft Excel) and while use of R is growing, general purpose modeling\nsoftware is still limited. hesim is an R package that helps fill this gap by\nfacilitating parameterization, simulation, and analysis of economic models in\nan integrated manner. Supported model types include cohort discrete time state\ntransition models (cDTSTMs), individual continuous time state transition models\n(iCTSTMs), and partitioned survival models (PSMs), encompassing Markov\n(time-homogeneous and time-inhomogeneous) and semi-Markov processes. A modular\ndesign based on R6 and S3 classes allows users to combine separate submodels\nfor disease progression, costs, and utility in a flexible way. Probabilistic\nsensitivity analysis (PSA) is used to propagate uncertainty in model parameters\nto model outputs. Simulation code is written in C++ so complex simulations such\nas those combining PSA and individual simulation can be run much more quickly\nthan previously possible. Decision analysis within a cost-effectiveness\nframework is performed using simulated costs and quality-adjusted life years\n(QALYs) from a PSA.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 15:57:08 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 00:48:04 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Incerti", "Devin", ""], ["Jansen", "Jeroen P", ""]]}, {"id": "2102.09446", "submitter": "Helmi Shat", "authors": "Helmi Shat and Rainer Schwabe", "title": "Experimental Designs for Accelerated Degradation Tests Based on Linear\n  Mixed Effects Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accelerated degradation tests are used to provide accurate estimation of\nlifetime properties of highly reliable products within a relatively short\ntesting time. There data from particular tests at high levels of stress\n(e.\\,g.\\ temperature, voltage, or vibration) are extrapolated, through a\nphysically meaningful model, to obtain estimates of lifetime quantiles under\nnormal use conditions. In this work, we consider repeated measures accelerated\ndegradation tests with multiple stress variables, where the degradation paths\nare assumed to follow a linear mixed effects model which is quite common in\nsettings when repeated measures are made. We derive optimal experimental\ndesigns for minimizing the asymptotic variance for estimating the median\nfailure time under normal use conditions when the time points for measurements\nare either fixed in advance or are also to be optimized.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 16:08:41 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Shat", "Helmi", ""], ["Schwabe", "Rainer", ""]]}, {"id": "2102.09612", "submitter": "Ka Kin Lam", "authors": "Ka Kin Lam, Bo Wang", "title": "Multipopulation mortality modelling and forecasting: The multivariate\n  functional principal component with time weightings approaches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human mortality patterns and trajectories in closely related populations are\nlikely linked together and share similarities. It is always desirable to model\nthem simultaneously while taking their heterogeneity into account. This paper\nintroduces two new models for joint mortality modelling and forecasting\nmultiple subpopulations in adaptations of the multivariate functional principal\ncomponent analysis techniques. The first model extends the independent\nfunctional data model to a multi-population modelling setting. In the second\none, we propose a novel multivariate functional principal component method for\ncoherent modelling. Its design primarily fulfils the idea that when several\nsubpopulation groups have similar socio-economic conditions or common\nbiological characteristics, such close connections are expected to evolve in a\nnon-diverging fashion. We demonstrate the proposed methods by using\nsex-specific mortality data. Their forecast performances are further compared\nwith several existing models, including the independent functional data model\nand the Product-Ratio model, through comparisons with mortality data of ten\ndeveloped countries. Our experiment results show that the first proposed model\nmaintains a comparable forecast ability with the existing methods. In contrast,\nthe second proposed model outperforms the first model as well as the current\nmodels in terms of forecast accuracy, in addition to several desirable\nproperties.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 21:01:58 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Lam", "Ka Kin", ""], ["Wang", "Bo", ""]]}, {"id": "2102.09625", "submitter": "Marco Mariani", "authors": "Giuseppe Francesco Gori, Patrizia Lattarulo, Marco Mariani", "title": "The Expediting Effect of Monitoring on Infrastructural Works. A\n  Regression-Discontinuity Approach with Multiple Assignment Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Decentralised government levels are often entrusted with the management of\npublic works and required to ensure well-timed infrastructure delivery to their\ncommunities. We investigate whether monitoring the activity of local procuring\nauthorities during the execution phase of the works they manage may expedite\nthe infrastructure delivery process. Focussing on an Italian regional law which\nimposes monitoring by the regional government on \"strategic\" works carried out\nby local buyers, we draw causal claims using a regression-discontinuity\napproach, made unusual by the presence of multiple assignment variables.\nEstimation is performed through discrete-time survival analysis techniques.\nResults show that monitoring does expedite infrastructure delivery.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 15:14:41 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Gori", "Giuseppe Francesco", ""], ["Lattarulo", "Patrizia", ""], ["Mariani", "Marco", ""]]}, {"id": "2102.09676", "submitter": "Ka Kin Lam", "authors": "Ka Kin Lam, Bo Wang", "title": "Robust non-parametric mortality and fertility modelling and forecasting:\n  Gaussian process regression approaches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A rapid decline in mortality and fertility has become major issues in many\ndeveloped countries over the past few decades. A precise model for forecasting\ndemographic movements is important for decision making in social welfare\npolicies and resource budgeting among the government and many industry sectors.\nThis article introduces a novel non-parametric approach using Gaussian process\nregression with a natural cubic spline mean function and a spectral mixture\ncovariance function for mortality and fertility modelling and forecasting.\nUnlike most of the existing approaches in demographic modelling literature,\nwhich rely on time parameters to decide the movements of the whole mortality or\nfertility curve shifting from one year to another over time, we consider the\nmortality and fertility curves from their components of all age-specific\nmortality and fertility rates and assume each of them following a Gaussian\nprocess over time to fit the whole curves in a discrete but intensive style.\nThe proposed Gaussian process regression approach shows significant\nimprovements in terms of preciseness and robustness compared to other\nmainstream demographic modelling approaches in the short-, mid- and long-term\nforecasting using the mortality and fertility data of several developed\ncountries in our numerical experiments.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 23:49:25 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Lam", "Ka Kin", ""], ["Wang", "Bo", ""]]}, {"id": "2102.09689", "submitter": "Ivor Cribben", "authors": "Likang Ding, Ivor Cribben, Armann Ingolfsson, Monica Tran", "title": "Do NHL goalies get hot in the playoffs? A multilevel logistic regression\n  analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The hot-hand theory posits that an athlete who has performed well in the\nrecent past will perform better in the present. We use multilevel logistic\nregression to test this theory for National Hockey League playoff goaltenders,\ncontrolling for a variety of shot-related and game-related characteristics. Our\ndata consists of 48,431 shots for 93 goaltenders in the 2008-2016 playoffs.\nUsing a wide range of shot-based and time-based windows to quantify recent save\nperformance, we consistently find that good recent save performance has a\nnegative effect on the next-shot save probability, which contradicts the\nhot-hand theory.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 00:20:40 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Ding", "Likang", ""], ["Cribben", "Ivor", ""], ["Ingolfsson", "Armann", ""], ["Tran", "Monica", ""]]}, {"id": "2102.09879", "submitter": "Jonathan Larson", "authors": "Jonathan Larson and Jukka-Pekka Onnela", "title": "Inferring the minimum spanning tree from a sample network", "comments": "26 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimum spanning trees (MSTs) are used in a variety of fields, from computer\nscience to geography. Infectious disease researchers have used them to infer\nthe transmission pathway of certain pathogens. However, these are often the\nMSTs of sample networks, not population networks, and surprisingly little is\nknown about what can be inferred about a population MST from a sample MST. We\nprove that if $n$ nodes (the sample) are selected uniformly at random from a\ncomplete graph with $N$ nodes and unique edge weights (the population), the\nprobability that an edge is in the population graph's MST given that it is in\nthe sample graph's MST is $\\frac{n}{N}$. We use simulation to investigate this\nconditional probability for $G(N,p)$ graphs, Barab\\'{a}si-Albert (BA) graphs,\ngraphs whose nodes are distributed in $\\mathbb{R}^2$ according to a bivariate\nstandard normal distribution, and an empirical HIV genetic distance network.\nBroadly, results for the complete, $G(N,p)$, and normal graphs are similar, and\nresults for the BA and empirical HIV graphs are similar. We recommend that\nresearchers use an edge-weighted random walk to sample nodes from the\npopulation so that they maximize the probability that an edge is in the\npopulation MST given that it is in the sample MST.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 11:41:21 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 15:55:13 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Larson", "Jonathan", ""], ["Onnela", "Jukka-Pekka", ""]]}, {"id": "2102.09948", "submitter": "Soichiro Yamauchi", "authors": "Naoki Egami, Soichiro Yamauchi", "title": "Using Multiple Pre-treatment Periods to Improve\n  Difference-in-Differences and Staggered Adoption Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While difference-in-differences (DID) was originally developed with one pre-\nand one post-treatment periods, data from additional pre-treatment periods is\noften available. How can researchers improve the DID design with such multiple\npre-treatment periods under what conditions? We first use potential outcomes to\nclarify three benefits of multiple pre-treatment periods: (1) assessing the\nparallel trends assumption, (2) improving estimation accuracy, and (3) allowing\nfor a more flexible parallel trends assumption. We then propose a new\nestimator, double DID, which combines all the benefits through the generalized\nmethod of moments and contains the two-way fixed effects regression as a\nspecial case. In a wide range of applications where several pre-treatment\nperiods are available, the double DID improves upon the standard DID both in\nterms of identification and estimation accuracy. We also generalize the double\nDID to the staggered adoption design where different units can receive the\ntreatment in different time periods. We illustrate the proposed method with two\nempirical applications, covering both the basic DID and staggered adoption\ndesigns. We offer an open-source R package that implements the proposed\nmethodologies.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 15:28:40 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Egami", "Naoki", ""], ["Yamauchi", "Soichiro", ""]]}, {"id": "2102.09958", "submitter": "Rachel Heyard", "authors": "Rachel Heyard and Manuela Ott and Georgia Salanti and Matthias Egger", "title": "Rethinking the Funding Line at the Swiss National Science Foundation:\n  Bayesian Ranking and Lottery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Funding agencies rely on peer review and expert panels to select the research\ndeserving funding. Peer review has limitations, including bias against risky\nproposals or interdisciplinary research. The inter-rater reliability between\nreviewers and panels is low, particularly for proposals near the funding line.\nFunding agencies are increasingly acknowledging the role of chance. The Swiss\nNational Science Foundation (SNSF) introduced a lottery for proposals in the\nmiddle group of good but not excellent proposals. In this article, we introduce\na Bayesian hierarchical model for the evaluation process. To rank the\nproposals, we estimate their expected ranks (ER), which incorporates both the\nmagnitude and uncertainty of the estimated differences between proposals. A\nprovisional funding line is defined based on ER and budget. The ER and its\ncredible interval are used to identify proposals with similar quality and\ncredible intervals that overlap with the funding line. These proposals are\nentered into a lottery. We illustrate the approach for two SNSF grant schemes\nin career and project funding. We argue that the method could reduce bias in\nthe evaluation process. R code, data and other materials for this article are\navailable online.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 14:46:53 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Heyard", "Rachel", ""], ["Ott", "Manuela", ""], ["Salanti", "Georgia", ""], ["Egger", "Matthias", ""]]}, {"id": "2102.10005", "submitter": "Federica Onori", "authors": "Federica Onori, Sara Viviani and Pierpaolo Brutti", "title": "Towards global monitoring: equating the Food Insecurity Experience Scale\n  (FIES) and food insecurity scales in Latin America", "comments": "23 pages, 3 figures, 6 tables, to appear in \"Models for Data\n  Analysis. Selected papers of 49th Meeting of Italian Statistical Society\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In order to face food insecurity as a global phenomenon, it is essential to\nrely on measurement tools that guarantee comparability across countries.\nAlthough the official indicators adopted by the United Nations in the context\nof the Sustainable Development Goals (SDGs) and based on the Food Insecurity\nExperience Scale (FIES) already embeds cross-country comparability, other\nexperiential scales of food insecurity currently employ national thresholds and\nissues of comparability thus arise. In this work we address comparability of\nfood insecurity experience-based scales by presenting two different studies.\nThe first one involves the FIES and three national scales (ELCSA, EMSA and\nEBIA) currently included in national surveys in Guatemala, Ecuador, Mexico and\nBrazil. The second study concerns the adult and children versions of these\nnational scales. Different methods from the equating practice of the\neducational testing field are explored: classical and based on the Item\nResponse Theory (IRT).\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 16:18:11 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Onori", "Federica", ""], ["Viviani", "Sara", ""], ["Brutti", "Pierpaolo", ""]]}, {"id": "2102.10015", "submitter": "Daniel Larsson", "authors": "Daniel T. Larsson, Dipankar Maity, Panagiotis Tsiotras", "title": "Information-Theoretic Abstractions for Resource-Constrained Agents via\n  Mixed-Integer Linear Programming", "comments": null, "journal-ref": "2021 Proceedings of the Workshop on Computation-Aware Algorithmic\n  Design for Cyber-Physical Systems", "doi": "10.1145/3457335.3461704", "report-no": null, "categories": "cs.RO cs.AI cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a mixed-integer linear programming formulation for the problem\nof obtaining task-relevant, multi-resolution, graph abstractions for\nresource-constrained agents is presented. The formulation leverages concepts\nfrom information-theoretic signal compression, specifically the information\nbottleneck (IB) method, to pose a graph abstraction problem as an optimal\nencoder search over the space of multi-resolution trees. The abstractions\nemerge in a task-relevant manner as a function of agent information-processing\nconstraints, and are not provided to the system a priori. We detail our\nformulation and show how the problem can be realized as an integer linear\nprogram. A non-trivial numerical example is presented to demonstrate the\nutility in employing our approach to obtain hierarchical tree abstractions for\nresource-limited agents.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 16:34:47 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Larsson", "Daniel T.", ""], ["Maity", "Dipankar", ""], ["Tsiotras", "Panagiotis", ""]]}, {"id": "2102.10050", "submitter": "David Leslie", "authors": "David Leslie", "title": "The Arc of the Data Scientific Universe", "comments": "43 pages", "journal-ref": "Harvard Data Science Review (Winter 2021)", "doi": "10.1162/99608f92.938a18d7", "report-no": null, "categories": "physics.hist-ph cs.AI cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper I explore the scaffolding of normative assumptions that\nsupports Sabina Leonelli's implicit appeal to the values of epistemic integrity\nand the global public good that conjointly animate the ethos of responsible and\nsustainable data work in the context of COVID-19. Drawing primarily on the\nwritings of sociologist Robert K. Merton, the thinkers of the Vienna Circle,\nand Charles Sanders Peirce, I make some of these assumptions explicit by\ntelling a longer story about the evolution of social thinking about the\nnormative structure of science from Merton's articulation of his well-known\nnorms (those of universalism, communism, organized skepticism, and\ndisinterestedness) to the present. I show that while Merton's norms and his\nintertwinement of these with the underlying mechanisms of democratic order\nprovide us with an especially good starting point to explore and clarify the\ncommitments and values of science, Leonelli's broader, more context-responsive,\nand more holistic vision of the epistemic integrity of data scientific\nunderstanding, and her discernment of the global and biospheric scope of its\nmoral-practical reach, move beyond Merton's schema in ways that effectively\ndraw upon important critiques. Stepping past Merton, I argue that a combination\nof situated universalism, methodological pluralism, strong objectivity, and\nunbounded communalism must guide the responsible and sustainable data work of\nthe future.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 13:29:58 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Leslie", "David", ""]]}, {"id": "2102.10179", "submitter": "Guoqing Wang", "authors": "Guoqing Wang, Abhirup Datta, Martin A. Lindquist", "title": "Bayesian Functional Registration of fMRI Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Functional magnetic resonance imaging (fMRI) has provided invaluable insight\ninto our understanding of human behavior. However, large inter-individual\ndifferences in both brain anatomy and functional localization after anatomical\nalignment remain a major limitation in conducting group analyses and performing\npopulation-level inference. This paper addresses this problem by developing and\nvalidating a new computational technique for reducing misalignment across\nindividuals in functional brain systems by spatially transforming each\nsubject's functional data to a common reference map. Our proposed Bayesian\nfunctional registration approach allows us to assess differences in brain\nfunction across subjects and individual differences in activation topology. It\ncombines intensity-based and feature-based information into an integrated\nframework and allows inference to be performed on the transformation via the\nposterior samples. We evaluate the method in a simulation study and apply it to\ndata from a study of thermal pain. We find that the proposed approach provides\nincreased sensitivity for group-level inference.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 22:13:42 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Wang", "Guoqing", ""], ["Datta", "Abhirup", ""], ["Lindquist", "Martin A.", ""]]}, {"id": "2102.10237", "submitter": "Evan Rosenman", "authors": "Evan Rosenman, Art B. Owen", "title": "Designing Experiments Informed by Observational Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing availability of passively observed data has yielded a growing\nmethodological interest in \"data fusion.\" These methods involve merging data\nfrom observational and experimental sources to draw causal conclusions -- and\nthey typically require a precarious tradeoff between the unknown bias in the\nobservational dataset and the often-large variance in the experimental dataset.\nWe propose an alternative approach to leveraging observational data, which\navoids this tradeoff: rather than using observational data for inference, we\nuse it to design a more efficient experiment. We consider the case of a\nstratified experiment with a binary outcome, and suppose pilot estimates for\nthe stratum potential outcome variances can be obtained from the observational\nstudy. We extend results from Zhao et al. (2019) in order to generate\nconfidence sets for these variances, while accounting for the possibility of\nunmeasured confounding. Then, we pose the experimental design problem as one of\nregret minimization, subject to the constraints imposed by our confidence sets.\nWe show that this problem can be converted into a convex minimization and\nsolved using conventional methods. Lastly, we demonstrate the practical utility\nof our methods using data from the Women's Health Initiative.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 02:55:18 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Rosenman", "Evan", ""], ["Owen", "Art B.", ""]]}, {"id": "2102.10265", "submitter": "Fadhah Alanazi", "authors": "Fadhah Amer Alanazi", "title": "The spread of COVID-19 at Hot-Temperature Places With Different Curfew\n  Situations Using Copula Models", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The infectious coronavirus disease 2019 (COVID-19) has become a serious\nglobal pandemic. Different studies have shown that increasing temperature can\nplay a crucial role in the spread of the virus. Most of these studies were\nlimited to winter or moderate temperature levels and were conducted using\nconventional models. However, traditional models are too simplistic to\ninvestigate complex, non-linear relationships and suffer from some\nrestrictions. Therefore, we employed copula models to examine the impact of\nhigh temperatures on virus transmission. The findings from the copula models\nshowed that there was a weak to moderate effect of temperature on the number of\ninfections and the effect almost vanished under a lockdown policy. Therefore,\nthis study provides new insight into the relationship between COVID-19 and\ntemperature, both with and without social isolation practices. Such results can\nlead to improvements in our understanding of this new virus. In particular, the\nresults derived from the copula models examined here, unlike existing\ntraditional models, provide evidence that there is no substantial influence of\nhigh temperatures on the active COVID-19 outbreak situation. In addition, the\nresults indicate that the transmission of COVID-19 is strongly influenced by\nsocial isolation practices. To the best of the author knowledge, this is the\nfirst copula model investigation applied to the COVID-19 pandemic.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 06:01:05 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Alanazi", "Fadhah Amer", ""]]}, {"id": "2102.10308", "submitter": "Abhik Ghosh PhD", "authors": "Abhik Ghosh, Olivia Mallick, Souvik Chattopadhay, Banasri Basu", "title": "Strata-based Quantification of Distributional Uncertainty in\n  Socio-Economic Indicators: A Comparative Study of Indian States", "comments": "Pre-print; Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper reports a comprehensive study of distributional uncertainty in a\nfew socio-economic indicators across the various states of India over the years\n2001-2011. We show that the DGB distribution, a typical rank order\ndistribution, provide excellent fits to the district-wise empirical data for\nthe population size, literacy rate (LR) and work participation rate (WPR)\nwithin every states in India, through its two distributional parameters.\nMoreover, taking resort to the entropy formulation of the DGB distribution, a\nproposed uncertainty percentage (UP) unveils the dynamics of the uncertainty of\nLR and WPR in all states of India. We have also commented on the changes in the\nestimated parameters and the UP values from the years 2001 to 2011.\nAdditionally, a gender based analysis of the distribution of these important\nsocio-economic variables within different states of India has also been\ndiscussed. Interestingly, it has been observed that, although the distributions\nof the numbers of literate and working people has a direct (linear)\ncorrespondence with that of the population size, the literacy and\nwork-participation rates are distributed independently of the population\ndistributions.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 10:20:24 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Ghosh", "Abhik", ""], ["Mallick", "Olivia", ""], ["Chattopadhay", "Souvik", ""], ["Basu", "Banasri", ""]]}, {"id": "2102.10331", "submitter": "Chee-Ming Ting PhD", "authors": "Chee-Ming Ting, Jeremy I. Skipper, Steven L. Small, Hernando Ombao", "title": "Separating Stimulus-Induced and Background Components of Dynamic\n  Functional Connectivity in Naturalistic fMRI", "comments": "Main paper: 10 pages, 8 figures. Supplemental file: 3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG cs.SD eess.AS eess.IV eess.SP stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We consider the challenges in extracting stimulus-related neural dynamics\nfrom other intrinsic processes and noise in naturalistic functional magnetic\nresonance imaging (fMRI). Most studies rely on inter-subject correlations (ISC)\nof low-level regional activity and neglect varying responses in individuals. We\npropose a novel, data-driven approach based on low-rank plus sparse (L+S)\ndecomposition to isolate stimulus-driven dynamic changes in brain functional\nconnectivity (FC) from the background noise, by exploiting shared network\nstructure among subjects receiving the same naturalistic stimuli. The\ntime-resolved multi-subject FC matrices are modeled as a sum of a low-rank\ncomponent of correlated FC patterns across subjects, and a sparse component of\nsubject-specific, idiosyncratic background activities. To recover the shared\nlow-rank subspace, we introduce a fused version of principal component pursuit\n(PCP) by adding a fusion-type penalty on the differences between the rows of\nthe low-rank matrix. The method improves the detection of stimulus-induced\ngroup-level homogeneity in the FC profile while capturing inter-subject\nvariability. We develop an efficient algorithm via a linearized alternating\ndirection method of multipliers to solve the fused-PCP. Simulations show\naccurate recovery by the fused-PCP even when a large fraction of FC edges are\nseverely corrupted. When applied to natural fMRI data, our method reveals FC\nchanges that were time-locked to auditory processing during movie watching,\nwith dynamic engagement of sensorimotor systems for speech-in-noise. It also\nprovides a better mapping to auditory content in the movie than ISC.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 11:35:39 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Ting", "Chee-Ming", ""], ["Skipper", "Jeremy I.", ""], ["Small", "Steven L.", ""], ["Ombao", "Hernando", ""]]}, {"id": "2102.10391", "submitter": "Matthew Deakin", "authors": "Matthew Deakin, Hannah Bloomfield, David Greenwood, Sarah Sheehy, Sara\n  Walker, Phil C. Taylor", "title": "Impacts of Heat Decarbonisation on System Adequacy considering Increased\n  Meteorological Sensitivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the impacts of decarbonisation of heat on demand and\nsubsequently on the generation capacity required to secure against system\nadequacy standards. Gas demand is explored as a proxy variable for modelling\nthe electrification of heating demand in existing housing stock, with a focus\non impacts on timescales of capacity markets (up to four years ahead). The work\nconsiders the systemic changes that electrification of heating could introduce,\nincluding biases that could be introduced if legacy modelling approaches\ncontinue to prevail. Covariates from gas and electrical regression models are\ncombined to form a novel, time-collapsed system model, with demand-weather\nsensitivities determined using lasso-regularized linear regression. It is\nshown, using a GB case study with one million domestic heat pump installations\nper year, that the sensitivity of electrical system demand to temperature (and\nsubsequently sensitivities to cold/warm winter seasons) could increase by 50%\nfollowing four years of heat demand electrification. A central estimate of 1.75\nkW additional peak demand per heat pump is estimated, with variability across\nthree published heat demand profiles leading to a range of more than 14 GW in\nthe most extreme cases. It is shown that the legacy approach of scaling\nhistoric demand, as compared to the explicit modelling of heat, could lead to\nover-procurement of 0.79 GW due to bias in estimates of additional capacity to\nsecure. Failure to address this issue could lead to {\\pounds}100m overspend on\ncapacity over ten years.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 17:02:57 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Deakin", "Matthew", ""], ["Bloomfield", "Hannah", ""], ["Greenwood", "David", ""], ["Sheehy", "Sarah", ""], ["Walker", "Sara", ""], ["Taylor", "Phil C.", ""]]}, {"id": "2102.10478", "submitter": "Xiao Wu", "authors": "Xiao Wu, Kate R. Weinberger, Gregory A. Wellenius, Francesca Dominici,\n  Danielle Braun", "title": "Assessing the causal effects of a stochastic intervention in time series\n  data: Are heat alerts effective in preventing deaths and hospitalizations?", "comments": "31 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce a new causal inference framework for time series data aimed at\nassessing the effectiveness of heat alerts in reducing mortality and\nhospitalization risks. We are interested in addressing the following question:\nhow many deaths and hospitalizations could be averted if we were to increase\nthe frequency of issuing heat alerts in a given location? In the context of\ntime series data, the overlap assumption - each unit must have a positive\nprobability of receiving the treatment - is often violated. This is because, in\na given location, issuing a heat alert is a rare event on an average\ntemperature day as heat alerts are almost always issued on extremely hot days.\nTo overcome this challenge, first we introduce a new class of causal estimands\nunder a stochastic intervention (i.e., increasing the odds of issuing a heat\nalert) for a single time series corresponding to a given location. We develop\nthe theory to show that these causal estimands can be identified and estimated\nunder a weaker version of the overlap assumption. Second, we propose\nnonparametric estimators based on time-varying propensity scores, and derive\npoint-wise confidence bands for these estimators. Third, we extend this\nframework to multiple time series corresponding to multiple locations. Via\nsimulations, we show that the proposed estimator has good performance with\nrespect to bias and root mean squared error. We apply our proposed method to\nestimate the causal effects of increasing the odds of issuing heat alerts in\nreducing deaths and hospitalizations among Medicare enrollees in 2817 U.S.\ncounties. We found weak evidence of a causal link between increasing the odds\nof issuing heat alerts during the warm seasons of 2006-2016 and a reduction in\ndeaths and cause-specific hospitalizations across the 2817 counties.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 23:49:57 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 16:58:29 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Wu", "Xiao", ""], ["Weinberger", "Kate R.", ""], ["Wellenius", "Gregory A.", ""], ["Dominici", "Francesca", ""], ["Braun", "Danielle", ""]]}, {"id": "2102.10537", "submitter": "Kwonsang Lee", "authors": "Kwonsang Lee, Francesca Dominici", "title": "Accounting for recall bias in case-control studies: a causal inference\n  approach", "comments": "19 pages, 3 figures, 4 tables. Supplementary materials are available.\n  The R files are available at\n  https://github.com/kwonsang/recall_bias_case_control_study", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A case-control study is designed to help determine if an exposure is\nassociated with an outcome. However, since case-control studies are\nretrospective, they are often subject to recall bias. Recall bias can occur\nwhen study subjects do not remember previous events accurately. In this paper,\nwe first define the estimand of interest: the causal odds ratio (COR) for a\ncase-control study. Second, we develop estimation approaches for the COR and\npresent estimates as a function of recall bias. Third, we define a new quantity\ncalled the \\textit{R-factor}, which denotes the minimal amount of recall bias\nthat leads to altering the initial conclusion. We show that a failure to\naccount for recall bias can significantly bias estimation of the COR. Finally,\nwe apply the proposed framework to a case-control study of the causal effect of\nchildhood physical abuse on adulthood mental health.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 07:33:02 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Lee", "Kwonsang", ""], ["Dominici", "Francesca", ""]]}, {"id": "2102.10551", "submitter": "Rohitash Chandra", "authors": "Animesh Tiwari, Rishabh Gupta, Rohitash Chandra", "title": "Delhi air quality prediction using LSTM deep learning models with a\n  focus on COVID-19 lockdown", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Air pollution has a wide range of implications on agriculture, economy, road\naccidents, and health. In this paper, we use novel deep learning methods for\nshort-term (multi-step-ahead) air-quality prediction in selected parts of\nDelhi, India. Our deep learning methods comprise of long short-term memory\n(LSTM) network models which also include some recent versions such as\nbidirectional-LSTM and encoder-decoder LSTM models. We use a multivariate time\nseries approach that attempts to predict air quality for 10 prediction horizons\ncovering total of 80 hours and provide a long-term (one month ahead) forecast\nwith uncertainties quantified. Our results show that the multivariate\nbidirectional-LSTM model provides best predictions despite COVID-19 impact on\nthe air-quality during full and partial lockdown periods. The effect of\nCOVID-19 on the air quality has been significant during full lockdown; however,\nthere was unprecedented growth of poor air quality afterwards.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 08:30:17 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Tiwari", "Animesh", ""], ["Gupta", "Rishabh", ""], ["Chandra", "Rohitash", ""]]}, {"id": "2102.10558", "submitter": "L\\'aszl\\'o Csat\\'o", "authors": "Kolos Csaba \\'Agoston and L\\'aszl\\'o Csat\\'o", "title": "Inconsistency thresholds for incomplete pairwise comparison matrices", "comments": "13 pages, 2 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.AI math.OC stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pairwise comparison matrices are increasingly used in settings where some\npairs are missing. However, there exist few inconsistency indices for similar\nincomplete data sets and no reasonable measure has an associated threshold.\nThis paper generalises the famous rule of thumb for the acceptable level of\ninconsistency, proposed by Saaty, to incomplete pairwise comparison matrices.\nThe extension is based on choosing the missing elements such that the maximal\neigenvalue of the incomplete matrix is minimised. Consequently, the\nwell-established values of the random index cannot be adopted: the\ninconsistency of random matrices is found to be the function of matrix size and\nthe number of missing elements, with a nearly linear dependence in the case of\nthe latter variable. Our results can be directly built into decision-making\nsoftware and used by practitioners as a statistical criterion for accepting or\nrejecting an incomplete pairwise comparison matrix.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 08:39:37 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 12:09:42 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["\u00c1goston", "Kolos Csaba", ""], ["Csat\u00f3", "L\u00e1szl\u00f3", ""]]}, {"id": "2102.10565", "submitter": "Rodrigo Labouriau", "authors": "Jeanett S. Pelck, Rafael Pimentel Maia, Hildete P. Pinheiro and\n  Rodrigo Labouriau", "title": "A Multivariate Methodology for Analysing Students' Performance Using\n  Register Data", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a new method for jointly modelling the students' results in the\nuniversity's admission exams and their performance in subsequent courses at the\nuniversity. The case considered involved all the students enrolled at the\nUniversity of Campinas in 2014 to evening studies programs in educational\nbranches related to exact sciences. We collected the number of attempts used\nfor passing the university course of geometry and the results of the admission\nexams of those students in seven disciplines. The method introduced involved a\ncombination of multivariate generalised linear mixed models (GLMM) and\ngraphical models for representing the covariance structure of the random\ncomponents. The models we used allowed us to discuss the association of\nquantities of very different nature. We used Gaussian GLMM for modelling the\nperformance in the admission exams and a frailty discrete-time Cox proportional\nmodel, represented by a GLMM, to describe the number of attempts for passing\nGeometry.\n  The analyses were stratified into two populations: the students who received\na bonus giving advantages in the university's admission process to compensate\nsocial and racial inequalities and those who did not receive the compensation.\nThe two populations presented different patterns. Using general properties of\ngraphical models, we argue that, on the one hand, the predicted performance in\nthe admission exam of Mathematics could solely be used as a predictor of the\nperformance in geometry for the students who received the bonus. On the other\nhand, the Portuguese admission exam's predicted performance could be used as a\nsingle predictor of the performance in geometry for the students who did not\nreceive the bonus.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 09:13:41 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Pelck", "Jeanett S.", ""], ["Maia", "Rafael Pimentel", ""], ["Pinheiro", "Hildete P.", ""], ["Labouriau", "Rodrigo", ""]]}, {"id": "2102.10741", "submitter": "Nicholas Irons", "authors": "Nicholas J. Irons and Adrian E. Raftery", "title": "Estimating SARS-CoV-2 Infections from Deaths, Confirmed Cases, Tests,\n  and Random Surveys", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many sources of data giving information about the number of\nSARS-CoV-2 infections in the population, but all have major drawbacks,\nincluding biases and delayed reporting. For example, the number of confirmed\ncases largely underestimates the number of infections, deaths lag infections\nsubstantially, while test positivity rates tend to greatly overestimate\nprevalence. Representative random prevalence surveys, the only putatively\nunbiased source, are sparse in time and space, and the results come with a big\ndelay. Reliable estimates of population prevalence are necessary for\nunderstanding the spread of the virus and the effects of mitigation strategies.\nWe develop a simple Bayesian framework to estimate viral prevalence by\ncombining the main available data sources. It is based on a discrete-time SIR\nmodel with time-varying reproductive parameter. Our model includes likelihood\ncomponents that incorporate data of deaths due to the virus, confirmed cases,\nand the number of tests administered on each day. We anchor our inference with\ndata from random sample testing surveys in Indiana and Ohio. We use the results\nfrom these two states to calibrate the model on positive test counts and\nproceed to estimate the infection fatality rate and the number of new\ninfections on each day in each state in the USA. We estimate the extent to\nwhich reported COVID cases have underestimated true infection counts, which was\nlarge, especially in the first months of the pandemic. We explore the\nimplications of our results for progress towards herd immunity.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 02:47:09 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Irons", "Nicholas J.", ""], ["Raftery", "Adrian E.", ""]]}, {"id": "2102.10753", "submitter": "Fred Ghanem", "authors": "Fred Ghanem and Kirti M. Yenkie", "title": "Modeling Chromate Removal Using Ion Exchangers in Drinking Water\n  Applications", "comments": "28 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chromates are widely used for their anticorrosive properties. Unfortunately,\nthey are highly hazardous with environmental agencies regulating their levels\nto below 10 ppb in drinking water. As anion exchange resins are typically used\nfor removal, predictive dynamic models are necessary to make quick decisions\nrather than relying on experimental data that could take several days to\nimplement. With various dynamic models currently applied to simulate the ion\nexchange process, the Thomas model was picked for its simplicity and better\naccuracy when compared to other models. The Thomas model contains two\nparameters, the constant (KT) and the maximum resin capacity (qm), which are\nempirically calculated. Unfortunately, the model demonstrated large parameter\nfluctuations with no correlation to varying contact times or inlet chromate\nconcentrations. Therefore, fixing both parameters will lead to failed model\npredictive behavior. By fixing the value of qm and proposing a linear\nrelationship of KT with resin contact time and inlet chromate concentration,\nthe accuracy of the model was improved five-fold, demonstrating its potential\nfor better process controls.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 03:33:59 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Ghanem", "Fred", ""], ["Yenkie", "Kirti M.", ""]]}, {"id": "2102.10783", "submitter": "Rahul Ghosal", "authors": "Rahul Ghosal, Vijay R. Varma, Dmitri Volfson, Inbar Hillel, Jacek\n  Urbanek, Jeffrey M. Hausdorff, Amber Watts and Vadim Zipunnikov", "title": "Distributional data analysis via quantile functions and its application\n  to modelling digital biomarkers of gait in Alzheimer's Disease", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of continuous health monitoring via wearable devices, users\nnow generate their unique streams of continuous data such as minute-level\nphysical activity or heart rate. Aggregating these streams into scalar\nsummaries ignores the distributional nature of data and often leads to the loss\nof critical information. We propose to capture the distributional properties of\nwearable data via user-specific quantile functions that are further used in\nfunctional regression and multi-modal distributional modelling. In addition, we\npropose to encode user-specific distributional information with user-specific\nL-moments, robust rank-based analogs of traditional moments. Importantly, this\nL-moment encoding results in mutually consistent functional and distributional\ninterpretation of the results of scalar-on-function regression. We also\ndemonstrate how L-moments can be flexibly employed for analyzing joint and\nindividual sources of variation in multi-modal distributional data. The\nproposed methods are illustrated in a study of association of\naccelerometry-derived digital gait biomarkers with Alzheimer's disease (AD) and\nin people with normal cognitive function. Our analysis shows that the proposed\nquantile-based representation results in a much higher predictive performance\ncompared to simple distributional summaries and attains much stronger\nassociations with clinical cognitive scales.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 05:30:40 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Ghosal", "Rahul", ""], ["Varma", "Vijay R.", ""], ["Volfson", "Dmitri", ""], ["Hillel", "Inbar", ""], ["Urbanek", "Jacek", ""], ["Hausdorff", "Jeffrey M.", ""], ["Watts", "Amber", ""], ["Zipunnikov", "Vadim", ""]]}, {"id": "2102.10906", "submitter": "Jordan Richards", "authors": "Jordan Richards and Jonathan A. Tawn and Simon Brown", "title": "Modelling Extremes of Spatial Aggregates of Precipitation using\n  Conditional Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inference on the extremal behaviour of spatial aggregates of precipitation is\nimportant for quantifying river flood risk. There are two classes of previous\napproach, with one failing to ensure self-consistency in inference across\ndifferent regions of aggregation and the other requiring highly inflexible\nmarginal and spatial dependence structure assumptions. To overcome these\nissues, we propose a model for high-resolution precipitation data, from which\nwe can simulate realistic fields and explore the behaviour of spatial\naggregates. Recent developments in spatial extremes literature have seen\npromising progress with spatial extensions of the Heffernan and Tawn (2004)\nmodel for conditional multivariate extremes, which can handle a wide range of\ndependence structures. Our contribution is twofold: new parametric forms for\nthe dependence parameters of this model; and a novel framework for deriving\naggregates addressing edge effects and sub-regions without rain. We apply our\nmodelling approach to gridded East-Anglia, UK precipitation data. Return-level\ncurves for spatial aggregates over different regions of various sizes are\nestimated and shown to fit very well to the data.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 11:14:12 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Richards", "Jordan", ""], ["Tawn", "Jonathan A.", ""], ["Brown", "Simon", ""]]}, {"id": "2102.10974", "submitter": "Junfeng Wu", "authors": "Wu Junfeng, and Mu Biqiang, and Yi Xinlei, and Wei Jieqiang, and\n  Johansson Karl Henrik", "title": "Localizability with Range-Difference Measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The physical position is crucial in location-aware services or protocols\nbased on geographic information, where localization is performed given a set of\nsensor measurements for acquiring the position of an object with respect to a\ncertain coordinate system. In this paper, we revisit the longstanding\nlocalization methods for locating a radiating source from range-difference\nmeasurements, or equivalently, time difference-of-arrival measurements from the\nperspective of least squares (LS). In particular, we focus on the spherical LS\nerror model, where the error function is defined as the difference between the\ntrue distance from a signal receiver (sensor) to the source and its measured\nvalue, and the resulting spherical LS estimation problem. This problem has been\nknown to be challenging due to the non-convex nature of the hyperbolic\nmeasurement model. First of all, we prove that the existence of least-square\nsolutions is universal and that solutions are bounded under some assumption on\nthe Jacobian matrix of the measurement model. Then a necessary and sufficient\ncondition is presented for the solution characterization based on the method of\nLagrange multipliers. Next, we derive a characterization for the uniqueness of\nthe solutions incorporating a second-order optimality condition. The solution\nstructures for some special cases are also established, contributing to\ninsights on the effects of the Lagrangian multipliers on global solutions.\nThese findings establish a comprehensive understanding of the localizability\nwith range-difference measurements, which are also illustrated with numerical\nexamples.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 13:25:00 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Junfeng", "Wu", ""], ["Biqiang", "Mu", ""], ["Xinlei", "Yi", ""], ["Jieqiang", "Wei", ""], ["Henrik", "Johansson Karl", ""]]}, {"id": "2102.10978", "submitter": "Rohan Yashraj Gupta", "authors": "Rohan Yashraj Gupta, Satya Sai Mudigonda, Pallav Kumar Baruah and\n  Phani Krishna Kandala", "title": "Markov model with machine learning integration for fraud detection in\n  health insurance", "comments": "6 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fraud has led to a huge addition of expenses in health insurance sector in\nIndia. The work is aimed to provide methods applied to health insurance fraud\ndetection. The work presents two approaches - a markov model and an improved\nmarkov model using gradient boosting method in health insurance claims. The\ndataset 382,587 claims of which 38,082 claims are fraudulent. The markov based\nmodel gave the accuracy of 94.07% with F1-score at 0.6683. However, the\nimproved markov model performed much better in comparison with the accuracy of\n97.10% and F1-score of 0.8546. It was observed that the improved markov model\ngave much lower false positives compared to markov model.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 13:01:28 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Gupta", "Rohan Yashraj", ""], ["Mudigonda", "Satya Sai", ""], ["Baruah", "Pallav Kumar", ""], ["Kandala", "Phani Krishna", ""]]}, {"id": "2102.11021", "submitter": "Ger Koole", "authors": "Ren\\'e Bekker, Michiel uit het Broek, and Ger Koole", "title": "Modeling COVID-19 hospital admissions and occupancy in the Netherlands", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We describe the models we built for hospital admissions and occupancy of\nCOVID-19 patients in the Netherlands. These models were used to make short-term\ndecisions about transfers of patients between regions and for long-term policy\nmaking. We motivate and describe the model we used for predicting admissions\nand how we use this to make predictions on occupancy.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 20:59:13 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Bekker", "Ren\u00e9", ""], ["Broek", "Michiel uit het", ""], ["Koole", "Ger", ""]]}, {"id": "2102.11022", "submitter": "Giulio D'Agostini", "authors": "Giulio D'Agostini and Alfredo Esposito", "title": "What is the probability that a vaccinated person is shielded from\n  Covid-19? A Bayesian MCMC based reanalysis of published data with emphasis on\n  what should be reported as `efficacy'", "comments": "29 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.HO q-bio.PE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on the information communicated in press releases, and finally\npublished towards the end of 2020 by Pfizer, Moderna and AstraZeneca, we have\nbuilt up a simple Bayesian model, in which the main quantity of interest plays\nthe role of {\\em vaccine efficacy} (`$\\epsilon$'). The resulting Bayesian\nNetwork is processed by a Markov Chain Monte Carlo (MCMC), implemented in JAGS\ninterfaced to R via rjags. As outcome, we get several probability density\nfunctions (pdf's) of $\\epsilon$, each conditioned on the data provided by the\nthree pharma companies. The result is rather stable against large variations of\nthe number of people participating in the trials and it is `somehow' in good\nagreement with the results provided by the companies, in the sense that their\nvalues correspond to the most probable value (`mode') of the pdf's resulting\nfrom MCMC, thus reassuring us about the validity of our simple model. However\nwe maintain that the number to be reported as `vaccine efficacy' should be the\nmean of the distribution, rather than the mode, as it was already very clear to\nLaplace about 250 years ago (its `rule of succession' follows from the simplest\nproblem of the kind). This is particularly important in the case in which the\nnumber of successes equals the numbers of trials, as it happens with the\nefficacy against `severe forms' of infection, claimed by Moderna to be 100%.\nThe implication of the various uncertainties on the predicted number of\nvaccinated infectees is also shown, using both MCMC and approximated formulae.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 22:02:35 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["D'Agostini", "Giulio", ""], ["Esposito", "Alfredo", ""]]}, {"id": "2102.11027", "submitter": "Alina Lazar", "authors": "Ling Jin, C. Anna Spurlock, Sam Borgeson, Alina Lazar, Daniel Fredman,\n  Annika Todd, Alexander Sim, Kesheng Wu", "title": "Investigating Underlying Drivers of Variability in Residential Energy\n  Usage Patterns with Daily Load Shape Clustering of Smart Meter Data", "comments": "11 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Residential customers have traditionally not been treated as individual\nentities due to the high volatility in residential consumption patterns as well\nas a historic focus on aggregated loads from the utility and system feeder\nperspective. Large-scale deployment of smart meters has motivated increasing\nstudies to explore disaggregated daily load patterns, which can reveal\nimportant heterogeneity across different time scales, weather conditions, as\nwell as within and across individual households. This paper aims to shed light\non the mechanisms by which electricity consumption patterns exhibit variability\nand the different constraints that may affect demand-response (DR) flexibility.\nWe systematically evaluate the relationship between daily time-of-use patterns\nand their variability to external and internal influencing factors, including\ntime scales of interest, meteorological conditions, and household\ncharacteristics by application of an improved version of the adaptive K-means\nclustering method to profile \"household-days\" of a summer peaking utility. We\nfind that for this summer-peaking utility, outdoor temperature is the most\nimportant external driver of the load shape variability relative to seasonality\nand day-of-week. The top three consumption patterns represent approximately 50%\nof usage on the highest temperature days. The variability in summer load shapes\nacross customers can be explained by the responsiveness of the households to\noutside temperature. Our results suggest that depending on the influencing\nfactors, not all the consumption variability can be readily translated to\nconsumption flexibility. Such information needs to be further explored in\nsegmenting customers for better program targeting and tailoring to meet the\nneeds of the rapidly evolving electricity grid.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 16:56:27 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Jin", "Ling", ""], ["Spurlock", "C. Anna", ""], ["Borgeson", "Sam", ""], ["Lazar", "Alina", ""], ["Fredman", "Daniel", ""], ["Todd", "Annika", ""], ["Sim", "Alexander", ""], ["Wu", "Kesheng", ""]]}, {"id": "2102.11043", "submitter": "Domenic Rosati", "authors": "Domenic Rosati", "title": "How are journals cited? characterizing journal citations by type of\n  citation", "comments": "2 Pages, 2 Tables, 2 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Evaluation of journals for quality is one of the dominant themes of\nbibliometrics since journals are the primary venue of vetting and distribution\nof scholarship. There are many criticisms of quantifying journal impact with\nbibliometrics including disciplinary differences among journals, what source\nmaterials are used, time windows for the inclusion of works to measure, and\nskewness of citation distributions (Lariviere & Sugimoto, 2019). However,\ndespite various attempts to remediate these in newly proposed indicators such\nas SJR, SNIP, and Eigenfactor (Walters, 2017) indicators still remain based on\ncitation counts and fail to acknowledge the critical differences that the type\nof citation made, whether it's supporting or disputing a work when quantifying\njournal impact. While various programs have been suggested to apply and\nencompass citation content analysis within bibliometrics projects, citation\ncontent analysis has not been done at the scale needed in order to supplement\nquantitate journal citation analysis until the scite citation index was\nproduced. Using this citation index containing citation types based on citation\nfunction (supporting, disputing, or mentioning) we present initial results on\nthe statistical characterization of citations to journals based on citation\nfunction. We also present initial results of characterizing the ratio of\nsupports and disputes received by a journal as a potential indicator of quality\nand show two interesting results: the ratio of supports and disputes do not\ncorrelate with total citations and that the distribution of this ratio is not\nskewed showing a normal distribution. We conclude with a proposal for future\nresearch using citation analysis qualified by citation function as well as the\nimplications of performing bibliometrics tasks such as research evaluation and\ninformation retrieval using citation function.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 14:15:50 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Rosati", "Domenic", ""]]}, {"id": "2102.11118", "submitter": "Lihao Yin", "authors": "Lihao Yin, Huiyan Sang, Douglas J. Schnoebelen, Brian Wels, Don\n  Simmons, Alyssa Mattson, Michael Schueller, Michael Pentelladan, Susie Y. Dai", "title": "Risk Based Arsenic Rational Sampling Design for Public and Environmental\n  Health Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Groundwater contaminated with arsenic has been recognized as a global threat,\nwhich negatively impacts human health. Populations that rely on private wells\nfor their drinking water are vulnerable to the potential arsenic-related health\nrisks such as cancer and birth defects. Arsenic exposure through drinking water\nis among one of the primary arsenic exposure routes that can be effectively\nmanaged by active testing and water treatment. From the public and\nenvironmental health management perspective, it is critical to allocate the\nlimited resources to establish an effective arsenic sampling and testing plan\nfor health risk mitigation. We present a spatially adaptive sampling design\napproach based on an estimation of the spatially varying underlying\ncontamination distribution. The method is different from traditional sampling\ndesign methods that often rely on a spatially constant or smoothly varying\ncontamination distribution. In contrast, we propose a statistical\nregularization method to automatically detect spatial clusters of the\nunderlying contamination risk from the currently available private well arsenic\ntesting data in the USA, Iowa. This approach allows us to develop a sampling\ndesign method that is adaptive to the changes in the contamination risk across\nthe identified clusters.\n  We provide the spatially adaptive sample size calculation and sampling\nlocation determination at different acceptance precision and confidence levels\nfor each cluster. The spatially adaptive sampling approach may effectively\nmitigate the arsenic risk from the resource management perspectives. The model\npresents a framework that can be widely used for other environmental\ncontaminant monitoring and sampling for public and environmental health.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 15:50:01 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Yin", "Lihao", ""], ["Sang", "Huiyan", ""], ["Schnoebelen", "Douglas J.", ""], ["Wels", "Brian", ""], ["Simmons", "Don", ""], ["Mattson", "Alyssa", ""], ["Schueller", "Michael", ""], ["Pentelladan", "Michael", ""], ["Dai", "Susie Y.", ""]]}, {"id": "2102.11157", "submitter": "Lihao Yin", "authors": "Lihao Yin, Huiyan Sang", "title": "Fused Spatial Point Process Intensity Estimation with Varying\n  Coefficients on Complex Constrained Domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of large spatial data geocoded at accurate locations has\nfueled a growing interest in spatial modeling and analysis of point processes.\nThe proposed research is motivated by the intensity estimation problem for\nlarge spatial point patterns on complex domains in $\\mathbb{R}^2$ (e.g.,\ndomains with irregular boundaries, sharp concavities, and/or interior holes due\nto geographic constraints) and linear networks, where many existing spatial\npoint process models suffer from the problems of \"leakage\" and computation. We\npropose an efficient intensity estimation algorithm to estimate the spatially\nvarying intensity function and to study the varying relationship between\nintensity and explanatory variables on complex domains. The method is built\nupon a graph regularization technique and hence can be flexibly applied to\npoint patterns on complex domains such as regions with irregular boundaries and\nholes, or linear networks. An efficient proximal gradient optimization\nalgorithm is proposed to handle large spatial point patterns. We also derive\nthe asymptotic error bound for the proposed estimator. Numerical studies are\nconducted to illustrate the performance of the method. Finally, we apply the\nmethod to study and visualize the intensity patterns of the accidents on the\nWestern Australia road network, and the spatial variations in the effects of\nincome, lights condition, and population density on the Toronto homicides\noccurrences.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 16:27:52 GMT"}, {"version": "v2", "created": "Fri, 16 Jul 2021 14:27:54 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Yin", "Lihao", ""], ["Sang", "Huiyan", ""]]}, {"id": "2102.11249", "submitter": "Iwona Hawryluk", "authors": "Iwona Hawryluk, Henrique Hoeltgebaum, Swapnil Mishra, Xenia\n  Miscouridou, Ricardo P Schnekenberg, Charles Whittaker, Michaela Vollmer,\n  Seth Flaxman, Samir Bhatt, Thomas A Mellan", "title": "Gaussian Process Nowcasting: Application to COVID-19 Mortality Reporting", "comments": "26 pages, 31 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Updating observations of a signal due to the delays in the measurement\nprocess is a common problem in signal processing, with prominent examples in a\nwide range of fields. An important example of this problem is the nowcasting of\nCOVID-19 mortality: given a stream of reported counts of daily deaths, can we\ncorrect for the delays in reporting to paint an accurate picture of the\npresent, with uncertainty? Without this correction, raw data will often mislead\nby suggesting an improving situation. We present a flexible approach using a\nlatent Gaussian process that is capable of describing the changing\nauto-correlation structure present in the reporting time-delay surface. This\napproach also yields robust estimates of uncertainty for the estimated\nnowcasted numbers of deaths. We test assumptions in model specification such as\nthe choice of kernel or hyper priors, and evaluate model performance on a\nchallenging real dataset from Brazil. Our experiments show that Gaussian\nprocess nowcasting performs favourably against both comparable methods, and\nagainst a small sample of expert human predictions. Our approach has\nsubstantial practical utility in disease modelling -- by applying our approach\nto COVID-19 mortality data from Brazil, where reporting delays are large, we\ncan make informative predictions on important epidemiological quantities such\nas the current effective reproduction number.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 18:32:44 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 20:20:28 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Hawryluk", "Iwona", ""], ["Hoeltgebaum", "Henrique", ""], ["Mishra", "Swapnil", ""], ["Miscouridou", "Xenia", ""], ["Schnekenberg", "Ricardo P", ""], ["Whittaker", "Charles", ""], ["Vollmer", "Michaela", ""], ["Flaxman", "Seth", ""], ["Bhatt", "Samir", ""], ["Mellan", "Thomas A", ""]]}, {"id": "2102.11279", "submitter": "Albert Navarro", "authors": "Gilma Hern\\'andez-Herrera, David Mori\\~na, Albert Navarro", "title": "Left-censored recurrent event analysis in epidemiological studies: a\n  proposal when the number of previous episodes is unknown", "comments": "1 table, 2 supplementary tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Left censoring can occur with relative frequency when analysing recurrent\nevents in epidemiological studies, especially observational ones. Concretely,\nthe inclusion of individuals that were already at risk before the effective\ninitiation in a cohort study, may cause the unawareness of prior episodes that\nhave already been experienced, and this will easily lead to biased and\ninefficient estimates. The objective of this paper is to propose a statistical\nmethod that performs successfully in these circumstances. Our proposal is based\non the use of models with specific baseline hazard, imputing the number of\nprior episodes when unknown, with a stratified model depending on whether the\nindividual had or had not previously been at risk, and the use of a frailty\nterm. The performance is examined in different scenarios through a\ncomprehensive simulation study.The proposed method achieves notable performance\neven when the percentage of subjects at risk before the beginning of the\nfollow-up is very elevated, with biases that are often under 10\\% and coverages\nof around 95\\%, sometimes somewhat conservative. If the baseline hazard is\nconstant, it seems to be that the ``Gap Time'' approach is better; if it is not\nconstant, the ``Counting Process'' seems to be a better choice. Because of the\nlack of knowledge of the prior episodes that have been experienced by a part\n(or all) of subjects, the use of common baseline methods is not advised. Our\nproposal seems to perform acceptably in the majority of the scenarios proposed,\nbecoming an interesting alternative in this context.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 15:26:24 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Hern\u00e1ndez-Herrera", "Gilma", ""], ["Mori\u00f1a", "David", ""], ["Navarro", "Albert", ""]]}, {"id": "2102.11401", "submitter": "Dan Li", "authors": "Dan Li, Nagi Gebraeel, Kamran Paynabar, and A.P. Sakis Meliopoulos", "title": "An Online Approach to Cyberattack Detection and Localization in Smart\n  Grid", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex interconnections between information technology and digital control\nsystems have significantly increased cybersecurity vulnerabilities in smart\ngrids. Cyberattacks involving data integrity can be very disruptive because of\ntheir potential to compromise physical control by manipulating measurement\ndata. This is especially true in large and complex electric networks that often\nrely on traditional intrusion detection systems focused on monitoring network\ntraffic. In this paper, we develop an online detection algorithm to detect and\nlocalize covert attacks on smart grids. Using a network system model, we\ndevelop a theoretical framework by characterizing a covert attack on a\ngenerator bus in the network as sparse features in the state-estimation\nresiduals. We leverage such sparsity via a regularized linear regression method\nto detect and localize covert attacks based on the regression coefficients. We\nconduct a comprehensive numerical study on both linear and nonlinear system\nmodels to validate our proposed method. The results show that our method\noutperforms conventional methods in both detection delay and localization\naccuracy.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 23:09:12 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Li", "Dan", ""], ["Gebraeel", "Nagi", ""], ["Paynabar", "Kamran", ""], ["Meliopoulos", "A. P. Sakis", ""]]}, {"id": "2102.11612", "submitter": "Martin Neil", "authors": "Martin Neil", "title": "Positive results from UK single gene testing for SARS-COV-2 may be\n  inconclusive, negative or detecting past infections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The UK Office for National Statistics (ONS) publish a regular infection\nsurvey that reports data on positive RT-PCR test results for SARS-COV-2 virus.\nThis survey reports that a large proportion of positive test results may be\nbased on the detection of a single target gene rather than on two or more\ntarget genes as required in the manufacturer instructions for use, and by the\nWHO in their emergency use assessment. Without diagnostic validation, for both\nthe original virus and any variants, it is not clear what can be concluded from\na positive test resulting from a single target gene call, especially if there\nwas no confirmatory testing. Given this, many of the reported positive results\nmay be inconclusive, negative or from people who suffered past infection for\nSARS-COV-2.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 10:49:09 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 22:29:41 GMT"}, {"version": "v3", "created": "Thu, 18 Mar 2021 12:35:38 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Neil", "Martin", ""]]}, {"id": "2102.11814", "submitter": "I. Esra Buyuktahtakin", "authors": "Xuecheng Yin and I. Esra Buyuktahtakin", "title": "A Multi-Stage Stochastic Programming Approach to Epidemic Resource\n  Allocation with Equity Considerations", "comments": "Accepted for publication in Health Care Management Science, Feb 19,\n  2021, 1-58", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.OH math.OC q-bio.PE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing compartmental models in epidemiology are limited in terms of\noptimizing the resource allocation to control an epidemic outbreak under\ndisease growth uncertainty. In this study, we address this core limitation by\npresenting a multi-stage stochastic programming compartmental model, which\nintegrates the uncertain disease progression and resource allocation to control\nan infectious disease outbreak. The proposed multi-stage stochastic program\ninvolves various disease growth scenarios and optimizes the distribution of\ntreatment centers and resources while minimizing the total expected number of\nnew infections and funerals. We define two new equity metrics, namely infection\nand capacity equity, and explicitly consider equity for allocating treatment\nfunds and facilities over multiple time stages. We also study the multi-stage\nvalue of the stochastic solution (VSS), which demonstrates the superiority of\nthe proposed stochastic programming model over its deterministic counterpart.\nWe apply the proposed formulation to control the Ebola Virus Disease (EVD) in\nGuinea, Sierra Leone, and Liberia of West Africa to determine the optimal and\nfair resource-allocation strategies. Our model balances the proportion of\ninfections over all regions, even without including the infection equity or\nprevalence equity constraints. Model results also show that allocating\ntreatment resources proportional to population is sub-optimal, and enforcing\nsuch a resource allocation policy might adversely impact the total number of\ninfections and deaths, and thus resulting in a high cost that we have to pay\nfor the fairness. Our multi-stage stochastic epidemic-logistics model is\npractical and can be adapted to control other infectious diseases in\nmeta-populations and dynamically evolving situations.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 17:28:40 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Yin", "Xuecheng", ""], ["Buyuktahtakin", "I. Esra", ""]]}, {"id": "2102.11904", "submitter": "Irina Degtiar", "authors": "Irina Degtiar and Sherri Rose", "title": "A Review of Generalizability and Transportability", "comments": "30 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When assessing causal effects, determining the target population to which the\nresults are intended to generalize is a critical decision. Randomized and\nobservational studies each have strengths and limitations for estimating causal\neffects in a target population. Estimates from randomized data may have\ninternal validity but are often not representative of the target population.\nObservational data may better reflect the target population, and hence be more\nlikely to have external validity, but are subject to potential bias due to\nunmeasured confounding. While much of the causal inference literature has\nfocused on addressing internal validity bias, both internal and external\nvalidity are necessary for unbiased estimates in a target population. This\npaper presents a framework for addressing external validity bias, including a\nsynthesis of approaches for generalizability and transportability, the\nassumptions they require, as well as tests for the heterogeneity of treatment\neffects and differences between study and target populations.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 19:34:13 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Degtiar", "Irina", ""], ["Rose", "Sherri", ""]]}, {"id": "2102.11926", "submitter": "Alexander Tarr", "authors": "Alexander Tarr and Kosuke Imai", "title": "Estimating Average Treatment Effects with Support Vector Machines", "comments": "37 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support vector machine (SVM) is one of the most popular classification\nalgorithms in the machine learning literature. We demonstrate that SVM can be\nused to balance covariates and estimate average causal effects under the\nunconfoundedness assumption. Specifically, we adapt the SVM classifier as a\nkernel-based weighting procedure that minimizes the maximum mean discrepancy\nbetween the treatment and control groups while simultaneously maximizing\neffective sample size. We also show that SVM is a continuous relaxation of the\nquadratic integer program for computing the largest balanced subset,\nestablishing its direct relation to the cardinality matching method. Another\nimportant feature of SVM is that the regularization parameter controls the\ntrade-off between covariate balance and effective sample size. As a result, the\nexisting SVM path algorithm can be used to compute the balance-sample size\nfrontier. We characterize the bias of causal effect estimation arising from\nthis trade-off, connecting the proposed SVM procedure to the existing kernel\nbalancing methods. Finally, we conduct simulation and empirical studies to\nevaluate the performance of the proposed methodology and find that SVM is\ncompetitive with the state-of-the-art covariate balancing methods.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 20:22:56 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 17:41:14 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Tarr", "Alexander", ""], ["Imai", "Kosuke", ""]]}, {"id": "2102.11948", "submitter": "Xiaojing Zhu", "authors": "Xiaojing Zhu, Heather Shappell, Mark A. Kramer, Catherine J. Chu, Eric\n  D. Kolaczyk", "title": "Inferring the Type of Phase Transitions Undergone in Epileptic Seizures\n  Using Random Graph Hidden Markov Models for Percolation in Noisy Dynamic\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In clinical neuroscience, epileptic seizures have been associated with the\nsudden emergence of coupled activity across the brain. The resulting functional\nnetworks - in which edges indicate strong enough coupling between brain regions\n- are consistent with the notion of percolation, which is a phenomenon in\ncomplex networks corresponding to the sudden emergence of a giant connected\ncomponent. Traditionally, work has concentrated on noise-free percolation with\na monotonic process of network growth, but real-world networks are more\ncomplex. We develop a class of random graph hidden Markov models (RG-HMMs) for\ncharacterizing percolation regimes in noisy, dynamically evolving networks in\nthe presence of edge birth and edge death, as well as noise. This class is used\nto understand the type of phase transitions undergone in a seizure, and in\nparticular, distinguishing between different percolation regimes in epileptic\nseizures. We develop a hypothesis testing framework for inferring putative\npercolation mechanisms. As a necessary precursor, we present an EM algorithm\nfor estimating parameters from a sequence of noisy networks only observed at a\nlongitudinal subsampling of time points. Our results suggest that different\ntypes of percolation can occur in human seizures. The type inferred may suggest\ntailored treatment strategies and provide new insights into the fundamental\nscience of epilepsy.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 21:31:18 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Zhu", "Xiaojing", ""], ["Shappell", "Heather", ""], ["Kramer", "Mark A.", ""], ["Chu", "Catherine J.", ""], ["Kolaczyk", "Eric D.", ""]]}, {"id": "2102.12039", "submitter": "Kun Meng", "authors": "Kun Meng, Ani Eloyan", "title": "Population-level Task-evoked Functional Connectivity", "comments": "30 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional magnetic resonance imaging (fMRI) is a non-invasive, in-vivo\nimaging technique essential for measuring brain activity. Functional\nconnectivity is used to study associations between brain regions either at rest\nor while study participants perform tasks. This paper proposes a rigorous\ndefinition of task-evoked functional connectivity at the population level\n(ptFC). Importantly, our proposed ptFC is interpretable in the context of\ntask-fMRI studies. Two algorithms for estimating ptFC are provided. We present\nthe performance of the proposed algorithms compared to existing functional\nconnectivity estimation approaches using simulations. Lastly, we apply the\nproposed framework to estimate functional connectivity in a motor-task study\nfrom the Human Connectome Project.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 03:08:01 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2021 21:23:57 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Meng", "Kun", ""], ["Eloyan", "Ani", ""]]}, {"id": "2102.12454", "submitter": "Panpan Zhang", "authors": "Tao Wang, Shiying Xiao, Jun Yan, Panpan Zhang", "title": "Regional and Sectoral Structures and Their Dynamics of Chinese Economy:\n  A Network Perspective from Multi-Regional Input-Output Tables", "comments": null, "journal-ref": null, "doi": "10.1016/j.physa.2021.126196", "report-no": null, "categories": "physics.soc-ph econ.GN q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multi-regional input-output table (MRIOT) containing the transactions among\nthe region-sectors in an economy defines a weighted and directed network. Using\nnetwork analysis tools, we analyze the regional and sectoral structure of the\nChinese economy and their temporal dynamics from 2007 to 2012 via the MRIOTs of\nChina. Global analyses are done with network topology measures. Growth-driving\nprovince-sector clusters are identified with community detection methods.\nInfluential province-sectors are ranked by weighted PageRank scores. The\nresults revealed a few interesting and telling insights. The level of\ninter-province-sector activities increased with the rapid growth of the\nnational economy, but not as fast as that of intra-province economic\nactivities. Regional community structures were deeply associated with\ngeographical factors. The community heterogeneity across the regions was high\nand the regional fragmentation increased during the study period. Quantified\nmetrics assessing the relative importance of the province-sectors in the\nnational economy echo the national and regional economic development policies\nto a certain extent.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 18:38:03 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Wang", "Tao", ""], ["Xiao", "Shiying", ""], ["Yan", "Jun", ""], ["Zhang", "Panpan", ""]]}, {"id": "2102.12526", "submitter": "Will Consagra", "authors": "William Consagra, Arun Venkataraman, and Zhengwu Zhang", "title": "Optimized Diffusion Imaging for Brain Structural Connectome Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High angular resolution diffusion imaging (HARDI), a type of diffusion\nmagnetic resonance imaging (dMRI) that measures diffusion signals on a sphere\nin q-space, is widely used in data acquisition for human brain structural\nconnectome analysis. Accurate estimation of the local diffusion, and thus the\nstructural connectome, typically requires dense sampling in HARDI, resulting in\nlong acquisition times and logistical challenges. We propose a method to select\nan optimal set of q-space directions for recovery of the local diffusion under\na sparsity constraint on the sampling budget. Relevant historical dMRI data is\nleveraged to estimate a prior distribution of the local diffusion in a template\nspace using reduced rank Gaussian process models. For a new subject to be\nscanned, the priors are mapped into the subject-specific coordinate and used to\nguide an optimized q-space sampling which minimizes the expected integrated\nsquared error of a diffusion function estimator from sparse samples. The\noptimized sampling locations are inferred by an efficient greedy algorithm with\ntheoretical bounds approximating the global optimum. Simulation studies and a\nreal data application using the Human Connectome Project data demonstrate that\nour proposed method provides substantial advantages over its competitors.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 19:42:27 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Consagra", "William", ""], ["Venkataraman", "Arun", ""], ["Zhang", "Zhengwu", ""]]}, {"id": "2102.12698", "submitter": "Nikola Surjanovic", "authors": "Nikola Surjanovic and Thomas M. Loughin", "title": "Improving the Hosmer-Lemeshow Goodness-of-Fit Test in Large Models with\n  Replicated Trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Hosmer-Lemeshow (HL) test is a commonly used global goodness-of-fit (GOF)\ntest that assesses the quality of the overall fit of a logistic regression\nmodel. In this paper, we give results from simulations showing that the type 1\nerror rate (and hence power) of the HL test decreases as model complexity\ngrows, provided that the sample size remains fixed and binary replicates are\npresent in the data. We demonstrate that the generalized version of the HL test\nby Surjanovic et al. (2020) can offer some protection against this power loss.\nWe conclude with a brief discussion explaining the behaviour of the HL test,\nalong with some guidance on how to choose between the two tests.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 05:53:40 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Surjanovic", "Nikola", ""], ["Loughin", "Thomas M.", ""]]}, {"id": "2102.13103", "submitter": "Marie Davidian", "authors": "Anastasios A. Tsiatis and Marie Davidian", "title": "Estimating Vaccine Efficacy Over Time After a Randomized Study is\n  Unblinded", "comments": "32 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The COVID-19 pandemic due to the novel coronavirus SARS CoV-2 has inspired\nremarkable breakthroughs in development of vaccines against the virus and the\nlaunch of several phase 3 vaccine trials in Summer 2020 to evaluate vaccine\nefficacy (VE). Trials of vaccine candidates using mRNA delivery systems\ndeveloped by Pfizer-BioNTech and Moderna have shown substantial VEs of 94-95%,\nleading the US Food and Drug Administration to issue Emergency Use\nAuthorizations and subsequent widespread administration of the vaccines. As the\ntrials continue, a key issue is the possibility that VE may wane over time.\nEthical considerations dictate that all trial participants be unblinded and\nthose randomized to placebo be offered vaccine, leading to trial protocol\namendments specifying unblinding strategies. Crossover of placebo subjects to\nvaccine complicates inference on waning of VE. We focus on the particular\nfeatures of the Moderna trial and propose a statistical framework based on a\npotential outcomes formulation within which we develop methods for inference on\nwhether or not VE wanes over time and estimation of VE at any post-vaccination\ntime. The framework clarifies assumptions made regarding individual- and\npopulation-level phenomena and acknowledges the possibility that subjects who\nare more or less likely to become infected may be crossed over to vaccine\ndifferentially over time. The principles of the framework can be adapted\nstraightforwardly to other trials.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 19:44:33 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Tsiatis", "Anastasios A.", ""], ["Davidian", "Marie", ""]]}, {"id": "2102.13107", "submitter": "Benoit Goussen", "authors": "Benoit Goussen, Cecilie Rendal, David Sheffield, Emma Butler, Oliver\n  R. Price, Roman Ashauer", "title": "Bioenergetics modelling to analyse and predict the joint effects of\n  multiple stressors: Meta-analysis and model corroboration", "comments": "10 pages", "journal-ref": "Science of The Total Environment. 749:141509 (2020)", "doi": "10.1016/j.scitotenv.2020.141509", "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding the consequences of the combined effects of multiple\nstressors-including stress from man-made chemicals is important for\nconservation management, the ecological risk assessment of chemicals, and many\nother ecological applications. Our current ability to predict and analyse the\njoint effects of multiple stressors is insufficient to make the prospective\nrisk assessment of chemicals more ecologically relevant because we lack a full\nunderstanding of how organisms respond to stress factors alone and in\ncombination. Here, we describe a Dynamic Energy Budget (DEB) based\nbioenergetics model that predicts the potential effects of single or multiple\nnatural and chemical stressors on life history traits. We demonstrate the\nplausibility of the model using a meta-analysis of 128 existing studies on\nfreshwater invertebrates. We then validate our model by comparing its\npredictions for a combination of three stressors (i.e. chemical, temperature,\nand food availability) with new, independent experimental data on life history\ntraits in the daphnid Ceriodaphnia dubia. We found that the model predictions\nare in agreement with observed growth curves and reproductive traits. To the\nbest of our knowledge, this is the first time that the combined effects of\nthree stress factors on life history traits observed in laboratory studies have\nbeen predicted successfully in invertebrates. We suggest that a re-analysis of\nexisting studies on multiple stressors within the modelling framework outlined\nhere will provide a robust null model for identifying stressor interactions,\nand expect that a better understanding of the underlying mechanisms will arise\nfrom these new analyses. Bioenergetics modelling could be applied more broadly\nto support environmental management decision making.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 12:23:15 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Goussen", "Benoit", ""], ["Rendal", "Cecilie", ""], ["Sheffield", "David", ""], ["Butler", "Emma", ""], ["Price", "Oliver R.", ""], ["Ashauer", "Roman", ""]]}, {"id": "2102.13209", "submitter": "Fotios Petropoulos", "authors": "Fotios Petropoulos and Yael Grushka-Cockayne", "title": "Fast and frugal time series forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the years, families of forecasting models, such as the exponential\nsmoothing family and Autoregressive Integrated Moving Average, have expanded to\ncontain multiple possible forms and forecasting profiles. In this paper, we\nquestion the need to consider such large families of models. We argue that\nparsimoniously identifying suitable subsets of models will not decrease the\nforecasting accuracy nor will it reduce the ability to estimate the forecast\nuncertainty. We propose a framework that balances forecasting performance\nversus computational cost, resulting in a set of reduced families of models and\nempirically demonstrate this trade-offs. We translate computational benefits to\nmonetary cost savings and discuss the implications of our results in the\ncontext of large retailers.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 22:00:55 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Petropoulos", "Fotios", ""], ["Grushka-Cockayne", "Yael", ""]]}, {"id": "2102.13282", "submitter": "Jonathan Lamontagne PhD", "authors": "Jonathan R. Lamontagne, Martin Jasek, Jared D. Smith", "title": "Coupling physical understanding and statistical modeling to estimate ice\n  jam flood frequency in the northern Peace-Athabasca Delta under climate\n  change", "comments": "Submitted to Cold Regions Science and Technology. 50 pages, 9\n  figures, 5 tables, 1 SI figure, 3 SI tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The Peace-Athabasca Delta (PAD) of northwestern Alberta is one of the largest\ninland freshwater deltas in the world, laying at the confluence of the Peace\nand Athabasca Rivers. The PAD is recognized as a having unique ecological\nsignificance and periodic ice jam flooding from both rivers is an important\nfeature of its current ecology. Past studies have debated whether a change in\nice jam flood (IJF) frequency on the Peace River has recently occurred, and\nwhat factors might be driving any perceived changes. This study contributes to\nthis debate by addressing two questions: (1) what factors are most predictive\nof Peace River IJFs, and (2) how might climate change impact IJF frequency?\nThis work starts with a physically-based conceptual model of the necessary\nconditions for a large Peace River IJF, and the factors that indicate whether\nthose conditions are met. Logistic regression is applied to the historical\nflood record to determine which combination of hydroclimatic and riverine\nfactors best predict IJFs and the uncertainty in those relationships given the\navailable data. Winter precipitation and temperature are most predictive of\nPeace River IJFs, while freeze-up elevation contains little predictive power\nand is not closely related to IJF occurrence. The best logistic regression\nmodel is forced with downscaled climate change scenarios from multiple climate\nmodels to project IJF frequency for a variety of plausible futures. Parametric\nuncertainty in the best logistic regression model is propagated into the\nprojections using a parametric bootstrap to sample many plausible statistical\nmodels. Although there is variability across emissions scenarios and climate\nmodels, all projections indicate that the frequency of Peace River IJFs is\nlikely to decrease substantially in the coming decades, and that average\nwaiting times between future IJFs will likely surpass recent experience.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 02:57:09 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Lamontagne", "Jonathan R.", ""], ["Jasek", "Martin", ""], ["Smith", "Jared D.", ""]]}, {"id": "2102.13284", "submitter": "Harlin Lee", "authors": "Harlin Lee, Boyue Li, Shelly DeForte, Mark Splaingard, Yungui Huang,\n  Yuejie Chi, Simon Lin", "title": "NCH Sleep DataBank: A Large Collection of Real-world Pediatric Sleep\n  Studies", "comments": "Dataset is available at https://sleepdata.org/datasets/nchsdb", "journal-ref": null, "doi": "10.25822/jpdr-vz50", "report-no": null, "categories": "eess.SP stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite being crucial to health and quality of life, sleep -- especially\npediatric sleep -- is not yet well understood. This is exacerbated by lack of\naccess to sufficient pediatric sleep data with clinical annotation. In order to\naccelerate research on pediatric sleep and its connection to health, we create\nthe Nationwide Children's Hospital (NCH) Sleep DataBank and publish it at the\nNational Sleep Research Resource (NSRR), which is a large sleep data common\nwith physiological data, clinical data, and tools for analyses. The NCH Sleep\nDataBank consists of 3,984 polysomnography studies and over 5.6 million\nclinical observations on 3,673 unique patients between 2017 and 2019 at NCH.\nThe novelties of this dataset include: 1) large-scale sleep dataset suitable\nfor discovering new insights via data mining, 2) explicit focus on pediatric\npatients, 3) gathered in a real-world clinical setting, and 4) the accompanying\nrich set of clinical data. The NCH Sleep DataBank is a valuable resource for\nadvancing automatic sleep scoring and real-time sleep disorder prediction,\namong many other potential scientific discoveries.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 03:02:02 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 19:11:29 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Lee", "Harlin", ""], ["Li", "Boyue", ""], ["DeForte", "Shelly", ""], ["Splaingard", "Mark", ""], ["Huang", "Yungui", ""], ["Chi", "Yuejie", ""], ["Lin", "Simon", ""]]}, {"id": "2102.13287", "submitter": "Xiaoping Shi", "authors": "Xiaoping Shi, Meiqian Chen and Yucheng Dong", "title": "Exploring the space-time pattern of log-transformed infectious count of\n  COVID-19: a clustering-segmented autoregressive sigmoid model", "comments": "29 pages and 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  At the end of April 20, 2020, there were only a few new COVID-19 cases\nremaining in China, whereas the rest of the world had shown increases in the\nnumber of new cases. It is of extreme importance to develop an efficient\nstatistical model of COVID-19 spread, which could help in the global fight\nagainst the virus. We propose a clustering-segmented autoregressive sigmoid\n(CSAS) model to explore the space-time pattern of the log-transformed\ninfectious count. Four key characteristics are included in this CSAS model,\nincluding unknown clusters, change points, stretched S-curves, and\nautoregressive terms, in order to understand how this outbreak is spreading in\ntime and in space, to understand how the spread is affected by epidemic control\nstrategies, and to apply the model to updated data from an extended period of\ntime. We propose a nonparametric graph-based clustering method for discovering\ndissimilarity of the curve time series in space, which is justified with\ntheoretical support to demonstrate how the model works under mild and easily\nverified conditions. We propose a very strict purity score that penalizes\noverestimation of clusters. Simulations show that our nonparametric graph-based\nclustering method is faster and more accurate than the parametric clustering\nmethod regardless of the size of data sets. We provide a Bayesian information\ncriterion (BIC) to identify multiple change points and calculate a confidence\ninterval for a mean response. By applying the CSAS model to the collected data,\nwe can explain the differences between prevention and control policies in China\nand selected countries.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 03:08:50 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Shi", "Xiaoping", ""], ["Chen", "Meiqian", ""], ["Dong", "Yucheng", ""]]}, {"id": "2102.13330", "submitter": "Eduardo Ogasawara", "authors": "Claudio Teixeira, Lucas Giusti, Jorge Soares, Joel dos Santos, Glauco\n  Amorim, Eduardo Ogasawara", "title": "Integrated Dataset of Brazilian Flights", "comments": null, "journal-ref": null, "doi": "10.21227/k10b-qn21", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Brazilian commercial aviation system achieved the first position among\nLatin American countries and the fifteenth place worldwide on the Revenue\nPassenger-Kilometer (RPK) ranking. The availability of data regarding flight,\nincluding flight information and meteorological conditions, enables studies\nabout the Brazilian flight system, such as flight delays and timetabling.\nTherefore, this paper contributes to such studies by offering an integrated\ndataset containing data on departure and arrival for flights departing and\narriving at Brazilian airports comprising the period from 2000 to 2019. This\npaper presents a dataset composed of $15,505,922$ records of flight data, each\ncontaining 45 attributes. The attributes include data regarding the airline,\nflight, airports, meteorological conditions, scheduled and elapsed times for\ndeparture and arrival.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 06:57:29 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Teixeira", "Claudio", ""], ["Giusti", "Lucas", ""], ["Soares", "Jorge", ""], ["Santos", "Joel dos", ""], ["Amorim", "Glauco", ""], ["Ogasawara", "Eduardo", ""]]}, {"id": "2102.13366", "submitter": "Ali Bereyhi", "authors": "Ali Bereyhi and Saba Asaad and Ralf R. M\\\"uller", "title": "Oversampled Adaptive Sensing via a Predefined Codebook", "comments": "6 pages, 4 figures. Presented in 2021 IEEE JC&S", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Oversampled adaptive sensing (OAS) is a Bayesian framework recently proposed\nfor effective sensing of structured signals in a time-limited setting. In\ncontrast to the conventional blind oversampling, OAS uses the prior information\non the signal to construct posterior beliefs sequentially. These beliefs help\nin constructive oversampling which iteratively evolves through a sequence of\ntime sub-frames.\n  The initial studies of OAS consider the idealistic assumption of full control\non sensing coefficients which is not feasible in many applications. In this\nwork, we extend the initial investigations on OAS to more realistic settings in\nwhich the sensing coefficients are selected from a predefined set of possible\nchoices, referred to as the codebook. We extend the OAS framework to these\nsettings and compare its performance with classical non-adaptive approaches.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 09:26:20 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Bereyhi", "Ali", ""], ["Asaad", "Saba", ""], ["M\u00fcller", "Ralf R.", ""]]}, {"id": "2102.13393", "submitter": "Michael Pfarrhofer", "authors": "Manfred M. Fischer, Niko Hauzenberger, Florian Huber, Michael\n  Pfarrhofer", "title": "General Bayesian time-varying parameter VARs for predicting government\n  bond yields", "comments": "JEL: C11, C30, E37, E43; KEYWORDS: Bayesian shrinkage, interest rate\n  forecasting, latent effect modifiers, MCMC sampling, time-varying parameter\n  regression", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Time-varying parameter (TVP) regressions commonly assume that time-variation\nin the coefficients is determined by a simple stochastic process such as a\nrandom walk. While such models are capable of capturing a wide range of dynamic\npatterns, the true nature of time variation might stem from other sources, or\narise from different laws of motion. In this paper, we propose a flexible TVP\nVAR that assumes the TVPs to depend on a panel of partially latent covariates.\nThe latent part of these covariates differ in their state dynamics and thus\ncapture smoothly evolving or abruptly changing coefficients. To determine which\nof these covariates are important, and thus to decide on the appropriate state\nevolution, we introduce Bayesian shrinkage priors to perform model selection.\nAs an empirical application, we forecast the US term structure of interest\nrates and show that our approach performs well relative to a set of competing\nmodels. We then show how the model can be used to explain structural breaks in\ncoefficients related to the US yield curve.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 11:02:58 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Fischer", "Manfred M.", ""], ["Hauzenberger", "Niko", ""], ["Huber", "Florian", ""], ["Pfarrhofer", "Michael", ""]]}, {"id": "2102.13404", "submitter": "Dohyun Chun", "authors": "Dohyun Chun, Donggyu Kim", "title": "State Heterogeneity Analysis of Financial Volatility Using\n  High-Frequency Financial Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, to account for low-frequency market dynamics, several volatility\nmodels, employing high-frequency financial data, have been developed. However,\nin financial markets, we often observe that financial volatility processes\ndepend on economic states, so they have a state heterogeneous structure. In\nthis paper, to study state heterogeneous market dynamics based on\nhigh-frequency data, we introduce a novel volatility model based on a\ncontinuous Ito diffusion process whose intraday instantaneous volatility\nprocess evolves depending on the exogenous state variable, as well as its\nintegrated volatility. We call it the state heterogeneous GARCH-Ito (SG-Ito)\nmodel. We suggest a quasi-likelihood estimation procedure with the realized\nvolatility proxy and establish its asymptotic behaviors. Moreover, to test the\nlow-frequency state heterogeneity, we develop a Wald test-type hypothesis\ntesting procedure. The results of empirical studies suggest the existence of\nleverage, investor attention, market illiquidity, stock market comovement, and\npost-holiday effect in S&P 500 index volatility.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 11:30:20 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Chun", "Dohyun", ""], ["Kim", "Donggyu", ""]]}, {"id": "2102.13550", "submitter": "Madan Kundu", "authors": "Madan G. Kundu, Sandipan Samanta and Shoubhik Mondal", "title": "An introduction to the determination of the probability of a successful\n  trial: Frequentist and Bayesian approaches", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Determination of posterior probability for go-no-go decision and predictive\npower are becoming increasingly common for resource optimization in clinical\ninvestigation. There are vast published literature on these topics; however,\nthe terminologies are not consistently used across the literature. Further,\nthere is a lack of consolidated presentation of various concepts of the\nprobability of success. We attempted to fill this gap. This paper first\nprovides a detailed derivation of these probability of success measures under\nthe frequentist and Bayesian paradigms in a general setting. Subsequently, we\nhave presented the analytical formula for these probability of success measures\nfor continuous, binary, and time-to-event endpoints separately. This paper can\nbe used as a single point reference to determine the following measures: (a)\nthe conditional power (CP) based on interim results, (b) the predictive power\nof success (PPoS) based on interim results with or without prior distribution,\nand (d) the probability of success (PoS) for a prospective trial at the design\nstage. We have discussed both clinical success and trial success. This paper's\ndiscussion is mostly based on the normal approximation for prior distribution\nand the estimate of the parameter of interest. Besides, predictive power using\nthe beta prior for the binomial case is also presented. Some examples are given\nfor illustration. R functions to calculate CP and PPoS are available through\nthe LongCART package. An R shiny app is also available at\nhttps://ppos.herokuapp.com/.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 15:42:20 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Kundu", "Madan G.", ""], ["Samanta", "Sandipan", ""], ["Mondal", "Shoubhik", ""]]}]