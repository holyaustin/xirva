[{"id": "1408.0047", "submitter": "Truyen Tran", "authors": "Truyen Tran, Dinh Phung, Svetha Venkatesh", "title": "Cumulative Restricted Boltzmann Machines for Ordinal Matrix Data\n  Analysis", "comments": "JMLR: Workshop and Conference Proceedings 25:1-16, 2012; Asian\n  Conference on Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ordinal data is omnipresent in almost all multiuser-generated feedback -\nquestionnaires, preferences etc. This paper investigates modelling of ordinal\ndata with Gaussian restricted Boltzmann machines (RBMs). In particular, we\npresent the model architecture, learning and inference procedures for both\nvector-variate and matrix-variate ordinal data. We show that our model is able\nto capture latent opinion profile of citizens around the world, and is\ncompetitive against state-of-art collaborative filtering techniques on\nlarge-scale public datasets. The model thus has the potential to extend\napplication of RBMs to diverse domains such as recommendation systems, product\nreviews and expert assessments.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jul 2014 23:54:16 GMT"}], "update_date": "2014-08-04", "authors_parsed": [["Tran", "Truyen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1408.0083", "submitter": "Jung-Ying Tzeng", "authors": "Jung-Ying Tzeng, Wenbin Lu, Fang-Chi Hsu", "title": "Gene-level pharmacogenetic analysis on survival outcomes using\n  gene-trait similarity regression", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS735 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 2, 1232-1255", "doi": "10.1214/14-AOAS735", "report-no": "IMS-AOAS-AOAS735", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gene/pathway-based methods are drawing significant attention due to their\nusefulness in detecting rare and common variants that affect disease\nsusceptibility. The biological mechanism of drug responses indicates that a\ngene-based analysis has even greater potential in pharmacogenetics. Motivated\nby a study from the Vitamin Intervention for Stroke Prevention (VISP) trial, we\ndevelop a gene-trait similarity regression for survival analysis to assess the\neffect of a gene or pathway on time-to-event outcomes. The similarity\nregression has a general framework that covers a range of survival models, such\nas the proportional hazards model and the proportional odds model. The\ninference procedure developed under the proportional hazards model is robust\nagainst model misspecification. We derive the equivalence between the\nsimilarity survival regression and a random effects model, which further\nunifies the current variance component-based methods. We demonstrate the\neffectiveness of the proposed method through simulation studies. In addition,\nwe apply the method to the VISP trial data to identify the genes that exhibit\nan association with the risk of a recurrent stroke. The TCN2 gene was found to\nbe associated with the recurrent stroke risk in the low-dose arm. This gene may\nimpact recurrent stroke risk in response to cofactor therapy.\n", "versions": [{"version": "v1", "created": "Fri, 1 Aug 2014 05:36:59 GMT"}], "update_date": "2014-08-04", "authors_parsed": [["Tzeng", "Jung-Ying", ""], ["Lu", "Wenbin", ""], ["Hsu", "Fang-Chi", ""]]}, {"id": "1408.0087", "submitter": "Ville A. Satop\\\"{a}\\\"{a}", "authors": "Ville A. Satop\\\"a\\\"a, Shane T. Jensen, Barbara A. Mellers, Philip E.\n  Tetlock, Lyle H. Ungar", "title": "Probability aggregation in time-series: Dynamic hierarchical modeling of\n  sparse expert beliefs", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS739 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 2, 1256-1280", "doi": "10.1214/14-AOAS739", "report-no": "IMS-AOAS-AOAS739", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most subjective probability aggregation procedures use a single probability\njudgment from each expert, even though it is common for experts studying real\nproblems to update their probability estimates over time. This paper advances\ninto unexplored areas of probability aggregation by considering a dynamic\ncontext in which experts can update their beliefs at random intervals. The\nupdates occur very infrequently, resulting in a sparse data set that cannot be\nmodeled by standard time-series procedures. In response to the lack of\nappropriate methodology, this paper presents a hierarchical model that takes\ninto account the expert's level of self-reported expertise and produces\naggregate probabilities that are sharp and well calibrated both in- and\nout-of-sample. The model is demonstrated on a real-world data set that includes\nover 2300 experts making multiple probability forecasts over two years on\ndifferent subsets of 166 international political events.\n", "versions": [{"version": "v1", "created": "Fri, 1 Aug 2014 06:22:11 GMT"}], "update_date": "2014-08-04", "authors_parsed": [["Satop\u00e4\u00e4", "Ville A.", ""], ["Jensen", "Shane T.", ""], ["Mellers", "Barbara A.", ""], ["Tetlock", "Philip E.", ""], ["Ungar", "Lyle H.", ""]]}, {"id": "1408.0095", "submitter": "Seongho Kim", "authors": "Seongho Kim, Ming Ouyang, Jaesik Jeong, Changyu Shen, Xiang Zhang", "title": "A new method of peak detection for analysis of comprehensive\n  two-dimensional gas chromatography mass spectrometry data", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS731 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 2, 1209-1231", "doi": "10.1214/14-AOAS731", "report-no": "IMS-AOAS-AOAS731", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel peak detection algorithm for the analysis of comprehensive\ntwo-dimensional gas chromatography time-of-flight mass spectrometry\n(GC$\\times$GC-TOF MS) data using normal-exponential-Bernoulli (NEB) and mixture\nprobability models. The algorithm first performs baseline correction and\ndenoising simultaneously using the NEB model, which also defines peak regions.\nPeaks are then picked using a mixture of probability distribution to deal with\nthe co-eluting peaks. Peak merging is further carried out based on the mass\nspectral similarities among the peaks within the same peak group. The algorithm\nis evaluated using experimental data to study the effect of different cutoffs\nof the conditional Bayes factors and the effect of different mixture models\nincluding Poisson, truncated Gaussian, Gaussian, Gamma and exponentially\nmodified Gaussian (EMG) distributions, and the optimal version is introduced\nusing a trial-and-error approach. We then compare the new algorithm with two\nexisting algorithms in terms of compound identification. Data analysis shows\nthat the developed algorithm can detect the peaks with lower false discovery\nrates than the existing algorithms, and a less complicated peak picking model\nis a promising alternative to the more complicated and widely used EMG mixture\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 1 Aug 2014 07:49:21 GMT"}], "update_date": "2014-08-04", "authors_parsed": [["Kim", "Seongho", ""], ["Ouyang", "Ming", ""], ["Jeong", "Jaesik", ""], ["Shen", "Changyu", ""], ["Zhang", "Xiang", ""]]}, {"id": "1408.0177", "submitter": "Alejandro Frery", "authors": "Juliana Gambini, Julia Cassetti, Mar\\'ia Magdalena Lucini, Alejandro\n  C. Frery", "title": "Parameter Estimation in SAR Imagery using Stochastic Distances and\n  Asymmetric Kernels", "comments": "Accepted for publication in the IEEE Journal of Selected Topics in\n  Applied Earth Observations and Remote Sensing (IEEE J-STARS), July 2014", "journal-ref": null, "doi": "10.1109/JSTARS.2014.2346017", "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we analyze several strategies for the estimation of the\nroughness parameter of the $\\mathcal G_I^0$ distribution. It has been shown\nthat this distribution is able to characterize a large number of targets in\nmonopolarized SAR imagery, deserving the denomination of \"Universal Model\" It\nis indexed by three parameters: the number of looks (which can be estimated in\nthe whole image), a scale parameter, and the roughness or texture parameter.\nThe latter is closely related to the number of elementary backscatters in each\npixel, one of the reasons for receiving attention in the literature. Although\nthere are efforts in providing improved and robust estimates for such quantity,\nits dependable estimation still poses numerical problems in practice. We\ndiscuss estimators based on the minimization of stochastic distances between\nempirical and theoretical densities, and argue in favor of using an estimator\nbased on the Triangular distance and asymmetric kernels built with Inverse\nGaussian densities. We also provide new results regarding the heavytailedness\nof this distribution.\n", "versions": [{"version": "v1", "created": "Fri, 1 Aug 2014 13:41:51 GMT"}, {"version": "v2", "created": "Mon, 4 Aug 2014 08:12:12 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Gambini", "Juliana", ""], ["Cassetti", "Julia", ""], ["Lucini", "Mar\u00eda Magdalena", ""], ["Frery", "Alejandro C.", ""]]}, {"id": "1408.0251", "submitter": "Bello Oyedele Adeshina", "authors": "Adeshina Oyedele Bello", "title": "Modeling Cassava Yield: A Response Surface Approach", "comments": null, "journal-ref": "International Journal on Computational Sciences & Applications\n  (IJCSA) Vol.4, No.3, June 2014", "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports on application of theory of experimental design using\ngraphical techniques in R programming language and application of nonlinear\nbootstrap regression method to demonstrate the invariant property of parameter\nestimates of the Inverse polynomial Model (IPM) in a nonlinear surface.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jul 2014 10:29:00 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2015 13:12:42 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Bello", "Adeshina Oyedele", ""]]}, {"id": "1408.0711", "submitter": "Florence Forbes", "authors": "Darren Wraith and Florence Forbes", "title": "Clustering using skewed multivariate heavy tailed distributions with\n  flexible tail behaviour", "comments": "31 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The family of location and scale mixtures of Gaussians has the ability to\ngenerate a number of flexible distributional forms. It nests as particular\ncases several important asymmetric distributions like the Generalised\nHyperbolic distribution. The Generalised Hyperbolic distribution in turn nests\nmany other well known distributions such as the Normal Inverse Gaussian (NIG)\nwhose practical relevance has been widely documented in the literature. In a\nmultivariate setting, we propose to extend the standard location and scale\nmixture concept into a so called multiple scaled framework which has the\nadvantage of allowing different tail and skewness behaviours in each dimension\nof the variable space with arbitrary correlation between dimensions. Estimation\nof the parameters is provided via an EM algorithm with a particular focus on\nNIG distributions. Inference is then extended to cover the case of mixtures of\nsuch multiple scaled distributions for application to clustering. Assessments\non simulated and real data confirm the gain in degrees of freedom and\nflexibility in modelling data of varying tail behaviour and directional shape.\n", "versions": [{"version": "v1", "created": "Fri, 1 Aug 2014 16:01:37 GMT"}], "update_date": "2014-08-05", "authors_parsed": [["Wraith", "Darren", ""], ["Forbes", "Florence", ""]]}, {"id": "1408.0777", "submitter": "Daniel Cervone", "authors": "Daniel Cervone, Alex D'Amour, Luke Bornn, and Kirk Goldsberry", "title": "A Multiresolution Stochastic Process Model for Predicting Basketball\n  Possession Outcomes", "comments": "31 pages, 9 figures", "journal-ref": "Journal Of The American Statistical Association Vol. 111, Iss.\n  514, 2016", "doi": "10.1080/01621459.2016.1141685", "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Basketball games evolve continuously in space and time as players constantly\ninteract with their teammates, the opposing team, and the ball. However,\ncurrent analyses of basketball outcomes rely on discretized summaries of the\ngame that reduce such interactions to tallies of points, assists, and similar\nevents. In this paper, we propose a framework for using optical player tracking\ndata to estimate, in real time, the expected number of points obtained by the\nend of a possession. This quantity, called \\textit{expected possession value}\n(EPV), derives from a stochastic process model for the evolution of a\nbasketball possession; we model this process at multiple levels of resolution,\ndifferentiating between continuous, infinitesimal movements of players, and\ndiscrete events such as shot attempts and turnovers. Transition kernels are\nestimated using hierarchical spatiotemporal models that share information\nacross players while remaining computationally tractable on very large data\nsets. In addition to estimating EPV, these models reveal novel insights on\nplayers' decision-making tendencies as a function of their spatial strategy.\n", "versions": [{"version": "v1", "created": "Mon, 4 Aug 2014 19:22:47 GMT"}, {"version": "v2", "created": "Fri, 22 May 2015 20:52:53 GMT"}, {"version": "v3", "created": "Thu, 25 Feb 2016 18:38:38 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Cervone", "Daniel", ""], ["D'Amour", "Alex", ""], ["Bornn", "Luke", ""], ["Goldsberry", "Kirk", ""]]}, {"id": "1408.1187", "submitter": "Mattia Ciollaro", "authors": "Mattia Ciollaro, Christopher Genovese, Jing Lei and Larry Wasserman", "title": "The functional mean-shift algorithm for mode hunting and clustering in\n  infinite dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the functional mean-shift algorithm, an iterative algorithm for\nestimating the local modes of a surrogate density from functional data. We show\nthat the algorithm can be used for cluster analysis of functional data. We\npropose a test based on the bootstrap for the significance of the estimated\nlocal modes of the surrogate density. We present two applications of our\nmethodology. In the first application, we demonstrate how the functional\nmean-shift algorithm can be used to perform spike sorting, i.e. cluster neural\nactivity curves. In the second application, we use the functional mean-shift\nalgorithm to distinguish between original and fake signatures.\n", "versions": [{"version": "v1", "created": "Wed, 6 Aug 2014 05:12:04 GMT"}], "update_date": "2014-08-07", "authors_parsed": [["Ciollaro", "Mattia", ""], ["Genovese", "Christopher", ""], ["Lei", "Jing", ""], ["Wasserman", "Larry", ""]]}, {"id": "1408.1239", "submitter": "Abhik Ghosh", "authors": "Abhik Ghosh and Ayanendranath Basu", "title": "The Minimum S-Divergence Estimator under Continuous Models: The\n  Basu-Lindsay Approach", "comments": "Pre-Print, 34 pages", "journal-ref": null, "doi": "10.1007/s00362-015-0701-3", "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust inference based on the minimization of statistical divergences has\nproved to be a useful alternative to the classical maximum likelihood based\ntechniques. Recently Ghosh et al. (2013) proposed a general class of divergence\nmeasures for robust statistical inference, named the S-Divergence Family. Ghosh\n(2014) discussed its asymptotic properties for the discrete model of densities.\nIn the present paper, we develop the asymptotic properties of the proposed\nminimum S-Divergence estimators under continuous models. Here we use the\nBasu-Lindsay approach (1994) of smoothing the model densities that, unlike\nprevious approaches, avoids much of the complications of the kernel bandwidth\nselection. Illustrations are presented to support the performance of the\nresulting estimators both in terms of efficiency and robustness through\nextensive simulation studies and real data examples.\n", "versions": [{"version": "v1", "created": "Wed, 6 Aug 2014 10:50:45 GMT"}, {"version": "v2", "created": "Thu, 18 Dec 2014 14:33:29 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Ghosh", "Abhik", ""], ["Basu", "Ayanendranath", ""]]}, {"id": "1408.1296", "submitter": "Enkelejd Hashorva", "authors": "Enkelejd Hashorva and Jinzhi Li", "title": "Tail Behaviour of Weighted Sums of Order Statistics of Dependent Risks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $X_{1},\\ldots ,X_{n}$ be $n$ real-valued dependent random variables. With\nmotivation from Mitra and Resnick (2009), we derive the tail asymptotic\nexpansion for the weighted sum of order statistics $X_{1:n}\\leq \\cdots \\leq\nX_{n:n}$ of $X_{1},\\ldots ,X_{n}$ under the general case in which the\ndistribution function of $X_{n:n}$ is long-tailed or rapidly varying and $%\nX_{1},\\ldots ,X_{n}$ may not be comparable in terms of their tail probability.\nWe also present two examples and an application of our results in risk theory.\n", "versions": [{"version": "v1", "created": "Wed, 6 Aug 2014 14:40:23 GMT"}], "update_date": "2014-08-07", "authors_parsed": [["Hashorva", "Enkelejd", ""], ["Li", "Jinzhi", ""]]}, {"id": "1408.1744", "submitter": "Julien Carron", "authors": "Julien Carron, Istv\\'an Szapudi", "title": "The impact of super-survey modes on cosmological constraints from cosmic\n  shear fields", "comments": "11 pages, 4 figures, matches version accepted for publication in\n  MNRAS", "journal-ref": null, "doi": "10.1093/mnras/stu2501", "report-no": null, "categories": "astro-ph.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Owing to the mass-sheet degeneracy, cosmic shear maps do not probe directly\nthe Fourier modes of the underlying mass distribution on scales comparable to\nthe survey size and larger. To assess the corresponding effect on attainable\ncosmological parameter constraints, we quantify the information on super-survey\nmodes in a lognormal model and, when interpreted as nuisance parameters, their\ndegeneracies to cosmological parameters. Our analytical and numerical\ncalculations clarify the central role of super-sample covariance (SSC) in\nshaping the statistical power of cosmological observables. Reconstructing the\nbackground modes from their non-Gaussian statistical dependence to small scales\nmodes yields the renormalized convergence. This diagonalizes the spectrum\ncovariance matrix, and the information content of the corresponding power\nspectrum is increased by a factor of two over standard methods. Unfortunately,\ncareful calculation of the Cramer-Rao bound shows that the information recovery\ncan never be made complete, any observable built from shear fields, including\noptimal sufficient statistics, are subject to severe information loss,\ntypically $80\\%$ to $90\\%$ below $\\ell \\sim 3000$ for generic cosmological\nparameters. The lost information can only be recovered from additional,\nnon-shear based data. Our predictions hold just as well for a tomographic\nanalysis, and/or full sky surveys.\n", "versions": [{"version": "v1", "created": "Fri, 8 Aug 2014 01:43:25 GMT"}, {"version": "v2", "created": "Tue, 25 Nov 2014 19:52:25 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Carron", "Julien", ""], ["Szapudi", "Istv\u00e1n", ""]]}, {"id": "1408.1889", "submitter": "Dianne Cook", "authors": "Niladri Roy Chowdhury, Dianne Cook, Heike Hofmann, Mahbubul Majumder,\n  Yifan Zhao", "title": "Utilizing Distance Metrics on Lineups to Examine What People Read From\n  Data Plots", "comments": "28 pages, lots of figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphics play a crucial role in statistical analysis and data mining. This\npaper describes metrics developed to assist the use of lineups for making\ninferential statements. Lineups embed the plot of the data among a set of null\nplots, and engage a human observer to select the plot that is most different\nfrom the rest. If the data plot is selected it corresponds to the rejection of\na null hypothesis. Metrics are calculated in association with lineups, to\nmeasure the quality of the lineup, and help to understand what people see in\nthe data plots. The null plots represent a finite sample from a null\ndistribution, and the selected sample potentially affects the ease or\ndifficulty of a lineup. Distance metrics are designed to describe how close the\ntrue data plot is to the null plots, and how close the null plots are to each\nother. The distribution of the distance metrics is studied to learn how well\nthis matches to what people detect in the plots, the effect of null generating\nmechanism and plot choices for particular tasks. The analysis was conducted on\ndata that has already been collected from Amazon Turk studies conducted with\nlineups for studying an array of data analysis tasks.\n", "versions": [{"version": "v1", "created": "Fri, 8 Aug 2014 15:30:13 GMT"}], "update_date": "2014-08-11", "authors_parsed": [["Chowdhury", "Niladri Roy", ""], ["Cook", "Dianne", ""], ["Hofmann", "Heike", ""], ["Majumder", "Mahbubul", ""], ["Zhao", "Yifan", ""]]}, {"id": "1408.1974", "submitter": "Mahbubul Majumder", "authors": "Mahbubul Majumder, Heike Hofmann, Dianne Cook", "title": "Human Factors Influencing Visual Statistical Inference", "comments": "18 pages, 8 figures, presented in JSM 2014 at Boston", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual statistical inference is a way to determine significance of patterns\nfound while exploring data. It is dependent on the evaluation of a lineup, of a\ndata plot among a sample of null plots, by human observers. Each individual is\ndifferent in their cognitive psychology and judiciousness, which can affect the\nvisual inference. The usual way to estimate the effectiveness of a statistical\ntest is its power. The estimate of power of a lineup can be controlled by\ncombining evaluations from multiple observers. Factors that may also affect the\npower of visual inference are the observers' demographics, visual skills, and\nexperience, the sample of null plots taken from the null distribution, the\nposition of the data plot in the lineup, and the signal strength in the data.\nThis paper examines these factors. Results from multiple visual inference\nstudies using Amazon's Mechanical Turk are examined to provide an assessment of\nthese. The experiments suggest that individual skills vary substantially, but\ndemographics do not have a huge effect on performance. There is evidence that a\nlearning effect exists but only in that observers get faster with repeated\nevaluations, but not more often correct. The placement of data plot in the\nlineup does not affect the inference.\n", "versions": [{"version": "v1", "created": "Fri, 8 Aug 2014 20:30:22 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Majumder", "Mahbubul", ""], ["Hofmann", "Heike", ""], ["Cook", "Dianne", ""]]}, {"id": "1408.2128", "submitter": "Paul McNicholas", "authors": "Antonio Punzo, Martin Blostein and Paul D. McNicholas", "title": "High-dimensional unsupervised classification via parsimonious\n  contaminated mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The contaminated Gaussian distribution represents a simple heavy-tailed\nelliptical generalization of the Gaussian distribution; unlike the\noften-considered t-distribution, it also allows for automatic detection of mild\noutlying or \"bad\" points in the same way that observations are typically\nassigned to the groups in the finite mixture model context. Starting from this\ndistribution, we propose the contaminated factor analysis model as a method for\ndimensionality reduction and detection of bad points in higher dimensions. A\nmixture of contaminated Gaussian factor analyzers (MCGFA) model follows\ntherefrom, and extends the recently proposed mixture of contaminated Gaussian\ndistributions to high-dimensional data. We introduce a family of 32\nparsimonious models formed by introducing constraints on the covariance and\ncontamination structures of the general MCGFA model. We outline a variant of\nthe expectation-maximization algorithm for parameter estimation. Various\nimplementation issues are discussed, and the novel family of models is compared\nto well-established approaches on both simulated and real data.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 15:39:13 GMT"}, {"version": "v2", "created": "Sun, 28 May 2017 22:13:32 GMT"}, {"version": "v3", "created": "Mon, 22 Jul 2019 21:23:37 GMT"}, {"version": "v4", "created": "Wed, 28 Aug 2019 18:33:38 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Punzo", "Antonio", ""], ["Blostein", "Martin", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1408.2319", "submitter": "Michela Gnaldi mg", "authors": "Michela Gnaldi, Silvia Bacci, Francesco Bartolucci", "title": "A multilevel finite mixture item response model to cluster examinees and\n  schools", "comments": "17 pages, original article. arXiv admin note: text overlap with\n  arXiv:1212.0378", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the educational context, a key goal is to assess students acquired\nskills and to cluster students according to their ability level. In this\nregard, a relevant element to be accounted for is the possible effect of the\nschool students come from. For this aim, we provide a methodological tool which\ntakes into account the multilevel structure of the data (i.e., students in\nschools) in a suitable way. This approach allows us to cluster both students\nand schools into homogeneous classes of ability and effectiveness, and to\nassess the effect of certain students and school characteristics on the\nprobability to belong to such classes. The approach relies on an extended class\nof multidimensional latent class IRT models characterized by: (i) latent traits\ndefined at student level and at school level, (ii) latent traits represented\nthrough random vectors with a discrete distribution, (iii) the inclusion of\ncovariates at student level and at school level, and (iv) a two-parameter\nlogistic parametrization for the conditional probability of a correct response\ngiven the ability. The approach is applied for the analysis of data collected\nby two national tests administered in Italy to middle school students in June\n2009: the INVALSI Italian Test and Mathematics Test. Results allow us to study\nthe relationships between observed characteristics and latent trait standing\nwithin each latent class at the different levels of the hierarchy. They show\nthat examinees and school expected observed scores, at a given latent trait\nlevel, are dependent on both unobserved (latent class) group membership and\nobserved first and second level covariates.\n", "versions": [{"version": "v1", "created": "Mon, 11 Aug 2014 06:02:25 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Gnaldi", "Michela", ""], ["Bacci", "Silvia", ""], ["Bartolucci", "Francesco", ""]]}, {"id": "1408.2616", "submitter": "Suyan Tian", "authors": "Suyan Tian, Chi Wang, Ming-Wen An", "title": "No evidence of histology subtype-specific prognostic signatures among\n  lung adenocarcinoma and squamous cell carcinoma patients at early stages", "comments": "This paper has been withdrawn by the authors due to a huge amount of\n  changes and modifications being made, and then the conclusions on no\n  subtype-specific prognostic genes being not hold anymore", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.GN q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background\n  Non-small cell lung cancer (NSCLC) is the predominant histological type of\nlung cancer, accounting for up to 85% of cases. Disease stage is commonly used\nto determine adjuvant treatment eligibility of NSCLC patients, however, it is\nan imprecise predictor of the prognosis of an individual patient. Currently,\nmany researchers resort to microarray technology for identifying relevant\ngenetic prognostic markers, with particular attention on trimming or extending\na Cox regression model.\n  Among NSCLC, adenocarcinoma (AC) and squamous cell carcinoma (SCC) are two\nmajor histology subtypes. It has been demonstrated that there exist fundamental\ndifferences in the underlying mechanisms between them, which motivated us to\npostulate there might exist specific genes relevant to prognosis of each\nhistology subtype.\n  Results\n  In this article, we propose a simple filterer feature selection algorithm\nwith a Cox regression model as the base. Applying this method to a real-world\nmicroarray data, no evidence has been found to support the existence of\nhistology-specific prognostic gene signature. Nevertheless, a 31-gene\nprognostic gene signature for the early-stage AC and SCC samples is obtained,\nwhich provides comparable performance when compared with other relevant\nsignatures.\n  Conclusions\n  Our proposal is conceptually simple and straightforward to implement.\nTherefore, it is expected that other researchers, especially those with less\nstatistical knowledge and experience, can adapt this method readily to test\ntheir own research hypotheses.\n", "versions": [{"version": "v1", "created": "Tue, 12 Aug 2014 04:43:04 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2015 07:23:18 GMT"}], "update_date": "2015-08-07", "authors_parsed": [["Tian", "Suyan", ""], ["Wang", "Chi", ""], ["An", "Ming-Wen", ""]]}, {"id": "1408.2700", "submitter": "Radu Horaud P", "authors": "Antoine Deleforge, Radu Horaud, Yoav Schechner and Laurent Girin", "title": "Co-Localization of Audio Sources in Images Using Binaural Features and\n  Locally-Linear Regression", "comments": "15 pages, 8 figures", "journal-ref": "IEEE Transactions on Audio, Speech, and Language Processing 23(4),\n  718-731, April, 2015", "doi": "10.1109/TASLP.2015.2405475", "report-no": null, "categories": "cs.SD cs.MM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of localizing audio sources using binaural\nmeasurements. We propose a supervised formulation that simultaneously localizes\nmultiple sources at different locations. The approach is intrinsically\nefficient because, contrary to prior work, it relies neither on source\nseparation, nor on monaural segregation. The method starts with a training\nstage that establishes a locally-linear Gaussian regression model between the\ndirectional coordinates of all the sources and the auditory features extracted\nfrom binaural measurements. While fixed-length wide-spectrum sounds (white\nnoise) are used for training to reliably estimate the model parameters, we show\nthat the testing (localization) can be extended to variable-length\nsparse-spectrum sounds (such as speech), thus enabling a wide range of\nrealistic applications. Indeed, we demonstrate that the method can be used for\naudio-visual fusion, namely to map speech signals onto images and hence to\nspatially align the audio and visual modalities, thus enabling to discriminate\nbetween speaking and non-speaking faces. We release a novel corpus of real-room\nrecordings that allow quantitative evaluation of the co-localization method in\nthe presence of one or two sound sources. Experiments demonstrate increased\naccuracy and speed relative to several state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 12 Aug 2014 12:08:13 GMT"}, {"version": "v2", "created": "Wed, 22 Oct 2014 10:35:07 GMT"}, {"version": "v3", "created": "Fri, 27 Mar 2015 13:45:06 GMT"}, {"version": "v4", "created": "Fri, 15 Apr 2016 10:00:02 GMT"}], "update_date": "2016-04-18", "authors_parsed": [["Deleforge", "Antoine", ""], ["Horaud", "Radu", ""], ["Schechner", "Yoav", ""], ["Girin", "Laurent", ""]]}, {"id": "1408.2724", "submitter": "Mark Kaminskiy", "authors": "Mark. P. Kaminskiy", "title": "Gini-Type Index for Ageing/Rejuvenating Populations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Application of some basic notions and statistics of ageing distributions used\nin mathematical theory of reliability including the Gini-type index is\ndiscussed as a methodological tool for investigation of human population ageing\nand longevity. In opposite to the traditionally used median and mean lifetime,\nthe GTI requires only one calendar year snap-shot data to reveal the population\nageing or rejuvenating. The case study illustrating the suggested techniques is\nbased on the Australia 1921 and Australia 2009 mortality data from the Human\nMortality Database (http://www.mortality.org)\n", "versions": [{"version": "v1", "created": "Tue, 12 Aug 2014 14:31:39 GMT"}], "update_date": "2014-08-13", "authors_parsed": [["Kaminskiy", "Mark. P.", ""]]}, {"id": "1408.2794", "submitter": "Patrick Zeng", "authors": "Angela Gu, Patrick Zeng", "title": "Sector-Based Factor Models for Asset Returns", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factor analysis is a statistical technique employed to evaluate how observed\nvariables correlate through common factors and unique variables. While it is\noften used to analyze price movement in the unstable stock market, it does not\nalways yield easily interpretable results. In this study, we develop improved\nfactor models by explicitly incorporating sector information on our studied\nstocks. We add eleven sectors of stocks as defined by the IBES, represented by\nrespective sector-specific factors, to non-specific market factors to revise\nthe factor model. We then develop an expectation maximization (EM) algorithm to\ncompute our revised model with 15 years' worth of S&P 500 stocks' daily close\nprices. Our results in most sectors show that nearly all of these factor\ncomponents have the same sign, consistent with the intuitive idea that stocks\nin the same sector tend to rise and fall in coordination over time. Results\nobtained by the classic factor model, in contrast, had a homogeneous blend of\npositive and negative components. We conclude that results produced by our\nsector-based factor model are more interpretable than those produced by the\nclassic non-sector-based model for at least some stock sectors.\n", "versions": [{"version": "v1", "created": "Mon, 11 Aug 2014 17:05:51 GMT"}], "update_date": "2014-08-13", "authors_parsed": [["Gu", "Angela", ""], ["Zeng", "Patrick", ""]]}, {"id": "1408.2805", "submitter": "Georg Hofmann", "authors": "Georg Hofmann", "title": "Accelerated Portfolio Optimization with Conditional Value-at-Risk\n  Constraints using a Cutting-Plane Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC q-fin.PM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Financial portfolios are often optimized for maximum profit while subject to\na constraint formulated in terms of the Conditional Value-at-Risk (CVaR). This\namounts to solving a linear problem. However, in its original formulation this\nlinear problem has a very large number of linear constraints, too many to be\nenforced in practice. In the literature this is addressed by a reformulation of\nthe problem using so-called dummy variables. This reduces the large number of\nconstraints in the original linear problem at the cost of increasing the number\nof variables. In the context of reinsurance portfolio optimization we observe\nthat the increase in variable count can lead to situations where solving the\nreformulated problem takes a long time. Therefore we suggest a different\napproach. We solve the original linear problem with cutting-plane method: The\nproposed algorithm starts with the solution of a relaxed problem and then\niteratively adds cuts until the solution is approximated within a preset\nthreshold. This is a new approach. For a reinsurance case study we show that a\nsignificant reduction of necessary computer resources can be achieved.\n", "versions": [{"version": "v1", "created": "Tue, 12 Aug 2014 19:01:24 GMT"}], "update_date": "2014-08-13", "authors_parsed": [["Hofmann", "Georg", ""]]}, {"id": "1408.3333", "submitter": "Amy Willis", "authors": "A. Willis and J. Bunge", "title": "Estimating Diversity via Frequency Ratios", "comments": "17 pages, 1 figure, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We wish to estimate the total number of classes in a population based on\nsample counts, especially in the presence of high latent diversity. Drawing on\nprobability theory that characterizes distributions on the integers by ratios\nof consecutive probabilities, we construct a nonlinear regression model for the\nratios of consecutive frequency counts. This allows us to predict the\nunobserved count and hence estimate the total diversity. We believe that this\nis the first approach to depart from the classical mixed Poisson model in this\nproblem. Our method is geometrically intuitive and yields good fits to data\nwith reasonable standard errors. It is especially well-suited to analyzing high\ndiversity datasets derived from next-generation sequencing in microbial\necology. We demonstrate the method's performance in this context and via\nsimulation, and we present a dataset for which our method outperforms all\ncompetitors.\n", "versions": [{"version": "v1", "created": "Thu, 14 Aug 2014 16:31:08 GMT"}, {"version": "v2", "created": "Tue, 9 Dec 2014 17:16:29 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Willis", "A.", ""], ["Bunge", "J.", ""]]}, {"id": "1408.3434", "submitter": "Bhavya Kailkhura", "authors": "Bhavya Kailkhura and Yunghsiang S. Han and Swastik Brahma and Pramod\n  K. Varshney", "title": "Asymptotic Analysis of Distributed Bayesian Detection with Byzantine\n  Data", "comments": "arXiv admin note: substantial text overlap with arXiv:1307.3544", "journal-ref": null, "doi": "10.1109/LSP.2014.2365196", "report-no": null, "categories": "stat.AP cs.CR cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this letter, we consider the problem of distributed Bayesian detection in\nthe presence of data falsifying Byzantines in the network. The problem of\ndistributed detection is formulated as a binary hypothesis test at the fusion\ncenter (FC) based on 1-bit data sent by the sensors. Adopting Chernoff\ninformation as our performance metric, we study the detection performance of\nthe system under Byzantine attack in the asymptotic regime. The expression for\nminimum attacking power required by the Byzantines to blind the FC is obtained.\nMore specifically, we show that above a certain fraction of Byzantine attackers\nin the network, the detection scheme becomes completely incapable of utilizing\nthe sensor data for detection. When the fraction of Byzantines is not\nsufficient to blind the FC, we also provide closed form expressions for the\noptimal attacking strategies for the Byzantines that most degrade the detection\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 14 Aug 2014 21:37:26 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Kailkhura", "Bhavya", ""], ["Han", "Yunghsiang S.", ""], ["Brahma", "Swastik", ""], ["Varshney", "Pramod K.", ""]]}, {"id": "1408.3685", "submitter": "Yong Huang Dr", "authors": "Yong Huang and James L. Beck", "title": "Hierarchical sparse Bayesian learning for structural health monitoring\n  with incomplete modal data", "comments": "41 pages, 8 figures", "journal-ref": "International Journal for Uncertainty Quantification 2015, 5(2),\n  139-169", "doi": "10.1615/Int.J.UncertaintyQuantification.2015011808", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For civil structures, structural damage due to severe loading events such as\nearthquakes, or due to long-term environmental degradation, usually occurs in\nlocalized areas of a structure. A new sparse Bayesian probabilistic framework\nfor computing the probability of localized stiffness reductions induced by\ndamage is presented that uses noisy incomplete modal data from before and after\npossible damage. This new approach employs system modal parameters of the\nstructure as extra variables for Bayesian model updating with incomplete modal\ndata. A specific hierarchical Bayesian model is constructed that promotes\nspatial sparseness in the inferred stiffness reductions in a way that is\nconsistent with the Bayesian Ockham razor. To obtain the most plausible model\nof sparse stiffness reductions together with its uncertainty within a specified\nclass of models, the method employs an optimization scheme that iterates among\nall uncertain parameters, including the hierarchical hyper-parameters. The\napproach has four important benefits: (1) it infers spatially-sparse stiffness\nchanges based on the identified modal parameters; (2) the uncertainty in the\ninferred stiffness reductions is quantified; (3) no matching of model and\nexperimental modes is needed, and (4) solving the nonlinear eigenvalue problem\nof a structural model is not required. The proposed method is applied to two\npreviously-studied examples using simulated data: a ten-story shear-building\nand the three-dimensional braced-frame model from the Phase II Simulated\nBenchmark problem sponsored by the IASC-ASCE Task Group on Structural Health\nMonitoring. The results show that the occurrence of false-positive and\nfalse-negative damage detection is clearly reduced in the presence of modeling\nerror. Furthermore, the identified most probable stiffness loss ratios are\nclose to their actual values.\n", "versions": [{"version": "v1", "created": "Sat, 16 Aug 2014 00:15:35 GMT"}], "update_date": "2015-07-02", "authors_parsed": [["Huang", "Yong", ""], ["Beck", "James L.", ""]]}, {"id": "1408.4111", "submitter": "Ali Rakhshan", "authors": "Ali Rakhshan, Evan Ray, Hossein Pishro-Nik", "title": "A New Approach to Customization of Collision Warning Systems to\n  Individual Drivers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the need for individualizing safety systems and proposes\nan approach including the Real-Time estimation of the distribution of brake\nresponse times for an individual driver. While maintaining high level of\nsafety, the collision warning system should send \"tailored\" responses to the\ndriver. This method could be the first step to show that safety applications\nwould potentially benefit from customizing to individual drivers'\ncharacteristics using VANET. Our simulation results show that, as one of the\nimminent and preliminary outcomes of the new improved system, the number of\nfalse alarms will be reduced by more than 40%. We think this tactic can reach\nto even beyond the safety applications for designing the future innovative\nsystems.\n", "versions": [{"version": "v1", "created": "Fri, 15 Aug 2014 23:39:22 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2015 00:30:41 GMT"}], "update_date": "2015-12-18", "authors_parsed": [["Rakhshan", "Ali", ""], ["Ray", "Evan", ""], ["Pishro-Nik", "Hossein", ""]]}, {"id": "1408.4158", "submitter": "Christian Mueller", "authors": "Zachary D. Kurtz, Christian L. Mueller, Emily R. Miraldi, Dan R.\n  Littman, Martin J. Blaser, Richard A. Bonneau", "title": "Sparse and compositionally robust inference of microbial ecological\n  networks", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pcbi.1004226", "report-no": null, "categories": "stat.AP q-bio.GN stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  16S-ribosomal sequencing and other metagonomic techniques provide snapshots\nof microbial communities, revealing phylogeny and the abundances of microbial\npopulations across diverse ecosystems. While changes in microbial community\nstructure are demonstrably associated with certain environmental conditions,\nidentification of underlying mechanisms requires new statistical tools, as\nthese datasets present several technical challenges. First, the abundances of\nmicrobial operational taxonomic units (OTUs) from 16S datasets are\ncompositional, and thus, microbial abundances are not independent. Secondly,\nmicrobial sequencing-based studies typically measure hundreds of OTUs on only\ntens to hundreds of samples; thus, inference of OTU-OTU interaction networks is\nseverely under-powered, and additional assumptions are required for accurate\ninference. Here, we present SPIEC-EASI (SParse InversE Covariance Estimation\nfor Ecological Association Inference), a statistical method for the inference\nof microbial ecological interactions from metagenomic datasets that addresses\nboth of these issues. SPIEC-EASI combines data transformations developed for\ncompositional data analysis with a graphical model inference framework that\nassumes the underlying ecological interaction network is sparse. To reconstruct\nthe interaction network, SPIEC-EASI relies on algorithms for sparse\nneighborhood and inverse covariance selection. Because no large-scale microbial\necological networks have been experimentally validated, SPIEC-EASI comprises\ncomputational tools to generate realistic OTU count data from a set of diverse\nunderlying network topologies. SPIEC-EASI outperforms state-of-the-art methods\nin terms of edge recovery and network properties on realistic synthetic data\nunder a variety of scenarios. SPIEC-EASI also reproducibly predicts previously\nunknown microbial interactions using data from the American Gut project.\n", "versions": [{"version": "v1", "created": "Mon, 18 Aug 2014 21:01:32 GMT"}, {"version": "v2", "created": "Mon, 25 Aug 2014 18:39:14 GMT"}, {"version": "v3", "created": "Sat, 14 Feb 2015 01:15:09 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Kurtz", "Zachary D.", ""], ["Mueller", "Christian L.", ""], ["Miraldi", "Emily R.", ""], ["Littman", "Dan R.", ""], ["Blaser", "Martin J.", ""], ["Bonneau", "Richard A.", ""]]}, {"id": "1408.4334", "submitter": "Erwan Koch", "authors": "Erwan Koch and Philippe Naveau", "title": "A frailty-contagion model for multi-site hourly precipitation driven by\n  atmospheric covariates", "comments": "Presented by Erwan Koch at the conferences: - 12th IMSC, Jeju\n  (Korea), June 2013 - ISI WSC 2013, Hong Kong, Aug.2013. Invited speaker in\n  the session \"Probabilistic and statistical contributions in climate research\"", "journal-ref": null, "doi": "10.1016/j.advwatres.2015.01.001", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate stochastic simulations of hourly precipitation are needed for impact\nstudies at local spatial scales. Statistically, hourly precipitation data\nrepresent a difficult challenge. They are non-negative, skewed, heavy tailed,\ncontain a lot of zeros (dry hours) and they have complex temporal structures\n(e.g., long persistence of dry episodes). Inspired by frailty-contagion\napproaches used in finance and insurance, we propose a multi-site precipitation\nsimulator that, given appropriate regional atmospheric variables, can\nsimultaneously handle dry events and heavy rainfall periods. One advantage of\nour model is its conceptual simplicity in its dynamical structure. In\nparticular, the temporal variability is represented by a common factor based on\na few classical atmospheric covariates like temperatures, pressures and others.\nOur inference approach is tested on simulated data and applied on measurements\nmade in the northern part of French Brittany.\n", "versions": [{"version": "v1", "created": "Tue, 19 Aug 2014 13:37:55 GMT"}, {"version": "v2", "created": "Wed, 20 Aug 2014 12:03:18 GMT"}, {"version": "v3", "created": "Thu, 27 Nov 2014 15:04:00 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Koch", "Erwan", ""], ["Naveau", "Philippe", ""]]}, {"id": "1408.4378", "submitter": "Mauro Ribeiro de Oliveira JR", "authors": "Mauro R. Oliveira and Francisco Louzada", "title": "An Evidence of Link between Default and Loss of Bank Loans from the\n  Modeling of Competing Risks", "comments": "8 pages", "journal-ref": "Singaporean Journal of Business Economics and Management Studies,\n  Vol 3 Ed.(1), p. 30-37, 2014", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a method that provides a useful technique to\ncompare relationship between risks involved that takes customer become\ndefaulter and debt collection process that might make this defaulter recovered.\nThrough estimation of competitive risks that lead to realization of the event\nof interest, we showed that there is a significant relation between the\nintensity of default and losses from defaulted loans in collection processes.\nTo reach this goal, we investigate a competing risks model applied to whole\ncredit risk cycle into a bank loans portfolio. We estimated competing causes\nrelated to occurrence of default, thereafter, comparing it with estimated\ncompeting causes that lead loans to write-off condition. In context of modeling\ncompeting risks, we used a specification of Poisson distribution for numbers\nfrom competing causes and Weibull distribution for failures times. The\nlikelihood maximum estimation is used to parameters estimation and the model is\napplied to a real data of personal loans\n", "versions": [{"version": "v1", "created": "Tue, 19 Aug 2014 16:10:38 GMT"}], "update_date": "2014-08-20", "authors_parsed": [["Oliveira", "Mauro R.", ""], ["Louzada", "Francisco", ""]]}, {"id": "1408.4380", "submitter": "Mauro Ribeiro de Oliveira JR", "authors": "Mauro R. Oliveira and Francisco Louzada", "title": "Recovery Risk: Application of the Latent Competing Risks Model to Non\n  performing Loans", "comments": "10 pages", "journal-ref": "Tecnologia de Cr\\'edito (Serasa-Experian), ed. 88, p. 44-53, 2014", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes a method for measuring the latent risks involved in the\nrecovery process of non performing loans in financial institutions and business\nfirms that deal with collection and recovery processes. To that end, we apply\nthe competing risks model referred to in the literature as the promotion time\nmodel. The result achieved is the probability of credit recovery for a\nportfolio segmented into groups based on the information available. Within the\ncontext of competing risks, application of the technique yielded an estimation\nof the number of latent events that concur to the credit recovery event. With\nthese results in hand, we were able to compare groups of defaulters in terms of\nrisk or susceptibility to the recovery event during the collection process, and\nthereby determine where collection actions are most efficient. We specify the\nPoisson distribution for the number of latent causes leading to recovery, and\nthe Weibull distribution for the time up to recovery. To estimate the model\nparameters, we use the maximum likelihood method. Finally, the model was\napplied to a sample of defaulted loans from a financial institution.\n", "versions": [{"version": "v1", "created": "Tue, 19 Aug 2014 16:18:39 GMT"}], "update_date": "2014-08-20", "authors_parsed": [["Oliveira", "Mauro R.", ""], ["Louzada", "Francisco", ""]]}, {"id": "1408.4383", "submitter": "Phillip Schumm", "authors": "Phillip Schumm and Caterina Scoglio and H Morgan Scott", "title": "An estimation of cattle movement parameters in the Central States of the\n  US", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The characterization of cattle demographics and especially movements is an\nessential component in the modeling of dynamics in cattle systems, yet for\ncattle systems of the United States (US), this is missing. Through a\nlarge-scale maximum entropy optimization formulation, we estimate cattle\nmovement parameters to characterize the movements of cattle across $10$ Central\nStates and $1034$ counties of the United States. Inputs to the estimation\nproblem are taken from the United States Department of Agriculture National\nAgricultural Statistics Service database and are pre-processed in a pair of\ntightly constrained optimization problems to recover non-disclosed elements of\ndata. We compare stochastic subpopulation-based movements generated from the\nestimated parameters to operation-based movements published by the United\nStates Department of Agriculture. For future Census of Agriculture\ndistributions, we propose a series of questions that enable improvements for\nour method without compromising the privacy of cattle operations. Our novel\nmethod to estimate cattle movements across large US regions characterizes\ncounty-level stratified subpopulations of cattle for data-driven livestock\nmodeling. Our estimated movement parameters suggest a significant risk level\nfor US cattle systems.\n", "versions": [{"version": "v1", "created": "Tue, 19 Aug 2014 16:23:55 GMT"}], "update_date": "2014-08-20", "authors_parsed": [["Schumm", "Phillip", ""], ["Scoglio", "Caterina", ""], ["Scott", "H Morgan", ""]]}, {"id": "1408.4435", "submitter": "Ashkan Panahi", "authors": "Ashkan Panahi, Marie Str\\\"om, Mats Viberg", "title": "Wideband Waveform Design for Robust Target Detection", "comments": "This paper is submitted for peer review to IEEE letters on signal\n  processing", "journal-ref": null, "doi": "10.1109/ICASSP.2015.7178707", "report-no": null, "categories": "stat.AP cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future radar systems are expected to use waveforms of a high bandwidth, where\nthe main advantage is an improved range resolution. In this paper, a technique\nto design robust wideband waveforms for a Multiple-Input-Single-Output system\nis developed. The context is optimal detection of a single object with\npartially unknown parameters. The waveforms are robust in the sense that, for a\nsingle transmission, detection capability is maintained over an interval of\ntime-delay and time-scaling (Doppler) parameters. A solution framework is\nderived, approximated, and formulated as an optimization by means of basis\nexpansion. In terms of probabilities of detection and false alarm, numerical\nevaluation shows the efficiency of the proposed method when compared with a\nLinear Frequency Modulated signal and a Gaussian pulse.\n", "versions": [{"version": "v1", "created": "Tue, 19 Aug 2014 19:29:52 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Panahi", "Ashkan", ""], ["Str\u00f6m", "Marie", ""], ["Viberg", "Mats", ""]]}, {"id": "1408.4542", "submitter": "Daniel Kosiorowski", "authors": "Daniel Kosiorowski and Zygmunt Zawadzki", "title": "DepthProc An R Package for Robust Exploration of Multidimensional\n  Economic Phenomena", "comments": "the package can be obtained from ## FOR DEVELOPER VERSION:\n  require(devtools) # for windows we need RTools install_github(\"DepthProc\",\n  \"zzawadz\", subdir = \"pkg\") ##and from CRAN ## We recommend the developer\n  version in a context of your comments, suggestions, recommendations, our\n  future collaboration; The paper to appear in Journal of Statistical Software\n  in 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data depth concept offers a variety of powerful and user friendly tools for\nrobust exploration and inference for multivariate socio-economic phenomena. The\noffered techniques may be successfully used in cases of lack of our knowledge\non parametric models generating data due to their nonparametric nature. This\npaper presents the R package DepthProc, which is available under GPL-2 licence\non CRAN and R-forge servers for Windows, Linux and OS X platform. The package\nconsist of among others successful implementations of several data depth\ntechniques involving multivariate quantile-quantile plots, multivariate scatter\nestimators, local Wilcoxon tests for multivariate as well as for functional\ndata, robust regressions. In order to show the package capabilities, real\ndatasets concerning United Nations Fourth Millennium Goal and the Internet\nusers activity are used.\n", "versions": [{"version": "v1", "created": "Wed, 20 Aug 2014 06:41:39 GMT"}, {"version": "v10", "created": "Sat, 31 Mar 2018 16:58:07 GMT"}, {"version": "v11", "created": "Tue, 19 Feb 2019 09:12:49 GMT"}, {"version": "v2", "created": "Thu, 21 Aug 2014 07:58:39 GMT"}, {"version": "v3", "created": "Sun, 12 Oct 2014 14:22:40 GMT"}, {"version": "v4", "created": "Sun, 5 Feb 2017 14:16:54 GMT"}, {"version": "v5", "created": "Wed, 8 Feb 2017 18:50:47 GMT"}, {"version": "v6", "created": "Sun, 12 Feb 2017 11:14:04 GMT"}, {"version": "v7", "created": "Fri, 24 Feb 2017 09:09:23 GMT"}, {"version": "v8", "created": "Fri, 21 Apr 2017 07:39:10 GMT"}, {"version": "v9", "created": "Mon, 4 Sep 2017 15:55:00 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Kosiorowski", "Daniel", ""], ["Zawadzki", "Zygmunt", ""]]}, {"id": "1408.4618", "submitter": "Erwan Koch", "authors": "Jean-Cyprien H\\'eam and Erwan Koch", "title": "Diversification and Endogenous Financial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We test the hypothesis that interconnections across financial institutions\ncan be explained by a diversification motive. This idea stems from the\nempirical evidence of the existence of long-term exposures that cannot be\nexplained by a liquidity motive (maturity or currency mismatch). We model\nendogenous interconnections of heterogenous financial institutions facing\nregulatory constraints using a maximization of their expected utility. Both\ntheoretical and simulation-based results are compared to a stylized genuine\nfinancial network. The diversification motive appears to plausibly explain\ninterconnections among key players. Using our model, the impact of regulation\non interconnections between banks -currently discussed at the Basel Committee\non Banking Supervision- is analyzed.\n", "versions": [{"version": "v1", "created": "Wed, 20 Aug 2014 11:55:04 GMT"}, {"version": "v2", "created": "Thu, 21 Aug 2014 07:41:40 GMT"}, {"version": "v3", "created": "Mon, 23 Feb 2015 19:28:15 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["H\u00e9am", "Jean-Cyprien", ""], ["Koch", "Erwan", ""]]}, {"id": "1408.4636", "submitter": "Tiancheng Li", "authors": "Tiancheng Li, Juan M. Corchado, Javier Bajo, Shudong Sun and Juan F.\n  De Paz", "title": "Do we always need a filter?", "comments": "52 pages (single column), 32 figures, 4 tables and a huge number of\n  interesting simulations are presented. The up to date version of this work\n  can be found at https://sites.google.com/site/tianchengli85/o2", "journal-ref": "Information Sciences: vol.329 (2016) vol.388 (2017), IEEE\n  Communications Letters, vol.22, no.10, pp. 2064-2067, 2018", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the groundbreaking work of the Kalman filter in the 1960s, considerable\neffort has been devoted to various discrete time filters for dynamic state\nestimation, especially including dozens of different types of suboptimal\nimplementations of the Bayes filters. This has been accompanied by the rapid\ndevelopment of simulation/approximation theories and technologies. While\nadmitting the success of filters in many cases, this study investigates the\nfailure cases when they are in fact ineffective for state estimation. Several\nclassic models have shown that the straightforward observation-only (O2)\ninference that does not need system modeling can perform better (in terms of\nboth accuracy and computing speed) for estimation than filters. Special\nattention has been paid to quantitatively analyze when and why a filter will\nnot outperform the O2 inference from the information fusion perspective. Thanks\nto the rapid development of advanced sensors, the O2 inference is not only\nengineering friendly and computationally fast but can also be very accurate and\nreliable by fusing the information received from multiple sensors. The\nstatistical attributes of the multi-sensor O2 inference are analyzed and\ndemonstrated through simulations. In the situation with limited sensors, the O2\napproach can work jointly with existing clutter filtering and data association\nalgorithms for multi-target tracking in clutter environments. Given an adequate\nnumber of sensors, the O2 approach can employ the multi-sensor data fusion to\ndeal with clutter and can handle the very general multi-target tracking\nscenario with no background information.\n", "versions": [{"version": "v1", "created": "Wed, 20 Aug 2014 12:54:58 GMT"}, {"version": "v2", "created": "Wed, 1 Oct 2014 16:29:39 GMT"}, {"version": "v3", "created": "Wed, 18 Feb 2015 22:10:00 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Li", "Tiancheng", ""], ["Corchado", "Juan M.", ""], ["Bajo", "Javier", ""], ["Sun", "Shudong", ""], ["De Paz", "Juan F.", ""]]}, {"id": "1408.4812", "submitter": "Adrian Raftery", "authors": "Adrian E. Raftery", "title": "Use and Communication of Probabilistic Forecasts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic forecasts are becoming more and more available. How should they\nbe used and communicated? What are the obstacles to their use in practice? I\nreview experience with five problems where probabilistic forecasting played an\nimportant role. This leads me to identify five types of potential users: Low\nStakes Users, who don't need probabilistic forecasts; General Assessors, who\nneed an overall idea of the uncertainty in the forecast; Change Assessors, who\nneed to know if a change is out of line with expectatations; Risk Avoiders, who\nwish to limit the risk of an adverse outcome; and Decision Theorists, who\nquantify their loss function and perform the decision-theoretic calculations.\nThis suggests that it is important to interact with users and to consider their\ngoals. The cognitive research tells us that calibration is important for trust\nin probability forecasts, and that it is important to match the verbal\nexpression with the task. The cognitive load should be minimized, reducing the\nprobabilistic forecast to a single percentile if appropriate. Probabilities of\nadverse events and percentiles of the predictive distribution of quantities of\ninterest seem often to be the best way to summarize probabilistic forecasts.\nFormal decision theory has an important role, but in a limited range of\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 20 Aug 2014 20:16:00 GMT"}], "update_date": "2014-08-22", "authors_parsed": [["Raftery", "Adrian E.", ""]]}, {"id": "1408.4834", "submitter": "Gianluca Mastrantonio", "authors": "Gianluca Mastrantonio and Antonello Maruotti and Giovanna Jona Lasinio", "title": "Bayesian Hidden Markov Modelling Using Circular-Linear General Projected\n  Normal Distribution", "comments": "Environmetrics, (2015)", "journal-ref": null, "doi": "10.1002/env.2326", "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a multivariate hidden Markov model to jointly cluster\ntime-series observations with different support, i.e. circular and linear.\nRelying on the general projected normal distribution, our approach allows for\nbimodal and/or skewed cluster-specific distributions for the circular variable.\nFurthermore, we relax the independence assumption between the circular and\nlinear components observed at the same time. Such an assumption is generally\nused to alleviate the computational burden involved in the parameter estimation\nstep, but it is hard to justify in empirical applications. We carry out a\nsimulation study using different data-generation schemes to investigate model\nbehavior, focusing on well recovering the hidden structure. Finally, the model\nis used to fit a real data example on a bivariate time series of wind speed and\ndirection.\n", "versions": [{"version": "v1", "created": "Wed, 20 Aug 2014 23:17:32 GMT"}, {"version": "v2", "created": "Sat, 24 Jan 2015 11:10:01 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Mastrantonio", "Gianluca", ""], ["Maruotti", "Antonello", ""], ["Lasinio", "Giovanna Jona", ""]]}, {"id": "1408.5007", "submitter": "Krzysztof Bartoszek", "authors": "Krzysztof Bartoszek and Serik Sagitov", "title": "A Consistent Estimator of the Evolutionary Rate", "comments": null, "journal-ref": "Journal of Theoretical Biology 371:69-78, 2015", "doi": "10.1016/j.jtbi.2015.01.019", "report-no": null, "categories": "q-bio.PE math.PR q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a branching particle system where particles reproduce according\nto the pure birth Yule process with the birth rate L, conditioned on the\nobserved number of particles to be equal n. Particles are assumed to move\nindependently on the real line according to the Brownian motion with the local\nvariance s2. In this paper we treat $n$ particles as a sample of related\nspecies. The spatial Brownian motion of a particle describes the development of\na trait value of interest (e.g. log-body-size). We propose an unbiased\nestimator Rn2 of the evolutionary rate r2=s2/L. The estimator Rn2 is\nproportional to the sample variance Sn2 computed from n trait values. We find\nan approximate formula for the standard error of Rn2 based on a neat asymptotic\nrelation for the variance of Sn2.\n", "versions": [{"version": "v1", "created": "Thu, 21 Aug 2014 14:08:47 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Bartoszek", "Krzysztof", ""], ["Sagitov", "Serik", ""]]}, {"id": "1408.5420", "submitter": "Adrian Dobra", "authors": "Nathalie E. Williams, Timothy A. Thomas, Matthew Dunbar, Nathan Eagle\n  and Adrian Dobra", "title": "Measures of Human Mobility Using Mobile Phone Records Enhanced with GIS\n  Data", "comments": "33 pages, 16 figures, 5 tables", "journal-ref": null, "doi": "10.1371/journal.pone.0133630", "report-no": null, "categories": "physics.soc-ph cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past decade, large scale mobile phone data have become available for\nthe study of human movement patterns. These data hold an immense promise for\nunderstanding human behavior on a vast scale, and with a precision and accuracy\nnever before possible with censuses, surveys or other existing data collection\ntechniques. There is already a significant body of literature that has made key\ninroads into understanding human mobility using this exciting new data source,\nand there have been several different measures of mobility used. However,\nexisting mobile phone based mobility measures are inconsistent, inaccurate, and\nconfounded with social characteristics of local context. New measures would\nbest be developed immediately as they will influence future studies of mobility\nusing mobile phone data. In this article, we do exactly this. We discuss\nproblems with existing mobile phone based measures of mobility and describe new\nmethods for measuring mobility that address these concerns. Our measures of\nmobility, which incorporate both mobile phone records and detailed GIS data,\nare designed to address the spatial nature of human mobility, to remain\nindependent of social characteristics of context, and to be comparable across\ngeographic regions and time. We also contribute a discussion of the variety of\nuses for these new measures in developing a better understanding of how human\nmobility influences micro-level human behaviors and well-being, and macro-level\nsocial organization and change.\n", "versions": [{"version": "v1", "created": "Wed, 20 Aug 2014 23:35:46 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Williams", "Nathalie E.", ""], ["Thomas", "Timothy A.", ""], ["Dunbar", "Matthew", ""], ["Eagle", "Nathan", ""], ["Dobra", "Adrian", ""]]}, {"id": "1408.5442", "submitter": "Nicola Mingotti", "authors": "Nicola Mingotti", "title": "Fooled by bursts. A Goal per Minute model for the World Cup", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On the occasion of the last FIFA World Cup in Brazil, The Economist published\na plot depicting how many goals have been scored in all World Cup competitions\nuntil present, minute by minute. The plot was followed by a naive and poorly\ngrounded qualitative analysis. In the present article we use The Economist\ndataset to check its conclusions, update previous results from literature and\noffer a new model. In particular, it will be shown that first and second half\ngame have different scoring rates. In the first half the scoring rate can be\nconsidered constant. In the second it increases linearly with time.\n", "versions": [{"version": "v1", "created": "Fri, 22 Aug 2014 23:10:53 GMT"}], "update_date": "2014-08-26", "authors_parsed": [["Mingotti", "Nicola", ""]]}, {"id": "1408.5621", "submitter": "Marian Grendar", "authors": "Marian Grend\\'ar and Vladim\\'ir \\v{S}pitalsk\\'y", "title": "Multinomial and empirical likelihood under convex constraints:\n  directions of recession, Fenchel duality, perturbations", "comments": null, "journal-ref": "Electron. J. Statist. 11 (2017), no. 1, 2547--2612", "doi": "10.1214/17-EJS1294", "report-no": null, "categories": "math.ST math.OC stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The primal problem of multinomial likelihood maximization restricted to a\nconvex closed subset of the probability simplex is studied. Contrary to widely\nheld belief, a solution of this problem may assign a positive mass to an\noutcome with zero count. Related flaws in the simplified Lagrange and Fenchel\ndual problems, which arise because the recession directions are ignored, are\nidentified and corrected.\n  A solution of the primal problem can be obtained by the PP (perturbed primal)\nalgorithm, that is, as the limit of a sequence of solutions of perturbed primal\nproblems. The PP algorithm may be implemented by the simplified Fenchel dual.\n  The results permit us to specify linear sets and data such that the empirical\nlikelihood-maximizing distribution exists and is the same as the multinomial\nlikelihood-maximizing distribution. The multinomial likelihood ratio reaches,\nin general, a different conclusion than the empirical likelihood ratio.\n  Implications for minimum discrimination information, compositional data\nanalysis, Lindsay geometry, bootstrap with auxiliary information, and Lagrange\nmultiplier tests are discussed.\n", "versions": [{"version": "v1", "created": "Sun, 24 Aug 2014 17:35:39 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Grend\u00e1r", "Marian", ""], ["\u0160pitalsk\u00fd", "Vladim\u00edr", ""]]}, {"id": "1408.5629", "submitter": "Nathan Baker", "authors": "Huan Lei, Xiu Yang, Bin Zheng, Guang Lin, Nathan A. Baker", "title": "Quantifying the influence of conformational uncertainty in biomolecular\n  solvation", "comments": "Accepted by Multiscale Modeling & Simulation", "journal-ref": "Multiscale.Model.Simul 13 (2015) 1327-1353", "doi": "10.1137/140981587", "report-no": null, "categories": "q-bio.BM stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Biomolecules exhibit conformational fluctuations near equilibrium states,\ninducing uncertainty in various biological properties in a dynamic way. We have\ndeveloped a general method to quantify the uncertainty of target properties\ninduced by conformational fluctuations. Using a generalized polynomial chaos\n(gPC) expansion, we construct a surrogate model of the target property with\nrespect to varying conformational states. We also propose a method to increase\nthe sparsity of the gPC expansion by defining a set of conformational \"active\nspace\" random variables. With the increased sparsity, we employ the compressive\nsensing method to accurately construct the surrogate model. We demonstrate the\nperformance of the surrogate model by evaluating fluctuation-induced\nuncertainty in solvent-accessible surface area for the bovine trypsin inhibitor\nprotein system and show that the new approach offers more accurate statistical\ninformation than standard Monte Carlo approaches. Further more, the constructed\nsurrogate model also enables us to directly evaluate the target property under\nvarious conformational states, yielding a more accurate response surface than\nstandard sparse grid collocation methods. In particular, the new method\nprovides higher accuracy in high-dimensional systems, such as biomolecules,\nwhere sparse grid performance is limited by the accuracy of the computed\nquantity of interest. Our new framework is generalizable and can be used to\ninvestigate the uncertainty of a wide variety of target properties in\nbiomolecular systems.\n", "versions": [{"version": "v1", "created": "Sun, 24 Aug 2014 19:40:55 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2015 13:39:09 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Lei", "Huan", ""], ["Yang", "Xiu", ""], ["Zheng", "Bin", ""], ["Lin", "Guang", ""], ["Baker", "Nathan A.", ""]]}, {"id": "1408.5924", "submitter": "Brian Godsey", "authors": "Brian Godsey", "title": "Comparing and Forecasting Performances in Different Events of Athletics\n  Using a Probabilistic Model", "comments": "Journal of Quantitative Analysis in Sports. Volume 8, Issue 2, ISSN\n  (Online) 1559-0410, June 2012", "journal-ref": null, "doi": "10.1515/1559-0410.1434", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though athletics statistics are abundant, it is a difficult task to\nquantitatively compare performances from different events of track, field, and\nroad running in a meaningful way. There are several commonly-used methods, but\neach has its limitations. Some methods, for example, are valid only for running\nevents, or are unable to compare men's performances to women's, while others\nare based largely on world records and are thus unsuitable for comparing world\nrecords to one other. The most versatile and widely-used statistic is a set of\nscoring tables compiled by the IAAF, which are updated and published every few\nyears. Unfortunately, these methods are not fully disclosed. In this paper, we\npropose a straight-forward, objective, model-based algorithm for assigning\nscores to athletic performances for the express purpose of comparing marks\nbetween different events. Specifically, the main score we propose is based on\nthe expected number of athletes who perform better than a given mark within a\ncalendar year. Computing this naturally interpretable statistic requires only a\nlist of the top performances in each event and is not overly dependent on a\nsmall number of marks, such as the world records. We found that this statistic\ncould predict the quality of future performances better than the IAAF scoring\ntables, and is thus better suited for comparing performances from different\nevents. In addition, the probabilistic model used to generate the performance\nscores allows for multiple interpretations which can be adapted for various\npurposes, such as calculating the expected top mark in a given event or\ncalculating the probability of a world record being broken within a certain\ntime period. In this paper, we give the details of the model and the scores, a\ncomparison with the IAAF scoring tables, and...\n", "versions": [{"version": "v1", "created": "Mon, 25 Aug 2014 20:47:06 GMT"}], "update_date": "2014-08-27", "authors_parsed": [["Godsey", "Brian", ""]]}, {"id": "1408.6434", "submitter": "Allan Axelrod", "authors": "Rakshit Allamaraju, Ben Reish, Allan Axelrod", "title": "Sensor Noise Rejection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.RO", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  An inaccessible control architecture caused an undesirable influence on a\nUAV. The encountered noise in the performance was modeled using stochastic\nmethods and a corrective term was implemented on an external controller. Our\nfindings suggest that the sonar noise problem is unconventional may warrant the\ndevelopment of a new methodology.\n", "versions": [{"version": "v1", "created": "Wed, 27 Aug 2014 15:17:04 GMT"}], "update_date": "2014-08-28", "authors_parsed": [["Allamaraju", "Rakshit", ""], ["Reish", "Ben", ""], ["Axelrod", "Allan", ""]]}, {"id": "1408.6451", "submitter": "Christoph Waldhauser", "authors": "Christoph Waldhauser", "title": "Public Spheres in Twitter- and Blogosphere. Evidence from the US", "comments": "15 pages, 3 figures, presented at the European Political Science\n  Conference EPSA 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The political requires a forum for its deliberation and Habermas has located\nit in the public spheres. Originally, mass media's role was one of a\nfacilitator of these debates. However, under the immense pressures of free\nmarket competition and mobile audiences, mass media prefers episodic over\nthematic news. On the opposite end of the spectrum, social media has been\nheralded as a new forum, a reincarnation of the ailing public spheres to\nfurther the deliberation of the political. But do the followers of political\nparties in social media endorse thematic or episodic content?\n  To answer this question, I look at the most recent 3,200 tweets that were\nbroadcast from the Republican and Democratic Twitter accounts. By employing\nLatent dirichlet allocation, I extract the prevailing topics of these tweets\nand linked websites. Generalized linear models are used to describe the\nrelationship between episodicity, thematicity and the endorsement counts of the\nposts analyzed.\n  I find that there is a stark contrast between the behavior of Democratic and\nRepublican followers. In general, there seems to be a slight preference for\nthematic messages. Interestingly, the distance to an election increases the\nodds of a message to be endorsed.\n", "versions": [{"version": "v1", "created": "Wed, 27 Aug 2014 16:07:57 GMT"}], "update_date": "2014-08-28", "authors_parsed": [["Waldhauser", "Christoph", ""]]}, {"id": "1408.6500", "submitter": "Igor Volobouev", "authors": "Igor Volobouev", "title": "On the Expectation-Maximization Unfolding with Smoothing", "comments": "Provided more details on the smoothing procedure. A minor bug\n  discovered in the software, so a number of figures and the table of results\n  were regenerated", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Error propagation formulae are derived for the expectation-maximization\niterative unfolding algorithm regularized by a smoothing step. The effective\nnumber of parameters in the fit to the observed data is defined for unfolding\nprocedures. Based upon this definition, the Akaike information criterion is\nproposed as a principle for choosing the smoothing parameters in an automatic,\ndata-dependent manner. The performance and the frequentist coverage of the\nresulting method are investigated using simulated samples. A number of issues\nof general relevance to all unfolding techniques are discussed, including\nirreducible bias, uncertainty increase due to a data-dependent choice of\nregularization strength, and presentation of results.\n", "versions": [{"version": "v1", "created": "Wed, 27 Aug 2014 19:28:54 GMT"}, {"version": "v2", "created": "Fri, 9 Jan 2015 23:13:19 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Volobouev", "Igor", ""]]}, {"id": "1408.6553", "submitter": "Daniele Ramazzotti", "authors": "Daniele Ramazzotti", "title": "An Observational Study: The Effect of Diuretics Administration on\n  Outcomes of Mortality and Mean Duration of I.C.U. Stay", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis conducts an observational study into whether diuretics should be\nadministered to ICU patients with sepsis when length of stay in the ICU and\n30-day post-hospital mortality are considered. The central contribution of the\nthesis is a stepwise, reusable software-based approach for examining the\noutcome of treatment vs no-treatment decisions with observational data. The\nthesis implements, demonstrates and draws findings via three steps: Step 1.\nForm a study group and prepare modeling variables. Step 2. Model the propensity\nof the study group with respect to the administration of diuretics with a\npropensity score function and create groups of patients balanced in this\npropensity. Step 3. Statistically model each outcome with study variables to\ndecide whether the administration of diuretics has a significant impact.\nAdditionally, the thesis presents a preliminary machine learning based method\nusing Genetic Programming to predict mortality and length of stay in ICU\noutcomes for the study group. The thesis finds, for its study group, in three\nof five propensity balanced quintiles, a statistically significant longer\nlength of stay when diuretics are administered. For a less sick subset of\npatients (SAPS ICU admission score < 17) the administration of diuretics has a\nsignificant negative effect on mortality.\n", "versions": [{"version": "v1", "created": "Mon, 11 Aug 2014 17:21:41 GMT"}], "update_date": "2014-08-29", "authors_parsed": [["Ramazzotti", "Daniele", ""]]}, {"id": "1408.6583", "submitter": "Chiu Man Ho", "authors": "Chiu Man Ho, Stephen D.H. Hsu", "title": "Determination of Nonlinear Genetic Architecture using Compressed Sensing", "comments": "20 pages, 8 figures. arXiv admin note: text overlap with\n  arXiv:1408.3421", "journal-ref": "GigaScience 4: 44 (2015)", "doi": null, "report-no": null, "categories": "q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a statistical method that can reconstruct nonlinear genetic\nmodels (i.e., including epistasis, or gene-gene interactions) from\nphenotype-genotype (GWAS) data. The computational and data resource\nrequirements are similar to those necessary for reconstruction of linear\ngenetic models (or identification of gene-trait associations), assuming a\ncondition of generalized sparsity, which limits the total number of gene-gene\ninteractions. An example of a sparse nonlinear model is one in which a typical\nlocus interacts with several or even many others, but only a small subset of\nall possible interactions exist. It seems plausible that most genetic\narchitectures fall in this category. Our method uses a generalization of\ncompressed sensing (L1-penalized regression) applied to nonlinear functions of\nthe sensing matrix. We give theoretical arguments suggesting that the method is\nnearly optimal in performance, and demonstrate its effectiveness on broad\nclasses of nonlinear genetic models using both real and simulated human\ngenomes.\n", "versions": [{"version": "v1", "created": "Wed, 27 Aug 2014 22:32:50 GMT"}, {"version": "v2", "created": "Sun, 19 Jul 2015 21:33:35 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Ho", "Chiu Man", ""], ["Hsu", "Stephen D. H.", ""]]}, {"id": "1408.6755", "submitter": "Tobias Kley", "authors": "Tobias Kley", "title": "Quantile-Based Spectral Analysis in an Object-Oriented Framework and a\n  Reference Implementation in R: The quantspec Package", "comments": "27 pages, 11 figures, R package available via CRAN\n  (http://cran.r-project.org/web/packages/quantspec) or GitHub\n  (https://github.com/tobiaskley/quantspec)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantile-based approaches to the spectral analysis of time series have\nrecently attracted a lot of attention. Despite a growing literature that\ncontains various estimation proposals, no systematic methods for computing the\nnew estimators are available to date. This paper contains two main\ncontributions. First, an extensible framework for quantile-based spectral\nanalysis of time series is developed and documented using object-oriented\nmodels. A comprehensive, open source, reference implementation of this\nframework, the R package quantspec, was recently contributed to CRAN by the\nauthor of this paper. The second contribution of the present paper is to\nprovide a detailed tutorial, with worked examples, to this R package. A reader\nwho is already familiar with quantile-based spectral analysis and whose primary\ninterest is not the design of the quantspec package, but how to use it, can\nread the tutorial and worked examples (Sections 3 and 4) independently.\n", "versions": [{"version": "v1", "created": "Thu, 28 Aug 2014 15:34:56 GMT"}], "update_date": "2014-08-29", "authors_parsed": [["Kley", "Tobias", ""]]}, {"id": "1408.6822", "submitter": "Dongjin Song", "authors": "Dongjin Song and David A. Meyer", "title": "A Model of Consistent Node Types in Signed Directed Social Networks", "comments": "To appear in the IEEE/ACM International Conference on Advances in\n  Social Network Analysis and Mining (ASONAM), 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Signed directed social networks, in which the relationships between users can\nbe either positive (indicating relations such as trust) or negative (indicating\nrelations such as distrust), are increasingly common. Thus the interplay\nbetween positive and negative relationships in such networks has become an\nimportant research topic. Most recent investigations focus upon edge sign\ninference using structural balance theory or social status theory. Neither of\nthese two theories, however, can explain an observed edge sign well when the\ntwo nodes connected by this edge do not share a common neighbor (e.g., common\nfriend). In this paper we develop a novel approach to handle this situation by\napplying a new model for node types. Initially, we analyze the local node\nstructure in a fully observed signed directed network, inferring underlying\nnode types. The sign of an edge between two nodes must be consistent with their\ntypes; this explains edge signs well even when there are no common neighbors.\nWe show, moreover, that our approach can be extended to incorporate directed\ntriads, when they exist, just as in models based upon structural balance or\nsocial status theory. We compute Bayesian node types within empirical studies\nbased upon partially observed Wikipedia, Slashdot, and Epinions networks in\nwhich the largest network (Epinions) has 119K nodes and 841K edges. Our\napproach yields better performance than state-of-the-art approaches for these\nthree signed directed networks.\n", "versions": [{"version": "v1", "created": "Thu, 28 Aug 2014 19:43:18 GMT"}], "update_date": "2014-08-29", "authors_parsed": [["Song", "Dongjin", ""], ["Meyer", "David A.", ""]]}, {"id": "1408.7025", "submitter": "Anne M. Presanis", "authors": "Anne M. Presanis, Richard G. Pebody, Paul J. Birrell, Brian D. M. Tom,\n  Helen K. Green, Hayley Durnall, Douglas Fleming, Daniela De Angelis", "title": "Synthesising evidence to estimate pandemic (2009) A/H1N1 influenza\n  severity in 2009-2011", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS775 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 4, 2378-2403", "doi": "10.1214/14-AOAS775", "report-no": "IMS-AOAS-AOAS775", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge of the severity of an influenza outbreak is crucial for informing\nand monitoring appropriate public health responses, both during and after an\nepidemic. However, case-fatality, case-intensive care admission and\ncase-hospitalisation risks are difficult to measure directly. Bayesian evidence\nsynthesis methods have previously been employed to combine fragmented,\nunder-ascertained and biased surveillance data coherently and consistently, to\nestimate case-severity risks in the first two waves of the 2009 A/H1N1\ninfluenza pandemic experienced in England. We present in detail the complex\nprobabilistic model underlying this evidence synthesis, and extend the analysis\nto also estimate severity in the third wave of the pandemic strain during the\n2010/2011 influenza season. We adapt the model to account for changes in the\nsurveillance data available over the three waves. We consider two approaches:\n(a) a two-stage approach using posterior distributions from the model for the\nfirst two waves to inform priors for the third wave model; and (b) a one-stage\napproach modelling all three waves simultaneously. Both approaches result in\nthe same key conclusions: (1) that the age-distribution of the case-severity\nrisks is \"u\"-shaped, with children and older adults having the highest\nseverity; (2) that the age-distribution of the infection attack rate changes\nover waves, school-age children being most affected in the first two waves and\nthe attack rate in adults over 25 increasing from the second to third waves;\nand (3) that when averaged over all age groups, case-severity appears to\nincrease over the three waves. The extent to which the final conclusion is\ndriven by the change in age-distribution of those infected over time is subject\nto discussion.\n", "versions": [{"version": "v1", "created": "Fri, 29 Aug 2014 14:12:09 GMT"}, {"version": "v2", "created": "Tue, 3 Feb 2015 08:41:06 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Presanis", "Anne M.", ""], ["Pebody", "Richard G.", ""], ["Birrell", "Paul J.", ""], ["Tom", "Brian D. M.", ""], ["Green", "Helen K.", ""], ["Durnall", "Hayley", ""], ["Fleming", "Douglas", ""], ["De Angelis", "Daniela", ""]]}]