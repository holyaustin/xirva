[{"id": "1203.0098", "submitter": "Ian L. Dryden", "authors": "Irina Czogiel, Ian L. Dryden, Christopher J. Brignell", "title": "Bayesian matching of unlabeled marked point sets using random fields,\n  with an application to molecular alignment", "comments": "Published in at http://dx.doi.org/10.1214/11-AOAS486 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 4, 2603-2629", "doi": "10.1214/11-AOAS486", "report-no": "IMS-AOAS-AOAS486", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical methodology is proposed for comparing unlabeled marked point\nsets, with an application to aligning steroid molecules in chemoinformatics.\nMethods from statistical shape analysis are combined with techniques for\npredicting random fields in spatial statistics in order to define a suitable\nmeasure of similarity between two marked point sets. Bayesian modeling of the\npredicted field overlap between pairs of point sets is proposed, and posterior\ninference of the alignment is carried out using Markov chain Monte Carlo\nsimulation. By representing the fields in reproducing kernel Hilbert spaces,\nthe degree of overlap can be computed without expensive numerical integration.\nSuperimposing entire fields rather than the configuration matrices of point\ncoordinates thereby avoids the problem that there is usually no clear\none-to-one correspondence between the points. In addition, mask parameters are\nintroduced in the model, so that partial matching of the marked point sets can\nbe carried out. We also propose an adaptation of the generalized Procrustes\nanalysis algorithm for the simultaneous alignment of multiple point sets. The\nmethodology is illustrated with a simulation study and then applied to a data\nset of 31 steroid molecules, where the relationship between shape and binding\nactivity to the corticosteroid binding globulin receptor is explored.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2012 06:26:42 GMT"}], "update_date": "2012-03-02", "authors_parsed": [["Czogiel", "Irina", ""], ["Dryden", "Ian L.", ""], ["Brignell", "Christopher J.", ""]]}, {"id": "1203.0133", "submitter": "Huiyan Sang", "authors": "Huiyan Sang, Mikyoung Jun, Jianhua Z. Huang", "title": "Covariance approximation for large multivariate spatial data sets with\n  an application to multiple climate model errors", "comments": "Published in at http://dx.doi.org/10.1214/11-AOAS478 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 4, 2519-2548", "doi": "10.1214/11-AOAS478", "report-no": "IMS-AOAS-AOAS478", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the cross-correlations across multiple climate model\nerrors. We build a Bayesian hierarchical model that accounts for the spatial\ndependence of individual models as well as cross-covariances across different\nclimate models. Our method allows for a nonseparable and nonstationary\ncross-covariance structure. We also present a covariance approximation approach\nto facilitate the computation in the modeling and analysis of very large\nmultivariate spatial data sets. The covariance approximation consists of two\nparts: a reduced-rank part to capture the large-scale spatial dependence, and a\nsparse covariance matrix to correct the small-scale dependence error induced by\nthe reduced rank approximation. We pay special attention to the case that the\nsecond part of the approximation has a block-diagonal structure. Simulation\nresults of model fitting and prediction show substantial improvement of the\nproposed approximation over the predictive process approximation and the\nindependent blocks analysis. We then apply our computational approach to the\njoint statistical modeling of multiple climate model errors.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2012 10:23:07 GMT"}], "update_date": "2012-03-02", "authors_parsed": [["Sang", "Huiyan", ""], ["Jun", "Mikyoung", ""], ["Huang", "Jianhua Z.", ""]]}, {"id": "1203.0191", "submitter": "Karen Kafadar", "authors": "Karen Kafadar", "title": "Special section on modern multivariate analysis", "comments": "Published in at http://dx.doi.org/10.1214/11-AOAS529 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 4, 2265-2265", "doi": "10.1214/11-AOAS529", "report-no": "IMS-AOAS-AOAS529", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A critically challenging problem facing statisticians is the identification\nof a suitable framework which consolidates data of various types, from\ndifferent sources, and across different time frames or scales (many of which\ncan be missing), and from which appropriate analysis and subsequent inference\ncan proceed.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2012 14:07:14 GMT"}], "update_date": "2012-03-02", "authors_parsed": [["Kafadar", "Karen", ""]]}, {"id": "1203.0335", "submitter": "John Clare", "authors": "John F Clare, Annette Koo, Robert B Davies", "title": "Evaluation of Measurement Comparisons Using Generalised Least Squares:\n  the Role of Participants' Estimates of Systematic Error", "comments": "11 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the evaluation of laboratory practice through the comparison of\nmeasurements made by participating metrology laboratories when the measurement\nprocedures are considered to have both fixed effects (the residual error due to\nunrecognised sources of error) and random effects (drawn from a distribution of\nknown variance after correction for all known systematic errors). We show that,\nwhen estimating the participant fixed effects, the random effects described can\nbe ignored. We also derive the adjustment to the variance estimates of the\nparticipant fixed effects due to these random effects.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2012 22:37:31 GMT"}, {"version": "v2", "created": "Tue, 8 May 2012 04:47:07 GMT"}], "update_date": "2012-05-09", "authors_parsed": [["Clare", "John F", ""], ["Koo", "Annette", ""], ["Davies", "Robert B", ""]]}, {"id": "1203.0489", "submitter": "Thomas Thorne", "authors": "Thomas Thorne and Michael P.H Stumpf", "title": "Inference of Temporally Varying Bayesian Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.MN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When analysing gene expression time series data an often overlooked but\ncrucial aspect of the model is that the regulatory network structure may change\nover time. Whilst some approaches have addressed this problem previously in the\nliterature, many are not well suited to the sequential nature of the data. Here\nwe present a method that allows us to infer regulatory network structures that\nmay vary between time points, utilising a set of hidden states that describe\nthe network structure at a given time point. To model the distribution of the\nhidden states we have applied the Hierarchical Dirichlet Process Hideen Markov\nModel, a nonparametric extension of the traditional Hidden Markov Model, that\ndoes not require us to fix the number of hidden states in advance. We apply our\nmethod to exisiting microarray expression data as well as demonstrating is\nefficacy on simulated test data.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2012 15:21:02 GMT"}], "update_date": "2012-03-05", "authors_parsed": [["Thorne", "Thomas", ""], ["Stumpf", "Michael P. H", ""]]}, {"id": "1203.0541", "submitter": "Mrinal Nandi Mr", "authors": "Mrinal Nandi, Amiya Nayak, Bimal Roy, Santanu Sarkar", "title": "Hypothesis Testing and Decision Theoretic Approach for Fault Detection\n  in Wireless Sensor Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensor networks aim at monitoring their surroundings for event detection and\nobject tracking. But due to failure or death of sensors, false signal can be\ntransmitted. In this paper, we consider the problem of fault detection in\nwireless sensor network (WSN), in particular, addressing both the noise-related\nmeasurement error and sensor fault simultaneously in fault detection. We assume\nthat the sensors are placed at the center of a square (or hexagonal) cell in\nregion of interest (ROI) and, if the event occurs, it occurs at a particular\ncell of the ROI. We propose fault detection schemes that take into account\nerror probabilities into the optimal event detection process. We develop the\nschemes under the consideration of Neyman-Pearson test and Bayes test.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2012 18:17:08 GMT"}], "update_date": "2012-03-05", "authors_parsed": [["Nandi", "Mrinal", ""], ["Nayak", "Amiya", ""], ["Roy", "Bimal", ""], ["Sarkar", "Santanu", ""]]}, {"id": "1203.0642", "submitter": "Florentina Pintea", "authors": "Ripunjai K. Shukla, M. Trivedi, Manoj Kumar", "title": "On the proficient use of GEV distribution: a case study of subtropical\n  monsoon region in India", "comments": "12 pages", "journal-ref": "Ann. Univ. Tibiscus Comp. Sci. Series VIII / 1 (2010), 81-92", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper deals with the probabilistic estimates of extreme maximum rainfall\n(Annual basis) in the Ranchi, Jharkhand (India). Extreme Value Distribution\nfamily models are tried to capture the uncertainty of data and finally\nGeneralized Extreme Value (GEV) distribution model is found as the best fitted\ndistribution model. The GEV model satisfied the selection criteria\n[Anderson-Darling test (A-D test or Goodness of fit test) and Normality test\n(Q-Q plot)], which are adopted under the present study. The return levels are\nestimated for 5, 10, 50, 100 and 200 years which are consistently increasing\nfor long run in future.\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2012 11:08:56 GMT"}], "update_date": "2012-03-06", "authors_parsed": [["Shukla", "Ripunjai K.", ""], ["Trivedi", "M.", ""], ["Kumar", "Manoj", ""]]}, {"id": "1203.0900", "submitter": "Laszlo Martinek", "authors": "Mikl\\'os Arat\\'o and L\\'aszl\\'o Martinek", "title": "Estimation of Claim Numbers in Automobile Insurance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of bonus-malus systems in compulsory liability automobile insurance\nis a worldwide applied method for premium pricing. If certain assumptions hold,\nlike the conditional Poisson distribution of the policyholders claim number,\nthen an interesting task is to evaluate the so called claims frequency of the\nindividuals. Here we introduce 3 techniques, two is based on the bonus-malus\nclass, and the third based on claims history. The article is devoted to choose\nthe method, which fits to the frequency parameters the best for certain input\nparameters. For measuring the goodness-of-fit we will use scores, similar to\nbetter known divergence measures. The detailed method is also suitable to\ncompare bonus-malus systems in the sense that how much information they contain\nabout drivers.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2012 13:06:44 GMT"}], "update_date": "2012-03-06", "authors_parsed": [["Arat\u00f3", "Mikl\u00f3s", ""], ["Martinek", "L\u00e1szl\u00f3", ""]]}, {"id": "1203.0937", "submitter": "Alexander Sch\\\"onhuth", "authors": "Tobias Marschall, Ivan Costa, Stefan Canzar, Markus Bauer, Gunnar\n  Klau, Alexander Schliep, Alexander Sch\\\"onhuth", "title": "CLEVER: Clique-Enumerating Variant Finder", "comments": "30 pages, 8 figures", "journal-ref": "Bioinformatics, 28(22), 2875-2882, 2012", "doi": null, "report-no": null, "categories": "q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Next-generation sequencing techniques have facilitated a large scale analysis\nof human genetic variation. Despite the advances in sequencing speeds, the\ncomputational discovery of structural variants is not yet standard. It is\nlikely that many variants have remained undiscovered in most sequenced\nindividuals. Here we present a novel internal segment size based approach,\nwhich organizes all, including also concordant reads into a read alignment\ngraph where max-cliques represent maximal contradiction-free groups of\nalignments. A specifically engineered algorithm then enumerates all max-cliques\nand statistically evaluates them for their potential to reflect insertions or\ndeletions (indels). For the first time in the literature, we compare a large\nrange of state-of-the-art approaches using simulated Illumina reads from a\nfully annotated genome and present various relevant performance statistics. We\nachieve superior performance rates in particular on indels of sizes 20--100,\nwhich have been exposed as a current major challenge in the SV discovery\nliterature and where prior insert size based approaches have limitations. In\nthat size range, we outperform even split read aligners. We achieve good\nresults also on real data where we make a substantial amount of correct\npredictions as the only tool, which complement the predictions of split-read\naligners. CLEVER is open source (GPL) and available from\nhttp://clever-sv.googlecode.com.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2012 14:51:41 GMT"}, {"version": "v2", "created": "Mon, 16 Jul 2012 00:18:50 GMT"}], "update_date": "2015-01-14", "authors_parsed": [["Marschall", "Tobias", ""], ["Costa", "Ivan", ""], ["Canzar", "Stefan", ""], ["Bauer", "Markus", ""], ["Klau", "Gunnar", ""], ["Schliep", "Alexander", ""], ["Sch\u00f6nhuth", "Alexander", ""]]}, {"id": "1203.1076", "submitter": "Shinsuke Koyama", "authors": "Shinsuke Koyama", "title": "On the Relation between Encoding and Decoding of Neuronal Spikes", "comments": "21 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural coding is a field of study that concerns how sensory information is\nrepresented in the brain by networks of neurons. The link between external\nstimulus and neural response can be studied from two parallel points of view.\nThe first, neural encoding refers to the mapping from stimulus to response, and\nprimarily focuses on understanding how neurons respond to a wide variety of\nstimuli, and on constructing models that accurately describe the\nstimulus-response relationship. Neural decoding, on the other hand, refers to\nthe reverse mapping, from response to stimulus, where the challenge is to\nreconstruct a stimulus from the spikes it evokes. Since neuronal response is\nstochastic, a one-to-one mapping of stimuli into neural responses does not\nexist, causing a mismatch between the two viewpoints of neural coding. Here, we\nuse these two perspectives to investigate the question of what rate coding is,\nin the simple setting of a single stationary stimulus parameter and a single\nstationary spike train represented by a renewal process. We show that when rate\ncodes are defined in terms of encoding, i.e., the stimulus parameter is mapped\nonto the mean firing rate, the rate decoder given by spike counts or the sample\nmean, does not always efficiently decode the rate codes, but can improve\nefficiency in reading certain rate codes, when correlations within a spike\ntrain are taken into account.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2012 00:25:23 GMT"}], "update_date": "2012-03-07", "authors_parsed": [["Koyama", "Shinsuke", ""]]}, {"id": "1203.1349", "submitter": "Alessandro Chessa", "authors": "Guido Caldarelli, Alessandro Chessa, Irene Crimaldi, Fabio Pammolli", "title": "The Evolution of Complex Networks: A New Framework", "comments": "4 pages and 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cond-mat.stat-mech cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new framework for the analysis of the dynamics of networks,\nbased on randomly reinforced urn (RRU) processes, in which the weight of the\nedges is determined by a reinforcement mechanism. We rigorously explain the\nempirical evidence that in many real networks there is a subset of \"dominant\nedges\" that control a major share of the total weight of the network.\nFurthermore, we introduce a new statistical procedure to study the evolution of\nnetworks over time, assessing if a given instance of the nework is taken at its\nsteady state or not. Our results are quite general, since they are not based on\na particular probability distribution or functional form of the weights. We\ntest our model in the context of the International Trade Network, showing the\nexistence of a core of dominant links and determining its size.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2012 22:55:13 GMT"}], "update_date": "2012-03-08", "authors_parsed": [["Caldarelli", "Guido", ""], ["Chessa", "Alessandro", ""], ["Crimaldi", "Irene", ""], ["Pammolli", "Fabio", ""]]}, {"id": "1203.1365", "submitter": "Matthew Johnson", "authors": "Matthew J. Johnson and Alan S. Willsky", "title": "Bayesian Nonparametric Hidden Semi-Markov Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is much interest in the Hierarchical Dirichlet Process Hidden Markov\nModel (HDP-HMM) as a natural Bayesian nonparametric extension of the ubiquitous\nHidden Markov Model for learning from sequential and time-series data. However,\nin many settings the HDP-HMM's strict Markovian constraints are undesirable,\nparticularly if we wish to learn or encode non-geometric state durations. We\ncan extend the HDP-HMM to capture such structure by drawing upon\nexplicit-duration semi-Markovianity, which has been developed mainly in the\nparametric frequentist setting, to allow construction of highly interpretable\nmodels that admit natural prior information on state durations.\n  In this paper we introduce the explicit-duration Hierarchical Dirichlet\nProcess Hidden semi-Markov Model (HDP-HSMM) and develop sampling algorithms for\nefficient posterior inference. The methods we introduce also provide new\nmethods for sampling inference in the finite Bayesian HSMM. Our modular Gibbs\nsampling methods can be embedded in samplers for larger hierarchical Bayesian\nmodels, adding semi-Markov chain modeling as another tool in the Bayesian\ninference toolbox. We demonstrate the utility of the HDP-HSMM and our inference\nmethods on both synthetic and real experiments.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2012 01:53:14 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2012 22:21:25 GMT"}], "update_date": "2012-09-11", "authors_parsed": [["Johnson", "Matthew J.", ""], ["Willsky", "Alan S.", ""]]}, {"id": "1203.1382", "submitter": "Martin Stumpe Martin Stumpe", "authors": "Martin C. Stumpe, Jeffrey C. Smith, Jeffrey E. Van Cleve, Joseph D.\n  Twicken, Thomas S. Barclay, Michael N. Fanelli, Forrest R. Girouard, Jon M.\n  Jenkins, Jeffery J. Kolodziejczak, Sean D. McCauliff, Robert L. Morris", "title": "Kepler Presearch Data Conditioning I - Architecture and Algorithms for\n  Error Correction in Kepler Light Curves", "comments": "Submitted to PASP. Also see companion paper \"Kepler Presearch Data\n  Conditioning II - A Bayesian Approach to Systematic Error Correction\" by Jeff\n  C. Smith et al", "journal-ref": null, "doi": "10.1086/667698", "report-no": null, "categories": "astro-ph.IM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kepler provides light curves of 156,000 stars with unprecedented precision.\nHowever, the raw data as they come from the spacecraft contain significant\nsystematic and stochastic errors. These errors, which include discontinuities,\nsystematic trends, and outliers, obscure the astrophysical signals in the light\ncurves. To correct these errors is the task of the Presearch Data Conditioning\n(PDC) module of the Kepler data analysis pipeline. The original version of PDC\nin Kepler did not meet the extremely high performance requirements for the\ndetection of miniscule planet transits or highly accurate analysis of stellar\nactivity and rotation. One particular deficiency was that astrophysical\nfeatures were often removed as a side-effect to removal of errors. In this\npaper we introduce the completely new and significantly improved version of PDC\nwhich was implemented in Kepler SOC 8.0. This new PDC version, which utilizes a\nBayesian approach for removal of systematics, reliably corrects errors in the\nlight curves while at the same time preserving planet transits and other\nastrophysically interesting signals. We describe the architecture and the\nalgorithms of this new PDC module, show typical errors encountered in Kepler\ndata, and illustrate the corrections using real light curve examples.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2012 05:35:25 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Stumpe", "Martin C.", ""], ["Smith", "Jeffrey C.", ""], ["Van Cleve", "Jeffrey E.", ""], ["Twicken", "Joseph D.", ""], ["Barclay", "Thomas S.", ""], ["Fanelli", "Michael N.", ""], ["Girouard", "Forrest R.", ""], ["Jenkins", "Jon M.", ""], ["Kolodziejczak", "Jeffery J.", ""], ["McCauliff", "Sean D.", ""], ["Morris", "Robert L.", ""]]}, {"id": "1203.1383", "submitter": "Jeffrey Smith", "authors": "Jeffrey C. Smith, Martin C. Stumpe, Jeffrey E. Van Cleve, Jon M.\n  Jenkins, Thomas S. Barclay, Michael N. Fanelli, Forrest R. Girouard, Jeffery\n  J. Kolodziejczak, Sean D. McCauliff, Robert L. Morris, Joseph D. Twicken", "title": "Kepler Presearch Data Conditioning II - A Bayesian Approach to\n  Systematic Error Correction", "comments": "43 pages, 21 figures, Submitted for publication in PASP. Also see\n  companion paper \"Kepler Presearch Data Conditioning I - Architecture and\n  Algorithms for Error Correction in Kepler Light Curves\" by Martin C. Stumpe,\n  et al", "journal-ref": null, "doi": "10.1086/667697", "report-no": null, "categories": "astro-ph.IM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the unprecedented photometric precision of the Kepler Spacecraft,\nsignificant systematic and stochastic errors on transit signal levels are\nobservable in the Kepler photometric data. These errors, which include\ndiscontinuities, outliers, systematic trends and other instrumental signatures,\nobscure astrophysical signals. The Presearch Data Conditioning (PDC) module of\nthe Kepler data analysis pipeline tries to remove these errors while preserving\nplanet transits and other astrophysically interesting signals. The completely\nnew noise and stellar variability regime observed in Kepler data poses a\nsignificant problem to standard cotrending methods such as SYSREM and TFA.\nVariable stars are often of particular astrophysical interest so the\npreservation of their signals is of significant importance to the astrophysical\ncommunity. We present a Bayesian Maximum A Posteriori (MAP) approach where a\nsubset of highly correlated and quiet stars is used to generate a cotrending\nbasis vector set which is in turn used to establish a range of \"reasonable\"\nrobust fit parameters. These robust fit parameters are then used to generate a\nBayesian Prior and a Bayesian Posterior Probability Distribution Function (PDF)\nwhich when maximized finds the best fit that simultaneously removes systematic\neffects while reducing the signal distortion and noise injection which commonly\nafflicts simple least-squares (LS) fitting. A numerical and empirical approach\nis taken where the Bayesian Prior PDFs are generated from fits to the light\ncurve distributions themselves.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2012 05:35:37 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Smith", "Jeffrey C.", ""], ["Stumpe", "Martin C.", ""], ["Van Cleve", "Jeffrey E.", ""], ["Jenkins", "Jon M.", ""], ["Barclay", "Thomas S.", ""], ["Fanelli", "Michael N.", ""], ["Girouard", "Forrest R.", ""], ["Kolodziejczak", "Jeffery J.", ""], ["McCauliff", "Sean D.", ""], ["Morris", "Robert L.", ""], ["Twicken", "Joseph D.", ""]]}, {"id": "1203.1421", "submitter": "Harish  Taneja", "authors": "Richa Thapliyal and H. C. Taneja", "title": "A note on characterization based on past entropy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ebrahimi (1996) has shown that the measure of residual entropy characterizes\nthe distribution function uniquely. In this communication we study an analogous\nresult for past entropy.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2012 09:54:39 GMT"}], "update_date": "2012-03-08", "authors_parsed": [["Thapliyal", "Richa", ""], ["Taneja", "H. C.", ""]]}, {"id": "1203.1504", "submitter": "Richard D. Gill", "authors": "Richard D. Gill", "title": "Does Geometric Algebra provide a loophole to Bell's Theorem? (with\n  corrections)", "comments": "11th version: minor but necessary correction to math requested by\n  myself and some corrections to tone of text requested by editor of Entropy\n  after evaluation of a complaint made by a third person", "journal-ref": "Entropy 2020, 22(1), 61 (21 pp.)", "doi": "10.3390/e22010061", "report-no": null, "categories": "quant-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2007, and in a series of later papers, Joy Christian claimed to refute\nBell's theorem, presenting an alleged local realistic model of the singlet\ncorrelations using techniques from Geometric Algebra (GA). Several authors\npublished papers refuting his claims, and Christian's ideas did not gain\nacceptance. However, he recently succeeded in publishing yet more ambitious and\ncomplex versions of his theory in fairly mainstream journals. How could this\nbe? The mathematics and logic of Bell's theorem is simple and transparent and\nhas been intensely studied and debated for over 50 years. Christian claims to\nhave a mathematical counterexample to a purely mathematical theorem. Each new\nversion of Christian's model used new devices to circumvent Bell's theorem or\ndepended on a new way to misunderstand Bell's work. These devices and\nmisinterpretations are in common use by other Bell critics, so it useful to\nidentify and name them. I hope that this paper can serve as a useful resource\nto those who need to evaluate new \"disproofs of Bell's theorem\". Christian's\nfundamental idea is simple and quite original: he gives a probabilistic\ninterpretation of the fundamental GA equation a.b = (ab + ba)/2. After that,\nambiguous notation and technical complexity allow sign errors to be hidden from\nsight, and new mathematical errors can be introduced.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2012 15:39:31 GMT"}, {"version": "v10", "created": "Sat, 27 Mar 2021 18:13:55 GMT"}, {"version": "v11", "created": "Tue, 30 Mar 2021 06:54:14 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2012 12:15:46 GMT"}, {"version": "v3", "created": "Sun, 11 Mar 2012 18:25:07 GMT"}, {"version": "v4", "created": "Mon, 14 May 2012 14:03:22 GMT"}, {"version": "v5", "created": "Mon, 28 Oct 2019 13:56:28 GMT"}, {"version": "v6", "created": "Wed, 30 Oct 2019 12:05:36 GMT"}, {"version": "v7", "created": "Fri, 27 Dec 2019 16:31:56 GMT"}, {"version": "v8", "created": "Wed, 29 Jan 2020 18:05:42 GMT"}, {"version": "v9", "created": "Tue, 18 Feb 2020 15:55:56 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Gill", "Richard D.", ""]]}, {"id": "1203.2062", "submitter": "Bruno Sudret", "authors": "Bruno Sudret", "title": "Meta-models for structural reliability and uncertainty quantification", "comments": "Keynote lecture Fifth Asian-Pacific Symposium on Structural\n  Reliability and its Applications (5th APSSRA) May 2012, Singapore", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A meta-model (or a surrogate model) is the modern name for what was\ntraditionally called a response surface. It is intended to mimic the behaviour\nof a computational model M (e.g. a finite element model in mechanics) while\nbeing inexpensive to evaluate, in contrast to the original model which may take\nhours or even days of computer processing time. In this paper various types of\nmeta-models that have been used in the last decade in the context of structural\nreliability are reviewed. More specifically classical polynomial response\nsurfaces, polynomial chaos expansions and kriging are addressed. It is shown\nhow the need for error estimates and adaptivity in their construction has\nbrought this type of approaches to a high level of efficiency. A new technique\nthat solves the problem of the potential biasedness in the estimation of a\nprobability of failure through the use of meta-models is finally presented.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2012 12:49:35 GMT"}], "update_date": "2012-03-12", "authors_parsed": [["Sudret", "Bruno", ""]]}, {"id": "1203.2287", "submitter": "Dirk Tasche", "authors": "Dirk Tasche", "title": "Bounds for rating override rates", "comments": "23 pages, 3 figures, 3 tables", "journal-ref": "Journal of Credit Risk 8(4), 3-29, 2012", "doi": null, "report-no": null, "categories": "q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Overrides of credit ratings are important correctives of ratings that are\ndetermined by statistical rating models. Financial institutions and banking\nregulators agree on this because on the one hand errors with ratings of\ncorporates or banks can have fatal consequences for the lending institutions\nand on the other hand errors by statistical methods can be minimised but not\ncompletely avoided. Nonetheless, rating overrides can be misused in order to\nconceal the real riskiness of borrowers or even entire portfolios. That is why\nrating overrides usually are strictly governed and carefully recorded. It is\nnot clear, however, which frequency of overrides is appropriate for a given\nrating model within a predefined time period. This paper argues that there is a\nnatural error rate associated with a statistical rating model that may be used\nto inform assessment of whether or not an observed override rate is adequate.\nThe natural error rate is closely related to the rating model's discriminatory\npower and can readily be calculated.\n", "versions": [{"version": "v1", "created": "Sat, 10 Mar 2012 21:50:11 GMT"}, {"version": "v2", "created": "Sat, 14 Apr 2012 09:58:20 GMT"}, {"version": "v3", "created": "Wed, 13 Jun 2012 19:01:14 GMT"}, {"version": "v4", "created": "Fri, 31 Aug 2012 08:32:06 GMT"}], "update_date": "2012-12-24", "authors_parsed": [["Tasche", "Dirk", ""]]}, {"id": "1203.2313", "submitter": "Webb Sprague", "authors": "W. Webb Sprague", "title": "Automatic parametrization of age/ sex Leslie matrices for human\n  populations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a technique for parameterizing Leslie transition\nmatrices from simple age and sex population counts, using an implementation of\n\"Wood's Method\" [wood]; these matrices can forecast population by age and sex\n(the \"cohort component\" method) using simple matrix multiplication and a\nstarting population. Our approach improves on previous methods for creating\nLeslie matrices in two respects: it eliminates the need to calculate input\ndemographic rates from \"raw\" data, and our new format for the Leslie matrix\nmore elegantly reveals the population's demographic components of change\n(fertility, mortality, and migration). The paper is organized around three main\nthemes. First, we describe the underlying algorithm, \"Wood's Method,\" which\nuses quadratic optimization to fit a transition matrix to age and sex\npopulation counts. Second, we use demographic theory to create constraint sets\nthat make the algorithm useable for human populations. Finally, we use the\nmethod to forecast 3,120 US counties and show that it holds promise for\nautomating cohort-component forecasts. This paper describes the first published\nsuccessful application of Wood's method to human populations; it also points to\nmore general promise of constrained optimization techniques in demographic\nmodeling.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2012 04:59:36 GMT"}, {"version": "v2", "created": "Thu, 15 Mar 2012 04:20:28 GMT"}, {"version": "v3", "created": "Thu, 29 Mar 2012 03:15:00 GMT"}, {"version": "v4", "created": "Sun, 22 Apr 2012 05:39:17 GMT"}], "update_date": "2012-04-24", "authors_parsed": [["Sprague", "W. Webb", ""]]}, {"id": "1203.2511", "submitter": "Victor Seal", "authors": "Victor Seal, Arnab Raha, Shovan Maity, Souvik Kr Mitra, Amitava\n  Mukherjee and Mrinal Kanti Naskar", "title": "A Simple Flood Forecasting Scheme Using Wireless Sensor Networks", "comments": "16 pages, 4 figures, published in International Journal Of Ad-Hoc,\n  Sensor And Ubiquitous Computing, February 2012; V. seal et al, 'A Simple\n  Flood Forecasting Scheme Using Wireless Sensor Networks', IJASUC, Feb.2012", "journal-ref": null, "doi": "10.5121/ijasuc.2012.3105", "report-no": null, "categories": "cs.LG cs.CE cs.NI cs.SY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a forecasting model designed using WSNs (Wireless Sensor\nNetworks) to predict flood in rivers using simple and fast calculations to\nprovide real-time results and save the lives of people who may be affected by\nthe flood. Our prediction model uses multiple variable robust linear regression\nwhich is easy to understand and simple and cost effective in implementation, is\nspeed efficient, but has low resource utilization and yet provides real time\npredictions with reliable accuracy, thus having features which are desirable in\nany real world algorithm. Our prediction model is independent of the number of\nparameters, i.e. any number of parameters may be added or removed based on the\non-site requirements. When the water level rises, we represent it using a\npolynomial whose nature is used to determine if the water level may exceed the\nflood line in the near future. We compare our work with a contemporary\nalgorithm to demonstrate our improvements over it. Then we present our\nsimulation results for the predicted water level compared to the actual water\nlevel.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2012 18:08:34 GMT"}], "update_date": "2012-03-13", "authors_parsed": [["Seal", "Victor", ""], ["Raha", "Arnab", ""], ["Maity", "Shovan", ""], ["Mitra", "Souvik Kr", ""], ["Mukherjee", "Amitava", ""], ["Naskar", "Mrinal Kanti", ""]]}, {"id": "1203.2564", "submitter": "Lorenzo Hern\\'andez", "authors": "Lorenzo Hern\\'andez, Jorge Tejero, Alberto Su\\'arez and Santiago\n  Carrillo-Men\\'endez", "title": "Percentiles of sums of heavy-tailed random variables: Beyond the\n  single-loss approximation", "comments": "18 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.RM q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A perturbative approach is used to derive approximations of arbitrary order\nto estimate high percentiles of sums of positive independent random variables\nthat exhibit heavy tails. Closed-form expressions for the successive\napproximations are obtained both when the number of terms in the sum is\ndeterministic and when it is random. The zeroth order approximation is the\npercentile of the maximum term in the sum. Higher orders in the perturbative\nseries involve the right-truncated moments of the individual random variables\nthat appear in the sum. These censored moments are always finite. As a result,\nand in contrast to previous approximations proposed in the literature, the\nperturbative series has the same form regardless of whether these random\nvariables have a finite mean or not. The accuracy of the approximations is\nillustrated for a variety of distributions and a wide range of parameters. The\nquality of the estimate improves as more terms are included in the perturbative\nseries, specially for higher percentiles and heavier tails.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2012 17:31:50 GMT"}, {"version": "v2", "created": "Wed, 31 Oct 2012 08:16:35 GMT"}, {"version": "v3", "created": "Tue, 18 Dec 2012 14:23:33 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Hern\u00e1ndez", "Lorenzo", ""], ["Tejero", "Jorge", ""], ["Su\u00e1rez", "Alberto", ""], ["Carrillo-Men\u00e9ndez", "Santiago", ""]]}, {"id": "1203.2835", "submitter": "Francesco Montorsi", "authors": "Francesco Montorsi, Fabrizio Pancaldi, Giorgio M. Vitetta", "title": "Statistical Characterization and Mitigation of NLOS Errors in UWB\n  Localization Systems", "comments": null, "journal-ref": "Montorsi, F.; Pancaldi, F.; Vitetta, G.; \"Statistical\n  Characterization and Mitigation of NLOS Errors in UWB Localization Systems,\"\n  Ultra-Wideband (ICUWB), 2011 IEEE International Conference on, pp. 86-90,\n  14-16 Sept. 2011", "doi": "10.1109/ICUWB.2011.6058928", "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper some new experimental results about the statistical\ncharacterization of the non-line-of-sight (NLOS) bias affecting time-of-arrival\n(TOA) estimation in ultrawideband (UWB) wireless localization systems are\nillustrated. Then, these results are exploited to assess the performance of\nvarious maximum-likelihood (ML) based algorithms for joint TOA localization and\nNLOS bias mitigation. Our numerical results evidence that the accuracy of all\nthe considered algorithms is appreciably influenced by the LOS/NLOS conditions\nof the propagation environment.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2012 15:30:16 GMT"}], "update_date": "2012-03-14", "authors_parsed": [["Montorsi", "Francesco", ""], ["Pancaldi", "Fabrizio", ""], ["Vitetta", "Giorgio M.", ""]]}, {"id": "1203.2879", "submitter": "Eric Laber", "authors": "Eric B. Laber, Kerby Shedden, Yang Yang", "title": "An imputation method for estimating the learning curve in classification\n  problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The learning curve expresses the error rate of a predictive modeling\nprocedure as a function of the sample size of the training dataset. It\ntypically is a decreasing, convex function with a positive limiting value. An\nestimate of the learning curve can be used to assess whether a modeling\nprocedure should be expected to become substantially more accurate if\nadditional training data become available. This article proposes a new\nprocedure for estimating learning curves using imputation. We focus on\nclassification, although the idea is applicable to other predictive modeling\nsettings. Simulation studies indicate that the learning curve can be estimated\nwith useful accuracy for a roughly four-fold increase in the size of the\ntraining set relative to the available data, and that the proposed imputation\napproach outperforms an alternative estimation approach based on parameterizing\nthe learning curve. We illustrate the method with an application that predicts\nthe risk of disease progression for people with chronic lymphocytic leukemia.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2012 18:02:05 GMT"}], "update_date": "2012-03-14", "authors_parsed": [["Laber", "Eric B.", ""], ["Shedden", "Kerby", ""], ["Yang", "Yang", ""]]}, {"id": "1203.2890", "submitter": "Francesco Montorsi", "authors": "Francesco Montorsi, Fabrizio Pancaldi, Giorgio M. Vitetta", "title": "Statistical Characterization and Mitigation of NLOS Bias in UWB\n  Localization Systems", "comments": null, "journal-ref": "Montorsi, F.; Pancaldi, F.; Vitetta, G.; \"Statistical\n  Characterization and Mitigation of NLOS Bias in UWB Localization Systems,\"\n  Advances in Electronics and Telecommunications, pp. 11-17, Issue no 4, ISSN\n  2081-8580", "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Propagation in non-line-of-sight (NLOS) conditions is one of the major\nimpairments in ultrawideband (UWB) wireless localization systems based on\ntime-of-arrival (TOA) measurements. In this paper the problem of the joint\nstatistical characterization of the NLOS bias and of the most representative\nfeatures of LOS/NLOS UWB waveforms is investigated. In addition, the\nperformance of various maximum-likelihood (ML) estimators for joint\nlocalization and NLOS bias mitigation is assessed. Our numerical results\nevidence that the accuracy of all the considered estimators is appreciably\ninfluenced by the LOS/NLOS conditions of the propagation environment and that a\nstatistical knowledge of multiple signal features can be exploited to mitigate\nthe NLOS bias, reducing the overall localization error.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2012 19:01:20 GMT"}], "update_date": "2012-03-14", "authors_parsed": [["Montorsi", "Francesco", ""], ["Pancaldi", "Fabrizio", ""], ["Vitetta", "Giorgio M.", ""]]}, {"id": "1203.3055", "submitter": "David Garcia Sanchez", "authors": "David Garcia Sanchez, Bruno Lacarri\\`ere, Marjorie Musy, Bernard\n  Bourges", "title": "Application of sensitivity analysis in building energy simulations:\n  combining first and second order elementary effects Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensitivity analysis plays an important role in the understanding of complex\nmodels. It helps to identify influence of input parameters in relation to the\noutputs. It can be also a tool to understand the behavior of the model and then\ncan help in its development stage. This study aims to analyze and illustrate\nthe potential usefulness of combining first and second-order sensitivity\nanalysis, applied to a building energy model (ESP-r). Through the example of a\ncollective building, a sensitivity analysis is performed using the method of\nelementary effects (also known as Morris method), including an analysis of\ninteractions between the input parameters (second order analysis). Importance\nof higher-order analysis to better support the results of first order analysis,\nhighlighted especially in such complex model. Several aspects are tackled to\nimplement efficiently the multi-order sensitivity analysis: interval size of\nthe variables, management of non-linearity, usefulness of various outputs.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2012 11:43:21 GMT"}, {"version": "v2", "created": "Fri, 28 Dec 2012 10:54:19 GMT"}], "update_date": "2013-01-01", "authors_parsed": [["Sanchez", "David Garcia", ""], ["Lacarri\u00e8re", "Bruno", ""], ["Musy", "Marjorie", ""], ["Bourges", "Bernard", ""]]}, {"id": "1203.3082", "submitter": "Korbinian Strimmer", "authors": "Verena Zuber and A. Pedro Duarte Silva and Korbinian Strimmer", "title": "A novel algorithm for simultaneous SNP selection in high-dimensional\n  genome-wide association studies", "comments": "15 pages, 2 figures, 4 tables", "journal-ref": "BMC Bioinformatics 2012, Vol. 13, 284", "doi": "10.1186/1471-2105-13-284", "report-no": null, "categories": "stat.AP q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Identification of causal SNPs in most genome wide association\nstudies relies on approaches that consider each SNP individually. However,\nthere is a strong correlation structure among SNPs that need to be taken into\naccount. Hence, increasingly modern computationally expensive regression\nmethods are employed for SNP selection that consider all markers simultaneously\nand thus incorporate dependencies among SNPs.\n  Results: We develop a novel multivariate algorithm for large scale SNP\nselection using CAR score regression, a promising new approach for prioritizing\nbiomarkers. Specifically, we propose a computationally efficient procedure for\nshrinkage estimation of CAR scores from high-dimensional data. Subsequently, we\nconduct a comprehensive comparison study including five advanced regression\napproaches (boosting, lasso, NEG, MCP, and CAR score) and a univariate approach\n(marginal correlation) to determine the effectiveness in finding true causal\nSNPs.\n  Conclusions: Simultaneous SNP selection is a challenging task. We demonstrate\nthat our CAR score-based algorithm consistently outperforms all competing\napproaches, both uni- and multivariate, in terms of correctly recovered causal\nSNPs and SNP ranking. An R package implementing the approach as well as R code\nto reproduce the complete study presented here is available from\nhttp://strimmerlab.org/software/care/ .\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2012 13:42:11 GMT"}, {"version": "v2", "created": "Fri, 1 Jun 2012 14:11:53 GMT"}, {"version": "v3", "created": "Thu, 25 Oct 2012 21:51:38 GMT"}], "update_date": "2012-11-02", "authors_parsed": [["Zuber", "Verena", ""], ["Silva", "A. Pedro Duarte", ""], ["Strimmer", "Korbinian", ""]]}, {"id": "1203.3278", "submitter": "Cheng Wang", "authors": "Cheng Wang, Jing Yang, Baiqi Miao and Longbing Cao", "title": "On Identity Tests for High Dimensional Data Using RMT", "comments": "16 pages, 2 figures, 3 tables, To be published in the Journal of\n  Multivariate Analysis", "journal-ref": null, "doi": "10.1016/j.jmva.2013.03.015", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we redefined two important statistics, the CLRT test (Bai\net.al., Ann. Stat. 37 (2009) 3822-3840) and the LW test (Ledoit and Wolf, Ann.\nStat. 30 (2002) 1081-1102) on identity tests for high dimensional data using\nrandom matrix theories. Compared with existing CLRT and LW tests, the new tests\ncan accommodate data which has unknown means and non-Gaussian distributions.\nSimulations demonstrate that the new tests have good properties in terms of\nsize and power. What is more, even for Gaussian data, our new tests perform\nfavorably in comparison to existing tests. Finally, we find the CLRT is more\nsensitive to eigenvalues less than 1 while the LW test has more advantages in\nrelation to detecting eigenvalues larger than 1.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 06:57:19 GMT"}, {"version": "v2", "created": "Wed, 21 Mar 2012 02:24:43 GMT"}, {"version": "v3", "created": "Thu, 11 Apr 2013 11:07:19 GMT"}], "update_date": "2013-04-12", "authors_parsed": [["Wang", "Cheng", ""], ["Yang", "Jing", ""], ["Miao", "Baiqi", ""], ["Cao", "Longbing", ""]]}, {"id": "1203.3422", "submitter": "Bernard Ycart", "authors": "Agn\\`es Hamon and Bernard Ycart", "title": "Statistics for the Luria-Delbr\\\"uck distribution", "comments": null, "journal-ref": "Electron. J. Statist. Volume 6 (2012), 1251-1272", "doi": "10.1214/12-EJS711", "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Luria-Delbr\\\"uck distribution is a classical model of mutations in cell\nkinetics. It is obtained as a limit when the probability of mutation tends to\nzero and the number of divisions to infinity. It can be interpreted as a\ncompound Poisson distribution (for the number of mutations) of exponential\nmixtures (for the developing time of mutant clones) of geometric distributions\n(for the number of cells produced by a mutant clone in a given time). The\nprobabilistic interpretation, and a rigourous proof of convergence in the\ngeneral case, are deduced from classical results on Bellman-Harris branching\nprocesses. The two parameters of the Luria-Delbr\\\"uck distribution are the\nexpected number of mutations, which is the parameter of interest, and the\nrelative fitness of normal cells compared to mutants, which is the heavy tail\nexponent. Both can be simultaneously estimated by the maximum likehood method.\nHowever, the computation becomes numerically unstable as soon as the maximal\nvalue of the sample is large, which occurs frequently due to the heavy tail\nproperty. Based on the empirical generating function, robust estimators are\nproposed and their asymptotic variance is given. They are comparable in\nprecision to maximum likelihood estimators, with a much broader range of\ncalculability, a better numerical stability, and a negligible computing time.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 17:27:29 GMT"}, {"version": "v2", "created": "Thu, 28 Jun 2012 07:57:19 GMT"}], "update_date": "2013-09-11", "authors_parsed": [["Hamon", "Agn\u00e8s", ""], ["Ycart", "Bernard", ""]]}, {"id": "1203.3638", "submitter": "Zhiwei Zhang", "authors": "Zhiwei Zhang, Paul S. Albert, Bruce Simons-Morton", "title": "Marginal analysis of longitudinal count data in long sequences: Methods\n  and applications to a driving study", "comments": "Published in at http://dx.doi.org/10.1214/11-AOAS507 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 1, 27-54", "doi": "10.1214/11-AOAS507", "report-no": "IMS-AOAS-AOAS507", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the available methods for longitudinal data analysis are designed and\nvalidated for the situation where the number of subjects is large and the\nnumber of observations per subject is relatively small. Motivated by the\nNaturalistic Teenage Driving Study (NTDS), which represents the exact opposite\nsituation, we examine standard and propose new methodology for marginal\nanalysis of longitudinal count data in a small number of very long sequences.\nWe consider standard methods based on generalized estimating equations, under\nworking independence or an appropriate correlation structure, and find them\nunsatisfactory for dealing with time-dependent covariates when the counts are\nlow. For this situation, we explore a within-cluster resampling (WCR) approach\nthat involves repeated analyses of random subsamples with a final analysis that\nsynthesizes results across subsamples. This leads to a novel WCR method which\noperates on separated blocks within subjects and which performs better than all\nof the previously considered methods. The methods are applied to the NTDS data\nand evaluated in simulation experiments mimicking the NTDS.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2012 09:00:26 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Zhang", "Zhiwei", ""], ["Albert", "Paul S.", ""], ["Simons-Morton", "Bruce", ""]]}, {"id": "1203.3653", "submitter": "Edward I. George", "authors": "Edward I. George, Sam K. Hui", "title": "Optimal pricing using online auction experiments: A P\\'olya tree\n  approach", "comments": "Published in at http://dx.doi.org/10.1214/11-AOAS503 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 1, 55-82", "doi": "10.1214/11-AOAS503", "report-no": "IMS-AOAS-AOAS503", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how a retailer can estimate the optimal price of a new product using\nobserved transaction prices from online second-price auction experiments. For\nthis purpose we propose a Bayesian P\\'olya tree approach which, given the\nlimited nature of the data, requires a specially tailored implementation.\nAvoiding the need for a priori parametric assumptions, the P\\'olya tree\napproach allows for flexible inference of the valuation distribution, leading\nto more robust estimation of optimal price than competing parametric\napproaches. In collaboration with an online jewelry retailer, we illustrate how\nour methodology can be combined with managerial prior knowledge to estimate the\nprofit maximizing price of a new jewelry product.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2012 10:02:58 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["George", "Edward I.", ""], ["Hui", "Sam K.", ""]]}, {"id": "1203.3672", "submitter": "Paul R. Rosenbaum", "authors": "Paul R. Rosenbaum", "title": "An exact adaptive test with superior design sensitivity in an\n  observational study of treatments for ovarian cancer", "comments": "Published in at http://dx.doi.org/10.1214/11-AOAS508 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 1, 83-105", "doi": "10.1214/11-AOAS508", "report-no": "IMS-AOAS-AOAS508", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A sensitivity analysis in an observational study determines the magnitude of\nbias from nonrandom treatment assignment that would need to be present to alter\nthe qualitative conclusions of a na\\\"{\\i}ve analysis that presumes all biases\nwere removed by matching or by other analytic adjustments. The power of a\nsensitivity analysis and the design sensitivity anticipate the outcome of a\nsensitivity analysis under an assumed model for the generation of the data. It\nis known that the power of a sensitivity analysis is affected by the choice of\ntest statistic, and, in particular, that a statistic with good Pitman\nefficiency in a randomized experiment, such as Wilcoxon's signed rank\nstatistic, may have low power in a sensitivity analysis and low design\nsensitivity when compared to other statistics. For instance, for an additive\ntreatment effect and errors that are Normal or logistic or $t$-distributed with\n3 degrees of freedom, Brown's combined quantile average test has Pitman\nefficiency close to that of Wilcoxon's test but has higher power in a\nsensitivity analysis, while a version of Noether's test has poor Pitman\nefficiency in a randomized experiment but much higher design sensitivity so it\nis vastly more powerful than Wilcoxon's statistic in a sensitivity analysis if\nthe sample size is sufficiently large.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2012 11:22:15 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Rosenbaum", "Paul R.", ""]]}, {"id": "1203.3680", "submitter": "Michael D. Porter", "authors": "Michael D. Porter, Gentry White", "title": "Self-exciting hurdle models for terrorist activity", "comments": "Published in at http://dx.doi.org/10.1214/11-AOAS513 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 1, 106-124", "doi": "10.1214/11-AOAS513", "report-no": "IMS-AOAS-AOAS513", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A predictive model of terrorist activity is developed by examining the daily\nnumber of terrorist attacks in Indonesia from 1994 through 2007. The dynamic\nmodel employs a shot noise process to explain the self-exciting nature of the\nterrorist activities. This estimates the probability of future attacks as a\nfunction of the times since the past attacks. In addition, the excess of\nnonattack days coupled with the presence of multiple coordinated attacks on the\nsame day compelled the use of hurdle models to jointly model the probability of\nan attack day and corresponding number of attacks. A power law distribution\nwith a shot noise driven parameter best modeled the number of attacks on an\nattack day. Interpretation of the model parameters is discussed and predictive\nperformance of the models is evaluated.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2012 11:59:32 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Porter", "Michael D.", ""], ["White", "Gentry", ""]]}, {"id": "1203.3712", "submitter": "St\\'{e}phanie Allassonni\\`{e}re", "authors": "St\\'ephanie Allassonni\\'ere, Laurent Younes", "title": "A stochastic algorithm for probabilistic independent component analysis", "comments": "Published in at http://dx.doi.org/10.1214/11-AOAS499 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 1, 125-160", "doi": "10.1214/11-AOAS499", "report-no": "IMS-AOAS-AOAS499", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The decomposition of a sample of images on a relevant subspace is a recurrent\nproblem in many different fields from Computer Vision to medical image\nanalysis. We propose in this paper a new learning principle and implementation\nof the generative decomposition model generally known as noisy ICA (for\nindependent component analysis) based on the SAEM algorithm, which is a\nversatile stochastic approximation of the standard EM algorithm. We demonstrate\nthe applicability of the method on a large range of decomposition models and\nillustrate the developments with experimental results on various data sets.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2012 14:12:04 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Allassonni\u00e9re", "St\u00e9phanie", ""], ["Younes", "Laurent", ""]]}, {"id": "1203.3747", "submitter": "Chanseok Park", "authors": "Chanseok Park", "title": "Note on the closed-form MLEs of k-component load-sharing systems", "comments": "9 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently Kim and Kvam (2004) and Singh, Sharma, Kumar (2008) proposed\ndifferent load-sharing models and developed parametric inference for the these\nmodels. However, their parametric estimates are calculated using iterative\nnumerical methods. In this note, we provide the general closed-form MLEs for\nthe two load-sharing models provided by them.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2012 15:54:50 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Park", "Chanseok", ""]]}, {"id": "1203.3882", "submitter": "Chanseok Park", "authors": "Mokin Lee and Chanseok Park", "title": "A Note On the Use of Fiducial Limits for Control Charts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many of the early works in the quality control literature construct control\nlimits through the use of graphs and tables as described in Wortham and Ringer\n(1972). However, the methods used in this literature are restricted to using\nonly the values that the graphs and tables can provide and to the case where\nthe parameters of the underlying distribution are known. In this note, we\nbriefly describe a technique which can be used to calculate exact control\nlimits without the use of graphs or tables. We also describe what are commonly\nreferred to in the literature as fiducial limits. Fiducial limits are often\nused as the limits in control charting when the parameters of the underlying\ndistribution are unknown.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2012 18:20:01 GMT"}], "update_date": "2012-03-20", "authors_parsed": [["Lee", "Mokin", ""], ["Park", "Chanseok", ""]]}, {"id": "1203.3896", "submitter": "Xi Luo", "authors": "Weidong Liu and Xi Luo", "title": "Fast and Adaptive Sparse Precision Matrix Estimation in High Dimensions", "comments": "Maintext: 24 pages. Supplement: 13 pages. R package scio implementing\n  the proposed method is available on CRAN at\n  https://cran.r-project.org/package=scio . Published in J of Multivariate\n  Analysis at\n  http://www.sciencedirect.com/science/article/pii/S0047259X14002607", "journal-ref": "Journal of Multivariate Analysis. 2015; 135:153 -62", "doi": "10.1016/J.Jmva.2014.11.005", "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new method for estimating sparse precision matrices in\nthe high dimensional setting. It has been popular to study fast computation and\nadaptive procedures for this problem. We propose a novel approach, called\nSparse Column-wise Inverse Operator, to address these two issues. We analyze an\nadaptive procedure based on cross validation, and establish its convergence\nrate under the Frobenius norm. The convergence rates under other matrix norms\nare also established. This method also enjoys the advantage of fast computation\nfor large-scale problems, via a coordinate descent algorithm. Numerical merits\nare illustrated using both simulated and real datasets. In particular, it\nperforms favorably on an HIV brain tissue dataset and an ADHD resting-state\nfMRI dataset.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2012 21:58:02 GMT"}, {"version": "v2", "created": "Thu, 22 Dec 2016 07:09:46 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Liu", "Weidong", ""], ["Luo", "Xi", ""]]}, {"id": "1203.4065", "submitter": "Lucio Barabesi", "authors": "Lucio Barabesi, Sara Franceschi, Marzia Marcheselli", "title": "Properties of design-based estimation under stratified spatial sampling\n  with application to canopy coverage estimation", "comments": "Published in at http://dx.doi.org/10.1214/11-AOAS509 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 1, 210-228", "doi": "10.1214/11-AOAS509", "report-no": "IMS-AOAS-AOAS509", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of the total of an attribute defined over a continuous planar\ndomain is required in many applied settings, such as the estimation of canopy\ncoverage in the Monterano Nature Reserve in Italy. If the design-based approach\nis considered, the scheme for the placement of the sample sites over the domain\nis fundamental in order to implement the survey. In real situations, a commonly\nadopted scheme is based on partitioning the domain into suitable strata, in\nsuch a way that a single sample site is uniformly placed (i.e., selected with\nuniform probability density) in each stratum and sample sites are independently\nlocated. Under mild conditions on the function representing the target\nattribute, it is shown that this scheme gives rise to an unbiased spatial total\nestimator which is \"superefficient\" with respect to the estimator based on the\nuniform placement of independent sample sites over the domain. In addition, the\nlarge-sample normality of the estimator is proven and variance estimation\nissues are discussed.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2012 09:46:18 GMT"}], "update_date": "2012-03-20", "authors_parsed": [["Barabesi", "Lucio", ""], ["Franceschi", "Sara", ""], ["Marcheselli", "Marzia", ""]]}, {"id": "1203.4119", "submitter": "Hedibert F. Lopes", "authors": "Hedibert F. Lopes, Alexandra M. Schmidt, Esther Salazar, Mariana\n  G\\'omez, Marcel Achkar", "title": "Measuring the vulnerability of the Uruguayan population to vector-borne\n  diseases via spatially hierarchical factor models", "comments": "Published in at http://dx.doi.org/10.1214/11-AOAS497 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 1, 284-303", "doi": "10.1214/11-AOAS497", "report-no": "IMS-AOAS-AOAS497", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a model-based vulnerability index of the population from Uruguay\nto vector-borne diseases. We have available measurements of a set of variables\nin the census tract level of the 19 Departmental capitals of Uruguay. In\nparticular, we propose an index that combines different sources of information\nvia a set of micro-environmental indicators and geographical location in the\ncountry. Our index is based on a new class of spatially hierarchical factor\nmodels that explicitly account for the different levels of hierarchy in the\ncountry, such as census tracts within the city level, and cities in the country\nlevel. We compare our approach with that obtained when data are aggregated in\nthe city level. We show that our proposal outperforms current and standard\napproaches, which fail to properly account for discrepancies in the region\nsizes, for example, number of census tracts. We also show that data aggregation\ncan seriously affect the estimation of the cities vulnerability rankings under\nbenchmark models.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2012 14:32:16 GMT"}], "update_date": "2012-03-20", "authors_parsed": [["Lopes", "Hedibert F.", ""], ["Schmidt", "Alexandra M.", ""], ["Salazar", "Esther", ""], ["G\u00f3mez", "Mariana", ""], ["Achkar", "Marcel", ""]]}, {"id": "1203.4122", "submitter": "Hao Wang", "authors": "Hao Wang, Jerome P. Reiter", "title": "Multiple imputation for sharing precise geographies in public use data", "comments": "Published in at http://dx.doi.org/10.1214/11-AOAS506 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 1, 229-252", "doi": "10.1214/11-AOAS506", "report-no": "IMS-AOAS-AOAS506", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When releasing data to the public, data stewards are ethically and often\nlegally obligated to protect the confidentiality of data subjects' identities\nand sensitive attributes. They also strive to release data that are informative\nfor a wide range of secondary analyses. Achieving both objectives is\nparticularly challenging when data stewards seek to release highly resolved\ngeographical information. We present an approach for protecting the\nconfidentiality of data with geographic identifiers based on multiple\nimputation. The basic idea is to convert geography to latitude and longitude,\nestimate a bivariate response model conditional on attributes, and simulate new\nlatitude and longitude values from these models. We illustrate the proposed\nmethods using data describing causes of death in Durham, North Carolina. In the\ncontext of the application, we present a straightforward tool for generating\nsimulated geographies and attributes based on regression trees, and we present\nmethods for assessing disclosure risks with such simulated data.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2012 14:42:41 GMT"}], "update_date": "2012-03-20", "authors_parsed": [["Wang", "Hao", ""], ["Reiter", "Jerome P.", ""]]}, {"id": "1203.4359", "submitter": "Peng Wei", "authors": "Peng Wei, Wei Pan", "title": "Bayesian joint modeling of multiple gene networks and diverse genomic\n  data to identify target genes of a transcription factor", "comments": "Published in at http://dx.doi.org/10.1214/11-AOAS502 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 1, 334-355", "doi": "10.1214/11-AOAS502", "report-no": "IMS-AOAS-AOAS502", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider integrative modeling of multiple gene networks and diverse\ngenomic data, including protein-DNA binding, gene expression and DNA sequence\ndata, to accurately identify the regulatory target genes of a transcription\nfactor (TF). Rather than treating all the genes equally and independently a\npriori in existing joint modeling approaches, we incorporate the biological\nprior knowledge that neighboring genes on a gene network tend to be (or not to\nbe) regulated together by a TF. A key contribution of our work is that, to\nmaximize the use of all existing biological knowledge, we allow incorporation\nof multiple gene networks into joint modeling of genomic data by introducing a\nmixture model based on the use of multiple Markov random fields (MRFs). Another\nimportant contribution of our work is to allow different genomic data to be\ncorrelated and to examine the validity and effect of the independence\nassumption as adopted in existing methods. Due to a fully Bayesian approach,\ninference about model parameters can be carried out based on MCMC samples.\nApplication to an E. coli data set, together with simulation studies,\ndemonstrates the utility and statistical efficiency gains with the proposed\njoint model.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2012 09:52:53 GMT"}], "update_date": "2012-03-21", "authors_parsed": [["Wei", "Peng", ""], ["Pan", "Wei", ""]]}, {"id": "1203.4394", "submitter": "The-Minh Luong", "authors": "The-Minh Luong, Yves Rozenholc, Gregory Nuel", "title": "Fast estimation of posterior probabilities in change-point models\n  through a constrained hidden Markov model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection of change-points in heterogeneous sequences is a statistical\nchallenge with applications across a wide variety of fields. In bioinformatics,\na vast amount of methodology exists to identify an ideal set of change-points\nfor detecting Copy Number Variation (CNV). While considerable efficient\nalgorithms are currently available for finding the best segmentation of the\ndata in CNV, relatively few approaches consider the important problem of\nassessing the uncertainty of the change-point location. Asymptotic and\nstochastic approaches exist but often require additional model assumptions to\nspeed up the computations, while exact methods have quadratic complexity which\nusually are intractable for large datasets of tens of thousands points or more.\nIn this paper, we suggest an exact method for obtaining the posterior\ndistribution of change-points with linear complexity, based on a constrained\nhidden Markov model. The methods are implemented in the R package postCP, which\nuses the results of a given change-point detection algorithm to estimate the\nprobability that each observation is a change-point. We present the results of\nthe package on a publicly available CNV data set (n=120). Due to its\nfrequentist framework, postCP obtains less conservative confidence intervals\nthan previously published Bayesian methods, but with linear complexity instead\nof quadratic. Simulations showed that postCP provided comparable loss to a\nBayesian MCMC method when estimating posterior means, specifically when\nassessing larger-scale changes, while being more computationally efficient. On\nanother high-resolution CNV data set (n=14,241), the implementation processed\ninformation in less than one second on a mid-range laptop computer.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2012 11:15:31 GMT"}, {"version": "v2", "created": "Tue, 22 Jan 2013 15:44:36 GMT"}], "update_date": "2013-01-23", "authors_parsed": [["Luong", "The-Minh", ""], ["Rozenholc", "Yves", ""], ["Nuel", "Gregory", ""]]}, {"id": "1203.4445", "submitter": "I-Shou Chang", "authors": "Chung-Hsing Chen, Wen-Chi Su, Chih-Yu Chen, Jing-Ying Huang, Fang-Yu\n  Tsai, Wen-Chang Wang, Chao A. Hsiung, King-Song Jeng, I-Shou Chang", "title": "A Bayesian measurement error model for two-channel cell-based RNAi data\n  with replicates", "comments": "Published in at http://dx.doi.org/10.1214/11-AOAS496 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 1, 356-382", "doi": "10.1214/11-AOAS496", "report-no": "IMS-AOAS-AOAS496", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RNA interference (RNAi) is an endogenous cellular process in which small\ndouble-stranded RNAs lead to the destruction of mRNAs with complementary\nnucleoside sequence. With the production of RNAi libraries, large-scale RNAi\nscreening in human cells can be conducted to identify unknown genes involved in\na biological pathway. One challenge researchers face is how to deal with the\nmultiple testing issue and the related false positive rate (FDR) and false\nnegative rate (FNR). This paper proposes a Bayesian hierarchical measurement\nerror model for the analysis of data from a two-channel RNAi high-throughput\nexperiment with replicates, in which both the activity of a particular\nbiological pathway and cell viability are monitored and the goal is to identify\nshort hair-pin RNAs (shRNAs) that affect the pathway activity without affecting\ncell activity. Simulation studies demonstrate the flexibility and robustness of\nthe Bayesian method and the benefits of having replicates in the experiment.\nThis method is illustrated through analyzing the data from a RNAi\nhigh-throughput screening that searches for cellular factors affecting HCV\nreplication without affecting cell viability; comparisons of the results from\nthis HCV study and some of those reported in the literature are included.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2012 14:09:50 GMT"}], "update_date": "2012-03-21", "authors_parsed": [["Chen", "Chung-Hsing", ""], ["Su", "Wen-Chi", ""], ["Chen", "Chih-Yu", ""], ["Huang", "Jing-Ying", ""], ["Tsai", "Fang-Yu", ""], ["Wang", "Wen-Chang", ""], ["Hsiung", "Chao A.", ""], ["Jeng", "King-Song", ""], ["Chang", "I-Shou", ""]]}, {"id": "1203.4468", "submitter": "Chanseok Park", "authors": "Chanseok Park", "title": "A Quantile Variant of the EM Algorithm and Its Applications to Parameter\n  Estimation with Interval Data", "comments": null, "journal-ref": null, "doi": "10.1177/1748301818779007", "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The expectation-maximization (EM) algorithm is a powerful computational\ntechnique for finding the maximum likelihood estimates for parametric models\nwhen the data are not fully observed. The EM is best suited for situations\nwhere the expectation in each E-step and the maximization in each M-step are\nstraightforward. A difficulty with the implementation of the EM algorithm is\nthat each E-step requires the integration of the log-likelihood function in\nclosed form. The explicit integration can be avoided by using what is known as\nthe Monte Carlo EM (MCEM) algorithm. The MCEM uses a random sample to estimate\nthe integral at each E-step. However, the problem with the MCEM is that it\noften converges to the integral quite slowly and the convergence behavior can\nalso be unstable, which causes a computational burden. In this paper, we\npropose what we refer to as the quantile variant of the EM (QEM) algorithm. We\nprove that the proposed QEM method has an accuracy of $O(1/K^2)$ while the MCEM\nmethod has an accuracy of $O_p(1/\\sqrt{K})$. Thus, the proposed QEM method\npossesses faster and more stable convergence properties when compared with the\nMCEM algorithm. The improved performance is illustrated through the numerical\nstudies. Several practical examples illustrating its use in interval-censored\ndata problems are also provided.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2012 15:23:27 GMT"}, {"version": "v2", "created": "Fri, 11 May 2018 10:34:13 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Park", "Chanseok", ""]]}, {"id": "1203.4661", "submitter": "Ying Wei", "authors": "Ying Wei, Zhibiao Zhao, Dennis K. J. Lin", "title": "Profile control charts based on nonparametric $L$-1 regression methods", "comments": "Published in at http://dx.doi.org/10.1214/11-AOAS501 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 1, 409-427", "doi": "10.1214/11-AOAS501", "report-no": "IMS-AOAS-AOAS501", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical statistical process control often relies on univariate\ncharacteristics. In many contemporary applications, however, the quality of\nproducts must be characterized by some functional relation between a response\nvariable and its explanatory variables. Monitoring such functional profiles has\nbeen a rapidly growing field due to increasing demands. This paper develops a\nnovel nonparametric $L$-1 location-scale model to screen the shapes of\nprofiles. The model is built on three basic elements: location shifts, local\nshape distortions, and overall shape deviations, which are quantified by three\nindividual metrics. The proposed approach is applied to the previously analyzed\nvertical density profile data, leading to some interesting insights.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2012 06:59:11 GMT"}], "update_date": "2012-03-22", "authors_parsed": [["Wei", "Ying", ""], ["Zhao", "Zhibiao", ""], ["Lin", "Dennis K. J.", ""]]}, {"id": "1203.4928", "submitter": "Marie Walschaerts", "authors": "Marie Walschaerts (EA 3694), Eve Leconte (GREMAQ), Philippe Besse\n  (IMT)", "title": "Stable variable selection for right censored data: comparison of methods", "comments": "nombre de pages : 29 nombre de tableaux : 2 nombre de figures : 9", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The instability in the selection of models is a major concern with data sets\ncontaining a large number of covariates. This paper deals with variable\nselection methodology in the case of high-dimensional problems where the\nresponse variable can be right censored. We focuse on new stable variable\nselection methods based on bootstrap for two methodologies: the Cox\nproportional hazard model and survival trees. As far as the Cox model is\nconcerned, we investigate the bootstrapping applied to two variable selection\ntechniques: the stepwise algorithm based on the AIC criterion and the\nL1-penalization of Lasso. Regarding survival trees, we review two\nmethodologies: the bootstrap node-level stabilization and random survival\nforests. We apply these different approaches to two real data sets. We compare\nthe methods on the prediction error rate based on the Harrell concordance index\nand the relevance of the interpretation of the corresponding selected models.\nThe aim is to find a compromise between a good prediction performance and ease\nto interpretation for clinicians. Results suggest that in the case of a small\nnumber of individuals, a bootstrapping adapted to L1-penalization in the Cox\nmodel or a bootstrap node-level stabilization in survival trees give a good\nalternative to the random survival forest methodology, known to give the\nsmallest prediction error rate but difficult to interprete by\nnon-statisticians. In a clinical perspective, the complementarity between the\nmethods based on the Cox model and those based on survival trees would permit\nto built reliable models easy to interprete by the clinician.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2012 09:28:48 GMT"}], "update_date": "2012-03-23", "authors_parsed": [["Walschaerts", "Marie", "", "EA 3694"], ["Leconte", "Eve", "", "GREMAQ"], ["Besse", "Philippe", "", "IMT"]]}, {"id": "1203.5124", "submitter": "Liang Zhang", "authors": "Rajiv Khanna, Liang Zhang, Deepak Agarwal, Beechung Chen", "title": "Parallel Matrix Factorization for Binary Response", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting user affinity to items is an important problem in applications\nlike content optimization, computational advertising, and many more. While\nbilinear random effect models (matrix factorization) provide state-of-the-art\nperformance when minimizing RMSE through a Gaussian response model on explicit\nratings data, applying it to imbalanced binary response data presents\nadditional challenges that we carefully study in this paper. Data in many\napplications usually consist of users' implicit response that are often binary\n-- clicking an item or not; the goal is to predict click rates, which is often\ncombined with other measures to calculate utilities to rank items at runtime of\nthe recommender systems. Because of the implicit nature, such data are usually\nmuch larger than explicit rating data and often have an imbalanced distribution\nwith a small fraction of click events, making accurate click rate prediction\ndifficult. In this paper, we address two problems. First, we show previous\ntechniques to estimate bilinear random effect models with binary data are less\naccurate compared to our new approach based on adaptive rejection sampling,\nespecially for imbalanced response. Second, we develop a parallel bilinear\nrandom effect model fitting framework using Map-Reduce paradigm that scales to\nmassive datasets. Our parallel algorithm is based on a \"divide and conquer\"\nstrategy coupled with an ensemble approach. Through experiments on the\nbenchmark MovieLens data, a small Yahoo! Front Page data set, and a large\nYahoo! Front Page data set that contains 8M users and 1B binary observations,\nwe show that careful handling of binary response as well as identifiability\nissues are needed to achieve good performance for click rate prediction, and\nthat the proposed adaptive rejection sampler and the partitioning as well as\nensemble techniques significantly improve model performance.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2012 20:54:53 GMT"}], "update_date": "2012-03-26", "authors_parsed": [["Khanna", "Rajiv", ""], ["Zhang", "Liang", ""], ["Agarwal", "Deepak", ""], ["Chen", "Beechung", ""]]}, {"id": "1203.5346", "submitter": "Tatiana Xifara", "authors": "Chris Sherlock, Tatiana Xifara, Sandra Telfer and Mike Begon", "title": "A coupled hidden Markov model for disease interactions", "comments": "25 pages, 2 figures, To appear in Journal of the Royal Statistical\n  Society: Series C", "journal-ref": "Journal of the Royal Statistical Society: Series C, 62 (2013)", "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To investigate interactions between parasite species in a host, a population\nof field voles was studied longitudinally, with presence or absence of six\ndifferent parasites measured repeatedly. Although trapping sessions were\nregular, a different set of voles was caught at each session leading to\nincomplete profiles for all subjects. We use a discrete-time hidden Markov\nmodel for each disease with transition probabilities dependent on covariates\nvia a set of logistic regressions. For each disease the hidden states for each\nof the other diseases at a given time point form part of the covariate set for\nthe Markov transition probabilities from that time point. This allows us to\ngauge the influence of each parasite species on the transition probabilities\nfor each of the other parasite species. Inference is performed via a Gibbs\nsampler, which cycles through each of the diseases, first using an adaptive\nMetropolis-Hastings step to sample from the conditional posterior of the\ncovariate parameters for that particular disease given the hidden states for\nall other diseases and then sampling from the hidden states for that disease\ngiven the parameters. We find evidence for interactions between several pairs\nof parasites and of an acquired immune response for two of the parasites.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2012 16:00:10 GMT"}, {"version": "v2", "created": "Sun, 27 Jan 2013 13:37:25 GMT"}], "update_date": "2013-01-29", "authors_parsed": [["Sherlock", "Chris", ""], ["Xifara", "Tatiana", ""], ["Telfer", "Sandra", ""], ["Begon", "Mike", ""]]}, {"id": "1203.5405", "submitter": "Daniel Straub Daniel Straub", "authors": "Daniel Straub", "title": "Reliability updating with equality information", "comments": null, "journal-ref": "Probabilistic Engineering Mechanics, 2011, 26(2): 254-258", "doi": "10.1016/j.probengmech.2010.08.003", "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many instances, information on engineering systems can be obtained through\nmeasurements, monitoring or direct observations of system performances and can\nbe used to update the system reliability estimate. In structural reliability\nanalysis, such information is expressed either by inequalities (e.g. for the\nobservation that no defect is present) or by equalities (e.g. for quantitative\nmeasurements of system characteristics). When information Z is of the equality\ntype, the a-priori probability of Z is zero and most structural reliability\nmethods (SRM) are not directly applicable to the computation of the updated\nreliability. Hitherto, the computation of the reliability of engineering\nsystems conditional on equality information was performed through first- and\nsecond order approximations. In this paper, it is shown how equality\ninformation can be transformed into inequality information, which enables\nreliability updating by solving a standard structural system reliability\nproblem. This approach enables the use of any SRM, including those based on\nsimulation, for reliability updating with equality information. It is\ndemonstrated on three numerical examples, including an application to fatigue\nreliability.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2012 11:06:22 GMT"}], "update_date": "2012-03-27", "authors_parsed": [["Straub", "Daniel", ""]]}, {"id": "1203.5446", "submitter": "Cyril Voyant", "authors": "Philippe Lauret (PIMENT), Auline Rodler (SPE), Marc Muselli (SPE),\n  Mathieu David (PIMENT), Hadja Diagne (PIMENT), Cyril Voyant (SPE, CHD\n  Castellucio)", "title": "A Bayesian Model Committee Approach to Forecasting Global Solar\n  Radiation", "comments": "WREF 2012 : World Renewable Energy Forum, Denver : United States\n  (2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes to use a rather new modelling approach in the realm of\nsolar radiation forecasting. In this work, two forecasting models:\nAutoregressive Moving Average (ARMA) and Neural Network (NN) models are\ncombined to form a model committee. The Bayesian inference is used to affect a\nprobability to each model in the committee. Hence, each model's predictions are\nweighted by their respective probability. The models are fitted to one year of\nhourly Global Horizontal Irradiance (GHI) measurements. Another year (the test\nset) is used for making genuine one hour ahead (h+1) out-of-sample forecast\ncomparisons. The proposed approach is benchmarked against the persistence\nmodel. The very first results show an improvement brought by this approach.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2012 20:58:48 GMT"}], "update_date": "2012-03-27", "authors_parsed": [["Lauret", "Philippe", "", "PIMENT"], ["Rodler", "Auline", "", "SPE"], ["Muselli", "Marc", "", "SPE"], ["David", "Mathieu", "", "PIMENT"], ["Diagne", "Hadja", "", "PIMENT"], ["Voyant", "Cyril", "", "SPE, CHD\n  Castellucio"]]}, {"id": "1203.5692", "submitter": "Mark Higgins", "authors": "Mark G. Higgins", "title": "Cube Handling In Backgammon Money Games Under a Jump Model", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A variation on Janowski's cubeful equity model is proposed for cube handling\nin backgammon money games. Instead of approximating the cubeful take point as\nan interpolation between the dead and live cube limits, a new model is\ndeveloped where the cubeless probability of win evolves through a series of\nrandom jumps instead of continuous diffusion. Each jump is drawn from a\ndistribution with zero mean and an expected absolute jump size called the \"jump\nvolatility\" that can be a function of game state but is assumed to be small\ncompared to the market window. Closed form approximations for cubeful equities\nand cube decision points are developed as a function of local and remote jump\nvolatility. The local jump volatility can be calculated for specific game\nstates, leading to crisper doubling decisions.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2012 14:54:39 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2012 16:21:02 GMT"}, {"version": "v3", "created": "Thu, 5 Apr 2012 19:10:23 GMT"}, {"version": "v4", "created": "Fri, 13 Apr 2012 12:52:17 GMT"}, {"version": "v5", "created": "Mon, 23 Apr 2012 17:11:02 GMT"}], "update_date": "2012-04-24", "authors_parsed": [["Higgins", "Mark G.", ""]]}, {"id": "1203.5885", "submitter": "Korbinian Strimmer", "authors": "Sebastian Gibb and Korbinian Strimmer", "title": "MALDIquant: a versatile R package for the analysis of mass spectrometry\n  data", "comments": "5 pages, 1 figure", "journal-ref": "Bioinformatics 2012, Vol. 28, Issue 17, 2270-2271", "doi": "10.1093/bioinformatics/bts447", "report-no": null, "categories": "q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Summary: MALDIquant is an R package providing a complete and modular analysis\npipeline for quantitative analysis of mass spectrometry data. MALDIquant is\nspecifically designed with application in clinical diagnostics in mind and\nimplements sophisticated routines for importing raw data, preprocessing,\nnon-linear peak alignment, and calibration. It also handles technical\nreplicates as well as spectra with unequal resolution.\n  Availability: MALDIquant and its associated R packages readBrukerFlexData and\nreadMzXmlData are freely available from the R archive CRAN\n(http://cran.r-project.org). The software is distributed under the GNU General\nPublic License (version 3 or later) and is accompanied by example files and\ndata. Additional documentation is available from\nhttp://strimmerlab.org/software/maldiquant/.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2012 07:45:50 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2012 08:23:05 GMT"}, {"version": "v3", "created": "Wed, 4 Jul 2012 13:14:00 GMT"}], "update_date": "2012-08-30", "authors_parsed": [["Gibb", "Sebastian", ""], ["Strimmer", "Korbinian", ""]]}, {"id": "1203.5950", "submitter": "Joseph Dureau", "authors": "Joseph Dureau, Konstantinos Kalogeropoulos and Marc Baguelin", "title": "Capturing the time-varying drivers of an epidemic using stochastic\n  dynamical systems", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Epidemics are often modelled using non-linear dynamical systems observed\nthrough partial and noisy data. In this paper, we consider stochastic\nextensions in order to capture unknown influences (changing behaviors, public\ninterventions, seasonal effects etc). These models assign diffusion processes\nto the time-varying parameters, and our inferential procedure is based on a\nsuitably adjusted adaptive particle MCMC algorithm. The performance of the\nproposed computational methods is validated on simulated data and the adopted\nmodel is applied to the 2009 H1N1 pandemic in England. In addition to\nestimating the effective contact rate trajectories, the methodology is applied\nin real time to provide evidence in related public health decisions. Diffusion\ndriven SEIR-type models with age structure are also introduced.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2012 12:25:00 GMT"}, {"version": "v2", "created": "Sat, 3 Nov 2012 14:18:44 GMT"}], "update_date": "2012-11-06", "authors_parsed": [["Dureau", "Joseph", ""], ["Kalogeropoulos", "Konstantinos", ""], ["Baguelin", "Marc", ""]]}, {"id": "1203.5985", "submitter": "Daniel Straub Daniel Straub", "authors": "Daniel Straub, Armen Der Kiureghian", "title": "Bayesian Network Enhanced with Structural Reliability Methods:\n  Application", "comments": null, "journal-ref": "Journal of Engineering Mechanics, Trans. ASCE, 2010, 136(10):\n  1259-1270", "doi": "10.1061/(ASCE)EM.1943-7889.0000170", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The enhanced Bayesian network (eBN) methodology described in the companion\npaper facilitates the assessment of reliability and risk of engineering systems\nwhen information about the system evolves in time. We present the application\nof the eBN (a) to the assessment of the life-cycle reliability of a structural\nsystem, (b) to the optimization of a decision on performing measurements in\nthat structural system, and (c) to the risk assessment of an infrastructure\nsystem subject to natural hazards and deterioration of constituent structures.\nIn all applications, observations of system performances or the hazards are\nmade at various points in time and the eBN efficiently includes these\nobservations in the analysis to provide an updated probabilistic model of the\nsystem at all times.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2012 14:50:45 GMT"}], "update_date": "2012-03-28", "authors_parsed": [["Straub", "Daniel", ""], ["Der Kiureghian", "Armen", ""]]}, {"id": "1203.5986", "submitter": "Daniel Straub Daniel Straub", "authors": "Daniel Straub, Armen Der Kiureghian", "title": "Bayesian Network Enhanced with Structural Reliability Methods:\n  Methodology", "comments": null, "journal-ref": "Journal of Engineering Mechanics, Trans. ASCE, 2010, 136(10):\n  1248-1258", "doi": "10.1061/(ASCE)EM.1943-7889.0000173", "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We combine Bayesian networks (BNs) and structural reliability methods (SRMs)\nto create a new computational framework, termed enhanced Bayesian network\n(eBN), for reliability and risk analysis of engineering structures and\ninfrastructure. BNs are efficient in representing and evaluating complex\nprobabilistic dependence structures, as present in infrastructure and\nstructural systems, and they facilitate Bayesian updating of the model when new\ninformation becomes available. On the other hand, SRMs enable accurate\nassessment of probabilities of rare events represented by computationally\ndemanding, physically-based models. By combining the two methods, the eBN\nframework provides a unified and powerful tool for efficiently computing\nprobabilities of rare events in complex structural and infrastructure systems\nin which information evolves in time. Strategies for modeling and efficiently\nanalyzing the eBN are described by way of several conceptual examples. The\ncompanion paper applies the eBN methodology to example structural and\ninfrastructure systems.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2012 14:50:56 GMT"}], "update_date": "2012-03-28", "authors_parsed": [["Straub", "Daniel", ""], ["Der Kiureghian", "Armen", ""]]}, {"id": "1203.6140", "submitter": "Darryl Veitch", "authors": "Anders Gorst-Rasmussen, Darryl Veitch, Andr\\'as Gefferth", "title": "Why FARIMA Models are Brittle", "comments": "20 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The FARIMA models, which have long-range-dependence (LRD), are widely used in\nmany areas. Through deriving a precise characterisation of the spectrum,\nautocovariance function, and variance time function, we show that this family\nis very atypical among LRD processes, being extremely close to the fractional\nGaussian noise in a precise sense. Furthermore, we show that this closeness\nproperty is not robust to additive noise. We argue that the use of FARIMA, and\nmore generally fractionally differenced time series, should be reassessed in\nsome contexts, in particular when convergence rate under rescaling is important\nand noise is expected.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2012 02:41:50 GMT"}], "update_date": "2012-03-29", "authors_parsed": [["Gorst-Rasmussen", "Anders", ""], ["Veitch", "Darryl", ""], ["Gefferth", "Andr\u00e1s", ""]]}, {"id": "1203.6276", "submitter": "Pekka Malo", "authors": "Ankur Sinha, Pekka Malo, Timo Kuosmanen", "title": "A Multi-objective Exploratory Procedure for Regression Model Selection", "comments": "in Journal of Computational and Graphical Statistics, Vol. 24, Iss.\n  1, 2015", "journal-ref": null, "doi": "10.1080/10618600.2014.899236", "report-no": null, "categories": "stat.CO cs.NE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection is recognized as one of the most critical steps in\nstatistical modeling. The problems encountered in engineering and social\nsciences are commonly characterized by over-abundance of explanatory variables,\nnon-linearities and unknown interdependencies between the regressors. An added\ndifficulty is that the analysts may have little or no prior knowledge on the\nrelative importance of the variables. To provide a robust method for model\nselection, this paper introduces the Multi-objective Genetic Algorithm for\nVariable Selection (MOGA-VS) that provides the user with an optimal set of\nregression models for a given data-set. The algorithm considers the regression\nproblem as a two objective task, and explores the Pareto-optimal (best subset)\nmodels by preferring those models over the other which have less number of\nregression coefficients and better goodness of fit. The model exploration can\nbe performed based on in-sample or generalization error minimization. The model\nselection is proposed to be performed in two steps. First, we generate the\nfrontier of Pareto-optimal regression models by eliminating the dominated\nmodels without any user intervention. Second, a decision making process is\nexecuted which allows the user to choose the most preferred model using\nvisualisations and simple metrics. The method has been evaluated on a recently\npublished real dataset on Communities and Crime within United States.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2012 14:15:24 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2012 15:54:33 GMT"}, {"version": "v3", "created": "Tue, 23 Jul 2013 22:34:01 GMT"}, {"version": "v4", "created": "Wed, 13 Jul 2016 07:53:01 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Sinha", "Ankur", ""], ["Malo", "Pekka", ""], ["Kuosmanen", "Timo", ""]]}, {"id": "1203.6297", "submitter": "Andria Sarri", "authors": "A. Sarri, S. Guillas, F. Dias", "title": "Statistical emulation of a tsunami model for sensitivity analysis and\n  uncertainty quantification", "comments": null, "journal-ref": null, "doi": "10.5194/nhess-12-2003-2012", "report-no": null, "categories": "stat.AP physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the catastrophic consequences of tsunamis, early warnings need to be\nissued quickly in order to mitigate the hazard. Additionally, there is a need\nto represent the uncertainty in the predictions of tsunami characteristics\ncorresponding to the uncertain trigger features (e.g. either position, shape\nand speed of a landslide, or sea floor deformation associated with an\nearthquake). Unfortunately, computer models are expensive to run. This leads to\nsignificant delays in predictions and makes the uncertainty quantification\nimpractical. Statistical emulators run almost instantaneously and may represent\nwell the outputs of the computer model. In this paper, we use the Outer Product\nEmulator to build a fast statistical surrogate of a landslide-generated tsunami\ncomputer model. This Bayesian framework enables us to build the emulator by\ncombining prior knowledge of the computer model properties with a few carefully\nchosen model evaluations. The good performance of the emulator is validated\nusing the Leave-One-Out method.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2012 15:34:57 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Sarri", "A.", ""], ["Guillas", "S.", ""], ["Dias", "F.", ""]]}, {"id": "1203.6507", "submitter": "Joachim Kaldasch", "authors": "Joachim Kaldasch", "title": "Evolutionary Model of the Personal Income Distribution", "comments": null, "journal-ref": "Physica A: Statistical Mechanics and its Applications, 391 (2012)\n  5628-5642", "doi": "10.1016/j.physa.2012.06.048", "report-no": null, "categories": "q-fin.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this work is to establish the personal income distribution from\nthe elementary constituents of a free market; products of a representative good\nand agents forming the economic network. The economy is treated as a\nself-organized system. Based on the idea that the dynamics of an economy is\ngoverned by slow modes, the model suggests that for short time intervals a\nfixed ratio of total labour income (capital income) to net income exists\n(Cobb-Douglas relation). Explicitly derived is Gibrat's law from an\nevolutionary market dynamics of short term fluctuations. The total private\nincome distribution is shown to consist of four main parts. From capital income\nof private firms the income distribution contains a lognormal distribution for\nsmall and a Pareto tail for large incomes. Labour income contributes an\nexponential distribution. Also included is the income from a social insurance\nsystem, approximated by a Gaussian peak. The evolutionary model is able to\nreproduce the stylized facts of the income distribution, shown by a comparison\nwith empirical data of a high resolution income distribution. The theory\nsuggests that in a free market competition between products is ultimately the\norigin of the uneven income distribution.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2012 12:39:34 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Kaldasch", "Joachim", ""]]}, {"id": "1203.6750", "submitter": "Marco Huber", "authors": "Marco F. Huber", "title": "Adaptive Gaussian Mixture Filter Based on Statistical Linearization", "comments": "8 pages, appeared in the proceedings of the 14th International\n  Conference on Information Fusion, Chicago, Illinois, USA, July 2011.\n  Correction of an error in formula (22).\n  http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5977694&isnumber=5977431", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian mixtures are a common density representation in nonlinear,\nnon-Gaussian Bayesian state estimation. Selecting an appropriate number of\nGaussian components, however, is difficult as one has to trade of computational\ncomplexity against estimation accuracy. In this paper, an adaptive Gaussian\nmixture filter based on statistical linearization is proposed. Depending on the\nnonlinearity of the considered estimation problem, this filter dynamically\nincreases the number of components via splitting. For this purpose, a measure\nis introduced that allows for quantifying the locally induced linearization\nerror at each Gaussian mixture component. The deviation between the nonlinear\nand the linearized state space model is evaluated for determining the splitting\ndirection. The proposed approach is not restricted to a specific statistical\nlinearization method. Simulations show the superior estimation performance\ncompared to related approaches and common filtering algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2012 09:05:35 GMT"}], "update_date": "2012-04-02", "authors_parsed": [["Huber", "Marco F.", ""]]}, {"id": "1203.6754", "submitter": "Marco Huber", "authors": "Marco F. Huber", "title": "On Multi-Step Sensor Scheduling via Convex Optimization", "comments": "6 pages, appeared in the proceedings of the 2nd International\n  Workshop on Cognitive Information Processing (CIP), Elba, Italy, June 2010", "journal-ref": null, "doi": "10.1109/CIP.2010.5604100", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective sensor scheduling requires the consideration of long-term effects\nand thus optimization over long time horizons. Determining the optimal sensor\nschedule, however, is equivalent to solving a binary integer program, which is\ncomputationally demanding for long time horizons and many sensors. For linear\nGaussian systems, two efficient multi-step sensor scheduling approaches are\nproposed in this paper. The first approach determines approximate but close to\noptimal sensor schedules via convex optimization. The second approach combines\nconvex optimization with a \\BB search for efficiently determining the optimal\nsensor schedule.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2012 09:40:33 GMT"}], "update_date": "2012-04-02", "authors_parsed": [["Huber", "Marco F.", ""]]}, {"id": "1203.6890", "submitter": "Allen Downey", "authors": "Allen B. Downey", "title": "Estimating the age of renal tumors", "comments": "3 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Bayesian method for estimating the age of a renal tumor given\nits size. We use a model of tumor growth based on published data from\nobservations of untreated tumors. We find, for example, that the median age of\na 5 cm tumor is 20 years, with interquartile range 16-23 and 90% confidence\ninterval 11-30 years.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2012 19:05:22 GMT"}], "update_date": "2012-04-02", "authors_parsed": [["Downey", "Allen B.", ""]]}]