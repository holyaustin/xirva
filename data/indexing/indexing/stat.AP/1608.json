[{"id": "1608.00086", "submitter": "Benjamin Fitzpatrick Mr", "authors": "Benjamin R. Fitzpatrick, David W. Lamb and Kerrie Mengersen", "title": "Assessing Site Effects and Geographic Transferability when Interpolating\n  Point Referenced Spatial Data: A Digital Soil Mapping Case Study", "comments": "40 pages, 10 Figures, 4 Supplementary Figures, 2 Appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When making inferences concerning the environment, ground truthed data will\nfrequently be available as point referenced (geostatistical) observations that\nare clustered into multiple sites rather than uniformly spaced across the area\nof interest. In such situations, the similarity of the dominant processes\ninfluencing the observed data across sites and the accuracy with which models\nfitted to data from one site can predict data from another site provide\nvaluable information for scientists seeking to make inferences from these data.\nSuch information may motivate a more informed second round of modelling of the\ndata and also provides insight into the generality of the models developed and\nan indication of how these models may perform at predicting observations from\nother sites. We have investigated the geographic transferability of site\nspecific models and compared the results of using different implementations of\nsite specific effects in models for data combined from two sites. Since we have\naccess to data on a broad collection of environmental characteristics that each\nheld potential to aid the interpolation of our geostatistical response\nobservations we have investigated these issues within the framework of a\ncomputationally efficient method for variable selection when the number of\nexplanatory variables exceeds the number of observations. We have applied Least\nAbsolute Shrinkage Selection Operator (LASSO) regularized Multiple Linear\nRegression (MLR) as fitted by the computationally efficient Least Angle\nRegression algorithm. The response variable in our case study, soil carbon, is\nof interest as a potential location for the sequestration of atmospheric carbon\ndioxide and for its positive contribution to soil health and fertility.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jul 2016 07:51:06 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Fitzpatrick", "Benjamin R.", ""], ["Lamb", "David W.", ""], ["Mengersen", "Kerrie", ""]]}, {"id": "1608.00264", "submitter": "Mingyuan Zhou", "authors": "Mingyuan Zhou, Stefano Favaro, Stephen G Walker", "title": "Frequency of Frequencies Distributions and Size Dependent Exchangeable\n  Random Partitions", "comments": "To appear in the Journal of the American Statistical Association\n  (Theory and Methods). 26 pages + 17 page supplement, 19 figures. arXiv admin\n  note: text overlap with arXiv:1410.3155", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the fundamental problem of modeling the frequency of frequencies\n(FoF) distribution, this paper introduces the concept of a cluster structure to\ndefine a probability function that governs the joint distribution of a random\ncount and its exchangeable random partitions. A cluster structure, naturally\narising from a completely random measure mixed Poisson process, allows the\nprobability distribution of the random partitions of a subset of a population\nto be dependent on the population size, a distinct and motivated feature that\nmakes it more flexible than a partition structure. This allows it to model an\nentire FoF distribution whose structural properties change as the population\nsize varies. A FoF vector can be simulated by drawing an infinite number of\nPoisson random variables, or by a stick-breaking construction with a finite\nrandom number of steps. A generalized negative binomial process model is\nproposed to generate a cluster structure, where in the prior the number of\nclusters is finite and Poisson distributed, and the cluster sizes follow a\ntruncated negative binomial distribution. We propose a simple Gibbs sampling\nalgorithm to extrapolate the FoF vector of a population given the FoF vector of\na sample taken without replacement from the population. We illustrate our\nresults and demonstrate the advantages of the proposed models through the\nanalysis of real text, genomic, and survey data.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jul 2016 21:26:50 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Zhou", "Mingyuan", ""], ["Favaro", "Stefano", ""], ["Walker", "Stephen G", ""]]}, {"id": "1608.00538", "submitter": "Chiwoo Park", "authors": "Ali Esmaieeli Sikaroudi, David A. Welch, Taylor Woehl, Roland Faller,\n  James E. Evans, Nigel D. Browning, and Chiwoo Park", "title": "Directional Statistics of Preferential Orientations of Two Shapes in\n  Their Aggregate and Its Application to Study Preferential Attachment of\n  Nanoparticles", "comments": "28 pages; 7 Figures; 2 Tables", "journal-ref": "2018 Technometrics, 60:3, 332-344", "doi": "10.1080/00401706.2017.1366949", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nanoscientists have long conjectured that adjacent nanoparticles aggregate\nwith one another in certain preferential directions during a chemical synthesis\nof nanoparticles, which is referred to the oriented attachment. For the study\nof the oriented attachment, the microscopy and nanoscience communities have\nused dynamic electron microscopy for direct observations of nanoparticle\naggregation and have been so far relying on manual and qualitative analysis of\nthe observations. We propose a statistical approach for studying the oriented\nattachment quantitatively with multiple aggregation examples in imagery\nobservations. We abstract an aggregation by an event of two primary geometric\nobjects merging into a secondary geometric object. We use a point set\nrepresentation to describe the geometric features of the primary objects and\nthe secondary object, and formulated the alignment of two point sets to one\npoint set to estimate the orientation angles of the primary objects in the\nsecondary object. The estimated angles are used as data to estimate the\nprobability distribution of the orientation angles and test important\nhypotheses statistically. The proposed approach was applied for our motivating\nexample, which demonstrated that nanoparticles of certain geometries have\nindeed preferential orientations in their aggregates.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 19:33:17 GMT"}, {"version": "v2", "created": "Wed, 19 Jul 2017 18:05:08 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Sikaroudi", "Ali Esmaieeli", ""], ["Welch", "David A.", ""], ["Woehl", "Taylor", ""], ["Faller", "Roland", ""], ["Evans", "James E.", ""], ["Browning", "Nigel D.", ""], ["Park", "Chiwoo", ""]]}, {"id": "1608.00615", "submitter": "Jose A. Lopez-Salcedo", "authors": "Daniel Egea-Roca, Gonzalo Seco-Granados, Jos\\'e A. L\\'opez-Salcedo", "title": "Closed-form approximations for the performance upper bound of\n  inhomogeneous quadratic tests", "comments": null, "journal-ref": null, "doi": "10.1016/j.dsp.2018.02.012", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on inhomogeneous quadratic tests, which involve the sum of\na dependent non-central chi-square with a Gaussian random variable.\nUnfortunately, no closed-form expression is available for the statistical\ndistribution of the resulting random variable, thus hindering the analytical\ncharacterization of these tests in terms of probability of detection and\nprobability of false alarm. In order to circumvent this limitation, two\nclosed-form approximations are proposed in this work based on results from\nEdgeworth series expansions and Extreme Value Theory (EVT). The use of these\napproximations is shown through a specific case of study in the context of\nintegrity transient detection for Global Navigation Satellite Systems (GNSS).\nNumerical results are provided to assess the goodness of the proposed\napproximations, and to highlight their interest in real life applications.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 21:00:33 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Egea-Roca", "Daniel", ""], ["Seco-Granados", "Gonzalo", ""], ["L\u00f3pez-Salcedo", "Jos\u00e9 A.", ""]]}, {"id": "1608.00794", "submitter": "Lisa Turner", "authors": "Lisa Turner, Nedialko B. Dimitrov and Paul Fearnhead", "title": "Bayes Linear Methods for Large-Scale Network Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the problem of searching a large set of items, such as emails, for a\nsmall set which are relevant to a given query. This can be implemented in a\nsequential manner whereby we use knowledge from earlier items that we have\nscreened to help us choose future items in an informed way. Often the items we\nare searching have an underlying network structure: for example emails can be\nrelated to a network of participants, where an edge in the network relates to\nthe presence of a communication between those two participants. Recent work by\nDimitrov, Kress and Nevo has shown that using the information about the network\nstructure together with a modelling assumption that relevant items and\nparticipants are likely to cluster together, can greatly increase the rate of\nscreening relevant items. However their approach is computationally expensive\nand thus limited in applicability to small networks. Here we show how Bayes\nLinear methods provide a natural approach to modelling such data; that they\noutput posterior summaries that are most relevant to heuristic policies for\nchoosing future items; and that they can easily be applied to large-scale\nnetworks. Both on simulated data, and data from the Enron Corpus, Bayes Linear\napproaches are shown to be applicable to situations where the method of\nDimitrov et al. is infeasible; and give substantially better performance than\nmethods that ignore the network structure.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 12:54:53 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Turner", "Lisa", ""], ["Dimitrov", "Nedialko B.", ""], ["Fearnhead", "Paul", ""]]}, {"id": "1608.00874", "submitter": "Fabrizio Leisen", "authors": "Jim Griffin and Fabrizio Leisen", "title": "Modelling and computation using NCoRM mixtures for density regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalized compound random measures are flexible nonparametric priors for\nrelated distributions. We consider building general nonparametric regression\nmodels using normalized compound random measure mixture models. Posterior\ninference is made using a novel pseudo-marginal Metropolis-Hastings sampler for\nnormalized compound random measure mixture models. The algorithm makes use of a\nnew general approach to the unbiased estimation of Laplace functionals of\ncompound random measures (which includes completely random measures as a\nspecial case). The approach is illustrated on problems of density regression.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 15:47:42 GMT"}, {"version": "v2", "created": "Wed, 3 Aug 2016 10:41:56 GMT"}, {"version": "v3", "created": "Thu, 31 Aug 2017 10:21:58 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Griffin", "Jim", ""], ["Leisen", "Fabrizio", ""]]}, {"id": "1608.01201", "submitter": "Fabio Rapallo", "authors": "Flavio Mignone, Fabio Rapallo", "title": "Detection of outlying proportions", "comments": "15 pages, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a new method for detecting outliers in a set of\nproportions. It is based on the construction of a suitable two-way contingency\ntable and on the application of an algorithm for the detection of outlying\ncells in such table. We exploit the special structure of the relevant\ncontingency table to increase the efficiency of the method. The main properties\nof our algorithm, together with a guide for the choice of the parameters, are\ninvestigated through simulations, and in simple cases some theoretical\njustifications are provided. Several examples on synthetic data and an example\nbased on pseudo-real data from biological experiments demonstrate the good\nperformances of our algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 14:23:07 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Mignone", "Flavio", ""], ["Rapallo", "Fabio", ""]]}, {"id": "1608.01274", "submitter": "Daniel Kessler", "authors": "Daniel Kessler, Michael Angstadt, Chandra Sripada", "title": "Which Findings from the Functional Neuromaging Literature Can We Trust?", "comments": "All authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In their recent \"Cluster Failure\" paper, Eklund and colleagues cast doubt on\nthe accuracy of a widely used statistical test in functional neuroimaging.\nHere, we leverage nonparametric methods that control the false discovery rate\nto offer more nuanced, quantitative guidance about which findings in the\nexisting literature can be trusted. We show that, in the task studies examined\nby Eklund et al., most clusters originally reported to be significant are\nindeed trustworthy by the false discovery rate benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 18:18:23 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Kessler", "Daniel", ""], ["Angstadt", "Michael", ""], ["Sripada", "Chandra", ""]]}, {"id": "1608.01365", "submitter": "Kazuhiko Nishimura", "authors": "Jiyoung Kim, Satoshi Nakano, Kazuhiko Nishimura", "title": "Multifactor CES General Equilibrium: Models and Applications", "comments": null, "journal-ref": null, "doi": "10.1016/j.econmod.2017.01.024", "report-no": null, "categories": "stat.AP q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sector specific multifactor CES elasticity of substitution and the\ncorresponding productivity growths are jointly measured by regressing the\ngrowths of factor-wise cost shares against the growths of factor prices. We use\nlinked input-output tables for Japan and the Republic of Korea as the data\nsource for factor price and cost shares in two temporally distant states. We\nthen construct a multi-sectoral general equilibrium model using the system of\nestimated CES unit cost functions, and evaluate the economy-wide propagation of\nan exogenous productivity stimuli, in terms of welfare. Further, we examine the\ndifferences between models based on a priori elasticity such as Leontief and\nCobb-Douglas.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 21:33:54 GMT"}, {"version": "v2", "created": "Tue, 10 Jan 2017 06:55:09 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Kim", "Jiyoung", ""], ["Nakano", "Satoshi", ""], ["Nishimura", "Kazuhiko", ""]]}, {"id": "1608.01440", "submitter": "Bienvenue Kouwaye", "authors": "Bienvenue Kouwaye (SAMM)", "title": "Anopheles number prediction on environmental and climate variables using\n  Lasso and stratified two levels cross validation", "comments": "arXiv admin note: text overlap with arXiv:1606.07578,\n  arXiv:1511.01284", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with prediction of anopheles number using environmental and\nclimate variables. The variables selection is performed by an automatic machine\nlearning method based on Lasso and stratified two levels cross validation.\nSelected variables are debiased while the predictionis generated by simple GLM\n(Generalized linear model). Finally, the results reveal to be qualitatively\nbetter, at selection, the prediction,and the CPU time point of view than those\nobtained by B-GLM method.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 06:54:24 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Kouwaye", "Bienvenue", "", "SAMM"]]}, {"id": "1608.01566", "submitter": "Tom Reynkens", "authors": "Tom Reynkens, Roel Verbelen, Jan Beirlant, Katrien Antonio", "title": "Modelling Censored Losses Using Splicing: a Global Fit Strategy With\n  Mixed Erlang and Extreme Value Distributions", "comments": null, "journal-ref": "Insurance Math. Econom. 77 (2017) 65-77", "doi": "10.1016/j.insmatheco.2017.08.005", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In risk analysis, a global fit that appropriately captures the body and the\ntail of the distribution of losses is essential. Modelling the whole range of\nthe losses using a standard distribution is usually very hard and often\nimpossible due to the specific characteristics of the body and the tail of the\nloss distribution. A possible solution is to combine two distributions in a\nsplicing model: a light-tailed distribution for the body which covers light and\nmoderate losses, and a heavy-tailed distribution for the tail to capture large\nlosses. We propose a splicing model with a mixed Erlang (ME) distribution for\nthe body and a Pareto distribution for the tail. This combines the flexibility\nof the ME distribution with the ability of the Pareto distribution to model\nextreme values. We extend our splicing approach for censored and/or truncated\ndata. Relevant examples of such data can be found in financial risk analysis.\nWe illustrate the flexibility of this splicing model using practical examples\nfrom risk measurement.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 14:55:47 GMT"}, {"version": "v2", "created": "Fri, 23 Dec 2016 15:59:30 GMT"}, {"version": "v3", "created": "Thu, 27 Apr 2017 14:34:18 GMT"}, {"version": "v4", "created": "Fri, 11 Aug 2017 14:55:27 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Reynkens", "Tom", ""], ["Verbelen", "Roel", ""], ["Beirlant", "Jan", ""], ["Antonio", "Katrien", ""]]}, {"id": "1608.02221", "submitter": "Sergei Kucherenko", "authors": "Sergei Kucherenko, Shufang Song", "title": "Quantile based global sensitivity measures", "comments": "25 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New global sensitivity measures based on quantiles of the output are\nintroduced. Such measures can be used for global sensitivity analysis of\nproblems in which quantiles are explicitly the functions of interest and for\nidentification of variables which are the most important in achieving extreme\nvalues of the model output. It is proven that there is a link between\nintroduced measures and Sobol main effect sensitivity indices. Two different\nMonte Carlo estimators are considered. It is shown that the double loop\nreordering approach is much more efficient than the brute force estimator.\nSeveral test cases and practical case studies related to structural safety are\nused to illustrate the developed method. Results of numerical calculations show\nthe efficiency of the presented technique.\n", "versions": [{"version": "v1", "created": "Sun, 7 Aug 2016 14:20:49 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Kucherenko", "Sergei", ""], ["Song", "Shufang", ""]]}, {"id": "1608.02477", "submitter": "Saeid Haghighatshoar", "authors": "Saeid Haghighatshoar and Giuseppe Caire", "title": "Low-Complexity Massive MIMO Subspace Estimation and Tracking from\n  Low-Dimensional Projections", "comments": "13 pages, 7 figures. Submitted to IEEE Transactions on Signal\n  Processing. This paper is the revised version of the previous paper with the\n  same title", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive MIMO is a variant of multiuser MIMO, where the number of antennas $M$\nat the base-station is large, and generally much larger than the number of\nspatially multiplexed data streams to/from the users. It has been observed that\nin many realistic propagation scenarios as well as in spatially correlated\nchannel models used in standardizations, although the user channel vectors have\na very high-dim $M$, they lie on low-dim subspaces due to their limited angular\nspread. This low-dim subspace structure remains stable across many coherence\nblocks and can be exploited in several ways to improve the system performance.\nA main challenge, however, is to estimate this signal subspace from samples of\nusers' channel vectors as fast and efficiently as possible. In a recent work,\nwe addressed this problem and proposed a very effective novel algorithm\nreferred to as Approximate Maximum-Likelihood (AML), which was formulated as a\nsemi-definite program (SDP). In this paper, we address two problems left open\nin our previous work: computational complexity and tracking. The algorithm\nproposed in this paper is reminiscent of Multiple Measurement Vectors (MMV)\nproblem in Compressed Sensing and is proved to be equivalent to the AML\nAlgorithm for sufficiently dense angular grids. It has also a very low\ncomputational complexity and is able to track sharp transitions in the channel\nstatistics very quickly. Although mainly motivated by massive MIMO\napplications, our proposed algorithm is of independent interest in other\nrelated subspace estimation applications. We assess the estimation/tracking\nperformance of our proposed algorithm empirically via numerical simulations,\nespecially in practically relevant situations where a direct implementation of\nthe SDP would be infeasible in real-time. We also compare the performance of\nour algorithm with other related subspace estimation algorithms in the\nliterature.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 15:12:21 GMT"}, {"version": "v2", "created": "Thu, 1 Dec 2016 11:03:38 GMT"}, {"version": "v3", "created": "Mon, 24 Jul 2017 17:26:54 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Haghighatshoar", "Saeid", ""], ["Caire", "Giuseppe", ""]]}, {"id": "1608.03022", "submitter": "Oleg Melnikov", "authors": "Oleg Melnikov, Loren H. Raun, Katherine B. Ensor", "title": "Dynamic Principal Component Analysis: Identifying the Relationship\n  between Multiple Air Pollutants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dynamic nature of air quality chemistry and transport makes it difficult\nto identify the mixture of air pollutants for a region. In this study of air\nquality in the Houston metropolitan area we apply dynamic principal component\nanalysis (DPCA) to a normalized multivariate time series of daily concentration\nmeasurements of five pollutants (O3, CO, NO2, SO2, PM2.5) from January 1, 2009\nthrough December 31, 2011 for each of the 24 hours in a day. The resulting\ndynamic components are examined by hour across days for the 3 year period.\nDiurnal and seasonal patterns are revealed underlining times when DPCA performs\nbest and two principal components (PCs) explain most variability in the\nmultivariate series. DPCA is shown to be superior to static principal component\nanalysis (PCA) in discovery of linear relations among transformed pollutant\nmeasurements. DPCA captures the time-dependent correlation structure of the\nunderlying pollutants recorded at up to 34 monitoring sites in the region. In\nwinter mornings the first principal component (PC1) (mainly CO and NO2)\nexplains up to 70% of variability. Augmenting with the second principal\ncomponent (PC2) (mainly driven by SO2) the explained variability rises to 90%.\nIn the afternoon, O3 gains prominence in the second principal component. The\nseasonal profile of PCs' contribution to variance loses its distinction in the\nafternoon, yet cumulatively PC1 and PC2 still explain up to 65% of variability\nin ambient air data. DPCA provides a strategy for identifying the changing air\nquality profile for the region studied.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 01:51:17 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Melnikov", "Oleg", ""], ["Raun", "Loren H.", ""], ["Ensor", "Katherine B.", ""]]}, {"id": "1608.03024", "submitter": "Candace Berrett", "authors": "Philip A. White, Candace Berrett, E. Shannon Neeley-Tass, Michael G.\n  Findley", "title": "Modeling Efficiency of Foreign Aid Allocation in Malawi", "comments": null, "journal-ref": "The American Statistician, 73(4), 385-399, (2018)", "doi": "10.1080/00031305.2018.1470032", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Open Aid Malawi initiative has collected an unprecedented database that\nidentifies as much location-specific information as possible for each of over\n2500 individual foreign aid donations to Malawi since 2003. Ensuring efficient\nuse and distribution of that aid is important to donors and to Malawi citizens.\nHowever, because of individual donor goals and difficulty in tracking donor\ncoordination, determining presence or absence of efficient aid allocation is\ndifficult. We compare several Bayesian spatial generalized linear mixed models\nto relate aid allocation to various economic indicators within seven donation\nsectors. We find that the spatial gamma regression model best predicts current\naid allocation. Using this model, first we use inferences on coefficients to\nexamine whether or not there is evidence of efficient aid allocation within\neach sector. Second, we use this model to determine a more efficient aid\nallocation scenario and compare this scenario to the current allocation to\nprovide insight for future aid donations.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 01:55:52 GMT"}, {"version": "v2", "created": "Wed, 1 Nov 2017 23:51:39 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["White", "Philip A.", ""], ["Berrett", "Candace", ""], ["Neeley-Tass", "E. Shannon", ""], ["Findley", "Michael G.", ""]]}, {"id": "1608.03091", "submitter": "Damien McParland", "authors": "Damien McParland, Szymon Baron, Sarah O'Rourke, Denis Dowling, Eamonn\n  Ahearne and Andrew Parnell", "title": "Prediction of tool-wear in turning of medical grade cobalt chromium\n  molybdenum alloy (ASTM F75) using non-parametric Bayesian models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to estimating the effect of control parameters on\ntool wear rates and related changes in the three force components in turning of\nmedical grade Co-Cr-Mo (ASTM F75) alloy. Co-Cr-Mo is known to be a difficult to\ncut material which, due to a combination of mechanical and physical properties,\nis used for the critical structural components of implantable medical\nprosthetics. We run a designed experiment which enables us to estimate tool\nwear from feed rate and cutting speed, and constrain them using a Bayesian\nhierarchical Gaussian Process model which enables prediction of tool wear rates\nfor untried experimental settings. The predicted tool wear rates are non-linear\nand, using our models, we can identify experimental settings which optimise the\nlife of the tool. This approach has potential in the future for realtime\napplication of data analytics to machining processes.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 08:59:43 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["McParland", "Damien", ""], ["Baron", "Szymon", ""], ["O'Rourke", "Sarah", ""], ["Dowling", "Denis", ""], ["Ahearne", "Eamonn", ""], ["Parnell", "Andrew", ""]]}, {"id": "1608.03102", "submitter": "Richard Wilkinson", "authors": "Richard D. Wilkinson, Apostolos Kapranas, Ian C.W. Hardy", "title": "Detecting non-binomial sex allocation when developmental mortality\n  operates", "comments": "To appear, Journal of Theoretical Biology", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal sex allocation theory is one of the most intricately developed areas\nof evolutionary ecology. Under a range of conditions, particularly under\npopulation sub-division, selection favours sex being allocated to offspring\nnon-randomly, generating non-binomial variances of offspring group sex ratios.\nDetecting non-binomial sex allocation is complicated by stochastic\ndevelopmental mortality, as offspring sex can often only be identified on\nmaturity with the sex of non-maturing offspring remaining unknown. We show that\ncurrent approaches for detecting non-binomiality have limited ability to detect\nnon-binomial sex allocation when developmental mortality has occurred. We\npresent a new procedure using an explicit model of sex allocation and mortality\nand develop a Bayesian model selection approach (available as an R package). We\nuse the double and multiplicative binomial distributions to model over- and\nunder-dispersed sex allocation and show how to calculate Bayes factors for\ncomparing these alternative models to the null hypothesis of binomial sex\nallocation. The ability to detect non-binomial sex allocation is greatly\nincreased, particularly in cases where mortality is common. The use of Bayesian\nmethods allows for the quantification of the evidence in favour of each\nhypothesis, and our modelling approach provides an improved descriptive\ncapability over existing approaches. We use a simulation study to demonstrate\nsubstantial improvements in power for detecting non-binomial sex allocation in\nsituations where current methods fail, and we illustrate the approach in real\nscenarios using empirically obtained datasets on the sexual composition of\ngroups of gregarious parasitoid wasps.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 09:19:16 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Wilkinson", "Richard D.", ""], ["Kapranas", "Apostolos", ""], ["Hardy", "Ian C. W.", ""]]}, {"id": "1608.03191", "submitter": "Joseph Lucas", "authors": "Ricardo Henao and Joseph E. Lucas", "title": "Efficient model-based clustering with coalescents: Application to\n  multiple outcomes using medical records data", "comments": "arXiv admin note: substantial text overlap with arXiv:1204.4708", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a sequential Monte Carlo sampler for coalescent based Bayesian\nhierarchical clustering. The model is appropriate for multivariate non-\\iid\ndata and our approach offers a substantial reduction in computational cost when\ncompared to the original sampler. We also propose a quadratic complexity\napproximation that in practice shows almost no loss in performance compared to\nits counterpart. Our formulation leads to a greedy algorithm that exhibits\nperformance improvement over other greedy algorithms, particularly in small\ndata sets. We incorporate the Coalescent into a hierarchical regression model\nthat allows joint modeling of multiple correlated outcomes. The approach does\nnot require {\\em a priori} knowledge of either the degree or structure of the\ncorrelation and, as a byproduct, generates additional models for a subset of\nthe composite outcomes. We demonstrate the utility of the approach by\npredicting multiple different types of outcomes using medical records data from\na cohort of diabetic patients.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 14:44:51 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Henao", "Ricardo", ""], ["Lucas", "Joseph E.", ""]]}, {"id": "1608.03343", "submitter": "Julio Albinati", "authors": "Julio Albinati and Wagner Meira Jr and Gisele Lobo Pappa", "title": "An Accurate Gaussian Process-Based Early Warning System for Dengue Fever", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dengue fever is a mosquito-borne disease present in all Brazilian territory.\nBrazilian government, however, lacks an accurate early warning system to\nquickly predict future dengue outbreaks. Such system would help health\nauthorities to plan their actions and to reduce the impact of the disease in\nthe country. However, most attempts to model dengue fever use parametric models\nwhich enforce a specific expected behaviour and fail to capture the inherent\ncomplexity of dengue dynamics. Therefore, we propose a new Bayesian\nnon-parametric model based on Gaussian processes to design an accurate and\nflexible model that outperforms previous/standard techniques and can be\nincorporated into an early warning system, specially at cities from Southeast\nand Center-West regions. The model also helps understanding dengue dynamics in\nBrazil through the analysis of the covariance functions generated.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 01:45:48 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Albinati", "Julio", ""], ["Meira", "Wagner", "Jr"], ["Pappa", "Gisele Lobo", ""]]}, {"id": "1608.03532", "submitter": "L\\'aszl\\'o Gyarmati", "authors": "Laszlo Gyarmati, Rade Stanojevic", "title": "QPass: a Merit-based Evaluation of Soccer Passes", "comments": "2016 ACM KDD Workshop on Large-Scale Sports Analytics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative analysis of soccer players' passing ability focuses on\ndescriptive statistics without considering the players' real contribution to\nthe passing and ball possession strategy of their team. Which player is able to\nhelp the build-up of an attack, or to maintain the possession of the ball? We\nintroduce a novel methodology called QPass to answer questions like these\nquantitatively. Based on the analysis of an entire season, we rank the players\nbased on the intrinsic value of their passes using QPass. We derive an album of\npass trajectories for different gaming styles. Our methodology reveals a quite\ncounterintuitive paradigm: losing the ball possession could lead to better\nchances to win a game.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 12:54:57 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Gyarmati", "Laszlo", ""], ["Stanojevic", "Rade", ""]]}, {"id": "1608.03585", "submitter": "Matthias Poloczek", "authors": "Matthias Poloczek, Jialei Wang, and Peter I. Frazier", "title": "Warm Starting Bayesian Optimization", "comments": "To Appear in the Proc. of the 2016 Winter Simulation Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a framework for warm-starting Bayesian optimization, that reduces\nthe solution time required to solve an optimization problem that is one in a\nsequence of related problems. This is useful when optimizing the output of a\nstochastic simulator that fails to provide derivative information, for which\nBayesian optimization methods are well-suited. Solving sequences of related\noptimization problems arises when making several business decisions using one\noptimization model and input data collected over different time periods or\nmarkets. While many gradient-based methods can be warm started by initiating\noptimization at the solution to the previous problem, this warm start approach\ndoes not apply to Bayesian optimization methods, which carry a full metamodel\nof the objective function from iteration to iteration. Our approach builds a\njoint statistical model of the entire collection of related objective\nfunctions, and uses a value of information calculation to recommend points to\nevaluate.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 19:56:27 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Poloczek", "Matthias", ""], ["Wang", "Jialei", ""], ["Frazier", "Peter I.", ""]]}, {"id": "1608.03723", "submitter": "Andriy Olenko", "authors": "Yuriy Kozachenko, Andriy Olenko", "title": "Aliasing-truncation Errors in Sampling Approximations of Sub-Gaussian\n  Signals", "comments": "15 pages, 5 figures. arXiv admin note: text overlap with\n  arXiv:1606.01062", "journal-ref": null, "doi": "10.1109/TIT.2016.2597146", "report-no": null, "categories": "cs.IT math.IT math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article starts with new aliasing-truncation error upper bounds in the\nsampling theorem for non-bandlimited stochastic signals. Then, it investigates\n$L_p([0,T])$ approximations of sub-Gaussian random signals. Explicit truncation\nerror upper bounds are established. The obtained rate of convergence provides a\nconstructive algorithm for determining the sampling rate and the sample size in\nthe truncated Whittaker-Kotel'nikov-Shannon expansions to ensure the\napproximation of sub-Gaussian signals with given accuracy and reliability. Some\nnumerical examples are presented.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 08:57:27 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Kozachenko", "Yuriy", ""], ["Olenko", "Andriy", ""]]}, {"id": "1608.03735", "submitter": "Bilal Qureshi", "authors": "Bilal Qureshi, Faisal Kamiran, Asim Karim, Salvatore Ruggieri, Dino\n  Pedreschi", "title": "Causal Inference for Social Discrimination Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discovery of discriminatory bias in human or automated decision making is\na task of increasing importance and difficulty, exacerbated by the pervasive\nuse of machine learning and data mining. Currently, discrimination discovery\nlargely relies upon correlation analysis of decisions records, disregarding the\nimpact of confounding biases. We present a method for causal discrimination\ndiscovery based on propensity score analysis, a statistical tool for filtering\nout the effect of confounding variables. We introduce causal measures of\ndiscrimination which quantify the effect of group membership on the decisions,\nand highlight causal discrimination/favoritism patterns by learning regression\ntrees over the novel measures. We validate our approach on two real world\ndatasets. Our proposed framework for causal discrimination has the potential to\nenhance the transparency of machine learning with tools for detecting\ndiscriminatory bias both in the training data and in the learning algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 10:13:37 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2019 05:32:06 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Qureshi", "Bilal", ""], ["Kamiran", "Faisal", ""], ["Karim", "Asim", ""], ["Ruggieri", "Salvatore", ""], ["Pedreschi", "Dino", ""]]}, {"id": "1608.03787", "submitter": "Haakon Bakka", "authors": "Haakon Bakka, Jarno Vanhatalo, Janine Illian, Daniel Simpson,\n  H{\\aa}vard Rue", "title": "Non-stationary Gaussian models with physical barriers", "comments": "The new version contains major changes and new materials, including a\n  much more appropriate proof of existence of solution to the SPDE", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical tools in spatial statistics are stationary models, like the\nMat\\'ern field. However, in some applications there are boundaries, holes, or\nphysical barriers in the study area, e.g. a coastline, and stationary models\nwill inappropriately smooth over these features, requiring the use of a\nnon-stationary model.\n  We propose a new model, the Barrier model, which is different from the\nestablished methods as it is not based on the shortest distance around the\nphysical barrier, nor on boundary conditions. The Barrier model is based on\nviewing the Mat\\'ern correlation, not as a correlation function on the shortest\ndistance between two points, but as a collection of paths through a\nSimultaneous Autoregressive (SAR) model. We then manipulate these local\ndependencies to cut off paths that are crossing the physical barriers. To make\nthe new SAR well behaved, we formulate it as a stochastic partial differential\nequation (SPDE) that can be discretised to represent the Gaussian field, with a\nsparse precision matrix that is automatically positive definite.\n  The main advantage with the Barrier model is that the computational cost is\nthe same as for the stationary model. The model is easy to use, and can deal\nwith both sparse data and very complex barriers, as shown in an application in\nthe Finnish Archipelago Sea. Additionally, the Barrier model is better at\nreconstructing the modified Horseshoe test function than the standard models\nused in R-INLA.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 13:32:13 GMT"}, {"version": "v2", "created": "Thu, 26 Apr 2018 11:48:02 GMT"}, {"version": "v3", "created": "Sat, 12 Jan 2019 16:00:04 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Bakka", "Haakon", ""], ["Vanhatalo", "Jarno", ""], ["Illian", "Janine", ""], ["Simpson", "Daniel", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "1608.03855", "submitter": "Zaid Sawlan", "authors": "Marco Iglesias, Zaid Sawlan, Marco Scavino, Raul Tempone, Christopher\n  Wood", "title": "Bayesian inferences of the thermal properties of a wall using\n  temperature and heat flux measurements", "comments": null, "journal-ref": "International Journal of Heat and Mass Transfer 116C (2018) pp.\n  417-431", "doi": "10.1016/j.ijheatmasstransfer.2017.09.022", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The assessment of the thermal properties of walls is essential for accurate\nbuilding energy simulations that are needed to make effective energy-saving\npolicies. These properties are usually investigated through in-situ\nmeasurements of temperature and heat flux over extended time periods. The\none-dimensional heat equation with unknown Dirichlet boundary conditions is\nused to model the heat transfer process through the wall. In [F. Ruggeri, Z.\nSawlan, M. Scavino, R. Tempone, A hierarchical Bayesian setting for an inverse\nproblem in linear parabolic PDEs with noisy boundary conditions, Bayesian\nAnalysis 12 (2) (2017) 407--433], it was assessed the uncertainty about the\nthermal diffusivity parameter using different synthetic data sets. In this\nwork, we adapt this methodology to an experimental study conducted in an\nenvironmental chamber, with measurements recorded every minute from temperature\nprobes and heat flux sensors placed on both sides of a solid brick wall over a\nfive-day period. The observed time series are locally averaged, according to a\nsmoothing procedure determined by the solution of a criterion function\noptimization problem, to fit the required set of noise model assumptions.\nTherefore, after preprocessing, we can reasonably assume that the temperature\nand the heat flux measurements have stationary Gaussian noise and we can avoid\nworking with full covariance matrices. The results show that our technique\nreduces the bias error of the estimated parameters when compared to other\napproaches. Finally, we compute the information gain under two experimental\nsetups to recommend how the user can efficiently determine the duration of the\nmeasurement campaign and the range of the external temperature oscillation.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 17:28:50 GMT"}, {"version": "v2", "created": "Wed, 1 Mar 2017 08:05:10 GMT"}, {"version": "v3", "created": "Tue, 12 Sep 2017 10:51:06 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Iglesias", "Marco", ""], ["Sawlan", "Zaid", ""], ["Scavino", "Marco", ""], ["Tempone", "Raul", ""], ["Wood", "Christopher", ""]]}, {"id": "1608.03991", "submitter": "Mingyuan Zhou", "authors": "Siamak Zamani Dadaneh, Xiaoning Qian, Mingyuan Zhou", "title": "BNP-Seq: Bayesian Nonparametric Differential Expression Analysis of\n  Sequencing Count Data", "comments": "To appear in Journal of the American Statistical Association", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.GN stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We perform differential expression analysis of high-throughput sequencing\ncount data under a Bayesian nonparametric framework, removing sophisticated\nad-hoc pre-processing steps commonly required in existing algorithms. We\npropose to use the gamma (beta) negative binomial process, which takes into\naccount different sequencing depths using sample-specific negative binomial\nprobability (dispersion) parameters, to detect differentially expressed genes\nby comparing the posterior distributions of gene-specific negative binomial\ndispersion (probability) parameters. These model parameters are inferred by\nborrowing statistical strength across both the genes and samples. Extensive\nexperiments on both simulated and real-world RNA sequencing count data show\nthat the proposed differential expression analysis algorithms clearly\noutperform previously proposed ones in terms of the areas under both the\nreceiver operating characteristic and precision-recall curves.\n", "versions": [{"version": "v1", "created": "Sat, 13 Aug 2016 15:16:56 GMT"}, {"version": "v2", "created": "Tue, 2 May 2017 20:04:42 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Dadaneh", "Siamak Zamani", ""], ["Qian", "Xiaoning", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "1608.04253", "submitter": "Benjamin Fitzpatrick Mr", "authors": "Benjamin R. Fitzpatrick, David W. Lamb, Kerrie Mengersen", "title": "Ultrahigh Dimensional Variable Selection for Mapping Soil Carbon", "comments": "69 pages, 4 Figures", "journal-ref": "PLoS ONE, 2016, 11(9): e0162489", "doi": "10.1371/journal.pone.0162489", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern soil mapping is characterised by the need to interpolate samples of\ngeostatistical response observations and the availability of relatively large\nnumbers of environmental characteristics for consideration as covariates to aid\nthis interpolation. We demonstrate the efficiency of the Least Angle Regression\nalgorithm for Least Absolute Shrinkage and Selection Operator (LASSO) penalized\nmultiple linear regression at selecting covariates to aid the spatial\ninterpolation of geostatistical soil carbon observations under an ultrahigh\ndimensional scenario. Where an exhaustive search of the models that could be\nconstructed from 800 potential covariate terms and 60 observations would be\nprohibitively demanding, LASSO variable selection is accomplished with trivial\ncomputational investment.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 12:19:48 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Fitzpatrick", "Benjamin R.", ""], ["Lamb", "David W.", ""], ["Mengersen", "Kerrie", ""]]}, {"id": "1608.04300", "submitter": "Madan Kundu", "authors": "Madan G. Kundu and Suddhasatta Acharyya", "title": "Surrogacy of progression free survival for overall survival in\n  metastatic breast cancer studies: meta-analyses of published studies", "comments": null, "journal-ref": "Contemporary Clinical Trials, 2017", "doi": "10.1016/j.cct.2016.12.004", "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: PFS is often used as a surrogate endpoint for OS in metastatic\nbreast cancer studies. We have evaluated the association of treatment effect on\nPFS with significant HR$_{OS}$ (and how this association is affected by other\nfactors) in published prospective metastatic breast cancer studies.\n  Methods: A systematic literature search in PubMed identified prospective\nmetastatic breast cancer studies. Treatments effects on PFS were determined\nusing hazard ratio (HR$_{PFS}$), increase in median PFS ($\\Delta$MED$_{PFS}$)\nand % increase in median PFS (%$\\Delta$MED$_{PFS}$). Diagnostic accuracy of PFS\nmeasures (HR$_{PFS}$, $\\Delta$MED$_{PFS}$ and %$\\Delta$MED$_{PFS}$) in\npredicting significant HR$_{OS}$ was assessed using receiver operating\ncharacteristics (ROC) curves and classification trees approach.\n  Results: Seventy-three cases (i.e., treatment to control comparisons) from 64\nindividual publications were identified for the analyses. Of these, 16 cases\nreported significant treatment effect on HR$_{OS}$ at 5% level of significance.\nMedian number of deaths reported in these cases were 156. Area under the ROC\ncurve (AUC) for diagnostic measures as HR$_{PFS}$, $\\Delta$MED$_{PFS}$ and\n%$\\Delta$MED$_{PFS}$ were 0.69, 0.70 and 0.75, respectively. Classification\ntree results identified %$\\Delta$MED$_{PFS}$ and number of deaths as diagnostic\nmeasure for significant HR$_{OS}$. Only 7.9\\% (3/39) cases with\n$\\Delta$MED$_{PFS}$ shorter than 48.27\\% reported significant HR$_{OS}$. There\nwere 7 cases with $\\Delta$MED$_{PFS}$ of 48.27\\% or more and number of deaths\nreported as 227 or more -- of these 5 cases reported significant HR$_{OS}$.\n  Conclusion: %$\\Delta$MED$_{PFS}$ was found as better diagnostic measure for\nsignificant HR$_{OS}$. Our analysis results also suggest that consideration of\ntotal number of deaths may further improve its diagnostic performance.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 15:16:56 GMT"}, {"version": "v2", "created": "Fri, 2 Dec 2016 02:37:12 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Kundu", "Madan G.", ""], ["Acharyya", "Suddhasatta", ""]]}, {"id": "1608.04341", "submitter": "Wendy Chan", "authors": "Wendy Chan", "title": "Partial Identification of Treatment Effects for Generalizability", "comments": "Presented at SREE 2016, Washington, D.C", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent methods to improve generalizations from nonrandom samples typically\ninvoke assumptions such as the strong ignorability of sample selection that are\noften controversial in practice to derive point estimates. Rather than focus on\nthe point estimate based inferences, this article considers inferences on\npartially identified estimates from fewer and weaker assumptions. We extend\npartial identification methods to causal generalization with nonrandom samples\nby using a cluster randomized trial in education. Bounds on the population\naverage treatment effect are derived under four cases, two under no assumptions\non the data, and two that assume bounded sample variation and monotonicity of\nresponse. This approach is amenable to incorporating population data frames to\ntighten bounds on the population average treatment effect. Under the\nassumptions of bounded sample variation and monotonicity, the interval\nestimates of the average treatment effect provide sufficiently informative\nbounds to rule out large treatment effects, which are consistent with the point\nestimates from the experimental study. This illustrates that partial\nidentification methods can provide an alternative perspective to causal\ngeneralization in the absence of strong ignorability of sample selection.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 17:46:14 GMT"}, {"version": "v2", "created": "Wed, 4 Jan 2017 17:08:06 GMT"}, {"version": "v3", "created": "Thu, 5 Jan 2017 04:50:51 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Chan", "Wendy", ""]]}, {"id": "1608.04503", "submitter": "Hung Hung", "authors": "Hung Hung, Zhi-Yu Jou, Su-Yun Huang", "title": "Robust mislabel logistic regression without modeling mislabel\n  probabilities", "comments": "4 figures, 2 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logistic regression is among the most widely used statistical methods for\nlinear discriminant analysis. In many applications, we only observe possibly\nmislabeled responses. Fitting a conventional logistic regression can then lead\nto biased estimation. One common resolution is to fit a mislabel logistic\nregression model, which takes into consideration of mislabeled responses.\nAnother common method is to adopt a robust M-estimation by down-weighting\nsuspected instances. In this work, we propose a new robust mislabel logistic\nregression based on gamma-divergence. Our proposal possesses two advantageous\nfeatures: (1) It does not need to model the mislabel probabilities. (2) The\nminimum gamma-divergence estimation leads to a weighted estimating equation\nwithout the need to subtract any bias correction term, i.e., it is\nautomatically bias corrected. These properties make the proposed gamma-logistic\nregression more robust in model fitting and more intuitive for model\ninterpretation through a simple weighting scheme. Our method is also easy to\nimplement, and two types of algorithms are included. Simulation and real data\napplication results are presented to demonstrate the performance of\ngamma-logistic.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 07:22:13 GMT"}, {"version": "v2", "created": "Mon, 20 Feb 2017 02:26:56 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Hung", "Hung", ""], ["Jou", "Zhi-Yu", ""], ["Huang", "Su-Yun", ""]]}, {"id": "1608.04556", "submitter": "Jan Lorenz", "authors": "Jan Lorenz and Christoph Brauer and Dirk A. Lorenz", "title": "Rank-optimal weighting or \"How to be best in the OECD Better Life\n  Index?\"", "comments": "16 pages, 6 figures", "journal-ref": null, "doi": "10.1007/s11205-016-1416-0", "report-no": null, "categories": "math.OC q-fin.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method of rank-optimal weighting which can be used to explore\nthe best possible position of a subject in a ranking based on a composite\nindicator by means of a mathematical optimization problem. As an example, we\nexplore the dataset of the OECD Better Life Index and compute for each country\na weight vector which brings it as far up in the ranking as possible with the\ngreatest advance of the immediate rivals. The method is able to answer the\nquestion \"What is the best possible rank a country can achieve with a given set\nof weighted indicators?\" Typically, weights in composite indicators are\njustified normatively and not empirically. Our approach helps to give bounds on\nwhat is achievable by such normative judgments from a purely output-oriented\nand strongly competitive perspective. The method can serve as a basis for exact\nbounds in sensitivity analysis focused on ranking positions.\n  In the OECD Better Life Index data we find that 19 out the 36 countries in\nthe OECD Better Life Index 2014 can be brought to the top of the ranking by\nspecific weights. We give a table of weights for each country which brings it\nto its highest possible position. Many countries achieve their best rank by\nfocusing on their strong dimensions and setting the weights of many others to\nzero. Although setting dimensions to zero is possible in the OECD's online\ntool, this contradicts the idea of better life being multidimensional in\nessence. We discuss modifications of the optimization problem which could take\nthis into account, e.g. by allowing only a minimal weight of one.\n  Methods to find rank-optimal weights can be useful for various\nmultidimensional datasets like the ones used to rank universities or employers.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 11:45:12 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Lorenz", "Jan", ""], ["Brauer", "Christoph", ""], ["Lorenz", "Dirk A.", ""]]}, {"id": "1608.04585", "submitter": "Evgeny Burnaev", "authors": "Evgeny Burnaev and Vladislav Ishimtsev", "title": "Conformalized density- and distance-based anomaly detection in\n  time-series data", "comments": "9 pages, 3 figures, conference proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomalies (unusual patterns) in time-series data give essential, and often\nactionable information in critical situations. Examples can be found in such\nfields as healthcare, intrusion detection, finance, security and flight safety.\nIn this paper we propose new conformalized density- and distance-based anomaly\ndetection algorithms for a one-dimensional time-series data. The algorithms use\na combination of a feature extraction method, an approach to assess a score\nwhether a new observation differs significantly from a previously observed\ndata, and a probabilistic interpretation of this score based on the conformal\nparadigm.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 13:32:05 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Burnaev", "Evgeny", ""], ["Ishimtsev", "Vladislav", ""]]}, {"id": "1608.04615", "submitter": "Joseph Futoma", "authors": "Joseph Futoma, Mark Sendak, C. Blake Cameron, Katherine Heller", "title": "Scalable Modeling of Multivariate Longitudinal Data for Prediction of\n  Chronic Kidney Disease Progression", "comments": "Presented at 2016 Machine Learning and Healthcare Conference (MLHC\n  2016), Los Angeles, CA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction of the future trajectory of a disease is an important challenge\nfor personalized medicine and population health management. However, many\ncomplex chronic diseases exhibit large degrees of heterogeneity, and\nfurthermore there is not always a single readily available biomarker to\nquantify disease severity. Even when such a clinical variable exists, there are\noften additional related biomarkers routinely measured for patients that may\nbetter inform the predictions of their future disease state. To this end, we\npropose a novel probabilistic generative model for multivariate longitudinal\ndata that captures dependencies between multivariate trajectories. We use a\nGaussian process based regression model for each individual trajectory, and\nbuild off ideas from latent class models to induce dependence between their\nmean functions. We fit our method using a scalable variational inference\nalgorithm to a large dataset of longitudinal electronic patient health records,\nand find that it improves dynamic predictions compared to a recent state of the\nart method. Our local accountable care organization then uses the model\npredictions during chart reviews of high risk patients with chronic kidney\ndisease.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 14:30:07 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Futoma", "Joseph", ""], ["Sendak", "Mark", ""], ["Cameron", "C. Blake", ""], ["Heller", "Katherine", ""]]}, {"id": "1608.04772", "submitter": "Jessica Lovelace Rainbolt", "authors": "Jessica Lovelace Rainbolt and Michael Schmitt", "title": "The Use of Minimal Spanning Trees in Particle Physics", "comments": "29 pages, 30 figures", "journal-ref": null, "doi": "10.1088/1748-0221/12/02/P02009", "report-no": "nuhep-ex/16-04", "categories": "stat.AP hep-ex physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimal spanning trees (MSTs) have been used in cosmology and astronomy to\ndistinguish distributions of points in a multi-dimensional space. They are\nessentially unknown in particle physics, however. We briefly define MSTs and\nillustrate their properties through a series of examples. We show how they\nmight be applied to study a typical event sample from a collider experiment and\nconclude that MSTs may prove useful in distinguishing different classes of\nevents.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 20:47:54 GMT"}, {"version": "v2", "created": "Fri, 20 Jan 2017 17:30:03 GMT"}, {"version": "v3", "created": "Tue, 7 Feb 2017 18:17:47 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Rainbolt", "Jessica Lovelace", ""], ["Schmitt", "Michael", ""]]}, {"id": "1608.04910", "submitter": "Christoph Kurz", "authors": "Christoph Kurz", "title": "Tweedie distributions for fitting semicontinuous health care utilization\n  cost data", "comments": null, "journal-ref": null, "doi": "10.1186/s12874-017-0445-y", "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore a statistical distribution that can simultaneously model the\nprobability of zero outcome for non-users of health care utilization and\ncontinuous costs for users. We compare this distribution to other com- monly\nused models on example data and show that it fits cost data well and has some\nappealing properties that provide flexible use.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 09:49:09 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Kurz", "Christoph", ""]]}, {"id": "1608.05127", "submitter": "Vikas Chawla", "authors": "Vikas Chawla, Hsiang Sing Naik, Adedotun Akintayo, Dermot Hayes,\n  Patrick Schnable, Baskar Ganapathysubramanian, Soumik Sarkar", "title": "A Bayesian Network approach to County-Level Corn Yield Prediction using\n  historical data and expert knowledge", "comments": "8 pages, In Proceedings of the 22nd ACM SIGKDD Workshop on Data\n  Science for Food, Energy and Water , 2016 (San Francisco, CA, USA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crop yield forecasting is the methodology of predicting crop yields prior to\nharvest. The availability of accurate yield prediction frameworks have enormous\nimplications from multiple standpoints, including impact on the crop commodity\nfutures markets, formulation of agricultural policy, as well as crop insurance\nrating. The focus of this work is to construct a corn yield predictor at the\ncounty scale. Corn yield (forecasting) depends on a complex, interconnected set\nof variables that include economic, agricultural, management and meteorological\nfactors. Conventional forecasting is either knowledge-based computer programs\n(that simulate plant-weather-soil-management interactions) coupled with\ntargeted surveys or statistical model based. The former is limited by the need\nfor painstaking calibration, while the latter is limited to univariate analysis\nor similar simplifying assumptions that fail to capture the complex\ninterdependencies affecting yield. In this paper, we propose a data-driven\napproach that is \"gray box\" i.e. that seamlessly utilizes expert knowledge in\nconstructing a statistical network model for corn yield forecasting. Our\nmultivariate gray box model is developed on Bayesian network analysis to build\na Directed Acyclic Graph (DAG) between predictors and yield. Starting from a\ncomplete graph connecting various carefully chosen variables and yield, expert\nknowledge is used to prune or strengthen edges connecting variables.\nSubsequently the structure (connectivity and edge weights) of the DAG that\nmaximizes the likelihood of observing the training data is identified via\noptimization. We curated an extensive set of historical data (1948-2012) for\neach of the 99 counties in Iowa as data to train the model.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 23:30:04 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Chawla", "Vikas", ""], ["Naik", "Hsiang Sing", ""], ["Akintayo", "Adedotun", ""], ["Hayes", "Dermot", ""], ["Schnable", "Patrick", ""], ["Ganapathysubramanian", "Baskar", ""], ["Sarkar", "Soumik", ""]]}, {"id": "1608.05292", "submitter": "Paul Birrell", "authors": "Paul J Birrell, Lorenz Wernisch, Brian D M Tom, Leonhard Held, Gareth\n  O Roberts, Richard G Pebody and Daniela De Angelis", "title": "Efficient real-time monitoring of an emerging influenza epidemic: how\n  feasible?", "comments": "30 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A prompt public health response to a new epidemic relies on the ability to\nmonitor and predict its evolution in real time as data accumulate. The 2009\nA/H1N1 outbreak in the UK revealed pandemic data as noisy, contaminated,\npotentially biased, and originating from multiple sources. This seriously\nchallenges the capacity for real-time monitoring. Here we assess the\nfeasibility of real-time inference based on such data by constructing an\nanalytic tool combining an age-stratified SEIR transmission model with various\nobservation models describing the data generation mechanisms. As batches of\ndata become available, a sequential Monte Carlo (SMC) algorithm is developed to\nsynthesise multiple imperfect data streams, iterate epidemic inferences and\nassess model adequacy amidst a rapidly evolving epidemic environment,\nsubstantially reducing computation time in comparison to standard MCMC, to\nensure timely delivery of real-time epidemic assessments. In application to\nsimulated data designed to mimic the 2009 A/H1N1 epidemic, SMC is shown to have\nadditional benefits in terms of assessing predictive performance and coping\nwith parameter non-identifiability.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 15:18:12 GMT"}, {"version": "v2", "created": "Thu, 28 Sep 2017 04:16:52 GMT"}, {"version": "v3", "created": "Fri, 3 May 2019 13:34:06 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Birrell", "Paul J", ""], ["Wernisch", "Lorenz", ""], ["Tom", "Brian D M", ""], ["Held", "Leonhard", ""], ["Roberts", "Gareth O", ""], ["Pebody", "Richard G", ""], ["De Angelis", "Daniela", ""]]}, {"id": "1608.05428", "submitter": "Wagner Hugo Bonat Bonat W. H.", "authors": "Wagner Bonat and Jesus Olivero and Maria Grande-Vega and Miguel\n  F\\'arfan and John Fa", "title": "Modelling the covariance structure in marginal multivariate count\n  models: Hunting in Bioko Island", "comments": "24 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a flexible statistical modelling framework to deal with\nmultivariate count data along with longitudinal and repeated measures\nstructures. The covariance structure for each response variable is defined in\nterms of a covariance link function combined with a matrix linear predictor\ninvolving known matrices. To specify the joint covariance matrix for the\nmultivariate response vector the generalized Kronecker product is employed. The\ncount nature of the data is taken into account by means of the power dispersion\nfunction associated with the Poisson-Tweedie distribution. Furthermore, the\nscore information criterion is extended for selecting the components of the\nmatrix linear predictor. We analyse a dataset consisting of prey animals (the\nmain hunted species, the blue duiker \\textit{Philantomba monticola} and other\ntaxa) shot or snared for bushmeat by $52$ commercial hunters over a $33$-month\nperiod in Pico Basil\\'e, Bioko Island, Equatorial Guinea. By taking into\naccount the severely unbalanced repeated measures and longitudinal structures\ninduced by the hunters and a set of potential covariates (which in turn affect\nthe mean and covariance structures), our method can be used to indicate whether\nthere was statistical evidence of a decline in blue duikers and other species\nhunted during the study period. Determining whether observed drops in the\nnumber of animals hunted are indeed true is crucial to assess whether species\ndepletion effects are taking place in exploited areas anywhere in the world. We\nsuggest that our method can be used to more accurately understand the\ntrajectories of animals hunted for commercial or subsistence purposes, and\nestablish clear policies to ensure sustainable hunting practices.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 20:39:25 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Bonat", "Wagner", ""], ["Olivero", "Jesus", ""], ["Grande-Vega", "Maria", ""], ["F\u00e1rfan", "Miguel", ""], ["Fa", "John", ""]]}, {"id": "1608.05498", "submitter": "Johanna F. Ziegel", "authors": "Natalia Nolde and Johanna F. Ziegel", "title": "Elicitability and backtesting: Perspectives for banking regulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional forecasts of risk measures play an important role in internal\nrisk management of financial institutions as well as in regulatory capital\ncalculations. In order to assess forecasting performance of a risk measurement\nprocedure, risk measure forecasts are compared to the realized financial losses\nover a period of time and a statistical test of correctness of the procedure is\nconducted. This process is known as backtesting. Such traditional backtests are\nconcerned with assessing some optimality property of a set of risk measure\nestimates. However, they are not suited to compare different risk estimation\nprocedures. We investigate the proposal of comparative backtests, which are\nbetter suited for method comparisons on the basis of forecasting accuracy, but\nnecessitate an elicitable risk measure. We argue that supplementing traditional\nbacktests with comparative backtests will enhance the existing trading book\nregulatory framework for banks by providing the correct incentive for accuracy\nof risk measure forecasts. In addition, the comparative backtesting framework\ncould be used by banks internally as well as by researchers to guide selection\nof forecasting methods. The discussion focuses on three risk measures,\nValue-at-Risk, expected shortfall and expectiles, and is supported by a\nsimulation study and data analysis.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 05:41:39 GMT"}, {"version": "v2", "created": "Tue, 21 Feb 2017 09:07:24 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Nolde", "Natalia", ""], ["Ziegel", "Johanna F.", ""]]}, {"id": "1608.05501", "submitter": "Hugo Gabriel Eyherabide Dr", "authors": "Hugo Gabriel Eyherabide", "title": "Disambiguating the role of noise correlations when decoding neural\n  populations together", "comments": "To improve readability, this version has more material, more\n  explanations, more figures, less symbols, and more demonstrations than the\n  previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.IT math.IT q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most controversial problems in neural decoding is quantifying the\ninformation loss caused by ignoring noise correlations during optimal brain\ncomputations. For more than a decade, the measure here called $ \\Delta I^{DL} $\nhas been believed exact. However, we have recently shown that it can exceed the\ninformation loss $ \\Delta I^{B} $ caused by optimal decoders constructed\nignoring noise correlations. Unfortunately, the different information notions\nunderlying $ \\Delta I^{DL} $ and $ \\Delta I^{B} $, and the putative rigorous\ninformation-theoretical derivation of $ \\Delta I^{DL} $, both render unclear\nwhether those findings indicate either flaws in $ \\Delta I^{DL} $ or major\ndepartures from traditional relations between information and decoding. Here we\nresolve this paradox and prove that, under certain conditions, observing $\n\\Delta I^{DL} {>}\\Delta I^{B} $ implies that $ \\Delta I^{DL} $ is flawed.\nMotivated by this analysis, we test both measures using neural populations that\ntransmit independent information. Our results show that $ \\Delta I^{DL} $ may\ndeem noise correlations more important when decoding the populations together\nthan when decoding them in parallel, whereas the opposite may occur for $\n\\Delta I^{B} $. We trace these phenomena back, for $ \\Delta I^{B} $, to the\nchoice of tie-breaking rules, and for $ \\Delta I^{DL} $, to unforeseen\nlimitations within its information-theoretical foundations. Our study\ncontributes with better estimates that potentially improve theoretical and\nexperimental inferences currently drawn from $ \\Delta I^{DL} $ without noticing\nthat it may constitute an upper bound. On the practical side, our results\npromote the design of optimal decoding algorithms and neuroprosthetics without\nrecording noise correlations, thereby saving experimental and computational\nresources.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 06:00:22 GMT"}, {"version": "v2", "created": "Fri, 13 Jan 2017 10:22:28 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Eyherabide", "Hugo Gabriel", ""]]}, {"id": "1608.05533", "submitter": "Adria Caballe", "authors": "Adria Caballe, Natalia Bochkina, Claus Mayer", "title": "Joint Estimation of Sparse Networks with application to Paired Gene\n  Expression data", "comments": "34 pages, 10 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a method to jointly estimate sparse precision matrices and their\nunderlying graph structures using dependent high-dimensional datasets. We\npresent a penalized maximum likelihood estimator which encourages both sparsity\nand similarity in the estimated precision matrices where tuning parameters are\nautomatically selected by controlling the expected number of false positive\nedges. We also incorporate an extra step to remove edges which represent an\noverestimation of triangular motifs. We conduct a simulation study to show that\nthe proposed methodology presents consistent results for different combinations\nof sample size and dimension. Then, we apply the suggested approaches to a\nhigh-dimensional real case study of gene expression data with samples in two\nmedical conditions, healthy and colon cancer tissues, to estimate a common\nnetwork of genes as well as the differentially connected genes that are\nimportant to the disease. We find denser graph structures for healthy samples\nthan for tumor samples, with groups of genes interacting together in the shape\nof clusters.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 08:42:01 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Caballe", "Adria", ""], ["Bochkina", "Natalia", ""], ["Mayer", "Claus", ""]]}, {"id": "1608.05583", "submitter": "Alison Parton", "authors": "Alison Parton, Paul G. Blackwell, Anna Skarin", "title": "Bayesian inference for continuous time animal movement based on steps\n  and turns", "comments": "8 pages, 2 figures, BAYSM 2016", "journal-ref": "Bayesian Statistics in Action: BAYSM 2016, Florence, Italy, June\n  19-21 (2017) Springer vol.194", "doi": "10.1007/978-3-319-54084-9", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although animal locations gained via GPS, etc. are typically observed on a\ndiscrete time scale, movement models formulated in continuous time are\npreferable in order to avoid the struggles experienced in discrete time when\nfaced with irregular observations or the prospect of comparing analyses on\ndifferent time scales. A class of models able to emulate a range of movement\nideas are defined by representing movement as a combination of stochastic\nprocesses describing both speed and bearing. A method for Bayesian inference\nfor such models is described through the use of a Markov chain Monte Carlo\napproach. Such inference relies on an augmentation of the animal's locations in\ndiscrete time that have been observed with error, with a more detailed movement\npath gained via simulation techniques. Analysis on real data on an individual\nreindeer (Rangifer tarandus) illustrates the presented methods.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 12:41:33 GMT"}, {"version": "v2", "created": "Fri, 4 Nov 2016 14:11:42 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Parton", "Alison", ""], ["Blackwell", "Paul G.", ""], ["Skarin", "Anna", ""]]}, {"id": "1608.05655", "submitter": "Mark Risser", "authors": "Mark D. Risser, Catherine A. Calder, Veronica J. Berrocal, and Candace\n  Berrett", "title": "Nonstationary Spatial Prediction of Soil Organic Carbon: Implications\n  for Stock Assessment Decision Making", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Rapid Carbon Assessment (RaCA) project was conducted by the US Department\nof Agriculture's National Resources Conservation Service between 2010-2012 in\norder to provide contemporaneous measurements of soil organic carbon (SOC)\nacross the US. Despite the broad extent of the RaCA data collection effort,\ndirect observations of SOC are not available at the high spatial resolution\nneeded for studying carbon storage in soil and its implications for important\nproblems in climate science and agriculture. As a result, there is a need for\npredicting SOC at spatial locations not included as part of the RaCA project.\nIn this paper, we compare spatial prediction of SOC using a subset of the RaCA\ndata for a variety of statistical methods. We investigate the performance of\nmethods with off-the-shelf software available (both stationary and\nnonstationary) as well as a novel nonstationary approach based on partitioning\nrelevant spatially-varying covariate processes. Our new method addresses open\nquestions regarding (1) how to partition the spatial domain for\nsegmentation-based nonstationary methods, (2) incorporating partially observed\ncovariates into a spatial model, and (3) accounting for uncertainty in the\npartitioning. In applying the various statistical methods we find that there\nare minimal differences in out-of-sample criteria for this particular data set,\nhowever, there are major differences in maps of uncertainty in SOC predictions.\nWe argue that the spatially-varying measures of prediction uncertainty produced\nby our new approach are valuable to decision makers, as they can be used to\nbetter benchmark mechanistic models, identify target areas for soil restoration\nprojects, and inform carbon sequestration projects.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 16:25:49 GMT"}, {"version": "v2", "created": "Thu, 29 Sep 2016 17:40:13 GMT"}, {"version": "v3", "created": "Mon, 28 Aug 2017 22:07:52 GMT"}, {"version": "v4", "created": "Fri, 8 Jun 2018 17:24:17 GMT"}, {"version": "v5", "created": "Mon, 11 Jun 2018 01:34:44 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Risser", "Mark D.", ""], ["Calder", "Catherine A.", ""], ["Berrocal", "Veronica J.", ""], ["Berrett", "Candace", ""]]}, {"id": "1608.05670", "submitter": "Michal Pe\\v{s}ta PhD", "authors": "Barbora Pe\\v{s}tov\\'a and Michal Pe\\v{s}ta", "title": "Change Point in Panel Data with Small Fixed Panel Size: Ratio and\n  Non-Ratio Test Statistics", "comments": "arXiv admin note: substantial text overlap with arXiv:1509.01291", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main goal is to develop and, consequently, compare stochastic methods for\ndetection whether a structural change in panel data occurred at some unknown\ntime or not. Panel data of our interest consist of a moderate or relatively\nlarge number of panels, while the panels contain a small number of\nobservations. Testing procedures to detect a possible common change in means of\nthe panels are established. Ratio and non-ratio type test statistics are\nconsidered. Their asymptotic distributions under the no change null hypothesis\nare derived. Moreover, we prove the consistency of the tests under the\nalternative. The main advantage of the ratio type statistics compared to the\nnon-ratio ones is that the variance of the observations neither has to be known\nnor estimated. A simulation study reveals that the proposed ratio statistic\noutperforms the non-ratio one by keeping the significance level under the null,\nmainly when stronger dependence within the panel is taken into account.\nHowever, the non-ratio statistic rejects the null in the simulations more often\nthan it should, which yields higher power compared to the ratio statistic.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 16:55:06 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Pe\u0161tov\u00e1", "Barbora", ""], ["Pe\u0161ta", "Michal", ""]]}, {"id": "1608.06048", "submitter": "Ajinkya More", "authors": "Ajinkya More", "title": "Survey of resampling techniques for improving classification performance\n  in unbalanced datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of classification problems need to deal with data imbalance between\nclasses. Often it is desired to have a high recall on the minority class while\nmaintaining a high precision on the majority class. In this paper, we review a\nnumber of resampling techniques proposed in literature to handle unbalanced\ndatasets and study their effect on classification performance.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 04:27:28 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["More", "Ajinkya", ""]]}, {"id": "1608.06257", "submitter": "Mark Handschy", "authors": "Mark Handschy, Stephen Rose, and Jay Apt", "title": "Reduction of wind power variability through geographic diversity", "comments": "12 pages, 9 figures, Chapter 12 from Variable Renewable Energy and\n  the Electricity Grid, by Jay Apt and Paulina Jaramillo, RFF/Routledge. 2014", "journal-ref": null, "doi": "10.4324/9781315848709", "report-no": null, "categories": "physics.ao-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The variability of wind-generated electricity can be reduced by aggregating\nthe outputs of wind generation plants spread over a large geographic area. In\nthis chapter we utilize Monte Carlo simulations to investigate upper bounds on\nthe degree of achievable smoothing and clarify how the degree of smoothing\ndepends on the number of plants and on the size of the geographic area over\nwhich they are spread.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 18:38:47 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Handschy", "Mark", ""], ["Rose", "Stephen", ""], ["Apt", "Jay", ""]]}, {"id": "1608.06309", "submitter": "Nicole Dalzell", "authors": "Nicole M. Dalzell and Jerome P. Reiter", "title": "Regression Modeling and File Matching Using Possibly Erroneous Matching\n  Variables", "comments": "Supplementary material follows main text; text clarifications;\n  results unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many analyses require linking records from two databases comprising\noverlapping sets of individuals. In the absence of unique identifiers, the\nlinkage procedure often involves matching on a set of categorical variables,\nsuch as demographics, common to both files. Typically, however, the resulting\nmatches are inexact: some cross-classifications of the matching variables do\nnot generate unique links across files. Further, the variables used for\nmatching can be subject to reporting errors, which introduce additional\nuncertainty in analyses. We present a Bayesian file matching methodology\ndesigned to estimate regression models and match records simultaneously when\ncategorical variables used for matching are subject to errors. The method\nrelies on a hierarchical model that includes (1) the regression of interest\ninvolving variables from the two files given a vector indicating the links, (2)\na model for the linking vector given the true values of the variables used for\nmatching, (3) a model for reported values of the variables used for matching\ngiven their true values, and (4) a model for the true values of the variables\nused for matching. We describe algorithms for sampling from the posterior\ndistribution of the model. We illustrate the methodology using artificial data\nand data from education records in the state of North Carolina.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 20:38:57 GMT"}, {"version": "v2", "created": "Tue, 6 Sep 2016 13:50:17 GMT"}, {"version": "v3", "created": "Thu, 8 Jun 2017 18:42:15 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Dalzell", "Nicole M.", ""], ["Reiter", "Jerome P.", ""]]}, {"id": "1608.06367", "submitter": "Viswanathan Arunachalam", "authors": "Viswanathan Arunachalam", "title": "Generalized Shock Model Based On The Frequency of Shocks: A Simple\n  Approach", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a $\\delta-$shock model, a system subject to randomly occurring shocks, the\nsystem fails when the time between two successive shocks lies below a threshold\n$\\delta$. In this note, we study the generalization of this model where such\n$\\delta-$shocks are accumulated and the system fails on the occurrence of\n$k^{th}$ such a $\\delta-$shock. The probability distribution of the system\nfailure time and the statistical characteristics are explicitly obtained.\nNormal approximation to the failure time distribution is proposed.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 03:10:52 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Arunachalam", "Viswanathan", ""]]}, {"id": "1608.06416", "submitter": "Elnura Irmatova", "authors": "Elnura Irmatova", "title": "RELARM: A rating model based on relative PCA attributes and k-means\n  clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following widely used in visual recognition concept of relative attributes,\nthe article establishes definition of the relative PCA attributes for a class\nof objects defined by vectors of their parameters. A new rating model (RELARM)\nis built using relative PCA attribute ranking functions for rating object\ndescription and k-means clustering algorithm. Rating assignment of each rating\nobject to a rating category is derived as a result of cluster centers\nprojection on the specially selected rating vector. Empirical study has shown a\nhigh level of approximation to the existing S & P, Moody's and Fitch ratings.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 08:25:15 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Irmatova", "Elnura", ""]]}, {"id": "1608.06552", "submitter": "Ana Nora Donaldson", "authors": "Nicholas Donaldson, Nora Donaldson and Grace Yang", "title": "What did the 2016 Brexit referendum data really say?", "comments": "Version 3, One author added. Also two minor changes in the last\n  paragraph of Conclusion (introducing the word \"essentially\" in the last\n  sentences; replacing \"conclusively\")", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Brexit referendum took place in the UK in June, 2016. The unweighted\npercentage of leavers over the whole population was 51.9%. In this paper,\nfirst, we demonstrate that a 52%-48% split represents only a difference that is\nnot sufficiently different from a 50-50 split to claim a majority for either\nside. Second, and most important, on this basis of the unweighted percentage,\nstatement like: The country voted to leave the EU, were made. When a statement\nabout a population is made based on a subset of it (the turnout rate for Brexit\nwas only 72% and therefore 37% of the eligible population voted Leave), it\ncomes with an element of uncertainty that should not be ignored. The unweighted\naverage disregards, not only between-region heterogeneity but also\nwithin-region variability. Our analysis, controlling for both, finds that the\nsplit of the Brexit is of negligible material significance and do not indicate\nmajority for either side.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 15:49:12 GMT"}, {"version": "v2", "created": "Mon, 3 Apr 2017 12:08:52 GMT"}, {"version": "v3", "created": "Wed, 9 Aug 2017 02:07:02 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Donaldson", "Nicholas", ""], ["Donaldson", "Nora", ""], ["Yang", "Grace", ""]]}, {"id": "1608.06667", "submitter": "Sonja Petrovic", "authors": "Vishesh Karwa, Sonja Petrovi\\'c", "title": "Coauthorship and citation networks for statisticians: Comment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.DL cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a comment on the paper arXiv:1410.2840 by Ji and Jin, to appear in\nthe AOAS.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 23:28:26 GMT"}], "update_date": "2016-08-25", "authors_parsed": [["Karwa", "Vishesh", ""], ["Petrovi\u0107", "Sonja", ""]]}, {"id": "1608.06677", "submitter": "Ana Subtil", "authors": "Ana Subtil, Maria Ros\\'ario Oliveira, Ant\\'onio Pacheco", "title": "The cost of not having a perfect reference in diagnostic accuracy\n  studies: theoretical results and a web visualisation tool", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dichotomous diagnostic tests are widely used to detect the presence or\nabsence of a biomedical condition of interest. A rigorous evaluation of the\naccuracy of a diagnostic test is critical to determine its practical value.\nPerformance measures, such as the sensitivity and specificity of the test,\nshould be estimated by comparison with a gold standard. Since an error-free\nreference test is frequently missing, approaches based on available imperfect\ndiagnostic tests are used, namely: comparisons with an imperfect gold standard\nor with a composite reference standard, discrepant analysis, and latent class\nmodels.\n  In this work, we compare these methods using a theoretical approach based on\nanalytical expressions for the deviations between the sensitivity and\nspecificity according to each method, and the corresponding true values. We\nexplore the impact on the deviations of varying conditions: tests sensitivities\nand specificities, prevalence of the condition and local dependence between the\ntests. An R interactive graphical application is made available for the\nvisualisation of the outcomes. Based on our findings, we discuss the methods\nvalidity and potential usefulness.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 00:55:24 GMT"}, {"version": "v2", "created": "Wed, 21 Sep 2016 10:35:33 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Subtil", "Ana", ""], ["Oliveira", "Maria Ros\u00e1rio", ""], ["Pacheco", "Ant\u00f3nio", ""]]}, {"id": "1608.06682", "submitter": "Anselmo Pitombeira-Neto", "authors": "Anselmo Ramalho Pitombeira-Neto, Carlos Felipe Grangeiro Loureiro and\n  Luis Eduardo Carvalho", "title": "Bayesian inference on dynamic linear models of day-to-day\n  origin-destination flows in transportation networks", "comments": "22 pages, 8 figures", "journal-ref": null, "doi": "10.3390/urbansci2040117", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of origin-destination (OD) demand plays a key role in successful\ntransportation studies. In this paper, we consider the estimation of\ntime-varying day-to-day OD flows given data on traffic volumes in a\ntransportation network for a sequence of days. We propose a dynamic linear\nmodel (DLM) in order to represent the stochastic evolution of OD flows over\ntime. DLM's are Bayesian state-space models which can capture non-stationarity.\nWe take into account the hierarchical relationships between the distribution of\nOD flows among routes and the assignment of traffic volumes on links. Route\nchoice probabilities are obtained through a utility model based on past route\ncosts. We propose a Markov chain Monte Carlo algorithm, which integrates Gibbs\nsampling and a forward filtering backward sampling technique, in order to\napproximate the joint posterior distribution of mean OD flows and parameters of\nthe route choice model. Our approach can be applied to congested networks and\nin the case when data are available on only a subset of links. We illustrate\nthe application of our approach through simulated experiments on a test network\nfrom the literature.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 01:40:39 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Pitombeira-Neto", "Anselmo Ramalho", ""], ["Loureiro", "Carlos Felipe Grangeiro", ""], ["Carvalho", "Luis Eduardo", ""]]}, {"id": "1608.06805", "submitter": "Guillaume Basse", "authors": "Guillaume Basse and Avi Feller", "title": "Analyzing two-stage experiments in the presence of interference", "comments": "Accepted for publication in the Journal of the American Statistical\n  Association", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-stage randomization is a powerful design for estimating treatment effects\nin the presence of interference; that is, when one individual's treatment\nassignment affects another individual's outcomes. Our motivating example is a\ntwo-stage randomized trial evaluating an intervention to reduce student\nabsenteeism in the School District of Philadelphia. In that experiment,\nhouseholds with multiple students were first assigned to treatment or control;\nthen, in treated households, one student was randomly assigned to treatment.\nUsing this example, we highlight key considerations for analyzing two-stage\nexperiments in practice. Our first contribution is to address additional\ncomplexities that arise when household sizes vary; in this case, researchers\nmust decide between assigning equal weight to households or equal weight to\nindividuals. We propose unbiased estimators for a broad class of individual-\nand household-weighted estimands, with corresponding theoretical and estimated\nvariances. Our second contribution is to connect two common approaches for\nanalyzing two-stage designs: linear regression and randomization inference. We\nshow that, with suitably chosen standard errors, these two approaches yield\nidentical point and variance estimates, which is somewhat surprising given the\ncomplex randomization scheme. Finally, we explore options for incorporating\ncovariates to improve precision. We confirm our analytic results via simulation\nstudies and apply these methods to the attendance study, finding substantively\nmeaningful spillover effects.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 13:34:13 GMT"}, {"version": "v2", "created": "Sun, 30 Apr 2017 15:20:17 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Basse", "Guillaume", ""], ["Feller", "Avi", ""]]}, {"id": "1608.07029", "submitter": "Han Lin Shang", "authors": "Han Lin Shang", "title": "Functional time series forecasting with dynamic updating: An application\n  to intraday particulate matter concentration", "comments": "31 pages, 12 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Environmental data often take the form of a collection of curves observed\nsequentially over time. An example of this includes daily pollution measurement\ncurves describing the concentration of a particulate matter in ambient air.\nThese curves can be viewed as a time series of functions observed at equally\nspaced intervals over a dense grid. The nature of high-dimensional data poses\nchallenges from a statistical aspect, due to the so-called `curse of\ndimensionality', but it also poses opportunities to analyze a rich source of\ninformation to better understand dynamic changes at short time intervals.\nStatistical methods are introduced and compared for forecasting one-day-ahead\nintraday concentrations of particulate matter; as new data are sequentially\nobserved, dynamic updating methods are proposed to update point and interval\nforecasts to achieve better accuracy. These forecasting methods are validated\nthrough an empirical study of half-hourly concentrations of airborne\nparticulate matter in Graz, Austria.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 06:52:31 GMT"}], "update_date": "2016-08-26", "authors_parsed": [["Shang", "Han Lin", ""]]}, {"id": "1608.07193", "submitter": "Heejoon Han", "authors": "Heejoon Han", "title": "Quantile Dependence between Stock Markets and its Application in\n  Volatility Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines quantile dependence between international stock markets\nand evaluates its use for improving volatility forecasting. First, we analyze\nquantile dependence and directional predictability between the US stock market\nand stock markets in the UK, Germany, France and Japan. We use the\ncross-quantilogram, which is a correlation statistic of quantile hit processes.\nThe detailed dependence between stock markets depends on specific quantile\nranges and this dependence is generally asymmetric; the negative spillover\neffect is stronger than the positive spillover effect and there exists strong\ndirectional predictability from the US market to the UK, Germany, France and\nJapan markets. Second, we consider a simple quantile-augmented volatility model\nthat accommodates the quantile dependence and directional predictability\nbetween the US market and these other markets. The quantile-augmented\nvolatility model provides superior in-sample and out-of-sample volatility\nforecasts.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 15:21:02 GMT"}], "update_date": "2016-08-26", "authors_parsed": [["Han", "Heejoon", ""]]}, {"id": "1608.07305", "submitter": "Mark J Panaggio", "authors": "Mark J Panaggio, Pak-Wing Fok, Ghan S Bhatt, Simon Burhoe, Michael\n  Capps, Christina J Edholm, Fadoua El Moustaid, Tegan Emerson, Star-Lena\n  Estock, Nathan Gold, Ryan Halabi, Madelyn Houser, Peter R Kramer, Hsuan-Wei\n  Lee, Qingxia Li, Weiqiang Li, Dan Lu, Yuzhou Qian, Louis F Rossi, Deborah\n  Shutt, Vicky Chuqiao Yang and Yingxiang Zhou", "title": "Prediction and Optimal Scheduling of Advertisements in Linear Television", "comments": "24 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advertising is a crucial component of marketing and an important way for\ncompanies to raise awareness of goods and services in the marketplace.\nAdvertising campaigns are designed to convey a marketing image or message to an\naudience of potential consumers and television commercials can be an effective\nway of transmitting these messages to a large audience. In order to meet the\nrequirements for a typical advertising order, television content providers must\nprovide advertisers with a predetermined number of \"impressions\" in the target\ndemographic. However, because the number of impressions for a given program is\nnot known a priori and because there are a limited number of time slots\navailable for commercials, scheduling advertisements efficiently can be a\nchallenging computational problem. In this case study, we compare a variety of\nmethods for estimating future viewership patterns in a target demographic from\npast data. We also present a method for using those predictions to generate an\noptimal advertising schedule that satisfies campaign requirements while\nmaximizing advertising revenue.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 20:34:42 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Panaggio", "Mark J", ""], ["Fok", "Pak-Wing", ""], ["Bhatt", "Ghan S", ""], ["Burhoe", "Simon", ""], ["Capps", "Michael", ""], ["Edholm", "Christina J", ""], ["Moustaid", "Fadoua El", ""], ["Emerson", "Tegan", ""], ["Estock", "Star-Lena", ""], ["Gold", "Nathan", ""], ["Halabi", "Ryan", ""], ["Houser", "Madelyn", ""], ["Kramer", "Peter R", ""], ["Lee", "Hsuan-Wei", ""], ["Li", "Qingxia", ""], ["Li", "Weiqiang", ""], ["Lu", "Dan", ""], ["Qian", "Yuzhou", ""], ["Rossi", "Louis F", ""], ["Shutt", "Deborah", ""], ["Yang", "Vicky Chuqiao", ""], ["Zhou", "Yingxiang", ""]]}, {"id": "1608.07330", "submitter": "Jessica Godwin", "authors": "Jessica Godwin and Adrian E. Raftery", "title": "Bayesian Projection of Life Expectancy Accounting for the HIV/AIDS\n  Epidemic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While probabilistic projection methods for projecting life expectancy exist,\nfew account for covariates related to life expectancy. Generalized HIV/AIDS\nepidemics have a large, immediate negative impact on the life expectancy in a\ncountry, but this impact can be mitigated by widespread use of antiretroviral\ntherapy (ART). Thus projection methods for countries with generalized HIV/AIDS\nepidemics could be improved by accounting for HIV prevalence, the future course\nof the epidemic and coverage of ART. We propose a method for making\nprobabilistic projections of life expectancy to 2100 for all countries in the\nworld accounting for HIV prevalence, the future course of the epidemic and its\nuncertainty, and adult ART coverage. We extend the current Bayesian\nprobabilistic life expectancy projection methods of Raftery et al. (2013) to\naccount for HIV prevalence and adult ART coverage. We evaluate our method using\nout-of-sample validation. We find that the proposed method performs better than\nthe method that does not account for HIV prevalence or ART coverage for\nprojections of life expectancy in countries with a generalized epidemic, while\nprojections for countries without an epidemic remain essentially unchanged.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 22:57:45 GMT"}, {"version": "v2", "created": "Tue, 27 Sep 2016 17:14:34 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Godwin", "Jessica", ""], ["Raftery", "Adrian E.", ""]]}, {"id": "1608.07598", "submitter": "Hari Iyer", "authors": "Steven P. Lund and Hari Iyer", "title": "Likelihood Ratio as Weight of Forensic Evidence: A Metrological\n  Perspective", "comments": "41 pages, 4 tables, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we provide a rebuttal against the possible perception that a\nsingle number, such as the Likelihood Ratio, can provide an objective,\nauthoritative or definitive weight of evidence. We also illustrate the extent\nto which conclusions can vary depending on the assumptions used in the\nanalysis, even under alternative assumptions that are judged to be consistent\nwith available empirical information. To facilitate these goals, we introduce\nthe notion of a Lattice of Assumptions and an Uncertainty Pyramid illustrated\nin the context of a previously published example involving glass evidence. We\ntake the position that rather than focusing on a single number summary as the\nweight of evidence it is the duty of the forensic expert to assist the trier of\nfact in forming their own interpretations from a clear understanding of the\nobjective and demonstrably available information. We hope the presented\narguments will inspire those in the forensic science community to pursue\nestablishing their practice on a solid foundation of measurement science.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 20:26:36 GMT"}, {"version": "v2", "created": "Mon, 19 Sep 2016 21:58:34 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Lund", "Steven P.", ""], ["Iyer", "Hari", ""]]}, {"id": "1608.07606", "submitter": "Elise Jennings Dr", "authors": "Elise Jennings, Maeve Madigan", "title": "astroABC: An Approximate Bayesian Computation Sequential Monte Carlo\n  sampler for cosmological parameter estimation", "comments": "19 pages, 3 figures. Comments welcome", "journal-ref": "Astronomy and Computing 2017", "doi": "10.1016/j.ascom.2017.01.001", "report-no": "FERMILAB-PUB-16-334-A", "categories": "astro-ph.IM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the complexity of modern cosmological parameter inference where we are\nfaced with non-Gaussian data and noise, correlated systematics and multi-probe\ncorrelated data sets, the Approximate Bayesian Computation (ABC) method is a\npromising alternative to traditional Markov Chain Monte Carlo approaches in the\ncase where the Likelihood is intractable or unknown. The ABC method is called\n\"Likelihood free\" as it avoids explicit evaluation of the Likelihood by using a\nforward model simulation of the data which can include systematics. We\nintroduce astroABC, an open source ABC Sequential Monte Carlo (SMC) sampler for\nparameter estimation. A key challenge in astrophysics is the efficient use of\nlarge multi-probe datasets to constrain high dimensional, possibly correlated\nparameter spaces. With this in mind astroABC allows for massive parallelization\nusing MPI, a framework that handles spawning of jobs across multiple nodes. A\nkey new feature of astroABC is the ability to create MPI groups with different\ncommunicators, one for the sampler and several others for the forward model\nsimulation, which speeds up sampling time considerably. For smaller jobs the\nPython multiprocessing option is also available. Other key features include: a\nSequential Monte Carlo sampler, a method for iteratively adapting tolerance\nlevels, local covariance estimate using scikit-learn's KDTree, modules for\nspecifying optimal covariance matrix for a component-wise or multivariate\nnormal perturbation kernel, output and restart files are backed up every\niteration, user defined metric and simulation methods, a module for specifying\nheterogeneous parameter priors including non-standard prior PDFs, a module for\nspecifying a constant, linear, log or exponential tolerance level,\nwell-documented examples and sample scripts. This code is hosted online at\nhttps://github.com/EliseJ/astroABC\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 20:49:13 GMT"}, {"version": "v2", "created": "Tue, 7 Mar 2017 14:25:43 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Jennings", "Elise", ""], ["Madigan", "Maeve", ""]]}, {"id": "1608.07618", "submitter": "Bailey Fosdick", "authors": "Bailey K. Fosdick, Tyler H. McCormick, Thomas Brendan Murphy, Tin Lok\n  James Ng and Ted Westling", "title": "Multiresolution network models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many existing statistical and machine learning tools for social network\nanalysis focus on a single level of analysis. Methods designed for clustering\noptimize a global partition of the graph, whereas projection based approaches\n(e.g. the latent space model in the statistics literature) represent in rich\ndetail the roles of individuals. Many pertinent questions in sociology and\neconomics, however, span multiple scales of analysis. Further, many questions\ninvolve comparisons across disconnected graphs that will, inevitably be of\ndifferent sizes, either due to missing data or the inherent heterogeneity in\nreal-world networks. We propose a class of network models that represent\nnetwork structure on multiple scales and facilitate comparison across graphs\nwith different numbers of individuals. These models differentially invest\nmodeling effort within subgraphs of high density, often termed communities,\nwhile maintaining a parsimonious structure between said subgraphs. We show that\nour model class is projective, highlighting an ongoing discussion in the social\nnetwork modeling literature on the dependence of inference paradigms on the\nsize of the observed graph. We illustrate the utility of our method using data\non household relations from Karnataka, India.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 22:14:37 GMT"}, {"version": "v2", "created": "Thu, 8 Sep 2016 20:49:46 GMT"}, {"version": "v3", "created": "Tue, 22 Nov 2016 06:21:11 GMT"}, {"version": "v4", "created": "Tue, 26 Sep 2017 15:28:39 GMT"}, {"version": "v5", "created": "Thu, 5 Jul 2018 16:12:33 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Fosdick", "Bailey K.", ""], ["McCormick", "Tyler H.", ""], ["Murphy", "Thomas Brendan", ""], ["Ng", "Tin Lok James", ""], ["Westling", "Ted", ""]]}, {"id": "1608.07801", "submitter": "Sergey Porotsky", "authors": "Sergey Porotsky", "title": "Is it Correct to Use MLE Method for GRP Parameter Estimation ?", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of repair systems usually uses an As Good As New or As Bad As Old\nrepair assumptions. In practice, repair actions do not result in such extreme\nsituations, but rather in a complex transitional one, that is imperfect\nmaintenance, i.e. Generalized Renewal Process. Maximum Likelihood Estimation\nmethod is often used for reliability parameter estimation, but is it correct to\nuse it for Generalized Renewal Process\n", "versions": [{"version": "v1", "created": "Sun, 28 Aug 2016 11:34:34 GMT"}, {"version": "v2", "created": "Thu, 1 Sep 2016 08:42:07 GMT"}, {"version": "v3", "created": "Mon, 5 Sep 2016 08:23:15 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Porotsky", "Sergey", ""]]}, {"id": "1608.08056", "submitter": "Matteo Ruggiero", "authors": "Antonio Canale and Matteo Ruggiero", "title": "Bayesian nonparametric forecasting of monotonic functional time series", "comments": "To appear on the Electronic Journal of Statistics", "journal-ref": "Electron. J. Statist. Volume 10, Number 2 (2016), 3265-3286", "doi": "10.1214/16-EJS1190", "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian nonparametric approach to modelling and predicting a\nclass of functional time series with application to energy markets, based on\nfully observed, noise-free functional data. Traders in such contexts conceive\nprofitable strategies if they can anticipate the impact of their bidding\nactions on the aggregate demand and supply curves, which in turn need to be\npredicted reliably. Here we propose a simple Bayesian nonparametric method for\npredicting such curves, which take the form of monotonic bounded step\nfunctions. We borrow ideas from population genetics by defining a class of\ninteracting particle systems to model the functional trajectory, and develop an\nimplementation strategy which uses ideas from Markov chain Monte Carlo and\napproximate Bayesian computation techniques and allows to circumvent the\nintractability of the likelihood. Our approach shows great adaptation to the\ndegree of smoothness of the curves and the volatility of the functional series,\nproves to be robust to an increase of the forecast horizon and yields an\nuncertainty quantification for the functional forecasts. We illustrate the\nmodel and discuss its performance with simulated datasets and on real data\nrelative to the Italian natural gas market.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 14:08:54 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Canale", "Antonio", ""], ["Ruggiero", "Matteo", ""]]}, {"id": "1608.08076", "submitter": "Andrew Correia", "authors": "Andrew W. Correia", "title": "Bayesian Sequentially Monitored Multi-arm Experiments with Multiple\n  Comparison Adjustments", "comments": "25 pages; 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized experiments play a major role in data-driven decision making\nacross many different fields and disciplines. In medicine, for example,\nrandomized controlled trials (RCTs) are the backbone of clinical trial\nmethodology for testing the efficacy of new drugs and therapies versus existing\ntreatments or placebo. In business and marketing, randomized experiments are\ntypically referred to as A/B tests when there are only two arms, or variants,\nin the experiment, and as multivariate A/B tests when there are more than two\narms. Typical applications of A/B tests include comparing the effectiveness of\ndifferent ad campaigns, evaluating how people respond to different website\nlayouts, or comparing different customer subpopulations to each other.\n  This paper focuses on multivariate A/B testing from a digital marketing\nperspective, and presents a method for the sequential monitoring of such\nexperiments while accounting for the issue of multiple comparisons. In adapting\nand combining the methods of two previous works, the method presented herein is\nstraightforward to implement using standard statistical software and performs\nquite well in various simulation studies, exhibiting better power and smaller\naverage sample sizes than comparable methods.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 14:39:58 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Correia", "Andrew W.", ""]]}, {"id": "1608.08168", "submitter": "Tauhid Zaman", "authors": "Zhengli Wang and Tauhid Zaman", "title": "Learning Preferences and User Engagement Using Choice and Time Data", "comments": "37 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Choice decisions made by users of online applications can suffer from biases\ndue to the users' level of engagement. For instance, low engagement users may\nmake random choices with no concern for the quality of items offered. This\nbiased choice data can corrupt estimates of user preferences for items.\nHowever, one can correct for these biases if additional behavioral data is\nutilized. To do this we construct a new choice engagement time model which\ncaptures the impact of user engagement on choice decisions and response times\nassociated with these choice decisions. Response times are the behavioral data\nwe choose because they are easily measured by online applications and reveal\ninformation about user engagement. To test our model we conduct online polls\nwith subject populations that have different levels of engagement and measure\ntheir choice decisions and response times. We have two main empirical findings.\nFirst, choice decisions and response times are correlated, with strong\npreferences having faster response times than weak preferences. Second, low\nuser engagement is manifested through more random choice data and faster\nresponse times. Both of these phenomena are captured by our choice engagement\ntime model and we find that this model fits the data better than traditional\nchoice models. Our work has direct implications for online applications. It\nlets these applications remove the bias of low engagement users when estimating\npreferences for items. It also allows for the segmentation of users according\nto their level of engagement, which can be useful for targeted advertising or\nmarketing campaigns.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 18:26:41 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Wang", "Zhengli", ""], ["Zaman", "Tauhid", ""]]}, {"id": "1608.08199", "submitter": "Dimosthenis Tsagkrasoulis", "authors": "Dimosthenis Tsagkrasoulis, Pirro Hysi, Tim Spector, Giovanni Montana", "title": "Heritability maps of human face morphology through large-scale automated\n  three-dimensional phenotyping", "comments": "21 pages, 5 figures Updated content", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human face is a complex trait under strong genetic control, as evidenced\nby the striking visual similarity between twins. Nevertheless, heritability\nestimates of facial traits have often been surprisingly low or difficult to\nreplicate. Furthermore, the construction of facial phenotypes that correspond\nto naturally perceived facial features remains largely a mystery. We present\nhere a large-scale heritability study of face geometry that aims to address\nthese issues. High-resolution, three-dimensional facial models have been\nacquired on a cohort of $952$ twins recruited from the TwinsUK registry, and\nprocessed through a novel landmarking workflow, GESSA (Geodesic Ensemble\nSurface Sampling Algorithm). The algorithm places thousands of landmarks\nthroughout the facial surface and automatically establishes point-wise\ncorrespondence across faces. These landmarks enabled us to intuitively\ncharacterize facial geometry at a fine level of detail through curvature\nmeasurements, yielding accurate heritability maps of the human face\n(www.heritabilitymaps.info).\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 19:53:58 GMT"}, {"version": "v2", "created": "Sat, 28 Jan 2017 15:32:06 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Tsagkrasoulis", "Dimosthenis", ""], ["Hysi", "Pirro", ""], ["Spector", "Tim", ""], ["Montana", "Giovanni", ""]]}, {"id": "1608.08291", "submitter": "Mike Ludkovski", "authors": "Mike Ludkovski, Jimmy Risk, Howard Zail", "title": "Gaussian Process Models for Mortality Rates and Improvement Factors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a Gaussian process (\"GP\") framework for modeling mortality rates\nand mortality improvement factors. GP regression is a nonparametric,\ndata-driven approach for determining the spatial dependence in mortality rates\nand jointly smoothing raw rates across dimensions, such as calendar year and\nage. The GP model quantifies uncertainty associated with smoothed historical\nexperience and generates full stochastic trajectories for out-of-sample\nforecasts. Our framework is well suited for updating projections when newly\navailable data arrives, and for dealing with \"edge\" issues where credibility is\nlower. We present a detailed analysis of Gaussian process model performance for\nUS mortality experience based on the CDC datasets. We investigate the\ninteraction between mean and residual modeling, Bayesian and non-Bayesian GP\nmethodologies, accuracy of in-sample and out-of-sample forecasting, and\nstability of model parameters. We also document the general decline, along with\nstrong age-dependency, in mortality improvement factors over the past few\nyears, contrasting our findings with the Society of Actuaries (\"SOA\") MP-2014\nand -2015 models that do not fully reflect these recent trends.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 00:55:11 GMT"}, {"version": "v2", "created": "Wed, 17 May 2017 06:55:35 GMT"}, {"version": "v3", "created": "Wed, 11 Apr 2018 22:48:30 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Ludkovski", "Mike", ""], ["Risk", "Jimmy", ""], ["Zail", "Howard", ""]]}, {"id": "1608.08468", "submitter": "Gregor Kastner", "authors": "Gregor Kastner", "title": "Sparse Bayesian time-varying covariance estimation in many dimensions", "comments": null, "journal-ref": "Journal of Econometrics 210(1), 98-115 (2019)", "doi": "10.1016/j.jeconom.2018.11.007", "report-no": null, "categories": "stat.ME econ.EM q-fin.PM stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the curse of dimensionality in dynamic covariance estimation by\nmodeling the underlying co-volatility dynamics of a time series vector through\nlatent time-varying stochastic factors. The use of a global-local shrinkage\nprior for the elements of the factor loadings matrix pulls loadings on\nsuperfluous factors towards zero. To demonstrate the merits of the proposed\nframework, the model is applied to simulated data as well as to daily\nlog-returns of 300 S&P 500 members. Our approach yields precise correlation\nestimates, strong implied minimum variance portfolio performance and superior\nforecasting accuracy in terms of log predictive scores when compared to typical\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 14:16:54 GMT"}, {"version": "v2", "created": "Fri, 16 Jun 2017 13:43:38 GMT"}, {"version": "v3", "created": "Sat, 11 Nov 2017 11:28:19 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Kastner", "Gregor", ""]]}, {"id": "1608.08485", "submitter": "Michela Baccini", "authors": "Michela Baccini, Alessandra Mattei, Fabrizia Mealli, Pier Alberto\n  Bertazzi, Michele Carugno", "title": "Potential outcome approach to causal inference in assessing the short\n  term impact of air pollution on mortality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The opportunity to assess short term impact of air pollution relies on the\ncausal interpretation of the exposure-outcome association, but up to now few\nstudies explicitly faced this issue within a causal inference framework. In\nthis paper, we reformulated the problem of assessing the short term impact of\nair pollution on health using the potential outcome approach to causal\ninference. We focused on the impact of high daily levels of PM10 on mortality\nwithin two days from the exposure in the metropolitan area of Milan (Italy),\nduring the period 2003-2006. After defining the number of attributable deaths\nin terms of difference between potential outcomes, we used the estimated\npropensity score to match each high exposure-day with a day with similar\nbackground characteristics but lower PM10 level. Then, we estimated the impact\nby comparing mortality between matched days. We found that during the study\nperiod daily exposures larger than 40 microgram per cubic meter were\nresponsible of 1079 deaths (116; 2042). The impact was more evident among the\nelderly than in the younger classes of age. The propensity score matching\nturned out to be an appealing method to assess historical impacts in this\nfield.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 14:57:53 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Baccini", "Michela", ""], ["Mattei", "Alessandra", ""], ["Mealli", "Fabrizia", ""], ["Bertazzi", "Pier Alberto", ""], ["Carugno", "Michele", ""]]}, {"id": "1608.08718", "submitter": "Han Lin Shang", "authors": "Han Lin Shang", "title": "Reconciling forecasts of infant mortality rates at national and\n  sub-national levels: Grouped time-series methods", "comments": "41 pages, 4 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mortality rates are often disaggregated by different attributes, such as sex,\nstate, education, religion or ethnicity. Forecasting mortality rates at the\nnational and sub-national levels plays an important role in making social\npolicies associated with the national and sub-national levels. However, base\nforecasts at the sub-national levels may not add up to the forecasts at the\nnational level. To address this issue, we consider the problem of reconciling\nmortality rate forecasts from the viewpoint of grouped time-series forecasting\nmethods (Hyndman et al., 2011). A bottom-up method and an optimal combination\nmethod are applied to produce point forecasts of infant mortality rates that\nare aggregated appropriately across the different levels of a hierarchy. We\nextend these two methods by considering the reconciliation of interval\nforecasts through a bootstrap procedure. Using the regional infant mortality\nrates in Australia, we investigate the one-step-ahead to 20-step-ahead point\nand interval forecast accuracies among the independent and these two grouped\ntime-series forecasting methods. The proposed methods are shown to be useful\nfor reconciling point and interval forecasts of demographic rates at the\nnational and sub-national levels, and would be beneficial for government policy\ndecisions regarding the allocations of current and future resources at both the\nnational and sub-national levels.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 03:23:29 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["Shang", "Han Lin", ""]]}, {"id": "1608.09010", "submitter": "Matjaz Perc", "authors": "Zhen Wang, Chris T. Bauch, Samit Bhattacharyya, Alberto d'Onofrio,\n  Piero Manfredi, Matjaz Perc, Nicola Perra, Marcel Salath\\'e, Dawei Zhao", "title": "Statistical physics of vaccination", "comments": "150 pages, 42 figures; published in Physics Reports", "journal-ref": "Phys. Rep. 664 (2016) 1-113", "doi": "10.1016/j.physrep.2016.10.006", "report-no": null, "categories": "physics.soc-ph cond-mat.stat-mech cs.SI q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Historically, infectious diseases caused considerable damage to human\nsocieties, and they continue to do so today. To help reduce their impact,\nmathematical models of disease transmission have been studied to help\nunderstand disease dynamics and inform prevention strategies. Vaccination - one\nof the most important preventive measures of modern times - is of great\ninterest both theoretically and empirically. And in contrast to traditional\napproaches, recent research increasingly explores the pivotal implications of\nindividual behavior and heterogeneous contact patterns in populations. Our\nreport reviews the developmental arc of theoretical epidemiology with emphasis\non vaccination, as it led from classical models assuming homogeneously mixing\n(mean-field) populations and ignoring human behavior, to recent models that\naccount for behavioral feedback and/or population spatial/social structure.\nMany of the methods used originated in statistical physics, such as lattice and\nnetwork models, and their associated analytical frameworks. Similarly, the\nfeedback loop between vaccinating behavior and disease propagation forms a\ncoupled nonlinear system with analogs in physics. We also review the new\nparadigm of digital epidemiology, wherein sources of digital data such as\nonline social media are mined for high-resolution information on\nepidemiologically relevant individual behavior. Armed with the tools and\nconcepts of statistical physics, and further assisted by new sources of digital\ndata, models that capture nonlinear interactions between behavior and disease\ndynamics offer a novel way of modeling real-world phenomena, and can help\nimprove health outcomes. We conclude the review by discussing open problems in\nthe field and promising directions for future research.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 19:36:30 GMT"}, {"version": "v2", "created": "Tue, 25 Oct 2016 17:08:23 GMT"}, {"version": "v3", "created": "Thu, 17 Nov 2016 14:15:02 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Wang", "Zhen", ""], ["Bauch", "Chris T.", ""], ["Bhattacharyya", "Samit", ""], ["d'Onofrio", "Alberto", ""], ["Manfredi", "Piero", ""], ["Perc", "Matjaz", ""], ["Perra", "Nicola", ""], ["Salath\u00e9", "Marcel", ""], ["Zhao", "Dawei", ""]]}]