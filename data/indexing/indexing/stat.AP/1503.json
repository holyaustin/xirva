[{"id": "1503.00080", "submitter": "Anand Guruswamy", "authors": "Anand Guruswamy, Rick S. Blum, Shalinee Kishore, Mark Bordogna", "title": "Minimax Optimum Estimators for Phase Synchronization in IEEE 1588", "comments": "11 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The IEEE 1588 protocol has received recent interest as a means of delivering\nsub-microsecond level clock phase synchronization over packet-switched mobile\nbackhaul networks. Due to the randomness of the end-to-end delays in packet\nnetworks, the recovery of clock phase from packet timestamps in IEEE 1588 must\nbe treated as a statistical estimation problem. A number of estimators for this\nproblem have been suggested in the literature, but little is known about the\nbest achievable performance. In this paper, we describe new minimax estimators\nfor this problem, that are optimum in terms of minimizing the maximum mean\nsquared error over all possible values of the unknown parameters. Minimax\nestimators that utilize information from past timestamps to improve accuracy\nare also introduced. Simulation results indicate that significant performance\ngains over conventional estimators can be obtained via such optimum processing\ntechniques. These minimax estimators also provide fundamental limits on the\nperformance of phase offset estimation schemes.\n", "versions": [{"version": "v1", "created": "Sat, 28 Feb 2015 06:44:06 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Guruswamy", "Anand", ""], ["Blum", "Rick S.", ""], ["Kishore", "Shalinee", ""], ["Bordogna", "Mark", ""]]}, {"id": "1503.00135", "submitter": "Lucas Theis", "authors": "Lucas Theis, Philipp Berens, Emmanouil Froudarakis, Jacob Reimer,\n  Miroslav Rom\\'an Ros\\'on, Tom Baden, Thomas Euler, Andreas Tolias, Matthias\n  Bethge", "title": "Supervised learning sets benchmark for robust spike detection from\n  calcium imaging signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental challenge in calcium imaging has been to infer the timing of\naction potentials from the measured noisy calcium fluorescence traces. We\nsystematically evaluate a range of spike inference algorithms on a large\nbenchmark dataset recorded from varying neural tissue (V1 and retina) using\ndifferent calcium indicators (OGB-1 and GCamp6). We show that a new algorithm\nbased on supervised learning in flexible probabilistic models outperforms all\npreviously published techniques, setting a new standard for spike inference\nfrom calcium signals. Importantly, it performs better than other algorithms\neven on datasets not seen during training. Future data acquired in new\nexperimental conditions can easily be used to further improve its spike\nprediction accuracy and generalization performance. Finally, we show that\ncomparing algorithms on artificial data is not informative about performance on\nreal population imaging data, suggesting that a benchmark dataset may greatly\nfacilitate future algorithmic developments.\n", "versions": [{"version": "v1", "created": "Sat, 28 Feb 2015 14:52:33 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Theis", "Lucas", ""], ["Berens", "Philipp", ""], ["Froudarakis", "Emmanouil", ""], ["Reimer", "Jacob", ""], ["Ros\u00f3n", "Miroslav Rom\u00e1n", ""], ["Baden", "Tom", ""], ["Euler", "Thomas", ""], ["Tolias", "Andreas", ""], ["Bethge", "Matthias", ""]]}, {"id": "1503.00226", "submitter": "Sarah Lemler", "authors": "Agathe Guilloux (LSTA), Sarah Lemler (LaMME), Marie-Luce Taupin\n  (Unit\\'e MIAJ, LaMME)", "title": "Adaptive estimation of the baseline hazard function in the Cox model by\n  model selection, with high-dimensional covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this article is to provide an adaptive estimator of the\nbaseline function in the Cox model with high-dimensional covariates. We\nconsider a two-step procedure : first, we estimate the regression parameter of\nthe Cox model via a Lasso procedure based on the partial log-likelihood,\nsecondly, we plug this Lasso estimator into a least-squares type criterion and\nthen perform a model selection procedure to obtain an adaptive penalized\ncontrast estimator of the baseline function. Using non-asymptotic estimation\nresults stated for the Lasso estimator of the regression parameter, we\nestablish a non-asymptotic oracle inequality for this penalized contrast\nestimator of the baseline function, which highlights the discrepancy of the\nrate of convergence when the dimension of the covariates increases.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2015 07:22:08 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2015 14:14:24 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["Guilloux", "Agathe", "", "LSTA"], ["Lemler", "Sarah", "", "LaMME"], ["Taupin", "Marie-Luce", "", "Unit\u00e9 MIAJ, LaMME"]]}, {"id": "1503.00299", "submitter": "Alexander Zeifman", "authors": "A. K. Gorshenin, V. Yu. Korolev, A. Yu. Korchagin, T. V. Zakharova, A.\n  I. Zeifman", "title": "Statistical detection of movement activities in a human brain by\n  separation of mixture distributions", "comments": null, "journal-ref": "Journal of Mathematical Sciences, Vol. 218. Iss. 3, Pp. 278-286\n  (2016)", "doi": "10.1007/s10958-016-3029-1", "report-no": null, "categories": "stat.AP math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of most popular experimental techniques for investigation of brain\nactivity is the so-called method of evoked potentials: the subject repeatedly\nmakes some movements (by his/her finger) whereas brain activity and some\nauxiliary signals are recorded for further analysis. The key problem is the\ndetection of points in the myogram which correspond to the beginning of the\nmovements. The more precisely the points are detected, the more successfully\nthe magnetoencephalogram is processed aiming at the identification of sensors\nwhich are closest to the activity areas. The paper proposes a statistical\napproach to this problem based on mixtures models which uses a specially\nmodified method of moving separation of mixtures of probability distributions\n(MSM-method) to detect the start points of the finger's movements. We\ndemonstrate the correctness of the new procedure and its advantages as compared\nwith the method based on the notion of the myogram window variance.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2015 16:20:37 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Gorshenin", "A. K.", ""], ["Korolev", "V. Yu.", ""], ["Korchagin", "A. Yu.", ""], ["Zakharova", "T. V.", ""], ["Zeifman", "A. I.", ""]]}, {"id": "1503.00339", "submitter": "Vladislav Kargin", "authors": "Vladislav Kargin", "title": "Variation of word frequencies in Russian literary texts", "comments": "17 pages", "journal-ref": null, "doi": "10.1016/j.physa.2015.11.014", "report-no": null, "categories": "cs.CL physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the variation of word frequencies in Russian literary texts. Our\nfindings indicate that the standard deviation of a word's frequency across\ntexts depends on its average frequency according to a power law with exponent\n$0.62,$ showing that the rarer words have a relatively larger degree of\nfrequency volatility (i.e., \"burstiness\").\n  Several latent factors models have been estimated to investigate the\nstructure of the word frequency distribution. The dependence of a word's\nfrequency volatility on its average frequency can be explained by the asymmetry\nin the distribution of latent factors.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2015 19:37:27 GMT"}, {"version": "v2", "created": "Tue, 5 May 2015 17:51:21 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Kargin", "Vladislav", ""]]}, {"id": "1503.00635", "submitter": "Erin Conlon", "authors": "Alexey Miroshnikov, Evgeny Savel'ev, Erin M. Conlon", "title": "BayesSummaryStatLM: An R package for Bayesian Linear Models for Big Data\n  and Data Science", "comments": "Updated URL in reference [12]; added to description of zero.intercept\n  on p. 11; added minor clarifications throughout; results unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in data science and big data research have produced an\nabundance of large data sets that are too big to be analyzed in their entirety,\ndue to limits on either computer memory or storage capacity. Here, we introduce\nour R package 'BayesSummaryStatLM' for Bayesian linear regression models with\nMarkov chain Monte Carlo implementation that overcomes these limitations. Our\nBayesian models use only summary statistics of data as input; these summary\nstatistics can be calculated from subsets of big data and combined over\nsubsets. Thus, complete data sets do not need to be read into memory in full,\nwhich removes any physical memory limitations of a user. Our package\nincorporates the R package 'ff' and its functions for reading in big data sets\nin chunks while simultaneously calculating summary statistics. We describe our\nBayesian linear regression models, including several choices of prior\ndistributions for unknown model parameters, and illustrate capabilities and\nfeatures of our R package using both simulated and real data sets.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 17:44:38 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2015 23:01:04 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Miroshnikov", "Alexey", ""], ["Savel'ev", "Evgeny", ""], ["Conlon", "Erin M.", ""]]}, {"id": "1503.00643", "submitter": "Alessandro Bessi", "authors": "Alessandro Bessi", "title": "Two samples test for discrete power-law distributions", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.data-an physics.soc-ph stat.AP", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Power-law distributions occur in wide variety of physical, biological, and\nsocial phenomena. In this paper, we propose a statistical hypothesis test based\non the log-likelihood ratio to assess whether two samples of discrete data are\ndrawn from the same power-law distribution.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 17:59:39 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Bessi", "Alessandro", ""]]}, {"id": "1503.00688", "submitter": "Zhilin Zhang", "authors": "Zhilin Zhang", "title": "Photoplethysmography-Based Heart Rate Monitoring in Physical Activities\n  via Joint Sparse Spectrum Reconstruction", "comments": "Published in IEEE Transactions on Biomedical Engineering, Vol. 62,\n  No. 8, PP. 1902-1910, August 2015", "journal-ref": "IEEE Transactions on Biomedical Engineering, Vol. 62, No. 8, PP.\n  1902-1910, August 2015", "doi": "10.1109/TBME.2015.2406332", "report-no": null, "categories": "cs.OH cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Goal: A new method for heart rate monitoring using photoplethysmography (PPG)\nduring physical activities is proposed. Methods: It jointly estimates spectra\nof PPG signals and simultaneous acceleration signals, utilizing the multiple\nmeasurement vector model in sparse signal recovery. Due to a common sparsity\nconstraint on spectral coefficients, the method can easily identify and remove\nspectral peaks of motion artifact (MA) in PPG spectra. Thus, it does not need\nany extra signal processing modular to remove MA as in some other algorithms.\nFurthermore, seeking spectral peaks associated with heart rate is simplified.\nResults: Experimental results on 12 PPG datasets sampled at 25 Hz and recorded\nduring subjects' fast running showed that it had high performance. The average\nabsolute estimation error was 1.28 beat per minute and the standard deviation\nwas 2.61 beat per minute. Conclusion and Significance: These results show that\nthe method has great potential to be used for PPG-based heart rate monitoring\nin wearable devices for fitness tracking and health monitoring.\n", "versions": [{"version": "v1", "created": "Sat, 21 Feb 2015 01:48:20 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2015 06:21:23 GMT"}], "update_date": "2015-07-22", "authors_parsed": [["Zhang", "Zhilin", ""]]}, {"id": "1503.00730", "submitter": "Florent Leclercq", "authors": "Florent Leclercq, Jens Jasche, Benjamin Wandelt", "title": "Cosmic web-type classification using decision theory", "comments": "6 pages, 2 figures, text matches A&A Letters published version,\n  figures contain more panels", "journal-ref": "A&A 576, L17 (2015)", "doi": "10.1051/0004-6361/201526006", "report-no": null, "categories": "astro-ph.CO astro-ph.IM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a decision criterion for segmenting the cosmic web into different\nstructure types (voids, sheets, filaments, and clusters) on the basis of their\nrespective probabilities and the strength of data constraints. Our approach is\ninspired by an analysis of games of chance where the gambler only plays if a\npositive expected net gain can be achieved based on some degree of privileged\ninformation. The result is a general solution for classification problems in\nthe face of uncertainty, including the option of not committing to a class for\na candidate object. As an illustration, we produce high-resolution maps of\nweb-type constituents in the nearby Universe as probed by the Sloan Digital Sky\nSurvey main galaxy sample. Other possible applications include the selection\nand labelling of objects in catalogues derived from astronomical survey data.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 21:00:15 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2015 16:02:41 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Leclercq", "Florent", ""], ["Jasche", "Jens", ""], ["Wandelt", "Benjamin", ""]]}, {"id": "1503.00999", "submitter": "Paul Moore", "authors": "P.J. Moore", "title": "A hierarchical narrative framework for OCD", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper gives an explanatory framework for obsessive-compulsive disorder\n(OCD) based on a generative model of cognition. The framework is constructed\nusing the new concept of a 'formal narrative' which is a sequence of cognitive\nstates inferred from sense data. First we propose that human cognition uses a\nhierarchy of narratives to predict changes in the natural and social\nenvironment. Each layer in the hierarchy represents a distinct 'view of the\nworld', but it also contributes to a global unitary perspective. Second, the\ngenerative models used for cognitive inference can create new narratives from\nthose states already experienced by an individual. We hypothesise that when a\nthreat is recognised, narratives are generated as a cognitive model of possible\nthreat scenarios. Using this framework, we suggest that OCD arises from a\ndysfunction in sub-surface levels of inference while the global unitary\nperspective remains intact. The failure of inference is felt as the external\nworld being 'not just right', and its automatic correction by the perceptual\nsystem is experienced as compulsion. Ordering and symmetry obsessions are the\neffects of the perceptual system trying to achieve precise inference. Checking\nbehaviour arises because the security system attempts to finesse inference as\npart of its protection behaviour. Similarly, fear of harm and distressing\nthoughts occur because the failure of inference results in an indistinct view\nof the past or the future. A wide variety of symptoms in OCD is thus explained\nby a single dysfunction.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 16:33:04 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["Moore", "P. J.", ""]]}, {"id": "1503.01081", "submitter": "Antti Honkela", "authors": "Antti Honkela, Jaakko Peltonen, Hande Topa, Iryna Charapitsa, Filomena\n  Matarese, Korbinian Grote, Hendrik G. Stunnenberg, George Reid, Neil D.\n  Lawrence, Magnus Rattray", "title": "Genome-wide modelling of transcription kinetics reveals patterns of RNA\n  production delays", "comments": "42 pages, 17 figures", "journal-ref": "PNAS 112(42):13115-13120, 2015", "doi": "10.1073/pnas.1420404112", "report-no": null, "categories": "q-bio.GN q-bio.QM stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Genes with similar transcriptional activation kinetics can display very\ndifferent temporal mRNA profiles due to differences in transcription time,\ndegradation rate and RNA processing kinetics. Recent studies have shown that a\nsplicing-associated RNA production delay can be significant. We introduce a\njoint model of transcriptional activation and mRNA accumulation which can be\nused for inference of transcription rate, RNA production delay and degradation\nrate given genome-wide data from high-throughput sequencing time course\nexperiments. We combine a mechanistic differential equation model with a\nnon-parametric statistical modelling approach allowing us to capture a broad\nrange of activation kinetics, and use Bayesian parameter estimation to quantify\nthe uncertainty in the estimates of the kinetic parameters. We apply the model\nto data from estrogen receptor (ER-{\\alpha}) activation in the MCF-7 breast\ncancer cell line. We use RNA polymerase II (pol-II) ChIP-Seq time course data\nto characterise transcriptional activation and mRNA-Seq time course data to\nquantify mature transcripts. We find that 11% of genes with a good signal in\nthe data display a delay of more than 20 minutes between completing\ntranscription and mature mRNA production. The genes displaying these long\ndelays are significantly more likely to be short. We also find a statistical\nassociation between high delay and late intron retention in pre-mRNA data,\nindicating significant splicing-associated production delays in many genes.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 20:00:31 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2015 19:07:43 GMT"}], "update_date": "2015-10-22", "authors_parsed": [["Honkela", "Antti", ""], ["Peltonen", "Jaakko", ""], ["Topa", "Hande", ""], ["Charapitsa", "Iryna", ""], ["Matarese", "Filomena", ""], ["Grote", "Korbinian", ""], ["Stunnenberg", "Hendrik G.", ""], ["Reid", "George", ""], ["Lawrence", "Neil D.", ""], ["Rattray", "Magnus", ""]]}, {"id": "1503.01271", "submitter": "Pascal Vallet", "authors": "Pascal Vallet, Xavier Mestre, Philippe Loubaton", "title": "Performance analysis of an improved MUSIC DoA estimator", "comments": "Revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper adresses the statistical performance of subspace DoA estimation\nusing a sensor array, in the asymptotic regime where the number of samples and\nsensors both converge to infinity at the same rate. Improved subspace DoA\nestimators were derived (termed as G-MUSIC) in previous works, and were shown\nto be consistent and asymptotically Gaussian distributed in the case where the\nnumber of sources and their DoA remain fixed. In this case, which models widely\nspaced DoA scenarios, it is proved in the present paper that the traditional\nMUSIC method also provides DoA consistent estimates having the same asymptotic\nvariances as the G-MUSIC estimates. The case of DoA that are spaced of the\norder of a beamwidth, which models closely spaced sources, is also considered.\nIt is shown that G-MUSIC estimates are still able to consistently separate the\nsources, while it is no longer the case for the MUSIC ones. The asymptotic\nvariances of G-MUSIC estimates are also evaluated.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 10:41:05 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2015 08:19:53 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Vallet", "Pascal", ""], ["Mestre", "Xavier", ""], ["Loubaton", "Philippe", ""]]}, {"id": "1503.01291", "submitter": "Giovanni Montana", "authors": "Zi Wang, Wei Yuan, Giovanni Montana", "title": "Sparse multi-view matrix factorisation: a multivariate approach to\n  multiple tissue comparisons", "comments": "in Bioinformatics 2015", "journal-ref": null, "doi": "10.1093/bioinformatics/btv344", "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gene expression levels in a population vary extensively across tissues. Such\nheterogeneity is caused by genetic variability and environmental factors, and\nis expected to be linked to disease development. The abundance of experimental\ndata now enables the identification of features of gene expression profiles\nthat are shared across tissues, and those that are tissue-specific. While most\ncurrent research is concerned with characterising differential expression by\ncomparing mean expression profiles across tissues, it is also believed that a\nsignificant difference in a gene expression's variance across tissues may also\nbe associated to molecular mechanisms that are important for tissue development\nand function. We propose a sparse multi-view matrix factorisation (sMVMF)\nalgorithm to jointly analyse gene expression measurements in multiple tissues,\nwhere each tissue provides a different \"view\" of the underlying organism. The\nproposed methodology can be interpreted as an extension of principal component\nanalysis in that it provides the means to decompose the total sample variance\nin each tissue into the sum of two components: one capturing the variance that\nis shared across tissues, and one isolating the tissue-specific variances.\nsMVMF has been used to jointly model mRNA expression profiles in three tissues\n- adipose, skin and LCL - which are available for a large and well-phenotyped\ntwins cohort, TwinsUK. Using sMVMF, we are able to prioritise genes based on\nwhether their variation patterns are specific to each tissue. Furthermore,\nusing DNA methylation profiles available, we provide supporting evidence that\nadipose-specific gene expression patterns may be driven by epigenetic effects.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 11:50:45 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2015 16:27:54 GMT"}], "update_date": "2015-06-26", "authors_parsed": [["Wang", "Zi", ""], ["Yuan", "Wei", ""], ["Montana", "Giovanni", ""]]}, {"id": "1503.01305", "submitter": "K. S. McGarrity", "authors": "K. S. McGarrity, J. Sietsma, G. Jongbloed", "title": "Nonparametric inference in a stereological model with oriented cylinders\n  applied to dual phase steel", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS787 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 4, 2538-2566", "doi": "10.1214/14-AOAS787", "report-no": "IMS-AOAS-AOAS787", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Oriented circular cylinders in an opaque medium are used to represent certain\nmicrostructural objects in steel. The opaque medium is sliced parallel to the\ncylinder axes of symmetry and the cut-plane contains the observable rectangular\nprofiles of the cylinders. A one-to-one relation between the joint density of\nthe squared radius and height of the 3D cylinders and the joint density of the\nsquared half-width and height of the observable 2D rectangles is established.\nWe propose a nonparametric estimation procedure to estimate the distributions\nand expectations of various quantities of interest, such as the cylinder\nradius, height, aspect ratio, surface area and volume from the observed 2D\nrectangle widths and heights. Also, the covariance between the radius and\nheight of a cylinder is estimated. The asymptotic behavior of these estimators\nis established to yield point-wise confidence intervals for the expectations\nand point-wise confidence sets for the distributions of the quantities of\ninterest. Many of these quantities can be linked to the mechanical properties\nof the material, and are, therefore, useful for industry. We illustrate the\nmathematical model and estimation procedures using a banded microstructure for\nwhich nearly 90 \\textmu m of depth have been observed via serial sectioning.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 13:20:20 GMT"}], "update_date": "2015-03-05", "authors_parsed": [["McGarrity", "K. S.", ""], ["Sietsma", "J.", ""], ["Jongbloed", "G.", ""]]}, {"id": "1503.01308", "submitter": "Georg M. Goerg", "authors": "Georg M. Goerg", "title": "Acknowledgment of priority: Usage of the Lambert W function in\n  statistics", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS790 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 4, 2567-2567", "doi": "10.1214/14-AOAS790", "report-no": "IMS-AOAS-AOAS790", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In my 2011 Annals of Applied Statistics article [Goerg (2011)] I wrote that\n\"Whereas the Lambert $W$ function plays an important role in mathematics,\nphysics, chemistry, biology and other fields, it has not yet been used in\nstatistics.\" This was incorrect. At the time of publication I was unaware of\nStehl\\'{\\i}k (2003), who used the Lambert $W$ function to derive the exact\ndistribution of the likelihood ratio test statistic. He has also used it in\nmore recent work such as Stehl\\'{\\i}k (2006), Stehl\\'{\\i}k et al. (2010), or\nStehl\\'{\\i}k (2014) amongst others. While Stehl\\'{i}k's use of the Lambert $W$\nfunction was focused on the distribution of the likelihood ratio test\nstatistic, my work dealt with the modeling of skewed random variables and\nsymmetrizing data using the Lambert $W$ function as a variable transformation.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 13:44:02 GMT"}], "update_date": "2015-03-05", "authors_parsed": [["Goerg", "Georg M.", ""]]}, {"id": "1503.01890", "submitter": "Aristides Moustakas", "authors": "Aristides Moustakas, and Matthew R. Evans", "title": "Coupling models of cattle and farms with models of badgers for\n  predicting the dynamics of bovine tuberculosis (TB)", "comments": null, "journal-ref": "Stochastic Environmental Research and Risk Assessment (2015), Vol\n  29, Issue 3, pp 623-635", "doi": "10.1007/s00477-014-1016-y", "report-no": null, "categories": "q-bio.PE cs.CE math.DS stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bovine TB is a major problem for the agricultural industry in several\ncountries. TB can be contracted and spread by species other than cattle and\nthis can cause a problem for disease control. In the UK and Ireland, badgers\nare a recognised reservoir of infection and there has been substantial\ndiscussion about potential control strategies. We present a coupling of\nindividual based models of bovine TB in badgers and cattle, which aims to\ncapture the key details of the natural history of the disease and of both\nspecies at approximately county scale. The model is spatially explicit it\nfollows a very large number of cattle and badgers on a different grid size for\neach species and includes also winter housing. We show that the model can\nreplicate the reported dynamics of both cattle and badger populations as well\nas the increasing prevalence of the disease in cattle. Parameter space used as\ninput in simulations was swept out using Latin hypercube sampling and\nsensitivity analysis to model outputs was conducted using mixed effect models.\nBy exploring a large and computationally intensive parameter space we show that\nof the available control strategies it is the frequency of TB testing and\nwhether or not winter housing is practised that have the most significant\neffects on the number of infected cattle, with the effect of winter housing\nbecoming stronger as farm size increases. Whether badgers were culled or not\nexplained about 5%, while the accuracy of the test employed to detect infected\ncattle explained less than 3% of the variance in the number of infected cattle.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2015 09:43:45 GMT"}], "update_date": "2015-03-09", "authors_parsed": [["Moustakas", "Aristides", ""], ["Evans", "Matthew R.", ""]]}, {"id": "1503.01898", "submitter": "Dmitriy Shutin", "authors": "Dmitriy Shutin and Nicolas Schneckenburger", "title": "Joint Detection and Super-Resolution Estimation of Multipath Signal\n  Parameters Using Incremental Automatic Relevance Determination", "comments": "This work has been submitted to IEEE Transactions on Signal\n  Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The presented work investigates a sparse Bayesian incremental automatic\nrelevance determination (IARD) algorithm in the context of multipath parameter\nestimation in a super-resolution regime. The corresponding estimation problem\nis highly nonlinear and, in general, requires an estimation of the number of\nmultipath components. In the IARD approach individual multipath components are\nprocessed sequentially, which permits a tractable convergence analysis of the\ncorresponding inference expressions. This leads to a simple condition, termed\nhere a pruning condition, that determines if a multipath component is\n\"sparsified\" or retained in the model, thus permitting a fast and adaptive\nrealization of the estimation algorithm. Yet previous experiments demonstrated\nthat IARD fails to select the correct number of components when the parameters\nentering nonlinearly the multipath model are also estimated. To understand this\neffect, an analysis of the statistical structure of the pruning condition is\nproposed. It is shown that the corresponding test statistic in the pruning\ncondition follows an extreme value distribution. As a result, the standard IARD\nalgorithm implements a statistical test with a very high probability of false\nalarm. This leads to insertion of estimation artifacts and underestimation of\nsignal sparsity. Moreover, the probability of false alarm worsens as the number\nof measured signal samples grows. Based on the developed statistical\ninterpretation of the IARD, an optimal adjustment of the pruning condition is\nproposed. This permits a reliable and efficient removal of estimation artifacts\nand joint estimation of signal parameters, as well as optimal model order\nselection within a sparse Bayesian learning framework.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2015 10:17:28 GMT"}], "update_date": "2015-03-09", "authors_parsed": [["Shutin", "Dmitriy", ""], ["Schneckenburger", "Nicolas", ""]]}, {"id": "1503.01978", "submitter": "Martin Kulldorff", "authors": "Martin Kulldorff and Ivair R. Silva", "title": "Continuous Post-Market Sequential Safety Surveillance with Minimum\n  Events to Signal", "comments": "22 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The CDC Vaccine Safety Datalink project has pioneered the use of near\nreal-time post-market vaccine safety surveillance for the rapid detection of\nadverse events. Doing weekly analyses, continuous sequential methods are used,\nallowing investigators to evaluate the data near-continuously while still\nmaintaining the correct overall alpha level. With continuous sequential\nmonitoring, the null hypothesis may be rejected after only one or two adverse\nevents are observed. In this paper, we explore continuous sequential monitoring\nwhen we do not allow the null to be rejected until a minimum number of observed\nevents have occurred. We also evaluate continuous sequential analysis with a\ndelayed start until a certain sample size has been attained. Tables with exact\ncritical values, statistical power and the average times to signal are\nprovided. We show that, with the first option, it is possible to both increase\nthe power and reduce the expected time to signal, while keeping the alpha level\nthe same. The second option is only useful if the start of the surveillance is\ndelayed for logistical reasons, when there is a group of data available at the\nfirst analysis, followed by continuous or near-continuous monitoring\nthereafter.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2015 14:47:52 GMT"}], "update_date": "2015-03-09", "authors_parsed": [["Kulldorff", "Martin", ""], ["Silva", "Ivair R.", ""]]}, {"id": "1503.02115", "submitter": "Avanti Athreya", "authors": "Vince Lyzinski, Minh Tang, Avanti Athreya, Youngser Park, Carey E.\n  Priebe", "title": "Community Detection and Classification in Hierarchical Stochastic\n  Blockmodels", "comments": "17 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a robust, scalable, integrated methodology for community detection\nand community comparison in graphs. In our procedure, we first embed a graph\ninto an appropriate Euclidean space to obtain a low-dimensional representation,\nand then cluster the vertices into communities. We next employ nonparametric\ngraph inference techniques to identify structural similarity among these\ncommunities. These two steps are then applied recursively on the communities,\nallowing us to detect more fine-grained structure. We describe a hierarchical\nstochastic blockmodel---namely, a stochastic blockmodel with a natural\nhierarchical structure---and establish conditions under which our algorithm\nyields consistent estimates of model parameters and motifs, which we define to\nbe stochastically similar groups of subgraphs. Finally, we demonstrate the\neffectiveness of our algorithm in both simulated and real data. Specifically,\nwe address the problem of locating similar subcommunities in a partially\nreconstructed Drosophila connectome and in the social network Friendster.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2015 01:40:25 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2015 23:36:37 GMT"}, {"version": "v3", "created": "Tue, 27 Oct 2015 04:30:47 GMT"}, {"version": "v4", "created": "Thu, 25 Aug 2016 18:59:30 GMT"}, {"version": "v5", "created": "Fri, 26 Aug 2016 00:13:19 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Lyzinski", "Vince", ""], ["Tang", "Minh", ""], ["Athreya", "Avanti", ""], ["Park", "Youngser", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1503.02332", "submitter": "Jing Wang", "authors": "Jing Wang, Ioannis Ch. Paschalidis", "title": "Robust Anomaly Detection in Dynamic Networks", "comments": "6 pages. MED conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two robust methods for anomaly detection in dynamic networks in\nwhich the properties of normal traffic are time-varying. We formulate the\nrobust anomaly detection problem as a binary composite hypothesis testing\nproblem and propose two methods: a model-free and a model-based one, leveraging\ntechniques from the theory of large deviations. Both methods require a family\nof Probability Laws (PLs) that represent normal properties of traffic. We\ndevise a two-step procedure to estimate this family of PLs. We compare the\nperformance of our robust methods and their vanilla counterparts, which assume\nthat normal traffic is stationary, on a network with a diurnal normal pattern\nand a common anomaly related to data exfiltration. Simulation results show that\nour robust methods perform better than their vanilla counterparts in dynamic\nnetworks.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2015 22:14:02 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Wang", "Jing", ""], ["Paschalidis", "Ioannis Ch.", ""]]}, {"id": "1503.02344", "submitter": "Han Lin Shang", "authors": "Han Lin Shang", "title": "Selection of the optimal Box-Cox transformation parameter for modelling\n  and forecasting age-specific fertility", "comments": "15 pages, 4 figures", "journal-ref": "Journal of Population Research, 2015, 32(1), 69-79", "doi": "10.1007/s12546-014-9138-0", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Box-Cox transformation can sometimes yield noticeable improvements in\nmodel simplicity, variance homogeneity and precision of estimation, such as in\nmodelling and forecasting age-specific fertility. Despite its importance, there\nhave been few studies focusing on the optimal selection of Box-Cox\ntransformation parameters in demographic forecasting. A simple method is\nproposed for selecting the optimal Box-Cox transformation parameter, along with\nan algorithm based on an in-sample forecast error measure. Illustrated by\nAustralian age-specific fertility, the out-of-sample accuracy of a forecasting\nmethod can be improved with the selected Box-Cox transformation parameter.\nFurthermore, the log transformation is not adequate for modelling and\nforecasting age-specific fertility. The Box-Cox transformation parameter should\nbe embedded in statistical analysis of age-specific demographic data, in order\nto fully capture forecast uncertainties.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2015 23:37:39 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Shang", "Han Lin", ""]]}, {"id": "1503.03022", "submitter": "Leonardo Bennun LB", "authors": "Felipe Quiero, Fabian Quintana and Leonardo Bennun", "title": "A novel method based on cross correlation maximization, for pattern\n  matching by means of a single parameter. Application to the human voice", "comments": "13 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work develops a cross correlation maximization technique, based on\nstatistical concepts, for pattern matching purposes in time series. The\ntechnique analytically quantifies the extent of similitude between a known\nsignal within a group of data, by means of a single parameter. Specifically,\nthe method was applied to voice recognition problem, by selecting samples from\na given individual recordings of the 5 vowels, in Spanish. The frequency of\nacquisition of the data was 11.250 Hz. A certain distinctive interval was\nestablished from each vowel time series as a representative test function and\nit was compared both to itself and to the rest of the vowels by means of an\nalgorithm, for a subsequent graphic illustration of the results.\n  We conclude that for a minimum distinctive length, the method meets\nresemblance between every vowel with itself, and also an irrefutable difference\nwith the rest of the vowels for an estimate length of 30 points (~2 10-3 s).\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2015 15:31:37 GMT"}], "update_date": "2015-03-11", "authors_parsed": [["Quiero", "Felipe", ""], ["Quintana", "Fabian", ""], ["Bennun", "Leonardo", ""]]}, {"id": "1503.03297", "submitter": "Kangrui Wang", "authors": "Satyendra Nath Chakrabartty, Kangrui Wang, Dalia Chakrabarty", "title": "Uncertainty in Test Score Data and Classically Defined Reliability of\n  Tests and Test Batteries, using a New Method for Test Dichotomisation", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As with all measurements, the measurement of examinee ability, in terms of\nscores that the examinee obtains in a test, is also error-ridden. The\nquantification of such error or uncertainty in the test score data--or rather\nthe complementary test reliability--is pursued within the paradigm of Classical\nTest Theory in a variety of ways, with no existing method of finding\nreliability, isomorphic to the theoretical definition that parametrises\nreliability as the ratio of the true score variance and observed (i.e.\nerror-ridden) score variance. Thus, multiple reliability coefficients for the\nsame test have been advanced. This paper describes a much needed method of\nobtaining reliability of a test as per its theoretical definition, via a single\nadministration of the test, by using a new fast method of splitting of a given\ntest into parallel halves, achieving near-coincident empirical distributions of\nthe two halves. The method has the desirable property of achieving splitting on\nthe basis of difficulty of the questions (or items) that constitute the test,\nthus allowing for fast computation of reliability even for very large test data\nsets, i.e. test data obtained by a very large examinee sample. An interval\nestimate for the true score is offered, given an examinee score, subsequent to\nthe determination of the test reliability. This method of finding test\nreliability as per the classical definition can be extended to find reliability\nof a set or battery of tests; a method for determination of the weights\nimplemented in the computation of the weighted battery score is discussed. We\nperform empirical illustration of our method on real and simulated tests, and\non a real test battery comprising two constituent tests.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 12:19:40 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2015 13:03:38 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Chakrabartty", "Satyendra Nath", ""], ["Wang", "Kangrui", ""], ["Chakrabarty", "Dalia", ""]]}, {"id": "1503.03355", "submitter": "Evangelos Papalexakis", "authors": "Evangelos E. Papalexakis", "title": "Automatic Unsupervised Tensor Mining with Quality Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular tool for unsupervised modelling and mining multi-aspect data is\ntensor decomposition. In an exploratory setting, where and no labels or ground\ntruth are available how can we automatically decide how many components to\nextract? How can we assess the quality of our results, so that a domain expert\ncan factor this quality measure in the interpretation of our results? In this\npaper, we introduce AutoTen, a novel automatic unsupervised tensor mining\nalgorithm with minimal user intervention, which leverages and improves upon\nheuristics that assess the result quality. We extensively evaluate AutoTen's\nperformance on synthetic data, outperforming existing baselines on this very\nhard problem. Finally, we apply AutoTen on a variety of real datasets,\nproviding insights and discoveries. We view this work as a step towards a fully\nautomated, unsupervised tensor mining tool that can be easily adopted by\npractitioners in academia and industry.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 14:34:46 GMT"}], "update_date": "2015-03-12", "authors_parsed": [["Papalexakis", "Evangelos E.", ""]]}, {"id": "1503.03405", "submitter": "Andr\\'e Beauducel", "authors": "Andre Beauducel", "title": "Constraints for non-zero secondary loadings in confirmatory factor\n  analysis", "comments": "This version contains further developments of the constraints for\n  confirmatory factor analysis, whereas the aspects referring to exploratory\n  factor analysis were removed. The concepts referring to buffered simple\n  structure for exploratory factor analysis can be found in the previous\n  version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of confirmatory factor analysis, the independent clusters\nmodel has been found to be overly restrictive in several research contexts.\nTherefore, a less restrictive criterion for parsimony of non-salient loadings\nin confirmatory factor analysis was proposed. The criterion is based on\n'buffered scales', which have been introduced by Cattell and Tsujioka (1964) as\noptimal indicators of corresponding factors. Variables with positive and\nnegative loadings on an unwanted factor are balanced out in a buffered scale,\nso that the variance of the unwanted factor is at minimum. It is proposed here\nto specify a balance of positive and negative secondary loadings by means of\nmodel constraints in order to achieve parsimony of loading patterns. The\nspecification of buffered simple structure by means of model constraints was\nillustrated by means of a simulation study and an empirical example.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 16:27:44 GMT"}, {"version": "v2", "created": "Sun, 20 Dec 2015 19:19:47 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Beauducel", "Andre", ""]]}, {"id": "1503.03453", "submitter": "Brian Ji Dr.", "authors": "Edward Y. Ji and Brian L. Ji", "title": "On the Variability Estimation of Lognormal Distribution Based on Sample\n  Harmonic and Arithmetic Means", "comments": "7 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the lognormal distribution, an unbiased estimator of the squared\ncoefficient of variation is derived from the relative ratio of sample\narithmetic to harmonic means. Analytical proofs and simulation results are\npresented.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 19:15:21 GMT"}], "update_date": "2015-03-12", "authors_parsed": [["Ji", "Edward Y.", ""], ["Ji", "Brian L.", ""]]}, {"id": "1503.03509", "submitter": "Sidney Redner", "authors": "A. Clauset, M. Kogan, and S. Redner", "title": "Safe Leads and Lead Changes in Competitive Team Sports", "comments": "11 pages, 16 figures, 2-column revtex format", "journal-ref": "Phys. Rev. E 91, 062815 (2015)", "doi": "10.1103/PhysRevE.91.062815", "report-no": null, "categories": "physics.data-an physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the time evolution of lead changes within individual games of\ncompetitive team sports. Exploiting ideas from the theory of random walks, the\nnumber of lead changes within a single game follows a Gaussian distribution. We\nshow that the probability that the last lead change and the time of the largest\nlead size are governed by the same arcsine law, a bimodal distribution that\ndiverges at the start and at the end of the game. We also determine the\nprobability that a given lead is \"safe\" as a function of its size $L$ and game\ntime $t$. Our predictions generally agree with comprehensive data on more than\n1.25 million scoring events in roughly 40,000 games across four professional or\nsemi-professional team sports, and are more accurate than popular heuristics\ncurrently used in sports analytics.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 21:20:44 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Clauset", "A.", ""], ["Kogan", "M.", ""], ["Redner", "S.", ""]]}, {"id": "1503.03512", "submitter": "Peter Sheridan Dodds", "authors": "Eitan Adam Pechenick, Christopher M. Danforth, Peter Sheridan Dodds", "title": "Is language evolution grinding to a halt? The scaling of lexical\n  turbulence in English fiction suggests it is not", "comments": "17 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IT math.IT physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Of basic interest is the quantification of the long term growth of a\nlanguage's lexicon as it develops to more completely cover both a culture's\ncommunication requirements and knowledge space. Here, we explore the usage\ndynamics of words in the English language as reflected by the Google Books 2012\nEnglish Fiction corpus. We critique an earlier method that found decreasing\nbirth and increasing death rates of words over the second half of the 20th\nCentury, showing death rates to be strongly affected by the imposed time cutoff\nof the arbitrary present and not increasing dramatically. We provide a robust,\nprincipled approach to examining lexical evolution by tracking the volume of\nword flux across various relative frequency thresholds. We show that while the\noverall statistical structure of the English language remains stable over time\nin terms of its raw Zipf distribution, we find evidence of an enduring `lexical\nturbulence': The flux of words across frequency thresholds from decade to\ndecade scales superlinearly with word rank and exhibits a scaling break we\nconnect to that of Zipf's law. To better understand the changing lexicon, we\nexamine the contributions to the Jensen-Shannon divergence of individual words\ncrossing frequency thresholds. We also find indications that scholarly works\nabout fiction are strongly represented in the 2012 English Fiction corpus, and\nsuggest that a future revision of the corpus should attempt to separate\ncritical works from fiction itself.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 21:32:01 GMT"}, {"version": "v2", "created": "Sat, 30 May 2015 16:38:59 GMT"}, {"version": "v3", "created": "Thu, 10 Mar 2016 04:12:30 GMT"}, {"version": "v4", "created": "Fri, 24 Mar 2017 10:42:31 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["Pechenick", "Eitan Adam", ""], ["Danforth", "Christopher M.", ""], ["Dodds", "Peter Sheridan", ""]]}, {"id": "1503.03666", "submitter": "Philip Dawid", "authors": "Peter B. Imrey and A. Philip Dawid", "title": "A Commentary on Statistical Assessment of Violence Recidivism Risk", "comments": "28 pages, 4 tables, 2 figures. This is an authors' original\n  (unrevised) manuscript of an article published in the journal \"Statistics and\n  Public Policy\", available freely online at http://www.tandfonline.com/", "journal-ref": "Statistics and Public Policy 2 (2015), e1029338, 1-18", "doi": "10.1080/2330443X.2015.1029338", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing integration and availability of data on large groups of persons\nhas been accompanied by proliferation of statistical and other algorithmic\nprediction tools in banking, insurance, marketiNg, medicine, and other FIelds\n(see e.g., Steyerberg (2009a;b)). Controversy may ensue when such tools are\nintroduced to fields traditionally reliant on individual clinical evaluations.\nSuch controversy has arisen about \"actuarial\" assessments of violence\nrecidivism risk, i.e., the probability that someone found to have committed a\nviolent act will commit another during a specified period. Recently Hart et al.\n(2007a) and subsequent papers from these authors in several reputable journals\nhave claimed to demonstrate that statistical assessments of such risks are\ninherently too imprecise to be useful, using arguments that would seem to apply\nto statistical risk prediction quite broadly. This commentary examines these\narguments from a technical statistical perspective, and finds them seriously\nmistaken in many particulars. They should play no role in reasoned discussions\nof violence recidivism risk assessment.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 10:54:02 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Imrey", "Peter B.", ""], ["Dawid", "A. Philip", ""]]}, {"id": "1503.03879", "submitter": "Sanjay Chaudhuri", "authors": "Sanjay Chaudhuri", "title": "Qualitative inequalities for squared partial correlations of a Gaussian\n  random vector", "comments": "21 pages, 13 figures", "journal-ref": "Annals of the Institute of Statistical Mathematics, 66(2),\n  345-367, 2014", "doi": "10.1007/s10463-013-0417-x", "report-no": "Department of Statistics and Applied Probability, National\n  University of Singapore technical report 201301", "categories": "math.ST stat.AP stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe various sets of conditional independence relationships,\nsufficient for qualitatively comparing non-vanishing squared partial\ncorrelations of a Gaussian random vector. These sufficient conditions are\nsatisfied by several graphical Markov models. Rules for comparing degree of\nassociation among the vertices of such Gaussian graphical models are also\ndeveloped. We apply these rules to compare conditional dependencies on Gaussian\ntrees. In particular for trees, we show that such dependence can be completely\ncharacterized by the length of the paths joining the dependent vertices to each\nother and to the vertices conditioned on. We also apply our results to\npostulate rules for model selection for polytree models. Our rules apply to\nmutual information of Gaussian random vectors as well.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 20:15:35 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Chaudhuri", "Sanjay", ""]]}, {"id": "1503.03902", "submitter": "Derek Manuge", "authors": "D.J. Manuge", "title": "L\\'evy Processes For Finance: An Introduction In R", "comments": "18 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This brief manuscript provides an introduction to L\\'evy processes and their\napplications in finance as the random process that drives asset models.\nCharacteristic functions and random variable generators of popular L\\'evy\nprocesses are presented in R.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 22:11:04 GMT"}], "update_date": "2015-03-16", "authors_parsed": [["Manuge", "D. J.", ""]]}, {"id": "1503.03970", "submitter": "Tu Xu", "authors": "Tu Xu, Yixin Fang, Alan Rong, Junhui Wang", "title": "Flexible combination of multiple diagnostic biomarkers to improve\n  diagnostic accuracy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In medical research, it is common to collect information of multiple\ncontinuous biomarkers to improve the accuracy of diagnostic tests. Combining\nthe measurements of these biomarkers into one single score is a popular\npractice to integrate the collected information, where the accuracy of the\nresultant diagnostic test is usually improved. To measure the accuracy of a\ndiagnostic test, the Youden index has been widely used in literature. Various\nparametric and nonparametric methods have been proposed to linearly combine\nbiomarkers so that the corresponding Youden index can be optimized. Yet there\nseems to be little justification of enforcing such a linear combination. This\npaper proposes a flexible approach that allows both linear and nonlinear\ncombinations of biomarkers. The proposed approach formulates the problem in a\nlarge margin classification framework, where the combination function is\nembedded in a flexible reproducing kernel Hilbert space. Advantages of the\nproposed approach are demonstrated in a variety of simulated experiments as\nwell as a real application to a liver disorder study.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2015 07:19:04 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2015 19:59:38 GMT"}], "update_date": "2015-07-08", "authors_parsed": [["Xu", "Tu", ""], ["Fang", "Yixin", ""], ["Rong", "Alan", ""], ["Wang", "Junhui", ""]]}, {"id": "1503.04313", "submitter": "Shyam Mohan M", "authors": "Shyam Mohan M, Naren Naik, R.M.O. Gemson, M.R. Ananthasayanam", "title": "Introduction to the Kalman Filter and Tuning its Statistics for Near\n  Optimal Estimates and Cramer Rao Bound", "comments": "Technical Report (Dept. of Electrical Engineering, IIT Kanpur, India)", "journal-ref": null, "doi": null, "report-no": "TR/EE2015/401", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report provides a brief historical evolution of the concepts in the\nKalman filtering theory since ancient times to the present. A brief description\nof the filter equations its aesthetics, beauty, truth, fascinating perspectives\nand competence are described. For a Kalman filter design to provide optimal\nestimates tuning of its statistics namely initial state and covariance, unknown\nparameters, and state and measurement noise covariances is important. The\nearlier tuning approaches are reviewed. The present approach is a reference\nrecursive recipe based on multiple filter passes through the data without any\noptimization to reach a `statistical equilibrium' solution. It utilizes the a\npriori, a posteriori, and smoothed states, their corresponding predicted\nmeasurements and the actual measurements help to balance the measurement\nequation and similarly the state equation to help form a generalized likelihood\ncost function. The filter covariance at the end of each pass is heuristically\nscaled up by the number of data points is further trimmed to statistically\nmatch the exact estimates and Cramer Rao Bounds (CRBs) available with no\nprocess noise provided the initial covariance for subsequent passes. During\nsimulation studies with process noise the matching of the input and estimated\nnoise sequence over time and in real data the generalized cost functions helped\nto obtain confidence in the results. Simulation studies of a constant signal, a\nramp, a spring, mass, damper system with a weak non linear spring constant,\nlongitudinal and lateral motion of an airplane was followed by similar but more\ninvolved real airplane data was carried out in MATLAB. In all cases the present\napproach was shown to provide internally consistent and best possible estimates\nand their CRBs.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2015 15:24:14 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["M", "Shyam Mohan", ""], ["Naik", "Naren", ""], ["Gemson", "R. M. O.", ""], ["Ananthasayanam", "M. R.", ""]]}, {"id": "1503.04499", "submitter": "Mariela Fernandez", "authors": "M. Fern\\'andez and V. A. Gonz\\'alez-L\\'opez", "title": "Cumulative Conditional Expectation Index", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the cumulative conditional expectation function (CCEF)\nin the copula context. It is shown how to compute CCEF in terms of the\ncumulative copula function, this natural representation allows to deduce some\nuseful properties, for instance with applications to convex combination of\ncopulas. We introduce approximations of CCEF based on Bernstein polynomial\ncopulas. We introduce estimators for CCEF, which were constructed through\nBernstein polynomial estimators for copulas. The estimators are asymptotically\nnormal and biased for CCEF.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2015 02:01:40 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Fern\u00e1ndez", "M.", ""], ["Gonz\u00e1lez-L\u00f3pez", "V. A.", ""]]}, {"id": "1503.04682", "submitter": "Marie Doumic Jauffret", "authors": "H. T. Banks (CRSC), M Doumic (INRIA-Paris-Rocquencourt, LJLL), C Kruse\n  (INRIA-Paris-Rocquencourt, LJLL), S Prigent (INRIA-Paris-Rocquencourt, LJLL),\n  H Rezaei (VIM)", "title": "Information Content in Data Sets for a Nucleated-Polymerization Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AP cs.CE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We illustrate the use of tools (asymptotic theories of standard error\nquantification using appropriate statistical models, bootstrapping, model\ncomparison techniques) in addition to sensitivity that may be employed to\ndetermine the information content in data sets. We do this in the context of\nrecent models [23] for nucleated polymerization in proteins, about which very\nlittle is known regarding the underlying mechanisms; thus the methodology we\ndevelop here may be of great help to experimentalists.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2015 15:17:09 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Banks", "H. T.", "", "CRSC"], ["Doumic", "M", "", "INRIA-Paris-Rocquencourt, LJLL"], ["Kruse", "C", "", "INRIA-Paris-Rocquencourt, LJLL"], ["Prigent", "S", "", "INRIA-Paris-Rocquencourt, LJLL"], ["Rezaei", "H", "", "VIM"]]}, {"id": "1503.04701", "submitter": "Silv\\`ere Bonnabel", "authors": "Silv\\`ere Bonnabel and Axel Barrau", "title": "An intrinsic Cram\\'er-Rao bound on SO(3) for (dynamic) attitude\n  filtering", "comments": "To appear in the proceedings of IEEE Conference on Decision and\n  Control 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note an intrinsic version of the Cram\\'er-Rao bound on estimation\naccuracy is established on the Special Orthogonal group $SO(3)$. It is\nintrinsic in the sense that it does not rely on a specific choice of\ncoordinates on $SO(3)$: the result is derived using rotation matrices, but\nremains valid when using other parameterizations, such as quaternions. For any\nestimator $\\hat R$ of $R\\in SO(3)$ we give indeed a lower bound on the quantity\n$E(\\log(R\\hat R^T))$, that is, the estimation error expressed in terms of group\nmultiplication, whereas the usual estimation error $E(\\hat R-R)$ is meaningless\non $SO(3)$. The result is first applied to Whaba's problem. Then, we consider\nthe problem of a continuous-time nonlinear deterministic system on $SO(3)$ with\ndiscrete measurements subject to additive isotropic Gaussian noise, and we\nderive a lower bound to the estimation error covariance matrix. We prove the\nintrinsic Cram\\'er-Rao bound coincides with the covariance matrix returned by\nthe Invariant EKF, and thus can be computed online. This is in sharp contrast\nwith the general case, where the bound can only be computed if the true\ntrajectory of the system is known.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2015 16:03:23 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2015 10:07:24 GMT"}, {"version": "v3", "created": "Mon, 12 Oct 2015 08:35:36 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Bonnabel", "Silv\u00e8re", ""], ["Barrau", "Axel", ""]]}, {"id": "1503.04784", "submitter": "Yoav Ram", "authors": "Yoav Ram, Ofer Moshaioff, Idan Cohen, Omri Dor", "title": "Forecasting the Israeli 2015 elections using a smartphone application", "comments": "5 pages, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We developed a smartphone application, Ha'Midgam, to poll and forecast the\nresults of the 2015 Israeli elections. The application was downloaded by over\n7,500 people. We present the method used to control bias in our sample and our\nforecasts. We discuss limitations of our approach and suggest possible\nsolutions to control bias in similar applications.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2015 19:41:50 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Ram", "Yoav", ""], ["Moshaioff", "Ofer", ""], ["Cohen", "Idan", ""], ["Dor", "Omri", ""]]}, {"id": "1503.05160", "submitter": "Enayetur Raheem", "authors": "A. K. Md. Ehsanes Saleh and Enayetur Raheem", "title": "Improved LASSO", "comments": "17 pages, 12 figures, 24 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an improved LASSO estimation technique based on Stein-rule. We\nshrink classical LASSO estimator using preliminary test, shrinkage, and\npositive-rule shrinkage principle. Simulation results have been carried out for\nvarious configurations of correlation coefficients ($r$), size of the parameter\nvector ($\\beta$), error variance ($\\sigma^2$) and number of non-zero\ncoefficients ($k$) in the model parameter vector. Several real data examples\nhave been used to demonstrate the practical usefulness of the proposed\nestimators. Our study shows that the risk ordering given by LSE $>$ LASSO $>$\nStein-type LASSO $>$ Stein-type positive rule LASSO, remains the same uniformly\nin the divergence parameter $\\Delta^2$ as in the traditional case.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2015 18:47:28 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Saleh", "A. K. Md. Ehsanes", ""], ["Raheem", "Enayetur", ""]]}, {"id": "1503.05210", "submitter": "Alessandro Bessi", "authors": "Alessandro Bessi", "title": "Speeding up lower bound estimation in powerlaw distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The traditional lower bound estimation method for powerlaw distributions\nbased on the Kolmogorov-Smirnov distance proved to perform better than other\ncompeting methods. However, if applied to very large collections of data, such\na method can be computationally demanding. In this paper, we propose two\nalternative methods with the aim to reduce the time required by the estimation\nprocedure. We apply the traditional method and the two proposed methods to\nlarge collections of data ($N = 500,000$) with varying values of the true lower\nbound. Both the proposed methods yield a significantly better performance and\naccuracy than the traditional method.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2015 20:20:15 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Bessi", "Alessandro", ""]]}, {"id": "1503.05215", "submitter": "Adrian Raftery", "authors": "Hana \\v{S}ev\\v{c}\\'ikov\\'a, Nan Li, Vladim\\'ira Kantorov\\'a, Patrick\n  Gerland, Adrian E. Raftery", "title": "Age-Specific Mortality and Fertility Rates for Probabilistic Population\n  Projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The United Nations released official probabilistic population projections\n(PPP) for all countries for the first time in July 2014. These were obtained by\nprojecting the period total fertility rate (TFR) and life expectancy at birth\n($e_0$) using Bayesian hierarchical models, yielding a large set of future\ntrajectories of TFR and $e_0$ for all countries and future time periods to\n2100, sampled from their joint predictive distribution. Each trajectory was\nthen converted to age-specific mortality and fertility rates, and population\nwas projected using the cohort-component method. This yielded a large set of\ntrajectories of future age- and sex-specific population counts and vital rates\nfor all countries. In this paper we describe the methodology used for deriving\nthe age-specific mortality and fertility rates in the 2014 PPP, we identify\nlimitations of these methods, and we propose several methodological\nimprovements to overcome them. The methods presented in this paper are\nimplemented in the publicly available bayesPop R package.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2015 20:47:30 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["\u0160ev\u010d\u00edkov\u00e1", "Hana", ""], ["Li", "Nan", ""], ["Kantorov\u00e1", "Vladim\u00edra", ""], ["Gerland", "Patrick", ""], ["Raftery", "Adrian E.", ""]]}, {"id": "1503.05236", "submitter": "Alexis Hannart", "authors": "Alexis Hannart, Alberto Carrassi, Marc Bocquet, Michael Ghil, Philippe\n  Naveau, Manuel Pulido, Juan Ruiz, Pierre Tandeo", "title": "DADA: Data Assimilation for the Detection and Attribution of Weather-\n  and Climate-related Events", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new approach allowing for systematic causal attribution of\nweather and climate-related events, in near-real time. The method is purposely\ndesigned to facilitate its implementation at meteorological centers by relying\non data treatments that are routinely performed when numerically forecasting\nthe weather. Namely, we show that causal attribution can be obtained as a\nby-product of so-called data assimilation procedures that are run on a daily\nbasis to update the meteorological model with new atmospheric observations;\nhence, the proposed methodology can take advantage of the powerful\ncomputational and observational capacity of weather forecasting centers. We\nexplain the theoretical rationale of this approach and sketch the most\nprominent features of a \"data assimilation-based detection and attribution\"\n(DADA) procedure. The proposal is illustrated in the context of the classical\nthree-variable Lorenz model with additional forcing. Several theoretical and\npractical research questions that need to be addressed to make the proposal\nreadily operational within weather forecasting centers are finally laid out.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2015 22:18:49 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Hannart", "Alexis", ""], ["Carrassi", "Alberto", ""], ["Bocquet", "Marc", ""], ["Ghil", "Michael", ""], ["Naveau", "Philippe", ""], ["Pulido", "Manuel", ""], ["Ruiz", "Juan", ""], ["Tandeo", "Pierre", ""]]}, {"id": "1503.05237", "submitter": "William Morrow", "authors": "Minhua Long and W. Ross Morrow", "title": "Should Optimal Designers Worry About Consideration?", "comments": "5 figures, 26 pages. In Press at ASME Journal of Mechanical Design\n  (as of 3/17/15)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Consideration set formation using non-compensatory screening rules is a vital\ncomponent of real purchasing decisions with decades of experimental validation.\nMarketers have recently developed statistical methods that can estimate\nquantitative choice models that include consideration set formation via\nnon-compensatory screening rules. But is capturing consideration within models\nof choice important for design? This paper reports on a simulation study of a\nvehicle portfolio design when households screen over vehicle body style built\nto explore the importance of capturing consideration rules for optimal\ndesigners. We generate synthetic market share data, fit a variety of discrete\nchoice models to the data, and then optimize design decisions using the\nestimated models. Model predictive power, design \"error\", and profitability\nrelative to ideal profits are compared as the amount of market data available\nincreases. We find that even when estimated compensatory models provide\nrelatively good predictive accuracy, they can lead to sub-optimal design\ndecisions when the population uses consideration behavior; convergence of\ncompensatory models to non-compensatory behavior is likely to require\nunrealistic amounts of data; and modeling heterogeneity in non-compensatory\nscreening is more valuable than heterogeneity in compensatory trade-offs. This\nsupports the claim that designers should carefully identify consideration\nbehaviors before optimizing product portfolios. We also find that higher model\npredictive power does not necessarily imply better design decisions; that is,\ndifferent model forms can provide \"descriptive\" rather than \"predictive\"\ninformation that is useful for design.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2015 22:34:00 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Long", "Minhua", ""], ["Morrow", "W. Ross", ""]]}, {"id": "1503.05258", "submitter": "Wendy Li", "authors": "Wendy Li", "title": "A Probabilistic Simulation Based VaR Computation and Sensitivity\n  Analysis Method", "comments": "15 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new method to compute VaR (value at risk) and perform\ncorresponding variance based sensitivity analysis. VaR has a long history of\nbeing applied in stock price prediction and investment portfolio analysis.\nTraditional method, however, is mainly analytical, and has certain limitations.\nThe VaR simulation, on the other hand, provides more realistic analysis but is\nvery slow which affects the applications. This study proposes a new VaR\ncomputation method based on a probabilistic simulation technique called\nSimulation As You Operate (SAYO). It is always helpful to know the most\ninfluential factors in an investment, and thus a sensitivity analysis method\nbased on SAYO is also introduced to enhance investment analysis.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2015 01:21:09 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Li", "Wendy", ""]]}, {"id": "1503.05480", "submitter": "Frederic Pascal", "authors": "Alice Combernoux and Frederic Pascal and Guillaume Ginolhac and Marc\n  Lesturgie", "title": "Convergence of Structured Quadratic Forms With Application to\n  Theoretical Performances of Adaptive Filters in Low Rank Gaussian Context", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of deriving the asymptotic performance of\nadaptive Low Rank (LR) filters used in target detection embedded in a\ndisturbance composed of a LR Gaussian noise plus a white Gaussian noise. In\nthis context, we use the Signal to Interference to Noise Ratio (SINR) loss as\nperformance measure which is a function of the estimated projector onto the LR\nnoise subspace. However, although the SINR loss can be determined through\nMonte-Carlo simulations or real data, this process remains quite time\nconsuming. Thus, this paper proposes to predict the SINR loss behavior in order\nto not depend on the data anymore and be quicker. To derive this theoretical\nresult, previous works used a restrictive hypothesis assuming that the target\nis orthogonal to the LR noise. In this paper, we propose to derive this\ntheoretical performance by relaxing this hypothesis and using Random Matrix\nTheory (RMT) tools. These tools will be used to present the convergences of\nsimple quadratic forms and perform new RMT convergences of structured quadratic\nforms and SINR loss in the large dimensional regime, i.e. the size and the\nnumber of the data tend to infinity at the same rate. We show through\nsimulations the interest of our approach compared to the previous works when\nthe restrictive hypothesis is no longer verified.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2015 16:45:07 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Combernoux", "Alice", ""], ["Pascal", "Frederic", ""], ["Ginolhac", "Guillaume", ""], ["Lesturgie", "Marc", ""]]}, {"id": "1503.05526", "submitter": "Fabrice Rossi", "authors": "Tsirizo Rabenoro (SAMM), J\\'er\\^ome Lacaille, Marie Cottrell (SAMM),\n  Fabrice Rossi (SAMM)", "title": "Interpretable Aircraft Engine Diagnostic via Expert Indicator\n  Aggregation", "comments": "arXiv admin note: substantial text overlap with arXiv:1408.6214,\n  arXiv:1409.4747, arXiv:1407.0880", "journal-ref": "Transactions on Machine Learning and Data Mining, 2014, 7 (2),\n  pp.39-64", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting early signs of failures (anomalies) in complex systems is one of\nthe main goal of preventive maintenance. It allows in particular to avoid\nactual failures by (re)scheduling maintenance operations in a way that\noptimizes maintenance costs. Aircraft engine health monitoring is one\nrepresentative example of a field in which anomaly detection is crucial.\nManufacturers collect large amount of engine related data during flights which\nare used, among other applications, to detect anomalies. This article\nintroduces and studies a generic methodology that allows one to build automatic\nearly signs of anomaly detection in a way that builds upon human expertise and\nthat remains understandable by human operators who make the final maintenance\ndecision. The main idea of the method is to generate a very large number of\nbinary indicators based on parametric anomaly scores designed by experts,\ncomplemented by simple aggregations of those scores. A feature selection method\nis used to keep only the most discriminant indicators which are used as inputs\nof a Naive Bayes classifier. This give an interpretable classifier based on\ninterpretable anomaly detectors whose parameters have been optimized indirectly\nby the selection process. The proposed methodology is evaluated on simulated\ndata designed to reproduce some of the anomaly types observed in real world\nengines.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2015 18:30:34 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Rabenoro", "Tsirizo", "", "SAMM"], ["Lacaille", "J\u00e9r\u00f4me", "", "SAMM"], ["Cottrell", "Marie", "", "SAMM"], ["Rossi", "Fabrice", "", "SAMM"]]}, {"id": "1503.05798", "submitter": "Aurelien Latouche", "authors": "Juliette P\\'enichoux (CESP), Thierry Moreau (CESP), Aur\\'elien\n  Latouche (CEDRIC)", "title": "Simulating recurrent events that mimic actual data: a review of the\n  literature with emphasis on event-dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We conduct a review to assess how the simulation of repeated or recurrent\nevents are planned. For such multivariate time-to-events, it is well\nestablished that the underlying mechanism is likely to be complex and to\ninvolve in particular both heterogeneity in the population and\nevent-dependence. In this respect, we particularly focused on these two\ndimensions of events dynamic when mimicking actual data. Next, we investigate\nwhether the processes generated in the simulation studies have similar\nproperties to those expected in the clinical data of interest. Finally we\ndescribe a simulation scheme for generating data according to the timescale of\nchoice (gap time/ calendar) and to whether heterogeneity and/or\nevent-dependence are to be considered. The main finding is that\nevent-dependence is less widely considered in simulation studies than\nheterogeneity. This is unfortunate since the occurrence of an event may alter\nthe risk of occurrence of new events.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 15:17:30 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["P\u00e9nichoux", "Juliette", "", "CESP"], ["Moreau", "Thierry", "", "CESP"], ["Latouche", "Aur\u00e9lien", "", "CEDRIC"]]}, {"id": "1503.05826", "submitter": "Luis Enrique Correa Rocha Dr", "authors": "Luis Enrique Correa Rocha, Anna Ekeus Thorson, Renaud Lambiotte,\n  Fredrik Liljeros", "title": "Respondent-driven sampling bias induced by clustering and community\n  structure in social networks", "comments": "14 pages, 11 figures", "journal-ref": "J. R. Stat. Soc. A, 180: 99 (2017)", "doi": "10.1111/rssa.12180", "report-no": null, "categories": "stat.AP cs.SI physics.data-an physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sampling hidden populations is particularly challenging using standard\nsampling methods mainly because of the lack of a sampling frame.\nRespondent-driven sampling (RDS) is an alternative methodology that exploits\nthe social contacts between peers to reach and weight individuals in these\nhard-to-reach populations. It is a snowball sampling procedure where the weight\nof the respondents is adjusted for the likelihood of being sampled due to\ndifferences in the number of contacts. In RDS, the structure of the social\ncontacts thus defines the sampling process and affects its coverage, for\ninstance by constraining the sampling within a sub-region of the network. In\nthis paper we study the bias induced by network structures such as social\ntriangles, community structure, and heterogeneities in the number of contacts,\nin the recruitment trees and in the RDS estimator. We simulate different\nscenarios of network structures and response-rates to study the potential\nbiases one may expect in real settings. We find that the prevalence of the\nestimated variable is associated with the size of the network community to\nwhich the individual belongs. Furthermore, we observe that low-degree nodes may\nbe under-sampled in certain situations if the sample and the network are of\nsimilar size. Finally, we also show that low response-rates lead to reasonably\naccurate average estimates of the prevalence but generate relatively large\nbiases.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 16:21:23 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Rocha", "Luis Enrique Correa", ""], ["Thorson", "Anna Ekeus", ""], ["Lambiotte", "Renaud", ""], ["Liljeros", "Fredrik", ""]]}, {"id": "1503.06171", "submitter": "Yuting Ji", "authors": "Yuting Ji, Robert J. Thomas, and Lang Tong", "title": "Probabilistic Forecast of Real-Time LMP and Network Congestion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The short-term forecasting of real-time locational marginal price (LMP) and\nnetwork congestion is considered from a system operator perspective. A new\nprobabilistic forecasting technique is proposed based on a multiparametric\nprogramming formulation that partitions the uncertainty parameter space into\ncritical regions from which the conditional probability distribution of the\nreal-time LMP/congestion is obtained. The proposed method incorporates\nload/generation forecast, time varying operation constraints, and contingency\nmodels. By shifting the computation cost associated with multiparametric\nprograms offline, the online computation cost is significantly reduced. An\nonline simulation technique by generating critical regions dynamically is also\nproposed, which results in several orders of magnitude improvement in the\ncomputational cost over standard Monte Carlo methods.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2015 17:26:08 GMT"}, {"version": "v2", "created": "Sat, 25 Jun 2016 01:45:19 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Ji", "Yuting", ""], ["Thomas", "Robert J.", ""], ["Tong", "Lang", ""]]}, {"id": "1503.06266", "submitter": "Steven Pav", "authors": "Steven E. Pav", "title": "Moments of the log non-central chi-square distribution", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cumulants and moments of the log of the non-central chi-square\ndistribution are derived. For example, the expected log of a chi-square random\nvariable with v degrees of freedom is log(2) + psi(v/2). Applications to\nmodeling probability distributions are discussed.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2015 05:13:42 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Pav", "Steven E.", ""]]}, {"id": "1503.06267", "submitter": "Yong Huang", "authors": "Yong Huang and James L. Beck", "title": "Hierarchical sparse Bayesian learning: theory and application for\n  inferring structural damage from incomplete modal data", "comments": "41 pages, 8 figures, 3 tables. arXiv admin note: text overlap with\n  arXiv:1408.3685", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural damage due to excessive loading or environmental degradation\ntypically occurs in localized areas in the absence of collapse. This prior\ninformation about the spatial sparseness of structural damage is exploited here\nby a hierarchical sparse Bayesian learning framework with the goal of reducing\nthe source of ill-conditioning in the stiffness loss inversion problem for\ndamage detection. Sparse Bayesian learning methodologies automatically prune\naway irrelevant or inactive features from a set of potential candidates, and so\nthey are effective probabilistic tools for producing sparse explanatory\nsubsets. We have previously proposed such an approach to establish the\nprobability of localized stiffness reductions that serve as a proxy for damage\nby using noisy incomplete modal data from before and after possible damage. The\ncore idea centers on a specific hierarchical Bayesian model that promotes\nspatial sparseness in the inferred stiffness reductions in a way that is\nconsistent with the Bayesian Ockham razor. In this paper, we improve the theory\nof our previously proposed sparse Bayesian learning approach by eliminating an\napproximation and, more importantly, incorporating a constraint on stiffness\nincreases. Our approach has many appealing features that are summarized at the\nend of the paper. We validate the approach by applying it to the Phase II\nsimulated and experimental benchmark studies sponsored by the IASC-ASCE Task\nGroup on Structural Health Monitoring. The results show that it can reliably\ndetect, locate and assess damage by inferring substructure stiffness losses\nfrom the identified modal parameters. The occurrence of missed and false damage\nalerts is effectively suppressed.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2015 05:45:16 GMT"}], "update_date": "2015-03-29", "authors_parsed": [["Huang", "Yong", ""], ["Beck", "James L.", ""]]}, {"id": "1503.06575", "submitter": "Sanja Brdar", "authors": "Sanja Brdar, Katarina Gavric, Dubravko Culibrk, Vladimir Crnojevic", "title": "Unveiling Spatial Epidemiology of HIV with Mobile Phone Data", "comments": "13 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An increasing amount of geo-referenced mobile phone data enables the\nidentification of behavioral patterns, habits and movements of people. With\nthis data, we can extract the knowledge potentially useful for many\napplications including the one tackled in this study - understanding spatial\nvariation of epidemics. We explored the datasets collected by a cell phone\nservice provider and linked them to spatial HIV prevalence rates estimated from\npublicly available surveys. For that purpose, 224 features were extracted from\nmobility and connectivity traces and related to the level of HIV epidemic in 50\nIvory Coast departments. By means of regression models, we evaluated predictive\nability of extracted features. Several models predicted HIV prevalence that are\nhighly correlated (>0.7) with actual values. Through contribution analysis we\nidentified key elements that impact the rate of infections. Our findings\nindicate that night connectivity and activity, spatial area covered by users\nand overall migrations are strongly linked to HIV. By visualizing the\ncommunication and mobility flows, we strived to explain the spatial structure\nof epidemics. We discovered that strong ties and hubs in communication and\nmobility align with HIV hot spots.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2015 09:47:16 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Brdar", "Sanja", ""], ["Gavric", "Katarina", ""], ["Culibrk", "Dubravko", ""], ["Crnojevic", "Vladimir", ""]]}, {"id": "1503.06885", "submitter": "Mahendra Saha", "authors": "Mahendra Saha and Sudhansu S. Maiti", "title": "Trends and Practices in Process Capability Studies", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantifying the \"capability\" of a manufacturing process is an important\ninitial step in any quality improvement program. Capability is usually defined\nin dictionaries as \"the ability to carry out a task, to achieve an objective\".\nProcess capability indices(PCIs) is defined as a combination of materials,\nmethods, equipments and people engaged in producing a measurable output. PCIs\nwhich establish the relationships between the actual process performance and\nthe manufacturing specifications, have been a focus of research in quality\nassurance and process capability analysis. Capability indices that qualify\nprocess potential and process performance are practical tools for successful\nquality improvement activities and quality program implementation. As a matter\nof fact, all processes have inherent statistical variability, which can be\nidentified, evaluated and reduced by statistical methods. Generalized Process\nCapability Index, defined as the ratio of proportion of specification\nconformance (or, process yield) to proportion of desired (or, natural)\nconformance. We review the process capability indices in case of normal,\nnon-normal, discrete and multivariate process distributions and discuss the\ninferential aspects of some of these process capability indices. Relations\namong the process capability indices have also been illustrated with examples.\nFinally we also consider the process capability indices using conditional\nordering and transforming multivariate data to univariate one using the concept\nof structural function.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2015 01:20:17 GMT"}], "update_date": "2015-03-25", "authors_parsed": [["Saha", "Mahendra", ""], ["Maiti", "Sudhansu S.", ""]]}, {"id": "1503.07301", "submitter": "Andrzej Jarynowski", "authors": "Andrzej Buda, Andrzej Jarynowski", "title": "Exploring patterns in European singles charts", "comments": "7p+appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.SI nlin.PS stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  European singles charts are important part of the music industry responsible\nfor creating popularity of songs. After modeling and exploring dynamics of\nglobal album sales in previous papers, we investigate patterns of hit singles\npopularity according to all data (1966-2015) from weekly charts (polls) in 12\nWestern European countries. The dynamics of building popularity in various\nnational charts is more than the economy because it depends on spread of\ninformation. In our research we have shown how countries may be affected by\ntheir neighbourhood and influenced by technological era. We have also computed\ncorrelations with geographical and cultural distances between countries in\nanalog, digital and Internet era. We have shown that time delay between the\nsingle premiere and the peak of popularity has become shorter under the\ninfluence of technology and the popularity of songs depends on geographical\ndistances in analog (1966-1987) and Internet (2004-2015) era. On the other\nhand, cultural distances between nations have influenced the peaks of\npopularity, but in the Compact Disc era only (1988-2003). We have also\nindicated the European countries in line with global trends e.g. The\nNetherlands, the United Kingdom and outsiders like Italy and Spain.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2015 08:12:40 GMT"}], "update_date": "2015-03-26", "authors_parsed": [["Buda", "Andrzej", ""], ["Jarynowski", "Andrzej", ""]]}, {"id": "1503.07709", "submitter": "Benjamin Taylor", "authors": "Benjamin M. Taylor", "title": "Spatial Modelling of Emergency Service Response Times", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article concerns the statistical modelling of emergency service response\ntimes. We apply advanced methods from spatial survival analysis to deliver\ninference for data collected by the London Fire Brigade on response times to\nreported dwelling fires. Existing approaches to the analysis of these data have\nbeen mainly descriptive; we describe and demonstrate the advantages of a more\nsophisticated approach. Our final parametric proportional hazards model\nincludes harmonic regression terms to describe how response time varies with\ntime-of-day and shared spatially correlated frailties on an auxiliary grid for\ncomputational efficiency.\n  We investigate the short-term impact of fire station closures in 2014. Whilst\nthe London Fire Brigade are working hard to keep response times down, our\nfindings suggest there is a limit to what can be achieved logistically: the\npresent article identifies areas around the now closed Belsize, Downham,\nKingsland, Knightsbridge, Silvertown, Southwark, Wesminster and Woolwich fire\nstations in which there should perhaps be some concern as to the provision of\nfire services.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2015 12:43:55 GMT"}], "update_date": "2015-03-27", "authors_parsed": [["Taylor", "Benjamin M.", ""]]}, {"id": "1503.07711", "submitter": "Abisheva Adiya", "authors": "David Garcia, Adiya Abisheva, Simon Schweighofer, Uwe Serd\\\"ult and\n  Frank Schweitzer", "title": "Ideological and Temporal Components of Network Polarization in Online\n  Political Participatory Media", "comments": "35 pages, 11 figures, Internet, Policy & Politics Conference,\n  University of Oxford, Oxford, UK, 25-26 September 2014", "journal-ref": "Policy & Internet, 7(1) (2015)", "doi": "10.1002/poi3.82", "report-no": null, "categories": "stat.AP cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Political polarization is traditionally analyzed through the ideological\nstances of groups and parties, but it also has a behavioral component that\nmanifests in the interactions between individuals. We present an empirical\nanalysis of the digital traces of politicians in politnetz.ch, a Swiss online\nplatform focused on political activity, in which politicians interact by\ncreating support links, comments, and likes. We analyze network polarization as\nthe level of intra- party cohesion with respect to inter-party connectivity,\nfinding that supports show a very strongly polarized structure with respect to\nparty alignment. The analysis of this multiplex network shows that each layer\nof interaction contains relevant information, where comment groups follow\ntopics related to Swiss politics. Our analysis reveals that polarization in the\nlayer of likes evolves in time, increasing close to the federal elections of\n2011. Furthermore, we analyze the internal social network of each party through\nmetrics related to hierarchical structures, information efficiency, and social\nresilience. Our results suggest that the online social structure of a party is\nrelated to its ideology, and reveal that the degree of connectivity across two\nparties increases when they are close in the ideological space of a multi-party\nsystem.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2015 12:49:10 GMT"}], "update_date": "2015-03-27", "authors_parsed": [["Garcia", "David", ""], ["Abisheva", "Adiya", ""], ["Schweighofer", "Simon", ""], ["Serd\u00fclt", "Uwe", ""], ["Schweitzer", "Frank", ""]]}, {"id": "1503.07810", "submitter": "Jiaming Zeng", "authors": "Jiaming Zeng, Berk Ustun, Cynthia Rudin", "title": "Interpretable Classification Models for Recidivism Prediction", "comments": "45 pages, 17 figures", "journal-ref": "Journal of Royal Statistics - Series A (2017)", "doi": "10.1111/rssa.12227", "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a long-debated question, which is how to create predictive\nmodels of recidivism that are sufficiently accurate, transparent, and\ninterpretable to use for decision-making. This question is complicated as these\nmodels are used to support different decisions, from sentencing, to determining\nrelease on probation, to allocating preventative social services. Each use case\nmight have an objective other than classification accuracy, such as a desired\ntrue positive rate (TPR) or false positive rate (FPR). Each (TPR, FPR) pair is\na point on the receiver operator characteristic (ROC) curve. We use popular\nmachine learning methods to create models along the full ROC curve on a wide\nrange of recidivism prediction problems. We show that many methods (SVM, Ridge\nRegression) produce equally accurate models along the full ROC curve. However,\nmethods that designed for interpretability (CART, C5.0) cannot be tuned to\nproduce models that are accurate and/or interpretable. To handle this\nshortcoming, we use a new method known as SLIM (Supersparse Linear Integer\nModels) to produce accurate, transparent, and interpretable models along the\nfull ROC curve. These models can be used for decision-making for many different\nuse cases, since they are just as accurate as the most powerful black-box\nmachine learning models, but completely transparent, and highly interpretable.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2015 18:21:29 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2015 04:32:31 GMT"}, {"version": "v3", "created": "Fri, 13 Nov 2015 01:09:31 GMT"}, {"version": "v4", "created": "Fri, 6 May 2016 14:50:11 GMT"}, {"version": "v5", "created": "Fri, 10 Jun 2016 02:05:32 GMT"}, {"version": "v6", "created": "Fri, 8 Jul 2016 01:22:05 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Zeng", "Jiaming", ""], ["Ustun", "Berk", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1503.07826", "submitter": "Hao He", "authors": "Hao He and Pramod K. Varshney", "title": "Fusing Censored Dependent Data for Distributed Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a distributed detection problem for a censoring\nsensor network where each sensor's communication rate is significantly reduced\nby transmitting only \"informative\" observations to the Fusion Center (FC), and\ncensoring those deemed \"uninformative\". While the independence of data from\ncensoring sensors is often assumed in previous research, we explore spatial\ndependence among observations. Our focus is on designing the fusion rule under\nthe Neyman-Pearson (NP) framework that takes into account the spatial\ndependence among observations. Two transmission scenarios are considered, one\nwhere uncensored observations are transmitted directly to the FC and second\nwhere they are first quantized and then transmitted to further improve\ntransmission efficiency. Copula-based Generalized Likelihood Ratio Test (GLRT)\nfor censored data is proposed with both continuous and discrete messages\nreceived at the FC corresponding to different transmission strategies. We\naddress the computational issues of the copula-based GLRTs involving\nmultidimensional integrals by presenting more efficient fusion rules, based on\nthe key idea of injecting controlled noise at the FC before fusion. Although,\nthe signal-to-noise ratio (SNR) is reduced by introducing controlled noise at\nthe receiver, simulation results demonstrate that the resulting noise-aided\nfusion approach based on adding artificial noise performs very closely to the\nexact copula-based GLRTs. Copula-based GLRTs and their noise-aided counterparts\nby exploiting the spatial dependence greatly improve detection performance\ncompared with the fusion rule under independence assumption.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2015 18:58:17 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2015 03:22:07 GMT"}], "update_date": "2015-03-30", "authors_parsed": [["He", "Hao", ""], ["Varshney", "Pramod K.", ""]]}, {"id": "1503.08234", "submitter": "Danica Ommen", "authors": "Danica M. Ommen, Christopher P. Saunders, Cedric Neumann", "title": "A Note on the Specific Source Identification Problem in Forensic Science\n  in the Presence of Uncertainty about the Background Population", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A goal in the forensic interpretation of scientific evidence is to make an\ninference about the source of a trace of unknown origin. The evidence is\ncomposed of the following three elements: (a) the trace of unknown origin, (b)\na sample from a specific source, and (c) a collection of samples from the\nalternative source population. The inference process usually considers two\npropositions. The first proposition is usually referred to as the prosecution\nhypothesis and states that a given specific source is the actual source of the\ntrace of unknown origin. The second, usually referred to as the defense\nhypothesis, states that the actual source of the trace of unknown origin is\nanother source from a relevant alternative source population. One approach is\nto calculate a Bayes Factor for deciding between the two competing hypotheses.\nThis approach commonly assumes that the alternative source population is\ncompletely known or uses point estimates for its parameters. Contrary to this\ncommon approach, we propose a development that incorporates the uncertainty on\nthe alternative source population parameters in a reasonable and coherent\nmanner into the Bayes Factor. We will illustrate the resulting effects on the\ncalculation of several Bayes Factors for different situations with a\nwell-studied collection of samples of glass fragments.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2015 21:46:27 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Ommen", "Danica M.", ""], ["Saunders", "Christopher P.", ""], ["Neumann", "Cedric", ""]]}, {"id": "1503.08272", "submitter": "Yong Huang", "authors": "Yong Huang, James L. Beck, Stephen Wu, Hui Li", "title": "Robust Bayesian compressive sensing with data loss recovery for\n  structural health monitoring signals", "comments": "41 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of compressive sensing (CS) to structural health monitoring\nis an emerging research topic. The basic idea in CS is to use a\nspecially-designed wireless sensor to sample signals that are sparse in some\nbasis (e.g. wavelet basis) directly in a compressed form, and then to\nreconstruct (decompress) these signals accurately using some inversion\nalgorithm after transmission to a central processing unit. However, most\nsignals in structural health monitoring are only approximately sparse, i.e.\nonly a relatively small number of the signal coefficients in some basis are\nsignificant, but the other coefficients are usually not exactly zero. In this\ncase, perfect reconstruction from compressed measurements is not expected. A\nnew Bayesian CS algorithm is proposed in which robust treatment of the\nuncertain parameters is explored, including integration over the\nprediction-error precision parameter to remove it as a \"nuisance\" parameter.\nThe performance of the new CS algorithm is investigated using compressed data\nfrom accelerometers installed on a space-frame structure and on a cable-stayed\nbridge. Compared with other state-of-the-art CS methods including our\npreviously-published Bayesian method which uses MAP (maximum a posteriori)\nestimation of the prediction-error precision parameter, the new algorithm shows\nsuperior performance in reconstruction robustness and posterior uncertainty\nquantification. Furthermore, our method can be utilized for recovery of lost\ndata during wireless transmission, regardless of the level of sparseness in the\nsignal.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2015 06:14:45 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Huang", "Yong", ""], ["Beck", "James L.", ""], ["Wu", "Stephen", ""], ["Li", "Hui", ""]]}, {"id": "1503.08278", "submitter": "Stefano Favaro", "authors": "M. De Iorio, L.T. Elliott, S. Favaro, K. Adhikari, Y.W. Teh", "title": "Modeling population structure under hierarchical Dirichlet processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian nonparametric model to infer population admixture,\nextending the Hierarchical Dirichlet Process to allow for correlation between\nloci due to Linkage Disequilibrium. Given multilocus genotype data from a\nsample of individuals, the model allows inferring classifying individuals as\nunadmixed or admixed, inferring the number of subpopulations ancestral to an\nadmixed population and the population of origin of chromosomal regions. Our\nmodel does not assume any specific mutation process and can be applied to most\nof the commonly used genetic markers. We present a MCMC algorithm to perform\nposterior inference from the model and discuss methods to summarise the MCMC\noutput for the analysis of population admixture. We demonstrate the performance\nof the proposed model in simulations and in a real application, using genetic\ndata from the EDAR gene, which is considered to be ancestry-informative due to\nwell-known variations in allele frequency as well as phenotypic effects across\nancestry. The structure analysis of this dataset leads to the identification of\na rare haplotype in Europeans.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2015 07:41:16 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["De Iorio", "M.", ""], ["Elliott", "L. T.", ""], ["Favaro", "S.", ""], ["Adhikari", "K.", ""], ["Teh", "Y. W.", ""]]}, {"id": "1503.08445", "submitter": "Xing He", "authors": "Y. Cao, L. Cai, C. Qiu, J. Gu, X. He, Q. Ai, Z.Jin", "title": "A Random Matrix Theoretical Approach to Early Event Detection Using\n  Experimental Data", "comments": "4 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, High-dimensional data analysis methods are proposed to deal\nwith random matrix which is composed by the real data from power network before\nand after the fault. The mean spectral radius (MSR) of non-Hermitian random\nmatrices is defined as a statistic analytic for the fault detection. By\nanalyzing the characteristics of random matrices and observing the changes of\nthe spectral radius of random matrices, grid failure detection will be\nachieved. This paper describes the basic mathematical theory of this big data\nmethod, and the real-world data of a certain China power grid is used to verify\nthe methods.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2015 15:28:10 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Cao", "Y.", ""], ["Cai", "L.", ""], ["Qiu", "C.", ""], ["Gu", "J.", ""], ["He", "X.", ""], ["Ai", "Q.", ""], ["Jin", "Z.", ""]]}, {"id": "1503.08692", "submitter": "James Russell", "authors": "James C. Russell, Ephraim M. Hanks, Murali Haran", "title": "Dynamic Models of Animal Movement with Spatial Point Process\n  Interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When analyzing animal movement, it is important to account for interactions\nbetween individuals. However, statistical models for incorporating interaction\nbehavior in movement models are limited. We propose an approach that models\ndependent movement by augmenting a dynamic marginal movement model with a\nspatial point process interaction function within a weighted distribution\nframework. The approach is flexible, as marginal movement behavior and\ninteraction behavior can be modeled independently. Inference for model\nparameters is complicated by intractable normalizing constants. We develop a\ndouble Metropolis-Hastings algorithm to perform Bayesian inference. We\nillustrate our approach through the analysis of movement tracks of guppies\n(Poecilia reticulata)\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2015 14:45:18 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2015 14:47:31 GMT"}, {"version": "v3", "created": "Fri, 31 Jul 2015 22:25:35 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Russell", "James C.", ""], ["Hanks", "Ephraim M.", ""], ["Haran", "Murali", ""]]}, {"id": "1503.08886", "submitter": "Luis Carvalho", "authors": "Hunter Glanz, Xiaoman Huang, Minhui Zheng, and Luis E. Carvalho", "title": "A Bayesian Change Point Model for Detecting Land Cover Changes in MODIS\n  Time Series", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As both a central task in Remote Sensing and a common problem in many other\nsituations involving time series data, change point detection boasts a thorough\nand well-documented history of study. However, the treatment of missing data\nand proper exploitation of the structure in multivariate time series during\nchange point detection remains lacking. Multispectral, high temporal resolution\ntime series data from NASA's Moderate Resolution Imaging Spectroradiometer\n(MODIS) instruments provide an attractive and challenging context to contribute\nto the change point detection literature. In an effort to better monitor change\nin land cover using MODIS data, we present a novel approach to identifying\nperiods of time in which regions experience some conversion-type of land cover\nchange. That is, we propose a method for parameter estimation and change point\ndetection in the presence of missing data which capitalizes on the high\ndimensionality of MODIS data. We test the quality of our method in a simulation\nstudy alongside a contemporary change point method and apply it in a case study\nat the Xingu River Basin in the Amazon. Not only does our method maintain a\nhigh accuracy, but can provide insight into the types of changes occurring via\nland cover conversion probabilities. In this way we can better characterize the\namount and types of forest disturbance in our study area in comparison to\ntraditional change point methods.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2015 02:22:27 GMT"}], "update_date": "2015-04-01", "authors_parsed": [["Glanz", "Hunter", ""], ["Huang", "Xiaoman", ""], ["Zheng", "Minhui", ""], ["Carvalho", "Luis E.", ""]]}, {"id": "1503.09027", "submitter": "Hans Dembinski", "authors": "H.P. Dembinski, B. K\\'egl, I.C. Mari\\c{s}, M. Roth, D. Veberi\\v{c}", "title": "A likelihood method to cross-calibrate air-shower detectors", "comments": "10 pages, 7 figures", "journal-ref": "Astropart. Phys. 73 (2016) 44-51", "doi": "10.1016/j.astropartphys.2015.08.001", "report-no": null, "categories": "astro-ph.IM hep-ex stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a detailed statistical treatment of the energy calibration of\nhybrid air-shower detectors, which combine a surface detector array and a\nfluorescence detector, to obtain an unbiased estimate of the calibration curve.\nThe special features of calibration data from air showers prevent unbiased\nresults, if a standard least-squares fit is applied to the problem. We develop\na general maximum-likelihood approach, based on the detailed statistical model,\nto solve the problem. Our approach was developed for the Pierre Auger\nObservatory, but the applied principles are general and can be transferred to\nother air-shower experiments, even to the cross-calibration of other\nobservables. Since our general likelihood function is expensive to compute, we\nderive two approximations with significantly smaller computational cost. In the\nrecent years both have been used to calibrate data of the Pierre Auger\nObservatory. We demonstrate that these approximations introduce negligible bias\nwhen they are applied to simulated toy experiments, which mimic realistic\nexperimental conditions.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2015 12:53:02 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Dembinski", "H. P.", ""], ["K\u00e9gl", "B.", ""], ["Mari\u015f", "I. C.", ""], ["Roth", "M.", ""], ["Veberi\u010d", "D.", ""]]}]