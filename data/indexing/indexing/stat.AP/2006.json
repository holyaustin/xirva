[{"id": "2006.00062", "submitter": "Sarod Yatawatta", "authors": "Sarod Yatawatta", "title": "Polarization-based online interference mitigation in radio\n  interferometry", "comments": "EUSIPCO 2020 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mitigation of radio frequency interference (RFI) is essential to deliver\nscience-ready radio interferometric data to astronomers. In this paper, using\ndual polarized radio interferometers, we propose to use the polarization\ninformation of post-correlation interference signals to detect and mitigate\nthem. We use the directional statistics of the polarized signals as the\ndetection criteria and formulate a distributed, wideband spectrum sensing\nproblem. Using consensus optimization, we solve this in an online manner,\nworking with mini-batches of data. We present extensive results based on\nsimulations to demonstrate the feasibility of our method.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 20:17:24 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 08:56:26 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Yatawatta", "Sarod", ""]]}, {"id": "2006.00073", "submitter": "Nicholas Reich", "authors": "Stephen A Lauer, Alexandria C Brown, Nicholas G Reich", "title": "Infectious Disease Forecasting for Public Health", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Forecasting transmission of infectious diseases, especially for vector-borne\ndiseases, poses unique challenges for researchers. Behaviors of and\ninteractions between viruses, vectors, hosts, and the environment each play a\npart in determining the transmission of a disease. Public health surveillance\nsystems and other sources provide valuable data that can be used to accurately\nforecast disease incidence. However, many aspects of common infectious disease\nsurveillance data are imperfect: cases may be reported with a delay or in some\ncases not at all, data on vectors may not be available, and case data may not\nbe available at high geographical or temporal resolution. In the face of these\nchallenges, researchers must make assumptions to either account for these\nunderlying processes in a mechanistic model or to justify their exclusion\naltogether in a statistical model. Whether a model is mechanistic or\nstatistical, researchers should evaluate their model using accepted best\npractices from the emerging field of infectious disease forecasting while\nadopting conventions from other fields that have been developing forecasting\nmethods for decades. Accounting for assumptions and properly evaluating models\nwill allow researchers to generate forecasts that have the potential to provide\nvaluable insights for public health officials. This chapter provides a\nbackground to the practice of forecasting in general, discusses the biological\nand statistical models used for infectious disease forecasting, presents\ntechnical details about making and evaluating forecasting models, and explores\nthe issues in communicating forecasting results in a public health context.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 20:44:05 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Lauer", "Stephen A", ""], ["Brown", "Alexandria C", ""], ["Reich", "Nicholas G", ""]]}, {"id": "2006.00105", "submitter": "Guanyu Hu", "authors": "Guanyu Hu, Zhihua Ma, Insu Paek", "title": "A Nonparametric Bayesian Item Response Modeling Approach for Clustering\n  Items and Individuals Simultaneously", "comments": "21 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Item response theory (IRT) is a popular modeling paradigm for measuring\nsubject latent traits and item properties according to discrete responses in\ntests or questionnaires. There are very limited discussions on heterogeneity\npattern detection for both items and individuals. In this paper, we introduce a\nnonparametric Bayesian approach for clustering items and individuals\nsimultaneously under the Rasch model. Specifically, our proposed method is\nbased on the mixture of finite mixtures (MFM) model. MFM obtains the number of\nclusters and the clustering configurations for both items and individuals\nsimultaneously. The performance of parameters estimation and parameters\nclustering under the MFM Rasch model is evaluated by simulation studies, and a\nreal date set is applied to illustrate the MFM Rasch modeling.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 22:34:50 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Hu", "Guanyu", ""], ["Ma", "Zhihua", ""], ["Paek", "Insu", ""]]}, {"id": "2006.00111", "submitter": "Thiago Oliveira", "authors": "Thiago de Paula Oliveira, Rafael de Andrade Moral", "title": "Global Short-Term Forecasting of Covid-19 Cases", "comments": null, "journal-ref": null, "doi": "10.1038/s41598-021-87230-x", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The continuously growing number of COVID-19 cases pressures healthcare\nservices worldwide. Accurate short-term forecasting is thus vital to support\ncountry-level policy making. The strategies adopted by countries to combat the\npandemic vary, generating different uncertainty levels about the actual number\nof cases. Accounting for the hierarchical structure of the data and\naccommodating extra-variability is therefore fundamental. We introduce a new\nmodelling framework to describe the course of the pandemic with great accuracy,\nand provide short-term daily forecasts for every country in the world. We show\nthat our model generates highly accurate forecasts up to six days ahead, and\nuse estimated model components to cluster countries based on recent events. We\nintroduce statistical novelty in terms of modelling the autoregressive\nparameter as a function of time, increasing predictive power and flexibility to\nadapt to each country. Our model can also be used to forecast the number of\ndeaths, study the effects of covariates (such as lockdown policies), and\ngenerate forecasts for smaller regions within countries. Consequently, it has\nstrong implications for global planning and decision making. We constantly\nupdate forecasts and make all results freely available to any country in the\nworld through an online Shiny dashboard.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 22:53:36 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Oliveira", "Thiago de Paula", ""], ["Moral", "Rafael de Andrade", ""]]}, {"id": "2006.00135", "submitter": "Waleed Almutiry", "authors": "Waleed Almutiry, Vineetha Warriyar K V and Rob Deardon", "title": "Continuous Time Individual-Level Models of Infectious Disease: a Package\n  EpiILMCT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the R package EpiILMCT, which allows users to study the\nspread of infectious disease using continuous time individual level models\n(ILMs). The package provides tools for simulation from continuous time ILMs\nthat are based on either spatial demographic, contact network, or a combination\nof both of them, and for the graphical summarization of epidemics. Model\nfitting is carried out within a Bayesian Markov Chain Monte Carlo (MCMC)\nframework. The continuous time ILMs can be implemented within either\nsusceptible-infected-removed (SIR) or susceptible-infected-notified-removed\n(SINR) compartmental frameworks. As infectious disease data is often partially\nobserved, data uncertainties in the form of missing infection times - and in\nsome situations missing removal times - are accounted for using data\naugmentation techniques. The package is illustrated using both simulated and an\nexperimental data set on the spread of the tomato spotted wilt virus (TSWV)\ndisease.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 00:06:17 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Almutiry", "Waleed", ""], ["K", "Vineetha Warriyar", "V"], ["Deardon", "Rob", ""]]}, {"id": "2006.00140", "submitter": "Cristian Candia", "authors": "M. P. Raveau, J. P. Couyoumdjian, C. Fuentes-Bravo, C.\n  Rodriguez-Sickert, Cristian Candia", "title": "Citizens at the forefront of the constitutional debate: Participation\n  determinants and emergent content in Chile", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few decades, constitution-making processes have shifted from\nclosed elite writing to incorporating democratic mechanisms. Yet, little is\nknown about democratic participation in deliberative constitution-making\nprocesses. Here, we study a deliberative constituent process held by the\nChilean government between 2015 and 2016. The Chilean process had the highest\nlevel of citizen participation in the world ($204,402$ people, i.e., $1.3\\%$ of\nthe population) for such a process and covered $98\\%$ of the national\nterritory. In its participatory phase, people gathered in self-convoked groups\nof 10 to 30 members, and they collectively selected, deliberated, and wrote\ndown an argument on why the new constitution should include those social\nrights. To understand the citizen participation drivers in this volunteer\nprocess, we first identify the determinants at the municipality level. We find\nthe educational level, engagement in politics, support for the (left-wing)\ngovernment, and Internet access increased participation. In contrast,\npopulation density and the share of evangelical Christians decreased\nparticipation. Moreover, we do not find evidence of political manipulation on\ncitizen participation. In light of those determinants, we analyze the\ncollective selection of social rights, and the content produced during the\ndeliberative phase. The findings suggest that the knowledge embedded in cities,\nproxied using education levels and main economic activity, facilitates\ndeliberation about themes, concepts, and ideas. These results can inform the\norganization of new deliberative processes that involve voluntary citizen\nparticipation, from citizen consultations to constitution-making processes.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 00:47:25 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Raveau", "M. P.", ""], ["Couyoumdjian", "J. P.", ""], ["Fuentes-Bravo", "C.", ""], ["Rodriguez-Sickert", "C.", ""], ["Candia", "Cristian", ""]]}, {"id": "2006.00150", "submitter": "Travis Hee Wai", "authors": "Travis Hee Wai, Michael T. Young, Adam A. Szpiro", "title": "Random Spatial Forests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce random spatial forests, a method of bagging regression trees\nallowing for spatial correlation. Our main contribution is the development of a\ncomputationally efficient tree building algorithm which selects each split of\nthe tree adjusting for spatial correlation. We evaluate two different\napproaches for estimation of random spatial forests, a pseudo-likelihood\napproach combining random forests with kriging and a non-parametric version for\na general class of spatial smoothers. We show improved prediction accuracy of\nour method compared to existing two-step approaches combining random forests\nand kriging across a range of numerical simulations and demonstrate its\nperformance on elemental carbon, organic carbon, silicon, and sulfur\nmeasurements across the continental United States from 2009-2010.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 02:22:15 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 21:49:15 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Wai", "Travis Hee", ""], ["Young", "Michael T.", ""], ["Szpiro", "Adam A.", ""]]}, {"id": "2006.00243", "submitter": "Mohamed Haddouche", "authors": "Mohamed Anis Haddouche, Dominique Fourdrinier and Fatiha Mezoued", "title": "Scale matrix estimation under data-based loss in high and low dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the scale matrix $\\Sigma$ of the\nadditif model $Y_{p\\times n} = M + \\mathcal{E}$, under a theoretical decision\npoint of view. Here, $ p $ is the number of variables, $ n$ is the number of\nobservations, $ M $ is a matrix of unknown parameters with rank $q<p$ and $\n\\mathcal {E}$ is a random noise, whose distribution is elliptically symmetric\nwith covariance matrix proportional to $ I_n \\otimes \\Sigma $\\,. We deal with a\ncanonical form of this model where $Y$ is decomposed in two matrices, namely,\n$Z_{q\\times p}$ which summarizes the information contained in $ M $, and $\nU_{m\\times p}$, where $m=n-q$, which summarizes the sufficient information to\nestimate $ \\Sigma $. As the natural estimators of the form ${\\hat\n{\\Sigma}}_a=a\\, S$ (where $ S=U^{T}\\,U$ and $a$ is a positive constant) perform\npoorly when $p >m$ (S non-invertible), we propose estimators of the form\n${\\hat{\\Sigma}}_{a, G} = a\\big( S+ S \\, {S^{+}\\,G(Z,S)}\\big)$ where ${S^{+}}$\nis the Moore-Penrose inverse of $ S$ (which coincides with $S^{-1}$ when $S$ is\ninvertible). We provide conditions on the correction matrix $SS^{+}{G(Z,S)}$\nsuch that ${\\hat {\\Sigma}}_{a, G}$ improves over ${\\hat {\\Sigma}}_a$ under the\ndata-based loss $L _S( \\Sigma, \\hat { \\Sigma}) ={\\rm tr} \\big (\nS^{+}\\Sigma\\,({\\hat{\\Sigma}} \\, {\\Sigma} ^ {- 1} - {I}_ {p} )^ {2}\\big) $. We\nadopt a unified approach of the two cases where $ S$ is invertible ($p \\leq m$)\nand $ S$ is non-invertible ($p>m$).\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 11:49:46 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Haddouche", "Mohamed Anis", ""], ["Fourdrinier", "Dominique", ""], ["Mezoued", "Fatiha", ""]]}, {"id": "2006.00268", "submitter": "Yujie Hu", "authors": "Yujie Hu, Joni Downs", "title": "Measuring and Visualizing Place-Based Space-Time Job Accessibility", "comments": null, "journal-ref": "Journal of Transport Geography, 74, 278-288 (2019)", "doi": "10.1016/j.jtrangeo.2018.12.002", "report-no": null, "categories": "cs.CY econ.GN q-fin.EC stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Place-based accessibility measures, such as the gravity-based model, are\nwidely applied to study the spatial accessibility of workers to job\nopportunities in cities. However, gravity-based measures often suffer from\nthree main limitations: (1) they are sensitive to the spatial configuration and\nscale of the units of analysis, which are not specifically designed for\ncapturing job accessibility patterns and are often too coarse; (2) they omit\nthe temporal dynamics of job opportunities and workers in the calculation,\ninstead assuming that they remain stable over time; and (3) they do not lend\nthemselves to dynamic geovisualization techniques. In this paper, a new\nmethodological framework for measuring and visualizing place-based job\naccessibility in space and time is presented that overcomes these three\nlimitations. First, discretization and dasymetric mapping approaches are used\nto disaggregate counts of jobs and workers over specific time intervals to a\nfine-scale grid. Second, Shen (1998) gravity-based accessibility measure is\nmodified to account for temporal fluctuations in the spatial distributions of\nthe supply of jobs and the demand of workers and is used to estimate hourly job\naccessibility at each cell. Third, a four-dimensional volumetric rendering\napproach is employed to integrate the hourly job access estimates into a\nspace-time cube environment, which enables the users to interactively visualize\nthe space-time job accessibility patterns. The integrated framework is\ndemonstrated in the context of a case study of the Tampa Bay region of Florida.\nThe findings demonstrate the value of the proposed methodology in job\naccessibility analysis and the policy-making process.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 13:39:07 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Hu", "Yujie", ""], ["Downs", "Joni", ""]]}, {"id": "2006.00271", "submitter": "Yujie Hu", "authors": "Georgios P. Balomenos, Yujie Hu, Jamie E. Padgett, Kyle Shelton", "title": "Impact of Coastal Hazards on Residents Spatial Accessibility to Health\n  Services", "comments": null, "journal-ref": "Journal of Infrastructure Systems, 25(4), 04019028 (2019)", "doi": "10.1061/(ASCE)IS.1943-555X.0000509", "report-no": null, "categories": "cs.CY stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The mobility of residents and their access to essential services can be\nhighly affected by transportation network closures that occur during and after\ncoastal hazard events. Few studies have used geographic information systems\ncoupled with infrastructure vulnerability models to explore how spatial\naccessibility to goods and services shifts after a hurricane. Models that\nexplore spatial accessibility to health services are particularly lacking. This\nstudy provides a framework to examine how the disruption of transportation\nnetworks during and after a hurricane can impact a residents ability to access\nhealth services over time. Two different bridge closure conditions, inundation\nand structural failure, along with roadway inundation are used to quantify\npost-hurricane accessibility at short- and long-term temporal scales.\nInundation may close a bridge for hours or days, but a structural failure may\nclose a route for weeks or months. Both forms of closure are incorporated using\nprobabilistic vulnerability models coupled with GIS-based models to assess\nspatial accessibility in the aftermath of a coastal hazard. Harris County, an\narea in Southeastern Texas prone to coastal hazards, is used as a case study.\nThe results indicate changes in the accessibility scores of specific areas\ndepending on the temporal scale of interest and intensity of the hazard\nscenario. Sociodemographic indicators are also examined for the study region,\nrevealing the populations most likely to suffer from lack of accessibility.\nOverall, the presented framework helps to understand how both short-term\nfunctionality loss and long-term damage affect access to critical services such\nas health care after a hazard. This information, in turn, can shape decisions\nabout future mitigation and planning efforts, while the presented framework can\nbe expanded to other hazard-prone areas.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 13:47:09 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Balomenos", "Georgios P.", ""], ["Hu", "Yujie", ""], ["Padgett", "Jamie E.", ""], ["Shelton", "Kyle", ""]]}, {"id": "2006.00272", "submitter": "Yujie Hu", "authors": "Yujie Hu, Fahui Wang, Cecile Guin, Haojie Zhu", "title": "A Spatio-Temporal Kernel Density Estimation Framework for Predictive\n  Crime Hotspot Mapping and Evaluation", "comments": null, "journal-ref": "Applied Geography, 99, 89-97 (2018)", "doi": "10.1016/j.apgeog.2018.08.001", "report-no": null, "categories": "stat.AP cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Predictive hotspot mapping plays a critical role in hotspot policing.\nExisting methods such as the popular kernel density estimation (KDE) do not\nconsider the temporal dimension of crime. Building upon recent works in related\nfields, this article proposes a spatio-temporal framework for predictive\nhotspot mapping and evaluation. Comparing to existing work in this scope, the\nproposed framework has four major features: (1) a spatio-temporal kernel\ndensity estimation (STKDE) method is applied to include the temporal component\nin predictive hotspot mapping, (2) a data-driven optimization technique, the\nlikelihood cross-validation, is used to select the most appropriate bandwidths,\n(3) a statistical significance test is designed to filter out false positives\nin the density estimates, and (4) a new metric, the predictive accuracy index\n(PAI) curve, is proposed to evaluate predictive hotspots at multiple areal\nscales. The framework is illustrated in a case study of residential burglaries\nin Baton Rouge, Louisiana in 2011, and the results validate its utility.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 13:51:05 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Hu", "Yujie", ""], ["Wang", "Fahui", ""], ["Guin", "Cecile", ""], ["Zhu", "Haojie", ""]]}, {"id": "2006.00275", "submitter": "Yujie Hu", "authors": "Yujie Hu, Fahui Wang, Imam Xierali", "title": "Automated Delineation of Hospital Service Areas and Hospital Referral\n  Regions by Modularity Optimization", "comments": null, "journal-ref": "Health Services Research, 53(1), 236-255 (2018)", "doi": "10.1111/1475-6773.12616", "report-no": null, "categories": "stat.AP cs.CY physics.soc-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Objective. To develop an automated, data-driven, and scale-flexible method to\ndelineate HSAs and HRRs that are up-to-date, representative of all patients,\nand have the optimal localization of hospital visits. Data Sources. The 2011\nState Inpatient Database (SID) in Florida from the Healthcare Cost and\nUtilization Project (HCUP). Study Design. A network optimization method was\nused to redefine HSAs and HRRs by maximizing patient-to-hospital flows within\neach HSA/HRR while minimizing flows between them. We first constructed as many\nHSAs/HRRs as existing Dartmouth units in Florida, and then compared the two by\nvarious metrics. Next, we sought to derive the optimal numbers and\nconfigurations of HSAs/HRRs that best reflect the modularity of hospitalization\npatterns in Florida. Principal Findings. The HSAs/HRRs by our method are\nfavored over the Dartmouth units in balance of region size and market\nstructure, shape, and most importantly, local hospitalization. Conclusions. The\nnew method is automated, scale-flexible, and effective in capturing the natural\nstructure of healthcare system. It has great potential for applications in\ndelineating other healthcare service areas or in larger geographic regions.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 13:58:29 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Hu", "Yujie", ""], ["Wang", "Fahui", ""], ["Xierali", "Imam", ""]]}, {"id": "2006.00326", "submitter": "Ander Wilson", "authors": "Ander Wilson, Jessica Tryner, Christian L'Orange, John Volckens", "title": "Bayesian Nonparametric Monotone Regression", "comments": null, "journal-ref": "Environmetrics 2020", "doi": "10.1002/env.2642", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications there is interest in estimating the relation between a\npredictor and an outcome when the relation is known to be monotone or otherwise\nconstrained due to the physical processes involved. We consider one such\napplication--inferring time-resolved aerosol concentration from a low-cost\ndifferential pressure sensor. The objective is to estimate a monotone function\nand make inference on the scaled first derivative of the function. We proposed\nBayesian nonparametric monotone regression which uses a Bernstein polynomial\nbasis to construct the regression function and puts a Dirichlet process prior\non the regression coefficients. The base measure of the Dirichlet process is a\nfinite mixture of a mass point at zero and a truncated normal. This\nconstruction imposes monotonicity while clustering the basis functions.\nClustering the basis functions reduces the parameter space and allows the\nestimated regression function to be linear. With the proposed approach we can\nmake closed-formed inference on the derivative of the estimated function\nincluding full quantification of uncertainty. In a simulation study the\nproposed method performs similar to other monotone regression approaches when\nthe true function is wavy but performs better when the true function is linear.\nWe apply the method to estimate time-resolved aerosol concentration with a\nnewly-developed portable aerosol monitor. The R package bnmr is made available\nto implement the method.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 17:43:10 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Wilson", "Ander", ""], ["Tryner", "Jessica", ""], ["L'Orange", "Christian", ""], ["Volckens", "John", ""]]}, {"id": "2006.00335", "submitter": "Siddharth Arora Dr.", "authors": "Siddharth Arora, James W. Taylor, Ho-Yin Mak", "title": "Probabilistic Forecasting of Patient Waiting Times in an Emergency\n  Department", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.35079.01443", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the estimation of the probability distribution of individual patient\nwaiting times in an emergency department (ED). Our feature-rich modelling\nallows for dynamic updating and refinement of waiting time estimates as\npatient- and ED-specific information (e.g., patient condition, ED congestion\nlevels) is revealed during the waiting process. Aspects relating to\ncommunicating forecast uncertainty to patients, and implementing this\nmethodology in practice, are also discussed.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 19:03:07 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Arora", "Siddharth", ""], ["Taylor", "James W.", ""], ["Mak", "Ho-Yin", ""]]}, {"id": "2006.00366", "submitter": "J. Ricardo G. Mendon\\c{c}a", "authors": "J. Ricardo G. Mendon\\c{c}a", "title": "A numerical investigation into the scaling behavior of the longest\n  increasing subsequences of the symmetric ultra-fat tailed random walk", "comments": "A brief report on the LIS of ultra-fat tailed random walks, the\n  construction of which may be of independent interest. Accepted for\n  publication in Physics Letters A (2020)", "journal-ref": "Physics Letters A 384 (2020) 126753", "doi": "10.1016/j.physleta.2020.126753", "report-no": null, "categories": "cond-mat.stat-mech math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The longest increasing subsequence (LIS) of a sequence of correlated random\nvariables is a basic quantity with potential applications that has started to\nreceive proper attention only recently. Here we investigate the behavior of the\nlength of the LIS of the so-called symmetric ultra-fat tailed random walk,\nintroduced earlier in an abstract setting in the mathematical literature. After\nexplicit constructing the ultra-fat tailed random walk, we found numerically\nthat the expected length $L_{n}$ of its LIS scales with the length $n$ of the\nwalk like $\\langle L_{n} \\rangle \\sim n^{0.716}$, indicating that, indeed, as\nfar as the behavior of the LIS is concerned the ultra-fat tailed distribution\ncan be thought of as equivalent to a very heavy tailed $\\alpha$-stable\ndistribution. We also found that the distribution of $L_{n}$ seems to be\nuniversal, in agreement with results obtained for other heavy tailed random\nwalks.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 21:09:28 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 16:01:49 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Mendon\u00e7a", "J. Ricardo G.", ""]]}, {"id": "2006.00426", "submitter": "Lingzhou Xue", "authors": "Xiufan Yu, Danning Li, and Lingzhou Xue", "title": "Fisher's combined probability test for high-dimensional covariance\n  matrices", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing large covariance matrices is of fundamental importance in statistical\nanalysis with high-dimensional data. In the past decade, three types of test\nstatistics have been studied in the literature: quadratic form statistics,\nmaximum form statistics, and their weighted combination. It is known that\nquadratic form statistics would suffer from low power against sparse\nalternatives and maximum form statistics would suffer from low power against\ndense alternatives. The weighted combination methods were introduced to enhance\nthe power of quadratic form statistics or maximum form statistics when the\nweights are appropriately chosen. In this paper, we provide a new perspective\nto exploit the full potential of quadratic form statistics and maximum form\nstatistics for testing high-dimensional covariance matrices. We propose a\nscale-invariant power enhancement test based on Fisher's method to combine the\np-values of quadratic form statistics and maximum form statistics. After\ncarefully studying the asymptotic joint distribution of quadratic form\nstatistics and maximum form statistics, we prove that the proposed combination\nmethod retains the correct asymptotic size and boosts the power against more\ngeneral alternatives. Moreover, we demonstrate the finite-sample performance in\nsimulation studies and a real application.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 03:32:26 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Yu", "Xiufan", ""], ["Li", "Danning", ""], ["Xue", "Lingzhou", ""]]}, {"id": "2006.00487", "submitter": "Kun Chen", "authors": "Xiaokang Liu, Xiaomei Cong, Gen Li, Kendra Maas, Kun Chen", "title": "Multivariate Log-Contrast Regression with Sub-Compositional Predictors:\n  Testing the Association Between Preterm Infants' Gut Microbiome and\n  Neurobehavioral Outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The so-called gut-brain axis has stimulated extensive research on\nmicrobiomes. One focus is to assess the association between certain clinical\noutcomes and the relative abundances of gut microbes, which can be presented as\nsub-compositional data in conformity with the taxonomic hierarchy of bacteria.\nMotivated by a study for identifying the microbes in the gut microbiome of\npreterm infants that impact their later neurobehavioral outcomes, we formulate\na constrained integrative multi-view regression, where the neurobehavioral\nscores form multivariate response, the sub-compositional microbiome data form\nmulti-view feature matrices, and a set of linear constraints on their\ncorresponding sub-coefficient matrices ensures the conformity to the simplex\ngeometry. To enable joint selection and inference of sub-compositions/views, we\nassume all the sub-coefficient matrices are possibly of low-rank, i.e., the\noutcomes are associated with the microbiome through different sets of latent\nsub-compositional factors from different taxa. We propose a scaled composite\nnuclear norm penalization approach for model estimation and develop a\nhypothesis testing procedure through de-biasing to assess the significance of\ndifferent views. Simulation studies confirm the effectiveness of the proposed\nprocedure. In the preterm infant study, the identified microbes are mostly\nconsistent with existing studies and biological understandings. Our approach\nsupports that stressful early life experiences imprint gut microbiome through\nthe regulation of the gut-brain axis.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 10:24:44 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Liu", "Xiaokang", ""], ["Cong", "Xiaomei", ""], ["Li", "Gen", ""], ["Maas", "Kendra", ""], ["Chen", "Kun", ""]]}, {"id": "2006.00523", "submitter": "Xueqin Wang", "authors": "Ting Tian (1), Wenxiang Luo (1), Yukang Jiang (1), Minqiong Chen (1),\n  Canhong Wen (2), Wenliang Pan (1), Xueqin Wang (2) ((1) School of\n  Mathematics, Sun Yat-sen University, (2) School of Management, University of\n  Science and Technology of China)", "title": "The Effects of Stringent Interventions for Coronavirus Pandemic", "comments": "29 pages, 6 figures, Ting Tian, Wenxiang Luo and Yukang Jiang\n  contributed equally to this article", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph q-bio.PE stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The pandemic of COVID-19 has caused severe public health consequences around\nthe world. Many interventions of COVID-19 have been implemented. It is of great\npublic health and societal importance to evaluate the effects of interventions\nin the pandemic of COVID-19. In this paper, with help of synthetic control\nmethod, regression discontinuity and a Susceptible-Infected and infectious\nwithout isolation-Hospitalized in isolation-Removed (SIHR) model, we evaluate\nthe horizontal and longitudinal effects of stringent interventions implemented\nin Wenzhou, a representative urban city of China, where stringent interventions\nwere enforced to curb its own epidemic situation with rapidly increasing newly\nconfirmed cases. We found that there were statistically significant treatment\neffects of those stringent interventions which reduced the cumulative confirmed\ncases of COVID-19. Those reduction effects would increase over time. Also, if\nthe stringent interventions were delayed by 2 days or mild interventions were\nimplemented instead, the expected number of cumulative confirmed cases would\nhave been nearly 2 times or 5 times of the actual number. The effects of\nstringent interventions are significant in mitigating the epidemic situation of\nCOVID-19. The slower the interventions were implemented, the more severe the\nepidemic would have been, and the stronger the interventions would have been\nrequired.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 13:45:15 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Tian", "Ting", ""], ["Luo", "Wenxiang", ""], ["Jiang", "Yukang", ""], ["Chen", "Minqiong", ""], ["Wen", "Canhong", ""], ["Pan", "Wenliang", ""], ["Wang", "Xueqin", ""]]}, {"id": "2006.00567", "submitter": "Weichi Yao", "authors": "Weichi Yao and Halina Frydman and Denis Larocque and Jeffrey S.\n  Simonoff", "title": "Ensemble Methods for Survival Data with Time-Varying Covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Survival data with time-varying covariates are common in practice. If\nrelevant, such covariates can improve on the estimation of a survival function.\nHowever, the traditional survival forests - conditional inference forest,\nrelative risk forest and random survival forest - have accommodated only\ntime-invariant covariates.\n  We generalize the conditional inference and relative risk forests to allow\ntime-varying covariates. We compare their performance with that of the extended\nCox model, a commonly used method, and the transformation forest method,\ndesigned to detect non-proportional hazards deviations and adapted here to\naccommodate time-varying covariates, through a comprehensive simulation study\nin which the Kaplan-Meier estimate serves as a benchmark and the integrated L2\ndifference between the true and estimated survival functions is used for\nevaluation.\n  In general, the performance of the two proposed forests substantially\nimproves over the Kaplan-Meier estimate. Under the proportional-hazard setting,\nthe best method is always one of the two proposed forests, while under the\nnon-proportional hazards setting, it is the adapted transformation forest. We\nuse K-fold cross-validation to choose between the methods, which is shown to be\nan effective tool to provide guidance in practice. The performance of the\nproposed forest methods for time-invariant covariate data is broadly similar to\nthat found for time-varying covariate data.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 17:25:04 GMT"}, {"version": "v2", "created": "Sat, 31 Oct 2020 16:51:29 GMT"}, {"version": "v3", "created": "Wed, 3 Mar 2021 17:33:17 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Yao", "Weichi", ""], ["Frydman", "Halina", ""], ["Larocque", "Denis", ""], ["Simonoff", "Jeffrey S.", ""]]}, {"id": "2006.00707", "submitter": "Emaad Manzoor", "authors": "Emaad Manzoor, George H. Chen, Dokyun Lee, Michael D. Smith", "title": "Influence via Ethos: On the Persuasive Power of Reputation in\n  Deliberation Online", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.CL stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deliberation among individuals online plays a key role in shaping the\nopinions that drive votes, purchases, donations and other critical offline\nbehavior. Yet, the determinants of opinion-change via persuasion in\ndeliberation online remain largely unexplored. Our research examines the\npersuasive power of $\\textit{ethos}$ -- an individual's \"reputation\" -- using a\n7-year panel of over a million debates from an argumentation platform\ncontaining explicit indicators of successful persuasion. We identify the causal\neffect of reputation on persuasion by constructing an instrument for reputation\nfrom a measure of past debate competition, and by controlling for unstructured\nargument text using neural models of language in the double machine-learning\nframework. We find that an individual's reputation significantly impacts their\npersuasion rate above and beyond the validity, strength and presentation of\ntheir arguments. In our setting, we find that having 10 additional reputation\npoints causes a 31% increase in the probability of successful persuasion over\nthe platform average. We also find that the impact of reputation is moderated\nby characteristics of the argument content, in a manner consistent with a\ntheoretical model that attributes the persuasive power of reputation to\nheuristic information-processing under cognitive overload. We discuss\nmanagerial implications for platforms that facilitate deliberative\ndecision-making for public and private organizations online.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 04:25:40 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Manzoor", "Emaad", ""], ["Chen", "George H.", ""], ["Lee", "Dokyun", ""], ["Smith", "Michael D.", ""]]}, {"id": "2006.00717", "submitter": "Benjamin Avanzi", "authors": "Benjamin Avanzi and Hayden Lau and Bernard Wong", "title": "On the optimality of joint periodic and extraordinary dividend\n  strategies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we model the cash surplus (or equity) of a risky business with\na Brownian motion. Owners can take cash out of the surplus in the form of\n\"dividends\", subject to transaction costs. However, if the surplus hits 0 then\nruin occurs and the business cannot operate any more.\n  We consider two types of dividend distributions: (i) periodic, regular ones\n(that is, dividends can be paid only at countable many points in time,\naccording to a specific arrival process); and (ii) extraordinary dividend\npayments that can be made immediately at any time (that is, the dividend\ndecision time space is continuous and matches that of the surplus process).\nBoth types of dividends attract proportional transaction costs, and\nextraordinary distributions also attracts fixed transaction costs, a realistic\nfeature. A dividend strategy that involves both types of distributions\n(periodic and extraordinary) is qualified as \"hybrid\".\n  We determine which strategies (either periodic, immediate, or hybrid) are\noptimal, that is, we show which are the strategies that maximise the expected\npresent value of dividends paid until ruin, net of transaction costs.\nSometimes, a liquidation strategy (which pays out all monies and stops the\nprocess) is optimal. Which strategy is optimal depends on the profitability of\nthe business, and the level of (proportional and fixed) transaction costs.\nResults are illustrated.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 04:52:08 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 04:41:50 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Avanzi", "Benjamin", ""], ["Lau", "Hayden", ""], ["Wong", "Bernard", ""]]}, {"id": "2006.00741", "submitter": "Edgar Santos-Fernandez", "authors": "Edgar Santos-Fernandez, Erin E. Peterson, Julie Vercelloni, Em\n  Rushworth, Kerrie Mengersen", "title": "Correcting misclassification errors in crowdsourced ecological data: A\n  Bayesian perspective", "comments": "18 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many research domains use data elicited from \"citizen scientists\" when a\ndirect measure of a process is expensive or infeasible. However, participants\nmay report incorrect estimates or classifications due to their lack of skill.\nWe demonstrate how Bayesian hierarchical models can be used to learn about\nlatent variables of interest, while accounting for the participants' abilities.\nThe model is described in the context of an ecological application that\ninvolves crowdsourced classifications of georeferenced coral-reef images from\nthe Great Barrier Reef, Australia. The latent variable of interest is the\nproportion of coral cover, which is a common indicator of coral reef health.\nThe participants' abilities are expressed in terms of sensitivity and\nspecificity of a correctly classified set of points on the images. The model\nalso incorporates a spatial component, which allows prediction of the latent\nvariable in locations that have not been surveyed. We show that the model\noutperforms traditional weighted-regression approaches used to account for\nuncertainty in citizen science data. Our approach produces more accurate\nregression coefficients and provides a better characterization of the latent\nprocess of interest. This new method is implemented in the probabilistic\nprogramming language Stan and can be applied to a wide number of problems that\nrely on uncertain citizen science data.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 06:41:24 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Santos-Fernandez", "Edgar", ""], ["Peterson", "Erin E.", ""], ["Vercelloni", "Julie", ""], ["Rushworth", "Em", ""], ["Mengersen", "Kerrie", ""]]}, {"id": "2006.00971", "submitter": "Aristides Moustakas", "authors": "Aristides Moustakas", "title": "Internet search effort on Covid-19 and the underlying public\n  interventions and epidemiological status", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disease spread is a complex phenomenon requiring an interdisciplinary\napproach. Covid-19 exhibited a global spatial spread in a very short time frame\nresulting in a global pandemic. Data of web search effort in Greece on Covid-19\nas a topic for one year on a weekly temporal scale were analyzed using\ngovernmental intervention measures such a s school closures, movement\nrestrictions, national and international travelling restrictions, stay at home\nrequirements, mask requirements, financial support measures, and\nepidemiological variables such as new cases and new deaths as potential\nexplanatory covariates. The relationship between web search effort on Covid-19\nand the 16 in total explanatory covariates was analyzed with machine learning.\nWeb search in time was compared with the corresponding epidemiological\nsituation, expressed by the Rt at the same week. Results indicated that the\ntrained model exhibited a fit of R2 = 91% between the actual and predicted web\nsearch effort. The top five variables for predicting web search effort were new\ndeaths, the opening of international borders to non-Greek nationals, new cases,\ntesting policy, and restrictions in internal movements. Web search peaked\nduring the same weeks that the Rt was peaking although new deaths or new cases\nwere not peaking during those dates, and Rt rarely is reported in public media.\nAs both web search effort and Rt peaked during 1-15 August 2020, the peak of\nthe tourist season, the implications of this are discussed.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 17:55:02 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 14:17:02 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Moustakas", "Aristides", ""]]}, {"id": "2006.01002", "submitter": "Kei Hirose", "authors": "Kei Hirose", "title": "Interpretable modeling for short- and medium-term electricity load\n  forecasting", "comments": "29 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of short- and medium-term electricity load\nforecasting by using past loads and daily weather forecast information.\nConventionally, many researchers have directly applied regression analysis.\nHowever, interpreting the effect of weather on these loads is difficult with\nthe existing methods. In this study, we build a statistical model that resolves\nthis interpretation issue. A varying coefficient model with basis expansion is\nused to capture the nonlinear structure of the weather effect. This approach\nresults in an interpretable model when the regression coefficients are\nnonnegative. To estimate the nonnegative regression coefficients, we employ\nnonnegative least squares. Three real data analyses show the practicality of\nour proposed statistical modeling. Two of them demonstrate good forecast\naccuracy and interpretability of our proposed method. In the third example, we\ninvestigate the effect of COVID-19 on electricity loads. The interpretation\nwould help make strategies for energy-saving interventions and demand response.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 15:09:50 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 08:07:06 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Hirose", "Kei", ""]]}, {"id": "2006.01061", "submitter": "Corinna Maier", "authors": "Corinna Maier, Niklas Hartung, Charlotte Kloft, Wilhelm Huisinga, and\n  Jana de Wiljes", "title": "Reinforcement learning and Bayesian data assimilation for model-informed\n  precision dosing in oncology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-informed precision dosing (MIPD) using therapeutic drug/biomarker\nmonitoring offers the opportunity to significantly improve the efficacy and\nsafety of drug therapies. Current strategies comprise model-informed dosing\ntables or are based on maximum a-posteriori estimates. These approaches,\nhowever, lack a quantification of uncertainty and/or consider only part of the\navailable patient-specific information. We propose three novel approaches for\nMIPD employing Bayesian data assimilation (DA) and/or reinforcement learning\n(RL) to control neutropenia, the major dose-limiting side effect in anticancer\nchemotherapy. These approaches have the potential to substantially reduce the\nincidence of life-threatening grade 4 and subtherapeutic grade 0 neutropenia\ncompared to existing approaches. We further show that RL allows to gain further\ninsights by identifying patient factors that drive dose decisions. Due to its\nflexibility, the proposed combined DA-RL approach can easily be extended to\nintegrate multiple endpoints or patient-reported outcomes, thereby promising\nimportant benefits for future personalized therapies.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 16:38:27 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Maier", "Corinna", ""], ["Hartung", "Niklas", ""], ["Kloft", "Charlotte", ""], ["Huisinga", "Wilhelm", ""], ["de Wiljes", "Jana", ""]]}, {"id": "2006.01230", "submitter": "Jingchen Hu", "authors": "Terrance D. Savitsky, Jingchen Hu, Matthew R. Williams", "title": "Re-weighting of Vector-weighted Mechanisms for Utility Maximization\n  under Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present practical aspects of implementing a pseudo posterior synthesizer\nfor microdata dissemination under a new re-weighting strategy for utility\nmaximization. Our re-weighting strategy applies to any vector-weighting\napproach under which a vector of observation-indexed weight are used to\ndownweight likelihood contributions for high disclosure risk records. We\ndemonstrate our method on two different vector-weighted schemes that target\nhigh-risk records by exponentiating each of their likelihood contributions with\na record-indexed weight, $\\alpha_i \\in [0,1]$ for record $i \\in (1,\\ldots,n)$.\nWe compute the overall Lipschitz bound, $\\Delta_{\\bm{\\alpha},\\mathbf{x}}$, for\nthe database $\\mathbf{x}$, under each vector-weighted scheme where a local\n$\\epsilon_{x} = 2\\Delta_{\\bm{\\alpha},\\mathbf{x}}$. Our new method for\nconstructing record-indexed downeighting maximizes the data utility under any\nprivacy budget for the vector-weighted synthesizers by adjusting the by-record\nweights, $(\\alpha_{i})_{i = 1}^{n}$, such that their individual Lipschitz\nbounds, $\\Delta_{\\bm{\\alpha},x_{i}}$, approach the bound for the entire\ndatabase, $\\Delta_{\\bm{\\alpha},\\mathbf{x}}$. We illustrate our methods using\nsimulated count data with and without over-dispersion-induced skewness and\ncompare the results to a scalar-weighted synthesizer under the Exponential\nMechanism (EM). We demonstrate our pDP result in a simulation study and our\nmethods on a sample of the Survey of Doctorate Recipients.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 20:03:37 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 14:36:33 GMT"}, {"version": "v3", "created": "Tue, 23 Mar 2021 15:56:21 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Savitsky", "Terrance D.", ""], ["Hu", "Jingchen", ""], ["Williams", "Matthew R.", ""]]}, {"id": "2006.01298", "submitter": "Ryan Hornby", "authors": "Ryan Hornby, Jingchen Hu", "title": "Identification Risks Evaluation of Partially Synthetic Data with the\n  $\\texttt{IdentificationRiskCalculation}$ R Package", "comments": "16 pages with 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend a general approach to evaluating identification risk of synthesized\nvariables in partially synthetic data. For multiple continuous synthesized\nvariables, we introduce the use of a radius $r$ in the construction of\nidentification risk probability of each target record, and illustrate with\nworking examples. We create the $\\texttt{IdentificationRiskCalculation}$ R\npackage to aid researchers and data disseminators in performing these\nidentification risks evaluation calculations. We demonstrate our methods\nthrough the R package with applications to a data sample from the Consumer\nExpenditure Surveys, and discuss the impacts on risk and data utility of 1) the\nchoice of radius $r$, 2) the choice of synthesized variables, and 3) the choice\nof number of synthetic datasets. We give recommendations for statistical\nagencies for synthesizing and evaluating identification risk of continuous\nvariables.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 22:35:36 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 00:03:00 GMT"}, {"version": "v3", "created": "Wed, 10 Mar 2021 06:47:11 GMT"}, {"version": "v4", "created": "Tue, 6 Apr 2021 02:05:50 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Hornby", "Ryan", ""], ["Hu", "Jingchen", ""]]}, {"id": "2006.01333", "submitter": "Lily Wang", "authors": "Guannan Wang, Zhiling Gu, Xinyi Li, Shan Yu, Myungjin Kim, Yueying\n  Wang, Lei Gao, and Li Wang", "title": "Comparing and Integrating US COVID-19 Data from Multiple Sources with\n  Anomaly Detection and Repairing", "comments": "34 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few months, the outbreak of Coronavirus disease (COVID-19) has\nbeen expanding over the world. A reliable and accurate dataset of the cases is\nvital for scientists to conduct related research and for policy-makers to make\nbetter decisions. We collect the United States COVID-19 daily reported data\nfrom four open sources: the New York Times, the COVID-19 Data Repository by\nJohns Hopkins University, the COVID Tracking Project at the Atlantic, and the\nUSAFacts, then compare the similarities and differences among them. To obtain\nreliable data for further analysis, we first examine the cyclical pattern and\nthe following anomalies, which frequently occur in the reported cases: (1) the\norder dependencies violation, (2) the point or period anomalies, and (3) the\nissue of reporting delay. To address these detected issues, we propose the\ncorresponding repairing methods and procedures if corrections are necessary. In\naddition, we integrate the COVID-19 reported cases with the county-level\nauxiliary information of the local features from official sources, such as\nhealth infrastructure, demographic, socioeconomic, and environmental\ninformation, which are also essential for understanding the spread of the\nvirus.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 01:28:23 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2020 15:47:16 GMT"}, {"version": "v3", "created": "Thu, 20 Aug 2020 16:18:01 GMT"}, {"version": "v4", "created": "Sat, 28 Nov 2020 19:14:58 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Wang", "Guannan", ""], ["Gu", "Zhiling", ""], ["Li", "Xinyi", ""], ["Yu", "Shan", ""], ["Kim", "Myungjin", ""], ["Wang", "Yueying", ""], ["Gao", "Lei", ""], ["Wang", "Li", ""]]}, {"id": "2006.01359", "submitter": "Antonio Quintero-Rincon", "authors": "Antonio Quintero-Rincon, Carlos D'Giano, Marcelo Risk", "title": "Epileptic seizure prediction using Pearson's product-moment correlation\n  coefficient of a linear classifier from generalized Gaussian modeling", "comments": "8 pages, 5 figures, 3 tables, manuscript in spanish", "journal-ref": "Neurologia Argentina 2018;10(4):210:217", "doi": "10.1016/j.neuarg.2018.06.004", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To predict an epileptic event means the ability to determine in advance the\ntime of the seizure with the highest possible accuracy. A correct prediction\nbenchmark for epilepsy events in clinical applications is a typical problem in\nbiomedical signal processing that helps to an appropriate diagnosis and\ntreatment of this disease. In this work, we use Pearson's product-moment\ncorrelation coefficient from generalized Gaussian distribution parameters\ncoupled with a linear-based classifier to predict between seizure and\nnon-seizure events in epileptic EEG signals. The performance in 36 epileptic\nevents from 9 patients showing good performance with 100% of effectiveness for\nsensitivity and specificity greater than 83% for seizures events in all brain\nrhythms. Pearson's test suggests that all brain rhythms are highly correlated\nin non-seizure events but no during the seizure events. This suggests that our\nmodel can be scaled with the Pearson's product-moment correlation coefficient\nfor the detection of epileptic seizures.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 02:49:37 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Quintero-Rincon", "Antonio", ""], ["D'Giano", "Carlos", ""], ["Risk", "Marcelo", ""]]}, {"id": "2006.01393", "submitter": "Hyunseung Kang", "authors": "Hyunseung Kang, Youjin Lee, T. Tony Cai, Dylan S. Small", "title": "Two Robust Tools for Inference about Causal Effects with Invalid\n  Instruments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instrumental variables have been widely used to estimate the causal effect of\na treatment on an outcome. Existing confidence intervals for causal effects\nbased on instrumental variables assume that all of the putative instrumental\nvariables are valid; a valid instrumental variable is a variable that affects\nthe outcome only by affecting the treatment and is not related to unmeasured\nconfounders. However, in practice, some of the putative instrumental variables\nare likely to be invalid. This paper presents two tools to conduct valid\ninference and tests in the presence of invalid instruments. First, we propose a\nsimple and general approach to construct confidence intervals based on taking\nunions of well-known confidence intervals. Second, we propose a novel test for\nthe null causal effect based on a collider bias. Our two proposals, especially\nwhen fused together, outperform traditional instrumental variable confidence\nintervals when invalid instruments are present, and can also be used as a\nsensitivity analysis when there is concern that instrumental variables\nassumptions are violated. The new approach is applied to a Mendelian\nrandomization study on the causal effect of low-density lipoprotein on the\nincidence of cardiovascular diseases.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 05:15:02 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Kang", "Hyunseung", ""], ["Lee", "Youjin", ""], ["Cai", "T. Tony", ""], ["Small", "Dylan S.", ""]]}, {"id": "2006.01569", "submitter": "Rapha\\\"el Huser", "authors": "Peng Zhong, Rapha\\\"el Huser, and Thomas Opitz", "title": "Modeling Non-Stationary Temperature Maxima Based on Extremal Dependence\n  Changing with Event Magnitude", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The modeling of spatio-temporal trends in temperature extremes can help\nbetter understand the structure and frequency of heatwaves in a changing\nclimate. Here, we study annual temperature maxima over Southern Europe using a\ncentury-spanning dataset observed at 44 monitoring stations. Extending the\nspectral representation of max-stable processes, our modeling framework relies\non a novel construction of max-infinitely divisible processes, which include\ncovariates to capture spatio-temporal non-stationarities. Our new model keeps a\npopular max-stable process on the boundary of the parameter space, while\nflexibly capturing weakening extremal dependence at increasing quantile levels\nand asymptotic independence. This is achieved by linking the overall magnitude\nof a spatial event to its spatial correlation range, in such a way that more\nextreme events become less spatially dependent, thus more localized. Our model\nreveals salient features of the spatio-temporal variability of European\ntemperature extremes, and it clearly outperforms natural alternative models.\nResults show that the spatial extent of heatwaves is smaller for more severe\nevents at higher altitudes, and that recent heatwaves are moderately wider. Our\nprobabilistic assessment of the 2019 annual maxima confirms the severity of the\n2019 heatwaves both spatially and at individual sites, especially when compared\nto climatic conditions prevailing in 1950-1975.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 12:40:44 GMT"}, {"version": "v2", "created": "Sun, 6 Sep 2020 09:16:26 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Zhong", "Peng", ""], ["Huser", "Rapha\u00ebl", ""], ["Opitz", "Thomas", ""]]}, {"id": "2006.01638", "submitter": "Yujie Hu", "authors": "Yujie Hu, Fahui Wang", "title": "Decomposing Excess Commuting: A Monte Carlo Simulation Approach", "comments": null, "journal-ref": "Journal of Transport Geography, 44, 43-52 (2015)", "doi": "10.1016/j.jtrangeo.2015.03.002", "report-no": null, "categories": "physics.soc-ph cs.CY stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Excess or wasteful commuting is measured as the proportion of actual commute\nthat is over minimum (optimal) commute when assuming that people could freely\nswap their homes and jobs in a city. Studies usually rely on survey data to\ndefine actual commute, and measure the optimal commute at an aggregate zonal\nlevel by linear programming (LP). Travel time from a survey could include\nreporting errors and respondents might not be representative of the areas they\nreside; and the derived optimal commute at an aggregate areal level is also\nsubject to the zonal effect. Both may bias the estimate of excess commuting.\nBased on the 2006-2010 Census for Transportation Planning Package (CTPP) data\nin Baton Rouge, Louisiana, this research uses a Monte Carlo approach to\nsimulate individual resident workers and individual jobs within census tracts,\nestimate commute distance and time from journey-to-work trips, and define the\noptimal commute based on simulated individual locations. Findings indicate that\nboth reporting errors and the use of aggregate zonal data contribute to\nmiscalculation of excess commuting.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 14:09:06 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Hu", "Yujie", ""], ["Wang", "Fahui", ""]]}, {"id": "2006.01672", "submitter": "Milan Straka", "authors": "Milan Straka, Rui Carvalho, Gijs van der Poel, \\v{L}ubo\\v{s} Buzna", "title": "Explaining the distribution of energy consumption at slow charging\n  infrastructure for electric vehicles from socio-economic data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG econ.EM math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here, we develop a data-centric approach enabling to analyse which\nactivities, function, and characteristics of the environment surrounding the\nslow charging infrastructure impact the distribution of the electricity\nconsumed at slow charging infrastructure. To gain a basic insight, we analysed\nthe probabilistic distribution of energy consumption and its relation to\nindicators characterizing charging events. We collected geospatial datasets and\nutilizing statistical methods for data pre-processing, we prepared features\nmodelling the spatial context in which the charging infrastructure operates. To\nenhance the statistical reliability of results, we applied the bootstrap method\ntogether with the Lasso method that combines regression with variable selection\nability. We evaluate the statistical distributions of the selected regression\ncoefficients. We identified the most influential features correlated with\nenergy consumption, indicating that the spatial context of the charging\ninfrastructure affects its utilization pattern. Many of these features are\nrelated to the economic prosperity of residents. Application of the methodology\nto a specific class of charging infrastructure enables the differentiation of\nselected features, e.g. by the used rollout strategy. Overall, the paper\ndemonstrates the application of statistical methodologies to energy data and\nprovides insights on factors potentially shaping the energy consumption that\ncould be utilized when developing models to inform charging infrastructure\ndeployment and planning of power grids.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 14:44:52 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2020 06:56:58 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Straka", "Milan", ""], ["Carvalho", "Rui", ""], ["van der Poel", "Gijs", ""], ["Buzna", "\u013dubo\u0161", ""]]}, {"id": "2006.01686", "submitter": "Jingchen Hu", "authors": "Kevin Ros, Henrik Olsson, Jingchen Hu", "title": "Two-Phase Data Synthesis for Income: An Application to the NHIS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a two-phase synthesis process for synthesizing income, a sensitive\nvariable which is usually highly-skewed and has a number of reported zeros. We\nconsider two forms of a continuous income variable: a binary form, which is\nmodeled and synthesized in phase 1; and a non-negative continuous form, which\nis modeled and synthesized in phase 2. Bayesian synthesis models are proposed\nfor the two-phase synthesis process, and other synthesis models are readily\nimplementable. We demonstrate our methods with applications to a sample from\nthe National Health Interview Survey (NHIS). Utility and risk profiles of\ngenerated synthetic datasets are evaluated and compared to results from a\nsingle-phase synthesis process.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 15:03:52 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 20:25:07 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Ros", "Kevin", ""], ["Olsson", "Henrik", ""], ["Hu", "Jingchen", ""]]}, {"id": "2006.01754", "submitter": "Gaetano Perone", "authors": "Gaetano Perone", "title": "ARIMA forecasting of COVID-19 incidence in Italy, Russia, and the USA", "comments": "20 pages, 5 tables and 9 figures in the main text. 4 tables and 3\n  figures in Appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The novel Coronavirus disease (COVID-19) is a severe respiratory infection\nthat officially occurred in Wuhan, China, in December 2019. In late February,\nthe disease began to spread quickly across the world, causing serious health,\nsocial, and economic emergencies. This paper aims to forecast the short to\nmedium-term incidence of COVID-19 epidemic through the medium of an\nautoregressive integrated moving average (ARIMA) model, applied to Italy,\nRussia, and the USA The analysis is carried out on the number of new daily\nconfirmed COVID-19 cases, collected by Worldometer website. The best ARIMA\nmodels are Italy (4,2,4), Russia (1,2,1), and the USA (6,2,3). The results show\nthat: i) ARIMA models are reliable enough when new daily cases begin to\nstabilize; ii) Italy, the USA, and Russia reached the peak of COVID-19\ninfections in mid-April, mid-May, and late May, respectively; and iii) Russia\nand the USA will require much more time than Italy to drop COVID-19 cases near\nzero. This may suggest the importance of the application of quick and effective\nlockdown measures, which have been relatively stricter in Italy. Therefore,\neven if the results should be interpreted with caution, ARIMA models seem to be\na good tool that can help the health authorities to monitor the diffusion of\nthe outbreak.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 18:53:14 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2020 13:12:00 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Perone", "Gaetano", ""]]}, {"id": "2006.01786", "submitter": "Yingying Ma", "authors": "Yingying Ma and Hansheng Wang", "title": "Hyperparameter Selection for Subsampling Bootstraps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive data analysis becomes increasingly prevalent, subsampling methods\nlike BLB (Bag of Little Bootstraps) serves as powerful tools for assessing the\nquality of estimators for massive data. However, the performance of the\nsubsampling methods are highly influenced by the selection of tuning parameters\n( e.g., the subset size, number of resamples per subset ). In this article we\ndevelop a hyperparameter selection methodology, which can be used to select\ntuning parameters for subsampling methods. Specifically, by a careful\ntheoretical analysis, we find an analytically simple and elegant relationship\nbetween the asymptotic efficiency of various subsampling estimators and their\nhyperparameters. This leads to an optimal choice of the hyperparameters. More\nspecifically, for an arbitrarily specified hyperparameter set, we can improve\nit to be a new set of hyperparameters with no extra CPU time cost, but the\nresulting estimator's statistical efficiency can be much improved. Both\nsimulation studies and real data analysis demonstrate the superior advantage of\nour method.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 17:10:45 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Ma", "Yingying", ""], ["Wang", "Hansheng", ""]]}, {"id": "2006.01864", "submitter": "Chiara Bocci", "authors": "Paul A. Smith, Chiara Bocci, Nikos Tzavidis, Sabine Krieg, Marc J.E.\n  Smeets", "title": "Robust estimation for small domains in business surveys", "comments": null, "journal-ref": "J R Stat Soc Series C, 70: 312-334 (2021)", "doi": "10.1111/rssc.12460", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Small area (or small domain) estimation is still rarely applied in business\nstatistics, because of challenges arising from the skewness and variability of\nvariables such as turnover. We examine a range of small area estimation methods\nas the basis for estimating the activity of industries within the retail sector\nin the Netherlands. We use tax register data and a sampling procedure which\nreplicates the sampling for the retail sector of Statistics Netherlands'\nStructural Business Survey as a basis for investigating the properties of small\narea estimators. In particular, we consider the use of the EBLUP under a random\neffects model and variations of the EBLUP derived under (a) a random effects\nmodel that includes a complex specification for the level 1 variance and (b) a\nrandom effects model that is fitted by using the survey weights. Although\naccounting for the survey weights in estimation is important, the impact of\ninfluential data points remains the main challenge in this case. The paper\nfurther explores the use of outlier robust estimators in business surveys, in\nparticular a robust version of the EBLUP, M-regression based synthetic\nestimators, and M-quantile small area estimators. The latter family of small\narea estimators includes robust projective (without and with survey weights)\nand robust predictive versions. M-quantile methods have the lowest empirical\nmean squared error and are substantially better than direct estimators, though\nthere is an open question about how to choose the tuning constant for bias\nadjustment in practice. The paper makes a further contribution by exploring a\ndoubly robust approach comprising the use of survey weights in conjunction with\noutlier robust methods in small area estimation.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 18:24:35 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Smith", "Paul A.", ""], ["Bocci", "Chiara", ""], ["Tzavidis", "Nikos", ""], ["Krieg", "Sabine", ""], ["Smeets", "Marc J. E.", ""]]}, {"id": "2006.01880", "submitter": "Chiara Bocci", "authors": "Chiara Bocci, Annalisa Caloffi, Marco Mariani, Alessandro Sterlacchini", "title": "Evaluating Public Supports to the Investment Activities of Business\n  Firms: A Multilevel Meta-Regression Analysis of Italian Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We conduct an extensive meta-regression analysis of counterfactual programme\nevaluations from Italy, considering both published and grey literature on\nenterprise and innovation policies. We specify a multilevel model for the\nprobability of finding positive effect estimates, also assessing correlation\npossibly induced by co-authorship networks. We find that the probability of\npositive effects is considerable, especially for weaker firms and outcomes that\nare directly targeted by public programmes. However, these policies are less\nlikely to trigger change in the long run.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 18:56:22 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Bocci", "Chiara", ""], ["Caloffi", "Annalisa", ""], ["Mariani", "Marco", ""], ["Sterlacchini", "Alessandro", ""]]}, {"id": "2006.01898", "submitter": "Helen Zhou", "authors": "Helen Zhou, Cheng Cheng, Zachary C. Lipton, George H. Chen, Jeremy C.\n  Weiss", "title": "Predicting Mortality Risk in Viral and Unspecified Pneumonia to Assist\n  Clinicians with COVID-19 ECMO Planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Respiratory complications due to coronavirus disease COVID-19 have claimed\ntens of thousands of lives in 2020. Many cases of COVID-19 escalate from Severe\nAcute Respiratory Syndrome (SARS-CoV-2) to viral pneumonia to acute respiratory\ndistress syndrome (ARDS) to death. Extracorporeal membranous oxygenation (ECMO)\nis a life-sustaining oxygenation and ventilation therapy that may be used for\npatients with severe ARDS when mechanical ventilation is insufficient to\nsustain life. While early planning and surgical cannulation for ECMO can\nincrease survival, clinicians report the lack of a risk score hinders these\nefforts. In this work, we leverage machine learning techniques to develop the\nPEER score, used to highlight critically ill patients with viral or unspecified\npneumonia at high risk of mortality or decompensation in a subpopulation\neligible for ECMO. The PEER score is validated on two large, publicly available\ncritical care databases and predicts mortality at least as well as other\nexisting risk scores. Stratifying our cohorts into low-risk and high-risk\ngroups, we find that the high-risk group also has a higher proportion of\ndecompensation indicators such as vasopressor and ventilator use. Finally, the\nPEER score is provided in the form of a nomogram for direct calculation of\npatient risk, and can be used to highlight at-risk patients among critical care\npatients eligible for ECMO.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 19:30:29 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Zhou", "Helen", ""], ["Cheng", "Cheng", ""], ["Lipton", "Zachary C.", ""], ["Chen", "George H.", ""], ["Weiss", "Jeremy C.", ""]]}, {"id": "2006.01936", "submitter": "Carsten Botts", "authors": "Carsten Botts", "title": "An Alternative Metric for Detecting Anomalous Ship Behavior Using a\n  Variation of the DBSCAN Clustering Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing need to quickly and accurately identify anomalous behavior\nin ships. This paper applies a variation of the Density Based Spatial\nClustering Among Noise (DBSCAN) algorithm to identify such anomalous behavior\ngiven a ship's Automatic Identification System (AIS) data. This variation of\nthe DBSCAN algorithm has been previously introduced in the literature, and in\nthis study, we elucidate and explore the mathematical details of this algorithm\nand introduce an alternative anomaly metric which is more statistically\ninformative than the one previously suggested.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 20:39:02 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 22:05:22 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Botts", "Carsten", ""]]}, {"id": "2006.01983", "submitter": "Jwala Dhamala", "authors": "Jwala Dhamala, John L. Sapp, B. Milan Hor\\'acek, Linwei Wang", "title": "Quantifying the Uncertainty in Model Parameters Using Gaussian\n  Process-Based Markov Chain Monte Carlo: An Application to Cardiac\n  Electrophysiological Models", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-59050-9_18", "report-no": null, "categories": "stat.ML cs.CV cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of patient-specific model parameters is important for personalized\nmodeling, although sparse and noisy clinical data can introduce significant\nuncertainty in the estimated parameter values. This importance source of\nuncertainty, if left unquantified, will lead to unknown variability in model\noutputs that hinder their reliable adoptions. Probabilistic estimation model\nparameters, however, remains an unresolved challenge because standard Markov\nChain Monte Carlo sampling requires repeated model simulations that are\ncomputationally infeasible. A common solution is to replace the simulation\nmodel with a computationally-efficient surrogate for a faster sampling.\nHowever, by sampling from an approximation of the exact posterior probability\ndensity function (pdf) of the parameters, the efficiency is gained at the\nexpense of sampling accuracy. In this paper, we address this issue by\nintegrating surrogate modeling into Metropolis Hasting (MH) sampling of the\nexact posterior pdfs to improve its acceptance rate. It is done by first\nquickly constructing a Gaussian process (GP) surrogate of the exact posterior\npdfs using deterministic optimization. This efficient surrogate is then used to\nmodify commonly-used proposal distributions in MH sampling such that only\nproposals accepted by the surrogate will be tested by the exact posterior pdf\nfor acceptance/rejection, reducing unnecessary model simulations at unlikely\ncandidates. Synthetic and real-data experiments using the presented method show\na significant gain in computational efficiency without compromising the\naccuracy. In addition, insights into the non-identifiability and heterogeneity\nof tissue properties can be gained from the obtained posterior distributions.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 23:48:15 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Dhamala", "Jwala", ""], ["Sapp", "John L.", ""], ["Hor\u00e1cek", "B. Milan", ""], ["Wang", "Linwei", ""]]}, {"id": "2006.02077", "submitter": "Nicklas Werge", "authors": "Nicklas Werge (LPSM), Olivier Wintenberger (LPSM)", "title": "AdaVol: An Adaptive Recursive Volatility Prediction Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quasi-Maximum Likelihood (QML) procedures are theoretically appealing and\nwidely used for statistical inference. While there are extensive references on\nQML estimation in batch settings, it has attracted little attention in\nstreaming settings until recently. An investigation of the convergence\nproperties of the QML procedure in a general conditionally heteroscedastic time\nseries model is conducted, and the classical batch optimization routines\nextended to the framework of streaming and large-scale problems. An adaptive\nrecursive estimation routine for GARCH models named AdaVol is presented. The\nAdaVol procedure relies on stochastic approximations combined with the\ntechnique of Variance Targeting Estimation (VTE). This recursive method has\ncomputationally efficient properties, while VTE alleviates some convergence\ndifficulties encountered by the usual QML estimation due to a lack of\nconvexity. Empirical results demonstrate a favorable trade-off between AdaVol's\nstability and the ability to adapt to time-varying estimates for real-life\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 07:28:31 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 13:11:49 GMT"}, {"version": "v3", "created": "Sun, 10 Jan 2021 15:22:43 GMT"}, {"version": "v4", "created": "Sun, 17 Jan 2021 09:38:02 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Werge", "Nicklas", "", "LPSM"], ["Wintenberger", "Olivier", "", "LPSM"]]}, {"id": "2006.02140", "submitter": "Yaneer Bar-Yam", "authors": "Chen Shen, Ron Mark, Nolan J. Kagetsu, Anton S. Becker, Yaneer Bar-Yam", "title": "Combining PCR and CT testing for COVID", "comments": "6 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": "New England Complex Systems Institute Research Report 101052020", "categories": "q-bio.PE nlin.AO physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the effect of using a screening CT-scan for evaluation of\npotential COVID-19 infections in order to isolate and perform contact tracing\nbased upon a viral pneumonia diagnosis. RT-PCR is then used for continued\nisolation based upon a COVID diagnosis. Both the low false negative rates and\nrapid results of CT-scans lead to dramatically reduced transmission. The\nreduction in cases after 60 days with widespread use of CT-scan screening\ncompared to PCR by itself is as high as $50\\times$, and the reduction of\neffective reproduction rate $R(t)$ is $0.20$. Our results imply that much more\nrapid extinction of COVID is possible by combining social distancing with\nCT-scans and contact tracing.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 19:56:55 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Shen", "Chen", ""], ["Mark", "Ron", ""], ["Kagetsu", "Nolan J.", ""], ["Becker", "Anton S.", ""], ["Bar-Yam", "Yaneer", ""]]}, {"id": "2006.02273", "submitter": "Clayton Miller", "authors": "Clayton Miller, Anjukan Kathirgamanathan, Bianca Picchetti,\n  Pandarasamy Arjunan, June Young Park, Zoltan Nagy, Paul Raftery, Brodie W.\n  Hobson, Zixiao Shi, and Forrest Meggers", "title": "The Building Data Genome Project 2, energy meter data from the ASHRAE\n  Great Energy Predictor III competition", "comments": null, "journal-ref": "Scientific Data volume 7, Article number: 368 (2020)", "doi": "10.1038/s41597-020-00712-x", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes an open data set of 3,053 energy meters from 1,636\nnon-residential buildings with a range of two full years (2016 and 2017) at an\nhourly frequency (17,544 measurements per meter resulting in approximately 53.6\nmillion measurements). These meters were collected from 19 sites across North\nAmerica and Europe, with one or more meters per building measuring whole\nbuilding electrical, heating and cooling water, steam, and solar energy as well\nas water and irrigation meters. Part of these data were used in the Great\nEnergy Predictor III (GEPIII) competition hosted by the ASHRAE organization in\nOctober-December 2019. GEPIII was a machine learning competition for long-term\nprediction with an application to measurement and verification. This paper\ndescribes the process of data collection, cleaning, and convergence of\ntime-series meter data, the meta-data about the buildings, and complementary\nweather data. This data set can be used for further prediction benchmarking and\nprototyping as well as anomaly detection, energy analysis, and building type\nclassification.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 13:43:40 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 23:56:08 GMT"}, {"version": "v3", "created": "Fri, 12 Jun 2020 00:52:45 GMT"}, {"version": "v4", "created": "Sun, 16 Aug 2020 05:52:08 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Miller", "Clayton", ""], ["Kathirgamanathan", "Anjukan", ""], ["Picchetti", "Bianca", ""], ["Arjunan", "Pandarasamy", ""], ["Park", "June Young", ""], ["Nagy", "Zoltan", ""], ["Raftery", "Paul", ""], ["Hobson", "Brodie W.", ""], ["Shi", "Zixiao", ""], ["Meggers", "Forrest", ""]]}, {"id": "2006.02355", "submitter": "Muhammad Osama", "authors": "Muhammad Osama, Dave Zachariah, Peter Stoica", "title": "Learning Robust Decision Policies from Observational Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of learning a decision policy from observational data\nof past decisions in contexts with features and associated outcomes. The past\npolicy maybe unknown and in safety-critical applications, such as medical\ndecision support, it is of interest to learn robust policies that reduce the\nrisk of outcomes with high costs. In this paper, we develop a method for\nlearning policies that reduce tails of the cost distribution at a specified\nlevel and, moreover, provide a statistically valid bound on the cost of each\ndecision. These properties are valid under finite samples -- even in scenarios\nwith uneven or no overlap between features for different decisions in the\nobserved data -- by building on recent results in conformal prediction. The\nperformance and statistical properties of the proposed method are illustrated\nusing both real and synthetic data.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 16:02:57 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Osama", "Muhammad", ""], ["Zachariah", "Dave", ""], ["Stoica", "Peter", ""]]}, {"id": "2006.02423", "submitter": "Ting Ye", "authors": "Ting Ye, Luke Keele, Raiden Hasegawa, Dylan S. Small", "title": "A Negative Correlation Strategy for Bracketing in\n  Difference-in-Differences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method of difference-in-differences (DID) is widely used to study the\ncausal effect of policy interventions in observational studies. DID employs a\nbefore and after comparison of the treated and control units to remove bias due\nto time-invariant unmeasured confounders under the parallel trends assumption.\nEstimates from DID, however, will be biased if the outcomes for the treated and\ncontrol units evolve differently in the absence of treatment, namely if the\nparallel trends assumption is violated. We propose a general identification\nstrategy that leverages two groups of control units whose outcomes relative to\nthe treated units exhibit a negative correlation, and achieves partial\nidentification of the average treatment effect for the treated. The identified\nset is of a union bounds form that involves the minimum and maximum operators,\nwhich makes the canonical bootstrap generally inconsistent and naive methods\noverly conservative. By utilizing the directional inconsistency of the\nbootstrap distribution, we develop a novel bootstrap method to construct\nuniformly valid confidence intervals for the identified set and parameter of\ninterest when the identified set is of a union bounds form, and we establish\nthe method's theoretical properties. We develop a simple falsification test and\nsensitivity analysis. We apply the proposed strategy for bracketing to study\nwhether minimum wage laws affect employment levels.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 17:53:30 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2021 03:17:58 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Ye", "Ting", ""], ["Keele", "Luke", ""], ["Hasegawa", "Raiden", ""], ["Small", "Dylan S.", ""]]}, {"id": "2006.02467", "submitter": "Javad Shaabani", "authors": "Javad Shaabani and Ali Akbar Jafari", "title": "A New Look to Three-Factor Fama-French Regression Model using Sample\n  Innovations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Fama-French model is widely used in assessing the portfolio's performance\ncompared to market returns. In Fama-French models, all factors are time-series\ndata. The cross-sectional data are slightly different from the time series\ndata. A distinct problem with time-series regressions is that R-squared in time\nseries regressions is usually very high, especially compared with typical\nR-squared for cross-sectional data. The high value of R-squared may cause\nmisinterpretation that the regression model fits the observed data well, and\nthe variance in the dependent variable is explained well by the independent\nvariables. Thus, to do regression analysis, and overcome with the serial\ndependence and volatility clustering, we use standard econometrics time series\nmodels to derive sample innovations. In this study, we revisit and validate the\nFama-French models in two different ways: using the factors and asset returns\nin the Fama-French model and considering the sample innovations in the\nFama-French model instead of studying the factors. Comparing the two methods\nconsidered in this study, we suggest the Fama-French model should be considered\nwith heavy tail distributions as the tail behavior is relevant in Fama-French\nmodels, including financial data, and the QQ plot does not validate that the\nchoice of the normal distribution as the theoretical distribution for the noise\nin the model.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 18:24:09 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Shaabani", "Javad", ""], ["Jafari", "Ali Akbar", ""]]}, {"id": "2006.02483", "submitter": "Carrie Manore", "authors": "Katherine Kempfert, Kaitlyn Martinez, Amir Siraj, Jessica Conrad,\n  Geoffrey Fairchild, Amanda Ziemann, Nidhi Parikh, David Osthus, Nicholas\n  Generous, Sara Del Valle, Carrie Manore", "title": "Time Series Methods and Ensemble Models to Nowcast Dengue at the State\n  Level in Brazil", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting an infectious disease can help reduce its impact by advising\npublic health interventions and personal preventive measures. Novel data\nstreams, such as Internet and social media data, have recently been reported to\nbenefit infectious disease prediction. As a case study of dengue in Brazil, we\nhave combined multiple traditional and non-traditional, heterogeneous data\nstreams (satellite imagery, Internet, weather, and clinical surveillance data)\nacross its 27 states on a weekly basis over seven years. For each state, we\nnowcast dengue based on several time series models, which vary in complexity\nand inclusion of exogenous data. The top-performing model varies by state,\nmotivating our consideration of ensemble approaches to automatically combine\nthese models for better outcomes at the state level. Model comparisons suggest\nthat predictions often improve with the addition of exogenous data, although\nsimilar performance can be attained by including only one exogenous data stream\n(either weather data or the novel satellite data) rather than combining all of\nthem. Our results demonstrate that Brazil can be nowcasted at the state level\nwith high accuracy and confidence, inform the utility of each individual data\nstream, and reveal potential geographic contributors to predictive performance.\nOur work can be extended to other spatial levels of Brazil, vector-borne\ndiseases, and countries, so that the spread of infectious disease can be more\neffectively curbed.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 19:04:34 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Kempfert", "Katherine", ""], ["Martinez", "Kaitlyn", ""], ["Siraj", "Amir", ""], ["Conrad", "Jessica", ""], ["Fairchild", "Geoffrey", ""], ["Ziemann", "Amanda", ""], ["Parikh", "Nidhi", ""], ["Osthus", "David", ""], ["Generous", "Nicholas", ""], ["Del Valle", "Sara", ""], ["Manore", "Carrie", ""]]}, {"id": "2006.02927", "submitter": "Shihao Yang", "authors": "Shihao Yang, Shaoyang Ning, S. C. Kou", "title": "Use Internet Search Data to Accurately Track State-Level Influenza\n  Epidemics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For epidemics control and prevention, timely insights of potential hot spots\nare invaluable. Alternative to traditional epidemic surveillance, which often\nlags behind real time by weeks, big data from the Internet provide important\ninformation of the current epidemic trends. Here we present a methodology,\nARGOX (Augmented Regression with GOogle data CROSS space), for accurate\nreal-time tracking of state-level influenza epidemics in the United States.\nARGOX combines Internet search data at the national, regional and state levels\nwith traditional influenza surveillance data from the Centers for Disease\nControl and Prevention, and accounts for both the spatial correlation structure\nof state-level influenza activities and the evolution of people's Internet\nsearch pattern. ARGOX achieves on average 28\\% error reduction over the best\nalternative for real-time state-level influenza estimation for 2014 to 2020.\nARGOX is robust and reliable and can be potentially applied to track county-\nand city-level influenza activity and other infectious diseases.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 15:11:23 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2020 15:16:19 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Yang", "Shihao", ""], ["Ning", "Shaoyang", ""], ["Kou", "S. C.", ""]]}, {"id": "2006.02985", "submitter": "L\\'eo Grinsztajn", "authors": "L\\'eo Grinsztajn, Elizaveta Semenova, Charles C. Margossian, Julien\n  Riou", "title": "Bayesian workflow for disease transmission modeling in Stan", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-bio.QM stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This tutorial shows how to build, fit, and criticize disease transmission\nmodels in Stan, and should be useful to researchers interested in modeling the\nSARS-CoV-2 pandemic and other infectious diseases in a Bayesian framework.\nBayesian modeling provides a principled way to quantify uncertainty and\nincorporate both data and prior knowledge into the model estimates. Stan is an\nexpressive probabilistic programming language that abstracts the inference and\nallows users to focus on the modeling. As a result, Stan code is readable and\neasily extensible, which makes the modeler's work more transparent.\nFurthermore, Stan's main inference engine, Hamiltonian Monte Carlo sampling, is\namiable to diagnostics, which means the user can verify whether the obtained\ninference is reliable. In this tutorial, we demonstrate how to formulate, fit,\nand diagnose a compartmental transmission model in Stan, first with a simple\nSusceptible-Infected-Recovered (SIR) model, then with a more elaborate\ntransmission model used during the SARS-CoV-2 pandemic. We also cover advanced\ntopics which can further help practitioners fit sophisticated models; notably,\nhow to use simulations to probe the model and priors, and computational\ntechniques to scale-up models based on ordinary differential equations.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 11:19:30 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 13:04:24 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Grinsztajn", "L\u00e9o", ""], ["Semenova", "Elizaveta", ""], ["Margossian", "Charles C.", ""], ["Riou", "Julien", ""]]}, {"id": "2006.02995", "submitter": "Silvia D'Angelo", "authors": "Silvia D'Angelo, Lorraine Brennan and Isobel Claire Gormley", "title": "Inferring food intake from multiple biomarkers using a latent variable\n  model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metabolomic based approaches have gained much attention in recent years due\nto their promising potential to deliver objective tools for assessment of food\nintake. In particular, multiple biomarkers have emerged for single foods.\nHowever, there is a lack of statistical tools available for combining multiple\nbiomarkers to infer food intake. Furthermore, there is a paucity of approaches\nfor estimating the uncertainty around biomarker based prediction of intake.\n  Here, to facilitate inference on the relationship between multiple\nmetabolomic biomarkers and food intake in an intervention study conducted under\nthe A-DIET research programme, a latent variable model, multiMarker, is\nproposed. The proposed model draws on factor analytic and mixture of experts\nmodels, describing intake as a continuous latent variable whose value gives\nraise to the observed biomarker values. We employ a mixture of Gaussian\ndistributions to flexibly model the latent variable. A Bayesian hierarchical\nmodelling framework provides flexibility to adapt to different biomarker\ndistributions and facilitates prediction of the latent intake along with its\nassociated uncertainty.\n  Simulation studies are conducted to assess the performance of the proposed\nmultiMarker framework, prior to its application to the motivating application\nof quantifying apple intake.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 16:29:16 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 14:03:49 GMT"}, {"version": "v3", "created": "Fri, 7 May 2021 10:27:43 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["D'Angelo", "Silvia", ""], ["Brennan", "Lorraine", ""], ["Gormley", "Isobel Claire", ""]]}, {"id": "2006.03105", "submitter": "Yongming Qu", "authors": "Yongming Qu, Linda Shurzinske, Shanthi Sethuraman", "title": "Defining Estimands Using a Mix of Strategies to Handle Intercurrent\n  Events in Clinical Trials", "comments": "21 page, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized controlled trials (RCT) are the gold standard for evaluation of\nthe efficacy and safety of investigational interventions. If every patient in\nan RCT were to adhere to the randomized treatment, one could simply analyze the\ncomplete data to infer the treatment effect. However, intercurrent events\n(ICEs) including the use of concomitant medication for unsatisfactory efficacy,\ntreatment discontinuation due to adverse events, or lack of efficacy, may lead\nto interventions that deviate from the original treatment assignment.\nTherefore, defining the appropriate estimand (the appropriate parameter to be\nestimated) based on the primary objective of the study is critical prior to\ndetermining the statistical analysis method and analyzing the data. The\nInternational Council for Harmonisation (ICH) E9 (R1), published on November\n20, 2019, provided 5 strategies to define the estimand: treatment policy,\nhypothetical, composite variable, while on treatment and principal stratum. In\nthis article, we propose an estimand using a mix of strategies in handling\nICEs. This estimand is an average of the null treatment difference for those\nwith ICEs potentially related to safety and the treatment difference for the\nother patients if they would complete the assigned treatments. Two examples\nfrom clinical trials evaluating anti-diabetes treatments are provided to\nillustrate the estimation of this proposed estimand and to compare it with the\nestimates for estimands using hypothetical and treatment policy strategies in\nhandling ICEs.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 19:25:08 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Qu", "Yongming", ""], ["Shurzinske", "Linda", ""], ["Sethuraman", "Shanthi", ""]]}, {"id": "2006.03140", "submitter": "Mireille Schnitzer", "authors": "Mireille E. Schnitzer, Daphna Harel, Vikki Ho, Anita Koushik and\n  Joanna Merckx", "title": "Identifiability and estimation under the test-negative design with\n  population controls with the goal of identifying risk and preventive factors\n  for SARS-CoV-2 infection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the rapidly evolving COVID-19 pandemic caused by the SARS-CoV-2 virus,\nquick public health investigations of the relationships between behaviours and\ninfection risk are essential. Recently the test-negative design was proposed to\nrecruit and survey participants who are symptomatic and being tested for\nSARS-CoV-2 infection with the goal of evaluating associations between the\nsurvey responses (including behaviours and environment) and testing positive on\nthe test. It was also proposed to recruit additional controls who are part of\nthe general population as a baseline comparison group in order to evaluate risk\nfactors specific to SARS-CoV-2 infection. In this study, we consider an\nalternative design where we recruit among all individuals, symptomatic and\nasymptomatic, being tested for the virus in addition to population controls. We\ndefine a regression parameter related to a prospective risk factor analysis and\ninvestigate its identifiability under the two study designs. We review the\ndifference between the prospective risk factor parameter and the parameter\ntargeted in the typical test-negative design where only symptomatic and tested\npeople are recruited.\n  Using missing data directed acyclic graphs we provide conditions and required\ndata collection under which identifiability of the prospective risk factor\nparameter is possible and compare the benefits and limitations of the\nalternative study designs and target parameters. We propose a novel inverse\nprobability weighting estimator and demonstrate the performance of this\nestimator through simulation study.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 21:41:51 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2020 15:18:22 GMT"}, {"version": "v3", "created": "Fri, 5 Feb 2021 22:36:40 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Schnitzer", "Mireille E.", ""], ["Harel", "Daphna", ""], ["Ho", "Vikki", ""], ["Koushik", "Anita", ""], ["Merckx", "Joanna", ""]]}, {"id": "2006.03141", "submitter": "Luca Pappalardo", "authors": "Paolo Cintia, Luca Pappalardo, Salvatore Rinzivillo, Daniele Fadda,\n  Tobia Boschi, Fosca Giannotti, Francesca Chiaromonte, Pietro Bonato,\n  Francesco Fabbri, Francesco Penone, Marcello Savarese, Francesco Calabrese,\n  Giorgio Guzzetta, Flavia Riccardo, Valentina Marziano, Piero Poletti, Filippo\n  Trentini, Antonino Bella, Xanthi Andrianou, Martina Del Manso, Massimo\n  Fabiani, Stefania Bellino, Stefano Boros, Alberto Mateo Urdiales, Maria\n  Fenicia Vescio, Silvio Brusaferro, Giovanni Rezza, Patrizio Pezzotti, Marco\n  Ajelli, Stefano Merler, Paolo Vineis, Dino Pedreschi", "title": "The relationship between human mobility and viral transmissibility\n  during the COVID-19 epidemics in Italy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2020, countries affected by the COVID-19 pandemic implemented various\nnon-pharmaceutical interventions to contrast the spread of the virus and its\nimpact on their healthcare systems and economies. Using Italian data at\ndifferent geographic scales, we investigate the relationship between human\nmobility, which subsumes many facets of the population's response to the\nchanging situation, and the spread of COVID-19. Leveraging mobile phone data\nfrom February through September 2020, we find a striking relationship between\nthe decrease in mobility flows and the net reproduction number. We find that\nthe time needed to switch off mobility and bring the net reproduction number\nbelow the critical threshold of 1 is about one week. Moreover, we observe a\nstrong relationship between the number of days spent above such threshold\nbefore the lockdown-induced drop in mobility flows and the total number of\ninfections per 100k inhabitants. Estimating the statistical effect of mobility\nflows on the net reproduction number over time, we document a 2-week lag\npositive association, strong in March and April, and weaker but still\nsignificant in June. Our study demonstrates the value of big mobility data to\nmonitor the epidemic and inform control interventions during its unfolding.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 21:48:23 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 20:13:58 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 07:47:20 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Cintia", "Paolo", ""], ["Pappalardo", "Luca", ""], ["Rinzivillo", "Salvatore", ""], ["Fadda", "Daniele", ""], ["Boschi", "Tobia", ""], ["Giannotti", "Fosca", ""], ["Chiaromonte", "Francesca", ""], ["Bonato", "Pietro", ""], ["Fabbri", "Francesco", ""], ["Penone", "Francesco", ""], ["Savarese", "Marcello", ""], ["Calabrese", "Francesco", ""], ["Guzzetta", "Giorgio", ""], ["Riccardo", "Flavia", ""], ["Marziano", "Valentina", ""], ["Poletti", "Piero", ""], ["Trentini", "Filippo", ""], ["Bella", "Antonino", ""], ["Andrianou", "Xanthi", ""], ["Del Manso", "Martina", ""], ["Fabiani", "Massimo", ""], ["Bellino", "Stefania", ""], ["Boros", "Stefano", ""], ["Urdiales", "Alberto Mateo", ""], ["Vescio", "Maria Fenicia", ""], ["Brusaferro", "Silvio", ""], ["Rezza", "Giovanni", ""], ["Pezzotti", "Patrizio", ""], ["Ajelli", "Marco", ""], ["Merler", "Stefano", ""], ["Vineis", "Paolo", ""], ["Pedreschi", "Dino", ""]]}, {"id": "2006.03146", "submitter": "Jiawei Long", "authors": "Jiawei Long", "title": "Analyzing the State of COVID-19: Real-time Visual Data Analysis,\n  Short-Term Forecasting, and Risk Factor Identification", "comments": "21 pages, https://peterljw.shinyapps.io/covid_dashboard/", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 outbreak was initially reported in Wuhan, China, and it has been\ndeclared as a Public Health Emergency of International Concern (PHEIC) on 30\nJanuary 2020 by WHO. It has now spread to over 180 countries, and it has\ngradually evolved into a worldwide pandemic, endangering the state of global\npublic health and becoming a serious threat to the global community. To combat\nand prevent the spread of the disease, all individuals should be well-informed\nof the rapidly changing state of COVID-19. To accomplish this objective, I have\nbuilt a website to analyze and deliver the latest state of the disease and\nrelevant analytical insights. The website is designed to cater to the general\naudience, and it aims to communicate insights through various straightforward\nand concise data visualizations that are supported by sound statistical\nmethods, accurate data modeling, state-of-the-art natural language processing\ntechniques, and reliable data sources. This paper discusses the major\nmethodologies which are utilized to generate the insights displayed on the\nwebsite, which include an automatic data ingestion pipeline, normalization\ntechniques, moving average computation, ARIMA time-series forecasting, and\nlogistic regression models. In addition, the paper highlights key discoveries\nthat have been derived in regard to COVID-19 using the methodologies.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 21:52:53 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 02:59:54 GMT"}, {"version": "v3", "created": "Mon, 3 May 2021 00:47:35 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Long", "Jiawei", ""]]}, {"id": "2006.03151", "submitter": "Theodore Papamarkou", "authors": "Matt Baucum, Anahita Khojandi, Theodore Papamarkou", "title": "Hidden Markov models as recurrent neural networks: An application to\n  Alzheimer's disease", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hidden Markov models (HMMs) are commonly used for disease progression\nmodeling when the true patient health state is not fully known. Since HMMs may\nhave multiple local optima, performance can be improved by incorporating\nadditional patient covariates to inform estimation. To allow for this, we\ndevelop hidden Markov recurrent neural networks (HMRNNs), a special case of\nrecurrent neural networks with the same likelihood function as a corresponding\ndiscrete-observation HMM. The HMRNN can be combined with any other predictive\nneural networks that take patient information as input, with all parameters\nestimated simultaneously via gradient descent. Using a dataset of Alzheimer's\ndisease patients, we demonstrate how combining the HMRNN with other predictive\nneural networks improves disease forecasting performance and offers a novel\nclinical interpretation compared with a standard HMM trained via\nexpectation-maximization.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 22:06:06 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 09:12:40 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Baucum", "Matt", ""], ["Khojandi", "Anahita", ""], ["Papamarkou", "Theodore", ""]]}, {"id": "2006.03284", "submitter": "Yi Yu", "authors": "Yaofang Hu and Wanjie Wang and Yi Yu", "title": "Graph matching beyond perfectly-overlapping Erd\\H{o}s--R\\'enyi random\n  graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph matching is a fruitful area in terms of both algorithms and theories.\nIn this paper, we exploit the degree information, which was previously used\nonly in noiseless graphs and perfectly-overlapping Erd\\H{o}s--R\\'enyi random\ngraphs matching. We are concerned with graph matching of partially-overlapping\ngraphs and stochastic block models, which are more useful in tackling real-life\nproblems. We propose the edge exploited degree profile graph matching method\nand two refined varations. We conduct a thorough analysis of our proposed\nmethods' performances in a range of challenging scenarios, including a\nzebrafish neuron activity data set and a coauthorship data set. Our methods are\nproved to be numerically superior than the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 08:07:43 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Hu", "Yaofang", ""], ["Wang", "Wanjie", ""], ["Yu", "Yi", ""]]}, {"id": "2006.03348", "submitter": "Peter Staab", "authors": "Peter Staab and Rick Cleary", "title": "Same-Score Streaks: A Case Study in Probability Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A same-score streak in sports is a sequence of games where the scores are\nequivalent in all games. The motivating problem arose from college basketball,\nhowever due to the difficulty in collecting data, streaks in Major League\nBaseball (MLB) were studied instead. This paper explores the historic data from\nregular-season games between 1901 and 2019 to include the likelihood of streaks\nof length 2, 3 and 4. Then we explore various probability models for the\ndistribution of runs scored during MLB games and seasons and generate simulated\nstatistics for the same length of streaks.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 10:12:55 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Staab", "Peter", ""], ["Cleary", "Rick", ""]]}, {"id": "2006.03360", "submitter": "Francesco Vidoli", "authors": "Roberto Benedetti, Federica Piersimoni, Giacomo Pignataro, Francesco\n  Vidoli", "title": "The identification of spatially constrained homogeneous clusters of\n  Covid-19 transmission", "comments": "Preliminary version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper introduces an approach to identify a set of spatially constrained\nhomogeneous areas maximally homogeneous in terms of epidemic trends. The\nproposed hierarchical algorithm is based on the Dynamic TimeWarping distances\nbetween epidemic time trends where units are constrained by a spatial proximity\ngraph. The paper includes two different applications of this approach to Italy,\nbased on different data (number of positive test and number of differential\ndeaths, with respect to the previous years) and on different observational\nunits (provinces and Labour Market Areas). Both applications, above all the one\nrelated to Labour Market Areas, show the existence of well-defined areas, where\nthe dynamics of growth of the infection have been strongly differentiated. The\nadoption of the same lock-down policy throughout the entire national territory\nhas been therefore sub-optimal, showing once again the urgent need for local\ndata-driven policies.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 10:48:09 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Benedetti", "Roberto", ""], ["Piersimoni", "Federica", ""], ["Pignataro", "Giacomo", ""], ["Vidoli", "Francesco", ""]]}, {"id": "2006.03373", "submitter": "Gilles Stoltz", "authors": "Malo Huard (LMO), R\\'emy Garnier, Gilles Stoltz (LMO, HEC Paris,\n  CELESTE)", "title": "Hierarchical robust aggregation of sales forecasts at aggregated levels\n  in e-commerce, based on exponential smoothing and Holt's linear trend method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the interest of classical statistical techniques for sales\nforecasting like exponential smoothing and extensions thereof (as Holt's linear\ntrend method). We do so by considering ensemble forecasts, given by several\ninstances of these classical techniques tuned with different (sets of)\nparameters, and by forming convex combinations of the elements of ensemble\nforecasts over time, in a robust and sequential manner. The machine-learning\ntheory behind this is called \"robust online aggregation\", or \"prediction with\nexpert advice\", or \"prediction of individual sequences\" (see Cesa-Bianchi and\nLugosi, 2006). We apply this methodology to a hierarchical data set of sales\nprovided by the e-commerce company Cdiscount and output forecasts at the levels\nof subsubfamilies, subfamilies and families of items sold, for various\nforecasting horizons (up to 6-week-ahead). The performance achieved is better\nthan what would be obtained by optimally tuning the classical techniques on a\ntrain set and using their forecasts on the test set. The performance is also\ngood from an intrinsic point of view (in terms of mean absolute percentage of\nerror). While getting these better forecasts of sales at the levels of\nsubsubfamilies, subfamilies and families is interesting per se, we also suggest\nto use them as additional features when forecasting demand at the item level.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 11:20:25 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Huard", "Malo", "", "LMO"], ["Garnier", "R\u00e9my", "", "LMO, HEC Paris,\n  CELESTE"], ["Stoltz", "Gilles", "", "LMO, HEC Paris,\n  CELESTE"]]}, {"id": "2006.03498", "submitter": "Yujie Hu", "authors": "Yujie Hu, Fahui Wang, Chester Wilmot", "title": "Commuting Variability by Wage Groups in Baton Rouge 1990-2010", "comments": null, "journal-ref": "Papers in Applied Geography, 3(1), 14-29 (2017)", "doi": "10.1080/23754931.2016.1248577", "report-no": null, "categories": "stat.AP cs.CY econ.GN q-fin.EC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Residential segregation recently has shifted to more class or income-based in\nthe United States, and neighborhoods are undergoing significant changes such as\ncommuting patterns over time. To better understand the commuting inequality\nacross neighborhoods of different income levels, this research analyzes\ncommuting variability (in both distance and time) across wage groups as well as\nstability over time using the CTPP data 1990-2010 in Baton Rouge. In comparison\nto previous work, commuting distance is estimated more accurately by Monte\nCarlo simulation of individual trips to mitigate aggregation error and scale\neffect. The results based on neighborhoods mean wage rate indicate that\ncommuting behaviors vary across areas of different wage rates and such\nvariability is captured by a convex shape. Affluent neighborhoods tended to\ncommute more but highest-wage neighborhoods retreated for less commuting. This\ntrend remains relatively stable over time despite an overall transportation\nimprovement in general. A complementary analysis based on the distribution of\nwage groups is conducted to gain more detailed insights and uncovers the\nlasting poor mobility (e.g., fewer location and transport options) of the\nlowest-wage workers in 1990-2010.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 14:03:42 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Hu", "Yujie", ""], ["Wang", "Fahui", ""], ["Wilmot", "Chester", ""]]}, {"id": "2006.03499", "submitter": "Yujie Hu", "authors": "Yujie Hu, Harvey J Miller, Xiang Li", "title": "Detecting and Analyzing Mobility Hotspots using Surface Networks", "comments": null, "journal-ref": "Transactions in GIS, 18(6), 911-935 (2014)", "doi": "10.1111/tgis.12076", "report-no": null, "categories": "stat.AP cs.CG cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Capabilities for collecting and storing data on mobile objects have increased\ndramatically over the past few decades. A persistent difficulty is summarizing\nlarge collections of mobile objects. This paper develops methods for extracting\nand analyzing hotspots or locations with relatively high levels of mobility\nactivity. We use kernel density estimation (KDE) to convert a large collection\nof mobile objects into a smooth, continuous surface. We then develop a\ntopological algorithm to extract critical geometric features of the surface;\nthese include critical points (peaks, pits and passes) and critical lines\n(ridgelines and course-lines). We connect the peaks and corresponding\nridgelines to produce a surface network that summarizes the topological\nstructure of the surface. We apply graph theoretic indices to analytically\ncharacterize the surface and its changes over time. To illustrate our approach,\nwe apply the techniques to taxi cab data collected in Shanghai, China. We find\nincreases in the complexity of the hotspot spatial distribution during normal\nactivity hours in the late morning, afternoon and evening and a spike in the\nconnectivity of the hotspot spatial distribution in the morning as taxis\nconcentrate on servicing travel to work. These results match with scientific\nand anecdotal knowledge about human activity patterns in the study area.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 14:11:26 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Hu", "Yujie", ""], ["Miller", "Harvey J", ""], ["Li", "Xiang", ""]]}, {"id": "2006.03610", "submitter": "Michael Kirchhof", "authors": "Michael Kirchhof and Klaus Haas and Thomas Kornas and Sebastian Thiede\n  and Mario Hirz and Christoph Herrmann", "title": "Root Cause Analysis in Lithium-Ion Battery Production with FMEA-Based\n  Large-Scale Bayesian Network", "comments": "Submitted to CIRP Journal of Manufacturing Science and Technology\n  (01.2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The production of lithium-ion battery cells is characterized by a high degree\nof complexity due to numerous cause-effect relationships between process\ncharacteristics. Knowledge about the multi-stage production is spread among\nseveral experts, rendering tasks as failure analysis challenging. In this\npaper, a new method is presented that includes expert knowledge acquisition in\nproduction ramp-up by combining Failure Mode and Effects Analysis (FMEA) with a\nBayesian Network. Special algorithms are presented that help detect and resolve\ninconsistencies between the expert-provided parameters which are bound to occur\nwhen collecting knowledge from several process experts. We show the\neffectiveness of this holistic method by building up a large scale,\ncross-process Bayesian Failure Network in lithium-ion battery production and\nits application for root cause analysis.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 18:04:37 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 13:49:52 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Kirchhof", "Michael", ""], ["Haas", "Klaus", ""], ["Kornas", "Thomas", ""], ["Thiede", "Sebastian", ""], ["Hirz", "Mario", ""], ["Herrmann", "Christoph", ""]]}, {"id": "2006.03700", "submitter": "Maria Lombardi", "authors": "Maria Lombardi, William H. Warren, M. di Bernardo", "title": "Leadership emergence in walking groups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the mechanisms underlying the emergence of leadership in\nmulti-agent systems is still under investigation in many areas of research\nwhere group coordination is involved. While leadership has been mostly\ninvestigated in the case of animal groups, only a few works address the problem\nof leadership emergence in human ensembles, e.g. pedestrian walking, group\ndance. In this paper we study the emergence of leadership in the specific\nscenario of a small walking group. Our aim is to unveil the main mechanisms\nemerging in a human group when leader or follower roles are not designated a\npriori. Two groups of participants were asked to walk together and turn or\nchange speed at self-selected times. Data were analysed using time-dependent\ncross correlation to infer leader-follower interactions between each pair of\ngroup members. The results indicate that leadership emergence is due both to\ncontextual factors, such as an individual's position in the group, and to\npersonal factors, such as an individual's characteristic locomotor behaviour.\nOur approach can easily be extended to larger groups and other scenarios such\nas team sports and emergency evacuations.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 21:31:43 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Lombardi", "Maria", ""], ["Warren", "William H.", ""], ["di Bernardo", "M.", ""]]}, {"id": "2006.03702", "submitter": "Yu Fan", "authors": "Sanguo Zhang, Yu Fan, Tingyan Zhong, Shuangge Ma", "title": "Histopathological imaging features- versus molecular measurements-based\n  cancer prognosis modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP eess.IV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For most if not all cancers, prognosis is of significant importance, and\nextensive modeling research has been conducted. With the genetic nature of\ncancer, in the past two decades, multiple types of molecular data (such as gene\nexpressions and DNA mutations) have been explored. More recently,\nhistopathological imaging data, which is routinely collected in biopsy, has\nbeen shown as informative for modeling prognosis. In this study, using the TCGA\nLUAD and LUSC data as a showcase, we examine and compare modeling lung cancer\noverall survival using gene expressions versus histopathological imaging\nfeatures. High-dimensional regularization methods are adopted for estimation\nand selection. Our analysis shows that gene expressions have slightly better\nprognostic performance. In addition, most of the gene expressions are found to\nbe weakly correlated imaging features. It is expected that this study can\nprovide some insight into utilizing the two types of important data in cancer\nprognosis modeling and into lung cancer overall survival.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 21:40:53 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Zhang", "Sanguo", ""], ["Fan", "Yu", ""], ["Zhong", "Tingyan", ""], ["Ma", "Shuangge", ""]]}, {"id": "2006.03729", "submitter": "Qiyao Wang", "authors": "Qiyao Wang, Ahmed Farahat, Chetan Gupta, Haiyan Wang", "title": "Health Indicator Forecasting for Improving Remaining Useful Life\n  Estimation", "comments": "Accepted by IEEE International Conference on Prognostics and Health\n  Management 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prognostics is concerned with predicting the future health of the equipment\nand any potential failures. With the advances in the Internet of Things (IoT),\ndata-driven approaches for prognostics that leverage the power of machine\nlearning models are gaining popularity. One of the most important categories of\ndata-driven approaches relies on a predefined or learned health indicator to\ncharacterize the equipment condition up to the present time and make inference\non how it is likely to evolve in the future. In these approaches, health\nindicator forecasting that constructs the health indicator curve over the\nlifespan using partially observed measurements (i.e., health indicator values\nwithin an initial period) plays a key role. Existing health indicator\nforecasting algorithms, such as the functional Empirical Bayesian approach, the\nregression-based formulation, a naive scenario matching based on the nearest\nneighbor, have certain limitations. In this paper, we propose a new `generative\n+ scenario matching' algorithm for health indicator forecasting. The key idea\nbehind the proposed approach is to first non-parametrically fit the underlying\nhealth indicator curve with a continuous Gaussian Process using a sample of\nrun-to-failure health indicator curves. The proposed approach then generates a\nrich set of random curves from the learned distribution, attempting to obtain\nall possible variations of the target health condition evolution process over\nthe system's lifespan. The health indicator extrapolation for a piece of\nfunctioning equipment is inferred as the generated curve that has the highest\nmatching level within the observed period. Our experimental results show the\nsuperiority of our algorithm over the other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 23:02:10 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Wang", "Qiyao", ""], ["Farahat", "Ahmed", ""], ["Gupta", "Chetan", ""], ["Wang", "Haiyan", ""]]}, {"id": "2006.03738", "submitter": "Stefano M. Iacus", "authors": "Stefano Maria Iacus and Carlos Santamaria and Francesco Sermi and\n  Spyridon Spyratos and Dario Tarchi and Michele Vespe", "title": "Human mobility and COVID-19 initial dynamics", "comments": "This is the final published version of the article appeared in\n  Nonlinear Dynamics under open access license CC 4.0", "journal-ref": null, "doi": "10.1007/s11071-020-05854-6", "report-no": null, "categories": "stat.AP physics.soc-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mobility data at EU scale can help understand the dynamics of the pandemic\nand possibly limit the impact of future waves. Still, since a reliable and\nconsistent method to measure the evolution of contagion at international level\nis missing, a systematic analysis of the relationship between human mobility\nand virus spread has never been conducted. A notable exceptions are France and\nItaly, for which data on excess deaths, an indirect indicator which is\ngenerally considered to be less affected by national and regional assumptions,\nare available at department and municipality level, respectively. Using this\ninformation together with anonymised and aggregated mobile data, this study\nshows that mobility alone can explain up to 92% of the initial spread in these\ntwo EU countries, while it has a slow decay effect after lockdown measures,\nmeaning that mobility restrictions seem to have effectively contribute to save\nlives. It also emerges that internal mobility is more important than mobility\nacross provinces and that the typical lagged positive effect of reduced human\nmobility on reducing excess deaths is around 14-20 days. An analogous analysis\nrelative to Spain, for which an IgG SARS-Cov-2 antibody screening study at\nprovince level is used instead of excess deaths statistics, confirms the\nfindings. The same approach adopted in this study can be easily extended to\nother European countries, as soon as reliable data on the spreading of the\nvirus at a suitable level of granularity will be available. Looking at past\ndata, relative to the initial phase of the outbreak in EU Member States, this\nstudy shows in which extent the spreading of the virus and human mobility are\nconnected.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 23:29:41 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2020 13:36:19 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Iacus", "Stefano Maria", ""], ["Santamaria", "Carlos", ""], ["Sermi", "Francesco", ""], ["Spyratos", "Spyridon", ""], ["Tarchi", "Dario", ""], ["Vespe", "Michele", ""]]}, {"id": "2006.03922", "submitter": "Nicholas Reich", "authors": "Nicholas G Reich, Matthew Cornell, Evan L Ray, Katie House, Khoa Le", "title": "The Zoltar forecast archive: a tool to facilitate standardization and\n  storage of interdisciplinary prediction research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Forecasting has emerged as an important component of informed, data-driven\ndecision-making in a wide array of fields. We introduce a new data model for\nprobabilistic predictions that encompasses a wide range of forecasting\nsettings. This framework clearly defines the constituent parts of a\nprobabilistic forecast and proposes one approach for representing these data\nelements. The data model is implemented in Zoltar, a new software application\nthat stores forecasts using the data model and provides standardized API access\nto the data. In one real-time case study, an instance of the Zoltar web\napplication was used to store, provide access to, and evaluate real-time\nforecast data on the order of 10$^7$ rows, provided by over 20 international\nresearch teams from academia and industry making forecasts of the COVID-19\noutbreak in the US. Tools and data infrastructure for probabilistic forecasts,\nsuch as those introduced here, will play an increasingly important role in\nensuring that future forecasting research adheres to a strict set of rigorous\nand reproducible standards.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 17:16:16 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Reich", "Nicholas G", ""], ["Cornell", "Matthew", ""], ["Ray", "Evan L", ""], ["House", "Katie", ""], ["Le", "Khoa", ""]]}, {"id": "2006.03929", "submitter": "Hao Sun", "authors": "Zhao Chen and Hao Sun", "title": "Sparse representation for damage identification of structural systems", "comments": "11 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG cs.NA cs.SY eess.SY math.NA physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying damage of structural systems is typically characterized as an\ninverse problem which might be ill-conditioned due to aleatory and epistemic\nuncertainties induced by measurement noise and modeling error. Sparse\nrepresentation can be used to perform inverse analysis for the case of sparse\ndamage. In this paper, we propose a novel two-stage sensitivity analysis-based\nframework for both model updating and sparse damage identification.\nSpecifically, an $\\ell_2$ Bayesian learning method is firstly developed for\nupdating the intact model and uncertainty quantification so as to set forward a\nbaseline for damage detection. A sparse representation pipeline built on a\nquasi-$\\ell_0$ method, e.g., Sequential Threshold Least Squares (STLS)\nregression, is then presented for damage localization and quantification.\nAdditionally, Bayesian optimization together with cross validation is developed\nto heuristically learn hyperparameters from data, which saves the computational\ncost of hyperparameter tuning and produces more reliable identification result.\nThe proposed framework is verified by three examples, including a 10-story\nshear-type building, a complex truss structure, and a shake table test of an\neight-story steel frame. Results show that the proposed approach is capable of\nboth localizing and quantifying structural damage with high accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 18:04:35 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Chen", "Zhao", ""], ["Sun", "Hao", ""]]}, {"id": "2006.03936", "submitter": "Ranjan Maitra", "authors": "Karin S. Dorman and Ranjan Maitra", "title": "An Efficient $k$-modes Algorithm for Clustering Categorical Datasets", "comments": "16 pages, 10 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining clusters from data is an important endeavor in many applications. The\n$k$-means method is a popular, efficient, and distribution-free approach for\nclustering numerical-valued data, but does not apply for categorical-valued\nobservations. The $k$-modes method addresses this lacuna by replacing the\nEuclidean with the Hamming distance and the means with the modes in the\n$k$-means objective function. We provide a novel, computationally efficient\nimplementation of $k$-modes, called OTQT. We prove that OTQT finds updates to\nimprove the objective function that are undetectable to existing $k$-modes\nalgorithms. Although slightly slower per iteration due to algorithmic\ncomplexity, OTQT is always more accurate per iteration and almost always faster\n(and only barely slower on some datasets) to the final optimum. Thus, we\nrecommend OTQT as the preferred, default algorithm for $k$-modes optimization.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 18:41:36 GMT"}, {"version": "v2", "created": "Sun, 15 Nov 2020 05:32:31 GMT"}, {"version": "v3", "created": "Wed, 23 Jun 2021 20:18:20 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Dorman", "Karin S.", ""], ["Maitra", "Ranjan", ""]]}, {"id": "2006.04071", "submitter": "Krishnam Kapoor", "authors": "Krishnam Kapoor", "title": "A Novel Algorithm for Optimized Real Time Anomaly Detection in\n  Timeseries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Observations in data which are significantly different from its neighbouring\npoints but cannot be classified as noise are known as anomalies or outliers.\nThese anomalies are a cause of concern and a timely warning about their\npresence could be valuable. In this paper, we have evaluated and compared the\nperformance of popular algorithms from domains of Machine Learning and\nStatistics in detecting anomalies on both offline data as well as real time\ndata. Our aim is to come up with an algorithm which can handle all types of\nseasonal and non-seasonal data effectively and is fast enough to be of\npractical utility in real time. It is not only important to detect anomalies at\nthe global but also the ones which are anomalies owing to their local\nsurroundings. Such outliers can be termed as contextual anomalies as they\nderive their context from the neighbouring observations. Also, we require a\nmethodology to automatically determine the presence of seasonality in the given\ndata. For detecting the seasonality, the proposed algorithm takes up a curve\nfitting approach rather than model based anomaly detection. The proposed model\nalso introduces a unique filter which assess the relative significance of local\noutliers and removes the ones deemed as insignificant. Since, the proposed\nmodel fits polynomial in buckets of timeseries data, it does not suffer from\nproblems such as heteroskedasticity and breakout as compared to its statistical\nalternatives such as ARIMA, SARIMA and Winter Holt. Experimental results the\nproposed algorithm performs better on both real time as well as artificial\ngenerated datasets.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 07:41:33 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Kapoor", "Krishnam", ""]]}, {"id": "2006.04141", "submitter": "Alberto Sorrentino", "authors": "Alessandro Viani, Gianvittorio Luria, Harald Bornfleth and Alberto\n  Sorrentino", "title": "Where Bayes tweaks Gauss: Conditionally Gaussian priors for stable\n  multi-dipole estimation", "comments": "23 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a very simple yet powerful generalization of a previously\ndescribed model and algorithm for estimation of multiple dipoles from\nmagneto/electro-encephalographic data. Specifically, the generalization\nconsists in the introduction of a log-uniform hyperprior on the standard\ndeviation of a set of conditionally linear/Gaussian variables. We use numerical\nsimulations and an experimental dataset to show that the approximation to the\nposterior distribution remains extremely stable under a wide range of values of\nthe hyperparameter, virtually removing the dependence on the hyperparameter.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 13:05:15 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Viani", "Alessandro", ""], ["Luria", "Gianvittorio", ""], ["Bornfleth", "Harald", ""], ["Sorrentino", "Alberto", ""]]}, {"id": "2006.04461", "submitter": "Diego Salmer\\'on", "authors": "Diego Salmer\\'on", "title": "Bayesian beta nonlinear models with constrained parameters to describe\n  ruminal degradation kinetics", "comments": null, "journal-ref": "Diego Salmer\\'on (2021) Bayesian beta nonlinear models with\n  constrained parameters to describe ruminal degradation kinetics. Journal of\n  Applied Statistics", "doi": "10.1080/02664763.2021.1913105", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The models used to describe the kinetics of ruminal degradation are usually\nnonlinear models where the dependent variable is the proportion of degraded\nfood. The method of least squares is the standard approach used to estimate the\nunknown parameters but this method can lead to unacceptable predictions. To\nsolve this issue, a beta nonlinear model and the Bayesian perspective is\nproposed in this article. The application of standard methodologies to obtain\nprior distributions, such as the Jeffreys prior or the reference priors,\ninvolves serious difficulties here because this model is a nonlinear non-normal\nregression model, and the constrained parameters appear in the log-likelihood\nfunction through the Gamma function. This paper proposes an objective method to\nobtain the prior distribution, which can be applied to other models with\nsimilar complexity, can be easily implemented in OpenBUGS, and solves the\nproblem of unacceptable predictions. The model is generalized to a larger class\nof models. The methodology was applied to real data with three models that were\ncompared using the Deviance Information Criterion and the root mean square\nprediction error. A simulation study was performed to evaluate the coverage of\nthe credible intervals.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 10:35:27 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 11:18:08 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Salmer\u00f3n", "Diego", ""]]}, {"id": "2006.04511", "submitter": "Alice Le Brigant", "authors": "Alice Le Brigant (UP1), Nicolas Guigui (UNATI), Sana Rebbah (ENAC),\n  St\\'ephane Puechmorel (ENAC)", "title": "Classifying histograms of medical data using information geometry of\n  beta distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we use tools of information geometry to compare, average and\nclassify histograms. Beta distributions are fitted to the histograms and the\ncorresponding Fisher information geometry is used for comparison. We show that\nthis geometry is negatively curved, which guarantees uniqueness of the notion\nof mean, and makes it suitable to classify histograms through the popular\nK-means algorithm. We illustrate the use of these geometric tools in supervised\nand unsupervised classification procedures of two medical data-sets, cardiac\nshape deformations for the detection of pulmonary hypertension and brain\ncortical thickness for the diagnosis of Alzheimer's disease.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 07:31:14 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Brigant", "Alice Le", "", "UP1"], ["Guigui", "Nicolas", "", "UNATI"], ["Rebbah", "Sana", "", "ENAC"], ["Puechmorel", "St\u00e9phane", "", "ENAC"]]}, {"id": "2006.04576", "submitter": "Patrick J. Laub", "authors": "Patrick J. Laub, Nicole El Karoui, St\\'ephane Loisel, Yahia Salhi", "title": "Quickest detection in practice in presence of seasonality: An\n  illustration with call center data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this chapter, we explain how quickest detection algorithms can be useful\nfor risk management in presence of seasonality. We investigate the problem of\ndetecting fast enough cases when a call center will need extra staff in a near\nfuture with a high probability. We illustrate our findings on real data\nprovided by a French insurer. We also discuss the relevance of the CUSUM\nalgorithm and of some machine-learning type competitor for this applied\nproblem.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 13:26:08 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Laub", "Patrick J.", ""], ["Karoui", "Nicole El", ""], ["Loisel", "St\u00e9phane", ""], ["Salhi", "Yahia", ""]]}, {"id": "2006.04608", "submitter": "Marina Vannucci", "authors": "Jeong Hwan Kook, Kelly A. Vaughn, Dana M. DeMaster, Linda Ewing-Cobbs\n  and Marina Vannucci", "title": "BVAR-Connect: A Variational Bayes Approach to Multi-Subject Vector\n  Autoregressive Models for Inference on Brain Connectivity Networks", "comments": "Neuroinformatics (2020), in press", "journal-ref": "Neuroinformatics,19, 39-56 (2021)", "doi": "10.1007/s12021-020-09472-w", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose BVAR-connect, a variational inference approach to a\nBayesian multi-subject vector autoregressive (VAR) model for inference on\neffective brain connectivity based on resting-state functional MRI data. The\nmodeling framework uses a Bayesian variable selection approach that flexibly\nintegrates multi-modal data, in particular structural diffusion tensor imaging\n(DTI) data, into the prior construction. The variational inference approach we\ndevelop allows scalability of the methods and results in the ability to\nestimate subject- and group-level brain connectivity networks over whole-brain\nparcellations of the data. We provide a brief description of a user-friendly\nMATLAB GUI released for public use. We assess performance on simulated data,\nwhere we show that the proposed inference method can achieve comparable\naccuracy to the sampling-based Markov Chain Monte Carlo approach but at a much\nlower computational cost. We also address the case of subject groups with\nimbalanced sample sizes. Finally, we illustrate the methods on resting-state\nfunctional MRI and structural DTI data on children with a history of traumatic\ninjury.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 14:05:28 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Kook", "Jeong Hwan", ""], ["Vaughn", "Kelly A.", ""], ["DeMaster", "Dana M.", ""], ["Ewing-Cobbs", "Linda", ""], ["Vannucci", "Marina", ""]]}, {"id": "2006.04937", "submitter": "Matteo Sesia", "authors": "Charmaine Chia, Matteo Sesia, Chi-Sing Ho, Stefanie S. Jeffrey,\n  Jennifer Dionne, Emmanuel J. Cand\\`es, Roger T. Howe", "title": "Interpretable Classification of Bacterial Raman Spectra with Knockoff\n  Wavelets", "comments": "9 pages, 6 figures, 4 tables", "journal-ref": null, "doi": "10.1109/JBHI.2021.3094873", "report-no": null, "categories": "eess.SP stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks and other sophisticated machine learning models are\nwidely applied to biomedical signal data because they can detect complex\npatterns and compute accurate predictions. However, the difficulty of\ninterpreting such models is a limitation, especially for applications involving\nhigh-stakes decision, including the identification of bacterial infections. In\nthis paper, we consider fast Raman spectroscopy data and demonstrate that a\nlogistic regression model with carefully selected features achieves accuracy\ncomparable to that of neural networks, while being much simpler and more\ntransparent. Our analysis leverages wavelet features with intuitive chemical\ninterpretations, and performs controlled variable selection with knockoffs to\nensure the predictors are relevant and non-redundant. Although we focus on a\nparticular data set, the proposed approach is broadly applicable to other types\nof signal data for which interpretability may be important.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 21:09:50 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2020 22:08:50 GMT"}, {"version": "v3", "created": "Sun, 2 May 2021 02:22:54 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Chia", "Charmaine", ""], ["Sesia", "Matteo", ""], ["Ho", "Chi-Sing", ""], ["Jeffrey", "Stefanie S.", ""], ["Dionne", "Jennifer", ""], ["Cand\u00e8s", "Emmanuel J.", ""], ["Howe", "Roger T.", ""]]}, {"id": "2006.05002", "submitter": "Ruoyu Wang", "authors": "Ruoyu Wang and Qihua Wang", "title": "Determination and estimation of optimal quarantine duration for\n  infectious diseases with application to data analysis of COVID-19", "comments": null, "journal-ref": "biometrics,2021", "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quarantine measure is a commonly used non-pharmaceutical intervention during\nthe outbreak of infectious diseases. A key problem for implementing quarantine\nmeasure is to determine the duration of quarantine. In this paper, a policy\nwith optimal quarantine duration is developed. The policy suggests different\nquarantine durations for every individual with different characteristic. The\npolicy is optimal in the sense that it minimizes the average quarantine\nduration of uninfected people with the constraint that the probability of\nsymptom presentation for infected people attains the given value closing to 1.\nThe optimal solution for the quarantine duration is obtained and estimated by\nsome statistic methods with application to analyzing COVID-19 data.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 01:11:51 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 10:03:38 GMT"}, {"version": "v3", "created": "Wed, 28 Apr 2021 07:41:44 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Wang", "Ruoyu", ""], ["Wang", "Qihua", ""]]}, {"id": "2006.05020", "submitter": "Drew Yarger", "authors": "Drew Yarger, Stilian Stoev, Tailen Hsing", "title": "A functional-data approach to the Argo data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Argo data is a modern oceanography dataset that provides unprecedented\nglobal coverage of temperature and salinity measurements in the upper 2,000\nmeters of depth of the ocean. We study the Argo data from the perspective of\nfunctional data analysis (FDA). We develop spatio-temporal functional kriging\nmethodology for mean and covariance estimation to predict temperature and\nsalinity at a fixed location as a smooth function of depth. By combining tools\nfrom FDA and spatial statistics, including smoothing splines, local regression,\nand multivariate spatial modeling and prediction, our approach provides\nadvantages over current methodology that consider pointwise estimation at fixed\ndepths. Our approach naturally leverages the irregularly-sampled data in space,\ntime, and depth to fit a space-time functional model for temperature and\nsalinity. The developed framework provides new tools to address fundamental\nscientific problems involving the entire upper water column of the oceans such\nas the estimation of ocean heat content, stratification, and thermohaline\noscillation. For example, we show that our functional approach yields more\naccurate ocean heat content estimates than ones based on discrete integral\napproximations in pressure. Further, using the derivative function estimates,\nwe obtain a new product of a global map of the mixed layer depth, a key\ncomponent in the study of heat absorption and nutrient circulation in the\noceans. The derivative estimates also reveal evidence for density inversions in\nareas distinguished by mixing of particularly different water masses.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 02:43:17 GMT"}, {"version": "v2", "created": "Sun, 9 May 2021 23:22:33 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Yarger", "Drew", ""], ["Stoev", "Stilian", ""], ["Hsing", "Tailen", ""]]}, {"id": "2006.05022", "submitter": "Qinqing Zheng", "authors": "Arun Kumar Kuchibhotla and Qinqing Zheng", "title": "Near-Optimal Confidence Sequences for Bounded Random Variables", "comments": "Accepted to ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.AI cs.LG stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many inference problems, such as sequential decision problems like A/B\ntesting, adaptive sampling schemes like bandit selection, are often online in\nnature. The fundamental problem for online inference is to provide a sequence\nof confidence intervals that are valid uniformly over the growing-into-infinity\nsample sizes. To address this question, we provide a near-optimal confidence\nsequence for bounded random variables by utilizing Bentkus' concentration\nresults. We show that it improves on the existing approaches that use the\nCram{\\'e}r-Chernoff technique such as the Hoeffding, Bernstein, and Bennett\ninequalities. The resulting confidence sequence is confirmed to be favorable in\nboth synthetic coverage problems and an application to adaptive stopping\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 02:50:01 GMT"}, {"version": "v2", "created": "Sun, 14 Feb 2021 20:35:34 GMT"}, {"version": "v3", "created": "Thu, 3 Jun 2021 20:43:21 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Kuchibhotla", "Arun Kumar", ""], ["Zheng", "Qinqing", ""]]}, {"id": "2006.05026", "submitter": "Cong Shen", "authors": "Cong Shen, Zhiyang Wang, Sofia S. Villar, and Mihaela van der Schaar", "title": "Learning for Dose Allocation in Adaptive Clinical Trials with Safety\n  Constraints", "comments": "Accepted to the 37th International Conference on Machine Learning\n  (ICML 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phase I dose-finding trials are increasingly challenging as the relationship\nbetween efficacy and toxicity of new compounds (or combination of them) becomes\nmore complex. Despite this, most commonly used methods in practice focus on\nidentifying a Maximum Tolerated Dose (MTD) by learning only from toxicity\nevents. We present a novel adaptive clinical trial methodology, called Safe\nEfficacy Exploration Dose Allocation (SEEDA), that aims at maximizing the\ncumulative efficacies while satisfying the toxicity safety constraint with high\nprobability. We evaluate performance objectives that have operational meanings\nin practical clinical trials, including cumulative efficacy,\nrecommendation/allocation success probabilities, toxicity violation\nprobability, and sample efficiency. An extended SEEDA-Plateau algorithm that is\ntailored for the increase-then-plateau efficacy behavior of molecularly\ntargeted agents (MTA) is also presented. Through numerical experiments using\nboth synthetic and real-world datasets, we show that SEEDA outperforms\nstate-of-the-art clinical trial designs by finding the optimal dose with higher\nsuccess rate and fewer patients.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 03:06:45 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 16:41:45 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Shen", "Cong", ""], ["Wang", "Zhiyang", ""], ["Villar", "Sofia S.", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "2006.05206", "submitter": "Jayden Macklin-Cordes", "authors": "Jayden L. Macklin-Cordes, Erich R. Round", "title": "Re-evaluating phoneme frequencies", "comments": "29pp (3 figures, 3 tables). This article has been provisionally\n  accepted for publication (Frontiers in Psychology, Language Sciences).\n  Supplementary information, data and code available at\n  http://doi.org/10.5281/zenodo.3886212", "journal-ref": null, "doi": "10.3389/fpsyg.2020.570895", "report-no": null, "categories": "cs.CL physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal processes can give rise to distinctive distributions in the linguistic\nvariables that they affect. Consequently, a secure understanding of a\nvariable's distribution can hold a key to understanding the forces that have\ncausally shaped it. A storied distribution in linguistics has been Zipf's law,\na kind of power law. In the wake of a major debate in the sciences around\npower-law hypotheses and the unreliability of earlier methods of evaluating\nthem, here we re-evaluate the distributions claimed to characterize phoneme\nfrequencies. We infer the fit of power laws and three alternative distributions\nto 166 Australian languages, using a maximum likelihood framework. We find\nevidence supporting earlier results, but also nuancing them and increasing our\nunderstanding of them. Most notably, phonemic inventories appear to have a\nZipfian-like frequency structure among their most-frequent members (though\nperhaps also a lognormal structure) but a geometric (or exponential) structure\namong the least-frequent. We compare these new insights the kinds of causal\nprocesses that affect the evolution of phonemic inventories over time, and\nidentify a potential account for why, despite there being an important role for\nphonetic substance in phonemic change, we could still expect inventories with\nhighly diverse phonetic content to share similar distributions of phoneme\nfrequencies. We conclude with priorities for future work in this promising\nprogram of research.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 12:05:10 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 03:56:14 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Macklin-Cordes", "Jayden L.", ""], ["Round", "Erich R.", ""]]}, {"id": "2006.05232", "submitter": "Edward Laurence", "authors": "Edward Laurence, Charles Murphy, Guillaume St-Onge, Xavier\n  Roy-Pomerleau, and Vincent Thibeault", "title": "Detecting structural perturbations from time series with deep learning", "comments": "Main paper:10 pages, 5 figures | Supplementary material: 8 pages, 2\n  figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Small disturbances can trigger functional breakdowns in complex systems. A\nchallenging task is to infer the structural cause of a disturbance in a\nnetworked system, soon enough to prevent a catastrophe. We present a graph\nneural network approach, borrowed from the deep learning paradigm, to infer\nstructural perturbations from functional time series. We show our data-driven\napproach outperforms typical reconstruction methods while meeting the accuracy\nof Bayesian inference. We validate the versatility and performance of our\napproach with epidemic spreading, population dynamics, and neural dynamics, on\nvarious network structures: random networks, scale-free networks, 25 real\nfood-web systems, and the C. Elegans connectome. Moreover, we report that our\napproach is robust to data corruption. This work uncovers a practical avenue to\nstudy the resilience of real-world complex systems.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 13:08:40 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Laurence", "Edward", ""], ["Murphy", "Charles", ""], ["St-Onge", "Guillaume", ""], ["Roy-Pomerleau", "Xavier", ""], ["Thibeault", "Vincent", ""]]}, {"id": "2006.05303", "submitter": "Sudhansu Sekhar Maiti", "authors": "Sudhansu S. Maiti and Sukanta Pramanik", "title": "A Generalized One Parameter Polynomial Exponential Generator Family of\n  Distributions", "comments": "arXiv admin note: substantial text overlap with arXiv:1805.03892", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new class of distributions, called Generalized One Parameter Polynomial\nExponential-G family of distributions is proposed for modelling lifetime data.\nAn account of the structural and reliability properties of the new class is\npresented. Maximum likelihood estimation of parameters of the class of\ndistributions has been described. Simulation study results have been reported.\nTwo data sets have been analyzed to illustrate its applicability.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 17:33:16 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Maiti", "Sudhansu S.", ""], ["Pramanik", "Sukanta", ""]]}, {"id": "2006.05488", "submitter": "Zahra Azadi", "authors": "Zahra Azadi, Sandra D. Eksioglu, H. Neil Geismar", "title": "Optimization of Distribution Network Configuration for Pediatric\n  Vaccines using Chance Constraint Programming", "comments": "36 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Millions of young people are not immunized in low- and middle-income (LMI)\ncountries because of low vaccine availability resulting from inefficiencies in\ncold supply chains. We create supply chain network design and distribution\nmodels to address the unique characteristics and challenges facing vaccine\nsupply chains in LMI countries. The models capture the uncertainties of demand\nfor vaccinations and the resulting impacts on immunization, the unique\nchallenges of vaccine administration (such as open vial wastage), the\ninteractions between technological improvements of vaccines and immunizations,\nand the trade-offs between immunization coverage rates and available resources.\nThe objective is to maximize both the percentage of fully immunized children\nand the vaccine availability in clinics. Our research examines how these two\nmetrics are affected by three factors: number of tiers in the supply chain,\nvaccine vial size, and new vaccine technologies. We tested the model using\nNiger's Expanded Program on Immunization, which is sponsored by the World\nHealth Organization. We make many observations and recommendations to help LMI\ncountries increase their immunization coverage.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 20:03:40 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Azadi", "Zahra", ""], ["Eksioglu", "Sandra D.", ""], ["Geismar", "H. Neil", ""]]}, {"id": "2006.05532", "submitter": "Andres Babino", "authors": "Andres Babino, Marcelo O. Magnasco", "title": "Masks and COVID-19: a causal framework for imputing value to\n  public-health interventions", "comments": "27 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the COVID-19 pandemic, the scientific community developed predictive\nmodels to evaluate potential governmental interventions. However, the analysis\nof the effects these interventions had is less advanced. Here, we propose a\ndata-driven framework to assess these effects retrospectively. We use a\nregularized regression to find a parsimonious model that fits the data with the\nleast changes in the Rt parameter. Then, we postulate each jump in Rt as the\neffect of an intervention. Following the do-operator prescriptions, we simulate\nthe counterfactual case by forcing Rt to stay at the pre-jump value. We then\nattribute a value to the intervention from the difference between true\nevolution and simulated counterfactual. We show that the recommendation to use\nfacemasks for all activities would reduce the number of cases by 170000 (95% CI\n160000 to 180000) in Connecticut, Massachusetts, and New York State. The\nframework presented here might be used in any case where cause and effects are\nsparse in time.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 22:24:25 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 21:34:25 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Babino", "Andres", ""], ["Magnasco", "Marcelo O.", ""]]}, {"id": "2006.05572", "submitter": "MohammadReza Ebrahimi", "authors": "MohammadReza Ebrahimi, Navona Calarco, Kieran Campbell, Colin Hawco,\n  Aristotle Voineskos, Ashish Khisti", "title": "Time-Resolved fMRI Shared Response Model using Gaussian Process Factor\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG eess.IV stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-subject fMRI studies are challenging due to the high variability of\nboth brain anatomy and functional brain topographies across participants. An\neffective way of aggregating multi-subject fMRI data is to extract a shared\nrepresentation that filters out unwanted variability among subjects. Some\nrecent work has implemented probabilistic models to extract a shared\nrepresentation in task fMRI. In the present work, we improve upon these models\nby incorporating temporal information in the common latent structures. We\nintroduce a new model, Shared Gaussian Process Factor Analysis (S-GPFA), that\ndiscovers shared latent trajectories and subject-specific functional\ntopographies, while modelling temporal correlation in fMRI data. We demonstrate\nthe efficacy of our model in revealing ground truth latent structures using\nsimulated data, and replicate experimental performance of time-segment matching\nand inter-subject similarity on the publicly available Raider and Sherlock\ndatasets. We further test the utility of our model by analyzing its learned\nmodel parameters in the large multi-site SPINS dataset, on a social cognition\ntask from participants with and without schizophrenia.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 00:15:01 GMT"}, {"version": "v2", "created": "Sat, 5 Sep 2020 01:13:56 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Ebrahimi", "MohammadReza", ""], ["Calarco", "Navona", ""], ["Campbell", "Kieran", ""], ["Hawco", "Colin", ""], ["Voineskos", "Aristotle", ""], ["Khisti", "Ashish", ""]]}, {"id": "2006.05581", "submitter": "Tianjian Zhou", "authors": "Tianjian Zhou and Yuan Ji", "title": "Semiparametric Bayesian Inference for the Transmission Dynamics of\n  COVID-19 with a State-Space Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The outbreak of Coronavirus Disease 2019 (COVID-19) is an ongoing pandemic\naffecting over 200 countries and regions. Inference about the transmission\ndynamics of COVID-19 can provide important insights into the speed of disease\nspread and the effects of mitigation policies. We develop a novel Bayesian\napproach to such inference based on a probabilistic compartmental model using\ndata of daily confirmed COVID-19 cases. In particular, we consider a\nprobabilistic extension of the classical susceptible-infectious-recovered\nmodel, which takes into account undocumented infections and allows the\nepidemiological parameters to vary over time. We estimate the disease\ntransmission rate via a Gaussian process prior, which captures nonlinear\nchanges over time without the need of specific parametric assumptions. We\nutilize a parallel-tempering Markov chain Monte Carlo algorithm to efficiently\nsample from the highly correlated posterior space. Predictions for future\nobservations are done by sampling from their posterior predictive\ndistributions. Performance of the proposed approach is assessed using simulated\ndatasets. Finally, our approach is applied to COVID-19 data from four states of\nthe United States: Washington, New York, California, and Illinois. An R package\nBaySIR is made available at https://github.com/tianjianzhou/BaySIR for the\npublic to conduct independent analysis or reproduce the results in this paper.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 00:46:41 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 21:00:15 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Zhou", "Tianjian", ""], ["Ji", "Yuan", ""]]}, {"id": "2006.05617", "submitter": "Emiliano Valdez", "authors": "Zhiyu Quan, Zhiguo Wang, Guojun Gan and Emiliano A. Valdez", "title": "Hybrid Tree-based Models for Insurance Claims", "comments": "24 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-part models and Tweedie generalized linear models (GLMs) have been used\nto model loss costs for short-term insurance contract. For most portfolios of\ninsurance claims, there is typically a large proportion of zero claims that\nleads to imbalances resulting in inferior prediction accuracy of these\ntraditional approaches. This article proposes the use of tree-based models with\na hybrid structure that involves a two-step algorithm as an alternative\napproach to these traditional models. The first step is the construction of a\nclassification tree to build the probability model for frequency. In the second\nstep, we employ elastic net regression models at each terminal node from the\nclassification tree to build the distribution model for severity. This hybrid\nstructure captures the benefits of tuning hyperparameters at each step of the\nalgorithm; this allows for improved prediction accuracy and tuning can be\nperformed to meet specific business objectives. We examine and compare the\npredictive performance of such a hybrid tree-based structure in relation to the\ntraditional Tweedie model using both real and synthetic datasets. Our empirical\nresults show that these hybrid tree-based models produce more accurate\npredictions without the loss of intuitive interpretation.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 02:21:31 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Quan", "Zhiyu", ""], ["Wang", "Zhiguo", ""], ["Gan", "Guojun", ""], ["Valdez", "Emiliano A.", ""]]}, {"id": "2006.05750", "submitter": "Christoph Berninger", "authors": "Christoph Berninger, Almond St\\\"ocker, David R\\\"ugamer", "title": "A Bayesian Time-Varying Autoregressive Model for Improved Short- and\n  Long-Term Prediction", "comments": "Revised Introduction, results unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the application to German interest rates, we propose a\ntimevarying autoregressive model for short and long term prediction of time\nseries that exhibit a temporary non-stationary behavior but are assumed to mean\nrevert in the long run. We use a Bayesian formulation to incorporate prior\nassumptions on the mean reverting process in the model and thereby regularize\npredictions in the far future. We use MCMC-based inference by deriving relevant\nfull conditional distributions and employ a Metropolis-Hastings within Gibbs\nSampler approach to sample from the posterior (predictive) distribution. In\ncombining data-driven short term predictions with long term distribution\nassumptions our model is competitive to the existing methods in the short\nhorizon while yielding reasonable predictions in the long run. We apply our\nmodel to interest rate data and contrast the forecasting performance to the one\nof a 2-Additive-Factor Gaussian model as well as to the predictions of a\ndynamic Nelson-Siegel model.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 09:39:07 GMT"}, {"version": "v2", "created": "Sun, 21 Feb 2021 18:24:11 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Berninger", "Christoph", ""], ["St\u00f6cker", "Almond", ""], ["R\u00fcgamer", "David", ""]]}, {"id": "2006.05788", "submitter": "Chiara Bocci", "authors": "Chiara Bocci, Laura Grassini, Emilia Rocco", "title": "A multiple inflated negative binomial hurdle regression model: analysis\n  of the Italians' tourism behaviour during the Great Recession", "comments": null, "journal-ref": null, "doi": "10.1007/s10260-020-00542-6", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyse tourism behaviour of Italian residents in the period covering the\n2008 Great Recession. Using the Trips of Italian Residents in Italy and Abroad\nquarterly survey, carried out by the Italian National Institute of Statistics,\nwe investigate whether and how the economic recession has affected the total\nnumber of overnight stays. The response variable is the result of a two-stage\ndecision process: first we choose to take a holiday, then for how long.\nMoreover, since the number of overnight stays is typically concentrated on\nspecific lengths (week-end, week, fortnight) we observe multiple peculiar\nspikes in its distribution. To take into account these two distinctive\ncharacteristics, we generalise the usual hurdle regression model by specifying\na multiple inflated truncated negative binomial distribution for the positive\nresponses. Results show that the economic recession impacted negatively on both\ncomponents of the decision process and that, by controlling for the inflated\nnature of the response variable's distribution, the proposed formulation\nprovides a better representation of the Italians' tourism behaviour in\ncomparison with non-inflated hurdle models. Given this, we believe that our\nmodel can be a useful tool for policy makers who are trying to forecast the\neffects of new targeted policies to support tourism economy.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 11:58:34 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Bocci", "Chiara", ""], ["Grassini", "Laura", ""], ["Rocco", "Emilia", ""]]}, {"id": "2006.05791", "submitter": "James Warner", "authors": "James E. Warner, Julian Cuevas, Geoffrey F. Bomarito, Patrick E.\n  Leser, William P. Leser", "title": "Inverse Estimation of Elastic Modulus Using Physics-Informed Generative\n  Adversarial Networks", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": "NASA/TM-2020-5001300", "categories": "eess.IV cs.CE cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While standard generative adversarial networks (GANs) rely solely on training\ndata to learn unknown probability distributions, physics-informed GANs\n(PI-GANs) encode physical laws in the form of stochastic partial differential\nequations (PDEs) using auto differentiation. By relating observed data to\nunobserved quantities of interest through PDEs, PI-GANs allow for the\nestimation of underlying probability distributions without their direct\nmeasurement (i.e. inverse problems). The scalable nature of GANs allows\nhigh-dimensional, spatially-dependent probability distributions (i.e., random\nfields) to be inferred, while incorporating prior information through PDEs\nallows the training datasets to be relatively small.\n  In this work, PI-GANs are demonstrated for the application of elastic modulus\nestimation in mechanical testing. Given measured deformation data, the\nunderlying probability distribution of spatially-varying elastic modulus\n(stiffness) is learned. Two feed-forward deep neural network generators are\nused to model the deformation and material stiffness across a two dimensional\ndomain. Wasserstein GANs with gradient penalty are employed for enhanced\nstability. In the absence of explicit training data, it is demonstrated that\nthe PI-GAN learns to generate realistic, physically-admissible realizations of\nmaterial stiffness by incorporating the PDE that relates it to the measured\ndeformation. It is shown that the statistics (mean, standard deviation,\npoint-wise distributions, correlation length) of these generated stiffness\nsamples have good agreement with the true distribution.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 20:14:10 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Warner", "James E.", ""], ["Cuevas", "Julian", ""], ["Bomarito", "Geoffrey F.", ""], ["Leser", "Patrick E.", ""], ["Leser", "William P.", ""]]}, {"id": "2006.05859", "submitter": "Chenshuo Sun", "authors": "Anindya Ghose, Beibei Li, Meghanath Macha, Chenshuo Sun, Natasha Ying\n  Zhang Foutz", "title": "Trading Privacy for the Greater Social Good: How Did America React\n  During COVID-19?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM econ.GN q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital contact tracing and analysis of social distancing from smartphone\nlocation data are two prime examples of non-therapeutic interventions used in\nmany countries to mitigate the impact of the COVID-19 pandemic. While many\nunderstand the importance of trading personal privacy for the public good,\nothers have been alarmed at the potential for surveillance via measures enabled\nthrough location tracking on smartphones. In our research, we analyzed massive\nyet atomic individual-level location data containing over 22 billion records\nfrom ten Blue (Democratic) and ten Red (Republican) cities in the U.S., based\non which we present, herein, some of the first evidence of how Americans\nresponded to the increasing concerns that government authorities, the private\nsector, and public health experts might use individual-level location data to\ntrack the COVID-19 spread. First, we found a significant decreasing trend of\nmobile-app location-sharing opt-out. Whereas areas with more Democrats were\nmore privacy-concerned than areas with more Republicans before the advent of\nthe COVID-19 pandemic, there was a significant decrease in the overall opt-out\nrates after COVID-19, and this effect was more salient among Democratic than\nRepublican cities. Second, people who practiced social distancing (i.e., those\nwho traveled less and interacted with fewer close contacts during the pandemic)\nwere also less likely to opt-out, whereas the converse was true for people who\npracticed less social-distancing. This relationship also was more salient among\nDemocratic than Republican cities. Third, high-income populations and males,\ncompared with low-income populations and females, were more\nprivacy-conscientious and more likely to opt-out of location tracking.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 14:36:17 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Ghose", "Anindya", ""], ["Li", "Beibei", ""], ["Macha", "Meghanath", ""], ["Sun", "Chenshuo", ""], ["Foutz", "Natasha Ying Zhang", ""]]}, {"id": "2006.05860", "submitter": "Phoebus Rosakis", "authors": "Phoebus Rosakis and Maria Marketou", "title": "Rethinking Case Fatality Ratios for COVID-19 from a data-driven\n  viewpoint", "comments": "accepted in Journal of Infection; 11 pages, 2 figures, 11 references,\n  supplementary appendix", "journal-ref": "The Journal of infection, S0163-4453(20)30391-1. 11 Jun. 2020", "doi": "10.1016/j.jinf.2020.06.010", "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The case fatality ratio (CFR) for COVID-19 is difficult to estimate. One\ndifficulty is due to ignoring or overestimating time delay between reporting\nand death. We claim that all of these cause large errors and artificial time\ndependence of the CFR. We find that for each country, there is a unique value\nof the time lag between reported cases and deaths versus time, that yields the\noptimal correlation between them is a specific sense. We find that the\nresulting corrected CFR (deaths shifted back by this time lag, divided by\ncases) is actually constant over many months, for many countries, but also for\nthe entire world. This optimal time lag and constant CFR for each country can\nbe found through a simple data driven algorithm. The traditional CFR (ignoring\ntime lag) is spuriously time-dependent and its evolution is hard to quantify.\nOur corrected CFR is constant over time, therefore an important index of the\npandemic in each country, and can be inferred from data earlier on,\nfacilitating improved early estimates of COVID-19 mortality.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 14:36:40 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Rosakis", "Phoebus", ""], ["Marketou", "Maria", ""]]}, {"id": "2006.06151", "submitter": "Jae Youn Ahn", "authors": "Rosy Oh, Himchan Jeong, Jae Youn Ahn, Emiliano A. Valdez", "title": "On a Multi-Year Microlevel Collective Risk Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a typical insurance portfolio, the claims process for a short period,\ntypically one year, is characterized by observing frequency of claims together\nwith the associated claims severities. The collective risk model describes this\nportfolio as a random sum of the aggregation of the claim amounts. In the\nclassical framework, for simplicity, the claim frequency and claim severities\nare assumed to be mutually independent. However, there is a growing interest in\nrelaxing this independence assumption which is more realistic and useful for\nthe practical insurance ratemaking. While the common thread has been capturing\nthe dependence between frequency and aggregate severity within a single period,\nthe work of Oh et al. (2020a) provides an interesting extension to the addition\nof capturing dependence among individual severities. In this paper, we extend\nthese works within a framework where we have a portfolio of microlevel\nfrequencies and severities for multiple years. This allows us to develop a\nfactor copula model framework that captures various types of dependence between\nclaim frequencies and claim severities over multiple years. It is therefore a\nclear extension of earlier works on one-year dependent frequency-severity\nmodels and on random effects model for capturing serial dependence of claims.\nWe focus on the results using a family of elliptical copulas to model the\ndependence. The paper further describes how to calibrate the proposed model\nusing illustrative claims data arising from a Singapore insurance company. The\nestimated results provide strong evidence of all forms of dependencies captured\nby our model.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 02:03:08 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Oh", "Rosy", ""], ["Jeong", "Himchan", ""], ["Ahn", "Jae Youn", ""], ["Valdez", "Emiliano A.", ""]]}, {"id": "2006.06274", "submitter": "Philipp Baumann", "authors": "Philipp Baumann, Enzo Rossi, Alexander Volkmann", "title": "What Drives Inflation and How: Evidence from Additive Mixed Models\n  Selected by cAIC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze which forces explain inflation and how in a large panel of 124\ncountries from 1997 to 2015. Models motivated by economic theory are compared\nto an approach based on model-based boosting and non-linearities are explicitly\nconsidered. We provide compelling evidence that the interaction of energy price\nand energy rents stand out among 40 explanatory variables. The output gap and\nglobalization are also relevant drivers of inflation. Credit and money growth,\na country's inflation history and demographic changes are comparably less\nimportant while central bank related variables as well as political variables\nturn out to have the least empirical relevance. In a subset of countries public\ndebt denomination and exchange rate arrangements also play a noteworthy role in\nthe inflation process. By contrast, other public-debt variables and an\ninflation targeting regime have weaker explanatory power. Finally, there is\nclear evidence of structural breaks in the effects since the financial crisis.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 09:30:00 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Baumann", "Philipp", ""], ["Rossi", "Enzo", ""], ["Volkmann", "Alexander", ""]]}, {"id": "2006.06739", "submitter": "Anna Heath", "authors": "Anna Heath, Maryna Yaskina, Petros Pechlivanoglou, Juan David Rios,\n  Martin Offringa, Terry P Klassen, Naveen Poonai, Eleanor Pullenayegum", "title": "A Bayesian response-adaptive dose finding and comparative effectiveness\n  trial", "comments": "10 pages, 2 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aims: Combinations of treatments can offer additional benefit over the\ntreatments individually. However, trials of these combinations are lower\npriority than the development of novel therapies, which can restrict funding,\ntimelines and patient availability. This paper develops a novel trial design to\nfacilitate the evaluation of novel combination therapies that combines elements\nof phase II and phase III trials. Methods: This trial uses response adaptive\nrandomisation to increase the information collected about successful novel drug\ncombinations and Bayesian dose-response modelling to undertake a\ncomparative-effectiveness analysis for the most successful dose combination\nagainst a relevant comparator. We used simulation methods to evaluate the\nprobability of selecting the correct optimal dose combination, the operating\ncharacteristics and predictive power of this design for a trial in pain\nmanagement and sedation in paediatric emergency departments. Results: With 410\nparticipants, 5 interim updates of the randomisation ratio and a probability of\neffectiveness of 0.93, 0.88 and 0.83 for the three dose combinations, we have\nan 83% chance of randomising the largest number of patients to the drug with\nthe highest probability of effectiveness. Based on this adaptive randomisation\nprocedure, the comparative effectiveness analysis has a type I error of less\nthan 5% and a 93% chance of correctly concluding non-inferiority when the\nprobability of effectiveness for the optimal combination therapy is 0.9. In\nthis case, the trial has a 77% chance of meeting its dual aims of dose finding\nand comparative effectiveness. Finally, the Bayesian predictive power of the\ntrial is over 90%. Conclusion: The proposed trial has high potential to meet\nthe dual study objectives within a feasible level of recruitment, minimising\nthe administrative burden and recruitment time for a trial.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 18:53:08 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 13:35:55 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Heath", "Anna", ""], ["Yaskina", "Maryna", ""], ["Pechlivanoglou", "Petros", ""], ["Rios", "Juan David", ""], ["Offringa", "Martin", ""], ["Klassen", "Terry P", ""], ["Poonai", "Naveen", ""], ["Pullenayegum", "Eleanor", ""]]}, {"id": "2006.06809", "submitter": "Berkay Gulcan", "authors": "Sandra D. Eksioglu and Berkay Gulcan and Mohammad Roni and Scott Mason", "title": "A Stochastic Biomass Blending Problem in Decentralized Supply Chains", "comments": "33 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blending biomass materials of different physical or chemical properties\nprovides an opportunity to adjust the quality of the feedstock to meet the\nspecifications of the conversion platform. We propose a model which identifies\nthe right mix of biomass to optimize the performance of the thermochemical\nconversion process at the minimum cost. This is a chance-constraint programming\n(CCP) model which takes into account the stochastic nature of biomass quality.\nThe proposed CCP model ensures that process requirements, which are impacted by\nphysical and chemical properties of biomass, are met most of the time. We\nconsider two problem settings, a centralized and a decentralized supply chain.\nWe propose a mixed-integer linear program to model the blending problem in the\ncentralized setting and a bilevel program to model the blending problem in the\ndecentralized setting. We use the sample average approximation (SAA) method to\napproximate the chance constraints, and propose solution algorithms to solve\nthis approximation. We develop a case study for South Carolina using data\nprovided by the Billion Ton Study. Based on our results, the blends identified\nconsist mainly of pine and softwood residues. The cost of the centralized\nsupply chain is 2 to 6% lower, which shows that the assumption of centralized\ndecision making leads to underestimating costs in the supply chain.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 20:49:48 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Eksioglu", "Sandra D.", ""], ["Gulcan", "Berkay", ""], ["Roni", "Mohammad", ""], ["Mason", "Scott", ""]]}, {"id": "2006.06819", "submitter": "Nuoa Lei", "authors": "Nuoa Lei", "title": "A robust modeling framework for energy analysis of data centers", "comments": null, "journal-ref": null, "doi": "10.1145/3401335.3401648", "report-no": null, "categories": "cs.CY stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Global digitalization has given birth to the explosion of digital services in\napproximately every sector of contemporary life. Applications of artificial\nintelligence, blockchain technologies, and internet of things are promising to\naccelerate digitalization further. As a consequence, the number of data\ncenters, which provide the services of data processing, storage, and\ncommunication services, is also increasing rapidly. Because data centers are\nenergy-intensive with significant and growing electricity demand, an energy\nmodel of data centers with temporal, spatial, and predictive analysis\ncapability is critical for guiding industry and governmental authorities for\nmaking technology investment decisions. However, current models fail to provide\nconsistent and high dimensional energy analysis for data centers due to severe\ndata gaps. This can be further attributed to the lack of the modeling\ncapabilities for energy analysis of data center components including IT\nequipment and data center cooling and power provisioning infrastructure in\ncurrent energy models. In this research, a technology-based modeling framework,\nin hybrid with a data-driven approach, is proposed to address the knowledge\ngaps in current data center energy models. The research aims to provide policy\nmakers and data center energy analysts with comprehensive understanding of data\ncenter energy use and efficiency opportunities and a better understanding of\nmacro-level data center energy demand and energy saving potentials, in addition\nto the technological barriers for adopting energy efficiency measures.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 21:05:20 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Lei", "Nuoa", ""]]}, {"id": "2006.06862", "submitter": "Donghan Liu", "authors": "Donghan Liu, Benjamin C. M. Fung, Tak Pan Wong", "title": "Deep Learning-based Stress Determinator for Mouse Psychiatric Analysis\n  using Hippocampus Activity", "comments": "The paper need re-evaluated and reviewed, may cause some significant\n  changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decoding neurons to extract information from transmission and employ them\ninto other use is the goal of neuroscientists' study. Due to that the field of\nneuroscience is utilizing the traditional methods presently, we hence combine\nthe state-of-the-art deep learning techniques with the theory of neuron\ndecoding to discuss its potential of accomplishment. Besides, the stress level\nthat is related to neuron activity in hippocampus is statistically examined as\nwell. The experiments suggest that our state-of-the-art deep learning-based\nstress determinator provides good performance with respect to its model\nprediction accuracy and additionally, there is strong evidence against\nequivalence of mouse stress level under diverse environments.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 22:32:31 GMT"}, {"version": "v2", "created": "Sat, 27 Jun 2020 21:31:14 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Liu", "Donghan", ""], ["Fung", "Benjamin C. M.", ""], ["Wong", "Tak Pan", ""]]}, {"id": "2006.06864", "submitter": "Ashton Wiens", "authors": "Ashton Wiens and William Kleiber and Douglas Nychka and Katherine R.\n  Barnhart", "title": "Nonrigid registration using Gaussian processes and local likelihood\n  estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surface registration, the task of aligning several multidimensional point\nsets, is a necessary task in many scientific fields. In this work, a novel\nstatistical approach is developed to solve the problem of nonrigid\nregistration. While the application of an affine transformation results in\nrigid registration, using a general nonlinear function to achieve nonrigid\nregistration is necessary when the point sets require deformations that change\nover space. The use of a local likelihood-based approach using windowed\nGaussian processes provides a flexible way to accurately estimate the nonrigid\ndeformation. This strategy also makes registration of massive data sets\nfeasible by splitting the data into many subsets. The estimation results yield\nspatially-varying local rigid registration parameters. Gaussian process surface\nmodels are then fit to the parameter fields, allowing prediction of the\ntransformation parameters at unestimated locations, specifically at observation\nlocations in the unregistered data set. Applying these transformations results\nin a global, nonrigid registration. A penalty on the transformation parameters\nis included in the likelihood objective function. Combined with smoothing of\nthe local estimates from the surface models, the nonrigid registration model\ncan prevent the problem of overfitting. The efficacy of the nonrigid\nregistration method is tested in two simulation studies, varying the number of\nwindows and number of points, as well as the type of deformation. The nonrigid\nmethod is applied to a pair of massive remote sensing elevation data sets\nexhibiting complex geological terrain, with improved accuracy and uncertainty\nquantification in a cross validation study versus two rigid registration\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 22:58:36 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Wiens", "Ashton", ""], ["Kleiber", "William", ""], ["Nychka", "Douglas", ""], ["Barnhart", "Katherine R.", ""]]}, {"id": "2006.07101", "submitter": "Fengqing Chao Dr.", "authors": "Fengqing Chao, Patrick Gerland, Alex R. Cook, Leontine Alkema", "title": "Global estimation and scenario-based projections of sex ratio at birth\n  and missing female births using a Bayesian hierarchical time series mixture\n  model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The sex ratio at birth (SRB) is defined as the ratio of male to female live\nbirths. The SRB imbalance in parts of the world over the past several decades\nis a direct consequence of sex-selective abortion, driven by the co-existence\nof son preference, readily available technology of prenatal sex determination,\nand fertility decline. Estimation and projection of the degree of SRB imbalance\nis complicated because of variability in SRB reference levels and because of\nthe uncertainty associated with SRB observations. We develop Bayesian\nhierarchical time series mixture models for SRB estimation and scenario-based\nprojections for all countries from 1950 to 2100. We model the SRB regional and\nnational reference levels, and the fluctuation around national reference\nlevels. We identify countries at risk of SRB imbalances and model both (i) the\nabsence or presence of sex ratio transitions in such countries and, if present,\n(ii) the transition process. The transition model of SRB imbalance captures\nthree stages (increase, stagnation and convergence back to SRB baselines). The\nmodel identifies countries with statistical evidence of SRB inflation in a\nfully Bayesian approach. The scenario-based SRB projections are based on the\nsex ratio transition model with varying assumptions regarding the occurrence of\na sex ratio transition in at-risk countries. Projections are used to quantify\nthe future burden of missing female births due to sex-selective abortions under\ndifferent scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 11:51:48 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 11:41:16 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Chao", "Fengqing", ""], ["Gerland", "Patrick", ""], ["Cook", "Alex R.", ""], ["Alkema", "Leontine", ""]]}, {"id": "2006.07199", "submitter": "Mathilde Fekom", "authors": "Mathilde Fekom, Nicolas Vayatis and Argyris Kalogeratos", "title": "Dynamic Epidemic Control via Sequential Resource Allocation", "comments": "arXiv admin note: text overlap with arXiv:1909.09678", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI math.DS stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Dynamic Resource Allocation (DRA) problem, an administrator has to\nallocate a limited amount of resources to the nodes of a network in order to\nreduce a diffusion process (DP) (e.g. an epidemic). In this paper we propose a\nmulti-round dynamic control framework, which we realize through two derived\nmodels: the Restricted and the Sequential DRA (RDRA, SDRA), that allows for\nrestricted information and access to the entire network, contrary to standard\nfull-information and full-access DRA models. At each intervention round, the\nadministrator has only access -- simultaneous for the former, sequential for\nthe latter -- to a fraction of the network nodes. This sequential aspect in the\ndecision process offers a completely new perspective to the dynamic DP control,\nmaking this work the first to cast the dynamic control problem as a series of\nsequential selection problems. Through in-depth SIS epidemic simulations we\ncompare the performance of our multi-round approach with other resource\nallocation strategies and several sequential selection algorithms on both\ngenerated, and real-data networks. The results provide evidence about the\nefficiency and applicability of the proposed framework for real-life problems.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 09:35:08 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Fekom", "Mathilde", ""], ["Vayatis", "Nicolas", ""], ["Kalogeratos", "Argyris", ""]]}, {"id": "2006.07305", "submitter": "Yanelli Nunez", "authors": "Yanelli Nunez, Elizabeth A. Gibson, Eva M. Tanner, Chris Gennings,\n  Brent A.Coull, Jeff A. Goldsmith, and Marianthi-Anna Kioumourtzoglou", "title": "Reflection on modern methods: Good practices for applied statistical\n  learning in epidemiology", "comments": "19 pages, 5 figures, 1 table. For associated code, visit\n  https://github.com/yanellinunez/Commentary-to-mixture-methods-paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical learning (SL) includes methods that extract knowledge from\ncomplex data. SL methods beyond generalized linear models are being\nincreasingly implemented in public health research and epidemiology because\nthey can perform better in instances with complex or high-dimensional\ndata---settings when traditional statistical methods fail. These novel methods,\nhowever, often include random sampling which may induce variability in results.\nBest practices in data science can help to ensure robustness. As a case study,\nwe included four SL models that have been applied previously to analyze the\nrelationship between environmental mixtures and health outcomes. We ran each\nmodel across 100 initializing values for random number generation, or \"seeds,\"\nand assessed variability in resulting estimation and inference. All methods\nexhibited some seed-dependent variability in results. The degree of variability\ndiffered across methods and exposure of interest. Any SL method reliant on a\nrandom seed will exhibit some degree of seed sensitivity. We recommend that\nresearchers repeat their analysis with various seeds as a sensitivity analysis\nwhen implementing these methods to enhance interpretability and robustness of\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 16:38:40 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 18:13:45 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Nunez", "Yanelli", ""], ["Gibson", "Elizabeth A.", ""], ["Tanner", "Eva M.", ""], ["Gennings", "Chris", ""], ["Coull", "Brent A.", ""], ["Goldsmith", "Jeff A.", ""], ["Kioumourtzoglou", "Marianthi-Anna", ""]]}, {"id": "2006.07363", "submitter": "Surya Effendy", "authors": "Surya Effendy, Juhyun Song, and Martin Z. Bazant", "title": "Analysis, Design, and Generalization of Electrochemical Impedance\n  Spectroscopy (EIS) Inversion Algorithms", "comments": "46 pages, to be submitted to the Journal of the Electrochemical\n  Society", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a framework for analyzing and designing EIS inversion\nalgorithms. Our framework stems from the observation of four features common to\nwell-defined EIS inversion algorithms, namely (1) the representation of unknown\ndistributions, (2) the minimization of a metric of error to estimate parameters\narising from the chosen representation, subject to constraints on (3) the\ncomplexity control parameters, and (4) a means for choosing optimal control\nparameter values. These features must be present to overcome the ill-posed\nnature of EIS inversion problems. We review three established EIS inversion\nalgorithms to illustrate the pervasiveness of these features, and show the\nutility of the framework by resolving ambiguities concerning three more\nalgorithms. Our framework is then used to design the generalized EIS inversion\n(gEISi) algorithm, which uses Gaussian basis function representation, modality\ncontrol parameter, and cross-validation for choosing the optimal control\nparameter value. The gEISi algorithm is applicable to the generalized EIS\ninversion problem, which allows for a wider range of underlying models. We also\nconsidered the construction of credible intervals for distributions arising\nfrom the algorithm. The algorithm is able to accurately reproduce distributions\nwhich have been difficult to obtain using existing algorithms. It is provided\ngratis on the repository https://github.com/suryaeff/gEISi.git.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 17:54:13 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Effendy", "Surya", ""], ["Song", "Juhyun", ""], ["Bazant", "Martin Z.", ""]]}, {"id": "2006.07513", "submitter": "Yishu Xue", "authors": "Guanyu Hu, Hou-Cheng Yang, Yishu Xue", "title": "Bayesian Group Learning for Shot Selection of Professional Basketball\n  Players", "comments": null, "journal-ref": null, "doi": "10.1002/sta4.324", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we develop a group learning approach to analyze the underlying\nheterogeneity structure of shot selection among professional basketball players\nin the NBA. We propose a mixture of finite mixtures (MFM) model to capture the\nheterogeneity of shot selection among different players based on Log Gaussian\nCox process (LGCP). Our proposed method can simultaneously estimate the number\nof groups and group configurations. An efficient Markov Chain Monte Carlo\n(MCMC) algorithm is developed for our proposed model. Simulation studies have\nbeen conducted to demonstrate its performance. Ultimately, our proposed\nlearning approach is further illustrated in analyzing shot charts of several\nplayers in the NBA's 2017-2018 regular season.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 23:49:08 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 19:15:09 GMT"}, {"version": "v3", "created": "Mon, 19 Oct 2020 23:41:30 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Hu", "Guanyu", ""], ["Yang", "Hou-Cheng", ""], ["Xue", "Yishu", ""]]}, {"id": "2006.07546", "submitter": "Peter Marcy", "authors": "Peter W. Marcy and Curtis B. Storlie", "title": "Bayesian Calibration of Computer Models with Informative Failures", "comments": null, "journal-ref": null, "doi": null, "report-no": "LA-UR-18-21204", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many practical difficulties in the calibration of computer models\nto experimental data. One such complication is the fact that certain\ncombinations of the calibration inputs can cause the code to output data\nlacking fundamental properties, or even to produce no output at all. In many\ncases the researchers want or need to exclude the possibility of these\n\"failures\" within their analyses. We propose a Bayesian (meta-)model in which\nthe posterior distribution for the calibration parameters naturally excludes\nregions of the input space corresponding to failed runs. That is, we define a\nstatistical selection model to rigorously couple the disjoint problems of\nbinary classification and computer model calibration. We demonstrate our\nmethodology using data from a carbon capture experiment in which the numerics\nof the computational fluid dynamics are prone to instability.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 03:23:46 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Marcy", "Peter W.", ""], ["Storlie", "Curtis B.", ""]]}, {"id": "2006.07785", "submitter": "Yuan Shijie", "authors": "Jiaying Lyu, Tianjian Zhou, Shijie Yuan, Wentian Guo, Yuan Ji", "title": "MUCE: Bayesian Hierarchical Modeling for the Design and Analysis of\n  Phase 1b Multiple Expansion Cohort Trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a multiple cohort expansion (MUCE) approach as a design or\nanalysis method for phase 1b multiple expansion cohort trials, which are novel\nfirst-in-human studies conducted following phase 1a dose escalation. The MUCE\ndesign is based on a class of Bayesian hierarchical models that adaptively\nborrow information across arms. Statistical inference is directly based on the\nposterior probability of each arm being efficacious, facilitating the decision\nmaking that decides which arm to select for further testing.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 03:39:18 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 07:19:00 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Lyu", "Jiaying", ""], ["Zhou", "Tianjian", ""], ["Yuan", "Shijie", ""], ["Guo", "Wentian", ""], ["Ji", "Yuan", ""]]}, {"id": "2006.07949", "submitter": "Kosuke Morikawa", "authors": "Kosuke Morikawa, Hiromichi Nagao, Shin-ichi Ito, Yoshikazu Terada,\n  Shin'ichi Sakai, and Naoshi Hirata", "title": "Forecasting temporal variation of aftershocks immediately after a main\n  shock using Gaussian process regression", "comments": null, "journal-ref": null, "doi": "10.1093/gji/ggab124", "report-no": null, "categories": "physics.geo-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncovering the distribution of magnitudes and arrival times of aftershocks is\na key to comprehend the characteristics of the sequence of earthquakes, which\nenables us to predict seismic activities and hazard assessments. However,\nidentifying the number of aftershocks immediately after the main shock is\npractically difficult due to contaminations of arriving seismic waves. To\novercome the difficulty, we construct a likelihood based on the detected data\nincorporating a detection function to which the Gaussian process regression\n(GPR) is applied. The GPR is capable of estimating not only the parameters of\nthe distribution of aftershocks together with the detection function but also\ncredible intervals for both of the parameters and the detection function. A\nproperty that distributions of both the Gaussian process and aftershocks are\nexponential functions leads to an efficient Bayesian computational algorithm to\nestimate the hyperparameters. After the validations through numerical tests,\nthe proposed method is retrospectively applied to the catalog data related to\nthe 2004 Chuetsu earthquake towards early forecasting of the aftershocks. The\nresult shows that the proposed method stably estimates the parameters of the\ndistribution simultaneously their credible intervals even within three hours\nafter the main shock.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 16:32:05 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2020 06:00:09 GMT"}, {"version": "v3", "created": "Tue, 15 Sep 2020 23:49:18 GMT"}, {"version": "v4", "created": "Wed, 3 Feb 2021 11:25:39 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Morikawa", "Kosuke", ""], ["Nagao", "Hiromichi", ""], ["Ito", "Shin-ichi", ""], ["Terada", "Yoshikazu", ""], ["Sakai", "Shin'ichi", ""], ["Hirata", "Naoshi", ""]]}, {"id": "2006.08008", "submitter": "Chaitanya Joshi Dr.", "authors": "Chaitanya Joshi, Clayton D'Ath, Sophie Curtis-Ham, Deane Searle", "title": "Considerations for developing predictive models of crime and new methods\n  for measuring their accuracy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing spatio-temporal crime prediction models, and to a lesser extent,\ndeveloping measures of accuracy and operational efficiency for them, has been\nan active area of research for almost two decades. Despite calls for rigorous\nand independent evaluations of model performance, such studies have been few\nand far between. In this paper, we argue that studies should focus not on\nfinding the one predictive model or the one measure that is the most\nappropriate at all times, but instead on careful consideration of several\nfactors that affect the choice of the model and the choice of the measure, to\nfind the best measure and the best model for the problem at hand. We argue that\nbecause each problem is unique, it is important to develop measures that\nempower the practitioner with the ability to input the choices and preferences\nthat are most appropriate for the problem at hand. We develop a new measure\ncalled the penalized predictive accuracy index (PPAI) which imparts such\nflexibility. We also propose the use of the expected utility function to\ncombine multiple measures in a way that is appropriate for a given problem in\norder to assess the models against multiple criteria. We further propose the\nuse of the average logarithmic score (ALS) measure that is appropriate for many\ncrime models and measures accuracy differently than existing measures. These\nmeasures can be used alongside existing measures to provide a more\ncomprehensive means of assessing the accuracy and potential utility of\nspatio-temporal crime prediction models.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 20:15:21 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Joshi", "Chaitanya", ""], ["D'Ath", "Clayton", ""], ["Curtis-Ham", "Sophie", ""], ["Searle", "Deane", ""]]}, {"id": "2006.08232", "submitter": "Thierry Mara", "authors": "Ivano Azzini (JRC), Thierry Mara (JRC, PIMENT, GdR MASCOT-NUM),\n  Rossana Rosati (JRC)", "title": "Monte Carlo estimators of first-and total-orders Sobol' indices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study compares the performances of two sampling-based strategies for the\nsimultaneous estimation of the first-and total-orders variance-based\nsensitivity indices (a.k.a Sobol' indices). The first strategy was introduced\nby [8] and is the current approach employed by practitioners. The second one\nwas only recently introduced by the authors of the present article. They both\nrely on different estimators of first-and total-orders Sobol' indices. The\nasymp-totic normal variances of the two sets of estimators are established and\ntheir accuracies are compared theoretically and numerically. The results show\nthat the new strategy outperforms the current one.Keywords: global sensitivity\nanalysis, variance-based sensitivity indices, first-order Sobol' index,\ntotal-order Sobol' index, Monte Carlo estimate, asymptotic normality\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 09:07:04 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Azzini", "Ivano", "", "JRC"], ["Mara", "Thierry", "", "JRC, PIMENT, GdR MASCOT-NUM"], ["Rosati", "Rossana", "", "JRC"]]}, {"id": "2006.08300", "submitter": "Oktay Karakus Dr", "authors": "Oktay Karaku\\c{s}, Ercan E. Kuruoglu, Alin Achim", "title": "A Generalized Gaussian Extension to the Rician Distribution for SAR\n  Image Modeling", "comments": "20 Pages, 11 figures, 5 tables", "journal-ref": null, "doi": "10.1109/TGRS.2021.3069091", "report-no": null, "categories": "eess.IV eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel statistical model, $\\textit{the\ngeneralized-Gaussian-Rician}$ (GG-Rician) distribution, for the\ncharacterization of synthetic aperture radar (SAR) images. Since accurate\nstatistical models lead to better results in applications such as target\ntracking, classification, or despeckling, characterizing SAR images of various\nscenes including urban, sea surface, or agricultural, is essential. The\nproposed statistical model is based on the Rician distribution to model the\namplitude of a complex SAR signal, the in-phase and quadrature components of\nwhich are assumed to be generalized-Gaussian distributed. The proposed\namplitude GG-Rician model is further extended to cover the intensity SAR\nsignals. In the experimental analysis, the GG-Rician model is investigated for\namplitude and intensity SAR images of various frequency bands and scenes in\ncomparison to state-of-the-art statistical models that include $\\mathcal{K}$,\nWeibull, Gamma, and Lognormal. In order to decide on the most suitable model,\nstatistical significance analysis via Kullback-Leibler divergence and\nKolmogorov-Smirnov statistics are performed. The results demonstrate the\nsuperior performance and flexibility of the proposed model for all frequency\nbands and scenes and its applicability on both amplitude and intensity SAR\nimages. The Matlab package is available at\nhttps://github.com/oktaykarakus/GG-Rician-SAR-Image-Modelling.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 11:49:15 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 10:35:47 GMT"}, {"version": "v3", "created": "Wed, 7 Apr 2021 15:05:27 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Karaku\u015f", "Oktay", ""], ["Kuruoglu", "Ercan E.", ""], ["Achim", "Alin", ""]]}, {"id": "2006.08350", "submitter": "Bertrand Hassani", "authors": "Bertrand K. Hassani", "title": "Societal biases reinforcement through machine learning: A credit scoring\n  perspective", "comments": "14 pages, 7 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Does machine learning and AI ensure that social biases thrive ? This paper\naims to analyse this issue. Indeed, as algorithms are informed by data, if\nthese are corrupted, from a social bias perspective, good machine learning\nalgorithms would learn from the data provided and reverberate the patterns\nlearnt on the predictions related to either the classification or the\nregression intended. In other words, the way society behaves whether positively\nor negatively, would necessarily be reflected by the models. In this paper, we\nanalyse how social biases are transmitted from the data into banks loan\napprovals by predicting either the gender or the ethnicity of the customers\nusing the exact same information provided by customers through their\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 12:40:21 GMT"}, {"version": "v2", "created": "Sat, 31 Oct 2020 12:48:35 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Hassani", "Bertrand K.", ""]]}, {"id": "2006.08446", "submitter": "Arthur Charpentier", "authors": "Olivier Cabrignac, Arthur Charpentier, Ewen Gallic", "title": "Modeling Joint Lives within Families", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.GN q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Family history is usually seen as a significant factor insurance companies\nlook at when applying for a life insurance policy. Where it is used, family\nhistory of cardiovascular diseases, death by cancer, or family history of high\nblood pressure and diabetes could result in higher premiums or no coverage at\nall. In this article, we use massive (historical) data to study dependencies\nbetween life length within families. If joint life contracts (between a husband\nand a wife) have been long studied in actuarial literature, little is known\nabout child and parents dependencies. We illustrate those dependencies using\n19th century family trees in France, and quantify implications in annuities\ncomputations. For parents and children, we observe a modest but significant\npositive association between life lengths. It yields different estimates for\nremaining life expectancy, present values of annuities, or whole life insurance\nguarantee, given information about the parents (such as the number of parents\nalive). A similar but weaker pattern is observed when using information on\ngrandparents.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 14:47:35 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Cabrignac", "Olivier", ""], ["Charpentier", "Arthur", ""], ["Gallic", "Ewen", ""]]}, {"id": "2006.08522", "submitter": "Ruobin Gong", "authors": "Ruobin Gong", "title": "Transparent Privacy is Principled Privacy", "comments": "2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy revolutionizes the way we think about statistical\ndisclosure limitation. Among the benefits it brings to the table, one is\nparticularly profound and impactful. Under this formal approach to privacy, the\nmechanism with which data is privatized can be spelled out in full\ntransparency, without sacrificing the privacy guarantee. Curators of\nopen-source demographic and scientific data are at a position to offer privacy\nwithout obscurity. This paper supplies a technical treatment to the pitfalls of\nobscure privacy, and establishes transparent privacy as a prerequisite to\ndrawing correct statistical inference. It advocates conceiving transparent\nprivacy as a dynamic component that can improve data quality from the total\nsurvey error perspective, and discusses the limited statistical usability of\nmere procedural transparency which may arise when dealing with mandated\ninvariants. Transparent privacy is the only viable path towards principled\ninference from privatized data releases. Its arrival marks great progress\ntowards improved reproducibility, accountability and public trust.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 16:30:29 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Gong", "Ruobin", ""]]}, {"id": "2006.08527", "submitter": "Thayer Alshaabi", "authors": "Thayer Alshaabi, David Rushing Dewhurst, James P. Bagrow, Peter\n  Sheridan Dodds, Christopher M. Danforth", "title": "The sociospatial factors of death: Analyzing effects of\n  geospatially-distributed variables in a Bayesian mortality model for Hong\n  Kong", "comments": "26 pages (15 main, 11 appendix), 22 figures (6 main, 11 appendix), 2\n  tables", "journal-ref": null, "doi": "10.1371/journal.pone.0247795", "report-no": null, "categories": "physics.soc-ph cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human mortality is in part a function of multiple socioeconomic factors that\ndiffer both spatially and temporally. Adjusting for other covariates, the human\nlifespan is positively associated with household wealth. However, the extent to\nwhich mortality in a geographical region is a function of socioeconomic factors\nin both that region and its neighbors is unclear. There is also little\ninformation on the temporal components of this relationship. Using the\ndistricts of Hong Kong over multiple census years as a case study, we\ndemonstrate that there are differences in how wealth indicator variables are\nassociated with longevity in (a) areas that are affluent but neighbored by\nsocially deprived districts versus (b) wealthy areas surrounded by similarly\nwealthy districts. We also show that the inclusion of spatially-distributed\nvariables reduces uncertainty in mortality rate predictions in each census year\nwhen compared with a baseline model. Our results suggest that geographic\nmortality models should incorporate nonlocal information (e.g., spatial\nneighbors) to lower the variance of their mortality estimates, and point to a\nmore in-depth analysis of sociospatial spillover effects on mortality rates.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 16:35:23 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 13:55:13 GMT"}, {"version": "v3", "created": "Mon, 26 Oct 2020 16:23:47 GMT"}, {"version": "v4", "created": "Mon, 25 Jan 2021 22:43:59 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Alshaabi", "Thayer", ""], ["Dewhurst", "David Rushing", ""], ["Bagrow", "James P.", ""], ["Dodds", "Peter Sheridan", ""], ["Danforth", "Christopher M.", ""]]}, {"id": "2006.08553", "submitter": "Chi Zhang", "authors": "Chi Zhang, Jennifer Ahern, Mark J. van der Laan, Oleg Sofrygin", "title": "tmleCommunity: A R Package Implementing Target Maximum Likelihood\n  Estimation for Community-level Data", "comments": "42 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the past years, many applications aim to assess the causal effect of\ntreatments assigned at the community level, while data are still collected at\nthe individual level among individuals of the community. In many cases, one\nwants to evaluate the effect of a stochastic intervention on the community,\nwhere all communities in the target population receive probabilistically\nassigned treatments based on a known specified mechanism (e.g., implementing a\ncommunity-level intervention policy that target stochastic changes in the\nbehavior of a target population of communities). The tmleCommunity package is\nrecently developed to implement targeted minimum loss-based estimation (TMLE)\nof the effect of community-level intervention(s) at a single time point on an\nindividual-based outcome of interest, including the average causal effect.\nImplementations of the inverse-probability-of-treatment-weighting (IPTW) and\nthe G-computation formula (GCOMP) are also available. The package supports\nmultivariate arbitrary (i.e., static, dynamic or stochastic) interventions with\na binary or continuous outcome. Besides, it allows user-specified data-adaptive\nmachine learning algorithms through SuperLearner, sl3 and h2oEnsemble packages.\nThe usage of the tmleCommunity package, along with a few examples, will be\ndescribed in this paper.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 17:16:40 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Zhang", "Chi", ""], ["Ahern", "Jennifer", ""], ["van der Laan", "Mark J.", ""], ["Sofrygin", "Oleg", ""]]}, {"id": "2006.08675", "submitter": "Chi Zhang", "authors": "Chi Zhang, Jennifer Ahern, Mark J. van der Laan", "title": "Targeted Maximum Likelihood Estimation of Community-based Causal Effect\n  of Community-Level Stochastic Interventions", "comments": "20 pages. arXiv admin note: substantial text overlap with\n  arXiv:2006.08553", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unlike the commonly used parametric regression models such as mixed models,\nthat can easily violate the required statistical assumptions and result in\ninvalid statistical inference, target maximum likelihood estimation allows more\nrealistic data-generative models and provides double-robust, semi-parametric\nand efficient estimators. Target maximum likelihood estimators (TMLEs) for the\ncausal effect of a community-level static exposure were previously proposed by\nBalzer et al. In this manuscript, we build on this work and present\nidentifiability results and develop two semi-parametric efficient TMLEs for the\nestimation of the causal effect of the single time-point community-level\nstochastic intervention whose assignment mechanism can depend on measured and\nunmeasured environmental factors and its individual-level covariates. The first\ncommunity-level TMLE is developed under a general hierarchical non-parametric\nstructural equation model, which can incorporate pooled individual-level\nregressions for estimating the outcome mechanism. The second individual-level\nTMLE is developed under a restricted hierarchical model in which the additional\nassumption of no covariate interference within communities holds. The proposed\nTMLEs have several crucial advantages. First, both TMLEs can make use of\nindividual level data in the hierarchical setting, and potentially reduce\nfinite sample bias and improve estimator efficiency. Second, the stochastic\nintervention framework provides a natural way for defining and estimating\ncasual effects where the exposure variables are continuous or discrete with\nmultiple levels, or even cannot be directly intervened on. Also, the positivity\nassumption needed for our proposed causal parameters can be weaker than the\nversion of positivity required for other casual parameters.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 18:22:38 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Zhang", "Chi", ""], ["Ahern", "Jennifer", ""], ["van der Laan", "Mark J.", ""]]}, {"id": "2006.08688", "submitter": "Ke Yang", "authors": "Ke Yang, Joshua R. Loftus, Julia Stoyanovich", "title": "Causal intersectionality for fair ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a causal modeling approach to intersectional\nfairness, and a flexible, task-specific method for computing intersectionally\nfair rankings. Rankings are used in many contexts, ranging from Web search\nresults to college admissions, but causal inference for fair rankings has\nreceived limited attention. Additionally, the growing literature on causal\nfairness has directed little attention to intersectionality. By bringing these\nissues together in a formal causal framework we make the application of\nintersectionality in fair machine learning explicit, connected to important\nreal world effects and domain knowledge, and transparent about technical\nlimitations. We experimentally evaluate our approach on real and synthetic\ndatasets, exploring its behaviour under different structural assumptions.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 18:57:46 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Yang", "Ke", ""], ["Loftus", "Joshua R.", ""], ["Stoyanovich", "Julia", ""]]}, {"id": "2006.08701", "submitter": "Jake Rhodes", "authors": "Jake S. Rhodes, Adele Cutler, Guy Wolf, Kevin R. Moon", "title": "Supervised Visualization for Data Exploration", "comments": "21 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.HC cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimensionality reduction is often used as an initial step in data\nexploration, either as preprocessing for classification or regression or for\nvisualization. Most dimensionality reduction techniques to date are\nunsupervised; they do not take class labels into account (e.g., PCA, MDS,\nt-SNE, Isomap). Such methods require large amounts of data and are often\nsensitive to noise that may obfuscate important patterns in the data. Various\nattempts at supervised dimensionality reduction methods that take into account\nauxiliary annotations (e.g., class labels) have been successfully implemented\nwith goals of increased classification accuracy or improved data visualization.\nMany of these supervised techniques incorporate labels in the loss function in\nthe form of similarity or dissimilarity matrices, thereby creating\nover-emphasized separation between class clusters, which does not realistically\nrepresent the local and global relationships in the data. In addition, these\napproaches are often sensitive to parameter tuning, which may be difficult to\nconfigure without an explicit quantitative notion of visual superiority. In\nthis paper, we describe a novel supervised visualization technique based on\nrandom forest proximities and diffusion-based dimensionality reduction. We\nshow, both qualitatively and quantitatively, the advantages of our approach in\nretaining local and global structures in data, while emphasizing important\nvariables in the low-dimensional embedding. Importantly, our approach is robust\nto noise and parameter tuning, thus making it simple to use while producing\nreliable visualizations for data exploration.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 19:10:17 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Rhodes", "Jake S.", ""], ["Cutler", "Adele", ""], ["Wolf", "Guy", ""], ["Moon", "Kevin R.", ""]]}, {"id": "2006.08720", "submitter": "Whitney Huang", "authors": "Whitney K. Huang, Adam H. Monahan, Francis W. Zwiers", "title": "Estimating Concurrent Climate Extremes: A Conditional Approach", "comments": "39 pages, 18 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simultaneous concurrence of extreme values across multiple climate variables\ncan result in large societal and environmental impacts. Therefore, there is\ngrowing interest in understanding these concurrent extremes. In many\napplications, not only the frequency but also the magnitude of concurrent\nextremes are of interest. One way to approach this problem is to study the\ndistribution of one climate variable given that another is extreme. In this\nwork we develop a statistical framework for estimating bivariate concurrent\nextremes via a conditional approach, where univariate extreme value modeling is\ncombined with dependence modeling of the conditional tail distribution using\ntechniques from quantile regression and extreme value analysis to quantify\nconcurrent extremes. We focus on the distribution of daily wind speed\nconditioned on daily precipitation taking its seasonal maximum. The Canadian\nRegional Climate Model large ensemble is used to assess the performance of the\nproposed framework both via a simulation study with specified dependence\nstructure and via an analysis of the climate model-simulated dependence\nstructure.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 19:37:47 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 16:38:15 GMT"}, {"version": "v3", "created": "Fri, 12 Mar 2021 19:14:30 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Huang", "Whitney K.", ""], ["Monahan", "Adam H.", ""], ["Zwiers", "Francis W.", ""]]}, {"id": "2006.08745", "submitter": "Andrew Gelman", "authors": "Jon Zelner, Julien Riou, Ruth Etzioni, and Andrew Gelman", "title": "Accounting for Uncertainty During a Pandemic", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph q-bio.PE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss several issues of statistical design, data collection, analysis,\ncommunication, and decision making that have arisen in recent and ongoing\ncoronavirus studies, focusing on tools for assessment and propagation of\nuncertainty. This paper does not purport to be a comprehensive survey of the\nresearch literature; rather, we use examples to illustrate statistical points\nthat we think are important.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 20:36:42 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Zelner", "Jon", ""], ["Riou", "Julien", ""], ["Etzioni", "Ruth", ""], ["Gelman", "Andrew", ""]]}, {"id": "2006.08804", "submitter": "Mingyuan Zhou", "authors": "Hao Zhang, Bo Chen, Yulai Cong, Dandan Guo, Hongwei Liu, Mingyuan Zhou", "title": "Deep Autoencoding Topic Model with Scalable Hybrid Bayesian Inference", "comments": "To appear in IEEE Transactions on Pattern Analysis and Machine\n  Intelligence. arXiv admin note: text overlap with arXiv:1803.01328", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To build a flexible and interpretable model for document analysis, we develop\ndeep autoencoding topic model (DATM) that uses a hierarchy of gamma\ndistributions to construct its multi-stochastic-layer generative network. In\norder to provide scalable posterior inference for the parameters of the\ngenerative network, we develop topic-layer-adaptive stochastic gradient\nRiemannian MCMC that jointly learns simplex-constrained global parameters\nacross all layers and topics, with topic and layer specific learning rates.\nGiven a posterior sample of the global parameters, in order to efficiently\ninfer the local latent representations of a document under DATM across all\nstochastic layers, we propose a Weibull upward-downward variational encoder\nthat deterministically propagates information upward via a deep neural network,\nfollowed by a Weibull distribution based stochastic downward generative model.\nTo jointly model documents and their associated labels, we further propose\nsupervised DATM that enhances the discriminative power of its latent\nrepresentations. The efficacy and scalability of our models are demonstrated on\nboth unsupervised and supervised learning tasks on big corpora.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 22:22:56 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Zhang", "Hao", ""], ["Chen", "Bo", ""], ["Cong", "Yulai", ""], ["Guo", "Dandan", ""], ["Liu", "Hongwei", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "2006.08864", "submitter": "Hangjin Jiang", "authors": "Hangjin Jiang", "title": "A Goodness-of-Fit Test for Statistical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical modeling plays a fundamental role in understanding the underlying\nmechanism of massive data (statistical inference) and predicting the future\n(statistical prediction). Although all models are wrong, researchers try their\nbest to make some of them be useful. The question here is how can we measure\nthe usefulness of a statistical model for the data in hand? This is key to\nstatistical prediction. The important statistical problem of testing whether\nthe observations follow the proposed statistical model has only attracted\nrelatively few attentions. In this paper, we proposed a new framework for this\nproblem through building its connection with two-sample distribution\ncomparison. The proposed method can be applied to evaluate a wide range of\nmodels. Examples are given to show the performance of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 01:58:21 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Jiang", "Hangjin", ""]]}, {"id": "2006.08918", "submitter": "Alvin Chua", "authors": "Alvin J. K. Chua, Michele Vallisneri", "title": "On parametric tests of relativity with false degrees of freedom", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "gr-qc astro-ph.IM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  General relativity can be tested by comparing the binary-inspiral signals\nfound in LIGO--Virgo data against waveform models that are augmented with\nartificial degrees of freedom. This approach suffers from a number of logical\nand practical pitfalls. 1) It is difficult to ascribe meaning to the stringency\nof the resultant constraints. 2) It is doubtful that the Bayesian model\ncomparison of relativity against these artificial models can offer actual\nvalidation for the former. 3) It is unknown to what extent these tests might\ndetect alternative theories of gravity for which there are no computed\nwaveforms; conversely, when waveforms are available, tests that employ them\nwill be superior.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 04:29:32 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Chua", "Alvin J. K.", ""], ["Vallisneri", "Michele", ""]]}, {"id": "2006.08988", "submitter": "Chiara Forlani", "authors": "Chiara Forlani, Samir Bhatt, Michela Cameletti, Elias Krainski, Marta\n  Blangiardo", "title": "A joint bayesian space-time model to integrate spatially misaligned air\n  pollution data in R-INLA", "comments": "This paper has been submitted to Environmetrics and is under revision", "journal-ref": "Environmetrics (2020) 1-17; e2644", "doi": "10.1002/env.2644", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In air pollution studies, dispersion models provide estimates of\nconcentration at grid level covering the entire spatial domain, and are then\ncalibrated against measurements from monitoring stations. However, these\ndifferent data sources are misaligned in space and time. If misalignment is not\nconsidered, it can bias the predictions. We aim at demonstrating how the\ncombination of multiple data sources, such as dispersion model outputs, ground\nobservations and covariates, leads to more accurate predictions of air\npollution at grid level. We consider nitrogen dioxide (NO2) concentration in\nGreater London and surroundings for the years 2007-2011, and combine two\ndifferent dispersion models. Different sets of spatial and temporal effects are\nincluded in order to obtain the best predictive capability. Our proposed model\nis framed in between calibration and Bayesian melding techniques for data\nfusion red. Unlike other examples, we jointly model the response (concentration\nlevel at monitoring stations) and the dispersion model outputs on different\nscales, accounting for the different sources of uncertainty. Our\nspatio-temporal model allows us to reconstruct the latent fields of each model\ncomponent, and to predict daily pollution concentrations. We compare the\npredictive capability of our proposed model with other established methods to\naccount for misalignment (e.g. bilinear interpolation), showing that in our\ncase study the joint model is a better alternative.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 08:33:51 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Forlani", "Chiara", ""], ["Bhatt", "Samir", ""], ["Cameletti", "Michela", ""], ["Krainski", "Elias", ""], ["Blangiardo", "Marta", ""]]}, {"id": "2006.09003", "submitter": "Paul Freulon", "authors": "Paul Freulon, J\\'er\\'emie Bigot and Boris P. Hejblum", "title": "CytOpT: Optimal Transport with Domain Adaptation for Interpreting Flow\n  Cytometry data", "comments": "21 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The automated analysis of flow cytometry measurements is an active research\nfield. We introduce a new algorithm, referred to as CytOpT, using regularized\noptimal transport to directly estimate the different cell population\nproportions from a biological sample characterized with flow cytometry\nmeasurements. We rely on the regularized Wasserstein metric to compare\ncytometry measurements from different samples, thus accounting for possible\nmis-alignment of a given cell population across sample (due to technical\nvariability from the technology of measurements). In this work, we rely on a\nsupervised learning technique based on the Wasserstein metric that is used to\nestimate an optimal re-weighting of class proportions in a mixture model from a\nsource distribution (with known segmentation into cell sub-populations) to fit\na target distribution with unknown segmentation. Due to the high-dimensionality\nof flow cytometry data, we use stochastic algorithms to approximate the\nregularized Wasserstein metric to solve the optimization problem involved in\nthe estimation of optimal weights representing the cell population proportions\nin the target distribution. Several flow cytometry data sets are used to\nillustrate the performances of CytOpT that are also compared to those of\nexisting algorithms for automatic gating based on supervised learning.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 09:01:58 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 16:53:17 GMT"}, {"version": "v3", "created": "Thu, 18 Jun 2020 17:25:28 GMT"}, {"version": "v4", "created": "Tue, 2 Feb 2021 17:49:34 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Freulon", "Paul", ""], ["Bigot", "J\u00e9r\u00e9mie", ""], ["Hejblum", "Boris P.", ""]]}, {"id": "2006.09012", "submitter": "Andrea Cappozzo", "authors": "Francesco Denti, Andrea Cappozzo and Francesca Greselin", "title": "A Two-Stage Bayesian Semiparametric Model for Novelty Detection with\n  Robust Prior Information", "comments": null, "journal-ref": null, "doi": "10.1007/s11222-021-10017-7", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novelty detection methods aim at partitioning the test units into already\nobserved and previously unseen patterns. However, two significant issues arise:\nthere may be considerable interest in identifying specific structures within\nthe novelty, and contamination in the known classes could completely blur the\nactual separation between manifest and new groups. Motivated by these problems,\nwe propose a two-stage Bayesian semiparametric novelty detector, building upon\nprior information robustly extracted from a set of complete learning units. We\ndevise a general-purpose multivariate methodology that we also extend to handle\nfunctional data objects. We provide insights on the model behavior by\ninvestigating the theoretical properties of the associated semiparametric\nprior. From the computational point of view, we propose a suitable\n$\\boldsymbol{\\xi}$-sequence to construct an independent slice-efficient sampler\nthat takes into account the difference between manifest and novelty components.\nWe showcase our model performance through an extensive simulation study and\napplications on both multivariate and functional datasets, in which diverse and\ndistinctive unknown patterns are discovered.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 09:20:55 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 11:19:18 GMT"}, {"version": "v3", "created": "Thu, 17 Jun 2021 08:24:48 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Denti", "Francesco", ""], ["Cappozzo", "Andrea", ""], ["Greselin", "Francesca", ""]]}, {"id": "2006.09015", "submitter": "Isabella Deutsch", "authors": "Isabella Deutsch and Gordon J. Ross", "title": "ABC Learning of Hawkes Processes with Missing or Noisy Event Times", "comments": "Added comparison to literature", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The self-exciting Hawkes process is widely used to model events which occur\nin bursts. However, many real world data sets contain missing events and/or\nnoisily observed event times, which we refer to as data distortion. The\npresence of such distortion can severely bias the learning of the Hawkes\nprocess parameters. To circumvent this, we propose modeling the distortion\nfunction explicitly. This leads to a model with an intractable likelihood\nfunction which makes it difficult to deploy standard parameter estimation\ntechniques. As such, we develop the ABC-Hawkes algorithm which is a novel\napproach to estimation based on Approximate Bayesian Computation (ABC) and\nMarkov Chain Monte Carlo. This allows the parameters of the Hawkes process to\nbe learned in settings where conventional methods induce substantial bias or\nare inapplicable. The proposed approach is shown to perform well on both real\nand simulated data.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 09:28:05 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 19:37:20 GMT"}, {"version": "v3", "created": "Wed, 2 Jun 2021 09:16:36 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Deutsch", "Isabella", ""], ["Ross", "Gordon J.", ""]]}, {"id": "2006.09016", "submitter": "Dorien Herremans", "authors": "Balamurali B T, Edwin Jonathan Aslim, Yun Shu Lynn Ng, Tricia Li,\n  Chuen Kuo, Jacob Shihang Chen, Dorien Herremans, Lay Guat Ng, Jer-Ming Chen", "title": "Acoustic prediction of flowrate: varying liquid jet stream onto a free\n  surface", "comments": null, "journal-ref": "Proceedings of the IEEE International Conference on Signal\n  Processing and Communications (SPCOM), 2020", "doi": null, "report-no": null, "categories": "physics.comp-ph cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information on liquid jet stream flow is crucial in many real world\napplications. In a large number of cases, these flows fall directly onto free\nsurfaces (e.g. pools), creating a splash with accompanying splashing sounds.\nThe sound produced is supplied by energy interactions between the liquid jet\nstream and the passive free surface. In this investigation, we collect the\nsound of a water jet of varying flowrate falling into a pool of water, and use\nthis sound to predict the flowrate and flowrate trajectory involved. Two\napproaches are employed: one uses machine-learning models trained using audio\nfeatures extracted from the collected sound to predict the flowrate (and\nsubsequently the flowrate trajectory). In contrast, the second method directly\nuses acoustic parameters related to the spectral energy of the liquid-liquid\ninteraction to estimate the flowrate trajectory. The actual flowrate, however,\nis determined directly using a gravimetric method: tracking the change in mass\nof the pooling liquid over time. We show here that the two methods agree well\nwith the actual flowrate and offer comparable performance in accurately\npredicting the flowrate trajectory, and accordingly offer insights for\npotential real-life applications using sound.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 09:29:09 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["T", "Balamurali B", ""], ["Aslim", "Edwin Jonathan", ""], ["Ng", "Yun Shu Lynn", ""], ["Li", "Tricia", ""], ["Kuo", "Chuen", ""], ["Chen", "Jacob Shihang", ""], ["Herremans", "Dorien", ""], ["Ng", "Lay Guat", ""], ["Chen", "Jer-Ming", ""]]}, {"id": "2006.09022", "submitter": "Shrey Dabhi", "authors": "Shrey Dabhi and Manojkumar Parmar", "title": "NodeNet: A Graph Regularised Neural Network for Node Classification", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world events exhibit a high degree of interdependence and connections,\nand hence data points generated also inherit the linkages. However, the\nmajority of AI/ML techniques leave out the linkages among data points. The\nrecent surge of interest in graph-based AI/ML techniques is aimed to leverage\nthe linkages. Graph-based learning algorithms utilize the data and related\ninformation effectively to build superior models. Neural Graph Learning (NGL)\nis one such technique that utilizes a traditional machine learning algorithm\nwith a modified loss function to leverage the edges in the graph structure. In\nthis paper, we propose a model using NGL - NodeNet, to solve node\nclassification task for citation graphs. We discuss our modifications and their\nrelevance to the task. We further compare our results with the current state of\nthe art and investigate reasons for the superior performance of NodeNet.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 09:41:58 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Dabhi", "Shrey", ""], ["Parmar", "Manojkumar", ""]]}, {"id": "2006.09271", "submitter": "Edward Raff", "authors": "Edward Raff, Charles Nicholas", "title": "A Survey of Machine Learning Methods and Challenges for Windows Malware\n  Classification", "comments": "To appear in NeurIPS 2020 Workshop: ML Retrospectives, Surveys &\n  Meta-Analyses (ML-RSA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Malware classification is a difficult problem, to which machine learning\nmethods have been applied for decades. Yet progress has often been slow, in\npart due to a number of unique difficulties with the task that occur through\nall stages of the developing a machine learning system: data collection,\nlabeling, feature creation and selection, model selection, and evaluation. In\nthis survey we will review a number of the current methods and challenges\nrelated to malware classification, including data collection, feature\nextraction, and model construction, and evaluation. Our discussion will include\nthoughts on the constraints that must be considered for machine learning based\nsolutions in this domain, and yet to be tackled problems for which machine\nlearning could also provide a solution. This survey aims to be useful both to\ncybersecurity practitioners who wish to learn more about how machine learning\ncan be applied to the malware problem, and to give data scientists the\nnecessary background into the challenges in this uniquely complicated space.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 17:46:12 GMT"}, {"version": "v2", "created": "Sun, 15 Nov 2020 16:35:36 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Raff", "Edward", ""], ["Nicholas", "Charles", ""]]}, {"id": "2006.09278", "submitter": "Aristidis K. Nikoloulopoulos", "authors": "Aristidis K. Nikoloulopoulos", "title": "An one-factor copula mixed model for joint meta-analysis of multiple\n  diagnostic tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the meta-analysis of more than one diagnostic tests can impact clinical\ndecision making and patient health, there is an increasing body of research in\nmodels and methods for meta-analysis of studies comparing multiple diagnostic\ntests. The application of the existing models to compare the accuracy of three\nor more tests suffers from the curse of multi-dimensionality, i.e., either the\nnumber of model parameters increase rapidly or high dimensional integration is\nrequired. To overcome these issues in joint meta-analysis of studies comparing\n$T >2$ diagnostic tests in a multiple tests design with a gold standard, we\npropose a model that assumes the true positives and true negatives for each\ntest are conditionally independent and binomially distributed given the\n$2T$-variate latent vector of sensitivities and specificities. For the random\neffects distribution, we employ an one-factor copula that provides tail\ndependence or tail asymmetry. Maximum likelihood estimation of the model is\nstraightforward as the derivation of the likelihood requires bi-dimensional\ninstead of $2T$-dimensional integration. Our methodology is demonstrated with\nan extensive simulation study and an application example that determines which\nis the best test for the diagnosis of rheumatoid arthritis.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 16:17:08 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 07:41:06 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Nikoloulopoulos", "Aristidis K.", ""]]}, {"id": "2006.09329", "submitter": "Philip White", "authors": "Philip White, Durban Keeler, Daniel Sheanshang, Summer Rupper", "title": "Improving Piecewise Linear Snow Density Models through Hierarchical\n  Spatial and Orthogonal Functional Smoothing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Snow density estimates as a function of depth are used for understanding\nclimate processes, evaluating water accumulation trends in polar regions, and\nestimating glacier mass balances. The common and interpretable\nphysically-derived differential equation models for snow density are piecewise\nlinear as a function of depth (on a transformed scale); thus, they can fail to\ncapture important data features. Moreover, the differential equation parameters\nshow strong spatial autocorrelation. To address these issues, we allow the\nparameters of the physical model, including random change points over depth, to\nvary spatially. We also develop a framework for functionally smoothing the\nphysically-motivated model. To preserve inference on the interpretable physical\nmodel, we project the smoothing function into the physical model's spatially\nvarying null space. The proposed spatially and functionally smoothed snow\ndensity model better fits the data while preserving inference on physical\nparameters. Using this model, we find significant spatial variation in the\nparameters that govern snow densification.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 17:11:03 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 20:28:47 GMT"}, {"version": "v3", "created": "Mon, 21 Jun 2021 17:11:31 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["White", "Philip", ""], ["Keeler", "Durban", ""], ["Sheanshang", "Daniel", ""], ["Rupper", "Summer", ""]]}, {"id": "2006.09474", "submitter": "Amarin Siripanich", "authors": "Amarin Siripanich and Taha Rashidi", "title": "A demographic microsimulation model with an integrated household\n  alignment method", "comments": "This paper has been submitted to a journal to be reviewed and\n  considered for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN physics.soc-ph q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many dynamic microsimulation models have shown their ability to reasonably\nproject detailed population and households using non-data based household\nformation and dissolution rules. Although, those rules allow modellers to\nsimplify changes in the household construction, they typically fall short in\nreplicating household projections or if applied retrospectively the observed\nhousehold numbers. Consequently, such models with biased estimation for\nhousehold size and other household related attributes lose their usefulness in\napplications that are sensitive to household size, such as in travel demand and\nhousing demand modelling. Nonetheless, these demographic microsimulation models\nwith their associated shortcomings have been commonly used to assess various\nplanning policies which can result in misleading judgements. In this paper, we\ncontribute to the literature of population microsimulation by introducing a\nfully integrated system of models for different life event where a household\nalignment method adjusts household size distribution to closely align with any\ngiven target distribution. Furthermore, some demographic events that are\ngenerally difficult to model, such as incorporating immigrant families into a\npopulation, can be included. We illustrated an example of the household\nalignment method and put it to test in a dynamic microsimulation model that we\ndeveloped using dymiumCore, a general-purpose microsimulation toolkit in R, to\nshow potential improvements and weaknesses of the method. The implementation of\nthis model has been made publicly available on GitHub.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 06:15:58 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Siripanich", "Amarin", ""], ["Rashidi", "Taha", ""]]}, {"id": "2006.09666", "submitter": "Xuan Yin", "authors": "Zenan Wang, Xuan Yin, Tianbo Li, Liangjie Hong", "title": "Causal Meta-Mediation Analysis: Inferring Dose-Response Function From\n  Summary Statistics of Many Randomized Experiments", "comments": "In Proceedings of the 26th ACM SIGKDD Conference on Knowledge\n  Discovery and Data Mining (KDD '20), August 23-27, 2020, Virtual Event, CA,\n  USA. ACM, New York, NY, USA, 11 pages", "journal-ref": "Proceedings of the 26th ACM SIGKDD Conference on Knowledge\n  Discovery and Data Mining (KDD '20), August 23-27, 2020, Virtual Event, CA,\n  USA. ACM, New York, NY, USA, 11 pages", "doi": "10.1145/3394486.3403313", "report-no": null, "categories": "stat.AP math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common in the internet industry to use offline-developed algorithms to\npower online products that contribute to the success of a business.\nOffline-developed algorithms are guided by offline evaluation metrics, which\nare often different from online business key performance indicators (KPIs). To\nmaximize business KPIs, it is important to pick a north star among all\navailable offline evaluation metrics. By noting that online products can be\nmeasured by online evaluation metrics, the online counterparts of offline\nevaluation metrics, we decompose the problem into two parts. As the offline A/B\ntest literature works out the first part: counterfactual estimators of offline\nevaluation metrics that move the same way as their online counterparts, we\nfocus on the second part: causal effects of online evaluation metrics on\nbusiness KPIs. The north star of offline evaluation metrics should be the one\nwhose online counterpart causes the most significant lift in the business KPI.\nWe model the online evaluation metric as a mediator and formalize its causality\nwith the business KPI as dose-response function (DRF). Our novel approach,\ncausal meta-mediation analysis, leverages summary statistics of many existing\nrandomized experiments to identify, estimate, and test the mediator DRF. It is\neasy to implement and to scale up, and has many advantages over the literature\nof mediation analysis and meta-analysis. We demonstrate its effectiveness by\nsimulation and implementation on real data.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 05:41:19 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Wang", "Zenan", ""], ["Yin", "Xuan", ""], ["Li", "Tianbo", ""], ["Hong", "Liangjie", ""]]}, {"id": "2006.09706", "submitter": "Manh Toan Ho Mr.", "authors": "Quan-Hoang Vuong, Manh-Toan Ho, Minh-Hoang Nguyen, Thanh-Hang Pham,\n  Hoang-Anh Ho, Thu-Trang Vuong, and Viet-Phuong La", "title": "On the environment-destructive probabilistic trends: a perceptual and\n  behavioral study on video game players", "comments": "6 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": "AISDL-2006-A", "categories": "cs.CY stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Currently, gaming is the world's favorite form of entertainment. Various\nstudies have shown how games impact players' perceptions and behaviors,\nprompting opportunities for purposes beyond entertainment. This study uses\nAnimal Crossing: New Horizons (ACNH), a real-time life-simulation game, as a\nunique case study of how video games can affect humans' environmental\nperceptions. A dataset of 584 observations from a survey of ACNH players and\nthe Hamiltonian MCMC technique has enabled us to explore the relationship\nbetween in-game behaviors and perceptions. The findings indicate a\nprobabilistic trend towards exploiting the in-game environment despite players'\nperceptions, suggesting that the simplification of commercial game design may\noverlook opportunities to engage players in pro-environmental activities.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 08:05:40 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Vuong", "Quan-Hoang", ""], ["Ho", "Manh-Toan", ""], ["Nguyen", "Minh-Hoang", ""], ["Pham", "Thanh-Hang", ""], ["Ho", "Hoang-Anh", ""], ["Vuong", "Thu-Trang", ""], ["La", "Viet-Phuong", ""]]}, {"id": "2006.09983", "submitter": "Trevor Hefley", "authors": "Narmadha M. Mohankumar, Trevor J. Hefley", "title": "Using machine learning to identify nontraditional spatial dependence in\n  occupancy data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial models for occupancy data are used to estimate and map the true\npresence of a species, which may depend on biotic and abiotic factors as well\nas spatial autocorrelation. Traditionally researchers have accounted for\nspatial autocorrelation in occupancy data by using a correlated normally\ndistributed site-level random effect, which might be incapable of identifying\nnontraditional spatial dependence such as discontinuities and abrupt\ntransitions. Machine learning approaches have the potential to identify and\nmodel nontraditional spatial dependence, but these approaches do not account\nfor observer errors such as false absences. By combining the flexibility of\nBayesian hierarchal modeling and machine learning approaches, we present a\ngeneral framework to model occupancy data that accounts for both traditional\nand nontraditional spatial dependence as well as false absences. We demonstrate\nour framework using six synthetic occupancy data sets and two real data sets.\nOur results demonstrate how to identify and model both traditional and\nnontraditional spatial dependence in occupancy data which enables a broader\nclass of spatial occupancy models that can be used to improve predictive\naccuracy and model adequacy.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 16:42:55 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 16:21:00 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Mohankumar", "Narmadha M.", ""], ["Hefley", "Trevor J.", ""]]}, {"id": "2006.10148", "submitter": "Christopher Kenny", "authors": "Benjamin Fifield, Kosuke Imai, Jun Kawahara, Christopher T. Kenny", "title": "The Essential Role of Empirical Validation in Legislative Redistricting\n  Simulation", "comments": "32 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As granular data about elections and voters become available, redistricting\nsimulation methods are playing an increasingly important role when legislatures\nadopt redistricting plans and courts determine their legality. These simulation\nmethods are designed to yield a representative sample of all redistricting\nplans that satisfy statutory guidelines and requirements such as contiguity,\npopulation parity, and compactness. A proposed redistricting plan can be\nconsidered gerrymandered if it constitutes an outlier relative to this sample\naccording to partisan fairness metrics. Despite their growing use, an\ninsufficient effort has been made to empirically validate the accuracy of the\nsimulation methods. We apply a recently developed computational method that can\nefficiently enumerate all possible redistricting plans and yield an independent\nuniform sample from this population. We show that this algorithm scales to a\nstate with a couple of hundred geographical units. Finally, we empirically\nexamine how existing simulation methods perform on realistic validation data\nsets.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 20:51:43 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Fifield", "Benjamin", ""], ["Imai", "Kosuke", ""], ["Kawahara", "Jun", ""], ["Kenny", "Christopher T.", ""]]}, {"id": "2006.10241", "submitter": "Aritra Guha", "authors": "Aritra Guha, Rayleigh Lei, Jiacheng Zhu, XuanLong Nguyen and Ding Zhao", "title": "Robust Unsupervised Learning of Temporal Dynamic Interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA cs.RO stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust representation learning of temporal dynamic interactions is an\nimportant problem in robotic learning in general and automated unsupervised\nlearning in particular. Temporal dynamic interactions can be described by\n(multiple) geometric trajectories in a suitable space over which unsupervised\nlearning techniques may be applied to extract useful features from raw and\nhigh-dimensional data measurements. Taking a geometric approach to robust\nrepresentation learning for temporal dynamic interactions, it is necessary to\ndevelop suitable metrics and a systematic methodology for comparison and for\nassessing the stability of an unsupervised learning method with respect to its\ntuning parameters. Such metrics must account for the (geometric) constraints in\nthe physical world as well as the uncertainty associated with the learned\npatterns. In this paper we introduce a model-free metric based on the\nProcrustes distance for robust representation learning of interactions, and an\noptimal transport based distance metric for comparing between distributions of\ninteraction primitives. These distance metrics can serve as an objective for\nassessing the stability of an interaction learning algorithm. They are also\nused for comparing the outcomes produced by different algorithms. Moreover,\nthey may also be adopted as an objective function to obtain clusters and\nrepresentative interaction primitives. These concepts and techniques will be\nintroduced, along with mathematical properties, while their usefulness will be\ndemonstrated in unsupervised learning of vehicle-to-vechicle interactions\nextracted from the Safety Pilot database, the world's largest database for\nconnected vehicles.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 02:39:45 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Guha", "Aritra", ""], ["Lei", "Rayleigh", ""], ["Zhu", "Jiacheng", ""], ["Nguyen", "XuanLong", ""], ["Zhao", "Ding", ""]]}, {"id": "2006.10245", "submitter": "David Frazier", "authors": "Veronika Czellar, David T. Frazier and Eric Renault", "title": "Approximate Maximum Likelihood for Complex Structural Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indirect Inference (I-I) is a popular technique for estimating complex\nparametric models whose likelihood function is intractable, however, the\nstatistical efficiency of I-I estimation is questionable. While the efficient\nmethod of moments, Gallant and Tauchen (1996), promises efficiency, the price\nto pay for this efficiency is a loss of parsimony and thereby a potential lack\nof robustness to model misspecification. This stands in contrast to simpler I-I\nestimation strategies, which are known to display less sensitivity to model\nmisspecification precisely due to their focus on specific elements of the\nunderlying structural model. In this research, we propose a new\nsimulation-based approach that maintains the parsimony of I-I estimation, which\nis often critical in empirical applications, but can also deliver estimators\nthat are nearly as efficient as maximum likelihood. This new approach is based\non using a constrained approximation to the structural model, which ensures\nidentification and can deliver estimators that are nearly efficient. We\ndemonstrate this approach through several examples, and show that this approach\ncan deliver estimators that are nearly as efficient as maximum likelihood, when\nfeasible, but can be employed in many situations where maximum likelihood is\ninfeasible.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 02:53:53 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Czellar", "Veronika", ""], ["Frazier", "David T.", ""], ["Renault", "Eric", ""]]}, {"id": "2006.10266", "submitter": "Taylor Okonek", "authors": "Jon Wakefield, Taylor Okonek, Jon Pedersen", "title": "Small Area Estimation of Health Outcomes", "comments": "53 pages, 27 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Small area estimation (SAE) entails estimating characteristics of interest\nfor domains, often geographical areas, in which there may be few or no samples\navailable. SAE has a long history and a wide variety of methods have been\nsuggested, from a bewildering range of philosophical standpoints. We describe\ndesign-based and model-based approaches and models that are specified at the\narea-level and at the unit-level, focusing on health applications and fully\nBayesian spatial models. The use of auxiliary information is a key ingredient\nfor successful inference when response data are sparse and we discuss a number\nof approaches that allow the inclusion of covariate data. SAE for HIV\nprevalence, using data collected from a Demographic Health Survey in Malawi in\n2015-2016, is used to illustrate a number of techniques. The potential use of\nSAE techniques for outcomes related to COVID-19 is discussed.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 04:04:22 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Wakefield", "Jon", ""], ["Okonek", "Taylor", ""], ["Pedersen", "Jon", ""]]}, {"id": "2006.10555", "submitter": "Youngki Shin", "authors": "Sokbae Lee, Yuan Liao, Myung Hwan Seo, Youngki Shin", "title": "Sparse HP Filter: Finding Kinks in the COVID-19 Contact Rate", "comments": "42 pages, 15 figures, 1 table", "journal-ref": null, "doi": "10.1016/j.jeconom.2020.08.008", "report-no": null, "categories": "econ.EM stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we estimate the time-varying COVID-19 contact rate of a\nSusceptible-Infected-Recovered (SIR) model. Our measurement of the contact rate\nis constructed using data on actively infected, recovered and deceased cases.\nWe propose a new trend filtering method that is a variant of the\nHodrick-Prescott (HP) filter, constrained by the number of possible kinks. We\nterm it the $\\textit{sparse HP filter}$ and apply it to daily data from five\ncountries: Canada, China, South Korea, the UK and the US. Our new method yields\nthe kinks that are well aligned with actual events in each country. We find\nthat the sparse HP filter provides a fewer kinks than the $\\ell_1$ trend\nfilter, while both methods fitting data equally well. Theoretically, we\nestablish risk consistency of both the sparse HP and $\\ell_1$ trend filters.\nUltimately, we propose to use time-varying $\\textit{contact growth rates}$ to\ndocument and monitor outbreaks of COVID-19.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 14:07:14 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 01:54:08 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Lee", "Sokbae", ""], ["Liao", "Yuan", ""], ["Seo", "Myung Hwan", ""], ["Shin", "Youngki", ""]]}, {"id": "2006.10628", "submitter": "Alejandro David De La Concha Duarte", "authors": "Alejandro de la Concha, Nicolas Vayatis, Argyris Kalogeratos", "title": "Offline detection of change-points in the mean for stationary graph\n  signals", "comments": "15 pages, 1 figure, 1 table, 1 annex. 8 pages of main text", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of segmenting a stream of graph signals: we\naim to detect changes in the mean of the multivariate signal defined over the\nnodes of a known graph. We propose an offline algorithm that relies on the\nconcept of graph signal stationarity and allows the convenient translation of\nthe problem from the original vertex domain to the spectral domain (Graph\nFourier Transform), where it is much easier to solve. Although the obtained\nspectral representation is sparse in real applications, to the best of our\nknowledge this property has not been much exploited in the existing related\nliterature. Our main contribution is a change-point detection algorithm that\nadopts a model selection perspective, which takes into account the sparsity of\nthe spectral representation and determines automatically the number of\nchange-points. Our detector comes with a proof of a non-asymptotic oracle\ninequality, numerical experiments demonstrate the validity of our method.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 15:51:38 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["de la Concha", "Alejandro", ""], ["Vayatis", "Nicolas", ""], ["Kalogeratos", "Argyris", ""]]}, {"id": "2006.10646", "submitter": "Alejandro Calle-Saldarriaga", "authors": "Alejandro Calle-Saldarriaga, Henry Laniado, Francisco Zuluaga", "title": "Homogeneity Test for Functional Data based on Data-Depth Plots", "comments": "25 pages, 17 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the classic concerns in statistics is determining if two samples come\nfrom thesame population, i.e. homogeneity testing. In this paper, we propose a\nhomogeneitytest in the context of Functional Data Analysis, adopting an idea\nfrom multivariatedata analysis: the data depth plot (DD-plot). This DD-plot is\na generalization of theunivariate Q-Q plot (quantile-quantile plot). We propose\nsome statistics based onthese DD-plots, and we use bootstrapping techniques to\nestimate their distributions.We estimate the finite-sample size and power of\nour test via simulation, obtainingbetter results than other homogeneity test\nproposed in the literature. Finally, weillustrate the procedure in samples of\nreal heterogeneous data and get consistent results.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 16:16:51 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 18:39:53 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Calle-Saldarriaga", "Alejandro", ""], ["Laniado", "Henry", ""], ["Zuluaga", "Francisco", ""]]}, {"id": "2006.10714", "submitter": "David Woods", "authors": "V. E. Bowman, D. S. Silk, U. Dalrymple, D. C. Woods", "title": "Uncertainty quantification for epidemiological forecasts of COVID-19\n  through combinations of model predictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common statistical problem is prediction, or forecasting, in the presence\nof an ensemble of multiple candidate models. For example, multiple candidate\nmodels may be available to predict case numbers in a disease epidemic,\nresulting from different modelling approaches (e.g. mechanistic or empirical)\nor differing assumptions about spatial or age mixing. Alternative models\ncapture genuine uncertainty in scientific understanding of disease dynamics,\nand/or different simplifying assumptions underpinning each model derivation.\nWhile the analysis of multi-model ensembles can be computationally challenging,\naccounting for this 'structural uncertainty' can improve forecast accuracy and\nreduce the risk of over-estimated confidence. In this paper we look at\ncombining epidemiological forecasts for COVID-19 daily deaths, hospital\nadmissions, and hospital and ICU occupancy, in order to improve the predictive\naccuracy of the short term forecasts. We combining models via combinations of\nindividual predictive densities with weights chosen via application of\npredictive scoring, as commonly applied in meteorological and economic\nforecasting.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 17:47:33 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 13:03:35 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Bowman", "V. E.", ""], ["Silk", "D. S.", ""], ["Dalrymple", "U.", ""], ["Woods", "D. C.", ""]]}, {"id": "2006.11265", "submitter": "Matteo Iacopini", "authors": "Matteo Iacopini and Francesco Ravazzolo and Luca Rossini", "title": "Proper scoring rules for evaluating asymmetry in density forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel asymmetric continuous probabilistic score (ACPS)\nfor evaluating and comparing density forecasts. It extends the proposed score\nand defines a weighted version, which emphasizes regions of interest, such as\nthe tails or the center of a variable's range. A test is also introduced to\nstatistically compare the predictive ability of different forecasts. The ACPS\nis of general use in any situation where the decision maker has asymmetric\npreferences in the evaluation of the forecasts. In an artificial experiment,\nthe implications of varying the level of asymmetry in the ACPS are illustrated.\nThen, the proposed score and test are applied to assess and compare density\nforecasts of macroeconomic relevant datasets (US employment growth) and of\ncommodity prices (oil and electricity prices) with particular focus on the\nrecent COVID-19 crisis period.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 17:53:02 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2020 14:17:35 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Iacopini", "Matteo", ""], ["Ravazzolo", "Francesco", ""], ["Rossini", "Luca", ""]]}, {"id": "2006.11628", "submitter": "Rahul Ladhania", "authors": "Rahul Ladhania, Amelia Haviland, Neeraj Sood, Edward Kennedy, Ateev\n  Mehrotra", "title": "Learning and Testing Sub-groups with Heterogeneous Treatment Effects:A\n  Sequence of Two Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is strong interest in estimating how the magnitude of treatment effects\nof an intervention vary across sub-groups of the population of interest. In our\npaper, we propose a two-study approach to first propose and then test\nheterogeneous treatment effects. In Study 1, we use a large observational\ndataset to learn sub-groups with the most distinctive treatment-outcome\nrelationships ('high/low-impact sub-groups'). We adopt a model-based recursive\npartitioning approach to propose the high/low impact sub-groups, and validate\nthem by using sample-splitting. While the first study rules out noise, there is\npotential bias in our estimated heterogeneous treatment effects. Study 2 uses\nan experimental design, and here we classify our sample units based on\nsub-groups learned in Study 1. We then estimate treatment effects within each\nof the groups, thereby testing the causal hypotheses proposed in Study 1. Using\npatient claims data from the NBER MarketScan database, we apply our approach to\nestimate heterogeneous effects of a switch to a high-deductible health\ninsurance plan on use of outpatient care by patients with a common chronic\ncondition. We extend the method to non-parametrically learn the sub-groups in\nStudy 1. We also compare the methods' performance to other state-of-the-art\nmethods in the literature that make use only of the Study 2 data.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 18:01:23 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Ladhania", "Rahul", ""], ["Haviland", "Amelia", ""], ["Sood", "Neeraj", ""], ["Kennedy", "Edward", ""], ["Mehrotra", "Ateev", ""]]}, {"id": "2006.11676", "submitter": "Tianjian Zhou", "authors": "Tianjian Zhou and Yuan Ji", "title": "A Unified Framework for Time-to-Event Dose-Finding Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In dose-finding trials, due to staggered enrollment, it might be desirable to\nmake dose assignment decisions in real-time in the presence of pending toxicity\noutcomes, for example, when patient accrual is fast or the dose-limiting\ntoxicity is late-onset. Patients' time-to-event information may be utilized to\nfacilitate such decisions. We propose a unified statistical framework for\ntime-to-event modeling in dose-finding trials, which leads to two classes of\ntime-to-event designs: TITE deigns and POD designs. TITE designs are based on\ninference on toxicity probabilities, while POD designs are based on inference\non dose-finding decisions. These two classes of designs contain existing\ndesigns as special cases and also give rise to new designs. We discuss and\nstudy the theoretical properties of these designs, including large-sample\nconvergence properties, coherence principles, and the underlying decision\nrules. To facilitate the use of time-to-event designs in practice, we introduce\nefficient computational algorithms and review common practical considerations,\nsuch as safety rules and suspension rules. Finally, the operating\ncharacteristics of several designs are evaluated and compared through computer\nsimulations.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 23:30:44 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Zhou", "Tianjian", ""], ["Ji", "Yuan", ""]]}, {"id": "2006.11759", "submitter": "Pavel Krupskii", "authors": "Pavel Krupskii and Marc G. Genton", "title": "Conditional Normal Extreme-Value Copulas", "comments": "42 pages, 6 tables and 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new class of extreme-value copulas which are extreme-value\nlimits of conditional normal models. Conditional normal models are\ngeneralizations of conditional independence models, where the dependence among\nobserved variables is modeled using one unobserved factor. Conditional on this\nfactor, the distribution of these variables is given by the Gaussian copula.\nThis structure allows one to build flexible and parsimonious models for data\nwith complex dependence structures, such as data with spatial dependence or\nfactor structure. We study the extreme-value limits of these models and show\nsome interesting special cases of the proposed class of copulas. We develop\nestimation methods for the proposed models and conduct a simulation study to\nassess the performance of these algorithms. Finally, we apply these copula\nmodels to analyze data on monthly wind maxima and stock return minima.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 10:32:01 GMT"}, {"version": "v2", "created": "Sun, 14 Feb 2021 08:12:02 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Krupskii", "Pavel", ""], ["Genton", "Marc G.", ""]]}, {"id": "2006.11865", "submitter": "Adway Mitra", "authors": "Adway Mitra", "title": "Electoral David vs Goliath: How does the Spatial Concentration of\n  Electors affect District-based Elections?", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many democratic countries use district-based elections where there is a\n\"seat\" for each district in the governing body. In each district, the party\nwhose candidate gets the maximum number of votes wins the corresponding seat.\nThe result of the election is decided based on the number of seats won by the\ndifferent parties. The electors (voters) can cast their votes only in the\ndistrict of their residence. Thus, locations of the electors and boundaries of\nthe districts may severely affect the election result even if the proportion of\npopular support (number of electors) of different parties remains unchanged.\nThis has led to significant amount of research on whether the districts may be\nredrawn or electors may be moved to maximize seats for a particular party. In\nthis paper, we frame the spatial distribution of electors in a probabilistic\nsetting, and explore different models to capture the intra-district\npolarization of electors in favour of a party, or the spatial concentration of\nsupporters of different parties. Our models are inspired by elections in India,\nwhere supporters of different parties tend to be concentrated in certain\ndistricts. We show with extensive simulations that our model can capture\ndifferent statistical properties of real elections held in India. We frame\nparameter estimation problems to fit our models to the observed election\nresults. Since analytical calculation of the likelihood functions are\ninfeasible for our complex models, we use Likelihood-free Inference methods\nunder the Approximate Bayesian Computation framework. Since this approach is\nhighly time-consuming, we explore how supervised regression using Logistic\nRegression or Deep Neural Networks can be used to speed it up. We also explore\nhow the election results can change by varying the spatial distributions of the\nvoters, even when the proportions of popular support of the parties remain\nconstant.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 18:17:57 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Mitra", "Adway", ""]]}, {"id": "2006.12180", "submitter": "Antonio Punzo", "authors": "Antonio Punzo and Luca Bagnato", "title": "The multivariate tail-inflated normal distribution and its application\n  in finance", "comments": "33 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the multivariate tail-inflated normal (MTIN)\ndistribution, an elliptical heavy-tails generalization of the multivariate\nnormal (MN). The MTIN belongs to the family of MN scale mixtures by choosing a\nconvenient continuous uniform as mixing distribution. Moreover, it has a\nclosed-form for the probability density function characterized by only one\nadditional ``inflation'' parameter, with respect to the nested MN, governing\nthe tail-weight. The first four moments are also computed; interestingly, they\nalways exist and the excess kurtosis can assume any positive value. The method\nof moments and maximum likelihood (ML) are considered for estimation. As\nconcerns the latter, a direct approach, as well as a variant of the EM\nalgorithm, are illustrated. The existence of the ML estimates is also\nevaluated. Since the inflation parameter is estimated from the data, robust\nestimates of the mean vector and covariance matrix of the nested MN\ndistribution are automatically obtained by down-weighting. Simulations are\nperformed to compare the estimation methods/algorithms and to investigate the\nability of AIC and BIC to select among a set of candidate elliptical models.\nFor illustrative purposes, the MTIN distribution is finally fitted to\nmultivariate financial data where its usefulness is also shown in comparison\nwith other well-established multivariate elliptical distributions.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 12:25:27 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Punzo", "Antonio", ""], ["Bagnato", "Luca", ""]]}, {"id": "2006.12269", "submitter": "Fiammetta Menchetti", "authors": "Fiammetta Menchetti and Iavor Bojinov", "title": "Estimating the effectiveness of permanent price reductions for competing\n  products using multivariate Bayesian structural time series models", "comments": "21 pages; further robustness checks", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Florence branch of an Italian supermarket chain recently implemented a\nstrategy that permanently lowered the price of numerous store brands in several\nproduct categories. To quantify the impact of such a policy change, researchers\noften use synthetic control methods for estimating causal effects when a subset\nof units receive a single persistent treatment, and the rest are unaffected by\nthe change. In our applications, however, competitor brands not assigned to\ntreatment are likely impacted by the intervention because of substitution\neffects; more broadly, this type of interference occurs whenever the treatment\nassignment of one unit affects the outcome of another. This paper extends the\nsynthetic control methods to accommodate partial interference, allowing\ninterference within predefined groups but not between them. Focusing on a class\nof causal estimands that capture the effect both on the treated and control\nunits, we develop a multivariate Bayesian structural time series model for\ngenerating synthetic controls that would have occurred in the absence of an\nintervention enabling us to estimate our novel effects. In a simulation study,\nwe explore our Bayesian procedure's empirical properties and show that it\nachieves good frequentists coverage even when the model is misspecified. We use\nour new methodology to make causal statements about the impact on sales of the\naffected store brands and their direct competitors. Our proposed approach is\nimplemented in the CausalMBSTS R package.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 14:03:55 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 20:57:50 GMT"}, {"version": "v3", "created": "Fri, 2 Oct 2020 17:33:52 GMT"}, {"version": "v4", "created": "Mon, 22 Feb 2021 15:38:02 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Menchetti", "Fiammetta", ""], ["Bojinov", "Iavor", ""]]}, {"id": "2006.12386", "submitter": "Federico Amato", "authors": "Federico Amato, Fabian Guignard, Vincent Humphrey, Mikhail Kanevski", "title": "Spatio-temporal evolution of global surface temperature distributions", "comments": "7 pages, 4 figures", "journal-ref": "Proceedings of the 10th International Conference on Climate\n  Informatics CI2020. Association for Computing Machinery, New York, NY, USA,\n  37 43", "doi": "10.1145/3429309.3429315", "report-no": null, "categories": "stat.AP cs.IT math.IT physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Climate is known for being characterised by strong non-linearity and chaotic\nbehaviour. Nevertheless, few studies in climate science adopt statistical\nmethods specifically designed for non-stationary or non-linear systems. Here we\nshow how the use of statistical methods from Information Theory can describe\nthe non-stationary behaviour of climate fields, unveiling spatial and temporal\npatterns that may otherwise be difficult to recognize. We study the maximum\ntemperature at two meters above ground using the NCEP CDAS1 daily reanalysis\ndata, with a spatial resolution of 2.5 by 2.5 degree and covering the time\nperiod from 1 January 1948 to 30 November 2018. The spatial and temporal\nevolution of the temperature time series are retrieved using the Fisher\nInformation Measure, which quantifies the information in a signal, and the\nShannon Entropy Power, which is a measure of its uncertainty -- or\nunpredictability. The results describe the temporal behaviour of the analysed\nvariable. Our findings suggest that tropical and temperate zones are now\ncharacterized by higher levels of entropy. Finally, Fisher-Shannon Complexity\nis introduced and applied to study the evolution of the daily maximum surface\ntemperature distributions.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 16:14:13 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 07:28:57 GMT"}, {"version": "v3", "created": "Tue, 12 Jan 2021 10:30:19 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Amato", "Federico", ""], ["Guignard", "Fabian", ""], ["Humphrey", "Vincent", ""], ["Kanevski", "Mikhail", ""]]}, {"id": "2006.12471", "submitter": "M. Amin Rahimian", "authors": "Abdullah Almaatouq, M. Amin Rahimian, Jason W. Burton, Abdulla Alhajri", "title": "When social influence promotes the wisdom of crowds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whether, and under what conditions, groups exhibit \"crowd wisdom\" has been a\nmajor focus of research across the social and computational sciences. Much of\nthis work has focused on the role of social influence in promoting the wisdom\nof the crowd versus leading the crowd astray, resulting in conflicting\nconclusions about how the social network structure determines the impact of\nsocial influence. Here, we demonstrate that it is not enough to consider the\nnetwork structure in isolation. Using theoretical analysis, numerical\nsimulation, and reanalysis of four experimental datasets (totaling 2,885 human\nsubjects), we find that the wisdom of crowds critically depends on the\ninteraction between (i) the centralization of the social influence network and\n(ii) the distribution of the initial, individual estimates. By adopting a\nframework that integrates both the structure of the social influence and the\ndistribution of the initial estimates, we bring previously conflicting results\nunder one theoretical framework and clarify the effects of social influence on\nthe wisdom of crowds.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 17:50:27 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 01:02:17 GMT"}, {"version": "v3", "created": "Thu, 29 Apr 2021 23:05:46 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Almaatouq", "Abdullah", ""], ["Rahimian", "M. Amin", ""], ["Burton", "Jason W.", ""], ["Alhajri", "Abdulla", ""]]}, {"id": "2006.12720", "submitter": "Konstantinos Pelechrinis", "authors": "Kristi Bushman, Konstantinos Pelechrinis, Alexandros Labrinidis", "title": "Effectiveness and Compliance to Social Distancing During COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the absence of pharmaceutical interventions to curb the spread of\nCOVID-19, countries relied on a number of nonpharmaceutical interventions to\nfight the first wave of the pandemic. The most prevalent one has been\nstay-at-home orders, whose the goal is to limit the physical contact between\npeople, which consequently will reduce the number of secondary infections\ngenerated. In this work, we use a detailed set of mobility data to evaluate the\nimpact that these interventions had on alleviating the spread of the virus in\nthe US as measured through the COVID-19-related deaths. To establish this\nimpact, we use the notion of Granger causality between two time-series. We show\nthat there is a unidirectional Granger causality, from the median percentage of\ntime spent daily at home to the daily number of COVID-19-related deaths with a\nlag of 2 weeks. We further analyze the mobility patterns at the census block\nlevel to identify which parts of the population might encounter difficulties in\nadhering and complying with social distancing measures. This information is\nimportant, since it can consequently drive interventions that aim at helping\nthese parts of the population.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 03:36:19 GMT"}, {"version": "v2", "created": "Sun, 19 Jul 2020 22:55:57 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Bushman", "Kristi", ""], ["Pelechrinis", "Konstantinos", ""], ["Labrinidis", "Alexandros", ""]]}, {"id": "2006.12759", "submitter": "Eduardo Ogasawara", "authors": "Balthazar Paix\\~ao, Lais Baroni, Rebecca Salles, Luciana Escobar,\n  Carlos de Sousa, Marcel Pedroso, Raphael Saldanha, Rafaelli Coutinho, Fabio\n  Porto, Eduardo Ogasawara", "title": "Estimation of COVID-19 under-reporting in Brazilian States through SARI", "comments": null, "journal-ref": null, "doi": "10.1007/s00354-021-00125-3", "report-no": null, "categories": "stat.AP cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to its impact, COVID-19 has been stressing the academy to search for\ncuring, mitigating, or controlling it. However, when it comes to controlling,\nthere are still few studies focused on under-reporting estimates. It is\nbelieved that under-reporting is a relevant factor in determining the actual\nmortality rate and, if not considered, can cause significant misinformation.\nTherefore, the objective of this work is to estimate the under-reporting of\ncases and deaths of COVID-19 in Brazilian states using data from the Infogripe\non notification of Severe Acute Respiratory Infection (SARI). The methodology\nis based on the concepts of inertia and the use of event detection techniques\nto study the time series of hospitalized SARI cases. The estimate of real cases\nof the disease, called novelty, is calculated by comparing the difference in\nSARI cases in 2020 (after COVID-19) with the total expected cases in recent\nyears (2016 to 2019) derived from a seasonal exponential moving average. The\nresults show that under-reporting rates vary significantly between states and\nthat there are no general patterns for states in the same region in Brazil.\n  The published version of this paper is made available at\nhttps://doi.org/10.1007/s00354-021-00125-3.\n  Please cite as: B. Paix\\~ao, L. Baroni, M. Pedroso, R. Salles, L. Escobar, C.\nde Sousa, R. de Freitas Saldanha, J. Soares, R. Coutinho, et al., 2021,\nEstimation of COVID-19 Under-Reporting in the Brazilian States Through SARI,\nNew Generation Computing\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 04:54:22 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 15:06:08 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Paix\u00e3o", "Balthazar", ""], ["Baroni", "Lais", ""], ["Salles", "Rebecca", ""], ["Escobar", "Luciana", ""], ["de Sousa", "Carlos", ""], ["Pedroso", "Marcel", ""], ["Saldanha", "Raphael", ""], ["Coutinho", "Rafaelli", ""], ["Porto", "Fabio", ""], ["Ogasawara", "Eduardo", ""]]}, {"id": "2006.12811", "submitter": "Thomas Burnett", "authors": "Thomas Burnett (1), Pavel Mozgunov (1), Philip Pallmann (2), Sofia S.\n  Villar (3), Graham M. Wheeler (4), Thomas Jaki (1,3) ((1) Lancaster\n  University, (2) Cardiff University, (3) University of Cambridge, (4)\n  University College London)", "title": "Adding flexibility to clinical trial designs: an example-based guide to\n  the practical use of adaptive designs", "comments": "35 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive designs for clinical trials permit alterations to a study in\nresponse to accumulating data in order to make trials more flexible, ethical\nand efficient. These benefits are achieved while preserving the integrity and\nvalidity of the trial, through the pre-specification and proper adjustment for\nthe possible alterations during the course of the trial. Despite much research\nin the statistical literature highlighting the potential advantages of adaptive\ndesigns over traditional fixed designs, the uptake of such methods in clinical\nresearch has been slow. One major reason for this is that different adaptations\nto trial designs, as well as their advantages and limitations, remain\nunfamiliar to large parts of the clinical community. The aim of this paper is\nto clarify where adaptive designs can be used to address specific questions of\nscientific interest; we introduce the main features of adaptive designs and\ncommonly used terminology, highlighting their utility and pitfalls, and\nillustrate their use through case studies of adaptive trials ranging from\nearly-phase dose escalation to confirmatory Phase III studies.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 08:03:48 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Burnett", "Thomas", ""], ["Mozgunov", "Pavel", ""], ["Pallmann", "Philip", ""], ["Villar", "Sofia S.", ""], ["Wheeler", "Graham M.", ""], ["Jaki", "Thomas", ""]]}, {"id": "2006.13054", "submitter": "Hau-tieng Wu", "authors": "John Malik and Yu-Ting Lin and Ronen Talmon and Hau-Tieng Wu", "title": "Unsupervised ensembling of multiple software sensors: a new approach for\n  electrocardiogram-derived respiration using one or two channels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While several electrocardiogram-derived respiratory (EDR) algorithms have\nbeen proposed to extract breathing activity from a single-channel ECG signal,\nconclusively identifying a superior technique is challenging. We propose\nviewing each EDR algorithm as a {\\em software sensor} that records the\nbreathing activity from the ECG signal, and ensembling those software sensors\nto achieve a higher quality EDR signal. We refer to the output of the proposed\nensembling algorithm as the {\\em ensembled EDR}. We test the algorithm on a\nlarge scale database of 116 whole-night polysomnograms and compare the\nensembled EDR signal with four respiratory signals recorded from four different\nhardware sensors. The proposed algorithm consistently improves upon other\nalgorithms, and we envision its clinical value and its application in future\nhealthcare.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 14:26:30 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Malik", "John", ""], ["Lin", "Yu-Ting", ""], ["Talmon", "Ronen", ""], ["Wu", "Hau-Tieng", ""]]}, {"id": "2006.13057", "submitter": "Omar Rivasplata", "authors": "Omar Rivasplata, Ilja Kuzborskij, Csaba Szepesvari, John Shawe-Taylor", "title": "PAC-Bayes Analysis Beyond the Usual Bounds", "comments": "In NeurIPS 2020. Version 3 is the final published paper. Note that\n  this paper is an enhanced version of the short paper with the same title that\n  was presented at the NeurIPS 2019 Workshop on Machine Learning with\n  Guarantees. Important update: the PAC-Bayes type inequality for unbounded\n  loss functions (Section 2.3) is new", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on a stochastic learning model where the learner observes a finite\nset of training examples and the output of the learning process is a\ndata-dependent distribution over a space of hypotheses. The learned\ndata-dependent distribution is then used to make randomized predictions, and\nthe high-level theme addressed here is guaranteeing the quality of predictions\non examples that were not seen during training, i.e. generalization. In this\nsetting the unknown quantity of interest is the expected risk of the\ndata-dependent randomized predictor, for which upper bounds can be derived via\na PAC-Bayes analysis, leading to PAC-Bayes bounds.\n  Specifically, we present a basic PAC-Bayes inequality for stochastic kernels,\nfrom which one may derive extensions of various known PAC-Bayes bounds as well\nas novel bounds. We clarify the role of the requirements of fixed 'data-free'\npriors, bounded losses, and i.i.d. data. We highlight that those requirements\nwere used to upper-bound an exponential moment term, while the basic PAC-Bayes\ntheorem remains valid without those restrictions. We present three bounds that\nillustrate the use of data-dependent priors, including one for the unbounded\nsquare loss.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 14:30:24 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 16:55:25 GMT"}, {"version": "v3", "created": "Mon, 28 Dec 2020 18:59:11 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Rivasplata", "Omar", ""], ["Kuzborskij", "Ilja", ""], ["Szepesvari", "Csaba", ""], ["Shawe-Taylor", "John", ""]]}, {"id": "2006.13072", "submitter": "Alvaro Cabrejas Egea", "authors": "Alvaro Cabrejas Egea and Colm Connaughton", "title": "Wavelet Augmented Regression Profiling (WARP): improved long-term\n  estimation of travel time series with recurrent congestion", "comments": "12 pages, 10 figures. Conference paper accepted to 23rd IEEE\n  International Conference on Intelligent Transportation Systems (ITSC2020)", "journal-ref": null, "doi": "10.1109/ITSC45102.2020.9294318", "report-no": null, "categories": "cs.CE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliable estimates of typical travel times allow road users to forward plan\njourneys to minimise travel time, potentially increasing overall system\nefficiency. On busy highways, however, congestion events can cause large,\nshort-term spikes in travel time. These spikes make direct forecasting of\ntravel time using standard time series models difficult on the timescales of\nhours to days that are relevant to forward planning. The problem is that some\nsuch spikes are caused by unpredictable incidents and should be filtered out,\nwhereas others are caused by recurrent peaks in demand and should be factored\ninto estimates. Here we present the Wavelet Augmented Regression Profiling\n(WARP) method for long-term estimation of typical travel times. WARP linearly\ndecomposes historical time series of travel times into two components:\nbackground and spikes. It then further separates the spikes into contributions\nfrom recurrent and residual congestion. This is achieved using a combination of\nwavelet transforms, spectral filtering and locally weighted regression. The\nbackground and recurrent congestion contributions are then used to estimate\ntypical travel times with horizon of one week in an accurate and\ncomputationally inexpensive manner. We train and test WARP on the M6 and M11\nmotorways in the United Kingdom using 12 weeks of link level travel time data\nobtained from the UK's National Traffic Information Service (NTIS). In\nout-of-sample validation tests, WARP compares favourably to estimates produced\nby a simple segmentation method and to the estimates published by NTIS.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 14:50:28 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Egea", "Alvaro Cabrejas", ""], ["Connaughton", "Colm", ""]]}, {"id": "2006.13094", "submitter": "Claudia Furlan", "authors": "Claudia Furlan and Cinzia Mortarino", "title": "The role of swabs in modeling the COVID-19 outbreak in the most affected\n  regions of Italy", "comments": "13 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The daily fluctuations in the released number of Covid-19 cases played a big\nrole both at the beginning and in the most critical weeks of the outbreak, when\nlocal authorities in Italy had to decide whether to impose a lockdown and at\nwhich level. Public opinion was focused on this information as well, to\nunderstand how quickly the epidemic was spreading. When an increase/decrease\nwas communicated, especially a large one, it was not easy to understand if it\nwas due to a change in the epidemic evolution or if it was a fluctuation due to\nother reasons, such as an increase in the number of swabs or a delay in the\nswab processing. In this work, we propose a nonlinear asymmetric diffusion\nmodel, which includes information on the daily number of swabs, to describe\ndaily fluctuations in the number of confirmed cases in addition to the main\ntrend of the outbreak evolution. The class of diffusion models has been\nselected to develop our proposal, as it also allows estimation of the total\nnumber of confirmed cases at the end of the outbreak. The proposed model is\ncompared to six existing models, among which the logistic and the SIRD models\nare used as benchmarks, in the five most affected Italian regions.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 15:35:02 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Furlan", "Claudia", ""], ["Mortarino", "Cinzia", ""]]}, {"id": "2006.13105", "submitter": "Erik Scharw\\\"achter", "authors": "Erik Scharw\\\"achter and Jonathan Lennartz and Emmanuel M\\\"uller", "title": "Differentiable Segmentation of Sequences", "comments": "source codes available at https://github.com/diozaka/diffseg", "journal-ref": "International Conference on Learning Representations (ICLR) 2021", "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmented models are widely used to describe non-stationary sequential data\nwith discrete change points. Their estimation usually requires solving a mixed\ndiscrete-continuous optimization problem, where the segmentation is the\ndiscrete part and all other model parameters are continuous. A number of\nestimation algorithms have been developed that are highly specialized for their\nspecific model assumptions. The dependence on non-standard algorithms makes it\nhard to integrate segmented models in state-of-the-art deep learning\narchitectures that critically depend on gradient-based optimization techniques.\nIn this work, we formulate a relaxed variant of segmented models that enables\njoint estimation of all model parameters, including the segmentation, with\ngradient descent. We build on recent advances in learning continuous warping\nfunctions and propose a novel family of warping functions based on the\ntwo-sided power (TSP) distribution. TSP-based warping functions are\ndifferentiable, have simple closed-form expressions, and can represent\nsegmentation functions exactly. Our formulation includes the important class of\nsegmented generalized linear models as a special case, which makes it highly\nversatile. We use our approach to model the spread of COVID-19 with Poisson\nregression, apply it on a change point detection task, and learn classification\nmodels with concept drift. The experiments show that our approach effectively\nlearns all these tasks with standard algorithms for gradient descent.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 15:51:48 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 11:11:04 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Scharw\u00e4chter", "Erik", ""], ["Lennartz", "Jonathan", ""], ["M\u00fcller", "Emmanuel", ""]]}, {"id": "2006.13107", "submitter": "Daniel Kowal", "authors": "Daniel R. Kowal", "title": "Fast, Optimal, and Targeted Predictions using Parametrized Decision\n  Analysis", "comments": null, "journal-ref": null, "doi": "10.1080/01621459.2021.1891926", "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction is critical for decision-making under uncertainty and lends\nvalidity to statistical inference. With targeted prediction, the goal is to\noptimize predictions for specific decision tasks of interest, which we\nrepresent via functionals. Although classical decision analysis extracts\npredictions from a Bayesian model, these predictions are often difficult to\ninterpret and slow to compute. Instead, we design a class of parametrized\nactions for Bayesian decision analysis that produce optimal, scalable, and\nsimple targeted predictions. For a wide variety of action parametrizations and\nloss functions--including linear actions with sparsity constraints for targeted\nvariable selection--we derive a convenient representation of the optimal\ntargeted prediction that yields efficient and interpretable solutions.\nCustomized out-of-sample predictive metrics are developed to evaluate and\ncompare among targeted predictors. Through careful use of the posterior\npredictive distribution, we introduce a procedure that identifies a set of\nnear-optimal, or acceptable targeted predictors, which provide unique insights\ninto the features and level of complexity needed for accurate targeted\nprediction. Simulations demonstrate excellent prediction, estimation, and\nvariable selection capabilities. Targeted predictions are constructed for\nphysical activity data from the National Health and Nutrition Examination\nSurvey (NHANES) to better predict and understand the characteristics of\nintraday physical activity.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 15:55:47 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2020 16:39:37 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Kowal", "Daniel R.", ""]]}, {"id": "2006.13135", "submitter": "Sebastian P\\\"olsterl", "authors": "Sebastian P\\\"olsterl, Christian Wachinger", "title": "Estimation of Causal Effects in the Presence of Unobserved Confounding\n  in the Alzheimer's Continuum", "comments": "Accepted as oral presentation at 2021 International Conference on\n  Information Processing in Medical Imaging (IPMI)", "journal-ref": "Information Processing in Medical Imaging (2021) 45-57", "doi": "10.1007/978-3-030-78191-0_4", "report-no": null, "categories": "stat.ME cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studying the relationship between neuroanatomy and cognitive decline due to\nAlzheimer's has been a major research focus in the last decade. However, to\ninfer cause-effect relationships rather than simple associations from\nobservational data, we need to (i) express the causal relationships leading to\ncognitive decline in a graphical model, and (ii) ensure the causal effect of\ninterest is identifiable from the collected data. We derive a causal graph from\nthe current clinical knowledge on cause and effect in the Alzheimer's disease\ncontinuum, and show that identifiability of the causal effect requires all\nconfounders to be known and measured. However, in complex neuroimaging studies,\nwe neither know all potential confounders nor do we have data on them. To\nalleviate this requirement, we leverage the dependencies among multiple causes\nby deriving a substitute confounder via a probabilistic latent factor model. In\nour theoretical analysis, we prove that using the substitute confounder enables\nidentifiability of the causal effect of neuroanatomy on cognition. We\nquantitatively evaluate the effectiveness of our approach on semi-synthetic\ndata, where we know the true causal effects, and illustrate its use on real\ndata on the Alzheimer's disease continuum, where it reveals important causes\nthat otherwise would have been missed.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 16:29:54 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 15:18:05 GMT"}, {"version": "v3", "created": "Mon, 12 Apr 2021 16:11:32 GMT"}, {"version": "v4", "created": "Sun, 20 Jun 2021 08:42:25 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["P\u00f6lsterl", "Sebastian", ""], ["Wachinger", "Christian", ""]]}, {"id": "2006.13152", "submitter": "Giulia Carella", "authors": "Giulia Carella, Andy Eschbacher, Dongjie Fan, Miguel \\'Alvarez,\n  \\'Alvaro Arredondo, Alejandro Polvillo Hall, Javier P\\'erez Trufero, and\n  Javier de la Torre", "title": "Magnify Your Population: Statistical Downscaling to Augment the Spatial\n  Resolution of Socioeconomic Census Data", "comments": "14 pages, 5 figures, accepted at KDD Workshop on Humanitarian\n  Mapping, August 24, 2020 (https://kdd-humanitarian-mapping.herokuapp.com/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fine resolution estimates of demographic and socioeconomic attributes are\ncrucial for planning and policy development. While several efforts have been\nmade to produce fine-scale gridded population estimates, socioeconomic features\nare typically not available at scales finer than Census units, which may hide\nlocal heterogeneity and disparity. In this paper we present a new statistical\ndownscaling approach to derive fine-scale estimates of key socioeconomic\nattributes. The method leverages demographic and geographical extensive\ncovariates available at multiple scales and additional Census covariates only\navailable at coarse resolution, which are included in the model hierarchically\nwithin a \"forward learning\" approach. For each selected socioeconomic variable,\na Random Forest model is trained on the source Census units and then used to\ngenerate fine-scale gridded predictions, which are then adjusted to ensure the\nbest possible consistency with the coarser Census data. As a case study, we\napply this method to Census data in the United States, downscaling the selected\nsocioeconomic variables available at the block group level, to a grid of ~300\nspatial resolution. The accuracy of the method is assessed at both spatial\nscales, first computing a pseudo cross-validation coefficient of determination\nfor the predictions at the block group level and then, for extensive variables\nonly, also for the (unadjusted) predicted counts summed by block group. Based\non these scores and on the inspection of the downscaled maps, we conclude that\nour method is able to provide accurate, smoother, and more detailed\nsocioeconomic estimates than the available Census data.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 16:52:18 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Carella", "Giulia", ""], ["Eschbacher", "Andy", ""], ["Fan", "Dongjie", ""], ["\u00c1lvarez", "Miguel", ""], ["Arredondo", "\u00c1lvaro", ""], ["Hall", "Alejandro Polvillo", ""], ["Trufero", "Javier P\u00e9rez", ""], ["de la Torre", "Javier", ""]]}, {"id": "2006.13154", "submitter": "George Stepaniants", "authors": "George Stepaniants, Bingni W. Brunton, J. Nathan Kutz", "title": "Inferring Causal Networks of Dynamical Systems through Transient\n  Dynamics and Perturbation", "comments": "11 pages, 9 figures", "journal-ref": "Phys. Rev. E 102, 042309 (2020)", "doi": "10.1103/PhysRevE.102.042309", "report-no": null, "categories": "math.DS nlin.AO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring causal relations from time series measurements is an ill-posed\nmathematical problem, where typically an infinite number of potential solutions\ncan reproduce the given data. We explore in depth a strategy to disambiguate\nbetween possible underlying causal networks by perturbing the network, where\nthe actuations are either targeted or applied at random. The resulting\ntransient dynamics provide the critical information necessary to infer\ncausality. Two methods are shown to provide accurate causal reconstructions:\nGranger causality (GC) with perturbations, and our proposed perturbation\ncascade inference (PCI). Perturbed GC is capable of inferring smaller networks\nunder low coupling strength regimes. Our proposed PCI method demonstrated\nconsistently strong performance in inferring causal relations for small (2-5\nnode) and large (10-20 node) networks, with both linear and nonlinear dynamics.\nThus the ability to apply a large and diverse set of perturbations/actuations\nto the network is critical for successfully and accurately determining causal\nrelations and disambiguating between various viable networks.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 16:54:35 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 21:34:56 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Stepaniants", "George", ""], ["Brunton", "Bingni W.", ""], ["Kutz", "J. Nathan", ""]]}, {"id": "2006.13275", "submitter": "Jordan Weiss", "authors": "Jordan Weiss, Eli Puterman, Aric A. Prather, Erin B. Ware, David H.\n  Rehkopf", "title": "A data-driven prospective study of incident dementia among older adults\n  in the United States", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0239994", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We conducted a prospective analysis of incident dementia and its association\nwith 65 sociodemographic, early-life, economic, health and behavioral, social,\nand genetic risk factors in a sample of 7,908 adults over the age of 50 from\nthe nationally representative US-based Health and Retirement Study. We used\ntraditional survival analysis methods (Fine-Gray models) and a data-driven\napproach (random survival forests for competing risks) which allowed us to\naccount for the competing risk of death with up to 14 years of follow-up.\nOverall, the top five predictors across all groups were lower education,\nloneliness, lower wealth and income, and lower self-reported health. However,\nwe observed variation in the leading predictors of dementia across\nracial/ethnic and gender groups. Our ranked lists may be useful for guiding\nfuture observational and quasi-experimental research that investigates\nunderstudied domains of risk and emphasizes life course economic and health\nconditions as well as disparities therein.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 10:13:18 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Weiss", "Jordan", ""], ["Puterman", "Eli", ""], ["Prather", "Aric A.", ""], ["Ware", "Erin B.", ""], ["Rehkopf", "David H.", ""]]}, {"id": "2006.13277", "submitter": "Yujie Hu", "authors": "Fahui Wang, Yujie Hu, Shuai Wang, Xiaojuan Li", "title": "Local Indicator of Colocation Quotient with a Statistical Significance\n  Test: Examining Spatial Association of Crime and Facilities", "comments": null, "journal-ref": "The Professional Geographer, 69(1), 22-31 (2017)", "doi": "10.1080/00330124.2016.1157498", "report-no": null, "categories": "stat.AP cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most existing point-based colocation methods are global measures (e.g., join\ncount statistic, cross K function, and global colocation quotient). Most\nrecently, a local indicator such as the local colocation quotient is proposed\nto capture the variability of colocation across areas. Our research advances\nthis line of work by developing a simulation-based statistic test for the local\nindicator of colocation quotient (LCLQ). The study applies the indicator to\nexamine the association of land use facilities with crime patterns. Moreover,\nwe use the street network distance in addition to the traditional Euclidean\ndistance in defining neighbors since human activities (including facilities and\ncrimes) usually occur along a street network. The method is applied to analyze\nthe colocation of three types of crimes and three categories of facilities in a\ncity in Jiangsu Province, China. The findings demonstrate the value of the\nproposed method in colocation analysis of crime and facilities, and in general\ncolocation analysis of point data.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 14:05:41 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Wang", "Fahui", ""], ["Hu", "Yujie", ""], ["Wang", "Shuai", ""], ["Li", "Xiaojuan", ""]]}, {"id": "2006.13455", "submitter": "Gail Potter", "authors": "Gail E. Potter, Nicole Bohme Carnegie, Jonathan D. Sugimoto, Aldiouma\n  Diallo, John C. Victor, Kathleen Neuzil, M. Elizabeth Halloran", "title": "Using social contact data to improve the overall effect estimate of a\n  cluster-randomized influenza vaccination program in Senegal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study estimates the overall effect of two influenza vaccination programs\nconsecutively administered in a cluster-randomized trial in western Senegal\nover the course of two influenza seasons from 2009-2011. We apply cutting-edge\nmethodology combining social contact data with infection data to reduce bias in\nestimation arising from contamination between clusters. Our time-varying\nestimates reveal a reduction in seasonal influenza from the intervention and a\nnonsignificant increase in H1N1 pandemic influenza. We estimate an additive\nchange in overall cumulative incidence (which was 6.13% in the control arm) of\n-0.68 percentage points during Year 1 of the study (95% CI: -2.53, 1.18). When\nH1N1 pandemic infections were excluded from analysis, the estimated change was\n-1.45 percentage points and was significant (95% CI, -2.81, -0.08). Because\ncross-cluster contamination was low (0-3% of contacts for most villages), an\nestimator assuming no contamination was only slightly attenuated (-0.65\npercentage points). These findings are encouraging for studies carefully\ndesigned to minimize spillover. Further work is needed to estimate\ncontamination, and its effect on estimation, in a variety of settings.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 03:35:04 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 02:13:34 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Potter", "Gail E.", ""], ["Carnegie", "Nicole Bohme", ""], ["Sugimoto", "Jonathan D.", ""], ["Diallo", "Aldiouma", ""], ["Victor", "John C.", ""], ["Neuzil", "Kathleen", ""], ["Halloran", "M. Elizabeth", ""]]}, {"id": "2006.13737", "submitter": "Carsten Eickhoff", "authors": "Gil Alon, Elizabeth Chen, Guergana Savova, Carsten Eickhoff", "title": "Diagnosis Prevalence vs. Efficacy in Machine-learning Based Diagnostic\n  Decision Support", "comments": "AMIA Joint Summits in Translational Science, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recent studies use machine learning to predict a small number of\nICD-9-CM codes. In practice, on the other hand, physicians have to consider a\nbroader range of diagnoses. This study aims to put these previously incongruent\nevaluation settings on a more equal footing by predicting ICD-9-CM codes based\non electronic health record properties and demonstrating the relationship\nbetween diagnosis prevalence and system performance. We extracted patient\nfeatures from the MIMIC-III dataset for each admission. We trained and\nevaluated 43 different machine learning classifiers. Among this pool, the most\nsuccessful classifier was a Multi-Layer Perceptron. In accordance with general\nmachine learning expectation, we observed all classifiers' F1 scores to drop as\ndisease prevalence decreased. Scores fell from 0.28 for the 50 most prevalent\nICD-9-CM codes to 0.03 for the 1000 most prevalent ICD-9-CM codes. Statistical\nanalyses showed a moderate positive correlation between disease prevalence and\nefficacy (0.5866).\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 13:52:04 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Alon", "Gil", ""], ["Chen", "Elizabeth", ""], ["Savova", "Guergana", ""], ["Eickhoff", "Carsten", ""]]}, {"id": "2006.13766", "submitter": "Rose Baker", "authors": "Rose Baker", "title": "Discrete distributions from a Markov chain", "comments": "21 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A discrete-time stochastic process derived from a model of basketball is used\nto generalize any discrete distribution. The generalized distributions can have\none or two more parameters than the parent distribution. Those derived from\nbinomial, Poisson and negative binomial distributions can be underdispersed or\noverdispersed. The mean can be simply expressed in terms of model parameters,\nthus making inference for the mean straightforward. Probabilities can be\nquickly computed, enabling likelihood-based inference. Random number generation\nis also straightforward. The properties of some of the new distributions are\ndescribed and their use is illustrated with examples.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 14:26:33 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Baker", "Rose", ""]]}, {"id": "2006.13786", "submitter": "Philo P\\\"ollmann", "authors": "Xiang Liu and Philo P\\\"ollmann", "title": "Dynamic Population Estimation Using Anonymized Mobility Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine population distribution both in space and in time is crucial for\nepidemic management, disaster prevention,urban planning and more. Human\nmobility data have a great potential for mapping population distribution at a\nhigh level of spatiotemporal resolution. Power law models are the most popular\nones for mapping mobility data to population. However,they fail to provide\nconsistent estimations under different spatial and temporal resolutions, i.e.\nthey have to be recalibrated whenever the spatial or temporal partitioning\nscheme changes. We propose a Bayesian model for dynamic population estimation\nusing static census data and anonymized mobility data. Our model gives\nconsistent population estimations under different spatial and temporal\nresolutions.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 15:00:22 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Liu", "Xiang", ""], ["P\u00f6llmann", "Philo", ""]]}, {"id": "2006.13798", "submitter": "Loic Le Folgoc", "authors": "L. Le Folgoc, V. Baltatzis, A. Alansary, S. Desai, A. Devaraj, S.\n  Ellis, O. E. Martinez Manzanera, F. Kanavati, A. Nair, J. Schnabel and B.\n  Glocker", "title": "Bayesian Sampling Bias Correction: Training with the Right Loss Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.QM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive a family of loss functions to train models in the presence of\nsampling bias. Examples are when the prevalence of a pathology differs from its\nsampling rate in the training dataset, or when a machine learning practioner\nrebalances their training dataset. Sampling bias causes large discrepancies\nbetween model performance in the lab and in more realistic settings. It is\nomnipresent in medical imaging applications, yet is often overlooked at\ntraining time or addressed on an ad-hoc basis. Our approach is based on\nBayesian risk minimization. For arbitrary likelihood models we derive the\nassociated bias corrected loss for training, exhibiting a direct connection to\ninformation gain. The approach integrates seamlessly in the current paradigm of\n(deep) learning using stochastic backpropagation and naturally with Bayesian\nmodels. We illustrate the methodology on case studies of lung nodule malignancy\ngrading.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 15:10:43 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Folgoc", "L. Le", ""], ["Baltatzis", "V.", ""], ["Alansary", "A.", ""], ["Desai", "S.", ""], ["Devaraj", "A.", ""], ["Ellis", "S.", ""], ["Manzanera", "O. E. Martinez", ""], ["Kanavati", "F.", ""], ["Nair", "A.", ""], ["Schnabel", "J.", ""], ["Glocker", "B.", ""]]}, {"id": "2006.13815", "submitter": "Primoz Kocbek", "authors": "Simon Kocbek, Primoz Kocbek, Leona Cilar, Gregor Stiglic", "title": "Local Interpretability of Calibrated Prediction Models: A Case of Type 2\n  Diabetes Mellitus Screening Test", "comments": "Submitted to the DSHealth 2020 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine Learning (ML) models are often complex and difficult to interpret due\nto their 'black-box' characteristics. Interpretability of a ML model is usually\ndefined as the degree to which a human can understand the cause of decisions\nreached by a ML model. Interpretability is of extremely high importance in many\nfields of healthcare due to high levels of risk related to decisions based on\nML models. Calibration of the ML model outputs is another issue often\noverlooked in the application of ML models in practice. This paper represents\nan early work in examination of prediction model calibration impact on the\ninterpretability of the results. We present a use case of a patient in diabetes\nscreening prediction scenario and visualize results using three different\ntechniques to demonstrate the differences between calibrated and uncalibrated\nregularized regression model.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 14:14:35 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Kocbek", "Simon", ""], ["Kocbek", "Primoz", ""], ["Cilar", "Leona", ""], ["Stiglic", "Gregor", ""]]}, {"id": "2006.13824", "submitter": "Ahmed Aziz Ezzat", "authors": "Ahmed Aziz Ezzat, Sheng Liu, Dorit S. Hochbaum, Yu Ding", "title": "A Graph-Theoretic Approach for Spatial Filtering and Its Impact on\n  Mixed-type Spatial Pattern Recognition in Wafer Bin Maps", "comments": null, "journal-ref": null, "doi": "10.1109/TSM.2021.3062943", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical quality control in semiconductor manufacturing hinges on\neffective diagnostics of wafer bin maps, wherein a key challenge is to detect\nhow defective chips tend to spatially cluster on a wafer--a problem known as\nspatial pattern recognition. Recently, there has been a growing interest in\nmixed-type spatial pattern recognition--when multiple defect patterns, of\ndifferent shapes, co-exist on the same wafer. Mixed-type spatial pattern\nrecognition entails two central tasks: (1) spatial filtering, to distinguish\nsystematic patterns from random noises; and (2) spatial clustering, to group\nfiltered patterns into distinct defect types. Observing that spatial filtering\nis instrumental to high-quality mixed-type pattern recognition, we propose to\nuse a graph-theoretic method, called adjacency-clustering, which leverages\nspatial dependence among adjacent defective chips to effectively filter the raw\nwafer maps. Tested on real-world data and compared against a state-of the-art\napproach, our proposed method achieves at least 46% gain in terms of internal\ncluster validation quality (i.e., validation without external class labels),\nand about ~5% gain in terms of Normalized Mutual Information--an external\ncluster validation metric based on external class labels. Interestingly, the\nmargin of improvement appears to be a function of the pattern complexity, with\nlarger gains achieved for more complex-shaped patterns.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 15:57:27 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 17:21:46 GMT"}, {"version": "v3", "created": "Wed, 25 Nov 2020 07:56:49 GMT"}, {"version": "v4", "created": "Fri, 26 Feb 2021 14:31:51 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Ezzat", "Ahmed Aziz", ""], ["Liu", "Sheng", ""], ["Hochbaum", "Dorit S.", ""], ["Ding", "Yu", ""]]}, {"id": "2006.13852", "submitter": "Arko Barman", "authors": "Arko Barman", "title": "Time Series Analysis and Forecasting of COVID-19 Cases Using LSTM and\n  ARIMA Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Coronavirus disease 2019 (COVID-19) is a global public health crisis that has\nbeen declared a pandemic by World Health Organization. Forecasting country-wise\nCOVID-19 cases is necessary to help policymakers and healthcare providers\nprepare for the future. This study explores the performance of several Long\nShort-Term Memory (LSTM) models and Auto-Regressive Integrated Moving Average\n(ARIMA) model in forecasting the number of confirmed COVID-19 cases. Time\nseries of daily cumulative COVID-19 cases were used for generating 1-day,\n3-day, and 5-day forecasts using several LSTM models and ARIMA. Two novel\nk-period performance metrics - k-day Mean Absolute Percentage Error (kMAPE) and\nk-day Median Symmetric Accuracy (kMdSA) - were developed for evaluating the\nperformance of the models in forecasting time series values for multiple days.\nErrors in prediction using kMAPE and kMdSA for LSTM models were both as low as\n0.05%, while those for ARIMA were 0.07% and 0.06% respectively. LSTM models\nslightly underestimated while ARIMA slightly overestimated the numbers in the\nforecasts. The performance of LSTM models is comparable to ARIMA in forecasting\nCOVID-19 cases. While ARIMA requires longer sequences, LSTMs can perform\nreasonably well with sequence sizes as small as 3. However, LSTMs require a\nlarge number of training samples. Further, the development of k-period\nperformance metrics proposed is likely to be useful for performance evaluation\nof time series models in predicting multiple periods. Based on the k-period\nperformance metrics proposed, both LSTMs and ARIMA are useful for time series\nanalysis and forecasting for COVID-19.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 20:07:48 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Barman", "Arko", ""]]}, {"id": "2006.13994", "submitter": "Ying Mao", "authors": "Ying Mao, Susiyan Jiang, Daniel Nametz, Yuxin Lin, Jake Hack, John\n  Hensley, Ryan Monaghan, Tess Gutenbrunner", "title": "Data-driven Analytical Models of COVID-2019 for Epidemic Prediction,\n  Clinical Diagnosis, Policy Effectiveness and Contact Tracing: A Survey", "comments": "Covid-19 Survey, Data-driven Analytics, Analytical Models", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widely spread CoronaVirus Disease (COVID)-19 is one of the worst\ninfectious disease outbreaks in history and has become an emergency of primary\ninternational concern. As the pandemic evolves, academic communities have been\nactively involved in various capacities, including accurate epidemic\nestimation, fast clinical diagnosis, policy effectiveness evaluation and\ndevelopment of contract tracing technologies. There are more than 23,000\nacademic papers on the COVID-19 outbreak, and this number is doubling every 20\ndays while the pandemic is still on-going [1]. The literature, however, at its\nearly stage, lacks a comprehensive survey from a data analytics perspective. In\nthis paper, we review the latest models for analyzing COVID19 related data,\nconduct post-publication model evaluations and cross-model comparisons, and\ncollect data sources from different projects.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 18:50:26 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 14:08:44 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Mao", "Ying", ""], ["Jiang", "Susiyan", ""], ["Nametz", "Daniel", ""], ["Lin", "Yuxin", ""], ["Hack", "Jake", ""], ["Hensley", "John", ""], ["Monaghan", "Ryan", ""], ["Gutenbrunner", "Tess", ""]]}, {"id": "2006.14061", "submitter": "Cem Tekin", "authors": "Andi Nika, Kerem Bozgan, \\c{C}a\\u{g}{\\i}n Ararat, Cem Tekin", "title": "Pareto Active Learning with Gaussian Processes and Adaptive\n  Discretization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of optimizing a vector-valued objective function\n$\\boldsymbol{f}$ sampled from a Gaussian Process (GP) whose index set is a\nwell-behaved, compact metric space $({\\cal X},d)$ of designs. We assume that\n$\\boldsymbol{f}$ is not known beforehand and that evaluating $\\boldsymbol{f}$\nat design $x$ results in a noisy observation of $\\boldsymbol{f}(x)$. Since\nidentifying the Pareto optimal designs via exhaustive search is infeasible when\nthe cardinality of ${\\cal X}$ is large, we propose an algorithm, called\nAdaptive $\\boldsymbol{\\epsilon}$-PAL, that exploits the smoothness of the\nGP-sampled function and the structure of $({\\cal X},d)$ to learn fast. In\nessence, Adaptive $\\boldsymbol{\\epsilon}$-PAL employs a tree-based adaptive\ndiscretization technique to identify an $\\boldsymbol{\\epsilon}$-accurate Pareto\nset of designs in as few evaluations as possible. We provide both\ninformation-type and metric dimension-type bounds on the sample complexity of\n$\\boldsymbol{\\epsilon}$-accurate Pareto set identification. We also\nexperimentally show that our algorithm outperforms other Pareto set\nidentification methods on several benchmark datasets.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 21:27:27 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Nika", "Andi", ""], ["Bozgan", "Kerem", ""], ["Ararat", "\u00c7a\u011f\u0131n", ""], ["Tekin", "Cem", ""]]}, {"id": "2006.14078", "submitter": "Dhagash Mehta", "authors": "Edgar A. Bernal, Jonathan D. Hauenstein, Dhagash Mehta, Margaret H.\n  Regan, Tingting Tang", "title": "Machine learning the real discriminant locus", "comments": "22 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SC math.AG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameterized systems of polynomial equations arise in many applications in\nscience and engineering with the real solutions describing, for example,\nequilibria of a dynamical system, linkages satisfying design constraints, and\nscene reconstruction in computer vision. Since different parameter values can\nhave a different number of real solutions, the parameter space is decomposed\ninto regions whose boundary forms the real discriminant locus. This article\nviews locating the real discriminant locus as a supervised classification\nproblem in machine learning where the goal is to determine classification\nboundaries over the parameter space, with the classes being the number of real\nsolutions. For multidimensional parameter spaces, this article presents a novel\nsampling method which carefully samples the parameter space. At each sample\npoint, homotopy continuation is used to obtain the number of real solutions to\nthe corresponding polynomial system. Machine learning techniques including\nnearest neighbor and deep learning are used to efficiently approximate the real\ndiscriminant locus. One application of having learned the real discriminant\nlocus is to develop a real homotopy method that only tracks the real solution\npaths unlike traditional methods which track all~complex~solution~paths.\nExamples show that the proposed approach can efficiently approximate\ncomplicated solution boundaries such as those arising from the equilibria of\nthe Kuramoto model.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 22:18:08 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Bernal", "Edgar A.", ""], ["Hauenstein", "Jonathan D.", ""], ["Mehta", "Dhagash", ""], ["Regan", "Margaret H.", ""], ["Tang", "Tingting", ""]]}, {"id": "2006.14102", "submitter": "Ethan Steinberg", "authors": "Ethan Steinberg, Steve Yadlowsky, Yizhe Xu, Nigam H. Shah", "title": "Using public clinical trial reports to evaluate observational study\n  methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Observational studies are valuable for estimating the effects of various\nmedical interventions, but are notoriously difficult to evaluate because the\nmethods used in observational studies require many untestable assumptions. This\nlack of verifiability makes it difficult both to compare different\nobservational study methods and to trust the results of any particular\nobservational study. In this work, we propose TrialVerify, a new approach for\nevaluating observational study methods based on ground truth sourced from\nclinical trial reports. We process trial reports into a denoised collection of\nknown causal relationships that can then be used to estimate the precision and\nrecall of various observational study methods. We then use TrialVerify to\nevaluate multiple observational study methods in terms of their ability to\nidentify the known causal relationships from a large national insurance claims\ndataset. We found that inverse propensity score weighting is an effective\napproach for accurately reproducing known causal relationships and outperforms\nother observational study methods. TrialVerify is made freely available for\nothers to evaluate observational study methods.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 23:38:32 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 22:52:33 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Steinberg", "Ethan", ""], ["Yadlowsky", "Steve", ""], ["Xu", "Yizhe", ""], ["Shah", "Nigam H.", ""]]}, {"id": "2006.14110", "submitter": "Filippo Pellegrino", "authors": "Thomas Hasenzagl, Filippo Pellegrino, Lucrezia Reichlin, Giovanni\n  Ricco", "title": "A Model of the Fed's View on Inflation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a medium-size semi-structural time series model of inflation\ndynamics that is consistent with the view - often expressed by central banks -\nthat three components are important: a trend anchored by long-run expectations,\na Phillips curve and temporary fluctuations in energy prices. We find that a\nstable long-term inflation trend and a well identified steep Phillips curve are\nconsistent with the data, but they imply potential output declining since the\nnew millennium and energy prices affecting headline inflation not only via the\nPhillips curve but also via an independent expectational channel. A\nhigh-frequency energy price cycle can be related to global factors affecting\nthe commodity market, and often overpowers the Phillips curve thereby\nexplaining the inflation puzzles of the last ten years.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 00:21:45 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Hasenzagl", "Thomas", ""], ["Pellegrino", "Filippo", ""], ["Reichlin", "Lucrezia", ""], ["Ricco", "Giovanni", ""]]}, {"id": "2006.14131", "submitter": "Han Lin Shang", "authors": "Han Lin Shang and Steven Haberman", "title": "Retiree mortality forecasting: A partial age-range or a full age-range\n  model?", "comments": "18 pages, 5 tables", "journal-ref": "Risks, 2020, 8(3), 69", "doi": "10.3390/risks8030069", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An essential input of annuity pricing is the future retiree mortality. From\nobserved age-specific mortality data, modeling and forecasting can be taken\nplace in two routes. On the one hand, we can first truncate the available data\nto retiree ages and then produce mortality forecasts based on a partial\nage-range model. On the other hand, with all available data, we can first apply\na full age-range model to produce forecasts and then truncate the mortality\nforecasts to retiree ages. We investigate the difference in modeling the\nlogarithmic transformation of the central mortality rates between a partial\nage-range and a full age-range model, using data from mainly developed\ncountries in the Human Mortality Database (2020). By evaluating and comparing\nthe short-term point and interval forecast accuracies, we recommend the first\nstrategy by truncating all available data to retiree ages and then produce\nmortality forecasts. However, when considering the long-term forecasts, it is\nunclear which strategy is better since it is more difficult to find a model and\nparameters that are optimal. This is a disadvantage of using methods based on\ntime series extrapolation for long-term forecasting. Instead, an expectation\napproach, in which experts set a future target, could be considered, noting\nthat this method has also had limited success in the past.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 01:37:50 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Shang", "Han Lin", ""], ["Haberman", "Steven", ""]]}, {"id": "2006.14137", "submitter": "Yujie Hu", "authors": "Yi-Jie Zhu, Yujie Hu, Jennifer M. Collins", "title": "Estimating Road Network Accessibility during a Hurricane Evacuation: A\n  Case Study of Hurricane Irma in Florida", "comments": null, "journal-ref": "Transportation Research Part D: Transport and Environment, 83,\n  102334 (2020)", "doi": "10.1016/j.trd.2020.102334", "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding the spatiotemporal road network accessibility during a\nhurricane evacuation, the level of ease of residents in an area in reaching\nevacuation destination sites through the road network, is a critical component\nof emergency management. While many studies have attempted to measure road\naccessibility (either in the scope of evacuation or beyond), few have\nconsidered both dynamic evacuation demand and characteristics of a hurricane.\nThis study proposes a methodological framework to achieve this goal. In an\ninterval of every six hours, the method first estimates the evacuation demand\nin terms of number of vehicles per household in each county subdivision by\nconsidering the hurricane's wind radius and track. The closest facility\nanalysis is then employed to model evacuees' route choices towards the\npredefined evacuation destinations. The potential crowdedness index (PCI), a\nmetric capturing the level of crowdedness of each road segment, is then\ncomputed by coupling the estimated evacuation demand and route choices.\nFinally, the road accessibility of each sub-county is measured by calculating\nthe reciprocal of the sum of PCI values of corresponding roads connecting\nevacuees from the sub-county to the designated destinations. The method is\napplied to the entire state of Florida during Hurricane Irma in September 2017.\nResults show that I-75 and I-95 northbound have a high level of congestion, and\nsub-counties along the northbound I-95 suffer from the worst road\naccessibility. In addition, this research performs a sensitivity analysis for\nexamining the impacts of different choices of behavioral response curves on\naccessibility results.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 02:17:22 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Zhu", "Yi-Jie", ""], ["Hu", "Yujie", ""], ["Collins", "Jennifer M.", ""]]}, {"id": "2006.14138", "submitter": "Yujie Hu", "authors": "Yujie Hu, Changzhen Wang, Ruiyang Li, Fahui Wang", "title": "Estimating a Large Drive Time Matrix between Zip Codes in the United\n  States: A Differential Sampling Approach", "comments": null, "journal-ref": "Journal of Transport Geography, 86, 102770 (2020)", "doi": "10.1016/j.jtrangeo.2020.102770", "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Estimating a massive drive time matrix between locations is a practical but\nchallenging task. The challenges include availability of reliable road network\n(including traffic) data, programming expertise, and access to high-performance\ncomputing resources. This research proposes a method for estimating a\nnationwide drive time matrix between ZIP code areas in the U.S.--a geographic\nunit at which many national datasets such as health information are compiled\nand distributed. The method (1) does not rely on intensive efforts in data\npreparation or access to advanced computing resources, (2) uses algorithms of\nvarying complexity and computational time to estimate drive times of different\ntrip lengths, and (3) accounts for both interzonal and intrazonal drive times.\nThe core design samples ZIP code pairs with various intensities according to\ntrip lengths and derives the drive times via Google Maps API, and the Google\ntimes are then used to adjust and improve some primitive estimates of drive\ntimes with low computational costs. The result provides a valuable resource for\nresearchers.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 02:20:44 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Hu", "Yujie", ""], ["Wang", "Changzhen", ""], ["Li", "Ruiyang", ""], ["Wang", "Fahui", ""]]}, {"id": "2006.14186", "submitter": "Soubhik Deb", "authors": "Yifan Mao, Soubhik Deb, Shaileshh Bojja Venkatakrishnan, Sreeram\n  Kannan, Kannan Srinivasan", "title": "Perigee: Efficient Peer-to-Peer Network Design for Blockchains", "comments": "Accepted at ACM PODC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.MA math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key performance metric in blockchains is the latency between when a\ntransaction is broadcast and when it is confirmed (the so-called, confirmation\nlatency). While improvements in consensus techniques can lead to lower\nconfirmation latency, a fundamental lower bound on confirmation latency is the\npropagation latency of messages through the underlying peer-to-peer (p2p)\nnetwork (inBitcoin, the propagation latency is several tens of seconds). The de\nfacto p2p protocol used by Bitcoin and other blockchains is based on random\nconnectivity: each node connects to a random subset of nodes. The induced p2p\nnetwork topology can be highly suboptimal since it neglects geographical\ndistance, differences in bandwidth, hash-power and computational abilities\nacross peers. We present Perigee, a decentralized algorithm that automatically\nlearns an efficient p2p topology tuned to the aforementioned network\nheterogeneities, purely based on peers' interactions with their neighbors.\nMotivated by the literature on the multi-armed bandit problem, Perigee\noptimally balances the tradeoff between retaining connections to known\nwell-connected neighbors, and exploring new connections to previously-unseen\nneighbors. Experimental evaluations show that Perigee reduces the latency to\nbroadcast by $33\\%$. Lastly Perigee is simple, computationally lightweight,\nadversary-resistant, and compatible with the selfish interests of peers, making\nit an attractive p2p protocol for blockchains.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 05:24:11 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Mao", "Yifan", ""], ["Deb", "Soubhik", ""], ["Venkatakrishnan", "Shaileshh Bojja", ""], ["Kannan", "Sreeram", ""], ["Srinivasan", "Kannan", ""]]}, {"id": "2006.14188", "submitter": "David Uminsky", "authors": "Stephen Devlin, David Uminsky", "title": "Identifying group contributions in NBA lineups with spectral analysis", "comments": "To appear in Journal of Sports Analytics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the question of how to quantify the contributions of groups of\nplayers to team success. Our approach is based on spectral analysis, a\ntechnique from algebraic signal processing, which has several appealing\nfeatures. First, our analysis decomposes the team success signal into\ncomponents that are naturally understood as the contributions of player groups\nof a given size: individuals, pairs, triples, fours, and full five-player\nlineups. Secondly, the decomposition is orthogonal so that contributions of a\nplayer group can be thought of as pure: Contributions attributed to a group of\nthree, for example, have been separated from the lower-order contributions of\nconstituent pairs and individuals. We present detailed a spectral analysis\nusing NBA play-by-play data and show how this can be a practical tool in\nunderstanding lineup composition and utilization.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 05:38:08 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Devlin", "Stephen", ""], ["Uminsky", "David", ""]]}, {"id": "2006.14411", "submitter": "Elizabeth Pei-Ting Chou", "authors": "Fushing Hsieh, Elizabeth P. Chou", "title": "Categorical Exploratory Data Analysis: From Multiclass Classification\n  and Response Manifold Analytics perspectives of baseball pitching dynamics", "comments": null, "journal-ref": null, "doi": "10.3390/e23070792", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From two coupled Multiclass Classification (MCC) and Response Manifold\nAnalytics (RMA) perspectives, we develop Categorical Exploratory Data Analysis\n(CEDA) on PITCHf/x database for the information content of Major League\nBaseball's (MLB) pitching dynamics. MCC and RMA information contents are\nrepresented by one collection of multi-scales pattern categories from mixing\ngeometries and one collection of global-to-local geometric localities from\nresponse-covariate manifolds, respectively. These collectives shed light on the\npitching dynamics and maps out uncertainty of popular machine learning\napproaches. On MCC setting, an indirect-distance-measure based label embedding\ntree leads to discover asymmetry of mixing geometries among labels'\npoint-clouds. A selected chain of complementary covariate feature groups\ncollectively brings out multi-order mixing geometric pattern categories. Such\ncategories then reveal the true nature of MCC predictive inferences. On RMA\nsetting, multiple response features couple with multiple major covariate\nfeatures to demonstrate physical principles bearing manifolds with a lattice of\nnatural localities. With minor features' heterogeneous effects being locally\nidentified, such localities jointly weave their focal characteristics into\nsystem understanding and provide a platform for RMA predictive inferences. Our\nCEDA works for universal data types, adopts non-linear associations and\nfacilitates efficient feature-selections and inferences.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 13:50:12 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Hsieh", "Fushing", ""], ["Chou", "Elizabeth P.", ""]]}, {"id": "2006.14609", "submitter": "Konstantinos Pelechrinis", "authors": "Konstantinos Pelechrinis, Wayne Winston", "title": "The Hot Hand in Actual Game Situations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Streaks of success have always fascinated people and a lot of research has\nbeen conducted to identify whether the \"hot hand\" effect is real. While sports\nhave provided an appropriate platform for studying this phenomenon, the\nmajority of existing literature examines scenarios in a vacuum with results\nthat might or might not be applicable in the wild. In this report, we build on\nthe existing literature and develop an appropriate framework to quantify the\nextend to which success can come in streaks -- beyond the stroke of chance --\nin a natural environment. Considering actual basketball game situations, our\nresults provide strong statistical evidence that the hot hand exists in this\nsetting. Even though our results are based on a sports setting, we believe that\nour study provides a path towards thinking of the hot hand outside of\nlaboratory-like, controlled environment. This is crucial if we want to use\nsimilar results to enhance our decision making and better understand short and\nlong term outcomes of repeated decisions.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 17:49:48 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 19:47:46 GMT"}, {"version": "v3", "created": "Mon, 19 Jul 2021 19:13:25 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Pelechrinis", "Konstantinos", ""], ["Winston", "Wayne", ""]]}, {"id": "2006.14979", "submitter": "Juan C. Correa", "authors": "J.C. Correa, H. Laverde-Rojas, F. Marmolejo-Ramos, J. Tejada, \\v{S}.\n  Bahn\\'ik", "title": "The Sci-hub Effect: Sci-hub downloads lead to more article citations", "comments": "19 pages, 8 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Citations are often used as a metric of the impact of scientific\npublications. Here, we examine how the number of downloads from Sci-hub as well\nas various characteristics of publications and their authors predicts future\ncitations. Using data from 12 leading journals in economics, consumer research,\nneuroscience, and multidisciplinary research, we found that articles downloaded\nfrom Sci-hub were cited 1.72 times more than papers not downloaded from Sci-hub\nand that the number of downloads from Sci-hub was a robust predictor of future\ncitations. Among other characteristics of publications, the number of figures\nin a manuscript consistently predicts its future citations. The results suggest\nthat limited access to publications may limit some scientific research from\nachieving its full impact.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 13:29:07 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 18:27:38 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Correa", "J. C.", ""], ["Laverde-Rojas", "H.", ""], ["Marmolejo-Ramos", "F.", ""], ["Tejada", "J.", ""], ["Bahn\u00edk", "\u0160.", ""]]}, {"id": "2006.14998", "submitter": "Qingliang Fan", "authors": "Qingliang Fan, Yaqian Wu", "title": "Endogenous Treatment Effect Estimation with some Invalid and Irrelevant\n  Instruments", "comments": "36 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instrumental variables (IV) regression is a popular method for the estimation\nof the endogenous treatment effects. Conventional IV methods require all the\ninstruments are relevant and valid. However, this is impractical especially in\nhigh-dimensional models when we consider a large set of candidate IVs. In this\npaper, we propose an IV estimator robust to the existence of both the invalid\nand irrelevant instruments (called R2IVE) for the estimation of endogenous\ntreatment effects. This paper extends the scope of Kang et al. (2016) by\nconsidering a true high-dimensional IV model and a nonparametric reduced form\nequation. It is shown that our procedure can select the relevant and valid\ninstruments consistently and the proposed R2IVE is root-n consistent and\nasymptotically normal. Monte Carlo simulations demonstrate that the R2IVE\nperforms favorably compared to the existing high-dimensional IV estimators\n(such as, NAIVE (Fan and Zhong, 2018) and sisVIVE (Kang et al., 2016)) when\ninvalid instruments exist. In the empirical study, we revisit the classic\nquestion of trade and growth (Frankel and Romer, 1999).\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 14:11:43 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Fan", "Qingliang", ""], ["Wu", "Yaqian", ""]]}, {"id": "2006.15030", "submitter": "Yue Wu", "authors": "Yue Wu and Terry J. Lyons and Kate E.A. Saunders", "title": "Deriving information from missing data: implications for mood prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of mobile technologies has enabled the efficient collection\nprospective longitudinal, ecologically valid self-reported mood data from\npsychiatric patients. These data streams have potential for improving the\nefficiency and accuracy of psychiatric diagnosis as well predicting future mood\nstates enabling earlier intervention. However, missing responses are common in\nsuch datasets and there is little consensus as to how this should be dealt with\nin practice. A signature-based method was used to capture different elements of\nself-reported mood alongside missing data to both classify diagnostic group and\npredict future mood in patients with bipolar disorder, borderline personality\ndisorder and healthy controls. The missing-response-incorporated\nsignature-based method achieves roughly 66\\% correct diagnosis, with f1 scores\nfor three different clinic groups 59\\% (bipolar disorder), 75\\% (healthy\ncontrol) and 61\\% (borderline personality disorder) respectively. This was\nsignificantly more efficient than the naive model which excluded missing data.\nAccuracies of predicting subsequent mood states and scores were also improved\nby inclusion of missing responses. The signature method provided an effective\napproach to the analysis of prospectively collected mood data where missing\ndata was common and should be considered as an approach in other similar\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 14:57:03 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 10:25:46 GMT"}, {"version": "v3", "created": "Wed, 8 Jul 2020 11:33:40 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Wu", "Yue", ""], ["Lyons", "Terry J.", ""], ["Saunders", "Kate E. A.", ""]]}, {"id": "2006.15077", "submitter": "Jonathan Von Schroeder", "authors": "Jonathan von Schroeder", "title": "Stable Feature Selection with Applications to MALDI Imaging Mass\n  Spectrometry Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses an approach, based on the subsampling boostrap and FDR\ncontrol, to improve the stability of feature selection. It furthermore presents\nthe finite sample distribution of the correlation coefficient recently proposed\nby Chatterjee (2020) under the setting relevant for this paper. Finally an\napplication to matrix-assisted laser desorption/ionization (MALDI) imaging mass\nspectroscopy data is discussed.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 16:15:37 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["von Schroeder", "Jonathan", ""]]}, {"id": "2006.15183", "submitter": "Francis Diebold", "authors": "Francis X. Diebold", "title": "Real-Time Real Economic Activity Entering the Pandemic Recession", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM econ.GN q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entering and exiting the Pandemic Recession, I study the high-frequency\nreal-activity signals provided by a leading nowcast, the ADS Index of Business\nConditions produced and released in real time by the Federal Reserve Bank of\nPhiladelphia. I track the evolution of real-time vintage beliefs and compare\nthem to a later-vintage chronology. Real-time ADS plunges and then swings as\nits underlying economic indicators swing, but the ADS paths quickly converge to\nindicate a return to brisk positive growth by mid-May. We show, moreover, that\ndaily real activity path was highly correlated with the daily COVID-19 cases.\nFinally, I provide a comparative assessment of the real-time ADS signals\nprovided when exiting the Great Recession.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 19:19:10 GMT"}, {"version": "v2", "created": "Sun, 1 Nov 2020 16:11:58 GMT"}, {"version": "v3", "created": "Thu, 6 May 2021 11:59:23 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Diebold", "Francis X.", ""]]}, {"id": "2006.15214", "submitter": "Ahmed Elsawah", "authors": "Zhongjun Wang, Mengye Sun, A. M. Elsawah", "title": "Improving MF-DFA model with applications in precious metals market", "comments": "23 pages, 17 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the aggravation of the global economic crisis and inflation, the\nprecious metals with safe-haven function have become more popular. An improved\nMF-DFA method is proposed to analyze price fluctuations of the precious metals\nmarket. Based on the widely used multifractal detrended fluctuation analysis\nmethod (MF-DFA), we compare these two methods and find that the Bi-OSW-MF-DFA\nmethod possesses better efficiency. This article analyzes the degree of\nmultifractality between spot gold market and spot silver market as well as\ntheir risks. From the numerical results and figures, it is found that two\nelements constitute the contributions in the formation of multifractality in\ntime series and the risk of the spot silver market is higher than that of the\nspot gold market. This attempt could lead to a better understanding of\ncomplicated precious metals market.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 21:07:39 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Wang", "Zhongjun", ""], ["Sun", "Mengye", ""], ["Elsawah", "A. M.", ""]]}, {"id": "2006.15283", "submitter": "Madan Kundu", "authors": "Madan G. Kundu and Shoubhik Mondal", "title": "Impact of the adjustment of stratification factors on time-to-event\n  analyses", "comments": "6 pages, 1 table, 0 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a stratified clinical trial design with time to event end points,\nstratification factors are often accounted for the log-rank test and the Cox\nregression analyses. In this work, we have evaluated the impact of inclusion of\nstratification factors on the power of the stratified log-rank test and have\ncompared the bias and standard error in HR estimate between multivariate and\nstratified Cox regression analyses through simulation. Results from our\ninvestigation suggests that both failing to consider stratification factor in\npresence of their prognostic effect and stratification with smaller number of\nevents may substantially reduce the power of the log-rank test. Further, the HR\nestimate from the multivariate Cox analysis is more accurate and precise\ncompared to the stratified Cox analysis. Our findings point towards the\nnecessity of evaluating the impact of stratification factors on the time to\nevent analyses at the time of study design which is presently not a norm.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 04:38:28 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Kundu", "Madan G.", ""], ["Mondal", "Shoubhik", ""]]}, {"id": "2006.15311", "submitter": "Rakshitha Godahewa", "authors": "Rakshitha Godahewa, Trevor Yann, Christoph Bergmeir, Francois\n  Petitjean", "title": "Seasonal Averaged One-Dependence Estimators: A Novel Algorithm to\n  Address Seasonal Concept Drift in High-Dimensional Stream Classification", "comments": null, "journal-ref": "International Joint Conference on Neural Networks (IJCNN 2020)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stream classification methods classify a continuous stream of data as new\nlabelled samples arrive. They often also have to deal with concept drift. This\npaper focuses on seasonal drift in stream classification, which can be found in\nmany real-world application data sources. Traditional approaches of stream\nclassification consider seasonal drift by including seasonal dummy/indicator\nvariables or building separate models for each season. But these approaches\nhave strong limitations in high-dimensional classification problems, or with\ncomplex seasonal patterns. This paper explores how to best handle seasonal\ndrift in the specific context of news article categorization (or\nclassification/tagging), where seasonal drift is overwhelmingly the main type\nof drift present in the data, and for which the data are high-dimensional. We\nintroduce a novel classifier named Seasonal Averaged One-Dependence Estimators\n(SAODE), which extends the AODE classifier to handle seasonal drift by\nincluding time as a super parent. We assess our SAODE model using two large\nreal-world text mining related datasets each comprising approximately a million\nrecords, against nine state-of-the-art stream and concept drift classification\nmodels, with and without seasonal indicators and with separate models built for\neach season. Across five different evaluation techniques, we show that our\nmodel consistently outperforms other methods by a large margin where the\nresults are statistically significant.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 08:04:53 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Godahewa", "Rakshitha", ""], ["Yann", "Trevor", ""], ["Bergmeir", "Christoph", ""], ["Petitjean", "Francois", ""]]}, {"id": "2006.15409", "submitter": "Chenchen Ma", "authors": "Chenchen Ma, Jimmy de la Torre, Gongjun Xu", "title": "Bridging Parametric and Nonparametric Methods in Cognitive Diagnosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of parametric and nonparametric methods for estimating cognitive\ndiagnosis models (CDMs) have been developed and applied in a wide range of\ncontexts. However, in the literature, a wide chasm exists between these two\nfamilies of methods, and their relationship to each other is not well\nunderstood. In this paper, we propose a unified estimation framework to bridge\nthe divide between parametric and nonparametric methods in cognitive diagnosis\nto better understand their relationship. We also develop iterative joint\nestimation algorithms and establish consistency properties within the proposed\nframework. Lastly, we present comprehensive simulation results to compare\ndifferent methods, and provide practical recommendations on the appropriate use\nof the proposed framework in various CDM contexts.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 17:26:34 GMT"}, {"version": "v2", "created": "Sun, 28 Feb 2021 21:32:16 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Ma", "Chenchen", ""], ["de la Torre", "Jimmy", ""], ["Xu", "Gongjun", ""]]}, {"id": "2006.15715", "submitter": "Kevin Kunzmann", "authors": "Kevin Kunzmann, Michael J. Grayling, Kim May Lee, David S. Robertson,\n  Kaspar Rufibach, James M. S. Wason", "title": "A review of Bayesian perspectives on sample size derivation for\n  confirmatory trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sample size derivation is a crucial element of the planning phase of any\nconfirmatory trial. A sample size is typically derived based on constraints on\nthe maximal acceptable type I error rate and a minimal desired power. Here,\npower depends on the unknown true effect size. In practice, power is typically\ncalculated either for the smallest relevant effect size or a likely point\nalternative. The former might be problematic if the minimal relevant effect is\nclose to the null, thus requiring an excessively large sample size. The latter\nis dubious since it does not account for the a priori uncertainty about the\nlikely alternative effect size. A Bayesian perspective on the sample size\nderivation for a frequentist trial naturally emerges as a way of reconciling\narguments about the relative a priori plausibility of alternative effect sizes\nwith ideas based on the relevance of effect sizes. Many suggestions as to how\nsuch `hybrid' approaches could be implemented in practice have been put forward\nin the literature. However, key quantities such as assurance, probability of\nsuccess, or expected power are often defined in subtly different ways in the\nliterature. Starting from the traditional and entirely frequentist approach to\nsample size derivation, we derive consistent definitions for the most commonly\nused `hybrid' quantities and highlight connections, before discussing and\ndemonstrating their use in the context of sample size derivation for clinical\ntrials.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 21:13:55 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Kunzmann", "Kevin", ""], ["Grayling", "Michael J.", ""], ["Lee", "Kim May", ""], ["Robertson", "David S.", ""], ["Rufibach", "Kaspar", ""], ["Wason", "James M. S.", ""]]}, {"id": "2006.15935", "submitter": "Stefano M. Iacus", "authors": "Tiziana Carpi and Stefano Maria Iacus", "title": "Is Japanese gendered language used on Twitter ? A large scale study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study analyzes the usage of Japanese gendered language on Twitter.\nStarting from a collection of 408 million Japanese tweets from 2015 till 2019\nand an additional sample of 2355 manually classified Twitter accounts timelines\ninto gender and categories (politicians, musicians, etc). A large scale textual\nanalysis is performed on this corpus to identify and examine sentence-final\nparticles (SFPs) and first-person pronouns appearing in the texts. It turns out\nthat gendered language is in fact used also on Twitter, in about 6% of the\ntweets, and that the prescriptive classification into \"male\" and \"female\"\nlanguage does not always meet the expectations, with remarkable exceptions.\nFurther, SFPs and pronouns show increasing or decreasing trends, indicating an\nevolution of the language used on Twitter.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 11:07:10 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 08:59:17 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Carpi", "Tiziana", ""], ["Iacus", "Stefano Maria", ""]]}, {"id": "2006.16051", "submitter": "Antonio Calcagn\\`i", "authors": "Antonio Calcagn\\`i and Luigi Lombardi", "title": "Modeling random and non-random decision uncertainty in ratings data: A\n  fuzzy beta model", "comments": "29 pages, 4 figures, 7 tables", "journal-ref": "AStA Advances in Statistical Analysis, Springer, 2021", "doi": "10.1007/s10182-021-00407-7", "report-no": null, "categories": "stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling human ratings data subject to raters' decision uncertainty is an\nattractive problem in applied statistics. In view of the complex interplay\nbetween emotion and decision making in rating processes, final raters' choices\nseldom reflect the true underlying raters' responses. Rather, they are\nimprecisely observed in the sense that they are subject to a non-random\ncomponent of uncertainty, namely the decision uncertainty. The purpose of this\narticle is to illustrate a statistical approach to analyse ratings data which\nintegrates both random and non-random components of the rating process. In\nparticular, beta fuzzy numbers are used to model raters' non-random decision\nuncertainty and a variable dispersion beta linear model is instead adopted to\nmodel the random counterpart of rating responses. The main idea is to quantify\ncharacteristics of latent and non-fuzzy rating responses by means of random\nobservations subject to fuzziness. To do so, a fuzzy version of the\nExpectation-Maximization algorithm is adopted to both estimate model's\nparameters and compute their standard errors. Finally, the characteristics of\nthe proposed fuzzy beta model are investigated by means of a simulation study\nas well as two case studies from behavioral and social contexts.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 13:41:28 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 06:46:19 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Calcagn\u00ec", "Antonio", ""], ["Lombardi", "Luigi", ""]]}, {"id": "2006.16059", "submitter": "Ritabrata Dutta", "authors": "Ritabrata Dutta and Susana Gomes and Dante Kalise and Lorenzo\n  Pacchiardi", "title": "Using mobility data in the design of optimal lockdown strategies for the\n  COVID-19 pandemic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mathematical model for the COVID-19 pandemic spread, which integrates\nage-structured Susceptible-Exposed-Infected-Recovered-Deceased dynamics with\nreal mobile phone data accounting for the population mobility, is presented.\nThe dynamical model adjustment is performed via Approximate Bayesian\nComputation. Optimal lockdown and exit strategies are determined based on\nnonlinear model predictive control, constrained to public-health and\nsocio-economic factors. Through an extensive computational validation of the\nmethodology, it is shown that it is possible to compute robust exit strategies\nwith realistic reduced mobility values to inform public policy making, and we\nexemplify the applicability of the methodology using datasets from England and\nFrance. Code implementing the described experiments is available at\nhttps://github.com/OptimalLockdown.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 13:58:19 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2020 12:40:16 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Dutta", "Ritabrata", ""], ["Gomes", "Susana", ""], ["Kalise", "Dante", ""], ["Pacchiardi", "Lorenzo", ""]]}, {"id": "2006.16063", "submitter": "Matteo Iacopini", "authors": "Carlo Romano Marcello Alessandro Santagiustina and Matteo Iacopini", "title": "Visualizing and comparing distributions with half-disk density strips", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a user-friendly graphical tool, the half-disk density strip\n(HDDS), for visualizing and comparing probability density functions. The HDDS\nexploits color shading for representing a distribution in an intuitive way. In\nunivariate settings, the half-disk density strip allows to immediately discern\nthe key characteristics of a density, such as symmetry, dispersion, and\nmulti-modality. In the multivariate settings, we define HDDS tables to\ngeneralize the concept of contingency tables. It is an array of half-disk\ndensity strips, which compactly displays the univariate marginal and\nconditional densities of a variable of interest, together with the joint and\nmarginal densities of the conditioning variables. Moreover, HDDSs are by\nconstruction well suited to easily compare pairs of densities. To highlight the\nconcrete benefits of the proposed methods, we show how to use HDDSs for\nanalyzing income distribution and life-satisfaction, conditionally on\ncontinuous and categorical controls, from survey data. The code for\nimplementing HDDS methods is made available through a dedicated R package.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 14:10:50 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Santagiustina", "Carlo Romano Marcello Alessandro", ""], ["Iacopini", "Matteo", ""]]}, {"id": "2006.16214", "submitter": "Panos Toulis", "authors": "Panos Toulis", "title": "Estimation of Covid-19 Prevalence from Serology Tests: A Partial\n  Identification Approach", "comments": "Journal of Econometrics (2020), forthcoming", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a partial identification method for estimating disease prevalence\nfrom serology studies. Our data are results from antibody tests in some\npopulation sample, where the test parameters, such as the true/false positive\nrates, are unknown. Our method scans the entire parameter space, and rejects\nparameter values using the joint data density as the test statistic. The\nproposed method is conservative for marginal inference, in general, but its key\nadvantage over more standard approaches is that it is valid in finite samples\neven when the underlying model is not point identified. Moreover, our method\nrequires only independence of serology test results, and does not rely on\nasymptotic arguments, normality assumptions, or other approximations. We use\nrecent Covid-19 serology studies in the US, and show that the parameter\nconfidence set is generally wide, and cannot support definite conclusions.\nSpecifically, recent serology studies from California suggest a prevalence\nanywhere in the range 0%-2% (at the time of study), and are therefore\ninconclusive. However, this range could be narrowed down to 0.7%-1.5% if the\nactual false positive rate of the antibody test was indeed near its empirical\nestimate (~0.5%). In another study from New York state, Covid-19 prevalence is\nconfidently estimated in the range 13%-17% in mid-April of 2020, which also\nsuggests significant geographic variation in Covid-19 exposure across the US.\nCombining all datasets yields a 5%-8% prevalence range. Our results overall\nsuggest that serology testing on a massive scale can give crucial information\nfor future policy design, even when such tests are imperfect and their\nparameters unknown.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 17:25:25 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Toulis", "Panos", ""]]}, {"id": "2006.16464", "submitter": "Johan Koskinen", "authors": "Johan Koskinen and Galina Daraganova", "title": "Bayesian Analysis of Social Influence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The network influence model is a model for binary outcome variables that\naccounts for dependencies between outcomes for units that are relationally\ntied. The basic influence model was previously extended to afford a suite of\nnew dependence assumptions and because of its relation to traditional Markov\nrandom field models it is often referred to as the auto logistic\nactor-attribute model (ALAAM). We extend on current approaches for fitting\nALAAMs by presenting a comprehensive Bayesian inference scheme that supports\ntesting of dependencies across subsets of data and the presence of missing\ndata. We illustrate different aspects of the procedures through three empirical\nexamples: masculinity attitudes in an all-male Australian school class,\neducational progression in Swedish schools, and un-employment among adults in a\ncommunity sample in Australia.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 01:36:25 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Koskinen", "Johan", ""], ["Daraganova", "Galina", ""]]}, {"id": "2006.16487", "submitter": "Thomas Mellan", "authors": "Swapnil Mishra, Tresnia Berah, Thomas A. Mellan, H. Juliette T. Unwin,\n  Michaela A Vollmer, Kris V Parag, Axel Gandy, Seth Flaxman, Samir Bhatt", "title": "On the derivation of the renewal equation from an age-dependent\n  branching process: an epidemic modelling perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Renewal processes are a popular approach used in modelling infectious disease\noutbreaks. In a renewal process, previous infections give rise to future\ninfections. However, while this formulation seems sensible, its application to\ninfectious disease can be difficult to justify from first principles. It has\nbeen shown from the seminal work of Bellman and Harris that the renewal\nequation arises as the expectation of an age-dependent branching process. In\nthis paper we provide a detailed derivation of the original Bellman Harris\nprocess. We introduce generalisations, that allow for time-varying reproduction\nnumbers and the accounting of exogenous events, such as importations. We show\nhow inference on the renewal equation is easy to accomplish within a Bayesian\nhierarchical framework. Using off the shelf MCMC packages, we fit to South\nKorea COVID-19 case data to estimate reproduction numbers and importations. Our\nderivation provides the mathematical fundamentals and assumptions underpinning\nthe use of the renewal equation for modelling outbreaks.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 02:42:06 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Mishra", "Swapnil", ""], ["Berah", "Tresnia", ""], ["Mellan", "Thomas A.", ""], ["Unwin", "H. Juliette T.", ""], ["Vollmer", "Michaela A", ""], ["Parag", "Kris V", ""], ["Gandy", "Axel", ""], ["Flaxman", "Seth", ""], ["Bhatt", "Samir", ""]]}, {"id": "2006.16504", "submitter": "Prabir Barooah", "authors": "Duzgun Agdas and Prabir Barooah", "title": "The COVID-19 pandemic's impact on U.S. electricity demand and supply: an\n  early view from the data", "comments": null, "journal-ref": null, "doi": "10.1109/ACCESS.2020.3016912", "report-no": null, "categories": "stat.AP cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After the onset of the recent COVID-19 pandemic, a number of studies reported\non possible changes in electricity consumption trends. The overall theme of\nthese reports was that ``electricity use has decreased during the pandemic, but\nthe power grid is still reliable''---mostly due to reduced economic activity.\nIn this paper we analyze electricity data upto end of May 2020, examining both\nelectricity demand and variables that can indicate stress on the power grid,\nsuch as peak demand and demand ramp-rate. We limit this study to three states\nin the USA: New York, California, and Florida. The results indicate that the\neffect of the pandemic on electricity demand is not a simple reduction from\ncomparable time frames, and there are noticeable differences among regions. The\nvariables that can indicate stress on the grid also conveyed mixed messages:\nsome indicate an increase in stress, some indicate a decrease, and some do not\nindicate any clear difference. A positive message is that some of the changes\nthat were observed around the time stay-at-home orders were issued appeared to\nrevert back by May 2020. A key challenge in ascribing any observed change to\nthe pandemic is correcting for weather. We provide a weather-correction method,\napply it to a small city-wide area, and discuss the implications of the\nestimated changes in demand. The weather correction exercise underscored that\nweather-correction is as challenging as it is important.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 03:25:23 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Agdas", "Duzgun", ""], ["Barooah", "Prabir", ""]]}, {"id": "2006.16509", "submitter": "Michael Lingzhi Li", "authors": "Dimitris Bertsimas, L\\'eonard Boussioux, Ryan Cory Wright, Arthur\n  Delarue, Vassilis Digalakis Jr., Alexandre Jacquillat, Driss Lahlou Kitane,\n  Galit Lukin, Michael Lingzhi Li, Luca Mingardi, Omid Nohadani, Agni\n  Orfanoudaki, Theodore Papalexopoulos, Ivan Paskov, Jean Pauphilet, Omar Skali\n  Lami, Bartolomeo Stellato, Hamza Tazi Bouardi, Kimberly Villalobos Carballo,\n  Holly Wiberg, Cynthia Zeng", "title": "From predictions to prescriptions: A data-driven response to COVID-19", "comments": "Submitted to PNAS", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.OC q-bio.PE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The COVID-19 pandemic has created unprecedented challenges worldwide.\nStrained healthcare providers make difficult decisions on patient triage,\ntreatment and care management on a daily basis. Policy makers have imposed\nsocial distancing measures to slow the disease, at a steep economic price. We\ndesign analytical tools to support these decisions and combat the pandemic.\nSpecifically, we propose a comprehensive data-driven approach to understand the\nclinical characteristics of COVID-19, predict its mortality, forecast its\nevolution, and ultimately alleviate its impact. By leveraging cohort-level\nclinical data, patient-level hospital data, and census-level epidemiological\ndata, we develop an integrated four-step approach, combining descriptive,\npredictive and prescriptive analytics. First, we aggregate hundreds of clinical\nstudies into the most comprehensive database on COVID-19 to paint a new\nmacroscopic picture of the disease. Second, we build personalized calculators\nto predict the risk of infection and mortality as a function of demographics,\nsymptoms, comorbidities, and lab values. Third, we develop a novel\nepidemiological model to project the pandemic's spread and inform social\ndistancing policies. Fourth, we propose an optimization model to re-allocate\nventilators and alleviate shortages. Our results have been used at the clinical\nlevel by several hospitals to triage patients, guide care management, plan ICU\ncapacity, and re-distribute ventilators. At the policy level, they are\ncurrently supporting safe back-to-work policies at a major institution and\nequitable vaccine distribution planning at a major pharmaceutical company, and\nhave been integrated into the US Center for Disease Control's pandemic\nforecast.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 03:34:00 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Bertsimas", "Dimitris", ""], ["Boussioux", "L\u00e9onard", ""], ["Wright", "Ryan Cory", ""], ["Delarue", "Arthur", ""], ["Digalakis", "Vassilis", "Jr."], ["Jacquillat", "Alexandre", ""], ["Kitane", "Driss Lahlou", ""], ["Lukin", "Galit", ""], ["Li", "Michael Lingzhi", ""], ["Mingardi", "Luca", ""], ["Nohadani", "Omid", ""], ["Orfanoudaki", "Agni", ""], ["Papalexopoulos", "Theodore", ""], ["Paskov", "Ivan", ""], ["Pauphilet", "Jean", ""], ["Lami", "Omar Skali", ""], ["Stellato", "Bartolomeo", ""], ["Bouardi", "Hamza Tazi", ""], ["Carballo", "Kimberly Villalobos", ""], ["Wiberg", "Holly", ""], ["Zeng", "Cynthia", ""]]}, {"id": "2006.16648", "submitter": "Jian Ma", "authors": "Jian Ma", "title": "Associations between finger tapping, gait and fall risk with application\n  to fall risk assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the world ages, elderly care becomes a big concern of the society. To\naddress the elderly's issues on dementia and fall risk, we have investigated\nsmart cognitive and fall risk assessment with machine learning methodology\nbased on the data collected from finger tapping test and Timed Up and Go (TUG)\ntest. Meanwhile, we have discovered the associations between cognition and\nfinger motion from finger tapping data and the association between fall risk\nand gait characteristics from TUG data. In this paper, we jointly analyze the\nfinger tapping and gait characteristics data with copula entropy. We find that\nthe associations between certain finger tapping characteristics ('number of\ntaps', 'average interval of tapping', 'frequency of tapping' of both hands of\nbimanual inphase and those of left hand of bimanual untiphase) and TUG score\nare relatively high. According to this finding, we propose to utilize this\nassociations to improve the predictive models of automatic fall risk assessment\nwe developed previously. Experimental results show that using the\ncharacteristics of both finger tapping and gait as inputs of the predictive\nmodels of predicting TUG score can considerably improve the prediction\nperformance in terms of MAE compared with using only one type of\ncharacteristics.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 10:17:41 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 10:50:59 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Ma", "Jian", ""]]}, {"id": "2006.16761", "submitter": "Andres Colubri", "authors": "Andres Colubri, Kailash Yadav, Abhishek Jha, Pardis C. Sabeti", "title": "Individual-level Modeling of COVID-19 Epidemic Risk", "comments": "16 pages, 5 figures. arXiv admin note: text overlap with\n  arXiv:1908.06822 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ongoing COVID-19 pandemic calls for a multi-faceted public health\nresponse comprising complementary interventions to control the spread of the\ndisease while vaccines and therapies are developed. Many of these interventions\nneed to be informed by epidemic risk predictions given available data,\nincluding symptoms, contact patterns, and environmental factors. Here we\npropose a novel probabilistic formalism based on Individual-Level Models (ILMs)\nthat offers rigorous formulas for the probability of infection of individuals,\nwhich can be parameterised via Maximum Likelihood Estimation (MLE) applied on\ncompartmental models defined at the population level. We describe an approach\nwhere individual data collected in real-time is integrated with overall case\ncounts to update the a predictor of the susceptibility of infection as a\nfunction of individual risk factors.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 14:51:00 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2020 23:54:43 GMT"}, {"version": "v3", "created": "Fri, 7 Aug 2020 06:33:03 GMT"}, {"version": "v4", "created": "Fri, 21 Aug 2020 03:34:42 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Colubri", "Andres", ""], ["Yadav", "Kailash", ""], ["Jha", "Abhishek", ""], ["Sabeti", "Pardis C.", ""]]}, {"id": "2006.16865", "submitter": "Luigi Lombardo", "authors": "Nan Wang, Luigi Lombardo, Marj Tonini, Weiming Cheng, Liang Guo,\n  Junnan Xiong", "title": "Space-time clustering of flash floods in a changing climate (China,\n  1950-2015)", "comments": null, "journal-ref": null, "doi": "10.5194/nhess-21-2109-2021", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The persistence over space and time of flash flood disasters -- flash floods\nthat have caused either economical or life losses, or both -- is a diagnostic\nmeasure of areas subjected to hydrological risk. The concept of persistence can\nbe assessed via clustering analyses, performed here to analyse the national\ninventory of flash floods disasters in China occurred in the period 1950-2015.\nSpecifically, we investigated the spatio-temporal pattern distribution of the\nflash floods and their clustering behavior by using both global and local\nmethods: the first, based on the Ripley's K-function, and the second on scan\nstatistics. As a result, we could visualize patterns of aggregated events,\nestimate the cluster duration and make assumptions about their evolution over\ntime, also with respect precipitations trend. Due to the large spatial (the\nwhole Chinese territory) and temporal scale of the dataset (66 years), we were\nable to capture whether certain clusters gather in specific locations and\ntimes, but also whether their magnitude tends to increase or decrease. Overall,\nthe eastern regions in China are much more subjected to flash floods compared\nto the rest of the country. Detected clusters revealed that these phenomena\npredominantly occur between July and October, a period coinciding with the wet\nseason in China. The number of detected clusters increases with time, but the\nassociated duration drastically decreases in the recent period. This may\nindicate a change towards triggering mechanisms which are typical of\nshort-duration extreme rainfall events. Finally, being flash floods directly\nlinked to precipitation and their extreme realization, we indirectly assessed\nwhether the magnitude of the trigger itself has also varied through space and\ntime, enabling considerations in the context of climatic changes.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 08:39:26 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Wang", "Nan", ""], ["Lombardo", "Luigi", ""], ["Tonini", "Marj", ""], ["Cheng", "Weiming", ""], ["Guo", "Liang", ""], ["Xiong", "Junnan", ""]]}, {"id": "2006.16923", "submitter": "Vinay Prabhu", "authors": "Vinay Uday Prabhu, Abeba Birhane", "title": "Large image datasets: A pyrrhic win for computer vision?", "comments": "Github: https://github.com/vinayprabhu/Dataset_audits. Update on July\n  23rd: (1) Added in the supplementary section (2) The curators of the Tiny\n  Images dataset decided to withdraw the dataset in response to the previous\n  version of this paper, a change that has duly been reflected in this version.\n  Their statement: https://groups.csail.mit.edu/vision/TinyImages/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate problematic practices and consequences of large\nscale vision datasets. We examine broad issues such as the question of consent\nand justice as well as specific concerns such as the inclusion of verifiably\npornographic images in datasets. Taking the ImageNet-ILSVRC-2012 dataset as an\nexample, we perform a cross-sectional model-based quantitative census covering\nfactors such as age, gender, NSFW content scoring, class-wise accuracy,\nhuman-cardinality-analysis, and the semanticity of the image class information\nin order to statistically investigate the extent and subtleties of ethical\ntransgressions. We then use the census to help hand-curate a look-up-table of\nimages in the ImageNet-ILSVRC-2012 dataset that fall into the categories of\nverifiably pornographic: shot in a non-consensual setting (up-skirt), beach\nvoyeuristic, and exposed private parts. We survey the landscape of harm and\nthreats both society broadly and individuals face due to uncritical and\nill-considered dataset curation practices. We then propose possible courses of\ncorrection and critique the pros and cons of these. We have duly open-sourced\nall of the code and the census meta-datasets generated in this endeavor for the\ncomputer vision community to build on. By unveiling the severity of the\nthreats, our hope is to motivate the constitution of mandatory Institutional\nReview Boards (IRB) for large scale dataset curation processes.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 06:41:32 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2020 02:55:13 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Prabhu", "Vinay Uday", ""], ["Birhane", "Abeba", ""]]}, {"id": "2006.16941", "submitter": "Saeed Khaki", "authors": "Saeed Khaki and Dan Nettleton", "title": "Conformal Prediction Intervals for Neural Networks Using Cross\n  Validation", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are among the most powerful nonlinear models used to address\nsupervised learning problems. Similar to most machine learning algorithms,\nneural networks produce point predictions and do not provide any prediction\ninterval which includes an unobserved response value with a specified\nprobability. In this paper, we proposed the $k$-fold prediction interval method\nto construct prediction intervals for neural networks based on $k$-fold cross\nvalidation. Simulation studies and analysis of 10 real datasets are used to\ncompare the finite-sample properties of the prediction intervals produced by\nthe proposed method and the split conformal (SC) method. The results suggest\nthat the proposed method tends to produce narrower prediction intervals\ncompared to the SC method while maintaining the same coverage probability. Our\nexperimental results also reveal that the proposed $k$-fold prediction interval\nmethod produces effective prediction intervals and is especially advantageous\nrelative to competing approaches when the number of training observations is\nlimited.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 16:23:28 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Khaki", "Saeed", ""], ["Nettleton", "Dan", ""]]}, {"id": "2006.16942", "submitter": "Feng Zhou", "authors": "Feng Zhou, Tao Chen, and Baiying Lei", "title": "Do not forget interaction: Predicting fatality of COVID-19 patients\n  using logistic regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Amid the ongoing COVID-19 pandemic, whether COVID-19 patients with high risks\ncan be recovered or not depends, to a large extent, on how early they will be\ntreated appropriately before irreversible consequences are caused to the\npatients by the virus. In this research, we reported an explainable, intuitive,\nand accurate machine learning model based on logistic regression to predict the\nfatality rate of COVID-19 patients using only three important blood biomarkers,\nincluding lactic dehydrogenase, lymphocyte (%) and high-sensitivity C-reactive\nprotein, and their interactions. We found that when the fatality probability\nproduced by the logistic regression model was over 0.8, the model had the\noptimal performance in that it was able to predict patient fatalities more than\n11.30 days on average with maximally 34.91 days in advance, an accumulative\nf1-score of 93.76% and and an accumulative accuracy score of 93.92%. Such a\nmodel can be used to identify COVID-19 patients with high risks with three\nblood biomarkers and help the medical systems around the world plan critical\nmedical resources amid this pandemic.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 16:28:41 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Zhou", "Feng", ""], ["Chen", "Tao", ""], ["Lei", "Baiying", ""]]}, {"id": "2006.16982", "submitter": "Trevor Hefley", "authors": "Trevor J. Hefley, Robin E. Russell, Anne E. Ballmann, Haoyu Zhang", "title": "When and where: estimating the date and location of introduction for\n  exotic pests and pathogens", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental question during the outbreak of a novel disease or invasion of\nan exotic pest is: At what location and date was it first introduced? With this\ninformation, future introductions can be anticipated and perhaps avoided. Point\nprocess models are commonly used for mapping species distribution and disease\noccurrence. If the time and location of introductions were known, then point\nprocess models could be used to map and understand the factors that influence\nintroductions; however, rarely is the process of introduction directly\nobserved. We propose embedding a point process within hierarchical Bayesian\nmodels commonly used to understand the spatio-temporal dynamics of invasion.\nIncluding a point process within a hierarchical Bayesian model enables\ninference regarding the location and date of introduction from indirect\nobservation of the process such as species or disease occurrence records. We\nillustrate our approach using disease surveillance data collected to monitor\nwhite-nose syndrome, which is a fungal disease that threatens many North\nAmerican species of bats. We use our model and surveillance data to estimate\nthe location and date that the pathogen was introduced into the United States.\nFinally, we compare forecasts from our model to forecasts obtained from\nstate-of-the-art regression-based statistical and machine learning methods. Our\nresults show that the pathogen causing white-nose syndrome was most likely\nintroduced into the United States 4 years prior to the first detection, but\nthere is a moderate level of uncertainty in this estimate. The location of\nintroduction could be up to 510 km east of the location of first discovery, but\nour results indicate that there is a relatively high probability the location\nof first detection could be the location of introduction.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 17:29:07 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Hefley", "Trevor J.", ""], ["Russell", "Robin E.", ""], ["Ballmann", "Anne E.", ""], ["Zhang", "Haoyu", ""]]}]