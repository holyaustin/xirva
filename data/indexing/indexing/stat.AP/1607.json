[{"id": "1607.00046", "submitter": "Yan Yuan", "authors": "Jian Yong, Sohaib H. Mohammad, Yan Yuan", "title": "A Two-Stage Patient-Focused Study Design for Rare Disease Controlled\n  Trials", "comments": "18 pages, 3 Table and 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We developed a study design for rare disease clinical trials (RDTs) that\nefficiently evaluate treatments, promotes access to new treatments during\ntreatment development, and optimizes healthcare resource utilization for future\ntreatment allocation, development, and prioritization. Comprehensive literature\nreview and focus group discussion were conducted. To address the multifaceted\nchallenges facing RDTs, four key issues for RDTs must be addressed, which are\n1) the opportunity to access the new treatment; 2) assessment of outcomes where\nclinically validated outcomes may be lacking; 3) patient heterogeneity; and 4)\nduration of the study and number of patients required. Our proposed study\ndesign has two stages. Stage 1 distinguishes patients who respond to the\ntreatment from those who do not respond to the treatment after assigning them\nall to the experimental treatment. Stage 2 evaluates the treatment effect\ncomparatively among patients responded in Stage 1. In addition to treatment\neffect evaluation, our design can greatly benefit rare disease patients and\nclinical practice by increasing opportunities to access experimental treatments\nand by providing relevant information that can be used for tailoring treatments\nto certain subgroups, aiding future research in treatment development, and\nimproving healthcare resource utilization.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 21:03:40 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Yong", "Jian", ""], ["Mohammad", "Sohaib H.", ""], ["Yuan", "Yan", ""]]}, {"id": "1607.00091", "submitter": "Elias Chaibub Neto", "authors": "Elias Chaibub Neto, Bruce R Hoff, Chris Bare, Brian M Bot, Thomas Yu,\n  Lara Magravite, Andrew D Trister, Thea Norman, Pablo Meyer, Julio\n  Saez-Rodrigues, James C Costello, Justin Guinney, Gustavo Stolovitzky", "title": "Reducing overfitting in challenge-based competitions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over-fitting is a dreaded foe in challenge-based competitions. Because\nparticipants rely on public leaderboards to evaluate and refine their models,\nthere is always the danger they might over-fit to the holdout data supporting\nthe leaderboard. The recently published Ladder algorithm aims to address this\nproblem by preventing the participants from exploiting willingly or\ninadvertently minor fluctuations in public leaderboard scores during model\nrefinement. In this paper, we report a vulnerability of the Ladder that induces\nsevere over-fitting of the leaderboard when the sample size is small. To\ncircumvent this attack, we propose a variation of the Ladder that releases a\nbootstrapped estimate of the public leaderboard score instead of providing\nparticipants with a direct measure of performance. We also extend the scope of\nthe Ladder to arbitrary performance metrics by relying on a more broadly\napplicable testing procedure based on the Bayesian bootstrap. Our method makes\nit possible to use a leaderboard, with the technical and social advantages that\nit provides, even in cases where data is scant.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 01:10:35 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Neto", "Elias Chaibub", ""], ["Hoff", "Bruce R", ""], ["Bare", "Chris", ""], ["Bot", "Brian M", ""], ["Yu", "Thomas", ""], ["Magravite", "Lara", ""], ["Trister", "Andrew D", ""], ["Norman", "Thea", ""], ["Meyer", "Pablo", ""], ["Saez-Rodrigues", "Julio", ""], ["Costello", "James C", ""], ["Guinney", "Justin", ""], ["Stolovitzky", "Gustavo", ""]]}, {"id": "1607.00411", "submitter": "R\\u{a}zvan \\c{S}tef\\u{a}nescu", "authors": "Razvan Stefanescu, Kathleen Schmidt, Jason Hite, Ralph Smith, John\n  Mattingly", "title": "Hybrid optimization and Bayesian inference techniques for a non-smooth\n  radiation detection problem", "comments": "36 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this investigation, we propose several algorithms to recover the location\nand intensity of a radiation source located in a simulated 250 m x 180 m block\nin an urban center based on synthetic measurements. Radioactive decay and\ndetection are Poisson random processes, so we employ likelihood functions based\non this distribution. Due to the domain geometry and the proposed response\nmodel, the negative logarithm of the likelihood is only piecewise continuous\ndifferentiable, and it has multiple local minima. To address these\ndifficulties, we investigate three hybrid algorithms comprised of mixed\noptimization techniques. For global optimization, we consider Simulated\nAnnealing (SA), Particle Swarm (PS) and Genetic Algorithm (GA), which rely\nsolely on objective function evaluations; i.e., they do not evaluate the\ngradient in the objective function. By employing early stopping criteria for\nthe global optimization methods, a pseudo-optimum point is obtained. This is\nsubsequently utilized as the initial value by the deterministic Implicit\nFiltering method (IF), which is able to find local extrema in non-smooth\nfunctions, to finish the search in a narrow domain. These new hybrid techniques\ncombining global optimization and Implicit Filtering address difficulties\nassociated with the non-smooth response, and their performances are shown to\nsignificantly decrease the computational time over the global optimization\nmethods alone. To quantify uncertainties associated with the source location\nand intensity, we employ the Delayed Rejection Adaptive Metropolis (DRAM) and\nDiffeRential Evolution Adaptive Metropolis (DREAM) algorithms. Marginal\ndensities of the source properties are obtained, and the means of the chains'\ncompare accurately with the estimates produced by the hybrid algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 21:27:40 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Stefanescu", "Razvan", ""], ["Schmidt", "Kathleen", ""], ["Hite", "Jason", ""], ["Smith", "Ralph", ""], ["Mattingly", "John", ""]]}, {"id": "1607.00448", "submitter": "Jinghai Shao", "authors": "Jinghai Shao, Siming Li, Yong Li", "title": "Estimation and prediction of credit risk based on rating transition\n  systems", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Risk management is an important practice in the banking industry. In this\npaper we develop a new methodology to estimate and predict the probability of\ndefault (PD) based on the rating transition matrices, which relates the rating\ntransition matrices to the macroeconomic variables. Our method can overcome the\nshortcomings of the framework of Belkin et al. (1998), and is especially useful\nin predicting the PD and doing stress testing. Simulation is conducted at the\nend, which shows that our method can provide more accurate estimate than that\nobtained by the method of Belkin et al. (1998).\n", "versions": [{"version": "v1", "created": "Sat, 2 Jul 2016 01:52:59 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2018 09:07:22 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Shao", "Jinghai", ""], ["Li", "Siming", ""], ["Li", "Yong", ""]]}, {"id": "1607.00674", "submitter": "David Jaur\\`es Fotsa Mbogne", "authors": "David Jaur\\`es Fotsa Mbogne", "title": "Estimation of anthracnose dynamics by nonlinear filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we apply the nonlinear filtering theory to the estimation of\nthe partially observed dynamics of anthracnose which is a phytopathology. The\nsignal here is the inhibition rate and the observations are the fruit volume\nant the rotted volume. We propose stochastic models based on the deterministic\nmodels given in the references [21, 22], in order to represent the noise\nintroduced by uncontrolled variation on parameters and errors on the\nmeasurements. Under the assumption of Brownian noises we prove the\nwell-posedness the models either they take into account the space variable or\nnot. The filtering problem is solved for the non-spatial model giving Zakai and\nKushner-Stratonovich equations satisfied respectively by the unnormalized and\nthe normalized conditional distribution of the signal with respect to the\nobservations. A prevision problem and a discrete filtering problem are also\nstudied for the realistic cases of discrete and possibly incomplete\nobservations. We illustrate the filter behaviour through numerical simulations\ncorresponding to different scenarios\n  KeyWords: Anthracnose modelling, State estimation, Nonlinear filtering.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jul 2016 19:34:23 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Mbogne", "David Jaur\u00e8s Fotsa", ""]]}, {"id": "1607.00959", "submitter": "Aleksey Polunchenko", "authors": "Aleksey S. Polunchenko", "title": "Optimal Design of the Shiryaev-Roberts Chart: Give Your Shiryaev-Roberts\n  a Headstart", "comments": "21 pages, 20 figures, 2 tables; To appear in Proceedings of the 12th\n  International Workshop on Intelligent Statistical Quality Control", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We offer a numerical study of the effect of headstarting on the performance\nof a Shiryaev-Roberts (SR) chart set up to control the mean of a normal\nprocess. The study is a natural extension of that previously carried out by\nLucas and Crosier for the CUSUM scheme in their seminal 1982 paper published in\nTechnometrics. The Fast Initial Response (FIR) feature exhibited by a\nheadstarted CUSUM turns out to be also characteristic of an SR chart\n(re-)started off a nonzero initial score. However, our main result is the\nobservation that a FIR SR with a carefully designed {\\em optimal} headstart is\nnot just faster to react to an initial out-of-control situation, it is nearly\n{\\em the} fastest {\\em uniformly}, i.e., assuming the process under\nsurveillance is equally likely to go out of control effective any sample\nnumber. The performance improvement is the greater, the fainter the change. We\nexplain the optimization strategy, and tabulate the optimal initial score,\ncontrol limit, and the corresponding \"worst possible\" out-of-control Average\nRun Length (ARL), considering mean-shifts of diverse magnitudes and a wide\nrange of levels of the in-control ARL.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 16:54:48 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Polunchenko", "Aleksey S.", ""]]}, {"id": "1607.01355", "submitter": "Ehsan Taghavi", "authors": "E. Taghavi, D. Song, R. Tharmarasa, T. Kirubarajan, Anne-Claire\n  Boury-Brisset and Bhashyam Balaji", "title": "Object Recognition and Identification Using ESM Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognition and identification of unknown targets is a crucial task in\nsurveillance and security systems. Electronic Support Measures (ESM) are one of\nthe most effective sensors for identification, especially for maritime and\nair--to--ground applications. In typical surveillance systems multiple ESM\nsensors are usually deployed along with kinematic sensors like radar. Different\nESM sensors may produce different types of reports ready to be sent to the\nfusion center. The focus of this paper is to develop a new architecture for\ntarget recognition and identification when non--homogeneous ESM and possibly\nkinematic reports are received at the fusion center. The new fusion\narchitecture is evaluated using simulations to show the benefit of utilizing\ndifferent ESM reports such as attributes and signal level ESM data.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 15:03:59 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Taghavi", "E.", ""], ["Song", "D.", ""], ["Tharmarasa", "R.", ""], ["Kirubarajan", "T.", ""], ["Boury-Brisset", "Anne-Claire", ""], ["Balaji", "Bhashyam", ""]]}, {"id": "1607.01367", "submitter": "Sacha Epskamp", "authors": "Sacha Epskamp and Eiko I. Fried", "title": "A Tutorial on Regularized Partial Correlation Networks", "comments": "In press in Psychological Methods (DOI: 10.1037/met0000167)", "journal-ref": null, "doi": "10.1037/met0000167", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen an emergence of network modeling applied to moods,\nattitudes, and problems in the realm of psychology. In this framework,\npsychological variables are understood to directly affect each other rather\nthan being caused by an unobserved latent entity. In this tutorial, we\nintroduce the reader to estimating the most popular network model for\npsychological data: the partial correlation network. We describe how\nregularization techniques can be used to efficiently estimate a parsimonious\nand interpretable network structure in psychological data. We show how to\nperform these analyses in R and demonstrate the method in an empirical example\non post-traumatic stress disorder data. In addition, we discuss the effect of\nthe hyperparameter that needs to be manually set by the researcher, how to\nhandle non-normal data, how to determine the required sample size for a network\nanalysis, and provide a checklist with potential solutions for problems that\ncan arise when estimating regularized partial correlation networks.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 18:55:19 GMT"}, {"version": "v2", "created": "Tue, 13 Sep 2016 20:01:32 GMT"}, {"version": "v3", "created": "Sun, 25 Sep 2016 10:24:17 GMT"}, {"version": "v4", "created": "Mon, 3 Oct 2016 15:34:25 GMT"}, {"version": "v5", "created": "Sun, 30 Apr 2017 18:29:41 GMT"}, {"version": "v6", "created": "Wed, 28 Jun 2017 16:15:15 GMT"}, {"version": "v7", "created": "Mon, 11 Sep 2017 20:46:08 GMT"}, {"version": "v8", "created": "Thu, 14 Sep 2017 11:09:41 GMT"}, {"version": "v9", "created": "Fri, 1 Dec 2017 13:35:57 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Epskamp", "Sacha", ""], ["Fried", "Eiko I.", ""]]}, {"id": "1607.01371", "submitter": "J\\\"orn Diedrichsen", "authors": "J\\\"orn Diedrichsen (1 and 2), Serge Provost (2), Hossein\n  Zareamoghaddam (2) ((1) Brain and Mind Institute, Western University, (2)\n  Department of Statistical and Actuarial Sciences, Western University)", "title": "On the distribution of cross-validated Mahalanobis distances", "comments": "24 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present analytical expressions for the means and covariances of the sample\ndistribution of the cross-validated Mahalanobis distance. This measure has\nproven to be especially useful in the context of representational similarity\nanalysis (RSA) of neural activity patterns as measured by means of functional\nmagnetic resonance imaging (fMRI). These expressions allow us to construct a\nnormal approximation to the estimated distances, which in turn enables powerful\ninference on the measured statistics. Using the results, the difference between\ntwo distances can be statistically assessed, and the measured structure of the\ndistances can be efficiently compared to predictions from computational models.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 19:17:30 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Diedrichsen", "J\u00f6rn", "", "1 and 2"], ["Provost", "Serge", ""], ["Zareamoghaddam", "Hossein", ""]]}, {"id": "1607.01631", "submitter": "Mike West", "authors": "Kaoru Irie and Mike West", "title": "Bayesian emulation for optimization in multi-step portfolio decisions", "comments": "24 pages, 7 figures, 2 tables", "journal-ref": null, "doi": "10.1214/18-BA1105", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the Bayesian emulation approach to computational solution of\nmulti-step portfolio studies in financial time series. \"Bayesian emulation for\ndecisions\" involves mapping the technical structure of a decision analysis\nproblem to that of Bayesian inference in a purely synthetic \"emulating\"\nstatistical model. This provides access to standard posterior analytic,\nsimulation and optimization methods that yield indirect solutions of the\ndecision problem. We develop this in time series portfolio analysis using\nclasses of economically and psychologically relevant multi-step ahead portfolio\nutility functions. Studies with multivariate currency, commodity and stock\nindex time series illustrate the approach and show some of the practical\nutility and benefits of the Bayesian emulation methodology.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 14:21:41 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Irie", "Kaoru", ""], ["West", "Mike", ""]]}, {"id": "1607.01735", "submitter": "Navid Dianati", "authors": "Navid Dianati", "title": "A maximum entropy approach to separating noise from signal in bimodal\n  affiliation networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In practice, many empirical networks, including co-authorship and collocation\nnetworks are unimodal projections of a bipartite data structure where one layer\nrepresents entities, the second layer consists of a number of sets representing\naffiliations, attributes, groups, etc., and an inter-layer link indicates\nmembership of an entity in a set. The edge weight in the unimodal projection,\nwhich we refer to as a co-occurrence network, counts the number of sets to\nwhich both end-nodes are linked. Interpreting such dense networks requires\nstatistical analysis that takes into account the bipartite structure of the\nunderlying data. Here we develop a statistical significance metric for such\nnetworks based on a maximum entropy null model which preserves both the\nfrequency sequence of the individuals/entities and the size sequence of the\nsets. Solving the maximum entropy problem is reduced to solving a system of\nnonlinear equations for which fast algorithms exist, thus eliminating the need\nfor expensive Monte-Carlo sampling techniques. We use this metric to prune and\nvisualize a number of empirical networks.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 18:27:09 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Dianati", "Navid", ""]]}, {"id": "1607.01756", "submitter": "Sameer Deshpande", "authors": "Sameer K. Deshpande, Raiden B. Hasegawa, Amanda R. Rabinowitz, John\n  Whyte, Carol L. Roan, Andrew Tabatabaei, Michael Baiocchi, Jason H.\n  Karlawish, Christina L. Master, Dylan S. Small", "title": "Protocol for an Observational Study on the Effects of Playing High\n  School Football on Later Life Cognitive Functioning and Mental Health", "comments": "Prior to performing the proposed analysis, we will register this\n  pre-analysis plan on clincialtrials.gov", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A potential causal relationship between head injuries sustained by NFL\nplayers and later-life neurological decline may have broad implications for\nparticipants in youth and high school football programs. However, brain trauma\nrisk at the professional level may be different than that at the youth and high\nschool levels and the long-term effects of participation at these levels is\nas-yet unclear. To investigate the effect of playing high school football on\nlater life depression and cognitive functioning, we propose a retrospective\nobservational study using data from the Wisconsin Longitudinal Study (WLS) of\ngraduates from Wisconsin high schools in 1957.\n  We compare 1,153 high school males who played varsity football to 2,751 male\nstudents who did not. 1,951 of the control subjects did not play any sport and\nthe remaining 800 controls played a non-contact sport. We focus on two primary\noutcomes measured at age 65: a composite cognitive outcome measuring verbal\nfluency and memory and the modified CES-D depression score. To control for\npotential confounders we adjust for pre-exposure covariates such as IQ with\nmatching and model-based covariate adjustment. We will conduct an ordered\ntesting procedure that uses all 2,751 controls while controlling for possible\nunmeasured differences between students who played sports and those who did\nnot. We will quantitatively assess the sensitivity of the results to potential\nunmeasured confounding. The study will also consider several secondary outcomes\nof clinical interest such as aggression and heavy drinking. The rich set of\npre-exposure variables, relatively unbiased sampling, and longitudinal nature\nof the WLS dataset make the proposed analysis unique among related studies that\nrely primarily on convenience samples of football players with reported\nneurological symptoms.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 19:37:18 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Deshpande", "Sameer K.", ""], ["Hasegawa", "Raiden B.", ""], ["Rabinowitz", "Amanda R.", ""], ["Whyte", "John", ""], ["Roan", "Carol L.", ""], ["Tabatabaei", "Andrew", ""], ["Baiocchi", "Michael", ""], ["Karlawish", "Jason H.", ""], ["Master", "Christina L.", ""], ["Small", "Dylan S.", ""]]}, {"id": "1607.02109", "submitter": "John J Nay", "authors": "John J. Nay", "title": "Predicting and Understanding Law-Making with Word Vectors and an\n  Ensemble Model", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0176999", "report-no": null, "categories": "cs.CL physics.soc-ph stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Out of nearly 70,000 bills introduced in the U.S. Congress from 2001 to 2015,\nonly 2,513 were enacted. We developed a machine learning approach to\nforecasting the probability that any bill will become law. Starting in 2001\nwith the 107th Congress, we trained models on data from previous Congresses,\npredicted all bills in the current Congress, and repeated until the 113th\nCongress served as the test. For prediction we scored each sentence of a bill\nwith a language model that embeds legislative vocabulary into a\nhigh-dimensional, semantic-laden vector space. This language representation\nenables our investigation into which words increase the probability of\nenactment for any topic. To test the relative importance of text and context,\nwe compared the text model to a context-only model that uses variables such as\nwhether the bill's sponsor is in the majority party. To test the effect of\nchanges to bills after their introduction on our ability to predict their final\noutcome, we compared using the bill text and meta-data available at the time of\nintroduction with using the most recent data. At the time of introduction\ncontext-only predictions outperform text-only, and with the newest data\ntext-only outperforms context-only. Combining text and context always performs\nbest. We conducted a global sensitivity analysis on the combined model to\ndetermine important variables predicting enactment.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 18:08:59 GMT"}, {"version": "v2", "created": "Sat, 29 Apr 2017 17:12:33 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Nay", "John J.", ""]]}, {"id": "1607.02188", "submitter": "Anders Hildeman", "authors": "Anders Hildeman, David Bolin, Jonas Wallin, Adam Johansson, Tufve\n  Nyholm, Thomas Asklund, Jun Yu", "title": "Whole-brain substitute CT generation using Markov random field mixture\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computed tomography (CT) equivalent information is needed for attenuation\ncorrection in PET imaging and for dose planning in radiotherapy. Prior work has\nshown that Gaussian mixture models can be used to generate a substitute CT\n(s-CT) image from a specific set of MRI modalities. This work introduces a more\nflexible class of mixture models for s-CT generation, that incorporates spatial\ndependency in the data through a Markov random field prior on the latent field\nof class memberships associated with a mixture model. Furthermore, the mixture\ndistributions are extended from Gaussian to normal inverse Gaussian (NIG),\nallowing heavier tails and skewness. The amount of data needed to train a model\nfor s-CT generation is of the order of 100 million voxels. The computational\nefficiency of the parameter estimation and prediction methods are hence\nparamount, especially when spatial dependency is included in the models. A\nstochastic Expectation Maximization (EM) gradient algorithm is proposed in\norder to tackle this challenge. The advantages of the spatial model and NIG\ndistributions are evaluated with a cross-validation study based on data from 14\npatients. The study show that the proposed model enhances the predictive\nquality of the s-CT images by reducing the mean absolute error with 17.9%.\nAlso, the distribution of CT values conditioned on the MR images are better\nexplained by the proposed model as evaluated using continuous ranked\nprobability scores.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 22:52:36 GMT"}, {"version": "v2", "created": "Wed, 28 Sep 2016 15:11:38 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Hildeman", "Anders", ""], ["Bolin", "David", ""], ["Wallin", "Jonas", ""], ["Johansson", "Adam", ""], ["Nyholm", "Tufve", ""], ["Asklund", "Thomas", ""], ["Yu", "Jun", ""]]}, {"id": "1607.02248", "submitter": "Oliver Lang", "authors": "Oliver Lang, Mario Huemer and Christian Hofbauer", "title": "On the Log-Likelihood Ratio Evaluation of CWCU Linear and Widely Linear\n  MMSE Data Estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In soft decoding of data bits, the log-likelihood ratios are evaluated from\nthe estimated data symbols. For proper constellation diagrams such as QPSK or\n16-QAM, these data symbols are often estimated using the linear minimum mean\nsquare error (LMMSE) estimator. The LMMSE estimator only fulfills the weak\nBayesian unbiasedness constraint. Recently, estimators fulfilling the more\nstringent component-wise conditionally unbiased (CWCU) constraints have been\ninvestigated, such as the CWCU LMMSE estimator. In this paper, we prove that\nthe CWCU LMMSE estimates result in the very same log-likelihood ratios as the\nLMMSE estimates. For improper constellation diagrams such as 8-QAM, widely\nlinear estimators are used. For this case, we show that the widely linear\nversions of the LMMSE estimator and the CWCU LMMSE estimator also yield\nidentical log-likelihood ratios. Finally, we give a simulation example which\nillustrates a number of interesting properties of the discussed widely linear\nestimators.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 06:38:09 GMT"}, {"version": "v2", "created": "Tue, 13 Dec 2016 08:22:32 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Lang", "Oliver", ""], ["Huemer", "Mario", ""], ["Hofbauer", "Christian", ""]]}, {"id": "1607.02283", "submitter": "Ryuji Uozumi", "authors": "Ryuji Uozumi and Chikuma Hamada", "title": "Adaptive seamless design for establishing pharmacokinetics and efficacy\n  equivalence in developing biosimilars", "comments": "This paper has been withdrawn because of the insufficiency of\n  numerical experiments in Section 4", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Recently, numerous pharmaceutical sponsors have expressed a great deal of\ninterest in the development of biosimilars, which requires clinical trials to\ndemonstrate the equivalence of pharmacokinetics (PK) and clinical efficacy.\nPharmacodynamics (PD) may be used in evaluating efficacy if there are relevant\nPD markers available. However, in their absence, it is necessary to design the\nassociated clinical trials to include efficacy measures as the primary\nendpoint. In this study, we propose an adaptive seamless PK and efficacy design\nwith the frameworks to remedy the risk of misspecification of both PK and\nefficacy parameters. Here, we consider the clinical development of biosimilars\nincluding their evaluation in patients rather than healthy volunteers under a\nsituation where both PK and efficacy parameters are required to demonstrate the\nequivalence. To avoid the risk associated with the failure to confirm\nequivalence, incorporating the new PK trial for PK equivalence within the PK\nportion, which is the early stage for the efficacy part, and sample size\nre-calculation for the efficacy equivalence are considered in the proposed\nmethod. This proposal provides appealing advantages such as a shorter period,\nadditional cost saving, and smaller number of patients required.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 09:30:04 GMT"}, {"version": "v2", "created": "Mon, 15 May 2017 10:09:07 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Uozumi", "Ryuji", ""], ["Hamada", "Chikuma", ""]]}, {"id": "1607.02633", "submitter": "Umberto Picchini", "authors": "Umberto Picchini and Julie Lyng Forman", "title": "Bayesian inference for stochastic differential equation mixed effects\n  models of a tumor xenography study", "comments": "Minor revision: posterior predictive checks for BSL have ben updated\n  (both theory and results). Code on GitHub has ben revised accordingly", "journal-ref": "Journal of the Royal Statistical Society: Series C (Applied\n  Statistics), 2019", "doi": "10.1111/rssc.12347", "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Bayesian inference for stochastic differential equation mixed\neffects models (SDEMEMs) exemplifying tumor response to treatment and regrowth\nin mice. We produce an extensive study on how a SDEMEM can be fitted using both\nexact inference based on pseudo-marginal MCMC and approximate inference via\nBayesian synthetic likelihoods (BSL). We investigate a two-compartments SDEMEM,\nthese corresponding to the fractions of tumor cells killed by and survived to a\ntreatment, respectively. Case study data considers a tumor xenography study\nwith two treatment groups and one control, each containing 5-8 mice. Results\nfrom the case study and from simulations indicate that the SDEMEM is able to\nreproduce the observed growth patterns and that BSL is a robust tool for\ninference in SDEMEMs. Finally, we compare the fit of the SDEMEM to a similar\nordinary differential equation model. Due to small sample sizes, strong prior\ninformation is needed to identify all model parameters in the SDEMEM and it\ncannot be determined which of the two models is the better in terms of\npredicting tumor growth curves. In a simulation study we find that with a\nsample of 17 mice per group BSL is able to identify all model parameters and\ndistinguish treatment groups.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jul 2016 16:30:33 GMT"}, {"version": "v2", "created": "Sat, 10 Dec 2016 16:03:46 GMT"}, {"version": "v3", "created": "Tue, 10 Oct 2017 17:16:32 GMT"}, {"version": "v4", "created": "Mon, 1 Oct 2018 17:19:19 GMT"}, {"version": "v5", "created": "Sun, 17 Feb 2019 10:51:35 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Picchini", "Umberto", ""], ["Forman", "Julie Lyng", ""]]}, {"id": "1607.02665", "submitter": "Anurag Kumar", "authors": "Anurag Kumar, Bhiksha Raj", "title": "Classifier Risk Estimation under Limited Labeling Resources", "comments": "PAKDD 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose strategies for estimating performance of a\nclassifier when labels cannot be obtained for the whole test set. The number of\ntest instances which can be labeled is very small compared to the whole test\ndata size. The goal then is to obtain a precise estimate of classifier\nperformance using as little labeling resource as possible. Specifically, we try\nto answer, how to select a subset of the large test set for labeling such that\nthe performance of a classifier estimated on this subset is as close as\npossible to the one on the whole test set. We propose strategies based on\nstratified sampling for selecting this subset. We show that these strategies\ncan reduce the variance in estimation of classifier accuracy by a significant\namount compared to simple random sampling (over 65% in several cases). Hence,\nour proposed methods are much more precise compared to random sampling for\naccuracy estimation under restricted labeling resources. The reduction in\nnumber of samples required (compared to random sampling) to estimate the\nclassifier accuracy with only 1% error is high as 60% in some cases.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jul 2016 21:18:23 GMT"}, {"version": "v2", "created": "Mon, 19 Feb 2018 20:18:35 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Kumar", "Anurag", ""], ["Raj", "Bhiksha", ""]]}, {"id": "1607.02735", "submitter": "Genya Kobayashi Mr.", "authors": "Genya Kobayashi and Kazuhiko Kakamu", "title": "Approximate Bayesian Computation for Lorenz Curves from Grouped Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new Bayesian approach to estimate the Gini coefficient\nfrom the Lorenz curve based on grouped data. The proposed approach assumes a\nhypothetical income distribution and estimates the parameter by directly\nworking on the likelihood function implied by the Lorenz curve of the income\ndistribution from the grouped data. It inherits the advantages of two existing\napproaches through which the Gini coefficient can be estimated more accurately\nand a straightforward interpretation about the underlying income distribution\nis provided. Since the likelihood function is implicitly defined, the\napproximate Bayesian computational approach based on the sequential Monte Carlo\nmethod is adopted. The usefulness of the proposed approach is illustrated\nthrough the simulation study and the Japanese income data.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jul 2016 11:46:37 GMT"}, {"version": "v2", "created": "Wed, 9 Aug 2017 15:19:07 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Kobayashi", "Genya", ""], ["Kakamu", "Kazuhiko", ""]]}, {"id": "1607.02788", "submitter": "Youssef Marzouk", "authors": "Patrick Conrad, Andrew Davis, Youssef Marzouk, Natesh Pillai, Aaron\n  Smith", "title": "Parallel local approximation MCMC for expensive models", "comments": "34 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing Bayesian inference via Markov chain Monte Carlo (MCMC) can be\nexceedingly expensive when posterior evaluations invoke the evaluation of a\ncomputationally expensive model, such as a system of partial differential\nequations. In recent work [Conrad et al. JASA 2016, arXiv:1402.1694], we\ndescribed a framework for constructing and refining local approximations of\nsuch models during an MCMC simulation. These posterior--adapted approximations\nharness regularity of the model to reduce the computational cost of inference\nwhile preserving asymptotic exactness of the Markov chain. Here we describe two\nextensions of that work. First, we prove that samplers running in parallel can\ncollaboratively construct a shared posterior approximation while ensuring\nergodicity of each associated chain, providing a novel opportunity for\nexploiting parallel computation in MCMC. Second, focusing on the\nMetropolis--adjusted Langevin algorithm, we describe how a proposal\ndistribution can successfully employ gradients and other relevant information\nextracted from the approximation. We investigate the practical performance of\nour strategies using two challenging inference problems, the first in\nsubsurface hydrology and the second in glaciology. Using local approximations\nconstructed via parallel chains, we successfully reduce the run time needed to\ncharacterize the posterior distributions in these problems from days to hours\nand from months to days, respectively, dramatically improving the tractability\nof Bayesian inference.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jul 2016 21:46:43 GMT"}, {"version": "v2", "created": "Mon, 25 Dec 2017 21:50:54 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Conrad", "Patrick", ""], ["Davis", "Andrew", ""], ["Marzouk", "Youssef", ""], ["Pillai", "Natesh", ""], ["Smith", "Aaron", ""]]}, {"id": "1607.02795", "submitter": "Donald Percival", "authors": "Donald B. Percival, Donald W. Denbo, Edison Gica, Paul Y. Huang,\n  Harold O. Mofjeld, Michael C. Spillane and Vasily V. Titov", "title": "Evaluating Effectiveness of DART Buoy Networks", "comments": "31 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": "Joint Institute for the Study of the Atmosphere and Ocean (JISAO)\n  Contribution No. 2714; NOAA/Pacific Marine Environmental Laboratory\n  Contribution No. 4507", "categories": "physics.ao-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A performance measure for a DART tsunami buoy network has been developed. The\nmeasure is based on a statistical analysis of simulated forecasts of wave\nheights outside an impact site and how much the forecasts are degraded in\naccuracy when one or more buoys are inoperative. The analysis uses simulated\ntsunami height time series collected at each buoy from selected source segments\nin the Short-term Inundation Forecast for Tsunamis (SIFT) database and involves\na set for 1000 forecasts for each buoy/segment pair at sites just offshore of\nselected impact communities. Random error-producing scatter in the time series\nis induced by uncertainties in the source location, addition of real oceanic\nnoise, and imperfect tidal removal. Comparison with an error-free standard\nleads to root-mean-square errors (RMSEs) for DART buoys located near a\nsubduction zone. The RMSEs indicate which buoy provides the best forecast\n(lowest RMSE) for sections of the zone, under a warning-time constraint for the\nforecasts of 3 hrs. The analysis also shows how the forecasts are degraded\n(larger minimum RMSE among the remaining buoys) when one or more buoys become\ninoperative. The RMSEs also provide a way to assess array augmentation or\nredesign such as moving buoys to more optimal locations. Examples are shown for\nbuoys off the Aleutian Islands and off the West Coast of South America for\nimpact sites at Hilo HI and along the U.S. West Coast (Crescent City CA and\nPort San Luis CA). A simple measure (coded green, yellow or red) of the current\nstatus of the network's ability to deliver accurate forecasts is proposed to\nflag the urgency of buoy repair.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jul 2016 23:49:13 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Percival", "Donald B.", ""], ["Denbo", "Donald W.", ""], ["Gica", "Edison", ""], ["Huang", "Paul Y.", ""], ["Mofjeld", "Harold O.", ""], ["Spillane", "Michael C.", ""], ["Titov", "Vasily V.", ""]]}, {"id": "1607.02883", "submitter": "Abhik Ghosh", "authors": "Abhik Ghosh and Magne Thoresen", "title": "Non-Concave Penalization in Linear Mixed-Effects Models and Regularized\n  Selection of Fixed Effects", "comments": "25 pages, under review", "journal-ref": "AStA Advances in Statistical Analysis (2018), Volume 102, Issue 2,\n  pp 179--210", "doi": "10.1007/s10182-017-0298-z", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed-effect models are very popular for analyzing data with a hierarchical\nstructure, e.g. repeated observations within subjects in a longitudinal design,\npatients nested within centers in a multicenter design. However, recently, due\nto the medical advances, the number of fixed effect covariates collected from\neach patient can be quite large, e.g. data on gene expressions of each patient,\nand all of these variables are not necessarily important for the outcome. So,\nit is very important to choose the relevant covariates correctly for obtaining\nthe optimal inference for the overall study. On the other hand, the relevant\nrandom effects will often be low-dimensional and pre-specified. In this paper,\nwe consider regularized selection of important fixed effect variables in linear\nmixed-effects models along with maximum penalized likelihood estimation of both\nfixed and random effect parameters based on general non-concave penalties.\nAsymptotic and variable selection consistency with oracle properties are proved\nfor low-dimensional cases as well as for high-dimensionality of non-polynomial\norder of sample size (number of parameters is much larger than sample size). We\nalso provide a suitable computationally efficient algorithm for implementation.\nAdditionally, all the theoretical results are proved for a general non-convex\noptimization problem that applies to several important situations well beyond\nthe mixed model set-up (like finite mixture of regressions etc.) illustrating\nthe huge range of applicability of our proposal.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 10:07:37 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Ghosh", "Abhik", ""], ["Thoresen", "Magne", ""]]}, {"id": "1607.02957", "submitter": "Hung Hung", "authors": "Hung Hung and Zhi-Yu Jou", "title": "A low-rank based estimation-testing procedure for matrix-covariate\n  regression", "comments": "1 figure and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix-covariate is now frequently encountered in many biomedical researches.\nIt is common to fit conventional statistical models by vectorizing\nmatrix-covariate. This strategy, however, results in a large number of\nparameters, while the available sample size is relatively too small to have\nreliable analysis results. To overcome the problem of high-dimensionality in\nhypothesis testing, variance component test has been proposed with promise\ndetection power, but is not straightforward to provide estimates of effect\nsize. In this work, we overcome the problem of high-dimensionality by utilizing\nthe inherent structure of the matrix-covariate. The advantage is that\nestimation and hypothesis testing can be conducted simultaneously as in the\nconventional case, while the estimation efficiency and detection power can be\nlargely improved, due to a parsimonious parameterization for the coefficients\nof matrix-covariate. Our method is applied to test the significance of\ngene-gene interactions in the PSQI data, and is applied to test if\nelectroencephalography is associated with the alcoholic status in the EEG data,\nwherein sparse effects and low-rank effects of matrix-covariates are\nidentified, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 14:03:58 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Hung", "Hung", ""], ["Jou", "Zhi-Yu", ""]]}, {"id": "1607.03000", "submitter": "David Valls-Gabaud", "authors": "D. Valls-Gabaud", "title": "Bayesian isochrone fitting and stellar ages", "comments": "43 pages, Proceedings of the Evry Schatzman School of Stellar\n  Astrophysics \"The ages of stars\", EAS Publications Series, Volume 65", "journal-ref": "EAS Publications Series, Volume 65, pp. 225-265 (2014)", "doi": "10.1051/eas/1465006", "report-no": null, "categories": "astro-ph.GA astro-ph.IM physics.hist-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stellar evolution theory has been extraordinarily successful at explaining\nthe different phases under which stars form, evolve and die. While the\nstrongest constraints have traditionally come from binary stars, the advent of\nasteroseismology is bringing unique measures in well-characterised stars. For\nstellar populations in general, however, only photometric measures are usually\navailable, and the comparison with the predictions of stellar evolution theory\nhave mostly been qualitative. For instance, the geometrical shapes of\nisochrones have been used to infer ages of coeval populations, but without any\nproper statistical basis. In this chapter we provide a pedagogical review on a\nBayesian formalism to make quantitative inferences on the properties of single,\nbinary and small ensembles of stars, including unresolved populations. As an\nexample, we show how stellar evolution theory can be used in a rigorous way as\na prior information to measure the ages of stars between the ZAMS and the\nHelium flash, and their uncertainties, using photometric data only.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 15:26:28 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Valls-Gabaud", "D.", ""]]}, {"id": "1607.03180", "submitter": "Korbinian Strimmer", "authors": "Sebastian Gibb and Korbinian Strimmer", "title": "Mass spectrometry analysis using MALDIquant", "comments": "26 pages, 12 figures. This is the draft version of a chapter to be\n  published in S. Datta and B. Mertens (eds). 2016. Statistical Analysis of\n  Proteomics, Metabolomics, and Lipidomics Data Using Mass Spectrometry.\n  Frontiers in Probability and the Statistical Sciences. Springer, New York", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MALDIquant and associated R packages provide a versatile and completely free\nopen-source platform for analyzing 2D mass spectrometry data as generated for\ninstance by MALDI and SELDI instruments. We first describe the various methods\nand algorithms available in MALDIquant . Subsequently, we illustrate a typical\nanalysis workflow using MALDIquant by investigating an experimental cancer data\nset, starting from raw mass spectrometry measurements and ending at\nmultivariate classification.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 22:02:05 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Gibb", "Sebastian", ""], ["Strimmer", "Korbinian", ""]]}, {"id": "1607.03202", "submitter": "Eric Lundquist", "authors": "Anders Drachen, Eric Thurston Lundquist, Yungjen Kung, Pranav Simha\n  Rao, Diego Klabjan, Rafet Sifa and Julian Runge", "title": "Rapid Prediction of Player Retention in Free-to-Play Mobile Games", "comments": "Draft Submitted to AIIDE-16. 7 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting and improving player retention is crucial to the success of mobile\nFree-to-Play games. This paper explores the problem of rapid retention\nprediction in this context. Heuristic modeling approaches are introduced as a\nway of building simple rules for predicting short-term retention. Compared to\ncommon classification algorithms, our heuristic-based approach achieves\nreasonable and comparable performance using information from the first session,\nday, and week of player activity.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 00:03:05 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Drachen", "Anders", ""], ["Lundquist", "Eric Thurston", ""], ["Kung", "Yungjen", ""], ["Rao", "Pranav Simha", ""], ["Klabjan", "Diego", ""], ["Sifa", "Rafet", ""], ["Runge", "Julian", ""]]}, {"id": "1607.03518", "submitter": "Bamdad Hosseini Mr.", "authors": "Bamdad Hosseini and John M. Stockie", "title": "Airborne contaminant source estimation using a finite-volume forward\n  solver coupled with a Bayesian inversion approach", "comments": "Fixed a few typos in figures", "journal-ref": "Computers & Fluids, 154:27-43, 2017", "doi": "10.1016/j.compfluid.2017.05.025", "report-no": null, "categories": "math.NA stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a numerical algorithm for solving the atmospheric dispersion\nproblem with elevated point sources and ground-level deposition. The problem is\nmodelled by the 3D advection-diffusion equation with delta-distribution source\nterms, as well as height-dependent advection speed and diffusion coefficients.\nWe construct a finite volume scheme using a splitting approach in which the\nClawpack software package is used as the advection solver and an implicit time\ndiscretization is proposed for the diffusion terms. The algorithm is then\napplied to an actual industrial scenario involving emissions of airborne\nparticulates from a zinc smelter using actual wind measurements. We also\naddress various practical considerations such as choosing appropriate methods\nfor regularizing noisy wind data and quantifying sensitivity of the model to\nparameter uncertainty. Afterwards, we use the algorithm within a Bayesian\nframework for estimating emission rates of zinc from multiple sources over the\nindustrial site. We compare our finite volume solver with a Gaussian plume\nsolver within the Bayesian framework and demonstrate that the finite volume\nsolver results in tighter uncertainty bounds on the estimated emission rates.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 21:07:45 GMT"}, {"version": "v2", "created": "Mon, 19 Sep 2016 16:29:37 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Hosseini", "Bamdad", ""], ["Stockie", "John M.", ""]]}, {"id": "1607.03534", "submitter": "Monica Alexander", "authors": "Monica Alexander, Emilio Zagheni, Magali Barbieri", "title": "A Flexible Bayesian Model for Estimating Subnational Mortality", "comments": null, "journal-ref": null, "doi": "10.1007/s13524-017-0618-7", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliable mortality estimates at the subnational level are essential in the\nstudy of health inequalities within a country. One of the difficulties in\nproducing such estimates is the presence of small populations, where the\nstochastic variation in death counts is relatively high, and so the underlying\nmortality levels are unclear. We present a Bayesian hierarchical model to\nestimate mortality at the subnational level. The model builds on characteristic\nage patterns in mortality curves, which are constructed using principal\ncomponents from a set of reference mortality curves. Information on mortality\nrates are pooled across geographic space and smoothed over time. Testing of the\nmodel shows reasonable estimates and uncertainty levels when the model is\napplied to both simulated data which mimic US counties, and real data for\nFrench departments. The estimates produced by the model have direct\napplications to the study of subregional health patterns and disparities.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 22:37:01 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Alexander", "Monica", ""], ["Zagheni", "Emilio", ""], ["Barbieri", "Magali", ""]]}, {"id": "1607.03592", "submitter": "Ahmed Attia", "authors": "Ahmed Attia, Azam Moosavi, Adrian Sandu", "title": "Cluster Sampling Filters for Non-Gaussian Data Assimilation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a fully non-Gaussian version of the Hamiltonian Monte\nCarlo (HMC) sampling filter. The Gaussian prior assumption in the original HMC\nfilter is relaxed. Specifically, a clustering step is introduced after the\nforecast phase of the filter, and the prior density function is estimated by\nfitting a Gaussian Mixture Model (GMM) to the prior ensemble. Using the data\nlikelihood function, the posterior density is then formulated as a mixture\ndensity, and is sampled using a HMC approach (or any other scheme capable of\nsampling multimodal densities in high-dimensional subspaces). The main filter\ndeveloped herein is named \"cluster HMC sampling filter\" (ClHMC). A multi-chain\nversion of the ClHMC filter, namely MC-ClHMC is also proposed to guarantee that\nsamples are taken from the vicinities of all probability modes of the\nformulated posterior. The new methodologies are tested using a\nquasi-geostrophic (QG) model with double-gyre wind forcing and bi-harmonic\nfriction. Numerical results demonstrate the usefulness of using GMMs to relax\nthe Gaussian prior assumption in the HMC filtering paradigm.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 04:37:31 GMT"}, {"version": "v2", "created": "Thu, 18 Aug 2016 15:14:18 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Attia", "Ahmed", ""], ["Moosavi", "Azam", ""], ["Sandu", "Adrian", ""]]}, {"id": "1607.03615", "submitter": "Sheng-Mao Chang", "authors": "Ray-Bing Chen, Kuang-Hung Cheng, Sheng-Mao Chang, Shuen-Lin Jeng,\n  Ping-Yang Chen, Chun-Hao Yang, and Chi-Chun Hsia", "title": "Multiple-Instance Logistic Regression with LASSO Penalty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider a manufactory process which can be described by a\nmultiple-instance logistic regression model. In order to compute the maximum\nlikelihood estimation of the unknown coefficient, an expectation-maximization\nalgorithm is proposed, and the proposed modeling approach can be extended to\nidentify the important covariates by adding the coefficient penalty term into\nthe likelihood function. In addition to essential technical details, we\ndemonstrate the usefulness of the proposed method by simulations and real\nexamples.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 07:30:57 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Chen", "Ray-Bing", ""], ["Cheng", "Kuang-Hung", ""], ["Chang", "Sheng-Mao", ""], ["Jeng", "Shuen-Lin", ""], ["Chen", "Ping-Yang", ""], ["Yang", "Chun-Hao", ""], ["Hsia", "Chi-Chun", ""]]}, {"id": "1607.03687", "submitter": "Daniele Marinazzo", "authors": "Frederik van de Steen, Luca Faes, Esin Karahan, Jitkomut Songsiri,\n  Pedro Antonio Valdes Sosa, Daniele Marinazzo", "title": "Critical comments on EEG sensor space dynamical connectivity analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many different analysis techniques have been developed and applied to EEG\nrecordings that allow one to investigate how different brain areas interact.\nOne particular class of methods, based on the linear parametric representation\nof multiple interacting time series, is widely used to study causal\nconnectivity in the brain. However, the results obtained by these methods\nshould be interpreted with great care. The goal of this paper is to show, both\ntheoretically and using simulations, that results obtained by applying causal\nconnectivity measures on the sensor (scalp) time series do not allow\ninterpretation in terms of interacting brain sources. This is because 1) the\nchannel locations cannot be seen as an approximation of a source's anatomical\nlocation and 2) spurious connectivity can occur between sensors. Although many\nmeasures of causal connectivity derived from EEG sensor time series are\naffected by the latter, here we will focus on the well-known time domain index\nof Granger causality (GC) and on the frequency domain directed transfer\nfunction (DTF). Using the state-space framework and designing two simulation\nstudies we show that mixing effects caused by volume conduction can lead to\nspurious connections, detected either by time domain GC or by DTF. Therefore,\nGC/DTF causal connectivity measures should be computed at the source level, or\nderived within analysis frameworks that model the effects of volume conduction.\nSince mixing effects can also occur in the source space, it is advised to\ncombine source space analysis with connectivity measures that are robust to\nmixing.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 11:54:08 GMT"}, {"version": "v2", "created": "Tue, 8 Nov 2016 13:18:45 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["van de Steen", "Frederik", ""], ["Faes", "Luca", ""], ["Karahan", "Esin", ""], ["Songsiri", "Jitkomut", ""], ["Sosa", "Pedro Antonio Valdes", ""], ["Marinazzo", "Daniele", ""]]}, {"id": "1607.03765", "submitter": "Antonio D'Ambrosio Dr.", "authors": "Roberta Siciliano, Massimo Aria, Antonio D'Ambrosio and Valentina\n  Cozza", "title": "Dynamic recursive tree-based partitioning for malignant melanoma\n  identification in skin lesion dermoscopic images", "comments": null, "journal-ref": null, "doi": null, "report-no": "STAD research report #01/2016", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, multivalued data or multiple values variables are defined.\nThey are typical when there is some intrinsic uncertainty in data production,\nas the result of imprecise measuring instruments, such as in image recognition,\nin human judgments and so on. \\noindent So far, contributions in symbolic data\nanalysis literature provide data preprocessing criteria allowing for the use of\nstandard methods such as factorial analysis, clustering, discriminant analysis,\ntree-based methods. As an alternative, this paper introduces a methodology for\nsupervised classification, the so-called Dynamic CLASSification TREE (D-CLASS\nTREE), dealing simultaneously with both standard and multivalued data as well.\nFor that, an innovative partitioning criterion with a tree-growing algorithm\nwill be defined. Main result is a dynamic tree structure characterized by the\nsimultaneous presence of binary and ternary partitions. A real world case study\nwill be considered to show the advantages of the proposed methodology and main\nissues of the interpretation of the final results. A comparative study with\nother approaches dealing with the same types of data will be also shown.\nD-CLASS TREE outperforms its competitors in terms of accuracy, which is a\nfundamental aspect for predictive learning.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 12:38:12 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Siciliano", "Roberta", ""], ["Aria", "Massimo", ""], ["D'Ambrosio", "Antonio", ""], ["Cozza", "Valentina", ""]]}, {"id": "1607.03855", "submitter": "Andrew Poppick", "authors": "Andrew Poppick, Elisabeth J. Moyer, and Michael L. Stein", "title": "Estimating trends in the global mean temperature record", "comments": "38 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given uncertainties in physical theory and numerical climate simulations, the\nhistorical temperature record is often used as a source of empirical\ninformation about climate change. Many historical trend analyses appear to\ndeemphasize physical and statistical assumptions: examples include regression\nmodels that treat time rather than radiative forcing as the relevant covariate\nand time series methods that account for internal variability\nnonparametrically. However, given a limited record and the presence of internal\nvariability, estimating radiatively forced historical temperature trends\nnecessarily requires assumptions. Ostensibly empirical methods can involve an\ninherent conflict in assumptions: they require data records that are short\nenough for naive trend models to apply but long enough for internal variability\nto be accounted for. In the context of global mean temperatures, methods that\ndeemphasize assumptions can therefore produce misleading inferences, because\nthe twentieth century trend is complex and the scale of correlation is long\nrelative to the data length. We illustrate how a simple but physically\nmotivated trend model can provide better-fitting and more broadly applicable\ntrend estimates and can address a wider array of questions. The model allows\none to distinguish, within a single framework, between uncertainties in the\nshorter-term versus longer-term response to radiative forcing, with\nimplications not only on historical trends but also on uncertainties in future\nprojections. We also investigate the consequence on inferred uncertainties of\nthe choice of a statistical description of internal variability. While\nnonparametric methods may seem to avoid making explicit assumptions, we\ndemonstrate how even misspecified parametric methods, if attuned to important\ncharacteristics of internal variability, can result in more accurate statements\nabout trend uncertainty.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 18:35:52 GMT"}, {"version": "v2", "created": "Fri, 30 Dec 2016 20:13:37 GMT"}, {"version": "v3", "created": "Sun, 14 May 2017 23:04:21 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Poppick", "Andrew", ""], ["Moyer", "Elisabeth J.", ""], ["Stein", "Michael L.", ""]]}, {"id": "1607.04157", "submitter": "Andrew Gelman", "authors": "Rayleigh Lei, Andrew Gelman, and Yair Ghitza", "title": "The 2008 election: A preregistered replication analysis", "comments": "This article is a review and preregistration plan. It will be\n  published, along with a new Section 5 describing the results of the\n  preregistered analysis, in Statistics and Public Policy", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an increasingly stringent set of replications of Ghitza & Gelman\n(2013), a multilevel regression and poststratification analysis of polls from\nthe 2008 U.S. presidential election campaign, focusing on a set of plots\nshowing the estimated Republican vote share for whites and for all voters, as a\nfunction of income level in each of the states.\n  We start with a nearly-exact duplication that uses the posted code and\nchanges only the model-fitting algorithm; we then replicate using\nalready-analyzed data from 2004; and finally we set up preregistered\nreplications using two surveys from 2008 that we had not previously looked at.\nWe have already learned from our preliminary, non-preregistered replication,\nwhich has revealed a potential problem with the published analysis of Ghitza &\nGelman (2013); it appears that our model may not sufficiently account for\nnonsampling error, and that some of the patterns presented in that earlier\npaper may simply reflect noise.\n  In addition to the substantive interest in validating earlier findings about\ndemographics, geography, and voting, the present project serves as a\ndemonstration of preregistration in a setting where the subject matter is\nhistorical (and thus the replication data exist before the preregistration plan\nis written) and where the analysis is exploratory (and thus a replication\ncannot be simply deemed successful or unsuccessful based on the statistical\nsignificance of some particular comparison).\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 22:51:57 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Lei", "Rayleigh", ""], ["Gelman", "Andrew", ""], ["Ghitza", "Yair", ""]]}, {"id": "1607.04215", "submitter": "Flora Alarcon", "authors": "Flora Alarcon and Gregory Nuel and Violaine Plante-Bordeneuve", "title": "Semi-Parametric Survival Estimation for pedigrees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mendelian diseases are determined by a single mutation in a given gene.\nHowever, in the case of diseases with late onset, the age at onset is variable;\nit can even be the case that the onset is not observed in a lifetime.\nEstimating the survival function of the mutation carriers and the effect of\nmodifying factors such as the sex, mutation, origin, etc, is a task of\nimportance, both for management of mutation carriers and for prevention. In\nthis work, we present a semi-parametric method based on a proportional to\nestimate the survival function using pedigrees ascertained through affected\nindividuals (probands). Not all members of the pedigree need to be genotyped.\nThe ascertainment bias is corrected by using only the phenotypic information\nfrom the relatives of the proband, and not of the proband himself. The method\nmanage ungenotyped individuals through belief propagation in Bayesian networks\nand uses an EM algorithm to compute a Kaplan-Meier estimator of the survival\nfunction. The method is illustrated on simulated data and on a samples of\nfamilies with transthyretin-related hereditary amyloidosis, a rare autosomal\ndominant disease with highly variable age of onset.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 17:12:28 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Alarcon", "Flora", ""], ["Nuel", "Gregory", ""], ["Plante-Bordeneuve", "Violaine", ""]]}, {"id": "1607.04424", "submitter": "Robert Dahl Jacobsen", "authors": "Robert Dahl Jacobsen and Jesper M{\\o}ller and Morten Nielsen and\n  Morten Grud Rasmussen", "title": "Investigations of the effects of random sampling patterns on the\n  stability of generalized sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate how the choice of spatial point process for generating random\nsampling patterns affects the numerical stability of non-uniform generalized\nsampling between Fourier bases and Daubechies scaling functions. Specifically,\nwe consider binomial, Poisson and determinantal point processes and demonstrate\nthat the more regular point patterns from the determinantal point process are\nsuperior.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 09:15:45 GMT"}, {"version": "v2", "created": "Sun, 17 Sep 2017 19:56:22 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Jacobsen", "Robert Dahl", ""], ["M\u00f8ller", "Jesper", ""], ["Nielsen", "Morten", ""], ["Rasmussen", "Morten Grud", ""]]}, {"id": "1607.04532", "submitter": "Gregor Kastner", "authors": "Florian Huber, Gregor Kastner, Martin Feldkircher", "title": "Should I stay or should I go? A latent threshold approach to large-scale\n  mixture innovation models", "comments": null, "journal-ref": "Journal of Applied Econometrics 34(5), 621-640 (2019)", "doi": "10.1002/jae.2680", "report-no": null, "categories": "stat.ME econ.EM stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a straightforward algorithm to carry out inference in\nlarge time-varying parameter vector autoregressions (TVP-VARs) with mixture\ninnovation components for each coefficient in the system. We significantly\ndecrease the computational burden by approximating the latent indicators that\ndrive the time-variation in the coefficients with a latent threshold process\nthat depends on the absolute size of the shocks. The merits of our approach are\nillustrated with two applications. First, we forecast the US term structure of\ninterest rates and demonstrate forecast gains of the proposed mixture\ninnovation model relative to other benchmark models. Second, we apply our\napproach to US macroeconomic data and find significant evidence for\ntime-varying effects of a monetary policy tightening.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 14:42:32 GMT"}, {"version": "v2", "created": "Sun, 11 Sep 2016 16:28:16 GMT"}, {"version": "v3", "created": "Tue, 5 Sep 2017 12:25:18 GMT"}, {"version": "v4", "created": "Fri, 12 Jan 2018 15:18:18 GMT"}, {"version": "v5", "created": "Thu, 26 Jul 2018 05:38:46 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Huber", "Florian", ""], ["Kastner", "Gregor", ""], ["Feldkircher", "Martin", ""]]}, {"id": "1607.04736", "submitter": "Jianxi Su", "authors": "Edward Furman, Alexey Kuznetsov, Jianxi Su, Ricardas Zitikis", "title": "Tail dependence of the Gaussian copula revisited", "comments": "Insurance Mathematics and Economics, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tail dependence refers to clustering of extreme events. In the context of\nfinancial risk management, the clustering of high-severity risks has a\ndevastating effect on the well-being of firms and is thus of pivotal importance\nin risk analysis.When it comes to quantifying the extent of tail dependence, it\nis generally agreed that measures of tail dependence must be independent of the\nmarginal distributions of the risks but rather solely copula-dependent. Indeed,\nall classical measures of tail dependence are such, but they investigate the\namount of tail dependence along the main diagonal of copulas, which has often\nlittle in common with the concentration of extremes in the copulas' domain of\ndefinition.In this paper we urge that the classical measures of tail dependence\nmay underestimate the level of tail dependence in copulas. For the Gaussian\ncopula, however, we prove that the classical measures are maximal. The\nimplication of the result is two-fold: On the one hand, it means that in the\nGaussian case, the (weak) measures of tail dependence that have been reported\nand used are of utmost prudence, which must be a reassuring news for\npractitioners. On the other hand, it further encourages substitution of the\nGaussian copula with other copulas that are more tail dependent.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jul 2016 13:09:27 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Furman", "Edward", ""], ["Kuznetsov", "Alexey", ""], ["Su", "Jianxi", ""], ["Zitikis", "Ricardas", ""]]}, {"id": "1607.04950", "submitter": "Michael Wojnowicz", "authors": "Michael Wojnowicz, Glenn Chisholm, Matt Wolff, Xuan Zhao", "title": "Wavelet decomposition of software entropy reveals symptoms of malicious\n  code", "comments": "Post print of paper published in Journal of Innovation in Digital\n  Ecosystems. This corrects typos introduced during editing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sophisticated malware authors can sneak hidden malicious code into portable\nexecutable files, and this code can be hard to detect, especially if encrypted\nor compressed. However, when an executable file switches between code regimes\n(e.g. native, encrypted, compressed, text, and padding), there are\ncorresponding shifts in the file's representation as an entropy signal. In this\npaper, we develop a method for automatically quantifying the extent to which\npatterned variations in a file's entropy signal make it \"suspicious.\" In\nExperiment 1, we use wavelet transforms to define a Suspiciously Structured\nEntropic Change Score (SSECS), a scalar feature that quantifies the\nsuspiciousness of a file based on its distribution of entropic energy across\nmultiple levels of spatial resolution. Based on this single feature, it was\npossible to raise predictive accuracy on a malware detection task from 50.0% to\n68.7%, even though the single feature was applied to a heterogeneous corpus of\nmalware discovered \"in the wild.\" In Experiment 2, we describe how\nwavelet-based decompositions of software entropy can be applied to a parasitic\nmalware detection task involving large numbers of samples and features. By\nextracting only string and entropy features (with wavelet decompositions) from\nsoftware samples, we are able to obtain almost 99% detection of parasitic\nmalware with fewer than 1% false positives on good files. Moreover, the\naddition of wavelet-based features uniformly improved detection performance\nacross plausible false positive rates, both in a strings-only model (e.g., from\n80.90% to 82.97%) and a strings-plus-entropy model (e.g. from 92.10% to 94.74%,\nand from 98.63% to 98.90%). Overall, wavelet decomposition of software entropy\ncan be useful for machine learning models for detecting malware based on\nextracting millions of features from executable files.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 05:12:20 GMT"}, {"version": "v2", "created": "Fri, 2 Feb 2018 06:32:38 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Wojnowicz", "Michael", ""], ["Chisholm", "Glenn", ""], ["Wolff", "Matt", ""], ["Zhao", "Xuan", ""]]}, {"id": "1607.05150", "submitter": "Patrick Steven Medina", "authors": "Patrick S. Medina and R.W. Doerge", "title": "Statistical Methods in Topological Data Analysis for Complex,\n  High-Dimensional Data", "comments": "15 pages, 7 Figures, 27th Annual Conference on Applied Statistics in\n  Agriculture", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The utilization of statistical methods an their applications within the new\nfield of study known as Topological Data Analysis has has tremendous potential\nfor broadening our exploration and understanding of complex, high-dimensional\ndata spaces. This paper provides an introductory overview of the mathematical\nunderpinnings of Topological Data Analysis, the workflow to convert samples of\ndata to topological summary statistics, and some of the statistical methods\ndeveloped for performing inference on these topological summary statistics. The\nintention of this non-technical overview is to motivate statisticians who are\ninterested in learning more about the subject.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 16:00:05 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Medina", "Patrick S.", ""], ["Doerge", "R. W.", ""]]}, {"id": "1607.05154", "submitter": "Giorgio Matteo Vitetta Prof.", "authors": "Martino Uccellari, Francesca Facchini, Matteo Sola, Emilio Sirignano,\n  Giorgio M. Vitetta, Andrea Barbieri and Stefano Tondelli", "title": "On the Application of Support Vector Machines to the Prediction of\n  Propagation Losses at 169 MHz for Smart Metering Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the need of deploying new wireless networks for smart gas metering\nhas raised the problem of radio planning in the169 MHz band. Unluckily,\nsoftware tools commonly adopted for radio planning in cellular communication\nsystems cannot be employed to solve this problem because of the substantially\nlower transmission frequencies characterizing this application. In this\nmanuscript a novel data-centric solution, based on the use of support vector\nmachine techniques for classification and regression, is proposed. Our method\nrequires the availability of a limited set of received signal strength\nmeasurements and the knowledge of a three-dimensional map of the propagation\nenvironment of interest, and generates both an estimate of the coverage area\nand a prediction of the field strength within it. Numerical results referring\nto different Italian villages and cities evidence that our method is able to\nachieve good accuracy at the price of an acceptable computational cost and of a\nlimited effort for the acquisition of measurements in the considered\nenvironments.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 16:08:36 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Uccellari", "Martino", ""], ["Facchini", "Francesca", ""], ["Sola", "Matteo", ""], ["Sirignano", "Emilio", ""], ["Vitetta", "Giorgio M.", ""], ["Barbieri", "Andrea", ""], ["Tondelli", "Stefano", ""]]}, {"id": "1607.05376", "submitter": "Camelia Simoiu", "authors": "Camelia Simoiu, Sam Corbett-Davies, Sharad Goel", "title": "The Problem of Infra-marginality in Outcome Tests for Discrimination", "comments": "To appear in The Annals of Applied Statistics, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outcome tests are a popular method for detecting bias in lending, hiring, and\npolicing decisions. These tests operate by comparing the success rate of\ndecisions across groups. For example, if loans made to minority applicants are\nobserved to be repaid more often than loans made to whites, it suggests that\nonly exceptionally qualified minorities are granted loans, indicating\ndiscrimination. Outcome tests, however, are known to suffer from the problem of\ninfra-marginality: even absent discrimination, the repayment rates for minority\nand white loan recipients might differ if the two groups have different risk\ndistributions. Thus, at least in theory, outcome tests can fail to accurately\ndetect discrimination. We develop a new statistical test of\ndiscrimination---the threshold test---that mitigates the problem of\ninfra-marginality by jointly estimating decision thresholds and risk\ndistributions via a hierarchical Bayesian latent variable model. Applying our\ntest to a dataset of 4.5 million police stops in North Carolina, we find that\nthe problem of infra-marginality is more than a theoretical possibility, and\ncan cause the outcome test to yield misleading results in practice.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 02:19:51 GMT"}, {"version": "v2", "created": "Thu, 5 Jan 2017 18:00:00 GMT"}, {"version": "v3", "created": "Fri, 6 Jan 2017 06:01:55 GMT"}, {"version": "v4", "created": "Wed, 17 May 2017 16:58:24 GMT"}, {"version": "v5", "created": "Tue, 20 Jun 2017 17:06:02 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Simoiu", "Camelia", ""], ["Corbett-Davies", "Sam", ""], ["Goel", "Sharad", ""]]}, {"id": "1607.05380", "submitter": "Keisuke Fujii", "authors": "Keisuke Fujii, Ichihiro Yamada, Masahiro Hasuo", "title": "Data-Driven Sensitivity Inference for Thomson Scattering Electron\n  Density Measurement Systems", "comments": null, "journal-ref": null, "doi": "10.1063/1.4974344", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We developed a method to infer the calibration parameters of multichannel\nmeasurement systems, such as channel variations of sensitivity and noise\namplitude, from experimental data. We regard such uncertainties of the\ncalibration parameters as dependent noise. The statistical properties of the\ndependent noise and that of the latent functions were modeled and implemented\nin the Gaussian process kernel. Based on their statistical difference, both\nparameters were inferred from the data.\n  We applied this method to the electron density measurement system by Thomson\nscattering for Large Helical Device plasma, which is equipped with 141 spatial\nchannels. Based on the 210 sets of experimental data, we evaluated the\ncorrection factor of the sensitivity and noise amplitude for each channel. The\ncorrection factor varies by $\\approx$ 10\\%, and the random noise amplitude is\n$\\approx$ 2\\%, i.e., the measurement accuracy increases by a factor of 5 after\nthis sensitivity correction. The certainty improvement in the spatial\nderivative inference was demonstrated.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 02:33:38 GMT"}, {"version": "v2", "created": "Tue, 23 Aug 2016 08:30:26 GMT"}, {"version": "v3", "created": "Thu, 13 Oct 2016 10:02:52 GMT"}, {"version": "v4", "created": "Mon, 12 Dec 2016 04:46:11 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Fujii", "Keisuke", ""], ["Yamada", "Ichihiro", ""], ["Hasuo", "Masahiro", ""]]}, {"id": "1607.05573", "submitter": "Ruimin Zhu", "authors": "Ruimin Zhu, Wenxin Jiang", "title": "Combining Random Walks and Nonparametric Bayesian Topic Model for\n  Community Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection has been an active research area for decades. Among all\nprobabilistic models, Stochastic Block Model has been the most popular one.\nThis paper introduces a novel probabilistic model: RW-HDP, based on random\nwalks and Hierarchical Dirichlet Process, for community extraction. In RW-HDP,\nrandom walks conducted in a social network are treated as documents; nodes are\ntreated as words. By using Hierarchical Dirichlet Process, a nonparametric\nBayesian model, we are not only able to cluster nodes into different\ncommunities, but also determine the number of communities automatically. We use\nStochastic Variational Inference for our model inference, which makes our\nmethod time efficient and can be easily extended to an online learning\nalgorithm.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 13:46:15 GMT"}, {"version": "v2", "created": "Wed, 3 Aug 2016 01:39:38 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Zhu", "Ruimin", ""], ["Jiang", "Wenxin", ""]]}, {"id": "1607.05861", "submitter": "Roberto Molinari Mr", "authors": "St\\'ephane Guerrier and Roberto Molinari", "title": "Fast and Robust Parametric Estimation for Time Series and Spatial Models", "comments": "arXiv admin note: substantial text overlap with arXiv:1512.09325", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new framework for robust estimation and inference on\nsecond-order stationary time series and random fields. This framework is based\non the Generalized Method of Wavelet Moments which uses the wavelet variance to\nachieve parameter estimation for complex models. Using an M-estimator of the\nwavelet variance, this method can be made robust therefore allowing to estimate\nthe parameters of a wide range of time series and spatial models when the data\nsuffers from outliers or different forms of contamination. The paper presents a\nseries of simulation studies as well as a range of applications where this new\napproach can be considered as a computationally efficient, numerically stable\nand robust method which performs at least as well as existing methods in\nbounding the influence of outliers on the estimation procedure.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 08:40:19 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Guerrier", "St\u00e9phane", ""], ["Molinari", "Roberto", ""]]}, {"id": "1607.06020", "submitter": "Yan Shang", "authors": "Andr\\'es Musalem, Yan Shang, Jing-Sheng Song", "title": "An Empirical Study of Customer Spillover Learning about Service Quality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"Spillover\" learning is defined as customers' learning about the quality of a\nservice (or product) from their previous experiences with similar yet not\nidentical services. In this paper, we propose a novel, parsimonious and general\nBayesian hierarchical learning framework for estimating customers' spillover\nlearning. We apply our model to a one-year shipping/sales historical data\nprovided by a world-leading third party logistics company and study how\ncustomers' experiences from shipping on a particular route affect their future\ndecisions about shipping not only on that route, but also on other routes\nserviced by the same logistics company. Our empirical results are consistent\nwith information spillovers driving customer choices. Customers also display an\nasymmetric response such that they are more sensitive to delays than early\ndeliveries. In addition, we find that customers are risk averse being more\nsensitive to their uncertainty about the mean service quality than to the\nintrinsic variability of the service. Finally, we develop policy simulation\nstudies to show the importance of accounting for customer learning when a firm\nconsiders service quality improvement decisions.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 16:47:46 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Musalem", "Andr\u00e9s", ""], ["Shang", "Yan", ""], ["Song", "Jing-Sheng", ""]]}, {"id": "1607.06247", "submitter": "Richard Tol", "authors": "Monika Novackova and Richard S.J. Tol", "title": "Effects of Sea Level Rise on Economy of the United States", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.EC econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report the first ex post study of the economic impact of sea level rise.\nWe apply two econometric approaches to estimate the past effects of sea level\nrise on the economy of the USA, viz. Barro type growth regressions adjusted for\nspatial patterns and a matching estimator. Unit of analysis is 3063 counties of\nthe USA. We fit growth regressions for 13 time periods and we estimated\nnumerous varieties and robustness tests for both growth regressions and\nmatching estimator. Although there is some evidence that sea level rise has a\npositive effect on economic growth, in most specifications the estimated\neffects are insignificant. We therefore conclude that there is no stable,\nsignificant effect of sea level rise on economic growth. This finding\ncontradicts previous ex ante studies.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 09:43:56 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Novackova", "Monika", ""], ["Tol", "Richard S. J.", ""]]}, {"id": "1607.06288", "submitter": "Matthias Eckardt", "authors": "Matthias Eckardt, Jorge Mateu", "title": "Point patterns occurring on complex structures in space and space-time:\n  An alternative network approach", "comments": null, "journal-ref": null, "doi": "10.1080/10618600.2017.1391695", "report-no": null, "categories": "stat.AP physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an alternative approach of analyzing possibly multitype\npoint patterns in space and space-time that occur on network structures, and\nintroduces several different graph-related intensity measures. The proposed\nformalism allows to control for processes on undirected, directional as well as\npartially directed network structures and is not restricted to linearity or\ncircularity.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 12:01:58 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Eckardt", "Matthias", ""], ["Mateu", "Jorge", ""]]}, {"id": "1607.06307", "submitter": "Jonas Wallin", "authors": "Jonas Wallin and Kjell Wallin", "title": "Estimating the unobservable moose - converting index to population size\n  using a Bayesian Hierarchical state space model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indirect information on population size, like pellet counts or volunteer\ncounts, is the main source of information in most ecological studies and\napplied population management situations. Often, such observations are treaded\nas if they were actual measurements of population size. This assumption results\nin incorrect conclusions about a population's size and its dynamics. We propose\na model with a temporal varying link, denoted countability, between indirect\nobservations and actual population size. We show that, when indirect\nmeasurement has high precision (for instance many observation hours) the\nassumption of temporal varying countability can have a crucial effect on the\nestimated population dynamic. We apply the model on two local moose populations\nin Sweden. The estimated population dynamics is found to explain 30-50 percent\nof the total variability in the observation data; thus, countability accounts\nfor most of the variation. This unreliability of the estimated dynamics has a\nsubstantial negative impact on the ability to manage populations; for example,\nreducing (increasing) the number of animals that needs to be harvested in order\nto sustain the population above (below) a fixed level. Finally, large\ndifference in countability between two study areas implies a substantial\nspatial variation in the countability; this variation in itself is highly\nworthy of study.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 13:11:37 GMT"}, {"version": "v2", "created": "Sun, 25 Sep 2016 11:06:09 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Wallin", "Jonas", ""], ["Wallin", "Kjell", ""]]}, {"id": "1607.06358", "submitter": "Ian Vernon Dr", "authors": "Ian Vernon and Junli Liu and Michael Goldstein and James Rowe and Jen\n  Topping and Keith Lindsey", "title": "Bayesian uncertainty analysis for complex systems biology models:\n  emulation, global parameter searches and evaluation of gene functions", "comments": "26 pages, 13 figures. Version accepted by BMC systems biology", "journal-ref": "BMC Systems Biology (2018), 12(1)", "doi": "10.1186/s12918-017-0484-3", "report-no": null, "categories": "q-bio.MN q-bio.CB q-bio.QM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Many mathematical models have now been employed across every area\nof systems biology. These models increasingly involve large numbers of unknown\nparameters, have complex structure which can result in substantial evaluation\ntime relative to the needs of the analysis, and need to be compared to observed\ndata. The correct analysis of such models usually requires a global parameter\nsearch, over a high dimensional parameter space, that incorporates and respects\nthe most important sources of uncertainty. This can be an extremely difficult\ntask, but it is essential for any meaningful inference or prediction to be made\nabout any biological system. It hence represents a fundamental challenge for\nthe whole of systems biology.\n  Results: Bayesian statistical methodology for the uncertainty analysis of\ncomplex models is introduced, which is designed to address the high dimensional\nglobal parameter search problem. Bayesian emulators that mimic the systems\nbiology model but which are extremely fast to evaluate are embedded within an\niterative history match: an efficient method to search high dimensional spaces\nwithin a more formal statistical setting, while incorporating major sources of\nuncertainty. The approach is demonstrated via application to two models of\nhormonal crosstalk in Arabidopsis root development, which have 32 rate\nparameters, for which we identify the sets of rate parameter values that lead\nto acceptable matches to observed trend data. The biological consequences of\nthe resulting comparison, including the evaluation of gene functions, are\ndescribed.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 15:10:57 GMT"}, {"version": "v2", "created": "Fri, 12 Jan 2018 11:36:41 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Vernon", "Ian", ""], ["Liu", "Junli", ""], ["Goldstein", "Michael", ""], ["Rowe", "James", ""], ["Topping", "Jen", ""], ["Lindsey", "Keith", ""]]}, {"id": "1607.06407", "submitter": "Julian Straub", "authors": "Julian Straub, Trevor Campbell, Jonathan P. How, John W. Fisher III", "title": "Small-Variance Nonparametric Clustering on the Hypersphere", "comments": null, "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\n  (pp. 334-342). (2015)", "doi": "10.1109/CVPR.2015.7298630", "report-no": null, "categories": "cs.CV math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural regularities in man-made environments reflect in the distribution\nof their surface normals. Describing these surface normal distributions is\nimportant in many computer vision applications, such as scene understanding,\nplane segmentation, and regularization of 3D reconstructions. Based on the\nsmall-variance limit of Bayesian nonparametric von-Mises-Fisher (vMF) mixture\ndistributions, we propose two new flexible and efficient k-means-like\nclustering algorithms for directional data such as surface normals. The first,\nDP-vMF-means, is a batch clustering algorithm derived from the Dirichlet\nprocess (DP) vMF mixture. Recognizing the sequential nature of data collection\nin many applications, we extend this algorithm to DDP-vMF-means, which infers\ntemporally evolving cluster structure from streaming data. Both algorithms\nnaturally respect the geometry of directional data, which lies on the unit\nsphere. We demonstrate their performance on synthetic directional data and real\n3D surface normals from RGB-D sensors. While our experiments focus on 3D data,\nboth algorithms generalize to high dimensional directional data such as protein\nbackbone configurations and semantic word vectors.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 17:52:08 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Straub", "Julian", ""], ["Campbell", "Trevor", ""], ["How", "Jonathan P.", ""], ["Fisher", "John W.", "III"]]}, {"id": "1607.06447", "submitter": "Andrea Gabrio", "authors": "Andrea Gabrio, Alexina Mason, Gianluca Baio", "title": "Handling Missing Data in Within-Trial Cost-Effectiveness Analysis: a\n  Review with Future Guidelines", "comments": "13 pages, 5 figures, 1 table, references omitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cost-Effectiveness Analyses (CEAs) alongside randomised controlled trials\n(RCTs) are increasingly often designed to collect resource use and\npreference-based health status data for the purpose of healthcare technology\nassessment. However, because of the way these measures are collected, they are\nprone to missing data, which can ultimately affect the decision of whether an\nintervention is good value for money. We examine how missing cost and effect\noutcome data are handled in RCT-based CEAs, complementing a previous review\n(covering 2003-2009, 88 articles) with a new systematic review (2009-2015, 81\narticles) focussing on two different perspectives. First, we review the\ndescription of the missing data, the statistical methods used to deal with\nthem, and the quality of the judgement underpinning the choice of these\nmethods. Second, we provide guidelines on how the information about missingness\nand related methods should be presented to improve the reporting and handling\nof missing data. Our review shows that missing data in within-RCT CEAs are\nstill often inadequately handled and the overall level of information provided\nto support the chosen methods is rarely satisfactory.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 19:54:15 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Gabrio", "Andrea", ""], ["Mason", "Alexina", ""], ["Baio", "Gianluca", ""]]}, {"id": "1607.06635", "submitter": "Lucio Anderlini", "authors": "Lucio Anderlini", "title": "Density Estimation Trees as fast non-parametric modelling tools", "comments": "Presented at the Workshop on Advanced Computing and Analysis\n  Techniques (ACAT2016)", "journal-ref": null, "doi": "10.1088/1742-6596/762/1/012042", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Density Estimation Trees (DETs) are decision trees trained on a multivariate\ndataset to estimate its probability density function. While not competitive\nwith kernel techniques in terms of accuracy, they are incredibly fast,\nembarrassingly parallel and relatively small when stored to disk. These\nproperties make DETs appealing in the resource-expensive horizon of the LHC\ndata analysis. Possible applications may include selection optimization, fast\nsimulation and fast detector calibration. In this contribution I describe the\nalgorithm, made available to the HEP community in a RooFit implementation. A\nset of applications under discussion within the LHCb Collaboration are also\nbriefly illustrated.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 11:23:15 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Anderlini", "Lucio", ""]]}, {"id": "1607.06685", "submitter": "Matthias Eckardt", "authors": "Matthias Eckardt, Jorge Mateu", "title": "Structured network regression for spatial point patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of spatial point patterns that occur in the network domain have\nrecently gained much attraction and various intensity functions and measures\nhave been proposed. However, the linkage of spatial network statistics to\nregression models has not been approached so far. This paper presents a new\nregression approach which treats a generic intensity function of a planar point\npattern that occurred on a network as the outcome of a set of different\ncovariates and various graph statistics. Different to all alternative\napproaches, our model is the first which permits the statistical analysis of\ncomplex regression data in the context of network intensity functions for\nspatial point patterns. The potential of our new technique to model the\nstructural dependencies of network intensity functions on various covariates\nand graph statistics is illustrated using call-in data on neighbour and\ncommunity disturbances in an urban context.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 14:06:57 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Eckardt", "Matthias", ""], ["Mateu", "Jorge", ""]]}, {"id": "1607.06779", "submitter": "Robert J. B. Goudie", "authors": "Robert J. B. Goudie, Anne M. Presanis, David Lunn, Daniela De Angelis\n  and Lorenz Wernisch", "title": "Joining and splitting models with Markov melding", "comments": null, "journal-ref": null, "doi": "10.1214/18-BA1104", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysing multiple evidence sources is often feasible only via a modular\napproach, with separate submodels specified for smaller components of the\navailable evidence. Here we introduce a generic framework that enables fully\nBayesian analysis in this setting. We propose a generic method for forming a\nsuitable joint model when joining submodels, and a convenient computational\nalgorithm for fitting this joint model in stages, rather than as a single,\nmonolithic model. The approach also enables splitting of large joint models\ninto smaller submodels, allowing inference for the original joint model to be\nconducted via our multi-stage algorithm. We motivate and demonstrate our\napproach through two examples: joining components of an evidence synthesis of\nA/H1N1 influenza, and splitting a large ecology model.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 18:12:30 GMT"}, {"version": "v2", "created": "Fri, 2 Sep 2016 17:40:12 GMT"}, {"version": "v3", "created": "Tue, 12 Sep 2017 20:46:30 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Goudie", "Robert J. B.", ""], ["Presanis", "Anne M.", ""], ["Lunn", "David", ""], ["De Angelis", "Daniela", ""], ["Wernisch", "Lorenz", ""]]}, {"id": "1607.07083", "submitter": "Matthias Eckardt", "authors": "Matthias Eckardt", "title": "Graphical modelling of multivariate spatial point processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel graphical model, termed the spatial dependence\ngraph model, which captures the global dependence structure of different events\nthat occur randomly in space. In the spatial dependence graph model, the edge\nset is identified by using the conditional partial spectral coherence. Thereby,\nnodes are related to the components of a multivariate spatial point process and\nedges express orthogonality relation between the single components. This paper\nintroduces an efficient approach towards pattern analysis of highly structured\nand high dimensional spatial point processes. Unlike all previous methods, our\nnew model permits the simultaneous analysis of all multivariate conditional\ninterrelations. The potential of our new technique to investigate multivariate\nstructural relations is illustrated using data on forest stands in Lansing\nWoods as well as monthly data on crimes committed in the City of London.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jul 2016 19:30:11 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Eckardt", "Matthias", ""]]}, {"id": "1607.07423", "submitter": "Arin Chaudhuri", "authors": "Deovrat Kakde, Sergriy Peredriy, Arin Chaudhuri, Anya Mcguirk", "title": "A Non-Parametric Control Chart For High Frequency Multivariate Data", "comments": null, "journal-ref": null, "doi": "10.1109/RAM.2017.7889786", "report-no": null, "categories": "cs.LG stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support Vector Data Description (SVDD) is a machine learning technique used\nfor single class classification and outlier detection. SVDD based K-chart was\nfirst introduced by Sun and Tsung for monitoring multivariate processes when\nunderlying distribution of process parameters or quality characteristics depart\nfrom Normality. The method first trains a SVDD model on data obtained from\nstable or in-control operations of the process to obtain a threshold $R^2$ and\nkernel center a. For each new observation, its Kernel distance from the Kernel\ncenter a is calculated. The kernel distance is compared against the threshold\n$R^2$ to determine if the observation is within the control limits. The\nnon-parametric K-chart provides an attractive alternative to the traditional\ncontrol charts such as the Hotelling's $T^2$ charts when distribution of the\nunderlying multivariate data is either non-normal or is unknown. But there are\nchallenges when K-chart is deployed in practice. The K-chart requires\ncalculating kernel distance of each new observation but there are no guidelines\non how to interpret the kernel distance plot and infer about shifts in process\nmean or changes in process variation. This limits the application of K-charts\nin big-data applications such as equipment health monitoring, where\nobservations are generated at a very high frequency. In this scenario, the\nanalyst using the K-chart is inundated with kernel distance results at a very\nhigh frequency, generally without any recourse for detecting presence of any\nassignable causes of variation. We propose a new SVDD based control chart,\ncalled as $K_T$ chart, which addresses challenges encountered when using\nK-chart for big-data applications. The $K_T$ charts can be used to\nsimultaneously track process variation and central tendency. We illustrate the\nsuccessful use of $K_T$ chart using the Tennessee Eastman process data.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 19:40:55 GMT"}, {"version": "v2", "created": "Wed, 27 Jul 2016 22:23:47 GMT"}, {"version": "v3", "created": "Fri, 29 Jul 2016 20:31:54 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Kakde", "Deovrat", ""], ["Peredriy", "Sergriy", ""], ["Chaudhuri", "Arin", ""], ["Mcguirk", "Anya", ""]]}, {"id": "1607.07515", "submitter": "Shawn Mankad", "authors": "Shawn Mankad, Shengli Hu, Anandasivam Gopal", "title": "Single Stage Prediction with Embedded Topic Modeling of Online Reviews\n  for Mobile App Management", "comments": "28 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IR cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile apps are one of the building blocks of the mobile digital economy. A\ndifferentiating feature of mobile apps to traditional enterprise software is\nonline reviews, which are available on app marketplaces and represent a\nvaluable source of consumer feedback on the app. We create a supervised topic\nmodeling approach for app developers to use mobile reviews as useful sources of\nquality and customer feedback, thereby complementing traditional software\ntesting. The approach is based on a constrained matrix factorization that\nleverages the relationship between term frequency and a given response variable\nin addition to co-occurrences between terms to recover topics that are both\npredictive of consumer sentiment and useful for understanding the underlying\ntextual themes. The factorization is combined with ordinal regression to\nprovide guidance from online reviews on a single app's performance as well as\nsystematically compare different apps over time for benchmarking of features\nand consumer sentiment. We apply our approach using a dataset of over 100,000\nmobile reviews over several years for three of the most popular online travel\nagent apps from the iTunes and Google Play marketplaces.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 01:23:17 GMT"}, {"version": "v2", "created": "Sat, 5 Aug 2017 14:37:29 GMT"}, {"version": "v3", "created": "Mon, 19 Feb 2018 18:34:29 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Mankad", "Shawn", ""], ["Hu", "Shengli", ""], ["Gopal", "Anandasivam", ""]]}, {"id": "1607.07664", "submitter": "Michelle Miranda", "authors": "Michelle F. Miranda and Hongtu Zhu and Joseph G. Ibrahim", "title": "Bayesian spatial transformation models with applications in neuroimaging\n  data", "comments": null, "journal-ref": null, "doi": "10.1111/biom.12085", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to develop a class of spatial transformation models\n(STM) to spatially model the varying association between imaging measures in a\nthree-dimensional (3D) volume (or 2D surface) and a set of covariates. Our STMs\ninclude a varying Box-Cox transformation model for dealing with the issue of\nnon-Gaussian distributed imaging data and a Gaussian Markov Random Field model\nfor incorporating spatial smoothness of the imaging data. Posterior computation\nproceeds via an efficient Markov chain Monte Carlo algorithm. Simulations and\nreal data analysis demonstrate that the STM significantly outperforms the\nvoxel-wise linear model with Gaussian noise in recovering meaningful geometric\npatterns. Our STM is able to reveal important brain regions with morphological\nchanges in children with attention deficit hyperactivity disorder.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 12:21:25 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Miranda", "Michelle F.", ""], ["Zhu", "Hongtu", ""], ["Ibrahim", "Joseph G.", ""]]}, {"id": "1607.07745", "submitter": "Arin Chaudhuri", "authors": "Deovrat Kakde, Arin Chaudhuri", "title": "Leveraging Unstructured Data to Detect Emerging Reliability Issues", "comments": null, "journal-ref": null, "doi": "10.1109/RAMS.2015.7105093", "report-no": null, "categories": "cs.AI stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unstructured data refers to information that does not have a predefined data\nmodel or is not organized in a pre-defined manner. Loosely speaking,\nunstructured data refers to text data that is generated by humans. In\nafter-sales service businesses, there are two main sources of unstructured\ndata: customer complaints, which generally describe symptoms, and technician\ncomments, which outline diagnostics and treatment information. A legitimate\ncustomer complaint can eventually be tracked to a failure or a claim. However,\nthere is a delay between the time of a customer complaint and the time of a\nfailure or a claim. A proactive strategy aimed at analyzing customer complaints\nfor symptoms can help service providers detect reliability problems in advance\nand initiate corrective actions such as recalls. This paper introduces\nessential text mining concepts in the context of reliability analysis and a\nmethod to detect emerging reliability issues. The application of the method is\nillustrated using a case study.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 15:19:12 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Kakde", "Deovrat", ""], ["Chaudhuri", "Arin", ""]]}, {"id": "1607.07762", "submitter": "Zi Wang", "authors": "Zi Wang, Stefanie Jegelka, Leslie Pack Kaelbling, Tom\\'as\n  Lozano-P\\'erez", "title": "Focused Model-Learning and Planning for Non-Gaussian Continuous\n  State-Action Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.RO stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a framework for model learning and planning in stochastic\ndomains with continuous state and action spaces and non-Gaussian transition\nmodels. It is efficient because (1) local models are estimated only when the\nplanner requires them; (2) the planner focuses on the most relevant states to\nthe current planning problem; and (3) the planner focuses on the most\ninformative and/or high-value actions. Our theoretical analysis shows the\nvalidity and asymptotic optimality of the proposed approach. Empirically, we\ndemonstrate the effectiveness of our algorithm on a simulated multi-modal\npushing problem.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 15:48:03 GMT"}, {"version": "v2", "created": "Thu, 22 Sep 2016 18:08:50 GMT"}, {"version": "v3", "created": "Sun, 2 Oct 2016 05:21:17 GMT"}, {"version": "v4", "created": "Sun, 23 Oct 2016 04:05:34 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Wang", "Zi", ""], ["Jegelka", "Stefanie", ""], ["Kaelbling", "Leslie Pack", ""], ["Lozano-P\u00e9rez", "Tom\u00e1s", ""]]}, {"id": "1607.07792", "submitter": "Ali Tarhini", "authors": "Mohammed Hassouna, Ali Tarhini, Tariq Elyas, Mohammad Saeed AbouTrab", "title": "Customer Churn in Mobile Markets A Comparison of Techniques", "comments": "14 pages, 7 figures, Journal paper (2015, Vol 8 No 6, 224-237)", "journal-ref": null, "doi": "10.5539/ibr.v8n6p224", "report-no": null, "categories": "cs.CY cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The high increase in the number of companies competing in mature markets\nmakes customer retention an important factor for any company to survive. Thus,\nmany methodologies (e.g., data mining and statistics) have been proposed to\nanalyse and study customer retention. The validity of such methods is not yet\nproved though. This paper tries to fill this gap by empirically comparing two\ntechniques: Customer churn-decision tree and logistic regression models. The\npaper proves the superiority of decision tree technique and stresses the needs\nfor more advanced methods to churn modelling.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 10:42:46 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Hassouna", "Mohammed", ""], ["Tarhini", "Ali", ""], ["Elyas", "Tariq", ""], ["AbouTrab", "Mohammad Saeed", ""]]}, {"id": "1607.07970", "submitter": "Krzysztof Bartoszek", "authors": "Krzysztof Bartoszek, Sylvain Gl\\'emin, Ingemar Kaj, Martin Lascoux", "title": "The Ornstein-Uhlenbeck process with migration: evolution with\n  interactions", "comments": null, "journal-ref": "Journal of Theoretical Biology 429:35-45, 2017", "doi": "10.1016/j.jtbi.2017.06.011", "report-no": null, "categories": "q-bio.PE math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Ornstein-Uhlenbeck (OU) process plays a major role in the analysis of the\nevolution of phenotypic traits along phylogenies. The standard OU process\nincludes drift and stabilizing selection and assumes that species evolve\nindependently. However, especially in plants, there is ample evidence of\nhybridization and introgression during evolution. In this work we present a\nstatistical approach with analytical solutions that allows for the inclusion of\nadaptation and migration in a common phylogenetic framework. We furthermore\npresent a detailed simulation study that clearly indicates the adverse effects\nof ignoring migration. Similarity between species due to migration could be\nmisinterpreted as very strong convergent evolution without proper correction\nfor these additional dependencies. Our model can also be useful for studying\nlocal adaptation among populations within the same species. Finally, we show\nthat our model can be interpreted in terms of ecological interactions between\nspecies, providing a general framework for the evolution of traits between\n\"interacting\" species or populations.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 06:10:03 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Bartoszek", "Krzysztof", ""], ["Gl\u00e9min", "Sylvain", ""], ["Kaj", "Ingemar", ""], ["Lascoux", "Martin", ""]]}, {"id": "1607.08096", "submitter": "S\\'andor Baran", "authors": "S\\'andor Baran and Sebastian Lerch", "title": "Combining predictive distributions for statistical post-processing of\n  ensemble forecasts", "comments": "33 pages, 8 figures, 2 tables", "journal-ref": "International Journal of Forecasting 2018, 34, 477--496", "doi": "10.1016/j.ijforecast.2018.01.005", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical post-processing techniques are now widely used to correct\nsystematic biases and errors in calibration of ensemble forecasts obtained from\nmultiple runs of numerical weather prediction models. A standard approach is\nthe ensemble model output statistics (EMOS) method, a distributional regression\napproach where the forecast distribution is given by a single parametric law\nwith parameters depending on the ensemble members. Choosing an appropriate\nparametric family for the weather variable of interest is a critical, however,\noften non-trivial task, and has been the focus of much recent research. In this\narticle, we assess the merits of combining predictive distributions from\nmultiple EMOS models based on different parametric families. In four case\nstudies with wind speed and precipitation forecasts from two ensemble\nprediction systems, we study whether state of the art forecast combination\nmethods are able to improve forecast skill.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 14:08:16 GMT"}, {"version": "v2", "created": "Wed, 16 Nov 2016 07:15:50 GMT"}, {"version": "v3", "created": "Thu, 13 Jul 2017 09:11:25 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Baran", "S\u00e1ndor", ""], ["Lerch", "Sebastian", ""]]}, {"id": "1607.08141", "submitter": "Marta Tallarita", "authors": "Marta Tallarita, Maria De Iorio, Alessandra Guglielmi and James\n  Malone-Lee", "title": "Bayesian Nonparametric Modelling of Joint Gap Time Distributions for\n  Recurrent Event Data", "comments": "23 pages,10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose autoregressive Bayesian semi-parametric models for waiting times\nbetween recurrent events. The aim is two-fold: inference on the effect of\npossibly time-varying covariates on the gap times and clustering of individuals\nbased on the time trajectory of the recurrent event. Time-dependency between\ngap times is taken into account through the specification of an autoregressive\ncomponent for the random effects parameters influencing the response at\ndifferent times. The order of the autoregression may be assumed unknown and\nobject of inference and we consider two alternative approaches to perform model\nselection under this scenario. Covariates may be easily included in the\nregression framework and censoring and missing data are easily accounted for.\nAs the proposed methodologies lies within the class of Dirichlet process\nmixtures, posterior inference can be performed through efficient MCMC\nalgorithms. We illustrate the approach through simulations and medical\napplications involving recurrent hospitalizations of cancer patients and\nsuccessive urinary tract infections.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 15:17:52 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Tallarita", "Marta", ""], ["De Iorio", "Maria", ""], ["Guglielmi", "Alessandra", ""], ["Malone-Lee", "James", ""]]}, {"id": "1607.08205", "submitter": "Tim Tierney", "authors": "Tim M. Tierney, Christopher A. Clark, David W. Carmichael", "title": "Is Bonferroni correction more sensitive than Random Field Theory for\n  most fMRI studies?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random Field Theory has been used in the fMRI literature to address the\nmultiple comparisons problem. The method provides an analytical solution for\nthe computation of precise p-values when its assumptions are met. When its\nassumptions are not met the thresholds generated by Random Field Theory can be\nmore conservative than Bonferroni corrections, which are arguably too stringent\nfor use in fMRI. As this has been well documented theoretically it is\nsurprising that a majority of current studies (~80%) would not meet the\nassumptions of Random Field Theory and therefore would have reduced\nsensitivity. Specifically most data is not smooth enough to meet the good\nlattice assumption. Current studies smooth data on average by twice the voxel\nsize which is rarely sufficient to meet the good lattice assumption. The amount\nof smoothing required for Random Field Theory to produce accurate p-values\nincreases with image resolution and decreases with degrees of freedom. There is\nno rule of thumb that is valid for all study designs but for typical data (3mm\nresolution, and greater than 20 subjects) residual smoothness with FWHM = 4\ntimes voxel size should produce valid results. However, it should be stressed\nthat for higher spatial resolution and lower degrees of freedom the critical\nsmoothness required will increase sharply. This implies that researchers should\ncarefully choose appropriate smoothing kernels. This can be facilitated by the\nsimulations we provide that identify the critical smoothness at which the\napplication of RFT becomes appropriate. For some applications such as\npresurgical mapping or, imaging of small structures, probing the\nlaminar/columnar structure of the cortex these smoothness requirements may be\ntoo great to preserve spatial structure. As such, this study suggests\ndevelopments are needed in Random Field Theory to fully exploit the resolution\nof modern neuroimaging.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 18:17:19 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Tierney", "Tim M.", ""], ["Clark", "Christopher A.", ""], ["Carmichael", "David W.", ""]]}, {"id": "1607.08255", "submitter": "Maria Xose Rodriguez-Alvarez", "authors": "Mar\\'ia Xos\\'e Rodr\\'iguez-\\'Alvarez, Martin P. Boer, Fred A. van\n  Eeuwijk and Paul H. C. Eilers", "title": "Spatial Models for Field Trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important aim of the analysis of agricultural field trials is to obtain\ngood predictions for genotypic performance, by correcting for spatial effects.\nIn practice these corrections turn out to be complicated, since there can be\ndifferent types of spatial effects; those due to management interventions\napplied to the field plots and those due to various kinds of erratic spatial\ntrends. This paper presents models for field trials in which the random spatial\ncomponent consists of tensor product Penalized splines (P-splines). A special\nANOVA-type reformulation leads to five smooth additive spatial components,\nwhich form the basis of a mixed model with five unknown variance components. On\ntop of this spatial field, effects of genotypes, blocks, replicates, and/or\nother sources of spatial variation are described by a mixed model in a standard\nway. We show the relation between several definitions of heritability and the\neffective dimension or the effective degrees of freedom associated to the\ngenetic component. The approach is illustrated with large-scale field trial\nexperiments. An R-package is provided.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 20:04:47 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Rodr\u00edguez-\u00c1lvarez", "Mar\u00eda Xos\u00e9", ""], ["Boer", "Martin P.", ""], ["van Eeuwijk", "Fred A.", ""], ["Eilers", "Paul H. C.", ""]]}, {"id": "1607.08458", "submitter": "Alexandre Gramfort", "authors": "Daniel Strohmeier, Yousra Bekhti, Jens Haueisen, Alexandre Gramfort", "title": "The iterative reweighted Mixed-Norm Estimate for spatio-temporal MEG/EEG\n  source reconstruction", "comments": null, "journal-ref": null, "doi": "10.1109/TMI.2016.2553445", "report-no": null, "categories": "stat.AP q-bio.NC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Source imaging based on magnetoencephalography (MEG) and\nelectroencephalography (EEG) allows for the non-invasive analysis of brain\nactivity with high temporal and good spatial resolution. As the\nbioelectromagnetic inverse problem is ill-posed, constraints are required. For\nthe analysis of evoked brain activity, spatial sparsity of the neuronal\nactivation is a common assumption. It is often taken into account using convex\nconstraints based on the l1-norm. The resulting source estimates are however\nbiased in amplitude and often suboptimal in terms of source selection due to\nhigh correlations in the forward model. In this work, we demonstrate that an\ninverse solver based on a block-separable penalty with a Frobenius norm per\nblock and a l0.5-quasinorm over blocks addresses both of these issues. For\nsolving the resulting non-convex optimization problem, we propose the iterative\nreweighted Mixed Norm Estimate (irMxNE), an optimization scheme based on\niterative reweighted convex surrogate optimization problems, which are solved\nefficiently using a block coordinate descent scheme and an active set strategy.\nWe compare the proposed sparse imaging method to the dSPM and the RAP-MUSIC\napproach based on two MEG data sets. We provide empirical evidence based on\nsimulations and analysis of MEG data that the proposed method improves on the\nstandard Mixed Norm Estimate (MxNE) in terms of amplitude bias, support\nrecovery, and stability.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 13:56:04 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Strohmeier", "Daniel", ""], ["Bekhti", "Yousra", ""], ["Haueisen", "Jens", ""], ["Gramfort", "Alexandre", ""]]}, {"id": "1607.08656", "submitter": "Kevin Dick", "authors": "Kevin Dick and Ardyn Nordstrom", "title": "Identifying Unvaccinated Individuals in Canada: A Predictive Model", "comments": "8 pages, engineering and economics collaborative work, proposed\n  public health policy", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, the media and public health officials have become increasingly\naware of the rise in anti-vaccine sentiment. Vaccinations have numerous health\nbenefits for immunized individuals as well as for the general public through\nherd immunity. Given the rise in immunization-preventable diseases, a\nconsequence of people opting out of their routine vaccinations, we determined\nthat Canadian health data can identify individuals over the age of 60 who chose\nnot to get vaccinated (80.1% negative predictive value) and individuals under\nthe age of 60 who have recently been vaccinated (96.4% positive predictive\nvalue). Using the 2009-2014 Canadian Community Health Surveys (CCHS), a probit\nmodel identified the variables that were most commonly associated with flu\nvaccination outcomes. Of 1,381 variables, 47 with the most significant marginal\neffects were selected, including the presence of diseases (e.g. diabetes and\ncancer), behavioral characteristics (e.g. smoking and exercise), exposure to\nthe medical system (e.g. whether the individual gets a regular check-up), and a\nperson's living situation (e.g. having young children in the household). These\nvariables were then used to generate a Random Forest classification model,\ntrained on the 2009-2013 dataset, and tested on the 2014 dataset. We achieved\nan overall accuracy of 87.8% between the two final models, each using 25\nclassification trees with bounded depth of 20 nodes, randomly selecting from\nall 47 variables. With the two proposed policies, this model can be leveraged\nto efficiently allocate vaccination promotion efforts. Additionally, it can be\napplied to future surveys, only requiring 3.6% of the variables in the CCHS for\nsuccessful prediction.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 22:35:20 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Dick", "Kevin", ""], ["Nordstrom", "Ardyn", ""]]}, {"id": "1607.08678", "submitter": "Yanan Fan Dr", "authors": "Y. Fan, S. R. Meikle, G. Angelis, A. Sitek", "title": "ABC in Nuclear Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the application of approximate Bayesian Computation (ABC) in the\ncontext of medical imaging data. We consider the parameter estimation of\ncompartmental models in PET imaging analysis, and provide a simple ABC\nalgorithm for its estimation. We demonstrate the utility of the proposed\nestimation methods on a neurotransmitter response model, and compare our\napproach to existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 02:18:46 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Fan", "Y.", ""], ["Meikle", "S. R.", ""], ["Angelis", "G.", ""], ["Sitek", "A.", ""]]}, {"id": "1607.08743", "submitter": "Mirko Signorelli", "authors": "Mirko Signorelli, Ernst C. Wit", "title": "A penalized inference approach to stochastic block modelling of\n  community structure in the Italian Parliament", "comments": "The original version of the paper is freely available (Open Access)\n  from the editor's website\n  (http://onlinelibrary.wiley.com/doi/10.1111/rssc.12234/full), Journal of the\n  Royal Statistical Society: Series C (Applied Statistics), 2017", "journal-ref": "Journal of the Royal Statistical Society: Series C (Applied\n  Statistics), 2017, 67 (2), 355-369", "doi": "10.1111/rssc.12234", "report-no": null, "categories": "stat.AP cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyse bill cosponsorship networks in the Italian Chamber of Deputies. In\ncomparison with other parliaments, a distinguishing feature of the Chamber is\nthe large number of political groups. Our analysis aims to infer the pattern of\ncollaborations between these groups from data on bill cosponsorships. We\npropose an extension of stochastic block models for edge-valued graphs and\nderive measures of group productivity and of collaboration between political\nparties. As the model proposed encloses a large number of parameters, we pursue\na penalized likelihood approach that enables us to infer a sparse reduced graph\ndisplaying collaborations between political parties.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 09:36:14 GMT"}, {"version": "v2", "created": "Fri, 17 Feb 2017 11:52:52 GMT"}, {"version": "v3", "created": "Mon, 31 Jul 2017 13:14:44 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Signorelli", "Mirko", ""], ["Wit", "Ernst C.", ""]]}, {"id": "1607.08827", "submitter": "Marina Knight Dr", "authors": "Jessica K. Hargreaves, Marina I. Knight, Jon W. Pitchford, Seth J.\n  Davis", "title": "Clustering nonstationary circadian rhythms using locally stationary\n  wavelet representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How does soil pollution affect a plant's circadian clock? Are there any\ndifferences between how the clock reacts when exposed to different\nconcentrations of elements of the periodic table? If so, can we characterise\nthese differences?\n  We approach these questions by analysing and modelling circadian plant data,\nwhere the levels of expression of a luciferase reporter gene were measured at\nregular intervals over a number of days after exposure to different\nconcentrations of lithium.\n  A key aspect of circadian data analysis is to determine whether a time series\n(derived from experimental data) is `rhythmic' and, if so, to determine the\nunderlying period. However, our dataset displays nonstationary traits such as\nchanges in amplitude, gradual changes in period and phase-shifts.\n  In this paper, we develop clustering methods using a wavelet transform.\nWavelets are chosen as they are ideally suited to identifying discriminant\nlocal time and scale features. Furthermore, we propose treating the observed\ntime series as realisations of locally stationary wavelet processes. This\nallows us to define and estimate the evolutionary wavelet spectrum. We can then\ncompare, in a quantitative way, using a functional principal components\nanalysis, the time-frequency patterns of the time series. Our approach uses a\nclustering algorithm to group the data according to their time-frequency\npatterns. We demonstrate the advantages of our methodology over alternative\napproaches and show that it successfully clusters our data.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 14:31:24 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Hargreaves", "Jessica K.", ""], ["Knight", "Marina I.", ""], ["Pitchford", "Jon W.", ""], ["Davis", "Seth J.", ""]]}, {"id": "1607.08882", "submitter": "Daniel Nevo", "authors": "Daniel Nevo, Reiko Nishihara, Shuji Ogino and Molin Wang", "title": "The competing risks Cox model with and without auxiliary case covariates\n  under weaker or no missing-at-random cause of failure", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the analysis of time-to-event data with multiple causes using a competing\nrisks Cox model, often the cause of failure is unknown for some of the cases.\nThe probability of a missing cause is typically assumed to be independent of\nthe cause given the time of the event and covariates measured before the event\noccurred. In practice, however, the underlying missing-at-random assumption\ndoes not necessarily hold. Motivated by colorectal cancer subtype analysis, we\ndevelop semiparametric methods to conduct valid analysis, first when additional\nauxiliary variables are available for cases only. We consider a weaker\nmissing-at-random assumption, with missing pattern depending on the observed\nquantities, which include the auxiliary covariates. Overlooking these\ncovariates will potentially result in biased estimates. We use an informative\nlikelihood approach that will yield consistent estimates even when the\nunderlying model for missing cause of failure is misspecified. We then consider\na method to conduct valid statistical analysis when there are no auxiliary\ncovariates in the not missing-at-random scenario. The superiority of our\nmethods in finite samples is demonstrated by simulation study results. We\nillustrate the use of our method in an analysis of colorectal cancer data from\nthe Nurses' Health Study cohort, where, apparently, the traditional\nmissing-at-random assumption fails to hold for particular molecular subtypes.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 18:19:58 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Nevo", "Daniel", ""], ["Nishihara", "Reiko", ""], ["Ogino", "Shuji", ""], ["Wang", "Molin", ""]]}]