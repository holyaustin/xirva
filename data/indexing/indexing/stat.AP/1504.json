[{"id": "1504.00327", "submitter": "Liang Zhao", "authors": "Ning Zhang, Liang Zhao", "title": "Mortality cohort effect detection and measurement based on differential\n  geometry", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper analyzes mortality cohort effect of birth year and develops an\napproach to identify and measure cohort effects in mortality data set. The\napproach is based on differential geometry and leads to an explicit result\nwhich can describe how strong the cohort effect is in any period. This\nquantitative measurement provides a possibility to compare cohort effects among\ndifferent countries or groups. The paper also suggests to use coefficient of\nvariation as a measurement of the whole cohort effect for one country. Data of\nseveral countries are taken as examples to explain our approach and results.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2015 06:06:20 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2015 03:49:29 GMT"}], "update_date": "2015-04-03", "authors_parsed": [["Zhang", "Ning", ""], ["Zhao", "Liang", ""]]}, {"id": "1504.00534", "submitter": "Ruth Heller", "authors": "Marina Bogomolov and Ruth Heller", "title": "Assessing replicability of findings across two studies of multiple\n  features", "comments": null, "journal-ref": "Biometrika (2018)", "doi": "10.1093/biomet/asy029", "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Replicability analysis aims to identify the findings that replicated across\nindependent studies that examine the same features. We provide powerful novel\nreplicability analysis procedures for two studies for FWER and for FDR control\non the replicability claims. The suggested procedures first select the\npromising features from each study solely based on that study, and then test\nfor replicability only the features that were selected in both studies. We\nincorporate the plug-in estimates of the fraction of null hypotheses in one\nstudy among the selected hypotheses by the other study. Since the fraction of\nnulls in one study among the selected features from the other study is\ntypically small, the power gain can be remarkable. We provide theoretical\nguarantees for the control of the appropriate error rates, as well as\nsimulations that demonstrate the excellent power properties of the suggested\nprocedures. We demonstrate the usefulness of our procedures on real data\nexamples from two application fields: behavioural genetics and microarray\nstudies.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2015 13:04:36 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Bogomolov", "Marina", ""], ["Heller", "Ruth", ""]]}, {"id": "1504.00680", "submitter": "Justin Cheng", "authors": "Justin Cheng, Cristian Danescu-Niculescu-Mizil, Jure Leskovec", "title": "Antisocial Behavior in Online Discussion Communities", "comments": "ICWSM 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User contributions in the form of posts, comments, and votes are essential to\nthe success of online communities. However, allowing user participation also\ninvites undesirable behavior such as trolling. In this paper, we characterize\nantisocial behavior in three large online discussion communities by analyzing\nusers who were banned from these communities. We find that such users tend to\nconcentrate their efforts in a small number of threads, are more likely to post\nirrelevantly, and are more successful at garnering responses from other users.\nStudying the evolution of these users from the moment they join a community up\nto when they get banned, we find that not only do they write worse than other\nusers over time, but they also become increasingly less tolerated by the\ncommunity. Further, we discover that antisocial behavior is exacerbated when\ncommunity feedback is overly harsh. Our analysis also reveals distinct groups\nof users with different levels of antisocial behavior that can change over\ntime. We use these insights to identify antisocial users early on, a task of\nhigh practical importance to community maintainers.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2015 20:04:28 GMT"}, {"version": "v2", "created": "Mon, 16 May 2016 17:31:50 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Cheng", "Justin", ""], ["Danescu-Niculescu-Mizil", "Cristian", ""], ["Leskovec", "Jure", ""]]}, {"id": "1504.00701", "submitter": "Chiara Sabatti", "authors": "Christine Peterson, Marina Bogomolov, Yoav Benjamini and Chiara\n  Sabatti", "title": "Many Phenotypes without Many False Discoveries: Error Controlling\n  Strategies for Multi-Traits Association Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The genetic basis of multiple phenotypes such as gene expression, metabolite\nlevels, or imaging features is often investigated by testing a large collection\nof hypotheses, probing the existence of association between each of the traits\nand hundreds of thousands of genotyped variants. Appropriate multiplicity\nadjustment is crucial to guarantee replicability of findings, and False\nDiscovery Rate (FDR) is frequently adopted as a measure of global error. In the\ninterest of interpretability, results are often summarized so that reporting\nfocuses on variants discovered to be associated to some phenotypes.\n  We show that applying FDR-controlling procedures on the entire collection of\nhypotheses fails to control the rate of false discovery of associated variants\nas well as the average rate of false discovery of phenotypes influenced by such\nvariants. We propose a simple hierarchical testing procedure which allows\ncontrol of both these error rates and provides a more reliable basis for the\nidentification of variants with functional effects. We demonstrate the utility\nof this approach through simulation studies comparing various error rates and\nmeasures of power for genetic association studies of multiple traits. Finally,\nwe apply the proposed method to identify genetic variants which impact\nflowering phenotypes in Arabdopsis thaliana, expanding the set of discoveries.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2015 22:21:16 GMT"}], "update_date": "2015-04-06", "authors_parsed": [["Peterson", "Christine", ""], ["Bogomolov", "Marina", ""], ["Benjamini", "Yoav", ""], ["Sabatti", "Chiara", ""]]}, {"id": "1504.00975", "submitter": "S. Stanley Young", "authors": "S. Stanley Young, Robert L. Obenchain, Christophe Lambert", "title": "Bias and response heterogeneity in an air quality data set", "comments": "15 pages, 3 Tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-known that claims coming from observational studies often fail to\nreplicate when rigorously re-tested. The technical problems include multiple\ntesting, multiple modeling and bias. Any or all of these problems can give rise\nto claims that will fail to replicate. There is a need for statistical methods\nthat are easily applied, are easy to understand, and are likely to give\nreliable results. In particular, simple ways for reducing the influence of bias\nare essential. In this paper, the Local Control method developed by Robert\nObenchain is explicated using a small air quality/longevity data set first\nanalyzed in the New England Journal of Medicine. The benefits of our paper are\ntwofold. First, we describe a reliable strategy for analysis of observational\ndata. Second and importantly, the global claim that longevity increases with\nimprovements in air quality made in the NEJM paper needs to be modified. There\nis subgroup heterogeneity in the effect of air quality on longevity (one size\ndoes not fit all), and this heterogeneity is largely explained by factors other\nthan air quality.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2015 03:03:21 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2015 02:34:29 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Young", "S. Stanley", ""], ["Obenchain", "Robert L.", ""], ["Lambert", "Christophe", ""]]}, {"id": "1504.01146", "submitter": "Joseph Stachelek", "authors": "Joseph Stachelek, Christopher J. Madden", "title": "Application of Inverse Path Distance Weighting for high-density spatial\n  mapping of coastal water quality patterns", "comments": "2015. International Journal of Geographical Information Science", "journal-ref": null, "doi": "10.1080/13658816.2015.1018833", "report-no": null, "categories": "stat.AP physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the primary goals of coastal water quality monitoring is to\ncharacterize spatial variation. Generally, this monitoring takes place at a\nlimited number of fixed sampling points. The alternative sampling methodology\nexplored in this paper involves high density sampling from an onboard\nflow-through water analysis system (Dataflow). Dataflow has the potential to\nprovide better spatial resolution of water quality features because it\ngenerates many closely spaced (< 10 m) measurements. Regardless of the\nmeasurement technique, parameter values at unsampled locations must be\ninterpolated from nearby measurement points in order to generate a\ncomprehensive picture of spatial variations. Standard Euclidean interpolations\nin coastal settings tend to yield inaccurate results because they extend\nthrough barriers in the landscape such as peninsulas, islands, and submerged\nbanks. We recently developed a method for non-Euclidean interpolation by\ninverse path distance weighting (IPDW) in order to account for these barriers.\nThe algorithms were implemented as part of an R package and made available from\nR repositories. The combination of IPDW with Dataflow provided more accurate\nestimates of salinity patterning relative to Euclidean inverse distance\nweighting (IDW). IPDW was notably more accurate than IDW in the presence of\nintense spatial gradients.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2015 18:24:49 GMT"}], "update_date": "2015-04-07", "authors_parsed": [["Stachelek", "Joseph", ""], ["Madden", "Christopher J.", ""]]}, {"id": "1504.01281", "submitter": "Santosh Kumar", "authors": "Santosh Kumar", "title": "Random matrix ensembles involving Gaussian Wigner and Wishart matrices,\n  and biorthogonal structure", "comments": "Published version", "journal-ref": "Phys. Rev. E 92, 032903 (2015)", "doi": "10.1103/PhysRevE.92.032903", "report-no": null, "categories": "math-ph math.MP math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider four nontrivial ensembles involving Gaussian Wigner and Wishart\nmatrices. These are relevant to problems ranging from multiantenna\ncommunication to random supergravity. We derive the matrix probability density,\nas well as the eigenvalue densities for these ensembles. In all cases the joint\neigenvalue density exhibits a biorthogonal structure. A determinantal\nrepresentation, based on a generalization of Andr\\'{e}ief's integration\nformula, is used to compactly express the $r$-point correlation function of\neigenvalues. This representation circumvents the complications encountered in\nthe usual approaches, and the answer is obtained immediately by examining the\njoint density of eigenvalues. We validate our analytical results using Monte\nCarlo simulations.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2015 13:18:40 GMT"}, {"version": "v2", "created": "Wed, 13 May 2015 06:56:41 GMT"}, {"version": "v3", "created": "Tue, 15 Sep 2015 19:41:38 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Kumar", "Santosh", ""]]}, {"id": "1504.01641", "submitter": "Javier  Gonz\\'alez", "authors": "Javier Gonz\\'alez, Alberto Mu\\~noz, Gabriel Martos", "title": "Asymmetric latent semantic indexing for gene expression experiments\n  visualization", "comments": "22 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method to visualize gene expression experiments inspired by\nthe latent semantic indexing, technique originally proposed in the textual\nanalysis context. By using the correspondence word-gene document-experiment, we\ndefine an asymmetric similarity measure of association for genes that accounts\nfor potential hierarchies in the data, the key to obtain meaningful gene\nmappings. We use the polar decomposition to obtain the sources of asymmetry of\nthe similarity matrix, which are later combined with previous knowledge.\nGenetic classes of genes are identified by means of a mixture model applied in\nthe genes latent space. We describe the steps of the procedure and we show its\nutility in the Human Cancer dataset.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 15:23:55 GMT"}], "update_date": "2015-04-08", "authors_parsed": [["Gonz\u00e1lez", "Javier", ""], ["Mu\u00f1oz", "Alberto", ""], ["Martos", "Gabriel", ""]]}, {"id": "1504.01661", "submitter": "Youssef Khmou", "authors": "Youssef Khmou, Said Safi", "title": "An original Propagator for large array", "comments": "Fourteen pages and four figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we demonstrate that when the ratio $n$ of the number of\nantenna elements $N$ to the number $P$ of radiating sources is superior or\nequal to $2$, then it is possible to choose a propagator from a set of\n$n(n+1)/2-1$ operators to compute the Angles of Arrival (AoA) of the narrowband\nincoming waves. This new non eigenbased approach is efficient when the Signal\nto Noise Ratio (SNR) is moderate, and gives multitude of possibilities, that\nare dependent of the random data, to construct the complex sets whose columns\nare orthogonal to the signal subspace generated by the radiating sources.\nElementary examples are given for $n=3$, $n=4$ and $n=6$. The simulation\nresults are presented to illustrate the performance of the proposed\ncomputational methods.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2015 14:40:41 GMT"}], "update_date": "2015-04-08", "authors_parsed": [["Khmou", "Youssef", ""], ["Safi", "Said", ""]]}, {"id": "1504.01702", "submitter": "Kevin Bleakley", "authors": "G\\'erard Biau (LPMA, LSTA), Kevin Bleakley (SELECT), David Mason", "title": "Long signal change-point detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection of change-points in a spatially or time ordered data sequence\nis an important problem in many fields such as genetics and finance. We derive\nthe asymptotic distribution of a statistic recently suggested for detecting\nchange-points. Simulation of its estimated limit distribution leads to a new\nand computationally efficient change-point detection algorithm, which can be\nused on very long signals. We assess the algorithm via simulations and on\npreviously benchmarked real-world data sets.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 18:35:36 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2015 07:59:43 GMT"}], "update_date": "2015-10-01", "authors_parsed": [["Biau", "G\u00e9rard", "", "LPMA, LSTA"], ["Bleakley", "Kevin", "", "SELECT"], ["Mason", "David", ""]]}, {"id": "1504.01865", "submitter": "Andrew Zammit-Mangion", "authors": "Noel Cressie and Andrew Zammit-Mangion", "title": "Multivariate Spatial Covariance Models: A Conditional Approach", "comments": "22 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate geostatistics is based on modelling all covariances between all\npossible combinations of two or more variables at any sets of locations in a\ncontinuously indexed domain. Multivariate spatial covariance models need to be\nbuilt with care, since any covariance matrix that is derived from such a model\nmust be nonnegative-definite. In this article, we develop a conditional\napproach for spatial-model construction whose validity conditions are easy to\ncheck. We start with bivariate spatial covariance models and go on to\ndemonstrate the approach's connection to multivariate models defined by\nnetworks of spatial variables. In some circumstances, such as modelling\nrespiratory illness conditional on air pollution, the direction of conditional\ndependence is clear. When it is not, the two directional models can be\ncompared. More generally, the graph structure of the network reduces the number\nof possible models to compare. Model selection then amounts to finding possible\ncausative links in the network. We demonstrate our conditional approach on\nsurface temperature and pressure data, where the role of the two variables is\nseen to be asymmetric.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 08:32:53 GMT"}, {"version": "v2", "created": "Wed, 28 Sep 2016 09:00:04 GMT"}, {"version": "v3", "created": "Wed, 5 Oct 2016 23:21:50 GMT"}, {"version": "v4", "created": "Fri, 7 Oct 2016 19:34:01 GMT"}], "update_date": "2016-10-10", "authors_parsed": [["Cressie", "Noel", ""], ["Zammit-Mangion", "Andrew", ""]]}, {"id": "1504.01933", "submitter": "Stefan Siegert", "authors": "Stefan Siegert, David B. Stephenson, Philip G. Sansom, Adam A. Scaife,\n  Rosie Eade, Alberto Arribas", "title": "A Bayesian framework for verification and recalibration of ensemble\n  forecasts: How uncertain is NAO predictability?", "comments": "36 pages, 10 figures", "journal-ref": null, "doi": "10.1175/JCLI-D-15-0196.1", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictability estimates of ensemble prediction systems are uncertain due to\nlimited numbers of past forecasts and observations. To account for such\nuncertainty, this paper proposes a Bayesian inferential framework that provides\na simple 6-parameter representation of ensemble forecasting systems and the\ncorresponding observations. The framework is probabilistic, and thus allows for\nquantifying uncertainty in predictability measures such as correlation skill\nand signal-to-noise ratios. It also provides a natural way to produce\nrecalibrated probabilistic predictions from uncalibrated ensembles forecasts.\nThe framework is used to address important questions concerning the skill of\nwinter hindcasts of the North Atlantic Oscillation for 1992-2011 issued by the\nMet Office GloSea5 climate prediction system. Although there is much\nuncertainty in the correlation between ensemble mean and observations, there is\nstrong evidence of skill: the 95% credible interval of the correlation\ncoefficient of [0.19,0.68] does not overlap zero. There is also strong evidence\nthat the forecasts are not exchangeable with the observations: With over 99%\ncertainty, the signal-to-noise ratio of the forecasts is smaller than the\nsignal-to-noise ratio of the observations, which suggests that raw forecasts\nshould not be taken as representative scenarios of the observations. Forecast\nrecalibration is thus required, which can be coherently addressed within the\nproposed framework.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 12:20:18 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2015 21:15:12 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Siegert", "Stefan", ""], ["Stephenson", "David B.", ""], ["Sansom", "Philip G.", ""], ["Scaife", "Adam A.", ""], ["Eade", "Rosie", ""], ["Arribas", "Alberto", ""]]}, {"id": "1504.01985", "submitter": "Mikyoung Jun", "authors": "Jaehong Jeong, Mikyoung Jun", "title": "Covariance models on the surface of a sphere: when does it matter?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing interest in developing covariance functions for processes\non the surface of a sphere due to wide availability of data on the globe.\nUtilizing the one-to-one mapping between the Euclidean distance and the great\ncircle distance, isotropic and positive definite functions in a Euclidean space\ncan be used as covariance functions on the surface of a sphere. This approach,\nhowever, may result in physically unrealistic distortion on the sphere\nespecially for large distances. We consider several classes of parametric\ncovariance functions on the surface of a sphere, defined with either the great\ncircle distance or the Euclidean distance, and investigate their impact upon\nspatial prediction. We fit several isotropic covariance models to simulated\ndata as well as real data from NCEP/NCAR reanalysis on the sphere. We\ndemonstrate that covariance functions originally defined with the Euclidean\ndistance may not be adequate for some global data.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 14:33:28 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Jeong", "Jaehong", ""], ["Jun", "Mikyoung", ""]]}, {"id": "1504.02057", "submitter": "Samuel Clark", "authors": "Samuel J. Clark", "title": "A Singular Value Decomposition-based Factorization and Parsimonious\n  Component Model of Demographic Quantities Correlated by Age: Predicting\n  Complete Demographic Age Schedules with Few Parameters", "comments": "65 pages, 40 figures, University of Washington Center for Statistics\n  and the Social Sciences (CSSS) Working Paper No. 143", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BACKGROUND. Formal demography has a long history of building simple models of\nage schedules of demographic quantities, e.g. mortality and fertility rates.\nThese are widely used in demographic methods to manipulate whole age schedules\nusing few parameters.\n  OBJECTIVE. The Singular Value Decomposition (SVD) factorizes a matrix into\nthree matrices with useful properties including the ability to reconstruct the\noriginal matrix using many fewer, simple matrices. This work demonstrates how\nthese properties can be exploited to build parsimonious models of whole age\nschedules of demographic quantities that can be further parameterized in terms\nof arbitrary covariates.\n  METHODS. The SVD is presented and explained in detail with attention to\ndeveloping an intuitive understanding. The SVD is used to construct a general,\ncomponent model of demographic age schedules, and that model is demonstrated\nwith age-specific mortality and fertility rates. Finally, the model is used (1)\nto predict age-specific mortality using HIV indicators and summary measures of\nage-specific mortality, and (2) to predict age-specific fertility using the\ntotal fertility rate (TFR).\n  RESULTS. The component model of age-specific mortality and fertility rates\nsucceeds in reproducing the data with two inputs, and acting through those two\ninputs, various covariates are able to accurately predict full age schedules.\n  CONCLUSIONS. The SVD is potentially useful as a way to summarize, smooth and\nmodel age-specific demographic quantities. The component model is a general\nmethod of relating covariates to whole age schedules.\n  COMMENTS. The focus of this work is the SVD and the component model. The\napplications are for illustrative purposes only.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 18:08:57 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Clark", "Samuel J.", ""]]}, {"id": "1504.02124", "submitter": "Samuel Clark", "authors": "Samuel J. Clark, Jon Wakefield, Tyler McCormick, Michelle Ross", "title": "Hyak Mortality Monitoring System: Innovative Sampling and Estimation\n  Methods - Proof of Concept by Simulation", "comments": "Updated version including new simulation study with two-stage cluster\n  and optimum allocation sampling", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally health statistics are derived from civil and/or vital\nregistration. Civil registration in low-income countries varies from partial\ncoverage to essentially nothing at all. Consequently the state of the art for\npublic health information in low-income countries is efforts to combine or\ntriangulate data from different sources to produce a more complete picture\nacross both time and space - data amalgamation. Data sources amenable to this\napproach include sample surveys, sample registration systems, health and\ndemographic surveillance systems, administrative records, census records,\nhealth facility records and others.\n  We propose a new statistical framework for gathering health and population\ndata - Hyak - that leverages the benefits of sampling and longitudinal,\nprospective surveillance to create a cheap, accurate, sustainable monitoring\nplatform. Hyak has three fundamental components:\n  1) Data Amalgamation: a sampling and surveillance component that organizes\ntwo or more data collection systems to work together: a) data from HDSS with\nfrequent, intense, linked, prospective follow-up and b) data from sample\nsurveys conducted in large areas surrounding the Health and Demographic\nSurveillance System sites using informed sampling so as to capture as many\nevents as possible;\n  2) Cause of Death: verbal autopsy to characterize the distribution of deaths\nby cause at the population level; and\n  3) SES: measurement of socioeconomic status in order to characterize poverty\nand wealth.\n  We conduct a simulation study of the informed sampling component of Hyak\nbased on the Agincourt HDSS site in South Africa. Compared to traditional\ncluster sampling, Hyak's informed sampling captures more deaths, and when\ncombined with an estimation model that includes spatial smoothing, produces\nestimates mortality that have lower variance and small bias.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 20:47:12 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 01:37:58 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Clark", "Samuel J.", ""], ["Wakefield", "Jon", ""], ["McCormick", "Tyler", ""], ["Ross", "Michelle", ""]]}, {"id": "1504.02129", "submitter": "Samuel Clark", "authors": "Samuel J. Clark, Tyler McCormick, Zehang Li, Jon Wakefield", "title": "InSilicoVA: A Method to Automate Cause of Death Assignment for Verbal\n  Autopsy", "comments": "20 pages, 4 figures, Center for Statistics and the Social Sciences\n  (CSSS), University of Washington Working Paper No. 133", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Verbal autopsies (VA) are widely used to provide cause-specific mortality\nestimates in developing world settings where vital registration does not\nfunction well. VAs assign cause(s) to a death by using information describing\nthe events leading up to the death, provided by care givers. Typically\nphysicians read VA interviews and assign causes using their expert knowledge.\nPhysician coding is often slow, and individual physicians bring bias to the\ncoding process that results in non-comparable cause assignments. These problems\nsignificantly limit the utility of physician-coded VAs. A solution to both is\nto use an algorithmic approach that formalizes the cause-assignment process.\nThis ensures that assigned causes are comparable and requires many fewer\nperson-hours so that cause assignment can be conducted quickly without\ndisrupting the normal work of physicians. Peter Byass' InterVA method is the\nmost widely used algorithmic approach to VA coding and is aligned with the WHO\n2012 standard VA questionnaire.\n  The statistical model underpinning InterVA can be improved; uncertainty needs\nto be quantified, and the link between the population-level CSMFs and the\nindividual-level cause assignments needs to be statistically rigorous.\nAddressing these theoretical concerns provides an opportunity to create new\nsoftware using modern languages that can run on multiple platforms and will be\nwidely shared. Building on the overall framework pioneered by InterVA, our work\ncreates a statistical model for automated VA cause assignment.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 21:01:53 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["Clark", "Samuel J.", ""], ["McCormick", "Tyler", ""], ["Li", "Zehang", ""], ["Wakefield", "Jon", ""]]}, {"id": "1504.02813", "submitter": "Camila Pedroso Estevam de Souza Dr.", "authors": "Camila P. E. de Souza, Nancy E. Heckman and Helena Xu", "title": "Switching nonparametric regression models for multi-curve data", "comments": "24 pages, 4 figues", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop and apply an approach for analyzing multi-curve data where each\ncurve is driven by a latent state process. The state at any particular point\ndetermines a smooth function, forcing the individual curve to switch from one\nfunction to another. Thus each curve follows what we call a switching\nnonparametric regression model. We develop an EM algorithm to estimate the\nmodel parameters. We also obtain standard errors for the parameter estimates of\nthe state process. We consider several types of state processes: independent\nand identically distributed, independent but depending on a covariate and\nMarkov. Simulation studies show the frequentist properties of our estimates. We\napply our methods to a data set of a building's power usage.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2015 23:04:27 GMT"}, {"version": "v2", "created": "Fri, 21 Oct 2016 21:36:30 GMT"}, {"version": "v3", "created": "Wed, 13 Sep 2017 16:42:09 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["de Souza", "Camila P. E.", ""], ["Heckman", "Nancy E.", ""], ["Xu", "Helena", ""]]}, {"id": "1504.02835", "submitter": "Enayetur Raheem", "authors": "Sanku Dey, Enayetur Raheem", "title": "A multilevel multinomial logistic regression model for identifying risk\n  factors of anemia in children aged 6-59 months in northeastern states of\n  India", "comments": "11 pages, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we use multilevel multinomial logistic regression model to\nidentify the risk factors of anemia in children of northeastern States of\nIndia. The data consisted of 10,136 children of age group 6-59 months. We\nconsidered the level of anemia as the outcome variable with four ordinal\ncategories (severe, moderate, mild, and non-anemic) based on hemoglobin\nconcentration in blood as per WHO guidelines. A two-level random intercept\nmodel was considered with state of residence as the level-2 variable. The\nintra-class correlation (ICC) between states is 0.0577 indicating approximately\n6% of the total variation in the response variable accounted for by the state\nof residence. Several multilevel models have been compared, and a final model\nwas decided based on deviance test. We observed that predicted probability of\nbeing at or below severely anemic level to be 0.1247, at moderately anemic\nlevel: 0.3578, at mildly anemic level: 0.0698, and being non-anemic to be\n0.4477. We found that age at marriage (OR=1.13, 95% CI: 1.05, 1.21) and the\nnumber of children even born (OR=1.09, 95% CI: 1.03, 1.15) have significant\neffect on being at or below lower hemoglobin level (severely anemic).\nFurthermore, age of child (OR=0.92, 95% CI: 0.86-1.00) was a significant\npredictor, indicating that odds of severe anemia decreases if the child is 48\nmonths or older.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2015 04:43:33 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Dey", "Sanku", ""], ["Raheem", "Enayetur", ""]]}, {"id": "1504.02911", "submitter": "Michal Koles\\'ar", "authors": "Michal Koles\\'ar", "title": "Minimum Distance Approach to Inference with Many Instruments", "comments": "43 pages plus 9 page supplement", "journal-ref": "Journal of Econometrics, Volume 204, Issue 1, May 2018, Pages\n  86-100", "doi": "10.1016/j.jeconom.2018.01.004", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I analyze a linear instrumental variables model with a single endogenous\nregressor and many instruments. I use invariance arguments to construct a new\nminimum distance objective function. With respect to a particular weight\nmatrix, the minimum distance estimator is equivalent to the random effects\nestimator of Chamberlain and Imbens (2004), and the estimator of the\ncoefficient on the endogenous regressor coincides with the limited information\nmaximum likelihood estimator. This weight matrix is inefficient unless the\nerrors are normal, and I construct a new, more efficient estimator based on the\noptimal weight matrix. Finally, I show that when the minimum distance objective\nfunction does not impose a proportionality restriction on the reduced-form\ncoefficients, the resulting estimator corresponds to a version of the\nbias-corrected two-stage least squares estimator. I use the objective function\nto construct confidence intervals that remain valid when the proportionality\nrestriction is violated.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2015 19:40:20 GMT"}, {"version": "v2", "created": "Tue, 11 Jul 2017 13:15:56 GMT"}, {"version": "v3", "created": "Wed, 10 Jan 2018 16:47:32 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Koles\u00e1r", "Michal", ""]]}, {"id": "1504.03381", "submitter": "Cedric Ginestet", "authors": "Cedric E. Ginestet, Richard Emsley, Sabine Landau", "title": "Convex Combination of Ordinary Least Squares and Two-stage Least Squares\n  Estimators", "comments": "33 pages. 8 figures, 1 table. To be presented at UK-CIM (Causal\n  Inference Meeting) in Bristol, in April 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the presence of confounders, the ordinary least squares (OLS) estimator is\nknown to be biased. This problem can be remedied by using the two-stage least\nsquares (TSLS) estimator, based on the availability of valid instrumental\nvariables (IVs). This reduction in bias, however, is offset by an increase in\nvariance. Under standard assumptions, the OLS has indeed a larger bias than the\nTSLS estimator; and moreover, one can prove that the sample variance of the OLS\nestimator is no greater than the one of the TSLS. Therefore, it is natural to\nask whether one could combine the desirable properties of the OLS and TSLS\nestimators. Such a trade-off can be achieved through a convex combination of\nthese two estimators, thereby producing our proposed convex least squares (CLS)\nestimator. The relative contribution of the OLS and TSLS estimators is here\nchosen to minimize a sample estimate of the mean squared error (MSE) of their\nconvex combination. This proportion parameter is proved to be unique, whenever\nthe OLS and TSLS differ in MSEs. Remarkably, we show that this proportion\nparameter can be estimated from the data, and that the resulting CLS estimator\nis consistent. We also show how the CLS framework can incorporate other\nasymptotically unbiased estimators, such as the jackknife IV estimator (JIVE).\nThe finite-sample properties of the CLS estimator are investigated using Monte\nCarlo simulations, in which we independently vary the amount of confounding and\nthe strength of the instrument. Overall, the CLS estimator is found to\noutperform the TSLS estimator in terms of MSE. The method is also applied to a\nclassic data set from econometrics, which models the financial return to\neducation.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2015 22:20:22 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Ginestet", "Cedric E.", ""], ["Emsley", "Richard", ""], ["Landau", "Sabine", ""]]}, {"id": "1504.03413", "submitter": "Bhavya Kailkhura", "authors": "Bhavya Kailkhura, Swastik Brahma, Pramod K. Varshney", "title": "Consensus based Detection in the Presence of Data Falsification Attacks", "comments": null, "journal-ref": null, "doi": "10.1109/TSIPN.2016.2607119", "report-no": null, "categories": "cs.SY cs.DC stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of detection in distributed networks in the\npresence of data falsification (Byzantine) attacks. Detection approaches\nconsidered in the paper are based on fully distributed consensus algorithms,\nwhere all of the nodes exchange information only with their neighbors in the\nabsence of a fusion center. In such networks, we characterize the negative\neffect of Byzantines on the steady-state and transient detection performance of\nthe conventional consensus based detection algorithms. To address this issue,\nwe study the problem from the network designer's perspective. More\nspecifically, we first propose a distributed weighted average consensus\nalgorithm that is robust to Byzantine attacks. We show that, under reasonable\nassumptions, the global test statistic for detection can be computed locally at\neach node using our proposed consensus algorithm. We exploit the statistical\ndistribution of the nodes' data to devise techniques for mitigating the\ninfluence of data falsifying Byzantines on the distributed detection system.\nSince some parameters of the statistical distribution of the nodes' data might\nnot be known a priori, we propose learning based techniques to enable an\nadaptive design of the local fusion or update rules.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 03:43:05 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Kailkhura", "Bhavya", ""], ["Brahma", "Swastik", ""], ["Varshney", "Pramod K.", ""]]}, {"id": "1504.03454", "submitter": "Keren Shen", "authors": "Keren Shen, Jianfeng Yao and Wai Keung Li", "title": "Forecasting High-Dimensional Realized Volatility Matrices Using A Factor\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling and forecasting covariance matrices of asset returns play a crucial\nrole in finance. The availability of high frequency intraday data enables the\nmodeling of the realized covariance matrix directly. However, most models in\nthe literature suffer from the curse of dimensionality. To solve the problem,\nwe propose a factor model with a diagonal CAW model for the factor realized\ncovariance matrices. Asymptotic theory is derived for the estimated parameters.\nIn an extensive empirical analysis, we find that the number of parameters can\nbe reduced significantly. Furthermore, the proposed model maintains a\ncomparable performance with a benchmark vector autoregressive model.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 08:43:32 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Shen", "Keren", ""], ["Yao", "Jianfeng", ""], ["Li", "Wai Keung", ""]]}, {"id": "1504.03608", "submitter": "Jan Macutek", "authors": "Michaela Koscov\\'a, J\\'an Macutek, Emmerich Kelih", "title": "A data-based classification of Slavic languages: Indices of qualitative\n  variation applied to grapheme frequencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Ord's graph is a simple graphical method for displaying frequency\ndistributions of data or theoretical distributions in the two-dimensional\nplane. Its coordinates are proportions of the first three moments, either\nempirical or theoretical ones. A modification of the Ord's graph based on\nproportions of indices of qualitative variation is presented. Such a\nmodification makes the graph applicable also to data of categorical character.\nIn addition, the indices are normalized with values between 0 and 1, which\nenables comparing data files divided into different numbers of categories. Both\nthe original and the new graph are used to display grapheme frequencies in\neleven Slavic languages. As the original Ord's graph requires an assignment of\nnumbers to the categories, graphemes were ordered decreasingly according to\ntheir frequencies. Data were taken from parallel corpora, i.e., we work with\ngrapheme frequencies from a Russian novel and its translations to ten other\nSlavic languages. Then, cluster analysis is applied to the graph coordinates.\nWhile the original graph yields results which are not linguistically\ninterpretable, the modification reveals meaningful relations among the\nlanguages.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 16:23:30 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Koscov\u00e1", "Michaela", ""], ["Macutek", "J\u00e1n", ""], ["Kelih", "Emmerich", ""]]}, {"id": "1504.03679", "submitter": "Hao He", "authors": "Hao He and Pramod K. Varshney", "title": "A Coalitional Game for Distributed Inference in Sensor Networks with\n  Dependent Observations", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2015.2508781", "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of collaborative inference in a sensor network with\nheterogeneous and statistically dependent sensor observations. Each sensor aims\nto maximize its inference performance by forming a coalition with other sensors\nand sharing information within the coalition. It is proved that the inference\nperformance is a nondecreasing function of the coalition size. However, in an\nenergy constrained network, the energy consumption of inter-sensor\ncommunication also increases with increasing coalition size, which discourages\nthe formation of the grand coalition (the set of all sensors). In this paper,\nthe formation of non-overlapping coalitions with statistically dependent\nsensors is investigated under a specific communication constraint. We apply a\ngame theoretical approach to fully explore and utilize the information\ncontained in the spatial dependence among sensors to maximize individual sensor\nperformance. Before formulating the distributed inference problem as a\ncoalition formation game, we first quantify the gain and loss in forming a\ncoalition by introducing the concepts of diversity gain and redundancy loss for\nboth estimation and detection problems. These definitions, enabled by the\nstatistical theory of copulas, allow us to characterize the influence of\nstatistical dependence among sensor observations on inference performance. An\niterative algorithm based on merge-and-split operations is proposed for the\nsolution and the stability of the proposed algorithm is analyzed. Numerical\nresults are provided to demonstrate the superiority of our proposed game\ntheoretical approach.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 19:59:34 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["He", "Hao", ""], ["Varshney", "Pramod K.", ""]]}, {"id": "1504.03934", "submitter": "Ahmed Bel Hadj Ayed", "authors": "Ahmed Bel Hadj Ayed, Gr\\'egoire Loeper, Fr\\'ed\\'eric Abergel", "title": "Forecasting trends with asset prices", "comments": "26 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST q-fin.PM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a stochastic asset price model where the trend is\nan unobservable Ornstein Uhlenbeck process. We first review some classical\nresults from Kalman filtering. Expectedly, the choice of the parameters is\ncrucial to put it into practice. For this purpose, we obtain the likelihood in\nclosed form, and provide two on-line computations of this function. Then, we\ninvestigate the asymptotic behaviour of statistical estimators. Finally, we\nquantify the effect of a bad calibration with the continuous time mis-specified\nKalman filter. Numerical examples illustrate the difficulty of trend\nforecasting in financial time series.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2015 14:54:15 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2015 16:18:29 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Ayed", "Ahmed Bel Hadj", ""], ["Loeper", "Gr\u00e9goire", ""], ["Abergel", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1504.03974", "submitter": "Thakshila Wimalajeewa", "authors": "Thakshila Wimalajeewa and Pramod K. Varshney", "title": "Wireless Compressive Sensing Over Fading Channels with Distributed\n  Sparse Random Projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of recovering a sparse signal observed by a resource\nconstrained wireless sensor network under channel fading. Sparse random\nmatrices are exploited to reduce the communication cost in forwarding\ninformation to a fusion center. The presence of channel fading leads to\ninhomogeneity and non Gaussian statistics in the effective measurement matrix\nthat relates the measurements collected at the fusion center and the sparse\nsignal being observed. We analyze the impact of channel fading on nonuniform\nrecovery of a given sparse signal by leveraging the properties of heavy-tailed\nrandom matrices. We quantify the additional number of measurements required to\nensure reliable signal recovery in the presence of nonidentical fading channels\ncompared to that is required with identical Gaussian channels. Our analysis\nprovides insights into how to control the probability of sensor transmissions\nat each node based on the channel fading statistics in order to minimize the\nnumber of measurements collected at the fusion center for reliable sparse\nsignal recovery. We further discuss recovery guarantees of a given sparse\nsignal with any random projection matrix where the elements are sub-exponential\nwith a given sub-exponential norm. Numerical results are provided to\ncorroborate the theoretical findings.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2015 17:17:23 GMT"}], "update_date": "2015-04-16", "authors_parsed": [["Wimalajeewa", "Thakshila", ""], ["Varshney", "Pramod K.", ""]]}, {"id": "1504.04211", "submitter": "Zied Ben Bouallegue", "authors": "Zied Ben Bouallegue, Pierre Pinson, Petra Friederichs", "title": "Quantile forecast discrimination ability and value", "comments": null, "journal-ref": null, "doi": "10.1002/qj.2624", "report-no": null, "categories": "physics.ao-ph physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While probabilistic forecast verification for categorical forecasts is well\nestablished, some of the existing concepts and methods have not found their\nequivalent for the case of continuous variables. New tools dedicated to the\nassessment of forecast discrimination ability and forecast value are introduced\nhere, based on quantile forecasts being the base product for the continuous\ncase (hence in a nonparametric framework). The relative user characteristic\n(RUC) curve and the quantile value plot allow analysing the performance of a\nforecast for a specific user in a decision-making framework. The RUC curve is\ndesigned as a user-based discrimination tool and the quantile value plot\ntranslates forecast discrimination ability in terms of economic value. The\nrelationship between the overall value of a quantile forecast and the\nrespective quantile skill score is also discussed. The application of these new\nverification approaches and tools is illustrated based on synthetic datasets,\nas well as for the case of global radiation forecasts from the high resolution\nensemble COSMO-DE-EPS of the German Weather Service.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2015 12:45:35 GMT"}], "update_date": "2015-10-02", "authors_parsed": [["Bouallegue", "Zied Ben", ""], ["Pinson", "Pierre", ""], ["Friederichs", "Petra", ""]]}, {"id": "1504.04354", "submitter": "Martin Gould", "authors": "Martin D. Gould and Mason A. Porter and Sam D. Howison", "title": "The Long Memory of Order Flow in the Foreign Exchange Spot Market", "comments": "38 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.TR nlin.AO physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the long memory of order flow for each of three liquid currency\npairs on a large electronic trading platform in the foreign exchange (FX) spot\nmarket. Due to the extremely high levels of market activity on the platform,\nand in contrast to existing empirical studies of other markets, our data\nenables us to perform statistically stable estimation without needing to\naggregate data from different trading days. We find strong evidence of long\nmemory, with a Hurst exponent of approximately 0.7, for each of the three\ncurrency pairs and on each trading day in our sample. We repeat our\ncalculations using data that spans different trading days, and we find no\nsignificant differences in our results. We test and reject the hypothesis that\nthe apparent long memory of order flow is an artifact caused by structural\nbreaks, in favour of the alternative hypothesis of true long memory. We\ntherefore conclude that the long memory of order flow in the FX spot market is\na robust empirical property that persists across daily boundaries.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2015 19:34:20 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2015 11:10:23 GMT"}], "update_date": "2015-10-23", "authors_parsed": [["Gould", "Martin D.", ""], ["Porter", "Mason A.", ""], ["Howison", "Sam D.", ""]]}, {"id": "1504.04531", "submitter": "Nicolas Dobigeon", "authors": "Laetitia Loncan, Luis B. Almeida, Jos\\'e M. Bioucas-Dias, Xavier\n  Briottet, Jocelyn Chanussot, Nicolas Dobigeon, Sophie Fabre, Wenzhi Liao,\n  Giorgio A. Licciardi, Miguel Sim\\~oes, Jean-Yves Tourneret, Miguel A.\n  Veganzones, Gemine Vivone, Qi Wei and Naoto Yokoya", "title": "Hyperspectral pansharpening: a review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pansharpening aims at fusing a panchromatic image with a multispectral one,\nto generate an image with the high spatial resolution of the former and the\nhigh spectral resolution of the latter. In the last decade, many algorithms\nhave been presented in the literature for pansharpening using multispectral\ndata. With the increasing availability of hyperspectral systems, these methods\nare now being adapted to hyperspectral images. In this work, we compare new\npansharpening techniques designed for hyperspectral data with some of the state\nof the art methods for multispectral pansharpening, which have been adapted for\nhyperspectral data. Eleven methods from different classes (component\nsubstitution, multiresolution analysis, hybrid, Bayesian and matrix\nfactorization) are analyzed. These methods are applied to three datasets and\ntheir effectiveness and robustness are evaluated with widely used performance\nindicators. In addition, all the pansharpening techniques considered in this\npaper have been implemented in a MATLAB toolbox that is made available to the\ncommunity.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2015 15:07:11 GMT"}], "update_date": "2015-04-20", "authors_parsed": [["Loncan", "Laetitia", ""], ["Almeida", "Luis B.", ""], ["Bioucas-Dias", "Jos\u00e9 M.", ""], ["Briottet", "Xavier", ""], ["Chanussot", "Jocelyn", ""], ["Dobigeon", "Nicolas", ""], ["Fabre", "Sophie", ""], ["Liao", "Wenzhi", ""], ["Licciardi", "Giorgio A.", ""], ["Sim\u00f5es", "Miguel", ""], ["Tourneret", "Jean-Yves", ""], ["Veganzones", "Miguel A.", ""], ["Vivone", "Gemine", ""], ["Wei", "Qi", ""], ["Yokoya", "Naoto", ""]]}, {"id": "1504.04566", "submitter": "Matthew Schofield", "authors": "Matthew R. Schofield and Simon J. Bonner", "title": "Connecting the latent multinomial", "comments": null, "journal-ref": null, "doi": "10.1111/biom.12333", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Link et al. (2010) define a general framework for analyzing capture-recapture\ndata with potential misidentifications. In this framework, the observed vector\nof counts, $y$, is considered as a linear function of a vector of latent\ncounts, $x$, such that $y = A x$, with $x$ assumed to follow a multinomial\ndistribution conditional on the model parameters, $\\theta$. Bayesian methods\nare then applied by sampling from the joint posterior distribution of both $x$\nand $\\theta$. In particular, Link et al. (2010) propose a Metropolis-Hastings\nalgorithm to sample from the full conditional distribution of $x$, where new\nproposals are generated by sequentially adding elements from a basis of the\nnull space (kernel) of $A$. We consider this algorithm and show that using\nelements from a simple basis for the kernel of $A$ may not produce an\nirreducible Markov chain. Instead, we require a Markov basis, as defined by\nDiaconis and Sturmfels (1998). We illustrate the importance of Markov bases\nwith three capture-recapture examples. We prove that a specific lattice basis\nis a Markov basis for a class of models including the original model considered\nby Link et al. (2010) and confirm that the specific basis used by Link et al.\n(2010) for their example with two sampling occasions is a Markov basis. The\nconstructive nature of our proof provides an immediate method to obtain a\nMarkov basis for any model in this class.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2015 02:57:46 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Schofield", "Matthew R.", ""], ["Bonner", "Simon J.", ""]]}, {"id": "1504.04873", "submitter": "Ronald Hochreiter", "authors": "Laura Vana and Ronald Hochreiter and Kurt Hornik", "title": "Computing a consensus journal meta-ranking using paired comparisons and\n  adaptive lasso estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a \"publish-or-perish culture\", the ranking of scientific journals plays a\ncentral role in assessing performance in the current research environment. With\na wide range of existing methods and approaches to deriving journal rankings,\nmeta-rankings have gained popularity as a means of aggregating different\ninformation sources. In this paper, we propose a method to create a consensus\nmeta-ranking using heterogeneous journal rankings. Using a parametric model for\npaired comparison data we estimate quality scores for 58 journals in the OR/MS\ncommunity, which together with a shrinkage procedure allows for the\nidentification of clusters of journals with similar quality. The use of paired\ncomparisons provides a flexible framework for deriving a consensus score while\neliminating the problem of data missingness.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2015 19:14:57 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Vana", "Laura", ""], ["Hochreiter", "Ronald", ""], ["Hornik", "Kurt", ""]]}, {"id": "1504.05004", "submitter": "Stephane Chretien", "authors": "St\\'ephane Chr\\'etien, Christophe Guyeux, Michael Boyer-Guittaut,\n  R\\'egis Delage-Mouroux and Fran\\c{c}oise Desc\\^otes", "title": "Using the LASSO for gene selection in bladder cancer data", "comments": "submitted to CIBB 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a gene expression data array of a list of bladder cancer patients with\ntheir tumor states, it may be difficult to determine which genes can operate as\ndisease markers when the array is large and possibly contains outliers and\nmissing data. An additional difficulty is that observations (tumor states) in\nthe regression problem are discrete ones. In this article, we solve these\nproblems on concrete data using first a clustering approach, followed by Least\nAbsolute Shrinkage and Selection Operator (LASSO) estimators in a nonlinear\nregression problem involving discrete variables, as described in the brand-new\nresearch work of Plan and Vershynin. Gene markers of the most severe tumor\nstate are finally provided using the proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2015 10:38:35 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Chr\u00e9tien", "St\u00e9phane", ""], ["Guyeux", "Christophe", ""], ["Boyer-Guittaut", "Michael", ""], ["Delage-Mouroux", "R\u00e9gis", ""], ["Desc\u00f4tes", "Fran\u00e7oise", ""]]}, {"id": "1504.05138", "submitter": "Ralph Stern", "authors": "Ralph H. Stern, Dean E. Smith, Hitinder S. Gurm", "title": "Improving the Presentation and Understanding of Risk Models", "comments": "14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The key concepts (calibration, discrimination, and discordance) important in\nunderstanding and comparing risk models are best conveyed graphically. To\nillustrate this, models predicting death and acute kidney injury in a large\ncohort of PCI patients differing in the number of predictors included are\npresented. Calibration plots, often presented in the current literature,\npresent the agreement between predicted and observed risk for deciles of risk.\nRisk distribution curves present the frequency of different levels of risk.\nScatterplots of the risks assigned to individuals by different models show the\ndiscordance of the individual risk estimates. Increasing the number of\npredictors in these models produce increasingly disperse and progressively\nskewed risk distribution curves. These resemble the lognormal distributions\nexpected when risk predictors interact multiplicatively. These changes in the\nrisk distribution curves correlate with improved measures of discrimination.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2015 17:57:32 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Stern", "Ralph H.", ""], ["Smith", "Dean E.", ""], ["Gurm", "Hitinder S.", ""]]}, {"id": "1504.05383", "submitter": "Andrzej Jarynowski", "authors": "Andrzej Jarynowski", "title": "HPV and cervical cancer in Moldova, epidemiological model with\n  intervention cost vs benefit and effectiveness analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human papillomavirus, or HPV, is a sexually transmittable virus infection,\nwhich is necessary risk factor for developing cervical cancer, first most\ncommon type of cancer in working age women in Moldova. We observe both\nbehavioral change (sexuality increase) and demographical change (population\nageing). We used data since 1998 (Moldovan peace treaty) to adjust model\nparameter and we project till around 2030 (for vaccination till 2050).\nAccording to provided information, interdisciplinary model was proposed. It iss\nset of deterministic differential equations. Stochasticity was introduced in\nsexual partner change rates. The model has aggregated the most important paths\nof infection, cancer development and prevention scenarios (more than 100\nequations and 200 parameters). Moldovan cervical cancer perspective looks much\nbetter, than in central western Europe countries, because of relatively young\nsociety. In our setup, obligatory vaccination seems to not be so crucial (for\nnone of realistic scenarios increase of cancer cases is possible) for public\nhealth, as in most countries in European Union. However, screening practice\ncould be verified in terms of efficiency, when cost benefit calculation would\nbe done. We propose more optimal screening guidelines (with prevention cost 5\n-10k EUR per QALY), which could provide saving perspective in 10-15 year in\nrange 150-300k EUR yearly. Targeted vaccination could be also consider, because\ncosts are similar to high frequencies screening schema with the same cancer\ncases projection. However, some positive side effects of vaccination as\nreduction of pathogen circulation in society, will cause decrease of other\npathologies related to HPV like genital warts and other cancer.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 10:54:10 GMT"}], "update_date": "2015-04-22", "authors_parsed": [["Jarynowski", "Andrzej", ""]]}, {"id": "1504.05436", "submitter": "Anna Heath", "authors": "Anna Heath, Ioanna Manolopoulou and Gianluca Baio", "title": "Estimating the Expected Value of Partial Perfect Information in Health\n  Economic Evaluations using Integrated Nested Laplace Approximation", "comments": null, "journal-ref": null, "doi": "10.1002/sim.6983", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Expected Value of Perfect Partial Information (EVPPI) is a\ndecision-theoretic measure of the \"cost\" of parametric uncertainty in decision\nmaking used principally in health economic decision making. Despite this\ndecision-theoretic grounding, the uptake of EVPPI calculations in practice has\nbeen slow. This is in part due to the prohibitive computational time required\nto estimate the EVPPI via Monte Carlo simulations. However, recent developments\nhave demonstrated that the EVPPI can be estimated by non-parametric regression\nmethods, which have significantly decreased the computation time required to\napproximate the EVPPI. Under certain circumstances, high-dimensional Gaussian\nProcess regression is suggested, but this can still be prohibitively expensive.\nApplying fast computation methods developed in spatial statistics using\nIntegrated Nested Laplace Approximations (INLA) and projecting from a\nhigh-dimensional into a low-dimensional input space allows us to decrease the\ncomputation time for fitting these high-dimensional Gaussian Processes, often\nsubstantially. We demonstrate that the EVPPI calculated using our method for\nGaussian Process regression is in line with the standard Gaussian Process\nregression method and that despite the apparent methodological complexity of\nthis new method, R functions are available in the package BCEA to implement it\nsimply and efficiently.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 13:59:33 GMT"}, {"version": "v2", "created": "Fri, 4 Nov 2016 13:45:57 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Heath", "Anna", ""], ["Manolopoulou", "Ioanna", ""], ["Baio", "Gianluca", ""]]}, {"id": "1504.05714", "submitter": "Martin \\v{S}m\\'id", "authors": "Martin \\v{S}m\\'id", "title": "Estimation of Zero Intelligence Models by L1 Data", "comments": null, "journal-ref": "Quantitative Finance vol.16, 9 (2016), p. 1423-1444", "doi": "10.1080/14697688.2016.1149612", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A unit volume zero intelligence (ZI) model is defined and the distribution of\nits L1 process is recursively described. Further, a generalized ZI (GZI) model\nallowing non-unit market orders, shifts of quotes and general in-spread events\nis proposed and a formula for the conditional distribution of its quotes is\ngiven, together with a formula for price impact. For both the models, MLE\nestimators are formulated and shown to be consistent and asymptotically normal.\nConsequently, the estimators are applied to data of six US stocks from nine\nelectronic markets. It is found that more complex variants of the models,\ndespite being significant, do not give considerably better predictions than\ntheir simple versions with constant intensities.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 10:12:48 GMT"}, {"version": "v2", "created": "Sat, 31 Oct 2015 14:35:35 GMT"}, {"version": "v3", "created": "Mon, 25 Apr 2016 19:44:56 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["\u0160m\u00edd", "Martin", ""]]}, {"id": "1504.05715", "submitter": "Fran\\c{c}ois Septier", "authors": "Francois Septier, Gareth W. Peters", "title": "Langevin and Hamiltonian based Sequential MCMC for Efficient Bayesian\n  Filtering in High-dimensional Spaces", "comments": null, "journal-ref": null, "doi": "10.1109/JSTSP.2015.2497211", "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear non-Gaussian state-space models arise in numerous applications in\nstatistics and signal processing. In this context, one of the most successful\nand popular approximation techniques is the Sequential Monte Carlo (SMC)\nalgorithm, also known as particle filtering. Nevertheless, this method tends to\nbe inefficient when applied to high dimensional problems. In this paper, we\nfocus on another class of sequential inference methods, namely the Sequential\nMarkov Chain Monte Carlo (SMCMC) techniques, which represent a promising\nalternative to SMC methods. After providing a unifying framework for the class\nof SMCMC approaches, we propose novel efficient strategies based on the\nprinciple of Langevin diffusion and Hamiltonian dynamics in order to cope with\nthe increasing number of high-dimensional applications. Simulation results show\nthat the proposed algorithms achieve significantly better performance compared\nto existing algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 10:13:51 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2015 17:41:25 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Septier", "Francois", ""], ["Peters", "Gareth W.", ""]]}, {"id": "1504.05753", "submitter": "Fran\\c{c}ois Septier", "authors": "Thi Le Thu Nguyen, Francois Septier, Gareth W. Peters, Yves Delignon", "title": "Efficient Sequential Monte-Carlo Samplers for Bayesian Inference", "comments": "arXiv admin note: text overlap with arXiv:1303.3123 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many problems, complex non-Gaussian and/or nonlinear models are required\nto accurately describe a physical system of interest. In such cases, Monte\nCarlo algorithms are remarkably flexible and extremely powerful approaches to\nsolve such inference problems. However, in the presence of a high-dimensional\nand/or multimodal posterior distribution, it is widely documented that standard\nMonte-Carlo techniques could lead to poor performance. In this paper, the study\nis focused on a Sequential Monte-Carlo (SMC) sampler framework, a more robust\nand efficient Monte Carlo algorithm. Although this approach presents many\nadvantages over traditional Monte-Carlo methods, the potential of this emergent\ntechnique is however largely underexploited in signal processing. In this work,\nwe aim at proposing some novel strategies that will improve the efficiency and\nfacilitate practical implementation of the SMC sampler specifically for signal\nprocessing applications. Firstly, we propose an automatic and adaptive strategy\nthat selects the sequence of distributions within the SMC sampler that\nminimizes the asymptotic variance of the estimator of the posterior\nnormalization constant. This is critical for performing model selection in\nmodelling applications in Bayesian signal processing. The second original\ncontribution we present improves the global efficiency of the SMC sampler by\nintroducing a novel correction mechanism that allows the use of the particles\ngenerated through all the iterations of the algorithm (instead of only\nparticles from the last iteration). This is a significant contribution as it\nremoves the need to discard a large portion of the samples obtained, as is\nstandard in standard SMC methods. This will improve estimation performance in\npractical settings where computational budget is important to consider.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 12:24:21 GMT"}], "update_date": "2015-04-23", "authors_parsed": [["Nguyen", "Thi Le Thu", ""], ["Septier", "Francois", ""], ["Peters", "Gareth W.", ""], ["Delignon", "Yves", ""]]}, {"id": "1504.05837", "submitter": "Fran\\c{c}ois Septier", "authors": "Thi Le Thu Nguyen, Francois Septier, Harizo Rajaona, Gareth W. Peters,\n  Ido Nevat, Yves Delignon", "title": "New Perspectives on Multiple Source Localization in Wireless Sensor\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the challenging problem of multiple source\nlocalization in Wireless Sensor Networks (WSN). We develop an efficient\nstatistical algorithm, based on the novel application of Sequential Monte Carlo\n(SMC) sampler methodology, that is able to deal with an unknown number of\nsources given quantized data obtained at the fusion center from different\nsensors with imperfect wireless channels. We also derive the Posterior\nCram\\'er-Rao Bound (PCRB) of the source location estimate. The PCRB is used to\nanalyze the accuracy of the proposed SMC sampler algorithm and the impact that\nquantization has on the accuracy of location estimates of the sources.\nExtensive experiments show that the benefits of the proposed scheme in terms of\nthe accuracy of the estimation method that are required for model selection\n(i.e., the number of sources) and the estimation of the source characteristics\ncompared to the classical importance sampling method.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 14:56:11 GMT"}], "update_date": "2015-04-23", "authors_parsed": [["Nguyen", "Thi Le Thu", ""], ["Septier", "Francois", ""], ["Rajaona", "Harizo", ""], ["Peters", "Gareth W.", ""], ["Nevat", "Ido", ""], ["Delignon", "Yves", ""]]}, {"id": "1504.05872", "submitter": "Leto Peel", "authors": "Leto Peel and Aaron Clauset", "title": "Predicting sports scoring dynamics with restoration and anti-persistence", "comments": "12 pages, 9 figures", "journal-ref": "Proc. 2015 IEEE International Conference on Data Mining (ICDM),\n  339-348 2015", "doi": null, "report-no": null, "categories": "physics.data-an cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Professional team sports provide an excellent domain for studying the\ndynamics of social competitions. These games are constructed with simple,\nwell-defined rules and payoffs that admit a high-dimensional set of possible\nactions and nontrivial scoring dynamics. The resulting gameplay and efforts to\npredict its evolution are the object of great interest to both sports\nprofessionals and enthusiasts. In this paper, we consider two online prediction\nproblems for team sports:~given a partially observed game Who will score next?\nand ultimately Who will win? We present novel interpretable generative models\nof within-game scoring that allow for dependence on lead size (restoration) and\non the last team to score (anti-persistence). We then apply these models to\ncomprehensive within-game scoring data for four sports leagues over a ten year\nperiod. By assessing these models' relative goodness-of-fit we shed new light\non the underlying mechanisms driving the observed scoring dynamics of each\nsport. Furthermore, in both predictive tasks, the performance of our models\nconsistently outperforms baselines models, and our models make quantitative\nassessments of the latent team skill, over time.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 16:35:23 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Peel", "Leto", ""], ["Clauset", "Aaron", ""]]}, {"id": "1504.06074", "submitter": "Ran Shi", "authors": "Ran Shi and Jian Kang", "title": "Thresholded Multiscale Gaussian Processes with Application to Bayesian\n  Feature Selection for Massive Neuroimaging Data", "comments": "37 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the needs of selecting important features for massive\nneuroimaging data, we propose a spatially varying coefficient model (SVCMs)\nwith sparsity and piecewise smoothness imposed on the coefficient functions. A\nnew class of nonparametric priors is developed based on thresholded\nmultiresolution Gaussian processes (TMGP). We show that the TMGP has a large\nsupport on a space of sparse and piecewise smooth functions, leading to\nposterior consistency in coefficient function estimation and feature selection.\nAlso, we develop a method for prior specifications of thresholding parameters\nin TMGPs and discuss their theoretical properties. Efficient posterior\ncomputation algorithms are developed by adopting a kernel convolution approach,\nwhere a modified square exponential kernel is chosen taking the advantage that\nthe analytical form of the eigen decomposition is available. Based on\nsimulation studies, we demonstrate that our methods can achieve better\nperformance in estimating the spatially varying coefficient. Also, the proposed\nmodel has been applied to an analysis of resting state functional magnetic\nresonance imaging (Rs-fMRI) data from the Autism Brain Imaging Data Exchange\n(ABIDE) study, it provides biologically meaningful results.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 08:12:22 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2015 23:36:41 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Shi", "Ran", ""], ["Kang", "Jian", ""]]}, {"id": "1504.06478", "submitter": "Florencia Leonardi", "authors": "Andressa Cerqueira, Daniel Fraiman, Claudia D. Vargas and Florencia\n  Leonardi", "title": "A test of hypotheses for random graph distributions built from EEG data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theory of random graphs is being applied in recent years to model neural\ninteractions in the brain. While the probabilistic properties of random graphs\nhas been extensively studied in the literature, the development of statistical\ninference methods for this class of objects has received less attention. In\nthis work we propose a non-parametric test of hypotheses to test if two samples\nof random graphs were originated from the same probability distribution. We\nshow how to compute efficiently the test statistic and we study its performance\non simulated data. We apply the test to compare graphs of brain functional\nnetwork interactions built from electroencephalographic (EEG) data collected\nduring the visualization of point light displays depicting human locomotion.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2015 11:45:59 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Cerqueira", "Andressa", ""], ["Fraiman", "Daniel", ""], ["Vargas", "Claudia D.", ""], ["Leonardi", "Florencia", ""]]}, {"id": "1504.06653", "submitter": "Paul Northrop", "authors": "Paul Northrop (1), Nicolas Attalides (1), Philip Jonathan (2) ((1)\n  University College London, UK (2) Shell Projects and Technology, Manchester,\n  UK)", "title": "Cross-validatory extreme value threshold selection and uncertainty with\n  application to ocean storm severity", "comments": "24 pages, 15 figures. Confidence intervals in Figure 2 corrected. The\n  final publication is available at Wiley via\n  http://dx.doi.org/10.1111/rssc.12159", "journal-ref": null, "doi": "10.1111/rssc.12159", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designs conditions for marine structures are typically informed by\nthreshold-based extreme value analyses of oceanographic variables, in which\nexcesses of a high threshold are modelled by a generalized Pareto (GP)\ndistribution. Too low a threshold leads to bias from model mis-specification;\nraising the threshold increases the variance of estimators: a bias-variance\ntrade-off. Many existing threshold selection methods do not address this\ntrade-off directly, but rather aim to select the lowest threshold above which\nthe GP model is judged to hold approximately. In this paper Bayesian\ncross-validation is used to address the trade-off by comparing thresholds based\non predictive ability at extreme levels. Extremal inferences can be sensitive\nto the choice of a single threshold. We use Bayesian model-averaging to combine\ninferences from many thresholds, thereby reducing sensitivity to the choice of\na single threshold. The methodology is applied to significant wave height\ndatasets from the northern North Sea and the Gulf of Mexico.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2015 22:10:06 GMT"}, {"version": "v2", "created": "Tue, 5 Apr 2016 20:36:31 GMT"}, {"version": "v3", "created": "Wed, 1 Jun 2016 09:39:56 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Northrop", "Paul", ""], ["Attalides", "Nicolas", ""], ["Jonathan", "Philip", ""]]}, {"id": "1504.06706", "submitter": "Yoshimasa Uematsu", "authors": "Yoshimasa Uematsu", "title": "Penalized Likelihood Estimation in High-Dimensional Time Series Models\n  and its Application", "comments": "This manuscript includes some theoretically insufficient points that\n  will be fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a general theoretical framework of penalized\nquasi-maximum likelihood (PQML) estimation in stationary multiple time series\nmodels when the number of parameters possibly diverges. We show the oracle\nproperty of the PQML estimator under high-level, but tractable, assumptions,\ncomprising the first half of the paper. Utilizing these results, we propose in\nthe latter half of the paper a method of sparse estimation in high-dimensional\nvector autoregressive (VAR) models. Finally, the usability of the sparse\nhigh-dimensional VAR model is confirmed with a simulation study and an\nempirical analysis on a yield curve forecast.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2015 09:49:30 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2015 08:27:26 GMT"}, {"version": "v3", "created": "Wed, 26 Apr 2017 21:27:42 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Uematsu", "Yoshimasa", ""]]}, {"id": "1504.06896", "submitter": "Titus von der Malsburg", "authors": "Titus von der Malsburg and Bernhard Angele", "title": "False Positives and Other Statistical Errors in Standard Analyses of Eye\n  Movements in Reading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In research on eye movements in reading, it is common to analyze a number of\ncanonical dependent measures to study how the effects of a manipulation unfold\nover time. Although this gives rise to the well-known multiple comparisons\nproblem, i.e. an inflated probability that the null hypothesis is incorrectly\nrejected (Type I error), it is accepted standard practice not to apply any\ncorrection procedures. Instead, there appears to be a widespread belief that\ncorrections are not necessary because the increase in false positives is too\nsmall to matter. To our knowledge, no formal argument has ever been presented\nto justify this assumption. Here, we report a computational investigation of\nthis issue using Monte Carlo simulations. Our results show that, contrary to\nconventional wisdom, false positives are increased to unacceptable levels when\nno corrections are applied. Our simulations also show that counter-measures\nlike the Bonferroni correction keep false positives in check while reducing\nstatistical power only moderately. Hence, there is little reason why such\ncorrections should not be made a standard requirement. Further, we discuss\nthree statistical illusions that can arise when statistical power is low, and\nwe show how power can be improved to prevent these illusions. In sum, our work\nrenders a detailed picture of the various types of statistical errors than can\noccur in studies of reading behavior and we provide concrete guidance about how\nthese errors can be avoided.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2015 23:46:57 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2016 23:39:11 GMT"}, {"version": "v3", "created": "Mon, 10 Oct 2016 18:07:20 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["von der Malsburg", "Titus", ""], ["Angele", "Bernhard", ""]]}, {"id": "1504.06964", "submitter": "Fulton Wang", "authors": "Fulton Wang and Tyler H. McCormick and Cynthia Rudin and John Gore", "title": "Modeling Recovery Curves With Application to Prostatectomy", "comments": "Accepted to Biostatistics, 2018. Includes supplementary material and\n  high resolution images of predictions for patients", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian model that predicts recovery curves based on\ninformation available before the disruptive event. A recovery curve of interest\nis the quantified sexual function of prostate cancer patients after\nprostatectomy surgery. We illustrate the utility of our model as a\npre-treatment medical decision aid, producing personalized predictions that are\nboth interpretable and accurate. We uncover covariate relationships that agree\nwith and supplement that in existing medical literature.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 08:14:33 GMT"}, {"version": "v2", "created": "Wed, 28 Sep 2016 16:52:41 GMT"}, {"version": "v3", "created": "Tue, 24 Jan 2017 04:45:09 GMT"}, {"version": "v4", "created": "Thu, 1 Jun 2017 15:54:40 GMT"}, {"version": "v5", "created": "Tue, 27 Feb 2018 14:18:27 GMT"}, {"version": "v6", "created": "Mon, 5 Mar 2018 03:42:28 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Wang", "Fulton", ""], ["McCormick", "Tyler H.", ""], ["Rudin", "Cynthia", ""], ["Gore", "John", ""]]}, {"id": "1504.07295", "submitter": "Matt Taddy", "authors": "Matt Taddy", "title": "Document Classification by Inversion of Distributed Language\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There have been many recent advances in the structure and measurement of\ndistributed language models: those that map from words to a vector-space that\nis rich in information about word choice and composition. This vector-space is\nthe distributed language representation. The goal of this note is to point out\nthat any distributed representation can be turned into a classifier through\ninversion via Bayes rule. The approach is simple and modular, in that it will\nwork with any language representation whose training can be formulated as\noptimizing a probability model. In our application to 2 million sentences from\nYelp reviews, we also find that it performs as well as or better than complex\npurpose-built algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 22:32:40 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2015 19:46:35 GMT"}, {"version": "v3", "created": "Fri, 24 Jul 2015 15:27:20 GMT"}], "update_date": "2015-07-27", "authors_parsed": [["Taddy", "Matt", ""]]}, {"id": "1504.07536", "submitter": "Sergei Rodionov", "authors": "Sergei Rodionov", "title": "A sequential method of detecting abrupt changes in the correlation\n  coefficient and its application to Bering Sea climate", "comments": "18 pages, 11 figures", "journal-ref": "Climate 2015, 3(3), 474-491", "doi": "10.3390/cli3030474", "report-no": null, "categories": "stat.ME physics.ao-ph physics.ins-det stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A new method of regime shift detection in the correlation coefficient is\nproposed. The method is designed to find multiple change-points with unknown\nlocations in time series. It signals a possible regime shift in real time and\nallows for its monitoring. The method is tested on randomly generated time\nseries with predefined change-points. It is applied to examine structural\nchanges in the Bering Sea climate. A major shift is found in 1967, which\ncoincides with a transition from a zonal type of atmospheric circulation to a\nmeridional one. The roles of the Siberian and Alaskan centers of action on\nwinter temperatures in the eastern Bering Sea have been investigated.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 19:13:51 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2015 16:32:07 GMT"}], "update_date": "2015-07-24", "authors_parsed": [["Rodionov", "Sergei", ""]]}, {"id": "1504.08080", "submitter": "Daniel Cooley", "authors": "Brook T. Russell, Daniel Cooley, William C. Porter, Brian J. Reich,\n  Colette L. Heald", "title": "Data Mining to Investigate the Meteorological Drivers for Extreme Ground\n  Level Ozone Events", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This project aims to explore which combinations of meteorological conditions\nare associated with extreme ground level ozone conditions. Our approach focuses\nonly on the tail by optimizing the tail dependence between the ozone response\nand functions of meteorological covariates. Since there is a long list of\npossible meteorological covariates, the space of possible models cannot be\nexplored completely. Consequently, we perform data mining within the model\nselection context, employing an automated model search procedure. Our study is\nunique among extremes applications as optimizing tail dependence has not\npreviously been attempted, and it presents new challenges, such as requiring a\nsmooth threshold. We present a simulation study which shows that the method can\ndetect complicated conditions leading to extreme responses and resists\noverfitting. We apply the method to ozone data for Atlanta and Charlotte and\nfind similar meteorological drivers for these two Southeastern US cities. We\nidentify several covariates which help to differentiate the meteorological\nconditions which lead to extreme ozone levels from those which lead to merely\nhigh levels.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 04:21:23 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2015 21:44:41 GMT"}, {"version": "v3", "created": "Thu, 1 Oct 2015 18:58:22 GMT"}, {"version": "v4", "created": "Fri, 11 Mar 2016 19:58:15 GMT"}], "update_date": "2016-03-14", "authors_parsed": [["Russell", "Brook T.", ""], ["Cooley", "Daniel", ""], ["Porter", "William C.", ""], ["Reich", "Brian J.", ""], ["Heald", "Colette L.", ""]]}, {"id": "1504.08218", "submitter": "Michael Ward", "authors": "Shahryar Minhas and Peter D. Hoff and Michael D. Ward", "title": "Relax, Tensors Are Here: Dependencies in International Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous models of international conflict have suffered two shortfalls. They\ntended not to embody dynamic changes, focusing rather on static slices of\nbehavior over time. These models have also been empirically evaluated in ways\nthat assumed the independence of each country, when in reality they are\nsearching for the interdependence among all countries. We illustrate a solution\nto these two hurdles and evaluate this new, dynamic, network based approach to\nthe dependencies among the ebb and flow of daily international interactions\nusing a newly developed, and openly available, database of events among\nnations.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 13:35:34 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Minhas", "Shahryar", ""], ["Hoff", "Peter D.", ""], ["Ward", "Michael D.", ""]]}]