[{"id": "1706.00062", "submitter": "Cornelis Potgieter", "authors": "Amy E. Nussbaum, Cornelis J. Potgieter, Michael Chmielewski", "title": "A Latent Trait Model for Multivariate Longitudinal Data With Two Sources\n  of Measurement Error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personality traits are latent variables, and as such, are impossible to\nmeasure without the use of an assessment. Responses on the assessments can be\ninfluenced by both transient (state-related) error and measurement error,\nobscuring the true trait levels. Typically, these assessments utilize Likert\nscales, which yield only discrete data. The loss of information due to the\ndiscrete nature of the data represents an additional challenge in assessing the\nability of these instruments to measure the latent trait of interest.\n  This paper is concerned with parameter estimation in a model relating a\nlatent variable, as well transient error and measurement error components when\ndata are longitudinal and measured using a Likert scale. Two methods for\nparameter estimation are detailed: correlation reconstruction, a method that\nuses polychoric correlations, and maximum likelihood implemented using a\nStochastic EM algorithm. These methods are applied to a motivating dataset of\n440 college students taking the Big Five inventory twice in a two month period.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 19:39:31 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Nussbaum", "Amy E.", ""], ["Potgieter", "Cornelis J.", ""], ["Chmielewski", "Michael", ""]]}, {"id": "1706.00122", "submitter": "Poulomi Ganguli", "authors": "Poulomi Ganguli, Paulin Coulibaly", "title": "Assessment of Future Changes in Intensity-Duration-Frequency Curves for\n  Southern Ontario using North American (NA)-CORDEX Models with Nonstationary\n  Methods", "comments": "61 pages. 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.ao-ph physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The evaluation of possible climate change consequence on extreme rainfall has\nsignificant implications for the design of engineering structure and\nsocioeconomic resources development. While many studies have assessed the\nimpact of climate change on design rainfall using global and regional climate\nmodel (RCM) predictions, to date, there has been no comprehensive comparison or\nevaluation of intensity-duration-frequency (IDF) statistics at regional scale,\nconsidering both stationary versus nonstationary models for the future climate.\nTo understand how extreme precipitation may respond to future IDF curves, we\nused an ensemble of three RCMs participating in the North-American (NA)-CORDEX\ndomain over eight rainfall stations across Southern Ontario, one of the most\ndensely populated and major economic region in Canada. The IDF relationships\nare derived from multi-model RCM simulations and compared with the\nstation-based observations. We modeled precipitation extremes, at different\ndurations using extreme value distributions considering parameters that are\neither stationary or nonstationary, as a linear function of time. Our results\nshowed that extreme precipitation intensity driven by future climate forcing\nshows a significant increase in intensity for 10-year events in 2050s\n(2030-2070) relative to 1970-2010 baseline period across most of the locations.\nHowever, for longer return periods, an opposite trend is noted. Surprisingly,\nin term of design storms, no significant differences were found when comparing\nstationary and nonstationary IDF estimation methods for the future (2050s) for\nthe larger return periods. The findings, which are specific to regional\nprecipitation extremes, suggest no immediate reason for alarm, but the need for\nprogressive updating of the design standards in light of global warming.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 23:31:15 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Ganguli", "Poulomi", ""], ["Coulibaly", "Paulin", ""]]}, {"id": "1706.00291", "submitter": "Manish Narwaria Dr", "authors": "Manish Narwaria, Lukas Krasula, and Patrick Le Callet", "title": "Data Analysis in Multimedia Quality Assessment: Revisiting the\n  Statistical Tests", "comments": null, "journal-ref": "IEEE Transactions on Multimedia 2018", "doi": "10.1109/TMM.2018.2794266", "report-no": null, "categories": "cs.MM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessment of multimedia quality relies heavily on subjective assessment, and\nis typically done by human subjects in the form of preferences or continuous\nratings. Such data is crucial for analysis of different multimedia processing\nalgorithms as well as validation of objective (computational) methods for the\nsaid purpose. To that end, statistical testing provides a theoretical framework\ntowards drawing meaningful inferences, and making well grounded conclusions and\nrecommendations. While parametric tests (such as t test, ANOVA, and error\nestimates like confidence intervals) are popular and widely used in the\ncommunity, there appears to be a certain degree of confusion in the application\nof such tests. Specifically, the assumption of normality and homogeneity of\nvariance is often not well understood. Therefore, the main goal of this paper\nis to revisit them from a theoretical perspective and in the process provide\nuseful insights into their practical implications. Experimental results on both\nsimulated and real data are presented to support the arguments made. A software\nimplementing the said recommendations is also made publicly available, in order\nto achieve the goal of reproducible research.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 13:35:13 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Narwaria", "Manish", ""], ["Krasula", "Lukas", ""], ["Callet", "Patrick Le", ""]]}, {"id": "1706.00308", "submitter": "Andrey Gorshenin", "authors": "V.Yu. Korolev, A.K. Gorshenin", "title": "On the asymptotic approximation to the probability distribution of\n  extremal precipitation", "comments": "17 pages, 6 figures. arXiv admin note: text overlap with\n  arXiv:1703.07276", "journal-ref": "Journal of Mathematical Sciences, 2018. Vol. 234. Iss. 6. P.\n  886-903", "doi": "10.1007/s10958-018-4052-1", "report-no": null, "categories": "math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on the negative binomial model for the duration of wet periods measured\nin days, an asymptotic approximation is proposed for the distribution of the\nmaximum daily precipitation volume within a wet period. This approximation has\nthe form of a scale mixture of the Frechet distribution with the gamma mixing\ndistribution and coincides with the distribution of a positive power of a\nrandom variable having the Snedecor-Fisher distribution. The proof of this\nresult is based on the representation of the negative binomial distribution as\na mixed geometric (and hence, mixed Poisson) distribution and limit theorems\nfor extreme order statistics in samples with random sizes having mixed Poisson\ndistributions. Some analytic properties of the obtained limit distribution are\ndescribed. In particular, it is demonstrated that under certain conditions the\nlimit distribution is mixed exponential and hence, is infinitely divisible. It\nis shown that under the same conditions the limit distribution can be\nrepresented as a scale mixture of stable or Weibull or Pareto or folded normal\nlaws. The corresponding product representations for the limit random variable\ncan be used for its computer simulation. Several methods are proposed for the\nestimation of the parameters of the distribution of the maximum daily\nprecipitation volume. The results of fitting this distribution to real data are\npresented illustrating high adequacy of the proposed model. The obtained\nmixture representations for the limit laws and the corresponding asymptotic\napproximations provide better insight into the nature of mixed probability\n(\"Bayesian\") models.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 11:21:20 GMT"}, {"version": "v2", "created": "Sun, 4 Jun 2017 17:58:39 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Korolev", "V. Yu.", ""], ["Gorshenin", "A. K.", ""]]}, {"id": "1706.00599", "submitter": "Fabrizio Leisen", "authors": "Fabrizio Leisen, Cristiano Villa, Stephen G. Walker", "title": "On a Class of Objective Priors from Scoring Rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective prior distributions represent an important tool that allows one to\nhave the advantages of using the Bayesian framework even when information about\nthe parameters of a model is not available. The usual objective approaches work\noff the chosen statistical model and in the majority of cases the resulting\nprior is improper, which can pose limitations to a practical implementation,\neven when the complexity of the model is moderate. In this paper we propose to\ntake a novel look at the construction of objective prior distributions, where\nthe connection with a chosen sampling distribution model is removed. We explore\nthe notion of defining objective prior distributions which allow one to have\nsome degree of flexibility, in particular in exhibiting some desirable\nfeatures, such as being proper, or centered on specific values which would be\nof interest in nested model comparisons. The basic tool we use are proper\nscoring rules and the main result is a class of objective prior distributions\nthat can be employed in scenarios where the usual model based priors fail, such\nas mixture models and model selection via Bayes factors. In addition, we show\nthat the proposed class of priors is the result of minimising the information\nit contains, providing solid interpretation to the method.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 09:17:54 GMT"}, {"version": "v2", "created": "Sun, 23 Sep 2018 19:53:43 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Leisen", "Fabrizio", ""], ["Villa", "Cristiano", ""], ["Walker", "Stephen G.", ""]]}, {"id": "1706.00636", "submitter": "Caifa Zhou", "authors": "Yang Gu, Caifa Zhou, Andreas Wieser, and Zhimin Zhou", "title": "WiFi based trajectory alignment, calibration and easy site survey using\n  smart phones and foot-mounted IMUs", "comments": "9 figures, 6 pages, paper under review of IPIN 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Foot-mounted inertial positioning (FMIP) can face problems of inertial drifts\nand unknown initial states in real applications, which renders the estimated\ntrajectories inaccurate and not obtained in a well defined coordinate system\nfor matching trajectories of different users. In this paper, an approach\nadopting received signal strength (RSS) measurements for Wifi access points\n(APs) are proposed to align and calibrate the trajectories estimated from foot\nmounted inertial measurement units (IMUs). A crowd-sourced radio map (RM) can\nbe built subsequently and can be used for fingerprinting based Wifi indoor\npositioning (FWIP). The foundation of the proposed approach is graph based\nsimultaneously localization and mapping (SLAM). The nodes in the graph denote\nusers poses and the edges denote the pairwise constrains between the nodes. The\nconstrains are derived from: (1) inertial estimated trajectories; (2) vicinity\nin the RSS space. With these constrains, an error functions is defined. By\nminimizing the error function, the graph is optimized and the\naligned/calibrated trajectories along with the RM are acquired. The\nexperimental results have corroborated the effectiveness of the approach for\ntrajectory alignment, calibration as well as RM construction.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 11:34:01 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Gu", "Yang", ""], ["Zhou", "Caifa", ""], ["Wieser", "Andreas", ""], ["Zhou", "Zhimin", ""]]}, {"id": "1706.00641", "submitter": "Dennis Te Beest", "authors": "Dennis E. te Beest, Steven W. Mes, Ruud H. Brakenhoff, Mark A. van de\n  Wiel", "title": "Improved high-dimensional prediction with Random Forests by the use of\n  co-data", "comments": "17 pages, 4 figures, to be published", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction in high dimensional settings is difficult due to large by number\nof variables relative to the sample size. We demonstrate how auxiliary\n\"co-data\" can be used to improve the performance of a Random Forest in such a\nsetting. Co-data are incorporated in the Random Forest by replacing the uniform\nsampling probabilities (used to draw candidate variables, the default for a\nRandom Forest) by co-data moderated sampling probabilities. Co-data here is\ndefined as any type information that is available on the variables of the\nprimary data, but does not use its response labels. These moderated sampling\nprobabilities are, inspired by empirical Bayes, learned from the data at hand.\nWe demonstrate this co-data moderated Random Forest (CoRF) with one example. In\nthe example we aim to predict a lymph node metastasis with gene expression\ndata. We demonstrate how a set of external p-values, a gene signature, and the\ncorrelation between gene expression and DNA copy number can improve the\npredictive performance.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 11:51:37 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Beest", "Dennis E. te", ""], ["Mes", "Steven W.", ""], ["Brakenhoff", "Ruud H.", ""], ["van de Wiel", "Mark A.", ""]]}, {"id": "1706.00959", "submitter": "Amanda Mejia", "authors": "Amanda Mejia, Yu Ryan Yue, David Bolin, Finn Lindren and Martin A.\n  Lindquist", "title": "A Bayesian General Linear Modeling Approach to Cortical Surface fMRI\n  Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cortical surface fMRI (cs-fMRI) has recently grown in popularity versus\ntraditional volumetric fMRI, as it allows for more meaningful spatial smoothing\nand is more compatible with the common assumptions of isotropy and stationarity\nin Bayesian spatial models. However, as no Bayesian spatial model has been\nproposed for cs-fMRI data, most analyses continue to employ the classical,\nvoxel-wise general linear model (GLM) (Worsley and Friston 1995). Here, we\npropose a Bayesian GLM for cs-fMRI, which employs a class of sophisticated\nspatial processes to flexibly model latent activation fields. We use integrated\nnested Laplacian approximation (INLA), a highly accurate and efficient Bayesian\ncomputation technique (Rue et al. 2009). To identify regions of activation, we\npropose an excursions set method based on the joint posterior distribution of\nthe latent fields, which eliminates the need for multiple comparisons\ncorrection. Finally, we address a gap in the existing literature by proposing a\nnovel Bayesian approach for multi-subject analysis. The methods are validated\nand compared to the classical GLM through simulation studies and a motor task\nfMRI study from the Human Connectome Project. The proposed Bayesian approach\nresults in smoother activation estimates, more accurate false positive control,\nand increased power to detect truly active regions.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jun 2017 14:38:14 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Mejia", "Amanda", ""], ["Yue", "Yu Ryan", ""], ["Bolin", "David", ""], ["Lindren", "Finn", ""], ["Lindquist", "Martin A.", ""]]}, {"id": "1706.01099", "submitter": "Christopher Fariss", "authors": "Christopher J. Fariss, Charles D. Crabtree, Therese Anders, Zachary M.\n  Jones, Fridolin J. Linder, Jonathan N. Markowitz", "title": "Latent Estimation of GDP, GDP per capita, and Population from Historic\n  and Contemporary Sources", "comments": "Gross Domestic Product, population, GDP per capita, latent variables,\n  measurement, construct validity", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concepts of Gross Domestic Product (GDP), GDP per capita, and population\nare central to the study of political science and economics. However, a growing\nliterature suggests that existing measures of these concepts contain\nconsiderable error or are based on overly simplistic modeling choices. We\naddress these problems by creating a dynamic, three-dimensional latent trait\nmodel, which uses observed information about GDP, GDP per capita, and\npopulation to estimate posterior prediction intervals for each of these\nimportant concepts. By combining historical and contemporary sources of\ninformation, we are able to extend the temporal and spatial coverage of\nexisting datasets for country-year units back to 1500 A.D through 2015 A.D.\nand, because the model makes use of multiple indicators of the underlying\nconcepts, we are able to estimate the relative precision of the different\ncountry-year estimates. Overall, our latent variable model offers a principled\nmethod for incorporating information from different historic and contemporary\ndata sources. It can be expanded or refined as researchers discover new or\nalternative sources of information about these concepts.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jun 2017 16:22:33 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Fariss", "Christopher J.", ""], ["Crabtree", "Charles D.", ""], ["Anders", "Therese", ""], ["Jones", "Zachary M.", ""], ["Linder", "Fridolin J.", ""], ["Markowitz", "Jonathan N.", ""]]}, {"id": "1706.01238", "submitter": "Lev B Klebanov", "authors": "Lev B. Klebanov", "title": "One look at the rating of scientific publications and corresponding\n  toy-model", "comments": "17 pages; 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A toy-model of publications and citations processes is proposed. The model\nshows that the role of randomness in the processes is essential and cannot be\nignored. Some other aspects of scientific publications rating are discussed.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 08:49:01 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Klebanov", "Lev B.", ""]]}, {"id": "1706.01242", "submitter": "Jos van der Westhuizen", "authors": "Jos van der Westhuizen and Joan Lasenby", "title": "Bayesian LSTMs in medicine", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The medical field stands to see significant benefits from the recent advances\nin deep learning. Knowing the uncertainty in the decision made by any machine\nlearning algorithm is of utmost importance for medical practitioners. This\nstudy demonstrates the utility of using Bayesian LSTMs for classification of\nmedical time series. Four medical time series datasets are used to show the\naccuracy improvement Bayesian LSTMs provide over standard LSTMs. Moreover, we\nshow cherry-picked examples of confident and uncertain classifications of the\nmedical time series. With simple modifications of the common practice for deep\nlearning, significant improvements can be made for the medical practitioner and\npatient.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 09:04:07 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["van der Westhuizen", "Jos", ""], ["Lasenby", "Joan", ""]]}, {"id": "1706.01498", "submitter": "Yizhe Zhang", "authors": "Yizhe Zhang, Changyou Chen, Zhe Gan, Ricardo Henao, Lawrence Carin", "title": "Stochastic Gradient Monomial Gamma Sampler", "comments": "Published on ICML 2017", "journal-ref": "Proceedings of the 34th International Conference on Machine\n  Learning, PMLR 70:3996-4005, 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in stochastic gradient techniques have made it possible to\nestimate posterior distributions from large datasets via Markov Chain Monte\nCarlo (MCMC). However, when the target posterior is multimodal, mixing\nperformance is often poor. This results in inadequate exploration of the\nposterior distribution. A framework is proposed to improve the sampling\nefficiency of stochastic gradient MCMC, based on Hamiltonian Monte Carlo. A\ngeneralized kinetic function is leveraged, delivering superior stationary\nmixing, especially for multimodal distributions. Techniques are also discussed\nto overcome the practical issues introduced by this generalization. It is shown\nthat the proposed approach is better at exploring complex multimodal posterior\ndistributions, as demonstrated on multiple applications and in comparison with\nother stochastic gradient MCMC methods.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 18:48:31 GMT"}, {"version": "v2", "created": "Wed, 10 Jan 2018 19:38:23 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Zhang", "Yizhe", ""], ["Chen", "Changyou", ""], ["Gan", "Zhe", ""], ["Henao", "Ricardo", ""], ["Carin", "Lawrence", ""]]}, {"id": "1706.01591", "submitter": "Wen Luo", "authors": "Wen Luo and Zdenek P. Bazant", "title": "Fishnet Statistics for Strength Scaling of Nacreous Imbricated Lamellar\n  Materials", "comments": null, "journal-ref": null, "doi": "10.1016/j.jmps.2017.07.023", "report-no": null, "categories": "stat.AP cond-mat.soft", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similar to nacre or brick-and-mortar structures, imbricated lamellar\nstructures are widely found in natural and man-made materials and are of\ninterest for biomimetics. These structures are known to be rather insensitive\nto defects and to have a very high fracture toughness. Their deterministic\nbehavior has been intensely studied, but statistical studies have been rare and\nis so far there is no undisputed theoretical basis for the probability\ndistribution of strength of nacre. This paper presents a numerical and\ntheoretical study of the PDF of strength and of the corresponding statistical\nsize effect. After a reasonable simplifications of the shear bonds, an axially\nloaded lamellar shell is statistically modelled as a square fishnet pulled\ndiagonally. A finite element (FE) model is developed and used in Monte Carlo\nsimulations of strength. An analytical model for failure probability of the\nfishnet is developed and matched to the computed statistical histograms of\nstrength for various sizes. It appears that, with increasing size, the pdf of\nstrength slowly transits from Gaussian to Weilbull distribution but the\ntransition is different from that previously obtained at Northwestern for\nquasibrittle materials of random heterogeneous mesostructure.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 03:23:02 GMT"}, {"version": "v2", "created": "Sat, 8 Jul 2017 00:35:56 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Luo", "Wen", ""], ["Bazant", "Zdenek P.", ""]]}, {"id": "1706.01625", "submitter": "Ananya Lahiri", "authors": "Kanika Saha, Ananya Lahiri", "title": "Understanding Betting Strategy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present betting strategy of a football game using\nprobability theory. We know all betting houses offer slightly unfair odds\ntowards the player. Here we discuss a simple way to figure out which betting\nhouse is offering relatively better odds compared to others for English Premier\nLeague. However, this methodology can be adopted for another league football\nmatch.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 06:53:09 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Saha", "Kanika", ""], ["Lahiri", "Ananya", ""]]}, {"id": "1706.01681", "submitter": "Jan Kieseler", "authors": "Jan Kieseler", "title": "A method and tool for combining differential or inclusive measurements\n  obtained with simultaneously constrained uncertainties", "comments": "12 pages, 15 figures (v3: changed figure ranges) Accepted by EPJC", "journal-ref": "Eur. Phys. J. C (2017) 77: 792", "doi": "10.1140/epjc/s10052-017-5345-0", "report-no": null, "categories": "physics.data-an hep-ex stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method is discussed that allows combining sets of differential or inclusive\nmeasurements. It is assumed that at least one measurement was obtained with\nsimultaneously fitting a set of nuisance parameters, representing sources of\nsystematic uncertainties. As a result of beneficial constraints from the data\nall such fitted parameters are correlated among each other. The best approach\nfor a combination of these measurements would be the maximisation of a combined\nlikelihood, for which the full fit model of each measurement and the original\ndata are required. However, only in rare cases this information is publicly\navailable. In absence of this information most commonly used combination\nmethods are not able to account for these correlations between uncertainties,\nwhich can lead to severe biases as shown in this article. The method discussed\nhere provides a solution for this problem. It relies on the public result and\nits covariance or Hessian, only, and is validated against the\ncombined-likelihood approach. A dedicated software package implementing this\nmethod is also presented. It provides a text-based user interface alongside a\nC++ interface. The latter also interfaces to ROOT classes for simple\ncombination of binned measurements such as differential cross sections.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 10:11:32 GMT"}, {"version": "v2", "created": "Wed, 2 Aug 2017 13:49:05 GMT"}, {"version": "v3", "created": "Fri, 27 Oct 2017 13:01:34 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Kieseler", "Jan", ""]]}, {"id": "1706.02289", "submitter": "Evgeny Burnaev", "authors": "Smolyakov Dmitry, Alexander Korotin, Pavel Erofeev, Artem Papanov,\n  Evgeny Burnaev", "title": "Meta-Learning for Resampling Recommendation Systems", "comments": "23 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One possible approach to tackle the class imbalance in classification tasks\nis to resample a training dataset, i.e., to drop some of its elements or to\nsynthesize new ones. There exist several widely-used resampling methods. Recent\nresearch showed that the choice of resampling method significantly affects the\nquality of classification, which raises resampling selection problem.\nExhaustive search for optimal resampling is time-consuming and hence it is of\nlimited use. In this paper, we describe an alternative approach to the\nresampling selection. We follow the meta-learning concept to build resampling\nrecommendation systems, i.e., algorithms recommending resampling for datasets\non the basis of their properties.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 22:02:27 GMT"}, {"version": "v2", "created": "Wed, 21 Jun 2017 21:19:48 GMT"}, {"version": "v3", "created": "Mon, 19 Feb 2018 08:08:13 GMT"}, {"version": "v4", "created": "Mon, 17 Sep 2018 07:50:39 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Dmitry", "Smolyakov", ""], ["Korotin", "Alexander", ""], ["Erofeev", "Pavel", ""], ["Papanov", "Artem", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "1706.02353", "submitter": "Dengdeng Yu", "authors": "Dengdeng Yu, Li Zhang, Ivan Mizera, Bei Jiang, and Linglong Kong", "title": "Sparse Wavelet Estimation in Quantile Regression with Multiple\n  Functional Predictors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this manuscript, we study quantile regression in partial functional linear\nmodel where response is scalar and predictors include both scalars and multiple\nfunctions. Wavelet basis are adopted to better approximate functional slopes\nwhile effectively detect local features. The sparse group lasso penalty is\nimposed to select important functional predictors while capture shared\ninformation among them. The estimation problem can be reformulated into a\nstandard second-order cone program and then solved by an interior point method.\nWe also give a novel algorithm by using alternating direction method of\nmultipliers (ADMM) which was recently employed by many researchers in solving\npenalized quantile regression problems. The asymptotic properties such as the\nconvergence rate and prediction error bound have been established. Simulations\nand a real data from ADHD-200 fMRI data are investigated to show the\nsuperiority of our proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 19:28:53 GMT"}, {"version": "v2", "created": "Sat, 2 Dec 2017 23:34:51 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Yu", "Dengdeng", ""], ["Zhang", "Li", ""], ["Mizera", "Ivan", ""], ["Jiang", "Bei", ""], ["Kong", "Linglong", ""]]}, {"id": "1706.02380", "submitter": "Anru Zhang", "authors": "Yuanpei Cao and Anru Zhang and Hongzhe Li", "title": "Multi-sample Estimation of Bacterial Composition Matrix in Metagenomics\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Metagenomics sequencing is routinely applied to quantify bacterial abundances\nin microbiome studies, where the bacterial composition is estimated based on\nthe sequencing read counts. Due to limited sequencing depth and DNA dropouts,\nmany rare bacterial taxa might not be captured in the final sequencing reads,\nwhich results in many zero counts. Naive composition estimation using count\nnormalization leads to many zero proportions, which tend to result in\ninaccurate estimates of bacterial abundance and diversity. This paper takes a\nmulti-sample approach to the estimation of bacterial abundances in order to\nborrow information across samples and across species. Empirical results from\nreal data sets suggest that the composition matrix over multiple samples is\napproximately low rank, which motivates a regularized maximum likelihood\nestimation with a nuclear norm penalty. An efficient optimization algorithm\nusing the generalized accelerated proximal gradient and Euclidean projection\nonto simplex space is developed. The theoretical upper bounds and the minimax\nlower bounds of the estimation errors, measured by the Kullback-Leibler\ndivergence and the Frobenius norm, are established. Simulation studies\ndemonstrate that the proposed estimator outperforms the naive estimators. The\nmethod is applied to an analysis of a human gut microbiome dataset.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 21:05:26 GMT"}, {"version": "v2", "created": "Sat, 24 Nov 2018 05:50:09 GMT"}, {"version": "v3", "created": "Tue, 27 Nov 2018 01:44:19 GMT"}, {"version": "v4", "created": "Wed, 24 Apr 2019 18:41:04 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Cao", "Yuanpei", ""], ["Zhang", "Anru", ""], ["Li", "Hongzhe", ""]]}, {"id": "1706.02447", "submitter": "Raquel Aoki", "authors": "Raquel YS Aoki, Renato M Assuncao, Pedro OS Vaz de Melo", "title": "Luck is Hard to Beat: The Difficulty of Sports Prediction", "comments": "10 pages, KDD2017, Applied Data Science track", "journal-ref": "Proceedings of the 23rd ACM SIGKDD International Conference on\n  Knowledge Discovery and Data Mining, 2017", "doi": "10.1145/3097983.3098045", "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the outcome of sports events is a hard task. We quantify this\ndifficulty with a coefficient that measures the distance between the observed\nfinal results of sports leagues and idealized perfectly balanced competitions\nin terms of skill. This indicates the relative presence of luck and skill. We\ncollected and analyzed all games from 198 sports leagues comprising 1503\nseasons from 84 countries of 4 different sports: basketball, soccer, volleyball\nand handball. We measured the competitiveness by countries and sports. We also\nidentify in each season which teams, if removed from its league, result in a\ncompletely random tournament. Surprisingly, not many of them are needed. As\nanother contribution of this paper, we propose a probabilistic graphical model\nto learn about the teams' skills and to decompose the relative weights of luck\nand skill in each game. We break down the skill component into factors\nassociated with the teams' characteristics. The model also allows to estimate\nas 0.36 the probability that an underdog team wins in the NBA league, with a\nhome advantage adding 0.09 to this probability. As shown in the first part of\nthe paper, luck is substantially present even in the most competitive\nchampionships, which partially explains why sophisticated and complex\nfeature-based models hardly beat simple models in the task of forecasting\nsports' outcomes.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 03:38:27 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Aoki", "Raquel YS", ""], ["Assuncao", "Renato M", ""], ["de Melo", "Pedro OS Vaz", ""]]}, {"id": "1706.02508", "submitter": "Loumpiana Koulai", "authors": "Loumpiana Koulai, Anne Presanis, Gary Murphy, Barbara Suligoi and\n  Daniela De Angelis", "title": "Quantifying the recency of HIV infection using multiple longitudinal\n  biomarkers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge of the time at which an HIV-infected individual seroconverts, when\nthe immune system starts responding to HIV infection, plays a vital role in the\ndesign and implementation of interventions to reduce the impact of the HIV\nepidemic. A number of biomarkers have been developed to distinguish between\nrecent and long-term HIV infection, based on the antibody response to HIV. To\nquantify the recency of infection at an individual level, we propose\ncharacterising the growth of such biomarkers from observations from a panel of\nindividuals with known seroconversion time, using Bayesian mixed effect models.\nWe combine this knowledge of the growth patterns with observations from a newly\ndiagnosed individual, to estimate the probability seroconversion occurred in\nthe X months prior to diagnosis. We explore, through a simulation study, the\ncharacteristics of different biomarkers that affect our ability to estimate\nrecency, such as the growth rate. In particular, we find that predictive\nability is improved by using joint models of two biomarkers, accounting for\ntheir correlation, rather than univariate models of single biomarkers.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 10:51:31 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Koulai", "Loumpiana", ""], ["Presanis", "Anne", ""], ["Murphy", "Gary", ""], ["Suligoi", "Barbara", ""], ["De Angelis", "Daniela", ""]]}, {"id": "1706.02527", "submitter": "Alice Corbella", "authors": "Alice Corbella (1), Xu-Sheng Zhang (2), Paul J. Birrell (1), Nicky\n  Boddington (2), Anne M. Presanis (1), Richard G. Pebody (2), and Daniela De\n  Angelis (1,2) ((1) MRC Biostatistics Unit, School of Clinical Medicine,\n  University of Cambridge (2) Centre for Infectious Disease Surveillance and\n  Control, Public Health England)", "title": "Exploiting routinely collected severe case data to monitor and predict\n  influenza outbreaks", "comments": "17 pages, 6 Figures, 3 tables and one ancillary file\n  (Additional_File_1) Replacement of \"Monitoring and predicting influenza\n  epidemics from routinely collected severe case data\" with the following\n  changes: we used a continuous-time system to describe transmission; we used a\n  time varying transmission; we improved the analysis of the waiting time from\n  symptoms to ICU admissions", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Influenza remains a significant burden on health systems. Effective responses\nrely on the timely understanding of the magnitude and the evolution of an\noutbreak. For monitoring purposes, data on severe cases of influenza in England\nare reported weekly to Public Health England. These data are both readily\navailable and have the potential to provide valuable information to estimate\nand predict the key transmission features of seasonal and pandemic influenza.\nWe propose an epidemic model that links the underlying unobserved influenza\ntransmission process to data on severe influenza cases. Within a Bayesian\nframework, we infer retrospectively the parameters of the epidemic model for\neach seasonal outbreak from 2012 to 2015, including: the effective reproduction\nnumber; the initial susceptibility; the probability of admission to intensive\ncare given infection; and the effect of school closure on transmission. The\nmodel is also implemented in real time to assess whether early forecasting of\nthe number of admission to intensive care is possible. Our model of admissions\ndata allows reconstruction of the underlying transmission dynamics revealing:\nincreased transmission during the season 2013/14 and a noticeable effect of\nChristmas school holiday on disease spread during season 2012/13 and 2014/15.\nWhen information on the initial immunity of the population is available,\nforecasts of the number of admissions to intensive care can be substantially\nimproved. Readily available severe case data can be effectively used to\nestimate epidemiological characteristics and to predict the evolution of an\nepidemic, crucially allowing real-time monitoring of the transmission and\nseverity of the outbreak.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 11:58:22 GMT"}, {"version": "v2", "created": "Mon, 13 Nov 2017 17:29:06 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Corbella", "Alice", ""], ["Zhang", "Xu-Sheng", ""], ["Birrell", "Paul J.", ""], ["Boddington", "Nicky", ""], ["Presanis", "Anne M.", ""], ["Pebody", "Richard G.", ""], ["De Angelis", "Daniela", ""]]}, {"id": "1706.02754", "submitter": "Mir Hadi Athari", "authors": "Mir Hadi Athari, Zhifang Wang", "title": "Statistically Characterizing the Electrical Parameters of the Grid\n  Transformers and Transmission Lines", "comments": "To be published (Accepted) in: The 10th Bulk Power Systems Dynamics\n  and Control Symposium (IREP2017), Espinho, Portugal, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a set of validation metrics for transmission network\nparameters that is applicable in both creation of synthetic power system test\ncases and validation of existing models. Using actual data from two real-world\npower grids, statistical analyses are performed to extract some useful\nstatistics on transformers and transmission lines electrical parameters\nincluding per unit reactance, MVA rating, and their X/R ratio. It is found that\nconversion of per unit reactance calculated on system common base to\ntransformer own power base will significantly stabilize its range and remove\nthe correlation between per unit X and MVA rating. This is fairly consistent\nfor transformers with different voltage levels and sizes and can be utilized as\na strong validation metric for synthetic models. It is found that transmission\nlines exhibit different statistical properties than transformers with different\ndistribution and range for the parameters. In addition, statistical analysis\nshows that the empirical PDF of transmission network electrical parameters can\nbe approximated with mathematical distribution functions which would help\nappropriately characterize them in synthetic power networks. Kullback-Leibler\ndivergence is used as a measure for goodness of fit for approximated\ndistributions.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 20:17:39 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Athari", "Mir Hadi", ""], ["Wang", "Zhifang", ""]]}, {"id": "1706.02813", "submitter": "Matthew Roughan", "authors": "George Michaelson, Matthew Roughan, Jonathan Tuke, Matt P. Wand, and\n  Randy Bush", "title": "Rigorous statistical analysis of HTTPS reachability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of secure connections using HTTPS as the default means, or even the\nonly means, to connect to web servers is increasing. It is being pushed from\nboth sides: from the bottom up by client distributions and plugins, and from\nthe top down by organisations such as Google. However, there are potential\ntechnical hurdles that might lock some clients out of the modern web. This\npaper seeks to measure and precisely quantify those hurdles in the wild. More\nthan three million measurements provide statistically significant evidence of\ndegradation. We show this through a variety of statistical techniques. Various\nfactors are shown to influence the problem, ranging from the client's browser,\nto the locale from which they connect.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 02:45:16 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Michaelson", "George", ""], ["Roughan", "Matthew", ""], ["Tuke", "Jonathan", ""], ["Wand", "Matt P.", ""], ["Bush", "Randy", ""]]}, {"id": "1706.02940", "submitter": "Theodore  Kypraios", "authors": "Theodore Kypraios and Philip D. O'Neill", "title": "Bayesian nonparametrics for stochastic epidemic models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vast majority of models for the spread of communicable diseases are\nparametric in nature and involve underlying assumptions about how the disease\nspreads through a population. In this article we consider the use of Bayesian\nnonparametric approaches to analysing data from disease outbreaks. Specifically\nwe focus on methods for estimating the infection process in simple models under\nthe assumption that this process has an explicit time-dependence.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 13:12:27 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Kypraios", "Theodore", ""], ["O'Neill", "Philip D.", ""]]}, {"id": "1706.02952", "submitter": "Amit Dhurandhar", "authors": "Amit Dhurandhar, Vijay Iyengar, Ronny Luss and Karthikeyan Shanmugam", "title": "TIP: Typifying the Interpretability of Procedures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a novel notion of what it means to be interpretable, looking past\nthe usual association with human understanding. Our key insight is that\ninterpretability is not an absolute concept and so we define it relative to a\ntarget model, which may or may not be a human. We define a framework that\nallows for comparing interpretable procedures by linking them to important\npractical aspects such as accuracy and robustness. We characterize many of the\ncurrent state-of-the-art interpretable methods in our framework portraying its\ngeneral applicability. Finally, principled interpretable strategies are\nproposed and empirically evaluated on synthetic data, as well as on the largest\npublic olfaction dataset that was made recently available \\cite{olfs}. We also\nexperiment on MNIST with a simple target model and different oracle models of\nvarying complexity. This leads to the insight that the improvement in the\ntarget model is not only a function of the oracle model's performance, but also\nits relative complexity with respect to the target model. Further experiments\non CIFAR-10, a real manufacturing dataset and FICO dataset showcase the benefit\nof our methods over Knowledge Distillation when the target models are simple\nand the complex model is a neural network.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 13:55:18 GMT"}, {"version": "v2", "created": "Tue, 26 Dec 2017 16:12:02 GMT"}, {"version": "v3", "created": "Mon, 29 Oct 2018 15:49:37 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Dhurandhar", "Amit", ""], ["Iyengar", "Vijay", ""], ["Luss", "Ronny", ""], ["Shanmugam", "Karthikeyan", ""]]}, {"id": "1706.03012", "submitter": "Antonio Linero", "authors": "Antonio R. Linero and Jonathan R. Bradley and Apurva Desai", "title": "Multi-rubric Models for Ordinal Spatial Data with Application to Online\n  Ratings from Yelp", "comments": "25 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interest in online rating data has increased in recent years in which ordinal\nratings of products or local businesses are provided by users of a website,\nsuch as Yelp or Amazon. One source of heterogeneity in ratings is that users\napply different standards when supplying their ratings; even if two users\nbenefit from a product the same amount, they may translate their benefit into\nratings in different ways. In this article we propose an ordinal data model,\nwhich we refer to as a multi-rubric model, which treats the criteria used to\nconvert a latent utility into a rating as user-specific random effects, with\nthe distribution of these random effects being modeled nonparametrically. We\ndemonstrate that this approach is capable of accounting for this type of\nvariability in addition to usual sources of heterogeneity due to item quality,\nuser biases, interactions between items and users, and the spatial structure of\nthe users and items. We apply the model developed here to publicly available\ndata from the website Yelp and demonstrate that it produces interpretable\nclusterings of users according to their rating behavior, in addition to\nproviding better predictions of ratings and better summaries of overall item\nquality.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 15:56:22 GMT"}, {"version": "v2", "created": "Thu, 15 Jun 2017 11:55:44 GMT"}, {"version": "v3", "created": "Fri, 22 Dec 2017 02:50:09 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Linero", "Antonio R.", ""], ["Bradley", "Jonathan R.", ""], ["Desai", "Apurva", ""]]}, {"id": "1706.03151", "submitter": "Le Zheng", "authors": "Le Zheng and Marco Lops and Xiaodong Wang", "title": "Adaptive Interference Removal for Un-coordinated Radar/Communication\n  Co-existence", "comments": null, "journal-ref": null, "doi": "10.1109/JSTSP.2017.2785783", "report-no": null, "categories": "cs.SY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing approaches to co-existing communication/radar systems assume\nthat the radar and communication systems are coordinated, i.e., they share\ninformation, such as relative position, transmitted waveforms and channel\nstate. In this paper, we consider an un-coordinated scenario where a\ncommunication receiver is to operate in the presence of a number of radars, of\nwhich only a sub-set may be active, which poses the problem of estimating the\nactive waveforms and the relevant parameters thereof, so as to cancel them\nprior to demodulation. Two algorithms are proposed for such a joint waveform\nestimation/data demodulation problem, both exploiting sparsity of a proper\nrepresentation of the interference and of the vector containing the errors of\nthe data block, so as to implement an iterative joint interference removal/data\ndemodulation process. The former algorithm is based on classical on-grid\ncompressed sensing (CS), while the latter forces an atomic norm (AN)\nconstraint: in both cases the radar parameters and the communication\ndemodulation errors can be estimated by solving a convex problem. We also\npropose a way to improve the efficiency of the AN-based algorithm. The\nperformance of these algorithms are demonstrated through extensive simulations,\ntaking into account a variety of conditions concerning both the interferers and\nthe respective channel states.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 23:24:08 GMT"}, {"version": "v2", "created": "Tue, 19 Dec 2017 22:38:33 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Zheng", "Le", ""], ["Lops", "Marco", ""], ["Wang", "Xiaodong", ""]]}, {"id": "1706.03412", "submitter": "Evgeny Burnaev", "authors": "Vladislav Ishimtsev, Ivan Nazarov, Alexander Bernstein and Evgeny\n  Burnaev", "title": "Conformal k-NN Anomaly Detector for Univariate Data Streams", "comments": "15 pages, 2 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomalies in time-series data give essential and often actionable information\nin many applications. In this paper we consider a model-free anomaly detection\nmethod for univariate time-series which adapts to non-stationarity in the data\nstream and provides probabilistic abnormality scores based on the conformal\nprediction paradigm. Despite its simplicity the method performs on par with\ncomplex prediction-based models on the Numenta Anomaly Detection benchmark and\nthe Yahoo! S5 dataset.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jun 2017 21:45:24 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Ishimtsev", "Vladislav", ""], ["Nazarov", "Ivan", ""], ["Bernstein", "Alexander", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "1706.03423", "submitter": "Xiaolei Fang", "authors": "Xiaolei Fang, Kamran Paynabar, Nagi Gebraeel", "title": "Image-Based Prognostics Using Penalized Tensor Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new methodology to predict and update the residual\nuseful lifetime of a system using a sequence of degradation images. The\nmethodology integrates tensor linear algebra with traditional location-scale\nregression widely used in reliability and prognosis. To address the high\ndimensionality challenge, the degradation image streams are first projected to\na low-dimensional tensor subspace that is able to preserve their information.\nNext, the projected image tensors are regressed against time-to-failure via\npenalized location-scale tensor regression. The coefficient tensor is then\ndecomposed using CANDECOMP/PARAFAC (CP) and Tucker decompositions, which\nenables parameter estimation in a high-dimensional setting. Two optimization\nalgorithms with a global convergence property are developed for model\nestimation. The effectiveness of our models is validated using a simulated\ndataset and infrared degradation image streams from a rotating machinery.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jun 2017 23:26:57 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Fang", "Xiaolei", ""], ["Paynabar", "Kamran", ""], ["Gebraeel", "Nagi", ""]]}, {"id": "1706.03442", "submitter": "Nishant Desai", "authors": "Alon Daks, Nishant Desai, Lisa R. Goldberg", "title": "Do Steph Curry and Klay Thompson Have Hot Hands?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Star Golden State Warriors Steph Curry, Klay Thompson, and Kevin Durant are\ngreat shooters but they are not streak shooters. Only rarely do they show signs\nof a hot hand. This conclusion is based on an empirical analysis of field goal\nand free throw data from the 82 regular season and 17 postseason games played\nby the Warriors in 2016--2017. Our analysis is inspired by the iconic 1985\nhot-hand study by Thomas Gilovitch, Robert Vallone and Amos Tversky, but uses a\npermutation test to automatically account for Josh Miller and Adam Sanjurjo's\nrecent small sample correction. In this study we show how long standing\nproblems can be reexamined using nonparametric statistics to avoid faulty\nhypothesis tests due to misspecified distributions.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 02:44:43 GMT"}, {"version": "v2", "created": "Sat, 4 Nov 2017 20:45:29 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Daks", "Alon", ""], ["Desai", "Nishant", ""], ["Goldberg", "Lisa R.", ""]]}, {"id": "1706.03671", "submitter": "Florian Pein", "authors": "Florian Pein, Inder Tecuapetla-G\\'omez, Ole M. Sch\\\"utte, Claudia\n  Steinem, Axel Munk", "title": "Fully-Automatic Multiresolution Idealization for Filtered Ion Channel\n  Recordings: Flickering Event Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new model-free segmentation method, JULES, which combines recent\nstatistical multiresolution techniques with local deconvolution for\nidealization of ion channel recordings. The multiresolution criterion takes\ninto account scales down to the sampling rate enabling the detection of\nflickering events, i.e., events on small temporal scales, even below the filter\nfrequency. For such small scales the deconvolution step allows for a precise\ndetermination of dwell times and, in particular, of amplitude levels, a task\nwhich is not possible with common thresholding methods. This is confirmed\ntheoretically and in a comprehensive simulation study. In addition, JULES can\nbe applied as a preprocessing method for a refined hidden Markov analysis. Our\nnew methodolodgy allows us to show that gramicidin A flickering events have the\nsame amplitude as the slow gating events. JULES is available as an R function\njules in the package clampSeg.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 14:51:02 GMT"}, {"version": "v2", "created": "Fri, 23 Feb 2018 17:15:57 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Pein", "Florian", ""], ["Tecuapetla-G\u00f3mez", "Inder", ""], ["Sch\u00fctte", "Ole M.", ""], ["Steinem", "Claudia", ""], ["Munk", "Axel", ""]]}, {"id": "1706.03736", "submitter": "Ignacio Heredia", "authors": "Ignacio Heredia", "title": "Large-Scale Plant Classification with Deep Neural Networks", "comments": "5 pages, 3 figures, 1 table. Published at Proocedings of ACM\n  Computing Frontiers Conference 2017", "journal-ref": "ACM CF'17 Proceedings of the Computing Frontiers Conference\n  (2017), 259-262", "doi": "10.1145/3075564.3075590", "report-no": null, "categories": "cs.LG cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the potential of applying deep learning techniques for\nplant classification and its usage for citizen science in large-scale\nbiodiversity monitoring. We show that plant classification using near\nstate-of-the-art convolutional network architectures like ResNet50 achieves\nsignificant improvements in accuracy compared to the most widespread plant\nclassification application in test sets composed of thousands of different\nspecies labels. We find that the predictions can be confidently used as a\nbaseline classification in citizen science communities like iNaturalist (or its\nSpanish fork, Natusfera) which in turn can share their data with biodiversity\nportals like GBIF.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 17:16:20 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Heredia", "Ignacio", ""]]}, {"id": "1706.03842", "submitter": "Subhrajit Bhattacharya", "authors": "Subhrajit Bhattacharya", "title": "Approximate Structure Construction Using Large Statistical Swarms", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe a novel local algorithm for large statistical\nswarms using \"harmonic attractor dynamics\", by means of which a swarm can\nconstruct harmonics of the environment. This in turn allows the swarm to\napproximately reconstruct desired structures in the environment. The robots\nnavigate in a discrete environment, completely free of localization, being able\nto communicate with other robots in its own discrete cell only, and being able\nto sense or take reliable action within a disk of radius $r$ around itself. We\npresent the mathematics that underlie such dynamics and present initial results\ndemonstrating the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 20:32:02 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Bhattacharya", "Subhrajit", ""]]}, {"id": "1706.03860", "submitter": "Mostafa Rahmani", "authors": "Mostafa Rahmani and George Atia", "title": "Subspace Clustering via Optimal Direction Search", "comments": null, "journal-ref": "IEEE Signal Processing Letters ( Volume: 24, Issue: 12, Dec. 2017\n  )", "doi": "10.1109/LSP.2017.2757901", "report-no": null, "categories": "cs.CV cs.IR cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This letter presents a new spectral-clustering-based approach to the subspace\nclustering problem. Underpinning the proposed method is a convex program for\noptimal direction search, which for each data point d finds an optimal\ndirection in the span of the data that has minimum projection on the other data\npoints and non-vanishing projection on d. The obtained directions are\nsubsequently leveraged to identify a neighborhood set for each data point. An\nalternating direction method of multipliers framework is provided to\nefficiently solve for the optimal directions. The proposed method is shown to\nnotably outperform the existing subspace clustering methods, particularly for\nunwieldy scenarios involving high levels of noise and close subspaces, and\nyields the state-of-the-art results for the problem of face clustering using\nsubspace segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 21:52:57 GMT"}, {"version": "v2", "created": "Thu, 13 Jul 2017 22:56:21 GMT"}, {"version": "v3", "created": "Sun, 23 Jul 2017 20:36:57 GMT"}, {"version": "v4", "created": "Sun, 26 Nov 2017 15:43:15 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Rahmani", "Mostafa", ""], ["Atia", "George", ""]]}, {"id": "1706.03901", "submitter": "Fred Lombard Prof", "authors": "F. Lombard and C. Van Zyl", "title": "Signed Sequential Rank CUSUMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CUSUMs based on the signed sequential ranks of observations are developed for\ndetecting location and scale changes in symmetric distributions. The CUSUMs are\ndistribution free and fully self-starting: given a specified in-control median\nand nominal in-control average run length, no parametric specification of the\nunderlying distribution is required in order to find the correct control\nlimits. If the underlying distribution is normal with unknown variance, a CUSUM\nbased on the Van der Waerden signed rank score produces out-of-control average\nrun lengths that are commensurate with those produced by the standard CUSUM for\na normal distribution with known variance. For heavier tailed distributions,\nuse of a CUSUM based on the Wilcoxon signed rank score is indicated. The\nmethodology is illustrated by application to real data from an industrial\nenvironment.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 04:48:46 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Lombard", "F.", ""], ["Van Zyl", "C.", ""]]}, {"id": "1706.03946", "submitter": "Isabelle Augenstein", "authors": "Ed Collins and Isabelle Augenstein and Sebastian Riedel", "title": "A Supervised Approach to Extractive Summarisation of Scientific Papers", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic summarisation is a popular approach to reduce a document to its\nmain arguments. Recent research in the area has focused on neural approaches to\nsummarisation, which can be very data-hungry. However, few large datasets exist\nand none for the traditionally popular domain of scientific publications, which\nopens up challenging research avenues centered on encoding large, complex\ndocuments. In this paper, we introduce a new dataset for summarisation of\ncomputer science publications by exploiting a large resource of author provided\nsummaries and show straightforward ways of extending it further. We develop\nmodels on the dataset making use of both neural sentence encoding and\ntraditionally used summarisation features and show that models which encode\nsentences as well as their local and global context perform best, significantly\noutperforming well-established baseline methods.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 08:15:25 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Collins", "Ed", ""], ["Augenstein", "Isabelle", ""], ["Riedel", "Sebastian", ""]]}, {"id": "1706.04152", "submitter": "Joseph Futoma", "authors": "Joseph Futoma, Sanjay Hariharan, Katherine Heller", "title": "Learning to Detect Sepsis with a Multitask Gaussian Process RNN\n  Classifier", "comments": "Presented at 34th International Conference on Machine Learning (ICML\n  2017), Sydney, Australia", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a scalable end-to-end classifier that uses streaming physiological\nand medication data to accurately predict the onset of sepsis, a\nlife-threatening complication from infections that has high mortality and\nmorbidity. Our proposed framework models the multivariate trajectories of\ncontinuous-valued physiological time series using multitask Gaussian processes,\nseamlessly accounting for the high uncertainty, frequent missingness, and\nirregular sampling rates typically associated with real clinical data. The\nGaussian process is directly connected to a black-box classifier that predicts\nwhether a patient will become septic, chosen in our case to be a recurrent\nneural network to account for the extreme variability in the length of patient\nencounters. We show how to scale the computations associated with the Gaussian\nprocess in a manner so that the entire system can be discriminatively trained\nend-to-end using backpropagation. In a large cohort of heterogeneous inpatient\nencounters at our university health system we find that it outperforms several\nbaselines at predicting sepsis, and yields 19.4% and 55.5% improved areas under\nthe Receiver Operating Characteristic and Precision Recall curves as compared\nto the NEWS score currently used by our hospital.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 16:42:01 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Futoma", "Joseph", ""], ["Hariharan", "Sanjay", ""], ["Heller", "Katherine", ""]]}, {"id": "1706.04182", "submitter": "Philip Ernst", "authors": "Quan Zhou, Philip Ernst, Kari Lock Morgan, Donald Rubin, and Anru\n  Zhang", "title": "Sequential rerandomization", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The seminal work of Morgan and Rubin (2012) considers rerandomization for all\nthe units at one time. In practice, however, experimenters may have to\nrerandomize units sequentially. For example, a clinician studying a rare\ndisease may be unable to wait to perform an experiment until all the\nexperimental units are recruited. Our work offers a mathematical framework for\nsequential rerandomization designs, where the experimental units are enrolled\nin groups. We formulate an adaptive rerandomization procedure for balancing\ntreatment/control assignments over some continuous or binary covariates, using\nMahalanobis distance as the imbalance measure. We prove in our key result,\nTheorem 3, that given the same number of rerandomizations (in expected value),\nunder certain mild assumptions, sequential rerandomization achieves better\ncovariate balance than rerandomization at one time.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 17:55:24 GMT"}, {"version": "v2", "created": "Tue, 20 Jun 2017 17:57:28 GMT"}, {"version": "v3", "created": "Tue, 4 Jul 2017 17:32:22 GMT"}, {"version": "v4", "created": "Sun, 6 Aug 2017 15:45:47 GMT"}, {"version": "v5", "created": "Mon, 16 Apr 2018 01:57:05 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Zhou", "Quan", ""], ["Ernst", "Philip", ""], ["Morgan", "Kari Lock", ""], ["Rubin", "Donald", ""], ["Zhang", "Anru", ""]]}, {"id": "1706.04229", "submitter": "David Hunter", "authors": "David Scott Hunter, Ajay Saini, Tauhid Zaman", "title": "Picking Winners: A Data Driven Approach to Evaluating the Quality of\n  Startup Companies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of evaluating the quality of startup companies. This\ncan be quite challenging due to the rarity of successful startup companies and\nthe complexity of factors which impact such success. In this work we collect\ndata on tens of thousands of startup companies, their performance, the\nbackgrounds of their founders, and their investors. We develop a novel model\nfor the success of a startup company based on the first passage time of a\nBrownian motion. The drift and diffusion of the Brownian motion associated with\na startup company are a function of features based its sector, founders, and\ninitial investors. All features are calculated using our massive dataset. Using\na Bayesian approach, we are able to obtain quantitative insights about the\nfeatures of successful startup companies from our model.\n  To test the performance of our model, we use it to build a portfolio of\ncompanies where the goal is to maximize the probability of having at least one\ncompany achieve an exit (IPO or acquisition), which we refer to as winning.\nThis $\\textit{picking winners}$ framework is very general and can be used to\nmodel many problems with low probability, high reward outcomes, such as\npharmaceutical companies choosing drugs to develop or studios selecting movies\nto produce. We frame the construction of a picking winners portfolio as a\ncombinatorial optimization problem and show that a greedy solution has strong\nperformance guarantees. We apply the picking winners framework to the problem\nof choosing a portfolio of startup companies. Using our model for the exit\nprobabilities, we are able to construct out of sample portfolios which achieve\nexit rates as high as 60%, which is nearly double that of top venture capital\nfirms.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 19:15:02 GMT"}, {"version": "v2", "created": "Sun, 8 Jul 2018 19:50:27 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Hunter", "David Scott", ""], ["Saini", "Ajay", ""], ["Zaman", "Tauhid", ""]]}, {"id": "1706.04336", "submitter": "David Carey", "authors": "David L. Carey, Kok-Leong Ong, Rod Whiteley, Kay M. Crossley, Justin\n  Crow and Meg E. Morris", "title": "Predictive modelling of training loads and injury in Australian football", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": "10.2478/ijcss-2018-0002", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To investigate whether training load monitoring data could be used to predict\ninjuries in elite Australian football players, data were collected from elite\nathletes over 3 seasons at an Australian football club. Loads were quantified\nusing GPS devices, accelerometers and player perceived exertion ratings.\nAbsolute and relative training load metrics were calculated for each player\neach day (rolling average, exponentially weighted moving average, acute:chronic\nworkload ratio, monotony and strain). Injury prediction models (regularised\nlogistic regression, generalised estimating equations, random forests and\nsupport vector machines) were built for non-contact, non-contact time-loss and\nhamstring specific injuries using the first two seasons of data. Injury\npredictions were generated for the third season and evaluated using the area\nunder the receiver operator characteristic (AUC). Predictive performance was\nonly marginally better than chance for models of non-contact and non-contact\ntime-loss injuries (AUC$<$0.65). The best performing model was a multivariate\nlogistic regression for hamstring injuries (best AUC=0.76). Learning curves\nsuggested logistic regression was underfitting the load-injury relationship and\nthat using a more complex model or increasing the amount of model building data\nmay lead to future improvements. Injury prediction models built using training\nload data from a single club showed poor ability to predict injuries when\ntested on previously unseen data, suggesting they are limited as a daily\ndecision tool for practitioners. Focusing the modelling approach on specific\ninjury types and increasing the amount of training data may lead to the\ndevelopment of improved predictive models for injury prevention.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 07:09:33 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Carey", "David L.", ""], ["Ong", "Kok-Leong", ""], ["Whiteley", "Rod", ""], ["Crossley", "Kay M.", ""], ["Crow", "Justin", ""], ["Morris", "Meg E.", ""]]}, {"id": "1706.04394", "submitter": "Emil B. Iversen", "authors": "Emil B. Iversen, Rune Juhl, Jan K. M{\\o}ller, Jan Kleissl, Henrik\n  Madsen, Juan M. Morales", "title": "Spatio-Temporal Forecasting by Coupled Stochastic Differential\n  Equations: Applications to Solar Power", "comments": "24 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatio-temporal problems exist in many areas of knowledge and disciplines\nranging from biology to engineering and physics. However, solution strategies\nbased on classical statistical techniques often fall short due to the large\nnumber of parameters that are to be estimated and the huge amount of data that\nneed to be handled. In this paper we apply known techniques in a novel way to\nprovide a framework for spatio-temporal modeling which is both computationally\nefficient and has a low dimensional parameter space. We present a\nmicro-to-macro approach whereby the local dynamics are first modeled and\nsubsequently combined to capture the global system behavior. The proposed\nmethodology relies on coupled stochastic differential equations and is applied\nto produce spatio-temporal forecasts for a solar power plant for very short\nhorizons, which essentially implies tracking clouds moving across the field of\nsolar power inverters. We outperform simple and complex benchmarks while\nproviding forecasts for 70 spatial dimensions and 24 lead times (i.e., for a\ntotal number of random variables equal to 1680). The resulting model can\nprovide all sorts of forecast products, ranging from point forecasts and\nco-variances to predictive densities, multi-horizon forecasts, and space-time\ntrajectories.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 10:25:05 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Iversen", "Emil B.", ""], ["Juhl", "Rune", ""], ["M\u00f8ller", "Jan K.", ""], ["Kleissl", "Jan", ""], ["Madsen", "Henrik", ""], ["Morales", "Juan M.", ""]]}, {"id": "1706.04633", "submitter": "Dimitri Abramov", "authors": "Dimitri Marques Abramov", "title": "Subjects classification from high-dimensional and small-sample size\n  datasets using a strategy based on Clustering Variables around Latent\n  Components (CLV) method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional complex systems can be studied through multivariate\nanalysis, as Principal Component Analysis, however large samples of\nobservations frequently are needed for it. Here it is examined a method for\nsmall samples based on clustering variables around latent variables (CLV) to\nsubject classification in two presumed groups. For it, a predictive model was\ndeveloped to generate datasets with two groups of cases whose variables show\nrandomness features (up to 30% of variables manifest difference between groups,\nand up to 7% of those are correlated between them). The method recovered the\ninformation of the latent factors to classify the subjects with 80 to 95% of\nagreement, with positive relationship between the classifier precision and the\nrate [number of variables / number of subjects].\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 18:46:28 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["Abramov", "Dimitri Marques", ""]]}, {"id": "1706.04643", "submitter": "James Zidek", "authors": "Chun-Hao Yang, James V. Zidek and Samuel W.K. Wong", "title": "Bayesian analysis of accumulated damage models in lumber reliability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wood products that are subjected to sustained stress over a period of long\nduration may weaken, and this effect must be considered in models for the\nlong-term reliability of lumber. The damage accumulation approach has been\nwidely used for this purpose to set engineering standards. In this article, we\nrevisit an accumulated damage model and propose a Bayesian framework for\nanalysis. For parameter estimation and uncertainty quantification, we adopt\napproximation Bayesian computation (ABC) techniques to handle the complexities\nof the model. We demonstrate the effectiveness of our approach using both\nsimulated and real data, and apply our fitted model to analyze long-term lumber\nreliability under a stochastic live loading scenario.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 19:19:57 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["Yang", "Chun-Hao", ""], ["Zidek", "James V.", ""], ["Wong", "Samuel W. K.", ""]]}, {"id": "1706.04677", "submitter": "Emmanuel Candes", "authors": "Matteo Sesia, Chiara Sabatti, Emmanuel J. Cand\\`es", "title": "Gene Hunting with Knockoffs for Hidden Markov Models", "comments": "35 pages, 13 figues, 9 tables", "journal-ref": "Biometrika, Volume 106, Issue 1, 1 March 2019, Pages 1-18", "doi": "10.1093/biomet/asy033", "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern scientific studies often require the identification of a subset of\nrelevant explanatory variables, in the attempt to understand an interesting\nphenomenon. Several statistical methods have been developed to automate this\ntask, but only recently has the framework of model-free knockoffs proposed a\ngeneral solution that can perform variable selection under rigorous type-I\nerror control, without relying on strong modeling assumptions. In this paper,\nwe extend the methodology of model-free knockoffs to a rich family of problems\nwhere the distribution of the covariates can be described by a hidden Markov\nmodel (HMM). We develop an exact and efficient algorithm to sample knockoff\ncopies of an HMM. We then argue that combined with the knockoffs selective\nframework, they provide a natural and powerful tool for performing principled\ninference in genome-wide association studies with guaranteed FDR control.\nFinally, we apply our methodology to several datasets aimed at studying the\nCrohn's disease and several continuous phenotypes, e.g. levels of cholesterol.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 21:42:12 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Sesia", "Matteo", ""], ["Sabatti", "Chiara", ""], ["Cand\u00e8s", "Emmanuel J.", ""]]}, {"id": "1706.04692", "submitter": "Dean Eckles", "authors": "Dean Eckles, Eytan Bakshy", "title": "Bias and high-dimensional adjustment in observational studies of peer\n  effects", "comments": "25 pages, 3 figures, 2 tables; supplementary information as ancillary\n  file", "journal-ref": "Journal of the American Statistical Association (2020)", "doi": "10.1080/01621459.2020.1796393", "report-no": null, "categories": "stat.ME cs.SI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Peer effects, in which the behavior of an individual is affected by the\nbehavior of their peers, are posited by multiple theories in the social\nsciences. Other processes can also produce behaviors that are correlated in\nnetworks and groups, thereby generating debate about the credibility of\nobservational (i.e. nonexperimental) studies of peer effects. Randomized field\nexperiments that identify peer effects, however, are often expensive or\ninfeasible. Thus, many studies of peer effects use observational data, and\nprior evaluations of causal inference methods for adjusting observational data\nto estimate peer effects have lacked an experimental \"gold standard\" for\ncomparison. Here we show, in the context of information and media diffusion on\nFacebook, that high-dimensional adjustment of a nonexperimental control group\n(677 million observations) using propensity score models produces estimates of\npeer effects statistically indistinguishable from those from using a large\nrandomized experiment (220 million observations). Naive observational\nestimators overstate peer effects by 320% and commonly used variables (e.g.,\ndemographics) offer little bias reduction, but adjusting for a measure of prior\nbehaviors closely related to the focal behavior reduces bias by 91%.\nHigh-dimensional models adjusting for over 3,700 past behaviors provide\nadditional bias reduction, such that the full model reduces bias by over 97%.\nThis experimental evaluation demonstrates that detailed records of individuals'\npast behavior can improve studies of social influence, information diffusion,\nand imitation; these results are encouraging for the credibility of some\nstudies but also cautionary for studies of rare or new behaviors. More\ngenerally, these results show how large, high-dimensional data sets and\nstatistical learning techniques can be used to improve causal inference in the\nbehavioral sciences.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 23:21:37 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Eckles", "Dean", ""], ["Bakshy", "Eytan", ""]]}, {"id": "1706.04943", "submitter": "Javier L\\'opez Pe\\~na", "authors": "Tarak Kharrat, Javier L\\'opez Pe\\~na and Ian McHale", "title": "Plus-Minus Player Ratings for Soccer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a plus-minus rating for use in association football\n(soccer). We first describe the standard plus-minus methodology as used in\nbasketball and ice-hockey and then adapt it for use in soccer. The usual\ngoal-differential plus-minus is considered before two variations are proposed.\nFor the first variation, we present a methodology to calculate an expected\ngoals plus-minus rating. The second variation makes use of in-play\nprobabilities of match outcome to evaluate an expected points plus-minus\nrating. We use the ratings to examine who are the best players in European\nfootball, and demonstrate how the players' ratings evolve over time. Finally,\nwe shed light on the debate regarding which is the strongest league. The model\nsuggests the English Premier League is the strongest, with the German\nBundesliga a close runner-up.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 16:01:58 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["Kharrat", "Tarak", ""], ["Pe\u00f1a", "Javier L\u00f3pez", ""], ["McHale", "Ian", ""]]}, {"id": "1706.04969", "submitter": "Kris Sankaran", "authors": "Kris Sankaran, Susan P. Holmes", "title": "Latent Variable Modeling for the Microbiome", "comments": "31 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human microbiome is a complex ecological system, and describing its\nstructure and function under different environmental conditions is important\nfrom both basic scientific and medical perspectives. Viewed through a\nbiostatistical lens, many microbiome analysis goals can be formulated as latent\nvariable modeling problems. However, although probabilistic latent variable\nmodels are a cornerstone of modern unsupervised learning, they are rarely\napplied in the context of microbiome data analysis, in spite of the\nevolutionary, temporal, and count structure that could be directly incorporated\nthrough such models. We explore the application of probabilistic latent\nvariable models to microbiome data, with a focus on Latent Dirichlet\nAllocation, Nonnegative Matrix Factorization, and Dynamic Unigram models. To\ndevelop guidelines for when different methods are appropriate, we perform a\nsimulation study. We further illustrate and compare these techniques using the\ndata of [10], a study on the effects of antibiotics on bacterial community\ncomposition. Code and data for all simulations and case studies are available\npublicly.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 17:07:00 GMT"}, {"version": "v2", "created": "Wed, 15 Nov 2017 23:47:28 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Sankaran", "Kris", ""], ["Holmes", "Susan P.", ""]]}, {"id": "1706.05029", "submitter": "Eduardo Garc\\'ia-Portugu\\'es", "authors": "M\\'onica Benito, Eduardo Garc\\'ia-Portugu\\'es, J. S. Marron, Daniel\n  Pe\\~na", "title": "Distance weighted discrimination of face images for gender\n  classification", "comments": "9 pages, 4 figures, 1 table", "journal-ref": "Stat, 6:231-240, 2017", "doi": "10.1002/sta4.151", "report-no": null, "categories": "stat.AP cs.CV stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We illustrate the advantages of distance weighted discrimination for\nclassification and feature extraction in a High Dimension Low Sample Size\n(HDLSS) situation. The HDLSS context is a gender classification problem of face\nimages in which the dimension of the data is several orders of magnitude larger\nthan the sample size. We compare distance weighted discrimination with Fisher's\nlinear discriminant, support vector machines, and principal component analysis\nby exploring their classification interpretation through insightful\nvisuanimations and by examining the classifiers' discriminant errors. This\nanalysis enables us to make new contributions to the understanding of the\ndrivers of human discrimination between males and females.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 18:25:50 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 10:14:26 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Benito", "M\u00f3nica", ""], ["Garc\u00eda-Portugu\u00e9s", "Eduardo", ""], ["Marron", "J. S.", ""], ["Pe\u00f1a", "Daniel", ""]]}, {"id": "1706.05280", "submitter": "Gregor Kastner", "authors": "Gregor Kastner, Sylvia Fr\\\"uhwirth-Schnatter", "title": "Ancillarity-Sufficiency Interweaving Strategy (ASIS) for Boosting MCMC\n  Estimation of Stochastic Volatility Models", "comments": null, "journal-ref": "Computational Statistics & Data Analysis 76, 408-423 (2014)", "doi": "10.1016/j.csda.2013.01.002", "report-no": null, "categories": "stat.ME econ.EM stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference for stochastic volatility models using MCMC methods highly\ndepends on actual parameter values in terms of sampling efficiency. While draws\nfrom the posterior utilizing the standard centered parameterization break down\nwhen the volatility of volatility parameter in the latent state equation is\nsmall, non-centered versions of the model show deficiencies for highly\npersistent latent variable series. The novel approach of\nancillarity-sufficiency interweaving has recently been shown to aid in\novercoming these issues for a broad class of multilevel models. In this paper,\nwe demonstrate how such an interweaving strategy can be applied to stochastic\nvolatility models in order to greatly improve sampling efficiency for all\nparameters and throughout the entire parameter range. Moreover, this method of\n\"combining best of different worlds\" allows for inference for parameter\nconstellations that have previously been infeasible to estimate without the\nneed to select a particular parameterization beforehand.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 14:11:43 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Kastner", "Gregor", ""], ["Fr\u00fchwirth-Schnatter", "Sylvia", ""]]}, {"id": "1706.05402", "submitter": "Ikuesan R. Adeyemi Mr.", "authors": "Ikuesan Richard Adeyemi, Shukor Abd Razak, Mazleena Salleh", "title": "A Conceptual Model for Holistic Classification of Insider", "comments": null, "journal-ref": "Asian J. Applied Sci., 7: 343-359 (2014)", "doi": "10.3923/ajaps.2014.343.359", "report-no": null, "categories": "cs.CY stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The process through which an insider to an organization can be described or\nclassified is lined within the orthodox paradigm of classification in which an\norganization considers only subject with requisite employee criterion as an\ninsider to that organization. This is further clouded with the relative\nrigidity in operational security policies being implemented in organizations.\nEstablishing investigation process in instances of misuse occurrence and or\nascertaining the efficiency of staff member using such archaic paradigm is\nmaligned with endless possibilities of uncertainties. This study, therefore,\nproposes a holistic model for which insider classification can be crystallized\nusing the combination of qualitative research process and analysis of moment\nstructure evaluation process. A full comprehension of this proposition could\nserve as a hinge through which insider misuse investigation can be thoroughly\ncarried out. In addition, integrating this paradigm into existing operational\nsecurity policies could serve as a metric upon which an organization can\nunderstand insider dynamics, in order to prevent misuses, and enhance staff\nmanagement.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 18:38:37 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Adeyemi", "Ikuesan Richard", ""], ["Razak", "Shukor Abd", ""], ["Salleh", "Mazleena", ""]]}, {"id": "1706.05416", "submitter": "Jacek Urbanek PhD", "authors": "Jacek K. Urbanek, Adam Spira, Junrui Di, Andrew Leroux, Ciprian\n  Crainiceanu, Vadim Zipunnikov", "title": "Epidemiology of Objectively Measured Bedtime and Chronotype in the US\n  adolescents and adults: NHANES 2003-2006", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: We propose a method for estimating the timing of in-bed intervals\nusing objective data in a large representative U.S. sample, and quantify the\nassociation between these intervals and age, sex, and day of the week. Methods:\nThe study included 11,951 participants six years and older from the National\nHealth and Nutrition Examination Survey (NHANES) 2003-2006, who wore\naccelerometers to measure physical activity for seven consecutive days.\nParticipants were instructed to remove the device just before the nighttime\nsleep period and put it back on immediately after. This nighttime period of\nnon-wear was defined in this paper as the objective bedtime (OBT), an\nobjectively estimated record of the in-bed-interval. For each night of the\nweek, we estimated two measures: the duration of the OBT (OBT-D) and, as a\nmeasure of the chronotype, the midpoint of the OBT (OBT-M). We estimated\nday-of-the-week-specific OBT-D and OBT-M using gender-specific population\npercentile curves. Differences in OBT-M (chronotype) and OBT-D (the amount of\ntime spent in bed) by age and sex were estimated using regression models.\nResults: The estimates of OBT-M and their differences among age groups were\nconsistent with the estimates of chronotype obtained via self-report in\nEuropean populations. The average OBT-M varied significantly by age, while\nOBT-D was less variable with age. The most pronounced differences were observed\nbetween OBT-M of weekday and weekend nights. Conclusions: The proposed\nmeasures, OBT-D and OBT-M, provide useful information of time in bed and\nchronotype in NHANES 2003-2006. They identify within-week patterns of bedtime\nand can be used to study associations between the bedtime and the large number\nof health outcomes collected in NHANES 2003-2006.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 19:45:59 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Urbanek", "Jacek K.", ""], ["Spira", "Adam", ""], ["Di", "Junrui", ""], ["Leroux", "Andrew", ""], ["Crainiceanu", "Ciprian", ""], ["Zipunnikov", "Vadim", ""]]}, {"id": "1706.05445", "submitter": "Raksha Ramakrishna", "authors": "Raksha Ramakrishna, Anna Scaglione and Vijay Vittal", "title": "A Stochastic Model for Short-Term Probabilistic Forecast of Solar\n  Photo-Voltaic Power", "comments": "Manuscript submitted to IEEE transactions on Sustainable Energy .\n  Extension of conference paper : R. Ramakrishna and A. Scaglione, A\n  Compressive Sensing Framework for the analysis of Solar Photo-Voltaic Power,\n  in Conference Record of the Fiftieth Asilomar Conference on Signals, Systems\n  and Computers, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a stochastic model with regime switching is developed for\nsolar photo-voltaic (PV) power in order to provide short-term probabilistic\nforecasts. The proposed model for solar PV power is physics inspired and\nexplicitly incorporates the stochasticity due to clouds using different\nparameters addressing the attenuation in power.Based on the statistical\nbehavior of parameters, a simple regime-switching process between the three\nclasses of sunny, overcast and partly cloudy is proposed. Then, probabilistic\nforecasts of solar PV power are obtained by identifying the present regime\nusing PV power measurements and assuming persistence in this regime. To\nillustrate the technique developed, a set of solar PV power data from a single\nrooftop installation in California is analyzed and the effectiveness of the\nmodel in fitting the data and in providing short-term point and probabilistic\nforecasts is verified. The proposed forecast method outperforms a variety of\nreference models that produce point and probabilistic forecasts and therefore\nportrays the merits of employing the proposed approach.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 22:30:08 GMT"}, {"version": "v2", "created": "Sat, 16 Sep 2017 19:50:07 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Ramakrishna", "Raksha", ""], ["Scaglione", "Anna", ""], ["Vittal", "Vijay", ""]]}, {"id": "1706.05446", "submitter": "Yaodong Yang Mr.", "authors": "Yaodong Yang, Rui Luo, Yuanyuan Liu", "title": "Adversarial Variational Bayes Methods for Tweedie Compound Poisson Mixed\n  Models", "comments": "ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Tweedie Compound Poisson-Gamma model is routinely used for modeling\nnon-negative continuous data with a discrete probability mass at zero. Mixed\nmodels with random effects account for the covariance structure related to the\ngrouping hierarchy in the data. An important application of Tweedie mixed\nmodels is pricing the insurance policies, e.g. car insurance. However, the\nintractable likelihood function, the unknown variance function, and the\nhierarchical structure of mixed effects have presented considerable challenges\nfor drawing inferences on Tweedie. In this study, we tackle the Bayesian\nTweedie mixed-effects models via variational inference approaches. In\nparticular, we empower the posterior approximation by implicit models trained\nin an adversarial setting. To reduce the variance of gradients, we\nreparameterize random effects, and integrate out one local latent variable of\nTweedie. We also employ a flexible hyper prior to ensure the richness of the\napproximation. Our method is evaluated on both simulated and real-world data.\nResults show that the proposed method has smaller estimation bias on the random\neffects compared to traditional inference methods including MCMC; it also\nachieves a state-of-the-art predictive performance, meanwhile offering a richer\nestimation of the variance function.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 22:33:13 GMT"}, {"version": "v2", "created": "Fri, 30 Jun 2017 14:27:25 GMT"}, {"version": "v3", "created": "Sun, 16 Jul 2017 23:06:29 GMT"}, {"version": "v4", "created": "Wed, 14 Feb 2018 16:30:46 GMT"}, {"version": "v5", "created": "Sun, 3 Feb 2019 11:33:39 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Yang", "Yaodong", ""], ["Luo", "Rui", ""], ["Liu", "Yuanyuan", ""]]}, {"id": "1706.05552", "submitter": "Daniel Egea-Roca Mr.", "authors": "Daniel Egea-Roca, Gonzalo Seco-Granados, Jos\\'e A. L\\'opez-Salcedo, H.\n  Vincent Poor", "title": "Performance Bounds for Finite Moving Average Change Detection:\n  Application to Global Navigation Satellite Systems", "comments": "12 pages, 2 figures, transaction paper, IEEE Transaction on Signal\n  Processing, 2017", "journal-ref": null, "doi": "10.1109/TSP.2017.2788416", "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the widespread deployment of Global Navigation Satellite Systems\n(GNSSs) for critical road or urban applications, one of the major challenges to\nbe solved is the provision of integrity to terrestrial environments, so that\nGNSS may be safety used in these applications. To do so, the integrity of the\nreceived GNSS signal must be analyzed in order to detect some local effect\ndisturbing the received signal. This is desirable because the presence of some\nlocal effect may cause large position errors, and hence compromise the signal\nintegrity. Moreover, the detection of such disturbing effects must be done\nbefore some pre-established delay. This kind of detection lies within the field\nof transient change detection. In this work, a finite moving average stopping\ntime is proposed in order to approach the signal integrity problem with a\ntransient change detection framework. The statistical performance of this\nstopping time is investigated and compared, in the context of multipath\ndetection, to other different methods available in the literature. Numerical\nresults are presented in order to assess their performance.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jun 2017 15:33:39 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Egea-Roca", "Daniel", ""], ["Seco-Granados", "Gonzalo", ""], ["L\u00f3pez-Salcedo", "Jos\u00e9 A.", ""], ["Poor", "H. Vincent", ""]]}, {"id": "1706.05568", "submitter": "Mareike Fischer", "authors": "Michelle Galla and Kristina Wicke and Mareike Fischer", "title": "On the statistical inconsistency of Maximum Parsimony for $k$-tuple-site\n  data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE math.CO math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main aims of phylogenetics is to reconstruct the \\enquote{Tree of\nLife}. In this respect, different methods and criteria are used to analyze DNA\nsequences of different species and to compare them in order to derive the\nevolutionary relationships of these species. Maximum Parsimony is one such\ncriterion for tree reconstruction and, it is the one which we will use in this\npaper. However, it is well-known that tree reconstruction methods can lead to\nwrong relationship estimates. One typical problem of Maximum Parsimony is long\nbranch attraction, which can lead to statistical inconsistency. In this work,\nwe will consider a blockwise approach to alignment analysis, namely so-called\n$k$-tuple analyses. For four taxa it has already been shown that\n$k$-tuple-based analyses are statistically inconsistent if and only if the\nstandard character-based (site-based) analyses are statistically inconsistent.\nSo, in the four-taxon case, going from individual sites to $k$-tuples does not\nlead to any improvement. However, real biological analyses often consider more\nthan only four taxa. Therefore, we analyze the case of five taxa for $2$- and\n$3$-tuple-site data and consider alphabets with two and four elements. We show\nthat the equivalence of single-site data and $k$-tuple-site data then no longer\nholds. Even so, we can show that Maximum Parsimony is statistically\ninconsistent for $k$-tuple site data and five taxa.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jun 2017 18:03:27 GMT"}, {"version": "v2", "created": "Thu, 14 Dec 2017 21:32:47 GMT"}, {"version": "v3", "created": "Thu, 4 Oct 2018 07:58:45 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Galla", "Michelle", ""], ["Wicke", "Kristina", ""], ["Fischer", "Mareike", ""]]}, {"id": "1706.05678", "submitter": "Emma Pierson", "authors": "Emma Pierson, Camelia Simoiu, Jan Overgoor, Sam Corbett-Davies,\n  Vignesh Ramachandran, Cheryl Phillips, and Sharad Goel", "title": "A large-scale analysis of racial disparities in police stops across the\n  United States", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To assess racial disparities in police interactions with the public, we\ncompiled and analyzed a dataset detailing over 60 million state patrol stops\nconducted in 20 U.S. states between 2011 and 2015. We find that black drivers\nare stopped more often than white drivers relative to their share of the\ndriving-age population, but that Hispanic drivers are stopped less often than\nwhites. Among stopped drivers -- and after controlling for age, gender, time,\nand location -- blacks and Hispanics are more likely to be ticketed, searched,\nand arrested than white drivers. These disparities may reflect differences in\ndriving behavior, and are not necessarily the result of bias. In the case of\nsearch decisions, we explicitly test for discrimination by examining both the\nrate at which drivers are searched and the likelihood searches turn up\ncontraband. We find evidence that the bar for searching black and Hispanic\ndrivers is lower than for searching whites. Finally, we find that legalizing\nrecreational marijuana in Washington and Colorado reduced the total number of\nsearches and misdemeanors for all race groups, though a race gap still\npersists. We conclude by offering recommendations for improving data\ncollection, analysis, and reporting by law enforcement agencies.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jun 2017 16:04:25 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Pierson", "Emma", ""], ["Simoiu", "Camelia", ""], ["Overgoor", "Jan", ""], ["Corbett-Davies", "Sam", ""], ["Ramachandran", "Vignesh", ""], ["Phillips", "Cheryl", ""], ["Goel", "Sharad", ""]]}, {"id": "1706.05717", "submitter": "Vahid Tadayon", "authors": "Vahid Tadayon", "title": "Bayesian Analysis of Censored Spatial Data Based on a Non-Gaussian Model", "comments": null, "journal-ref": "Journal of Statistical Research of Iran. 2017, 13 (2) :155-180", "doi": "10.18869/acadpub.jsri.13.2.155", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we suggest using a skew Gaussian-log Gaussian model for the\nanalysis of spatial censored data from a Bayesian point of view. This approach\nfurnishes an extension of the skew log Gaussian model to accommodate to both\nskewness and heavy tails and also censored data. All of the characteristics\nmentioned are three pervasive features of spatial data. We utilize data\naugmentation method and Markov chain Monte Carlo (MCMC) algorithms to do\nposterior calculations. The methodology is illustrated using simulated data, as\nwell as applying it to a real data set. Keywords: Censored data, data\naugmentation, non-Gaussian spatial models, outlier, unified skew Gaussian.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jun 2017 20:17:43 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 08:10:48 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Tadayon", "Vahid", ""]]}, {"id": "1706.06089", "submitter": "Alberto Sorrentino", "authors": "Filippo Rossi, Gianvittorio Luria, Sara Sommariva and Alberto\n  Sorrentino", "title": "Bayesian multi--dipole localization and uncertainty quantification from\n  simultaneous EEG and MEG recordings", "comments": "4 pages, 3 figures -- conference paper from EMBEC 2017, Tampere,\n  Finland", "journal-ref": null, "doi": "10.1007/978-981-10-5122-7_211", "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We deal with estimation of multiple dipoles from combined MEG and EEG\ntime--series. We use a sequential Monte Carlo algorithm to characterize the\nposterior distribution of the number of dipoles and their locations. By\nconsidering three test cases, we show that using the combined data the method\ncan localize sources that are not easily (or not at all) visible with either of\nthe two individual data alone. In addition, the posterior distribution from\ncombined data exhibits a lower variance, i.e. lower uncertainty, than the\nposterior from single device.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 17:59:44 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Rossi", "Filippo", ""], ["Luria", "Gianvittorio", ""], ["Sommariva", "Sara", ""], ["Sorrentino", "Alberto", ""]]}, {"id": "1706.06354", "submitter": "Javier \\'Alvarez-Li\\'ebana", "authors": "J. \\'Alvarez-Li\\'ebana, D. Bosq and M. D. Ruiz-Medina", "title": "Consistency of the plug-in functional predictor of the\n  Ornstein-Uhlenbeck process in Hilbert and Banach spaces", "comments": "30 pages with 10 figures. Supplementary material (8 pages) is also\n  included", "journal-ref": "Statistics & Probability Letters, 117, pp. 12-22 (2016)", "doi": "10.1016/j.spl.2016.04.023", "report-no": null, "categories": "math.ST math.FA stat.AP stat.OT stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  New results on functional prediction of the Ornstein-Uhlenbeck process in an\nautoregressive Hilbert-valued and Banach-valued frameworks are derived.\nSpecifically, consistency of the maximum likelihood estimator of the\nautocorrelation operator, and of the associated plug-in predictor is obtained\nin both frameworks.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 10:18:14 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2018 09:54:09 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["\u00c1lvarez-Li\u00e9bana", "J.", ""], ["Bosq", "D.", ""], ["Ruiz-Medina", "M. D.", ""]]}, {"id": "1706.06380", "submitter": "Yunfan Tang", "authors": "Yunfan Tang and Dan L. Nicolae", "title": "Mixed Effect Dirichlet-Tree Multinomial for Longitudinal Microbiome Data\n  and Weight Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantifying the relation between gut microbiome and body weight can provide\ninsights into personalized strategies for improving digestive health. In this\npaper, we present an algorithm that predicts weight fluctuations using gut\nmicrobiome in a healthy cohort of newborns from a previously published dataset.\nMicrobial data has been known to present unique statistical challenges that\ndefy most conventional models. We propose a mixed effect Dirichlet-tree\nmultinomial (DTM) model to untangle these difficulties as well as incorporate\ncovariate information and account for species relatedness. The DTM setup allows\none to easily invoke empirical Bayes shrinkage on each node for enhanced\ninference of microbial proportions. Using these estimates, we subsequently\napply random forest for weight prediction and obtain a microbiome-inferred\nweight metric. Our result demonstrates that microbiome-inferred weight is\nsignificantly associated with weight changes in the future and its non-trivial\neffect size makes it a viable candidate to forecast weight progression.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 12:03:12 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Tang", "Yunfan", ""], ["Nicolae", "Dan L.", ""]]}, {"id": "1706.06498", "submitter": "Javier \\'Alvarez-Li\\'ebana", "authors": "J. \\'Alvarez-Li\\'ebana, D. Bosq and M. Dolores Ruiz-Medina", "title": "Asymptotic properties of a componentwise ARH(1) plug-in predictor", "comments": "50 pages (with 4 figures)", "journal-ref": "Journal of Multivariate Analysis, 155, pp. 12-34 (2017)", "doi": "10.1016/j.jmva.2016.11.009", "report-no": null, "categories": "math.ST math.FA stat.AP stat.OT stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents new results on prediction of linear processes in function\nspaces. The autoregressive Hilbertian process framework of order one (ARH(1)\nprocess framework) is adopted. A componentwise estimator of the autocorrelation\noperator is formulated, from the moment-based estimation of its diagonal\ncoefficients, with respect to the orthogonal eigenvectors of the\nauto-covariance operator, which are assumed to be known. Mean-square\nconvergence to the theoretical autocorrelation operator, in the space of\nHilbert-Schmidt operators, is proved. Consistency then follows in that space.\nFor the associated ARH(1) plug-in predictor, mean absolute convergence to the\ncorresponding conditional expectation, in the considered Hilbert space, is\nobtained. Hence, consistency in that space also holds. A simulation study is\nundertaken to illustrate the finite-large sample behavior of the formulated\ncomponentwise estimator and predictor. The performance of the presented\napproach is compared with alternative approaches in the previous and current\nARH(1) framework literature, including the case of unknown eigenvectors.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 14:50:55 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2018 10:24:01 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["\u00c1lvarez-Li\u00e9bana", "J.", ""], ["Bosq", "D.", ""], ["Ruiz-Medina", "M. Dolores", ""]]}, {"id": "1706.06611", "submitter": "Nicholas Henderson", "authors": "Nicholas C. Henderson, Thomas A.Louis, Gary L. Rosner and Ravi\n  Varadhan", "title": "Individualized Treatment Effects with Censored Data via Fully\n  Nonparametric Bayesian Accelerated Failure Time Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Individuals often respond differently to identical treatments, and\ncharacterizing such variability in treatment response is an important aim in\nthe practice of personalized medicine. In this article, we describe a\nnon-parametric accelerated failure time model that can be used to analyze\nheterogeneous treatment effects (HTE) when patient outcomes are time-to-event.\nBy utilizing Bayesian additive regression trees and a mean-constrained\nDirichlet process mixture model, our approach offers a flexible model for the\nregression function while placing few restrictions on the baseline hazard. Our\nnon-parametric method leads to natural estimates of individual treatment effect\nand has the flexibility to address many major goals of HTE assessment.\nMoreover, our method requires little user input in terms of tuning parameter\nselection or subgroup specification. We illustrate the merits of our proposed\napproach with a detailed analysis of two large clinical trials for the\nprevention and treatment of congestive heart failure using an\nangiotensin-converting enzyme inhibitor. The analysis revealed considerable\nevidence for the presence of HTE in both trials as demonstrated by substantial\nestimated variation in treatment effect and by high proportions of patients\nexhibiting strong evidence of having treatment effects which differ from the\noverall treatment effect.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 18:07:48 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Henderson", "Nicholas C.", ""], ["Louis", "Thomas A.", ""], ["Rosner", "Gary L.", ""], ["Varadhan", "Ravi", ""]]}, {"id": "1706.06657", "submitter": "Sophia Sulis", "authors": "Sophia Sulis, David Mary, Lionel Bigot", "title": "A Bootstrap Method for Sinusoid Detection in Colored Noise and Uneven\n  Sampling. Application to Exoplanet Detection", "comments": "Accepted in the European Signal Processing Conference (EUSIPCO 2017).\n  5 pages, 3 figures", "journal-ref": "Signal Processing Conference (EUSIPCO), 2017 25th European", "doi": "10.23919/EUSIPCO.2017.8081377", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study is motivated by the problem of evaluating reliable false alarm\n(FA) rates for sinusoid detection tests applied to unevenly sampled time series\ninvolving colored noise, when a (small) training data set of this noise is\navailable. While analytical expressions for the FA rate are out of reach in\nthis situation, we show that it is possible to combine specific periodogram\nstandardization and bootstrap techniques to consistently estimate the FA rate.\nWe also show that the procedure can be improved by using generalized\nextremevalue distributions. The paper presents several numerical results\nincluding a case study in exoplanet detection from radial velocity data.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 20:37:56 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Sulis", "Sophia", ""], ["Mary", "David", ""], ["Bigot", "Lionel", ""]]}, {"id": "1706.06908", "submitter": "Vaclav Smidl", "authors": "Lukas Ulrych, Vaclav Smidl", "title": "Sparse and Smooth Prior for Bayesian Linear Regression with Application\n  to ETEX Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparsity of the solution of a linear regression model is a common\nrequirement, and many prior distributions have been designed for this purpose.\nA combination of the sparsity requirement with smoothness of the solution is\nalso common in application, however, with considerably fewer existing prior\nmodels. In this paper, we compare two prior structures, the Bayesian fused\nlasso (BFL) and least-squares with adaptive prior covariance matrix (LS-APC).\nSince only variational solution was published for the latter, we derive a Gibbs\nsampling algorithm for its inference and Bayesian model selection. The method\nis designed for high dimensional problems, therefore, we discuss numerical\nissues associated with evaluation of the posterior. In simulation, we show that\nthe LS-APC prior achieves results comparable to that of the Bayesian Fused\nLasso for piecewise constant parameter and outperforms the BFL for parameters\nof more general shapes. Another advantage of the LS-APC priors is revealed in\nreal application to estimation of the release profile of the European Tracer\nExperiment (ETEX). Specifically, the LS-APC model provides more conservative\nuncertainty bounds when the regressor matrix is not informative.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 13:57:20 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Ulrych", "Lukas", ""], ["Smidl", "Vaclav", ""]]}, {"id": "1706.06910", "submitter": "Bangalore Ravi Kiran", "authors": "B Ravi Kiran", "title": "Multi-scale streaming anomalies detection for time series", "comments": "10 pages, two columns, Accepted at Conference d'Apprentissage 2017\n  Grenoble", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the class of streaming anomaly detection algorithms for univariate time\nseries, the size of the sliding window over which various statistics are\ncalculated is an important parameter. To address the anomalous variation in the\nscale of the pseudo-periodicity of time series, we define a streaming\nmulti-scale anomaly score with a streaming PCA over a multi-scale lag-matrix.\nWe define three methods of aggregation of the multi-scale anomaly scores. We\nevaluate their performance on Yahoo! and Numenta dataset for unsupervised\nanomaly detection benchmark. To the best of authors' knowledge, this is the\nfirst time a multi-scale streaming anomaly detection has been proposed and\nsystematically studied.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 13:59:41 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Kiran", "B Ravi", ""]]}, {"id": "1706.06976", "submitter": "Javier \\'Alvarez-Li\\'ebana", "authors": "J. \\'Alvarez-Li\\'ebana, M. D. Ruiz-Medina", "title": "The effect of the spatial domain in FANOVA models with ARH(1) error term", "comments": "56 pages (with 11 figures). Supplementary material is also included", "journal-ref": "Statistics and Its Interface, 10, pp. 607-628 (2017)", "doi": "10.4310/SII.2017.v10.n4.a7", "report-no": null, "categories": "stat.AP math.FA math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Functional Analysis of Variance (FANOVA) from Hilbert-valued correlated data\nwith spatial rectangular or circular supports is analyzed, when Dirichlet\nconditions are assumed on the boundary. Specifically, a Hilbert-valued fixed\neffect model with error term defined from an Autoregressive Hilbertian process\nof order one (ARH(1) process) is considered, extending the formulation given in\nRuiz-Medina (2016). A new statistical test is also derived to contrast the\nsignificance of the functional fixed effect parameters. The Dirichlet\nconditions established at the boundary affect the dependence range of the\ncorrelated error term. While the rate of convergence to zero of the eigenvalues\nof the covariance kernels, characterizing the Gaussian functional error\ncomponents, directly affects the stability of the generalized least-squares\nparameter estimation problem. A simulation study and a real-data application\nrelated to fMRI analysis are undertaken to illustrate the performance of the\nparameter estimator and statistical test derived.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 16:01:54 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2018 10:44:38 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["\u00c1lvarez-Li\u00e9bana", "J.", ""], ["Ruiz-Medina", "M. D.", ""]]}, {"id": "1706.06995", "submitter": "Shannon McCurdy", "authors": "Shannon R. McCurdy, Annette Molinaro, and Lior Pachter", "title": "A latent variable model for survival time prediction with censoring and\n  diverse covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fulfilling the promise of precision medicine requires accurately and\nprecisely classifying disease states. For cancer, this includes prediction of\nsurvival time from a surfeit of covariates. Such data presents an opportunity\nfor improved prediction, but also a challenge due to high dimensionality.\nFurthermore, disease populations can be heterogeneous. Integrative modeling is\nsensible, as the underlying hypothesis is that joint analysis of multiple\ncovariates provides greater explanatory power than separate analyses. We\npropose an integrative latent variable model that combines factor analysis for\nvarious data types and an exponential Cox proportional hazards model for\ncontinuous survival time with informative censoring. The factor and Cox models\nare connected through low-dimensional latent variables that can be interpreted\nand visualized to identify subpopulations. We use this model to predict\nsurvival time. We demonstrate this model's utility in simulation and on four\nCancer Genome Atlas datasets: diffuse lower-grade glioma, glioblastoma\nmultiforme, lung adenocarcinoma, and lung squamous cell carcinoma. These\ndatasets have small sample sizes, high-dimensional diverse covariates, and high\ncensorship rates. We compare the predictions from our model to two alternative\nmodels. Our model outperforms in simulation and is competitive on real\ndatasets. Furthermore, the low-dimensional visualization for diffuse\nlower-grade glioma displays known subpopulations.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 16:37:53 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["McCurdy", "Shannon R.", ""], ["Molinaro", "Annette", ""], ["Pachter", "Lior", ""]]}, {"id": "1706.06996", "submitter": "Nicolas Pr\\\"ollochs", "authors": "Nicolas Pr\\\"ollochs, Stefan Feuerriegel, Dirk Neumann", "title": "Statistical Inferences for Polarity Identification in Natural Language", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0209323", "report-no": null, "categories": "cs.CL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information forms the basis for all human behavior, including the ubiquitous\ndecision-making that people constantly perform in their every day lives. It is\nthus the mission of researchers to understand how humans process information to\nreach decisions. In order to facilitate this task, this work proposes a novel\nmethod of studying the reception of granular expressions in natural language.\nThe approach utilizes LASSO regularization as a statistical tool to extract\ndecisive words from textual content and draw statistical inferences based on\nthe correspondence between the occurrences of words and an exogenous response\nvariable. Accordingly, the method immediately suggests significant implications\nfor social sciences and Information Systems research: everyone can now identify\ntext segments and word choices that are statistically relevant to authors or\nreaders and, based on this knowledge, test hypotheses from behavioral research.\nWe demonstrate the contribution of our method by examining how authors\ncommunicate subjective information through narrative materials. This allows us\nto answer the question of which words to choose when communicating negative\ninformation. On the other hand, we show that investors trade not only upon\nfacts in financial disclosures but are distracted by filler words and\nnon-informative language. Practitioners - for example those in the fields of\ninvestor communications or marketing - can exploit our insights to enhance\ntheir writings based on the true perception of word choice.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 16:37:54 GMT"}, {"version": "v2", "created": "Thu, 5 Apr 2018 23:45:33 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Pr\u00f6llochs", "Nicolas", ""], ["Feuerriegel", "Stefan", ""], ["Neumann", "Dirk", ""]]}, {"id": "1706.07094", "submitter": "Ben Letham", "authors": "Benjamin Letham, Brian Karrer, Guilherme Ottoni, Eytan Bakshy", "title": "Constrained Bayesian Optimization with Noisy Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized experiments are the gold standard for evaluating the effects of\nchanges to real-world systems. Data in these tests may be difficult to collect\nand outcomes may have high variance, resulting in potentially large measurement\nerror. Bayesian optimization is a promising technique for efficiently\noptimizing multiple continuous parameters, but existing approaches degrade in\nperformance when the noise level is high, limiting its applicability to many\nrandomized experiments. We derive an expression for expected improvement under\ngreedy batch optimization with noisy observations and noisy constraints, and\ndevelop a quasi-Monte Carlo approximation that allows it to be efficiently\noptimized. Simulations with synthetic functions show that optimization\nperformance on noisy, constrained problems outperforms existing methods. We\nfurther demonstrate the effectiveness of the method with two real-world\nexperiments conducted at Facebook: optimizing a ranking system, and optimizing\nserver compiler flags.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 19:29:14 GMT"}, {"version": "v2", "created": "Tue, 26 Jun 2018 17:06:00 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Letham", "Benjamin", ""], ["Karrer", "Brian", ""], ["Ottoni", "Guilherme", ""], ["Bakshy", "Eytan", ""]]}, {"id": "1706.07136", "submitter": "Daniele Marinazzo", "authors": "Luca Faes, Daniele Marinazzo, Sebastiano Stramaglia", "title": "Multiscale Information Decomposition: Exact Computation for Multivariate\n  Gaussian Processes", "comments": null, "journal-ref": "Entropy, 19, 408 (2017)", "doi": "10.3390/e19080408", "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploiting the theory of state space models, we derive the exact expressions\nof the information transfer, as well as redundant and synergistic transfer, for\ncoupled Gaussian processes observed at multiple temporal scales. All of the\nterms, constituting the frameworks known as interaction information\ndecomposition and partial information decomposition, can thus be analytically\nobtained for different time scales from the parameters of the VAR model that\nfits the processes. We report the application of the proposed methodology\nfirstly to benchmark Gaussian systems, showing that this class of systems may\ngenerate patterns of information decomposition characterized by mainly\nredundant or synergistic information transfer persisting across multiple time\nscales or even by the alternating prevalence of redundant and synergistic\nsource interaction depending on the time scale. Then, we apply our method to an\nimportant topic in neuroscience, i.e., the detection of causal interactions in\nhuman epilepsy networks, for which we show the relevance of partial information\ndecomposition to the detection of multiscale information transfer spreading\nfrom the seizure onset zone.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 23:02:11 GMT"}, {"version": "v2", "created": "Fri, 18 Aug 2017 08:02:15 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Faes", "Luca", ""], ["Marinazzo", "Daniele", ""], ["Stramaglia", "Sebastiano", ""]]}, {"id": "1706.07355", "submitter": "Carlo Biffi", "authors": "Carlo Biffi, Antonio de Marvao, Mark I. Attard, Timothy J.W. Dawes,\n  Nicola Whiffin, Wenjia Bai, Wenzhe Shi, Catherine Francis, Hannah Meyer,\n  Rachel Buchan, Stuart A. Cook, Daniel Rueckert, Declan P. O'Regan", "title": "Three-dimensional Cardiovascular Imaging-Genetics: A Mass Univariate\n  Framework", "comments": "14 pages, 11 figures. Version accepted by Bioinformatics (Sept 2017).\n  Includes Supplementary Materials", "journal-ref": null, "doi": "10.1093/bioinformatics/btx552", "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MOTIVATION: Left ventricular (LV) hypertrophy is a strong predictor of\ncardiovascular outcomes, but its genetic regulation remains largely\nunexplained. Conventional phenotyping relies on manual calculation of LV mass\nand wall thickness, but advanced cardiac image analysis presents an opportunity\nfor high-throughput mapping of genotype-phenotype associations in three\ndimensions (3D). RESULTS: High-resolution cardiac magnetic resonance images\nwere automatically segmented in 1,124 healthy volunteers to create a 3D shape\nmodel of the heart. Mass univariate regression was used to plot a 3D\neffect-size map for the association between wall thickness and a set of\npredictors at each vertex in the mesh. The vertices where a significant effect\nexists were determined by applying threshold-free cluster enhancement to boost\nareas of signal with spatial contiguity. Experiments on simulated phenotypic\nsignals and SNP replication show that this approach offers a substantial gain\nin statistical power for cardiac genotype-phenotype associations while\nproviding good control of the false discovery rate. This framework models the\neffects of genetic variation throughout the heart and can be automatically\napplied to large population cohorts. AVAILABILITY: The proposed approach has\nbeen coded in an R package freely available at\nhttps://doi.org/10.5281/zenodo.834610 together with the clinical data used in\nthis work.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 15:06:13 GMT"}, {"version": "v2", "created": "Thu, 7 Sep 2017 22:38:22 GMT"}, {"version": "v3", "created": "Wed, 13 Sep 2017 21:45:23 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Biffi", "Carlo", ""], ["de Marvao", "Antonio", ""], ["Attard", "Mark I.", ""], ["Dawes", "Timothy J. W.", ""], ["Whiffin", "Nicola", ""], ["Bai", "Wenjia", ""], ["Shi", "Wenzhe", ""], ["Francis", "Catherine", ""], ["Meyer", "Hannah", ""], ["Buchan", "Rachel", ""], ["Cook", "Stuart A.", ""], ["Rueckert", "Daniel", ""], ["O'Regan", "Declan P.", ""]]}, {"id": "1706.07466", "submitter": "Maha Bakoben", "authors": "Maha Bakoben, Tony Bellotti and Niall Adams", "title": "Identification of Credit Risk Based on Cluster Analysis of Account\n  Behaviours", "comments": "9 pages", "journal-ref": null, "doi": "10.1080/01605682.2019.1582586", "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessment of risk levels for existing credit accounts is important to the\nimplementation of bank policies and offering financial products. This paper\nuses cluster analysis of behaviour of credit card accounts to help assess\ncredit risk level. Account behaviour is modelled parametrically and we then\nimplement the behavioural cluster analysis using a recently proposed\ndissimilarity measure of statistical model parameters. The advantage of this\nnew measure is the explicit exploitation of uncertainty associated with\nparameters estimated from statistical models. Interesting clusters of real\ncredit card behaviours data are obtained, in addition to superior prediction\nand forecasting of account default based on the clustering outcomes.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 09:45:41 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Bakoben", "Maha", ""], ["Bellotti", "Tony", ""], ["Adams", "Niall", ""]]}, {"id": "1706.07467", "submitter": "Juste Raimbault", "authors": "Antonin Bergeaud and Juste Raimbault", "title": "An empirical analysis of the spatial variability of fuel prices in the\n  United States", "comments": "17 pages; 7 figures; 5 tables", "journal-ref": "Transportation Research Part A: Policy and Practice, 132, 131-143\n  (2020)", "doi": "10.1016/j.tra.2019.10.016", "report-no": null, "categories": "stat.AP physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we use a newly constructed dataset to study the geographic\ndistribution of fuel price across the US at a very high resolution. We study\nthe influence of socio-economic variables through different and complementary\nstatistical methods. We highlight an optimal spatial range roughly\ncorresponding to stationarity scale, and significant influence of variables\nsuch as median income, wage with a non-simple spatial behavior that confirms\nthe importance of geographical particularities. On the other hand, multi-level\nmodeling reveals a strong influence of the state in the level of price but also\nof some local characteristics including population density. Through the\ncombination of such methods, we unveil the superposition of a governance\nprocess with a local socio-economical spatial process. The influence of\npopulation density on prices is furthermore consistent with a minimal\ntheoretical model of competition between gas stations, that we introduce and\nsolve numerically. We discuss developments and applications, including the\nelaboration of locally parametrized car-regulation policies.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 19:30:07 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 10:38:21 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Bergeaud", "Antonin", ""], ["Raimbault", "Juste", ""]]}, {"id": "1706.07502", "submitter": "Timothy Brathwaite", "authors": "Timothy Brathwaite and Joan Walker", "title": "Causal Inference in Travel Demand Modeling (and the lack thereof)", "comments": "21 pages, 1 figure", "journal-ref": "Journal of Choice Modelling 26 (2018) 1-18", "doi": "10.1016/j.jocm.2017.12.001", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is about the general disconnect that we see, both in practice and\nin literature, between the disciplines of travel demand modeling and causal\ninference. In this paper, we assert that travel demand modeling should be one\nof the many fields that focuses on the production of valid causal inferences,\nand we hypothesize about reasons for the current disconnect between the two\nbodies of research. Furthermore, we explore the potential benefits of uniting\nthese two disciplines. We consider what travel demand modeling can gain from\ngreater incorporation of techniques and perspectives from the causal inference\nliteratures, and we briefly discuss what the causal inference literature might\ngain from the work of travel demand modelers. In this paper, we do not attempt\nto \"solve\" issues related to the drawing of causal inferences from travel\ndemand models. Instead, we hope to spark a larger discussion both within and\nbetween the travel demand modeling and causal inference literatures. In\nparticular, we hope to incite discussion about the necessity of drawing causal\ninferences in travel demand applications and the methods by which one might\ncredibly do so.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 21:54:17 GMT"}, {"version": "v2", "created": "Thu, 7 Dec 2017 18:11:49 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Brathwaite", "Timothy", ""], ["Walker", "Joan", ""]]}, {"id": "1706.07677", "submitter": "Arnab Bhattacharya", "authors": "Arnab Bhattacharya and Simon P. Wilson and Refik Soyer", "title": "A Bayesian approach to modeling mortgage default and prepayment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a Bayesian competing risk proportional hazards model\nto describe mortgage defaults and prepayments. We develop Bayesian inference\nfor the model using Markov chain Monte Carlo methods. Implementation of the\nmodel is illustrated using actual default/prepayment data and additional\ninsights that can be obtained from the Bayesian analysis are discussed.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 12:40:55 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Bhattacharya", "Arnab", ""], ["Wilson", "Simon P.", ""], ["Soyer", "Refik", ""]]}, {"id": "1706.07682", "submitter": "Debasis Kundu Professor", "authors": "Shuvashree Mondal and Debasis Kundu", "title": "Point and Interval Estimation of Weibull Parameters Based on Joint\n  Progressively Censored Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of progressively censored data has received considerable\nattention in the last few years. In this paper we consider the joint\nprogressive censoring scheme for two populations. It is assumed that the\nlifetime distribution of the items from the two populations follow Weibull\ndistribution with the same shape but different scale parameters. Based on the\njoint progressive censoring scheme first we consider the maximum likelihood\nestimators of the unknown parameters whenever they exist. We provide the\nBayesian inferences of the unknown parameters under a fairly general priors on\nthe shape and scale parameters. The Bayes estimators and the associated\ncredible intervals cannot be obtained in closed form, and we propose to use the\nimportance sampling technique to compute the same. Further, we consider the\nproblem when it is known apriori that the expected lifetime of one population\nis smaller than the other. We provide the order restricted classical and\nBayesian inferences of the unknown parameters. Monte Carlo simulations are\nperformed to observe the performances of the different estimators and the\nassociated confidence and credible intervals. One real data set has been\nanalyzed for illustrative purpose.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 13:15:46 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Mondal", "Shuvashree", ""], ["Kundu", "Debasis", ""]]}, {"id": "1706.07840", "submitter": "Neil Shephard", "authors": "Iavor Bojinov and Neil Shephard", "title": "Time series experiments and causal estimands: exact randomization tests\n  and trading", "comments": null, "journal-ref": null, "doi": "10.1080/01621459.2018.1527225", "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define causal estimands for experiments on single time series, extending\nthe potential outcome framework to dealing with temporal data. Our approach\nallows the estimation of some of these estimands and exact randomization based\np-values for testing causal effects, without imposing stringent assumptions. We\ntest our methodology on simulated \"potential autoregressions,\"which have a\ncausal interpretation. Our methodology is partially inspired by data from a\nlarge number of experiments carried out by a financial company who compared the\nimpact of two different ways of trading equity futures contracts. We use our\nmethodology to make causal statements about their trading methods.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 19:14:38 GMT"}, {"version": "v2", "created": "Tue, 18 Jul 2017 17:35:30 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Bojinov", "Iavor", ""], ["Shephard", "Neil", ""]]}, {"id": "1706.08041", "submitter": "Pavitra Krishnaswamy", "authors": "Pavitra Krishnaswamy, Gabriel Obregon-Henao, Jyrki Ahveninen, Sheraz\n  Khan, Behtash Babadi, Juan Eugenio Iglesias, Matti S. Hamalainen, Patrick L.\n  Purdon", "title": "Sparsity Enables Estimation of both Subcortical and Cortical Activity\n  from MEG and EEG", "comments": "12 pages with 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subcortical structures play a critical role in brain function. However,\noptions for assessing electrophysiological activity in these structures are\nlimited. Electromagnetic fields generated by neuronal activity in subcortical\nstructures can be recorded non-invasively using magnetoencephalography (MEG)\nand electroencephalography (EEG). However, these subcortical signals are much\nweaker than those due to cortical activity. In addition, we show here that it\nis difficult to resolve subcortical sources, because distributed cortical\nactivity can explain the MEG and EEG patterns due to deep sources. We then\ndemonstrate that if the cortical activity can be assumed to be spatially\nsparse, both cortical and subcortical sources can be resolved with M/EEG.\nBuilding on this insight, we develop a novel hierarchical sparse inverse\nsolution for M/EEG. We assess the performance of this algorithm on realistic\nsimulations and auditory evoked response data and show that thalamic and\nbrainstem sources can be correctly estimated in the presence of cortical\nactivity. Our analysis and method suggest new opportunities and offer practical\ntools for characterizing electrophysiological activity in the subcortical\nstructures of the human brain.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jun 2017 06:52:23 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Krishnaswamy", "Pavitra", ""], ["Obregon-Henao", "Gabriel", ""], ["Ahveninen", "Jyrki", ""], ["Khan", "Sheraz", ""], ["Babadi", "Behtash", ""], ["Iglesias", "Juan Eugenio", ""], ["Hamalainen", "Matti S.", ""], ["Purdon", "Patrick L.", ""]]}, {"id": "1706.08058", "submitter": "Niklas Pfister", "authors": "Niklas Pfister, Peter B\\\"uhlmann and Jonas Peters", "title": "Invariant Causal Prediction for Sequential Data", "comments": "55 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of inferring the causal predictors of a response\n$Y$ from a set of $d$ explanatory variables $(X^1,\\dots,X^d)$. Classical\nordinary least squares regression includes all predictors that reduce the\nvariance of $Y$. Using only the causal predictors instead leads to models that\nhave the advantage of remaining invariant under interventions, loosely speaking\nthey lead to invariance across different \"environments\" or \"heterogeneity\npatterns\". More precisely, the conditional distribution of $Y$ given its causal\npredictors remains invariant for all observations. Recent work exploits such a\nstability to infer causal relations from data with different but known\nenvironments. We show that even without having knowledge of the environments or\nheterogeneity pattern, inferring causal relations is possible for time-ordered\n(or any other type of sequentially ordered) data. In particular, this allows\ndetecting instantaneous causal relations in multivariate linear time series\nwhich is usually not the case for Granger causality. Besides novel methodology,\nwe provide statistical confidence bounds and asymptotic detection results for\ninferring causal predictors, and present an application to monetary policy in\nmacroeconomics.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jun 2017 08:25:25 GMT"}, {"version": "v2", "created": "Mon, 28 May 2018 15:35:51 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Pfister", "Niklas", ""], ["B\u00fchlmann", "Peter", ""], ["Peters", "Jonas", ""]]}, {"id": "1706.08110", "submitter": "Yunfei Ye", "authors": "Yunfei Ye", "title": "The Matrix Hilbert Space and Its Application to Matrix Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Theoretical studies have proven that the Hilbert space has remarkable\nperformance in many fields of applications. Frames in tensor product of Hilbert\nspaces were introduced to generalize the inner product to high-order tensors.\nHowever, these techniques require tensor decomposition which could lead to the\nloss of information and it is a NP-hard problem to determine the rank of\ntensors. Here, we present a new framework, namely matrix Hilbert space to\nperform a matrix inner product space when data observations are represented as\nmatrices. We preserve the structure of initial data and multi-way correlation\namong them is captured in the process. In addition, we extend the reproducing\nkernel Hilbert space (RKHS) to reproducing kernel matrix Hilbert space (RKMHS)\nand propose an equivalent condition of the space uses of the certain kernel\nfunction. A new family of kernels is introduced in our framework to apply the\nclassifier of Support Tensor Machine(STM) and comparative experiments are\nperformed on a number of real-world datasets to support our contributions.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jun 2017 14:07:44 GMT"}, {"version": "v2", "created": "Tue, 14 Nov 2017 08:57:33 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Ye", "Yunfei", ""]]}, {"id": "1706.08171", "submitter": "Pierre Ablin", "authors": "Pierre Ablin, Jean-Fran\\c{c}ois Cardoso and Alexandre Gramfort", "title": "Faster independent component analysis by preconditioning with Hessian\n  approximations", "comments": "23 pages, 3 figures", "journal-ref": null, "doi": "10.1109/TSP.2018.2844203", "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent Component Analysis (ICA) is a technique for unsupervised\nexploration of multi-channel data that is widely used in observational\nsciences. In its classic form, ICA relies on modeling the data as linear\nmixtures of non-Gaussian independent sources. The maximization of the\ncorresponding likelihood is a challenging problem if it has to be completed\nquickly and accurately on large sets of real data. We introduce the\nPreconditioned ICA for Real Data (Picard) algorithm, which is a relative L-BFGS\nalgorithm preconditioned with sparse Hessian approximations. Extensive\nnumerical comparisons to several algorithms of the same class demonstrate the\nsuperior performance of the proposed technique, especially on real data, for\nwhich the ICA model does not necessarily hold.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jun 2017 20:47:33 GMT"}, {"version": "v2", "created": "Tue, 27 Jun 2017 00:51:32 GMT"}, {"version": "v3", "created": "Fri, 8 Sep 2017 16:43:28 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Ablin", "Pierre", ""], ["Cardoso", "Jean-Fran\u00e7ois", ""], ["Gramfort", "Alexandre", ""]]}, {"id": "1706.08281", "submitter": "Camille Coron", "authors": "Camille Coron (LMO), Cl\\'ement Calenge (ONCFS), Christophe Giraud\n  (LMO), Romain Julliard (CESCO)", "title": "Estimation of species relative abundances and habitat preferences using\n  opportunistic data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new statistical procedure to monitor, with opportunist data,\nrelative species abundances and their respective preferences for dierent\nhabitat types. Following Giraud et al. (2015), we combine the opportunistic\ndata with some standardized data in order to correct the bias inherent to the\nopportunistic data collection. Our main contributions are (i) to tackle the\nbias induced by habitat selection behaviors, (ii) to handle data where the\nhabitat type associated to each observation is unknown, (iii) to estimate\nprobabilities of selection of habitat for the species. As an illustration, we\nestimate common bird species habitat preferences and abundances in the region\nof Aquitaine (France).\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 08:46:55 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Coron", "Camille", "", "LMO"], ["Calenge", "Cl\u00e9ment", "", "ONCFS"], ["Giraud", "Christophe", "", "LMO"], ["Julliard", "Romain", "", "CESCO"]]}, {"id": "1706.08320", "submitter": "Soraia Pereira", "authors": "Soraia Pereira, Kamil Feridun Turkman, Luis Correia, Haavard Rue", "title": "Unemployment estimation: Spatial point referenced methods and models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Portuguese Labor force survey, from 4th quarter of 2014 onwards, started\ngeo-referencing the sampling units, namely the dwellings in which the surveys\nare carried. This opens new possibilities in analysing and estimating\nunemployment and its spatial distribution across any region. The labor force\nsurvey choose, according to an preestablished sampling criteria, a certain\nnumber of dwellings across the nation and survey the number of unemployed in\nthese dwellings. Based on this survey, the National Statistical Institute of\nPortugal presently uses direct estimation methods to estimate the national\nunemployment figures. Recently, there has been increased interest in estimating\nthese figures in smaller areas. Direct estimation methods, due to reduced\nsampling sizes in small areas, tend to produce fairly large sampling variations\ntherefore model based methods, which tend to \"borrow strength\" from area to\narea by making use of the areal dependence, should be favored. These model\nbased methods tend use areal counting processes as models and typically\nintroduce spatial dependence through the model parameters by a latent random\neffect. In this paper, we suggest modeling the spatial distribution of\nresidential buildings across Portugal by a Log Gaussian Cox process and the\nnumber of unemployed per residential unit as a mark attached to these random\npoints. Thus the main focus of the study is to model the spatial intensity\nfunction of this marked point process. Number of unemployed in any region can\nthen be estimated using a proper functional of this marked point process. The\nprincipal objective of this point referenced method for unemployment estimation\nis to get reliable estimates at higher spatial resolutions and at the same time\nincorporate in the model the auxiliary information available at residential\nunits such as average income or education level of individuals surveyed in\nthese units.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 11:04:45 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Pereira", "Soraia", ""], ["Turkman", "Kamil Feridun", ""], ["Correia", "Luis", ""], ["Rue", "Haavard", ""]]}, {"id": "1706.08418", "submitter": "Ivan Fernandez-Val", "authors": "Victor Chernozhukov, Iv\\'an Fern\\'andez-Val and Whitney Newey", "title": "Nonseparable Multinomial Choice Models in Cross-Section and Panel Data", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multinomial choice models are fundamental for empirical modeling of economic\nchoices among discrete alternatives. We analyze identification of binary and\nmultinomial choice models when the choice utilities are nonseparable in\nobserved attributes and multidimensional unobserved heterogeneity with\ncross-section and panel data. We show that derivatives of choice probabilities\nwith respect to continuous attributes are weighted averages of utility\nderivatives in cross-section models with exogenous heterogeneity. In the\nspecial case of random coefficient models with an independent additive effect,\nwe further characterize that the probability derivative at zero is proportional\nto the population mean of the coefficients. We extend the identification\nresults to models with endogenous heterogeneity using either a control function\nor panel data. In time stationary panel models with two periods, we find that\ndifferences over time of derivatives of choice probabilities identify utility\nderivatives \"on the diagonal,\" i.e. when the observed attributes take the same\nvalues in the two periods. We also show that time stationarity does not\nidentify structural derivatives \"off the diagonal\" both in continuous and\nmultinomial choice panel models.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 14:46:45 GMT"}, {"version": "v2", "created": "Wed, 9 May 2018 15:07:50 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Chernozhukov", "Victor", ""], ["Fern\u00e1ndez-Val", "Iv\u00e1n", ""], ["Newey", "Whitney", ""]]}, {"id": "1706.08440", "submitter": "Elizabeth Ogburn", "authors": "Elizabeth L. Ogburn", "title": "Challenges to estimating contagion effects from observational data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A growing body of literature attempts to learn about contagion using\nobservational (i.e. non-experimental) data collected from a single social\nnetwork. While the conclusions of these studies may be correct, the methods\nrely on assumptions that are likely--and sometimes guaranteed to be--false, and\ntherefore the evidence for the conclusions is often weaker than it seems.\nDeveloping methods that do not need to rely on implausible assumptions is an\nincredibly challenging and important open problem in statistics. Appropriate\nmethods don't (yet!) exist, so researchers hoping to learn about contagion from\nobservational social network data are sometimes faced with a dilemma: they can\nabandon their research program, or they can use inappropriate methods. This\nchapter will focus on the challenges and the open problems and will not weigh\nin on that dilemma, except to mention here that the most responsible way to use\nany statistical method, especially when it is well-known that the assumptions\non which it rests do not hold, is with a healthy dose of skepticism, with\nhonest acknowledgment and deep understanding of the limitations, and with\ncopious caveats about how to interpret the results.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 15:36:15 GMT"}, {"version": "v2", "created": "Thu, 29 Jun 2017 15:16:25 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Ogburn", "Elizabeth L.", ""]]}, {"id": "1706.08938", "submitter": "Bryan Yates", "authors": "Bryan Yates, Aleksander Labuda, Martin Lysy", "title": "Robust and Efficient Parametric Spectral Estimation in Atomic Force\n  Microscopy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An atomic force microscope (AFM) is capable of producing ultra-high\nresolution measurements of nanoscopic objects and forces. It is an\nindispensable tool for various scientific disciplines such as molecular\nengineering, solid-state physics, and cell biology. Prior to a given\nexperiment, the AFM must be calibrated by fitting a spectral density model to\nbaseline recordings. However, since AFM experiments typically collect large\namounts of data, parameter estimation by maximum likelihood can be\nprohibitively expensive. Thus, practitioners routinely employ a much faster\nleast-squares estimation method, at the cost of substantially reduced\nstatistical efficiency. Additionally, AFM data is often contaminated by\nperiodic electronic noise, to which parameter estimates are highly sensitive.\nThis article proposes a two-stage estimator to address these issues.\nPreliminary parameter estimates are first obtained by a variance-stabilizing\nprocedure, by which the simplicity of least-squares combines with the\nefficiency of maximum likelihood. A test for spectral periodicities then\neliminates high-impact outliers, considerably and robustly protecting the\nsecond-stage estimator from the effects of electronic noise. Simulation and\nexperimental results indicate that a two- to ten-fold reduction in mean squared\nerror can be expected by applying our methodology.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 16:48:20 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Yates", "Bryan", ""], ["Labuda", "Aleksander", ""], ["Lysy", "Martin", ""]]}, {"id": "1706.09072", "submitter": "Shahryar Minhas", "authors": "Shahryar Minhas, Peter D. Hoff, Michael D. Ward", "title": "Influence Networks in International Relations", "comments": "23 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring influence and determining what drives it are persistent questions\nin political science and in network analysis more generally. Herein we focus on\nthe domain of international relations. Our major substantive question is: How\ncan we determine what characteristics make an actor influential? To address the\ntopic of influence, we build on a multilinear tensor regression framework\n(MLTR) that captures influence relationships using a tensor generalization of a\nvector autoregression model. Influence relationships in that approach are\ncaptured in a pair of n x n matrices and provide measurements of how the\nnetwork actions of one actor may influence the future actions of another. A\nlimitation of the MLTR and earlier latent space approaches is that there are no\ndirect mechanisms through which to explain why a certain actor is more or less\ninfluential than others. Our new framework, social influence regression,\nprovides a way to statistically model the influence of one actor on another as\na function of characteristics of the actors. Thus we can move beyond just\nestimating that an actor influences another to understanding why. To highlight\nthe utility of this approach, we apply it to studying monthly-level conflictual\nevents between countries as measured through the Integrated Crisis Early\nWarning System (ICEWS) event data project.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 23:12:16 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Minhas", "Shahryar", ""], ["Hoff", "Peter D.", ""], ["Ward", "Michael D.", ""]]}, {"id": "1706.09574", "submitter": "Elias Chaibub Neto", "authors": "Elias Chaibub Neto, Thanneer M Perumal, Abhishek Pratap, Brian M Bot,\n  Lara Mangravite, Larsson Omberg", "title": "On the analysis of personalized medication response and classification\n  of case vs control patients in mobile health studies: the mPower case study", "comments": "27 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we provide a couple of contributions to the analysis of\nlongitudinal data collected by smartphones in mobile health applications.\nFirst, we propose a novel statistical approach to disentangle personalized\ntreatment and \"time-of-the-day\" effects in observational studies. Under the\nassumption of no unmeasured confounders, we show how to use conditional\nindependence relations in the data in order to determine if a difference in\nperformance between activity tasks performed before and after the participant\nhas taken medication, are potentially due to an effect of the medication or to\na \"time-of-the-day\" effect (or still to both). Second, we show that smartphone\ndata collected from a given study participant can represent a \"digital\nfingerprint\" of the participant, and that classifiers of case/control labels,\nconstructed using longitudinal data, can show artificially improved performance\nwhen data from each participant is included in both training and test sets. We\nillustrate our contributions using data collected during the first 6 months of\nthe mPower study.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 04:27:55 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Neto", "Elias Chaibub", ""], ["Perumal", "Thanneer M", ""], ["Pratap", "Abhishek", ""], ["Bot", "Brian M", ""], ["Mangravite", "Lara", ""], ["Omberg", "Larsson", ""]]}, {"id": "1706.09602", "submitter": "Aasthaa Bansal", "authors": "Aasthaa Bansal, Nicole Mayer-Hamblett, Christopher H. Goss, Patrick J.\n  Heagerty", "title": "A Novel Tool to Evaluate the Accuracy of Predicting Survival in Cystic\n  Fibrosis", "comments": "20 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Effective allocation of limited donor lungs in cystic fibrosis\n(CF) requires accurate survival predictions, so that high-risk patients may be\nprioritized for transplantation. In practice, decisions about allocation are\nmade dynamically, using routinely updated assessments. We present a novel tool\nfor evaluating risk prediction models that, unlike traditional methods,\ncaptures the dynamic nature of decision-making. Methods: Predicted risk is used\nas a score to rank incident deaths versus patients who survive, with the goal\nof ranking the deaths higher. The mean rank across deaths at a given time\nmeasures time-specific predictive accuracy; when assessed over time, it\nreflects time-varying accuracy. Results: Applying this approach to CF Registry\ndata on patients followed from 1993-2011, we show that traditional methods do\nnot capture the performance of models used dynamically in the clinical setting.\nPreviously proposed multivariate risk scores perform no better than forced\nexpiratory volume in 1 second as a percentage of predicted normal (FEV1%)\nalone. Despite its value for survival prediction, FEV1% has a low sensitivity\nof 45% over time (for fixed specificity of 95%), leaving room for improvement\nin prediction. Finally, prediction accuracy with annually-updated FEV1% shows\nminor differences compared to FEV1% updated every 2 years, which may have\nclinical implications regarding the optimal frequency of updating clinical\ninformation. Conclusions: It is imperative to continue to develop models that\naccurately predict survival in CF. Our proposed approach can serve as the basis\nfor evaluating the predictive ability of these models by better accounting for\ntheir dynamic clinical use.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 07:26:30 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Bansal", "Aasthaa", ""], ["Mayer-Hamblett", "Nicole", ""], ["Goss", "Christopher H.", ""], ["Heagerty", "Patrick J.", ""]]}, {"id": "1706.09796", "submitter": "David R\\\"ugamer", "authors": "David R\\\"ugamer and Sonja Greven", "title": "Selective inference after likelihood- or test-based model selection in\n  linear models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical inference after model selection requires an inference framework\nthat takes the selection into account in order to be valid. Following recent\nwork on selective inference, we derive analytical expressions for inference\nafter likelihood- or test-based model selection for linear models.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 15:09:33 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 11:42:31 GMT"}, {"version": "v3", "created": "Sat, 23 Sep 2017 16:37:27 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["R\u00fcgamer", "David", ""], ["Greven", "Sonja", ""]]}, {"id": "1706.09839", "submitter": "Peter Klimek", "authors": "Peter Klimek, Raul Jimenez, Manuel Hidalgo, Abraham Hinteregger,\n  Stefan Thurner", "title": "Election forensic analysis of the Turkish Constitutional Referendum 2017", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With a majority of 'Yes' votes in the Constitutional Referendum of 2017,\nTurkey continues its transition from democracy to autocracy. By the will of the\nTurkish people, this referendum transferred practically all executive power to\npresident Erdogan. However, the referendum was confronted with a substantial\nnumber of allegations of electoral misconducts and irregularities, ranging from\nstate coercion of 'No' supporters to the controversial validity of unstamped\nballots. In this note we report the results of an election forensic analysis of\nthe 2017 referendum to clarify to what extent these voting irregularities were\npresent and if they were able to influence the outcome of the referendum. We\nspecifically apply novel statistical forensics tests to further identify the\nspecific nature of electoral malpractices. In particular, we test whether the\ndata contains fingerprints for ballot-stuffing (submission of multiple ballots\nper person during the vote) and voter rigging (coercion and intimidation of\nvoters). Additionally, we perform tests to identify numerical anomalies in the\nelection results. We find systematic and highly significant support for the\npresence of both, ballot-stuffing and voter rigging. In 6% of stations we find\nsigns for ballot-stuffing with an error (probability of ballot-stuffing not\nhappening) of 0.15% (3 sigma event). The influence of these vote distortions\nwere large enough to tip the overall balance from 'No' to a majority of 'Yes'\nvotes.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 16:33:28 GMT"}, {"version": "v2", "created": "Mon, 3 Jul 2017 09:43:57 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Klimek", "Peter", ""], ["Jimenez", "Raul", ""], ["Hidalgo", "Manuel", ""], ["Hinteregger", "Abraham", ""], ["Thurner", "Stefan", ""]]}, {"id": "1706.10129", "submitter": "Joseph Saleh", "authors": "Joseph Homer Saleh, Fan Geng, Michelle Ku, Mitchell L. R. Walker II", "title": "Electric propulsion reliability: statistical analysis of on-orbit\n  anomalies and comparative analysis of electric versus chemical propulsion\n  failure rates", "comments": "36 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.space-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With a few hundred spacecraft launched to date with electric propulsion (EP),\nit is possible to conduct an epidemiological study of EP on orbit reliability.\nThe first objective of the present work was to undertake such a study and\nanalyze EP track record of on orbit anomalies and failures by different\ncovariates. The second objective was to provide a comparative analysis of EP\nfailure rates with those of chemical propulsion. After a thorough data\ncollection, 162 EP-equipped satellites launched between January 1997 and\nDecember 2015 were included in our dataset for analysis. Several statistical\nanalyses were conducted, at the aggregate level and then with the data\nstratified by severity of the anomaly, by orbit type, and by EP technology.\nMean Time To Anomaly (MTTA) and the distribution of the time to anomaly were\ninvestigated, as well as anomaly rates. The important findings in this work\ninclude the following: (1) Post-2005, EP reliability has outperformed that of\nchemical propulsion; (2) Hall thrusters have robustly outperformed chemical\npropulsion, and they maintain a small but shrinking reliability advantage over\ngridded ion engines. Other results were also provided, for example the\ndifferentials in MTTA of minor and major anomalies for gridded ion engines and\nHall thrusters. It was shown that: (3) Hall thrusters exhibit minor anomalies\nvery early on orbit, which might be indicative of infant anomalies, and thus\nwould benefit from better ground testing and acceptance procedures; (4) Strong\nevidence exists that EP anomalies (onset and likelihood) and orbit type are\ndependent, a dependence likely mediated by either the space environment or\ndifferences in thrusters duty cycles; (5) Gridded ion thrusters exhibit both\ninfant and wear-out failures, and thus would benefit from a reliability growth\nprogram that addresses both these types of problems.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 21:18:56 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Saleh", "Joseph Homer", ""], ["Geng", "Fan", ""], ["Ku", "Michelle", ""], ["Walker", "Mitchell L. R.", "II"]]}, {"id": "1706.10180", "submitter": "David Puelz", "authors": "David Puelz, P. Richard Hahn, Carlos Carvalho", "title": "Regret-based Selection for Sparse Dynamic Portfolios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers portfolio construction in a dynamic setting. We specify\na loss function comprised of utility and complexity components with an unknown\ntradeoff parameter. We develop a novel regret-based criterion for selecting the\ntradeoff parameter to construct optimal sparse portfolios over time.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 13:03:27 GMT"}, {"version": "v2", "created": "Mon, 10 Jul 2017 15:05:41 GMT"}, {"version": "v3", "created": "Sun, 23 Jul 2017 19:28:13 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Puelz", "David", ""], ["Hahn", "P. Richard", ""], ["Carvalho", "Carlos", ""]]}]