[{"id": "1103.0377", "submitter": "Mehdi Molkaraie", "authors": "Mehdi Molkaraie and Payam Pakzad", "title": "On Properties of the Minimum Entropy Sub-tree to Compute Lower Bounds on\n  the Partition Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IT math.IT physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing the partition function and the marginals of a global probability\ndistribution are two important issues in any probabilistic inference problem.\nIn a previous work, we presented sub-tree based upper and lower bounds on the\npartition function of a given probabilistic inference problem. Using the\nentropies of the sub-trees we proved an inequality that compares the lower\nbounds obtained from different sub-trees. In this paper we investigate the\nproperties of one specific lower bound, namely the lower bound computed by the\nminimum entropy sub-tree. We also investigate the relationship between the\nminimum entropy sub-tree and the sub-tree that gives the best lower bound.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2011 09:46:44 GMT"}], "update_date": "2011-03-03", "authors_parsed": [["Molkaraie", "Mehdi", ""], ["Pakzad", "Payam", ""]]}, {"id": "1103.0818", "submitter": "Jie Peng", "authors": "Ru Wang, Jie Peng, Pei Wang", "title": "A note on logistic regression and logistic kernel machine models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a note on logistic regression models and logistic kernel machine\nmodels. It contains derivations to some of the expressions in a paper -- SNP\nSet Analysis for Detecting Disease Association Using Exon Sequence Data --\nsubmitted to BMC proceedings by these authors.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2011 04:25:01 GMT"}], "update_date": "2011-03-07", "authors_parsed": [["Wang", "Ru", ""], ["Peng", "Jie", ""], ["Wang", "Pei", ""]]}, {"id": "1103.1046", "submitter": "Michael Stumpf", "authors": "Chris Barnes, Daniel Silk, Xia Sheng and Michael P.H. Stumpf", "title": "Bayesian design of synthetic biological systems", "comments": "36 pages, 16 figures", "journal-ref": null, "doi": "10.1073/pnas.1017972108", "report-no": null, "categories": "q-bio.MN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we introduce a new design framework for synthetic biology that exploits\nthe advantages of Bayesian model selection. We will argue that the difference\nbetween inference and design is that in the former we try to reconstruct the\nsystem that has given rise to the data that we observe, while in the latter, we\nseek to construct the system that produces the data that we would like to\nobserve, i.e. the desired behavior. Our approach allows us to exploit methods\nfrom Bayesian statistics, including efficient exploration of models spaces and\nhigh-dimensional parameter spaces, and the ability to rank models with respect\nto their ability to generate certain types of data. Bayesian model selection\nfurthermore automatically strikes a balance between complexity and (predictive\nor explanatory) performance of mathematical models. In order to deal with the\ncomplexities of molecular systems we employ an approximate Bayesian computation\nscheme which only requires us to simulate from different competing models in\norder to arrive at rational criteria for choosing between them. We illustrate\nthe advantages resulting from combining the design and modeling (or in-silico\nprototyping) stages currently seen as separate in synthetic biology by\nreference to deterministic and stochastic model systems exhibiting adaptive and\nswitch-like behavior, as well as bacterial two-component signaling systems.\n", "versions": [{"version": "v1", "created": "Sat, 5 Mar 2011 11:51:27 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Barnes", "Chris", ""], ["Silk", "Daniel", ""], ["Sheng", "Xia", ""], ["Stumpf", "Michael P. H.", ""]]}, {"id": "1103.1303", "submitter": "Tim Jupp", "authors": "Tim E. Jupp, Rachel Lowe, Caio A.S. Coelho and David B. Stephenson", "title": "On the visualisation, verification and recalibration of ternary\n  probabilistic forecasts", "comments": null, "journal-ref": null, "doi": "10.1098/rsta.2011.0350", "report-no": null, "categories": "stat.AP physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a geometrical interpretation of ternary probabilistic forecasts in\nwhich forecasts and observations are regarded as points inside a triangle.\nWithin the triangle, we define a continuous colour palette in which hue and\ncolour saturation are defined with reference to the observed climatology. In\ncontrast to current methods, forecast maps created with this colour scheme\nconvey all of the information present in each ternary forecast. The geometrical\ninterpretation is then extended to verification under quadratic scoring rules\n(of which the Brier Score and the Ranked Probability Score are well--known\nexamples). Each scoring rule defines an associated triangle in which the square\nroots of the score, the reliability, the uncertainty and the resolution all\nhave natural interpretations as root--mean--square distances. This leads to our\nproposal for a Ternary Reliability Diagram in which data relating to\nverification and calibration can be summarised. We illustrate these ideas with\ndata relating to seasonal forecasting of precipitation in South America,\nincluding an example of nonlinear forecast calibration. Codes implementing\nthese ideas have been produced using the statistical software package R and are\navailable from the authors.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2011 15:38:30 GMT"}, {"version": "v2", "created": "Tue, 2 Aug 2011 08:12:47 GMT"}, {"version": "v3", "created": "Fri, 12 Aug 2011 08:11:14 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Jupp", "Tim E.", ""], ["Lowe", "Rachel", ""], ["Coelho", "Caio A. S.", ""], ["Stephenson", "David B.", ""]]}, {"id": "1103.1861", "submitter": "Kenny Chowdhary", "authors": "Kamaljit Chowdhary and Paul Dupuis", "title": "Distinguishing and integrating aleatoric and epistemic variation in\n  uncertainty quantification", "comments": "37 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of uncertainty quantification to date has focused on determining the\neffect of variables modeled probabilistically, and with a known distribution,\non some physical or engineering system. We develop methods to obtain\ninformation on the system when the distributions of some variables are known\nexactly, others are known only approximately, and perhaps others are not\nmodeled as random variables at all. The main tool used is the duality between\nrisk-sensitive integrals and relative entropy, and we obtain explicit bounds on\nstandard performance measures (variances, exceedance probabilities) over\nfamilies of distributions whose distance from a nominal distribution is\nmeasured by relative entropy. The evaluation of the risk-sensitive expectations\nis based on polynomial chaos expansions, which help keep the computational\naspects tractable.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2011 19:44:46 GMT"}, {"version": "v2", "created": "Fri, 18 Mar 2011 03:37:57 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Chowdhary", "Kamaljit", ""], ["Dupuis", "Paul", ""]]}, {"id": "1103.1963", "submitter": "Hung Hung", "authors": "Hung Hung and Chin-Tsang Chiang", "title": "Nonparametric Methodology for the Time-Dependent Partial Area under the\n  ROC Curve", "comments": "20 pages, 4 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To assess the classification accuracy of a continuous diagnostic result, the\nreceiver operating characteristic (ROC) curve is commonly used in applications.\nThe partial area under the ROC curve (pAUC) is one of widely accepted summary\nmeasures due to its generality and ease of probability interpretation. In the\nfield of life science, a direct extension of the pAUC into the time-to-event\nsetting can be used to measure the usefulness of a biomarker for disease\ndetection over time. Without using a trapezoidal rule, we propose nonparametric\nestimators, which are easily computed and have closed-form expressions, for the\ntime-dependent pAUC. The asymptotic Gaussian processes of the estimators are\nestablished and the estimated variance-covariance functions are provided, which\nare essential in the construction of confidence intervals. The finite sample\nperformance of the proposed inference procedures are investigated through a\nseries of simulations. Our method is further applied to evaluate the\nclassification ability of CD4 cell counts on patient's survival time in the\nAIDS Clinical Trials Group (ACTG) 175 study. In addition, the inferences can be\ngeneralized to compare the time-dependent pAUCs between patients received the\nprior antiretroviral therapy and those without it.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2011 08:46:16 GMT"}], "update_date": "2011-03-11", "authors_parsed": [["Hung", "Hung", ""], ["Chiang", "Chin-Tsang", ""]]}, {"id": "1103.2128", "submitter": "Matthew Kerr", "authors": "Matthew Kerr", "title": "Improving Sensitivity to Weak Pulsations with Photon Probability\n  Weighting", "comments": "10 pages, 11 figures, published by ApJ; v2 fixes an error in Eq. 5", "journal-ref": null, "doi": "10.1088/0004-637X/732/1/38", "report-no": null, "categories": "astro-ph.IM astro-ph.HE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  All gamma-ray telescopes suffer from source confusion due to their inability\nto focus incident high-energy radiation, and the resulting background\ncontamination can obscure the periodic emission from faint pulsars. In the\ncontext of the Fermi Large Area Telescope, we outline enhanced statistical\ntests for pulsation in which each photon is weighted by its probability to have\noriginated from the candidate pulsar. The probabilities are calculated using\nthe instrument response function and a full spectral model, enabling powerful\nbackground rejection. With Monte Carlo methods, we demonstrate that the new\ntests increase the sensitivity to pulsars by more than 50% under a wide range\nof conditions. This improvement may appreciably increase the completeness of\nthe sample of radio-loud gamma-ray pulsars. Finally, we derive the asymptotic\nnull distribution for the H-test, expanding its domain of validity to\narbitrarily complex light curves.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2011 21:00:01 GMT"}, {"version": "v2", "created": "Mon, 5 Sep 2011 18:12:05 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Kerr", "Matthew", ""]]}, {"id": "1103.2208", "submitter": "Francois-Xavier Dupe", "authors": "Fran\\c{c}ois-Xavier Dup\\'e (DSM), Jalal Fadili (GREYC), Jean-Luc\n  Starck (DSM)", "title": "Linear inverse problems with noise: primal and primal-dual splitting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose two algorithms for solving linear inverse problems\nwhen the observations are corrupted by noise. A proper data fidelity term\n(log-likelihood) is introduced to reflect the statistics of the noise (e.g.\nGaussian, Poisson). On the other hand, as a prior, the images to restore are\nassumed to be positive and sparsely represented in a dictionary of waveforms.\nPiecing together the data fidelity and the prior terms, the solution to the\ninverse problem is cast as the minimization of a non-smooth convex functional.\nWe establish the well-posedness of the optimization problem, characterize the\ncorresponding minimizers, and solve it by means of primal and primal-dual\nproximal splitting algorithms originating from the field of non-smooth convex\noptimization theory. Experimental results on deconvolution, inpainting and\ndenoising with some comparison to prior methods are also reported.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2011 08:06:20 GMT"}], "update_date": "2011-03-14", "authors_parsed": [["Dup\u00e9", "Fran\u00e7ois-Xavier", "", "DSM"], ["Fadili", "Jalal", "", "GREYC"], ["Starck", "Jean-Luc", "", "DSM"]]}, {"id": "1103.2209", "submitter": "Francois-Xavier Dupe", "authors": "Fran\\c{c}ois-Xavier Dup\\'e (DSM), Jalal Fadili (GREYC), Jean-Luc\n  Starck (DSM)", "title": "Inverse Problems with Poisson noise: Primal and Primal-Dual Splitting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose two algorithms for solving linear inverse problems\nwhen the observations are corrupted by Poisson noise. A proper data fidelity\nterm (log-likelihood) is introduced to reflect the Poisson statistics of the\nnoise. On the other hand, as a prior, the images to restore are assumed to be\npositive and sparsely represented in a dictionary of waveforms. Piecing\ntogether the data fidelity and the prior terms, the solution to the inverse\nproblem is cast as the minimization of a non-smooth convex functional. We\nestablish the well-posedness of the optimization problem, characterize the\ncorresponding minimizers, and solve it by means of primal and primal-dual\nproximal splitting algorithms originating from the field of non-smooth convex\noptimization theory. Experimental results on deconvolution and comparison to\nprior methods are also reported.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2011 08:06:56 GMT"}], "update_date": "2011-03-14", "authors_parsed": [["Dup\u00e9", "Fran\u00e7ois-Xavier", "", "DSM"], ["Fadili", "Jalal", "", "GREYC"], ["Starck", "Jean-Luc", "", "DSM"]]}, {"id": "1103.2210", "submitter": "Francois-Xavier Dupe", "authors": "Fran\\c{c}ois-Xavier Dup\\'e (DSM), Jalal Fadili (GREYC), Jean-Luc\n  Starck (DSM)", "title": "Data augmentation for galaxy density map reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The matter density is an important knowledge for today cosmology as many\nphenomena are linked to matter fluctuations. However, this density is not\ndirectly available, but estimated through lensing maps or galaxy surveys. In\nthis article, we focus on galaxy surveys which are incomplete and noisy\nobservations of the galaxy density. Incomplete, as part of the sky is\nunobserved or unreliable. Noisy as they are count maps degraded by Poisson\nnoise. Using a data augmentation method, we propose a two-step method for\nrecovering the density map, one step for inferring missing data and one for\nestimating of the density. The results show that the missing areas are\nefficiently inferred and the statistical properties of the maps are very well\npreserved.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2011 08:07:35 GMT"}], "update_date": "2011-03-14", "authors_parsed": [["Dup\u00e9", "Fran\u00e7ois-Xavier", "", "DSM"], ["Fadili", "Jalal", "", "GREYC"], ["Starck", "Jean-Luc", "", "DSM"]]}, {"id": "1103.2213", "submitter": "Francois-Xavier Dupe", "authors": "Fran\\c{c}ois-Xavier Dup\\'e (DSM), Jalal Fadili (GREYC), Jean-Luc\n  Starck (DSM)", "title": "Deconvolution under Poisson noise using exact data fidelity and\n  synthesis or analysis sparsity priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a Bayesian MAP estimator for solving the\ndeconvolution problems when the observations are corrupted by Poisson noise.\nTowards this goal, a proper data fidelity term (log-likelihood) is introduced\nto reflect the Poisson statistics of the noise. On the other hand, as a prior,\nthe images to restore are assumed to be positive and sparsely represented in a\ndictionary of waveforms such as wavelets or curvelets. Both analysis and\nsynthesis-type sparsity priors are considered. Piecing together the data\nfidelity and the prior terms, the deconvolution problem boils down to the\nminimization of non-smooth convex functionals (for each prior). We establish\nthe well-posedness of each optimization problem, characterize the corresponding\nminimizers, and solve them by means of proximal splitting algorithms\noriginating from the realm of non-smooth convex optimization theory.\nExperimental results are conducted to demonstrate the potential applicability\nof the proposed algorithms to astronomical imaging datasets.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2011 08:13:16 GMT"}], "update_date": "2011-03-14", "authors_parsed": [["Dup\u00e9", "Fran\u00e7ois-Xavier", "", "DSM"], ["Fadili", "Jalal", "", "GREYC"], ["Starck", "Jean-Luc", "", "DSM"]]}, {"id": "1103.2697", "submitter": "Julien Chiquet", "authors": "Julien Chiquet, Yves Grandvalet, Camille Charbonnier", "title": "Sparsity with sign-coherent groups of variables via the\n  cooperative-Lasso", "comments": "Published in at http://dx.doi.org/10.1214/11-AOAS520 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 2, 795-830", "doi": "10.1214/11-AOAS520", "report-no": "IMS-AOAS-AOAS520", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problems of estimation and selection of parameters endowed\nwith a known group structure, when the groups are assumed to be sign-coherent,\nthat is, gathering either nonnegative, nonpositive or null parameters. To\ntackle this problem, we propose the cooperative-Lasso penalty. We derive the\noptimality conditions defining the cooperative-Lasso estimate for generalized\nlinear models, and propose an efficient active set algorithm suited to\nhigh-dimensional problems. We study the asymptotic consistency of the estimator\nin the linear regression setup and derive its irrepresentable conditions, which\nare milder than the ones of the group-Lasso regarding the matching of groups\nwith the sparsity pattern of the true parameters. We also address the problem\nof model selection in linear regression by deriving an approximation of the\ndegrees of freedom of the cooperative-Lasso estimator. Simulations comparing\nthe proposed estimator to the group and sparse group-Lasso comply with our\ntheoretical results, showing consistent improvements in support recovery for\nsign-coherent groups. We finally propose two examples illustrating the wide\napplicability of the cooperative-Lasso: first to the processing of ordinal\nvariables, where the penalty acts as a monotonicity prior; second to the\nprocessing of genomic data, where the set of differentially expressed probes is\nenriched by incorporating all the probes of the microarray that are related to\nthe corresponding genes.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2011 15:54:20 GMT"}, {"version": "v2", "created": "Wed, 28 Sep 2011 04:49:49 GMT"}, {"version": "v3", "created": "Mon, 2 Jul 2012 09:13:29 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Chiquet", "Julien", ""], ["Grandvalet", "Yves", ""], ["Charbonnier", "Camille", ""]]}, {"id": "1103.2872", "submitter": "Holger Drees", "authors": "Holger Drees", "title": "Extreme value analysis of actuarial risks: estimation and model\n  validation", "comments": "to appear in Advances in Statistical Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an overview of several aspects arising in the statistical analysis of\nextreme risks with actuarial applications in view. In particular it is\ndemonstrated that empirical process theory is a very powerful tool, both for\nthe asymptotic analysis of extreme value estimators and to devise tools for the\nvalidation of the underlying model assumptions. While the focus of the paper is\non univariate tail risk analysis, the basic ideas of the analysis of the\nextremal dependence between different risks are also outlined. Here we\nemphasize some of the limitation of classical multivariate extreme value theory\nand sketch how a different model proposed by Ledford and Tawn can help to avoid\npitfalls. Finally, these theoretical results are used to analyze a data set of\nlarge claim sizes from health insurance.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2011 10:03:53 GMT"}, {"version": "v2", "created": "Wed, 16 Nov 2011 09:19:38 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Drees", "Holger", ""]]}, {"id": "1103.2876", "submitter": "Charlotte Soneson", "authors": "Charlotte Soneson and Magnus Fontes", "title": "A framework for list representation, enabling list stabilization through\n  incorporation of gene exchangeabilities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of multivariate data sets from e.g. microarray studies frequently\nresults in lists of genes which are associated with some response of interest.\nThe biological interpretation is often complicated by the statistical\ninstability of the obtained gene lists with respect to sampling variations,\nwhich may partly be due to the functional redundancy among genes, implying that\nmultiple genes can play exchangeable roles in the cell. In this paper we use\nthe concept of exchangeability of random variables to model this functional\nredundancy and thereby account for the instability attributable to sampling\nvariations. We present a flexible framework to incorporate the exchangeability\ninto the representation of lists. The proposed framework supports\nstraightforward robust comparison between any two lists. It can also be used to\ngenerate new, more stable gene rankings incorporating more information from the\nexperimental data. Using a microarray data set from lung cancer patients we\nshow that the proposed method provides more robust gene rankings than existing\nmethods with respect to sampling variations, without compromising the\nbiological significance.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2011 10:37:10 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Soneson", "Charlotte", ""], ["Fontes", "Magnus", ""]]}, {"id": "1103.3300", "submitter": "Georg M Goerg", "authors": "Georg M. Goerg", "title": "A Nonparametric Frequency Domain EM Algorithm for Time Series\n  Classification with Applications to Spike Sorting and Macro-Economics", "comments": "Winner of the JSM 2011 student paper competition in \"Statistical\n  Learning and Data Mining (SAM)\"; 34 pages. Accepted for publication in SAM", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML physics.data-an stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I propose a frequency domain adaptation of the Expectation Maximization (EM)\nalgorithm to group a family of time series in classes of similar dynamic\nstructure. It does this by viewing the magnitude of the discrete Fourier\ntransform (DFT) of each signal (or power spectrum) as a probability\ndensity/mass function (pdf/pmf) on the unit circle: signals with similar\ndynamics have similar pdfs; distinct patterns have distinct pdfs. An advantage\nof this approach is that it does not rely on any parametric form of the dynamic\nstructure, but can be used for non-parametric, robust and model-free\nclassification. This new method works for non-stationary signals of similar\nshape as well as stationary signals with similar auto-correlation structure.\nApplications to neural spike sorting (non-stationary) and pattern-recognition\nin socio-economic time series (stationary) demonstrate the usefulness and wide\napplicability of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2011 21:31:17 GMT"}, {"version": "v2", "created": "Wed, 13 Apr 2011 03:41:54 GMT"}, {"version": "v3", "created": "Sun, 2 Oct 2011 19:07:38 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Goerg", "Georg M.", ""]]}, {"id": "1103.3817", "submitter": "Anuj Srivastava", "authors": "Anuj Srivastava and Wei Wu and Sebastian Kurtek and Eric Klassen and\n  J. S. Marron", "title": "Registration of Functional Data Using Fisher-Rao Metric", "comments": "Revised paper. More focused on a subproblem and more theoretical\n  results", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel geometric framework for separating the phase and the\namplitude variability in functional data of the type frequently studied in\ngrowth curve analysis. This framework uses the Fisher-Rao Riemannian metric to\nderive a proper distance on the quotient space of functions modulo the\ntime-warping group. A convenient square-root velocity function (SRVF)\nrepresentation transforms the Fisher-Rao metric into the standard $\\ltwo$\nmetric, simplifying the computations. This distance is then used to define a\nKarcher mean template and warp the individual functions to align them with the\nKarcher mean template. The strength of this framework is demonstrated by\nderiving a consistent estimator of a signal observed under random warping,\nscaling, and vertical translation. These ideas are demonstrated using both\nsimulated and real data from different application domains: the Berkeley growth\nstudy, handwritten signature curves, neuroscience spike trains, and gene\nexpression signals. The proposed method is empirically shown to be be superior\nin performance to several recently published methods for functional alignment.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2011 23:00:07 GMT"}, {"version": "v2", "created": "Mon, 16 May 2011 18:32:25 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Srivastava", "Anuj", ""], ["Wu", "Wei", ""], ["Kurtek", "Sebastian", ""], ["Klassen", "Eric", ""], ["Marron", "J. S.", ""]]}, {"id": "1103.4601", "submitter": "Lihong Li", "authors": "Miroslav Dudik and John Langford and Lihong Li", "title": "Doubly Robust Policy Evaluation and Learning", "comments": "Published at ICML 2011, 8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study decision making in environments where the reward is only partially\nobserved, but can be modeled as a function of an action and an observed\ncontext. This setting, known as contextual bandits, encompasses a wide variety\nof applications including health-care policy and Internet advertising. A\ncentral task is evaluation of a new policy given historic data consisting of\ncontexts, actions and received rewards. The key challenge is that the past data\ntypically does not faithfully represent proportions of actions taken by a new\npolicy. Previous approaches rely either on models of rewards or models of the\npast policy. The former are plagued by a large bias whereas the latter have a\nlarge variance.\n  In this work, we leverage the strength and overcome the weaknesses of the two\napproaches by applying the doubly robust technique to the problems of policy\nevaluation and optimization. We prove that this approach yields accurate value\nestimates when we have either a good (but not necessarily consistent) model of\nrewards or a good (but not necessarily consistent) model of past policy.\nExtensive empirical comparison demonstrates that the doubly robust approach\nuniformly improves over existing techniques, achieving both lower variance in\nvalue estimation and better policies. As such, we expect the doubly robust\napproach to become common practice.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2011 19:37:45 GMT"}, {"version": "v2", "created": "Fri, 6 May 2011 02:38:18 GMT"}], "update_date": "2011-05-09", "authors_parsed": [["Dudik", "Miroslav", ""], ["Langford", "John", ""], ["Li", "Lihong", ""]]}, {"id": "1103.4866", "submitter": "Andres Christen", "authors": "Marcos Capistr\\'an and J. Andr\\'es Christen", "title": "A Generic Multivariate Distribution for Counting Data", "comments": "9 pages, 1 fgure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the need, in some Bayesian likelihood free inference problems,\nof imputing a multivariate counting distribution based on its vector of means\nand variance-covariance matrix, we define a generic multivariate discrete\ndistribution. Based on blending the Binomial, Poisson and Negative-Binomial\ndistributions, and using a normal multivariate copula, the required\ndistribution is defined. This distribution tends to the Multivariate Normal for\nlarge counts and has an approximate pmf version that is quite simple to\nevaluate.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2011 20:17:17 GMT"}], "update_date": "2011-03-28", "authors_parsed": [["Capistr\u00e1n", "Marcos", ""], ["Christen", "J. Andr\u00e9s", ""]]}, {"id": "1103.5142", "submitter": "Paolo Braca Paolo Braca", "authors": "Paolo Braca, Stefano Marano, and Vincenzo Matta", "title": "Asymptotic Properties of One-Bit Distributed Detection with Ordered\n  Transmissions", "comments": "Submitted to IEEE Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.MA math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a sensor network made of remote nodes connected to a common fusion\ncenter. In a recent work Blum and Sadler [1] propose the idea of ordered\ntransmissions -sensors with more informative samples deliver their messages\nfirst- and prove that optimal detection performance can be achieved using only\na subset of the total messages. Taking to one extreme this approach, we show\nthat just a single delivering allows making the detection errors as small as\ndesired, for a sufficiently large network size: a one-bit detection scheme can\nbe asymptotically consistent. The transmission ordering is based on the modulus\nof some local statistic (MO system). We derive analytical results proving the\nasymptotic consistency and, for the particular case that the local statistic is\nthe log-likelihood (\\ell-MO system), we also obtain a bound on the error\nconvergence rate. All the theorems are proved under the general setup of random\nnumber of sensors. Computer experiments corroborate the analysis and address\ntypical examples of applications including: non-homogeneous Poisson-deployed\nnetworks, detection by per-sensor censoring, monitoring of energy-constrained\nphenomenon.\n", "versions": [{"version": "v1", "created": "Sat, 26 Mar 2011 16:48:29 GMT"}], "update_date": "2011-03-29", "authors_parsed": [["Braca", "Paolo", ""], ["Marano", "Stefano", ""], ["Matta", "Vincenzo", ""]]}, {"id": "1103.5178", "submitter": "Zack Almquist", "authors": "Zack W. Almquist and Carter T. Butts", "title": "Logistic Network Regression for Scalable Analysis of Networks with Joint\n  Edge/Vertex Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": "IMBS Technical Report MBS 11-03, University of California, Irvine", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network dynamics may be viewed as a process of change in the edge structure\nof a network, in the vertex set on which edges are defined, or in both\nsimultaneously. Though early studies of such processes were primarily\ndescriptive, recent work on this topic has increasingly turned to formal\nstatistical models. While showing great promise, many of these modern dynamic\nmodels are computationally intensive and scale very poorly in the size of the\nnetwork under study and/or the number of time points considered. Likewise,\ncurrently employed models focus on edge dynamics, with little support for\nendogenously changing vertex sets. Here, we show how an existing approach based\non logistic network regression can be extended to serve as highly scalable\nframework for modeling large networks with dynamic vertex sets. We place this\napproach within a general dynamic exponential family (ERGM) context, clarifying\nthe assumptions underlying the framework (and providing a clear path for\nextensions), and show how model assessment methods for cross-sectional networks\ncan be extended to the dynamic case. Finally, we illustrate this approach on a\nclassic data set involving interactions among windsurfers on a California\nbeach.\n", "versions": [{"version": "v1", "created": "Sun, 27 Mar 2011 02:13:05 GMT"}], "update_date": "2011-03-29", "authors_parsed": [["Almquist", "Zack W.", ""], ["Butts", "Carter T.", ""]]}, {"id": "1103.5268", "submitter": "David Bortz", "authors": "David Bortz and Andrew Christlieb", "title": "Scandalously Parallelizable Mesh Generation", "comments": "21 pages, 25 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach which employs random sampling to generate an\naccurate non-uniform mesh for numerically solving Partial Differential Equation\nBoundary Value Problems (PDE-BVP's). From a uniform probability distribution U\nover a 1D domain, we sample M discretizations of size N where M>>N. The\nstatistical moments of the solutions to a given BVP on each of the M\nultra-sparse meshes provide insight into identifying highly accurate\nnon-uniform meshes. Essentially, we use the pointwise mean and variance of the\ncoarse-grid solutions to construct a mapping Q(x) from uniformly to\nnon-uniformly spaced mesh-points. The error convergence properties of the\napproximate solution to the PDE-BVP on the non-uniform mesh are superior to a\nuniform mesh for a certain class of BVP's. In particular, the method works well\nfor BVP's with locally non-smooth solutions. We present a framework for\nstudying the sampled sparse-mesh solutions and provide numerical evidence for\nthe utility of this approach as applied to a set of example BVP's. We conclude\nwith a discussion of how the near-perfect paralellizability of our approach\nsuggests that these strategies have the potential for highly efficient\nutilization of massively parallel multi-core technologies such as General\nPurpose Graphics Processing Units (GPGPU's). We believe that the proposed\nalgorithm is beyond embarrassingly parallel; implementing it on anything but a\nmassively multi-core architecture would be scandalous.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2011 01:21:08 GMT"}], "update_date": "2011-03-29", "authors_parsed": [["Bortz", "David", ""], ["Christlieb", "Andrew", ""]]}, {"id": "1103.5904", "submitter": "Nicolas Barbey", "authors": "Nicolas Barbey and Chlo\\'e Guennou and Fr\\'ed\\'eric Auch\\`ere", "title": "TomograPy: A Fast, Instrument-Independent, Solar Tomography Software", "comments": "21 pages, 6 figures, 5 tables", "journal-ref": null, "doi": "10.1007/s11207-011-9792-8", "report-no": null, "categories": "astro-ph.SR astro-ph.IM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solar tomography has progressed rapidly in recent years thanks to the\ndevelopment of robust algorithms and the availability of more powerful\ncomputers. It can today provide crucial insights in solving issues related to\nthe line-of-sight integration present in the data of solar imagers and\ncoronagraphs. However, there remain challenges such as the increase of the\navailable volume of data, the handling of the temporal evolution of the\nobserved structures, and the heterogeneity of the data in multi-spacecraft\nstudies.\n  We present a generic software package that can perform fast tomographic\ninversions that scales linearly with the number of measurements, linearly with\nthe length of the reconstruction cube (and not the number of voxels) and\nlinearly with the number of cores and can use data from different sources and\nwith a variety of physical models: TomograPy\n(http://nbarbey.github.com/TomograPy/), an open-source software freely\navailable on the Python Package Index. For performance, TomograPy uses a\nparallelized-projection algorithm. It relies on the World Coordinate System\nstandard to manage various data sources. A variety of inversion algorithms are\nprovided to perform the tomographic-map estimation. A test suite is provided\nalong with the code to ensure software quality. Since it makes use of the\nSiddon algorithm it is restricted to rectangular parallelepiped voxels but the\nspherical geometry of the corona can be handled through proper use of priors.\n  We describe the main features of the code and show three practical examples\nof multi-spacecraft tomographic inversions using STEREO/EUVI and STEREO/COR1\ndata. Static and smoothly varying temporal evolution models are presented.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2011 12:23:33 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Barbey", "Nicolas", ""], ["Guennou", "Chlo\u00e9", ""], ["Auch\u00e8re", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1103.5946", "submitter": "An Zeng", "authors": "Zhifang Li, Yanqing Hu, Beishan Xu, Zengru Di, Ying Fan", "title": "Detecting the optimal number of communities in complex networks", "comments": "8 pages, 6 figs", "journal-ref": null, "doi": "10.1016/j.physa.2011.06.023", "report-no": null, "categories": "physics.soc-ph cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To obtain the optimal number of communities is an important problem in\ndetecting community structure. In this paper, we extend the measurement of\ncommunity detecting algorithms to find the optimal community number. Based on\nthe normalized mutual information index, which has been used as a measure for\nsimilarity of communities, a statistic $\\Omega(c)$ is proposed to detect the\noptimal number of communities. In general, when $\\Omega(c)$ reaches its local\nmaximum, especially the first one, the corresponding number of communities\n\\emph{c} is likely to be optimal in community detection. Moreover, the\nstatistic $\\Omega(c)$ can also measure the significance of community structures\nin complex networks, which has been paid more attention recently. Numerical and\nempirical results show that the index $\\Omega(c)$ is effective in both\nartificial and real world networks.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2011 14:43:59 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Li", "Zhifang", ""], ["Hu", "Yanqing", ""], ["Xu", "Beishan", ""], ["Di", "Zengru", ""], ["Fan", "Ying", ""]]}, {"id": "1103.6034", "submitter": "Joseph Richards", "authors": "Joseph W. Richards, Darren Homrighausen, Peter E. Freeman, Chad M.\n  Schafer, and Dovi Poznanski", "title": "Semi-supervised Learning for Photometric Supernova Classification", "comments": "16 pages, 11 figures, accepted for publication in MNRAS", "journal-ref": null, "doi": "10.1111/j.1365-2966.2011.19768.x", "report-no": null, "categories": "astro-ph.IM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a semi-supervised method for photometric supernova typing. Our\napproach is to first use the nonlinear dimension reduction technique diffusion\nmap to detect structure in a database of supernova light curves and\nsubsequently employ random forest classification on a spectroscopically\nconfirmed training set to learn a model that can predict the type of each newly\nobserved supernova. We demonstrate that this is an effective method for\nsupernova typing. As supernova numbers increase, our semi-supervised method\nefficiently utilizes this information to improve classification, a property not\nenjoyed by template based methods. Applied to supernova data simulated by\nKessler et al. (2010b) to mimic those of the Dark Energy Survey, our methods\nachieve (cross-validated) 95% Type Ia purity and 87% Type Ia efficiency on the\nspectroscopic sample, but only 50% Type Ia purity and 50% efficiency on the\nphotometric sample due to their spectroscopic follow-up strategy. To improve\nthe performance on the photometric sample, we search for better spectroscopic\nfollow-up procedures by studying the sensitivity of our machine learned\nsupernova classification on the specific strategy used to obtain training sets.\nWith a fixed amount of spectroscopic follow-up time, we find that deeper\nmagnitude-limited spectroscopic surveys are better for producing training sets.\nFor supernova Ia (II-P) typing, we obtain a 44% (1%) increase in purity to 72%\n(87%) and 30% (162%) increase in efficiency to 65% (84%) of the sample using a\n25th (24.5th) magnitude-limited survey instead of the shallower spectroscopic\nsample used in the original simulations. When redshift information is\navailable, we incorporate it into our analysis using a novel method of altering\nthe diffusion map representation of the supernovae. Incorporating host\nredshifts leads to a 5% improvement in Type Ia purity and 13% improvement in\nType Ia efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2011 20:00:03 GMT"}, {"version": "v2", "created": "Tue, 27 Sep 2011 21:23:02 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Richards", "Joseph W.", ""], ["Homrighausen", "Darren", ""], ["Freeman", "Peter E.", ""], ["Schafer", "Chad M.", ""], ["Poznanski", "Dovi", ""]]}]