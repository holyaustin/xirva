[{"id": "1212.0018", "submitter": "Simon DeDeo", "authors": "Simon DeDeo", "title": "Collective Phenomena and Non-Finite State Computation in a Human Social\n  System", "comments": "23 pages, 4 figures, 3 tables; to appear in PLoS ONE", "journal-ref": "PLoS ONE 8(10): e75818 (2013)", "doi": "10.1371/journal.pone.0075818", "report-no": null, "categories": "cs.SI cs.FL nlin.AO physics.soc-ph q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the computational structure of a paradigmatic example of\ndistributed social interaction: that of the open-source Wikipedia community. We\nexamine the statistical properties of its cooperative behavior, and perform\nmodel selection to determine whether this aspect of the system can be described\nby a finite-state process, or whether reference to an effectively unbounded\nresource allows for a more parsimonious description. We find strong evidence,\nin a majority of the most-edited pages, in favor of a collective-state model,\nwhere the probability of a \"revert\" action declines as the square root of the\nnumber of non-revert actions seen since the last revert. We provide evidence\nthat the emergence of this social counter is driven by collective interaction\neffects, rather than properties of individual users.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2012 21:22:24 GMT"}, {"version": "v2", "created": "Tue, 18 Dec 2012 17:26:18 GMT"}, {"version": "v3", "created": "Thu, 19 Sep 2013 17:11:27 GMT"}], "update_date": "2013-10-14", "authors_parsed": [["DeDeo", "Simon", ""]]}, {"id": "1212.0178", "submitter": "Alexander Blocker", "authors": "Edoardo M. Airoldi and Alexander W. Blocker", "title": "Estimating latent processes on a network from indirect measurements", "comments": "39 pages, 6 figures, 4 tables. Journal of the American Statistical\n  Association. To appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a communication network, point-to-point traffic volumes over time are\ncritical for designing protocols that route information efficiently and for\nmaintaining security, whether at the scale of an internet service provider or\nwithin a corporation. While technically feasible, the direct measurement of\npoint-to-point traffic imposes a heavy burden on network performance and is\ntypically not implemented. Instead, indirect aggregate traffic volumes are\nroutinely collected. We consider the problem of estimating point-to-point\ntraffic volumes, x_t, from aggregate traffic volumes, y_t, given information\nabout the network routing protocol encoded in a matrix A. This estimation task\ncan be reformulated as finding the solutions to a sequence of ill-posed linear\ninverse problems, y_t = A x_t, since the number of origin-destination routes of\ninterest is higher than the number of aggregate measurements available.\n  Here, we introduce a novel multilevel state-space model of aggregate traffic\nvolumes with realistic features. We implement a naive strategy for estimating\nunobserved point-to-point traffic volumes from indirect measurements of\naggregate traffic, based on particle filtering. We then develop a more\nefficient two-stage inference strategy that relies on model-based\nregularization: a simple model is used to calibrate regularization parameters\nthat lead to efficient and scalable inference in the multilevel state-space\nmodel. We apply our methods to corporate and academic networks, where we show\nthat the proposed inference strategy outperforms existing approaches and scales\nto larger networks. We also design a simulation study to explore the factors\nthat influence the performance. Our results suggest that model-based\nregularization may be an efficient strategy for inference in other complex\nmultilevel models.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2012 03:06:09 GMT"}], "update_date": "2012-12-04", "authors_parsed": [["Airoldi", "Edoardo M.", ""], ["Blocker", "Alexander W.", ""]]}, {"id": "1212.0181", "submitter": "Bin Zhu", "authors": "Bin Zhu and David B. Dunson", "title": "Stochastic Volatility Regression for Functional Data Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Although there are many methods for functional data analysis (FDA), little\nemphasis is put on characterizing variability among volatilities of individual\nfunctions. In particular, certain individuals exhibit erratic swings in their\ntrajectory while other individuals have more stable trajectories. There is\nevidence of such volatility heterogeneity in blood pressure trajectories during\npregnancy, for example, and reason to suspect that volatility is a biologically\nimportant feature. Most FDA models implicitly assume similar or identical\nsmoothness of the individual functions, and hence can lead to misleading\ninferences on volatility and an inadequate representation of the functions. We\npropose a novel class of FDA models characterized using hierarchical stochastic\ndifferential equations. We model the derivatives of a mean function and\ndeviation functions using Gaussian processes, while also allowing covariate\ndependence including on the volatilities of the deviation functions. Following\na Bayesian approach to inference, a Markov chain Monte Carlo algorithm is used\nfor posterior computation. The methods are tested on simulated data and applied\nto blood pressure trajectories during pregnancy.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2012 03:13:59 GMT"}], "update_date": "2012-12-04", "authors_parsed": [["Zhu", "Bin", ""], ["Dunson", "David B.", ""]]}, {"id": "1212.0304", "submitter": "Lutz Bornmann Dr.", "authors": "Lutz Bornmann, Moritz Stefaner, Felix de Moya Anegon, Ruediger Mutz", "title": "Ranking and mapping of universities and research-focused institutions\n  worldwide based on highly-cited papers: A visualization of results from\n  multi-level models", "comments": "Accepted for publication in the journal Online Information Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The web application presented in this paper allows for an analysis to reveal\ncentres of excellence in different fields worldwide using publication and\ncitation data. Only specific aspects of institutional performance are taken\ninto account and other aspects such as teaching performance or societal impact\nof research are not considered. Based on data gathered from Scopus,\nfield-specific excellence can be identified in institutions where highly-cited\npapers have been frequently published. The web application combines both a list\nof institutions ordered by different indicator values and a map with circles\nvisualizing indicator values for geocoded institutions. Compared to the mapping\nand ranking approaches introduced hitherto, our underlying statistics\n(multi-level models) are analytically oriented by allowing (1) the estimation\nof values for the number of excellent papers for an institution which are\nstatistically more appropriate than the observed values; (2) the calculation of\nconfidence intervals as measures of accuracy for the institutional citation\nimpact; (3) the comparison of a single institution with an \"average\"\ninstitution in a subject area, and (4) the direct comparison of at least two\ninstitutions.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2012 08:07:20 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2013 06:34:03 GMT"}], "update_date": "2013-07-25", "authors_parsed": [["Bornmann", "Lutz", ""], ["Stefaner", "Moritz", ""], ["Anegon", "Felix de Moya", ""], ["Mutz", "Ruediger", ""]]}, {"id": "1212.0378", "submitter": "Silvia Bacci Dr", "authors": "Michela Gnaldi, Francesco Bartolucci, Silvia Bacci", "title": "Joint Assessment of the Differential Item Functioning and Latent Trait\n  Dimensionality of Students' National Tests", "comments": "30 pages, 3 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the educational context, students' assessment tests are routinely\nvalidated through Item Response Theory (IRT) models which assume\nunidimensionality and absence of Differential Item Functioning (DIF). In this\npaper, we investigate if such assumptions hold for two national tests\nadministered in Italy to middle school students in June 2009: the Italian Test\nand the Mathematics Test. To this aim, we rely on an extended class of\nmultidimensional latent class IRT models characterised by: (i) a two-parameter\nlogistic parameterisation for the conditional probability of a correct\nresponse, (ii) latent traits represented through a random vector with a\ndiscrete distribution, and (iii) the inclusion of (uniform) DIF to account for\nstudents' gender and geographical area. A classification of the items into\nunidimensional groups is also proposed and represented by a dendrogram, which\nis obtained from a hierarchical clustering algorithm. The results provide\nevidence for DIF effects for both Tests. Besides, the assumption of\nunidimensionality is strongly rejected for the Italian Test, whereas it is\nreasonable for the Mathematics Test.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2012 13:17:48 GMT"}], "update_date": "2012-12-04", "authors_parsed": [["Gnaldi", "Michela", ""], ["Bartolucci", "Francesco", ""], ["Bacci", "Silvia", ""]]}, {"id": "1212.0451", "submitter": "Sirisha Rambhatla", "authors": "Sirisha Rambhatla and Jarvis D. Haupt", "title": "Semi-blind Source Separation via Sparse Representations and Online\n  Dictionary Learning", "comments": "5 pages, In Proceedings of the 47th Asilomar Conference on Signals\n  Systems and Computers, 2013", "journal-ref": null, "doi": "10.1109/ACSSC.2013.6810587", "report-no": null, "categories": "cs.SD stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work examines a semi-blind single-channel source separation problem. Our\nspecific aim is to separate one source whose local structure is approximately\nknown, from another a priori unspecified background source, given only a single\nlinear combination of the two sources. We propose a separation technique based\non local sparse approximations along the lines of recent efforts in sparse\nrepresentations and dictionary learning. A key feature of our procedure is the\nonline learning of dictionaries (using only the data itself) to sparsely model\nthe background source, which facilitates its separation from the\npartially-known source. Our approach is applicable to source separation\nproblems in various application domains; here, we demonstrate the performance\nof our proposed approach via simulation on a stylized audio source separation\ntask.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2012 17:06:41 GMT"}, {"version": "v2", "created": "Sun, 25 Jan 2015 02:31:09 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Rambhatla", "Sirisha", ""], ["Haupt", "Jarvis D.", ""]]}, {"id": "1212.0462", "submitter": "Bailey Fosdick", "authors": "Bailey K. Fosdick and Adrian E. Raftery", "title": "Regional Probabilistic Fertility Forecasting by Modeling Between-Country\n  Correlations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The United Nations (UN) Population Division is considering producing\nprobabilistic projections for the total fertility rate (TFR) using the Bayesian\nhierarchical model of Alkema et al. (2011), which produces predictive\ndistributions of TFR for individual countries. The UN is interested in\npublishing probabilistic projections for aggregates of countries, such as\nregions and trading blocs. This requires joint probabilistic projections of\nfuture country-specific TFRs, taking account of the correlations between them.\nWe propose an extension of the Bayesian hierarchical model that allows for\nprobabilistic projection of TFR for any set of countries. We model the\ncorrelation between country forecast errors as a linear function of time\ninvariant covariates, namely whether the countries are contiguous, whether they\nhad a common colonizer after 1945, and whether they are in the same UN region.\nThe resulting correlation model is incorporated into the Bayesian hierarchical\nmodel's error distribution. We produce predictive distributions of TFR for\n1990-2010 for each of the UN's primary regions. We find that the proportions of\nthe observed values that fall within the prediction intervals from our method\nare closer to their nominal levels than those produced by the current model.\nOur results suggest that a significant proportion of the correlation between\nforecast errors for TFR in different countries is due to countries' geographic\nproximity to one another, and that if this correlation is accounted for, the\nquality of probabilitistic projections of TFR for regions and other aggregates\nis improved.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2012 17:40:41 GMT"}], "update_date": "2012-12-04", "authors_parsed": [["Fosdick", "Bailey K.", ""], ["Raftery", "Adrian E.", ""]]}, {"id": "1212.0645", "submitter": "Maria S\\\"uveges Dr", "authors": "M. S\\\"uveges", "title": "Extreme-value modelling for the significance assessment of periodogram\n  peaks", "comments": "16 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I propose a new procedure to estimate the False Alarm Probability, the\nmeasure of significance for peaks of periodograms. The key element of the new\nprocedure is the use of generalized extreme-value distributions, the limiting\ndistribution for maxima of variables from most continuous distributions. This\ntechnique allows reliable extrapolation to the very high probability levels\nrequired by multiple hypothesis testing, and enables the derivation of\nconfidence intervals of the estimated levels. The estimates are stable against\ndeviations from distributional assumptions, which are otherwise usually made\neither about the observations themselves or about the theoretical univariate\ndistribution of the periodogram. The quality and the performance of the\nprocedure is demonstrated on simulations and on two multimode variable stars\nfrom Sloan Digital Sky Survey Stripe 82.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2012 09:07:15 GMT"}], "update_date": "2012-12-05", "authors_parsed": [["S\u00fcveges", "M.", ""]]}, {"id": "1212.0849", "submitter": "Sinan Yildirim", "authors": "Sinan Yildirim, Lan Jiang, Sumeetpal S. Singh, Tom Dean", "title": "Estimating the Static Parameters in Linear Gaussian Multiple Target\n  Tracking Models", "comments": "17 double column pages, 9 figures", "journal-ref": null, "doi": null, "report-no": "CUED/F-INFENG/TR.681", "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present both offline and online maximum likelihood estimation (MLE)\ntechniques for inferring the static parameters of a multiple target tracking\n(MTT) model with linear Gaussian dynamics. We present the batch and online\nversions of the expectation-maximisation (EM) algorithm for short and long data\nsets respectively, and we show how Monte Carlo approximations of these methods\ncan be implemented. Performance is assessed in numerical examples using\nsimulated data for various scenarios and a comparison with a Bayesian\nestimation procedure is also provided.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2012 20:51:07 GMT"}], "update_date": "2014-10-09", "authors_parsed": [["Yildirim", "Sinan", ""], ["Jiang", "Lan", ""], ["Singh", "Sumeetpal S.", ""], ["Dean", "Tom", ""]]}, {"id": "1212.1224", "submitter": "Dmitry I. Podolsky", "authors": "Dmitry Podolsky and Konstantin Turitsyn", "title": "Random load fluctuations and collapse probability of a power system\n  operating near codimension 1 saddle-node bifurcation", "comments": "5 pages, 1 figure, submission to IEEE PES General Meeting 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.SY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a power system operating in the vicinity of the power transfer limit of\nits transmission system, effect of stochastic fluctuations of power loads can\nbecome critical as a sufficiently strong such fluctuation may activate voltage\ninstability and lead to a large scale collapse of the system. Considering the\neffect of these stochastic fluctuations near a codimension 1 saddle-node\nbifurcation, we explicitly calculate the autocorrelation function of the state\nvector and show how its behavior explains the phenomenon of critical\nslowing-down often observed for power systems on the threshold of blackout. We\nalso estimate the collapse probability/mean clearing time for the power system\nand construct a new indicator function signaling the proximity to a large scale\ncollapse. The new indicator function is easy to estimate in real time using PMU\ndata feeds as well as SCADA information about fluctuations of power load on the\nnodes of the power grid. We discuss control strategies leading to the\nminimization of the collapse probability.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2012 02:46:08 GMT"}], "update_date": "2012-12-07", "authors_parsed": [["Podolsky", "Dmitry", ""], ["Turitsyn", "Konstantin", ""]]}, {"id": "1212.1339", "submitter": "Nataliya Rybnikova", "authors": "Svyatoslav Rybnikov and Nataliya Rybnikova", "title": "Using Correlation Adaptometry Method in Assessing Societal Stress: a\n  Ukrainian Case", "comments": "19 pages, 4 figures, 3 tables, 2 appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Societal stress may cause far reaching political, economic and even\ngeological effects. Nevertheless, it is still scarcely investigated, contrary\nto social stress, which an individual faces in their interactions within a\nsociety. It is natural to suppose that in its adaptation, society demonstrates\nthe same objective laws that biological population does, since they are, in\nfact, the closest systems. In the survey, the hypothesis is tested that the\ncollective stress effect holds true in society, which must appear (as it\nhappens according to correlation adaptometry method in biological systems) in\nescalation of both correlations between societal characteristics and their\ndispersion. Both tends are observed in Ukrainian society during 2009-2012, as a\nresult of political elections that affect societal anxiety.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2012 14:47:44 GMT"}], "update_date": "2012-12-07", "authors_parsed": [["Rybnikov", "Svyatoslav", ""], ["Rybnikova", "Nataliya", ""]]}, {"id": "1212.1440", "submitter": "Richard Warr", "authors": "Richard L. Warr and David H. Collins", "title": "A Comprehensive Method for Solving Finite-State Semi-Markov Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-Markov processes (SMPs) provide a rich framework for many real-world\nproblems. However, due to difficulty implementing practical solutions they are\nrarely used with their full capability. The theory of SMPs is quite mature but\nwas mainly developed at a time when computational resources were not widely\navailable. With the exception of some of the simplest cases, solutions to SMPs\nare inherently numerical, and SMPs have been underutilized by practitioners\nbecause of difficulty implementing the theory in applications. This paper\ndemonstrates the theory and computational methods needed to implement SMP\nmodels in practical settings. Methods are illustrated with an application\nmodeling the movement of coronary patients in a hospital. Our aim is to allow\npractitioners to use richer SMP models without being burdened with the rigorous\nmathematical theory.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2012 20:29:13 GMT"}, {"version": "v2", "created": "Thu, 18 Sep 2014 14:44:31 GMT"}, {"version": "v3", "created": "Mon, 17 May 2021 16:16:02 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Warr", "Richard L.", ""], ["Collins", "David H.", ""]]}, {"id": "1212.1778", "submitter": "Vittorio Perduca", "authors": "The Minh Luong, Vittorio Perduca, Gregory Nuel", "title": "Hidden Markov Model Applications in Change-Point Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection of change-points in heterogeneous sequences is a statistical\nchallenge with many applications in fields such as finance, signal analysis and\nbiology. A wide variety of literature exists for finding an ideal set of\nchange-points for characterizing the data. In this tutorial we elaborate on the\nHidden Markov Model (HMM) and present two different frameworks for applying HMM\nto change-point models. Then we provide a summary of two procedures for\ninference in change-point analysis, which are particular cases of the\nforward-backward algorithm for HMMs, and discuss common implementation\nproblems. Lastly, we provide two examples of the HMM methods on available data\nsets and we shortly discuss about the applications to current genomics studies.\nThe R code used in the examples is provided in the appendix.\n", "versions": [{"version": "v1", "created": "Sat, 8 Dec 2012 11:07:03 GMT"}], "update_date": "2012-12-11", "authors_parsed": [["Luong", "The Minh", ""], ["Perduca", "Vittorio", ""], ["Nuel", "Gregory", ""]]}, {"id": "1212.1779", "submitter": "Marco Iglesias", "authors": "Marco A. Iglesias, Kody J.H. Law and Andrew M. Stuart", "title": "Evaluation of Gaussian approximations for data assimilation in reservoir\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose to numerically assess the performance of standard\nGaussian approximations to probe the posterior distribution that arises from\nBayesian data assimilation in petroleum reservoirs. In particular we assess the\nperformance of (i) the linearization around the maximum a posterior estimate,\n(ii) the randomized maximum likelihood and (iii) standard ensemble Kalman\nfilter-type methods. In order to fully resolve the posterior distribution we\nimplement a state-of-the art MCMC method that scales well with respect to the\ndimension of the parameter space. Our implementation of the MCMC method\nprovides the gold standard against which to assess the aforementioned Gaussian\napproximations. We present numerical synthetic experiments where we quantify\nthe capability of each of the {\\em ad hoc} Gaussian approximation in\nreproducing the mean and the variance of the posterior distribution\n(characterized via MCMC) associated to a data assimilation problem. The main\nobjective of our controlled experiments is to exhibit the substantial\ndiscrepancies of the approximation properties of standard {\\em ad hoc} Gaussian\napproximations. Numerical investigations of the type we present here will lead\nto greater understanding of the cost-efficient, but {\\em ad hoc}, Bayesian\ntechniques used for data assimilation in petroleum reservoirs, and hence\nultimately to improved techniques with more accurate uncertainty\nquantification.\n", "versions": [{"version": "v1", "created": "Sat, 8 Dec 2012 11:29:27 GMT"}], "update_date": "2012-12-11", "authors_parsed": [["Iglesias", "Marco A.", ""], ["Law", "Kody J. H.", ""], ["Stuart", "Andrew M.", ""]]}, {"id": "1212.1829", "submitter": "Grigory Sokolov", "authors": "Alexander G. Tartakovsky, Aleksey S. Polunchenko, Grigory Sokolov", "title": "Efficient Computer Network Anomaly Detection by Changepoint Detection\n  Methods", "comments": "7 pages, 6 figures, to appear in \"IEEE Journal of Selected Topics in\n  Signal Processing\"", "journal-ref": "IEEE Journal of Selected Topics in Signal Processing, vol.7, no.1,\n  pp.7-11, February 2013", "doi": "10.1109/JSTSP.2012.2233713", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of efficient on-line anomaly detection in computer\nnetwork traffic. The problem is approached statistically, as that of sequential\n(quickest) changepoint detection. A multi-cyclic setting of quickest change\ndetection is a natural fit for this problem. We propose a novel score-based\nmulti-cyclic detection algorithm. The algorithm is based on the so-called\nShiryaev-Roberts procedure. This procedure is as easy to employ in practice and\nas computationally inexpensive as the popular Cumulative Sum chart and the\nExponentially Weighted Moving Average scheme. The likelihood ratio based\nShiryaev-Roberts procedure has appealing optimality properties, particularly it\nis exactly optimal in a multi-cyclic setting geared to detect a change\noccurring at a far time horizon. It is therefore expected that an intrusion\ndetection algorithm based on the Shiryaev-Roberts procedure will perform better\nthan other detection schemes. This is confirmed experimentally for real traces.\nWe also discuss the possibility of complementing our anomaly detection\nalgorithm with a spectral-signature intrusion detection system with false alarm\nfiltering and true attack confirmation capability, so as to obtain a\nsynergistic system.\n", "versions": [{"version": "v1", "created": "Sat, 8 Dec 2012 19:16:07 GMT"}], "update_date": "2013-07-23", "authors_parsed": [["Tartakovsky", "Alexander G.", ""], ["Polunchenko", "Aleksey S.", ""], ["Sokolov", "Grigory", ""]]}, {"id": "1212.2044", "submitter": "Gabriel Kronberger", "authors": "Gabriel Kronberger, Stefan Fink, Michael Kommenda and Michael\n  Affenzeller", "title": "Macro-Economic Time Series Modeling and Interaction Networks", "comments": "The original publication is available at\n  http://link.springer.com/chapter/10.1007/978-3-642-20520-0_11", "journal-ref": "Applications of Evolutionary Computation, LNCS 6625 (Springer\n  Berlin Heidelberg), pp. 101-110 (2011)", "doi": "10.1007/978-3-642-20520-0_11", "report-no": null, "categories": "cs.NE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Macro-economic models describe the dynamics of economic quantities. The\nestimations and forecasts produced by such models play a substantial role for\nfinancial and political decisions. In this contribution we describe an approach\nbased on genetic programming and symbolic regression to identify variable\ninteractions in large datasets. In the proposed approach multiple symbolic\nregression runs are executed for each variable of the dataset to find\npotentially interesting models. The result is a variable interaction network\nthat describes which variables are most relevant for the approximation of each\nvariable of the dataset. This approach is applied to a macro-economic dataset\nwith monthly observations of important economic indicators in order to identify\npotentially interesting dependencies of these indicators. The resulting\ninteraction network of macro-economic indicators is briefly discussed and two\nof the identified models are presented in detail. The two models approximate\nthe help wanted index and the CPI inflation in the US.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2012 12:04:58 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2013 16:58:12 GMT"}], "update_date": "2013-09-24", "authors_parsed": [["Kronberger", "Gabriel", ""], ["Fink", "Stefan", ""], ["Kommenda", "Michael", ""], ["Affenzeller", "Michael", ""]]}, {"id": "1212.2358", "submitter": "Anne Gegout-Petit Dr", "authors": "Camille Baysse and Didier Bihannic and Anne G\\'egout-Petit and Michel\n  Prenat and J\\'er\\^ome Saracco", "title": "Hidden Markov Model for the detection of a degraded operating mode of\n  optronic equipment", "comments": "11 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As part of optimizing the reliability, Thales Optronics now includes systems\nthat examine the state of its equipment. The aim of this paper is to use hidden\nMarkov Model to detect as soon as possible a change of state of optronic\nequipment in order to propose maintenance before failure. For this, we\ncarefully observe the dynamic of a variable called \"cool down time\" and noted\nTmf, which reflects the state of the cooling system. Indeed, the Tmf is an\nindirect observation of the hidden state of the system. This one is modelled by\na Markov chain and the Tmf is a noisy function of it. Thanks to filtering\nequations, we obtain results on the probability that an appliance is in\ndegraded state at time $t$, knowing the history of the Tmf until this moment.\nWe have evaluated the numerical behavior of our approach on simulated data.\nThen we have applied this methodology on our real data and we have checked that\nthe results are consistent with the reality. This method can be implemented in\na HUMS (Health and Usage Monitoring System). This simple example of HUMS would\nallow the Thales Optronics Company to improve its maintenance system. This\ncompany will be able to recall appliances which are estimated to be in degraded\nstate and do not control to soon those estimated in stable state.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2012 09:56:34 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Baysse", "Camille", ""], ["Bihannic", "Didier", ""], ["G\u00e9gout-Petit", "Anne", ""], ["Prenat", "Michel", ""], ["Saracco", "J\u00e9r\u00f4me", ""]]}, {"id": "1212.2393", "submitter": "Halis Sak", "authors": "Halis Sak and Wolfgang H\\\"ormann", "title": "Simulating the Continuation of a Time Series in R", "comments": "9 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The simulation of the continuation of a given time series is useful for many\npractical applications. But no standard procedure for this task is suggested in\nthe literature. It is therefore demonstrated how to use the seasonal ARIMA\nprocess to simulate the continuation of an observed time series. The R-code\npresented uses well-known modeling procedures for ARIMA models and conditional\nsimulation of a SARIMA model with known parameters. A small example\ndemonstrates the correctness and practical relevance of the new idea.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2012 11:56:34 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Sak", "Halis", ""], ["H\u00f6rmann", "Wolfgang", ""]]}, {"id": "1212.2467", "submitter": "Darya Chudova", "authors": "Darya Chudova, Scott Gaffney, Padhraic Smyth", "title": "Probabilistic models for joint clustering and time-warping of\n  multidimensional curves", "comments": "Appears in Proceedings of the Nineteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2003)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2003-PG-134-141", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a family of algorithms that can simultaneously align\nand cluster sets of multidimensional curves measured on a discrete time grid.\nOur approach is based on a generative mixture model that allows non-linear time\nwarping of the observed curves relative to the mean curves within the clusters.\nWe also allow for arbitrary discrete-valued translation of the time axis,\nrandom real-valued offsets of the measured curves, and additive measurement\nnoise. The resulting model can be viewed as a dynamic Bayesian network with a\nspecial transition structure that allows effective inference and learning. The\nExpectation-Maximization (EM) algorithm can be used to simultaneously recover\nboth the curve models for each cluster, and the most likely time warping,\ntranslation, offset, and cluster membership for each curve. We demonstrate how\nBayesian estimation methods improve the results for smaller sample sizes by\nenforcing smoothness in the cluster mean curves. We evaluate the methodology on\ntwo real-world data sets, and show that the DBN models provide systematic\nimprovements in predictive power over competing approaches.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 15:04:32 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Chudova", "Darya", ""], ["Gaffney", "Scott", ""], ["Smyth", "Padhraic", ""]]}, {"id": "1212.2616", "submitter": "Matjaz Perc", "authors": "Alexander M. Petersen, Joel N. Tenenbaum, Shlomo Havlin, H. Eugene\n  Stanley, Matjaz Perc", "title": "Languages cool as they expand: Allometric scaling and the decreasing\n  need for new words", "comments": "9 two-column pages, 7 figures; accepted for publication in Scientific\n  Reports", "journal-ref": "Sci. Rep. 2 (2012) 943", "doi": "10.1038/srep00943", "report-no": null, "categories": "physics.soc-ph cond-mat.stat-mech cs.CL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the occurrence frequencies of over 15 million words recorded in\nmillions of books published during the past two centuries in seven different\nlanguages. For all languages and chronological subsets of the data we confirm\nthat two scaling regimes characterize the word frequency distributions, with\nonly the more common words obeying the classic Zipf law. Using corpora of\nunprecedented size, we test the allometric scaling relation between the corpus\nsize and the vocabulary size of growing languages to demonstrate a decreasing\nmarginal need for new words, a feature that is likely related to the underlying\ncorrelations between words. We calculate the annual growth fluctuations of word\nuse which has a decreasing trend as the corpus size increases, indicating a\nslowdown in linguistic evolution following language expansion. This \"cooling\npattern\" forms the basis of a third statistical regularity, which unlike the\nZipf and the Heaps law, is dynamical in nature.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2012 20:32:05 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Petersen", "Alexander M.", ""], ["Tenenbaum", "Joel N.", ""], ["Havlin", "Shlomo", ""], ["Stanley", "H. Eugene", ""], ["Perc", "Matjaz", ""]]}, {"id": "1212.2617", "submitter": "Peter Richtarik", "authors": "William Hulme, Peter Richt\\'arik, Lynne McGuire and Alison Green", "title": "Optimal diagnostic tests for sporadic Creutzfeldt-Jakob disease based on\n  support vector machine classification of RT-QuIC data", "comments": "32 pages, 12 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we study numerical construction of optimal clinical diagnostic\ntests for detecting sporadic Creutzfeldt-Jakob disease (sCJD). A cerebrospinal\nfluid sample (CSF) from a suspected sCJD patient is subjected to a process\nwhich initiates the aggregation of a protein present only in cases of sCJD.\nThis aggregation is indirectly observed in real-time at regular intervals, so\nthat a longitudinal set of data is constructed that is then analysed for\nevidence of this aggregation. The best existing test is based solely on the\nfinal value of this set of data, which is compared against a threshold to\nconclude whether or not aggregation, and thus sCJD, is present. This test\ncriterion was decided upon by analysing data from a total of 108 sCJD and\nnon-sCJD samples, but this was done subjectively and there is no supporting\nmathematical analysis declaring this criterion to be exploiting the available\ndata optimally. This paper addresses this deficiency, seeking to validate or\nimprove the test primarily via support vector machine (SVM) classification.\nBesides this, we address a number of additional issues such as i) early\nstopping of the measurement process, ii) the possibility of detecting the\nparticular type of sCJD and iii) the incorporation of additional patient data\nsuch as age, sex, disease duration and timing of CSF sampling into the\nconstruction of the test.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2012 20:33:16 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Hulme", "William", ""], ["Richt\u00e1rik", "Peter", ""], ["McGuire", "Lynne", ""], ["Green", "Alison", ""]]}, {"id": "1212.2758", "submitter": "Quentin Giai Gianetto", "authors": "Quentin Giai Gianetto, Jean-Marc Le Caillec, Erwan Marrec", "title": "Estimating the predictability of economic and financial time series", "comments": "24 pages, 7 figures Keywords : Chaos theory; Sensitivity to initial\n  conditions; Non-linear predictability; Time series", "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.CD stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The predictability of a time series is determined by the sensitivity to\ninitial conditions of its data generating process. In this paper our goal is to\ncharacterize this sensitivity from a finite sample by assuming few hypotheses\non the data generating model structure. In order to measure the distance\nbetween two trajectories induced by a same noisy chaotic dynamic from two close\ninitial conditions, a symmetric Kullback-Leiber divergence measure is used. Our\napproach allows to take into account the dependence of the residual variance on\ninitial conditions. We show it is linked to a Fisher information matrix and we\ninvestigated its expressions in the cases of covariance-stationary processes\nand ARCH($\\infty$) processes. Moreover, we propose a consistent non-parametric\nestimator of this sensitivity matrix in the case of conditionally\nheteroscedastic autoregressive nonlinear processes. Various statistical\nhypotheses can so be tested as for instance the hypothesis that the data\ngenerating process is \"almost\" independently distributed at a given moment.\nApplications to simulated data and to the stock market index S&P500 illustrate\nour findings. More particularly, we highlight a significant relationship\nbetween the sensitivity to initial conditions of the daily returns of the S&P\n500 and their volatility.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2012 10:13:52 GMT"}], "update_date": "2012-12-13", "authors_parsed": [["Gianetto", "Quentin Giai", ""], ["Caillec", "Jean-Marc Le", ""], ["Marrec", "Erwan", ""]]}, {"id": "1212.2831", "submitter": "Mohamed Kafsi", "authors": "Mohamed Kafsi, Matthias Grossglauser and Patrick Thiran", "title": "The Entropy of Conditional Markov Trajectories", "comments": "Accepted for publication in IEEE Transactions on Information Theory", "journal-ref": null, "doi": "10.1109/TIT.2013.2262497", "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To quantify the randomness of Markov trajectories with fixed initial and\nfinal states, Ekroot and Cover proposed a closed-form expression for the\nentropy of trajectories of an irreducible finite state Markov chain. Numerous\napplications, including the study of random walks on graphs, require the\ncomputation of the entropy of Markov trajectories conditioned on a set of\nintermediate states. However, the expression of Ekroot and Cover does not allow\nfor computing this quantity. In this paper, we propose a method to compute the\nentropy of conditional Markov trajectories through a transformation of the\noriginal Markov chain into a Markov chain that exhibits the desired conditional\ndistribution of trajectories. Moreover, we express the entropy of Markov\ntrajectories - a global quantity - as a linear combination of local entropies\nassociated with the Markov chain states.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2012 14:54:56 GMT"}, {"version": "v2", "created": "Tue, 14 May 2013 10:05:37 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Kafsi", "Mohamed", ""], ["Grossglauser", "Matthias", ""], ["Thiran", "Patrick", ""]]}, {"id": "1212.3436", "submitter": "Jonathan Roseblatt", "authors": "Jonathan D. Rosenblatt and Matthijs Vink and Yoav Benjamini", "title": "Revisiting Multi-Subject Random Effects in fMRI: Advocating Prevalence\n  Estimation", "comments": null, "journal-ref": null, "doi": "10.1016/j.neuroimage.2013.08.025", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random Effects analysis has been introduced into fMRI research in order to\ngeneralize findings from the study group to the whole population. Generalizing\nfindings is obviously harder than detecting activation in the study group since\nin order to be significant, an activation has to be larger than the\ninter-subject variability. Indeed, detected regions are smaller when using\nrandom effect analysis versus fixed effects. The statistical assumptions behind\nthe classic random effects model are that the effect in each location is\nnormally distributed over subjects, and \"activation\" refers to a non-null mean\neffect. We argue this model is unrealistic compared to the true population\nvariability, where, due to functional plasticity and registration anomalies, at\neach brain location some of the subjects are active and some are not. We\npropose a finite-Gaussian--mixture--random-effect. A model that amortizes\nbetween-subject spatial disagreement and quantifies it using the \"prevalence\"\nof activation at each location. This measure has several desirable properties:\n(a) It is more informative than the typical active/inactive paradigm. (b) In\ncontrast to the hypothesis testing approach (thus t-maps) which are trivially\nrejected for large sample sizes, the larger the sample size, the more\ninformative the prevalence statistic becomes.\n  In this work we present a formal definition and an estimation procedure of\nthis prevalence. The end result of the proposed analysis is a map of the\nprevalence at locations with significant activation, highlighting activations\nregions that are common over many brains.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2012 10:46:06 GMT"}, {"version": "v2", "created": "Sun, 31 Mar 2013 12:18:47 GMT"}], "update_date": "2013-09-03", "authors_parsed": [["Rosenblatt", "Jonathan D.", ""], ["Vink", "Matthijs", ""], ["Benjamini", "Yoav", ""]]}, {"id": "1212.3716", "submitter": "Dirk Tasche", "authors": "Dirk Tasche", "title": "The art of probability-of-default curve calibration", "comments": "35 pages, 1 figure, 12 tables, minor changes", "journal-ref": "Journal of Credit Risk 9(4), 63-103, 2013", "doi": null, "report-no": null, "categories": "q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PD curve calibration refers to the transformation of a set of rating grade\nlevel probabilities of default (PDs) to another average PD level that is\ndetermined by a change of the underlying portfolio-wide PD. This paper presents\na framework that allows to explore a variety of calibration approaches and the\nconditions under which they are fit for purpose. We test the approaches\ndiscussed by applying them to publicly available datasets of agency rating and\ndefault statistics that can be considered typical for the scope of application\nof the approaches. We show that the popular 'scaled PDs' approach is\ntheoretically questionable and identify an alternative calibration approach\n('scaled likelihood ratio') that is both theoretically sound and performs\nbetter on the test datasets.\n  Keywords: Probability of default, calibration, likelihood ratio, Bayes'\nformula, rating profile, binary classification.\n", "versions": [{"version": "v1", "created": "Sat, 15 Dec 2012 19:08:46 GMT"}, {"version": "v2", "created": "Thu, 24 Jan 2013 20:08:42 GMT"}, {"version": "v3", "created": "Sat, 27 Apr 2013 15:30:34 GMT"}, {"version": "v4", "created": "Fri, 16 Aug 2013 17:37:38 GMT"}, {"version": "v5", "created": "Sat, 19 Oct 2013 08:51:29 GMT"}, {"version": "v6", "created": "Tue, 26 Nov 2013 18:43:50 GMT"}], "update_date": "2013-12-23", "authors_parsed": [["Tasche", "Dirk", ""]]}, {"id": "1212.3739", "submitter": "Kingsley Jones", "authors": "K.R.W. Jones", "title": "A robust Bayesian formulation of the optimal phase measurement problem", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": "Alphaxon-TR-2012/01", "categories": "quant-ph stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Optical phase measurement is a simple example of a quantum--limited\nmeasurement problem with important applications in metrology such as\ngravitational wave detection. The formulation of optimal strategies for such\nmeasurements is an important test-bed for the development of robust statistical\nmethods for instrument evaluation. However, the class of possible distributions\nexhibits extreme pathologies not commonly encountered in conventional\nstatistical analysis. To overcome these difficulties we reformulate the basic\nvariational problem of optimal phase measurement within a Bayesian paradigm and\nemploy the Shannon information as a robust figure of merit. Single-mode\nperformance bounds are discussed, and we invoke a general theorem that reduces\nthe problem of finding the multi-mode performance bounds to the bounding of a\nsingle integral, without need of the central limit theorem.\n", "versions": [{"version": "v1", "created": "Sat, 15 Dec 2012 23:43:47 GMT"}], "update_date": "2012-12-18", "authors_parsed": [["Jones", "K. R. W.", ""]]}, {"id": "1212.3827", "submitter": "Inti  Pedroso", "authors": "Inti Pedroso, Mark J. F. Brown, Seirian Sumner", "title": "Detecting gene innovations for phenotypic diversity across multiple\n  genomes", "comments": "28 pages, 4 tables and 3 figures. Includes Supp Material", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE q-bio.GN q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gene innovation is a key mechanism on the evolution and phenotypic diversity\nof life forms. There is a need for tools able to study gene innovation across\nan increasingly large number of genomic sequences to maximally capitalise our\nunderstanding of biological systems. Here we present\nComparative-Phylostratigraphy, an open-source software suite that enables to\ntime the emergence of new genes across evolutionary time and to correlate\npatterns of gene emergence with species traits simultaneously across whole\ngenomes from multiple species. Such a comparative strategy is a new powerful\ntool for starting to dissect the relationship between gene innovation and\nphenotypic diversity. We describe and showcase our method by analysing recently\npublished ant genomes. This new methodology identified significant bouts of new\ngene evolution in ant clades, that are associated with shifts in life-history\ntraits. Our method allows easy integration of new genomic data as it becomes\navailable, and thus will be a valuable analytical tool for evolutionary\nbiologists interested in explaining the evolution of diversity of life at the\nlevel of the genes.\n", "versions": [{"version": "v1", "created": "Sun, 16 Dec 2012 20:29:54 GMT"}], "update_date": "2012-12-18", "authors_parsed": [["Pedroso", "Inti", ""], ["Brown", "Mark J. F.", ""], ["Sumner", "Seirian", ""]]}, {"id": "1212.3938", "submitter": "Aurore Laurendeau", "authors": "Aurore Laurendeau (ISTerre), Fabrice Cotton (ISTerre), Luis Fabian\n  Bonilla", "title": "Nonstationary Stochastic Simulation of Strong Ground-Motion Time\n  Histories : Application to the Japanese Database", "comments": "10 pages; 15th World Conference on Earthquake Engineering, Lisbon :\n  Portugal (2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For earthquake-resistant design, engineering seismologists employ\ntime-history analysis for nonlinear simulations. The nonstationary stochastic\nmethod previously developed by Pousse et al. (2006) has been updated. This\nmethod has the advantage of being both simple, fast and taking into account the\nbasic concepts of seismology (Brune's source, realistic time envelope function,\nnonstationarity and ground-motion variability). Time-domain simulations are\nderived from the signal spectrogram and depend on few ground-motion parameters:\nArias intensity, significant relative duration and central frequency. These\nindicators are obtained from empirical attenuation equations that relate them\nto the magnitude of the event, the source-receiver distance, and the site\nconditions. We improve the nonstationary stochastic method by using new\nfunctional forms (new surface rock dataset, analysis of both intra-event and\ninter-event residuals, consideration of the scaling relations and VS30), by\nassessing the central frequency with S-transform and by better considering the\nstress drop variability.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2012 08:47:29 GMT"}], "update_date": "2012-12-18", "authors_parsed": [["Laurendeau", "Aurore", "", "ISTerre"], ["Cotton", "Fabrice", "", "ISTerre"], ["Bonilla", "Luis Fabian", ""]]}, {"id": "1212.3967", "submitter": "Michele Piana", "authors": "Sara Garbarino, Giacomo Caviglia, Massimo Brignone, Michela Massollo,\n  Gianmario Sambuceti, Michele Piana", "title": "Compartmental analysis of renal physiology using nuclear medicine data\n  and statistical optimization", "comments": "submitted to SIAM Journal on Applied Mathematics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA physics.med-ph q-bio.TO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a general approach to the compartmental modeling of\nnuclear data based on spectral analysis and statistical optimization. We\nutilize the renal physiology as test case and validate the method against both\nsynthetic data and real measurements acquired during two micro-PET experiments\nwith murine models.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2012 12:04:33 GMT"}], "update_date": "2012-12-18", "authors_parsed": [["Garbarino", "Sara", ""], ["Caviglia", "Giacomo", ""], ["Brignone", "Massimo", ""], ["Massollo", "Michela", ""], ["Sambuceti", "Gianmario", ""], ["Piana", "Michele", ""]]}, {"id": "1212.4678", "submitter": "Jeffrey Shaman", "authors": "Jeffrey Shaman, Alicia Karspeck, Marc Lipstich", "title": "Week 49 Influenza Forecast for the 2012-2013 U.S. Season", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present results of a forecast initiated Week 49 (beginning December 9,\n2012) of the 2012-2013 influenza season for municipalities in the United\nStates. The forecast was made on December 14, 2012. Results from forecasts\ninitiated the two previous weeks (Weeks 47 and 48) are also presented. Also\nresults from the forecast generated with the SIRS model without AH forcing (no\nAH) are shown\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2012 14:44:06 GMT"}], "update_date": "2012-12-20", "authors_parsed": [["Shaman", "Jeffrey", ""], ["Karspeck", "Alicia", ""], ["Lipstich", "Marc", ""]]}, {"id": "1212.4786", "submitter": "Timoth\\'ee Flutre", "authors": "Timoth\\'ee Flutre, Xiaoquan Wen, Jonathan Pritchard, Matthew Stephens", "title": "A statistical framework for joint eQTL analysis in multiple tissues", "comments": "Summitted to PLoS Genetics", "journal-ref": "PLoS Genetics 2013, Vol. 9, No. 5", "doi": "10.1371/journal.pgen.1003486", "report-no": null, "categories": "q-bio.QM q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mapping expression Quantitative Trait Loci (eQTLs) represents a powerful and\nwidely-adopted approach to identifying putative regulatory variants and linking\nthem to specific genes. Up to now eQTL studies have been conducted in a\nrelatively narrow range of tissues or cell types. However, understanding the\nbiology of organismal phenotypes will involve understanding regulation in\nmultiple tissues, and ongoing studies are collecting eQTL data in dozens of\ncell types. Here we present a statistical framework for powerfully detecting\neQTLs in multiple tissues or cell types (or, more generally, multiple\nsubgroups). The framework explicitly models the potential for each eQTL to be\nactive in some tissues and inactive in others. By modeling the sharing of\nactive eQTLs among tissues this framework increases power to detect eQTLs that\nare present in more than one tissue compared with \"tissue-by-tissue\" analyses\nthat examine each tissue separately. Conversely, by modeling the inactivity of\neQTLs in some tissues, the framework allows the proportion of eQTLs shared\nacross different tissues to be formally estimated as parameters of a model,\naddressing the difficulties of accounting for incomplete power when comparing\noverlaps of eQTLs identified by tissue-by-tissue analyses. Applying our\nframework to re-analyze data from transformed B cells, T cells and fibroblasts\nwe find that it substantially increases power compared with tissue-by-tissue\nanalysis, identifying 63% more genes with eQTLs (at FDR=0.05). Further the\nresults suggest that, in contrast to previous analyses of the same data, the\nmajority of eQTLs detectable in these data are shared among all three tissues.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2012 18:32:23 GMT"}], "update_date": "2013-08-12", "authors_parsed": [["Flutre", "Timoth\u00e9e", ""], ["Wen", "Xiaoquan", ""], ["Pritchard", "Jonathan", ""], ["Stephens", "Matthew", ""]]}, {"id": "1212.4788", "submitter": "Dominik Grimm dg", "authors": "Dominik Grimm, Bastian Greshake, Stefan Kleeberger, Christoph Lippert,\n  Oliver Stegle, Bernhard Sch\\\"olkopf, Detlef Weigel and Karsten Borgwardt", "title": "easyGWAS: An integrated interspecies platform for performing genome-wide\n  association studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.CE cs.DL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: The rapid growth in genome-wide association studies (GWAS) in\nplants and animals has brought about the need for a central resource that\nfacilitates i) performing GWAS, ii) accessing data and results of other GWAS,\nand iii) enabling all users regardless of their background to exploit the\nlatest statistical techniques without having to manage complex software and\ncomputing resources.\n  Results: We present easyGWAS, a web platform that provides methods, tools and\ndynamic visualizations to perform and analyze GWAS. In addition, easyGWAS makes\nit simple to reproduce results of others, validate findings, and access larger\nsample sizes through merging of public datasets.\n  Availability: Detailed method and data descriptions as well as tutorials are\navailable in the supplementary materials. easyGWAS is available at\nhttp://easygwas.tuebingen.mpg.de/.\n  Contact: dominik.grimm@tuebingen.mpg.de\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2012 18:39:06 GMT"}], "update_date": "2012-12-20", "authors_parsed": [["Grimm", "Dominik", ""], ["Greshake", "Bastian", ""], ["Kleeberger", "Stefan", ""], ["Lippert", "Christoph", ""], ["Stegle", "Oliver", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Weigel", "Detlef", ""], ["Borgwardt", "Karsten", ""]]}, {"id": "1212.4890", "submitter": "Mark Leeds", "authors": "Mark Leeds", "title": "Bollinger Bands Thirty Years Later", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this study is to explain and examine the statistical\nunderpinnings of the Bollinger Band methodology. We start off by elucidating\nthe rolling regression time series model and deriving its explicit relationship\nto Bollinger Bands. Next we illustrate the use of Bollinger Bands in pairs\ntrading and prove the existence of a specific return duration relationship in\nBollinger Band pairs trading.Then by viewing the Bollinger Band moving average\nas an approximation to the random walk plus noise (RWPN) time series model, we\ndevelop a pairs trading variant that we call \"Fixed Forecast Maximum Duration'\nBands\" (FFMDPT). Lastly, we conduct pairs trading simulations using SAP and\nNikkei index data in order to compare the performance of the variant with\nBollinger Bands.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2012 00:37:15 GMT"}, {"version": "v2", "created": "Tue, 1 Jan 2013 18:13:40 GMT"}], "update_date": "2013-01-03", "authors_parsed": [["Leeds", "Mark", ""]]}, {"id": "1212.5049", "submitter": "Gabriele Cantaluppi", "authors": "Gabriele Cantaluppi", "title": "A Partial Least Squares Algorithm Handling Ordinal Variables also in\n  Presence of a Small Number of Categories", "comments": "33 pages, 13 figures, 9 tables, submitted", "journal-ref": null, "doi": null, "report-no": "Universit\\`a Cattolica del Sacro Cuore, Milano, Dipartimento di\n  Scienze statistiche, Quaderno di Dipartimento N. 14, Serie E.P. N. 144", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The partial least squares (PLS) is a popular modeling technique commonly used\nin social sciences. The traditional PLS algorithm deals with variables measured\non interval scales while data are often collected on ordinal scales: a\nreformulation of the algorithm, named ordinal PLS (OPLS), is introduced, which\nproperly deals with ordinal variables. An application to customer satisfaction\ndata and some simulations are also presented. The technique seems to perform\nbetter than the traditional PLS when the number of categories of the items in\nthe questionnaire is small (4 or 5) which is typical in the most common\npractical situations.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2012 14:20:43 GMT"}], "update_date": "2012-12-21", "authors_parsed": [["Cantaluppi", "Gabriele", ""]]}, {"id": "1212.5090", "submitter": "Jouchi Nakajima", "authors": "Jouchi Nakajima", "title": "Bayesian analysis of multivariate stochastic volatility with skew\n  distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate stochastic volatility models with skew distributions are\nproposed. Exploiting Cholesky stochastic volatility modeling, univariate\nstochastic volatility processes with leverage effect and generalized hyperbolic\nskew t-distributions are embedded to multivariate analysis with time-varying\ncorrelations. Bayesian prior works allow this approach to provide parsimonious\nskew structure and to easily scale up for high-dimensional problem. Analyses of\ndaily stock returns are illustrated. Empirical results show that the\ntime-varying correlations and the sparse skew structure contribute to improved\nprediction performance and VaR forecasts.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2012 15:40:54 GMT"}], "update_date": "2012-12-21", "authors_parsed": [["Nakajima", "Jouchi", ""]]}, {"id": "1212.5180", "submitter": "Javier Contreras-Reyes JCR", "authors": "Javier E. Contreras-Reyes, Reinaldo B. Arellano-Valle", "title": "Growth curve based on scale mixtures of skew-normal distributions to\n  model the age-length relationship of Cardinalfish (Epigonus Crassicaudus)", "comments": "16 pages, 6 figures", "journal-ref": "Fisheries Research (2013), 147, 137-144", "doi": "10.1016/j.fishres.2013.05.002", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our article presents a robust and flexible statistical modeling for the\ngrowth curve associated to the age-length relationship of Cardinalfish\n(Epigonus Crassicaudus). Specifically, we consider a non-linear regression\nmodel, in which the error distribution allows heteroscedasticity and belongs to\nthe family of scale mixture of the skewnormal (SMSN) distributions, thus\neliminating the need to transform the dependent variable into many data sets.\nThe SMSN is a tractable and flexible class of asymmetric heavy-tailed\ndistributions that are useful for robust inference when the normality\nassumption for error distribution is questionable. Two well-known important\nmembers of this class are the proper skew-normal and skew-t distributions. In\nthis work emphasis is given to the skew-t model. However, the proposed\nmethodology can be adapted for each of the SMSN models with some basic changes.\nThe present work is motivated by previous analysis about of Cardinalfish age,\nin which a maximum age of 15 years has been determined. Therefore, in this\nstudy we carry out the mentioned methodology over a data set that include a\nlong-range of ages based on an otolith sample where the determined longevity is\nhigher than 54 years.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2012 18:24:26 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Contreras-Reyes", "Javier E.", ""], ["Arellano-Valle", "Reinaldo B.", ""]]}, {"id": "1212.5203", "submitter": "Michael Larsen", "authors": "Michael D. Larsen", "title": "An Experiment with Hierarchical Bayesian Record Linkage", "comments": "14 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In record linkage (RL), or exact file matching, the goal is to identify the\nlinks between entities with information on two or more files. RL is an\nimportant activity in areas including counting the population, enhancing survey\nframes and data, and conducting epidemiological and follow-up studies. RL is\nchallenging when files are very large, no accurate personal identification (ID)\nnumber is present on all files for all units, and some information is recorded\nwith error. Without an unique ID number one must rely on comparisons of names,\naddresses, dates, and other information to find the links. Latent class models\ncan be used to automatically score the value of information for determining\nmatch status. Data for fitting models come from comparisons made within groups\nof units that pass initial file blocking requirements. Data distributions can\nvary across blocks. This article examines the use of prior information and\nhierarchical latent class models in the context of RL.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2012 19:37:25 GMT"}], "update_date": "2012-12-21", "authors_parsed": [["Larsen", "Michael D.", ""]]}, {"id": "1212.5389", "submitter": "Alexandros Kalousis", "authors": "Nabil Stendardo and Alexandros Kalousis", "title": "Relationship-aware sequential pattern mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relationship-aware sequential pattern mining is the problem of mining\nfrequent patterns in sequences in which the events of a sequence are mutually\nrelated by one or more concepts from some respective hierarchical taxonomies,\nbased on the type of the events. Additionally events themselves are also\ndescribed with a certain number of taxonomical concepts. We present RaSP an\nalgorithm that is able to mine relationship-aware patterns over such sequences;\nRaSP follows a two stage approach. In the first stage it mines for frequent\ntype patterns and {\\em all} their occurrences within the different sequences.\nIn the second stage it performs hierarchical mining where for each frequent\ntype pattern and its occurrences it mines for more specific frequent patterns\nin the lower levels of the taxonomies. We test RaSP on a real world medical\napplication, that provided the inspiration for its development, in which we\nmine for frequent patterns of medical behavior in the antibiotic treatment of\nmicrobes and show that it has a very good computational performance given the\ncomplexity of the relationship-aware sequential pattern mining problem.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2012 10:42:06 GMT"}], "update_date": "2012-12-24", "authors_parsed": [["Stendardo", "Nabil", ""], ["Kalousis", "Alexandros", ""]]}, {"id": "1212.5533", "submitter": "Anna  Deluca", "authors": "Anna Deluca and Alvaro Corral", "title": "Scale Invariant Events and Dry Spells for Medium Resolution Local Rain\n  Data", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ao-ph physics.data-an physics.geo-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze distributions of rain-event sizes, rain-event durations, and\ndry-spell durations for data obtained from a network of 20 rain gauges\nscattered in a region of the NW Mediterranean coast. While power-law\ndistributions model the dry-spell durations with a common exponent 1.50 +-\n0.05, density analysis is inconclusive for event sizes and event durations, due\nto finite size effects. However, we present alternative evidence of the\nexistence of scale invariance in these distributions by means of different data\ncollapses of the distributions. These results are in agreement with the\nexpectations from the Self-Organized Criticality paradigm, and demonstrate that\nscaling properties of rain events and dry spells can also be observed for\nmedium resolution rain data.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2012 15:42:26 GMT"}], "update_date": "2012-12-24", "authors_parsed": [["Deluca", "Anna", ""], ["Corral", "Alvaro", ""]]}, {"id": "1212.5654", "submitter": "Aditya Vempaty", "authors": "Aditya Vempaty, Priyadip Ray and Pramod K. Varshney", "title": "False Discovery Rate Based Distributed Detection in the Presence of\n  Byzantines", "comments": "28 pages, 9 figures, to appear in IEEE Transactions on Aerospace and\n  Electronic Systems", "journal-ref": null, "doi": "10.1109/TAES.2014.120645", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent literature has shown that the control of False Discovery Rate (FDR)\nfor distributed detection in wireless sensor networks (WSNs) can provide\nsubstantial improvement in detection performance over conventional design\nmethodologies. In this paper, we further investigate system design issues in\nFDR based distributed detection. We demonstrate that improved system design may\nbe achieved by employing the Kolmogorov-Smirnov distance metric instead of the\ndeflection coefficient, as originally proposed in Ray&VarshneyAES11. We also\nanalyze the performance of FDR based distributed detection in the presence of\nByzantines. Byzantines are malicious sensors which send falsified information\nto the Fusion Center (FC) to deteriorate system performance. We provide\nanalytical and simulation results on the global detection probability as a\nfunction of the fraction of Byzantines in the network. It is observed that the\ndetection performance degrades considerably when the fraction of Byzantines is\nlarge. Hence, we propose an adaptive algorithm at the FC which learns the\nByzantines' behavior over time and changes the FDR parameter to overcome the\nloss in detection performance. Detailed simulation results are provided to\ndemonstrate the robustness of the proposed adaptive algorithm to Byzantine\nattacks in WSNs.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2012 04:20:55 GMT"}, {"version": "v2", "created": "Sun, 7 Apr 2013 19:08:07 GMT"}, {"version": "v3", "created": "Thu, 12 Sep 2013 20:18:11 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Vempaty", "Aditya", ""], ["Ray", "Priyadip", ""], ["Varshney", "Pramod K.", ""]]}, {"id": "1212.5729", "submitter": "Timothy Armstrong", "authors": "Timothy B. Armstrong and Hock Peng Chan", "title": "Multiscale Adaptive Inference on Conditional Moment Inequalities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers inference for conditional moment inequality models using\na multiscale statistic. We derive the asymptotic distribution of this test\nstatistic and use the result to propose feasible critical values that have a\nsimple analytic formula, and to prove the asymptotic validity of a modified\nbootstrap procedure. The asymptotic distribution is extreme value, and the\nproof uses new techniques to overcome several technical obstacles. The test\ndetects local alternatives that approach the identified set at the best rate\namong available tests in a broad class of models, and is adaptive to the\nsmoothness properties of the data generating process. Our results also have\nimplications for the use of moment selection procedures in this setting. We\nprovide a monte carlo study and an empirical illustration to inference in a\nregression model with endogenously censored and missing data.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2012 21:19:49 GMT"}, {"version": "v2", "created": "Fri, 17 Oct 2014 12:42:09 GMT"}, {"version": "v3", "created": "Tue, 8 Dec 2015 23:10:52 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Armstrong", "Timothy B.", ""], ["Chan", "Hock Peng", ""]]}, {"id": "1212.5750", "submitter": "Jeffrey Shaman", "authors": "Jeffrey Shaman, Alicia Karspeck, Marc Lipsitch", "title": "Week 50 Influenza Forecast for the 2012-2013 U.S. Season", "comments": "arXiv admin note: text overlap with arXiv:1212.4678", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present results of a forecast initiated following assimilation of\nobservations for week Week 50 (i.e. the forecast begins December 16, 2012) of\nthe 2012-2013 influenza season for municipalities in the United States. The\nforecast was made on December 21, 2012. Results from forecasts initiated the\nthree previous weeks (Weeks 47-49) are also presented. Also results from\nforecasts generated with an SIRS model without absolute humidity forcing (no\nAH) are shown.\n", "versions": [{"version": "v1", "created": "Sun, 23 Dec 2012 01:37:07 GMT"}], "update_date": "2012-12-27", "authors_parsed": [["Shaman", "Jeffrey", ""], ["Karspeck", "Alicia", ""], ["Lipsitch", "Marc", ""]]}, {"id": "1212.5828", "submitter": "Anna  Deluca", "authors": "Alvaro Corral and Anna Deluca", "title": "Fitting and goodness-of-fit test of non-truncated and truncated\n  power-law distributions", "comments": "26 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph nlin.AO physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power-law distributions contain precious information about a large variety of\nprocesses in geoscience and elsewhere. Although there are sound theoretical\ngrounds for these distributions, the empirical evidence in favor of power laws\nhas been traditionally weak. Recently, Clauset et al. have proposed a\nsystematic method to find over which range (if any) a certain distribution\nbehaves as a power law. However, their method has been found to fail, in the\nsense that true (simulated) power-law tails are not recognized as such in some\ninstances, and then the power-law hypothesis is rejected. Moreover, the method\ndoes not work well when extended to power-law distributions with an upper\ntruncation. We explain in detail a similar but alternative procedure, valid for\ntruncated as well as for non-truncated power-law distributions, based in\nmaximum likelihood estimation, the Kolmogorov-Smirnov goodness-of-fit test, and\nMonte Carlo simulations. An overview of the main concepts as well as a recipe\nfor their practical implementation is provided. The performance of our method\nis put to test on several empirical data which were previously analyzed with\nless systematic approaches. The databases presented here include the half-lives\nof the radionuclides, the seismic moment of earthquakes in the whole world and\nin Southern California, a proxy for the energy dissipated by tropical cyclones\nelsewhere, the area burned by forest fires in Italy, and the waiting times\ncalculated over different spatial subdivisions of Southern California. We find\nthe functioning of the method very satisfactory.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2012 14:51:00 GMT"}, {"version": "v2", "created": "Mon, 11 Mar 2013 12:13:54 GMT"}], "update_date": "2013-03-12", "authors_parsed": [["Corral", "Alvaro", ""], ["Deluca", "Anna", ""]]}, {"id": "1212.5829", "submitter": "Harpreet S. Dhillon", "authors": "Harpreet S. Dhillon, Radha Krishna Ganti, Jeffrey G. Andrews", "title": "Modeling Non-Uniform UE Distributions in Downlink Cellular Networks", "comments": "Submitted to IEEE Wireless Communications Letters", "journal-ref": null, "doi": "10.1109/WCL.2013.040513.120942", "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent way to model and analyze downlink cellular networks is by using\nrandom spatial models. Assuming user equipment (UE) distribution to be uniform,\nthe analysis is performed at a typical UE located at the origin. While this\nmethod of sampling UEs provides statistics averaged over the UE locations, it\nis not possible to sample cell interior and cell edge UEs separately. This\ncomplicates the problem of analyzing deployment scenarios involving non-uniform\ndistribution of UEs, especially when the locations of the UEs and the base\nstations (BSs) are dependent. To facilitate this separation, we propose a new\ntractable method of sampling UEs by conditionally thinning the BS point process\nand show that the resulting framework can be used as a tractable generative\nmodel to study cellular networks with non-uniform UE distribution.\n", "versions": [{"version": "v1", "created": "Sun, 23 Dec 2012 19:34:12 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Dhillon", "Harpreet S.", ""], ["Ganti", "Radha Krishna", ""], ["Andrews", "Jeffrey G.", ""]]}, {"id": "1212.5932", "submitter": "Leo Lahti", "authors": "Leo Lahti, Aurora Torrente, Laura L. Elo, Alvis Brazma, Johan Rung", "title": "Fully scalable online-preprocessing algorithm for short oligonucleotide\n  microarray atlases", "comments": "20 pages, 3 figures, 1 supplementary PDF", "journal-ref": "Leo Lahti, Aurora Torrente, Laura L. Elo, Alvis Brazma, Johan\n  Rung. A fully scalable online pre-processing algorithm for short\n  oligonucleotide microarray atlases. Nucleic Acids Research, Online April 5,\n  2013", "doi": "10.1093/nar/gkt229", "report-no": null, "categories": "q-bio.QM cs.CE cs.LG q-bio.GN stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Accumulation of standardized data collections is opening up novel\nopportunities for holistic characterization of genome function. The limited\nscalability of current preprocessing techniques has, however, formed a\nbottleneck for full utilization of contemporary microarray collections. While\nshort oligonucleotide arrays constitute a major source of genome-wide profiling\ndata, scalable probe-level preprocessing algorithms have been available only\nfor few measurement platforms based on pre-calculated model parameters from\nrestricted reference training sets. To overcome these key limitations, we\nintroduce a fully scalable online-learning algorithm that provides tools to\nprocess large microarray atlases including tens of thousands of arrays. Unlike\nthe alternatives, the proposed algorithm scales up in linear time with respect\nto sample size and is readily applicable to all short oligonucleotide\nplatforms. This is the only available preprocessing algorithm that can learn\nprobe-level parameters based on sequential hyperparameter updates at small,\nconsecutive batches of data, thus circumventing the extensive memory\nrequirements of the standard approaches and opening up novel opportunities to\ntake full advantage of contemporary microarray data collections. Moreover,\nusing the most comprehensive data collections to estimate probe-level effects\ncan assist in pinpointing individual probes affected by various biases and\nprovide new tools to guide array design and quality control. The implementation\nis freely available in R/Bioconductor at\nhttp://www.bioconductor.org/packages/devel/bioc/html/RPA.html\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2012 16:41:08 GMT"}, {"version": "v2", "created": "Thu, 27 Dec 2012 11:23:39 GMT"}], "update_date": "2013-04-09", "authors_parsed": [["Lahti", "Leo", ""], ["Torrente", "Aurora", ""], ["Elo", "Laura L.", ""], ["Brazma", "Alvis", ""], ["Rung", "Johan", ""]]}, {"id": "1212.6006", "submitter": "Tomokazu Konishi", "authors": "Tomokazu Konishi", "title": "Principal Component Analysis for Experiments", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Although principal component analysis is frequently applied to\nreduce the dimensionality of matrix data, the method is sensitive to noise and\nbias and has difficulty with comparability and interpretation. These issues are\naddressed by improving the fidelity to the study design. Principal axes and the\ncomponents for variables are found through the arrangement of the training data\nset, and the centers of data are found according to the design. By using both\nthe axes and the center, components for an observation that belong to various\nstudies can be separately estimated. Both of the components for variables and\nobservations are scaled to a unit length, which enables relationships to be\nseen between them.\n  Results: Analyses in transcriptome studies showed an improvement in the\nseparation of experimental groups and in robustness to bias and noise. Unknown\nsamples were appropriately classified on predetermined axes. These axes well\nreflected the study design, and this facilitated the interpretation. Together,\nthe introduced concepts resulted in improved generality and objectivity in the\nanalytical results, with the ability to locate hidden structures in the data.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2012 09:33:06 GMT"}], "update_date": "2012-12-27", "authors_parsed": [["Konishi", "Tomokazu", ""]]}, {"id": "1212.6016", "submitter": "Gordon J Ross", "authors": "Gordon J. Ross", "title": "Modeling Financial Volatility in the Presence of Abrupt Changes", "comments": null, "journal-ref": "Physica A: Statistical Mechanics and its Applications (2013).\n  192(2) 350-360", "doi": "10.1016/j.physa.2012.08.015", "report-no": null, "categories": "q-fin.ST physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The volatility of financial instruments is rarely constant, and usually\nvaries over time. This creates a phenomenon called volatility clustering, where\nlarge price movements on one day are followed by similarly large movements on\nsuccessive days, creating temporal clusters. The GARCH model, which treats\nvolatility as a drift process, is commonly used to capture this behavior.\nHowever research suggests that volatility is often better described by a\nstructural break model, where the volatility undergoes abrupt jumps in addition\nto drift. Most efforts to integrate these jumps into the GARCH methodology have\nresulted in models which are either very computationally demanding, or which\nmake problematic assumptions about the distribution of the instruments, often\nassuming that they are Gaussian. We present a new approach which uses ideas\nfrom nonparametric statistics to identify structural break points without\nmaking such distributional assumptions, and then models drift separately within\neach identified regime. Using our method, we investigate the volatility of\nseveral major stock indexes, and find that our approach can potentially give an\nimproved fit compared to more commonly used techniques.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2012 10:50:31 GMT"}], "update_date": "2015-05-08", "authors_parsed": [["Ross", "Gordon J.", ""]]}, {"id": "1212.6018", "submitter": "Gordon J Ross", "authors": "Gordon J. Ross, Niall M. Adams, Dimitris K. Tasoulis, David J. Hand", "title": "Exponentially Weighted Moving Average Charts for Detecting Concept Drift", "comments": null, "journal-ref": "Pattern Recognition Letters, 33(2) 191-198, 2012", "doi": "10.1016/j.patrec.2011.08.019", "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifying streaming data requires the development of methods which are\ncomputationally efficient and able to cope with changes in the underlying\ndistribution of the stream, a phenomenon known in the literature as concept\ndrift. We propose a new method for detecting concept drift which uses an\nExponentially Weighted Moving Average (EWMA) chart to monitor the\nmisclassification rate of an streaming classifier. Our approach is modular and\ncan hence be run in parallel with any underlying classifier to provide an\nadditional layer of concept drift detection. Moreover our method is\ncomputationally efficient with overhead O(1) and works in a fully online manner\nwith no need to store data points in memory. Unlike many existing approaches to\nconcept drift detection, our method allows the rate of false positive\ndetections to be controlled and kept constant over time.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2012 11:01:48 GMT"}], "update_date": "2012-12-27", "authors_parsed": [["Ross", "Gordon J.", ""], ["Adams", "Niall M.", ""], ["Tasoulis", "Dimitris K.", ""], ["Hand", "David J.", ""]]}, {"id": "1212.6690", "submitter": "Zhaonan Sun", "authors": "Zhaonan Sun, Thomas Kuczek, Yu Zhu", "title": "Statistical calibration of qRT-PCR, microarray and RNA-Seq gene\n  expression data with measurement error models", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS721 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 2, 1022-1044", "doi": "10.1214/14-AOAS721", "report-no": "IMS-AOAS-AOAS721", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accurate quantification of gene expression levels is crucial for\ntranscriptome study. Microarray platforms are commonly used for simultaneously\ninterrogating thousands of genes in the past decade, and recently RNA-Seq has\nemerged as a promising alternative. The gene expression measurements obtained\nby microarray and RNA-Seq are, however, subject to various measurement errors.\nA third platform called qRT-PCR is acknowledged to provide more accurate\nquantification of gene expression levels than microarray and RNA-Seq, but it\nhas limited throughput capacity. In this article, we propose to use a system of\nfunctional measurement error models to model gene expression measurements and\ncalibrate the microarray and RNA-Seq platforms with qRT-PCR. Based on the\nsystem, a two-step approach was developed to estimate the biases and error\nvariance components of the three platforms and calculate calibrated estimates\nof gene expression levels. The estimated biases and variance components shed\nlight on the relative strengths and weaknesses of the three platforms and the\ncalibrated estimates provide a more accurate and consistent quantification of\ngene expression levels. Theoretical and simulation studies were conducted to\nestablish the properties of those estimates. The system was applied to analyze\ntwo gene expression data sets from the Microarray Quality Control (MAQC) and\nSequencing Quality Control (SEQC) projects.\n", "versions": [{"version": "v1", "created": "Sun, 30 Dec 2012 05:47:08 GMT"}, {"version": "v2", "created": "Thu, 31 Jul 2014 13:12:36 GMT"}], "update_date": "2014-08-01", "authors_parsed": [["Sun", "Zhaonan", ""], ["Kuczek", "Thomas", ""], ["Zhu", "Yu", ""]]}, {"id": "1212.6757", "submitter": "Denis Chetverikov", "authors": "Denis Chetverikov", "title": "Testing Regression Monotonicity in Econometric Models", "comments": null, "journal-ref": "Econom. Theory 35 (2019) 729-776", "doi": "10.1017/S0266466618000282", "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monotonicity is a key qualitative prediction of a wide array of economic\nmodels derived via robust comparative statics. It is therefore important to\ndesign effective and practical econometric methods for testing this prediction\nin empirical analysis. This paper develops a general nonparametric framework\nfor testing monotonicity of a regression function. Using this framework, a\nbroad class of new tests is introduced, which gives an empirical researcher a\nlot of flexibility to incorporate ex ante information she might have. The paper\nalso develops new methods for simulating critical values, which are based on\nthe combination of a bootstrap procedure and new selection algorithms. These\nmethods yield tests that have correct asymptotic size and are asymptotically\nnonconservative. It is also shown how to obtain an adaptive rate optimal test\nthat has the best attainable rate of uniform consistency against models whose\nregression function has Lipschitz-continuous first-order derivatives and that\nautomatically adapts to the unknown smoothness of the regression function.\nSimulations show that the power of the new tests in many cases significantly\nexceeds that of some prior tests, e.g. that of Ghosal, Sen, and Van der Vaart\n(2000). An application of the developed procedures to the dataset of Ellison\nand Ellison (2011) shows that there is some evidence of strategic entry\ndeterrence in pharmaceutical industry where incumbents may use strategic\ninvestment to prevent generic entries when their patents expire.\n", "versions": [{"version": "v1", "created": "Sun, 30 Dec 2012 17:59:23 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2013 19:25:17 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Chetverikov", "Denis", ""]]}]