[{"id": "1508.00127", "submitter": "Francesca Greselin", "authors": "Francesca Greselin and Ricardas Zitikis", "title": "Measuring economic inequality and risk: a unifying approach based on\n  personal gambles, societal preferences and references", "comments": "29 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The underlying idea behind the construction of indices of economic inequality\nis based on measuring deviations of various portions of low incomes from\ncertain references or benchmarks, that could be point measures like population\nmean or median, or curves like the hypotenuse of the right triangle where every\nLorenz curve falls into. In this paper we argue that by appropriately choosing\npopulation-based references, called societal references, and distributions of\npersonal positions, called gambles, which are random, we can meaningfully unify\nclassical and contemporary indices of economic inequality, as well as various\nmeasures of risk. To illustrate the herein proposed approach, we put forward\nand explore a risk measure that takes into account the relativity of large\nrisks with respect to small ones.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2015 14:26:41 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Greselin", "Francesca", ""], ["Zitikis", "Ricardas", ""]]}, {"id": "1508.00129", "submitter": "William Barcella", "authors": "William Barcella, Maria De Iorio, Gianluca Baio", "title": "A comparative review of variable selection techniques for covariate\n  dependent Dirichlet process mixture models", "comments": "26 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dirichlet Process Mixture (DPM) models have been increasingly employed to\nspecify random partition models that take into account possible patterns within\nthe covariates. Furthermore, to deal with large numbers of covariates, methods\nfor selecting the most important covariates have been proposed. Commonly, the\ncovariates are chosen either for their importance in determining the clustering\nof the observations or for their effect on the level of a response variable\n(when a regression model is specified). Typically both strategies involve the\nspecification of latent indicators that regulate the inclusion of the\ncovariates in the model. Common examples involve the use of spike and slab\nprior distributions. In this work we review the most relevant DPM models that\ninclude covariate information in the induced partition of the observations and\nwe focus on available variable selection techniques for these models. We\nhighlight the main features of each model and demonstrate them in simulations\nand in a real data application.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2015 14:53:52 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2015 08:54:34 GMT"}, {"version": "v3", "created": "Fri, 15 Apr 2016 13:03:49 GMT"}, {"version": "v4", "created": "Sat, 29 Oct 2016 16:34:21 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Barcella", "William", ""], ["De Iorio", "Maria", ""], ["Baio", "Gianluca", ""]]}, {"id": "1508.00225", "submitter": "Paul Bastide", "authors": "Paul Bastide and Mahendra Mariadassou and St\\'ephane Robin", "title": "Detection of adaptive shifts on phylogenies using shifted stochastic\n  processes on a tree", "comments": "Corrected version. Also deposed on BioRxiv:\n  http://dx.doi.org/10.1101/023804", "journal-ref": "J. R. Stat. Soc. B 79 (2017) 1067-1093", "doi": "10.1111/rssb.12206", "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparative and evolutive ecologists are interested in the distribution of\nquantitative traits among related species. The classical framework for these\ndistributions consists of a random process running along the branches of a\nphylogenetic tree relating the species. We consider shifts in the process\nparameters, which reveal fast adaptation to changes of ecological niches. We\nshow that models with shifts are not identifiable in general. Constraining the\nmodels to be parsimonious in the number of shifts partially alleviates the\nproblem but several evolutionary scenarios can still provide the same joint\ndistribution for the extant species. We provide a recursive algorithm to\nenumerate all the equivalent scenarios and to count the effectively different\nscenarios. We introduce an incomplete-data framework and develop a maximum\nlikelihood estimation procedure based on the EM algorithm. Finally, we propose\na model selection procedure, based on the cardinal of effective scenarios, to\nestimate the number of shifts and prove an oracle inequality.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2015 12:54:49 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2016 17:54:24 GMT"}, {"version": "v3", "created": "Wed, 13 Jul 2016 16:00:05 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Bastide", "Paul", ""], ["Mariadassou", "Mahendra", ""], ["Robin", "St\u00e9phane", ""]]}, {"id": "1508.00281", "submitter": "Florian Heinrichs", "authors": "Kirsten Schorning, Bj\\\"orn Bornkamp, Frank Bretz, Holger Dette", "title": "Model Selection versus Model Averaging in Dose Finding Studies", "comments": "Keywords and Phrases: Model selection; model averaging; clinical\n  trials; simulation study", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phase II dose finding studies in clinical drug development are typically\nconducted to adequately characterize the dose response relationship of a new\ndrug. An important decision is then on the choice of a suitable dose response\nfunction to support dose selection for the subsequent Phase III studies. In\nthis paper we compare different approaches for model selection and model\naveraging using mathematical properties as well as simulations. Accordingly, we\nreview and illustrate asymptotic properties of model selection criteria and\ninvestigate their behavior when changing the sample size but keeping the effect\nsize constant. In a large scale simulation study we investigate how the various\napproaches perform in realistically chosen settings. Finally, the different\nmethods are illustrated with a recently conducted Phase II dosefinding study in\npatients with chronic obstructive pulmonary disease.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2015 20:19:04 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Schorning", "Kirsten", ""], ["Bornkamp", "Bj\u00f6rn", ""], ["Bretz", "Frank", ""], ["Dette", "Holger", ""]]}, {"id": "1508.00408", "submitter": "Yasin Yilmaz", "authors": "Yasin Yilmaz, Alfred O. Hero", "title": "Multimodal Factor Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multimodal system with Poisson, Gaussian, and multinomial observations is\nconsidered. A generative graphical model that combines multiple modalities\nthrough common factor loadings is proposed. In this model, latent factors are\nlike summary objects that has latent factor scores in each modality, and the\nobserved objects are represented in terms of such summary objects. This\npotentially brings about a significant dimensionality reduction. It also\nnaturally enables a powerful means of clustering based on a diverse set of\nobservations. An expectation-maximization (EM) algorithm to find the model\nparameters is provided. The algorithm is tested on a Twitter dataset which\nconsists of the counts and geographical coordinates of hashtag occurrences,\ntogether with the bag of words for each hashtag. The resultant factors\nsuccessfully localizes the hashtags in all dimensions: counts, coordinates,\ntopics. The algorithm is also extended to accommodate von Mises-Fisher\ndistribution, which is used to model the spherical coordinates.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2015 13:18:39 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Yilmaz", "Yasin", ""], ["Hero", "Alfred O.", ""]]}, {"id": "1508.00459", "submitter": "Ka-Chun Wong", "authors": "Ka-Chun Wong, Yue Li, Zhaolei Zhang", "title": "Unsupervised Learning in Genome Informatics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN q-bio.QM stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With different genomes available, unsupervised learning algorithms are\nessential in learning genome-wide biological insights. Especially, the\nfunctional characterization of different genomes is essential for us to\nunderstand lives. In this book chapter, we review the state-of-the-art\nunsupervised learning algorithms for genome informatics from DNA to MicroRNA.\n  DNA (DeoxyriboNucleic Acid) is the basic component of genomes. A significant\nfraction of DNA regions (transcription factor binding sites) are bound by\nproteins (transcription factors) to regulate gene expression at different\ndevelopment stages in different tissues. To fully understand genetics, it is\nnecessary of us to apply unsupervised learning algorithms to learn and infer\nthose DNA regions. Here we review several unsupervised learning methods for\ndeciphering the genome-wide patterns of those DNA regions.\n  MicroRNA (miRNA), a class of small endogenous non-coding RNA (RiboNucleic\nacid) species, regulate gene expression post-transcriptionally by forming\nimperfect base-pair with the target sites primarily at the 3$'$ untranslated\nregions of the messenger RNAs. Since the 1993 discovery of the first miRNA\n\\emph{let-7} in worms, a vast amount of studies have been dedicated to\nfunctionally characterizing the functional impacts of miRNA in a network\ncontext to understand complex diseases such as cancer. Here we review several\nrepresentative unsupervised learning frameworks on inferring miRNA regulatory\nnetwork by exploiting the static sequence-based information pertinent to the\nprior knowledge of miRNA targeting and the dynamic information of miRNA\nactivities implicated by the recently available large data compendia, which\ninterrogate genome-wide expression profiles of miRNAs and/or mRNAs across\nvarious cell conditions.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2015 15:52:38 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Wong", "Ka-Chun", ""], ["Li", "Yue", ""], ["Zhang", "Zhaolei", ""]]}, {"id": "1508.00662", "submitter": "Brendon Brewer", "authors": "Brendon J. Brewer, David Huijser, Geraint F. Lewis", "title": "Trans-Dimensional Bayesian Inference for Gravitational Lens\n  Substructures", "comments": "Accepted for publication in MNRAS. 12 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM astro-ph.CO astro-ph.GA physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a Bayesian solution to the problem of inferring the density\nprofile of strong gravitational lenses when the lens galaxy may contain\nmultiple dark or faint substructures. The source and lens models are based on a\nsuperposition of an unknown number of non-negative basis functions (or \"blobs\")\nwhose form was chosen with speed as a primary criterion. The prior distribution\nfor the blobs' properties is specified hierarchically, so the mass function of\nsubstructures is a natural output of the method. We use reversible jump Markov\nChain Monte Carlo (MCMC) within Diffusive Nested Sampling (DNS) to sample the\nposterior distribution and evaluate the marginal likelihood of the model,\nincluding the summation over the unknown number of blobs in the source and the\nlens. We demonstrate the method on two simulated data sets: one with a single\nsubstructure, and one with ten. We also apply the method to the g-band image of\nthe \"Cosmic Horseshoe\" system, and find evidence for more than zero\nsubstructures. However, these have large spatial extent and probably only point\nto misspecifications in the model (such as the shape of the smooth lens\ncomponent or the point spread function), which are difficult to guard against\nin full generality.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2015 05:33:01 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2015 15:12:10 GMT"}], "update_date": "2015-10-12", "authors_parsed": [["Brewer", "Brendon J.", ""], ["Huijser", "David", ""], ["Lewis", "Geraint F.", ""]]}, {"id": "1508.01023", "submitter": "Bokai Cao", "authors": "Bokai Cao, Xiangnan Kong, Philip S. Yu", "title": "A review of heterogeneous data mining for brain disorders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE cs.DB q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With rapid advances in neuroimaging techniques, the research on brain\ndisorder identification has become an emerging area in the data mining\ncommunity. Brain disorder data poses many unique challenges for data mining\nresearch. For example, the raw data generated by neuroimaging experiments is in\ntensor representations, with typical characteristics of high dimensionality,\nstructural complexity and nonlinear separability. Furthermore, brain\nconnectivity networks can be constructed from the tensor data, embedding subtle\ninteractions between brain regions. Other clinical measures are usually\navailable reflecting the disease status from different perspectives. It is\nexpected that integrating complementary information in the tensor data and the\nbrain network data, and incorporating other clinical parameters will be\npotentially transformative for investigating disease mechanisms and for\ninforming therapeutic interventions. Many research efforts have been devoted to\nthis area. They have achieved great success in various applications, such as\ntensor-based modeling, subgraph pattern mining, multi-view feature analysis. In\nthis paper, we review some recent data mining methods that are used for\nanalyzing brain disorders.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 09:57:32 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Cao", "Bokai", ""], ["Kong", "Xiangnan", ""], ["Yu", "Philip S.", ""]]}, {"id": "1508.01122", "submitter": "Ali Akbar Jafari", "authors": "Rasool Roozegar and Ali Akbar Jafari", "title": "On Bivariate Generalized Linear Failure Rate-Power Series Class of\n  Distributions", "comments": "arXiv admin note: substantial text overlap with arXiv:1508.00219", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently it has been observed that the bivariate generalized linear failure\nrate distribution can be used quite effectively to analyze lifetime data in two\ndimensions. This paper introduces a more general class of bivariate\ndistributions. We refer to this new class of distributions as bivariate\ngeneralized linear failure rate power series model. This new class of bivariate\ndistributions contains several lifetime models such as: generalized linear\nfailure rate-power series, bivariate generalized linear failure rate and\nbivariate generalized linear failure rate geometric distributions as special\ncases among others. The construction and characteristics of the proposed\nbivariate distribution are presented along with estimation procedures for the\nmodel parameters based on maximum likelihood. The marginal and conditional laws\nare also studied. We present an application to the real data set where our\nmodel provides a better fit than other models.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 16:22:11 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Roozegar", "Rasool", ""], ["Jafari", "Ali Akbar", ""]]}, {"id": "1508.01217", "submitter": "Lorin Crawford", "authors": "Lorin Crawford, Kris C. Wood, Xiang Zhou, and Sayan Mukherjee", "title": "Bayesian Approximate Kernel Regression with Variable Selection", "comments": "22 pages, 3 figures, 3 tables; theory added; new simulations\n  presented; references added", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear kernel regression models are often used in statistics and machine\nlearning because they are more accurate than linear models. Variable selection\nfor kernel regression models is a challenge partly because, unlike the linear\nregression setting, there is no clear concept of an effect size for regression\ncoefficients. In this paper, we propose a novel framework that provides an\neffect size analog of each explanatory variable for Bayesian kernel regression\nmodels when the kernel is shift-invariant --- for example, the Gaussian kernel.\nWe use function analytic properties of shift-invariant reproducing kernel\nHilbert spaces (RKHS) to define a linear vector space that: (i) captures\nnonlinear structure, and (ii) can be projected onto the original explanatory\nvariables. The projection onto the original explanatory variables serves as an\nanalog of effect sizes. The specific function analytic property we use is that\nshift-invariant kernel functions can be approximated via random Fourier bases.\nBased on the random Fourier expansion we propose a computationally efficient\nclass of Bayesian approximate kernel regression (BAKR) models for both\nnonlinear regression and binary classification for which one can compute an\nanalog of effect sizes. We illustrate the utility of BAKR by examining two\nimportant problems in statistical genetics: genomic selection (i.e. phenotypic\nprediction) and association mapping (i.e. inference of significant variants or\nloci). State-of-the-art methods for genomic selection and association mapping\nare based on kernel regression and linear models, respectively. BAKR is the\nfirst method that is competitive in both settings.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 20:40:11 GMT"}, {"version": "v2", "created": "Thu, 21 Apr 2016 14:37:03 GMT"}, {"version": "v3", "created": "Tue, 16 May 2017 18:22:14 GMT"}, {"version": "v4", "created": "Sat, 10 Jun 2017 00:57:28 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Crawford", "Lorin", ""], ["Wood", "Kris C.", ""], ["Zhou", "Xiang", ""], ["Mukherjee", "Sayan", ""]]}, {"id": "1508.01278", "submitter": "Nicholas Chamandy", "authors": "Nicholas Chamandy, Omkar Muralidharan, Stefan Wager", "title": "Teaching Statistics at Google Scale", "comments": "To appear in The American Statistician", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern data and applications pose very different challenges from those of the\n1950s or even the 1980s. Students contemplating a career in statistics or data\nscience need to have the tools to tackle problems involving massive,\nheavy-tailed data, often interacting with live, complex systems. However,\ndespite the deepening connections between engineering and modern data science,\nwe argue that training in classical statistical concepts plays a central role\nin preparing students to solve Google-scale problems. To this end, we present\nthree industrial applications where significant modern data challenges were\novercome by statistical thinking.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2015 04:22:21 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2015 03:05:02 GMT"}, {"version": "v3", "created": "Sun, 16 Aug 2015 17:19:39 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Chamandy", "Nicholas", ""], ["Muralidharan", "Omkar", ""], ["Wager", "Stefan", ""]]}, {"id": "1508.01280", "submitter": "Zhou Fan", "authors": "Zhou Fan and Lester Mackey", "title": "Empirical Bayesian analysis of simultaneous changepoints in multiple\n  data sequences", "comments": "31 pages, 11 figures v3: Modify synthetic data comparisons based on\n  reviewer feedback", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Copy number variations in cancer cells and volatility fluctuations in stock\nprices are commonly manifested as changepoints occurring at the same positions\nacross related data sequences. We introduce a Bayesian modeling framework,\nBASIC, that employs a changepoint prior to capture the co-occurrence tendency\nin data of this type. We design efficient algorithms to sample from and\nmaximize over the BASIC changepoint posterior and develop a Monte Carlo\nexpectation-maximization procedure to select prior hyperparameters in an\nempirical Bayes fashion. We use the resulting BASIC framework to analyze DNA\ncopy number variations in the NCI-60 cancer cell lines and to identify\nimportant events that affected the price volatility of S&P 500 stocks from 2000\nto 2009.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2015 04:42:37 GMT"}, {"version": "v2", "created": "Sun, 24 Jul 2016 00:48:27 GMT"}, {"version": "v3", "created": "Fri, 14 Apr 2017 03:08:27 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Fan", "Zhou", ""], ["Mackey", "Lester", ""]]}, {"id": "1508.01351", "submitter": "Vanesa Jorda", "authors": "Vanesa Jorda, Jose M. Alonso", "title": "Measuring educational attainment as a continuous variable: a new\n  database (1970-2010)", "comments": "29 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new comprehensive data set on educational\nattainment and inequality measures of education for 142 countries over the\nperiod 1970 to 2010. Most of the previous attempts to measure educational\nattainment have treated education as a categorical variable, whose mean is\ncomputed as a weighted average of the official duration of each cycle and\nattainment rates, thus omitting differences in educational achievement within\nlevels of education. This aggregation into different groups may result in a\nloss of information introducing, therefore, a potential source of measurement\nerror. We explore here a more nuanced alternative to estimate educational\nattainment, which considers the continuous nature of the educational variable.\nThis `continuous approach' allows us to impose more plausible assumptions about\nthe distribution of years of schooling within each level of education, and to\ntake into account the right censoring of the data in the estimation, thus\nleading to more accurate estimates of educational attainment and education\ninequality. These improved series may help to better understand the role of\neducation on different socio-economic aspects, such as quality of life and\nhuman capital formation.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2015 10:32:00 GMT"}, {"version": "v2", "created": "Tue, 7 Jun 2016 11:54:53 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Jorda", "Vanesa", ""], ["Alonso", "Jose M.", ""]]}, {"id": "1508.01397", "submitter": "Annette M\\\"oller", "authors": "Annette M\\\"oller and J\\\"urgen Gro{\\ss}", "title": "Probabilistic temperature forecasting based on an ensemble AR\n  modification", "comments": "18 pages, 4 figures, 5 tables", "journal-ref": null, "doi": "10.1002/qj.2741", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To address the uncertainty in outputs of numerical weather prediction (NWP)\nmodels, ensembles of forecasts are used. To obtain such an ensemble of\nforecasts the NWP model is run multiple times, each time with different\nformulations and/or initial or boundary conditions. To correct for possible\nbiases and dispersion errors in the ensemble, statistical postprocessing models\nare frequently employed. These statistical models yield full predictive\nprobability distributions for a weather quantity of interest and thus allow for\na more accurate assessment of forecast uncertainty. This paper proposes to\ncombine the state of the art Ensemble Model Output Statistics (EMOS) with an\nensemble that is adjusted by an AR process fitted to the respective error\nseries by a spread-adjusted linear pool (SLP) in case of temperature forecasts.\nThe basic ensemble modification technique we introduce may be used to simply\nadjust the ensemble itself as well as to obtain a full predictive distribution\nfor the weather quantity. As demonstrated for temperature forecasts of the\nEuropean Centre for Medium-Range Weather Forecasts (ECMWF) ensemble, the\nproposed procedure gives rise to improved results upon the basic (local) EMOS\nmethod.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2015 13:46:04 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["M\u00f6ller", "Annette", ""], ["Gro\u00df", "J\u00fcrgen", ""]]}, {"id": "1508.01551", "submitter": "Yan Li", "authors": "Yan Li, Kristofer G. Reyes, Jorge Vazquez-Anderson, Yingfei Wang,\n  Lydia M. Contreras, Warren B. Powell", "title": "A Knowledge Gradient Policy for Sequencing Experiments to Identify the\n  Structure of RNA Molecules Using a Sparse Additive Belief Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a sparse knowledge gradient (SpKG) algorithm for adaptively\nselecting the targeted regions within a large RNA molecule to identify which\nregions are most amenable to interactions with other molecules. Experimentally,\nsuch regions can be inferred from fluorescence measurements obtained by binding\na complementary probe with fluorescence markers to the targeted regions. We use\na biophysical model which shows that the fluorescence ratio under the log scale\nhas a sparse linear relationship with the coefficients describing the\naccessibility of each nucleotide, since not all sites are accessible (due to\nthe folding of the molecule). The SpKG algorithm uniquely combines the Bayesian\nranking and selection problem with the frequentist $\\ell_1$ regularized\nregression approach Lasso. We use this algorithm to identify the sparsity\npattern of the linear model as well as sequentially decide the best regions to\ntest before experimental budget is exhausted. Besides, we also develop two\nother new algorithms: batch SpKG algorithm, which generates more suggestions\nsequentially to run parallel experiments; and batch SpKG with a procedure which\nwe call length mutagenesis. It dynamically adds in new alternatives, in the\nform of types of probes, are created by inserting, deleting or mutating\nnucleotides within existing probes. In simulation, we demonstrate these\nalgorithms on the Group I intron (a mid-size RNA molecule), showing that they\nefficiently learn the correct sparsity pattern, identify the most accessible\nregion, and outperform several other policies.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2015 22:03:34 GMT"}], "update_date": "2015-08-10", "authors_parsed": [["Li", "Yan", ""], ["Reyes", "Kristofer G.", ""], ["Vazquez-Anderson", "Jorge", ""], ["Wang", "Yingfei", ""], ["Contreras", "Lydia M.", ""], ["Powell", "Warren B.", ""]]}, {"id": "1508.01976", "submitter": "Hau-tieng Wu", "authors": "Yae-lin Sheu and Hau-tieng Wu and Liang-Yan Hsu", "title": "Exploring laser-driven quantum phenomena from a time-frequency analysis\n  perspective: A comprehensive study", "comments": null, "journal-ref": "Optics Express Vol. 23, Issue 23, pp. 30459-30482 (2015)", "doi": "10.1364/OE.23.030459", "report-no": null, "categories": "physics.data-an physics.optics stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-frequency (TF) analysis is a powerful tool for exploring ultrafast\ndynamics in atoms and molecules. While some TF methods have demonstrated their\nusefulness and potential in several of quantum systems, a systematic comparison\namong these methods is still lacking. To this end, we compare a series of\nclassical and contemporary TF methods by taking hydrogen atom in a strong laser\nfield as a benchmark. In addition, several TF methods such as Cohen class\ndistribution other than the Wigner-Ville distribution, reassignment methods,\nand the empirical mode decomposition method are first introduced to exploration\nof ultrafast dynamics. Among these TF methods, the synchrosqueezing transform\nsuccessfully illustrates the physical mechanisms in the multiphoton ionization\nregime and in the tunneling ionization regime. Furthermore, an empirical\nprocedure to analyze an unknown complicated quantum system is provided,\nindicating the versatility of TF analysis as a new viable venue for exploring\nquantum dynamics.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2015 00:41:34 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Sheu", "Yae-lin", ""], ["Wu", "Hau-tieng", ""], ["Hsu", "Liang-Yan", ""]]}, {"id": "1508.02010", "submitter": "Niamh Cahill", "authors": "Niamh Cahill, Andrew C. Kemp, Benjamin P. Horton and Andrew C. Parnell", "title": "A Bayesian Hierarchical Model for Reconstructing Sea Levels: From Raw\n  Data to Rates of Change", "comments": "27 pages, 7 figures", "journal-ref": null, "doi": "10.5194/cp-12-525-2016", "report-no": null, "categories": "stat.AP physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a holistic Bayesian hierarchical model for reconstructing the\ncontinuous and dynamic evolution of relative sea-level (RSL) change with fully\nquantified uncertainty. The reconstruction is produced from biological\n(foraminifera) and geochemical ({\\delta}13C) sea-level indicators preserved in\ndated cores of salt-marsh sediment. Our model is comprised of three modules:\n(1) A Bayesian transfer function for the calibration of foraminifera into tidal\nelevation, which is flexible enough to formally accommodate additional proxies\n(in this case bulk-sediment {\\delta}13C values); (2) A chronology developed\nfrom an existing Bchron age-depth model, and (3) An existing\nerrors-in-variables integrated Gaussian process (EIV-IGP) model for estimating\nrates of sea-level change. We illustrate our approach using a case study of\nCommon Era sea-level variability from New Jersey, U.S.A. We develop a new\nBayesian transfer function (B-TF), with and without the {\\delta}13C proxy and\ncompare our results to those from a widely-used weighted-averaging transfer\nfunction (WA-TF). The formal incorporation of a second proxy into the B-TF\nmodel results in smaller vertical uncertainties and improved accuracy for\nreconstructed RSL. The vertical uncertainty from the multi-proxy B-TF is ~28%\nsmaller on average compared to the WA-TF. When evaluated against historic\ntide-gauge measurements, the multi-proxy B-TF most accurately reconstructs the\nRSL changes observed in the instrumental record (MSE = 0.003). The holistic\nmodel provides a single, unifying framework for reconstructing and analysing\nsea level through time. This approach is suitable for reconstructing other\npaleoenvironmental variables using biological proxies.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2015 10:30:57 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Cahill", "Niamh", ""], ["Kemp", "Andrew C.", ""], ["Horton", "Benjamin P.", ""], ["Parnell", "Andrew C.", ""]]}, {"id": "1508.02244", "submitter": "Marcel Ausloos", "authors": "Marcel Ausloos", "title": "France new regions planning? Better order or more disorder ?", "comments": "13 pages; 5 Tables; 8 figures; 28 references; prepared for and to be\n  published in Entropy", "journal-ref": "Entropy, 17(8), 5695-5710 (2015)", "doi": "10.3390/e17085695", "report-no": null, "categories": "physics.soc-ph nlin.AO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper grounds the critique of the 'reduction of regions in a country'\nnot only in its geographical and social context but also in its entropic space.\nThe various recent plans leading to the reduction of the number of regions in\nmetropolitan France are discussed, based on the mere distribution in the number\nof cities in the plans and analyzed according to various distribution laws.\nEach case, except the present distribution with 22 regions, on the mainland,\ndoes not seem to fit presently used theoretical models. Beside, the number of\ninhabitants is examined in each plan. The same conclusion holds. Therefore a\ntheoretical argument based on entropy considerations is proposed, thereby\npointing to whether more order or less disorder is the key question, -\ndiscounting political considerations.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2015 13:47:26 GMT"}], "update_date": "2015-08-17", "authors_parsed": [["Ausloos", "Marcel", ""]]}, {"id": "1508.02669", "submitter": "Yubo Wang", "authors": "Yubo Wang, Bin Wang, Rui Huang, Chi-Cheng Chu, Hemanshu R. Pota and\n  Rajit Gadh", "title": "Two-Tier Prediction of Solar Power Generation with Limited Sensing\n  Resource", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a typical solar installations scenario with limited\nsensing resources. In the literature, there exist either day-ahead solar\ngeneration prediction methods with limited accuracy, or high accuracy short\ntimescale methods that are not suitable for applications requiring longer term\nprediction. We propose a two-tier (global-tier and local-tier) prediction\nmethod to improve accuracy for long term (24 hour) solar generation prediction\nusing only the historical power data. In global-tier, we examine two popular\nheuristic methods: weighted k-Nearest Neighbors (k-NN) and Neural Network (NN).\nIn local-tier, the global-tier results are adaptively updated using real-time\nanalytical residual analysis. The proposed method is validated using the UCLA\nMicrogrid with 35kW of solar generation capacity. Experimental results show\nthat the proposed two-tier prediction method achieves higher accuracy compared\nto day-ahead predictions while providing the same prediction length. The\ndifference in the overall prediction performance using either weighted k-NN\nbased or NN based in the global-tier are carefully discussed and reasoned. Case\nstudies with a typical sunny day and a cloudy day are carried out to\ndemonstrate the effectiveness of the proposed two-tier predictions.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2015 17:53:19 GMT"}], "update_date": "2015-08-12", "authors_parsed": [["Wang", "Yubo", ""], ["Wang", "Bin", ""], ["Huang", "Rui", ""], ["Chu", "Chi-Cheng", ""], ["Pota", "Hemanshu R.", ""], ["Gadh", "Rajit", ""]]}, {"id": "1508.02824", "submitter": "Paul Larsen", "authors": "Paul Larsen", "title": "Asyptotic Normality for Maximum Likelihood Estimation and Operational\n  Risk", "comments": "Split previous arXiv submission into two parts for journal\n  submission. A slightly modified version of this paper will appear in the\n  Journal of Operational Risk. The second part on stability of OpVar will be\n  posted separately", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Operational risk models commonly employ maximum likelihood estimation (MLE)\nto fit loss data to heavy-tailed distributions. Yet several desirable\nproperties of MLE (e.g. asymptotic normality) are generally valid only for\nlarge sample-sizes, a situation rarely encountered in operational risk. In this\npaper, we study how asymptotic normality does--or does not--hold for common\nseverity distributions in operational risk models. We then apply these results\nto evaluate errors caused by failure of asymptotic normality in constructing\nconfidence intervals around the MLE fitted parameters.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2015 06:36:15 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2015 17:46:26 GMT"}, {"version": "v3", "created": "Thu, 25 Aug 2016 16:03:56 GMT"}], "update_date": "2016-08-26", "authors_parsed": [["Larsen", "Paul", ""]]}, {"id": "1508.02846", "submitter": "Ines Wilms", "authors": "Ines Wilms and Sarah Gelper and Christophe Croux", "title": "The predictive power of the business and bank sentiment of firms: A\n  high-dimensional Granger Causality approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the predictive power of industry-specific economic sentiment\nindicators for future macro-economic developments. In addition to the sentiment\nof firms towards their own business situation, we study their sentiment with\nrespect to the banking sector - their main credit providers. The use of\nindustry-specific sentiment indicators results in a high-dimensional\nforecasting problem. To identify the most predictive industries, we present a\nbootstrap Granger Causality test based on the Adaptive Lasso. This test is more\npowerful than the standard Wald test in such high-dimensional settings.\nForecast accuracy is improved by using only the most predictive industries\nrather than all industries.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2015 08:28:47 GMT"}], "update_date": "2015-08-13", "authors_parsed": [["Wilms", "Ines", ""], ["Gelper", "Sarah", ""], ["Croux", "Christophe", ""]]}, {"id": "1508.02942", "submitter": "Hao Han", "authors": "Hao Han, Yeming Ma, Wei Zhu", "title": "Galton's Family Heights Data Revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Galton's family heights data has been a preeminent historical dataset in\nregression analysis, on which the original model and basic results have\nsurvived the close scrutiny of statisticians for 125 years. However by\nrevisiting Galton's family data, we challenge whether Galton's classic model\nand his regression towards mean interpretation are proper. Using Galton's data\nas a benchmark for different regression methods, such as least squares,\northogonal regression, geometric mean regression, and least sine squares\nregression - a newly developed nonparametric robust regression approach, we\nelucidate that his regression model has fundamental drawbacks not only in\nvariable and model selection by \"transmuting\" women into men thus the simple\nlinear model, but also a strong bias in least squares regression leading to\notherwise alternative conclusions on the true relationships between the heights\nof the child and his or her parents.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2015 14:55:42 GMT"}], "update_date": "2015-08-13", "authors_parsed": [["Han", "Hao", ""], ["Ma", "Yeming", ""], ["Zhu", "Wei", ""]]}, {"id": "1508.02985", "submitter": "Gloria Gheno", "authors": "Gloria Gheno", "title": "The Causal Effects for a Causal Loglinear Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of the causality is important in many fields of research. I\npropose a causal theory to obtain the causal effects in a causal loglinear\nmodel. It calculates them using the odds ratio and Pearl's causal theory. The\neffects are calculated distinguishing between a simple mediation model (model\nwithout the multiplicative interaction effect) and a mediation model with the\nmultiplicative interaction effect. In both models it is possible also to\nanalyze the cell effect, which is a new interaction effect. Then in a causal\nloglinear model there are three interaction effects: multiplicative interaction\neffect, additive interaction effect and cell effect\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2015 16:43:45 GMT"}], "update_date": "2015-08-13", "authors_parsed": [["Gheno", "Gloria", ""]]}, {"id": "1508.03103", "submitter": "Dwueng-Chwuan Jhwueng", "authors": "Dwueng-Chwuan Jhwueng, Vasileios Maroulas", "title": "Adaptive Trait Evolution in Random Environment", "comments": "17 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current phylogenetic comparative methods generally employ the\nOrnstein-Uhlenbeck(OU) process for modeling trait evolution. Being able of\ntracking the optimum of a trait within a group of related species, the OU\nprocess provides information about the stabilizing selection where the\npopulation mean adopts a particular trait value. The optima of a trait may\nfollow certain stochastic dynamics along the evolutionary history. In this\npaper, we extend the current framework by adopting a rate of evolution which\nbehave according to pertinent stochastic dynamics. The novel model is applied\nto analyze about 225 datasets collected from the existing literature. Results\nvalidate that the new framework provides a better fit for the majority of these\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2015 02:31:45 GMT"}], "update_date": "2015-08-14", "authors_parsed": [["Jhwueng", "Dwueng-Chwuan", ""], ["Maroulas", "Vasileios", ""]]}, {"id": "1508.03162", "submitter": "Dante Javier Paz", "authors": "Dante J. Paz, Ariel G. Sanchez", "title": "Improving the precision matrix for precision cosmology", "comments": "9 pages, 7 figures, minor changes to match version accepted by MNRAS", "journal-ref": null, "doi": "10.1093/mnras/stv2259", "report-no": null, "categories": "astro-ph.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of cosmological constraints from observations of the large\nscale structure of the Universe, such as the power spectrum or the correlation\nfunction, requires the knowledge of the inverse of the associated covariance\nmatrix, namely the precision matrix, $\\mathbf{\\Psi}$. In most analyses,\n$\\mathbf{\\Psi}$ is estimated from a limited set of mock catalogues. Depending\non how many mocks are used, this estimation has an associated error which must\nbe propagated into the final cosmological constraints. For future surveys such\nas Euclid and DESI, the control of this additional uncertainty requires a\nprohibitively large number of mock catalogues. In this work we test a novel\ntechnique for the estimation of the precision matrix, the covariance tapering\nmethod, in the context of baryon acoustic oscillation measurements. Even though\nthis technique was originally devised as a way to speed up maximum likelihood\nestimations, our results show that it also reduces the impact of noisy\nprecision matrix estimates on the derived confidence intervals, without\nintroducing biases on the target parameters. The application of this technique\ncan help future surveys to reach their true constraining power using a\nsignificantly smaller number of mock catalogues.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2015 09:52:04 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2015 16:47:40 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Paz", "Dante J.", ""], ["Sanchez", "Ariel G.", ""]]}, {"id": "1508.03216", "submitter": "Danilo Orlando", "authors": "Antonio De Maio and Danilo Orlando", "title": "Adaptive Radar Detection of a Subspace Signal Embedded in Subspace\n  Structured plus Gaussian Interference Via Invariance", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2015.2507544", "report-no": null, "categories": "stat.AP cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with adaptive radar detection of a subspace signal competing\nwith two sources of interference. The former is Gaussian with unknown\ncovariance matrix and accounts for the joint presence of clutter plus thermal\nnoise. The latter is structured as a subspace signal and models coherent pulsed\njammers impinging on the radar antenna. The problem is solved via the Principle\nof Invariance which is based on the identification of a suitable group of\ntransformations leaving the considered hypothesis testing problem invariant. A\nmaximal invariant statistic, which completely characterizes the class of\ninvariant decision rules and significantly compresses the original data domain,\nas well as its statistical characterization are determined. Thus, the existence\nof the optimum invariant detector is addressed together with the design of\npractically implementable invariant decision rules. At the analysis stage, the\nperformance of some receivers belonging to the new invariant class is\nestablished through the use of analytic expressions.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2015 13:55:57 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["De Maio", "Antonio", ""], ["Orlando", "Danilo", ""]]}, {"id": "1508.03454", "submitter": "Conor Lawless", "authors": "Jonathan Heydari, Conor Lawless, David A. Lydall and Darren J.\n  Wilkinson", "title": "Bayesian hierarchical modelling for inferring genetic interactions in\n  yeast", "comments": "To appear in Journal of Royal Statistical Society, Series C", "journal-ref": null, "doi": "10.1111/rssc.12126", "report-no": null, "categories": "q-bio.QM q-bio.GN stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Quantitative Fitness Analysis (QFA) is a high-throughput experimental and\ncomputational methodology for measuring the growth of microbial populations.\nQFA screens can be used to compare the health of cell populations with and\nwithout a mutation in a query gene in order to infer genetic interaction\nstrengths genome-wide, examining thousands of separate genotypes. We introduce\nBayesian, hierarchical models of population growth rates and genetic\ninteractions that better reflect QFA experimental design than current\napproaches. Our new approach models population dynamics and genetic interaction\nsimultaneously, thereby avoiding passing information between models via a\nunivariate fitness summary. Matching experimental structure more closely,\nBayesian hierarchical approaches use data more efficiently and find new\nevidence for genes which interact with yeast telomeres within a published\ndataset.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2015 09:53:19 GMT"}], "update_date": "2016-01-18", "authors_parsed": [["Heydari", "Jonathan", ""], ["Lawless", "Conor", ""], ["Lydall", "David A.", ""], ["Wilkinson", "Darren J.", ""]]}, {"id": "1508.03498", "submitter": "Xin Yuan", "authors": "Xin Yuan, Hong Jiang, Gang Huang, Paul Wilford", "title": "Lensless Compressive Imaging", "comments": "37 pages, 10 figures. Submitted to SIAM Journal on Imaging Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a lensless compressive imaging architecture, which consists of an\naperture assembly and a single sensor, without using any lens. An anytime\nalgorithm is proposed to reconstruct images from the compressive measurements;\nthe algorithm produces a sequence of solutions that monotonically converge to\nthe true signal (thus, anytime). The algorithm is developed based on the\nsparsity of local overlapping patches (in the transformation domain) and\nstate-of-the-art results have been obtained. Experiments on real data\ndemonstrate that encouraging results are obtained by measuring about 10% (of\nthe image pixels) compressive measurements. The reconstruction results of the\nproposed algorithm are compared with the JPEG compression (based on file sizes)\nand the reconstructed image quality is close to the JPEG compression, in\nparticular at a high compression rate.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2015 13:34:10 GMT"}], "update_date": "2015-08-17", "authors_parsed": [["Yuan", "Xin", ""], ["Jiang", "Hong", ""], ["Huang", "Gang", ""], ["Wilford", "Paul", ""]]}, {"id": "1508.03662", "submitter": "Zhenxue Dai", "authors": "Stephen Shield and Zhenxue Dai", "title": "Comparison of Uncertainty of Two Precipitation Prediction Models", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ao-ph physics.geo-ph stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Meteorological inputs are an important part of subsurface flow and transport\nmodeling. The choice of source for meteorological data used as inputs has\nsignificant impacts on the results of subsurface flow and transport studies.\nOne method to obtain the meteorological data required for flow and transport\nstudies is the use of weather generating models. This paper compares the\ndifference in performance of two weather generating models at Technical Area 54\nof Los Alamos National Lab. Technical Area 54 is contains several waste pits\nfor low-level radioactive waste and is the site for subsurface flow and\ntransport studies. This makes the comparison of the performance of the two\nweather generators at this site particularly valuable.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2015 21:05:00 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Shield", "Stephen", ""], ["Dai", "Zhenxue", ""]]}, {"id": "1508.03690", "submitter": "Sijia Liu", "authors": "Sijia Liu, Sundeep Prabhakar Chepuri, Makan Fardad, Engin Masazade,\n  Geert Leus, Pramod K. Varshney", "title": "Sensor Selection for Estimation with Correlated Measurement Noise", "comments": "IEEE Transactions on Signal Processing (accepted)", "journal-ref": null, "doi": "10.1109/TSP.2016.2550005", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of sensor selection for parameter\nestimation with correlated measurement noise. We seek optimal sensor\nactivations by formulating an optimization problem, in which the estimation\nerror, given by the trace of the inverse of the Bayesian Fisher information\nmatrix, is minimized subject to energy constraints. Fisher information has been\nwidely used as an effective sensor selection criterion. However, existing\ninformation-based sensor selection methods are limited to the case of\nuncorrelated noise or weakly correlated noise due to the use of approximate\nmetrics. By contrast, here we derive the closed form of the Fisher information\nmatrix with respect to sensor selection variables that is valid for any\narbitrary noise correlation regime, and develop both a convex relaxation\napproach and a greedy algorithm to find near-optimal solutions. We further\nextend our framework of sensor selection to solve the problem of sensor\nscheduling, where a greedy algorithm is proposed to determine non-myopic\n(multi-time step ahead) sensor schedules. Lastly, numerical results are\nprovided to illustrate the effectiveness of our approach, and to reveal the\neffect of noise correlation on estimation performance.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2015 03:08:48 GMT"}, {"version": "v2", "created": "Mon, 21 Mar 2016 21:18:48 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Liu", "Sijia", ""], ["Chepuri", "Sundeep Prabhakar", ""], ["Fardad", "Makan", ""], ["Masazade", "Engin", ""], ["Leus", "Geert", ""], ["Varshney", "Pramod K.", ""]]}, {"id": "1508.03747", "submitter": "Subhadeep Mukhopadhyay", "authors": "Scott Bruce, Zeda Li, Hsiang-Chieh Yang, and Subhadeep Mukhopadhyay", "title": "Nonparametric Distributed Learning Architecture for Big Data: Algorithm\n  and Applications", "comments": "The purpose of this paper is to answer the question: What is the\n  relevance of small-data-ideas in this big-data world? The bigger question is:\n  Should we make difficult things easy or easy things look difficult? The first\n  option will probably make some impact in the long-run, but the second one\n  will surely earn prestigious journal publications in short-run, IEEE\n  Transactions on Big Data (forthcoming). The first report came out in 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dramatic increases in the size and complexity of modern datasets have made\ntraditional \"centralized\" statistical inference prohibitive. In addition to\ncomputational challenges associated with big data learning, the presence of\nnumerous data types (e.g. discrete, continuous, categorical, etc.) makes\nautomation and scalability difficult. A question of immediate concern is how to\ndesign a data-intensive statistical inference architecture without changing the\nbasic statistical modeling principles developed for \"small\" data over the last\ncentury. To address this problem, we present MetaLP, a flexible, distributed\nstatistical modeling framework.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2015 16:13:29 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2015 21:05:41 GMT"}, {"version": "v3", "created": "Fri, 5 Feb 2016 03:10:19 GMT"}, {"version": "v4", "created": "Sat, 2 Apr 2016 16:14:33 GMT"}, {"version": "v5", "created": "Mon, 26 Feb 2018 16:24:30 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Bruce", "Scott", ""], ["Li", "Zeda", ""], ["Yang", "Hsiang-Chieh", ""], ["Mukhopadhyay", "Subhadeep", ""]]}, {"id": "1508.03768", "submitter": "Jack Bowden", "authors": "Jack Bowden and Chris Jackson", "title": "On the physical interpretation of a meta-analysis in the presence of\n  heterogeneity and bias: from clinical trials to Mendelian randomization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The funnel plot is a graphical visualisation of summary data estimates from a\nmeta-analysis, and is a useful tool for detecting departures from the standard\nmodelling assumptions. Although perhaps not widely appreciated, a simple\nextension of the funnel plot can help to facilitate an intuitive interpretation\nof the mathematics underlying a meta-analysis at a more fundamental level, by\nequating it to determining the centre of mass of a physical system. We used\nthis analogy, with some success, to explain the concepts of weighing evidence\nand of biased evidence to a young audience at the Cambridge Science Festival,\nwithout recourse to precise definitions or statistical formulae. In this paper\nwe aim to formalise this analogy at a more technical level using the estimating\nequation framework: firstly, to help elucidate some of the basic statistical\nmodels employed in a meta-analysis and secondly, to forge new connections\nbetween bias adjustment in the evidence synthesis and causal inference\nliteratures.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2015 20:25:00 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Bowden", "Jack", ""], ["Jackson", "Chris", ""]]}, {"id": "1508.03950", "submitter": "Lutz Bornmann Dr.", "authors": "Lutz Bornmann, Moritz Stefaner, Felix de Moya Anegon, and Ruediger\n  Mutz", "title": "Excellence networks in science: A Web-based application based on\n  Bayesian multilevel logistic regression (BMLR) for the identification of\n  institutions collaborating successfully", "comments": "accepted for publication in the Journal of Informetrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study we present an application which can be accessed via\nwww.excellence-networks.net and which represents networks of scientific\ninstitutions worldwide. The application is based on papers (articles, reviews\nand conference papers) published between 2007 and 2011. It uses (network) data,\non which the SCImago Institutions Ranking is based (Scopus data from Elsevier).\nUsing this data, institutional networks have been estimated with statistical\nmodels (Bayesian multilevel logistic regression, BMLR) for a number of Scopus\nsubject areas. Within single subject areas, we have investigated and visualized\nhow successfully overall an institution (reference institution) has\ncollaborated (compared to all the other institutions in a subject area), and\nwith which other institutions (network institutions) a reference institution\nhas collaborated particularly successfully. The \"best paper rate\"\n(statistically estimated) was used as an indicator for evaluating the\ncollaboration success of an institution. This gives the proportion of highly\ncited papers from an institution, and is considered generally as an indicator\nfor measuring impact in bibliometrics.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2015 08:39:02 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2015 11:51:49 GMT"}, {"version": "v3", "created": "Mon, 11 Jan 2016 16:01:58 GMT"}, {"version": "v4", "created": "Tue, 12 Jan 2016 07:36:53 GMT"}], "update_date": "2016-01-13", "authors_parsed": [["Bornmann", "Lutz", ""], ["Stefaner", "Moritz", ""], ["Anegon", "Felix de Moya", ""], ["Mutz", "Ruediger", ""]]}, {"id": "1508.03981", "submitter": "Olga Kolchyna", "authors": "Olga Kolchyna, Th'arsis T. P. Souza, Tomaso Aste, Philip C. Treleaven", "title": "In Quest of Significance: Identifying Types of Twitter Sentiment Events\n  that Predict Spikes in Sales", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the power of Twitter events to predict consumer sales events by\nanalysing sales for 75 companies from the retail sector and over 150 million\ntweets mentioning those companies along with their sentiment. We suggest an\napproach for events identification on Twitter extending existing methodologies\nof event study. We also propose a robust method for clustering Twitter events\ninto different types based on their shape, which captures the varying dynamics\nof information propagation through the social network. We provide empirical\nevidence that through events differentiation based on their shape we can\nclearly identify types of Twitter events that have a more significant power to\npredict spikes in sales than the aggregated Twitter signal.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2015 11:35:13 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Kolchyna", "Olga", ""], ["Souza", "Th'arsis T. P.", ""], ["Aste", "Tomaso", ""], ["Treleaven", "Philip C.", ""]]}, {"id": "1508.04126", "submitter": "Robert McKilliam", "authors": "Assad Akhlaq, R. G. McKilliam, and R. Subramanian", "title": "Basis construction for range estimation by phase unwrapping", "comments": "submitted to IEEE Signal Processing Letters", "journal-ref": null, "doi": "10.1109/LSP.2015.2465153", "report-no": null, "categories": "stat.AP cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the distance, or range, between two\nlocations by measuring the phase of a sinusoidal signal transmitted between the\nlocations. This method is only capable of unambiguously measuring range within\nan interval of length equal to the wavelength of the signal. To address this\nproblem signals of multiple different wavelengths can be transmitted. The range\ncan then be measured within an interval of length equal to the least common\nmultiple of these wavelengths. Estimation of the range requires solution of a\nproblem from computational number theory called the closest lattice point\nproblem. Algorithms to solve this problem require a basis for this lattice.\nConstructing a basis is non-trivial and an explicit construction has only been\ngiven in the case that the wavelengths can be scaled to pairwise relatively\nprime integers. In this paper we present an explicit construction of a basis\nwithout this assumption on the wavelengths. This is important because the\naccuracy of the range estimator depends upon the wavelengths. Simulations\nindicate that significant improvement in accuracy can be achieved by using\nwavelengths that cannot be scaled to pairwise relatively prime integers.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 07:38:30 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Akhlaq", "Assad", ""], ["McKilliam", "R. G.", ""], ["Subramanian", "R.", ""]]}, {"id": "1508.04149", "submitter": "Yen-Chi Chen", "authors": "Yen-Chi Chen, Shirley Ho, Ananth Tenneti, Rachel Mandelbaum, Rupert\n  Croft, Tiziana DiMatteo, Peter E. Freeman, Christopher R. Genovese, Larry\n  Wasserman", "title": "Investigating Galaxy-Filament Alignments in Hydrodynamic Simulations\n  using Density Ridges", "comments": "11 pages, 10 figures", "journal-ref": null, "doi": "10.1093/mnras/stv2260", "report-no": null, "categories": "astro-ph.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the filamentary structures and the galaxy alignment\nalong filaments at redshift $z=0.06$ in the MassiveBlack-II simulation, a\nstate-of-the-art, high-resolution hydrodynamical cosmological simulation which\nincludes stellar and AGN feedback in a volume of (100 Mpc$/h$)$^3$. The\nfilaments are constructed using the subspace constrained mean shift (SCMS;\nOzertem & Erdogmus (2011) and Chen et al. (2015a)). First, we show that\nreconstructed filaments using galaxies and reconstructed filaments using dark\nmatter particles are similar to each other; over $50\\%$ of the points on the\ngalaxy filaments have a corresponding point on the dark matter filaments within\ndistance $0.13$ Mpc$/h$ (and vice versa) and this distance is even smaller at\nhigh-density regions. Second, we observe the alignment of the major principal\naxis of a galaxy with respect to the orientation of its nearest filament and\ndetect a $2.5$ Mpc$/h$ critical radius for filament's influence on the\nalignment when the subhalo mass of this galaxy is between $10^9M_\\odot/h$ and\n$10^{12}M_\\odot/h$. Moreover, we find the alignment signal to increase\nsignificantly with the subhalo mass. Third, when a galaxy is close to filaments\n(less than $0.25$ Mpc$/h$), the galaxy alignment toward the nearest galaxy\ngroup depends on the galaxy subhalo mass. Finally, we find that galaxies close\nto filaments or groups tend to be rounder than those away from filaments or\ngroups.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2015 20:29:14 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Chen", "Yen-Chi", ""], ["Ho", "Shirley", ""], ["Tenneti", "Ananth", ""], ["Mandelbaum", "Rachel", ""], ["Croft", "Rupert", ""], ["DiMatteo", "Tiziana", ""], ["Freeman", "Peter E.", ""], ["Genovese", "Christopher R.", ""], ["Wasserman", "Larry", ""]]}, {"id": "1508.04152", "submitter": "Ilaria Spassiani M.Sc.", "authors": "Ilaria Spassiani and Giovanni Sebastiani", "title": "Exploring the relationship between the magnitudes of seismic events", "comments": null, "journal-ref": null, "doi": "10.1002/2015JB012398", "report-no": null, "categories": "stat.AP physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distribution of the magnitudes of seismic events is generally assumed to\nbe independent on past seismicity. However, by considering events in causal\nrelation, for example mother-daughter, it seems natural to assume that the\nmagnitude of a daughter event is conditionally dependent on the one of the\ncorresponding mother event. In order to find experimental evidence supporting\nthis hypothesis, we analyze different catalogs, both real and simulated, in two\ndifferent ways. From each catalog, we obtain the law of triggered events'\nmagnitude by kernel density. The results obtained show that the distribution\ndensity of triggered events' magnitude varies with the magnitude of their\ncorresponding mother events. As the intuition suggests, an increase of mother\nevents' magnitude induces an increase of the probability of having \"high\"\nvalues of triggered events' magnitude. In addition, we see a statistically\nsignificant increasing linear dependence of the magnitude means.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2015 10:46:41 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Spassiani", "Ilaria", ""], ["Sebastiani", "Giovanni", ""]]}, {"id": "1508.04153", "submitter": "J\\'er\\'emie Boulanger", "authors": "J\\'er\\'emie Boulanger, Ludovic Seifert, Romain H\\'erault,\n  Jean-Francois Coeurjolly", "title": "Automatic sensor-based detection and classification of climbing\n  activities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a method to automatically detect and classify climbing\nactivities using inertial measurement units (IMUs) attached to the wrists, feet\nand pelvis of the climber. The IMUs record limb acceleration and angular\nvelocity. Detection requires a learning phase with manual annotation to\nconstruct the statistical models used in the cusum algorithm. Full-body\nactivity is then classified based on the detection of each IMU.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2015 13:08:10 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Boulanger", "J\u00e9r\u00e9mie", ""], ["Seifert", "Ludovic", ""], ["H\u00e9rault", "Romain", ""], ["Coeurjolly", "Jean-Francois", ""]]}, {"id": "1508.04154", "submitter": "Marie Cottrell", "authors": "Anastasios Bellas (SAMM), Charles Bouveyron (MAP5), Marie Cottrell\n  (SAMM), Jerome Lacaille", "title": "Anomaly Detection Based on Confidence Intervals Using SOM with an\n  Application to Health Monitoring", "comments": null, "journal-ref": "T. Villmann, F.M. Schleif, M. Kaden, M. Lange. 10th International\n  Workshop on Self-Organizing Maps, Jul 2014, Mittweida, Germany. Springer,\n  295, pp.145-155, 2014, Advances in Self-Organizing Maps and Learning Vector\n  Quantization AISC", "doi": "10.1007/978-3-319-07695-9_14", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an application of SOM for the task of anomaly detection and\nvisualization. To remove the effect of exogenous independent variables, we use\na correction model which is more accurate than the usual one, since we apply\ndifferent linear models in each cluster of context. We do not assume any\nparticular probability distribution of the data and the detection method is\nbased on the distance of new data to the Kohonen map learned with corrected\nhealthy data. We apply the proposed method to the detection of aircraft engine\nanomalies.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 10:12:03 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Bellas", "Anastasios", "", "SAMM"], ["Bouveyron", "Charles", "", "MAP5"], ["Cottrell", "Marie", "", "SAMM"], ["Lacaille", "Jerome", ""]]}, {"id": "1508.04155", "submitter": "Alexander Koplenig", "authors": "Alexander Koplenig", "title": "Autocorrelated errors explain the apparent relationship between\n  disapproval of the US Congress and prosocial language", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, it has been claimed by Frimer et al. (2015) that there is a linear\nrelationship between the level of prosocial language and the level of public\ndisapproval of US Congress. A re-analysis demonstrates that this relationship\nis the result of a misspecified model that does not account for first-order\nautocorrelated disturbances. A Stata script to reproduce all presented results\nis available as an appendix.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 08:42:41 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Koplenig", "Alexander", ""]]}, {"id": "1508.04217", "submitter": "Yoshimasa Uematsu", "authors": "Yoshimasa Uematsu and Shinya Tanaka", "title": "Macroeconomic Forecasting and Variable Selection with a Very Large\n  Number of Predictors: A Penalized Regression Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies macroeconomic forecasting and variable selection using a\nfolded-concave penalized regression with a very large number of predictors. The\npenalized regression approach leads to sparse estimates of the regression\ncoefficients, and is applicable even if the dimensionality of the model is much\nlarger than the sample size. The first half of the paper discusses the\ntheoretical aspects of a folded-concave penalized regression when the model\nexhibits time series dependence. Specifically, we show the oracle inequality\nand the oracle property for ultrahigh-dimensional time-dependent regressors.\nThe latter half of the paper shows the validity of the penalized regression\nusing two motivating empirical applications. The first forecasts U.S. GDP with\nthe FRED-MD data using the MIDAS regression framework, where there are more\nthan 1000 covariates, while the sample size is at most 200. The second examines\nhow well the penalized regression screens the hidden portfolio with around 40\nstocks from more than 1800 potential stocks using NYSE stock price data. Both\napplications reveal that the penalized regression provides remarkable results\nin terms of forecasting performance and variable selection.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 05:33:06 GMT"}, {"version": "v2", "created": "Sun, 5 Mar 2017 06:44:29 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Uematsu", "Yoshimasa", ""], ["Tanaka", "Shinya", ""]]}, {"id": "1508.04325", "submitter": "Marie Auger-M\\'eth\\'e", "authors": "Marie Auger-M\\'eth\\'e, Chris Field, Christoffer M. Albertsen, Andrew\n  E. Derocher, Mark A. Lewis, Ian D. Jonsen and Joanna Mills Flemming", "title": "State-space models' dirty little secrets: even simple linear Gaussian\n  models can have estimation problems", "comments": null, "journal-ref": "Scientific Reports (2016) 6: 26677", "doi": "10.1038/srep26677", "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-space models (SSMs) are increasingly used in ecology to model\ntime-series such as animal movement paths and population dynamics. This type of\nhierarchical model is often structured to account for two levels of\nvariability: biological stochasticity and measurement error. SSMs are flexible.\nThey can model linear and nonlinear processes using a variety of statistical\ndistributions. Recent ecological SSMs are often complex, with a large number of\nparameters to estimate. Through a simulation study, we show that even simple\nlinear Gaussian SSMs can suffer from parameter- and state-estimation problems.\nWe demonstrate that these problems occur primarily when measurement error is\nlarger than biological stochasticity, the condition that often drives\necologists to use SSMs. Using an animal movement example, we show how these\nestimation problems can affect ecological inference. Biased parameter estimates\nof a SSM describing the movement of polar bears (\\textit{Ursus maritimus})\nresult in overestimating their energy expenditure. We suggest potential\nsolutions, but show that it often remains difficult to estimate parameters.\nWhile SSMs are powerful tools, they can give misleading results and we urge\necologists to assess whether the parameters can be estimated accurately before\ndrawing ecological conclusions from their results.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 14:10:49 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2015 19:05:41 GMT"}, {"version": "v3", "created": "Tue, 29 Mar 2016 14:54:28 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Auger-M\u00e9th\u00e9", "Marie", ""], ["Field", "Chris", ""], ["Albertsen", "Christoffer M.", ""], ["Derocher", "Andrew E.", ""], ["Lewis", "Mark A.", ""], ["Jonsen", "Ian D.", ""], ["Flemming", "Joanna Mills", ""]]}, {"id": "1508.04554", "submitter": "Bokai Cao", "authors": "Bokai Cao, Xiangnan Kong, Jingyuan Zhang, Philip S. Yu and Ann B.\n  Ragin", "title": "Mining Brain Networks using Multiple Side Views for Neurological\n  Disorder Identification", "comments": "in Proceedings of IEEE International Conference on Data Mining (ICDM)\n  2015", "journal-ref": null, "doi": "10.1109/ICDM.2015.50", "report-no": null, "categories": "cs.LG cs.CV cs.CY stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining discriminative subgraph patterns from graph data has attracted great\ninterest in recent years. It has a wide variety of applications in disease\ndiagnosis, neuroimaging, etc. Most research on subgraph mining focuses on the\ngraph representation alone. However, in many real-world applications, the side\ninformation is available along with the graph data. For example, for\nneurological disorder identification, in addition to the brain networks derived\nfrom neuroimaging data, hundreds of clinical, immunologic, serologic and\ncognitive measures may also be documented for each subject. These measures\ncompose multiple side views encoding a tremendous amount of supplemental\ninformation for diagnostic purposes, yet are often ignored. In this paper, we\nstudy the problem of discriminative subgraph selection using multiple side\nviews and propose a novel solution to find an optimal set of subgraph features\nfor graph classification by exploring a plurality of side views. We derive a\nfeature evaluation criterion, named gSide, to estimate the usefulness of\nsubgraph patterns based upon side views. Then we develop a branch-and-bound\nalgorithm, called gMSV, to efficiently search for optimal subgraph features by\nintegrating the subgraph mining process and the procedure of discriminative\nfeature selection. Empirical studies on graph classification tasks for\nneurological disorders using brain networks demonstrate that subgraph patterns\nselected by the multi-side-view guided subgraph selection approach can\neffectively boost graph classification performances and are relevant to disease\ndiagnosis.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2015 07:51:14 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Cao", "Bokai", ""], ["Kong", "Xiangnan", ""], ["Zhang", "Jingyuan", ""], ["Yu", "Philip S.", ""], ["Ragin", "Ann B.", ""]]}, {"id": "1508.04688", "submitter": "Albrecht Zimmermann", "authors": "Albrecht Zimmermann", "title": "Exploring chance in NCAA basketball", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There seems to be an upper limit to predicting the outcome of matches in\n(semi-)professional sports. Recent work has proposed that this is due to chance\nand attempts have been made to simulate the distribution of win percentages to\nidentify the most likely proportion of matches decided by chance. We argue that\nthe approach that has been chosen so far makes some simplifying assumptions\nthat cause its result to be of limited practical value. Instead, we propose to\nuse clustering of statistical team profiles and observed scheduling information\nto derive limits on the predictive accuracy for particular seasons, which can\nbe used to assess the performance of predictive models on those seasons. We\nshow that the resulting simulated distributions are much closer to the observed\ndistributions and give higher assessments of chance and tighter limits on\npredictive accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2015 16:05:32 GMT"}], "update_date": "2015-08-21", "authors_parsed": [["Zimmermann", "Albrecht", ""]]}, {"id": "1508.04819", "submitter": "Brian Keegan", "authors": "Brian C. Keegan, Shakked Lev, Ofer Arazy", "title": "Analyzing Organizational Routines in Online Knowledge Collaborations: A\n  Case for Sequence Analysis in CSCW", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC cs.IR physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research into socio-technical systems like Wikipedia has overlooked important\nstructural patterns in the coordination of distributed work. This paper argues\nfor a conceptual reorientation towards sequences as a fundamental unit of\nanalysis for understanding work routines in online knowledge collaboration. We\noutline a research agenda for researchers in computer-supported cooperative\nwork (CSCW) to understand the relationships, patterns, antecedents, and\nconsequences of sequential behavior using methods already developed in fields\nlike bio-informatics. Using a data set of 37,515 revisions from 16,616 unique\neditors to 96 Wikipedia articles as a case study, we analyze the prevalence and\nsignificance of different sequences of editing patterns. We illustrate the\nmixed method potential of sequence approaches by interpreting the frequent\npatterns as general classes of behavioral motifs. We conclude by discussing the\nmethodological opportunities for using sequence analysis for expanding existing\napproaches to analyzing and theorizing about co-production routines in online\nknowledge collaboration.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2015 22:36:21 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2015 15:17:09 GMT"}], "update_date": "2015-08-31", "authors_parsed": [["Keegan", "Brian C.", ""], ["Lev", "Shakked", ""], ["Arazy", "Ofer", ""]]}, {"id": "1508.04904", "submitter": "Brendan Guillouet", "authors": "Philippe Besse (INSA Toulouse, IMT), Brendan Guillouet (IMT),\n  Jean-Michel Loubes, Royer Fran\\c{c}ois", "title": "Review and Perspective for Distance Based Trajectory Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we tackle the issue of clustering trajectories of geolocalized\nobservations. Using clustering technics based on the choice of a distance\nbetween the observations, we first provide a comprehensive review of the\ndifferent distances used in the literature to compare trajectories. Then based\non the limitations of these methods, we introduce a new distance : Symmetrized\nSegment-Path Distance (SSPD). We finally compare this new distance to the\nothers according to their corresponding clustering results obtained using both\nhierarchical clustering and affinity propagation methods.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 07:46:15 GMT"}], "update_date": "2015-08-21", "authors_parsed": [["Besse", "Philippe", "", "INSA Toulouse, IMT"], ["Guillouet", "Brendan", "", "IMT"], ["Loubes", "Jean-Michel", ""], ["Fran\u00e7ois", "Royer", ""]]}, {"id": "1508.04939", "submitter": "Bertrand Roehner", "authors": "Peter Richmond and Bertrand M. Roehner", "title": "Effect of marital status on death rates. Part 1: High accuracy\n  exploration of the Farr-Bertillon effect", "comments": "30 pages, 17 figures", "journal-ref": null, "doi": "10.1016/j.physa.2015.12.136", "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Farr-Bertillon law says that for all age-groups the death rate of married\npeople is lower than the death rate of people who are not married (i.e. single,\nwidowed or divorced). Although this law has been known for over 150 years, it\nhas never been established with great accuracy. This even let some authors\nargue that it was a statistical artefact. It is true that the data must be\nselected and analyzed with great care, especially for age groups of small size\nsuch as widowers under 25. The observations reported in this paper were\nselected and designed in the same way as experiments in physics, that is to say\nwith the objective of minimizing the error bars for all age-groups. It will be\nseen that data appropriate for mid-age groups may be unsuitable for young age\ngroups and vice versa. The investigation led to the following results. (1) The\nFB effect is basically the same for men and women, except that on average it is\nabout 20\\% stronger for men. (2) There is a marked difference between single or\ndivorced persons on the one hand, for whom the effect is largest around the age\nof 45, and widowed persons on the other hand, for whom the effect is largest\naround the age of 25. (3) When different causes of death are distinguished, the\neffect is largest for suicide and smallest for cancer. (4) For young widowers\nthe death rates are up to 10 times higher than for married persons of same age.\nThis extreme form of the FB effect will be referred to as the \"young widower\neffect.\" A possible connection between the FB effect and Martin Raff's \"Stay\nalive\" effect for cells in an organism is discussed in the last section.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 10:12:28 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Richmond", "Peter", ""], ["Roehner", "Bertrand M.", ""]]}, {"id": "1508.04944", "submitter": "Bertrand Roehner", "authors": "Peter Richmond and Bertrand M. Roehner", "title": "Effect of marital status on death rates. Part 2: Transient mortality\n  spikes", "comments": "42 pages, 18 figures", "journal-ref": null, "doi": "10.1016/j.physa.2015.12.138", "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine what happens in a population when it experiences an abrupt change\nin surrounding conditions. Several cases of such \"abrupt transitions\" for both\nphysical and living social systems are analyzed from which it can be seen that\nall share a common pattern. First, a steep rising death rate followed by a much\nslower relaxation process during which the death rate decreases as a power law\n(with an exponent close to 0.7). This leads us to propose a general principle\nwhich can be summarized as follows: \"ANY abrupt change in living conditions\ngenerates a mortality spike which acts as a kind of selection process.\" This we\nterm the Transient Shock conjecture. It provides a qualitative model which\nleads to testable predictions. For example, marriage certainly brings about a\nmajor change in environmental and social conditions and according to our\nconjecture one would expect a mortality spike in the months following marriage.\nAt first sight this may seem an unlikely proposition but we demonstrate (by\nthree different methods) that even here the existence of mortality spikes is\nsupported by solid empirical evidence.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 10:35:52 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Richmond", "Peter", ""], ["Roehner", "Bertrand M.", ""]]}, {"id": "1508.05047", "submitter": "Konstantin Zuev M", "authors": "James L. Beck and Konstantin M. Zuev", "title": "Rare Event Simulation", "comments": "Contribution to the Springer Handbook on Uncertainty Quantification.\n  13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rare events are events that are expected to occur infrequently, or more\ntechnically, those that have low probabilities (say, order of $10^{-3}$ or\nless) of occurring according to a probability model. In the context of\nuncertainty quantification, the rare events often correspond to failure of\nsystems designed for high reliability, meaning that the system performance\nfails to meet some design or operation specifications. As reviewed in this\nsection, computation of such rare-event probabilities is challenging.\nAnalytical solutions are usually not available for non-trivial problems and\nstandard Monte Carlo simulation is computationally inefficient. Therefore, much\nresearch effort has focused on developing advanced stochastic simulation\nmethods that are more efficient. In this section, we address the problem of\nestimating rare-event probabilities by Monte Carlo simulation, Importance\nSampling and Subset Simulation for highly reliable dynamic systems.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 17:06:55 GMT"}], "update_date": "2015-08-21", "authors_parsed": [["Beck", "James L.", ""], ["Zuev", "Konstantin M.", ""]]}, {"id": "1508.05412", "submitter": "Jun Ye", "authors": "Jun Ye, Yehua Li, Yongtao Guan", "title": "Joint modeling of longitudinal drug using pattern and time to first\n  relapse in cocaine dependence treatment data", "comments": "Published at http://dx.doi.org/10.1214/15-AOAS852 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 3, 1621-1642", "doi": "10.1214/15-AOAS852", "report-no": "IMS-AOAS-AOAS852", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important endpoint variable in a cocaine rehabilitation study is the time\nto first relapse of a patient after the treatment. We propose a joint modeling\napproach based on functional data analysis to study the relationship between\nthe baseline longitudinal cocaine-use pattern and the interval censored time to\nfirst relapse. For the baseline cocaine-use pattern, we consider both\nself-reported cocaine-use amount trajectories and dichotomized use\ntrajectories. Variations within the generalized longitudinal trajectories are\nmodeled through a latent Gaussian process, which is characterized by a few\nleading functional principal components. The association between the baseline\nlongitudinal trajectories and the time to first relapse is built upon the\nlatent principal component scores. The mean and the eigenfunctions of the\nlatent Gaussian process as well as the hazard function of time to first relapse\nare modeled nonparametrically using penalized splines, and the parameters in\nthe joint model are estimated by a Monte Carlo EM algorithm based on\nMetropolis-Hastings steps. An Akaike information criterion (AIC) based on\neffective degrees of freedom is proposed to choose the tuning parameters, and a\nmodified empirical information is proposed to estimate the variance-covariance\nmatrix of the estimators.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2015 21:10:53 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2015 12:15:52 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Ye", "Jun", ""], ["Li", "Yehua", ""], ["Guan", "Yongtao", ""]]}, {"id": "1508.05414", "submitter": "Joshua Vogelstein", "authors": "Raag D. Airan, Joshua T. Vogelstein, Jay J. Pillai, Brian Caffo, James\n  J. Pekar, and Haris I. Sair", "title": "Stability and Localization of inter-individual differences in functional\n  connectivity", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much recent attention has been paid to quantifying anatomic and functional\nneuroimaging on the individual subject level. For optimal individual subject\ncharacterization, specific acquisition and analysis features need to be\nidentified that maximize inter-individual variability while concomitantly\nminimizing intra-subject variability. Here we develop a non-parametric\nstatistical metric that quantifies the degree to which a parameter set allows\nthis individual subject differentiation. We apply this metric to analyzing\npublicly available test-retest resting-state fMRI (rs-fMRI) data sets. We find\nthat for the question of maximizing individual differentiation, there is a\nrelative tradeoff between increasing sampling through increased sampling\nfrequency or increased acquisition time; that for the sizes of the interrogated\ndata sets, only 4-5 min of acquisition time is necessary to perfectly\ndifferentiate each subject; and that brain regions that most contribute to\nindividuals unique characterization lie in association cortices thought to\ncontribute to higher cognitive function. These findings may guide optimal\nrs-fMRI experiment design and may aid elucidation of the neural bases for\nsubject-to-subject differences.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2015 21:15:25 GMT"}, {"version": "v2", "created": "Wed, 11 May 2016 21:52:21 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Airan", "Raag D.", ""], ["Vogelstein", "Joshua T.", ""], ["Pillai", "Jay J.", ""], ["Caffo", "Brian", ""], ["Pekar", "James J.", ""], ["Sair", "Haris I.", ""]]}, {"id": "1508.05453", "submitter": "Andrew Gelman", "authors": "Andrew Gelman and Christian Hennig", "title": "Beyond subjective and objective in statistics", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We argue that the words \"objectivity\" and \"subjectivity\" in statistics\ndiscourse are used in a mostly unhelpful way, and we propose to replace each of\nthem with broader collections of attributes, with objectivity replaced by\ntransparency, consensus, impartiality, and correspondence to observable\nreality, and subjectivity replaced by awareness of multiple perspectives and\ncontext dependence. The advantage of these reformulations is that the\nreplacement terms do not oppose each other. Instead of debating over whether a\ngiven statistical method is subjective or objective (or normatively debating\nthe relative merits of subjectivity and objectivity in statistical practice),\nwe can recognize desirable attributes such as transparency and acknowledgment\nof multiple perspectives as complementary goals. We demonstrate the\nimplications of our proposal with recent applied examples from pharmacology,\nelection polling, and socioeconomic stratification.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2015 01:58:18 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Gelman", "Andrew", ""], ["Hennig", "Christian", ""]]}, {"id": "1508.05502", "submitter": "Daniel Oberski", "authors": "Daniel Leonard Oberski, Antje Kirchner, Stephanie Eckman, Frauke\n  Kreuter", "title": "Evaluating the quality of survey and administrative data with\n  generalized multitrait-multimethod models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Administrative register data are increasingly important in statistics, but,\nlike other types of data, may contain measurement errors. To prevent such\nerrors from invalidating analyses of scientific interest, it is therefore\nessential to estimate the extent of measurement errors in administrative data.\nCurrently, however, most approaches to evaluate such errors involve either\nprohibitively expensive audits or comparison with a survey that is assumed\nperfect.\n  We introduce the \"generalized multitrait-multimethod\" (GMTMM) model, which\ncan be seen as a general framework for evaluating the quality of administrative\nand survey data simultaneously. This framework allows both survey and register\nto contain random and systematic measurement errors. Moreover, it accommodates\ncommon features of administrative data such as discreteness, nonlinearity, and\nnonnormality, improving similar existing models. The use of the GMTMM model is\ndemonstrated by application to linked survey-register data from the German\nFederal Employment Agency on income from and duration of employment, and a\nsimulation study evaluates the estimates obtained.\n  KEY WORDS: Measurement error, Latent Variable Models, Official statistics,\nRegister data, Reliability\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2015 12:31:33 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Oberski", "Daniel Leonard", ""], ["Kirchner", "Antje", ""], ["Eckman", "Stephanie", ""], ["Kreuter", "Frauke", ""]]}, {"id": "1508.05580", "submitter": "Stefano Nasini", "authors": "Stefano Nasini and V\\'ictor Mart\\'inez-de-Alb\\'eniz and Tahereh\n  Dehdarirad", "title": "A joint model for authors characteristics and collaboration pattern in\n  bibliometric networks: a Bayesian approach", "comments": "4th Amsterdam workshop on research advances in social and semantic\n  networks", "journal-ref": null, "doi": "10.13140/RG.2.1.2862.2566", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Demographic and behavioral characteristics of journal authors are important\nindicators of homophily in co-authorship networks. In the presence of\ncorrelations between adjacent nodes (assortative mixing), combining the\nestimation of the individual characteristics and the network structure results\nin a well-fitting model, which is capable to provide a deep understanding of\nthe linkage between individual and social properties. This paper aims to\npropose a novel probabilistic model for the joint distribution of nodal\nproperties (authors' demographic and behavioral characteristics) and network\nstructure (co-authorship connections), based on the nodal similarity effect. A\nBayesian approach is used to estimate the model parameters, providing insights\nabout the probabilistic properties of the observed data set. After a detailed\nanalysis of the proposed statistical methodology, we illustrate our approach\nwith an empirical analysis of co-authorship of 1007 journal articles indexed in\nthe ISI Web of Science database in the field of neuroscience between 2009 and\n2013.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2015 09:17:01 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Nasini", "Stefano", ""], ["Mart\u00ednez-de-Alb\u00e9niz", "V\u00edctor", ""], ["Dehdarirad", "Tahereh", ""]]}, {"id": "1508.05740", "submitter": "Sebastian Meyer", "authors": "Sebastian Meyer and Johannes Elias and Michael H\\\"ohle", "title": "A space-time conditional intensity model for invasive meningococcal\n  disease occurrence", "comments": "Accepted Author Manuscript", "journal-ref": "Biometrics 68, 607--616", "doi": "10.1111/j.1541-0420.2011.01684.x", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel point process model continuous in space-time is proposed for\nquantifying the transmission dynamics of the two most common meningococcal\nantigenic sequence types observed in Germany 2002-2008. Modelling is based on\nthe conditional intensity function (CIF) which is described by a superposition\nof additive and multiplicative components. As an epidemiological interesting\nfinding, spread behaviour was shown to depend on type in addition to age: basic\nreproduction numbers were 0.25 (95% CI 0.19-0.34) and 0.11 (95% CI 0.07-0.17)\nfor types B:P1.7-2,4:F1-5 and C:P1.5,2:F3-3, respectively. Altogether, the\nproposed methodology represents a comprehensive and universal regression\nframework for the modelling, simulation and inference of self-exciting\nspatio-temporal point processes based on the CIF. Usability of the modelling in\nbiometric practice is promoted by an implementation in the R package\nsurveillance.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2015 10:05:59 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Meyer", "Sebastian", ""], ["Elias", "Johannes", ""], ["H\u00f6hle", "Michael", ""]]}, {"id": "1508.06115", "submitter": "Bashar Ahmad", "authors": "Bashar I. Ahmad, James K. Murphy, Patrick M. Langdon and Simon J.\n  Godsill", "title": "Bayesian Intent Prediction in Object Tracking Using Bridging\n  Distributions", "comments": "Submitted to IEEE Transactions on Cybernetics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In several application areas, such as human computer interaction,\nsurveillance and defence, determining the intent of a tracked object enables\nsystems to aid the user/operator and facilitate effective, possibly automated,\ndecision making. In this paper, we propose a probabilistic inference approach\nthat permits the prediction, well in advance, of the intended destination of a\ntracked object and its future trajectory. Within the framework introduced here,\nthe observed partial track of the object is modeled as being part of a Markov\nbridge terminating at its destination, since the target path, albeit random,\nmust end at the intended endpoint. This captures the underlying long term\ndependencies in the trajectory, as dictated by the object intent. By\ndetermining the likelihood of the partial track being drawn from a particular\nconstructed bridge, the probability of each of a number of possible\ndestinations is evaluated. These bridges can also be employed to produce\nrefined estimates of the latent system state (e.g. object position, velocity,\netc.), predict its future values (up until reaching the designated endpoint)\nand estimate the time of arrival. This is shown to lead to a low complexity\nKalman-filter-based implementation of the inference routine, where any linear\nGaussian motion model, including the destination reverting ones, can be\napplied. Free hand pointing gestures data collected in an instrumented vehicle\nand synthetic trajectories of a vessel heading towards multiple possible\nharbours are utilised to demonstrate the effectiveness of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2015 11:41:46 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2015 08:18:42 GMT"}, {"version": "v3", "created": "Thu, 4 Feb 2016 09:49:21 GMT"}], "update_date": "2016-02-05", "authors_parsed": [["Ahmad", "Bashar I.", ""], ["Murphy", "James K.", ""], ["Langdon", "Patrick M.", ""], ["Godsill", "Simon J.", ""]]}, {"id": "1508.06228", "submitter": "Igor Mackarov Dr.", "authors": "Igor Mackarov", "title": "Statistical look at reasons of involvement in wars", "comments": "Keywords: Correlates of War, Variance, Two Factorial ANOVA,\n  Normality, R", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Correlates of War project scrupulously collects information about\ndisputes between the countries over a long historical period together with\nother data relevant to the character and reasons of international conflicts.\nUsing methods of modern Data Science implemented in the R statistical software,\nwe investigate the datasets from the project. We study political, economic, and\nreligious factors with respect to the emergence of conflicts and wars between\nthe countries. The results obtained lead to certain conclusions about variances\nand causalities between the factors considered. Some unpredictable features are\npresented.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2015 11:30:11 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2015 04:42:31 GMT"}, {"version": "v3", "created": "Tue, 8 Sep 2015 15:43:44 GMT"}], "update_date": "2015-09-09", "authors_parsed": [["Mackarov", "Igor", ""]]}, {"id": "1508.06374", "submitter": "Alexander Koplenig", "authors": "Alexander Koplenig", "title": "A fully data-driven method to identify (correlated) changes in\n  diachronic corpora", "comments": "typological changes only: reference-source-not-found-errors removed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a method for measuring synchronic corpus (dis-)similarity put\nforward by Kilgarriff (2001) is adapted and extended to identify trends and\ncorrelated changes in diachronic text data, using the Corpus of Historical\nAmerican English (Davies 2010a) and the Google Ngram Corpora (Michel et al.\n2010a). This paper shows that this fully data-driven method, which extracts\nword types that have undergone the most pronounced change in frequency in a\ngiven period of time, is computationally very cheap and that it allows\ninterpretations of diachronic trends that are both intuitively plausible and\nmotivated from the perspective of information theory. Furthermore, it\ndemonstrates that the method is able to identify correlated linguistic changes\nand diachronic shifts that can be linked to historical events. Finally, it can\nhelp to improve diachronic POS tagging and complement existing NLP approaches.\nThis indicates that the approach can facilitate an improved understanding of\ndiachronic processes in language change.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 06:18:51 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2015 06:29:43 GMT"}], "update_date": "2015-08-28", "authors_parsed": [["Koplenig", "Alexander", ""]]}, {"id": "1508.06476", "submitter": "Tobias Michael Erhardt", "authors": "Tobias M. Erhardt and Claudia Czado", "title": "Standardized drought indices: A novel uni- and multivariate approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As drought is among the natural hazards which affects people and economies\nworldwide and often results in huge monetary losses sophisticated methods for\ndrought monitoring and decision making are needed. Several different approaches\nto quantify drought have been developed during past decades. However, most of\nthese drought indices suffer from different shortcomings and do not account for\nthe multiple driving factors which promote drought conditions and their\ninter-dependencies. We provide a novel methodology for the calculation of\n(multivariate) drought indices, which combines the advantages of existing\napproaches and omits their disadvantages. Moreover, our approach benefits from\nthe flexibility of vine copulas in modeling multivariate non-Gaussian\ninter-variable dependence structures. A three-variate data example is used in\norder to investigate drought conditions in Europe and to illustrate and reason\nthe different modeling steps. The data analysis shows the appropriateness of\nthe described methodology. Comparison to well-established drought indices shows\nthe benefits of our multivariate approach. The validity of the new methodology\nis verified by comparing the spatial extent of historic drought events based on\ndifferent drought indices. Further, we show that the assumption of non-Gaussian\ndependence structures is well-grounded in this real-world application.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 12:56:53 GMT"}], "update_date": "2015-08-27", "authors_parsed": [["Erhardt", "Tobias M.", ""], ["Czado", "Claudia", ""]]}, {"id": "1508.06618", "submitter": "Le Bao", "authors": "Le Bao, Xiaoyue Niu, Mary Mahy, Peter D. Ghys", "title": "Estimating HIV Epidemics for Sub-National Areas", "comments": "arXiv admin note: substantial text overlap with arXiv:1411.4219", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As the global HIV pandemic enters its fourth decade, increasing numbers of\nsurveillance sites have been established which allows countries to look into\nthe epidemics at a finer scale, e.g. at sub-national levels. Currently, the\nepidemic models have been applied independently to the sub-national areas\nwithin countries. However, the availability and quality of the data vary\nwidely, which leads to biased and unreliable estimates for areas with very few\ndata. We propose to overcome this issue by introducing the dependence of the\nparameters across areas in a mixture model. The joint distribution of the\nparameters in multiple areas can be approximated directly from the results of\nindependent fits without needing to refit the data or unpack the software. As a\nresult, the mixture model has better predictive ability than the independent\nmodel as shown in examples of multiple countries in Sub-Saharan Africa.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 19:32:05 GMT"}], "update_date": "2015-08-27", "authors_parsed": [["Bao", "Le", ""], ["Niu", "Xiaoyue", ""], ["Mahy", "Mary", ""], ["Ghys", "Peter D.", ""]]}, {"id": "1508.06686", "submitter": "Shawn Mankad", "authors": "Shawn Mankad, George Michailidis", "title": "Analysis of multiview legislative networks with structured matrix\n  factorization: Does Twitter influence translate to the real world?", "comments": "Published at http://dx.doi.org/10.1214/15-AOAS858 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 4, 1950-1972", "doi": "10.1214/15-AOAS858", "report-no": "IMS-AOAS-AOAS858", "categories": "cs.SI physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of social media platforms has fundamentally altered the public\ndiscourse by providing easy to use and ubiquitous forums for the exchange of\nideas and opinions. Elected officials often use such platforms for\ncommunication with the broader public to disseminate information and engage\nwith their constituencies and other public officials. In this work, we\ninvestigate whether Twitter conversations between legislators reveal their\nreal-world position and influence by analyzing multiple Twitter networks that\nfeature different types of link relations between the Members of Parliament\n(MPs) in the United Kingdom and an identical data set for politicians within\nIreland. We develop and apply a matrix factorization technique that allows the\nanalyst to emphasize nodes with contextual local network structures by\nspecifying network statistics that guide the factorization solution. Leveraging\nonly link relation data, we find that important politicians in Twitter networks\nare associated with real-world leadership positions, and that rankings from the\nproposed method are correlated with the number of future media headlines.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 23:34:44 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2016 13:45:19 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["Mankad", "Shawn", ""], ["Michailidis", "George", ""]]}, {"id": "1508.06715", "submitter": "Wei Jiang", "authors": "Wei Jiang, Jing-Hao Xue, Weichuan Yu", "title": "Estimating Reproducibility in Genome-Wide Association Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genome-wide association studies (GWAS) are widely used to discover genetic\nvariants associated with diseases. To control false positives, all findings\nfrom GWAS need to be verified with additional evidences, even for associations\ndiscovered from a high power study. Replication study is a common verification\nmethod by using independent samples. An association is regarded as true\npositive with a high confidence when it can be identified in both primary study\nand replication study. Currently, there is no systematic study on the behavior\nof positives in the replication study when the positive results of primary\nstudy are considered as the prior information.\n  In this paper, two probabilistic measures named Reproducibility Rate (RR) and\nFalse Irreproducibility Rate (FIR) are proposed to quantitatively describe the\nbehavior of primary positive associations (i.e. positive associations\nidentified in the primary study) in the replication study. RR is a conditional\nprobability measuring how likely a primary positive association will also be\npositive in the replication study. This can be used to guide the design of\nreplication study, and to check the consistency between the results of primary\nstudy and those of replication study. FIR, on the contrary, measures how likely\na primary positive association may still be a true positive even when it is\nnegative in the replication study. This can be used to generate a list of\npotentially true associations in the irreproducible findings for further\nscrutiny. The estimation methods of these two measures are given. Simulation\nresults and real experiments show that our estimation methods have high\naccuracy and good prediction performance.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 03:34:30 GMT"}], "update_date": "2015-08-28", "authors_parsed": [["Jiang", "Wei", ""], ["Xue", "Jing-Hao", ""], ["Yu", "Weichuan", ""]]}, {"id": "1508.06756", "submitter": "Didier Fraix-Burnet", "authors": "Didier Fraix-Burnet (IPAG), Marc Thuillard, Asis Kumar Chattopadhyay", "title": "Multivariate Approaches to Classification in Extragalactic Astronomy", "comments": "Open Access paper.\n  http://www.frontiersin.org/milky\\_way\\_and\\_galaxies/10.3389/fspas.2015.00003/abstract\\&gt;.\n  \\&lt;10.3389/fspas.2015.00003 \\&gt", "journal-ref": "Frontiers in Astronomy and Space Sciences, Lee Samuel Finn, 2015,\n  2 (3), pp.00", "doi": "10.3389/fspas.2015.00003", "report-no": null, "categories": "astro-ph.GA astro-ph.CO q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering objects into synthetic groups is a natural activity of any\nscience. Astrophysics is not an exception and is now facing a deluge of data.\nFor galaxies, the one-century old Hubble classification and the Hubble tuning\nfork are still largely in use, together with numerous mono-or bivariate\nclassifications most often made by eye. However, a classification must be\ndriven by the data, and sophisticated multivariate statistical tools are used\nmore and more often. In this paper we review these different approaches in\norder to situate them in the general context of unsupervised and supervised\nlearning. We insist on the astrophysical outcomes of these studies to show that\nmultivariate analyses provide an obvious path toward a renewal of our\nclassification of galaxies and are invaluable tools to investigate the physics\nand evolution of galaxies.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 09:06:08 GMT"}], "update_date": "2015-08-28", "authors_parsed": [["Fraix-Burnet", "Didier", "", "IPAG"], ["Thuillard", "Marc", ""], ["Chattopadhyay", "Asis Kumar", ""]]}, {"id": "1508.06773", "submitter": "L\\'aszl\\'o Csat\\'o", "authors": "L\\'aszl\\'o Csat\\'o", "title": "Ranking by pairwise comparisons for Swiss-system tournaments", "comments": null, "journal-ref": "Central European Journal of Operations Research, 21(4): 783-803,\n  2013", "doi": "10.1007/s10100-012-0261-8", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pairwise comparison matrices are widely used in Multicriteria Decision\nMaking. This article applies incomplete pairwise comparison matrices in the\narea of sport tournaments, namely proposing alternative rankings for the 2010\nChess Olympiad Open tournament. It is shown that results are robust regarding\nscaling technique. In order to compare different rankings, a distance function\nis introduced with the aim of taking into account the subjective nature of\nhuman perception. Analysis of the weight vectors implies that methods based on\npairwise comparisons have common roots. Visualization of the results is\nprovided by Multidimensional Scaling on the basis of the defined distance. The\nproposed rankings give in some cases intuitively better outcome than currently\nused lexicographical orders.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 09:34:13 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Csat\u00f3", "L\u00e1szl\u00f3", ""]]}, {"id": "1508.06864", "submitter": "Vartan Choulakian", "authors": "Vartan Choulakian", "title": "Matrix Factorizations Based on Induced Norms", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We decompose a matrix Y into a sum of bilinear terms in a stepwise manner, by\nconsidering Y as a mapping from a finite dimensional Banach space into another\nfinite dimensional Banach space. We provide transition formulas, and represent\nthem in a duality diagram, thus generalizing the well known duality diagram in\nthe french school of data analysis. As an application, we introduce a family of\nEuclidean multidimensional scaling models.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 14:09:41 GMT"}], "update_date": "2015-08-28", "authors_parsed": [["Choulakian", "Vartan", ""]]}, {"id": "1508.06885", "submitter": "Vartan Choulakian", "authors": "Vartan Choulakian", "title": "Taxicab Correspondence Analysis of Sparse Contingency Tables", "comments": "23 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualization and interpretation of contingency tables by correspondence\nanalysis (CA), as developed by Benzecri, has a rich structure based on\nEuclidean geometry. However, it is a well established fact that, often CA is\nvery sensitive to sparse contingency tables, where we caracterize sparsity as\nthe existence of relatively high-valued counts, rare observations discussed by\nRao (1995), and zero-block structure emphasized by Novak and Bar-Hen (2005) and\nGreenacre (2013). In this paper, we aim to emphasize the important roles played\nby L1 and L2 geometries. This will be done by comparing the maps obtained by CA\nwith the maps obtained by taxicab correspondence analysis (TCA), where TCA is a\nrobust L1 variant of correspondence analysis. If the projections of view of\nboth maps are quite different, we refer to this phenomenon as parallax. In\nastronomy, parallax means the apparent change in the position of an object as\nseen from two different points. In our case the two different points correspond\nto the two different geometries, Euclidean and Taxicab. The existence of a\nparallax highlights the important, but hidden, role of the underlying geometry\nin the interpretation of the maps obtained in multivariate data analysis. We\nemphasize the following fact: Only by comparing CA and TCA graphical displays,\nwe are able to reveal the phenomenon of parallax. Examples are provided.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 14:50:44 GMT"}, {"version": "v2", "created": "Wed, 24 May 2017 13:03:45 GMT"}, {"version": "v3", "created": "Fri, 2 Jun 2017 16:52:42 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Choulakian", "Vartan", ""]]}, {"id": "1508.06901", "submitter": "Xin Yuan", "authors": "Xin Yuan, Hong Jiang, Gang Huang, Paul A. Wilford", "title": "Compressive Sensing via Low-Rank Gaussian Mixture Models", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new compressive sensing (CS) inversion algorithm by utilizing\nthe Gaussian mixture model (GMM). While the compressive sensing is performed\nglobally on the entire image as implemented in our lensless camera, a low-rank\nGMM is imposed on the local image patches. This low-rank GMM is derived via\neigenvalue thresholding of the GMM trained on the projection of the measurement\ndata, thus learned {\\em in situ}. The GMM and the projection of the measurement\ndata are updated iteratively during the reconstruction. Our GMM algorithm\ndegrades to the piecewise linear estimator (PLE) if each patch is represented\nby a single Gaussian model. Inspired by this, a low-rank PLE algorithm is also\ndeveloped for CS inversion, constituting an additional contribution of this\npaper. Extensive results on both simulation data and real data captured by the\nlensless camera demonstrate the efficacy of the proposed algorithm.\nFurthermore, we compare the CS reconstruction results using our algorithm with\nthe JPEG compression. Simulation results demonstrate that when limited\nbandwidth is available (a small number of measurements), our algorithm can\nachieve comparable results as JPEG.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 15:35:11 GMT"}], "update_date": "2015-08-28", "authors_parsed": [["Yuan", "Xin", ""], ["Jiang", "Hong", ""], ["Huang", "Gang", ""], ["Wilford", "Paul A.", ""]]}, {"id": "1508.06941", "submitter": "Mauricio Santillana", "authors": "Mauricio Santillana, Andre T. Nguyen, Mark Dredze, Michael J. Paul,\n  John S. Brownstein", "title": "Combining Search, Social Media, and Traditional Data Sources to Improve\n  Influenza Surveillance", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pcbi.1004513", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a machine learning-based methodology capable of providing\nreal-time (\"nowcast\") and forecast estimates of influenza activity in the US by\nleveraging data from multiple data sources including: Google searches, Twitter\nmicroblogs, nearly real-time hospital visit records, and data from a\nparticipatory surveillance system. Our main contribution consists of combining\nmultiple influenza-like illnesses (ILI) activity estimates, generated\nindependently with each data source, into a single prediction of ILI utilizing\nmachine learning ensemble approaches. Our methodology exploits the information\nin each data source and produces accurate weekly ILI predictions for up to four\nweeks ahead of the release of CDC's ILI reports. We evaluate the predictive\nability of our ensemble approach during the 2013-2014 (retrospective) and\n2014-2015 (live) flu seasons for each of the four weekly time horizons. Our\nensemble approach demonstrates several advantages: (1) our ensemble method's\npredictions outperform every prediction using each data source independently,\n(2) our methodology can produce predictions one week ahead of GFT's real-time\nestimates with comparable accuracy, and (3) our two and three week forecast\nestimates have comparable accuracy to real-time predictions using an\nautoregressive model. Moreover, our results show that considerable insight is\ngained from incorporating disparate data streams, in the form of social media\nand crowd sourced data, into influenza predictions in all time horizons\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 17:10:18 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Santillana", "Mauricio", ""], ["Nguyen", "Andre T.", ""], ["Dredze", "Mark", ""], ["Paul", "Michael J.", ""], ["Brownstein", "John S.", ""]]}, {"id": "1508.07083", "submitter": "Raymond K. W. Wong", "authors": "Raymond K. W. Wong, Vinay L. Kashyap, Thomas C. M. Lee, David A. van\n  Dyk", "title": "Detecting Abrupt Changes in the Spectra of High-Energy Astrophysical\n  Sources", "comments": "30 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable-intensity astronomical sources are the result of complex and often\nextreme physical processes. Abrupt changes in source intensity are typically\naccompanied by equally sudden spectral shifts, i.e., sudden changes in the\nwavelength distribution of the emission. This article develops a method for\nmodeling photon counts collected from observation of such sources. We embed\nchange points into a marked Poisson process, where photon wavelengths are\nregarded as marks and both the Poisson intensity parameter and the distribution\nof the marks are allowed to change. To the best of our knowledge this is the\nfirst effort to embed change points into a marked Poisson process. Between the\nchange points, the spectrum is modeled non-parametrically using a mixture of a\nsmooth radial basis expansion and a number of local deviations from the smooth\nterm representing spectral emission lines. Because the model is over\nparameterized we employ an $\\ell_1$ penalty. The tuning parameter in the\npenalty and the number of change points are determined via the minimum\ndescription length principle. Our method is validated via a series of\nsimulation studies and its practical utility is illustrated in the analysis of\nthe ultra-fast rotating yellow giant star known as FK Com.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2015 03:24:36 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2015 23:55:28 GMT"}], "update_date": "2015-12-14", "authors_parsed": [["Wong", "Raymond K. W.", ""], ["Kashyap", "Vinay L.", ""], ["Lee", "Thomas C. M.", ""], ["van Dyk", "David A.", ""]]}, {"id": "1508.07131", "submitter": "Jerome Bobin", "authors": "Jerome Bobin and Florent Sureau and Jean-Luc Starck", "title": "Polarized CMB recovery with sparse component separation", "comments": "Accepted to A&A, august 2015", "journal-ref": "A&A 583, A92 (2015)", "doi": "10.1051/0004-6361/201526001", "report-no": null, "categories": "astro-ph.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The polarization modes of the cosmological microwave background are an\ninvaluable source of information for cosmology, and a unique window to probe\nthe energy scale of inflation. Extracting such information from microwave\nsurveys requires disentangling between foreground emissions and the\ncosmological signal, which boils down to solving a component separation\nproblem. Component separation techniques have been widely studied for the\nrecovery of CMB temperature anisotropies but quite rarely for the polarization\nmodes. In this case, most component separation techniques make use of\nsecond-order statistics to discriminate between the various components. More\nrecent methods, which rather emphasize on the sparsity of the components in the\nwavelet domain, have been shown to provide low-foreground, full-sky estimate of\nthe CMB temperature anisotropies. Building on sparsity, the present paper\nintroduces a new component separation technique dubbed PolGMCA (Polarized\nGeneralized Morphological Component Analysis), which refines previous work to\nspecifically tackle the estimation of the polarized CMB maps: i) it benefits\nfrom a recently introduced sparsity-based mechanism to cope with partially\ncorrelated components, ii) it builds upon estimator aggregation techniques to\nfurther yield a better noise contamination/non-Gaussian foreground residual\ntrade-off. The PolGMCA algorithm is evaluated on simulations of full-sky\npolarized microwave sky simulations using the Planck Sky Model (PSM), which\nshow that the proposed method achieve a precise recovery of the CMB map in\npolarization with low noise/foreground contamination residuals. It provides\nimprovements with respect to standard methods, especially on the galactic\ncenter where estimating the CMB is challenging.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2015 08:54:13 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Bobin", "Jerome", ""], ["Sureau", "Florent", ""], ["Starck", "Jean-Luc", ""]]}, {"id": "1508.07497", "submitter": "William Nicholson", "authors": "William Nicholson, David Matteson, Jacob Bien", "title": "VARX-L: Structured Regularization for Large Vector Autoregressions with\n  Exogenous Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vector autoregression (VAR) has long proven to be an effective method for\nmodeling the joint dynamics of macroeconomic time series as well as\nforecasting. A major shortcoming of the VAR that has hindered its applicability\nis its heavy parameterization: the parameter space grows quadratically with the\nnumber of series included, quickly exhausting the available degrees of freedom.\nConsequently, forecasting using VARs is intractable for low-frequency,\nhigh-dimensional macroeconomic data. However, empirical evidence suggests that\nVARs that incorporate more component series tend to result in more accurate\nforecasts. Conventional methods that allow for the estimation of large VARs\neither tend to require ad hoc subjective specifications or are computationally\ninfeasible. Moreover, as global economies become more intricately intertwined,\nthere has been substantial interest in incorporating the impact of stochastic,\nunmodeled exogenous variables. Vector autoregression with exogenous variables\n(VARX) extends the VAR to allow for the inclusion of unmodeled variables, but\nit similarly faces dimensionality challenges.\n  We introduce the VARX-L framework, a structured family of VARX models, and\nprovide methodology that allows for both efficient estimation and accurate\nforecasting in high-dimensional analysis. VARX-L adapts several prominent\nscalar regression regularization techniques to a vector time series context in\norder to greatly reduce the parameter space of VAR and VARX models. We also\nhighlight a compelling extension that allows for shrinking toward reference\nmodels, such as a vector random walk. We demonstrate the efficacy of VARX-L in\nboth low- and high-dimensional macroeconomic forecasting applications and\nsimulated data examples. Our methodology is easily reproducible in a publicly\navailable R package.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2015 19:52:26 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2015 18:06:57 GMT"}, {"version": "v3", "created": "Mon, 16 May 2016 14:07:06 GMT"}, {"version": "v4", "created": "Mon, 27 Feb 2017 13:40:01 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Nicholson", "William", ""], ["Matteson", "David", ""], ["Bien", "Jacob", ""]]}, {"id": "1508.07509", "submitter": "Christopher Paciorek", "authors": "Christopher J. Paciorek, Simon J. Goring, Andrew L. Thurman, Charles\n  V. Cogbill, John W. Williams, David J. Mladenoff, Jody A. Peters, Jun Zhu,\n  Jason S. McLachlan", "title": "Statistically-estimated tree composition for the northeastern United\n  States at the time of Euro-American settlement", "comments": "23 pages, 5 tables, 3 figures", "journal-ref": "PLoS ONE (2016) 11(2): e0150087", "doi": "10.1371/journal.pone.0150087", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a gridded 8 km-resolution data product of the estimated\ncomposition of tree taxa at the time of Euro-American settlement of the\nnortheastern United States and the statistical methodology used to produce the\nproduct from trees recorded by land surveyors. Composition is defined as the\nproportion of stems larger than approximately 20 cm diameter at breast height\nfor 22 tree taxa, generally at the genus level. The data come from\nsettlement-era public survey records that are transcribed and then aggregated\nspatially, giving count data. The domain is divided into two regions, eastern\n(Maine to Ohio) and midwestern (Indiana to Minnesota). Public Land Survey point\ndata in the midwestern region (ca. 0.8-km resolution) are aggregated to a\nregular 8 km grid, while data in the eastern region, from Town Proprietor\nSurveys, are aggregated at the township level in irregularly-shaped local\nadministrative units. The product is based on a Bayesian statistical model fit\nto the count data that estimates composition on a regular 8 km grid across the\nentire domain. The statistical model is designed to handle data from both the\nregular grid and the irregularly-shaped townships and allows us to estimate\ncomposition at locations with no data and to smooth over noise caused by\nlimited counts in locations with data. The model also allows us to quantify\nuncertainty in our composition estimates, making the product suitable for\napplications employing data assimilation. We expect this data product to be\nuseful for understanding the state of vegetation in the northeastern United\nStates prior to large-scale Euro-American settlement. In addition to specific\nregional questions, the data product can also serve as a baseline against which\nto investigate how forests and ecosystems change after intensive settlement.\nThe data product is available at the NIS data portal as version 1.0.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2015 22:19:53 GMT"}, {"version": "v2", "created": "Sun, 3 Apr 2016 19:56:59 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Paciorek", "Christopher J.", ""], ["Goring", "Simon J.", ""], ["Thurman", "Andrew L.", ""], ["Cogbill", "Charles V.", ""], ["Williams", "John W.", ""], ["Mladenoff", "David J.", ""], ["Peters", "Jody A.", ""], ["Zhu", "Jun", ""], ["McLachlan", "Jason S.", ""]]}, {"id": "1508.07511", "submitter": "Rebecca Yates Coley", "authors": "R. Yates Coley (1), Aaron J. Fisher (1), Mufaddal Mamawala (2), H.\n  Ballentine Carter (2), Kenneth J. Pienta (2), and Scott L. Zeger (1) ((1)\n  Department of Biostatistics, Johns Hopkins Bloomberg School of Public Health,\n  Baltimore, USA, (2) The James Buchanan Brady Urological Institute, Johns\n  Hopkins Medical Institutions, Baltimore, USA)", "title": "A Bayesian Hierarchical Model for Prediction of Latent Health States\n  from Multiple Data Sources with Application to Active Surveillance of\n  Prostate Cancer", "comments": "25 pages (double-spaced, with references), 6 figures for primary\n  paper and 44 pages, 22 figures, 15 tables for appendix; Revision submittied\n  for review 5/16; Changes from original version include changed title,\n  additional simulations and figures, editing for clarity", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we present a Bayesian hierarchical model for predicting a\nlatent health state from longitudinal clinical measurements. Model development\nis motivated by the need to integrate multiple sources of data to improve\nclinical decisions about whether to remove or irradiate a patient's prostate\ncancer. Existing modeling approaches are extended to accommodate measurement\nerror in cancer state determinations based on biopsied tissue, clinical\nmeasurements possibly not missing at random, and informative partial\nobservation of the true state. The proposed model enables estimation of whether\nan individual's underlying prostate cancer is aggressive, requiring surgery\nand/or radiation, or indolent, permitting continued surveillance. These\nindividualized predictions can then be communicated to clinicians and patients\nto inform decision-making. We demonstrate the model with data from a cohort of\nlow risk prostate cancer patients at Johns Hopkins University and assess\npredictive accuracy among a subset for whom true cancer state is observed.\nSimulation studies confirm model performance and explore the impact of\nadjusting for informative missingness on true state predictions. R code and\nsimulated data available at\nhttps://github.com/rycoley/prediction-prostate-surveillance.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2015 22:42:24 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2015 04:13:52 GMT"}, {"version": "v3", "created": "Fri, 1 Jan 2016 20:46:24 GMT"}, {"version": "v4", "created": "Wed, 1 Jun 2016 20:01:24 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Coley", "R. Yates", ""], ["Fisher", "Aaron J.", ""], ["Mamawala", "Mufaddal", ""], ["Carter", "H. Ballentine", ""], ["Pienta", "Kenneth J.", ""], ["Zeger", "Scott L.", ""]]}, {"id": "1508.07534", "submitter": "Daniya Tlegenova", "authors": "Daniya Tlegenova", "title": "Forecasting Exchange Rates Using Time Series Analysis: The sample of the\n  currency of Kazakhstan", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper models yearly exchange rates between USD/KZT, EUR/KZT and SGD/KZT,\nand compares the actual data with developed forecasts using time series\nanalysis over the period from 2006 to 2014. The official yearly data of\nNational Bank of the Republic of Kazakhstan is used for present study. The main\ngoal of this paper is to apply the ARIMA model for forecasting of yearly\nexchange rates of USD/KZT, EUR/KZT and SGD/KZT. The accuracy of the forecast is\ncompared with Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE)\nand Root Mean Squared Error (RMSE).\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2015 06:30:07 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Tlegenova", "Daniya", ""]]}, {"id": "1508.07582", "submitter": "Christopher Rook", "authors": "Christopher J. Rook, Mitchell Kerman", "title": "Approximating the Sum of Correlated Lognormals: An Implementation", "comments": "Fully documented source code is included", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN cs.MS stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lognormal random variables appear naturally in many engineering disciplines,\nincluding wireless communications, reliability theory, and finance. So, too,\ndoes the sum of (correlated) lognormal random variables. Unfortunately, no\nclosed form probability distribution exists for such a sum, and it requires\napproximation. Some approximation methods date back over 80 years and most take\none of two approaches, either: 1) an approximate probability distribution is\nderived mathematically, or 2) the sum is approximated by a single lognormal\nrandom variable. In this research, we take the latter approach and review a\nfairly recent approximation procedure proposed by Mehta, Wu, Molisch, and Zhang\n(2007), then implement it using C++. The result is applied to a discrete time\nmodel commonly encountered within the field of financial economics.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2015 14:47:50 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Rook", "Christopher J.", ""], ["Kerman", "Mitchell", ""]]}, {"id": "1508.07937", "submitter": "Vahed Maroufy", "authors": "Vahed Maroufy and Paul Marriott", "title": "Local and global robustness in conjugate Bayesian analysis", "comments": "11 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the influence of perturbations of conjugate priors in\nBayesian inference. A perturbed prior is defined inside a larger family, local\nmixture models, and the effect on posterior inference is studied. The\nperturbation, in some sense, generalizes the linear perturbation studied in\n\\cite{Gustafson1996}. It is intuitive, naturally normalized and is flexible for\nstatistical applications. Both global and local sensitivity analyses are\nconsidered. A geometric approach is employed for optimizing the sensitivity\ndirection function, the difference between posterior means and the divergence\nfunction between posterior predictive models. All the sensitivity measure\nfunctions are defined on a convex space with non-trivial boundary which is\nshown to be a smooth manifold.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2015 18:09:05 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Maroufy", "Vahed", ""], ["Marriott", "Paul", ""]]}]