[{"id": "2011.00321", "submitter": "Fan Yin", "authors": "Fan Yin, Domarin Khago, Rachel W. Martin, Carter T. Butts", "title": "Bayesian Analysis of Static Light Scattering Data for Globular Proteins", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Static light scattering is a popular physical chemistry technique that\nenables calculation of physical attributes such as the radius of gyration and\nthe second virial coefficient for a macromolecule (e.g., a polymer or a\nprotein) in solution. The second virial coefficient is a physical quantity that\ncharacterizes the magnitude and sign of pairwise interactions between\nparticles, and hence is related to aggregation propensity, a property of\nconsiderable scientific and practical interest. Estimating the second virial\ncoefficient from experimental data is challenging due both to the degree of\nprecision required and the complexity of the error structure involved. In\ncontrast to conventional approaches based on heuristic OLS estimates, Bayesian\ninference for the second virial coefficient allows explicit modeling of error\nprocesses, incorporation of prior information, and the ability to directly test\ncompeting physical models. Here, we introduce a fully Bayesian model for static\nlight scattering experiments on small-particle systems, with joint inference\nfor concentration, index of refraction, oligomer size, and the second virial\ncoefficient. We apply our proposed model to study the aggregation behavior of\nhen egg-white lysozyme and human gammaS-crystallin using in-house experimental\ndata. Based on these observations, we also perform a simulation study on the\nprimary drivers of uncertainty in this family of experiments, showing in\nparticular the potential for improved monitoring and control of concentration\nto aid inference.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 17:32:05 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Yin", "Fan", ""], ["Khago", "Domarin", ""], ["Martin", "Rachel W.", ""], ["Butts", "Carter T.", ""]]}, {"id": "2011.00327", "submitter": "Reza Foudazi", "authors": "Sonia Ghayem and Reza Foudazi", "title": "Statistical Analysis of Behavioral Intention Towards Private Umbilical\n  Cord Blood Banking", "comments": "16 Pages, 1 Figure, Submitted in the 4th International conference on\n  intelligent decision science in Istanbul", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a conceptual framework to identify the key\ndimensions affecting behavioral intention to bank umbilical cord blood in Iran.\nWe examine the impact of awareness, reference group, usability, disease\nhistory, and price on perceived risk and behavioral intention to use umbilical\ncord blood banking service. To evaluate the proposed model of umbilical cord\nblood banking behavioral intention and to test our hypotheses, we apply field\nexploratory research. We use a five-point Likert scale to form a questionnaire\nto collect the data. The model is estimated with a sample of 242 Royan cord\nblood bank customers in Tehran. We use Pearson correlation and structural\nequation modeling to analyze the structural relationships between research\nvariables, perceived risk, and behavioral intention. This research gives\nnovelty on the determinants of behavioral intention in private umbilical cord\nblood banking, which adds value to literature and future managerial practices.\nResults show that usability is the primary determinant in cord blood banking.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 18:06:24 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Ghayem", "Sonia", ""], ["Foudazi", "Reza", ""]]}, {"id": "2011.00353", "submitter": "S. Stanley Young", "authors": "S. Stanley Young and Warren Kindzierski", "title": "PM2.5 and all-cause mortality", "comments": "6 pages, one table, one figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The US EPA and the WHO claim that PM2.5 is causal of all-cause deaths. Both\nsupport and fund research on air quality and health effects. WHO funded a\nmassive systematic review and meta-analyses of air quality and health-effect\npapers. 1,632 literature papers were reviewed and 196 were selected for\nmeta-analyses. The standard air components, particulate matter, PM10 and PM2.5,\nnitrogen dioxide, NO2, and ozone, were selected as causes and all-cause and\ncause-specific mortalities were selected as outcomes. A claim was made for\nPM2.5 and all-cause deaths, risk ratio of 1.0065, with confidence limits of\n1.0044 to 1.0086. There is a need to evaluate the reliability of this causal\nclaim. Based on a p-value plot and discussion of several forms of bias, we\nconclude that the association is not causal.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 20:17:26 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Young", "S. Stanley", ""], ["Kindzierski", "Warren", ""]]}, {"id": "2011.00360", "submitter": "Yajuan Si", "authors": "Yajuan Si", "title": "On the Use of Auxiliary Variables in Multilevel Regression and\n  Poststratification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilevel regression and poststratification (MRP) has been a popular\napproach for selection bias adjustment and subgroup estimation, with successful\nand widespread applications from social sciences to health sciences. We\ndemonstrate the capability of MRP to handle the methodological and\ncomputational issues in data integration and inferences of probability and\nnonprobability-based surveys, and the broad extensions in practical\napplications. Our development is motivated by the Adolescent Brain Cognitive\nDevelopment (ABCD) Study that has collected children across 21 U.S. geographic\nlocations for national representation but is subject to selection bias, a\ncommon problem of nonprobability samples. Though treated as the gold standard\nin public opinion research, MRP is a statistical technique that has assumptions\nand pitfalls, the validity of which prominently depends on the quality of\navailable auxiliary information. In this paper, we develop the statistical\nfoundation of how to incorporate auxiliary variables under MRP. We build up a\nsystematic framework under MRP for statistical data integration and inferences.\nOur simulation studies indicate the statistical validity of MRP with a tradeoff\nbetween robustness and efficiency and present the improvement over alternative\nmethods. We apply the approach to evaluate cognition performances of diverse\ngroups of children in the ABCD study and find that the adjustment of auxiliary\nvariables has a substantial effect on the inference results.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 20:58:25 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Si", "Yajuan", ""]]}, {"id": "2011.00546", "submitter": "Rodrigo Labouriau", "authors": "Jeanett S. Pelck and Rodrigo Labouriau", "title": "Using Multivariate Generalised Linear Mixed Models for Studying Roots\n  Development: An Example Based on Minirhizotron Observations", "comments": "18 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The characterisation of the spatial and temporal distribution of the root\nsystem in a cultivated field depends on the soil volume occupied by the root\nsystems (the scatter), and the local intensity of the root colonisation in the\nfield (the intensity). We introduce a multivariate generalised linear mixed\nmodel for simultaneously describing the scatter and the intensity using data\nobtained with minirhizotrons (i.e., tubes with observation windows, which are\ninserted in the soil, enabling to observe the roots directly). The models\npresented allow studying intricate spatial and temporal dependence patterns\nusing a graphical model to represent the dependence structure of latent random\ncomponents.\n  The scatter is described by a binomial mixed model (presence of roots in\nobservation windows). The number of roots crossing the reference lines in the\nobservational windows of the minirhizotron is used to estimate the intensity\nthrough a specially defined Poisson mixed model. We explore the fact that it is\npossible to construct multivariate extensions of generalised linear mixed\nmodels that allow to simultaneously represent patterns of dependency of the\nscatter and the intensity along with time and space.\n  We present an example where the intensity and scatter are simultaneously\ndetermined at three different time points. A positive association between the\nintensity and scatter at each time point was found, suggesting that the plants\nare not compensating a reduced occupation of the soil by increasing the number\nof roots per volume of soil. Using the general properties of graphical models,\nwe identify a first-order Markovian dependence pattern between successively\nobserved scatters and intensities. This lack of memory indicates that no\nlong-lasting temporal causal effects are affecting the roots' development. The\ntwo dependence patterns described above cannot be detected with univariate\nmodels.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 16:10:35 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Pelck", "Jeanett S.", ""], ["Labouriau", "Rodrigo", ""]]}, {"id": "2011.00591", "submitter": "Alex Diana", "authors": "Alex Diana, Eleni Matechou, Jim Griffin, Todd Arnold, Richard\n  Griffiths, John Pickering, Simone Tenan, Stefano Volponi", "title": "A general modelling framework for open wildlife populations based on the\n  Polya Tree prior", "comments": "34 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wildlife monitoring for open populations can be performed using a number of\ndifferent survey methods. Each survey method gives rise to a type of data and,\nin the last five decades, a large number of associated statistical models have\nbeen developed for analysing these data. Although these models have been\nparameterised and fitted using different approaches, they have all been\ndesigned to model the pattern with which individuals enter and exit the\npopulation and to estimate the population size. However, existing approaches\nrely on a predefined model structure and complexity, either by assuming that\nparameters are specific to sampling occasions, or by employing parametric\ncurves. Instead, we propose a novel Bayesian nonparametric framework for\nmodelling entry and exit patterns based on the Polya Tree (PT) prior for\ndensities. Our Bayesian non-parametric approach avoids overfitting when\ninferring entry and exit patterns while simultaneously allowing more\nflexibility than is possible using parametric curves. We apply our new\nframework to capture-recapture, count and ring-recovery data and we introduce\nthe replicated PT prior for defining classes of models for these data.\nAdditionally, we define the Hierarchical Logistic PT prior for jointly\nmodelling related data and we consider the Optional PT prior for modelling long\ntime series of data. We demonstrate our new approach using five different case\nstudies on birds, amphibians and insects.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 18:46:44 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Diana", "Alex", ""], ["Matechou", "Eleni", ""], ["Griffin", "Jim", ""], ["Arnold", "Todd", ""], ["Griffiths", "Richard", ""], ["Pickering", "John", ""], ["Tenan", "Simone", ""], ["Volponi", "Stefano", ""]]}, {"id": "2011.00700", "submitter": "Jiani Yang", "authors": "Jiani Yang, Yuan Wang, Joseph Pinto, Le Kuai, King-Fai Li, Stanley P\n  Sander, Yuk L Yung", "title": "The Improvement of the Air Quality due to Traffic Halting in Los Angeles\n  and Potential Health Care Risk during the COVID-19 Outbreak", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ao-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: On March 19, 2020, the government of California ordered all 40\nmillion Californians to stay at home in the coming weeks as the result of the\nescalation of the coronavirus disease 2019 (COVID-19) pandemic. As lockdowns\nwere implemented, the significant changes caused by these restrictions brought\na dramatic improvement in air quality in metropolitan cities such as Los\nAngeles (LA Basin).Methods: We use real-time data from The South Coast Air\nQuality Management District (South Coast AQMD), and the California Department\nof Transportation to evaluate the drivers of the pollution sources. We also\nmapped monthly spatial variations and constructed hourly heatmaps of those\npollutants in 2020 to understand the impacts of the lockdown on different\nlocations and times of day in the LA Basin. Results: Compared to the same dates\nin 2019, traffic flow on highways in the Los Angeles Basin dropped by 20.86 %\nwhen the stay at home order was initiated and it continued to decrease along\nwith dramatic declines in NO2, CO, and PM2.5. The correlation (Pierson r)\nbetween truck flow change and changes of NO2, CO, and PM2.5 is statistically\nsignificant. Conclusion: The declines in truck flow are mainly responsible for\nthe drop of NO2 and CO, with traffic having a slightly smaller effect on PM2.5.\nThe lockdowns provided a large-scale experiment into air quality research. The\nresult of this research would provide an important reference for the policy\nmarkers regarding truck management in light of air quality control to prepare\nfor the 2028 Summer Olympics in LA.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 03:14:03 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Yang", "Jiani", ""], ["Wang", "Yuan", ""], ["Pinto", "Joseph", ""], ["Kuai", "Le", ""], ["Li", "King-Fai", ""], ["Sander", "Stanley P", ""], ["Yung", "Yuk L", ""]]}, {"id": "2011.00733", "submitter": "Sergey Alyaev", "authors": "Sergey Alyaev, Reidar Brumer Bratvold, Sofija Ivanova, Andrew\n  Holsaeter, Morten Bendiksen", "title": "An interactive sequential-decision benchmark from geosteering", "comments": "arXiv admin note: substantial text overlap with arXiv:2005.08916", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SY eess.SY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geosteering workflows are increasingly based on the quantification of\nsubsurface uncertainties during real-time operations. As a consequence\noperational decision making is becoming both better informed and more complex.\nThis paper presents an experimental web-based decision support system, which\ncan be used to both aid expert decisions under uncertainty or further develop\ndecision optimization algorithms in controlled environment. A user of the\nsystem (either human or AI) controls the decisions to steer the well or stop\ndrilling. Whenever a user drills ahead, the system produces simulated\nmeasurements along the selected well trajectory which are used to update the\nuncertainty represented by model realizations using the ensemble Kalman filter.\nTo enable informed decisions the system is equipped with functionality to\nevaluate the value of the selected trajectory under uncertainty with respect to\nthe objectives of the current experiment.\n  To illustrate the utility of the system as a benchmark, we present the\ninitial experiment, in which we compare the decision skills of geoscientists\nwith those of a recently published automatic decision support algorithm. The\nexperiment and the survey after it showed that most participants were able to\nuse the interface and complete the three test rounds. At the same time, the\nautomated algorithm outperformed 28 out of 29 qualified human participants.\n  Such an experiment is not sufficient to draw conclusions about practical\ngeosteering, but is nevertheless useful for geoscience. First, this\ncommunication-by-doing made 76% of respondents more curious about and/or\nconfident in the presented technologies. Second, the system can be further used\nas a benchmark for sequential decisions under uncertainty. This can accelerate\ndevelopment of algorithms and improve the training for decision making.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 12:54:11 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Alyaev", "Sergey", ""], ["Bratvold", "Reidar Brumer", ""], ["Ivanova", "Sofija", ""], ["Holsaeter", "Andrew", ""], ["Bendiksen", "Morten", ""]]}, {"id": "2011.00938", "submitter": "David Kohns Mr", "authors": "David Kohns, Arnab Bhattacharjee", "title": "Developments on the Bayesian Structural Time Series Model: Trending\n  Growth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the added benefit of internet search data in the form\nof Google Trends for nowcasting real U.S. GDP growth in real time through the\nlens of the mixed frequency augmented Bayesian Structural Time Series model\n(BSTS) of Scott and Varian (2014). We show that a large dimensional set of\nsearch terms are able to improve nowcasts before other macro data becomes\navailable early on the quarter. Search terms with high inclusion probability\nhave negative correlation with GDP growth, which we reason to stem from them\nsignalling special attention likely due to expected large troughs. We further\noffer several improvements on the priors: we allow to shrink state variances to\nzero to avoid overfitting states, extend the SSVS prior to the more flexible\nnormal-inverse-gamma prior of Ishwaran et al. (2005) which stays agnostic about\nthe underlying model size, as well as adapt the horseshoe prior of Carvalho et\nal. (2010) to the BSTS. The application to nowcasting GDP growth as well as a\nsimulation study show that the horseshoe prior BSTS improves markedly over the\nSSVS and the original BSTS model, with largest gains to be expected in dense\ndata-generating-processes.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 12:36:30 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Kohns", "David", ""], ["Bhattacharjee", "Arnab", ""]]}, {"id": "2011.01002", "submitter": "Alvason Zhenhua Li", "authors": "Alvason Zhenhua Li, Karsten Eichholz, Anton Sholukh, Daniel Stone,\n  Michelle A. Loprieno, Keith R. Jerome, Khamsone Phasouk, Kurt Diem, Jia Zhu,\n  Lawrence Corey", "title": "RRScell method for automated single-cell profiling of multiplexed\n  immunofluorescence cancer tissue", "comments": "8 pages, 6 figures, markerUMAP cell clustering", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM eess.IV q-bio.TO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiplexed immuno-fluorescence tissue imaging, allowing simultaneous\ndetection of molecular properties of cells, is an essential tool for\ncharacterizing the complex cellular mechanisms in translational research and\nclinical practice. New image analysis approaches are needed because tissue\nsection stained with a mixture of protein, DNA and RNA biomarkers are\nintroducing various complexities, including spurious edges due to fluorescent\nstaining artifacts between touching or overlapping cells. We have developed the\nRRScell method harnessing the stochastic random-reaction-seed (RRS) algorithm\nand deep neural learning U-net to extract single-cell resolution profiling-map\nof gene expression over a million cells tissue section accurately and\nautomatically. Furthermore, with the use of manifold learning technique UMAP\nfor cell phenotype cluster analysis, the AI-driven RRScell has equipped with a\nmarker-based image cytometry analysis tool (markerUMAP) in quantifying spatial\ndistribution of cell phenotypes from tissue images with a mixture of\nbiomarkers. The results achieved in this study suggest that RRScell provides a\nrobust enough way for extracting cytometric single cell morphology as well as\nbiomarker content in various tissue types, while the build-in markerUMAP tool\nsecures the efficiency of dimension reduction, making it viable as a general\ntool in the spatial analysis of high dimensional tissue image.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 06:41:19 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 08:08:04 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Li", "Alvason Zhenhua", ""], ["Eichholz", "Karsten", ""], ["Sholukh", "Anton", ""], ["Stone", "Daniel", ""], ["Loprieno", "Michelle A.", ""], ["Jerome", "Keith R.", ""], ["Phasouk", "Khamsone", ""], ["Diem", "Kurt", ""], ["Zhu", "Jia", ""], ["Corey", "Lawrence", ""]]}, {"id": "2011.01092", "submitter": "Martin Spindler", "authors": "Philipp Bach, Victor Chernozhukov, Martin Spindler", "title": "Insights from Optimal Pandemic Shielding in a Multi-Group SEIR Framework", "comments": "39 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN physics.soc-ph q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic constitutes one of the largest threats in recent\ndecades to the health and economic welfare of populations globally. In this\npaper, we analyze different types of policy measures designed to fight the\nspread of the virus and minimize economic losses. Our analysis builds on a\nmulti-group SEIR model, which extends the multi-group SIR model introduced by\nAcemoglu et al.~(2020). We adjust the underlying social interaction patterns\nand consider an extended set of policy measures. The model is calibrated for\nGermany. Despite the trade-off between COVID-19 prevention and economic\nactivity that is inherent to shielding policies, our results show that\nefficiency gains can be achieved by targeting such policies towards different\nage groups. Alternative policies such as physical distancing can be employed to\nreduce the degree of targeting and the intensity and duration of shielding. Our\nresults show that a comprehensive approach that combines multiple policy\nmeasures simultaneously can effectively mitigate population mortality and\neconomic harm.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 16:23:41 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Bach", "Philipp", ""], ["Chernozhukov", "Victor", ""], ["Spindler", "Martin", ""]]}, {"id": "2011.01106", "submitter": "Haiyan Zheng", "authors": "Haiyan Zheng, Thomas Jaki, James M. S. Wason", "title": "Bayesian sample size determination using commensurate priors to leverage\n  pre-experimental data", "comments": "A submitted paper formatted with the double spacing, including 34\n  pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper develops Bayesian sample size formulae for experiments comparing\ntwo groups. We assume the experimental data will be analysed in the Bayesian\nframework, where pre-experimental information from multiple sources can be\nrepresented into robust priors. In particular, such robust priors account for\npreliminary belief about the pairwise commensurability between parameters that\nunderpin the historical and new experiments, to permit flexible borrowing of\ninformation. Averaged over the probability space of the new experimental data,\nappropriate sample sizes are found according to criteria that control certain\naspects of the posterior distribution, such as the coverage probability or\nlength of a defined density region. Our Bayesian methodology can be applied to\ncircumstances where the common variance in the new experiment is known or\nunknown. Exact solutions are available based on most of the criteria considered\nfor Bayesian sample size determination, while a search procedure is described\nin cases for which there are no closed-form expressions. We illustrate the\napplication of our Bayesian sample size formulae in the setting of designing a\nclinical trial. Hypothetical data examples, motivated by a rare-disease trial\nwith elicitation of expert prior opinion, and a comprehensive performance\nevaluation of the proposed methodology are presented.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 16:39:33 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Zheng", "Haiyan", ""], ["Jaki", "Thomas", ""], ["Wason", "James M. S.", ""]]}, {"id": "2011.01179", "submitter": "Emma Pierson", "authors": "Emma Pierson", "title": "Assessing racial inequality in COVID-19 testing with Bayesian threshold\n  tests", "comments": "Spotlight presentation, Machine Learning for Health (ML4H) at NeurIPS\n  2020 - Extended Abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There are racial disparities in the COVID-19 test positivity rate, suggesting\nthat minorities may be under-tested. Here, drawing on the literature on\nstatistically assessing racial disparities in policing, we 1) illuminate a\nstatistical flaw, known as infra-marginality, in using the positivity rate as a\nmetric for assessing racial disparities in under-testing; 2) develop a new type\nof Bayesian threshold test to measure disparities in COVID-19 testing and 3)\napply the test to measure racial disparities in testing thresholds in a\nreal-world COVID-19 dataset.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 18:17:03 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Pierson", "Emma", ""]]}, {"id": "2011.01214", "submitter": "R\\'obert Rajk\\'o", "authors": "Rudolf Ferenc (1), Istv\\'an Siket (1), P\\'eter Heged\\H{u}s (1 and 2),\n  R\\'obert Rajk\\'o (3) ((1) Department of Software Engineering, University of\n  Szeged, Szeged, Hungary, (2) MTA-SZTE Research Group on Artificial\n  Intelligence, University of Szeged, Szeged, Hungary, (3) Institute of\n  Mathematics and Informatics, University of P\\'ecs, P\\'ecs, Hungary)", "title": "Employing Partial Least Squares Regression with Discriminant Analysis\n  for Bug Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Forecasting defect proneness of source code has long been a major research\nconcern. Having an estimation of those parts of a software system that most\nlikely contain bugs may help focus testing efforts, reduce costs, and improve\nproduct quality. Many prediction models and approaches have been introduced\nduring the past decades that try to forecast bugged code elements based on\nstatic source code metrics, change and history metrics, or both. However, there\nis still no universal best solution to this problem, as most suitable features\nand models vary from dataset to dataset and depend on the context in which we\nuse them. Therefore, novel approaches and further studies on this topic are\nhighly necessary. In this paper, we employ a chemometric approach - Partial\nLeast Squares with Discriminant Analysis (PLS-DA) - for predicting bug prone\nClasses in Java programs using static source code metrics. To our best\nknowledge, PLS-DA has never been used before as a statistical approach in the\nsoftware maintenance domain for predicting software errors. In addition, we\nhave used rigorous statistical treatments including bootstrap resampling and\nrandomization (permutation) test, and evaluation for representing the software\nengineering results. We show that our PLS-DA based prediction model achieves\nsuperior performances compared to the state-of-the-art approaches (i.e.\nF-measure of 0.44-0.47 at 90% confidence level) when no data re-sampling\napplied and comparable to others when applying up-sampling on the largest open\nbug dataset, while training the model is significantly faster, thus finding\noptimal parameters is much easier. In terms of completeness, which measures the\namount of bugs contained in the Java Classes predicted to be defective, PLS-DA\noutperforms every other algorithm: it found 69.3% and 79.4% of the total bugs\nwith no re-sampling and up-sampling, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 18:58:17 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Ferenc", "Rudolf", "", "1 and 2"], ["Siket", "Istv\u00e1n", "", "1 and 2"], ["Heged\u0171s", "P\u00e9ter", "", "1 and 2"], ["Rajk\u00f3", "R\u00f3bert", ""]]}, {"id": "2011.01263", "submitter": "Jiachen Zhang", "authors": "Jiachen Zhang, Paola Crippa, Marc G. Genton, Stefano Castruccio", "title": "Assessing the Reliability of Wind Power Operations under a Changing\n  Climate with a Non-Gaussian Bias Correction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facing increasing societal and economic pressure, many countries have\nestablished strategies to develop renewable energy portfolios, whose\npenetration in the market can alleviate the dependence on fossil fuels. In the\ncase of wind, there is a fundamental question related to the resilience, and\nhence profitability of future wind farms to a changing climate, given that\ncurrent wind turbines have lifespans of up to thirty years. In this work, we\ndevelop a new non-Gaussian method data to simulations and to estimate future\nwind, predicated on a trans-Gaussian transformation and a cluster-wise\nminimization of the Kullback-Leibler divergence. Future winds abundance will be\ndetermined for Saudi Arabia, a country with a recently established plan to\ndevelop a portfolio of up to 16 GW of wind energy. Further, we estimate the\nchange in profits over future decades using additional high-resolution\nsimulations, an improved method for vertical wind extrapolation, and power\ncurves from a collection of popular wind turbines. We find an overall increase\nin the daily profit of $272,000 for the wind energy market for the optimal\nlocations for wind farming in the country.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 19:10:15 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 22:46:35 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Zhang", "Jiachen", ""], ["Crippa", "Paola", ""], ["Genton", "Marc G.", ""], ["Castruccio", "Stefano", ""]]}, {"id": "2011.01332", "submitter": "Sotiris Tegos", "authors": "Sotiris A. Tegos, George K. Karagiannidis, Panagiotis D.\n  Diamantoulakis, and Nestor D. Chatzidiamantis", "title": "New Results for Pearson Type III Family of Distributions and Application\n  in Wireless Power Transfer", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pearson and log Pearson type III distributions have been considered in\nseveral scientific fields, as in hydrology and seismology. In this paper, we\npresent new results for these distributions and we utilize them, for first time\nin the literature, to investigate the statistical behavior of wireless power\ntransfer (WPT), assuming that the harvested energy follows a well-established\nnonlinear energy harvesting model based on the logistic function. Specifically,\nwe present new closed-form expressions for the statistical properties of a\ngeneral form of Pearson and log Pearson type III distributions and we utilize\nthem to introduce a new member of the Pearson type III family, the logit\nPearson type III distribution, through which the logit gamma distribution is\nalso defined. Moreover, we derive closed-form expressions for the probability\ndensity function, the cumulative distribution function and moments of the\ndistributions of the sum, the log sum and the logit sum of Pearson type III\nrandom variables. Furthermore, taking into account that Pearson type III family\nof distributions is closely related to the considered nonlinear harvesting\nmodel the statistical properties of the distribution of the harvested power and\nderived, for both single input single output and multiple input single output\nscenarios.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 21:39:06 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Tegos", "Sotiris A.", ""], ["Karagiannidis", "George K.", ""], ["Diamantoulakis", "Panagiotis D.", ""], ["Chatzidiamantis", "Nestor D.", ""]]}, {"id": "2011.01423", "submitter": "Sayani Gupta", "authors": "Sayani Gupta, Puneet Chitkara", "title": "Day Ahead Price Forecasting Models in Thin Electricity Market", "comments": "6 pages, 6 figures, Power and Energy Conference at Illinois (PECI)\n  2017", "journal-ref": "2017 {IEEE} Power and Energy Conference at Illinois ({PECI})", "doi": "10.1109/PECI.2017.7935757", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Day Ahead Electricity Markets (DAMs) in India are thin but growing.\nConsistent price forecasts are important for their utilization in portfolio\noptimization models. Univariate or multivariate models with standard exogenous\nvariables such as special day effects etc. are not always useful. Drivers of\ndemand and supply include weather variations over large geographic areas,\noutages of power system elements and sudden changes in contracts which lead the\nplayers to access power exchanges. These needs to be considered in forecasting\nmodels. Such models are observed to considerably reduce forecasting errors by\noutperforming other models under conditions, which are neither infrequent nor\nrecur at defined intervals. This paper develops models for India and tests the\nutility of these models using Model Confidence Set (MCS) approach which picks\nup the best models. The approach has been developed for a power utility in\nIndia over a period of two years in live business environment.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 02:12:28 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Gupta", "Sayani", ""], ["Chitkara", "Puneet", ""]]}, {"id": "2011.01501", "submitter": "Qilong Pan", "authors": "Pan Qilong, Yin Jieru, and Xiao Xinping", "title": "Novel Compositional Data's Grey Model for Structurally Forecasting\n  Arctic Crude Oil Import", "comments": "22 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reserve of crude oil in the Arctic area is abundant. Ice melting is\nmaking it possible to have intermediate access to the Arctic crude oil and its\ntransportation. A novel compositional data's grey model is proposed in this\npaper to structurally forecast Arctic crude oil import. Firstly, the general\naccumulative operation sequence of multivariate compositional data is defined\naccording to Aitchison geometry, then obtaining the novel model with the form\nof the compositional data vectors. Secondly, this paper studies the least\nsquare parameter estimation of the model. The novel model is deduced and\nselected as the time-response expression of the solution. Thirdly, this paper\ninfuses the novel model with traditional grey model to improve its robustness.\nDifferential Evolution algorithm is introduced to determine the optimal value\nof the general matrix. Lastly, two validation examples are provided for\nconfirming the effectiveness of the novel model by comparing it with other\nexisting models, before being employed to forecast the crude oil import\nstructure in China. The results show that the novel model provides better\nperformance in all crude oil cases in short-term forecasting. Therefore, by\nusing the new model, China's development parameter is 0.5214 and Determination\nFactor of the novel model is 0.5999, which means that the crude oil import\nstructure of China is being changed. Specifically, the amount of crude oil\nimported from the Arctic area is obviously increasing in the next 6 years,\nshowing sufficient proof of the edge owned by the Arctic area: abundant crude\noil reserves and shortening transportation distance.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 06:31:47 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Qilong", "Pan", ""], ["Jieru", "Yin", ""], ["Xinping", "Xiao", ""]]}, {"id": "2011.01567", "submitter": "Sida Chen", "authors": "Sida Chen and B\\\"arbel Finkenst\\\"adt Rand", "title": "Bayesian inference for spline-based hidden Markov models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  B-spline-based hidden Markov models (HMMs), where the emission densities are\nspecified as mixtures of normalized B-spline basis functions, offer a more\nflexible modelling approach to data than conventional parametric HMMs. We\nintroduce a fully Bayesian framework for inference in these nonparametric\nmodels where the number of states may be unknown along with other model\nparameters. We propose the use of a trans-dimensional Markov chain inference\nalgorithm to identify a parsimonious knot configuration of the B-splines while\nmodel selection regarding the number of states can be performed within a\nparallel sampling framework. The feasibility and efficiency of our proposed\nmethodology is shown in a simulation study. Its explorative use for real data\nis demonstrated for activity acceleration data in animals, i.e.\nwhitetip-sharks. The flexibility of a Bayesian approach allows us to extend the\nmodelling framework in a straightforward way and we demonstrate this by\ndeveloping a hierarchical conditional HMM to analyse human accelerator activity\ndata to focus on studying small movements and/or inactivity during sleep.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 08:47:27 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Chen", "Sida", ""], ["Rand", "B\u00e4rbel Finkenst\u00e4dt", ""]]}, {"id": "2011.01661", "submitter": "Subhadip Maji", "authors": "Indranil Basu and Subhadip Maji", "title": "Multicollinearity Correction and Combined Feature Effect in Shapley\n  Values", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model interpretability is one of the most intriguing problems in most of the\nMachine Learning models, particularly for those that are mathematically\nsophisticated. Computing Shapley Values are arguably the best approach so far\nto find the importance of each feature in a model, at the row level. In other\nwords, Shapley values represent the importance of a feature for a particular\nrow, especially for Classification or Regression problems. One of the biggest\nlimitations of Shapley vales is that, Shapley value calculations assume all the\nfeatures are uncorrelated (independent of each other), this assumption is often\nincorrect. To address this problem, we present a unified framework to calculate\nShapley values with correlated features. To be more specific, we do an\nadjustment (Matrix formulation) of the features while calculating Independent\nShapley values for the rows. Moreover, we have given a Mathematical proof\nagainst the said adjustments. With these adjustments, Shapley values\n(Importance) for the features become independent of the correlations existing\nbetween them. We have also enhanced this adjustment concept for more than\nfeatures. As the Shapley values are additive, to calculate combined effect of\ntwo features, we just have to add their individual Shapley values. This is\nagain not right if one or more of the features (used in the combination) are\ncorrelated with the other features (not in the combination). We have addressed\nthis problem also by extending the correlation adjustment for one feature to\nmultiple features in the said combination for which Shapley values are\ndetermined. Our implementation of this method proves that our method is\ncomputationally efficient also, compared to original Shapley method.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 12:28:42 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Basu", "Indranil", ""], ["Maji", "Subhadip", ""]]}, {"id": "2011.01725", "submitter": "Vincent Valton PhD", "authors": "Vincent Valton, Toby Wise, Oliver J. Robinson", "title": "Recommendations for Bayesian hierarchical model specifications for\n  case-control studies in mental health", "comments": "Machine Learning for Health (ML4H) at NeurIPS 2020 - Extended\n  Abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical model fitting has become commonplace for case-control studies of\ncognition and behaviour in mental health. However, these techniques require us\nto formalise assumptions about the data-generating process at the group level,\nwhich may not be known. Specifically, researchers typically must choose whether\nto assume all subjects are drawn from a common population, or to model them as\nderiving from separate populations. These assumptions have profound\nimplications for computational psychiatry, as they affect the resulting\ninference (latent parameter recovery) and may conflate or mask true group-level\ndifferences. To test these assumptions we ran systematic simulations on\nsynthetic multi-group behavioural data from a commonly used multi-armed bandit\ntask (reinforcement learning task). We then examined recovery of group\ndifferences in latent parameter space under the two commonly used generative\nmodelling assumptions: (1) modelling groups under a common shared group-level\nprior (assuming all participants are generated from a common distribution, and\nare likely to share common characteristics); (2) modelling separate groups\nbased on symptomatology or diagnostic labels, resulting in separate group-level\npriors. We evaluated the robustness of these approaches to variations in data\nquality and prior specifications on a variety of metrics. We found that fitting\ngroups separately (assumptions 2), provided the most accurate and robust\ninference across all conditions. Our results suggest that when dealing with\ndata from multiple clinical groups, researchers should analyse patient and\ncontrol groups separately as it provides the most accurate and robust recovery\nof the parameters of interest.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 14:19:59 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Valton", "Vincent", ""], ["Wise", "Toby", ""], ["Robinson", "Oliver J.", ""]]}, {"id": "2011.01876", "submitter": "Asma Azizi", "authors": "Asma Azizi, Anamika Mubayi, Anuj Mubayi", "title": "The Impact of Individual's Ecological Factors on the Dynamics of Alcohol\n  Drinking among Arizona State University Students: An Application of the\n  Survey Data-driven Agent-based Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  College-aged students are one of the most vulnerable populations to high-risk\nalcohol drinking behaviors that could cause them consequences such as injury or\nsexual assault. An important factor that may influence college students'\ndecision on alcohol drinking behavior is socializing at certain contexts across\nuniversity environment. The present study aims to identify and better\nunderstand ecological conditions driving the dynamics of distribution of\nalcohol use among college-aged students.\n  To this end, a pilot study is conducted to evaluate students' movement\npatterns to different contexts across the Arizona State University (ASU)\ncampus, and to use its results to develop an agent-based simulation model\ndesigned for examining the role of environmental factors on development and\nmaintenance of alcohol drinking behavior by a representative sample of ASU\nstudents. The proposed model that resembles an approximate reaction-diffusion\nmodel accounts for movement of agents to various contexts (i.e. diffusion) and\nalcohol drinking influences within those contexts (i.e., reaction) via a\nSIR-type model. Of the four most visited contexts at ASU Tempe campus --\nLibrary, Memorial Union, Fitness Center, and Dorm -- the context with the\nhighest visiting probability, Memorial Union, is the most influential and most\nsensitive context (around $16$ times higher impact of alcohol related\ninfluences than the other contexts) on spreading alcohol drinking behavior.\n  Our findings highlight the crucial role of socialization at local\nenvironments on the dynamics of students' alcohol use as well as on the\nlong-term prediction of the college drinking prevalence.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 04:18:32 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Azizi", "Asma", ""], ["Mubayi", "Anamika", ""], ["Mubayi", "Anuj", ""]]}, {"id": "2011.01964", "submitter": "Bingyu Zhao", "authors": "Gerard Casey, Bingyu Zhao, Krishna Kumar, Kenichi Soga", "title": "Context-specific volume-delay curves by combining crowd-sourced traffic\n  data with Automated Traffic Counters (ATC): a case study for London", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Traffic congestion across the world has reached chronic levels. Despite many\ntechnological disruptions, one of the most fundamental and widely used\nfunctions within traffic modelling, the volume delay function, has seen little\nin the way of change since it was developed in the 1960's. Traditionally\nmacroscopic methods have been employed to relate traffic volume to vehicular\njourney time. The general nature of these functions enables their ease of use\nand gives widespread applicability. However, they lack the ability to consider\nindividual road characteristics (i.e. geometry, presence of traffic furniture,\nroad quality and surrounding environment). This research investigates the\nfeasibility to reconstruct the model using two different data sources, namely\nthe traffic speed from Google Maps' Directions Application Programming\nInterface (API) and traffic volume data from automated traffic counters (ATC).\nGoogle's traffic speed data are crowd-sourced from the smartphone Global\nPositioning System (GPS) of road users, able to reflect real-time,\ncontext-specific traffic condition of a road. On the other hand, the ATCs\nenable the harvesting of the vehicle volume data over equally fine temporal\nresolutions (hourly or less). By combining them for different road types in\nLondon, new context-specific volume-delay functions can be generated. This\nmethod shows promise in selected locations with the generation of robust\nfunctions. In other locations it highlights the need to better understand other\ninfluencing factors, such as the presence of on road parking or weather events.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 19:13:30 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Casey", "Gerard", ""], ["Zhao", "Bingyu", ""], ["Kumar", "Krishna", ""], ["Soga", "Kenichi", ""]]}, {"id": "2011.02034", "submitter": "Qixiang Fang", "authors": "Qixiang Fang, Joep Burger, Ralph Meijers, Kees van Berkel", "title": "The Role of Time, Weather and Google Trends in Understanding and\n  Predicting Web Survey Response", "comments": "Accepted by Survey Research Methods. For replication package, see\n  http://doi.org/10.5281/zenodo.4159915", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the literature about web survey methodology, significant efforts have been\nmade to understand the role of time-invariant factors (e.g. gender, education\nand marital status) in (non-)response mechanisms. Time-invariant factors alone,\nhowever, cannot account for most variations in (non-)responses, especially\nfluctuations of response rates over time. This observation inspires us to\ninvestigate the counterpart of time-invariant factors, namely time-varying\nfactors and the potential role they play in web survey (non-)response.\nSpecifically, we study the effects of time, weather and societal trends\n(derived from Google Trends data) on the daily (non-)response patterns of the\n2016 and 2017 Dutch Health Surveys. Using discrete-time survival analysis, we\nfind, among others, that weekends, holidays, pleasant weather, disease\noutbreaks and terrorism salience are associated with fewer responses.\nFurthermore, we show that using these variables alone achieves satisfactory\nprediction accuracy of both daily and cumulative response rates when the\ntrained model is applied to future unseen data. This approach has the further\nbenefit of requiring only non-personal contextual information and thus\ninvolving no privacy issues. We discuss the implications of the study for\nsurvey research and data collection.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 14:04:28 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Fang", "Qixiang", ""], ["Burger", "Joep", ""], ["Meijers", "Ralph", ""], ["van Berkel", "Kees", ""]]}, {"id": "2011.02051", "submitter": "Johannes Rahlf", "authors": "Johannes Rahlf and Marius Hauglin and Rasmus Astrup and Johannes\n  Breidenbach", "title": "Timber Volume Estimation Based on Airborne Laser Scanning -- Comparing\n  the Use of National Forest Inventory and Forest Management Inventory Data", "comments": null, "journal-ref": null, "doi": "10.1007/s13595-021-01061-4", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale forest resource maps based on national forest inventory (NFI)\ndata and airborne laser scanning may facilitate synergies between NFIs and\nforest management inventories (FMIs). A comparison of models used in such a\nNFI-based map and a FMI indicate that NFI-based maps can directly be used in\nFMIs to estimate timber volume of mature spruce forests. Traditionally, FMIs\nand NFIs have been separate activities. The increasing availability of detailed\nNFI-based forest resource maps provides the possibility to eliminate or reduce\nthe need of field sample plot measurements in FMIs if their accuracy is\nsimilar. We aim to 1) compare a timber volume model used in a NFI-based map and\nmodels used in a FMI, and 2) evaluate utilizing additional local sample plots\nin the model of the NFI-based map. Accuracies of timber volume estimates using\nmodels from an existing NFI-based map and a FMI were compared at plot and stand\nlevel. Estimates from the NFI-based map were similar to or more accurate than\nthe FMI. The addition of local plots to the modeling data did not clearly\nimprove the model of the NFI-based map.The comparison indicates that NFI-based\nmaps can directly be used in FMIs for timber volume estimation in mature spruce\nstands, leading to potentially large cost savings.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 22:56:13 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 09:07:52 GMT"}, {"version": "v3", "created": "Wed, 28 Apr 2021 08:50:44 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Rahlf", "Johannes", ""], ["Hauglin", "Marius", ""], ["Astrup", "Rasmus", ""], ["Breidenbach", "Johannes", ""]]}, {"id": "2011.02089", "submitter": "Grace Deng", "authors": "Grace Deng, Cuize Han, David S. Matteson", "title": "Learning to Rank with Missing Data via Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the role of Conditional Generative Adversarial Networks (GAN) in\nimputing missing data and apply GAN imputation on a novel use case in\ne-commerce: a learning-to-rank problem with incomplete training data.\nConventional imputation methods often make assumptions regarding the underlying\ndistribution of the missing data, while GANs offer an alternative framework to\nsidestep approximating intractable distributions. First, we prove that GAN\nimputation offers theoretical guarantees beyond the naive Missing Completely At\nRandom (MCAR) scenario. Next, we show that empirically, the Conditional GAN\nstructure is well suited for data with heterogeneous distributions and across\nunbalanced classes, improving performance metrics such as RMSE. Using an Amazon\nSearch ranking dataset, we produce standard ranking models trained on\nGAN-imputed data that are comparable to training on ground-truth data based on\nstandard ranking quality metrics NDCG and MRR. We also highlight how different\nneural net features such as convolution and dropout layers can improve\nperformance given different missing value settings.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 01:15:41 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 20:42:19 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Deng", "Grace", ""], ["Han", "Cuize", ""], ["Matteson", "David S.", ""]]}, {"id": "2011.02179", "submitter": "Nafiseh Ghoroghchian Ms.", "authors": "Nafiseh Ghoroghchian, David M. Groppe, Roman Genov, Taufik A.\n  Valiante, and Stark C. Draper", "title": "Node-Centric Graph Learning from Data for Brain State Identification", "comments": null, "journal-ref": "IEEE Transactions on Signal and Information Processing over\n  Networks, 6, 120-132 (2020)", "doi": "10.1109/TSIPN.2020.2964230", "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven graph learning models a network by determining the strength of\nconnections between its nodes. The data refers to a graph signal which\nassociates a value with each graph node. Existing graph learning methods either\nuse simplified models for the graph signal, or they are prohibitively expensive\nin terms of computational and memory requirements. This is particularly true\nwhen the number of nodes is high or there are temporal changes in the network.\nIn order to consider richer models with a reasonable computational\ntractability, we introduce a graph learning method based on representation\nlearning on graphs. Representation learning generates an embedding for each\ngraph node, taking the information from neighbouring nodes into account. Our\ngraph learning method further modifies the embeddings to compute the graph\nsimilarity matrix. In this work, graph learning is used to examine brain\nnetworks for brain state identification. We infer time-varying brain graphs\nfrom an extensive dataset of intracranial electroencephalographic (iEEG)\nsignals from ten patients. We then apply the graphs as input to a classifier to\ndistinguish seizure vs. non-seizure brain states. Using the binary\nclassification metric of area under the receiver operating characteristic curve\n(AUC), this approach yields an average of 9.13 percent improvement when\ncompared to two widely used brain network modeling methods.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 08:44:44 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Ghoroghchian", "Nafiseh", ""], ["Groppe", "David M.", ""], ["Genov", "Roman", ""], ["Valiante", "Taufik A.", ""], ["Draper", "Stark C.", ""]]}, {"id": "2011.02362", "submitter": "Maria Michela Dickson", "authors": "Lucia Savadori, Giuseppe Espa, Maria Michela Dickson", "title": "The polarizing impact of numeracy, economic literacy, and science\n  literacy on attitudes toward immigration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Political orientation polarizes the attitudes of more educated individuals on\ncontroversial issues. A highly controversial issue in Europe is immigration. We\nfound the same polarizing pattern for opinion toward immigration in a\nrepresentative sample of citizens of a southern European middle-size city.\nCitizens with higher numeracy, scientific and economic literacy presented a\nmore polarized view of immigration, depending on their worldview orientation.\nHighly knowledgeable individuals endorsing an egalitarian-communitarian\nworldview were more in favor of immigration, whereas highly knowledgeable\nindividuals with a hierarchical-individualist worldview were less in favor of\nimmigration. Those low in numerical, economic, and scientific literacy did not\nshow a polarized attitude. Results highlight the central role of\nsocio-political orientation over information theories in shaping attitudes\ntoward immigration.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 15:44:33 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Savadori", "Lucia", ""], ["Espa", "Giuseppe", ""], ["Dickson", "Maria Michela", ""]]}, {"id": "2011.02399", "submitter": "S. Stanley Young", "authors": "S. Stanley Young and Warren Kindzierski", "title": "Particulate Matter Exposure and Lung Cancer: A Review of two\n  Meta-Analysis Studies", "comments": "approximately 10 pages,4 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current regulatory paradigm is that PM2.5, over time causes lung cancer.\nThis claim is based on cohort studies and meta-analysis that use cohort studies\nas their base studies. There is a need to evaluate the reliability of this\ncausal claim. Our idea is to examine the base studies with respect to multiple\ntesting and multiple modeling and to look closer at the meta-analysis using\np-value plots. For two meta-analysis we investigated, some extremely small\np-values were observed in some of the base studies, which we think are due to a\ncombination of bias and small standard errors. The p-value plot for one\nmeta-analysis indicates no effect. For the other meta-analysis, we note the\np-value plot is consistent with a two-component mixture. Small p-values might\nbe real or due to some combination of p-hacking, publication bias, covariate\nproblems, etc. The large p-values could indicate no real effect, or be wrong\ndue to low power, missing covariates, etc. We conclude that the results are\nambiguous at best. These meta-analyses do not establish that PM2.5 is causal of\nlung tumors.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 16:51:48 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Young", "S. Stanley", ""], ["Kindzierski", "Warren", ""]]}, {"id": "2011.02420", "submitter": "Christian Staerk", "authors": "Christian Staerk, Tobias Wistuba, Andreas Mayr", "title": "Estimating effective infection fatality rates during the course of the\n  COVID-19 pandemic in Germany", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The infection fatality rate (IFR) of the Coronavirus Disease 2019 (COVID-19)\nis one of the most discussed figures in the context of this pandemic. Using\nGerman COVID-19 surveillance data and age-group specific IFR estimates from\nmultiple international studies, this work investigates time-dependent\nvariations in effective IFR over the course of the pandemic. Three different\nmethods for estimating (effective) IFRs are presented: (a) population-averaged\nIFRs based on the assumption that the infection risk is independent of age and\ntime, (b) effective IFRs based on the assumption that the age distribution of\nconfirmed cases approximately reflects the age distribution of infected\nindividuals, and (c) effective IFRs accounting for age- and time-dependent dark\nfigures of infections. Results show that effective IFRs in Germany are\nestimated to vary over time, as the age distributions of confirmed cases and\nestimated infections are changing during the course of the pandemic. In\nparticular during the first and second waves of infections in spring and\nautumn/winter 2020, there has been a pronounced shift in the age distribution\nof confirmed cases towards older age groups, resulting in larger effective IFR\nestimates. The temporary increase in effective IFR during the first wave is\nestimated to be smaller but still remains when adjusting for age- and\ntime-dependent dark figures. A comparison of effective IFRs with observed CFRs\nindicates that a substantial fraction of the time-dependent variability in\nobserved mortality can be explained by changes in the age distribution of\ninfections. Furthermore, a vanishing gap between effective IFRs and observed\nCFRs is apparent after the first infection wave, while a moderately increasing\ngap can be observed during the second wave. Further research is warranted to\nobtain timely age-stratified IFR estimates.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 17:20:31 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2021 15:16:31 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Staerk", "Christian", ""], ["Wistuba", "Tobias", ""], ["Mayr", "Andreas", ""]]}, {"id": "2011.02460", "submitter": "Mason Youngblood", "authors": "Mason Youngblood, Karim Baraghith, Patrick E. Savage", "title": "Phylogenetic reconstruction of the cultural evolution of electronic\n  music via dynamic community detection (1975-1999)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cultural phylogenies, or \"trees\" of culture, are typically built using\nmethods from biology that use similarities and differences in artifacts to\ninfer the historical relationships between the populations that produced them.\nWhile these methods have yielded important insights, particularly in\nlinguistics, researchers continue to debate the extent to which cultural\nphylogenies are tree-like or reticulated due to high levels of horizontal\ntransmission. In this study, we propose a novel method for phylogenetic\nreconstruction using dynamic community detection that explicitly accounts for\ntransmission between lineages. We used data from 1,498,483 collaborative\nrelationships between electronic music artists to construct a cultural\nphylogeny based on observed population structure. The results suggest that,\nalthough the phylogeny is fundamentally tree-like, horizontal transmission is\ncommon and populations never become fully isolated from one another. In\naddition, we found evidence that electronic music diversity has increased\nbetween 1975 and 1999. The method used in this study is available as a new R\npackage called DynCommPhylo. Future studies should apply this method to other\ncultural systems such as academic publishing and film, as well as biological\nsystems where high resolution reproductive data is available, to assess how\nlevels of reticulation in evolution vary across domains.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 18:21:14 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 17:09:55 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Youngblood", "Mason", ""], ["Baraghith", "Karim", ""], ["Savage", "Patrick E.", ""]]}, {"id": "2011.02824", "submitter": "Tao Li", "authors": "Zixuan Han, Tao Li, Jinghong You", "title": "These Unprecedented Times: The Dynamic Pattern Of COVID-19 Deaths Around\n  The World", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we deal with COVID-19 data to study the trend of the\nepidemic at the global situation. Choosing the mortality rate as an appropriate\nmetric which measures the relative relation between the cumulative confirmed\ncases and death cases, we utilize the modified kernel estimator for random\ndensity function (Petersen and Muller, 2016) to obtain the density of the\nmortality rate, and apply Frechet change point detection (Dubey and Muller,\n2020) to check if there is any significant change on the dynamic pattern of\nCOVID-19 deaths. The analysis shows that the pattern of global COVID-19\nmortality rate seems to have an obvious change at 104 days after first day when\nthe reported death cases exceed 30. Then we discuss the reasons of abrupt\nchange of mortality rate trend from aspects both subjective and objective.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 14:06:03 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Han", "Zixuan", ""], ["Li", "Tao", ""], ["You", "Jinghong", ""]]}, {"id": "2011.02838", "submitter": "Ushnish Sengupta", "authors": "Ushnish Sengupta, Maximilian L. Croci, Matthew P. Juniper", "title": "Real-time parameter inference in reduced-order flame models with\n  heteroscedastic Bayesian neural network ensembles", "comments": null, "journal-ref": "Machine Learning and the Physical Sciences Workshop at the 34th\n  Conference on Neural Information Processing Systems (NeurIPS) 2020", "doi": null, "report-no": null, "categories": "cs.LG physics.flu-dyn stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of model parameters with uncertainties from observed data is a\nubiquitous inverse problem in science and engineering. In this paper, we\nsuggest an inexpensive and easy to implement parameter estimation technique\nthat uses a heteroscedastic Bayesian Neural Network trained using anchored\nensembling. The heteroscedastic aleatoric error of the network models the\nirreducible uncertainty due to parameter degeneracies in our inverse problem,\nwhile the epistemic uncertainty of the Bayesian model captures uncertainties\nwhich may arise from an input observation's out-of-distribution nature. We use\nthis tool to perform real-time parameter inference in a 6 parameter G-equation\nmodel of a ducted, premixed flame from observations of acoustically excited\nflames. We train our networks on a library of 2.1 million simulated flame\nvideos. Results on the test dataset of simulated flames show that the network\nrecovers flame model parameters, with the correlation coefficient between\npredicted and true parameters ranging from 0.97 to 0.99, and well-calibrated\nuncertainty estimates. The trained neural networks are then used to infer model\nparameters from real videos of a premixed Bunsen flame captured using a\nhigh-speed camera in our lab. Re-simulation using inferred parameters shows\nexcellent agreement between the real and simulated flames. Compared to Ensemble\nKalman Filter-based tools that have been proposed for this problem in the\ncombustion literature, our neural network ensemble achieves better\ndata-efficiency and our sub-millisecond inference times represent a savings on\ncomputational costs by several orders of magnitude. This allows us to calibrate\nour reduced-order flame model in real-time and predict the thermoacoustic\ninstability behaviour of the flame more accurately.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 15:04:34 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Sengupta", "Ushnish", ""], ["Croci", "Maximilian L.", ""], ["Juniper", "Matthew P.", ""]]}, {"id": "2011.03002", "submitter": "Eugene Furman Dr.", "authors": "Eugene Furman, Alex Cressman, Saeha Shin, Alexey Kuznetsov, Fahad\n  Razak, Amol Verma, Adam Diamant", "title": "Prediction of Personal Protective Equipment Use in Hospitals During\n  COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Demand for Personal Protective Equipment (PPE) such as surgical masks,\ngloves, and gowns has increased significantly since the onset of the COVID-19\npandemic. In hospital settings, both medical staff and patients are required to\nwear PPE. As these facilities resume regular operations, staff will be required\nto wear PPE at all times while additional PPE will be mandated during medical\nprocedures. This will put increased pressure on hospitals which have had\nproblems predicting PPE usage and sourcing its supply. To meet this challenge,\nwe propose an approach to predict demand for PPE. Specifically, we model the\nadmission of patients to a medical department using multiple independent\nqueues. Each queue represents a class of patients with similar treatment plans\nand hospital length-of-stay. By estimating the total workload of each class, we\nderive closed-form estimates for the expected amount of PPE required over a\nspecified time horizon using current PPE guidelines. We apply our approach to a\ndata set of 22,039 patients admitted to the general internal medicine\ndepartment at St. Michael's hospital in Toronto, Canada from April 2010 to\nNovember 2019. We find that gloves and surgical masks represent approximately\n90% of predicted PPE usage. We also find that while demand for gloves is driven\nentirely by patient-practitioner interactions, 86% of the predicted demand for\nsurgical masks can be attributed to the requirement that medical practitioners\nwill need to wear them when not interacting with patients.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 19:47:45 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 19:18:43 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Furman", "Eugene", ""], ["Cressman", "Alex", ""], ["Shin", "Saeha", ""], ["Kuznetsov", "Alexey", ""], ["Razak", "Fahad", ""], ["Verma", "Amol", ""], ["Diamant", "Adam", ""]]}, {"id": "2011.03121", "submitter": "Naoki Awaya", "authors": "Naoki Awaya, Li Ma", "title": "Hidden Markov P\\'olya trees for high-dimensional distributions", "comments": "50 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The P\\'olya tree (PT) process is a general-purpose Bayesian nonparametric\nmodel that has found wide application in a range of inference problems. The PT\nhas a simple analytic form and the resulting posterior computation boils down\nto straight-forward beta-binomial conjugate updates along a partition tree over\nthe sample space. Recent development in PT models shows that performance of\nthese models can be substantially improved by (i) incorporating latent state\nvariables that characterize local features of the underlying distributions and\n(ii) allowing the partition tree to adapt to the structure of the underlying\ndistribution. Despite these advances, however, some important limitations of\nthe PT that remain include---(i) the sensitivity in the posterior inference\nwith respect to the choice of the partition points, and (ii) the lack of\ncomputational scalability to multivariate problems beyond a small number\n($<10$) of dimensions. We consider a modeling strategy for PT models that\nincorporates a very flexible prior on the partition tree along with latent\nstates that can be first-order dependent (i.e., following a Markov process),\nand introduce a hybrid algorithm that combines sequential Monte Carlo (SMC) and\nrecursive message passing for posterior inference that can readily accommodate\nPT models with or without latent states as well as flexible partition points in\nproblems up to 100 dimensions. Moreover, we investigate the large sample\nproperties of the tree structures and latent states under the posterior model.\nWe carry out extensive numerical experiments in the context of density\nestimation and two-sample testing, which show that flexible partitioning can\nsubstantially improve the performance of PT models in both inference tasks. We\ndemonstrate an application to a flow cytometry data set with 19 dimensions and\nover 200,000 observations.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 22:14:03 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 04:23:23 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Awaya", "Naoki", ""], ["Ma", "Li", ""]]}, {"id": "2011.03140", "submitter": "Colin Lewis-Beck", "authors": "Colin Lewis-Beck, Qinglong Tian, William Q. Meeker", "title": "Prediction of Future Failures for Heterogeneous Reliability Field Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces methods for constructing prediction bounds or\nintervals for the number of future failures from heterogeneous reliability\nfield data. We focus on within-sample prediction where early data from a\nfailure-time process is used to predict future failures from the same process.\nEarly data from high-reliability products, however, often have limited\ninformation due to some combination of small sample sizes, censoring, and\ntruncation. In such cases, we use a Bayesian hierarchical model to model\njointly multiple lifetime distributions arising from different subpopulations\nof similar products. By borrowing information across subpopulations, our method\nenables stable estimation and the computation of corresponding prediction\nintervals, even in cases where there are few observed failures. Three\napplications are provided to illustrate this methodology, and a simulation\nstudy is used to validate the coverage performance of the prediction intervals.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 23:40:46 GMT"}, {"version": "v2", "created": "Sat, 23 Jan 2021 17:34:59 GMT"}, {"version": "v3", "created": "Sun, 11 Apr 2021 15:16:21 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Lewis-Beck", "Colin", ""], ["Tian", "Qinglong", ""], ["Meeker", "William Q.", ""]]}, {"id": "2011.03270", "submitter": "Helen Barnett", "authors": "Helen Yvette Barnett, Sofia S Villar, Helena Geys and Thomas Jaki", "title": "A Novel Statistical Test for Treatment Differences in Clinical Trials\n  using a Response Adaptive Forward Looking Gittins Index Rule", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most common objective for response adaptive clinical trials is to seek to\nensure that patients within a trial have a high chance of receiving the best\ntreatment available by altering the chance of allocation on the basis of\naccumulating data. Approaches which yield good patient benefit properties\nsuffer from low power from a frequentist perspective when testing for a\ntreatment difference at the end of the study due to the high imbalance in\ntreatment allocations. In this work we develop an alternative pairwise test for\ntreatment difference on the basis of allocation probabilities of the\ncovariate-adjusted response-adaptive randomization with forward looking Gittins\nindex rule (CARA-FLGI). The performance of the novel test is evaluated in\nsimulations for two-armed studies and then its applications to multi-armed\nstudies is illustrated. The proposed test has markedly improved power over the\ntraditional Fisher exact test when this class of non-myopic response adaptation\nis used. We also find that the test's power is close to the power of a Fisher\nexact test under equal randomization.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 10:29:45 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Barnett", "Helen Yvette", ""], ["Villar", "Sofia S", ""], ["Geys", "Helena", ""], ["Jaki", "Thomas", ""]]}, {"id": "2011.03392", "submitter": "Robert Kaestner Dr.", "authors": "Robert Kaestner", "title": "Did Hurricane Katrina Reduce Mortality?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent article in the American Economic Review, Tatyana Deryugina and\nDavid Molitor (DM) analyzed the effect of Hurricane Katrina on the mortality of\nelderly and disabled residents of New Orleans. The authors concluded that\nHurricane Katrina improved the eight-year survival rate of elderly and disabled\nresidents of New Orleans by 3% and that most of this decline in mortality was\ndue to declines in mortality among those who moved to places with lower\nmortality. In this article, I provide a critical assessment of the evidence\nprovided by DM to support their conclusions. There are three main problems.\nFirst, DM generally fail to account for the fact that people of different ages,\nraces or sex will have different probabilities of dying as time goes by, and\nwhen they do allow for this, results change markedly. Second, DM do not account\nfor the fact that residents in New Orleans are likely to be selected\nnon-randomly on the basis of health because of the relatively high mortality\nrate in New Orleans compared to the rest of the country. Third, there is\nconsiderable evidence that among those who moved from New Orleans, the\ndestination chosen was non-random. Finally, DM never directly assessed changes\nin mortality of those who moved, or stayed, in New Orleans before and after\nHurricane Katrina. These problems lead me to conclude that the evidence\npresented by DM does not support their inferences.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 14:49:13 GMT"}, {"version": "v2", "created": "Sat, 28 Nov 2020 15:15:44 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Kaestner", "Robert", ""]]}, {"id": "2011.03515", "submitter": "Paul Parker", "authors": "Paul A. Parker and Scott H. Holan", "title": "A Bayesian Functional Data Model for Surveys Collected under Informative\n  Sampling with Application to Mortality Estimation using NHANES", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional data are often extremely high-dimensional and exhibit strong\ndependence structures but can often prove valuable for both prediction and\ninference. The literature on functional data analysis is well developed;\nhowever, there has been very little work involving functional data in complex\nsurvey settings. Motivated by physical activity monitor data from the National\nHealth and Nutrition Examination Survey (NHANES), we develop a Bayesian model\nfor functional covariates that can properly account for the survey design. Our\napproach is intended for non-Gaussian data and can be applied in multivariate\nsettings. In addition, we make use of a variety of Bayesian modeling techniques\nto ensure that the model is fit in a computationally efficient manner. We\nillustrate the value of our approach through an empirical simulation study as\nwell as an example of mortality estimation using NHANES data.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 18:46:17 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Parker", "Paul A.", ""], ["Holan", "Scott H.", ""]]}, {"id": "2011.03567", "submitter": "Michael Lindon", "authors": "Michael Lindon, Alan Malek", "title": "Sequential Testing of Multinomial Hypotheses with Applications to\n  Detecting Implementation Errors and Missing Data in Randomized Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simply randomized designs are one of the most common controlled experiments\nused to study causal effects. Failure of the assignment mechanism, to provide\nproper randomization of units across treatments, or the data collection\nmechanism, when data is missing not at random, can render subsequent analysis\ninvalid if not properly identified. In this paper we demonstrate that such\npractical implementation errors can often be identified, fortunately, through\nconsideration of the total unit counts resulting in each treatment group. Based\non this observation, we introduce a sequential hypothesis test constructed from\nBayesian multinomial-Dirichlet families for detecting practical implementation\nerrors in simply randomized experiments. By establishing a Martingale property\nof the posterior odds under the null hypothesis, frequentist Type-I error is\ncontrolled under both optional stopping and continuation via maximal\ninequalities, preventing practitioners from potentially inflating false\npositive probabilities through continuous monitoring. In contrast to other\nstatistical tests that are performed once all data collection is completed, the\nproposed test is sequential - frequently rejecting the null during the process\nof data collection itself, saving further units from entering an\nimproperly-executed experiment. We illustrate the utility of this test in the\ncontext of online controlled experiments (OCEs), where the assignment is\nautomated through code and data collected through complex processing pipelines,\noften in the presence of unintended bugs and logical errors. Confidence\nsequences possessing desired sequential frequentist coverage probabilities are\nprovided and their connection to the Bayesian support interval is examined. The\ndifferences between pure Bayesian and sequential frequentist testing procedures\nare finally discussed through a conditional frequentist testing perspective.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 19:17:38 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Lindon", "Michael", ""], ["Malek", "Alan", ""]]}, {"id": "2011.03599", "submitter": "Samuel Tickle PhD", "authors": "S. O. Tickle and I. A. Eckley and P. Fearnhead", "title": "A computationally efficient, high-dimensional multiple changepoint\n  procedure with application to global terrorism incidence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting changepoints in datasets with many variates is a data science\nchallenge of increasing importance. Motivated by the problem of detecting\nchanges in the incidence of terrorism from a global terrorism database, we\npropose a novel approach to multiple changepoint detection in multivariate time\nseries. Our method, which we call SUBSET, is a model-based approach which uses\na penalised likelihood to detect changes for a wide class of parametric\nsettings. We provide theory that guides the choice of penalties to use for\nSUBSET, and that shows it has high power to detect changes regardless of\nwhether only a few variates or many variates change. Empirical results show\nthat SUBSET out-performs many existing approaches for detecting changes in mean\nin Gaussian data; additionally, unlike these alternative methods, it can be\neasily extended to non-Gaussian settings such as are appropriate for modelling\ncounts of terrorist events.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 21:21:33 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 18:36:02 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Tickle", "S. O.", ""], ["Eckley", "I. A.", ""], ["Fearnhead", "P.", ""]]}, {"id": "2011.03715", "submitter": "Marzieh Ajirak", "authors": "Marzieh Ajirak, Cassandra Heiselman, Anna Fuchs, Mia Heiligenstein,\n  Kimberly Herrera, Diana Garretto, Petar Djuric", "title": "GP-LVM of categorical data from test-positive COVID-19 pregnant women", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novel coronavirus disease 2019 (COVID-19) is rapidly spreading throughout the\nworld and while pregnant women present the same adverse outcome rates, they are\nunderrepresented in clinical research. In this paper, we model categorical\nvariables of 89 test-positive COVID-19 pregnant women within the unsupervised\nBayesian framework. We model the data using latent Gaussian processes for\ndensity estimation of multivariate categorical data. The results show that the\nmodel can find latent patterns in the data, which in turn could provide\nadditional insights into the study of pregnant women that are COVID-19\npositive.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 07:11:29 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Ajirak", "Marzieh", ""], ["Heiselman", "Cassandra", ""], ["Fuchs", "Anna", ""], ["Heiligenstein", "Mia", ""], ["Herrera", "Kimberly", ""], ["Garretto", "Diana", ""], ["Djuric", "Petar", ""]]}, {"id": "2011.03741", "submitter": "Constandina Koki", "authors": "Constandina Koki, Stefanos Leonardos, Georgios Piliouras", "title": "Exploring the Predictability of Cryptocurrencies via Bayesian Hidden\n  Markov Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a variety of multi-state Hidden Markov models for\npredicting and explaining the Bitcoin, Ether and Ripple returns in the presence\nof state (regime) dynamics. In addition, we examine the effects of several\nfinancial, economic and cryptocurrency specific predictors on the\ncryptocurrency return series. Our results indicate that the Non-Homogeneous\nHidden Markov (NHHM) model with four states has the best one-step-ahead\nforecasting performance among all competing models for all three series. The\ndominance of the predictive densities over the single regime random walk model\nrelies on the fact that the states capture alternating periods with distinct\nreturn characteristics. In particular, the four state NHHM model distinguishes\nbull, bear and calm regimes for the Bitcoin series, and periods with different\nprofit and risk magnitudes for the Ether and Ripple series. Also, conditionally\non the hidden states, it identifies predictors with different linear and\nnon-linear effects on the cryptocurrency returns. These empirical findings\nprovide important insight for portfolio management and policy implementation.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 10:13:49 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 08:35:45 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Koki", "Constandina", ""], ["Leonardos", "Stefanos", ""], ["Piliouras", "Georgios", ""]]}, {"id": "2011.03818", "submitter": "Peter Congdon Prof", "authors": "Peter Congdon", "title": "Mid-Epidemic Forecasts of COVID-19 Cases and Deaths: A Bivariate Model\n  applied to the UK", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The evolution of the COVID-19 epidemic has been accompanied by accumulating\nevidence on the underlying epidemiological parameters. Hence there is potential\nfor models providing mid-term forecasts of the epidemic trajectory using such\ninformation. The effectiveness of lockdown interventions can also be assessed\nby modelling later epidemic stages, possibly using a multiphase epidemic model.\nCommonly applied methods to analyze epidemic trajectories include\nphenomenological growth models (e.g. the Richards), and variants of the\nsusceptible-infected-recovered (SIR) compartment model. Here we focus on a\npractical forecasting approach, applied to interim UK COVID data, using a\nbivariate Reynolds model (cases and deaths). We show the utility of informative\npriors in developing and estimating the model, and compare error densities\n(Poisson-gamma, Poisson-lognormal, Poisson-logStudent) for overdispersed data\non new cases and deaths. We use cross-validation to assess medium term\nforecasts. We also consider the longer term post-lockdown epidemic profile to\nassess epidemic containment, using a two phase model. Fit to mid-epidemic data\nshows better fit to training data and better cross validation performance for a\nPoisson-logStudent model. Estimation of longer term epidemic data after\nlockdown relaxation, characterised by protracted slow downturn and then upturn\nin cases, casts doubt on effective containment. Many applications of\nphenomenological models have been to complete epidemics. However, evaluation of\nsuch models based simply on their fit to observed data may give only a partial\npicture, and cross-validation against actual trends is also useful. Similarly,\nit may be preferable to model incidence rather than cumulative data, though\nthis raises questions about suitable error densities for modelling often\nerratic fluctuations. Hence there may be utility in evaluating alternative\nerror assumptions.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 17:40:49 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Congdon", "Peter", ""]]}, {"id": "2011.03872", "submitter": "Gabriel Martos Venturini", "authors": "Miguel de Carvalho, Gabriel Martos", "title": "Modeling Interval Trendlines: Symbolic Singular Spectrum Analysis for\n  Interval Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this article we propose an extension of singular spectrum analysis for\ninterval-valued time series. The proposed methods can be used to decompose and\nforecast the dynamics governing a set-valued stochastic process. The resulting\ncomponents on which the interval time series is decomposed can be understood as\ninterval trendlines, cycles, or noise. Forecasting can be conducted through a\nlinear recurrent method, and we devised generalizations of the decomposition\nmethod for the multivariate setting. The performance of the proposed methods is\nshowcased in a simulation study. We apply the proposed methods so to track the\ndynamics governing the Argentina Stock Market (MERVAL) in real time, in a case\nstudy that covers the most recent period of turbulence that led to discussions\nof the government of Argentina with the International Monetary Fund.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 00:16:38 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["de Carvalho", "Miguel", ""], ["Martos", "Gabriel", ""]]}, {"id": "2011.03938", "submitter": "Miguel A. Martinez-Beneito", "authors": "Miguel A. Martinez-Beneito and Jorge Mateu and Paloma Botella-Rocamora", "title": "Spatio-temporal small area surveillance of the Covid-19 pandemics", "comments": "13 pages, 4 figures no tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of Covid-19 requires new effective tools for epidemiological\nsurveillance. Spatio-temporal disease mapping models, which allow dealing with\nhighly disaggregated spatial and temporal units of analysis, are a priority in\nthis sense. Spatio-temporal models provide a geographically detailed and\ntemporally updated overview of the current state of the pandemics, making\npublic health interventions to be more effective. Moreover, the use of\nspatio-temporal disease mapping models in the new Covid-19 epidemic context,\nfacilitates estimating newly demanded epidemiological indicators, such as the\ninstantaneous reproduction number (R_t), even for small areas. This, in turn,\nallows to adapt traditional disease mapping models to these new circumstancies\nand make their results more useful in this particular context.\n  In this paper we propose a new spatio-temporal disease mapping model,\nparticularly suited to Covid-19 surveillance. As an additional result, we\nderive instantaneous reproduction number estimates for small areas, enabling\nmonitoring this parameter with a high spatial disaggregation. We illustrate the\nuse of our proposal with the separate study of the disease pandemics in two\nSpanish regions. As a result, we illustrate how touristic flows could haved\nshaped the spatial distribution of the disease. In these real studies, we also\npropose new surveillance tools that can be used by regional public health\nservices to make a more efficient use of their resources.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 09:34:06 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Martinez-Beneito", "Miguel A.", ""], ["Mateu", "Jorge", ""], ["Botella-Rocamora", "Paloma", ""]]}, {"id": "2011.03996", "submitter": "Marcelo Medeiros", "authors": "Jianqing Fan, Ricardo P. Masini, Marcelo C. Medeiros", "title": "Do We Exploit all Information for Counterfactual Analysis? Benefits of\n  Factor Models and Idiosyncratic Correction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.LG stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The measurement of treatment (intervention) effects on a single (or just a\nfew) treated unit(s) based on counterfactuals constructed from artificial\ncontrols has become a popular practice in applied statistics and economics\nsince the proposal of the synthetic control method. In high-dimensional\nsetting, we often use principal component or (weakly) sparse regression to\nestimate counterfactuals. Do we use enough data information? To better estimate\nthe effects of price changes on the sales in our case study, we propose a\ngeneral framework on counterfactual analysis for high dimensional dependent\ndata. The framework includes both principal component regression and sparse\nlinear regression as specific cases. It uses both factor and idiosyncratic\ncomponents as predictors for improved counterfactual analysis, resulting a\nmethod called Factor-Adjusted Regularized Method for Treatment (FarmTreat)\nevaluation. We demonstrate convincingly that using either factors or sparse\nregression is inadequate for counterfactual analysis in many applications and\nthe case for information gain can be made through the use of idiosyncratic\ncomponents. We also develop theory and methods to formally answer the question\nif common factors are adequate for estimating counterfactuals. Furthermore, we\nconsider a simple resampling approach to conduct inference on the treatment\neffect as well as bootstrap test to access the relevance of the idiosyncratic\ncomponents. We apply the proposed method to evaluate the effects of price\nchanges on the sales of a set of products based on a novel large panel of sale\ndata from a major retail chain in Brazil and demonstrate the benefits of using\nadditional idiosyncratic components in the treatment effect evaluations.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 15:07:48 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 12:27:40 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Fan", "Jianqing", ""], ["Masini", "Ricardo P.", ""], ["Medeiros", "Marcelo C.", ""]]}, {"id": "2011.04002", "submitter": "Patrick Schmidt", "authors": "Patrick W. Schmidt", "title": "Inference under Superspreading: Determinants of SARS-CoV-2 Transmission\n  in Germany", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Superspreading complicates the study of SARS-CoV-2 transmission. I propose a\nmodel for aggregated case data that accounts for superspreading and improves\nstatistical inference. In a Bayesian framework, the model is estimated on\nGerman data featuring over 60,000 cases with date of symptom onset and age\ngroup. Several factors were associated with a strong reduction in transmission:\npublic awareness rising, testing and tracing, information on local incidence,\nand high temperature. Immunity after infection, school and restaurant closures,\nstay-at-home orders, and mandatory face covering were associated with a smaller\nreduction in transmission. The data suggests that public distancing rules\nincreased transmission in young adults. Information on local incidence was\nassociated with a reduction in transmission of up to 44% (95%-CI: [40%, 48%]),\nwhich suggests a prominent role of behavioral adaptations to local risk of\ninfection. Testing and tracing reduced transmission by 15% (95%-CI: [9%,20%]),\nwhere the effect was strongest among the elderly. Extrapolating weather\neffects, I estimate that transmission increases by 53% (95%-CI: [43%, 64%]) in\ncolder seasons.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 15:30:03 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Schmidt", "Patrick W.", ""]]}, {"id": "2011.04117", "submitter": "Johannes Hendriks", "authors": "Johannes Hendriks, Adrian Wills, Brett Ninness and Johan Dahlin", "title": "Practical Bayesian System Identification using Hamiltonian Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses Bayesian system identification using a Markov Chain\nMonte Carlo approach. In particular, the Metroplis-Hastings algorithm with a\nHamiltonian proposal - known as Hamiltonian Monte Carlo - is reviewed and\nadapted to linear and nonlinear system identification problems. The paper\ndetails how the Hamiltonian proposal can be arranged so that the associated\nMarkov Chain performs well within the Metropolis-Hastings setting, which is a\nkey practical challenge faced when using the latter approach for many system\nidentification problems. This combination is demonstrated on several examples,\nranging from simple linear to more complex nonlinear systems, on both simulated\nand real data.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 00:15:45 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Hendriks", "Johannes", ""], ["Wills", "Adrian", ""], ["Ninness", "Brett", ""], ["Dahlin", "Johan", ""]]}, {"id": "2011.04135", "submitter": "Guanyu Hu", "authors": "Junxian Geng, Guanyu Hu", "title": "Mixture of Finite Mixtures Model for Basket Trial", "comments": "18 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the recent paradigm shift from cytotoxic drugs to new generation of\ntarget therapy and immuno-oncology therapy during oncology drug developments,\npatients with various cancer (sub)types may be eligible to participate in a\nbasket trial if they have the same molecular target. Bayesian hierarchical\nmodeling (BHM) are widely used in basket trial data analysis, where they\nadaptively borrow information among different cohorts (subtypes) rather than\nfully pool the data together or doing stratified analysis based on each cohort.\nThose approaches, however, may have the risk of over shrinkage estimation\nbecause of the invalidated exchangeable assumption. We propose a two-step\nprocedure to find the balance between pooled and stratified analysis. In the\nfirst step, we treat it as a clustering problem by grouping cohorts into\nclusters that share the similar treatment effect. In the second step, we use\nshrinkage estimator from BHM to estimate treatment effects for cohorts within\neach cluster under exchangeable assumption. For clustering part, we adapt the\nmixture of finite mixtures (MFM) approach to have consistent estimate of the\nnumber of clusters. We investigate the performance of our proposed method in\nsimulation studies and apply this method to Vemurafenib basket trial data\nanalysis.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 01:43:03 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Geng", "Junxian", ""], ["Hu", "Guanyu", ""]]}, {"id": "2011.04348", "submitter": "Stefano M. Iacus", "authors": "Marcello Carammia, Stefano Maria Iacus, Teddy Wilkin", "title": "Forecasting asylum-related migration flows with machine learning and\n  data at scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effects of the so-called \"refugee crisis\" of 2015-16 continue to dominate\nthe political agenda in Europe. Migration flows were sudden and unexpected,\nleaving governments unprepared and exposing significant shortcomings in the\nfield of migration forecasting. Migration is a complex system typified by\nepisodic variation, underpinned by causal factors that are interacting, highly\ncontext dependent and short-lived. Correspondingly, migration monitoring relies\non scattered data, while approaches to forecasting focus on specific migration\nflows and often have inconsistent results that are difficult to generalise at\nthe regional or global levels.\n  Here we show that adaptive machine learning algorithms that integrate\nofficial statistics and non-traditional data sources at scale can effectively\nforecast asylum-related migration flows. We focus on asylum applications lodged\nin countries of the European Union (EU) by nationals of all countries of origin\nworldwide; the same approach can be applied in any context provided adequate\nmigration or asylum data are available.\n  We exploit three tiers of data - geolocated events and internet searches in\ncountries of origin, detections of irregular crossings at the EU border, and\nasylum recognition rates in countries of destination - to effectively forecast\nindividual asylum-migration flows up to four weeks ahead with high accuracy.\nUniquely, our approach a) monitors potential drivers of migration in countries\nof origin to detect changes early onset; b) models individual\ncountry-to-country migration flows separately and on moving time windows; c)\nestimates the effects of individual drivers, including lagged effects; d)\nprovides forecasts of asylum applications up to four weeks ahead; e) assesses\nhow patterns of drivers shift over time to describe the functioning and change\nof migration systems.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 11:31:17 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 05:42:43 GMT"}, {"version": "v3", "created": "Fri, 20 Nov 2020 13:19:46 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Carammia", "Marcello", ""], ["Iacus", "Stefano Maria", ""], ["Wilkin", "Teddy", ""]]}, {"id": "2011.04356", "submitter": "Stefano M. Iacus", "authors": "Stefano Maria Iacus, Francesco Sermi, Spyridon Spyratos, Dario Tarchi,\n  Michele Vespe", "title": "Anomaly Detection of Mobility Data with Applications to COVID-19\n  Situational Awareness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces a live anomaly detection system for high frequency and\nhigh-dimensional data collected at regional scale such as Origin Destination\nMatrices of mobile positioning data. To take into account different granularity\nin time and space of the data coming from different sources, the system is\ndesigned to be simple, yet robust to the data diversity, with the aim of\ndetecting abrupt increase of mobility towards specific regions as well as\nsudden drops of movements. The methodology is designed to help policymakers or\npractitioners, and makes it possible to visualise anomalies as well as estimate\nthe effect of COVID-19 related containment or lifting measures in terms of\ntheir impact on human mobility as well as spot potential new outbreaks related\nto large gatherings.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 11:42:24 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 10:06:21 GMT"}, {"version": "v3", "created": "Wed, 25 Nov 2020 00:11:15 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Iacus", "Stefano Maria", ""], ["Sermi", "Francesco", ""], ["Spyratos", "Spyridon", ""], ["Tarchi", "Dario", ""], ["Vespe", "Michele", ""]]}, {"id": "2011.04384", "submitter": "Sina Mews", "authors": "Sina Mews and Marius \\\"Otting", "title": "Continuous-time state-space modelling of the hot hand in basketball", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the hot hand phenomenon using data on 110,513 free throws\ntaken in the National Basketball Association (NBA). As free throws occur at\nunevenly spaced time points within a game, we consider a state-space model\nformulated in continuous time to investigate serial dependence in players'\nsuccess probabilities. In particular, the underlying state process can be\ninterpreted as a player's (latent) varying form and is modelled using the\nOrnstein-Uhlenbeck process. Our results support the existence of the hot hand,\nbut the magnitude of the estimated effect is rather small.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 12:34:40 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Mews", "Sina", ""], ["\u00d6tting", "Marius", ""]]}, {"id": "2011.04391", "submitter": "Tadeu Ferreira", "authors": "Tadeu A. Ferreira", "title": "Reinforced Deep Markov Models With Applications in Automatic Trading", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.TR cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the developments in deep generative models, we propose a\nmodel-based RL approach, coined Reinforced Deep Markov Model (RDMM), designed\nto integrate desirable properties of a reinforcement learning algorithm acting\nas an automatic trading system. The network architecture allows for the\npossibility that market dynamics are partially visible and are potentially\nmodified by the agent's actions. The RDMM filters incomplete and noisy data, to\ncreate better-behaved input data for RL planning. The policy search\noptimisation also properly accounts for state uncertainty. Due to the\ncomplexity of the RKDF model architecture, we performed ablation studies to\nunderstand the contributions of individual components of the approach better.\nTo test the financial performance of the RDMM we implement policies using\nvariants of Q-Learning, DynaQ-ARIMA and DynaQ-LSTM algorithms. The experiments\nshow that the RDMM is data-efficient and provides financial gains compared to\nthe benchmarks in the optimal execution problem. The performance improvement\nbecomes more pronounced when price dynamics are more complex, and this has been\ndemonstrated using real data sets from the limit order book of Facebook, Intel,\nVodafone and Microsoft.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 12:46:30 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Ferreira", "Tadeu A.", ""]]}, {"id": "2011.04464", "submitter": "\\'Angel F. Garc\\'ia-Fern\\'andez", "authors": "\\'Angel F. Garc\\'ia-Fern\\'andez, Jason L. Williams, Lennart Svensson,\n  Yuxuan Xia", "title": "A Poisson multi-Bernoulli mixture filter for coexisting point and\n  extended targets", "comments": "Matlab files can be found at\n  https://github.com/Agarciafernandez/Coexisting-point-extended-target-PMBM-filter\n  and\n  https://github.com/yuhsuansia/Coexisting-point-extended-target-PMBM-filter. A\n  relevant multi-object tracking course can be found at\n  https://www.youtube.com/channel/UCa2-fpj6AV8T6JK1uTRuFpw", "journal-ref": "in IEEE Transactions on Signal Processing, vol. 69, pp. 2600-2610,\n  2021", "doi": "10.1109/TSP.2021.3072006", "report-no": null, "categories": "stat.ME cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a Poisson multi-Bernoulli mixture (PMBM) filter for\ncoexisting point and extended targets, i.e., for scenarios where there may be\nsimultaneous point and extended targets. The PMBM filter provides a recursion\nto compute the multi-target filtering posterior based on probabilistic\ninformation on data associations, and single-target predictions and updates. In\nthis paper, we first derive the PMBM filter update for a generalised\nmeasurement model, which can include measurements originated from point and\nextended targets. Second, we propose a single-target space that accommodates\nboth point and extended targets and derive the filtering recursion that\npropagates Gaussian densities for point targets and gamma Gaussian inverse\nWishart densities for extended targets. As a computationally efficient\napproximation of the PMBM filter, we also develop a Poisson multi-Bernoulli\n(PMB) filter for coexisting point and extended targets. The resulting filters\nare analysed via numerical simulations.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 14:41:40 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 06:28:22 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Garc\u00eda-Fern\u00e1ndez", "\u00c1ngel F.", ""], ["Williams", "Jason L.", ""], ["Svensson", "Lennart", ""], ["Xia", "Yuxuan", ""]]}, {"id": "2011.04466", "submitter": "Vinay Reddy Venumuddala", "authors": "Vinay Reddy Venumuddala", "title": "Occupational Network Structure and Vector Assortativity for illustrating\n  patterns of social mobility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study we arrive at a closed form expression for measuring vector\nassortativity in networks motivated by our use-case which is to observe\npatterns of social mobility in a society. Based on existing works on social\nmobility within economics literature, and social reproduction within sociology\nliterature, we motivate the construction of an occupational network structure\nto observe mobility patterns. Basing on existing literature, over this\nstructure, we define mobility as assortativity of occupations attributed by the\nrepresentation of categories such as gender, geography or social groups. We\ncompare the results from our vector assortativity measure and averaged scalar\nassortativity in the Indian context, relying on NSSO 68th round on employment\nand unemployment. Our findings indicate that the trends indicated by our vector\nassortativity measure is very similar to what is indicated by the averaged\nscalar assortativity index. We discuss some implications of this work and\nsuggest future directions.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 08:31:03 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Venumuddala", "Vinay Reddy", ""]]}, {"id": "2011.04577", "submitter": "Michael Pfarrhofer", "authors": "Niko Hauzenberger, Michael Pfarrhofer, Luca Rossini", "title": "Sparse time-varying parameter VECMs with an application to modeling\n  electricity prices", "comments": "JEL: C11, C32, C53, Q40; Keywords: Cointegration, reduced rank\n  regression, sparsification, hierarchical shrinkage priors, error correction\n  models", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we propose a time-varying parameter (TVP) vector error\ncorrection model (VECM) with heteroscedastic disturbances. We combine a set of\neconometric techniques for dynamic model specification in an automatic fashion.\nWe employ continuous global-local shrinkage priors for pushing the parameter\nspace towards sparsity. In a second step, we post-process the cointegration\nrelationships, the autoregressive coefficients and the covariance matrix via\nminimizing Lasso-type loss functions to obtain truly sparse estimates. This\ntwo-step approach alleviates overfitting concerns and reduces parameter\nestimation uncertainty, while providing estimates for the number of\ncointegrating relationships that varies over time. Our proposed econometric\nframework is applied to modeling European electricity prices and shows gains in\nforecast performance against a set of established benchmark models.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 17:22:31 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Hauzenberger", "Niko", ""], ["Pfarrhofer", "Michael", ""], ["Rossini", "Luca", ""]]}, {"id": "2011.04643", "submitter": "Jan Medina-L\\'opez", "authors": "Jan Medina-L\\'opez and Jorge Finke", "title": "Characterizing the head of the degree distributions of growing networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph math.PR stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The analysis in this paper helps to explain the formation of growing networks\nwith degree distributions that follow extended exponential or power-law tails.\nWe present a generic model in which edge dynamics are driven by a continuous\nattachment of new nodes and a mixed attachment mechanism that triggers random\nor preferential attachment. Furthermore, reciprocal edges to newly added nodes\nare established according to a response mechanism. The proposed framework\nextends previous mixed attachment models by allowing the number of new edges to\nvary according to various discrete probability distributions, including\nPoisson, Binomial, Zeta, and Log-Series. We derive analytical expressions for\nthe limit in-degree distribution that results from the mixed attachment and\nresponse mechanisms. Moreover, we describe the evolution of the dynamics of the\ncumulative in-degree distribution. Simulation results illustrate how the number\nof new edges and the process of reciprocity significantly impact the head of\nthe degree distribution.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 18:55:47 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 19:18:47 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Medina-L\u00f3pez", "Jan", ""], ["Finke", "Jorge", ""]]}, {"id": "2011.04739", "submitter": "Stephen Guth", "authors": "Stephen Guth and Themistoklis P. Sapsis", "title": "Probabilistic characterization of the effect of transient stochastic\n  loads on the fatigue-crack nucleation time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph math.AP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rainflow counting algorithm for material fatigue is both simple to\nimplement and extraordinarily successful for predicting material failure times.\nHowever, it neglects memory effects and time-ordering dependence, and therefore\nruns into difficulties dealing with highly intermittent or transient stochastic\nloads with heavy tailed distributions. Such loads appear frequently in a wide\nrange of applications in ocean and mechanical engineering, such as wind\nturbines and offshore structures. In this work we employ the Serebrinsky-Ortiz\ncohesive envelope model for material fatigue to characterize the effects of\nload intermittency on the fatigue-crack nucleation time. We first formulate\nefficient numerical integration schemes, which allow for the direct\ncharacterization of the fatigue life in terms of any given load time-series.\nSubsequently, we consider the case of stochastic intermittent loads with given\nstatistical characteristics. To overcome the need for expensive Monte-Carlo\nsimulations, we formulate the fatigue life as an up-crossing problem of the\ncoherent envelope. Assuming statistical independence for the large intermittent\nspikes and using probabilistic arguments we derive closed expressions for the\nup-crossing properties of the coherent envelope and obtain analytical\napproximations for the probability mass function of the failure time. The\nanalytical expressions are derived directly in terms of the probability density\nfunction of the load, as well as the coherent envelope. We examine the accuracy\nof the analytical approximations and compare the predicted failure time with\nthe standard rainflow algorithm for various loads. Finally, we use the\nanalytical expressions to examine the robustness of the derived probability\ndistribution for the failure time with respect to the coherent envelope\ngeometrical properties.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 03:08:57 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Guth", "Stephen", ""], ["Sapsis", "Themistoklis P.", ""]]}, {"id": "2011.04763", "submitter": "Philip Waggoner", "authors": "Philip D. Waggoner", "title": "Pandemic Policymaking: Learning the Lower Dimensional Manifold of\n  Congressional Responsiveness", "comments": "23 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A recent study leveraging text of pandemic-related policymaking from\n1973--2020 explored whether pandemic policymaking has evolved, or whether we\nare witnessing a new, unique era of policymaking as it relates to large-scale\ncrises like COVID-19. This research picks up on this approach over the same\nperiod of study and based on the same data, but excluding text. Instead, using\nhigh dimensional manifold learning, this study explores the latent structure of\nthe pandemic policymaking space based only on bill-level characteristics.\nResults indicate the COVID-19 era of policymaking maps extremely closely onto\nprior periods of related policymaking. This suggests that there is less of an\n\"evolutionary trend\" in pandemic policymaking, where instead there is striking\nuniformity in Congressional policymaking related to these types of large-scale\ncrises, despite being in a unique era of hyperpolarization, division, and\nineffective governance.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 21:06:59 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 13:44:34 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Waggoner", "Philip D.", ""]]}, {"id": "2011.05036", "submitter": "Sven Otto", "authors": "Florian Stark and Sven Otto", "title": "Testing and Dating Structural Changes in Copula-based Dependence\n  Measures", "comments": null, "journal-ref": null, "doi": "10.1080/02664763.2020.1850655", "report-no": null, "categories": "econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with testing and dating structural breaks in the\ndependence structure of multivariate time series. We consider a cumulative sum\n(CUSUM) type test for constant copula-based dependence measures, such as\nSpearman's rank correlation and quantile dependencies. The asymptotic null\ndistribution is not known in closed form and critical values are estimated by\nan i.i.d. bootstrap procedure. We analyze size and power properties in a\nsimulation study under different dependence measure settings, such as skewed\nand fat-tailed distributions. To date break points and to decide whether two\nestimated break locations belong to the same break event, we propose a pivot\nconfidence interval procedure. Finally, we apply the test to the historical\ndata of ten large financial firms during the last financial crisis from 2002 to\nmid-2013.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 10:57:31 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Stark", "Florian", ""], ["Otto", "Sven", ""]]}, {"id": "2011.05098", "submitter": "Ludwig Hothorn", "authors": "Ludwig A. Hothorn", "title": "Statistical evaluation of in-vivo bioassays in regulatory toxicology\n  considering males and females", "comments": "2 figures; 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The separate evaluation for males and females is the recent standard in\nin-vivo toxicology for dose or treatment effects using Dunnett tests. The\nalternative pre-test for sex-by-treatment interaction is problematic. Here a\njoint test is proposed considering the two sex-specific and the pooled\nDunnett-type comparisons. The calculation of either simultaneous confidence\nintervals or adjusted p-values with the R-package multcomp is demonstrated\nusing a real data example.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 13:49:26 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Hothorn", "Ludwig A.", ""]]}, {"id": "2011.05149", "submitter": "Eva van Weenen", "authors": "Eva van Weenen and Stefan Feuerriegel", "title": "Estimating Risk-Adjusted Hospital Performance", "comments": "Accepted at the 2020 IEEE International Conference on Big Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quality of healthcare provided by hospitals is subject to considerable\nvariability. Consequently, accurate measurements of hospital performance are\nessential for various decision-makers, including patients, hospital managers\nand health insurers. Hospital performance is assessed via the health outcomes\nof their patients. However, as the risk profiles of patients between hospitals\nvary, measuring hospital performance requires adjustment for patient risk. This\ntask is formalized in the state-of-the-art procedure through a hierarchical\ngeneralized linear model, that isolates hospital fixed-effects from the effect\nof patient risk on health outcomes. Due to the linear nature of this approach,\nany non-linear relations or interaction terms between risk variables are\nneglected.\n  In this work, we propose a novel method for measuring hospital performance\nadjusted for patient risk. This method captures non-linear relationships as\nwell as interactions among patient risk variables, specifically the effect of\nco-occurring health conditions on health outcomes. For this purpose, we develop\na tailored neural network architecture that is partially interpretable: a\nnon-linear part is used to encode risk factors, while a linear structure models\nhospital fixed-effects, such that the risk-adjusted hospital performance can be\nestimated. We base our evaluation on more than 13 million patient admissions\nacross almost 1,900 US hospitals as provided by the Nationwide Readmissions\nDatabase. Our model improves the ROC-AUC over the state-of-the-art by 4.1\npercent. These findings demonstrate that a large portion of the variance in\nhealth outcomes can be attributed to non-linear relationships between patient\nrisk variables and implicate that the current approach of measuring hospital\nperformance should be expanded.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 15:14:51 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 10:43:05 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["van Weenen", "Eva", ""], ["Feuerriegel", "Stefan", ""]]}, {"id": "2011.05548", "submitter": "Michele Guindani", "authors": "Xiao Li, Michele Guindani, Chaan S.Ng, Brian P.Hobbs", "title": "A Bayesian Nonparametric model for textural pattern heterogeneity", "comments": "45 pages, 7 figures, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Cancer radiomics is an emerging discipline promising to elucidate lesion\nphenotypes and tumor heterogeneity through patterns of enhancement, texture,\nmorphology, and shape. The prevailing technique for image texture analysis\nrelies on the construction and synthesis of Gray-Level Co-occurrence Matrices\n(GLCM). Practice currently reduces the structured count data of a GLCM to\nreductive and redundant summary statistics for which analysis requires variable\nselection and multiple comparisons for each application, thus limiting\nreproducibility. In this article, we develop a Bayesian multivariate\nprobabilistic framework for the analysis and unsupervised clustering of a\nsample of GLCM objects. By appropriately accounting for skewness and\nzero-inflation of the observed counts and simultaneously adjusting for existing\nspatial autocorrelation at nearby cells, the methodology facilitates estimation\nof texture pattern distributions within the GLCM lattice itself. The techniques\nare applied to cluster images of adrenal lesions obtained from CT scans with\nand without administration of contrast. We further assess whether the resultant\nsubtypes are clinically oriented by investigating their correspondence with\npathological diagnoses. Additionally, we compare performance to a class of\nmachine-learning approaches currently used in cancer radiomics with simulation\nstudies.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 05:14:31 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Li", "Xiao", ""], ["Guindani", "Michele", ""], ["Ng", "Chaan S.", ""], ["Hobbs", "Brian P.", ""]]}, {"id": "2011.05561", "submitter": "Colin Daly", "authors": "Colin Daly", "title": "An application of an Embedded Model Estimator to a synthetic\n  non-stationary reservoir model with multiple secondary variables", "comments": "arXiv admin note: text overlap with arXiv:2011.04116", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method (Ember) for non-stationary spatial modelling with multiple secondary\nvariables by combining Geostatistics with Random Forests is applied to a\nthree-dimensional Reservoir Model. It extends the Random Forest method to an\ninterpolation algorithm retaining similar consistency properties to both\nGeostatistical algorithms and Random Forests. It allows embedding of simpler\ninterpolation algorithms into the process, combining them through the Random\nForest training process. The algorithm estimates a conditional distribution at\neach target location. The family of such distributions is called the model\nenvelope. An algorithm to produce stochastic simulations from the envelope is\ndemonstrated. This algorithm allows the influence of the secondary variables as\nwell as the variability of the result to vary by location in the simulation.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 00:15:06 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Daly", "Colin", ""]]}, {"id": "2011.05601", "submitter": "Katherine Tsai", "authors": "Katherine Tsai, Mladen Kolar, Oluwasanmi Koyejo", "title": "A Nonconvex Framework for Structured Dynamic Covariance Recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a flexible yet interpretable model for high-dimensional data with\ntime-varying second order statistics, motivated and applied to functional\nneuroimaging data. Motivated by the neuroscience literature, we factorize the\ncovariances into sparse spatial and smooth temporal components. While this\nfactorization results in both parsimony and domain interpretability, the\nresulting estimation problem is nonconvex. To this end, we design a two-stage\noptimization scheme with a carefully tailored spectral initialization, combined\nwith iteratively refined alternating projected gradient descent. We prove a\nlinear convergence rate up to a nontrivial statistical error for the proposed\ndescent scheme and establish sample complexity guarantees for the estimator. We\nfurther quantify the statistical error for the multivariate Gaussian case.\nEmpirical results using simulated and real brain imaging data illustrate that\nour approach outperforms existing baselines.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 07:09:44 GMT"}, {"version": "v2", "created": "Sat, 19 Jun 2021 19:42:15 GMT"}, {"version": "v3", "created": "Sun, 18 Jul 2021 01:46:08 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Tsai", "Katherine", ""], ["Kolar", "Mladen", ""], ["Koyejo", "Oluwasanmi", ""]]}, {"id": "2011.05658", "submitter": "Gian Maria Campedelli", "authors": "Gian Maria Campedelli, Serena Favarin, Alberto Aziani, Alex R. Piquero", "title": "Disentangling Community-level Changes in Crime Trends During the\n  COVID-19 Pandemic in Chicago", "comments": "33 pages, 9 figures, published in Crime Science", "journal-ref": "Crime Sci 9, 21 (2020)", "doi": "10.1186/s40163-020-00131-8", "report-no": null, "categories": "econ.GN physics.soc-ph q-fin.EC stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent studies exploiting city-level time series have shown that, around the\nworld, several crimes declined after COVID-19 containment policies have been\nput in place. Using data at the community-level in Chicago, this work aims to\nadvance our understanding on how public interventions affected criminal\nactivities at a finer spatial scale. The analysis relies on a two-step\nmethodology. First, it estimates the community-wise causal impact of social\ndistancing and shelter-in-place policies adopted in Chicago via Structural\nBayesian Time-Series across four crime categories (i.e., burglary, assault,\nnarcotics-related offenses, and robbery). Once the models detected the\ndirection, magnitude and significance of the trend changes, Firth's Logistic\nRegression is used to investigate the factors associated to the statistically\nsignificant crime reduction found in the first step of the analyses.\nStatistical results first show that changes in crime trends differ across\ncommunities and crime types. This suggests that beyond the results of aggregate\nmodels lies a complex picture characterized by diverging patterns. Second,\nregression models provide mixed findings regarding the correlates associated\nwith significant crime reduction: several relations have opposite directions\nacross crimes with population being the only factor that is stably and\npositively associated with significant crime reduction.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 09:34:49 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Campedelli", "Gian Maria", ""], ["Favarin", "Serena", ""], ["Aziani", "Alberto", ""], ["Piquero", "Alex R.", ""]]}, {"id": "2011.05721", "submitter": "Brijesh Singh", "authors": "Brijesh P. Singh, Sandeep Singh and Utpal Dhar Das", "title": "A General Class of New Continuous Mixture Distribution and Application", "comments": "14 pages, 14 figures and 2 tables", "journal-ref": "Journal of Mathematical and Computational Science (2021), Vol. 11,\n  No. 1, 585-602", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A generalization of a distribution increases the flexibility particularly in\nstudying of a phenomenon and its properties. Many generalizations of continuous\nunivariate distributions are available in literature. In this study, an\ninvestigation is conducted on a distribution and its generalization. Several\navailable generalizations of the distribution are reviewed and recent trends in\nthe construction of generalized classes with a generalized mixing parameter are\ndiscussed. To check the suitability and comparability, real data set have been\nused.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 11:53:32 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Singh", "Brijesh P.", ""], ["Singh", "Sandeep", ""], ["Das", "Utpal Dhar", ""]]}, {"id": "2011.05900", "submitter": "Marta Avalos", "authors": "Marta Avalos-Fernandez and Helene Touchais and Marcela\n  Henriquez-Henriquez", "title": "A decision-making tool to fine-tune abnormal levels in the complete\n  blood count tests", "comments": "Machine Learning for Health (ML4H) at NeurIPS 2020 - Extended\n  Abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The complete blood count (CBC) performed by automated hematology analyzers is\none of the most ordered laboratory tests. It is a first-line tool for assessing\na patient's general health status, or diagnosing and monitoring disease\nprogression. When the analysis does not fit an expected setting, technologists\nmanually review a blood smear using a microscope. The International Consensus\nGroup for Hematology Review published in 2005 a set of criteria for reviewing\nCBCs. Commonly, adjustments are locally needed to account for laboratory\nresources and populations characteristics. Our objective is to provide a\ndecision support tool to identify which CBC variables are associated with\nhigher risks of abnormal smear and at which cutoff values. We propose a\ncost-sensitive Lasso-penalized additive logistic regression combined with\nstability selection. Using simulated and real CBC data, we demonstrate that our\ntool correctly identify the true cutoff values, provided that there is enough\navailable data in their neighbourhood.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 16:47:03 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 16:11:54 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Avalos-Fernandez", "Marta", ""], ["Touchais", "Helene", ""], ["Henriquez-Henriquez", "Marcela", ""]]}, {"id": "2011.05951", "submitter": "Gen Li", "authors": "Gen Li, Yan Li, Kun Chen", "title": "It's All Relative: New Regression Paradigm for Microbiome Compositional\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microbiome data are complex in nature, involving high dimensionality,\ncompositionally, zero inflation, and taxonomic hierarchy. Compositional data\nreside in a simplex that does not admit the standard Euclidean geometry. Most\nexisting compositional regression methods rely on transformations that are\ninadequate or even inappropriate in modeling data with excessive zeros and\ntaxonomic structure. We develop a novel relative-shift regression framework\nthat directly uses compositions as predictors. The new framework provides a\nparadigm shift for compositional regression and offers a superior biological\ninterpretation. New equi-sparsity and taxonomy-guided regularization methods\nand an efficient smoothing proximal gradient algorithm are developed to\nfacilitate feature aggregation and dimension reduction in regression. As a\nresult, the framework can automatically identify clinically relevant microbes\neven if they are important at different taxonomic levels. A unified\nfinite-sample prediction error bound is developed for the proposed regularized\nestimators. We demonstrate the efficacy of the proposed methods in extensive\nsimulation studies. The application to a preterm infant study reveals novel\ninsights of association between the gut microbiome and neurodevelopment.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 18:15:21 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Li", "Gen", ""], ["Li", "Yan", ""], ["Chen", "Kun", ""]]}, {"id": "2011.06019", "submitter": "Wilpen Gorr", "authors": "Dylan J. Fitzpatrick (1), Wilpen L. Gorr (2), Daniel B. Neill (3) ((1)\n  University of Chicago, (2) Carnegie Mellon University, (3) New York\n  University)", "title": "Policing Chronic and Temporary Hot Spots of Violent Crime: A Controlled\n  Field Experiment", "comments": "15 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hot-spot-based policing programs aim to deter crime through increased\nproactive patrols at high-crime locations. While most hot spot programs target\neasily identified chronic hot spots, we introduce models for predicting\ntemporary hot spots to address effectiveness and equity objectives for crime\nprevention, and present findings from a crossover experiment evaluating\napplication of hot spot predictions to prevent serious violent crime in\nPittsburgh, PA. Over a 12-month experimental period, the Pittsburgh Bureau of\nPolice assigned uniformed patrol officers to weekly predicted chronic and\ntemporary hot spots of serious violent crimes comprising 0.5 percent of the\ncity's area. We find statistically and practically significant reductions in\nserious violent crime counts within treatment hot spots as compared to control\nhot spots, with an overall reduction of 25.3 percent in the FBI-classified Part\n1 Violent (P1V) crimes of homicide, rape, robbery, and aggravated assault, and\na 39.7 percent reduction of African-American and other non-white victims of P1V\ncrimes. We find that temporary hot spots increase spatial dispersion of patrols\nand have a greater percentage reduction in P1V crimes than chronic hot spots\nbut fewer total number of crimes prevented. Only foot patrols, not car patrols,\nhad statistically significant crime reductions in hot spots. We find no\nevidence of crime displacement; instead, we find weakly statistically\nsignificant spillover of crime prevention benefits to adjacent areas. In\naddition, we find no evidence that the community-oriented hot spot patrols\nproduced over-policing arrests of minority or other populations.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 19:12:06 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Fitzpatrick", "Dylan J.", ""], ["Gorr", "Wilpen L.", ""], ["Neill", "Daniel B.", ""]]}, {"id": "2011.06031", "submitter": "Jiachen Chen", "authors": "Jiachen Chen, Xin Zhou, Fan Li, Donna Spiegelman", "title": "swdpwr: A SAS Macro and An R Package for Power Calculation in Stepped\n  Wedge Cluster Randomized Trials", "comments": "29 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Background and objective: The stepped wedge cluster randomized trial is a\nstudy design increasingly used for public health intervention evaluations. Most\nprevious literature focuses on power calculations for this particular type of\ncluster randomized trials for continuous outcomes, along with an approximation\nto this approach for binary outcomes. Although not accurate for binary\noutcomes, it has been widely used. To improve the approximation for binary\noutcomes, two new methods for stepped wedge designs (SWDs) of binary outcomes\nhave recently been published. However, these new methods have not been\nimplemented in publicly available software. The objective of this paper is to\npresent power calculation software for SWDs in various settings for both\ncontinuous and binary outcomes.\n  Methods: We have developed a SAS macro %swdpwr and an R package swdpwr for\npower calculation in SWDs. Different scenarios including cross-sectional and\ncohort designs, binary and continuous outcomes, marginal and conditional\nmodels, three link functions, with and without time effects are accommodated in\nthis software.\n  Results: swdpwr provides an efficient tool to support investigators in the\ndesign and analysis of stepped wedge cluster randomized trails. swdpwr\naddresses the implementation gap between newly proposed methodology and their\napplication to obtain more accurate power calculations in SWDs.\n  Conclusions: This user-friendly software makes the new methods more\naccessible and incorporates as many variations as currently available, which\nwere not supported in other related packages. swdpwr is implemented under two\nplatforms: SAS and R, satisfying the needs of investigators from various\nbackgrounds.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 19:31:40 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 05:26:48 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Chen", "Jiachen", ""], ["Zhou", "Xin", ""], ["Li", "Fan", ""], ["Spiegelman", "Donna", ""]]}, {"id": "2011.06045", "submitter": "Konstantinos Perrakis", "authors": "Konstantinos Perrakis, Dimitris Karlis, Mario Cools, Davy Janssens", "title": "Bayesian inference for transportation origin-destination matrices: the\n  Poisson-inverse Gaussian and other Poisson mixtures", "comments": null, "journal-ref": null, "doi": "10.1111/rssa.12057", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we present Poisson mixture approaches for origin-destination\n(OD) modeling in transportation analysis. We introduce covariate-based models\nwhich incorporate different transport modeling phases and also allow for direct\nprobabilistic inference on link traffic based on Bayesian predictions. Emphasis\nis placed on the Poisson-inverse Gaussian as an alternative to the\ncommonly-used Poisson-gamma and Poisson-lognormal models. We present a first\nfull Bayesian formulation and demonstrate that the Poisson-inverse Gaussian is\nparticularly suited for OD analysis due to desirable marginal and hierarchical\nproperties. In addition, the integrated nested Laplace approximation (INLA) is\nconsidered as an alternative to Markov chain Monte Carlo and the two\nmethodologies are compared under specific modeling assumptions. The case study\nis based on 2001 Belgian census data and focuses on a large,\nsparsely-distributed OD matrix containing trip information for 308 Flemish\nmunicipalities.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 19:57:00 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Perrakis", "Konstantinos", ""], ["Karlis", "Dimitris", ""], ["Cools", "Mario", ""], ["Janssens", "Davy", ""]]}, {"id": "2011.06049", "submitter": "Jeanne N. Clelland", "authors": "Jeanne Clelland, Haley Colgate, Daryl DeFord, Beth Malmskog, Flavia\n  Sancier-Barbosa", "title": "Colorado in Context: Congressional Redistricting and Competing Fairness\n  Criteria in Colorado", "comments": "39 pages, 21 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY physics.soc-ph stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we apply techniques of ensemble analysis to understand the\npolitical baseline for Congressional representation in Colorado. We generate a\nlarge random sample of reasonable redistricting plans and determine the\npartisan balance of each district using returns from state-wide elections in\n2018, and analyze the 2011/2012 enacted districts in this context. Colorado\nrecently adopted a new framework for redistricting, creating an independent\ncommission to draw district boundaries, prohibiting partisan bias and\nincumbency considerations, requiring that political boundaries (such as\ncounties) be preserved as much as possible, and also requiring that mapmakers\nmaximize the number of competitive districts. We investigate the relationships\nbetween partisan outcomes, number of counties which are split, and number of\ncompetitive districts in a plan. This paper also features two novel\nimprovements in methodology--a more rigorous statistical framework for\nunderstanding necessary sample size, and a weighted-graph method for generating\nrandom plans which split approximately as few counties as acceptable\nhuman-drawn maps.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 20:05:50 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 22:13:57 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Clelland", "Jeanne", ""], ["Colgate", "Haley", ""], ["DeFord", "Daryl", ""], ["Malmskog", "Beth", ""], ["Sancier-Barbosa", "Flavia", ""]]}, {"id": "2011.06060", "submitter": "Kritika Phulli", "authors": "Deepanshu Sharma and Kritika Phulli", "title": "Forecasting and Analyzing the Military Expenditure of India Using\n  Box-Jenkins ARIMA Model", "comments": "12 pages,5 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The advancement in the field of statistical methodologies to economic data\nhas paved its path towards the dire need for designing efficient military\nmanagement policies. India is ranked as the third largest country in terms of\nmilitary spender for the year 2019. Therefore, this study aims at utilizing the\nBox-Jenkins ARIMA model for time series forecasting of the military expenditure\nof India in forthcoming times. The model was generated on the SIPRI dataset of\nIndian military expenditure of 60 years from the year 1960 to 2019. The trend\nwas analysed for the generation of the model that best fitted the forecasting.\nThe study highlights the minimum AIC value and involves ADF testing (Augmented\nDickey-Fuller) to transform expenditure data into stationary form for model\ngeneration. It also focused on plotting the residual error distribution for\nefficient forecasting. This research proposed an ARIMA (0,1,6) model for\noptimal forecasting of military expenditure of India with an accuracy of 95.7%.\nThe model, thus, acts as a Moving Average (MA) model and predicts the\nsteady-state exponential growth of 36.94% in military expenditure of India by\n2024.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 11:41:11 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Sharma", "Deepanshu", ""], ["Phulli", "Kritika", ""]]}, {"id": "2011.06100", "submitter": "Tony Sun", "authors": "Tony Y. Sun, Oliver J. Bear Don't Walk IV, Jennifer L. Chen, Harry\n  Reyes Nieva, No\\'emie Elhadad", "title": "Exploring Gender Disparities in Time to Diagnosis", "comments": "Machine Learning for Health (ML4H) at NeurIPS 2020 - Extended\n  Abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sex and gender-based healthcare disparities contribute to differences in\nhealth outcomes. We focus on time to diagnosis (TTD) by conducting two\nlarge-scale, complementary analyses among men and women across 29 phenotypes\nand 195K patients. We first find that women are consistently more likely to\nexperience a longer TTD than men, even when presenting with the same\nconditions. We further explore how TTD disparities affect diagnostic\nperformance between genders, both across and persistent to time, by evaluating\ngender-agnostic disease classifiers across increasing diagnostic information.\nIn both fairness analyses, the diagnostic process favors men over women,\ncontradicting the previous observation that women may demonstrate relevant\nsymptoms earlier than men. These analyses suggest that TTD is an important yet\ncomplex aspect when studying gender disparities, and warrants further\ninvestigation.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 22:27:14 GMT"}, {"version": "v2", "created": "Sun, 15 Nov 2020 02:05:21 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Sun", "Tony Y.", ""], ["Walk", "Oliver J. Bear Don't", "IV"], ["Chen", "Jennifer L.", ""], ["Nieva", "Harry Reyes", ""], ["Elhadad", "No\u00e9mie", ""]]}, {"id": "2011.06172", "submitter": "Han Du", "authors": "Han Du and Ge Jiang and Zijun Ke", "title": "A Bootstrap Based Between-Study Heterogeneity Test in Meta-Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-analysis combines pertinent information from existing studies to provide\nan overall estimate of population parameters/effect sizes, as well as to\nquantify and explain the differences between studies. However, testing the\nbetween-study heterogeneity is one of the most troublesome topics in\nmeta-analysis research. Additionally, no methods have been proposed to test\nwhether the size of the heterogeneity is larger than a specific level. The\nexisting methods, such as the Q test and likelihood ratio (LR) tests, are\ncriticized for their failure to control the Type I error rate and/or failure to\nattain enough statistical power. Although better reference distribution\napproximations have been proposed in the literature, the expression is\ncomplicated and the application is limited. In this article, we propose\nbootstrap based heterogeneity tests combining the restricted maximum likelihood\n(REML) ratio test or Q test with bootstrap procedures, denoted as B-REML-LRT\nand B-Q respectively. Simulation studies were conducted to examine and compare\nthe performance of the proposed methods with the regular LR tests, the regular\nQ test, and the improved Q test in both the random-effects meta-analysis and\nmixed-effects meta-analysis. Based on the results of Type I error rates and\nstatistical power, B-Q is recommended. An R package \\mathtt{boot.heterogeneity}\nis provided to facilitate the implementation of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 02:44:00 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Du", "Han", ""], ["Jiang", "Ge", ""], ["Ke", "Zijun", ""]]}, {"id": "2011.06334", "submitter": "Antonio Remiro-Az\\'ocar Mr.", "authors": "Antonio Remiro-Az\\'ocar, Anna Heath, Gianluca Baio", "title": "Conflating marginal and conditional treatment effects: Comments on\n  'Assessing the performance of population adjustment methods for anchored\n  indirect comparisons: A simulation study'", "comments": "6 pages, submitted to Statistics in Medicine. Response to `Assessing\n  the performance of population adjustment methods for anchored indirect\n  comparisons: A simulation study' by Phillippo, Dias, Ades and Welton,\n  published in Statistics in Medicine (2020). Updated after Ph.D. proposal\n  defense/transfer viva comments", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this commentary, we highlight the importance of: (1) carefully considering\nand clarifying whether a marginal or conditional treatment effect is of\ninterest in a population-adjusted indirect treatment comparison; and (2)\ndeveloping distinct methodologies for estimating the different measures of\neffect. The appropriateness of each methodology depends on the preferred target\nof inference.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 12:08:28 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 17:44:54 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Remiro-Az\u00f3car", "Antonio", ""], ["Heath", "Anna", ""], ["Baio", "Gianluca", ""]]}, {"id": "2011.06422", "submitter": "Philip Waggoner", "authors": "Philip D. Waggoner, Alec Macmillen", "title": "Pursuing Open-Source Development of Predictive Algorithms: The Case of\n  Criminal Sentencing Algorithms", "comments": "26 pages, 3 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Currently, there is uncertainty surrounding the merits of open-source versus\nproprietary algorithm development. Though justification in favor of each\nexists, we argue that open-source algorithm development should be the standard\nin highly consequential contexts that affect people's lives for reasons of\ntransparency and collaboration, which contribute to greater predictive accuracy\nand enjoy the additional advantage of cost-effectiveness. To make this case, we\nfocus on criminal sentencing algorithms, as criminal sentencing is highly\nconsequential, and impacts society and individual people. Further, the\npopularity of this topic has surged in the wake of recent studies uncovering\nracial bias in proprietary sentencing algorithms among other issues of\nover-fitting and model complexity. We suggest these issues are exacerbated by\nthe proprietary and expensive nature of virtually all widely used criminal\nsentencing algorithms. Upon replicating a major algorithm using real criminal\nprofiles, we fit three penalized regressions and demonstrate an increase in\npredictive power of these open-source and relatively computationally\ninexpensive options. The result is a data-driven suggestion that if judges who\nare making sentencing decisions want to craft appropriate sentences based on a\nhigh degree of accuracy and at low costs, then they should be pursuing\nopen-source options.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 14:53:43 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Waggoner", "Philip D.", ""], ["Macmillen", "Alec", ""]]}, {"id": "2011.06501", "submitter": "Fabien Panloup", "authors": "Piotr Sobczyk, Stanislaw Wilczynski, Malgorzata Bogdan, Piotr Graczyk,\n  Julie Josse, Fabien Panloup, Val\\'erie Seegers, Mateusz Staniak", "title": "VARCLUST: clustering variables using dimensionality reduction", "comments": "24 pages, 34 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  VARCLUST algorithm is proposed for clustering variables under the assumption\nthat variables in a given cluster are linear combinations of a small number of\nhidden latent variables, corrupted by the random noise. The entire clustering\ntask is viewed as the problem of selection of the statistical model, which is\ndefined by the number of clusters, the partition of variables into these\nclusters and the 'cluster dimensions', i.e. the vector of dimensions of linear\nsubspaces spanning each of the clusters. The optimal model is selected using\nthe approximate Bayesian criterion based on the Laplace approximations and\nusing a non-informative uniform prior on the number of clusters. To solve the\nproblem of the search over a huge space of possible models we propose an\nextension of the ClustOfVar algorithm which was dedicated to subspaces of\ndimension only 1, and which is similar in structure to the $K$-centroid\nalgorithm. We provide a complete methodology with theoretical guarantees,\nextensive numerical experimentations, complete data analyses and\nimplementation. Our algorithm assigns variables to appropriate clusterse based\non the consistent Bayesian Information Criterion (BIC), and estimates the\ndimensionality of each cluster by the PEnalized SEmi-integrated Likelihood\nCriterion (PESEL), whose consistency we prove. Additionally, we prove that each\niteration of our algorithm leads to an increase of the Laplace approximation to\nthe model posterior probability and provide the criterion for the estimation of\nthe number of clusters. Numerical comparisons with other algorithms show that\nVARCLUST may outperform some popular machine learning tools for sparse subspace\nclustering. We also report the results of real data analysis including TCGA\nbreast cancer data and meteorological data. The proposed method is implemented\nin the publicly available R package varclust.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 17:11:50 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 15:00:07 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Sobczyk", "Piotr", ""], ["Wilczynski", "Stanislaw", ""], ["Bogdan", "Malgorzata", ""], ["Graczyk", "Piotr", ""], ["Josse", "Julie", ""], ["Panloup", "Fabien", ""], ["Seegers", "Val\u00e9rie", ""], ["Staniak", "Mateusz", ""]]}, {"id": "2011.06593", "submitter": "Tiffany Tang", "authors": "Xiao Li, Tiffany M. Tang, Xuewei Wang, Jean-Pierre A. Kocher, Bin Yu", "title": "A stability-driven protocol for drug response interpretable prediction\n  (staDRIP)", "comments": "Machine Learning for Health (ML4H) at NeurIPS 2020 - Extended\n  Abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern cancer -omics and pharmacological data hold great promise in precision\ncancer medicine for developing individualized patient treatments. However, high\nheterogeneity and noise in such data pose challenges for predicting the\nresponse of cancer cell lines to therapeutic drugs accurately. As a result,\narbitrary human judgment calls are rampant throughout the predictive modeling\npipeline. In this work, we develop a transparent stability-driven pipeline for\ndrug response interpretable predictions, or staDRIP, which builds upon the PCS\nframework for veridical data science (Yu and Kumbier, 2020) and mitigates the\nimpact of human judgment calls. Here we use the PCS framework for the first\ntime in cancer research to extract proteins and genes that are important in\npredicting the drug responses and stable across appropriate data and model\nperturbations. Out of the 24 most stable proteins we identified using data from\nthe Cancer Cell Line Encyclopedia (CCLE), 18 have been associated with the drug\nresponse or identified as a known or possible drug target in previous\nliterature, demonstrating the utility of our stability-driven pipeline for\nknowledge discovery in cancer drug response prediction modeling.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 06:46:51 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 16:17:54 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Li", "Xiao", ""], ["Tang", "Tiffany M.", ""], ["Wang", "Xuewei", ""], ["Kocher", "Jean-Pierre A.", ""], ["Yu", "Bin", ""]]}, {"id": "2011.06689", "submitter": "Ezekiel Adebayo Ogundepo", "authors": "Ezekiel Ogundepo, Sakinat Folorunso, Olubayo Adekanmbi, Olalekan\n  Akinsande, Oluwatobi Banjo, Emeka Ogbuju, Francisca Oladipo, Olawale\n  Abimbola, Ehizokhale Oseghale, and Oluwatobi Babajide", "title": "An exploratory assessment of a multidimensional healthcare and economic\n  data on COVID-19 in Nigeria", "comments": null, "journal-ref": "Volume 33, December 2020, 106424", "doi": "10.1016/j.dib.2020.106424", "report-no": null, "categories": "stat.AP stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The coronavirus disease of 2019 (COVID-19) is a pandemic that is ravaging\nNigeria and the world at large. This data article provides a dataset of daily\nupdates of COVID-19 as reported online by the Nigeria Centre for Disease\nControl (NCDC) from February 27, 2020 to September 29, 2020. The data were\nobtained through web scraping from different sources and it includes some\neconomic variables such as the Nigeria budget for each state in 2020,\npopulation estimate, healthcare facilities, and the COVID-19 laboratories in\nNigeria. The dataset has been processed using the standard of the FAIR data\nprinciple which encourages its findability, accessibility, interoperability,\nand reusability and will be relevant to researchers in different fields such as\nData Science, Epidemiology, Earth Modelling, and Health Informatics.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 23:50:14 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Ogundepo", "Ezekiel", ""], ["Folorunso", "Sakinat", ""], ["Adekanmbi", "Olubayo", ""], ["Akinsande", "Olalekan", ""], ["Banjo", "Oluwatobi", ""], ["Ogbuju", "Emeka", ""], ["Oladipo", "Francisca", ""], ["Abimbola", "Olawale", ""], ["Oseghale", "Ehizokhale", ""], ["Babajide", "Oluwatobi", ""]]}, {"id": "2011.06795", "submitter": "Alberto Baccini", "authors": "Federica Baccini, Lucio Barabesi, Alberto Baccini, Mahdi Khelfaoui,\n  Yves Gingras", "title": "Similarity network fusion for scholarly journals", "comments": "20 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores intellectual and social proximity among scholarly\njournals by using network fusion techniques. Similarities among journals are\ninitially represented by means of a three-layer network based on co-citations,\ncommon authors and common editors. The information contained in the three\nlayers is then combined by building a fused similarity network. The fusion\nconsists in an unsupervised process that exploits the structural properties of\nthe layers. Subsequently, partial distance correlations are adopted for\nmeasuring the contribution of each layer to the structure of the fused network.\nFinally, the community morphology of the fused network is explored by using\nmodularity. In the three fields considered (i.e. economics, information and\nlibrary sciences and statistics) the major contribution to the structure of the\nfused network arises from editors. This result suggests that the role of\neditors as gatekeepers of journals is the most relevant in defining the\nboundaries of scholarly communities. In information and library sciences and\nstatistics, the clusters of journals reflect sub-field specializations. In\neconomics, clusters of journals appear to be better interpreted in terms of\nalternative methodological approaches. Thus, the graphs representing the\nclusters of journals in the fused network are powerful instruments for\nexploring research fields.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 07:41:12 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 13:57:24 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Baccini", "Federica", ""], ["Barabesi", "Lucio", ""], ["Baccini", "Alberto", ""], ["Khelfaoui", "Mahdi", ""], ["Gingras", "Yves", ""]]}, {"id": "2011.06909", "submitter": "Yasuhiro Omori", "authors": "Yuta Yamauchi and Yasuhiro Omori", "title": "Dynamic factor, leverage and realized covariances in multivariate\n  stochastic volatility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the stochastic volatility models for multivariate daily stock returns, it\nhas been found that the estimates of parameters become unstable as the\ndimension of returns increases. To solve this problem, we focus on the factor\nstructure of multiple returns and consider two additional sources of\ninformation: first, the realized stock index associated with the market factor,\nand second, the realized covariance matrix calculated from high frequency data.\nThe proposed dynamic factor model with the leverage effect and realized\nmeasures is applied to ten of the top stocks composing the exchange traded fund\nlinked with the investment return of the SP500 index and the model is shown to\nhave a stable advantage in portfolio performance.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 13:48:24 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Yamauchi", "Yuta", ""], ["Omori", "Yasuhiro", ""]]}, {"id": "2011.06916", "submitter": "Amanda Fern\\'andez-Fontelo Dr.", "authors": "Amanda Fern\\'andez-Fontelo, Pascal J. Kieslich, Felix Henninger,\n  Frauke Kreuter and Sonja Greven", "title": "Predicting respondent difficulty in web surveys: A machine-learning\n  approach based on mouse movement features", "comments": "40 pages, 2 Figures, 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central goal of survey research is to collect robust and reliable data from\nrespondents. However, despite researchers' best efforts in designing\nquestionnaires, respondents may experience difficulty understanding questions'\nintent and therefore may struggle to respond appropriately. If it were possible\nto detect such difficulty, this knowledge could be used to inform real-time\ninterventions through responsive questionnaire design, or to indicate and\ncorrect measurement error after the fact. Previous research in the context of\nweb surveys has used paradata, specifically response times, to detect\ndifficulties and to help improve user experience and data quality. However,\nricher data sources are now available, in the form of the movements respondents\nmake with the mouse, as an additional and far more detailed indicator for the\nrespondent-survey interaction. This paper uses machine learning techniques to\nexplore the predictive value of mouse-tracking data with regard to respondents'\ndifficulty. We use data from a survey on respondents' employment history and\ndemographic information, in which we experimentally manipulate the difficulty\nof several questions. Using features derived from the cursor movements, we\npredict whether respondents answered the easy or difficult version of a\nquestion, using and comparing several state-of-the-art supervised learning\nmethods. In addition, we develop a personalization method that adjusts for\nrespondents' baseline mouse behavior and evaluate its performance. For all\nthree manipulated survey questions, we find that including the full set of\nmouse movement features improved prediction performance over response-time-only\nmodels in nested cross-validation. Accounting for individual differences in\nmouse movements led to further improvements.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 10:54:33 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Fern\u00e1ndez-Fontelo", "Amanda", ""], ["Kieslich", "Pascal J.", ""], ["Henninger", "Felix", ""], ["Kreuter", "Frauke", ""], ["Greven", "Sonja", ""]]}, {"id": "2011.06917", "submitter": "Bo Zhang", "authors": "Bo Zhang, Siyu Heng, Ting Ye, Dylan S. Small", "title": "Social Distancing and COVID-19: Randomization Inference for a Structured\n  Dose-Response Relationship", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Social distancing is widely acknowledged as an effective public health policy\ncombating the novel coronavirus. But extreme social distancing has costs and it\nis not clear how much social distancing is needed to achieve public health\neffects. In this article, we develop a design-based framework to make inference\nabout the dose-response relationship between social distancing and COVID-19\nrelated death toll and case numbers. We first discuss how to embed\nobservational data with a time-independent, continuous treatment dose into an\napproximate randomized experiment, and develop a randomization-based procedure\nthat tests if a structured dose-response relationship fits the data. We then\ngeneralize the design and testing procedure to accommodate a time-dependent,\ntreatment dose trajectory, and generalize a dose-response relationship to a\nlongitudinal setting. Finally, we apply the proposed design and testing\nprocedures to investigate the effect of social distancing during the phased\nreopening in the United States on public health outcomes using data compiled\nfrom sources including Unacast, the United States Census Bureau, and the County\nHealth Rankings and Roadmaps Program. We test a primary analysis hypothesis\nthat states the social distancing from April 27th to June 28th had no effect on\nthe COVID-19-related death toll from June 29th to August 2nd (p-value < 0.001)\nand conducted extensive secondary analyses that investigate the dose-response\nrelationship between social distancing and COVID-19 case numbers.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 18:59:10 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 00:11:03 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Zhang", "Bo", ""], ["Heng", "Siyu", ""], ["Ye", "Ting", ""], ["Small", "Dylan S.", ""]]}, {"id": "2011.07030", "submitter": "Lucy D'Agostino McGowan", "authors": "Lucy D'Agostino McGowan and Robert A. Greevy, Jr", "title": "Contextualizing E-values for Interpretable Sensitivity to Unmeasured\n  Confounding Analyses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The strength of evidence provided by epidemiological and observational\nstudies is inherently limited by the potential for unmeasured confounding.\nResearchers should present a quantified sensitivity to unmeasured confounding\nanalysis that is contextualized by the study's observed covariates. VanderWeele\nand Ding's E-value provides an easily calculated metric for the magnitude of\nthe hypothetical unmeasured confounding required to render the study's result\ninconclusive. We propose the Observed Covariate E-value to contextualize the\nsensitivity analysis' hypothetical E-value within the actual impact of observed\ncovariates, individually or within groups. We introduce a sensitivity analysis\nfigure that presents the Observed Covariate E-values, on the E-value scale,\nnext to their corresponding observed bias effects, on the original scale of the\nstudy results. This observed bias plot allows easy comparison of the\nhypothetical E-values, Observed Covariate E-values, and observed bias effects.\nWe illustrate the methods with a specific example and provide a supplemental\nappendix with modifiable code that teaches how to implement the method and\ncreate a publication quality figure.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 17:37:34 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["McGowan", "Lucy D'Agostino", ""], ["Greevy,", "Robert A.", "Jr"]]}, {"id": "2011.07175", "submitter": "Yifei Sun", "authors": "Yifei Sun, Sy Han Chiou, Colin O. Wu, Meghan McGarry, Chiung-Yu Huang", "title": "Dynamic Risk Prediction Using Survival Tree Ensembles with Application\n  to Cystic Fibrosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the availability of massive amounts of data from electronic health\nrecords and registry databases, incorporating time-varying patient information\nto improve risk prediction has attracted great attention. To exploit the\ngrowing amount of predictor information over time, we develop a unified\nframework for landmark prediction using survival tree ensembles, where an\nupdated prediction can be performed when new information becomes available.\nCompared to the conventional landmark prediction, our framework enjoys great\nflexibility in that the landmark times can be subject-specific and triggered by\nan intermediate clinical event. Moreover, the nonparametric approach\ncircumvents the thorny issue in model incompatibility at different landmark\ntimes. When both the longitudinal predictors and the outcome event time are\nsubject to right censoring, existing tree-based approaches cannot be directly\napplied. To tackle the analytical challenges, we consider a risk-set-based\nensemble procedure by averaging martingale estimating equations from individual\ntrees. Extensive simulation studies are conducted to evaluate the performance\nof our methods. The methods are applied to the Cystic Fibrosis Patient Registry\n(CFFPR) data to perform dynamic prediction of lung disease in cystic fibrosis\npatients and to identify important prognosis factors.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 00:31:07 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Sun", "Yifei", ""], ["Chiou", "Sy Han", ""], ["Wu", "Colin O.", ""], ["McGarry", "Meghan", ""], ["Huang", "Chiung-Yu", ""]]}, {"id": "2011.07194", "submitter": "Amanda Coston", "authors": "Amanda Coston, Neel Guha, Derek Ouyang, Lisa Lu, Alexandra\n  Chouldechova, and Daniel E. Ho", "title": "Leveraging Administrative Data for Bias Audits: Assessing Disparate\n  Coverage with Mobility Data for COVID-19 Policy", "comments": null, "journal-ref": "Proceedings of the 2021 ACM Conference on Fairness,\n  Accountability, and Transparency. pp. 173-184", "doi": "10.1145/3442188.3445881", "report-no": null, "categories": "stat.AP cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anonymized smartphone-based mobility data has been widely adopted in devising\nand evaluating COVID-19 response strategies such as the targeting of public\nhealth resources. Yet little attention has been paid to measurement validity\nand demographic bias, due in part to the lack of documentation about which\nusers are represented as well as the challenge of obtaining ground truth data\non unique visits and demographics. We illustrate how linking large-scale\nadministrative data can enable auditing mobility data for bias in the absence\nof demographic information and ground truth labels. More precisely, we show\nthat linking voter roll data -- containing individual-level voter turnout for\nspecific voting locations along with race and age -- can facilitate the\nconstruction of rigorous bias and reliability tests. These tests illuminate a\nsampling bias that is particularly noteworthy in the pandemic context: older\nand non-white voters are less likely to be captured by mobility data. We show\nthat allocating public health resources based on such mobility data could\ndisproportionately harm high-risk elderly and minority groups.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 02:04:14 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 01:42:13 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Coston", "Amanda", ""], ["Guha", "Neel", ""], ["Ouyang", "Derek", ""], ["Lu", "Lisa", ""], ["Chouldechova", "Alexandra", ""], ["Ho", "Daniel E.", ""]]}, {"id": "2011.07270", "submitter": "Cheuk Ting Li", "authors": "Cheuk Ting Li and Kim-Hung Li", "title": "Species Abundance Distribution and Species Accumulation Curve: A General\n  Framework and Results", "comments": "38 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We build a general framework which establishes a one-to-one correspondence\nbetween species abundance distribution (SAD) and species accumulation curve\n(SAC). The appearance rates of the species and the appearance times of\nindividuals in each species are modeled as Poisson processes. The number of\nspecies can be finite or infinite. We introduce a linear derivative ratio\nfamily of models, $\\mathrm{LDR}_1$, of which the ratio of the first and the\nsecond derivatives of the expected SAC is a linear function. A D1/D2 plot is\nproposed to detect this linear pattern in the data. The SAD of $\\mathrm{LDR}_1$\nis the Engen's extended negative binomial distribution, and the SAC encompasses\nseveral popular parametric forms including the power law. Family\n$\\mathrm{LDR}_1$ is extended in two ways: $\\mathrm{LDR}_2$ which allows species\nwith zero detection probability, and $\\mathrm{RDR}_1$ where the derivative\nratio is a rational function. We also consider the scenario where we record\nonly a few leading appearance times of each species. We show how maximum\nlikelihood inference can be performed when only the empirical SAC is observed,\nand elucidate its advantages over the traditional curve-fitting method.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 11:23:31 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 18:53:47 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Li", "Cheuk Ting", ""], ["Li", "Kim-Hung", ""]]}, {"id": "2011.07362", "submitter": "Santosh Kumar", "authors": "Santosh Kumar and S. Sai Charan", "title": "Spectral statistics for the difference of two Wishart matrices", "comments": "23 pages, 5 figures, Mathematica codes included as ancillary files\n  (Accepted for publication in J. Phys. A: Math. Theor.)", "journal-ref": null, "doi": "10.1088/1751-8121/abc3fe", "report-no": null, "categories": "math-ph math.MP physics.data-an quant-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider the weighted difference of two independent complex\nWishart matrices and derive the joint probability density function of the\ncorresponding eigenvalues in a finite-dimension scenario using two distinct\napproaches. The first derivation involves the use of unitary group integral,\nwhile the second one relies on applying the derivative principle. The latter\nrelates the joint probability density of eigenvalues of a matrix drawn from a\nunitarily invariant ensemble to the joint probability density of its diagonal\nelements. Exact closed form expressions for an arbitrary order correlation\nfunction are also obtained and spectral densities are contrasted with Monte\nCarlo simulation results. Analytical results for moments as well as\nprobabilities quantifying positivity aspects of the spectrum are also derived.\nAdditionally, we provide a large-dimension asymptotic result for the spectral\ndensity using the Stieltjes transform approach for algebraic random matrices.\nFinally, we point out the relationship of these results with the corresponding\nresults for difference of two random density matrices and obtain some explicit\nand closed form expressions for the spectral density and absolute mean.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 18:43:34 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Kumar", "Santosh", ""], ["Charan", "S. Sai", ""]]}, {"id": "2011.07476", "submitter": "Shengjia Zhao", "authors": "Shengjia Zhao, Stefano Ermon", "title": "Right Decisions from Wrong Predictions: A Mechanism Design Alternative\n  to Individual Calibration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.GT cs.LG math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision makers often need to rely on imperfect probabilistic forecasts.\nWhile average performance metrics are typically available, it is difficult to\nassess the quality of individual forecasts and the corresponding utilities. To\nconvey confidence about individual predictions to decision-makers, we propose a\ncompensation mechanism ensuring that the forecasted utility matches the\nactually accrued utility. While a naive scheme to compensate decision-makers\nfor prediction errors can be exploited and might not be sustainable in the long\nrun, we propose a mechanism based on fair bets and online learning that\nprovably cannot be exploited. We demonstrate an application showing how\npassengers could confidently optimize individual travel plans based on flight\ndelay probabilities estimated by an airline.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 08:22:39 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 06:03:57 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Zhao", "Shengjia", ""], ["Ermon", "Stefano", ""]]}, {"id": "2011.07539", "submitter": "Niklas Hartung", "authors": "Niklas Hartung, Martin Wahl, Abhishake Rastogi, Wilhelm Huisinga", "title": "Nonparametric goodness-of-fit testing for parametric covariate models in\n  pharmacometric analyses", "comments": null, "journal-ref": null, "doi": "10.1002/psp4.12614", "report-no": null, "categories": "stat.ME q-bio.QM stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The characterization of covariate effects on model parameters is a crucial\nstep during pharmacokinetic/pharmacodynamic analyses. While covariate selection\ncriteria have been studied extensively, the choice of the functional\nrelationship between covariates and parameters, however, has received much less\nattention. Often, a simple particular class of covariate-to-parameter\nrelationships (linear, exponential, etc.) is chosen ad hoc or based on domain\nknowledge, and a statistical evaluation is limited to the comparison of a small\nnumber of such classes. Goodness-of-fit testing against a nonparametric\nalternative provides a more rigorous approach to covariate model evaluation,\nbut no such test has been proposed so far. In this manuscript, we derive and\nevaluate nonparametric goodness-of-fit tests for parametric covariate models,\nthe null hypothesis, against a kernelized Tikhonov regularized alternative,\ntransferring concepts from statistical learning to the pharmacological setting.\nThe approach is evaluated in a simulation study on the estimation of the\nage-dependent maturation effect on the clearance of a monoclonal antibody.\nScenarios of varying data sparsity and residual error are considered. The\ngoodness-of-fit test correctly identified misspecified parametric models with\nhigh power for relevant scenarios. The case study provides proof-of-concept of\nthe feasibility of the proposed approach, which is envisioned to be beneficial\nfor applications that lack well-founded covariate models.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 14:36:47 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Hartung", "Niklas", ""], ["Wahl", "Martin", ""], ["Rastogi", "Abhishake", ""], ["Huisinga", "Wilhelm", ""]]}, {"id": "2011.07584", "submitter": "Alfredo Kalaitzis", "authors": "Dolores Garcia, Gonzalo Mateo-Garcia, Hannes Bernhardt, Ron\n  Hagensieker, Ignacio G. Lopez Francos, Jonathan Stock, Guy Schumann, Kevin\n  Dobbs, Freddie Kalaitzis", "title": "Pix2Streams: Dynamic Hydrology Maps from Satellite-LiDAR Fusion", "comments": "Work completed during the 2020 Frontier Development Lab research\n  accelerator, a private-public partnership with NASA in the US, and ESA in\n  Europe. Accepted as a spotlight/long oral talk at AI for Earth Sciences\n  Workshop at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Where are the Earth's streams flowing right now? Inland surface waters expand\nwith floods and contract with droughts, so there is no one map of our streams.\nCurrent satellite approaches are limited to monthly observations that map only\nthe widest streams. These are fed by smaller tributaries that make up much of\nthe dendritic surface network but whose flow is unobserved. A complete map of\nour daily waters can give us an early warning for where droughts are born: the\nreceding tips of the flowing network. Mapping them over years can give us a map\nof impermanence of our waters, showing where to expect water, and where not to.\nTo that end, we feed the latest high-res sensor data to multiple deep learning\nmodels in order to map these flowing networks every day, stacking the times\nseries maps over many years. Specifically, i) we enhance water segmentation to\n$50$ cm/pixel resolution, a 60$\\times$ improvement over previous\nstate-of-the-art results. Our U-Net trained on 30-40cm WorldView3 images can\ndetect streams as narrow as 1-3m (30-60$\\times$ over SOTA). Our multi-sensor,\nmulti-res variant, WasserNetz, fuses a multi-day window of 3m PlanetScope\nimagery with 1m LiDAR data, to detect streams 5-7m wide. Both U-Nets produce a\nwater probability map at the pixel-level. ii) We integrate this water map over\na DEM-derived synthetic valley network map to produce a snapshot of flow at the\nstream level. iii) We apply this pipeline, which we call Pix2Streams, to a\n2-year daily PlanetScope time-series of three watersheds in the US to produce\nthe first high-fidelity dynamic map of stream flow frequency. The end result is\na new map that, if applied at the national scale, could fundamentally improve\nhow we manage our water resources around the world.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 17:14:28 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Garcia", "Dolores", ""], ["Mateo-Garcia", "Gonzalo", ""], ["Bernhardt", "Hannes", ""], ["Hagensieker", "Ron", ""], ["Francos", "Ignacio G. Lopez", ""], ["Stock", "Jonathan", ""], ["Schumann", "Guy", ""], ["Dobbs", "Kevin", ""], ["Kalaitzis", "Freddie", ""]]}, {"id": "2011.07614", "submitter": "Paul Roediger", "authors": "Paul A. Roediger", "title": "A Picture's Worth a Thousand Words: Visualizing n-dimensional Overlap in\n  Logistic Regression Models with Empirical Likelihood", "comments": "Contains 1 pdf file consisting of 22 pages, 12 tables, 10 figures,\n  and 1 appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this note, conditions for the existence and uniqueness of the maximum\nlikelihood estimate for multidimensional predictor, binary response models are\nintroduced from a sensitivity testing point of view. The well known condition\nof Silvapulle is translated to be an empirical likelihood maximization which,\nwith existing R code, mechanizes the process of assessing overlap status. The\ntranslation shifts the meaning of overlap, defined by geometrical properties of\nthe two-predictor groups, from the intersection of their convex cones is\nnon-empty to the more understandable requirement that the convex hull of their\ndifferences contains zero. The code is applied to reveal the character of\noverlap by examining minimal overlapping structures and cataloging them in\ndimensions fewer than four. Rules to generate minimal higher dimensional\nstructures which account for overlap are provided. Supplementary materials are\navailable online.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 19:39:56 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Roediger", "Paul A.", ""]]}, {"id": "2011.07664", "submitter": "Han Lin Shang", "authors": "Ufuk Beyaztas and Han Lin Shang", "title": "Robust bootstrap prediction intervals for univariate and multivariate\n  autoregressive time series models", "comments": "34 pages, 15 figures, to appear at Journal of Applied Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The bootstrap procedure has emerged as a general framework to construct\nprediction intervals for future observations in autoregressive time series\nmodels. Such models with outlying data points are standard in real data\napplications, especially in the field of econometrics. These outlying data\npoints tend to produce high forecast errors, which reduce the forecasting\nperformances of the existing bootstrap prediction intervals calculated based on\nnon-robust estimators. In the univariate and multivariate autoregressive time\nseries, we propose a robust bootstrap algorithm for constructing prediction\nintervals and forecast regions. The proposed procedure is based on the weighted\nlikelihood estimates and weighted residuals. Its finite sample properties are\nexamined via a series of Monte Carlo studies and two empirical data examples.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 00:12:55 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Beyaztas", "Ufuk", ""], ["Shang", "Han Lin", ""]]}, {"id": "2011.08076", "submitter": "Sebastian Niehaus", "authors": "Nastassya Horlava, Alisa Mironenko, Sebastian Niehaus, Sebastian\n  Wagner, Ingo Roeder, Nico Scherf", "title": "A comparative study of semi- and self-supervised semantic segmentation\n  of biomedical microscopy data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Convolutional Neural Networks (CNNs) have become the\nstate-of-the-art method for biomedical image analysis. However, these networks\nare usually trained in a supervised manner, requiring large amounts of labelled\ntraining data. These labelled data sets are often difficult to acquire in the\nbiomedical domain. In this work, we validate alternative ways to train CNNs\nwith fewer labels for biomedical image segmentation using. We adapt two semi-\nand self-supervised image classification methods and analyse their performance\nfor semantic segmentation of biomedical microscopy images.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 20:57:10 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 13:03:10 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Horlava", "Nastassya", ""], ["Mironenko", "Alisa", ""], ["Niehaus", "Sebastian", ""], ["Wagner", "Sebastian", ""], ["Roeder", "Ingo", ""], ["Scherf", "Nico", ""]]}, {"id": "2011.08140", "submitter": "Chirag Manchanda", "authors": "Chirag Manchanda, Mayank Kumar, Vikram Singh, Mohd Faisal, Naba\n  Hazarika, Ashutosh Shukla, Vipul Lalchandani, Vikas Goel, Navaneeth Thamban,\n  Dilip Ganguly, Sachchida Nand Tripathi", "title": "Variation in chemical composition and sources of PM2.5 during the\n  COVID-19 lockdown in Delhi", "comments": "The article has been accepted for publication in Environment\n  International ISSN: 0160-4120. Please note that the article below has not yet\n  been subjected to formatting and copyediting in line with journal\n  requirements, which may result in some changes prior to publication", "journal-ref": "Environment International 153 (2021)", "doi": "10.1016/j.envint.2021.106541", "report-no": null, "categories": "physics.ao-ph stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The Government of India (GOI) announced a nationwide lockdown starting 25th\nMarch 2020 to contain the spread of COVID-19, leading to an unprecedented\ndecline in anthropogenic activities and in turn improvements in ambient air\nquality. This is the first study to focus on highly time-resolved chemical\nspeciation and source apportionment of PM$_{2.5}$ to assess the impact of the\nlockdown and subsequent relaxations on the sources of ambient PM$_{2.5}$ in\nDelhi, India. The elemental, organic, and black carbon fractions of PM$_{2.5}$\nwere measured at the IIT Delhi campus from February 2020 to May 2020. We report\nsource apportionment results using positive matrix factorization (PMF) of\norganic and elemental fractions of PM$_{2.5}$ during the different phases of\nthe lockdown. The resolved sources such as vehicular emissions, domestic coal\ncombustion, and semi-volatile oxygenated organic aerosol (SVOOA) were found to\ndecrease by 96%, 95%, and 86%, respectively, during lockdown phase-1 as\ncompared to pre-lockdown. An unforeseen rise in O$_3$ concentrations with\ndeclining NO$_x$ levels was observed, similar to other parts of the globe,\nleading to the low-volatility oxygenated organic aerosols (LVOOA) increasing to\nalmost double the pre-lockdown concentrations during the last phase of the\nlockdown. The effect of the lockdown was found to be less pronounced on other\nresolved sources like secondary chloride, power plants, dust-related,\nhydrocarbon-like organic aerosols (HOA), and biomass burning related emissions,\nwhich were also swayed by the changing meteorological conditions during the\nfour lockdown phases. The results presented in this study provide a basis for\nfuture emission control strategies, quantifying the extent to which\nconstraining certain anthropogenic activities can ameliorate the ambient air.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 18:07:49 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 06:57:18 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Manchanda", "Chirag", ""], ["Kumar", "Mayank", ""], ["Singh", "Vikram", ""], ["Faisal", "Mohd", ""], ["Hazarika", "Naba", ""], ["Shukla", "Ashutosh", ""], ["Lalchandani", "Vipul", ""], ["Goel", "Vikas", ""], ["Thamban", "Navaneeth", ""], ["Ganguly", "Dilip", ""], ["Tripathi", "Sachchida Nand", ""]]}, {"id": "2011.08148", "submitter": "Vinko Zlati\\'c", "authors": "Irena Barja\\v{s}i\\'c, Hrvoje \\v{S}tefan\\v{c}i\\'c, Vedrana\n  Pribi\\v{c}evi\\'c, Vinko Zlati\\'c", "title": "Causal motifs and existence of endogenous cascades in directed networks\n  with application to company defaults", "comments": "19 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": "RBI-ThPhys-2020-44", "categories": "physics.soc-ph cond-mat.stat-mech cs.SI econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by detection of cascades of defaults in economy, we developed a\ndetection framework for endogenous spreading based on causal motifs we define\nin this paper. We assume that vertex change of state can be triggered by\nendogenous or exogenous event, that underlying network is directed and that\ntimes when vertices changed their states are available. In addition to data of\ncompany defaults we use, we simulate cascades driven by different stochastic\nprocesses on different synthetic networks. We also extended an approximate\nmaster equation method to directed networks with temporal stamps in order to\nunderstand in which cases detection is possible. We show that some of the\nsmallest motifs can robustly detect cascades.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 18:20:38 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Barja\u0161i\u0107", "Irena", ""], ["\u0160tefan\u010di\u0107", "Hrvoje", ""], ["Pribi\u010devi\u0107", "Vedrana", ""], ["Zlati\u0107", "Vinko", ""]]}, {"id": "2011.08171", "submitter": "Zhiyuan Wei", "authors": "Sayanti Mukherjee, Zhiyuan Wei", "title": "Suicide disparities across urban and suburban areas in the U.S.: A\n  comparative assessment of socio-environmental factors using a data-driven\n  predictive approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disparity in suicide rates between urban and suburban/rural areas is growing,\nwith rural areas typically witnessing higher suicide rates in the U.S. However,\nprevious studies often ignored the effect of socio-environmental factors on the\nsuicide rates and its regional disparity. To address these gaps, we propose a\nholistic data-driven framework to model the associations of social\n(demographic, socioeconomic) and environmental (climate) factors on suicide\nrates, and study the disparities across urban and suburban areas. Leveraging\nthe county-level suicide data from 2000--2017 along with the\nsocio-environmental features, we trained, tested and validated a suite of\nadvanced statistical learning algorithms to identify, assess and predict the\ninfluence of key socio-environmental factors on suicide rates. Random forest\noutperformed all other models in terms of goodness-of-fit and predictive\naccuracy, and selected as the final model to make inferences. Our results\nindicate that population demographics is significantly associated with both\nurban and suburban suicide rates. We found that suburban population is more\nvulnerable to suicides compared to urban communities, with suburban suicide\nrate being particularly sensitive to unemployment rate and median household\nincome. Our analysis revealed that suicide mortality is correlated to climate,\nshowing that urban suicide rate is more sensitive to higher temperatures,\nseasonal-heating-degree-days and precipitation, while suburban suicide rate is\nsensitive to only seasonal-cooling-degree-days. This work provides deeper\ninsights on interactions between key socio-environmental factors and suicides\nacross different urbanized areas, and can help the public health agencies\ndevelop suicide prevention strategies to reduce the growing risk of suicides.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 18:56:31 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Mukherjee", "Sayanti", ""], ["Wei", "Zhiyuan", ""]]}, {"id": "2011.08299", "submitter": "Harrison Wilde", "authors": "Harrison Wilde, Jack Jewson, Sebastian Vollmer and Chris Holmes", "title": "Foundations of Bayesian Learning from Synthetic Data", "comments": "43 pages (10 main text, 33 supplement), 32 figures (4 main text, 28\n  supplement)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is significant growth and interest in the use of synthetic data as an\nenabler for machine learning in environments where the release of real data is\nrestricted due to privacy or availability constraints. Despite a large number\nof methods for synthetic data generation, there are comparatively few results\non the statistical properties of models learnt on synthetic data, and fewer\nstill for situations where a researcher wishes to augment real data with\nanother party's synthesised data. We use a Bayesian paradigm to characterise\nthe updating of model parameters when learning in these settings, demonstrating\nthat caution should be taken when applying conventional learning algorithms\nwithout appropriate consideration of the synthetic data generating process and\nlearning task. Recent results from general Bayesian updating support a novel\nand robust approach to Bayesian synthetic-learning founded on decision theory\nthat outperforms standard approaches across repeated experiments on supervised\nlearning and inference problems.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 21:49:17 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 15:01:22 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Wilde", "Harrison", ""], ["Jewson", "Jack", ""], ["Vollmer", "Sebastian", ""], ["Holmes", "Chris", ""]]}, {"id": "2011.08309", "submitter": "Han Lin Shang", "authors": "Han Lin Shang and Ruofan Xu", "title": "Change point detection for COVID-19 excess deaths in Belgium", "comments": "11 pages, 4 figures, to appear at Journal of Population Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Emerging at the end of 2019, COVID-19 has become a public health threat to\npeople worldwide. Apart from the deaths who tested positive for COVID-19, many\nothers have died from causes indirectly related to COVID-19. Therefore, the\nCOVID-19 confirmed deaths underestimate the influence of the pandemic on the\nsociety; instead, the measure of `excess deaths' is a more objective and\ncomparable way to assess the scale of the epidemic and formulate lessons. One\ncommon practical issue in analyzing the impact of COVID-19 is to determine the\n`pre-COVID-19' period and the `post-COVID-19' period. We apply a change point\ndetection method to identify any change points using the excess deaths in\nBelgium.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 22:13:52 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Shang", "Han Lin", ""], ["Xu", "Ruofan", ""]]}, {"id": "2011.08350", "submitter": "Nikola Po\\v{c}u\\v{c}a", "authors": "Nikola Pocuca, Mark Farrell, and Paul D. McNicholas", "title": "Defying the Circadian Rhythm: Clustering Participant Telemetry in the UK\n  Biobank Data", "comments": "28 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The UK Biobank dataset follows over 500,000 volunteers and contains a diverse\nset of information related to societal outcomes. Among this vast collection, a\nlarge quantity of telemetry collected from wrist-worn accelerometers provides a\nsnapshot of participant activity. Using this data, a population of shift\nworkers, subjected to disrupted circadian rhythms, is analysed using a mixture\nmodel-based approach to yield protective effects from physical activity on\nsurvival outcomes. In this paper, we develop a scalable, standardized, and\nunique methodology that efficiently clusters a vast quantity of participant\ntelemetry. By building upon the work of Doherty et al. (2017), we introduce a\nstandardized, low-dimensional feature for clustering purposes. Participants are\nclustered using a matrix variate mixture model-based approach. Once clustered,\nsurvival analysis is performed to demonstrate distinct lifetime outcomes for\nindividuals within each cluster. In summary, we process, cluster, and analyse a\nsubset of UK Biobank participants to show the protective effects from physical\nactivity on circadian disrupted individuals.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 00:33:36 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Pocuca", "Nikola", ""], ["Farrell", "Mark", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "2011.08405", "submitter": "Daniel Kennedy", "authors": "Daniel William Kennedy, Jessica Cameron, Paul Pao-Yen Wu, Kerrie\n  Mengersen", "title": "Peer groups for organisational learning: clustering with practical\n  constraints", "comments": "22 pages, 4 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0251723", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Peer-grouping is used in many sectors for organisational learning, policy\nimplementation, and benchmarking. Clustering provides a statistical,\ndata-driven method for constructing meaningful peer groups, but peer groups\nmust be compatible with business constraints such as size and stability\nconsiderations. Additionally, statistical peer groups are constructed from many\ndifferent variables, and can be difficult to understand, especially for\nnon-statistical audiences. We developed methodology to apply business\nconstraints to clustering solutions and allow the decision-maker to choose the\nbalance between statistical goodness-of-fit and conformity to business\nconstraints. Several tools were utilised to identify complex distinguishing\nfeatures in peer groups, and a number of visualisations are developed to\nexplain high-dimensional clusters for non-statistical audiences. In a case\nstudy where peer group size was required to be small ($\\leq 100$ members), we\napplied constrained clustering to a noisy high-dimensional data-set over two\nsubsequent years, ensuring that the clusters were sufficiently stable between\nyears. Our approach not only satisfied clustering constraints on the test data,\nbut maintained an almost monotonic negative relationship between\ngoodness-of-fit and stability between subsequent years. We demonstrated in the\ncontext of the case study how distinguishing features between clusters can be\ncommunicated clearly to different stakeholders with substantial and limited\nstatistical knowledge.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 03:42:29 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Kennedy", "Daniel William", ""], ["Cameron", "Jessica", ""], ["Wu", "Paul Pao-Yen", ""], ["Mengersen", "Kerrie", ""]]}, {"id": "2011.08407", "submitter": "Daniel Kennedy", "authors": "Daniel W. Kennedy, Jessica Cameron, Paul P.-Y. Wu, Kerrie Mengersen", "title": "A statistical machine learning approach for benchmarking in the presence\n  of complex contextual factors and peer groups", "comments": "18 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to compare between individuals or organisations fairly is\nimportant for the development of robust and meaningful quantitative benchmarks.\nTo make fair comparisons, contextual factors must be taken into account, and\ncomparisons should only be made between similar organisations such as peer\ngroups. Previous benchmarking methods have used linear regression to adjust for\ncontextual factors, however linear regression is known to be sub-optimal when\nnonlinear relationships exist between the comparative measure and covariates.\nIn this paper we propose a random forest model for benchmarking that can adjust\nfor these potential nonlinear relationships, and validate the approach in a\ncase-study of high noise data. We provide new visualisations and numerical\nsummaries of the fitted models and comparative measures to facilitate\ninterpretation by both analysts and non-technical audiences. Comparisons can be\nmade across the cohort or within peer groups, and bootstrapping provides a\nmeans of estimating uncertainty in both adjusted measures and rankings. We\nconclude that random forest models can facilitate fair comparisons between\norganisations for quantitative measures including in cases on complex\ncontextual factor relationships, and that the models and outputs are readily\ninterpreted by stakeholders.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 03:51:54 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Kennedy", "Daniel W.", ""], ["Cameron", "Jessica", ""], ["Wu", "Paul P. -Y.", ""], ["Mengersen", "Kerrie", ""]]}, {"id": "2011.08606", "submitter": "Deeksha Sinha", "authors": "Vivek F. Farias, Andrew A. Li, and Deeksha Sinha", "title": "Optimizing Offer Sets in Sub-Linear Time", "comments": "30 pages, 3 figures. Proceedings of the 21st ACM Conference on\n  Economics and Computation. 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Personalization and recommendations are now accepted as core competencies in\njust about every online setting, ranging from media platforms to e-commerce to\nsocial networks. While the challenge of estimating user preferences has\ngarnered significant attention, the operational problem of using such\npreferences to construct personalized offer sets to users is still a challenge,\nparticularly in modern settings where a massive number of items and a\nmillisecond response time requirement mean that even enumerating all of the\nitems is impossible. Faced with such settings, existing techniques are either\n(a) entirely heuristic with no principled justification, or (b) theoretically\nsound, but simply too slow to work.\n  Thus motivated, we propose an algorithm for personalized offer set\noptimization that runs in time sub-linear in the number of items while enjoying\na uniform performance guarantee. Our algorithm works for an extremely general\nclass of problems and models of user choice that includes the mixed multinomial\nlogit model as a special case. We achieve a sub-linear runtime by leveraging\nthe dimensionality reduction from learning an accurate latent factor model,\nalong with existing sub-linear time approximate near neighbor algorithms. Our\nalgorithm can be entirely data-driven, relying on samples of the user, where a\n`sample' refers to the user interaction data typically collected by firms. We\nevaluate our approach on a massive content discovery dataset from Outbrain that\nincludes millions of advertisements. Results show that our implementation\nindeed runs fast and with increased performance relative to existing fast\nheuristics.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 13:02:56 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Farias", "Vivek F.", ""], ["Li", "Andrew A.", ""], ["Sinha", "Deeksha", ""]]}, {"id": "2011.08669", "submitter": "Li-Chun Zhang", "authors": "Li-Chun Zhang", "title": "Sampling designs for epidemic prevalence estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Intuitively, sampling is likely to be more efficient for prevalence\nestimation, if the cases (or positives) have a relatively higher representation\nin the sample than in the population. In case the virus is transmitted via\npersonal contacts, contact tracing of the observed cases (but not noncases), to\nbe referred to as \\emph{adaptive network tracing}, can generate a higher yield\nof cases than random sampling from the population. The efficacy of relevant\ndesigns for cross-sectional and change estimation is investigated. The\navailability of these designs allows one unite tracing for combating the\nepidemic and sampling for estimating the prevalence in a single endeavour.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 11:13:06 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Zhang", "Li-Chun", ""]]}, {"id": "2011.08719", "submitter": "Shujaat Khan", "authors": "Seongyong Park, Shujaat Khan, Muhammad Moinuddin, Ubaid M. Al-Saggaf", "title": "GSSMD: A new standardized effect size measure to improve robustness and\n  interpretability in biological applications", "comments": "Accepted in International Conference on Bioinformatics and\n  Biomedicine (BIBM) 2020. arXiv admin note: text overlap with arXiv:2001.06384", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IT math.IT q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many biological applications, the primary objective of study is to\nquantify the magnitude of treatment effect between two groups. Cohens'd or\nstrictly standardized mean difference (SSMD) can be used to measure effect size\nhowever, it is sensitive to violation of assumption of normality. Here, we\npropose an alternative metric of standardized effect size measure to improve\nrobustness and interpretability, based on the overlap between two sample\ndistributions. The proposed method is a non-parametric generalized variant of\nSSMD (Strictly Standardized Mean Difference). We characterized proposed measure\nin various simulation settings to illustrate its behavior. We also investigated\nfinite sample properties on the estimation of effect size and draw some\nguidelines. As a case study, we applied our measure for hit selection problem\nin an RNAi experiment and showed superiority of proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 11:25:30 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Park", "Seongyong", ""], ["Khan", "Shujaat", ""], ["Moinuddin", "Muhammad", ""], ["Al-Saggaf", "Ubaid M.", ""]]}, {"id": "2011.08810", "submitter": "Matthew Kunz", "authors": "M. Ross Kunz, Adam Yonge, Zongtang Fang, Andrew J. Medford, Denis\n  Constales, Gregory Yablonsky, Rebecca Fushimi", "title": "Data Driven Reaction Mechanism Estimation via Transient Kinetics and\n  Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding the set of elementary steps and kinetics in each reaction is\nextremely valuable to make informed decisions about creating the next\ngeneration of catalytic materials. With physical and mechanistic complexity of\nindustrial catalysts, it is critical to obtain kinetic information through\nexperimental methods. As such, this work details a methodology based on the\ncombination of transient rate/concentration dependencies and machine learning\nto measure the number of active sites, the individual rate constants, and gain\ninsight into the mechanism under a complex set of elementary steps. This new\nmethodology was applied to simulated transient responses to verify its ability\nto obtain correct estimates of the micro-kinetic coefficients. Furthermore,\nexperimental CO oxidation data was analyzed to reveal the Langmuir-Hinshelwood\nmechanism driving the reaction. As oxygen accumulated on the catalyst, a\ntransition in the mechanism was clearly defined in the machine learning\nanalysis due to the large amount of kinetic information available from\ntransient reaction techniques. This methodology is proposed as a new data\ndriven approach to characterize how materials control complex reaction\nmechanisms relying exclusively on experimental data.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 18:14:10 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 14:26:49 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Kunz", "M. Ross", ""], ["Yonge", "Adam", ""], ["Fang", "Zongtang", ""], ["Medford", "Andrew J.", ""], ["Constales", "Denis", ""], ["Yablonsky", "Gregory", ""], ["Fushimi", "Rebecca", ""]]}, {"id": "2011.08901", "submitter": "Gustau Camps-Valls", "authors": "Jose M. Tarraga, Maria Piles and Gustau Camps-Valls", "title": "Learning drivers of climate-induced human migrations with Gaussian\n  processes", "comments": "Presented at NeurIPS 2020 Workshop on Machine Learning for the\n  Developing World", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the current context of climate change, extreme heatwaves, droughts, and\nfloods are not only impacting the biosphere and atmosphere but the\nanthroposphere too. Human populations are forcibly displaced, which are now\nreferred to as climate-induced migrants. In this work, we investigate which\nclimate and structural factors forced major human displacements in the presence\nof floods and storms during the years 2017-2019. We built, curated, and\nharmonized a database of meteorological and remote sensing indicators along\nwith structural factors of 27 developing countries worldwide. We show how we\ncan use Gaussian Processes to learn what variables can explain the impact of\nfloods and storms in the context of forced displacements and to develop models\nthat reproduce migration flows. Our results at regional, global, and\ndisaster-specific scales show the importance of structural factors in the\ndetermination of the magnitude of displacements. The study may have both\nsocietal, political, and economical implications.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 19:38:26 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Tarraga", "Jose M.", ""], ["Piles", "Maria", ""], ["Camps-Valls", "Gustau", ""]]}, {"id": "2011.09033", "submitter": "David Kline", "authors": "David Kline, Zehang Li, Yue Chu, Jon Wakefield, William C. Miller,\n  Abigail Norris Turner, Samuel J Clark", "title": "Estimating Seroprevalence of SARS-CoV-2 in Ohio: A Bayesian Multilevel\n  Poststratification Approach with Multiple Diagnostic Tests", "comments": null, "journal-ref": "PNAS June 29, 2021 118 (26) e2023947118", "doi": "10.1073/pnas.2023947118", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Globally the SARS-CoV-2 coronavirus has infected more than 59 million people\nand killed more than 1.39 million. Designing and monitoring interventions to\nslow and stop the spread of the virus require knowledge of how many people have\nbeen and are currently infected, where they live, and how they interact. The\nfirst step is an accurate assessment of the population prevalence of past\ninfections. There are very few population-representative prevalence studies of\nthe SARS-CoV-2 coronavirus, and only two American states -- Indiana and\nConnecticut -- have reported probability-based sample surveys that characterize\nstate-wide prevalence of the SARS-CoV-2 coronavirus. One of the difficulties is\nthe fact that the tests to detect and characterize SARS-CoV-2 coronavirus\nantibodies are new, not well characterized, and generally function poorly.\nDuring July, 2020, a survey representing all adults in the State of Ohio in the\nUnited States collected biomarkers and information on protective behavior\nrelated to the SARS-CoV-2 coronavirus. Several features of the survey make it\ndifficult to estimate past prevalence: 1) a low response rate, 2) very low\nnumber of positive cases, and 3) the fact that multiple, poor quality\nserological tests were used to detect SARS-CoV-2 antibodies. We describe a new\nBayesian approach for analyzing the biomarker data that simultaneously\naddresses these challenges and characterizes the potential effect of selective\nresponse. The model does not require survey sample weights, accounts for\nmultiple, imperfect antibody test results, and characterizes uncertainty\nrelated to the sample survey and the multiple, imperfect, potentially\ncorrelated tests.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 01:50:04 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 15:27:06 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Kline", "David", ""], ["Li", "Zehang", ""], ["Chu", "Yue", ""], ["Wakefield", "Jon", ""], ["Miller", "William C.", ""], ["Turner", "Abigail Norris", ""], ["Clark", "Samuel J", ""]]}, {"id": "2011.09070", "submitter": "Sudipta Bhattacharya", "authors": "Sudipta Bhattacharya and Jyotirmoy Dey", "title": "Assessing contribution of treatment phases through tipping point\n  analyses using rank preserving structural failure time models", "comments": "33 pages, 6 figures and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In clinical trials, an experimental treatment is sometimes added on to a\nstandard of care or control therapy in multiple treatment phases (e.g.,\nconcomitant and maintenance phases) to improve patient outcomes. When the new\nregimen provides meaningful benefit over the control therapy in such cases, it\nproves difficult to separately assess the contribution of each phase to the\noverall effect observed. This article provides an approach for assessing the\nimportance of a specific treatment phase in such a situation through tipping\npoint analyses of a time-to-event endpoint using\nrank-preserving-structural-failure-time (RPSFT) modeling. A tipping-point\nanalysis is commonly used in situations where it is suspected that a\nstatistically significant difference between treatment arms could be a result\nof missing or unobserved data instead of a real treatment effect.\nRank-preserving-structural-failure-time modeling is an approach for causal\ninference that is typically used to adjust for treatment switching in clinical\ntrials with time to event endpoints. The methodology proposed in this article\nis an amalgamation of these two ideas to investigate the contribution of a\ntreatment phase of interest to the effect of a regimen comprising multiple\ntreatment phases. We provide two different variants of the method corresponding\nto two different effects of interest. We provide two different tipping point\nthresholds depending on inferential goals. The proposed approaches are\nmotivated and illustrated with data from a recently concluded, real-life phase\n3 cancer clinical trial. We then conclude with several considerations and\nrecommendations.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 03:39:43 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Bhattacharya", "Sudipta", ""], ["Dey", "Jyotirmoy", ""]]}, {"id": "2011.09152", "submitter": "Michael Gallaugher Ph.D.", "authors": "Michael P.B. Gallaugher, Paul D. McNicholas, Volodymyr Melnykov, and\n  Xuwen Zhu", "title": "Skewed Distributions or Transformations? Modelling Skewness for a\n  Cluster Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because of its mathematical tractability, the Gaussian mixture model holds a\nspecial place in the literature for clustering and classification. For all its\nbenefits, however, the Gaussian mixture model poses problems when the data is\nskewed or contains outliers. Because of this, methods have been developed over\nthe years for handling skewed data, and fall into two general categories. The\nfirst is to consider a mixture of more flexible skewed distributions, and the\nsecond is based on incorporating a transformation to near normality. Although\nthese methods have been compared in their respective papers, there has yet to\nbe a detailed comparison to determine when one method might be more suitable\nthan the other. Herein, we provide a detailed comparison on many benchmarking\ndatasets, as well as describe a novel method to assess cluster separation.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 08:12:20 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Gallaugher", "Michael P. B.", ""], ["McNicholas", "Paul D.", ""], ["Melnykov", "Volodymyr", ""], ["Zhu", "Xuwen", ""]]}, {"id": "2011.09240", "submitter": "Pranay Seshadri", "authors": "Pranay Seshadri and Andrew Duncan and George Thorne", "title": "Bayesian mass averaging", "comments": "Journal submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.flu-dyn stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mass averaging is pivotal in turbomachinery. In both experiments and CFD\nsimulations mass averages of flow quantities may be taken circumferentially,\nradially, and often both. These are critical for arriving at 1D or 2D\nperformance metrics that shape our understanding of losses and their control.\nSuch understanding empowers design advances, affords anomaly detection, informs\nmaintenance and diagnostic efforts. This paper presents a new statistical\nframework for obtaining mass averages of flow quantities in turbomachinery,\ntailored for rig tests, heavily instrumented engines, and cruise-level engines.\nThe proposed Bayesian framework is tailored for computing mass averages of\npressures and temperatures, given their values from circumferentially scattered\nrakes. Two variants of this methodology are offered: (i) for the case where\nmassflow rate distributions can not be experimentally determined and\ncomputational fluid dynamics (CFD) based profiles are available, and (ii) where\nexperimental massflow rate distributions are available. In scope, this\nframework addresses limitations with existing measurement budget calculation\npractices and with a view towards facilitating principled aggregation of\nuncertainties over measurement chains.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 12:28:25 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Seshadri", "Pranay", ""], ["Duncan", "Andrew", ""], ["Thorne", "George", ""]]}, {"id": "2011.09325", "submitter": "Corentin Martens", "authors": "Corentin Martens, Olivier Debeir, Christine Decaestecker, Thierry\n  Metens, Laetitia Lebrun, Gil Leurquin-Sterk, Nicola Trotta, Serge Goldman and\n  Gaetan Van Simaeys", "title": "Voxelwise principal component analysis of dynamic\n  [S-methyl-11C]methionine PET data in glioma patients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph eess.IV eess.SP q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have demonstrated the added value of dynamic amino acid positron\nemission tomography (PET) for glioma grading and genotyping, biopsy targeting,\nand recurrence diagnosis. However, most of these studies are exclusively based\non hand-crafted qualitative or semi-quantitative dynamic features extracted\nfrom the mean time activity curve (TAC) within predefined volumes. Voxelwise\ndynamic PET data analysis could instead provide a better insight into\nintra-tumour heterogeneity of gliomas. In this work, we investigate the ability\nof the widely used principal component analysis (PCA) method to extract\nmeaningful quantitative dynamic features from high-dimensional motion-corrected\ndynamic [S-methyl-11C]methionine PET data in a first cohort of 20 glioma\npatients. By means of realistic numerical simulations, we demonstrate the\nrobustness of our methodology to noise. In a second cohort of 13 glioma\npatients, we compare the resulting parametric maps to these provided by\nstandard one- and two-tissue compartment pharmacokinetic (PK) models. We show\nthat our PCA model outperforms PK models in the identification of intra-tumour\nuptake dynamics heterogeneity while being much less computationally expensive.\nSuch parametric maps could be valuable to assess tumour aggressiveness locally\nwith applications in treatment planning as well as in the evaluation of tumour\nprogression and response to treatment. This work also provides further\nencouraging results on the added value of dynamic over static analysis of\n[S-methyl-11C]methionine PET data in gliomas, as previously demonstrated for\nO-(2-[18F]fluoroethyl)-L-tyrosine.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 15:00:09 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 17:57:05 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Martens", "Corentin", ""], ["Debeir", "Olivier", ""], ["Decaestecker", "Christine", ""], ["Metens", "Thierry", ""], ["Lebrun", "Laetitia", ""], ["Leurquin-Sterk", "Gil", ""], ["Trotta", "Nicola", ""], ["Goldman", "Serge", ""], ["Van Simaeys", "Gaetan", ""]]}, {"id": "2011.09362", "submitter": "Owais Sarwar", "authors": "Owais Sarwar and Benjamin Sauk and Nikolaos V. Sahinidis", "title": "A Discussion on Practical Considerations with Sparse Regression\n  Methodologies", "comments": "12 pages", "journal-ref": "Statist. Sci. Volume 35, Number 4 (2020), 593-601", "doi": "10.1214/20-STS806", "report-no": null, "categories": "cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse linear regression is a vast field and there are many different\nalgorithms available to build models. Two new papers published in Statistical\nScience study the comparative performance of several sparse regression\nmethodologies, including the lasso and subset selection. Comprehensive\nempirical analyses allow the researchers to demonstrate the relative merits of\neach estimator and provide guidance to practitioners. In this discussion, we\nsummarize and compare the two studies and we examine points of agreement and\ndivergence, aiming to provide clarity and value to users. The authors have\nstarted a highly constructive dialogue, our goal is to continue it.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 15:58:35 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 19:43:24 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Sarwar", "Owais", ""], ["Sauk", "Benjamin", ""], ["Sahinidis", "Nikolaos V.", ""]]}, {"id": "2011.09370", "submitter": "Gurcan Comert", "authors": "Gurcan Comert, Negash Begashaw", "title": "Cycle-to-Cycle Queue Length Estimation from Connected Vehicles with\n  Filtering on Primary Parameters", "comments": "16 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.AI cs.RO cs.SY eess.SY stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Estimation models from connected vehicles often assume low level parameters\nsuch as arrival rates and market penetration rates as known or estimate them in\nreal-time. At low market penetration rates, such parameter estimators produce\nlarge errors making estimated queue lengths inefficient for control or\noperations applications. In order to improve accuracy of low level parameter\nestimations, this study investigates the impact of connected vehicles\ninformation filtering on queue length estimation models. Filters are used as\nmultilevel real-time estimators. Accuracy is tested against known arrival rate\nand market penetration rate scenarios using microsimulations. To understand the\neffectiveness for short-term or for dynamic processes, arrival rates, and\nmarket penetration rates are changed every 15 minutes. The results show that\nwith Kalman and Particle filters, parameter estimators are able to find the\ntrue values within 15 minutes and meet and surpass the accuracy of known\nparameter scenarios especially for low market penetration rates. In addition,\nusing last known estimated queue lengths when no connected vehicle is present\nperforms better than inputting average estimated values. Moreover, the study\nshows that both filtering algorithms are suitable for real-time applications\nthat require less than 0.1 second computational time.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 16:09:45 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Comert", "Gurcan", ""], ["Begashaw", "Negash", ""]]}, {"id": "2011.09377", "submitter": "Teng Zhang", "authors": "Linying Yang, Teng Zhang, Peter Glynn, David Scheinker", "title": "The Development and Deployment of a Model for Hospital-level COVID-19\n  Associated Patient Demand Intervals from Consistent Estimators (DICE)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hospitals commonly project demand for their services by combining their\nhistorical share of regional demand with forecasts of total regional demand.\nHospital-specific forecasts of demand that provide prediction intervals, rather\nthan point estimates, may facilitate better managerial decisions, especially\nwhen demand overage and underage are associated with high, asymmetric costs.\nRegional forecasts of patient demand are commonly available as a Poisson random\nvariable, e.g., for the number of people requiring hospitalization due to an\nepidemic such as COVID-19. However, even in this common setting, no\nprobabilistic, consistent, computationally tractable forecast is available for\nthe fraction of patients in a region that a particular institution should\nexpect. We introduce such a forecast, DICE (Demand Intervals from Consistent\nEstimators). We describe its development and deployment at an academic medical\ncenter in California during the `second wave' of COVID-19 in the Unite States.\nWe show that DICE is consistent under mild assumptions and suitable for use\nwith perfect, biased, unbiased regional forecasts. We evaluate its performance\non empirical data from a large academic medical center as well as on synthetic\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 16:22:12 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 03:10:38 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Yang", "Linying", ""], ["Zhang", "Teng", ""], ["Glynn", "Peter", ""], ["Scheinker", "David", ""]]}, {"id": "2011.09397", "submitter": "Gurcan Comert", "authors": "Gurcan Comert, Mecit Cetin", "title": "Queue Length Estimation at Traffic Signals: Connected Vehicles with\n  Range Measurement Sensors", "comments": "17 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Today vehicles are becoming a rich source of data as they are equipped with\nlocalization or tracking and with wireless communications technologies. With\nthe increasing interest in automated- or self- driving technologies, vehicles\nare also being equipped with range measuring sensors (e.g., LIDAR, stereo\ncameras, and ultrasonic) to detect other vehicles and objects in the\nsurrounding environment. It is possible to envision that such vehicles could\nshare their data with the transportation infrastructure elements (e.g., a\ntraffic signal controller) to enable different mobility and safety\napplications. Data from these connected vehicles could then be used to estimate\nthe system state in real-time. This paper develops queue length estimators from\nconnected vehicles equipped with range measurement sensors. Simple\nplug-and-play models are developed for queue length estimations without needing\nground truth queue lengths by extending the previous formulations. The proposed\nmethod is simple to implement and can be adopted to cyclic queues at traffic\nsignals with known phase lengths. The derived models are evaluated with data\nfrom microscopic traffic simulations. From numerical experiments, the QLE model\nwith range sensors improves the errors as much as 25% in variance-to-mean ratio\nand 5\\% in coefficient of variation at low less than 20% market penetration\nrates.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 16:56:12 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Comert", "Gurcan", ""], ["Cetin", "Mecit", ""]]}, {"id": "2011.09436", "submitter": "Gurcan Comert", "authors": "Gurcan Comert, Mashrur Chowdhury, David M. Nicol", "title": "Assessment of System-Level Cyber Attack Vulnerability for Connected and\n  Autonomous Vehicles Using Bayesian Networks", "comments": "21 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.SY eess.SY stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This study presents a methodology to quantify vulnerability of cyber attacks\nand their impacts based on probabilistic graphical models for intelligent\ntransportation systems under connected and autonomous vehicles framework. Cyber\nattack vulnerabilities from various types and their impacts are calculated for\nintelligent signals and cooperative adaptive cruise control (CACC) applications\nbased on the selected performance measures. Numerical examples are given that\nshow impact of vulnerabilities in terms of average intersection queue lengths,\nnumber of stops, average speed, and delays. At a signalized network with and\nwithout redundant systems, vulnerability can increase average queues and delays\nby $3\\%$ and $15\\%$ and $4\\%$ and $17\\%$, respectively. For CACC application,\nimpact levels reach to $50\\%$ delay difference on average when low amount of\nspeed information is perturbed. When significantly different speed\ncharacteristics are inserted by an attacker, delay difference increases beyond\n$100\\%$ of normal traffic conditions.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 18:13:57 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Comert", "Gurcan", ""], ["Chowdhury", "Mashrur", ""], ["Nicol", "David M.", ""]]}, {"id": "2011.09447", "submitter": "Kelly Geyer", "authors": "Kelly Geyer, Frederick Campbell, Andersen Chang, John Magnotti,\n  Michael Beauchamp, Genevera I. Allen", "title": "Interpretable Visualization and Higher-Order Dimension Reduction for\n  ECoG Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.LG eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ElectroCOrticoGraphy (ECoG) technology measures electrical activity in the\nhuman brain via electrodes placed directly on the cortical surface during\nneurosurgery. Through its capability to record activity at a fast temporal\nresolution, ECoG experiments have allowed scientists to better understand how\nthe human brain processes speech. By its nature, ECoG data is difficult for\nneuroscientists to directly interpret for two major reasons. Firstly, ECoG data\ntends to be large in size, as each individual experiment yields data up to\nseveral gigabytes. Secondly, ECoG data has a complex, higher-order nature.\nAfter signal processing, this type of data may be organized as a 4-way tensor\nwith dimensions representing trials, electrodes, frequency, and time. In this\npaper, we develop an interpretable dimension reduction approach called\nRegularized Higher Order Principal Components Analysis, as well as an extension\nto Regularized Higher Order Partial Least Squares, that allows neuroscientists\nto explore and visualize ECoG data. Our approach employs a sparse and\nfunctional Candecomp-Parafac (CP) decomposition that incorporates sparsity to\nselect relevant electrodes and frequency bands, as well as smoothness over time\nand frequency, yielding directly interpretable factors. We demonstrate the\nperformance and interpretability of our method with an ECoG case study on audio\nand visual processing of human speech.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 18:19:43 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 01:52:01 GMT"}, {"version": "v3", "created": "Sat, 12 Dec 2020 19:11:05 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Geyer", "Kelly", ""], ["Campbell", "Frederick", ""], ["Chang", "Andersen", ""], ["Magnotti", "John", ""], ["Beauchamp", "Michael", ""], ["Allen", "Genevera I.", ""]]}, {"id": "2011.09469", "submitter": "Gurcan Comert", "authors": "Gurcan Comert, Negash Begashaw, Nathan Huynh", "title": "Improved Grey System Models for Predicting Traffic Parameters", "comments": "16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In transportation applications such as real-time route guidance, ramp\nmetering, congestion pricing and special events traffic management, accurate\nshort-term traffic flow prediction is needed. For this purpose, this paper\nproposes several novel \\textit{online} Grey system models (GM):\nGM(1,1$|cos(\\omega t)$), GM(1,1$|sin(\\omega t)$, $cos(\\omega t)$), and\nGM(1,1$|e^{-at}$,$sin(\\omega t)$,$cos(\\omega t)$). To evaluate the performance\nof the proposed models, they are compared against a set of benchmark models:\nGM(1,1) model, Grey Verhulst models with and without Fourier error corrections,\nlinear time series model, and nonlinear time series model. The evaluation is\nperformed using loop detector and probe vehicle data from California, Virginia,\nand Oregon. Among the benchmark models, the error corrected Grey Verhulst model\nwith Fourier outperformed the GM(1,1) model, linear time series, and non-linear\ntime series models. In turn, the three proposed models, GM(1,1$|cos(\\omega\nt)$), GM(1,1$|sin(\\omega t)$,$cos(\\omega t)$), and GM(1,1$|e^{-at}$,$sin(\\omega\nt)$,$cos(\\omega t)$), outperformed the Grey Verhulst model in prediction by at\nleast $65\\%$, $16\\%$, and $11\\%$, in terms of Root Mean Squared Error, and by\n$82\\%$, $58\\%$, and $42\\%$, in terms of Mean Absolute Percentage Error,\nrespectively. It is observed that the proposed Grey system models are more\nadaptive to location (e.g., perform well for all roadway types) and traffic\nparameters (e.g., speed, travel time, occupancy, and volume), and they do not\nrequire as many data points for training (4 observations are found to be\nsufficient).\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 18:55:03 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Comert", "Gurcan", ""], ["Begashaw", "Negash", ""], ["Huynh", "Nathan", ""]]}, {"id": "2011.09549", "submitter": "Thomas Faulkenberry", "authors": "Thomas J. Faulkenberry", "title": "The Pearson Bayes factor: An analytic formula for computing evidential\n  value from minimal summary statistics", "comments": "Accepted for publication in Biometrical Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In Bayesian hypothesis testing, evidence for a statistical model is\nquantified by the Bayes factor, which represents the relative likelihood of\nobserved data under that model compared to another competing model. In general,\ncomputing Bayes factors is difficult, as computing the marginal likelihood of\ndata under a given model requires integrating over a prior distribution of\nmodel parameters. In this paper, I capitalize on a particular choice of prior\ndistribution that allows the Bayes factor to be expressed without integral\nrepresentation and I develop a simple formula -- the Pearson Bayes factor --\nthat requires only minimal summary statistics commonly reported in scientific\npapers, such as the $t$ or $F$ score and the degrees of freedom. In addition to\npresenting this new result, I provide several examples of its use and report a\nsimulation study validating its performance. Importantly, the Pearson Bayes\nfactor gives applied researchers the ability to compute exact Bayes factors\nfrom minimal summary data, and thus easily assess the evidential value of any\ndata for which these summary statistics are provided, even when the original\ndata is not available.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 21:17:25 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2021 11:29:52 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Faulkenberry", "Thomas J.", ""]]}, {"id": "2011.09686", "submitter": "Shivshanker Singh Patel", "authors": "Anirban Ghatak, Shivshanker Singh Patel, Soham Bonnerjee, Subhrajyoty\n  Roy", "title": "A Generalized Epidemiological Model for COVID-19 with Dynamic and\n  Asymptomatic Population", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE physics.soc-ph stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we develop an extension of standard epidemiological models,\nsuitable for COVID-19. This extension incorporates the transmission due to\npre-symptomatic or asymptomatic carriers of the virus. Furthermore, this model\nalso captures the spread of the disease due to the movement of people to/from\ndifferent administrative boundaries within a country. The model describes the\nprobabilistic rise in the number of confirmed cases due to the concomitant\neffects of (incipient) human transmission and multiple compartments. The\nassociated parameters in the model can help architect the public health policy\nand operational management of the pandemic. For instance, this model\ndemonstrates that increasing the testing for symptomatic patients does not have\nany major effect on the progression of the pandemic, but testing rate of the\nasymptomatic population has an extremely crucial role to play. The model is\nexecuted using the data obtained for the state of Chhattisgarh in the Republic\nof India. The model is shown to have significantly better predictive capability\nthan the other epidemiological models. This model can be readily applied to any\nadministrative boundary (state or country). Moreover, this model can be applied\nfor any other epidemic as well.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 06:33:19 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Ghatak", "Anirban", ""], ["Patel", "Shivshanker Singh", ""], ["Bonnerjee", "Soham", ""], ["Roy", "Subhrajyoty", ""]]}, {"id": "2011.09707", "submitter": "Yizhou Qian", "authors": "Yizhou Qian, Mojtaba Forghani, Jonghyun Harry Lee, Matthew Farthing,\n  Tyler Hesser, Peter Kitanidis, Eric Darve", "title": "Application of Deep Learning-based Interpolation Methods to Nearshore\n  Bathymetry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nearshore bathymetry, the topography of the ocean floor in coastal zones, is\nvital for predicting the surf zone hydrodynamics and for route planning to\navoid subsurface features. Hence, it is increasingly important for a wide\nvariety of applications, including shipping operations, coastal management, and\nrisk assessment. However, direct high resolution surveys of nearshore\nbathymetry are rarely performed due to budget constraints and logistical\nrestrictions. Another option when only sparse observations are available is to\nuse Gaussian Process regression (GPR), also called Kriging. But GPR has\ndifficulties recognizing patterns with sharp gradients, like those found around\nsand bars and submerged objects, especially when observations are sparse. In\nthis work, we present several deep learning-based techniques to estimate\nnearshore bathymetry with sparse, multi-scale measurements. We propose a Deep\nNeural Network (DNN) to compute posterior estimates of the nearshore\nbathymetry, as well as a conditional Generative Adversarial Network (cGAN) that\nsamples from the posterior distribution. We train our neural networks based on\nsynthetic data generated from nearshore surveys provided by the U.S.\\ Army\nCorps of Engineer Field Research Facility (FRF) in Duck, North Carolina. We\ncompare our methods with Kriging on real surveys as well as surveys with\nartificially added sharp gradients. Results show that direct estimation by DNN\ngives better predictions than Kriging in this application. We use bootstrapping\nwith DNN for uncertainty quantification. We also propose a method, named\nDNN-Kriging, that combines deep learning with Kriging and shows further\nimprovement of the posterior estimates.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 08:22:00 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Qian", "Yizhou", ""], ["Forghani", "Mojtaba", ""], ["Lee", "Jonghyun Harry", ""], ["Farthing", "Matthew", ""], ["Hesser", "Tyler", ""], ["Kitanidis", "Peter", ""], ["Darve", "Eric", ""]]}, {"id": "2011.09718", "submitter": "Hyunjoo Yang", "authors": "Hyunjoo Yang and Jaeyong Lee", "title": "Variational Bayes method for ODE parameter estimation with application\n  to time-varying SIR model for COVID-19 epidemic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ordinary differential equation (ODE) is a mathematical model used in many\napplication areas such as climatology, bioinformatics, and chemical engineering\nwith its intuitive appeal to modeling. Despite ODE's wide usage in modeling,\nfrequent absence of their analytic solutions makes it difficult to estimate ODE\nparameters from the data, especially when the model has lots of variables and\nparameters. This paper proposes a Bayesian ODE parameter estimating algorithm\nwhich is fast and accurate even for models with many parameters. The proposed\nmethod approximates an ODE model with a state-space model based on equations of\na numeric solver. It allows fast estimation by avoiding computations of a whole\nnumerical solution in the likelihood. The posterior is obtained by a\nvariational Bayes method, more specifically, the approximate Riemannian\nconjugate gradient method (Honkela et al. 2010), which avoids samplings based\non Markov chain Monte Carlo (MCMC). In simulation studies, we compared the\nspeed and performance of the proposed method with existing methods. The\nproposed method showed the best performance in the reproduction of the true ODE\ncurve with strong stability as well as the fastest computation, especially in a\nlarge model with more than 30 parameters. As a real-world data application, a\nSIR model with time-varying parameters was fitted to the COVID-19 data. Taking\nadvantage of our proposed algorithm, 30 parameters were adequately fitted for\neach country.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 08:44:35 GMT"}, {"version": "v2", "created": "Sat, 6 Mar 2021 11:21:29 GMT"}, {"version": "v3", "created": "Wed, 7 Apr 2021 06:33:16 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Yang", "Hyunjoo", ""], ["Lee", "Jaeyong", ""]]}, {"id": "2011.09770", "submitter": "Nathan Wong", "authors": "Nathan Wong and Daehwan Kim and Zachery Robinson and Connie Huang and\n  Irina M. Conboy", "title": "Freecyto: Quantized Flow Cytometry Analysis for the Web", "comments": "for associated application, see https://freecyto.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Flow cytometry (FCM) is an analytic technique that is capable of detecting\nand recording the emission of fluorescence and light scattering of cells or\nparticles (that are collectively called \"events\") in a population. A typical\nFCM experiment can produce a large array of data making the analysis\ncomputationally intensive. Current FCM data analysis platforms (FlowJo, etc.),\nwhile very useful, do not allow interactive data processing online due to the\ndata size limitations. Here we report a more effective way to analyze FCM data.\nFreecyto is a free, easy-to-learn, Python-flask-based web application that uses\na weighted k-means clustering algorithm to facilitate the interactive analysis\nof flow cytometry data. A key limitation of web browsers is their inability to\ninteractively display large amounts of data. Freecyto addresses this bottleneck\nthrough the use of the k-means algorithm to quantize the data, allowing the\nuser to access a representative set of data points for interactive\nvisualization of complex datasets. Moreover, Freecyto enables the interactive\nanalyses of large complex datasets while preserving the standard FCM\nvisualization features, such as the generation of scatterplots (dotplots),\nhistograms, heatmaps, boxplots, as well as a SQL-based sub-population gating\nfeature. We also show that Freecyto can be applied to the analysis of various\nexperimental setups that frequently require the use of FCM. Finally, we\ndemonstrate that the data accuracy is preserved when Freecyto is compared to\nconventional FCM software.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 11:07:22 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Wong", "Nathan", ""], ["Kim", "Daehwan", ""], ["Robinson", "Zachery", ""], ["Huang", "Connie", ""], ["Conboy", "Irina M.", ""]]}, {"id": "2011.09815", "submitter": "Lijing Lin", "authors": "Lijing Lin, Matthew Sperrin, David A. Jenkins, Glen P. Martin, Niels\n  Peek", "title": "A scoping review of causal methods enabling predictions under\n  hypothetical interventions", "comments": null, "journal-ref": "Diagnostic and Prognostic Research, 2021", "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background and Aims: The methods with which prediction models are usually\ndeveloped mean that neither the parameters nor the predictions should be\ninterpreted causally. However, when prediction models are used to support\ndecision making, there is often a need for predicting outcomes under\nhypothetical interventions. We aimed to identify published methods for\ndeveloping and validating prediction models that enable risk estimation of\noutcomes under hypothetical interventions, utilizing causal inference: their\nmain methodological approaches, underlying assumptions, targeted estimands, and\npotential pitfalls and challenges with using the method, and unresolved\nmethodological challenges.\n  Methods: We systematically reviewed literature published by December 2019,\nconsidering papers in the health domain that used causal considerations to\nenable prediction models to be used for predictions under hypothetical\ninterventions.\n  Results: We identified 4919 papers through database searches and a further\n115 papers through manual searches, of which 13 were selected for inclusion,\nfrom both the statistical and the machine learning literature. Most of the\nidentified methods for causal inference from observational data were based on\nmarginal structural models and g-estimation.\n  Conclusions: There exist two broad methodological approaches for allowing\nprediction under hypothetical intervention into clinical prediction models: 1)\nenriching prediction models derived from observational studies with estimated\ncausal effects from clinical trials and meta-analyses; and 2) estimating\nprediction models and causal effects directly from observational data. These\nmethods require extending to dynamic treatment regimes, and consideration of\nmultiple interventions to operationalise a clinical decision support system.\nTechniques for validating 'causal prediction models' are still in their\ninfancy.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 13:36:26 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 15:31:04 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Lin", "Lijing", ""], ["Sperrin", "Matthew", ""], ["Jenkins", "David A.", ""], ["Martin", "Glen P.", ""], ["Peek", "Niels", ""]]}, {"id": "2011.09853", "submitter": "Hamed Majidifard", "authors": "Hamed Majidifard, Behnam Jahangiri, Punyaslok Rath, Amir H. Alavi,\n  William G. Buttlar", "title": "A Deep Learning Approach to Predict Hamburg Rutting Curve", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rutting continues to be one of the principal distresses in asphalt pavements\nworldwide. This type of distress is caused by permanent deformation and shear\nfailure of the asphalt mix under the repetition of heavy loads. The Hamburg\nwheel tracking test (HWTT) is a widely used testing procedure designed to\naccelerate, and to simulate the rutting phenomena in the laboratory. Rut depth,\nas one of the outputs of the HWTT, is dependent on a number of parameters\nrelated to mix design and testing conditions. This study introduces a new model\nfor predicting the rutting depth of asphalt mixtures using a deep learning\ntechnique - the convolution neural network (CNN). A database containing a\ncomprehensive collection of HWTT results was used to develop a CNN-based\nmachine learning prediction model. The database includes 10,000 rutting depth\ndata points measured across a large variety of asphalt mixtures. The model has\nbeen formulated in terms of known influencing mixture variables such as asphalt\nbinder high temperature performance grade, mixture type, aggregate size,\naggregate gradation, asphalt content, total asphalt binder recycling content,\nand testing parameters, including testing temperature and number of wheel\npasses. A rigorous validation process was used to assess the accuracy of the\nmodel to predict total rut depth and the HWTT rutting curve. A sensitivity\nanalysis is presented, which evaluates the effect of the investigated variables\non rutting depth predictions by the CNN model. The model can be used as a tool\nto estimate the rut depth in asphalt mixtures when laboratory testing is not\nfeasible, or for cost saving, pre-design trials.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 22:10:54 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Majidifard", "Hamed", ""], ["Jahangiri", "Behnam", ""], ["Rath", "Punyaslok", ""], ["Alavi", "Amir H.", ""], ["Buttlar", "William G.", ""]]}, {"id": "2011.09918", "submitter": "William Kleiber", "authors": "William Kleiber, Stephan Sain, Luke Madaus and Patrick Harr", "title": "Stochastic Tropical Cyclone Precipitation Field Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tropical cyclones are important drivers of coastal flooding which have severe\nnegative public safety and economic consequences. Due to the rare occurrence of\nsuch events, high spatial and temporal resolution historical storm\nprecipitation data are limited in availability. This paper introduces a\nstatistical tropical cyclone space-time precipitation generator given limited\ninformation from storm track datasets. Given a handful of predictor variables\nthat are common in either historical or simulated storm track ensembles such as\npressure deficit at the storm's center, radius of maximal winds, storm center\nand direction, and distance to coast, the proposed stochastic model generates\nspace-time fields of quantitative precipitation over the study domain.\nStatistically novel aspects include that the model is developed in Lagrangian\ncoordinates with respect to the dynamic storm center that uses ideas from\nlow-rank representations along with circular process models. The model is\ntrained on a set of tropical cyclone data from an advanced weather forecasting\nmodel over the Gulf of Mexico and southern United States, and is validated by\ncross-validation. Results show the model appropriately captures spatial\nasymmetry of cyclone precipitation patterns, total precipitation as well as the\nlocal distribution of precipitation at a set of case study locations along the\ncoast.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 15:52:13 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Kleiber", "William", ""], ["Sain", "Stephan", ""], ["Madaus", "Luke", ""], ["Harr", "Patrick", ""]]}, {"id": "2011.10020", "submitter": "Lin Yu", "authors": "L. Yu, Z. Lu, P. C. Nathan, S. Mostoufi-Moab, Y. Yuan", "title": "Modelling fertility potential in survivors of childhood cancer: An\n  introduction to modern statistical and computational methods", "comments": "15 pages, 9 figures and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical and computational methods are widely used in today's scientific\nstudies. Using a female fertility potential in childhood cancer survivors as an\nexample, we illustrate how these methods can be used to extract insight\nregarding biological processes from noisy observational data in order to inform\ndecision making. We start by contextualizing the computational methods with the\nworking example: the modelling of acute ovarian failure risk in female\nchildhood cancer survivors to quantify the risk of permanent ovarian failure\ndue to exposure to lifesaving but nonetheless toxic cancer treatments. This is\nfollowed by a description of the general framework of classification problems.\nWe provide an overview of the modelling algorithms employed in our example,\nincluding one classic model (logistic regression) and two popular modern\nlearning methods (random forest and support vector machines). Using the working\nexample, we show the general steps of data preparation for modelling, variable\nselection steps for the classic model, and how model performance might be\nimproved utilizing visualization tools. We end with a note on the importance of\nmodel evaluation.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 18:38:57 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Yu", "L.", ""], ["Lu", "Z.", ""], ["Nathan", "P. C.", ""], ["Mostoufi-Moab", "S.", ""], ["Yuan", "Y.", ""]]}, {"id": "2011.10086", "submitter": "Caitlyn Singam", "authors": "Caitlyn A. K. Singam, Jacob Haqq-Misra, Amedeo Balbi, Alexander M.\n  Sessa, Beatriz Villarroel, Gabriel G. De la Torre, David Grinspoon, Hikaru\n  Furukawa, Virisha Timmaraju", "title": "Evaluation of investigational paradigms for the discovery of\n  non-canonical astrophysical phenomena", "comments": "A product of the TechnoClimes 2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM astro-ph.EP physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-canonical phenomena - defined here as observables which are either\ninsufficiently characterized by existing theory, or otherwise represent\ninconsistencies with prior observations - are of burgeoning interest in the\nfield of astrophysics, particularly due to their relevance as potential signs\nof past and/or extant life in the universe (e.g. off-nominal spectroscopic data\nfrom exoplanets). However, an inherent challenge in investigating such\nphenomena is that, by definition, they do not conform to existing predictions,\nthereby making it difficult to constrain search parameters and develop an\nassociated falsifiable hypothesis.\n  In this Expert Recommendation, the authors evaluate the suitability of two\ndifferent approaches - conventional parameterized investigation (wherein\nexperimental design is tailored to optimally test a focused, explicitly\nparameterized hypothesis of interest) and the alternative approach of anomaly\nsearches (wherein broad-spectrum observational data is collected with the aim\nof searching for potential anomalies across a wide array of metrics) - in terms\nof their efficacy in achieving scientific objectives in this context. The\nauthors provide guidelines on the appropriate use-cases for each paradigm, and\ncontextualize the discussion through its applications to the interdisciplinary\nfield of technosignatures (a discipline at the intersection of astrophysics and\nastrobiology), which essentially specializes in searching for non-canonical\nastrophysical phenomena.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 20:00:51 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Singam", "Caitlyn A. K.", ""], ["Haqq-Misra", "Jacob", ""], ["Balbi", "Amedeo", ""], ["Sessa", "Alexander M.", ""], ["Villarroel", "Beatriz", ""], ["De la Torre", "Gabriel G.", ""], ["Grinspoon", "David", ""], ["Furukawa", "Hikaru", ""], ["Timmaraju", "Virisha", ""]]}, {"id": "2011.10398", "submitter": "Rowan Iskandar", "authors": "Rowan Iskandar", "title": "Probability bound analysis: A novel approach for quantifying parameter\n  uncertainty in decision-analytic modeling and cost-effectiveness analysis", "comments": "17 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decisions about health interventions are often made using limited evidence.\nMathematical models used to inform such decisions often include uncertainty\nanalysis to account for the effect of uncertainty in the current evidence-base\non decisional-relevant quantities. However, current uncertainty quantification\nmethodologies require modelers to specify a precise probability distribution to\nrepresent the uncertainty of a model parameter. This study introduces a novel\napproach for propagating parameter uncertainty, probability bounds analysis\n(PBA), where the uncertainty about the unknown probability distribution of a\nmodel parameter is expressed in terms of an interval bounded by lower and upper\nbounds on the cumulative distribution function (p-box) and without assuming a\nparticular form of the distribution function. We give the formulas of the\np-boxes for common situations (given combinations of data on minimum, maximum,\nmedian, mean, or standard deviation), describe an approach to propagate p-boxes\ninto a black-box mathematical model, and introduce an approach for\ndecision-making based on the results of PBA. Then, we demonstrate an\napplication of PBA using a case study. In sum, this study will provide modelers\nwith tools to conduct parameter uncertainty quantification given constraints of\navailable data with the fewest number of assumptions.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 13:28:30 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Iskandar", "Rowan", ""]]}, {"id": "2011.10661", "submitter": "Uwe Aickelin", "authors": "Ian Dent, Tony Craig, Uwe Aickelin, Tom Rodden", "title": "A method for evaluating options for motif detection in electricity meter\n  data", "comments": "Journal of Datascience, Volume 16, Issue 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Investigation of household electricity usage patterns, and matching the\npatterns to behaviours, is an important area of research given the centrality\nof such patterns in addressing the needs of the electricity industry.\nAdditional knowledge of household behaviours will allow more effective\ntargeting of demand side management (DSM) techniques.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 05:39:49 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Dent", "Ian", ""], ["Craig", "Tony", ""], ["Aickelin", "Uwe", ""], ["Rodden", "Tom", ""]]}, {"id": "2011.10718", "submitter": "Mohammad Javad Khojasteh", "authors": "Anshuka Rangi, Mohammad Javad Khojasteh and Massimo Franceschetti", "title": "Learning-based attacks in Cyber-Physical Systems: Exploration,\n  Detection, and Control Cost trade-offs", "comments": "To appear in L4DC 2021. First two authors contributed equally", "journal-ref": "Learning for Dynamics and Control 2021, PMLR", "doi": null, "report-no": null, "categories": "eess.SY cs.CR cs.LG cs.MA cs.SY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning-based attacks in linear systems, where the\ncommunication channel between the controller and the plant can be hijacked by a\nmalicious attacker. We assume the attacker learns the dynamics of the system\nfrom observations, then overrides the controller's actuation signal, while\nmimicking legitimate operation by providing fictitious sensor readings to the\ncontroller. On the other hand, the controller is on a lookout to detect the\npresence of the attacker and tries to enhance the detection performance by\ncarefully crafting its control signals. We study the trade-offs between the\ninformation acquired by the attacker from observations, the detection\ncapabilities of the controller, and the control cost. Specifically, we provide\ntight upper and lower bounds on the expected $\\epsilon$-deception time, namely\nthe time required by the controller to make a decision regarding the presence\nof an attacker with confidence at least $(1-\\epsilon\\log(1/\\epsilon))$. We then\nshow a probabilistic lower bound on the time that must be spent by the attacker\nlearning the system, in order for the controller to have a given expected\n$\\epsilon$-deception time. We show that this bound is also order optimal, in\nthe sense that if the attacker satisfies it, then there exists a learning\nalgorithm with the given order expected deception time. Finally, we show a\nlower bound on the expected energy expenditure required to guarantee detection\nwith confidence at least $1-\\epsilon \\log(1/\\epsilon)$.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 04:08:16 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 02:11:55 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Rangi", "Anshuka", ""], ["Khojasteh", "Mohammad Javad", ""], ["Franceschetti", "Massimo", ""]]}, {"id": "2011.10732", "submitter": "Andrea Gabrio", "authors": "Andrea Gabrio", "title": "A Bayesian framework for patient-level partitioned survival cost-utility\n  analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Patient-level health economic data collected alongside clinical trials are an\nimportant component of the process of technology appraisal, with a view to\ninforming resource allocation decisions. For end of life treatments, such as\ncancer treatments, modelling of cost-effectiveness/utility data may involve\nsome form of partitioned survival analysis, where measures of health-related\nquality of life and survival time for both pre- and post-progression periods\nare combined to generate some aggregate measure of clinical benefits (e.g.\nquality-adjusted survival). In addition, resource use data are often collected\nfrom health records on different services from which different cost components\nare obtained (e.g. treatment, hospital or adverse events costs). A critical\nproblem in these analyses is that both effectiveness and cost data present some\ncomplexities, including non-normality, spikes, and missingness, that should be\naddressed using appropriate methods. Bayesian modelling provides a powerful\ntool which has become more and more popular in the recent health economics and\nstatistical literature to jointly handle these issues in a relatively easy way.\nThis paper presents a general Bayesian framework that takes into account the\ncomplex relationships of trial-based partitioned survival cost-utility data,\npotentially providing a more adequate evidence for policymakers to inform the\ndecision-making process. Our approach is motivated by, and applied to, a\nworking example based on data from a trial assessing the cost-effectiveness of\na new treatment for patients with advanced non-small-cell lung cancer.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 06:43:23 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Gabrio", "Andrea", ""]]}, {"id": "2011.10861", "submitter": "Xiaowei Yue", "authors": "Cheolhei Lee, Jianguo Wu, Wenjia Wang, Xiaowei Yue", "title": "Neural Network Gaussian Process Considering Input Uncertainty for\n  Composite Structures Assembly", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing machine learning enabled smart manufacturing is promising for\ncomposite structures assembly process. To improve production quality and\nefficiency of the assembly process, accurate predictive analysis on dimensional\ndeviations and residual stress of the composite structures is required. The\nnovel composite structures assembly involves two challenges: (i) the highly\nnonlinear and anisotropic properties of composite materials; and (ii)\ninevitable uncertainty in the assembly process. To overcome those problems, we\npropose a neural network Gaussian process model considering input uncertainty\nfor composite structures assembly. Deep architecture of our model allows us to\napproximate a complex process better, and consideration of input uncertainty\nenables robust modeling with complete incorporation of the process uncertainty.\nBased on simulation and case study, the NNGPIU can outperform other benchmark\nmethods when the response function is nonsmooth and nonlinear. Although we use\ncomposite structure assembly as an example, the proposed methodology can be\napplicable to other engineering systems with intrinsic uncertainties.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 20:21:28 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Lee", "Cheolhei", ""], ["Wu", "Jianguo", ""], ["Wang", "Wenjia", ""], ["Yue", "Xiaowei", ""]]}, {"id": "2011.10863", "submitter": "Mengyang Gu", "authors": "Mengyang Gu and Hanmo Li", "title": "Gaussian orthogonal latent factor processes for large incomplete\n  matrices of correlated data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Gaussian orthogonal latent factor processes for modeling and\npredicting large correlated data. To handle the computational challenge, we\nfirst decompose the likelihood function of the Gaussian random field with a\nmulti-dimensional input domain into a product of densities at the orthogonal\ncomponents with lower-dimensional inputs. The continuous-time Kalman filter is\nimplemented to compute the likelihood function efficiently without making\napproximations. We also show that the posterior distribution of the factor\nprocesses is independent, as a consequence of prior independence of factor\nprocesses and orthogonal factor loading matrix. For studies with large sample\nsizes, we propose a flexible way to model the mean, and we derive the marginal\nposterior distribution to solve identifiability issues in sampling these\nparameters. Both simulated and real data applications confirm the outstanding\nperformance of this method.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 20:39:38 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 19:28:38 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Gu", "Mengyang", ""], ["Li", "Hanmo", ""]]}, {"id": "2011.11023", "submitter": "Silvia Noirjean", "authors": "Silvia Noirjean, Marco Mariani, Alessandra Mattei, Fabrizia Mealli", "title": "Exploiting network information to disentangle spillover effects in a\n  field experiment on teens' museum attendance", "comments": "Original article, 36 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nudging youths to visit historical and artistic heritage is a key goal\npursued by cultural organizations. The field experiment we analyze is a\nclustered encouragement design (CED) conducted in Florence (Italy) and devised\nto assess how appropriate incentives assigned to high-school classes may induce\nteens to visit museums in their free time. In CEDs, where the focus is on\ncausal effects for individuals, interference between units is generally\nunavoidable. The presence of noncompliance and spillover effects makes causal\ninference particularly challenging. We propose to deal with these complications\nby creatively blending the principal stratification framework and causal\nmediation methods, and exploiting information on interpersonal networks. We\nformally define principal natural direct and indirect effects and principal\ncontrolled direct and indirect effects, and use them to disentangle spillovers\nfrom other causal channels. The key insights are that overall principal causal\neffects for sub-populations of units defined by the compliance behavior combine\nencouragement, treatment and spillovers effects. In this situation, a synthesis\nof the network information may be used as a possible mediator, such that the\npart of the effect that is channeled by it can be attributed to spillovers. A\nBayesian approach is used for inference, invoking latent ignorability\nassumptions on the mediator conditional on principal stratum membership.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 14:30:02 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Noirjean", "Silvia", ""], ["Mariani", "Marco", ""], ["Mattei", "Alessandra", ""], ["Mealli", "Fabrizia", ""]]}, {"id": "2011.11031", "submitter": "Chun Wang", "authors": "Martin B. Haugh (1) and Chun Wang (2) ((1) Department of Analytics,\n  Marketing & Operations, Imperial College Business School, Imperial College,\n  (2) Department of Management Science and Engineering, School of Economics and\n  Management, Tsinghua University)", "title": "Play Like the Pros? Solving the Game of Darts as a Dynamic Zero-Sum Game", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The game of darts has enjoyed great growth over the past decade with the\nperception of darts moving from that of a pub game to a game that is regularly\nscheduled on prime-time television in many countries including the U.K.,\nGermany, the Netherlands and Australia among others. In this paper we analyze a\nnovel data-set on sixteen of the top professional darts players in the world\nduring the 2019 season. We use this data-set to fit skill-models to the players\nand use the fitted models to understand the variation in skills across these\nplayers. We then formulate and solve the dynamic zero-sum-games (ZSG's) that\ndarts players face and to the best of our knowledge we are the first to do so.\nUsing the fitted skill models and our ZSG problem formulation we quantify the\nimportance of playing strategically in darts. We are also able to analyze\ninteresting specific game situations including some real-world situations that\nhave been the subject of some debate among darts fans and experts.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 14:57:53 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Haugh", "Martin B.", ""], ["Wang", "Chun", ""]]}, {"id": "2011.11047", "submitter": "Jeffrey Doser", "authors": "Jeffrey W. Doser, Andrew O. Finley, Aaron S. Weed, Elise F. Zipkin", "title": "Integrating automated acoustic vocalization data and point count surveys\n  for estimation of bird abundance", "comments": "25 pages, 4 figures", "journal-ref": "Methods in Ecology and Evolution (2021)", "doi": "10.1111/2041-210X.13578", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Monitoring wildlife abundance across space and time is an essential task to\nstudy their population dynamics and inform effective management. Acoustic\nrecording units are a promising technology for efficiently monitoring bird\npopulations and communities. We present an integrated modeling framework that\ncombines high-quality but temporally sparse bird point count survey data with\nacoustic recordings. Using simulations, we compare the accuracy and precision\nof abundance estimates using differing amounts of acoustic vocalizations\nobtained from a clustering algorithm, point count data, and a subset of\nmanually validated acoustic vocalizations. We also use our modeling framework\nin a case study to estimate abundance of the Eastern Wood-Pewee (Contopus\nvirens) in Vermont, U.S.A. The simulation study reveals that combining acoustic\nand point count data via an integrated model improves accuracy and precision of\nabundance estimates compared with models informed by either acoustic or point\ncount data alone. Combining acoustic data with only a small number of point\ncount surveys yields estimates of abundance without the need for validating any\nof the identified vocalizations from the acoustic data. Within our case study,\nthe integrated models provided moderate support for a decline of the Eastern\nWood-Pewee in this region. Our integrated modeling approach combines dense\nacoustic data with few point count surveys to deliver reliable estimates of\nspecies abundance without the need for manual identification of acoustic\nvocalizations or a prohibitively expensive large number of repeated point count\nsurveys. Our proposed approach offers an efficient monitoring alternative for\nlarge spatio-temporal regions when point count data are difficult to obtain or\nwhen monitoring is focused on rare species with low detection probability.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 16:02:27 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 10:37:50 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Doser", "Jeffrey W.", ""], ["Finley", "Andrew O.", ""], ["Weed", "Aaron S.", ""], ["Zipkin", "Elise F.", ""]]}, {"id": "2011.11109", "submitter": "Luca Dede'", "authors": "Nicola Parolini, Giovanni Ardenghi, Luca Dede', Alfio Quarteroni", "title": "A Mathematical Dashboard for the Analysis of Italian COVID-19 Epidemic\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.NA math.NA stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An analysis of the COVID-19 epidemic is proposed on the basis of the epiMOX\ndashboard (publicly accessible at https://www.epimox.polimi.it) that deals with\ndata of the epidemic trends and outbreaks in Italy from late February 2020. Our\nanalysis provides an immediate appreciation of the past epidemic development,\ntogether with its current trends by fostering a deeper interpretation of\navailable data through several critical epidemic indicators. In addition, we\ncomplement the epiMOX dashboard with a predictive tool based on an\nepidemiological compartmental model, named SUIHTER, for the forecast on the\nnear future epidemic evolution.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 21:27:21 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 15:56:36 GMT"}, {"version": "v3", "created": "Mon, 15 Mar 2021 11:20:12 GMT"}, {"version": "v4", "created": "Mon, 24 May 2021 12:28:34 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Parolini", "Nicola", ""], ["Ardenghi", "Giovanni", ""], ["Dede'", "Luca", ""], ["Quarteroni", "Alfio", ""]]}, {"id": "2011.11177", "submitter": "Paul Roediger", "authors": "Paul A. Roediger", "title": "Gonogo: An R Implementation of Test Methods to Perform, Analyze and\n  Simulate Sensitivity Experiments", "comments": "This documentation is 58 pages in length and contains 31 figures, 40\n  tables and 2 flow diagrams. The subject of much of the paper, the gonogo.R\n  file, contains 118 functions plus 2 constants and is available online", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME stat.ML stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work provides documentation for a suite of R functions contained in\ngonogo.R. The functions provide sensitivity testing practitioners and\nresearchers with an ability to conduct, analyze and simulate various\nsensitivity experiments involving binary responses and a single stimulus level\n(e.g., drug dosage, drop height, velocity, etc.). Included are the modern Neyer\nand 3pod adaptive procedures, as well as the Bruceton and Langlie. The latter\ntwo benchmark procedures are capable of being performed according to\ngeneralized up-down transformed-response rules. Each procedure is designated\nphase-one of a three-phase experiment. The goal of phase-one is to achieve\noverlapping data. The two additional (and optional) refinement phases utilize\nthe D-optimal criteria and the Robbins-Monro-Joseph procedure. The goals of the\ntwo refinement phases are to situate testing in the vicinity of the median and\ntails of the latent response distribution, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 02:28:29 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Roediger", "Paul A.", ""]]}, {"id": "2011.11178", "submitter": "Fan Yin", "authors": "Fan Yin, Jieying Jiao, Guanyu Hu, Jun Yan", "title": "Bayesian Nonparametric Estimation for Point Processes with Spatial\n  Homogeneity: A Spatial Analysis of NBA Shot Locations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Basketball shot location data provide valuable summary information regarding\nplayers to coaches, sports analysts, fans, statisticians, as well as players\nthemselves. Represented by spatial points, such data are naturally analyzed\nwith spatial point process models. We present a novel nonparametric Bayesian\nmethod for learning the underlying intensity surface built upon a combination\nof Dirichlet process and Markov random field. Our method has the advantage of\neffectively encouraging local spatial homogeneity when estimating a globally\nheterogeneous intensity surface. Posterior inferences are performed with an\nefficient Markov chain Monte Carlo (MCMC) algorithm. Simulation studies show\nthat the inferences are accurate and that the method is superior compared to\nthe competing methods. Application to the shot location data of $20$\nrepresentative NBA players in the 2017-2018 regular season offers interesting\ninsights about the shooting patterns of these players. A comparison against the\ncompeting method shows that the proposed method can effectively incorporate\nspatial contiguity into the estimation of intensity surfaces.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 02:32:49 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Yin", "Fan", ""], ["Jiao", "Jieying", ""], ["Hu", "Guanyu", ""], ["Yan", "Jun", ""]]}, {"id": "2011.11186", "submitter": "Ziliang Zhong", "authors": "Ziliang Zhong, Muhang Zheng, Huafeng Mai, Jianan Zhao, Xinyi Liu", "title": "Cancer image classification based on DenseNet model", "comments": null, "journal-ref": "2004-present Journal of Physics: Conference Series", "doi": "10.1088/1742-6596/1651/1/012143", "report-no": null, "categories": "cs.CV stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computer-aided diagnosis establishes methods for robust assessment of medical\nimage-based examination. Image processing introduced a promising strategy to\nfacilitate disease classification and detection while diminishing unnecessary\nexpenses. In this paper, we propose a novel metastatic cancer image\nclassification model based on DenseNet Block, which can effectively identify\nmetastatic cancer in small image patches taken from larger digital pathology\nscans. We evaluate the proposed approach to the slightly modified version of\nthe PatchCamelyon (PCam) benchmark dataset. The dataset is the slightly\nmodified version of the PatchCamelyon (PCam) benchmark dataset provided by\nKaggle competition, which packs the clinically-relevant task of metastasis\ndetection into a straight-forward binary image classification task. The\nexperiments indicated that our model outperformed other classical methods like\nResnet34, Vgg19. Moreover, we also conducted data augmentation experiment and\nstudy the relationship between Batches processed and loss value during the\ntraining and validation process.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 03:05:42 GMT"}], "update_date": "2020-12-30", "authors_parsed": [["Zhong", "Ziliang", ""], ["Zheng", "Muhang", ""], ["Mai", "Huafeng", ""], ["Zhao", "Jianan", ""], ["Liu", "Xinyi", ""]]}, {"id": "2011.11354", "submitter": "Manmeet Singh", "authors": "Manmeet Singh, Tanuj Chopra", "title": "Use of Computer Applications for Determining the Best Possible Runway\n  Orientation using Wind Rose Diagrams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As technology advances, there are more uses of computer applications in a\nCivil Engineering Projects. To simplify the work, minimize errors, and obviate\ncomputational time required in designing and other structural analysis work,\nsoftware which give accurate results to the given inputs are the need of the\nhour and this paper highlights the simulation of a manual procedure to find out\nthe orientation of a Runway from the Wind Data available. A Computer\napplication in Java on Net beans platform has been used to develop this Wind\nRose Software. This software has been developed using the concepts of Wind Rose\nDiagram Type-II which will calculate the best possible orientation of the\nRunway along with the Coverage.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 12:14:16 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Singh", "Manmeet", ""], ["Chopra", "Tanuj", ""]]}, {"id": "2011.11483", "submitter": "Vik Shirvaikar", "authors": "Vik Shirvaikar and Choudur Lakshminarayan", "title": "Social Determinants of Recidivism: A Machine Learning Solution", "comments": "12 main pages, 5 appendix pages, 12 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In criminal justice analytics, the widely-studied problem of recidivism\nprediction (forecasting re-offenses after release or parole) is fraught with\nethical missteps. In particular, Machine Learning (ML) models rely on\nhistorical patterns of behavior to predict future outcomes, engendering a\nvicious feedback loop of recidivism and incarceration. This paper repurposes ML\nto instead identify social factors that can serve as levers to prevent\nrecidivism. Our contributions are along three dimensions. (1) Recidivism models\ntypically agglomerate individuals into one dataset, but we invoke unsupervised\nlearning to extract homogeneous subgroups with similar features. (2) We then\napply subgroup-level supervised learning to determine factors correlated to\nrecidivism. (3) We therefore shift the focus from predicting which individuals\nwill re-offend to identifying broader underlying factors that explain\nrecidivism, with the goal of informing preventative policy intervention. We\ndemonstrate that this approach can guide the ethical application of ML using\nreal-world data.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 04:15:41 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2020 18:25:26 GMT"}, {"version": "v3", "created": "Sun, 9 May 2021 18:43:01 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Shirvaikar", "Vik", ""], ["Lakshminarayan", "Choudur", ""]]}, {"id": "2011.11555", "submitter": "Cansu Alakus", "authors": "Cansu Alakus, Denis Larocque, Sebastien Jacquemont, Fanny Barlaam,\n  Charles-Olivier Martin, Kristian Agbogba, Sarah Lippe, Aurelie Labbe", "title": "Conditional canonical correlation estimation based on covariates with\n  random forests", "comments": "27 pages, 8 figures, 1 table", "journal-ref": null, "doi": "10.1093/bioinformatics/btab158", "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Investigating the relationships between two sets of variables helps to\nunderstand their interactions and can be done with canonical correlation\nanalysis (CCA). However, the correlation between the two sets can sometimes\ndepend on a third set of covariates, often subject-related ones such as age,\ngender, or other clinical measures. In this case, applying CCA to the whole\npopulation is not optimal and methods to estimate conditional CCA, given the\ncovariates, can be useful. We propose a new method called Random Forest with\nCanonical Correlation Analysis (RFCCA) to estimate the conditional canonical\ncorrelations between two sets of variables given subject-related covariates.\nThe individual trees in the forest are built with a splitting rule specifically\ndesigned to partition the data to maximize the canonical correlation\nheterogeneity between child nodes. We also propose a significance test to\ndetect the global effect of the covariates on the relationship between two sets\nof variables. The performance of the proposed method and the global\nsignificance test is evaluated through simulation studies that show it provides\naccurate canonical correlation estimations and well-controlled Type-1 error. We\nalso show an application of the proposed method with EEG data.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 17:09:46 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 22:55:03 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Alakus", "Cansu", ""], ["Larocque", "Denis", ""], ["Jacquemont", "Sebastien", ""], ["Barlaam", "Fanny", ""], ["Martin", "Charles-Olivier", ""], ["Agbogba", "Kristian", ""], ["Lippe", "Sarah", ""], ["Labbe", "Aurelie", ""]]}, {"id": "2011.11558", "submitter": "Jos\\'e Antonio Perusqu\\'ia Cort\\'es", "authors": "Jos\\'e A. Perusqu\\'ia and Jim E. Griffin and Cristiano Villa", "title": "On a Bayesian Approach to Malware Detection and Classification through\n  $n$-gram Profiles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting and correctly classifying malicious executables has become one of\nthe major concerns in cyber security, especially because traditional detection\nsystems have become less effective with the increasing number and danger of\nthreats found nowadays. One way to differentiate benign from malicious\nexecutables is to leverage on their hexadecimal representation by creating a\nset of binary features that completely characterise each executable. In this\npaper we present a novel supervised learning Bayesian nonparametric approach\nfor binary matrices, that provides an effective probabilistic approach for\nmalware detection. Moreover, and due to the model's flexible assumptions, we\nare able to use it in a multi-class framework where the interest relies in\nclassifying malware into known families. Finally, a generalisation of the model\nwhich provides a deeper understanding of the behaviour across groups for each\nfeature is also developed.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 17:12:34 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Perusqu\u00eda", "Jos\u00e9 A.", ""], ["Griffin", "Jim E.", ""], ["Villa", "Cristiano", ""]]}, {"id": "2011.11583", "submitter": "Geoffrey Johnson", "authors": "Geoffrey S Johnson", "title": "Tolerance and Prediction Intervals for Non-normal Models", "comments": "Clinical Trial Recruitment, Time on Treatment, Probability of\n  Success, Prediction Interval, Tolerance Interval", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A prediction interval covers a future observation from a random process in\nrepeated sampling, and is typically constructed by identifying a pivotal\nquantity that is also an ancillary statistic. Analogously, a tolerance interval\ncovers a population percentile in repeated sampling and is often based on a\npivotal quantity. One approach we consider in non-normal models leverages a\nlink function resulting in a pivotal quantity that is approximately normally\ndistributed. In settings where this normal approximation does not hold we\nconsider a second approach for tolerance and prediction based on a confidence\ninterval for the mean. These methods are intuitive, simple to implement, have\nproper operating characteristics, and are computationally efficient compared to\nBayesian, re-sampling, and machine learning methods. This is demonstrated in\nthe context of multi-site clinical trial recruitment with staggered site\ninitiation, real-world time on treatment, and end-of-study success for a\nclinical endpoint.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 17:48:09 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 15:07:05 GMT"}, {"version": "v3", "created": "Fri, 12 Mar 2021 00:34:36 GMT"}, {"version": "v4", "created": "Mon, 7 Jun 2021 18:43:56 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Johnson", "Geoffrey S", ""]]}, {"id": "2011.11636", "submitter": "Chun Yui Wong", "authors": "Chun Yui Wong, Pranay Seshadri, Ashley Scillitoe, Andrew B. Duncan,\n  Geoffrey Parks", "title": "Blade Envelopes Part I: Concept and Methodology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Blades manufactured through flank and point milling will likely exhibit\ngeometric variability. Gauging the aerodynamic repercussions of such\nvariability, prior to manufacturing a component, is challenging enough, let\nalone trying to predict what the amplified impact of any in-service degradation\nwill be. While rules of thumb that govern the tolerance band can be devised\nbased on expected boundary layer characteristics at known regions and levels of\ndegradation, it remains a challenge to translate these insights into\nquantitative bounds for manufacturing. In this work, we tackle this challenge\nby leveraging ideas from dimension reduction to construct low-dimensional\nrepresentations of aerodynamic performance metrics. These low-dimensional\nmodels can identify a subspace which contains designs that are invariant in\nperformance -- the inactive subspace. By sampling within this subspace, we\ndesign techniques for drafting manufacturing tolerances and for quantifying\nwhether a scanned component should be used or scrapped. We introduce the blade\nenvelope as a visual and computational manufacturing guide for a blade. In this\npaper, the first of two parts, we discuss its underlying concept and detail its\ncomputational methodology, assuming one is interested only in the single\nobjective of ensuring that the loss of all manufactured blades remains\nconstant. To demonstrate the utility of our ideas we devise a series of\ncomputational experiments with the Von Karman Institute's LS89 turbine blade.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 11:34:59 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 17:21:51 GMT"}, {"version": "v3", "created": "Fri, 25 Dec 2020 14:39:52 GMT"}, {"version": "v4", "created": "Tue, 5 Jan 2021 15:03:16 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Wong", "Chun Yui", ""], ["Seshadri", "Pranay", ""], ["Scillitoe", "Ashley", ""], ["Duncan", "Andrew B.", ""], ["Parks", "Geoffrey", ""]]}, {"id": "2011.11691", "submitter": "Connor Gibbs", "authors": "Connor Gibbs and Ryan Elmore and Bailey Fosdick", "title": "The causal effect of a timeout at stopping an opposing run in the NBA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the summer of 2017, the National Basketball Association reduced the number\nof total timeouts, along with other rule changes, to regulate the flow of the\ngame. With these rule changes, it becomes increasingly important for coaches to\neffectively manage their timeouts. Understanding the utility of a timeout under\nvarious game scenarios, e.g., during an opposing team's run, is of the utmost\nimportance. There are two schools of thought when the opposition is on a run:\n(1) call a timeout and allow your team to rest and regroup, or (2) save a\ntimeout and hope your team can make corrections during play. This paper\ninvestigates the credence of these tenets using the Rubin causal model\nframework to quantify the causal effect of a timeout in the presence of an\nopposing team's run. Too often overlooked, we carefully consider the stable\nunit-treatment-value assumption (SUTVA) in this context and use SUTVA to\nmotivate our definition of units. To measure the effect of a timeout, we\nintroduce a novel, interpretable outcome based on the score difference to\ndescribe broad changes in the scoring dynamics. This outcome is well-suited for\nsituations where the quantity of interest fluctuates frequently, a commonality\nin many sports analytics applications. We conclude from our analysis that while\ncomebacks frequently occur after a run, it is slightly disadvantageous to call\na timeout during a run by the opposing team and further demonstrate that the\nmagnitude of this effect varies by franchise.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 19:42:42 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 20:53:11 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Gibbs", "Connor", ""], ["Elmore", "Ryan", ""], ["Fosdick", "Bailey", ""]]}, {"id": "2011.11740", "submitter": "Charilaos Mylonas Mr.", "authors": "Charilaos Mylonas and Eleni Chatzi", "title": "Remaining Useful Life Estimation Under Uncertainty with Causal GraphNets", "comments": "A preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, a novel approach for the construction and training of time\nseries models is presented that deals with the problem of learning on large\ntime series with non-equispaced observations, which at the same time may\npossess features of interest that span multiple scales. The proposed method is\nappropriate for constructing predictive models for non-stationary stochastic\ntime series.The efficacy of the method is demonstrated on a simulated\nstochastic degradation dataset and on a real-world accelerated life testing\ndataset for ball-bearings. The proposed method, which is based on GraphNets,\nimplicitly learns a model that describes the evolution of the system at the\nlevel of a state-vector rather than of a raw observation. The proposed approach\nis compared to a recurrent network with a temporal convolutional feature\nextractor head (RNN-tCNN) which forms a known viable alternative for the\nproblem context considered. Finally, by taking advantage of recent advances in\nthe computation of reparametrization gradients for learning probability\ndistributions, a simple yet effective technique for representing prediction\nuncertainty as a Gamma distribution over remaining useful life predictions is\nemployed.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 21:28:03 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Mylonas", "Charilaos", ""], ["Chatzi", "Eleni", ""]]}, {"id": "2011.11771", "submitter": "Camila Olarte Parra", "authors": "Camila Olarte Parra, Ingeborg Waernbaum, Staffan Sch\\\"on and Els\n  Goetghebeur", "title": "Trial emulation and survival analysis for disease incidence registers: a\n  case study on the causal effect of pre-emptive kidney transplantation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous tutorials and research papers focus on methods in either survival\nanalysis or causal inference, leaving common complications in medical studies\nunaddressed. In practice one must handle problems jointly, without the luxury\nof ignoring essential features of the data structure. In this paper, we follow\nincident cases of end-stage renal disease and examine the effect on all-cause\nmortality of starting treatment with transplant, so-called pre-emptive kidney\ntransplantation, versus dialysis. The question is relatively simple: which\ntreatment start is expected to bring the best survival for a target population?\nTo address the question, we emulate a target trial drawing on the Swedish Renal\nRegistry to estimate a causal effect on survival curves. Aware of important\nchallenges, we see how previous studies have selected patients into treatment\ngroups based on events occurring post treatment initiation. Our study reveals\nthe dramatic impact of resulting immortal time bias and other typical features\nof long term incident disease registries, including: missing or mismeasured\ncovariates during (the early) phases of the register, varying risk profile of\npatients entering treatment groups over calendar time and changes in risk as\ncare improves over the years. With characteristics of cases and versions of\ntreatment evolving over time, informative censoring is introduced in unadjusted\nKaplan-Meier curves and also their IPW version is no longer valid. Here we\ndiscuss feasible ways of handling these features and answer different research\nquestions relying on the no unmeasured baseline confounders assumption.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 22:29:37 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Parra", "Camila Olarte", ""], ["Waernbaum", "Ingeborg", ""], ["Sch\u00f6n", "Staffan", ""], ["Goetghebeur", "Els", ""]]}, {"id": "2011.11780", "submitter": "Anindya Bhaduri", "authors": "Anindya Bhaduri, Christopher S. Meyer, John W. Gillespie Jr., Bazle Z.\n  Haque, Michael D. Shields, Lori Graham-Brady", "title": "Probabilistic modeling of discrete structural response with application\n  to composite plate penetration models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete response of structures is often a key probabilistic quantity of\ninterest. For example, one may need to identify the probability of a binary\nevent, such as, whether a structure has buckled or not. In this study, an\nadaptive domain-based decomposition and classification method, combined with\nsparse grid sampling, is used to develop an efficient classification surrogate\nmodeling algorithm for such discrete outputs. An assumption of monotonic\nbehaviour of the output with respect to all model parameters, based on the\nphysics of the problem, helps to reduce the number of model evaluations and\nmakes the algorithm more efficient. As an application problem, this paper deals\nwith the development of a computational framework for generation of\nprobabilistic penetration response of S-2 glass/SC-15 epoxy composite plates\nunder ballistic impact. This enables the computationally feasible generation of\nthe probabilistic velocity response (PVR) curve or the $V_0-V_{100}$ curve as a\nfunction of the impact velocity, and the ballistic limit velocity prediction as\na function of the model parameters. The PVR curve incorporates the variability\nof the model input parameters and describes the probability of penetration of\nthe plate as a function of impact velocity.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 22:45:09 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Bhaduri", "Anindya", ""], ["Meyer", "Christopher S.", ""], ["Gillespie", "John W.", "Jr."], ["Haque", "Bazle Z.", ""], ["Shields", "Michael D.", ""], ["Graham-Brady", "Lori", ""]]}, {"id": "2011.11820", "submitter": "Benjamin Guedj", "authors": "Florent Dewez and Benjamin Guedj and Arthur Talpaert and Vincent\n  Vandewalle", "title": "An end-to-end data-driven optimisation framework for constrained\n  trajectories", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG math.OC stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Many real-world problems require to optimise trajectories under constraints.\nClassical approaches are based on optimal control methods but require an exact\nknowledge of the underlying dynamics, which could be challenging or even out of\nreach. In this paper, we leverage data-driven approaches to design a new\nend-to-end framework which is dynamics-free for optimised and realistic\ntrajectories. We first decompose the trajectories on function basis, trading\nthe initial infinite dimension problem on a multivariate functional space for a\nparameter optimisation problem. A maximum \\emph{a posteriori} approach which\nincorporates information from data is used to obtain a new optimisation problem\nwhich is regularised. The penalised term focuses the search on a region\ncentered on data and includes estimated linear constraints in the problem. We\napply our data-driven approach to two settings in aeronautics and sailing\nroutes optimisation, yielding commanding results. The developed approach has\nbeen implemented in the Python library PyRotor.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 00:54:17 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 09:24:54 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Dewez", "Florent", ""], ["Guedj", "Benjamin", ""], ["Talpaert", "Arthur", ""], ["Vandewalle", "Vincent", ""]]}, {"id": "2011.11866", "submitter": "Gurcan Comert", "authors": "Gurcan Comert", "title": "Gaussian Processes for Traffic Speed Prediction at Different Aggregation\n  Levels", "comments": "13 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dynamic behavior of traffic adversely affect the performance of the\nprediction models in intelligent transportation applications. This study\napplies Gaussian processes (GPs) to traffic speed prediction. Such predictions\ncan be used by various transportation applications, such as real-time route\nguidance, ramp metering, congestion pricing and special events traffic\nmanagement. One-step predictions with various aggregation levels (1 to\n60-minute) are tested for performance of the generated models. Univariate and\nmultivariate GPs are compared with several other linear, nonlinear time series,\nand Grey system models using loop and Inrix probe vehicle datasets from\nCalifornia, Portland, and Virginia freeways respectively. Based on the test\ndata samples, results are promising that GP models are able to consistently\noutperform compared models with similar computational times.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 03:05:01 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Comert", "Gurcan", ""]]}, {"id": "2011.11874", "submitter": "Sarah Reifeis", "authors": "Sarah A. Reifeis and Michael G. Hudgens", "title": "On variance of the treatment effect in the treated using inverse\n  probability weighting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the analysis of observational studies, inverse probability weighting (IPW)\nis commonly used to consistently estimate the average treatment effect (ATE) or\nthe average treatment effect in the treated (ATT). The variance of the IPW ATE\nestimator is often estimated by assuming the weights are known and then using\nthe so-called \"robust\" (Huber-White) sandwich estimator, which results in\nconservative standard error (SE) estimation. Here it is shown that using such\nan approach when estimating the variance of the IPW ATT estimator does not\nnecessarily result in conservative SE estimates. That is, assuming the weights\nare known, the robust sandwich estimator may be conservative or\nanti-conservative. Thus confidence intervals of the ATT using the robust SE\nestimate will not be valid in general. Instead, stacked estimating equations\nwhich account for the weight estimation can be used to compute a consistent,\nclosed-form variance estimator for the IPW ATT estimator. The two variance\nestimators are compared via simulation studies and in a data analysis of the\neffect of smoking on gene expression.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 03:29:59 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Reifeis", "Sarah A.", ""], ["Hudgens", "Michael G.", ""]]}, {"id": "2011.11939", "submitter": "Uri Keich", "authors": "Dong Luo, Yilun He, Kristen Emery, William Stafford Noble, Uri Keich", "title": "Competition-based control of the false discovery proportion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, Barber and Cand\\`es laid the theoretical foundation for a general\nframework for false discovery rate (FDR) control based on the notion of\n\"knockoffs.\" A closely related FDR control methodology has long been employed\nin the analysis of mass spectrometry data, referred to there as \"target-decoy\ncompetition\" (TDC). However, any approach that aims to control the FDR, which\nis defined as the expected value of the false discovery proportion (FDP),\nsuffers from a problem. Specifically, even when successfully controlling the\nFDR at level {\\alpha}, the FDP in the list of discoveries can significantly\nexceed {\\alpha}. We offer two new procedures to address this problem, both of\nwhich are compatible with the knockoff framework or TDC. FDP-SD rigorously\ncontrols the FDP in the competition setup by guaranteeing that the FDP is\nbounded by {\\alpha} at any desired confidence level. The complementary TDC-UB\nis designed to bound the FDP for any list of top-scoring target discoveries.\nCompared with the just-published general framework of Katsevich and Ramdas, our\nproposed procedures generally offer more power (FDP-SD) and tighter bounds\n(TDC-UB) and often substantially so in simulated as well as real data.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 07:31:48 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 14:02:27 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Luo", "Dong", ""], ["He", "Yilun", ""], ["Emery", "Kristen", ""], ["Noble", "William Stafford", ""], ["Keich", "Uri", ""]]}, {"id": "2011.12044", "submitter": "Davide Risso", "authors": "Thi Kim Hue Nguyen, Koen Van den Berge, Monica Chiogna, Davide Risso", "title": "Structure learning for zero-inflated counts, with an application to\n  single-cell RNA sequencing data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The problem of estimating the structure of a graph from observed data is of\ngrowing interest in the context of high-throughput genomic data, and\nsingle-cell RNA sequencing in particular. These, however, are challenging\napplications, since the data consist of high-dimensional counts with high\nvariance and over-abundance of zeros. Here, we present a general framework for\nlearning the structure of a graph from single-cell RNA-seq data, based on the\nzero-inflated negative binomial distribution. We demonstrate with simulations\nthat our approach is able to retrieve the structure of a graph in a variety of\nsettings and we show the utility of the approach on real data.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 11:37:48 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Nguyen", "Thi Kim Hue", ""], ["Berge", "Koen Van den", ""], ["Chiogna", "Monica", ""], ["Risso", "Davide", ""]]}, {"id": "2011.12161", "submitter": "Yixiao Lu", "authors": "Jie Ni, Jiayi Qian, Yixiao Lu, Hong Cheng", "title": "The thermal power generation and economic growth in the central and\n  western China: A heterogeneous mixed panel Granger-Causality approach", "comments": "5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The problem of the new energy economy has become a global hot issue. This\nstudy examines the causal relationship between the ratio of thermal power in\ntotal power generation (RTPG) and economic growth (GDP) in the western and\ncentral China by using the heterogeneous mixed panel Granger causality approach\nthat accounts for both slope heterogeneity and cross-sectional dependence. For\nthe overall panel, the empirical findings support the presence of\nunidirectional causality running from GDP to RTPG (in northwest China), and\nfrom RTPG to GDP (in central). At the provincial level, there is causality from\nGDP to RTPG in NeiMongol and Ningxia, and causality from RTPG to GDP in Shanxi,\nAnhui, and Jiangxi. As for the cross regions relationships, we find that GDP\n(in western) Granger-cause RTPG (in central), and RTPG (in southwest)\nGranger-cause GDP (in central and northwest). Moreover, panel regressions show\nthe negative impact from GDP to RTPG in the northwest, and RTPG to GDP in the\ncentral. However, RTPG has a positive influence on GDP in the northwest.\nTherefore, to improve economic development without compromising the regions'\ncompetitiveness in central and western China, we can adjust the power\ngeneration structure, and increase investments in the renewable energy supply\nand energy efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 15:24:48 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Ni", "Jie", ""], ["Qian", "Jiayi", ""], ["Lu", "Yixiao", ""], ["Cheng", "Hong", ""]]}, {"id": "2011.12247", "submitter": "Micha{\\l} Kozielski", "authors": "Joanna Henzel, Joanna Tobiasz, Micha{\\l} Kozielski, Ma{\\l}gorzata\n  Bach, Pawe{\\l} Foszner, Aleksandra Gruca, Mateusz Kania, Justyna Mika, Anna\n  Papiez, Aleksandra Werner, Joanna Zyla, and Jerzy Jaroszewicz, Joanna\n  Polanska, Marek Sikora", "title": "Classification supporting COVID-19 diagnostics based on patient survey\n  data", "comments": "39 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distinguishing COVID-19 from other flu-like illnesses can be difficult due to\nambiguous symptoms and still an initial experience of doctors. Whereas, it is\ncrucial to filter out those sick patients who do not need to be tested for\nSARS-CoV-2 infection, especially in the event of the overwhelming increase in\ndisease. As a part of the presented research, logistic regression and XGBoost\nclassifiers, that allow for effective screening of patients for COVID-19, were\ngenerated. Each of the methods was tuned to achieve an assumed acceptable\nthreshold of negative predictive values during classification. Additionally, an\nexplanation of the obtained classification models was presented. The\nexplanation enables the users to understand what was the basis of the decision\nmade by the model. The obtained classification models provided the basis for\nthe DECODE service (decode.polsl.pl), which can serve as support in screening\npatients with COVID-19 disease. Moreover, the data set constituting the basis\nfor the analyses performed is made available to the research community. This\ndata set consisting of more than 3,000 examples is based on questionnaires\ncollected at a hospital in Poland.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 17:44:01 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Henzel", "Joanna", ""], ["Tobiasz", "Joanna", ""], ["Kozielski", "Micha\u0142", ""], ["Bach", "Ma\u0142gorzata", ""], ["Foszner", "Pawe\u0142", ""], ["Gruca", "Aleksandra", ""], ["Kania", "Mateusz", ""], ["Mika", "Justyna", ""], ["Papiez", "Anna", ""], ["Werner", "Aleksandra", ""], ["Zyla", "Joanna", ""], ["Jaroszewicz", "Jerzy", ""], ["Polanska", "Joanna", ""], ["Sikora", "Marek", ""]]}, {"id": "2011.12345", "submitter": "Maria Josefsson", "authors": "Maria Josefsson, Michael J. Daniels, Sara Pudas", "title": "A Bayesian semi-parametric approach for inference on the population\n  partly conditional mean from longitudinal data with dropout", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studies of memory trajectories using longitudinal data often result in highly\nnon-representative samples due to selective study enrollment and attrition. An\nadditional bias comes from practice effects that result in improved or\nmaintained performance due to familiarity with test content or context. These\nchallenges may bias study findings and severely distort the ability to\ngeneralize to the target population. In this study we propose an approach for\nestimating the finite population mean of a longitudinal outcome conditioning on\nbeing alive at a specific time point. We develop a flexible Bayesian\nsemi-parametric predictive estimator for population inference when longitudinal\nauxiliary information is known for the target population. We evaluate\nsensitivity of the results to untestable assumptions and further compare our\napproach to other methods used for population inference in a simulation study.\nThe proposed approach is motivated by 15-year longitudinal data from the Betula\nlongitudinal cohort study. We apply our approach to estimate lifespan\ntrajectories in episodic memory, with the aim to generalize findings to a\ntarget population.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 19:54:42 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 23:42:33 GMT"}, {"version": "v3", "created": "Mon, 22 Mar 2021 08:43:09 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Josefsson", "Maria", ""], ["Daniels", "Michael J.", ""], ["Pudas", "Sara", ""]]}, {"id": "2011.12397", "submitter": "James Tucker", "authors": "Xiao Zang, Sebastian Kurtek, Oksana Chkrebtii, J. Derek Tucker", "title": "Elastic $k$-means clustering of functional data for posterior\n  exploration, with an application to inference on acute respiratory infection\n  dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose a new method for clustering of functional data using a $k$-means\nframework. We work within the elastic functional data analysis framework, which\nallows for decomposition of the overall variation in functional data into\namplitude and phase components. We use the amplitude component to partition\nfunctions into shape clusters using an automated approach. To select an\nappropriate number of clusters, we additionally propose a novel Bayesian\nInformation Criterion defined using a mixture model on principal components\nestimated using functional Principal Component Analysis. The proposed method is\nmotivated by the problem of posterior exploration, wherein samples obtained\nfrom Markov chain Monte Carlo algorithms are naturally represented as\nfunctions. We evaluate our approach using a simulated dataset, and apply it to\na study of acute respiratory infection dynamics in San Luis Potos\\'{i}, Mexico.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 21:27:10 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Zang", "Xiao", ""], ["Kurtek", "Sebastian", ""], ["Chkrebtii", "Oksana", ""], ["Tucker", "J. Derek", ""]]}, {"id": "2011.12487", "submitter": "Prateek Bansal", "authors": "Anupriya, Daniel J. Graham, Prateek Bansal, Daniel H\\\"orcher, Richard\n  Anderson", "title": "Congestion in near capacity metro operations: optimum boardings and\n  alightings at bottleneck stations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  During peak hours, metro systems often operate at high service frequencies to\ntransport large volumes of passengers. However, the punctuality of such\noperations can be severely impacted by a vicious circle of passenger congestion\nand train delays. In particular, high volumes of passenger boardings and\nalightings may lead to increased dwell times at stations, that may eventually\ncause queuing of trains in upstream. Such stations act as active bottlenecks in\nthe metro network and congestion may propagate from these bottlenecks to the\nentire network. Thus, understanding the mechanism that drives passenger\ncongestion at these bottleneck stations is crucial to develop informed control\nstrategies, such as control of inflow of passengers entering these stations. To\nthis end, we conduct the first station-level econometric analysis to estimate a\ncausal relationship between boarding-alighting movements and train flow using\ndata from entry/exit gates and train movement data of the Mass Transit Railway,\nHong Kong. We adopt a Bayesian non-parametric spline-based regression approach\nand apply instrumental variables estimation to control for confounding bias\nthat may occur due to unobserved characteristics of metro operations. Through\nthe results of the empirical study, we identify bottleneck stations and provide\nestimates of optimum passenger movements per train and service frequencies at\nthe bottleneck stations. These estimates, along with real data on daily demand,\ncould assist metro operators in devising station-level control strategies.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 02:28:57 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Anupriya", "", ""], ["Graham", "Daniel J.", ""], ["Bansal", "Prateek", ""], ["H\u00f6rcher", "Daniel", ""], ["Anderson", "Richard", ""]]}, {"id": "2011.12595", "submitter": "Andrea Gilardi", "authors": "Andrea Gilardi and Jorge Mateu and Riccardo Borgoni and Robin Lovelace", "title": "Multivariate hierarchical analysis of car crashes data considering a\n  spatial network lattice", "comments": "23 pages, 5 tables, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Road traffic casualties represent a hidden global epidemic, demanding\nevidence-based interventions. This paper demonstrates a network lattice\napproach for identifying road segments of particular concern, based on a case\nstudy of a major city (Leeds, UK), in which 5,862 crashes of different\nseverities were recorded over an eight-year period (2011-2018). We consider a\nfamily of Bayesian hierarchical models that include spatially structured and\nunstructured random effects, to capture the dependencies between the severity\nlevels. Results highlight roads that are more prone to collisions, relative to\nestimated traffic volumes, in the northwest and south of city-centre. We\nanalyse the Modifiable Areal Unit Problem (MAUP), proposing a novel procedure\nto investigate the presence of MAUP on a network lattice. We conclude that our\nmethods enable a reliable estimation of road safety levels to help identify\n\"hotspots\" on the road network and to inform effective local interventions.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 09:13:19 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 12:18:29 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Gilardi", "Andrea", ""], ["Mateu", "Jorge", ""], ["Borgoni", "Riccardo", ""], ["Lovelace", "Robin", ""]]}, {"id": "2011.12756", "submitter": "Farid Mohammadi", "authors": "Stefania Scheurer and Aline Sch\\\"afer Rodrigues Silva and Farid\n  Mohammadi and Johannes Hommel and Sergey Oladyshkin and Bernd Flemisch and\n  Wolfgang Nowak", "title": "Surrogate-based Bayesian Comparison of Computationally Expensive Models:\n  Application to Microbially Induced Calcite Precipitation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geochemical processes in subsurface reservoirs affected by microbial activity\nchange the material properties of porous media. This is a complex\nbiogeochemical process in subsurface reservoirs that currently contains strong\nconceptual uncertainty. This means, several modeling approaches describing the\nbiogeochemical process are plausible and modelers face the uncertainty of\nchoosing the most appropriate one. Once observation data becomes available, a\nrigorous Bayesian model selection accompanied by a Bayesian model\njustifiability analysis could be employed to choose the most appropriate model,\ni.e. the one that describes the underlying physical processes best in the light\nof the available data. However, biogeochemical modeling is computationally very\ndemanding because it conceptualizes different phases, biomass dynamics,\ngeochemistry, precipitation and dissolution in porous media. Therefore, the\nBayesian framework cannot be based directly on the full computational models as\nthis would require too many expensive model evaluations. To circumvent this\nproblem, we suggest performing both Bayesian model selection and justifiability\nanalysis after constructing surrogates for the competing biogeochemical models.\nHere, we use the arbitrary polynomial chaos expansion. We account for the\napproximation error in the Bayesian analysis by introducing novel correction\nfactors for the resulting model weights. Thereby, we extend the Bayesian\njustifiability analysis and assess model similarities for computationally\nexpensive models. We demonstrate the method on a representative scenario for\nmicrobially induced calcite precipitation in a porous medium. Our extension of\nthe justifiability analysis provides a suitable approach for the comparison of\ncomputationally demanding models and gives an insight on the necessary amount\nof data for a reliable model performance.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 14:18:40 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Scheurer", "Stefania", ""], ["Silva", "Aline Sch\u00e4fer Rodrigues", ""], ["Mohammadi", "Farid", ""], ["Hommel", "Johannes", ""], ["Oladyshkin", "Sergey", ""], ["Flemisch", "Bernd", ""], ["Nowak", "Wolfgang", ""]]}, {"id": "2011.12776", "submitter": "Maria Fabiana Laguna", "authors": "Julian Ne\\~ner and Mar\\'ia Fabiana Laguna", "title": "Optimal risk in wealth exchange models: agent dynamics from a\n  microscopic perspective", "comments": "17 pages, 11 figures", "journal-ref": null, "doi": "10.1016/j.physa.2020.125625", "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we study the individual strategies carried out by agents\nundergoing transactions in wealth exchange models. We analyze the role of risk\npropensity in the behavior of the agents and find a critical risk, such that\nagents with risk above that value always end up losing everything when the\nsystem approaches equilibrium. Moreover, we find that the wealth of the agents\nis maximum for a range of risk values that depends on particular\ncharacteristics of the model, such as the social protection factor. Our\nfindings allow to determine a region of parameters for which the strategies of\nthe economic agents are successful.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 14:39:09 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Ne\u00f1er", "Julian", ""], ["Laguna", "Mar\u00eda Fabiana", ""]]}, {"id": "2011.12988", "submitter": "Shahana Ibrahim", "authors": "Shahana Ibrahim, Xiao Fu", "title": "Mixed Membership Graph Clustering via Systematic Edge Query", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work considers clustering nodes of a largely incomplete graph. Under the\nproblem setting, only a small amount of queries about the edges can be made,\nbut the entire graph is not observable. This problem finds applications in\nlarge-scale data clustering using limited annotations, community detection\nunder restricted survey resources, and graph topology inference under\nhidden/removed node interactions. Prior works tackled this problem from various\nperspectives, e.g., convex programming-based low-rank matrix completion and\nactive query-based clique finding. Nonetheless, many existing methods are\ndesigned for estimating the single-cluster membership of the nodes, but nodes\nmay often have mixed (i.e., multi-cluster) membership in practice. Some query\nand computational paradigms, e.g., the random query patterns and nuclear\nnorm-based optimization advocated in the convex approaches, may give rise to\nscalability and implementation challenges. This work aims at learning mixed\nmembership of nodes using queried edges. The proposed method is developed\ntogether with a systematic query principle that can be controlled and adjusted\nby the system designers to accommodate implementation challenges -- e.g., to\navoid querying edges that are physically hard to acquire. Our framework also\nfeatures a lightweight and scalable algorithm with membership learning\nguarantees. Real-data experiments on crowdclustering and community detection\nare used to showcase the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 19:19:05 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 04:07:43 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Ibrahim", "Shahana", ""], ["Fu", "Xiao", ""]]}, {"id": "2011.13012", "submitter": "Babak Alipanahi", "authors": "Babak Alipanahi, Farhad Hormozdiari, Babak Behsaz, Justin Cosentino,\n  Zachary R. McCaw, Emanuel Schorsch, D. Sculley, Elizabeth H. Dorfman, Sonia\n  Phene, Naama Hammel, Andrew Carroll, Anthony P. Khawaja, Cory Y. McLean", "title": "Large-scale machine learning-based phenotyping significantly improves\n  genomic discovery for optic nerve head morphology", "comments": "Includes Supplementary Information and Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Genome-wide association studies (GWAS) require accurate cohort phenotyping,\nbut expert labeling can be costly, time-intensive, and variable. Here we\ndevelop a machine learning (ML) model to predict glaucomatous optic nerve head\nfeatures from color fundus photographs. We used the model to predict vertical\ncup-to-disc ratio (VCDR), a diagnostic parameter and cardinal endophenotype for\nglaucoma, in 65,680 Europeans in the UK Biobank (UKB). A GWAS of ML-based VCDR\nidentified 299 independent genome-wide significant (GWS; $P\\leq5\\times10^{-8}$)\nhits in 156 loci. The ML-based GWAS replicated 62 of 65 GWS loci from a recent\nVCDR GWAS in the UKB for which two ophthalmologists manually labeled images for\n67,040 Europeans. The ML-based GWAS also identified 92 novel loci,\nsignificantly expanding our understanding of the genetic etiologies of glaucoma\nand VCDR. Pathway analyses support the biological significance of the novel\nhits to VCDR, with select loci near genes involved in neuronal and synaptic\nbiology or known to cause severe Mendelian ophthalmic disease. Finally, the\nML-based GWAS results significantly improve polygenic prediction of VCDR and\nprimary open-angle glaucoma in the independent EPIC-Norfolk cohort.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 20:42:30 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Alipanahi", "Babak", ""], ["Hormozdiari", "Farhad", ""], ["Behsaz", "Babak", ""], ["Cosentino", "Justin", ""], ["McCaw", "Zachary R.", ""], ["Schorsch", "Emanuel", ""], ["Sculley", "D.", ""], ["Dorfman", "Elizabeth H.", ""], ["Phene", "Sonia", ""], ["Hammel", "Naama", ""], ["Carroll", "Andrew", ""], ["Khawaja", "Anthony P.", ""], ["McLean", "Cory Y.", ""]]}, {"id": "2011.13031", "submitter": "Babak Jalalzadeh Fard", "authors": "Babak Jalalzadeh Fard, Udit Bhatia and Auroop R. Ganguly", "title": "Mega Regional Heat Patterns in US Urban Corridors", "comments": "18 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.geo-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Current literature suggests that urban heat-islands and their consequences\nare intensifying under climate change and urbanization. Here we explore the\nrelatively unexplored hypothesis that emerging urban corridors (UCs) spawn\nmegaregions of intense heat which are evident from observations. A delineation\nof the eleven United States UCs relative to their underlying climatological\nregions (non-UCs) suggest a surprisingly mixed trend. Medians and trends of\nwinter temperatures over the last 60-years are generally higher in the UCs but\nno such general trends are observed in the summer. Heat wave metrics related to\npublic health, energy demand and relative intensity do not exhibit\nsignificantly higher overall trends. Temperature and heat wave indices in the\nUCs exhibit high correlations with each other including across seasons.\nSpatiotemporal patterns in population, along with urbanization, agriculture and\nelevation, exhibit high (positive or negative) rank correlations with warming\nand heatwave intensification. The findings can inform climate adaptation in\nmegalopolises.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 21:24:32 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Fard", "Babak Jalalzadeh", ""], ["Bhatia", "Udit", ""], ["Ganguly", "Auroop R.", ""]]}, {"id": "2011.13216", "submitter": "Farid Mohammadi", "authors": "Farid Mohammadi", "title": "Development and Realization of Validation Benchmarks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of modeling, the word validation refers to simple comparisons\nbetween model outputs and experimental data. Usually, this comparison\nconstitutes plotting the model results against data on the same axes to provide\na visual assessment of agreement or lack thereof. However, there are a number\nof concerns with such naive comparisons. First, these comparisons tend to\nprovide qualitative rather than quantitative assessments and are clearly\ninsufficient for making decisions regarding model validity. Second, they often\ndisregard or only partly account for existing uncertainties in the experimental\nobservations or the model input parameters. Third, such comparisons can not\nreveal whether the model is appropriate for the intended purposes, as they\nmainly focus on the agreement in the observable quantities.\n  These pitfalls give rise to the need for an uncertainty-aware framework that\nincludes a validation metric. This metric shall provide a measure for\ncomparison of the system response quantities of an experiment with the ones\nfrom a computational model, while accounting for uncertainties in both. To\naddress this need, we have developed a statistical framework that incorporates\na probabilistic modeling technique using a fully Bayesian approach. A Bayesian\nperspective yields an optimal bias-variance trade-off against the experimental\ndata and provide an integrative metric for model validation that incorporates\nparameter and conceptual uncertainty. Additionally, to accelerate the analysis\nfor computationally demanding flow and transport models in porous media, the\nframework is equipped with a model reduction technique, namely Bayesian Sparse\nPolynomial Chaos Expansion. We demonstrate the capabilities of the\naforementioned Bayesian validation framework by applying it to an application\nfor validation as well as uncertainty quantification of fluid flow in fractured\nporous media.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 10:26:57 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 07:05:13 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Mohammadi", "Farid", ""]]}, {"id": "2011.13231", "submitter": "Jesse Gioannini", "authors": "Haotian Zhu, Denise Mak, Jesse Gioannini, Fei Xia", "title": "NLPStatTest: A Toolkit for Comparing NLP System Performance", "comments": "Will appear in AACL-IJCNLP 2020", "journal-ref": "Proceedings of the 1st Conference of the Asia-Pacific Chapter of\n  the Association for Computational Linguistics and the 10th International\n  Joint Conference on Natural Language Processing: System Demonstrations (2020)\n  40-46", "doi": null, "report-no": null, "categories": "cs.CL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical significance testing centered on p-values is commonly used to\ncompare NLP system performance, but p-values alone are insufficient because\nstatistical significance differs from practical significance. The latter can be\nmeasured by estimating effect size. In this paper, we propose a three-stage\nprocedure for comparing NLP system performance and provide a toolkit,\nNLPStatTest, that automates the process. Users can upload NLP system evaluation\nscores and the toolkit will analyze these scores, run appropriate significance\ntests, estimate effect size, and conduct power analysis to estimate Type II\nerror. The toolkit provides a convenient and systematic way to compare NLP\nsystem performance that goes beyond statistical significance testing\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 10:59:23 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Zhu", "Haotian", ""], ["Mak", "Denise", ""], ["Gioannini", "Jesse", ""], ["Xia", "Fei", ""]]}, {"id": "2011.13327", "submitter": "Dennis Klinkhammer", "authors": "Dennis Klinkhammer", "title": "Analysing Social Media Network Data with R: Semi-Automated Screening of\n  Users, Comments and Communication Patterns", "comments": "14 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CV stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Communication on social media platforms is not only culturally and\npolitically relevant, it is also increasingly widespread across societies.\nUsers not only communicate via social media platforms, but also search\nspecifically for information, disseminate it or post information themselves.\nHowever, fake news, hate speech and even radicalizing elements are part of this\nmodern form of communication: Sometimes with far-reaching effects on\nindividuals and societies. A basic understanding of these mechanisms and\ncommunication patterns could help to counteract negative forms of\ncommunication, e.g. bullying among children or extreme political points of\nview. To this end, a method will be presented in order to break down the\nunderlying communication patterns, to trace individual users and to inspect\ntheir comments and range on social media platforms; Or to contrast them later\non via qualitative research. This approeach can identify particularly active\nusers with an accuracy of 100 percent, if the framing social networks as well\nas the topics are taken into account. However, methodological as well as\ncounteracting approaches must be even more dynamic and flexible to ensure\nsensitivity and specifity regarding users who spread hate speech, fake news and\nradicalizing elements.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 14:52:01 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Klinkhammer", "Dennis", ""]]}, {"id": "2011.13353", "submitter": "Hongwei Wang", "authors": "Hongwei Wang, Yuanyuan Liu, Wei Zhang, and Junyi Zuo", "title": "Outlier-robust Kalman Filter in the Presence of Correlated Measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the robust filtering problem for a state-space model with\noutliers in correlated measurements. We propose a new robust filtering\nframework to further improve the robustness of conventional robust filters.\nSpecifically, the measurement fitting error is processed separately during the\nreweighting procedure, which differs from existing solutions where a jointly\nprocessed scheme is involved. Simulation results reveal that, under the same\nsetup, the proposed method outperforms the existing robust filter when the\noutlier-contaminated measurements are correlated, while it has the same\nperformance as the existing one in the presence of uncorrelated measurements\nsince these two types of robust filters are equivalent under such a\ncircumstance.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 15:35:40 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Wang", "Hongwei", ""], ["Liu", "Yuanyuan", ""], ["Zhang", "Wei", ""], ["Zuo", "Junyi", ""]]}, {"id": "2011.13593", "submitter": "Sarah Juricic", "authors": "Sarah Juricic (LOCIE), Jeanne Goffart (LOCIE), Simon Rouchier (LOCIE),\n  Aur\\'elie Foucquier (LITEN), Nicolas Cellier (LOCIE), Gilles Fraisse (LOCIE)", "title": "Influence of weather natural variability on the thermal characterisation\n  of a building envelope", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The thermal characterisation of a building envelope is usually best performed\nfrom on site measurements with controlled heating power set points.\nOccupant-friendly measurement conditions provide on the contrary less\ninformative data. Notwithstanding occupancy, the boundary conditions alone\ncontribute to a greater extent to the energy balance. Non intrusive conditions\nquestion therefore the repeatability and relevance of such experiment.This\npaper proposes an original numerical methodology to assess the repeatability\nand accuracy of the estimation of an envelope's overall thermalresistance under\nvariable weather conditions. A comprehensive building energy model serves as\nreference model to produce multiple synthetic datasets. Each is run with a\ndifferent weather dataset from a single location and serves for the calibration\nof an appropriate model, which provides a thermal resistance estimate. The\nestimate's accuracy is then assessed in the light of the particular weather\nconditions that served for data generation. The originality also lies in the\nuse of stochastically generated weather datasets to perform an uncertaintyand\nglobal sensitivity analysis of all estimates with respect to 6 weather\nvariables.The methodology is applied on simulated data from a one-storey house\ncase study serving as reference model. The thermal resistance estimations are\ninferred from calibrated stochastic RC models. It is found that 11 days are\nnecessary to achieve robust estimations. The large air change rate in the case\nstudy explains why the outdoor temperature and the wind speed are found highly\ninfluential.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 07:45:13 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Juricic", "Sarah", "", "LOCIE"], ["Goffart", "Jeanne", "", "LOCIE"], ["Rouchier", "Simon", "", "LOCIE"], ["Foucquier", "Aur\u00e9lie", "", "LITEN"], ["Cellier", "Nicolas", "", "LOCIE"], ["Fraisse", "Gilles", "", "LOCIE"]]}, {"id": "2011.13758", "submitter": "Ludwig Hothorn", "authors": "Ludwig A. Hothorn", "title": "Comparisons of proportions in k dose groups against a negative control\n  assuming order restriction: Williams-type test vs. closed test procedures", "comments": "2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.OT", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The comparison of proportions is considered in the asymptotic generalized\nlinear model with the odds ratio as effect size. When several doses are\ncompared with a control assuming an order restriction, a Williams-type trend\ntest can be used. As an alternative, two variants of the closed testing\napproach are considered, one using global Williams-tests in the partition\nhypotheses, one with pairwise contrasts. Their advantages in terms of power and\nsimplicity are demonstrated. Related R-code is provided.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 14:44:45 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Hothorn", "Ludwig A.", ""]]}, {"id": "2011.13960", "submitter": "Abhinandan Dalal", "authors": "Navonil Deb, Abhinandan Dalal and Gopal Krishna Basak", "title": "Finding Optimal Cancer Treatment using Markov Decision Process to\n  Improve Overall Health and Quality of Life", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.GN q-bio.QM q-fin.EC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Decision Processes and Dynamic Treatment Regimes have grown\nincreasingly popular in the treatment of diseases, including cancer. However,\ncancer treatment often impacts quality of life drastically, and people often\nfail to take treatments that are sustainable, affordable and can be adhered to.\nIn this paper, we emphasize the usage of ambient factors like profession,\nradioactive exposure, food habits on the treatment choice, keeping in mind that\nthe aim is not just to relieve the patient of his disease, but rather to\nmaximize his overall physical, social and mental well being. We delineate a\ngeneral framework which can directly incorporate a net benefit function from a\nphysician as well as patient's utility, and can incorporate the varying\nprobabilities of exposure and survival of patients of varying medical profiles.\nWe also show by simulations that the optimal choice of actions often is\nsensitive to extraneous factors, like the financial status of a person (as a\nproxy for the affordability of treatment), and that these actions should be\nwelcome keeping in mind the overall quality of life.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 19:06:04 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Deb", "Navonil", ""], ["Dalal", "Abhinandan", ""], ["Basak", "Gopal Krishna", ""]]}, {"id": "2011.14005", "submitter": "Naga Karthik Enamundram", "authors": "Enamundram M. V. Naga Karthik, Catherine Laporte, Farida Cheriet", "title": "Three-dimensional Segmentation of the Scoliotic Spine from MRI using\n  Unsupervised Volume-based MR-CT Synthesis", "comments": "To appear in the Proceedings of the SPIE Medical Imaging Conference\n  2021, San Diego, CA. 9 pages, 4 figures in total", "journal-ref": "Proceedings Volume 11596, SPIE Medical Imaging 2021: Image\n  Processing; 115961H", "doi": "10.1117/12.2580677", "report-no": null, "categories": "eess.IV cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vertebral bone segmentation from magnetic resonance (MR) images is a\nchallenging task. Due to the inherent nature of the modality to emphasize soft\ntissues of the body, common thresholding algorithms are ineffective in\ndetecting bones in MR images. On the other hand, it is relatively easier to\nsegment bones from CT images because of the high contrast between bones and the\nsurrounding regions. For this reason, we perform a cross-modality synthesis\nbetween MR and CT domains for simple thresholding-based segmentation of the\nvertebral bones. However, this implicitly assumes the availability of paired\nMR-CT data, which is rare, especially in the case of scoliotic patients. In\nthis paper, we present a completely unsupervised, fully three-dimensional (3D)\ncross-modality synthesis method for segmenting scoliotic spines. A 3D CycleGAN\nmodel is trained for an unpaired volume-to-volume translation across MR and CT\ndomains. Then, the Otsu thresholding algorithm is applied to the synthesized CT\nvolumes for easy segmentation of the vertebral bones. The resulting\nsegmentation is used to reconstruct a 3D model of the spine. We validate our\nmethod on 28 scoliotic vertebrae in 3 patients by computing the\npoint-to-surface mean distance between the landmark points for each vertebra\nobtained from pre-operative X-rays and the surface of the segmented vertebra.\nOur study results in a mean error of 3.41 $\\pm$ 1.06 mm. Based on qualitative\nand quantitative results, we conclude that our method is able to obtain a good\nsegmentation and 3D reconstruction of scoliotic spines, all after training from\nunpaired data in an unsupervised manner.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 18:34:52 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Karthik", "Enamundram M. V. Naga", ""], ["Laporte", "Catherine", ""], ["Cheriet", "Farida", ""]]}, {"id": "2011.14075", "submitter": "Benjamin Laufer", "authors": "Benjamin Laufer", "title": "Feedback Effects in Repeat-Use Criminal Risk Assessments", "comments": "10 pages. arXiv admin note: substantial text overlap with\n  arXiv:2005.13404", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DS cs.LG cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the criminal legal context, risk assessment algorithms are touted as\ndata-driven, well-tested tools. Studies known as validation tests are typically\ncited by practitioners to show that a particular risk assessment algorithm has\npredictive accuracy, establishes legitimate differences between risk groups,\nand maintains some measure of group fairness in treatment. To establish these\nimportant goals, most tests use a one-shot, single-point measurement. Using a\nPolya Urn model, we explore the implication of feedback effects in sequential\nscoring-decision processes. We show through simulation that risk can propagate\nover sequential decisions in ways that are not captured by one-shot tests. For\nexample, even a very small or undetectable level of bias in risk allocation can\namplify over sequential risk-based decisions, leading to observable group\ndifferences after a number of decision iterations. Risk assessment tools\noperate in a highly complex and path-dependent process, fraught with historical\ninequity. We conclude from this study that these tools do not properly account\nfor compounding effects, and require new approaches to development and\nauditing.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 06:40:05 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Laufer", "Benjamin", ""]]}, {"id": "2011.14186", "submitter": "Chau-Wai Wong", "authors": "Ritesh Goenka, Shu-Jie Cao, Chau-Wai Wong, Ajit Rajwade, Dror Baron", "title": "Contact Tracing Enhances the Efficiency of COVID-19 Group Testing", "comments": "This version includes a supplemental document", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Group testing can save testing resources in the context of the ongoing\nCOVID-19 pandemic. In group testing, we are given $n$ samples, one per\nindividual, and arrange them into $m < n$ pooled samples, where each pool is\nobtained by mixing a subset of the $n$ individual samples. Infected individuals\nare then identified using a group testing algorithm. In this paper, we use side\ninformation (SI) collected from contact tracing (CT) within\nnon-adaptive/single-stage group testing algorithms. We generate data by\nincorporating CT SI and characteristics of disease spread between individuals.\nThese data are fed into two signal and measurement models for group testing,\nwhere numerical results show that our algorithms provide improved sensitivity\nand specificity. While Nikolopoulos et al. utilized family structure to improve\nnon-adaptive group testing, ours is the first work to explore and demonstrate\nhow CT SI can further improve group testing performance.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 18:32:14 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Goenka", "Ritesh", ""], ["Cao", "Shu-Jie", ""], ["Wong", "Chau-Wai", ""], ["Rajwade", "Ajit", ""], ["Baron", "Dror", ""]]}, {"id": "2011.14216", "submitter": "Soonwoo Kwon", "authors": "Koohyun Kwon, Soonwoo Kwon", "title": "Inference in Regression Discontinuity Designs under Monotonicity", "comments": "55 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide an inference procedure for the sharp regression discontinuity\ndesign (RDD) under monotonicity, with possibly multiple running variables.\nSpecifically, we consider the case where the true regression function is\nmonotone with respect to (all or some of) the running variables and assumed to\nlie in a Lipschitz smoothness class. Such a monotonicity condition is natural\nin many empirical contexts, and the Lipschitz constant has an intuitive\ninterpretation. We propose a minimax two-sided confidence interval (CI) and an\nadaptive one-sided CI. For the two-sided CI, the researcher is required to\nchoose a Lipschitz constant where she believes the true regression function to\nlie in. This is the only tuning parameter, and the resulting CI has uniform\ncoverage and obtains the minimax optimal length. The one-sided CI can be\nconstructed to maintain coverage over all monotone functions, providing maximum\ncredibility in terms of the choice of the Lipschitz constant. Moreover, the\nmonotonicity makes it possible for the (excess) length of the CI to adapt to\nthe true Lipschitz constant of the unknown regression function. Overall, the\nproposed procedures make it easy to see under what conditions on the underlying\nregression function the given estimates are significant, which can add more\ntransparency to research using RDD methods.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 21:11:54 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Kwon", "Koohyun", ""], ["Kwon", "Soonwoo", ""]]}, {"id": "2011.14219", "submitter": "Soonwoo Kwon", "authors": "Koohyun Kwon, Soonwoo Kwon", "title": "Adaptive Inference in Multivariate Nonparametric Regression Models Under\n  Monotonicity", "comments": "52 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of adaptive inference on a regression function at a\npoint under a multivariate nonparametric regression setting. The regression\nfunction belongs to a H\\\"older class and is assumed to be monotone with respect\nto some or all of the arguments. We derive the minimax rate of convergence for\nconfidence intervals (CIs) that adapt to the underlying smoothness, and provide\nan adaptive inference procedure that obtains this minimax rate. The procedure\ndiffers from that of Cai and Low (2004), intended to yield shorter CIs under\npractically relevant specifications. The proposed method applies to general\nlinear functionals of the regression function, and is shown to have favorable\nperformance compared to existing inference procedures.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 21:21:04 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Kwon", "Koohyun", ""], ["Kwon", "Soonwoo", ""]]}, {"id": "2011.14383", "submitter": "Reimar Heinrich Leike", "authors": "Reimar Leike, Silvia Celli, Alberto Krone-Martins, Celine Boehm,\n  Martin Glatzle, Yasou Fukui, Hidetoshi Sano, Gavin Rowell", "title": "Optical reconstruction of dust in the region of SNR RX J1713.7-3946 from\n  astrometric data", "comments": null, "journal-ref": null, "doi": "10.1038/s41550-021-01344-w", "report-no": null, "categories": "astro-ph.HE astro-ph.GA physics.comp-ph physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The origin of the radiation observed in the region of the supernova remnant\nRX J1713.7-3946, one of the brightest TeV emitters, has been debated since its\ndiscovery. The existence of atomic and molecular clouds in this object supports\nthe idea that part of the GeV gamma rays in this region originate from\nproton-proton collisions. However, the observed column density of protons\nderived from gas observations cannot explain the whole emission. Yet there\ncould be a fraction of protons contained in fainter structures that have note\nbeen detected so far. Here we search for faint objects in the line of sight of\nRX J1713.7-3946 using the principle of light extinction and the ESA/Gaia DR2\nastrometric and photometric data. We reveal and locate with precision a number\nof dust clouds and note that only one appears to be in the vicinity of RX\nJ1713.7-3946. We estimate the embedded mass to $M_{dust} = (7.0 \\pm 0.6) \\times\n10^3 \\, M_{\\odot}$ which might be big enough to contain the missing protons.\nFinally, using the fact that the supernova remnant is expected to be located in\na dusty environment and that there appears to be only one such structure in the\nvicinity of RX J1713.7-3946, we set a very precise constrain to the supernova\nremnant distance, at ($1.12 \\pm 0.01$) kpc.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 15:05:46 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 14:54:05 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Leike", "Reimar", ""], ["Celli", "Silvia", ""], ["Krone-Martins", "Alberto", ""], ["Boehm", "Celine", ""], ["Glatzle", "Martin", ""], ["Fukui", "Yasou", ""], ["Sano", "Hidetoshi", ""], ["Rowell", "Gavin", ""]]}, {"id": "2011.14402", "submitter": "Chirag Manchanda", "authors": "Chirag Manchanda, Mayank Kumar, Vikram Singh, Naba Hazarika, Mohd\n  Faisal, Vipul Lalchandani, Ashutosh Shukla, Jay Dave, Neeraj Rastogi,\n  Sachchida Nand Tripathi", "title": "Chemical characterization of PM2.5 bound species during the Diwali\n  fireworks in Delhi: An insight from source apportionment and airborne\n  hazardous elements", "comments": "The manuscript is under consideration for publication in Atmospheric\n  Environment. Present Status: Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ao-ph stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Diwali is among the most important Indian festivals, and elaborate firework\ndisplays mark the festivities in the evening. This study assesses the impact of\nDiwali on the concentration, composition, and sources of ambient PM2.5. The\ntotal PM2.5 concentrations were observed to rise by ~16 times their\npre-firework levels, while each of the elements, organics, and black carbon\nwere augmented by a factor of 12.3, 1.6, and 2.3, respectively. Concentration\nlevels of species like K, Al, Sr, Ba, S, and Bi displayed distinct peaks during\nthe firework event and were identified as tracers for the same. The average\nconcentrations of potential carcinogens like As, exceeded US EPA screening\nlevels for industrial air by a factor of ~9.6, while peak levels reached up to\n16.1 times the screening levels. The source apportionment study, undertaken\nusing positive matrix factorization, revealed the fireworks to account for 95%\nof the total elemental PM2.5 during Diwali. The resolved primary organic\nemissions, too, were enhanced by a factor of ~8 during Diwali. Delhi has\nencountered serious haze events following Diwali in recent years, this study\nhighlights that it is the biomass burning emissions rather than the fireworks\nthat dominate Delhi's air quality in the days following Diwali.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 17:09:58 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Manchanda", "Chirag", ""], ["Kumar", "Mayank", ""], ["Singh", "Vikram", ""], ["Hazarika", "Naba", ""], ["Faisal", "Mohd", ""], ["Lalchandani", "Vipul", ""], ["Shukla", "Ashutosh", ""], ["Dave", "Jay", ""], ["Rastogi", "Neeraj", ""], ["Tripathi", "Sachchida Nand", ""]]}, {"id": "2011.14412", "submitter": "Panpan Zhang", "authors": "Jianmin Chen, Jun Yan, Panpan Zhang", "title": "Clustering US States by Time Series of COVID-19 New Case Counts with\n  Non-negative Matrix Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spreading pattern of COVID-19 differ a lot across the US states under\ndifferent quarantine measures and reopening policies. We proposed to cluster\nthe US states into distinct communities based on the daily new confirmed case\ncounts via a nonnegative matrix factorization (NMF) followed by a k-means\nclustering procedure on the coefficients of the NMF basis. A cross-validation\nmethod was employed to select the rank of the NMF. Applying the method to the\nentire study period from March 22 to July 25, we clustered the 49 continental\nstates (including District of Columbia) into 7 groups, two of which contained a\nsingle state. To investigate the dynamics of the clustering results over time,\nthe same method was successively applied to the time periods with increment of\none week, starting from the period of March 22 to March 28. The results\nsuggested a change point in the clustering in the week starting on May 30,\nwhich might be explained by a combined impact of both quarantine measures and\nreopening policies.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 18:27:02 GMT"}, {"version": "v2", "created": "Sun, 6 Dec 2020 02:07:36 GMT"}, {"version": "v3", "created": "Fri, 15 Jan 2021 05:53:45 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Chen", "Jianmin", ""], ["Yan", "Jun", ""], ["Zhang", "Panpan", ""]]}, {"id": "2011.14423", "submitter": "Bryan Cai", "authors": "Bryan Cai, John P.A. Ioannidis, Eran Bendavid, Lu Tian", "title": "Exact Inference for Disease Prevalence Based on a Test with Unknown\n  Specificity and Sensitivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To make informative public policy decisions in battling the ongoing COVID-19\npandemic, it is important to know the disease prevalence in a population. There\nare two intertwined difficulties in estimating this prevalence based on testing\nresults from a group of subjects. First, the test is prone to measurement error\nwith unknown sensitivity and specificity. Second, the prevalence tends to be\nlow at the initial stage of the pandemic and we may not be able to determine if\na positive test result is a false positive due to the imperfect specificity of\nthe test. The statistical inference based on large sample approximation or\nconventional bootstrap may not be sufficiently reliable and yield confidence\nintervals that do not cover the true prevalence at the nominal level. In this\npaper, we have proposed a set of 95% confidence intervals, whose validity is\nguaranteed and doesn't depend on the sample size in the unweighted setting. For\nthe weighted setting, the proposed inference is equivalent to a class of hybrid\nbootstrap methods, whose performance is also more robust to the sample size\nthan those based on asymptotic approximations. The methods are used to\nreanalyze data from a study investigating the antibody prevalence in Santa\nClara county, California, which was the motivating example of this research, in\naddition to several other seroprevalence studies where authors had tried to\ncorrect their estimates for test performance. Extensive simulation studies have\nbeen conducted to examine the finite-sample performance of the proposed\nconfidence intervals.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 19:15:24 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Cai", "Bryan", ""], ["Ioannidis", "John P. A.", ""], ["Bendavid", "Eran", ""], ["Tian", "Lu", ""]]}, {"id": "2011.14437", "submitter": "Yuxiang Xie", "authors": "Yuxiang Xie and Meng Xu and Evan Chow and Xiaolin Shi", "title": "How to Measure Your App: A Couple of Pitfalls and Remedies in Measuring\n  App Performance in Online Controlled Experiments", "comments": "WSDM '21: Proceedings of the 14th International Conference on Web\n  Search and Data Mining", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effectively measuring, understanding, and improving mobile app performance is\nof paramount importance for mobile app developers. Across the mobile Internet\nlandscape, companies run online controlled experiments (A/B tests) with\nthousands of performance metrics in order to understand how app performance\ncausally impacts user retention and to guard against service or app regressions\nthat degrade user experiences. To capture certain characteristics particular to\nperformance metrics, such as enormous observation volume and high skewness in\ndistribution, an industry-standard practice is to construct a performance\nmetric as a quantile over all performance events in control or treatment\nbuckets in A/B tests. In our experience with thousands of A/B tests provided by\nSnap, we have discovered some pitfalls in this industry-standard way of\ncalculating performance metrics that can lead to unexplained movements in\nperformance metrics and unexpected misalignment with user engagement metrics.\nIn this paper, we discuss two major pitfalls in this industry-standard practice\nof measuring performance for mobile apps. One arises from strong heterogeneity\nin both mobile devices and user engagement, and the other arises from\nself-selection bias caused by post-treatment user engagement changes. To remedy\nthese two pitfalls, we introduce several scalable methods including user-level\nperformance metric calculation and imputation and matching for missing metric\nvalues. We have extensively evaluated these methods on both simulation data and\nreal A/B tests, and have deployed them into Snap's in-house experimentation\nplatform.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 20:06:43 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Xie", "Yuxiang", ""], ["Xu", "Meng", ""], ["Chow", "Evan", ""], ["Shi", "Xiaolin", ""]]}, {"id": "2011.14646", "submitter": "Ivan Stelmakh", "authors": "Ivan Stelmakh, Nihar B. Shah, Aarti Singh, and Hal Daum\\'e III", "title": "Prior and Prejudice: The Novice Reviewers' Bias against Resubmissions in\n  Conference Peer Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern machine learning and computer science conferences are experiencing a\nsurge in the number of submissions that challenges the quality of peer review\nas the number of competent reviewers is growing at a much slower rate. To curb\nthis trend and reduce the burden on reviewers, several conferences have started\nencouraging or even requiring authors to declare the previous submission\nhistory of their papers. Such initiatives have been met with skepticism among\nauthors, who raise the concern about a potential bias in reviewers'\nrecommendations induced by this information. In this work, we investigate\nwhether reviewers exhibit a bias caused by the knowledge that the submission\nunder review was previously rejected at a similar venue, focusing on a\npopulation of novice reviewers who constitute a large fraction of the reviewer\npool in leading machine learning and computer science conferences. We design\nand conduct a randomized controlled trial closely replicating the relevant\ncomponents of the peer-review pipeline with $133$ reviewers (master's, junior\nPhD students, and recent graduates of top US universities) writing reviews for\n$19$ papers. The analysis reveals that reviewers indeed become negatively\nbiased when they receive a signal about paper being a resubmission, giving\nalmost 1 point lower overall score on a 10-point Likert item ($\\Delta = -0.78,\n\\ 95\\% \\ \\text{CI} = [-1.30, -0.24]$) than reviewers who do not receive such a\nsignal. Looking at specific criteria scores (originality, quality, clarity and\nsignificance), we observe that novice reviewers tend to underrate quality the\nmost.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 09:35:37 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Stelmakh", "Ivan", ""], ["Shah", "Nihar B.", ""], ["Singh", "Aarti", ""], ["Daum\u00e9", "Hal", "III"]]}, {"id": "2011.14817", "submitter": "Christophe Ley", "authors": "Sla{\\dj}ana Babi\\'c and Christophe Ley and Lorenzo Ricci and David\n  Veredas", "title": "TailCoR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Economic and financial crises are characterised by unusually large events.\nThese tail events co-move because of linear and/or nonlinear dependencies. We\nintroduce TailCoR, a metric that combines (and disentangles) these linear and\nnon-linear dependencies. TailCoR between two variables is based on the tail\ninter quantile range of a simple projection. It is dimension-free, it performs\nwell in small samples, and no optimisations are needed.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 20:58:58 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Babi\u0107", "Sla\u0111ana", ""], ["Ley", "Christophe", ""], ["Ricci", "Lorenzo", ""], ["Veredas", "David", ""]]}, {"id": "2011.14996", "submitter": "Emily C Hector", "authors": "Emily C. Hector and Peter X.-K. Song", "title": "Joint integrative analysis of multiple data sources with correlated\n  vector outcomes", "comments": "26 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a distributed quadratic inference function framework to jointly\nestimate regression parameters from multiple potentially heterogeneous data\nsources with correlated vector outcomes. The primary goal of this joint\nintegrative analysis is to estimate covariate effects on all outcomes through a\nmarginal regression model in a statistically and computationally efficient way.\nWe develop a data integration procedure for statistical estimation and\ninference of regression parameters that is implemented in a fully distributed\nand parallelized computational scheme. To overcome computational and modeling\nchallenges arising from the high-dimensional likelihood of the correlated\nvector outcomes, we propose to analyze each data source using Qu, Lindsay and\nLi (2000)'s quadratic inference functions, and then to jointly reestimate\nparameters from each data source by accounting for correlation between data\nsources using a combined meta-estimator in a similar spirit to Hansen (1982)'s\ngeneralised method of moments. We show both theoretically and numerically that\nthe proposed method yields efficiency improvements and is computationally fast.\nWe illustrate the proposed methodology with the joint integrative analysis of\nthe association between smoking and metabolites in a large multi-cohort study\nand provide an R package for ease of implementation.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 17:03:04 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Hector", "Emily C.", ""], ["Song", "Peter X. -K.", ""]]}, {"id": "2011.15034", "submitter": "Dorsa Mohammadi Arezooji", "authors": "Dorsa Mohammadi Arezooji", "title": "A Markov Chain Monte-Carlo Approach to Dose-Response Optimization Using\n  Probabilistic Programming (RStan)", "comments": "12 pages, 9 figures \"for code source, see\n  https://github.com/Dorsa-Arezooji/Sick-Pigs\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A hierarchical logistic regression Bayesian model is proposed and implemented\nin R to model the probability of patient improvement corresponding to any given\ndosage of a certain drug. RStan is used to obtain samples from the posterior\ndistributions via Markov Chain Monte-Carlo (MCMC) sampling. The effects of\nselecting different families of prior distributions are examined and finally,\nthe posterior distributions are compared across RStan, and two other\nenvironments, namely PyMC, and AgenaRisk.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 17:37:42 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Arezooji", "Dorsa Mohammadi", ""]]}, {"id": "2011.15083", "submitter": "Ivan Stelmakh", "authors": "Ivan Stelmakh, Charvi Rastogi, Nihar B. Shah, Aarti Singh, and Hal\n  Daum\\'e III", "title": "A Large Scale Randomized Controlled Trial on Herding in Peer-Review\n  Discussions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Peer review is the backbone of academia and humans constitute a cornerstone\nof this process, being responsible for reviewing papers and making the final\nacceptance/rejection decisions. Given that human decision making is known to be\nsusceptible to various cognitive biases, it is important to understand which\n(if any) biases are present in the peer-review process and design the pipeline\nsuch that the impact of these biases is minimized. In this work, we focus on\nthe dynamics of between-reviewers discussions and investigate the presence of\nherding behaviour therein. In that, we aim to understand whether reviewers and\nmore senior decision makers get disproportionately influenced by the first\nargument presented in the discussion when (in case of reviewers) they form an\nindependent opinion about the paper before discussing it with others.\nSpecifically, in conjunction with the review process of ICML 2020 -- a large,\ntop tier machine learning conference -- we design and execute a randomized\ncontrolled trial with the goal of testing for the conditional causal effect of\nthe discussion initiator's opinion on the outcome of a paper.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 18:23:07 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Stelmakh", "Ivan", ""], ["Rastogi", "Charvi", ""], ["Shah", "Nihar B.", ""], ["Singh", "Aarti", ""], ["Daum\u00e9", "Hal", "III"]]}, {"id": "2011.15099", "submitter": "Roy Adams", "authors": "Roy Adams, Suchi Saria, Michael Rosenblum", "title": "The Impact of Time Series Length and Discretization on Longitudinal\n  Causal Estimation Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of observational time series data to assess the impact of multi-time\npoint interventions is becoming increasingly common as more health and activity\ndata are collected and digitized via wearables, social media, and electronic\nhealth records. Such time series may involve hundreds or thousands of\nirregularly sampled observations. One common analysis approach is to simplify\nsuch time series by first discretizing them into sequences before applying a\ndiscrete-time estimation method that adjusts for time-dependent confounding. In\ncertain settings, this discretization results in sequences with many time\npoints; however, the empirical properties of longitudinal causal estimators\nhave not been systematically compared on long sequences. We compare three\nrepresentative longitudinal causal estimation methods on simulated and real\nclinical data. Our simulations and analyses assume a Markov structure and that\nlongitudinal treatments/exposures are binary-valued and have at most a single\njump point. We identify sources of bias that arise from temporally discretizing\nthe data and provide practical guidance for discretizing data and choosing\nbetween methods when working with long sequences. Additionally, we compare\nthese estimators on real electronic health record data, evaluating the impact\nof early treatment for patients with a life-threatening complication of\ninfection called sepsis.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 18:32:19 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Adams", "Roy", ""], ["Saria", "Suchi", ""], ["Rosenblum", "Michael", ""]]}]