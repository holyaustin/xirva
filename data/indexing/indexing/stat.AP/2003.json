[{"id": "2003.00029", "submitter": "Yaoyuan Vincent Tan", "authors": "Yaoyuan Vincent Tan, Donna Coffman, Megan Piper, and Jason Roy", "title": "Estimating the impact of treatment compliance over time on smoking\n  cessation using data from ecological momentary assessments (EMA)", "comments": "26 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Wisconsin Smoker's Health Study (WSHS2) was a longitudinal trial\nconducted to compare the effectiveness of two commonly used smoking cessation\ntreatments, varenicline and combination nicotine replacement therapy (cNRT)\nwith the less intense standard of care, nicotine patch. The main outcome of the\nWSHS2 study was that all three treatments had equivalent treatment effects.\nHowever, in-depth analysis of the compliance data collected via ecological\nmomentary assessment (EMA) were not analyzed. Compliance to the treatment\nregimens may represent a confounder as varenicline and cNRT are more intense\ntreatments and would likely have larger treatment effects if all subjects\ncomplied. In order to estimate the causal compliance effect, we view the\ncounterfactual, the outcome that would have been observed if the subject was\nallocated to the treatment counter to the fact, as a missing data problem and\nproceed to impute the counterfactual. Our contribution to the methodological\nliterature lies in the extension of this idea to a more general analytic\napproach that includes mediators and confounders of the mediator-outcome\nrelationship. Simulation results suggest that our method works well and\napplication to the WSHS2 data suggest that the treatment effect of nicotine\npatch, varenicline, and cNRT are equivalent after accounting for differences in\ntreatment compliance.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 19:16:13 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Tan", "Yaoyuan Vincent", ""], ["Coffman", "Donna", ""], ["Piper", "Megan", ""], ["Roy", "Jason", ""]]}, {"id": "2003.00043", "submitter": "Irene Epifanio", "authors": "Ismael Cabero, Irene Epifanio", "title": "Finding archetypal patterns for binary questionnaires", "comments": null, "journal-ref": "SORT, 44(1): 39-66. 2020", "doi": "10.2436/20.8080.02.94", "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Archetypal analysis is an exploratory tool that explains a set of\nobservations as mixtures of pure (extreme) patterns. If the patterns are actual\nobservations of the sample, we refer to them as archetypoids. For the first\ntime, we propose to use archetypoid analysis for binary observations. This tool\ncan contribute to the understanding of a binary data set, as in the\nmultivariate case. We illustrate the advantages of the proposed methodology in\na simulation study and two applications, one exploring objects (rows) and the\nother exploring items (columns). One is related to determining student skill\nset profiles and the other to describing item response functions.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 20:01:24 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Cabero", "Ismael", ""], ["Epifanio", "Irene", ""]]}, {"id": "2003.00060", "submitter": "Xiao Hui Tai", "authors": "Xiao Hui Tai and William F. Eddy", "title": "Automatically matching topographical measurements of cartridge cases\n  using a record linkage framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Firing a gun leaves marks on cartridge cases which purportedly uniquely\nidentify the gun. Firearms examiners typically use a visual examination to\nevaluate if two cartridge cases were fired from the same gun, and this is a\nsubjective process that has come under scrutiny. Matching can be done in a more\nreproducible manner using automated algorithms. In this paper, we develop\nmethodology to compare topographical measurements of cartridge cases. We\ndemonstrate the use of a record linkage framework in this context. We compare\nperformance using topographical measurements to older reflectance microscopy\nimages, investigating the extent to which the former produce more accurate\ncomparisons. Using a diverse collection of images of over 1,100 cartridge\ncases, we find that overall performance is generally improved using\ntopographical data. Some subsets of the data achieve almost perfect predictive\nperformance in terms of precision and recall, while some produce extremely poor\nperformance. Further work needs to be done to assess if examiners face similar\ndifficulties on certain gun and ammunition combinations. For automatic methods,\na fuller investigation into their fairness and robustness is necessary before\nthey can be deployed in practice.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 20:49:41 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Tai", "Xiao Hui", ""], ["Eddy", "William F.", ""]]}, {"id": "2003.00167", "submitter": "Wangsheng Liu Dr.", "authors": "Wang-Sheng Liu, Sai Hung Cheung", "title": "Design optimization of stochastic complex systems via iterative density\n  estimation", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliability-based design optimization (RBDO) provides a rational and sound\nframework for finding the optimal design while taking uncertainties into\nac-count. The main issue in implementing RBDO methods, particularly stochastic\nsimu-lation based ones, is the computational burden arising from the evaluation\nof reliability constraints. In this contribution, we propose an efficient\nmethod which ap-proximates the failure probability functions (FPF) to decouple\nreliability. Based on the augmentation concept, the approximation of FPF is\nequivalent to density estimation of failure design samples. Unlike traditional\ndensity estimation schemes, where the esti-mation is conducted in the entire\ndesign space, in the proposed method we iteratively partition the design space\ninto several subspaces according to the distribution of fail-ure design\nsamples. Numerical results of an illustrative example indicate that the\npro-posed method can improve the computational performance considerably.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 03:12:40 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Liu", "Wang-Sheng", ""], ["Cheung", "Sai Hung", ""]]}, {"id": "2003.00316", "submitter": "Mohsen Sadatsafavi", "authors": "Mohsen Sadatsafavi, Paramita Saha-Chaudhuri, John Petkau", "title": "Model-based ROC (mROC) curve: examining the effect of case-mix and model\n  calibration on the ROC plot", "comments": "44 pages, 1 table, 5 figures, 6 appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of risk prediction models is often characterized in terms of\ndiscrimination and calibration. The Receiver Operating Characteristic (ROC)\ncurve is widely used for evaluating model discrimination. When evaluating the\nperformance of a risk prediction model in a new sample, the shape of the ROC\ncurve is affected by both case-mix and the postulated model. Further, compared\nto discrimination, evaluating calibration has not received the same level of\nattention. Commonly used methods for model calibration involve subjective\nspecification of smoothing or grouping. Leveraging the familiar ROC framework,\nwe introduce the model-based ROC (mROC) curve to assess the calibration of a\npre-specified model in a new sample. mROC curve is the ROC curve that should be\nobserved if a pre-specified model is calibrated in the sample. We show the\nempirical ROC and mROC curves for a sample converge asymptotically if the model\nis calibrated in that sample. As a consequence, the mROC curve can be used to\nassess visually the effect of case-mix and model mis-calibration. Further, we\npropose a novel statistical test for calibration that does not require any\nsmoothing or grouping. Simulations support the adequacy of the test. A case\nstudy puts these developments in a practical context. We conclude that mROC can\neasily be constructed and used to evaluate the effect of case-mix and model\ncalibration on the ROC plot, thus adding to the utility of ROC curve analysis\nin the evaluation of risk prediction models. R code for the proposed\nmethodology is provided (https://github.com/msadatsafavi/mROC/).\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 17:34:41 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 22:24:06 GMT"}, {"version": "v3", "created": "Mon, 12 Jul 2021 15:21:48 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Sadatsafavi", "Mohsen", ""], ["Saha-Chaudhuri", "Paramita", ""], ["Petkau", "John", ""]]}, {"id": "2003.00401", "submitter": "Austin Schumacher", "authors": "Austin E Schumacher, Tyler H McCormick, Jon Wakefield, Yue Chu, Jamie\n  Perin, Francisco Villavicencio, Noah Simon and Li Liu", "title": "A flexible Bayesian framework to estimate age- and cause-specific child\n  mortality over time from sample registration data", "comments": "16 pages, 4 figures, submitted to The Annals of Applied Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to implement disease-specific interventions in young age groups,\npolicy makers in low- and middle-income countries require timely and accurate\nestimates of age- and cause-specific child mortality. High quality data is not\navailable in settings where these interventions are most needed, but there is a\npush to create sample registration systems that collect detailed mortality\ninformation. Current methods that estimate mortality from this data employ\nmultistage frameworks without rigorous statistical justification that\nseparately estimate all-cause and cause-specific mortality and are not\nsufficiently adaptable to capture important features of the data. We propose a\nflexible Bayesian modeling framework to estimate age- and cause-specific child\nmortality from sample registration data. We provide a theoretical justification\nfor the framework, explore its properties via simulation, and use it to\nestimate mortality trends using data from the Maternal and Child Health\nSurveillance System in China.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 04:47:27 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 06:29:02 GMT"}, {"version": "v3", "created": "Tue, 18 May 2021 17:56:58 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Schumacher", "Austin E", ""], ["McCormick", "Tyler H", ""], ["Wakefield", "Jon", ""], ["Chu", "Yue", ""], ["Perin", "Jamie", ""], ["Villavicencio", "Francisco", ""], ["Simon", "Noah", ""], ["Liu", "Li", ""]]}, {"id": "2003.00501", "submitter": "Dominik Reinhard", "authors": "Dominik Reinhard and Michael Fau{\\ss} and Abdelhak M. Zoubir", "title": "Distributed Joint Detection and Estimation: A Sequential Approach", "comments": "6 pages, 3 figures, accepted for publication in the proceedings of\n  the 54th Annual Conference on Information Sciences and Systems 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of jointly testing two hypotheses and estimating a\nrandom parameter based on data that is observed sequentially by sensors in a\ndistributed network. In particular, we assume the data to be drawn from a\nGaussian distribution, whose random mean is to be estimated. Forgoing the need\nfor a fusion center, the processing is performed locally and the sensors\ninteract with their neighbors following the consensus+innovations approach. We\ndesign the test at the individual sensors such that the performance measures,\nnamely, error probabilities and mean-squared error, do not exceed pre-defined\nlevels while the average sample number is minimized. After converting the\nconstrained problem to an unconstrained problem and the subsequent reduction to\nan optimal stopping problem, we solve the latter utilizing dynamic programming.\nThe solution is shown to be characterized by a set of non-linear Bellman\nequations, parametrized by cost coefficients, which are then determined by\nlinear programming as to fulfill the performance specifications. A numerical\nexample validates the proposed theory.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 15:20:17 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 08:03:19 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Reinhard", "Dominik", ""], ["Fau\u00df", "Michael", ""], ["Zoubir", "Abdelhak M.", ""]]}, {"id": "2003.00898", "submitter": "George Adam", "authors": "Benjamin Haibe-Kains, George Alexandru Adam, Ahmed Hosny, Farnoosh\n  Khodakarami, MAQC Society Board, Levi Waldron, Bo Wang, Chris McIntosh,\n  Anshul Kundaje, Casey S. Greene, Michael M. Hoffman, Jeffrey T. Leek,\n  Wolfgang Huber, Alvis Brazma, Joelle Pineau, Robert Tibshirani, Trevor\n  Hastie, John P.A. Ioannidis, John Quackenbush, Hugo J.W.L. Aerts", "title": "The importance of transparency and reproducibility in artificial\n  intelligence research", "comments": null, "journal-ref": "Nature 586 (2020) E14-E16", "doi": "10.1038/s41586-020-2766-y", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In their study, McKinney et al. showed the high potential of artificial\nintelligence for breast cancer screening. However, the lack of detailed methods\nand computer code undermines its scientific value. We identify obstacles\nhindering transparent and reproducible AI research as faced by McKinney et al\nand provide solutions with implications for the broader field.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 16:23:31 GMT"}, {"version": "v2", "created": "Sat, 7 Mar 2020 17:06:27 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Haibe-Kains", "Benjamin", ""], ["Adam", "George Alexandru", ""], ["Hosny", "Ahmed", ""], ["Khodakarami", "Farnoosh", ""], ["Board", "MAQC Society", ""], ["Waldron", "Levi", ""], ["Wang", "Bo", ""], ["McIntosh", "Chris", ""], ["Kundaje", "Anshul", ""], ["Greene", "Casey S.", ""], ["Hoffman", "Michael M.", ""], ["Leek", "Jeffrey T.", ""], ["Huber", "Wolfgang", ""], ["Brazma", "Alvis", ""], ["Pineau", "Joelle", ""], ["Tibshirani", "Robert", ""], ["Hastie", "Trevor", ""], ["Ioannidis", "John P. A.", ""], ["Quackenbush", "John", ""], ["Aerts", "Hugo J. W. L.", ""]]}, {"id": "2003.00899", "submitter": "George Cevora", "authors": "Kate Wilkinson, George Cevora", "title": "Demonstrating Rosa: the fairness solution for any Data Analytic pipeline", "comments": "corrected typo in fig 8 caption", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Most datasets of interest to the analytics industry are impacted by various\nforms of human bias. The outcomes of Data Analytics [DA] or Machine Learning\n[ML] on such data are therefore prone to replicating the bias. As a result, a\nlarge number of biased decision-making systems based on DA/ML have recently\nattracted attention. In this paper we introduce Rosa, a free, web-based tool to\neasily de-bias datasets with respect to a chosen characteristic. Rosa is based\non the principles of Fair Adversarial Networks, developed by illumr Ltd., and\ncan therefore remove interactive, non-linear, and non-binary bias. Rosa is\nstand-alone pre-processing step / API, meaning it can be used easily with any\nDA/ML pipeline. We test the efficacy of Rosa in removing bias from data-driven\ndecision making systems by performing standard DA tasks on five real-world\ndatasets, selected for their relevance to current DA problems, and also their\nhigh potential for bias. We use simple ML models to model a characteristic of\nanalytical interest, and compare the level of bias in the model output both\nwith and without Rosa as a pre-processing step. We find that in all cases there\nis a substantial decrease in bias of the data-driven decision making systems\nwhen the data is pre-processed with Rosa.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 10:02:58 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 15:59:13 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Wilkinson", "Kate", ""], ["Cevora", "George", ""]]}, {"id": "2003.01168", "submitter": "Erin Schliep", "authors": "Erin M. Schliep, Alan E. Gelfand, Jesus Abaurrea, Jesus Asin, Maria A.\n  Beamonte, Ana C. Cebrian", "title": "Long-term Spatial Modeling for Characteristics of Extreme Heat Events", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is increasing evidence that global warming manifests itself in more\nfrequent warm days and that heat waves will become more frequent. Presently, a\nformal definition of a heat wave is not agreed upon in the literature. To avoid\nthis debate, we consider extreme heat events, which, at a given location, are\nwell-defined as a run of consecutive days above an associated local threshold.\nCharacteristics of EHEs are of primary interest, such as incidence and\nduration, as well as the magnitude of the average exceedance and maximum\nexceedance above the threshold during the EHE.\n  Using approximately 60-year time series of daily maximum temperature data\ncollected at 18 locations in a given region, we propose a spatio-temporal model\nto study the characteristics of EHEs over time. The model enables prediction of\nthe behavior of EHE characteristics at unobserved locations within the region.\nSpecifically, our approach employs a two-state space-time model for EHEs with\nlocal thresholds where one state defines above threshold daily maximum\ntemperatures and the other below threshold temperatures. We show that our model\nis able to recover the EHE characteristics of interest and outperforms a\ncorresponding autoregressive model that ignores thresholds based on\nout-of-sample prediction.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 20:03:58 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 20:08:46 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Schliep", "Erin M.", ""], ["Gelfand", "Alan E.", ""], ["Abaurrea", "Jesus", ""], ["Asin", "Jesus", ""], ["Beamonte", "Maria A.", ""], ["Cebrian", "Ana C.", ""]]}, {"id": "2003.01176", "submitter": "Chirag Nagpal", "authors": "Chirag Nagpal, Xinyu Rachel Li and Artur Dubrawski", "title": "Deep Survival Machines: Fully Parametric Survival Regression and\n  Representation Learning for Censored Data with Competing Risks", "comments": "Also appeared in NeurIPS 2019 Workshop on Machine Learning for\n  Healthcare (ML4H)", "journal-ref": "IEEE Journal of Biomedical and Health Informatics, 2021", "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new approach to estimating relative risks in time-to-event\nprediction problems with censored data in a fully parametric manner. Our\napproach does not require making strong assumptions of constant proportional\nhazard of the underlying survival distribution, as required by the\nCox-proportional hazard model. By jointly learning deep nonlinear\nrepresentations of the input covariates, we demonstrate the benefits of our\napproach when used to estimate survival risks through extensive experimentation\non multiple real world datasets with different levels of censoring. We further\ndemonstrate advantages of our model in the competing risks scenario. To the\nbest of our knowledge, this is the first work involving fully parametric\nestimation of survival times with competing risks in the presence of censoring.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 20:21:59 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2021 22:18:44 GMT"}, {"version": "v3", "created": "Wed, 9 Jun 2021 12:09:21 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Nagpal", "Chirag", ""], ["Li", "Xinyu Rachel", ""], ["Dubrawski", "Artur", ""]]}, {"id": "2003.01319", "submitter": "Joe Watson", "authors": "Joe Watson", "title": "A fast Monte Carlo test for preferential sampling", "comments": "24 pages, 4 figures, plus 10 pages of supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The preferential sampling of locations chosen to observe a spatio-temporal\nprocess has been identified as a major problem across multiple fields.\nPredictions of the process can be severely biased when standard statistical\nmethodologies are applied to preferentially sampled data without adjustment.\nCurrently, methods that can adjust for preferential sampling are rarely\nimplemented in the software packages most popular with researchers.\nFurthermore, they are technically demanding to design and fit. This paper\npresents a fast and intuitive Monte Carlo test for detecting preferential\nsampling. The test can be applied across a wide range of data types.\nImportantly, the method can also help with the discovery of a set of\ninformative covariates that can sufficiently control for the preferential\nsampling. The discovery of these covariates can justify continued use of\nstandard methodologies. A thorough simulation study is presented to demonstrate\nboth the power and validity of the test in various data settings. The test is\nshown to attain high power for non-Gaussian data with sample sizes as low as\n50. Finally, two previously-published case studies are revisited and new\ninsights into the nature of the informative sampling are gained. The test can\nbe implemented with the R package PStestR\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 03:48:48 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 02:22:14 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Watson", "Joe", ""]]}, {"id": "2003.01352", "submitter": "Sarit Agami", "authors": "Sarit Agami", "title": "Comparison of Persistence Diagrams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topological Data Analysis (TDA) is an approach to handle with big data by\nstudying its shape. A main tool of TDA is the persistence diagram, and one can\nuse it to compare data sets. One approach to learn on the similarity between\ntwo persistence diagrams is to use the Bottleneck and the Wasserstein\ndistances. Another approach is to fit a parametric model for each diagram, and\nthen to compare the model coefficients. We study the behaviour of both distance\nmeasures and the RST parametric model. The theoretical behaviour of the\ndistance measures is difficult to be developed, and therefore we study their\nbehaviour numerically. We conclude that the RST model has an advantage over the\nBottleneck and the Wasserstein distances in sense that it can give a definite\nconclusion regarding the similarity between two persistence diagrams. More of\nthat, a great advantage of the RST is its ability to distinguish between two\ndata sets that are geometrically different but topologically are the same,\nwhich is impossible to have by the two distance measures.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 06:12:17 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Agami", "Sarit", ""]]}, {"id": "2003.01489", "submitter": "Dong Liu", "authors": "Dong Liu and Viktoria Fodor and Lars K. Rasmussen", "title": "Will Scale-free Popularity Develop Scale-free Geo-social Networks?", "comments": null, "journal-ref": "IEEE Transactions on Network Science and Engineering, 2019", "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical results show that spatial factors such as distance, population\ndensity and communication range affect our social activities, also reflected by\nthe development of ties in social networks. This motivates the need for social\nnetwork models that take these spatial factors into account. Therefore, in this\npaper we propose a gravity-low-based geo-social network model, where\nconnections develop according to the popularity of the individuals, but are\nconstrained through their geographic distance and the surrounding population\ndensity. Specifically, we consider a power-law distributed popularity, and\nrandom node positions governed by a Poisson point process. We evaluate the\ncharacteristics of the emerging networks, considering the degree distribution,\nthe average degree of neighbors and the local clustering coefficient. These\nlocal metrics reflect the robustness of the network, the information\ndissemination speed and the communication locality. We show that unless the\ncommunication range is strictly limited, the emerging networks are scale-free,\nwith a rank exponent affected by the spatial factors. Even the average neighbor\ndegree and the local clustering coefficient show tendencies known in\nnon-geographic scale-free networks, at least when considering individuals with\nlow popularity. At high-popularity values, however, the spatial constraints\nlead to popularity-independent average neighbor degrees and clustering\ncoefficients.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 12:57:43 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Liu", "Dong", ""], ["Fodor", "Viktoria", ""], ["Rasmussen", "Lars K.", ""]]}, {"id": "2003.01712", "submitter": "Jan Van Haaren", "authors": "Lotte Bransen, Jan Van Haaren", "title": "Player Chemistry: Striving for a Perfectly Balanced Soccer Team", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Soccer scouts typically ignore the team balance and team chemistry when\nevaluating potential signings for their teams. Instead, they focus on the\nindividual qualities of the players in isolation. To overcome this limitation\nof their recruitment process, this paper takes a first step towards objectively\nproviding insight into the question: How well does a team of soccer players\ngel? We address that question in both an observational and a predictive\nsetting. In the former setting, we observe the chemistry between players who\nhave actually played together, which is relevant when selecting the best\npossible line-up for a match. In the latter setting, we predict the chemistry\nbetween players who have never played together before, which is particularly\nrelevant to assess the fit of a potential signing with the players who are\nalready on the team.\n  We introduce two chemistry metrics that measure the offensive and defensive\nchemistry for a pair of players, respectively. The offensive chemistry metric\nmeasures the pair's joint performance in terms of scoring goals, whereas the\ndefensive chemistry metric measures their joint performance in preventing their\nopponents from scoring goals. We compute our metrics for 361 seasons in 106\ndifferent competitions and present a number of concrete use cases. For\ninstance, we show that the partnership between Mohamed Salah and Roberto\nFirmino in Liverpool's 2017/2018 Champions League campaign exhibited the\nhighest mutual chemistry between two players. Furthermore, we show that Mesut\n\\\"Ozil's chemistry has rapidly started declining following Alexis S\\'anchez'\ndeparture to Manchester United in 2018.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 07:25:32 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Bransen", "Lotte", ""], ["Van Haaren", "Jan", ""]]}, {"id": "2003.01728", "submitter": "Benjamin Laevens", "authors": "Benjamin P. M. Laevens, Olav ten Bosch, Frank P. Pijpers and Wilfried\n  G. J. H. M. van Sark", "title": "Observational daily and regional photovoltaic solar energy production\n  for the Netherlands", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.data-an physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a classical estimation problem for calculating the energy\ngenerated by photovoltaic solar energy systems in the Netherlands, on a daily,\nannual and regional basis. We identify two data sources to construct our\nmethodology: pvoutput, an online portal with solar energy yield measurements,\nand modelled irradiance data, from the Royal Netherlands Meteorological\nInstitute. Combining these, we obtain probability functions of observing energy\nyields, given the irradiance, which we then apply to our PV systems database,\nallowing us to calculate daily and annual solar energy yields. We examine the\nvariation in our daily and annual estimates as a result of taking different\nsubsets of pvoutput systems with certain specifications such as orientation,\ntilt and inverter to PV capacity ratio. Hence we obtain specific annual energy\nyields in the range of 877-946 kWh/kWp and 838-899 kWh/kWp for 2016 and 2017\nrespectively. The current method used at Statistics Netherlands assumes this to\nbe 875 kWh/kWp, meaning the yields were underestimated and overestimated for\n2016 and 2017 respectively. Finally, we translate our national estimates into\nsolar energy yields per municipality. This research demonstrates that an\nirradiance based measure of solar energy generation is necessary to obtain more\naccurate energy yields on both a national and regional level.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 19:00:02 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 16:03:04 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Laevens", "Benjamin P. M.", ""], ["Bosch", "Olav ten", ""], ["Pijpers", "Frank P.", ""], ["van Sark", "Wilfried G. J. H. M.", ""]]}, {"id": "2003.01750", "submitter": "Olivier Walther", "authors": "Olivier J. Walther, Steven M. Radil, David Russell, Marie\n  Tr\\'emoli\\`eres", "title": "Introducing the Spatial Conflict Dynamics indicator of political\n  violence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern armed conflicts have a tendency to cluster together and spread\ngeographically. However, the geography of most conflicts remains under-studied.\nTo fill this gap, this article presents a new indicator that measures two key\ngeographical properties of subnational political violence: the conflict\nintensity within a region on the one hand, and the spatial distribution of\nconflict within a region on the other. We demonstrate the indicator in North\nand West Africa between 1997 to 2019 to show that it can clarify how conflicts\ncan spread from place to place and how the geography of conflict changes over\ntime.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 19:25:08 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 13:45:04 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Walther", "Olivier J.", ""], ["Radil", "Steven M.", ""], ["Russell", "David", ""], ["Tr\u00e9moli\u00e8res", "Marie", ""]]}, {"id": "2003.01850", "submitter": "Jiayin Zheng", "authors": "Zheng Jiayin, Zheng Yingye and Hsu Li", "title": "Risk Projection for Time-to-event Outcome Leveraging Summary Statistics\n  With Source Individual-level Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting risks of chronic diseases has become increasingly important in\nclinical practice. When a prediction model is developed in a given source\ncohort, there is often a great interest to apply the model to other cohorts.\nHowever, due to potential discrepancy in baseline disease incidences between\ndifferent cohorts and shifts in patient composition, the risk predicted by the\noriginal model often under- or over-estimates the risk in the new cohort. The\nremedy of such a poorly calibrated prediction is needed for proper medical\ndecision-making. In this article, we assume the relative risks of predictors\nare the same between the two cohorts, and propose a novel weighted estimating\nequation approach to re-calibrating the projected risk for the targeted\npopulation through updating the baseline risk. The recalibration leverages the\nknowledge about the overall survival probabilities for the disease of interest\nand competing events, and the summary information of risk factors from the\ntargeted population. The proposed re-calibrated risk estimators gain efficiency\nif the risk factor distributions are the same for both the source and target\ncohorts, and are robust with little bias if they differ. We establish the\nconsistency and asymptotic normality of the proposed estimators. Extensive\nsimulation studies demonstrate that the proposed estimators perform very well\nin terms of robustness and efficiency in finite samples. A real data\napplication to colorectal cancer risk prediction also illustrates that the\nproposed method can be used in practice for model recalibration.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 01:19:30 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Jiayin", "Zheng", ""], ["Yingye", "Zheng", ""], ["Li", "Hsu", ""]]}, {"id": "2003.01860", "submitter": "Jae Youn Ahn", "authors": "Rosy Oh, Joseph H.T. Kim, Jae Youn Ahn", "title": "Designing a Bonus-Malus system reflecting the claim size under the\n  dependent frequency-severity model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In auto insurance, a Bonus-Malus System (BMS) is commonly used as a\nposteriori risk classification mechanism to set the premium for the next\ncontract period based on a policyholder's claim history. Even though recent\nliterature reports evidence of a significant dependence between frequency and\nseverity, the current BMS practice is to use a frequency-based transition rule\nwhile ignoring severity information. Although Oh et al. (2019) claim that the\nfrequency-driven BMS transition rule can accommodate the dependence between\nfrequency and severity, their proposal is only a partial solution, as the\ntransition rule still completely ignores the claim severity and is unable to\npenalize large claims. In this study, we propose to use the BMS with a\ntransition rule based on both frequency and size of claim, based on the\nbivariate random effect model, which conveniently allows dependence between\nfrequency and severity. We analytically derive the optimal relativities under\nthe proposed BMS framework and show that the proposed BMS outperforms the\nexisting frequency-driven BMS. Later numerical experiments are also provided\nusing both hypothetical and actual datasets in order to assess the effect of\nvarious dependencies on the BMS risk classification and confirm our theoretical\nfindings.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 02:12:13 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Oh", "Rosy", ""], ["Kim", "Joseph H. T.", ""], ["Ahn", "Jae Youn", ""]]}, {"id": "2003.01864", "submitter": "Esdras Medeiros", "authors": "Esdras Medeiros, Jorge Lira, Romildo Silva and Caio Azevedo", "title": "Visualizing and Understanding Large-Scale Assessments in Mathematics\n  through Dimensionality Reduction", "comments": "To be submitted for a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we apply the Logistic PCA (LPCA) as a dimensionality reduction\ntool for visualizing patterns and characterizing the relevance of mathematics\nabilities from a given population measured by a large-scale assessment. We\nestablish an equivalence of parameters between LPCA, Inner Product\nRepresentation (IPR) and the two paramenter logistic model (2PL) from the Item\nResponse Theory (IRT). This equivalence provides three complemetary ways of\nlooking at data that assists professionals in education to perform in-context\ninterpretations. Particularly, we analyse the data collected from SPAECE, a\nlarge-scale assessment in Mathematics that has been applied yearly in the\npublic educational system of the state of Cear\\'a, Brazil. As the main result,\nwe show that the the poor performance of examinees in the end of middle school\nis primarily caused by their disabilities in number sense.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 02:39:17 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 22:41:48 GMT"}, {"version": "v3", "created": "Sun, 31 May 2020 14:53:10 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Medeiros", "Esdras", ""], ["Lira", "Jorge", ""], ["Silva", "Romildo", ""], ["Azevedo", "Caio", ""]]}, {"id": "2003.01946", "submitter": "Aritz Adin", "authors": "A. Adin, T. Goicoa, J. S. Hodges, P. Schnell and M. D. Ugarte", "title": "Alleviating confounding in spatio-temporal areal models with an\n  application on crimes against women in India", "comments": null, "journal-ref": "Statistical Modelling 2021", "doi": "10.1177/1471082X211015452", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing associations between a response of interest and a set of covariates\nin spatial areal models is the leitmotiv of ecological regression. However, the\npresence of spatially correlated random effects can mask or even bias estimates\nof such associations due to confounding effects if they are not carefully\nhandled. Though potentially harmful, confounding issues have often been ignored\nin practice leading to wrong conclusions about the underlying associations\nbetween the response and the covariates. In spatio-temporal areal models, the\ntemporal dimension may emerge as a new source of confounding, and the problem\nmay be even worse. In this work, we propose two approaches to deal with\nconfounding of fixed effects by spatial and temporal random effects, while\nobtaining good model predictions. In particular, restricted regression and an\napparently -- though in fact not -- equivalent procedure using constraints are\nproposed within both fully Bayes and empirical Bayes approaches. The methods\nare compared in terms of fixed-effect estimates and model selection criteria.\nThe techniques are used to assess the association between dowry deaths and\ncertain socio-demographic covariates in the districts of Uttar Pradesh, India.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 08:36:11 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 10:03:31 GMT"}, {"version": "v3", "created": "Wed, 7 Apr 2021 17:09:04 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Adin", "A.", ""], ["Goicoa", "T.", ""], ["Hodges", "J. S.", ""], ["Schnell", "P.", ""], ["Ugarte", "M. D.", ""]]}, {"id": "2003.02106", "submitter": "Markus Loecher", "authors": "Markus Loecher", "title": "Unbiased variable importance for random forests", "comments": null, "journal-ref": null, "doi": "10.1080/03610926.2020.1764042", "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The default variable-importance measure in random Forests, Gini importance,\nhas been shown to suffer from the bias of the underlying Gini-gain splitting\ncriterion. While the alternative permutation importance is generally accepted\nas a reliable measure of variable importance, it is also computationally\ndemanding and suffers from other shortcomings. We propose a simple solution to\nthe misleading/untrustworthy Gini importance which can be viewed as an\noverfitting problem: we compute the loss reduction on the out-of-bag instead of\nthe in-bag training samples.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 14:40:31 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 07:47:01 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Loecher", "Markus", ""]]}, {"id": "2003.02155", "submitter": "Thomas Opitz", "authors": "Patrizia Zamberletti, Julien Papa\\\"ix, Edith Gabriel, Thomas Opitz", "title": "Landscape allocation: stochastic generators and statistical inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In agricultural landscapes, the composition and spatial configuration of\ncultivated and semi-natural elements strongly impact species dynamics, their\ninteractions and habitat connectivity. To allow for landscape structural\nanalysis and scenario generation, we here develop statistical tools for real\nlandscapes composed of geometric elements including 2D patches but also 1D\nlinear elements such as hedges. We design generative stochastic models that\ncombine a multiplex network representation and Gibbs energy terms to\ncharacterize the distributional behavior of landscape descriptors for land-use\ncategories. We implement Metropolis-Hastings for this new class of models to\nsample agricultural scenarios featuring parameter-controlled spatial and\ntemporal patterns (e.g., geometry, connectivity, crop-rotation).\nPseudolikelihood-based inference allows studying the relevance of model\ncomponents in real landscapes through statistical and functional validation,\nthe latter achieved by comparing commonly used landscape metrics between\nobserved and simulated landscapes. Models fitted to subregions of the Lower\nDurance Valley (France) indicate strong deviation from random allocation, and\nthey realistically capture small-scale landscape patterns. In summary, our\napproach of statistical modeling improves the understanding of structural and\nfunctional aspects of agro-ecosystems, and it enables simulation-based\ntheoretical analysis of how landscape patterns shape biological and ecological\nprocesses.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 09:07:33 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Zamberletti", "Patrizia", ""], ["Papa\u00efx", "Julien", ""], ["Gabriel", "Edith", ""], ["Opitz", "Thomas", ""]]}, {"id": "2003.02205", "submitter": "Marco Broccardo", "authors": "Ziqi Wang, Marco Broccardo, Junho Song", "title": "Probabilistic Performance-Pattern Decomposition (PPPD): analysis\n  framework and applications to stochastic mechanical systems", "comments": "Autoencoder, clustering, diffusion map, manifold learning, Monte\n  Carlo simulation, pattern recognition, stochastic dynamics, uncertainty\n  quantification. 44 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the early 1900s, numerous research efforts have been devoted to\ndeveloping quantitative solutions to stochastic mechanical systems. In general,\nthe problem is perceived as solved when a complete or partial probabilistic\ndescription on the quantity of interest (QoI) is determined. However, in the\npresence of complex system behavior, there is a critical need to go beyond mere\nprobabilistic descriptions. In fact, to gain a full understanding of the\nsystem, it is crucial to extract physical characterizations from the\nprobabilistic structure of the QoI, especially when the QoI solution is\nobtained in a data-driven fashion. Motivated by this perspective, the paper\nproposes a framework to obtain structuralized characterizations on behaviors of\nstochastic systems. The framework is named Probabilistic Performance-Pattern\nDecomposition (PPPD). PPPD analysis aims to decompose complex response\nbehaviors, conditional to a prescribed performance state, into meaningful\npatterns in the space of system responses, and to investigate how the patterns\nare triggered in the space of basic random variables. To illustrate the\napplication of PPPD, the paper studies three numerical examples: 1) an\nillustrative example with hypothetical stochastic processes input and output;\n2) a stochastic Lorenz system with periodic as well as chaotic behaviors; and\n3) a simplified shear-building model subjected to a stochastic ground motion\nexcitation.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 17:18:43 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Wang", "Ziqi", ""], ["Broccardo", "Marco", ""], ["Song", "Junho", ""]]}, {"id": "2003.02208", "submitter": "Michael Schomaker", "authors": "Philipp F.M. Baumann, Michael Schomaker, Enzo Rossi", "title": "Estimating the Effect of Central Bank Independence on Inflation Using\n  Longitudinal Targeted Maximum Likelihood Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion that an independent central bank reduces a country's inflation is\na controversial hypothesis. To date, it has not been possible to satisfactorily\nanswer this question because the complex macroeconomic structure that gives\nrise to the data has not been adequately incorporated into statistical\nanalyses. We develop a causal model that summarizes the economic process of\ninflation. Based on this causal model and recent data, we discuss and identify\nthe assumptions under which the effect of central bank independence on\ninflation can be identified and estimated. Given these and alternative\nassumptions, we estimate this effect using modern doubly robust effect\nestimators, i.e., longitudinal targeted maximum likelihood estimators. The\nestimation procedure incorporates machine learning algorithms and is tailored\nto address the challenges associated with complex longitudinal macroeconomic\ndata. We do not find strong support for the hypothesis that having an\nindependent central bank for a long period of time necessarily lowers\ninflation. Simulation studies evaluate the sensitivity of the proposed methods\nin complex settings when certain assumptions are violated and highlight the\nimportance of working with appropriate learning algorithms for estimation.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 17:26:51 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 14:32:49 GMT"}, {"version": "v3", "created": "Mon, 18 May 2020 07:58:08 GMT"}, {"version": "v4", "created": "Fri, 24 Jul 2020 10:24:19 GMT"}, {"version": "v5", "created": "Wed, 29 Jul 2020 07:20:39 GMT"}, {"version": "v6", "created": "Mon, 15 Mar 2021 07:48:55 GMT"}, {"version": "v7", "created": "Fri, 14 May 2021 09:26:12 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Baumann", "Philipp F. M.", ""], ["Schomaker", "Michael", ""], ["Rossi", "Enzo", ""]]}, {"id": "2003.02213", "submitter": "Samuel Thiriot", "authors": "Samuel Thiriot", "title": "Generate Descriptive Social Networks for Large Populations from\n  Available Observations: A Novel Methodology and a Generator", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When modeling a social dynamics with an agent-oriented approach, researchers\nhave to describe the structure of interactions within the population. Given the\nintractability of extensive network collecting, they rely on random network\ngenerators that are supposed to explore the space of plausible networks. We\nfirst identify the needs of modelers, including placing heterogeneous agents on\nthe network given their attributes and differentiating the various types of\nsocial links that lead to different interactions. We point out the existence of\ndata in the form of scattered statistics and qualitative observations, that\nshould be used to parameter the generator. We propose a new approach peculiar\nto agent-based modeling, in which we will generate social links from\nindividuals' observed attributes, and return them as a multiplex network.\nInterdependencies between socioeconomic attributes, and generative rules, are\nencoded as Bayesian networks. A methodology guides modelers through the\nformalization of these parameters. This approach is illustrated by describing\nthe structure of interactions that supports diffusion of contraceptive\nsolutions in rural Kenya.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 17:35:29 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Thiriot", "Samuel", ""]]}, {"id": "2003.02334", "submitter": "Dan Wang", "authors": "Parisa Golbayani, Dan Wang, Ionut Florescu", "title": "Application of Deep Neural Networks to assess corporate Credit Rating", "comments": "19 pages, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent literature implements machine learning techniques to assess corporate\ncredit rating based on financial statement reports. In this work, we analyze\nthe performance of four neural network architectures (MLP, CNN, CNN2D, LSTM) in\npredicting corporate credit rating as issued by Standard and Poor's. We analyze\ncompanies from the energy, financial and healthcare sectors in US. The goal of\nthe analysis is to improve application of machine learning algorithms to credit\nassessment. To this end, we focus on three questions. First, we investigate if\nthe algorithms perform better when using a selected subset of features, or if\nit is better to allow the algorithms to select features themselves. Second, is\nthe temporal aspect inherent in financial data important for the results\nobtained by a machine learning algorithm? Third, is there a particular neural\nnetwork architecture that consistently outperforms others with respect to input\nfeatures, sectors and holdout set? We create several case studies to answer\nthese questions and analyze the results using ANOVA and multiple comparison\ntesting procedure.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 21:29:22 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Golbayani", "Parisa", ""], ["Wang", "Dan", ""], ["Florescu", "Ionut", ""]]}, {"id": "2003.02340", "submitter": "Emily Diller", "authors": "Emily Diller and Jason Parker", "title": "Variation in correlation between prognosis and histologic feature based\n  on biopsy selection", "comments": "9 Pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Glioblastoma multiform carries a dismal prognosis with poor response to gold\nstandard treatment. Innovative data analysis methods have been developed to\ncharacterize tumor genomic expression with histologic features. In a clinical\nsetting, biopsy selection methods may be constrained by time and financial\nburden to the patient. Thus, we investigate the impact biopsy selection has on\ncorrelation between prognostic and histologic features in 35 patients with GBM.\nWe compared methods using limited volumes, moderate volumes, and enblock tumor\nvolumes. Additionally, we investigated the impact of random versus strategic\nmethods for limited and moderate volume biopsies. Finally, we compared\ncorrelation results by selecting one to five small biopsy. We observed a wide\nrange in correlation significance across selection methods. These findings may\naid clinical management of GBM and direct better biopsy selection necessary for\nthe development and deployment of targeted therapies.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 21:35:14 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Diller", "Emily", ""], ["Parker", "Jason", ""]]}, {"id": "2003.02443", "submitter": "Mike Ludkovski", "authors": "Nhan Huynh and Mike Ludkovski", "title": "Multi-Output Gaussian Processes for Multi-Population Longevity Modeling", "comments": "26 pages, 9 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate joint modeling of longevity trends using the spatial\nstatistical framework of Gaussian Process regression. Our analysis is motivated\nby the Human Mortality Database (HMD) that provides unified raw mortality\ntables for nearly 40 countries. Yet few stochastic models exist for handling\nmore than two populations at a time. To bridge this gap, we leverage a spatial\ncovariance framework from machine learning that treats populations as distinct\nlevels of a factor covariate, explicitly capturing the cross-population\ndependence. The proposed multi-output Gaussian Process models straightforwardly\nscale up to a dozen populations and moreover intrinsically generate coherent\njoint longevity scenarios. In our numerous case studies we investigate\npredictive gains from aggregating mortality experience across nations and\ngenders, including by borrowing the most recently available \"foreign\" data. We\nshow that in our approach, information fusion leads to more precise (and\nstatistically more credible) forecasts. We implement our models in \\texttt{R},\nas well as a Bayesian version in \\texttt{Stan} that provides further\nuncertainty quantification regarding the estimated mortality covariance\nstructure. All examples utilize public HMD datasets.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 05:55:07 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Huynh", "Nhan", ""], ["Ludkovski", "Mike", ""]]}, {"id": "2003.02453", "submitter": "Kevin Kuo", "authors": "Kevin Kuo", "title": "Individual Claims Forecasting with Bayesian Mixture Density Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an individual claims forecasting framework utilizing Bayesian\nmixture density networks that can be used for claims analytics tasks such as\ncase reserving and triaging. The proposed approach enables incorporating claims\ninformation from both structured and unstructured data sources, producing\nmulti-period cash flow forecasts, and generating different scenarios of future\npayment patterns. We implement and evaluate the modeling framework using\npublicly available data.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 06:35:51 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Kuo", "Kevin", ""]]}, {"id": "2003.02476", "submitter": "Matthias Eckardt", "authors": "Matthias Eckardt, Jonatan A. Gonz\\'alez, and Jorge Mateu", "title": "Graphical modelling and partial characteristics for multitype and\n  multivariate-marked spatio-temporal point processes", "comments": "Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper contributes to the multivariate analysis of marked spatio-temporal\npoint process data by introducing different partial point characteristics and\nextending the spatial dependence graph model formalism. Our approach yields a\nunified framework for different types of spatio-temporal data including both,\npurely qualitatively (multivariate) cases and multivariate cases with\nadditional quantitative marks. The proposed graphical model is defined through\npartial spectral density characteristics, it is highly computationally\nefficient and reflects the conditional similarity among sets of spatio-temporal\nsub-processes of either points or marked points with identical discrete marks.\nThe paper considers three applications, two on crime data and a third one on\nforestry.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 08:14:48 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Eckardt", "Matthias", ""], ["Gonz\u00e1lez", "Jonatan A.", ""], ["Mateu", "Jorge", ""]]}, {"id": "2003.02580", "submitter": "Shuangge Ma", "authors": "Qingzhao Zhang, Hao Chai, Shuangge Ma", "title": "Robust Identification of Gene-Environment Interactions under\n  High-Dimensional Accelerated Failure Time Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For complex diseases, beyond the main effects of genetic (G) and\nenvironmental (E) factors, gene-environment (G-E) interactions also play an\nimportant role. Many of the existing G-E interaction methods conduct marginal\nanalysis, which may not appropriately describe disease biology. Joint analysis\nmethods have been developed, with most of the existing loss functions\nconstructed based on likelihood. In practice, data contamination is not\nuncommon. Development of robust methods for interaction analysis that can\naccommodate data contamination is very limited. In this study, we consider\ncensored survival data and adopt an accelerated failure time (AFT) model. An\nexponential squared loss is adopted to achieve robustness. A sparse group\npenalization approach, which respects the \"main effects, interactions\"\nhierarchy, is adopted for estimation and identification. Consistency properties\nare rigorously established. Simulation shows that the proposed method\noutperforms direct competitors. In data analysis, the proposed method makes\nbiologically sensible findings.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 12:52:33 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Zhang", "Qingzhao", ""], ["Chai", "Hao", ""], ["Ma", "Shuangge", ""]]}, {"id": "2003.02769", "submitter": "Manuel J. A. Eugster", "authors": "Shafi Kamalbasha and Manuel J. A. Eugster", "title": "Bayesian A/B Testing for Business Decisions", "comments": "Conference paper at iDSC'20 -- 3rd International Data Science\n  Conference 2020; see https://idsc.at/", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Controlled experiments (A/B tests or randomized field experiments) are the de\nfacto standard to make data-driven decisions when implementing changes and\nobserving customer responses. The methodology to analyze such experiments\nshould be easily understandable to stakeholders like product and marketing\nmanagers. Bayesian inference recently gained a lot of popularity and, in terms\nof A/B testing, one key argument is the easy interpretability. For\nstakeholders, \"probability to be best\" (with corresponding credible intervals)\nprovides a natural metric to make business decisions. In this paper, we\nmotivate the quintessential questions a business owner typically has and how to\nanswer them with a Bayesian approach. We present three experiment scenarios\nthat are common in our company, how they are modeled in a Bayesian fashion, and\nhow to use the models to draw business decisions. For each of the scenarios, we\npresent a real-world experiment, the results and the final business decisions\ndrawn.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 16:58:47 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Kamalbasha", "Shafi", ""], ["Eugster", "Manuel J. A.", ""]]}, {"id": "2003.02878", "submitter": "Shane Barratt", "authors": "Shane Barratt, Jonathan Tuck, Stephen Boyd", "title": "Convex Optimization Over Risk-Neutral Probabilities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a collection of derivatives that depend on the price of an\nunderlying asset at expiration or maturity. The absence of arbitrage is\nequivalent to the existence of a risk-neutral probability distribution on the\nprice; in particular, any risk neutral distribution can be interpreted as a\ncertificate establishing that no arbitrage exists. We are interested in the\ncase when there are multiple risk-neutral probabilities. We describe a number\nof convex optimization problems over the convex set of risk neutral price\nprobabilities. These include computation of bounds on the cumulative\ndistribution, VaR, CVaR, and other quantities, over the set of risk-neutral\nprobabilities. After discretizing the underlying price, these problems become\nfinite dimensional convex or quasiconvex optimization problems, and therefore\nare tractable. We illustrate our approach using real options and futures\npricing data for the S&P 500 index and Bitcoin.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 19:17:21 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Barratt", "Shane", ""], ["Tuck", "Jonathan", ""], ["Boyd", "Stephen", ""]]}, {"id": "2003.02895", "submitter": "Monica Alexander", "authors": "Monica Alexander, Kivan Polimis, Emilio Zagheni", "title": "Combining social media and survey data to nowcast migrant stocks in the\n  United States", "comments": null, "journal-ref": null, "doi": "10.1007/s11113-020-09599-3", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Measuring and forecasting migration patterns, and how they change over time,\nhas important implications for understanding broader population trends, for\ndesigning policy effectively and for allocating resources. However, data on\nmigration and mobility are often lacking, and those that do exist are not\navailable in a timely manner. Social media data offer new opportunities to\nprovide more up-to-date demographic estimates and to complement more\ntraditional data sources. Facebook, for example, can be thought of as a large\ndigital census that is regularly updated. However, its users are not\nrepresentative of the underlying population. This paper proposes a statistical\nframework to combine social media data with traditional survey data to produce\ntimely `nowcasts' of migrant stocks by state in the United States. The model\nincorporates bias adjustment of the Facebook data, and a pooled principal\ncomponent time series approach, to account for correlations across age, time\nand space. We illustrate the results for migrants from Mexico, India and\nGermany, and show that the model outperforms alternatives that rely solely on\neither social media or survey data.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 19:57:48 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Alexander", "Monica", ""], ["Polimis", "Kivan", ""], ["Zagheni", "Emilio", ""]]}, {"id": "2003.02906", "submitter": "Vartan Choulakian", "authors": "Choulakian Vartan and Abou Samra Ghassan", "title": "Mean absolute deviations about the mean, the cut norm and taxicab\n  correspondence analysis", "comments": "18 pages, 4 figures", "journal-ref": "Open Journal of Statistics 2020", "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization has two faces, minimization of a loss function or maximization\nof a gain function. We show that the mean absolute deviations about the mean,\nd, maximizes a gain function based on the power set of the individuals, and it\nis equal to twice the value of its cut-norm. This property is generalized to\ndouble-centered and triple-centered data sets. Furthermore, we show that among\nthe three well known dispersion measures, standard deviation, least absolute\ndeviation and d, d is the most robust based on the relative contribution\ncriterion. More importantly, we show that the computation of each principal\ndimension of taxicab correspondence analysis corresponds to balanced 2-blocks\nseriation. Examples are provided.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 20:28:22 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Vartan", "Choulakian", ""], ["Ghassan", "Abou Samra", ""]]}, {"id": "2003.02930", "submitter": "Fei Zhou", "authors": "Fei Zhou, Jie Ren, Xi Lu, Shuangge Ma, Cen Wu", "title": "Gene-Environment Interaction: A Variable Selection Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gene-environment interactions have important implications to elucidate the\ngenetic basis of complex diseases beyond the joint function of multiple genetic\nfactors and their interactions (or epistasis). In the past, G$\\times$E\ninteractions have been mainly conducted within the framework of genetic\nassociation studies. The high dimensionality of G$\\times$E interactions, due to\nthe complicated form of environmental effects and presence of a large number of\ngenetic factors including gene expressions and SNPs, has motivated the recent\ndevelopment of penalized variable selection methods for dissecting G$\\times$E\ninteractions, which has been ignored in majority of published reviews on\ngenetic interaction studies. In this article, we first survey existing\noverviews on both gene-environment and gene-gene interactions. Then, after a\nbrief introduction on the variable selection methods, we review penalization\nand relevant variable selection methods in marginal and joint paradigms\nrespectively under a variety of conceptual models. Discussions on strengths and\nlimitations, as well as computational aspects of the variable selection methods\ntailored for G$\\times$E studies have also been provided.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 21:21:46 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Zhou", "Fei", ""], ["Ren", "Jie", ""], ["Lu", "Xi", ""], ["Ma", "Shuangge", ""], ["Wu", "Cen", ""]]}, {"id": "2003.02941", "submitter": "Mickael Albertus", "authors": "Mickael Albertus", "title": "Exponential increase of test power for Z-test and Chi-square test with\n  auxiliary information", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.ME stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main goal of this article is to study how an auxiliary information can be\nused to improve the power of two famous statistical tests: the $ Z$-test and\nthe chi-square test. This information can be of any nature - probability of\nsets of partitions, expectation of a function, ... - and is not even required\nto be an exact information, it can be given by an estimate based on a larger\nsample for example. Some definitions of auxiliary information can be found in\nthe statistical literature and will be recalled. In this article, the notion of\nauxiliary information is discussed here from a very general point of view.\nThese two statistical tests are modified so that the auxiliary information is\ntaken into account. One show in particular that the power of these tests is\nincreased exponentially. Some statistical examples are treated to show the\nconcreteness of this method.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 21:46:21 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Albertus", "Mickael", ""]]}, {"id": "2003.02978", "submitter": "Markus Foote", "authors": "Markus D. Foote (1), Philip E. Dennison (2), Andrew K. Thorpe (3),\n  David R. Thompson (3), Siraput Jongaramrungruang (4), Christian Frankenberg\n  (3 and 4), Sarang C. Joshi (1) ((1) Scientific Computing and Imaging\n  Institute, University of Utah, Salt Lake City, UT (2) Department of\n  Geography, University of Utah, Salt Lake City, UT, (3) Jet Propulsion\n  Laboratory, California Institute of Technology, Pasadena, CA, (4) Division of\n  Geological and Planetary Sciences, California Institute of Technology,\n  Pasadena, CA)", "title": "Fast and Accurate Retrieval of Methane Concentration from Imaging\n  Spectrometer Data Using Sparsity Prior", "comments": "13 pages, 11 figures", "journal-ref": "IEEE Transactions on Geoscience and Remote Sensing, 2020, pp. 1-13", "doi": "10.1109/TGRS.2020.2976888", "report-no": null, "categories": "eess.IV cs.DC physics.ao-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The strong radiative forcing by atmospheric methane has stimulated interest\nin identifying natural and anthropogenic sources of this potent greenhouse gas.\nPoint sources are important targets for quantification, and anthropogenic\ntargets have potential for emissions reduction. Methane point source plume\ndetection and concentration retrieval have been previously demonstrated using\ndata from the Airborne Visible InfraRed Imaging Spectrometer Next Generation\n(AVIRIS-NG). Current quantitative methods have tradeoffs between computational\nrequirements and retrieval accuracy, creating obstacles for processing\nreal-time data or large datasets from flight campaigns. We present a new\ncomputationally efficient algorithm that applies sparsity and an albedo\ncorrection to matched filter retrieval of trace gas concentration-pathlength.\nThe new algorithm was tested using AVIRIS-NG data acquired over several point\nsource plumes in Ahmedabad, India. The algorithm was validated using simulated\nAVIRIS-NG data including synthetic plumes of known methane concentration.\nSparsity and albedo correction together reduced the root mean squared error of\nretrieved methane concentration-pathlength enhancement by 60.7% compared with a\nprevious robust matched filter method. Background noise was reduced by a factor\nof 2.64. The new algorithm was able to process the entire 300 flightline 2016\nAVIRIS-NG India campaign in just over 8 hours on a desktop computer with GPU\nacceleration.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 00:31:42 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Foote", "Markus D.", "", "3 and 4"], ["Dennison", "Philip E.", "", "3 and 4"], ["Thorpe", "Andrew K.", "", "3 and 4"], ["Thompson", "David R.", "", "3 and 4"], ["Jongaramrungruang", "Siraput", "", "3 and 4"], ["Frankenberg", "Christian", "", "3 and 4"], ["Joshi", "Sarang C.", ""]]}, {"id": "2003.03004", "submitter": "Hisashi Noma", "authors": "Katsuhiro Iba, Tomohiro Shinozaki, Kazushi Maruo and Hisashi Noma", "title": "Re-evaluation of the comparative effectiveness of bootstrap-based\n  optimism correction methods in the development of multivariable clinical\n  prediction models", "comments": null, "journal-ref": "BMC Med Res Methodol 2021 7;21(1):9", "doi": "10.1186/s12874-020-01201-w", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariable predictive models are important statistical tools for providing\nsynthetic diagnosis and prognostic algorithms based on multiple patients'\ncharacteristics. Their apparent discriminant and calibration measures usually\nhave overestimation biases (known as 'optimism') relative to the actual\nperformances for external populations. Existing statistical evidence and\nguidelines suggest that three bootstrap-based bias correction methods are\npreferable in practice, namely Harrell's bias correction and the .632 and .632+\nestimators. Although Harrell's method has been widely adopted in clinical\nstudies, simulation-based evidence indicates that the .632+ estimator may\nperform better than the other two methods. However, there is limited evidence\nand these methods' actual comparative effectiveness is still unclear. In this\narticle, we conducted extensive simulations to compare the effectiveness of\nthese methods, particularly using the following modern regression models:\nconventional logistic regression, stepwise variable selections, Firth's\npenalized likelihood method, ridge, lasso, and elastic-net. Under relatively\nlarge sample settings, the three bootstrap-based methods were comparable and\nperformed well. However, all three methods had biases under small sample\nsettings, and the directions and sizes of the biases were inconsistent. In\ngeneral, the .632+ estimator is recommended, but we provide several notes\nconcerning the operating characteristics of each method.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 02:23:34 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Iba", "Katsuhiro", ""], ["Shinozaki", "Tomohiro", ""], ["Maruo", "Kazushi", ""], ["Noma", "Hisashi", ""]]}, {"id": "2003.03006", "submitter": "Guanyu Hu", "authors": "Lijiang Geng, Guanyu Hu", "title": "Bayesian Spatial Homogeneity Pursuit for Survival Data with an\n  Application to the SEER Respiratory Cancer Data", "comments": "27 pages, 3 figures", "journal-ref": "Biometrics 2021", "doi": "10.1111/biom.13439", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this work, we propose a new Bayesian spatial homogeneity pursuit method\nfor survival data under the proportional hazards model to detect spatially\nclustered patterns in baseline hazard and regression coefficients. Specially,\nregression coefficients and baseline hazard are assumed to have spatial\nhomogeneity pattern over space. To capture such homogeneity, we develop a\ngeographically weighted Chinese restaurant process prior to simultaneously\nestimate coefficients and baseline hazards and their uncertainty measures. An\nefficient Markov chain Monte Carlo (MCMC) algorithm is designed for our\nproposed methods. Performance is evaluated using simulated data, and further\napplied to a real data analysis of respiratory cancer in the state of\nLouisiana.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 02:30:15 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 02:40:09 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Geng", "Lijiang", ""], ["Hu", "Guanyu", ""]]}, {"id": "2003.03028", "submitter": "Stephen Wu", "authors": "Yong Huang, Haoyu Zhang, Hui Li, Stephen Wu", "title": "Recovering compressed images for automatic crack segmentation using\n  generative models", "comments": "34 pages, 15 figures, 3 tables", "journal-ref": null, "doi": "10.1016/j.ymssp.2020.107061", "report-no": null, "categories": "eess.IV stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a structural health monitoring (SHM) system that uses digital cameras to\nmonitor cracks of structural surfaces, techniques for reliable and effective\ndata compression are essential to ensure a stable and energy efficient crack\nimages transmission in wireless devices, e.g., drones and robots with high\ndefinition cameras installed. Compressive sensing (CS) is a signal processing\ntechnique that allows accurate recovery of a signal from a sampling rate much\nsmaller than the limitation of the Nyquist sampling theorem. The conventional\nCS method is based on the principle that, through a regularized optimization,\nthe sparsity property of the original signals in some domain can be exploited\nto get the exact reconstruction with a high probability. However, the strong\nassumption of the signals being highly sparse in an invertible space is\nrelatively hard for real crack images. In this paper, we present a new approach\nof CS that replaces the sparsity regularization with a generative model that is\nable to effectively capture a low dimension representation of targeted images.\nWe develop a recovery framework for automatic crack segmentation of compressed\ncrack images based on this new CS method and demonstrate the remarkable\nperformance of the method taking advantage of the strong capability of\ngenerative models to capture the necessary features required in the crack\nsegmentation task even the backgrounds of the generated images are not well\nreconstructed. The superior performance of our recovery framework is\nillustrated by comparing with three existing CS algorithms. Furthermore, we\nshow that our framework is extensible to other common problems in automatic\ncrack segmentation, such as defect recovery from motion blurring and occlusion.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 04:48:05 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Huang", "Yong", ""], ["Zhang", "Haoyu", ""], ["Li", "Hui", ""], ["Wu", "Stephen", ""]]}, {"id": "2003.03032", "submitter": "Zhaoyuan Yu", "authors": "Zhaoyuan Yu, Xinxin Zhou, Xu Hu, Wen Luo, Linwang Yuan, and A-Xing Zhu", "title": "Modeling Spontaneous Exit Choices in Intercity Expressway Traffic with\n  Quantum Walk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph math.QA quant-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In intercity expressway traffic, a driver frequently makes decisions to\nadjust driving behavior according to time, location and traffic conditions,\nwhich further affects when and where the driver will leave away from the\nexpressway traffic. Spontaneous exit choices by drivers are hard to observe and\nthus it is a challenge to model intercity expressway traffic sufficiently. In\nthis paper, we developed a Spontaneous Quantum Traffic Model (SQTM), which\nmodels the stochastic traffic fluctuation caused by spontaneous exit choices\nand the residual regularity fluctuation with Quantum Walk and Autoregressive\nMoving Average model (ARMA), respectively. SQTM considers the spontaneous exit\nchoice of a driver as a quantum stochastic process with a dynamical probability\nfunction varies according to time, location and traffic conditions. A quantum\nwalk is applied to update the probability function, which simulates when and\nwhere a driver will leave the traffic affected by spontaneous exit choices. We\nvalidate our model with hourly traffic data from 7 exits from the\nNanjing-Changzhou expressway in Eastern China. For the 7 exits, the\ncoefficients of determination of SQTM ranged from 0.5 to 0.85. Compared with\nclassical random walk and ARMA model, the coefficients of determination were\nincreased by 21.28% to 104.98%, and relative mean square error decreased by\n11.61% to 32.92%. We conclude that SQTM provides new potential for modeling\ntraffic dynamics with consideration of unobservable spontaneous driver's\ndecision-making.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 05:03:19 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Yu", "Zhaoyuan", ""], ["Zhou", "Xinxin", ""], ["Hu", "Xu", ""], ["Luo", "Wen", ""], ["Yuan", "Linwang", ""], ["Zhu", "A-Xing", ""]]}, {"id": "2003.03129", "submitter": "Bjoern Sprungk", "authors": "Oliver G. Ernst, Alois Pichler, Bj\\\"orn Sprungk", "title": "Sensitivity of Uncertainty Propagation for the Elliptic Diffusion\n  Equation", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.NA math.NA math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When propagating uncertainty in the data of differential equations, the\nprobability laws describing the uncertainty are typically themselves subject to\nuncertainty. We present a sensitivity analysis of uncertainty propagation for\ndifferential equations with random inputs to perturbations of the input\nmeasures. We focus on the elliptic diffusion equation with random coefficient\nand source term, for which the probability measure of the solution random field\nis shown to be Lipschitz-continuous in both total variation and Wasserstein\ndistance. The result generalizes to the solution map of any differential\nequation with locally H\\\"older dependence on input parameters. In addition,\nthese results extend to Lipschitz continuous quantities of interest of the\nsolution as well as to coherent risk functionals of these applied to evaluate\nthe impact of their uncertainty. Our analysis is based on the sensitivity of\nrisk functionals and pushforward measures for locally H\\\"older mappings with\nrespect to the Wasserstein distance of perturbed input distributions. The\nestablished results are applied, in particular, to the case of lognormal\ndiffusion and the truncation of series representations of input random fields.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 10:54:11 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 21:10:08 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Ernst", "Oliver G.", ""], ["Pichler", "Alois", ""], ["Sprungk", "Bj\u00f6rn", ""]]}, {"id": "2003.03183", "submitter": "Stijn van Weezel", "authors": "Michael Spagat and Stijn van Weezel", "title": "Excess deaths and Hurricane Mar\\'ia", "comments": "24 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We clarify the distinction between direct and indirect effects of disasters\nsuch as Hurricane Mar\\'ia and use data from the Puerto Rico Vital Statistics\nSystem to estimate monthly excess deaths in the immediate aftermath of the\nhurricane which struck the island in September of 2017. We use a Bayesian\nlinear regression model fitted to monthly data for 2010--16 to predict monthly\ndeath tallies for all months in 2017, finding large deviations of actual\nnumbers above predicted ones in September and October of 2017 but much weaker\nevidence of excess mortality in November and December of 2017. These deviations\ntranslate into 910 excess deaths with a 95 percent uncertainty interval of 440\nto 1,390. We also find little evidence of big pre-hurricane mortality spikes in\n2017, suggesting that such large spikes do not just happen randomly and,\ntherefore, the post-hurricane mortality spike can reasonably be attributed to\nthe hurricane.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 13:22:46 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Spagat", "Michael", ""], ["van Weezel", "Stijn", ""]]}, {"id": "2003.03241", "submitter": "Theodore Papamarkou", "authors": "Theodore Papamarkou, Hayley Guy, Bryce Kroencke, Jordan Miller,\n  Preston Robinette, Daniel Schultz, Jacob Hinkle, Laura Pullum, Catherine\n  Schuman, Jeremy Renshaw, Stylianos Chatzidakis", "title": "Automated detection of corrosion in used nuclear fuel dry storage\n  canisters using residual neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nondestructive evaluation methods play an important role in ensuring\ncomponent integrity and safety in many industries. Operator fatigue can play a\ncritical role in the reliability of such methods. This is important for\ninspecting high value assets or assets with a high consequence of failure, such\nas aerospace and nuclear components. Recent advances in convolution neural\nnetworks can support and automate these inspection efforts. This paper proposes\nusing residual neural networks (ResNets) for real-time detection of corrosion,\nincluding iron oxide discoloration, pitting and stress corrosion cracking, in\ndry storage stainless steel canisters housing used nuclear fuel. The proposed\napproach crops nuclear canister images into smaller tiles, trains a ResNet on\nthese tiles, and classifies images as corroded or intact using the per-image\ncount of tiles predicted as corroded by the ResNet. The results demonstrate\nthat such a deep learning approach allows to detect the locus of corrosion via\nsmaller tiles, and at the same time to infer with high accuracy whether an\nimage comes from a corroded canister. Thereby, the proposed approach holds\npromise to automate and speed up nuclear fuel canister inspections, to minimize\ninspection costs, and to partially replace human-conducted onsite inspections,\nthus reducing radiation doses to personnel.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 14:42:07 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 17:24:21 GMT"}, {"version": "v3", "created": "Mon, 13 Jul 2020 16:06:36 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Papamarkou", "Theodore", ""], ["Guy", "Hayley", ""], ["Kroencke", "Bryce", ""], ["Miller", "Jordan", ""], ["Robinette", "Preston", ""], ["Schultz", "Daniel", ""], ["Hinkle", "Jacob", ""], ["Pullum", "Laura", ""], ["Schuman", "Catherine", ""], ["Renshaw", "Jeremy", ""], ["Chatzidakis", "Stylianos", ""]]}, {"id": "2003.03508", "submitter": "Stephanus Marnus Stoltz", "authors": "Marnus Stoltz, Gene Stoltz, Kazushige Obara, Ting Wang, David Bryant", "title": "A 1000-fold Acceleration of Hidden Markov Model Fitting using Graphical\n  Processing Units, with application to Nonvolcanic Tremor Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hidden Markov models (HMMs) are general purpose models for time-series data\nwidely used across the sciences because of their flexibility and elegance.\nHowever fitting HMMs can often be computationally demanding and time consuming,\nparticularly when the the number of hidden states is large or the Markov chain\nitself is long. Here we introduce a new Graphical Processing Unit (GPU) based\nalgorithm designed to fit long chain HMMs, applying our approach to an HMM for\nnonvolcanic tremor events developed by Wang et al.(2018). Even on a modest GPU,\nour implementation resulted in a 1000-fold increase in speed over the standard\nsingle processor algorithm, allowing a full Bayesian inference of uncertainty\nrelated to model parameters. Similar improvements would be expected for HMM\nmodels given large number of observations and moderate state spaces (<80 states\nwith current hardware). We discuss the model, general GPU architecture and\nalgorithms and report performance of the method on a tremor dataset from the\nShikoku region, Japan.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 03:44:21 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Stoltz", "Marnus", ""], ["Stoltz", "Gene", ""], ["Obara", "Kazushige", ""], ["Wang", "Ting", ""], ["Bryant", "David", ""]]}, {"id": "2003.03537", "submitter": "Bijju Veduruparthi Mr", "authors": "Bijju Kranthi Veduruparthi, Jayanta Mukherjee, Partha Pratim Das,\n  Moses Arunsingh, Raj Kumar Shrimali, Sriram Prasath, Soumendranath Ray and\n  Sanjay Chatterjee", "title": "Novel Radiomic Feature for Survival Prediction of Lung Cancer Patients\n  using Low-Dose CBCT Images", "comments": "Under review in SPIE Journal of Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Prediction of survivability in a patient for tumor progression is useful to\nestimate the effectiveness of a treatment protocol. In our work, we present a\nmodel to take into account the heterogeneous nature of a tumor to predict\nsurvival. The tumor heterogeneity is measured in terms of its mass by combining\ninformation regarding the radiodensity obtained in images with the gross tumor\nvolume (GTV). We propose a novel feature called Tumor Mass within a GTV (TMG),\nthat improves the prediction of survivability, compared to existing models\nwhich use GTV. Weekly variation in TMG of a patient is computed from the image\ndata and also estimated from a cell survivability model. The parameters\nobtained from the cell survivability model are indicatives of changes in TMG\nover the treatment period. We use these parameters along with other patient\nmetadata to perform survival analysis and regression. Cox's Proportional Hazard\nsurvival regression was performed using these data. Significant improvement in\nthe average concordance index from 0.47 to 0.64 was observed when TMG is used\nin the model instead of GTV. The experiments show that there is a difference in\nthe treatment response in responsive and non-responsive patients and that the\nproposed method can be used to predict patient survivability.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 08:47:26 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Veduruparthi", "Bijju Kranthi", ""], ["Mukherjee", "Jayanta", ""], ["Das", "Partha Pratim", ""], ["Arunsingh", "Moses", ""], ["Shrimali", "Raj Kumar", ""], ["Prasath", "Sriram", ""], ["Ray", "Soumendranath", ""], ["Chatterjee", "Sanjay", ""]]}, {"id": "2003.03621", "submitter": "Moritz Herrmann", "authors": "Moritz Herrmann, Philipp Probst, Roman Hornung, Vindi Jurinovic,\n  Anne-Laure Boulesteix", "title": "Large-scale benchmark study of survival prediction methods using\n  multi-omics data", "comments": "23 pages, 6 tables, 3 figures", "journal-ref": "Briefings in Bioinformatics (2020) bbaa167", "doi": "10.1093/bib/bbaa167", "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-omics data, that is, datasets containing different types of\nhigh-dimensional molecular variables (often in addition to classical clinical\nvariables), are increasingly generated for the investigation of various\ndiseases. Nevertheless, questions remain regarding the usefulness of\nmulti-omics data for the prediction of disease outcomes such as survival time.\nIt is also unclear which methods are most appropriate to derive such prediction\nmodels. We aim to give some answers to these questions by means of a\nlarge-scale benchmark study using real data. Different prediction methods from\nmachine learning and statistics were applied on 18 multi-omics cancer datasets\nfrom the database \"The Cancer Genome Atlas\", containing from 35 to 1,000\nobservations and from 60,000 to 100,000 variables. The considered outcome was\nthe (censored) survival time. Twelve methods based on boosting, penalized\nregression and random forest were compared, comprising both methods that do and\nthat do not take the group structure of the omics variables into account. The\nKaplan-Meier estimate and a Cox model using only clinical variables were used\nas reference methods. The methods were compared using several repetitions of\n5-fold cross-validation. Uno's C-index and the integrated Brier-score served as\nperformance metrics. The results show that, although multi-omics data can\nimprove the prediction performance, this is not generally the case. Only the\nmethod block forest slightly outperformed the Cox model on average over all\ndatasets. Taking into account the multi-omics structure improves the predictive\nperformance and protects variables in low-dimensional groups - especially\nclinical variables - from not being included in the model. All analyses are\nreproducible using freely available R code.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 18:03:17 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Herrmann", "Moritz", ""], ["Probst", "Philipp", ""], ["Hornung", "Roman", ""], ["Jurinovic", "Vindi", ""], ["Boulesteix", "Anne-Laure", ""]]}, {"id": "2003.03667", "submitter": "Thayer Alshaabi", "authors": "Thayer Alshaabi, David R. Dewhurst, Joshua R. Minot, Michael V.\n  Arnold, Jane L. Adams, Christopher M. Danforth, and Peter Sheridan Dodds", "title": "The growing amplification of social media: Measuring temporal and social\n  contagion dynamics for over 150 languages on Twitter for 2009-2020", "comments": "26 pages (15 main, 11 appendix), 13 figures (6 main, 7 appendix), and\n  4 online appendices available at\n  http://compstorylab.org/storywrangler/papers/tlid/", "journal-ref": null, "doi": "10.1140/epjds/s13688-021-00271-0", "report-no": null, "categories": "cs.CL cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Working from a dataset of 118 billion messages running from the start of 2009\nto the end of 2019, we identify and explore the relative daily use of over 150\nlanguages on Twitter. We find that eight languages comprise 80% of all tweets,\nwith English, Japanese, Spanish, and Portuguese being the most dominant. To\nquantify social spreading in each language over time, we compute the 'contagion\nratio': The balance of retweets to organic messages. We find that for the most\ncommon languages on Twitter there is a growing tendency, though not universal,\nto retweet rather than share new content. By the end of 2019, the contagion\nratios for half of the top 30 languages, including English and Spanish, had\nreached above 1 -- the naive contagion threshold. In 2019, the top 5 languages\nwith the highest average daily ratios were, in order, Thai (7.3), Hindi, Tamil,\nUrdu, and Catalan, while the bottom 5 were Russian, Swedish, Esperanto,\nCebuano, and Finnish (0.26). Further, we show that over time, the contagion\nratios for most common languages are growing more strongly than those of rare\nlanguages.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 21:42:50 GMT"}, {"version": "v2", "created": "Sun, 15 Mar 2020 04:43:52 GMT"}, {"version": "v3", "created": "Fri, 10 Jul 2020 14:49:46 GMT"}, {"version": "v4", "created": "Wed, 30 Sep 2020 21:40:16 GMT"}, {"version": "v5", "created": "Mon, 16 Nov 2020 22:44:32 GMT"}, {"version": "v6", "created": "Mon, 7 Dec 2020 20:29:26 GMT"}, {"version": "v7", "created": "Wed, 20 Jan 2021 16:21:27 GMT"}, {"version": "v8", "created": "Tue, 9 Mar 2021 03:32:41 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Alshaabi", "Thayer", ""], ["Dewhurst", "David R.", ""], ["Minot", "Joshua R.", ""], ["Arnold", "Michael V.", ""], ["Adams", "Jane L.", ""], ["Danforth", "Christopher M.", ""], ["Dodds", "Peter Sheridan", ""]]}, {"id": "2003.03697", "submitter": "Feng Yin", "authors": "Feng Yin, Zhidi Lin, Yue Xu, Qinglei Kong, Deshi Li, Sergios\n  Theodoridis, Shuguang (Robert) Cui", "title": "FedLoc: Federated Learning Framework for Data-Driven Cooperative\n  Localization and Location Data Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.SY eess.SP eess.SY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this overview paper, data-driven learning model-based cooperative\nlocalization and location data processing are considered, in line with the\nemerging machine learning and big data methods. We first review (1)\nstate-of-the-art algorithms in the context of federated learning, (2) two\nwidely used learning models, namely the deep neural network model and the\nGaussian process model, and (3) various distributed model hyper-parameter\noptimization schemes. Then, we demonstrate various practical use cases that are\nsummarized from a mixture of standard, newly published, and unpublished works,\nwhich cover a broad range of location services, including collaborative static\nlocalization/fingerprinting, indoor target tracking, outdoor navigation using\nlow-sampling GPS, and spatio-temporal wireless traffic data modeling and\nprediction. Experimental results show that near centralized data fitting- and\nprediction performance can be achieved by a set of collaborative mobile users\nrunning distributed algorithms. All the surveyed use cases fall under our newly\nproposed Federated Localization (FedLoc) framework, which targets on\ncollaboratively building accurate location services without sacrificing user\nprivacy, in particular, sensitive information related to their geographical\ntrajectories. Future research directions are also discussed at the end of this\npaper.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 01:51:56 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 04:21:47 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Yin", "Feng", "", "Robert"], ["Lin", "Zhidi", "", "Robert"], ["Xu", "Yue", "", "Robert"], ["Kong", "Qinglei", "", "Robert"], ["Li", "Deshi", "", "Robert"], ["Theodoridis", "Sergios", "", "Robert"], ["Shuguang", "", "", "Robert"], ["Cui", "", ""]]}, {"id": "2003.03881", "submitter": "Zijun Gao", "authors": "Zijun Gao, Trevor Hastie, Robert Tibshirani", "title": "Assessment of Heterogeneous Treatment Effect Estimation Accuracy via\n  Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the assessment of the accuracy of heterogeneous treatment effect\n(HTE) estimation, where the HTE is not directly observable so standard\ncomputation of prediction errors is not applicable. To tackle the difficulty,\nwe propose an assessment approach by constructing pseudo-observations of the\nHTE based on matching. Our contributions are three-fold: first, we introduce a\nnovel matching distance derived from proximity scores in random forests;\nsecond, we formulate the matching problem as an average minimum-cost flow\nproblem and provide an efficient algorithm; third, we propose a\nmatch-then-split principle for the assessment with cross-validation. We\ndemonstrate the efficacy of the assessment approach on synthetic data and data\ngenerated from a real dataset.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 01:50:15 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Gao", "Zijun", ""], ["Hastie", "Trevor", ""], ["Tibshirani", "Robert", ""]]}, {"id": "2003.03887", "submitter": "Oliver Cliff", "authors": "Oliver M. Cliff, Leonardo Novelli, Ben D. Fulcher, James M. Shine and\n  Joseph T. Lizier", "title": "Assessing the Significance of Directed and Multivariate Measures of\n  Linear Dependence Between Time Series", "comments": "27 pages, 14 figures, final submission to Phys Rev. Research editors\n  (before copyediting)", "journal-ref": "Phys. Rev. Research 3, 013145 (2021)", "doi": "10.1103/PhysRevResearch.3.013145", "report-no": null, "categories": "stat.ME cs.IT math.IT math.ST physics.data-an q-bio.NC stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring linear dependence between time series is central to our\nunderstanding of natural and artificial systems. Unfortunately, the hypothesis\ntests that are used to determine statistically significant directed or\nmultivariate relationships from time-series data often yield spurious\nassociations (Type I errors) or omit causal relationships (Type II errors).\nThis is due to the autocorrelation present in the analysed time series -- a\nproperty that is ubiquitous across diverse applications, from brain dynamics to\nclimate change. Here we show that, for limited data, this issue cannot be\nmediated by fitting a time-series model alone (e.g., in Granger causality or\nprewhitening approaches), and instead that the degrees of freedom in\nstatistical tests should be altered to account for the effective sample size\ninduced by cross-correlations in the observations. This insight enabled us to\nderive modified hypothesis tests for any multivariate correlation-based\nmeasures of linear dependence between covariance-stationary time series,\nincluding Granger causality and mutual information with Gaussian marginals. We\nuse both numerical simulations (generated by autoregressive models and digital\nfiltering) as well as recorded fMRI-neuroimaging data to show that our tests\nare unbiased for a variety of stationary time series. Our experiments\ndemonstrate that the commonly used $F$- and $\\chi^2$-tests can induce\nsignificant false-positive rates of up to $100\\%$ for both measures, with and\nwithout prewhitening of the signals. These findings suggest that many\ndependencies reported in the scientific literature may have been, and may\ncontinue to be, spuriously reported or missed if modified hypothesis tests are\nnot used when analysing time series.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 02:06:01 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 11:02:18 GMT"}, {"version": "v3", "created": "Wed, 27 Jan 2021 10:13:03 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Cliff", "Oliver M.", ""], ["Novelli", "Leonardo", ""], ["Fulcher", "Ben D.", ""], ["Shine", "James M.", ""], ["Lizier", "Joseph T.", ""]]}, {"id": "2003.04170", "submitter": "Edward Wheatcroft", "authors": "Victoria Volodina, Edward Wheatcroft and Henry Wynn", "title": "Cheap, robust and low carbon: comparing district heating scenarios using\n  stochastic ordering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Strategies for meeting low carbon objectives in energy are likely to take\ngreater account of the benefits of district heating. Currently, district\nheating schemes typically use combined heat and power (CHP) supplemented with\nheat pumps attached to low temperature waste heat sources, powered either by\nelectricity from the CHP itself or from the National Grid. Schemes have\ncompeting objectives, of which we identify three: the need for inexpensive\nenergy, meeting low carbon objectives and robustness against, particularly,\nvariation in demand and electricity prices. This paper compares different\nsystem designs under three scenarios, using ideas from stochastic dominance\nclose in spirit to traditional ideas of robust design. One conclusion is that,\nunder all considered scenarios, a heat pump provides the most robust solution.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 14:26:58 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Volodina", "Victoria", ""], ["Wheatcroft", "Edward", ""], ["Wynn", "Henry", ""]]}, {"id": "2003.04185", "submitter": "Gurcan Comert", "authors": "Gurcan Comert, Mizanur Rahman, Mhafuzul Islam, and Mashrur Chowdhury", "title": "Change Point Models for Real-time Cyber Attack Detection in Connected\n  Vehicle Environment", "comments": "11 pages, 4 figures, submitted to IEEE Transactions on Intelligent\n  Transportation Systems. arXiv admin note: substantial text overlap with\n  arXiv:1811.12620", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Connected vehicle (CV) systems are cognizant of potential cyber attacks\nbecause of increasing connectivity between its different components such as\nvehicles, roadside infrastructure, and traffic management centers. However, it\nis a challenge to detect security threats in real-time and develop appropriate\nor effective countermeasures for a CV system because of the dynamic behavior of\nsuch attacks, high computational power requirement, and a historical data\nrequirement for training detection models. To address these challenges,\nstatistical models, especially change point models, have potentials for\nreal-time anomaly detections. Thus, the objective of this study is to\ninvestigate the efficacy of two change point models, Expectation Maximization\n(EM) and two forms of Cumulative Summation (CUSUM) algorithms (i.e., typical\nand adaptive), for real-time V2I cyber attack detection in a CV Environment. To\nprove the efficacy of these models, we evaluated these two models for three\ndifferent type of cyber attack, denial of service (DOS), impersonation, and\nfalse information, using basic safety messages (BSMs) generated from CVs\nthrough simulation. Results from numerical analysis revealed that EM, CUSUM,\nand adaptive CUSUM could detect these cyber attacks, DOS, impersonation, and\nfalse information, with an accuracy of (99%, 100%, 100%), (98%, 10%, 100%), and\n(100%, 98%, 100%) respectively.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 21:19:42 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Comert", "Gurcan", ""], ["Rahman", "Mizanur", ""], ["Islam", "Mhafuzul", ""], ["Chowdhury", "Mashrur", ""]]}, {"id": "2003.04238", "submitter": "Thomas Stringham", "authors": "Thomas Stringham", "title": "Fast Bayesian Record Linkage With Record-Specific Disagreement\n  Parameters", "comments": null, "journal-ref": null, "doi": "10.1080/07350015.2021.1934478", "report-no": null, "categories": "stat.ME econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers are often interested in linking individuals between two datasets\nthat lack a common unique identifier. Matching procedures often struggle to\nmatch records with common names, birthplaces or other field values.\nComputational feasibility is also a challenge, particularly when linking large\ndatasets. We develop a Bayesian method for automated probabilistic record\nlinkage and show it recovers more than 50% more true matches, holding accuracy\nconstant, than comparable methods in a matching of military recruitment data to\nthe 1900 US Census for which expert-labelled matches are available. Our\napproach, which builds on a recent state-of-the-art Bayesian method, refines\nthe modelling of comparison data, allowing disagreement probability parameters\nconditional on non-match status to be record-specific in the smaller of the two\ndatasets. This flexibility significantly improves matching when many records\nshare common field values. We show that our method is computationally feasible\nin practice, despite the added complexity, with an R/C++ implementation that\nachieves significant improvement in speed over comparable recent methods. We\nalso suggest a lightweight method for treatment of very common names and show\nhow to estimate true positive rate and positive predictive value when true\nmatch status is unavailable.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 16:23:54 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 13:12:53 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Stringham", "Thomas", ""]]}, {"id": "2003.04265", "submitter": "Claudia Neves", "authors": "John H.J. Einmahl, Ana Ferreira, Laurens de Haan, Claudia Neves and\n  Chen Zhou", "title": "Spatial dependence and space-time trend in extreme events", "comments": "Supporting information: the detailed proof of Theorem 6, referenced\n  in Section 4, as well as simulations showcasing finite sample performance of\n  the proposed methods are available with this paper at https://bit.ly/3aJFM6B", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical theory of extremes is extended to observations that are\nnon-stationary and not independent. The non-stationarity over time and space is\ncontrolled via the scedasis (tail scale) in the marginal distributions. Spatial\ndependence stems from multivariate extreme value theory. We establish\nasymptotic theory for both the weighted sequential tail empirical process and\nthe weighted tail quantile process based on all observations, taken over time\nand space. The results yield two statistical tests for homoscedasticity in the\ntail, one in space and one in time. Further, we show that the common extreme\nvalue index can be estimated via a pseudo-maximum likelihood procedure based on\npooling all (non-stationary and dependent) observations. Our leading example\nand application is rainfall in Northern Germany.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 17:16:08 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Einmahl", "John H. J.", ""], ["Ferreira", "Ana", ""], ["de Haan", "Laurens", ""], ["Neves", "Claudia", ""], ["Zhou", "Chen", ""]]}, {"id": "2003.04430", "submitter": "Zidi Xiu", "authors": "Zidi Xiu, Chenyang Tao, Benjamin A. Goldstein, Ricardo Henao", "title": "Variational Learning of Individual Survival Distributions", "comments": null, "journal-ref": null, "doi": "10.1145/3368555.3384454", "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The abundance of modern health data provides many opportunities for the use\nof machine learning techniques to build better statistical models to improve\nclinical decision making. Predicting time-to-event distributions, also known as\nsurvival analysis, plays a key role in many clinical applications. We introduce\na variational time-to-event prediction model, named Variational Survival\nInference (VSI), which builds upon recent advances in distribution learning\ntechniques and deep neural networks. VSI addresses the challenges of\nnon-parametric distribution estimation by ($i$) relaxing the restrictive\nmodeling assumptions made in classical models, and ($ii$) efficiently handling\nthe censored observations, {\\it i.e.}, events that occur outside the\nobservation window, all within the variational framework. To validate the\neffectiveness of our approach, an extensive set of experiments on both\nsynthetic and real-world datasets is carried out, showing improved performance\nrelative to competing solutions.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 22:09:51 GMT"}, {"version": "v2", "created": "Sun, 13 Dec 2020 05:01:25 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Xiu", "Zidi", ""], ["Tao", "Chenyang", ""], ["Goldstein", "Benjamin A.", ""], ["Henao", "Ricardo", ""]]}, {"id": "2003.04433", "submitter": "Somabha Mukherjee", "authors": "Somabha Mukherjee, Rohit K. Patra, Andrew L. Johnson, Hiroshi Morita", "title": "Least Squares Estimation of a Monotone Quasiconvex Regression Function", "comments": "34 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new approach for the estimation of a multivariate function based\non the economic axioms of monotonicity and quasiconvexity. We prove the\nexistence of the nonparametric least squares estimator (LSE) for a monotone and\nquasiconvex function and provide two characterizations for it. One of these\ncharacterizations is useful from the theoretical point of view, while the other\nhelps in the computation of the estimator. We show that the LSE is almost\nsurely unique and is the solution to a mixed-integer quadratic optimization\nproblem. We prove consistency and find finite sample risk bounds for the LSE\nunder both fixed lattice and random design settings for the covariates. We\nillustrate the superior performance of the LSE against existing estimators via\nsimulation. Finally, we use the LSE to estimate the production function for the\nJapanese plywood industry and the cost function for hospitals across the US.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 22:16:57 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Mukherjee", "Somabha", ""], ["Patra", "Rohit K.", ""], ["Johnson", "Andrew L.", ""], ["Morita", "Hiroshi", ""]]}, {"id": "2003.04598", "submitter": "Hisashi Noma", "authors": "Hisashi Noma, Kengo Nagashima, Shogo Kato, Satoshi Teramukai and Toshi\n  A. Furukawa", "title": "Flexible random-effects distribution models for meta-analysis", "comments": null, "journal-ref": "J Epidemiol. 2021", "doi": "10.2188/jea.JE20200376", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In meta-analysis, the random-effects models are standard tools to address\nbetween-study heterogeneity in evidence synthesis analyses. For the\nrandom-effects distribution models, the normal distribution model has been\nadopted in most systematic reviews due to its computational and conceptual\nsimplicity. However, the restrictive model assumption might have serious\ninfluences on the overall conclusions in practices. In this article, we first\nprovide two examples of real-world evidence that clearly show that the normal\ndistribution assumption is unsuitable. To address the model restriction\nproblem, we propose alternative flexible random-effects models that can\nflexibly regulate skewness, kurtosis and tailweight: skew normal distribution,\nskew t-distribution, asymmetric Subbotin distribution, Jones-Faddy\ndistribution, and sinh-arcsinh distribution. We also developed a R package,\nflexmeta, that can easily perform these methods. Using the flexible\nrandom-effects distribution models, the results of the two meta-analyses were\nmarkedly altered, potentially influencing the overall conclusions of these\nsystematic reviews. The flexible methods and computational tools can provide\nmore precise evidence, and these methods would be recommended at least as\nsensitivity analysis tools to assess the influence of the normal distribution\nassumption of the random-effects model.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 09:25:55 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 18:21:14 GMT"}, {"version": "v3", "created": "Sat, 1 Aug 2020 04:09:37 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Noma", "Hisashi", ""], ["Nagashima", "Kengo", ""], ["Kato", "Shogo", ""], ["Teramukai", "Satoshi", ""], ["Furukawa", "Toshi A.", ""]]}, {"id": "2003.04736", "submitter": "Theja Tulabandhula", "authors": "Theja Tulabandhula and Deeksha Sinha and Saketh Karra", "title": "Optimizing Revenue while showing Relevant Assortments at Scale", "comments": "53 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scalable real-time assortment optimization has become essential in e-commerce\noperations due to the need for personalization and the availability of a large\nvariety of items. While this can be done when there are simplistic assortment\nchoices to be made, the optimization process becomes difficult when imposing\nconstraints on the collection of relevant assortments based on insights by\nstore-managers and historically well-performing assortments. We design fast and\nflexible algorithms based on variations of binary search that find the\n(approximately) optimal assortment in this difficult regime. In particular, we\nrevisit the problem of large-scale assortment optimization under the\nmultinomial logit choice model without any assumptions on the structure of the\nfeasible assortments. We speed up the comparison steps using advances in\nsimilarity search in the field of information retrieval/machine learning. For\nan arbitrary collection of assortments, our algorithms can find a solution in\ntime that is sub-linear in the number of assortments, and for the simpler case\nof cardinality constraints - linear in the number of items (existing methods\nare quadratic or worse). Empirical validations using a real world dataset (in\naddition to experiments using semi-synthetic data based on the Billion Prices\ndataset and several retail transaction datasets) show that our algorithms are\ncompetitive even when the number of items is $\\sim 10^5$ ($10\\times$ larger\ninstances than previously studied).\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 20:16:49 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 01:06:15 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Tulabandhula", "Theja", ""], ["Sinha", "Deeksha", ""], ["Karra", "Saketh", ""]]}, {"id": "2003.04787", "submitter": "Kun Chen", "authors": "Yan Li, Chun Yu, Yize Zhao, Robert H. Aseltine, Weixin Yao, Kun Chen", "title": "Pursuing Sources of Heterogeneity in Modeling Clustered Population", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers often have to deal with heterogeneous population with mixed\nregression relationships, increasingly so in the era of data explosion. In such\nproblems, when there are many candidate predictors, it is not only of interest\nto identify the predictors that are associated with the outcome, but also to\ndistinguish the true sources of heterogeneity, i.e., to identify the predictors\nthat have different effects among the clusters and thus are the true\ncontributors to the formation of the clusters. We clarify the concepts of the\nsource of heterogeneity that account for potential scale differences of the\nclusters and propose a regularized finite mixture effects regression to achieve\nheterogeneity pursuit and feature selection simultaneously. As the name\nsuggests, the problem is formulated under an effects-model parameterization, in\nwhich the cluster labels are missing and the effect of each predictor on the\noutcome is decomposed to a common effect term and a set of cluster-specific\nterms. A constrained sparse estimation of these effects leads to the\nidentification of both the variables with common effects and those with\nheterogeneous effects. We propose an efficient algorithm and show that our\napproach can achieve both estimation and selection consistency. Simulation\nstudies further demonstrate the effectiveness of our method under various\npractical scenarios. Three applications are presented, namely, an imaging\ngenetics study for linking genetic factors and brain neuroimaging traits in\nAlzheimer's disease, a public health study for exploring the association\nbetween suicide risk among adolescents and their school district\ncharacteristics, and a sport analytics study for understanding how the salary\nlevels of baseball players are associated with their performance and\ncontractual status.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 14:59:35 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 20:03:13 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Li", "Yan", ""], ["Yu", "Chun", ""], ["Zhao", "Yize", ""], ["Aseltine", "Robert H.", ""], ["Yao", "Weixin", ""], ["Chen", "Kun", ""]]}, {"id": "2003.04805", "submitter": "Razvan Marinescu", "authors": "Razvan V. Marinescu", "title": "Modelling the Neuroanatomical Progression of Alzheimer's Disease and\n  Posterior Cortical Atrophy", "comments": "PhD thesis; Defended in Jan 2019 at University College London", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.NC stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In order to find effective treatments for Alzheimer's disease (AD), we need\nto identify subjects at risk of AD as early as possible. To this end, recently\ndeveloped disease progression models can be used to perform early diagnosis, as\nwell as predict the subjects' disease stages and future evolution. However,\nthese models have not yet been applied to rare neurodegenerative diseases, are\nnot suitable to understand the complex dynamics of biomarkers, work only on\nlarge multimodal datasets, and their predictive performance has not been\nobjectively validated. In this work I developed novel models of disease\nprogression and applied them to estimate the progression of Alzheimer's disease\nand Posterior Cortical atrophy, a rare neurodegenerative syndrome causing\nvisual deficits. My first contribution is a study on the progression of\nPosterior Cortical Atrophy, using models already developed: the Event-based\nModel (EBM) and the Differential Equation Model (DEM). My second contribution\nis the development of DIVE, a novel spatio-temporal model of disease\nprogression that estimates fine-grained spatial patterns of pathology,\npotentially enabling us to understand complex disease mechanisms relating to\npathology propagation along brain networks. My third contribution is the\ndevelopment of Disease Knowledge Transfer (DKT), a novel disease progression\nmodel that estimates the multimodal progression of rare neurodegenerative\ndiseases from limited, unimodal datasets, by transferring information from\nlarger, multimodal datasets of typical neurodegenerative diseases. My fourth\ncontribution is the development of novel extensions for the EBM and the DEM,\nand the development of novel measures for performance evaluation of such\nmodels. My last contribution is the organization of the TADPOLE challenge, a\ncompetition which aims to identify algorithms and features that best predict\nthe evolution of AD.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 21:59:52 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Marinescu", "Razvan V.", ""]]}, {"id": "2003.04810", "submitter": "Fabao Gao", "authors": "Fabao Gao, Xia Liu", "title": "Revisiting the distributions of Jupiter's irregular moons: I. physical\n  characteristics", "comments": "Bulgarian Astronomical Journal, Accepted, (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.EP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the identified number of Jupiter's moons has skyrocketed to 79, some of\nthem have been regrouped. In this work, we continue to identify the potential\ndistributions of the physical characteristics of Jupiter's irregular moons. By\nusing nonparametric Kolmogorov-Smirnov tests, we verified more than 20 commonly\nused distributions and found that surprisingly, almost all the physical\ncharacteristics (i.e., the equatorial radius, equatorial circumference,\ncircumference, volume, mass, surface gravity and escape velocity) of the moons\nin the Ananke and Carme groups follow log-logistic distributions. Additionally,\nmore than half of the physical characteristics of the moons in the Pasiphae\ngroup are theoretically subject to this type of distribution. The discovery of\nan increasing number of Jupiter's irregular moons combined with strict\nanalytical derivations, it is increasingly clear and possible to anticipate\nthat the physical characteristics of most irregular moons follow log-logistic\ndistributions.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 15:36:34 GMT"}, {"version": "v2", "created": "Sat, 14 Mar 2020 00:04:17 GMT"}, {"version": "v3", "created": "Wed, 5 Aug 2020 22:05:38 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Gao", "Fabao", ""], ["Liu", "Xia", ""]]}, {"id": "2003.04851", "submitter": "Fabao Gao", "authors": "Fabao Gao, Xia Liu", "title": "Revisiting the distributions of Jupiter's irregular moons: II. orbital\n  characteristics", "comments": "Bulgarian Astronomical Journal, Accepted, (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.EP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper statistically describes the orbital distribution laws of Jupiter's\nirregular moons, most of which are members of the Ananke, Carme and Pasiphae\ngroups. By comparing 19 known continuous distributions, it is verified that\nsuitable distribution functions exist to describe the orbital distributions of\nthese natural satellites. For each distribution type, interval estimation is\nused to estimate the corresponding parameter values. At a given significance\nlevel, a one-sample Kolmogorov-Smirnov non-parametric test is applied to verify\nthe specified distribution, and we often select the one with the largest\n$p$-value. The results show that the semi-major axis, mean inclination and\norbital period of the moons in the Ananke group and Carme group obey Stable\ndistributions. In addition, according to Kepler's third law of planetary motion\nand by comparing the theoretically calculated best-fitting cumulative\ndistribution function (CDF) with the observed CDF, we demonstrate that the\ntheoretical distribution is in good agreement with the empirical distribution.\nTherefore, these characteristics of Jupiter's irregular moons are indeed very\nlikely to follow some specific distribution laws, and it will be possible to\nuse these laws to help study certain features of poorly investigated moons or\neven predict undiscovered ones.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 16:55:31 GMT"}, {"version": "v2", "created": "Sat, 14 Mar 2020 00:02:24 GMT"}, {"version": "v3", "created": "Wed, 5 Aug 2020 22:12:17 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Gao", "Fabao", ""], ["Liu", "Xia", ""]]}, {"id": "2003.04855", "submitter": "Joaquim Dias Garcia", "authors": "Julio Alberto Dias, Guilherme Machado, Alessandro Soares, Joaquim Dias\n  Garcia", "title": "Modeling Multiscale Variable Renewable Energy and Inflow Scenarios in\n  Very Large Regions with Nonparametric Bayesian Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a non-parametric Bayesian network method to\ngenerate synthetic scenarios of hourly generation for variable renewable\nenergy(VRE) plants. The methodology consists of a non-parametric estimation of\nthe probability distribution of VRE generation, followed by an inverse\nprobability integral transform, in order to obtain normally distributed\nvariables of VRE generation. Then, we build a Bayesian network based on the\nevaluation of the spatial correlation between variables (VRE generation and\nhydro inflows, but load forecast, temperature, and other types of random\nvariables could also be used with the proposed framework), to generate future\nsynthetic scenarios while keeping the historical spatial correlation structure.\nFinally, we present a real-life case study, that uses real data from the\nBrazilian power system, to show the improvements that the present methodology\nallows for real-life studies.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 17:04:53 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Dias", "Julio Alberto", ""], ["Machado", "Guilherme", ""], ["Soares", "Alessandro", ""], ["Garcia", "Joaquim Dias", ""]]}, {"id": "2003.04963", "submitter": "Vineetha Warriyar Kodalore Vijayan PhD", "authors": "Vineetha Warriyar K. V., Waleed Almutiry and Rob Deardon", "title": "Individual-Level Modelling of Infectious Disease Data: EpiILM", "comments": "15 pages, 10 figures. This paper will be submitted to the R journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we introduce the R package EpiILM, which provides tools for\nsimulation from, and inference for, discrete-time individual-level models of\ninfectious disease transmission proposed by Deardon et al. (2010). The\ninference is set in a Bayesian framework and is carried out via\nMetropolis-Hastings Markov chain Monte Carlo (MCMC). For its fast\nimplementation, key functions are coded in Fortran. Both spatial and contact\nnetwork models are implemented in the package and can be set in either\nsusceptible-infected (SI) or susceptible-infected-removed (SIR) compartmental\nframeworks. The use of the package is demonstrated through examples involving\nboth simulated and real data.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 20:44:30 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["V.", "Vineetha Warriyar K.", ""], ["Almutiry", "Waleed", ""], ["Deardon", "Rob", ""]]}, {"id": "2003.05024", "submitter": "Sam Ganzfried", "authors": "Max Chiswick and Sam Ganzfried", "title": "Prediction of Bayesian Intervals for Tropical Storms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG physics.ao-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building on recent research for prediction of hurricane trajectories using\nrecurrent neural networks (RNNs), we have developed improved methods and\ngeneralized the approach to predict Bayesian intervals in addition to simple\npoint estimates. Tropical storms are capable of causing severe damage, so\naccurately predicting their trajectories can bring significant benefits to\ncities and lives, especially as they grow more intense due to climate change\neffects. By implementing the Bayesian interval using dropout in an RNN, we\nimprove the actionability of the predictions, for example by estimating the\nareas to evacuate in the landfall region. We used an RNN to predict the\ntrajectory of the storms at 6-hour intervals. We used latitude, longitude,\nwindspeed, and pressure features from a Statistical Hurricane Intensity\nPrediction Scheme (SHIPS) dataset of about 500 tropical storms in the Atlantic\nOcean. Our results show how neural network dropout values affect predictions\nand intervals.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 22:31:58 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Chiswick", "Max", ""], ["Ganzfried", "Sam", ""]]}, {"id": "2003.05084", "submitter": "Qian Guan", "authors": "Qian Guan, Brian J. Reich, Eric B. Laber", "title": "A spatiotemporal recommendation engine for malaria control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Malaria is an infectious disease affecting a large population across the\nworld, and interventions need to be efficiently applied to reduce the burden of\nmalaria. We develop a framework to help policy-makers decide how to allocate\nlimited resources in realtime for malaria control. We formalize a policy for\nthe resource allocation as a sequence of decisions, one per intervention\ndecision, that map up-to-date disease related information to a resource\nallocation. An optimal policy must control the spread of the disease while\nbeing interpretable and viewed as equitable to stakeholders. We construct an\ninterpretable class of resource allocation policies that can accommodate\nallocation of resources residing in a continuous domain, and combine a\nhierarchical Bayesian spatiotemporal model for disease transmission with a\npolicy-search algorithm to estimate an optimal policy for resource allocation\nwithin the pre-specified class. The estimated optimal policy under the proposed\nframework improves the cumulative long-term outcome compared with naive\napproaches in both simulation experiments and application to malaria\ninterventions in the Democratic Republic of the Congo.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 02:36:45 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Guan", "Qian", ""], ["Reich", "Brian J.", ""], ["Laber", "Eric B.", ""]]}, {"id": "2003.05092", "submitter": "Xiaohuan Xue", "authors": "Xiaohuan Xue", "title": "Estimation of within-study covariances in multivariate meta-analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.NA math.NA q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate meta-analysis can be adapted to a wide range of situations for\nmultiple outcomes and multiple treatment groups when combining studies\ntogether. The within-study correlation between effect sizes is often assumed\nknown in multivariate meta-analysis while it is not always known practically.\nIn this paper, we propose a generic method to approximate the within-study\ncovariance for effect sizes in multivariate meta-analysis and apply this method\nto the scenarios with multiple outcomes and one outcome with multiple treatment\ngroups respectively.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 03:13:23 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Xue", "Xiaohuan", ""]]}, {"id": "2003.05157", "submitter": "Wagner Barreto-Souza", "authors": "Wagner Barreto-Souza, Vin\\'icius D. Mayrink and Alexandre B. Simas", "title": "Bessel regression model: Robustness to analyze bounded data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Beta regression has been extensively used by statisticians and practitioners\nto model bounded continuous data and there is no strong and similar competitor\nhaving its main features. A class of normalized inverse-Gaussian (N-IG) process\nwas introduced in the literature, being explored in the Bayesian context as a\npowerful alternative to the Dirichlet process. Until this moment, no attention\nhas been paid for the univariate N-IG distribution in the classical inference.\nIn this paper, we propose the bessel regression based on the univariate N-IG\ndistribution, which is a robust alternative to the beta model. This robustness\nis illustrated through simulated and real data applications. The estimation of\nthe parameters is done through an Expectation-Maximization algorithm and the\npaper discusses how to perform inference. A useful and practical discrimination\nprocedure is proposed for model selection between bessel and beta regressions.\nMonte Carlo simulation results are presented to verify the finite-sample\nbehavior of the EM-based estimators and the discrimination procedure. Further,\nthe performances of the regressions are evaluated under misspecification, which\nis a critical point showing the robustness of the proposed model. Finally,\nthree empirical illustrations are explored to confront results from bessel and\nbeta regressions.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 08:30:26 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Barreto-Souza", "Wagner", ""], ["Mayrink", "Vin\u00edcius D.", ""], ["Simas", "Alexandre B.", ""]]}, {"id": "2003.05200", "submitter": "Luisa Di Paola", "authors": "Luisa Di Paola and Alessandro Giuliani", "title": "Mapping active allosteric loci SARS-CoV Spike Proteins by means of\n  Protein Contact Networks", "comments": "13 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Coronaviruses are a class of virus responsible of the recent outbreak of\nHuman Severe Acute Respiratory Syndrome. The molecular machinery behind the\nviral entry and thus infectivity is based on the formation of the complex of\nvirus spike protein with the angiotensin-converting enzyme 2 (ACE2). The\ndetection of putative allosteric sites on the viral spike protein can trace the\npath to develop allosteric drugs to weaken the strength of the spike-ACE2\ninterface and, thus, reduce the viral infectivity. In this work we present\nresults of the application of the Protein Contact Network (PCN) paradigm to the\ncomplex SARS-CoV spike - ACE2 relative to both 2003 SARS and the recent 2019 -\nCoV. Results point to a specific region, present in both structures, that is\npredicted to act as allosteric site modulating the binding of the spike protein\nwith ACE2.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 10:20:45 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Di Paola", "Luisa", ""], ["Giuliani", "Alessandro", ""]]}, {"id": "2003.05227", "submitter": "Oscar Rodriguez De Rivera Ortega", "authors": "Oscar Rodriguez de Rivera, Antonio L\\'opez-Qu\\'ilez, Marta Blangiardo\n  and Martyna Wasilewska", "title": "A spatio-temporal model to understand forest fires causality in Europe", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Forest fires are the outcome of a complex interaction between environmental\nfactors, topography and socioeconomic factors (Bedia et al, 2014). Therefore,\nunderstand causality and early prediction are crucial elements for controlling\nsuch phenomenon and saving lives.The aim of this study is to build\nspatio-temporal model to understand causality of forest fires in Europe, at\nNUTS2 level between 2012 and 2016, using environmental and socioeconomic\nvariables.We have considered a disease mapping approach, commonly used in small\narea studies to assess thespatial pattern and to identify areas characterised\nby unusually high or low relative risk.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 11:40:55 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["de Rivera", "Oscar Rodriguez", ""], ["L\u00f3pez-Qu\u00edlez", "Antonio", ""], ["Blangiardo", "Marta", ""], ["Wasilewska", "Martyna", ""]]}, {"id": "2003.05331", "submitter": "Maximilian Pichler", "authors": "Maximilian Pichler, Florian Hartig", "title": "A new method for faster and more accurate inference of species\n  associations from big community data", "comments": "65 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.PE stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  1. Joint Species Distribution models (JSDMs) explain spatial variation in\ncommunity composition by contributions of the environment, biotic associations,\nand possibly spatially structured residual covariance. They show great promise\nas a general analytical framework for community ecology and macroecology, but\ncurrent JSDMs, even when approximated by latent variables, scale poorly on\nlarge datasets, limiting their usefulness for currently emerging big (e.g.,\nmetabarcoding and metagenomics) community datasets. 2. Here, we present a\nnovel, more scalable JSDM (sjSDM) that circumvents the need to use latent\nvariables by using a Monte-Carlo integration of the joint JSDM likelihood and\nallows flexible elastic net regularization on all model components. We\nimplemented sjSDM in PyTorch, a modern machine learning framework that can make\nuse of CPU and GPU calculations. Using simulated communities with known\nspecies-species associations and different number of species and sites, we\ncompare sjSDM with state-of-the-art JSDM implementations to determine\ncomputational runtimes and accuracy of the inferred species-species and\nspecies-environmental associations. 3. We find that sjSDM is orders of\nmagnitude faster than existing JSDM algorithms (even when run on the CPU) and\ncan be scaled to very large datasets. Despite the dramatically improved speed,\nsjSDM produces more accurate estimates of species association structures than\nalternative JSDM implementations. We demonstrate the applicability of sjSDM to\nbig community data using eDNA case study with thousands of fungi operational\ntaxonomic units (OTU). 4. Our sjSDM approach makes the analysis of JSDMs to\nlarge community datasets with hundreds or thousands of species possible,\nsubstantially extending the applicability of JSDMs in ecology. We provide our\nmethod in an R package to facilitate its applicability for practical data\nanalysis.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 14:37:02 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2020 08:11:01 GMT"}, {"version": "v3", "created": "Tue, 16 Jun 2020 13:26:47 GMT"}, {"version": "v4", "created": "Mon, 12 Oct 2020 11:46:42 GMT"}, {"version": "v5", "created": "Fri, 2 Jul 2021 09:24:25 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Pichler", "Maximilian", ""], ["Hartig", "Florian", ""]]}, {"id": "2003.05355", "submitter": "Igor Mikol\\'a\\v{s}ek", "authors": "Igor Mikolasek", "title": "New stochastic highway capacity estimation method and why product limit\n  method is unsuitable", "comments": "22 pages, 7 figures, 7 tables. Originally submitted to Transportation\n  Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kaplan-Meier estimate, commonly known as product limit method (PLM), and\nmaximum likelihood estimate (MLE) methods in general are often cited as means\nof stochastic highway capacity estimation. This article discusses their\nunsuitability for such application as properties of traffic flow do not meet\nthe assumptions for use of the methods. They assume the observed subject has a\nhistory which it went through and did not fail. However, due to its nature,\neach traffic flow measurement behaves as a separate subject which did not go\nthrough all the lower levels of intensity (did not \"age\"). An alternative\nmethod is proposed. It fits the resulting cumulative frequency of breakdowns\nwith respect to the traffic flow intensity leading to the breakdown instead of\ndirectly estimating the underlying probability distribution of capacity.\nAnalyses of accuracy and sensitivity to data quantity and censoring rate of the\nnew method are provided along with comparison to the PLM. The results prove\nunsuitability of the PLM and MLE methods in general. The new method is then\nused in a case study which compares capacity of a work-zone with and without a\ntraffic flow speed harmonisation system installed. The results confirm positive\neffect of harmonisation on capacity.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 15:25:58 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Mikolasek", "Igor", ""]]}, {"id": "2003.05408", "submitter": "Bijju Veduruparthi Mr", "authors": "Bijju Kranthi Veduruparthi, Jayanta Mukherjee, Partha Pratim Das,\n  Mandira Saha, Sanjoy Chatterjee, Raj Kumar Shrimali, Soumendranath Ray and\n  Sriram Prasath", "title": "Early Response Assessment in Lung Cancer Patients using Spatio-temporal\n  CBCT Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV eess.IV stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We report a model to predict patient's radiological response to curative\nradiation therapy (RT) for non-small-cell lung cancer (NSCLC).\n  Cone-Beam Computed Tomography images acquired weekly during the six-week\ncourse of RT were contoured with the Gross Tumor Volume (GTV) by senior\nradiation oncologists for 53 patients (7 images per patient).\n  Deformable registration of the images yielded six deformation fields for each\npair of consecutive images per patient.\n  Jacobian of a field provides a measure of local expansion/contraction and is\nused in our model.\n  Delineations were compared post-registration to compute unchanged ($U$),\nnewly grown ($G$), and reduced ($R$) regions within GTV.\n  The mean Jacobian of these regions $\\mu_U$, $\\mu_G$ and $\\mu_R$ are\nstatistically compared and a response assessment model is proposed.\n  A good response is hypothesized if $\\mu_R < 1.0$, $\\mu_R < \\mu_U$, and $\\mu_G\n< \\mu_U$.\n  For early prediction of post-treatment response, first, three weeks' images\nare used.\n  Our model predicted clinical response with a precision of $74\\%$.\n  Using reduction in CT numbers (CTN) and percentage GTV reduction as features\nin logistic regression, yielded an area-under-curve of 0.65 with p=0.005.\n  Combining logistic regression model with the proposed hypothesis yielded an\nodds ratio of 20.0 (p=0.0).\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 08:20:22 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Veduruparthi", "Bijju Kranthi", ""], ["Mukherjee", "Jayanta", ""], ["Das", "Partha Pratim", ""], ["Saha", "Mandira", ""], ["Chatterjee", "Sanjoy", ""], ["Shrimali", "Raj Kumar", ""], ["Ray", "Soumendranath", ""], ["Prasath", "Sriram", ""]]}, {"id": "2003.05428", "submitter": "Mitchell Kinney", "authors": "Mitchell Kinney", "title": "Template Matching Route Classification", "comments": null, "journal-ref": null, "doi": "10.1515/jqas-2019-0051", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper details a route classification method for American football using\na template matching scheme that is quick and does not require manual labeling.\nPre-defined routes from a standard receiver route tree are aligned closely with\ngame routes in order to determine the closest match. Based on a test game with\nmanually labeled routes, the method achieves moderate success with an overall\naccuracy of 72\\% of the 232 routes labeled correctly.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 17:36:44 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Kinney", "Mitchell", ""]]}, {"id": "2003.05463", "submitter": "Andreas F. Haselsteiner", "authors": "Ed Mackay, Andreas F. Haselsteiner", "title": "Marginal and total exceedance probabilities of environmental contours", "comments": "24 pages, 23 figures", "journal-ref": null, "doi": "10.1016/j.marstruc.2020.102863", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Various methods have been proposed for defining an environmental contour,\nbased on different concepts of exceedance probability. In the inverse\nfirst-order reliability method (IFORM) and the direct sampling (DS) method,\ncontours are defined in terms of exceedances within a region bounded by a\nhyperplane in either standard normal space or the original parameter space,\ncorresponding to marginal exceedance probabilities under rotations of the\ncoordinate system. In contrast, the more recent inverse second-order\nreliability method (ISORM) and highest density (HD) contours are defined in\nterms of an isodensity contour of the joint density function in either standard\nnormal space or the original parameter space, where an exceedance is defined to\nbe anywhere outside the contour. Contours defined in terms of the total\nprobability outside the contour are significantly more conservative than\ncontours defined in terms of marginal exceedance probabilities. In this work we\nstudy the relationship between the marginal exceedance probability of the\nmaximum value of each variable along an environmental contour and the total\nprobability outside the contour. The marginal exceedance probability of the\ncontour maximum can be orders of magnitude lower than the total exceedance\nprobability of the contour, with the differences increasing with the number of\nvariables. The full abstract is longer than arxiv's requirement of 1,920\ncharacters (see PDF).\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 18:04:35 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 11:08:54 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Mackay", "Ed", ""], ["Haselsteiner", "Andreas F.", ""]]}, {"id": "2003.05510", "submitter": "Mariano Amo-Salas", "authors": "Jes\\'us L\\'opez-Fidalgo and Mariano Amo-Salas", "title": "Optimal dose calibration in radiotherapy", "comments": "17 pages, 3 figures, 2 tables", "journal-ref": null, "doi": "10.1016/j.radphyschem.2020.108917", "report-no": null, "categories": "stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the tools provided by the theory of Optimal Experimental\nDesign are applied to a nonlinear calibration model. This is motivated by the\nneed of estimating radiation doses using radiochromic films for radiotherapy\npurposes. The calibration model is in this case nonlinear and the explanatory\nvariable cannot be worked out explicitly from the model. In this case an\nexperimental design has to be found on the dependent variable. For that, the\ninverse function theorem will be used to obtain an information matrix to be\noptimized. Optimal designs on the response variable are computed from two\ndifferent perspectives, first for fitting the model and estimating each of the\nparameters and then for predicting the proper dose to be applied to the\npatient. While the first is a common point of view in a general context of the\nOptimal Experimental Design, the latter is actually the main objective of the\ncalibration problem for the practitioners and algorithms for computing these\noptimal designs are also provided.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 20:23:04 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["L\u00f3pez-Fidalgo", "Jes\u00fas", ""], ["Amo-Salas", "Mariano", ""]]}, {"id": "2003.05681", "submitter": "Ke Wu", "authors": "Ke Wu, Didier Darcet, Qian Wang, Didier Sornette", "title": "Generalized logistic growth modeling of the COVID-19 outbreak: comparing\n  the dynamics in the 29 provinces in China and in the rest of the world", "comments": null, "journal-ref": "Nonlinear Dynamics, 2020", "doi": "10.1007/s11071-020-05862-6", "report-no": null, "categories": "q-bio.PE physics.bio-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Started in Wuhan, China, the COVID-19 has been spreading all over the world.\nWe calibrate the logistic growth model, the generalized logistic growth model,\nthe generalized Richards model and the generalized growth model to the reported\nnumber of infected cases for the whole of China, 29 provinces in China, and 33\ncountries and regions that have been or are undergoing major outbreaks. We\ndissect the development of the epidemics in China and the impact of the drastic\ncontrol measures both at the aggregate level and within each province. We\nquantitatively document four phases of the outbreak in China with a detailed\nanalysis on the heterogeneous situations across provinces. The extreme\ncontainment measures implemented by China were very effective with some\ninstructive variations across provinces. Borrowing from the experience of\nChina, we made scenario projections on the development of the outbreak in other\ncountries. We identified that outbreaks in 14 countries (mostly in western\nEurope) have ended, while resurgences of cases have been identified in several\namong them. The modeling results clearly show longer after-peak trajectories in\nwestern countries, in contrast to most provinces in China where the after-peak\ntrajectory is characterized by a much faster decay. We identified three groups\nof countries in different level of outbreak progress, and provide informative\nimplications for the current global pandemic.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 09:45:27 GMT"}, {"version": "v2", "created": "Sat, 9 May 2020 14:12:49 GMT"}, {"version": "v3", "created": "Wed, 23 Sep 2020 03:43:08 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Wu", "Ke", ""], ["Darcet", "Didier", ""], ["Wang", "Qian", ""], ["Sornette", "Didier", ""]]}, {"id": "2003.05686", "submitter": "Shovanur Haque", "authors": "Shovanur Haque, Kerrie Mengersen", "title": "Assessing the accuracy of individual link with varying block sizes and\n  cut-off values using MaCSim approach", "comments": "24 pages, 6 figures. arXiv admin note: text overlap with\n  arXiv:1901.04779", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DB stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Record linkage is the process of matching together records from different\ndata sources that belong to the same entity. Record linkage is increasingly\nbeing used by many organizations including statistical, health, government etc.\nto link administrative, survey, and other files to create a robust file for\nmore comprehensive analysis. Therefore, it becomes necessary to assess the\nability of a linking method to achieve high accuracy or compare between methods\nwith respect to accuracy. In this paper, we evaluate the accuracy of individual\nlink using varying block sizes and different cut-off values by utilizing a\nMarkov Chain based Monte Carlo simulation approach (MaCSim). MaCSim utilizes\ntwo linked files to create an agreement matrix. The agreement matrix is\nsimulated to generate re-sampled versions of the agreement matrix. A defined\nlinking method is used in each simulation to link the files and the accuracy of\nthe linking method is assessed. The aim of this paper is to facilitate optimal\nchoice of block size and cut-off value to achieve high accuracy in terms of\nminimizing average False Discovery Rate and False Negative Rate. The analyses\nhave been performed using a synthetic dataset provided by the Australian Bureau\nof Statistics (ABS) and indicated promising results.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 10:03:20 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 04:46:31 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2020 22:50:28 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Haque", "Shovanur", ""], ["Mengersen", "Kerrie", ""]]}, {"id": "2003.05777", "submitter": "Ranjan Maitra", "authors": "Souradeep Chattopadhyay and Steven D. Kawaler and Ranjan Maitra", "title": "Multi-layered Characterisation of hot stellar systems with confidence", "comments": "10 pages; 7 figures; 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.GA astro-ph.SR stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the physical and evolutionary properties of Hot Stellar Systems\n(HSS) is a major challenge in astronomy. We studied the dataset on 13456 HSS of\nMisgeld and Hilker (2011) that includes 12763 candidate globular clusters and\nfound multi-layered homogeneous grouping among these stellar systems. Our\nmethods elicited eight homogeneous ellipsoidal groups at the finest sub-group\nlevel. Some of these groups have high overlap and were merged through a\nmulti-phased syncytial algorithm motivated from Almod\\'ovar-Rivera and Maitra\n(2020). Five groups were merged in the first phase, resulting in three\ncomplex-structured groups. Our algorithm determined further complex structure\nand permitted one more merging phase, revealing two complex-structured groups\nat the highest level. A nonparametric bootstrap procedure found our group\nassignments to generally have high confidences in classification, indicating\nstability of our HSS assignments. The physical and kinematic properties of the\ntwo highest-level groups were assessed in terms of mass, effective radius,\nsurface density and mass-luminosity ratio. The first group consisted of older,\nsmaller and less bright HSS while the second group consisted of the brighter\nand younger HSS. Our analysis provides novel insight into the physical and\nevolutionary properties of HSS and specifically of %also helps to understand\nphysical and evolutionary properties of candidate globular clusters. Further,\nthe candidate globular clusters are seen to have very high probability of being\nglobular clusters rather than dwarfs or dwarf ellipticals that are also\nindicated to be quite distinct from each other.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 13:12:14 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 05:23:42 GMT"}, {"version": "v3", "created": "Wed, 23 Dec 2020 08:30:15 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Chattopadhyay", "Souradeep", ""], ["Kawaler", "Steven D.", ""], ["Maitra", "Ranjan", ""]]}, {"id": "2003.05780", "submitter": "Stefano Mazzuco", "authors": "Ainhoa-Elena L\\'eger and Stefano Mazzuco", "title": "What can we learn from functional clustering of mortality data? An\n  application to HMD data", "comments": "18 pages, 26 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most cases, mortality is analysed considering summary indicators (e.~g.\n$e_0$ or $e^{\\dagger}_0$) that either focus on a specific mortality component\nor pool all component-specific information in one measure. This can be a\nlimitation, when we are interested to analyse the global evolution of mortality\npatterns without loosing sight of specific components evolution. The paper\nanalyses whether there are different patterns of mortality decline among\ndeveloped countries, identifying the role played by all the mortality\ncomponents. We implement a cluster analysis using a Functional Data Analysis\n(FDA) approach, which allows us to consider age-specific mortality rather than\nsummary measures as it analyses curves rather than scalar data. Combined with a\nFunctional Principal Component Analysis (PCA) method it can identify what part\nof the curves (mortality components) is responsible for assigning one country\nto a specific cluster. FDA clustering is applied to 32 countries of Human\nMortality Database and years 1960--2010. The results show that the evolutions\nof developed countries follow the same pattern (with different timing): (1) a\nreduction of infant mortality, (2) an increase of premature mortality, (3) a\nshift and compression of deaths. Some countries are following this scheme and\nrecovering the gap with precursors, others do not show signs of recovery.\nEastern Europe countries are still at stage (2) and it is not clear if and when\nthey will enter into phase (3). All the country differences relates the\ndifferent timing with which countries undergo the stages identified by\nclusters. The cluster analysis based on FDA allows therefore a comprehensive\nunderstanding of the patterns of mortality decline for considered countries.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 13:13:43 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["L\u00e9ger", "Ainhoa-Elena", ""], ["Mazzuco", "Stefano", ""]]}, {"id": "2003.05854", "submitter": "Marco Oesting", "authors": "Marco Oesting, Philippe Naveau", "title": "Spatial Modeling of Heavy Precipitation by Coupling Weather Station\n  Recordings and Ensemble Forecasts with Max-Stable Processes", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to complex physical phenomena, the distribution of heavy rainfall events\nis difficult to model spatially. Physically based numerical models can often\nprovide physically coherent spatial patterns, but may miss some important\nprecipitation features like heavy rainfall intensities. Measurements at\nground-based weather stations, however, supply adequate rainfall intensities,\nbut most national weather recording networks are often spatially too sparse to\ncapture rainfall patterns adequately. To bring the best out of these two\nsources of information, climatologists and hydrologists have been seeking\nmodels that can efficiently merge different types of rainfall data. One\ninherent difficulty is to capture the appropriate multivariate dependence\nstructure among rainfall maxima. For this purpose, multivariate extreme value\ntheory suggests the use of a max-stable process. Such a process can be\nrepresented by a max-linear combination of independent copies of a hidden\nstochastic process weighted by a Poisson point process. In practice, the choice\nof this hidden process is non-trivial, especially if anisotropy,\nnon-stationarity and nugget effects are present in the spatial data at hand. By\ncoupling forecast ensemble data from the French national weather service\n(M\\'et\\'eo-France) with local observations, we construct and compare different\ntypes of data driven max-stable processes that are parsimonious in parameters,\neasy to simulate and capable of reproducing nugget effects and spatial\nnon-stationarities. We also compare our new method with classical approaches\nfrom spatial extreme value theory such as Brown-Resnick processes.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 15:37:59 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Oesting", "Marco", ""], ["Naveau", "Philippe", ""]]}, {"id": "2003.05979", "submitter": "Bonnie Shook-Sa", "authors": "Bonnie E. Shook-Sa and Michael G. Hudgens", "title": "Power and Sample Size for Marginal Structural Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Marginal structural models fit via inverse probability of treatment weighting\nare commonly used to control for confounding when estimating causal effects\nfrom observational data. When planning a study that will be analyzed with\nmarginal structural modeling, determining the required sample size for a given\nlevel of statistical power is challenging because of the effect of weighting on\nthe variance of the estimated causal means. This paper considers the utility of\nthe design effect to quantify the effect of weighting on the precision of\ncausal estimates. The design effect is defined as the ratio of the variance of\nthe causal mean estimator divided by the variance of a naive estimator if,\ncounter to fact, no confounding had been present and weights were not needed. A\nsimple, closed-form approximation of the design effect is derived that is\noutcome invariant and can be estimated during the study design phase. Once the\ndesign effect is approximated for each treatment group, sample size\ncalculations are conducted as for a randomized trial, but with variances\ninflated by the design effects to account for weighting. Simulations\ndemonstrate the accuracy of the design effect approximation, and practical\nconsiderations are discussed.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 19:04:57 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Shook-Sa", "Bonnie E.", ""], ["Hudgens", "Michael G.", ""]]}, {"id": "2003.05980", "submitter": "Zichao Wang", "authors": "Zichao Wang, Sebastian Tschiatschek, Simon Woodhead, Jose Miguel\n  Hernandez-Lobato, Simon Peyton Jones, Richard G. Baraniuk, Cheng Zhang", "title": "Educational Question Mining At Scale: Prediction, Analysis and\n  Personalization", "comments": "Accepted at AAAI-EAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online education platforms enable teachers to share a large number of\neducational resources such as questions to form exercises and quizzes for\nstudents. With large volumes of available questions, it is important to have an\nautomated way to quantify their properties and intelligently select them for\nstudents, enabling effective and personalized learning experiences. In this\nwork, we propose a framework for mining insights from educational questions at\nscale. We utilize the state-of-the-art Bayesian deep learning method, in\nparticular partial variational auto-encoders (p-VAE), to analyze real students'\nanswers to a large collection of questions. Based on p-VAE, we propose two\nnovel metrics that quantify question quality and difficulty, respectively, and\na personalized strategy to adaptively select questions for students. We apply\nour proposed framework to a real-world dataset with tens of thousands of\nquestions and tens of millions of answers from an online education platform.\nOur framework not only demonstrates promising results in terms of statistical\nmetrics but also obtains highly consistent results with domain experts'\nevaluation.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 19:07:49 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 04:04:32 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Wang", "Zichao", ""], ["Tschiatschek", "Sebastian", ""], ["Woodhead", "Simon", ""], ["Hernandez-Lobato", "Jose Miguel", ""], ["Jones", "Simon Peyton", ""], ["Baraniuk", "Richard G.", ""], ["Zhang", "Cheng", ""]]}, {"id": "2003.05990", "submitter": "Ranjan Maitra", "authors": "Karl T. Pazdernik and Ranjan Maitra", "title": "Estimating Basis Functions in Massive Fields under the Spatial Mixed\n  Effects Model", "comments": "21 pages, 18 figures, 7 tables", "journal-ref": null, "doi": "10.1002/SAM.11537", "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial prediction is commonly achieved under the assumption of a Gaussian\nrandom field (GRF) by obtaining maximum likelihood estimates of parameters, and\nthen using the kriging equations to arrive at predicted values. For massive\ndatasets, fixed rank kriging using the Expectation-Maximization (EM) algorithm\nfor estimation has been proposed as an alternative to the usual but\ncomputationally prohibitive kriging method. The method reduces computation cost\nof estimation by redefining the spatial process as a linear combination of\nbasis functions and spatial random effects. A disadvantage of this method is\nthat it imposes constraints on the relationship between the observed locations\nand the knots. We develop an alternative method that utilizes the Spatial Mixed\nEffects (SME) model, but allows for additional flexibility by estimating the\nrange of the spatial dependence between the observations and the knots via an\nAlternating Expectation Conditional Maximization (AECM) algorithm. Experiments\nshow that our methodology improves estimation without sacrificing prediction\naccuracy while also minimizing the additional computational burden of extra\nparameter estimation. The methodology is applied to a temperature data set\narchived by the United States National Climate Data Center, with improved\nresults over previous methodology.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 19:36:40 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Pazdernik", "Karl T.", ""], ["Maitra", "Ranjan", ""]]}, {"id": "2003.06002", "submitter": "Jami Mulgrave", "authors": "Jami J. Mulgrave, David Madigan, George Hripcsak", "title": "Bayesian Posterior Interval Calibration to Improve the Interpretability\n  of Observational Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Observational healthcare data offer the potential to estimate causal effects\nof medical products on a large scale. However, the confidence intervals and\np-values produced by observational studies only account for random error and\nfail to account for systematic error. As a consequence, operating\ncharacteristics such as confidence interval coverage and Type I error rates\noften deviate sharply from their nominal values and render interpretation\nimpossible. While there is longstanding awareness of systematic error in\nobservational studies, analytic approaches to empirically account for\nsystematic error are relatively new. Several authors have proposed approaches\nusing negative controls (also known as \"falsification hypotheses\") and positive\ncontrols. The basic idea is to adjust confidence intervals and p-values in\nlight of the bias (if any) detected in the analyses of the negative and\npositive control. In this work, we propose a Bayesian statistical procedure for\nposterior interval calibration that uses negative and positive controls. We\nshow that the posterior interval calibration procedure restores nominal\ncharacteristics, such as 95% coverage of the true effect size by the 95%\nposterior interval.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 20:13:04 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 20:34:56 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Mulgrave", "Jami J.", ""], ["Madigan", "David", ""], ["Hripcsak", "George", ""]]}, {"id": "2003.06067", "submitter": "Han Lin Shang", "authors": "Ufuk Beyaztas and Han Lin Shang", "title": "A comparison of parameter estimation in function-on-function regression", "comments": "43 pages, 9 figures, 8 tables, to appear at Communications in\n  Statistics - Simulation and Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent technological developments have enabled us to collect complex and\nhigh-dimensional data in many scientific fields, such as population health,\nmeteorology, econometrics, geology, and psychology. It is common to encounter\nsuch datasets collected repeatedly over a continuum. Functional data, whose\nsample elements are functions in the graphical forms of curves, images, and\nshapes, characterize these data types. Functional data analysis techniques\nreduce the complex structure of these data and focus on the dependences within\nand (possibly) between the curves. A common research question is to investigate\nthe relationships in regression models that involve at least one functional\nvariable. However, the performance of functional regression models depends on\nseveral factors, such as the smoothing technique, the number of basis\nfunctions, and the estimation method. This paper provides a selective\ncomparison for function-on-function regression models where both the response\nand predictor(s) are functions, to determine the optimal choice of basis\nfunction from a set of model evaluation criteria. We also propose a bootstrap\nmethod to construct a confidence interval for the response function. The\nnumerical comparisons are implemented through Monte Carlo simulations and two\nreal data examples.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 23:52:17 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Beyaztas", "Ufuk", ""], ["Shang", "Han Lin", ""]]}, {"id": "2003.06143", "submitter": "Nemanja Djuric", "authors": "Sai Yalamanchi, Tzu-Kuo Huang, Galen Clark Haynes, Nemanja Djuric", "title": "Long-term Prediction of Vehicle Behavior using Short-term\n  Uncertainty-aware Trajectories and High-definition Maps", "comments": "Accepted for publication at IEEE International Conference on\n  Intelligent Transportation Systems (ITSC) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion prediction of surrounding vehicles is one of the most important tasks\nhandled by a self-driving vehicle, and represents a critical step in the\nautonomous system necessary to ensure safety for all the involved traffic\nactors. Recently a number of researchers from both academic and industrial\ncommunities have focused on this important problem, proposing ideas ranging\nfrom engineered, rule-based methods to learned approaches, shown to perform\nwell at different prediction horizons. In particular, while for longer-term\ntrajectories the engineered methods outperform the competing approaches, the\nlearned methods have proven to be the best choice at short-term horizons. In\nthis work we describe how to overcome the discrepancy between these two\nresearch directions, and propose a method that combines the disparate\napproaches under a single unifying framework. The resulting algorithm fuses\nlearned, uncertainty-aware trajectories with lane-based paths in a principled\nmanner, resulting in improved prediction accuracy at both shorter- and\nlonger-term horizons. Experiments on real-world, large-scale data strongly\nsuggest benefits of the proposed unified method, which outperformed the\nexisting state-of-the-art. Moreover, following offline evaluation the proposed\nmethod was successfully tested onboard a self-driving vehicle.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 07:54:26 GMT"}, {"version": "v2", "created": "Sat, 13 Jun 2020 02:54:42 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Yalamanchi", "Sai", ""], ["Huang", "Tzu-Kuo", ""], ["Haynes", "Galen Clark", ""], ["Djuric", "Nemanja", ""]]}, {"id": "2003.06207", "submitter": "Giuseppe Arbia Dr", "authors": "Giuseppe Arbia", "title": "A Note on Early Epidemiological Analysis of Coronavirus Disease 2019\n  Outbreak using Crowdsourced Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing data can prove of paramount importance in monitoring and\ncontrolling the spread of infectious diseases. The recent paper by Sun, Chen\nand Viboud (2020) is important because it contributes to the understanding of\nthe epidemiology and of the spreading of Covid-19 in a period when most of the\nepidemic characteristics are still unknown. However, the use of crowdsourcing\ndata raises a number of problems from the statistical point of view which run\nthe risk of invalidating the results and of biasing estimation and hypothesis\ntesting. While the work by Sun, Chen and Viboud (2020) has to be commended,\ngiven the importance of the topic for worldwide health security, in this paper\nwe deem important to remark the presence of the possible sources of statistical\nbiases and to point out possible solutions to them\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 11:22:02 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Arbia", "Giuseppe", ""]]}, {"id": "2003.06271", "submitter": "Johannes Haupt", "authors": "Johannes Haupt and Stefan Lessmann", "title": "Targeting Customers under Response-Dependent Costs", "comments": "20 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study provides a formal analysis of the customer targeting decision\nproblem in settings where the cost for marketing action is stochastic and\nproposes a framework to efficiently estimate the decision variables for\ncampaign profit optimization. Targeting a customer is profitable if the\npositive impact of the marketing treatment on the customer and the associated\nprofit to the company is higher than the cost of the treatment. While there is\na growing literature on developing causal or uplift models to identify the\ncustomers who are impacted most strongly by the marketing action, no research\nhas investigated optimal targeting when the costs of the action are uncertain\nat the time of the targeting decision. Because marketing incentives are\nroutinely conditioned on a positive response by the customer, e.g. a purchase\nor contract renewal, stochastic costs are ubiquitous in direct marketing and\ncustomer retention campaigns. This study makes two contributions to the\nliterature, which are evaluated on a coupon targeting campaign in an e-commerce\nsetting. First, the authors formally analyze the targeting decision problem\nunder response-dependent costs. Profit-optimal targeting requires an estimate\nof the treatment effect on the customer and an estimate of the customer\nresponse probability under treatment. The empirical results demonstrate that\nthe consideration of treatment cost substantially increases campaign profit\nwhen used for customer targeting in combination with the estimation of the\naverage or customer-level treatment effect. Second, the authors propose a\nframework to jointly estimate the treatment effect and the response probability\ncombining methods for causal inference with a hurdle mixture model. The\nproposed causal hurdle model achieves competitive campaign profit while\nstreamlining model building. The code for the empirical analysis is available\non Github.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 13:23:03 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Haupt", "Johannes", ""], ["Lessmann", "Stefan", ""]]}, {"id": "2003.06291", "submitter": "Shovanur Haque", "authors": "Shovanur Haque, Kerrie Mengersen", "title": "Improved assessment of the accuracy of record linkage via an extended\n  MaCSim approach", "comments": "32 pages, 4 figures. arXiv admin note: text overlap with\n  arXiv:1901.04779", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DB stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Record linkage is the process of bringing together the same entity from\noverlapping data sources while removing duplicates. Huge amounts of data are\nnow being collected by public or private organizations as well as by\nresearchers and individuals. Linking and analysing relevant information from\nthis massive data reservoir can provide new insights into society. However,\nthis increase in the amount of data may also increase the likelihood of\nincorrectly linked records among databases. It has become increasingly\nimportant to have effective and efficient methods for linking data from\ndifferent sources. Therefore, it becomes necessary to assess the ability of a\nlinking method to achieve high accuracy or to compare between methods with\nrespect to accuracy. In this paper, we improve on a Markov Chain based Monte\nCarlo simulation approach (MaCSim) for assessing a linking method. MaCSim\nutilizes two linked files that have been previously linked on similar types of\ndata to create an agreement matrix and then simulates the matrix using a\nproposed algorithm developed to generate re-sampled versions of the agreement\nmatrix. A defined linking method is used in each simulation to link the files\nand the accuracy of the linking method is assessed. The improvement proposed\nhere involves calculation of a similarity weight for every linking variable\nvalue for each record pair, which allows partial agreement of the linking\nvariable values. A threshold is calculated for every linking variable based on\nadjustable parameter \"tolerance\" for that variable. To assess the accuracy of\nlinking method, correctly linked proportions are investigated for each record.\nThe extended MaCSim approach is illustrated using a synthetic dataset provided\nby the Australian Bureau of Statistics (ABS) based on realistic data settings.\nTest results show higher accuracy of the assessment of linkages.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 10:41:21 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 05:23:21 GMT"}, {"version": "v3", "created": "Tue, 13 Oct 2020 01:16:17 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Haque", "Shovanur", ""], ["Mengersen", "Kerrie", ""]]}, {"id": "2003.06299", "submitter": "Aritra Halder", "authors": "Aritra Halder, Shariq Mohammed, Kun Chen and Dipak K. Dey", "title": "Spatial Tweedie exponential dispersion models", "comments": "26 pages, 3 figures and 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a general modeling framework that allows for uncertainty\nquantification at the individual covariate level and spatial referencing,\noperating withing a double generalized linear model (DGLM). DGLMs provide a\ngeneral modeling framework allowing dispersion to depend in a link-linear\nfashion on chosen covariates. We focus on working with Tweedie exponential\ndispersion models while considering DGLMs, the reason being their recent\nwide-spread use for modeling mixed response types. Adopting a regularization\nbased approach, we suggest a class of flexible convex penalties derived from an\nun-directed graph that facilitates estimation of the unobserved spatial effect.\nDevelopments are concisely showcased by proposing a co-ordinate descent\nalgorithm that jointly explains variation from covariates in mean and\ndispersion through estimation of respective model coefficients while estimating\nthe unobserved spatial effect. Simulations performed show that proposed\napproach is superior to competitors like the ridge and un-penalized versions.\nFinally, a real data application is considered while modeling insurance losses\narising from automobile collisions in the state of Connecticut, USA for the\nyear 2008.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 06:16:41 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Halder", "Aritra", ""], ["Mohammed", "Shariq", ""], ["Chen", "Kun", ""], ["Dey", "Dipak K.", ""]]}, {"id": "2003.06368", "submitter": "Naveed Merchant", "authors": "Jeffery Hart and Taeryon Choi and Naveed Merchant", "title": "Use of Cross-validation Bayes Factors to Test Equality of Two Densities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a non-parametric, two-sample Bayesian test for checking whether or\nnot two data sets share a common distribution. The test makes use of data\nsplitting ideas and does not require priors for high-dimensional parameter\nvectors as do other nonparametric Bayesian procedures. We provide evidence that\nthe new procedure provides more stable Bayes factors than do methods based on\nP\\'olya trees. Somewhat surprisingly, the behavior of the proposed Bayes\nfactors when the two distributions are the same is usually superior to that of\nP\\'olya tree Bayes factors. We showcase the effectiveness of the test by\nproving its consistency, conducting a simulation study and applying the test to\nHiggs boson data.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 16:26:00 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Hart", "Jeffery", ""], ["Choi", "Taeryon", ""], ["Merchant", "Naveed", ""]]}, {"id": "2003.06378", "submitter": "Samer Katicha", "authors": "Samer Katicha, John Khoury, Gerardo Flintsch", "title": "Spatial multiresolution analysis approach to identify crash hotspots and\n  estimate crash risk", "comments": "19 pages, 10 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the authors evaluate the performance of a spatial\nmultiresolution analysis (SMA) method that behaves like a variable bandwidth\nkernel density estimation (KDE) method, for hazardous road segments\nidentification (HRSI) and crash risk (expected number of crashes) estimation.\nThe proposed SMA, is similar to the KDE method with the additional benefit of\nallowing for the bandwidth to be different at different road segments depending\non how homogenous the segments are. Furthermore, the optimal bandwidth at each\nroad segment is determined solely based on the data by minimizing an unbiased\nestimate of the mean square error. The authors compare the SMA method with the\nstate of the practice crash analysis method, the empirical Bayes (EB) method,\nin terms of their HRSI ability and their ability to predict future crashes. The\nresults indicate that the SMA may outperform the EB method, at least with the\ncrash data of the entire Virginia interstate network used in this paper. The\nSMA is implemented in an Excel spreadsheet that is freely available for\ndownload.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 16:50:39 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Katicha", "Samer", ""], ["Khoury", "John", ""], ["Flintsch", "Gerardo", ""]]}, {"id": "2003.06418", "submitter": "Roberto Buizza", "authors": "Roberto Buizza", "title": "Weather-inspired ensemble-based probabilistic prediction of COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this work is to predict the spread of COVID-19 starting from\nobserved data, using a forecast method inspired by probabilistic weather\nprediction systems operational today.\n  Results show that this method works well for China: on day 25 we could have\npredicted well the outcome for the next 35 days. The same method has been\napplied to Italy and South Korea, and forecasts for the forthcoming weeks are\nincluded in this work. For Italy, forecasts based on data collected up to today\n(24 March) indicate that number of observed cases could grow from the current\nvalue of 69,176, to between 101k-180k, with a 50% probability of being between\n110k-135k. For South Korea, it suggests that the number of observed cases could\ngrow from the current value of 9,018 (as of the 23rd of March), to values\nbetween 8,500 and 9,300, with a 50% probability of being between 8,700 and\n8,900.\n  We conclude by suggesting that probabilistic disease prediction systems are\npossible and could be developed following key ideas and methods from weather\nforecasting. Having access to skilful daily updated forecasts could help taking\nbetter informed decisions on how to manage the spread of diseases such as\nCOVID-19.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 00:13:01 GMT"}, {"version": "v2", "created": "Sun, 29 Mar 2020 11:26:50 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Buizza", "Roberto", ""]]}, {"id": "2003.06541", "submitter": "Jami Mulgrave", "authors": "Jami J. Mulgrave, Matthew E. Levine, David J. Albers, Joon Ha, Arthur\n  Sherman, and George Hripcsak", "title": "Using Data Assimilation of Mechanistic Models to Estimate Glucose and\n  Insulin Metabolism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.med-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Motivation: There is a growing need to integrate mechanistic models of\nbiological processes with computational methods in healthcare in order to\nimprove prediction. We apply data assimilation in the context of Type 2\ndiabetes to understand parameters associated with the disease.\n  Results: The data assimilation method captures how well patients improve\nglucose tolerance after their surgery. Data assimilation has the potential to\nimprove phenotyping in Type 2 diabetes.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 03:24:25 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Mulgrave", "Jami J.", ""], ["Levine", "Matthew E.", ""], ["Albers", "David J.", ""], ["Ha", "Joon", ""], ["Sherman", "Arthur", ""], ["Hripcsak", "George", ""]]}, {"id": "2003.06664", "submitter": "Diego Giuliani", "authors": "Diego Giuliani, Maria Michela Dickson, Giuseppe Espa, and Flavio Santi", "title": "Modelling and predicting the spatio-temporal spread of Coronavirus\n  disease 2019 (COVID-19) in Italy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Official freely available data about the number of infected at the finest\npossible level of spatial areal aggregation (Italian provinces) are used to\nmodel the spatio-temporal distribution of COVID-19 infections at local level.\nData time horizon ranges from 26 February 20020, which is the date when the\nfirst case not directly connected with China has been discovered in northern\nItaly, to 18 March 2020. An endemic-epidemic multivariate time-series\nmixed-effects generalized linear model for areal disease counts has been\nimplemented to understand and predict spatio-temporal diffusion of the\nphenomenon. Previous literature has shown that these class of models provide\nreliable predictions of infectious diseases in time and space. Three\nsubcomponents characterize the estimated model. The first is related to the\nevolution of the disease over time; the second is characterized by transmission\nof the illness among inhabitants of the same province; the third remarks the\neffects of spatial neighbourhood and try to capture the contagion effects of\nnearby areas. Focusing on the aggregated time-series of the daily counts in\nItaly, the contribution of any of the three subcomponents do not dominate on\nthe others and our predictions are excellent for the whole country, with an\nerror of 3 per thousand compared to the late available data. At local level,\ninstead, interesting distinct patterns emerge. In particular, the provinces\nfirst concerned by containment measures are those that are not affected by the\neffects of spatial neighbours. On the other hand, for the provinces the are\ncurrently strongly affected by contagions, the component accounting for the\nspatial interaction with surrounding areas is prevalent. Moreover, the proposed\nmodel provides good forecasts of the number of infections at local level while\ncontrolling for delayed reporting.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 16:04:59 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 14:07:09 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Giuliani", "Diego", ""], ["Dickson", "Maria Michela", ""], ["Espa", "Giuseppe", ""], ["Santi", "Flavio", ""]]}, {"id": "2003.06797", "submitter": "David Salgado", "authors": "David Salgado and Bogdan Oancea", "title": "On new data sources for the production of official statistics", "comments": "38 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the past years we have witnessed the rise of new data sources for the\npotential production of official statistics, which, by and large, can be\nclassified as survey, administrative, and digital data. Apart from the\ndifferences in their generation and collection, we claim that their lack of\nstatistical metadata, their economic value, and their lack of ownership by data\nholders pose several entangled challenges lurking the incorporation of new data\ninto the routinely production of official statistics. We argue that every\nchallenge must be duly overcome in the international community to bring new\nstatistical products based on these sources. These challenges can be naturally\nclassified into different entangled issues regarding access to data,\nstatistical methodology, quality, information technologies, and management. We\nidentify the most relevant to be necessarily tackled before new data sources\ncan be definitively considered fully incorporated into the production of\nofficial statistics.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 11:33:18 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Salgado", "David", ""], ["Oancea", "Bogdan", ""]]}, {"id": "2003.06843", "submitter": "Bohai Zhang", "authors": "Bohai Zhang and Noel Cressie", "title": "Bayesian Inference of Spatio-Temporal Changes of Arctic Sea Ice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arctic sea ice extent has drawn increasing interest and alarm from\ngeoscientists, owing to its rapid decline. In this article, we propose a\nBayesian spatio-temporal hierarchical statistical model for binary Arctic sea\nice data over two decades, where a latent dynamic spatio-temporal Gaussian\nprocess is used to model the data-dependence through a logit link function. Our\nultimate goal is to perform inference on the dynamic spatial behavior of Arctic\nsea ice over a period of two decades. Physically motivated covariates are\nassessed using autologistic diagnostics. Our Bayesian spatio-temporal model\nshows how parameter uncertainty in such a complex hierarchical model can\ninfluence spatio-temporal prediction. The posterior distributions of new\nsummary statistics are proposed to detect the changing patterns of Arctic sea\nice over two decades since 1997.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 14:48:50 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Zhang", "Bohai", ""], ["Cressie", "Noel", ""]]}, {"id": "2003.06924", "submitter": "Joshua North", "authors": "Joshua S. North, Erin M. Schliep, Christopher K. Wikle", "title": "On the spatial and temporal shift in the archetypal seasonal temperature\n  cycle as driven by annual and semi-annual harmonics", "comments": "25 pages, 11 color figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical methods are required to evaluate and quantify the uncertainty in\nenvironmental processes, such as land and sea surface temperature, in a\nchanging climate. Typically, annual harmonics are used to characterize the\nvariation in the seasonal temperature cycle. However, an often overlooked\nfeature of the climate seasonal cycle is the semi-annual harmonic, which can\naccount for a significant portion of the variance of the seasonal cycle and\nvaries in amplitude and phase across space. Together, the spatial variation in\nthe annual and semi-annual harmonics can play an important role in driving\nprocesses that are tied to seasonality (e.g., ecological and agricultural\nprocesses). We propose a multivariate spatio-temporal model to quantify the\nspatial and temporal change in minimum and maximum temperature seasonal cycles\nas a function of the annual and semi-annual harmonics. Our approach captures\nspatial dependence, temporal dynamics, and multivariate dependence of these\nharmonics through spatially and temporally-varying coefficients. We apply the\nmodel to minimum and maximum temperature over North American for the years 1979\nto 2018. Formal model inference within the Bayesian paradigm enables the\nidentification of regions experiencing significant changes in minimum and\nmaximum temperature seasonal cycles due to the relative effects of changes in\nthe two harmonics.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 21:14:13 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["North", "Joshua S.", ""], ["Schliep", "Erin M.", ""], ["Wikle", "Christopher K.", ""]]}, {"id": "2003.06966", "submitter": "Edgar Santos-Fernandez", "authors": "Edgar Santos-Fernandez, Kerrie Mengersen", "title": "Bayesian item response models for citizen science ecological data", "comments": "under review, 24 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  So-called 'citizen science' data elicited from crowds has become increasingly\npopular in many fields including ecology. However, the quality of this\ninformation is being frequently debated by many within the scientific\ncommunity. Therefore, modern citizen science implementations require measures\nof the users' proficiency that account for the difficulty of the tasks. We\nintroduce a new methodological framework of item response and linear logistic\ntest models with application to citizen science data used in ecology research.\nThis approach accommodates spatial autocorrelation within the item difficulties\nand produces relevant ecological measures of species and site-related\ndifficulties, discriminatory power and guessing behavior. These, along with\nestimates of the subject abilities allow better management of these programs\nand provide deeper insights. This paper also highlights the fit of item\nresponse models to big data via divide-and-conquer. We found that the suggested\nmethods outperform the traditional item response models in terms of RMSE,\naccuracy, and WAIC based on leave-one-out cross-validation on simulated and\nempirical data. We present a comprehensive implementation using a case study of\nspecies identification in the Serengeti, Tanzania. The R and Stan codes are\nprovided for full reproducibility. Multiple statistical illustrations and\nvisualizations are given which allow practitioners the extrapolation to a wide\nrange of citizen science ecological problems.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 01:33:26 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 23:03:52 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Santos-Fernandez", "Edgar", ""], ["Mengersen", "Kerrie", ""]]}, {"id": "2003.07150", "submitter": "Kamil Makiela", "authors": "Kamil Makie{\\l}a, B{\\l}a\\.zej Mazur", "title": "Stochastic Frontier Analysis with Generalized Errors: inference, model\n  comparison and averaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Contribution of this paper lies in the formulation and estimation of a\ngeneralized model for stochastic frontier analysis (SFA) that nests virtually\nall forms used and includes some that have not been considered so far. The\nmodel is based on the generalized t distribution for the observation error and\nthe generalized beta distribution of the second kind for the\ninefficiency-related term. We use this general error structure framework for\nformal testing, to compare alternative specifications and to conduct model\naveraging. This allows us to deal with model specification uncertainty, which\nis one of the main unresolved issues in SFA, and to relax a number of\npotentially restrictive assumptions embedded within existing SF models. We also\ndevelop Bayesian inference methods that are less restrictive compared to the\nones used so far and demonstrate feasible approximate alternatives based on\nmaximum likelihood.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 12:38:02 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 12:09:28 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Makie\u0142a", "Kamil", ""], ["Mazur", "B\u0142a\u017cej", ""]]}, {"id": "2003.07268", "submitter": "Rosa Candela", "authors": "Rosa Candela, Pietro Michiardi, Maurizio Filippone, Maria A. Zuluaga", "title": "Model Monitoring and Dynamic Model Selection in Travel Time-series\n  Forecasting", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-67667-4_31", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate travel products price forecasting is a highly desired feature that\nallows customers to take informed decisions about purchases, and companies to\nbuild and offer attractive tour packages. Thanks to machine learning (ML), it\nis now relatively cheap to develop highly accurate statistical models for price\ntime-series forecasting. However, once models are deployed in production, it is\ntheir monitoring, maintenance and improvement which carry most of the costs and\ndifficulties over time. We introduce a data-driven framework to continuously\nmonitor and maintain deployed time-series forecasting models' performance, to\nguarantee stable performance of travel products price forecasting models. Under\na supervised learning approach, we predict the errors of time-series\nforecasting models over time, and use this predicted performance measure to\nachieve both model monitoring and maintenance. We validate the proposed method\non a dataset of 18K time-series from flight and hotel prices collected over two\nyears and on two public benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 15:07:32 GMT"}, {"version": "v2", "created": "Sat, 21 Mar 2020 09:02:33 GMT"}, {"version": "v3", "created": "Mon, 30 Mar 2020 09:15:54 GMT"}, {"version": "v4", "created": "Fri, 11 Sep 2020 14:35:17 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Candela", "Rosa", ""], ["Michiardi", "Pietro", ""], ["Filippone", "Maurizio", ""], ["Zuluaga", "Maria A.", ""]]}, {"id": "2003.07347", "submitter": "Dave DeCaprio", "authors": "Dave DeCaprio, Joseph Gartner, Thadeus Burgess, Kristian Garcia,\n  Sarthak Kothari, Shaayan Sayed, Carol J. McCall (FSA, MPH)", "title": "Building a COVID-19 Vulnerability Index", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  COVID-19 is an acute respiratory disease that has been classified as a\npandemic by the World Health Organization. Characterization of this disease is\nstill in its early stages. However, it is known to have high mortality rates,\nparticularly among individuals with preexisting medical conditions. Creating\nmodels to identify individuals who are at the greatest risk for severe\ncomplications due to COVID-19 will be useful for outreach campaigns to help\nmitigate the disease's worst effects. While information specific to COVID-19 is\nlimited, a model using complications due to other upper respiratory infections\ncan be used as a proxy to help identify those individuals who are at the\ngreatest risk. We present the results for three models predicting such\ncomplications, with each model increasing predictive effectiveness at the\nexpense of ease of implementation.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 17:50:47 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 14:41:39 GMT"}, {"version": "v3", "created": "Sat, 18 Jul 2020 13:53:24 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["DeCaprio", "Dave", "", "FSA, MPH"], ["Gartner", "Joseph", "", "FSA, MPH"], ["Burgess", "Thadeus", "", "FSA, MPH"], ["Garcia", "Kristian", "", "FSA, MPH"], ["Kothari", "Sarthak", "", "FSA, MPH"], ["Sayed", "Shaayan", "", "FSA, MPH"], ["McCall", "Carol J.", "", "FSA, MPH"]]}, {"id": "2003.07364", "submitter": "Alireza Vafaei Sadr", "authors": "A. Vafaei Sadr, S. M. S. Movahed", "title": "Clustering of Local Extrema in Planck CMB maps", "comments": "17 pages, 7 figures, and 3 tables. Including major revision and\n  matched to the accepted version that appeared in MNRAS", "journal-ref": null, "doi": "10.1093/mnras/stab368", "report-no": null, "categories": "astro-ph.CO astro-ph.HE astro-ph.IM stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The clustering of local extrema will be exploited to examine Gaussianity,\nasymmetry, and the footprint of the cosmic-string network on the CMB observed\nby Planck. The number density of local extrema ($n_{\\rm pk}$ for peak and\n$n_{\\rm tr}$ for trough) and sharp clipping ($n_{\\rm pix}$) statistics support\nthe Gaussianity hypothesis for all component separations. However, the pixel at\nthe threshold reveals a more consistent treatment with respect to end-to-end\nsimulations. A very tiny deviation from associated simulations in the context\nof trough density, in the threshold range $\\theta\\in [-2-0]$ for NILC and CR\ncomponent separations, are detected. The unweighted two-point correlation\nfunction, of the local extrema, illustrates good consistency between different\ncomponent separations and corresponding Gaussian simulations for almost all\navailable thresholds. However, for high thresholds, a small deficit in the\nclustering of peaks is observed with respect to the Planck fiducial\n$\\Lambda$CDM model. To put a significant constraint on the amplitude of the\nmass function based on the value of $\\Psi$ around the Doppler peak\n($\\theta\\approx 70-75$ arcmin), we should consider $\\vartheta\\lesssim 0.0$. The\nscale-independent bias factors for the peak above a threshold for large\nseparation angle and high threshold level are in agreement with the value\nexpected for a pure Gaussian CMB. Applying the $n_{\\rm pk}$, $n_{\\rm tr}$,\n$\\Psi_{\\rm pk-pk}$ and $\\Psi_{\\rm tr-tr}$ measures on the tessellated CMB map\nwith patches of $7.5^2$ deg$^2$ size prove statistical isotropy in the Planck\nmaps. The peak clustering analysis puts the upper bound on the cosmic-string\ntension, $G\\mu^{(\\rm up)} \\lesssim 5.59\\times 10^{-7}$, in SMICA.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 18:00:01 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 10:37:40 GMT"}, {"version": "v3", "created": "Thu, 22 Apr 2021 09:18:07 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Sadr", "A. Vafaei", ""], ["Movahed", "S. M. S.", ""]]}, {"id": "2003.07398", "submitter": "Jiacong Du", "authors": "Jiacong Du, Jonathan Boss, Peisong Han, Lauren J Beesley, Stephen A\n  Goutman, Stuart Batterman, Eva L Feldman, Bhramar Mukherjee", "title": "Variable selection with multiply-imputed datasets: choosing between\n  stacked and grouped methods", "comments": "23 pages, 6 figures. This paper has been submitted to Statistics in\n  Medicine", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Penalized regression methods, such as lasso and elastic net, are used in many\nbiomedical applications when simultaneous regression coefficient estimation and\nvariable selection is desired. However, missing data complicates the\nimplementation of these methods, particularly when missingness is handled using\nmultiple imputation. Applying a variable selection algorithm on each imputed\ndataset will likely lead to different sets of selected predictors, making it\ndifficult to ascertain a final active set without resorting to ad hoc\ncombination rules. In this paper we consider a general class of penalized\nobjective functions which, by construction, force selection of the same\nvariables across multiply-imputed datasets. By pooling objective functions\nacross imputations, optimization is then performed jointly over all imputed\ndatasets rather than separately for each dataset. We consider two objective\nfunction formulations that exist in the literature, which we will refer to as\n\"stacked\" and \"grouped\" objective functions. Building on existing work, we (a)\nderive and implement efficient cyclic coordinate descent and\nmajorization-minimization optimization algorithms for both continuous and\nbinary outcome data, (b) incorporate adaptive shrinkage penalties, (c) compare\nthese methods through simulation, and (d) develop an R package miselect for\neasy implementation. Simulations demonstrate that the \"stacked\" objective\nfunction approaches tend to be more computationally efficient and have better\nestimation and selection properties. We apply these methods to data from the\nUniversity of Michigan ALS Patients Repository (UMAPR) which aims to identify\nthe association between persistent organic pollutants and ALS risk.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 18:39:30 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Du", "Jiacong", ""], ["Boss", "Jonathan", ""], ["Han", "Peisong", ""], ["Beesley", "Lauren J", ""], ["Goutman", "Stephen A", ""], ["Batterman", "Stuart", ""], ["Feldman", "Eva L", ""], ["Mukherjee", "Bhramar", ""]]}, {"id": "2003.07429", "submitter": "Lili Zheng", "authors": "Lili Zheng, Garvesh Raskutti, Rebecca Willett, Benjamin Mark", "title": "Context-dependent self-exciting point processes: models, methods, and\n  risk bounds in high dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional autoregressive point processes model how current events\ntrigger or inhibit future events, such as activity by one member of a social\nnetwork can affect the future activity of his or her neighbors. While past work\nhas focused on estimating the underlying network structure based solely on the\ntimes at which events occur on each node of the network, this paper examines\nthe more nuanced problem of estimating context-dependent networks that reflect\nhow features associated with an event (such as the content of a social media\npost) modulate the strength of influences among nodes. Specifically, we\nleverage ideas from compositional time series and regularization methods in\nmachine learning to conduct network estimation for high-dimensional marked\npoint processes. Two models and corresponding estimators are considered in\ndetail: an autoregressive multinomial model suited to categorical marks and a\nlogistic-normal model suited to marks with mixed membership in different\ncategories. Importantly, the logistic-normal model leads to a convex negative\nlog-likelihood objective and captures dependence across categories. We provide\ntheoretical guarantees for both estimators, which we validate by simulations\nand a synthetic data-generating model. We further validate our methods through\ntwo real data examples and demonstrate the advantages and disadvantages of both\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 20:22:43 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Zheng", "Lili", ""], ["Raskutti", "Garvesh", ""], ["Willett", "Rebecca", ""], ["Mark", "Benjamin", ""]]}, {"id": "2003.07494", "submitter": "Mostafa Karimi", "authors": "Kahkashan Afrin, Ashif S. Iquebal, Mostafa Karimi, Allyson Souris, Se\n  Yoon Lee, and Bani K. Mallick", "title": "Directionally Dependent Multi-View Clustering Using Copula Model", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0238996", "report-no": null, "categories": "stat.ME q-bio.GN stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent biomedical scientific problems, it is a fundamental issue to\nintegratively cluster a set of objects from multiple sources of datasets. Such\nproblems are mostly encountered in genomics, where data is collected from\nvarious sources, and typically represent distinct yet complementary\ninformation. Integrating these data sources for multi-source clustering is\nchallenging due to their complex dependence structure including directional\ndependency. Particularly in genomics studies, it is known that there is certain\ndirectional dependence between DNA expression, DNA methylation, and RNA\nexpression, widely called The Central Dogma.\n  Most of the existing multi-view clustering methods either assume an\nindependent structure or pair-wise (non-directional) dependency, thereby\nignoring the directional relationship. Motivated by this, we propose a\ncopula-based multi-view clustering model where a copula enables the model to\naccommodate the directional dependence existing in the datasets. We conduct a\nsimulation experiment where the simulated datasets exhibiting inherent\ndirectional dependence: it turns out that ignoring the directional dependence\nnegatively affects the clustering performance. As a real application, we\napplied our model to the breast cancer tumor samples collected from The Cancer\nGenome Altas (TCGA).\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 02:04:10 GMT"}, {"version": "v2", "created": "Sat, 22 Aug 2020 15:34:44 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Afrin", "Kahkashan", ""], ["Iquebal", "Ashif S.", ""], ["Karimi", "Mostafa", ""], ["Souris", "Allyson", ""], ["Lee", "Se Yoon", ""], ["Mallick", "Bani K.", ""]]}, {"id": "2003.07500", "submitter": "Benjamin Ackerman", "authors": "Benjamin Ackerman, Catherine R. Lesko, Juned Siddique, Ryoko Susukida\n  and Elizabeth A. Stuart", "title": "Generalizing Randomized Trial Findings to a Target Population using\n  Complex Survey Population Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized trials are considered the gold standard for estimating causal\neffects. Trial findings are often used to inform policy and programming\nefforts, yet their results may not generalize well to a relevant target\npopulation due to potential differences in effect moderators between the trial\nand population. Statistical methods have been developed to improve\ngeneralizability by combining trials and population data, and weighting the\ntrial to resemble the population on baseline covariates.Large-scale surveys in\nfields such as health and education with complex survey designs are a logical\nsource for population data; however, there is currently no best practice for\nincorporating survey weights when generalizing trial findings to a complex\nsurvey. We propose and investigate ways to incorporate survey weights in this\ncontext. We examine the performance of our proposed estimator in simulations by\ncomparing its performance to estimators that ignore the complex survey\ndesign.We then apply the methods to generalize findings from two trials - a\nlifestyle intervention for blood pressure reduction and a web-based\nintervention to treat substance use disorders - to their respective target\npopulations using population data from complex surveys. The work highlights the\nimportance in properly accounting for the complex survey design when\ngeneralizing trial findings to a population represented by a complex survey\nsample.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 02:29:58 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 18:59:00 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Ackerman", "Benjamin", ""], ["Lesko", "Catherine R.", ""], ["Siddique", "Juned", ""], ["Susukida", "Ryoko", ""], ["Stuart", "Elizabeth A.", ""]]}, {"id": "2003.07657", "submitter": "Ick Hoon Jin", "authors": "Alex Brodersen and Ick Hoon Jin and Ying Cheng and Minjeong Jeon", "title": "Applying the Network Item Response Model to Student Assessment Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study discusses an alternative tool for modeling student assessment\ndata. The model constructs networks from a matrix item responses and attempts\nto represent these data in low dimensional Euclidean space. This procedure has\nadvantages over common methods used for modeling student assessment data such\nas Item Response Theory because it relaxes the highly restrictive\nlocal-independence assumption. This article provides a deep discussion of the\nmodel and the steps one must take to estimate it. To enable extending a present\nmodel by adding data, two methods for estimating the positions of new\nindividuals in the network are discussed. Then, a real data analysis is then\nprovided as a case study on using the model and how to interpret the results.\nFinally, the model is compared and contrasted to other popular models in\npsychological and educational measurement: Item response theory (IRT) and\nnetwork psychometric Ising model for binary data.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 12:14:49 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Brodersen", "Alex", ""], ["Jin", "Ick Hoon", ""], ["Cheng", "Ying", ""], ["Jeon", "Minjeong", ""]]}, {"id": "2003.07778", "submitter": "Aboul Ella Hassanien Abo", "authors": "Haytham H. Elmousalami and Aboul Ella Hassanien (Scientific Research\n  Group in Egypt -- SRGE)", "title": "Day Level Forecasting for Coronavirus Disease (COVID-19) Spread:\n  Analysis, Modeling and Recommendations", "comments": "19 pages, 19 figure 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In mid of March 2020, Coronaviruses such as COVID-19 is declared as an\ninternational epidemic. More than 125000 confirmed cases and 4,607 death cases\nhave been recorded around more than 118 countries. Unfortunately, a coronavirus\nvaccine is expected to take at least 18 months if it works at all. Moreover,\nCOVID -19 epidemics can mutate into a more aggressive form. Day level\ninformation about the COVID -19 spread is crucial to measure the behavior of\nthis new virus globally. Therefore, this study presents a comparison of day\nlevel forecasting models on COVID-19 affected cases using time series models\nand mathematical formulation. The forecasting models and data strongly suggest\nthat the number of coronavirus cases grows exponentially in countries that do\nnot mandate quarantines, restrictions on travel and public gatherings, and\nclosing of schools, universities, and workplaces (Social Distancing).\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 16:07:09 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Elmousalami", "Haytham H.", "", "Scientific Research\n  Group in Egypt -- SRGE"], ["Hassanien", "Aboul Ella", "", "Scientific Research\n  Group in Egypt -- SRGE"]]}, {"id": "2003.07860", "submitter": "Eric Blankmeyer", "authors": "Eric Blankmeyer", "title": "NISE Estimation of an Economic Model of Crime", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An economic model of crime is used to explore the consistent estimation of a\nsimultaneous linear equation without recourse to instrumental variables. A\nmaximum-likelihood procedure (NISE) is introduced, and its results are compared\nto ordinary least squares and two-stage least squares. The paper is motivated\nby previous research on the crime model and by the well-known practical problem\nthat valid instruments are frequently unavailable.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 14:10:57 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Blankmeyer", "Eric", ""]]}, {"id": "2003.07899", "submitter": "Abhinav Prakash", "authors": "Abhinav Prakash, Rui Tuo, Yu Ding", "title": "Gaussian process aided function comparison using noisy scattered data", "comments": null, "journal-ref": null, "doi": "10.1080/00401706.2021.1905073", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a nonparametric method to compare the underlying mean\nfunctions given two noisy datasets. The motivation for the work stems from an\napplication of comparing wind turbine power curves. Comparing wind turbine data\npresents new problems, namely the need to identify the regions of difference in\nthe input space and to quantify the extent of difference that is statistically\nsignificant. Our proposed method, referred to as funGP, estimates the\nunderlying functions for different data samples using Gaussian process models.\nWe build a confidence band using the probability law of the estimated function\ndifferences under the null hypothesis. Then, the confidence band is used for\nthe hypothesis test as well as for identifying the regions of difference. This\nidentification of difference regions is a distinct feature, as existing methods\ntend to conduct an overall hypothesis test stating whether two functions are\ndifferent. Understanding the difference regions can lead to further practical\ninsights and help devise better control and maintenance strategies for wind\nturbines. The merit of funGP is demonstrated by using three simulation studies\nand four real wind turbine datasets.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 19:15:16 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 01:47:02 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Prakash", "Abhinav", ""], ["Tuo", "Rui", ""], ["Ding", "Yu", ""]]}, {"id": "2003.07900", "submitter": "Ben Lambert", "authors": "Ben Lambert, Aki Vehtari", "title": "$R^*$: A robust MCMC convergence diagnostic with uncertainty using\n  decision tree classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) has transformed Bayesian model inference over\nthe past three decades: mainly because of this, Bayesian inference is now a\nworkhorse of applied scientists. Under general conditions, MCMC sampling\nconverges asymptotically to the posterior distribution, but this provides no\nguarantees about its performance in finite time. The predominant method for\nmonitoring convergence is to run multiple chains and monitor individual chains'\ncharacteristics and compare these to the population as a whole: if within-chain\nand between-chain summaries are comparable, then this is taken to indicate that\nthe chains have converged to a common stationary distribution. Here, we\nintroduce a new method for diagnosing convergence based on how well a machine\nlearning classifier model can successfully discriminate the individual chains.\nWe call this convergence measure $R^*$. In contrast to the predominant\n$\\widehat{R}$, $R^*$ is a single statistic across all parameters that indicates\nlack of mixing, although individual variables' importance for this metric can\nalso be determined. Additionally, $R^*$ is not based on any single\ncharacteristic of the sampling distribution; instead it uses all the\ninformation in the chain, including that given by the joint sampling\ndistribution, which is currently largely overlooked by existing approaches. We\nrecommend calculating $R^*$ using two different machine learning classifiers -\ngradient-boosted regression trees and random forests - which each work well in\nmodels of different dimensions. Because each of these methods outputs a\nclassification probability, as a byproduct, we obtain uncertainty in $R^*$. The\nmethod is straightforward to implement and could be a complementary additional\ncheck on MCMC convergence for applied analyses.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 19:17:11 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2020 17:14:39 GMT"}, {"version": "v3", "created": "Thu, 3 Sep 2020 11:36:25 GMT"}, {"version": "v4", "created": "Fri, 4 Sep 2020 12:14:57 GMT"}, {"version": "v5", "created": "Thu, 19 Nov 2020 10:00:45 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Lambert", "Ben", ""], ["Vehtari", "Aki", ""]]}, {"id": "2003.07928", "submitter": "Marco Bonetti", "authors": "Marco Bonetti and Ugofilippo Basellini", "title": "Epilocal: a real-time tool for local epidemic monitoring", "comments": "8 pages, 2 figures, link to R code and to shinyapp", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe Epilocal, a simple R program designed to automatically download\nthe most recent data on reported infected SARS-CoV-2 cases for all Italian\nprovinces and regions, and to provide a simple descriptive analysis. For each\nprovince the cumulative number of reported infected cases is available each\nday. In addition, the current numbers of hospitalized patients (separately for\nintensive care or not) and the cumulative number of deceased individuals are\navailable at the region level. The data are analyzed through Poisson\ngeneralized linear models with logarithmic link function and polynomial\nregression on time. For cumulative data, we also consider a logistic\nparameterisation of the hazard function. Automatic model selection is performed\nto choose among the different model specifications, based on the statistical\nsignificance of the corresponding estimated parameters and on goodness-of-fit\nassessment. The chosen model is used to produce up-to-today estimates of the\ngrowth rate of the counts. Results are plotted on a map of the country to allow\nfor a visual assessment of the geographic distribution of the areas with\ndifferential prevalence and rates of growth.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 20:38:04 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 14:14:21 GMT"}, {"version": "v3", "created": "Tue, 21 Apr 2020 21:17:37 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Bonetti", "Marco", ""], ["Basellini", "Ugofilippo", ""]]}, {"id": "2003.07952", "submitter": "Raquel Aoki", "authors": "Raquel Aoki and Martin Ester", "title": "ParKCa: Causal Inference with Partially Known Causes", "comments": "12 pages, 4 figures, Pacific Symposium on Biocomputing - 2021 World\n  Scientific Publishing Co., Singapore, http://psb.stanford.edu/", "journal-ref": "Pacific Symposium on Biocomputing - 2021 World Scientific\n  Publishing Co., Singapore, http://psb.stanford.edu/", "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for causal inference from observational data are an alternative for\nscenarios where collecting counterfactual data or realizing a randomized\nexperiment is not possible. Adopting a stacking approach, our proposed method\nParKCA combines the results of several causal inference methods to learn new\ncauses in applications with some known causes and many potential causes. We\nvalidate ParKCA in two Genome-wide association studies, one real-world and one\nsimulated dataset. Our results show that ParKCA can infer more causes than\nexisting methods.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 21:36:56 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 18:08:23 GMT"}, {"version": "v3", "created": "Wed, 29 Jul 2020 01:14:59 GMT"}, {"version": "v4", "created": "Wed, 11 Nov 2020 22:13:54 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Aoki", "Raquel", ""], ["Ester", "Martin", ""]]}, {"id": "2003.07998", "submitter": "Hsien-Wei Chen", "authors": "Hsien-Wei Chen", "title": "Modeling of Multisite Precipitation Occurrences Using Latent\n  Gaussian-based Multivariate Binary Response Time Series", "comments": null, "journal-ref": "Journal of Hydrology 2020, Vol 590, 125069", "doi": "10.1016/j.jhydrol.2020.125069", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new stochastic model for daily precipitation occurrence processes observed\nat multiple locations is developed. The modeling concept is to use the\nindicator function and the elliptical shape of multivariate Gaussian\ndistribution to represent the joint probabilities of daily precipitation\noccurrences. By using this concept, the number of parameters needed for\nprecipitation occurrence modeling can be largely reduced when compared to the\ncommonly used two-state Markov chain approach. With this parameter reduction,\nthe modeling of spatio-temporal dependence of daily precipitation occurrence\nprocesses observed at different locations is no longer difficult. Results of an\nillustrative application using the precipitation record available from a\nnetwork of ten raingauges in the southern Quebec region, also demonstrate the\naccuracy and the feasibility of the proposed model.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 00:51:36 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Chen", "Hsien-Wei", ""]]}, {"id": "2003.08087", "submitter": "Andrew Karl", "authors": "Andrew T. Karl and Dale L. Zimmerman", "title": "A Diagnostic for Bias in Linear Mixed Model Estimators Induced by\n  Dependence Between the Random Effects and the Corresponding Model Matrix", "comments": "26 pages, 4 figures, 2 tables", "journal-ref": "Journal of Statistical Planning and Inference, 211, March 2021,\n  Pages 107-118", "doi": "10.1016/j.jspi.2020.06.004", "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore how violations of the often-overlooked standard assumption that\nthe random effects model matrix in a linear mixed model is fixed (and thus\nindependent of the random effects vector) can lead to bias in estimators of\nestimable functions of the fixed effects. However, if the random effects of the\noriginal mixed model are instead also treated as fixed effects, or if the fixed\nand random effects model matrices are orthogonal with respect to the inverse of\nthe error covariance matrix (with probability one), or if the random effects\nand the corresponding model matrix are independent, then these estimators are\nunbiased. The bias in the general case is quantified and compared to a\nrandomized permutation distribution of the predicted random effects, producing\nan informative summary graphic for each estimator of interest. This is\ndemonstrated through the examination of sporting outcomes used to estimate a\nhome field advantage.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 08:25:02 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 20:01:04 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Karl", "Andrew T.", ""], ["Zimmerman", "Dale L.", ""]]}, {"id": "2003.08270", "submitter": "Andrew R. McCluskey", "authors": "Andrew R. McCluskey", "title": "Neutron reflectometry analysis: using model-dependent methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.ins-det", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Neutron reflectometry analysis is an inherently ill-posed, which is to say\nthat there are many possible solutions which agree equally well with the\nmeasured data. This leads to the application of model-dependent analysis, where\ninformation that we know about the system is integrated into our analysis. This\ntutorial briefly covers the mathematics underlying the use of model-dependent\nanalysis in neutron reflectometry.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 15:22:24 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 08:42:03 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["McCluskey", "Andrew R.", ""]]}, {"id": "2003.08283", "submitter": "Tatiana Savina", "authors": "Tatiana Savina and Ivan Sterligov (National Research University Higher\n  School of Economics, Moscow, Russian Federation)", "title": "Prevalence of Potentially Predatory Publishing in Scopus on the Country\n  Level", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the results of a large-scale study of potentially predatory\njournals (PPJ) represented in the Scopus database, which is widely used for\nresearch evaluation. Both journal metrics and country, disciplinary data have\nbeen evaluated for different groups of PPJ: those listed by Jeffrey Beall and\nthose delisted by Scopus because of \"publication concerns\". Our results show\nthat even after years of delisting, PPJ are still highly visible in the Scopus\ndatabase with hundreds of active potentially predatory journals. PPJ papers are\ncontinuously produced by all major countries, but with different shares. All\nmajor subject areas are affected. The largest number of PPJ papers are in\nengineering and medicine. On average, PPJ have much lower citation metrics than\nother Scopus-indexed journals. We conclude with a brief survey of the case of\nKazakhstan where the share of PPJ papers at one time amounted to almost a half\nof all Kazakhstan papers in Scopus, and propose a link between PPJ share and\nnational research evaluation policies (in particular, rules of awarding\nacademic degrees). The progress of potentially predatory journal research will\nbe increasingly important because such evaluation methods are becoming more\nwidespread in times of the Metric Tide.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 15:44:23 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 07:00:08 GMT"}, {"version": "v3", "created": "Sun, 7 Feb 2021 16:15:49 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Savina", "Tatiana", "", "National Research University Higher\n  School of Economics, Moscow, Russian Federation"], ["Sterligov", "Ivan", "", "National Research University Higher\n  School of Economics, Moscow, Russian Federation"]]}, {"id": "2003.08449", "submitter": "Tyrel Stokes", "authors": "Tyrel Stokes, Russell Steele, Ian Shrier", "title": "Causal Simulation Experiments: Lessons from Bias Amplification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent theoretical work in causal inference has explored an important class\nof variables which, when conditioned on, may further amplify existing\nunmeasured confounding bias (bias amplification). Despite this theoretical\nwork, existing simulations of bias amplification in clinical settings have\nsuggested bias amplification may not be as important in many practical cases as\nsuggested in the theoretical literature.We resolve this tension by using tools\nfrom the semi-parametric regression literature leading to a general\ncharacterization in terms of the geometry of OLS estimators which allows us to\nextend current results to a larger class of DAGs, functional forms, and\ndistributional assumptions. We further use these results to understand the\nlimitations of current simulation approaches and to propose a new framework for\nperforming causal simulation experiments to compare estimators. We then\nevaluate the challenges and benefits of extending this simulation approach to\nthe context of a real clinical data set with a binary treatment, laying the\ngroundwork for a principled approach to sensitivity analysis for bias\namplification in the presence of unmeasured confounding.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 19:33:16 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Stokes", "Tyrel", ""], ["Steele", "Russell", ""], ["Shrier", "Ian", ""]]}, {"id": "2003.08474", "submitter": "Karel Mundnich", "authors": "Karel Mundnich, Brandon M. Booth, Michelle L'Hommedieu, Tiantian Feng,\n  Benjamin Girault, Justin L'Hommedieu, Mackenzie Wildman, Sophia Skaaden,\n  Amrutha Nadarajan, Jennifer L. Villatte, Tiago H. Falk, Kristina Lerman,\n  Emilio Ferrara, and Shrikanth Narayanan", "title": "TILES-2018, a longitudinal physiologic and behavioral data set of\n  hospital workers", "comments": "57 pages, 9 figures, journal paper", "journal-ref": "Sci Data 7, 354 (2020)", "doi": "10.1038/s41597-020-00655-3", "report-no": null, "categories": "eess.SP cs.CY cs.HC stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present a novel longitudinal multimodal corpus of physiological and\nbehavioral data collected from direct clinical providers in a hospital\nworkplace. We designed the study to investigate the use of off-the-shelf\nwearable and environmental sensors to understand individual-specific constructs\nsuch as job performance, interpersonal interaction, and well-being of hospital\nworkers over time in their natural day-to-day job settings. We collected\nbehavioral and physiological data from $n = 212$ participants through\nInternet-of-Things Bluetooth data hubs, wearable sensors (including a\nwristband, a biometrics-tracking garment, a smartphone, and an audio-feature\nrecorder), together with a battery of surveys to assess personality traits,\nbehavioral states, job performance, and well-being over time. Besides the\ndefault use of the data set, we envision several novel research opportunities\nand potential applications, including multi-modal and multi-task behavioral\nmodeling, authentication through biometrics, and privacy-aware and\nprivacy-preserving machine learning.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 21:07:16 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 19:09:17 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Mundnich", "Karel", ""], ["Booth", "Brandon M.", ""], ["L'Hommedieu", "Michelle", ""], ["Feng", "Tiantian", ""], ["Girault", "Benjamin", ""], ["L'Hommedieu", "Justin", ""], ["Wildman", "Mackenzie", ""], ["Skaaden", "Sophia", ""], ["Nadarajan", "Amrutha", ""], ["Villatte", "Jennifer L.", ""], ["Falk", "Tiago H.", ""], ["Lerman", "Kristina", ""], ["Ferrara", "Emilio", ""], ["Narayanan", "Shrikanth", ""]]}, {"id": "2003.08573", "submitter": "Hrushikesh Loya", "authors": "Hrushikesh Loya, Pranav Poduval, Deepak Anand, Neeraj Kumar, and Amit\n  Sethi", "title": "Uncertainty Estimation in Cancer Survival Prediction", "comments": "5 pages, Accepted at AI4AH Workshop at ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Survival models are used in various fields, such as the development of cancer\ntreatment protocols. Although many statistical and machine learning models have\nbeen proposed to achieve accurate survival predictions, little attention has\nbeen paid to obtain well-calibrated uncertainty estimates associated with each\nprediction. The currently popular models are opaque and untrustworthy in that\nthey often express high confidence even on those test cases that are not\nsimilar to the training samples, and even when their predictions are wrong. We\npropose a Bayesian framework for survival models that not only gives more\naccurate survival predictions but also quantifies the survival uncertainty\nbetter. Our approach is a novel combination of variational inference for\nuncertainty estimation, neural multi-task logistic regression for estimating\nnonlinear and time-varying risk models, and an additional sparsity-inducing\nprior to work with high dimensional data.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 05:08:01 GMT"}, {"version": "v2", "created": "Wed, 25 Mar 2020 16:40:03 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Loya", "Hrushikesh", ""], ["Poduval", "Pranav", ""], ["Anand", "Deepak", ""], ["Kumar", "Neeraj", ""], ["Sethi", "Amit", ""]]}, {"id": "2003.08787", "submitter": "Han Lin Shang", "authors": "Han Lin Shang", "title": "A comparison of Hurst exponent estimators in long-range dependent curve\n  time series", "comments": "36 pages, 4 tables", "journal-ref": "Journal of Time Series Econometrics, 2020, 12(1)", "doi": "10.1515/jtse-2019-0009", "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Hurst exponent is the simplest numerical summary of self-similar\nlong-range dependent stochastic processes. We consider the estimation of Hurst\nexponent in long-range dependent curve time series. Our estimation method\nbegins by constructing an estimate of the long-run covariance function, which\nwe use, via dynamic functional principal component analysis, in estimating the\northonormal functions spanning the dominant sub-space of functional time\nseries. Within the context of functional autoregressive fractionally integrated\nmoving average models, we compare finite-sample bias, variance and mean square\nerror among some time- and frequency-domain Hurst exponent estimators and make\nour recommendations.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 21:51:45 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Shang", "Han Lin", ""]]}, {"id": "2003.08858", "submitter": "Frederic Schoenberg", "authors": "Frederic Paik Schoenberg", "title": "Nonparametric estimation of variable productivity Hawkes processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An extension of the Hawkes model where the productivity is variable is\nconsidered. In particular, the case is considered where each point may have its\nown productivity and a simple analytic formula is derived for the maximum\nlikelihood estimators of these productivities. This estimator is compared with\nan empirical estimator and ways are explored of stabilizing both estimators by\nlower truncating, smoothing, and rescaling the estimates. Properties of the\nestimators are explored in simulations, and the methods are applied to\nseismological and epidemic datasets to show and quantify substantial variation\nin productivity.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 15:15:05 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Schoenberg", "Frederic Paik", ""]]}, {"id": "2003.08874", "submitter": "Christopher Ren", "authors": "Christopher X. Ren, Matthew T. Calef, Alice M.S. Durieux, A. Ziemann,\n  J. Theiler", "title": "On the Detectability of Conflict: a Remote Sensing Study of the Rohingya\n  Conflict", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The detection and quantification of conflict through remote sensing\nmodalities represents a challenging but crucial aspect of human rights\nmonitoring. In this work we demonstrate how utilizing multi-modal data sources\ncan help build a comprehensive picture of conflict and human displacement,\nusing the Rohingya conflict in the state of Rakhine, Myanmar as a case study.\nWe show that time series analysis of fire detections from the Moderate\nResolution Imaging Spectroradiometer (MODIS) and Visible Infrared Imaging\nRadiometer Suite (VIIRS) can reveal anomalous spatial and temporal\ndistributions of fires related to conflict. This work also shows that Synthetic\nAperture Radar (SAR) backscatter and coherence data can detect the razing and\nburning of buildings and villages, even in cloudy conditions. These techniques\nmay be further developed in the future to enable the monitoring and detection\nof signals originating from these types of conflict.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 15:54:48 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Ren", "Christopher X.", ""], ["Calef", "Matthew T.", ""], ["Durieux", "Alice M. S.", ""], ["Ziemann", "A.", ""], ["Theiler", "J.", ""]]}, {"id": "2003.08921", "submitter": "Jack Kennedy", "authors": "Jack C. Kennedy, Daniel A. Henderson, Kevin J. Wilson", "title": "Multilevel Emulation for Stochastic Computer Models with Application to\n  Large Offshore Wind farms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large renewable energy projects, such as large offshore wind farms, are\ncritical to achieving low-emission targets set by governments. Stochastic\ncomputer models allow us to explore future scenarios to aid decision making\nwhilst considering the most relevant uncertainties. Complex stochastic computer\nmodels can be prohibitively slow and thus an emulator may be constructed and\ndeployed to allow for efficient computation. We present a novel heteroscedastic\nGaussian Process emulator which exploits cheap approximations to a stochastic\noffshore wind farm simulator. We conduct a probabilistic sensitivity analysis\nto understand the influence of key parameters in the wind farm simulator which\nwill help us to plan a probability elicitation in the future.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 17:36:22 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 08:33:20 GMT"}, {"version": "v3", "created": "Wed, 25 Mar 2020 14:03:36 GMT"}, {"version": "v4", "created": "Tue, 13 Oct 2020 15:08:16 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Kennedy", "Jack C.", ""], ["Henderson", "Daniel A.", ""], ["Wilson", "Kevin J.", ""]]}, {"id": "2003.08965", "submitter": "Katrin Madjar", "authors": "Katrin Madjar and J\\\"org Rahnenf\\\"uhrer", "title": "Weighted Cox regression for the prediction of heterogeneous patient\n  subgroups", "comments": "under review, 15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important task in clinical medicine is the construction of risk prediction\nmodels for specific subgroups of patients based on high-dimensional molecular\nmeasurements such as gene expression data. Major objectives in modeling\nhigh-dimensional data are good prediction performance and feature selection to\nfind a subset of predictors that are truly associated with a clinical outcome\nsuch as a time-to-event endpoint. In clinical practice, this task is\nchallenging since patient cohorts are typically small and can be heterogeneous\nwith regard to their relationship between predictors and outcome. When data of\nseveral subgroups of patients with the same or similar disease are available,\nit is tempting to combine them to increase sample size, such as in multicenter\nstudies. However, heterogeneity between subgroups can lead to biased results\nand subgroup-specific effects may remain undetected. For this situation, we\npropose a penalized Cox regression model with a weighted version of the Cox\npartial likelihood that includes patients of all subgroups but assigns them\nindividual weights based on their subgroup affiliation. Patients who are likely\nto belong to the subgroup of interest obtain higher weights in the\nsubgroup-specific model. Our proposed approach is evaluated through simulations\nand application to real lung cancer cohorts. Simulation results demonstrate\nthat our model can achieve improved prediction and variable selection accuracy\nover standard approaches.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 18:17:54 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Madjar", "Katrin", ""], ["Rahnenf\u00fchrer", "J\u00f6rg", ""]]}, {"id": "2003.08991", "submitter": "Lev B Klebanov", "authors": "Lev Klebanov, Yulia Kuvaeva, Zeev Volkovich", "title": "Statistical Indicators of the Scientific Publications Importance: a\n  Stochastic Model and Critical Look", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A model of scientific citation distribution is given. We apply it to\nunderstand the role of the Hirsch index as an indicator of scientific\npublication importance in Mathematics and some related fields. The proposed\nmodel is based on a generalization of such well-known distributions as\ngeometric and Sibuja laws included now in a family of distributions. Real data\nanalysis of the Hirsch index and corresponding citation numbers is given.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 19:19:36 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Klebanov", "Lev", ""], ["Kuvaeva", "Yulia", ""], ["Volkovich", "Zeev", ""]]}, {"id": "2003.08993", "submitter": "Prajamitra Bhuyan Dr.", "authors": "Prajamitra Bhuyan, Emma J. McCoy, Haojie Li, Daniel J. Graham", "title": "Analysing the causal effect of London cycle superhighways on traffic\n  congestion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transport operators have a range of intervention options available to improve\nor enhance their networks. Such interventions are often made in the absence of\nsound evidence on resulting outcomes. Cycling superhighways were promoted as a\nsustainable and healthy travel mode, one of the aims of which was to reduce\ntraffic congestion. Estimating the impacts that cycle superhighways have on\ncongestion is complicated due to the non-random assignment of such intervention\nover the transport network. In this paper, we analyse the causal effect of\ncycle superhighways utilising pre-intervention and post-intervention\ninformation on traffic and road characteristics along with socio-economic\nfactors. We propose a modeling framework based on the propensity score and\noutcome regression model. The method is also extended to the doubly robust\nset-up. Simulation results show the superiority of the performance of the\nproposed method over existing competitors. The method is applied to analyse a\nreal dataset on the London transport network. The methodology proposed can\nassist in effective decision making to improve network performance.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 19:25:27 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 12:39:34 GMT"}, {"version": "v3", "created": "Tue, 23 Feb 2021 18:59:57 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Bhuyan", "Prajamitra", ""], ["McCoy", "Emma J.", ""], ["Li", "Haojie", ""], ["Graham", "Daniel J.", ""]]}, {"id": "2003.09002", "submitter": "Iqbal H. Sarker", "authors": "Sohrab Hossain, Dhiman Sarma, Rana Joyti Chakma, Wahidul Alam,\n  Mohammed Moshiul Hoque and Iqbal H. Sarker", "title": "A Rule Based Expert System to Assess Coronary Artery Disease under\n  Uncertainty", "comments": "International Conference on Computing Science, Communication and\n  Security (COMS2), Springer, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The coronary artery disease (CAD) involves narrowing and damaging the major\nblood vessels has become the most life threating disease in the world\nespecially in south Asian reason. Although outstanding medical facilities are\navailable in Singapore and India for CAD patients, early detection of CAD\nstages are necessary to minimize the patients' sufferings and expenses. It is\nreally challenging for doctors to incorporate numerous factors for details\nanalysis and CAD detections are expensive as it needs expensive medical\nfacilities. Clinical Decision Support Systems (CDSS) may assist to analyze\nnumerous factors for patients. In this paper, a Rule Based Expert System (RBES)\nis proposed which can predict five different stages of CAD. RBES contains five\ndifferent Belief Rule Based (BRB) systems and the final output is produced by\ncombining all BRBs using the Evidential Reasoning (ER). Success, Error,\nFailure, False Omission rates are calculated to measures the performance of the\nRBES. The Success Rate and False Omission Rate show better performance\ncomparing to existing CDSS.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 15:53:20 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Hossain", "Sohrab", ""], ["Sarma", "Dhiman", ""], ["Chakma", "Rana Joyti", ""], ["Alam", "Wahidul", ""], ["Hoque", "Mohammed Moshiul", ""], ["Sarker", "Iqbal H.", ""]]}, {"id": "2003.09039", "submitter": "Andres Christen", "authors": "J Andr\\'es Christen and Al Parker", "title": "Systematic statistical analysis of microbial data from dilution series", "comments": "31 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In microbial studies, samples are often treated under different experimental\nconditions and then tested for microbial survival. A technique, dating back to\nthe 1880's, consists of diluting the samples several times and incubating each\ndilution to verify the existence of microbial Colony Forming Units or CFU's,\nseen by the naked eye. The main problem in the dilution series data analysis is\nthe uncertainty quantification of the simple point estimate of the original\nnumber of CFU's in the sample (i.e., at dilution zero). Common approaches such\nas log-normal or Poisson models do not seem to handle well extreme cases with\nlow or high counts, among other issues. We build a novel binomial model, based\non the actual design of the experimental procedure including the dilution\nseries. For repetitions we construct a hierarchical model for experimental\nresults from a single lab and in turn a higher hierarchy for inter-lab\nanalyses. Results seem promising, with a systematic treatment of all data\ncases, including zeros, censored data, repetitions, intra and inter-laboratory\nstudies. Using a Bayesian approach, a robust and efficient MCMC method is used\nto analyze several real data sets.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 22:49:08 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Christen", "J Andr\u00e9s", ""], ["Parker", "Al", ""]]}, {"id": "2003.09202", "submitter": "David Mori\\~na Prof.", "authors": "David Mori\\~na, Amanda Fern\\'andez-Fontelo, Alejandra Caba\\~na, Pedro\n  Puig", "title": "New statistical model for misreported data with application to current\n  public health challenges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The main goal of this work is to present a new model able to deal with\npotentially misreported continuous time series. The proposed model is able to\nhandle the autocorrelation structure in continuous time series data, which\nmight be partially or totally underreported or overreported. Its performance is\nillustrated through a comprehensive simulation study considering several\nautocorrelation structures and two real data applications on human\npapillomavirus incidence in Girona (Catalunya, Spain) and COVID-19 incidence in\nthe Chinese region of Heilongjiang.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 11:32:06 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 08:27:35 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Mori\u00f1a", "David", ""], ["Fern\u00e1ndez-Fontelo", "Amanda", ""], ["Caba\u00f1a", "Alejandra", ""], ["Puig", "Pedro", ""]]}, {"id": "2003.09213", "submitter": "David Mori\\~na Prof.", "authors": "David Mori\\~na, Amanda Fern\\'andez-Fontelo, Alejandra Caba\\~na, Pedro\n  Puig, Laura Monfil, Maria Brotons, Mireia Diaz", "title": "Quantifying the under-reporting of genital warts cases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genital warts are a common and highly contagious sexually transmitted\ndisease. They have a large economic burden and affect several aspects of\nquality of life. Incidence data underestimate the real occurrence of genital\nwarts because this infection is often under-reported, mostly due to their\nspecific characteristics such as the asymptomatic course. Genital warts cases\nfor the analysis were obtained from the catalan public health system database\n(SIDIAP) for the period 2009-2016, covering 74\\% of the Catalan population.\nPeople under 15 and over 94 years old were excluded from the analysis as the\nincidence of genital warts in this population is negligible. This work\nintroduces a time series model based on a mixture of two distributions, capable\nof detecting the presence of under-reporting in the data. In order to identify\npotential differences in the magnitude of the under-reporting issue depending\non sex and age, these covariates were included in the model. This work shows\nthat only about 80\\% in average of genital warts incidence in Catalunya in the\nperiod 2009-2016 was registered, although the frequency of under-reporting has\nbeen decreasing over the study period. It can also be seen that the\nunder-reported issue has a deeper impact on women over 30 years old. The\nregistered incidence in the Catalan public health system is underestimating the\nreal burden in almost 10,000 cases in Catalunya, around 23\\% of the registered\ncases. The total annual cost in Catalunya is underestimated in at least about\n10 million Euros respect the 54 million Euros annually devoted to genital warts\nin Catalunya, representing 0.4\\% of the total budget of the public health\nsystem.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 11:55:30 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Mori\u00f1a", "David", ""], ["Fern\u00e1ndez-Fontelo", "Amanda", ""], ["Caba\u00f1a", "Alejandra", ""], ["Puig", "Pedro", ""], ["Monfil", "Laura", ""], ["Brotons", "Maria", ""], ["Diaz", "Mireia", ""]]}, {"id": "2003.09384", "submitter": "Anthony Constantinou", "authors": "Anthony Constantinou", "title": "Asian Handicap football betting with Rating-based Hybrid Bayesian\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the massive popularity of the Asian Handicap (AH) football betting\nmarket, it has not been adequately studied by the relevant literature. This\npaper combines rating systems with hybrid Bayesian networks and presents the\nfirst published model specifically developed for prediction and assessment of\nthe AH betting market. The results are based on 13 English Premier League\nseasons and are compared to the traditional 1X2 market. Different betting\nsituations have been examined including a) both average and maximum (best\navailable) market odds, b) all possible betting decision thresholds between\npredicted and published odds, c) optimisations for both return-on-investment\nand profit, and d) simple stake adjustments to investigate how the variance of\nreturns changes when targeting equivalent profit in both 1X2 and AH markets.\nWhile the AH market is found to share the inefficiencies of the traditional 1X2\nmarket, the findings reveal both interesting differences as well as\nsimilarities between the two.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 09:50:07 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Constantinou", "Anthony", ""]]}, {"id": "2003.09460", "submitter": "Denise Rava", "authors": "Denise Rava and Ronghui Xu", "title": "Explained Variation under the Additive Hazards Model", "comments": "20 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study explained variation under the additive hazards regression model for\nright-censored data. We consider different approaches for developing such a\nmeasure, and focus on one that estimates the proportion of variation in the\nfailure time explained by the covariates. We study the properties of the\nmeasure both analytically, and through extensive simulations. We apply the\nmeasure to a well-known survival data set as well as the linked Surveillance,\nEpidemiology and End Results (SEER)-Medicare database for prediction of\nmortality in early-stage prostate cancer patients using high dimensional claims\ncodes.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 18:56:13 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2020 22:43:42 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Rava", "Denise", ""], ["Xu", "Ronghui", ""]]}, {"id": "2003.09944", "submitter": "Rudolf Hanel Ass Prof Dr", "authors": "Rudolf Hanel and Stefan Thurner", "title": "Boosting test-efficiency by pooled testing strategies for SARS-CoV-2", "comments": "2 figures; figure one with 2, figure 2 with 3 panes", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the current COVID19 crisis many national healthcare systems are confronted\nwith an acute shortage of tests for confirming SARS-CoV-2 infections. For low\noverall infection levels in the population, pooling of samples can drastically\namplify the testing efficiency. Here we present a formula to estimate the\noptimal pooling size, the efficiency gain (tested persons per test), and the\nexpected upper bound of missed infections in the pooled testing, all as a\nfunction of the populationwide infection levels and the false negative/positive\nrates of the currently used PCR tests. Assuming an infection level of 0.1 % and\na false negative rate of 2 %, the optimal pool size is about 32, the efficiency\ngain is about 15 tested persons per test. For an infection level of 1 % the\noptimal pool size is 11, the efficiency gain is 5.1 tested persons per test.\nFor an infection level of 10 % the optimal pool size reduces to about 4, the\nefficiency gain is about 1.7 tested persons per test. For infection levels of\n30 % and higher there is no more benefit from pooling. To see to what extent\nreplicates of the pooled tests improve the estimate of the maximal number of\nmissed infections, we present all results for 1, 3, and 5 replicates.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 16:49:15 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Hanel", "Rudolf", ""], ["Thurner", "Stefan", ""]]}, {"id": "2003.09957", "submitter": "Evgeny Burnaev", "authors": "Dmitrii Smolyakov and Evgeny Burnaev", "title": "Software System for Road Condition Forecast Correction", "comments": "11 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SP eess.SY stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a monitoring system that allows increasing road\nsafety by predicting ice formation. The system consists of a network of road\nweather stations and intelligence data processing program module. The results\nwere achieved by combining physical models for forecasting road conditions\nbased on measurements from stations and machine learning models for detecting\nincorrect data and forecast correction.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 17:47:02 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Smolyakov", "Dmitrii", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "2003.09967", "submitter": "Pedro Hespanhol", "authors": "Pedro Hespanhol, Anil Aswani", "title": "Hypothesis Testing Approach to Detecting Collusion in Competitive\n  Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is growing concern about tacit collusion using algorithmic pricing, and\nregulators need tools to help detect the possibility of such collusion. This\npaper studies how to design a hypothesis testing framework in order to decide\nwhether agents are behaving competitively or not. In our setting, agents are\nutility-maximizing and compete over prices of items. A regulator, with no\nknowledge of the agent's utility function, has access only to the agents'\nstrategies (i.e., pricing decisions) and external shock values in order to\ndecide if agents are behaving in competition according to some equilibrium\nproblem. We leverage the formulation of such a problem as an inverse\nvariational inequality and design a hypothesis test under a minimal set of\nassumptions. We demonstrate our method with computational experiments of the\nBertrand competition game (with and without collusion) and show how our method\nperforms.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 18:49:47 GMT"}, {"version": "v2", "created": "Sat, 28 Mar 2020 18:51:32 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Hespanhol", "Pedro", ""], ["Aswani", "Anil", ""]]}, {"id": "2003.09983", "submitter": "Marcelo Ruas", "authors": "Marcelo Ruas and Alexandre Street and Cristiano Fernandes", "title": "A Multi-Quantile Regression Time Series Model with Interquantile\n  Lipschitz Regularization for Wind Power Probabilistic Forecasting", "comments": "17 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern decision-making processes require uncertainty-aware models, especially\nthose relying on non-symmetric costs and risk-averse profiles. The objective of\nthis work is to propose a dynamic model for the conditional non-parametric\ndistribution function (CDF) to generate probabilistic forecasts for a renewable\ngeneration time series. To do that, we propose an adaptive non-parametric\ntime-series model driven by a regularized multiple-quantile-regression (MQR)\nframework. In our approach, all regression models are jointly estimated through\na single linear optimization problem that finds the global-optimal parameters\nin polynomial time. An innovative feature of our work is the consideration of a\nLipschitz regularization of the first derivative of coefficients in the\nquantile space, which imposes coefficient smoothness. The proposed\nregularization induces a coupling effect among quantiles creating a single\nnon-parametric CDF model with improved out-of-sample performance. A case study\nwith realistic wind-power generation data from the Brazilian system shows: 1)\nthe regularization model is capable to improve the performance of MQR\nprobabilistic forecasts, and 2) our MQR model outperforms five relevant\nbenchmarks: two based on the MQR framework, and three based on parametric\nmodels, namely, SARIMA, and GAS with Beta and Weibull CDF.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 20:13:17 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 15:36:10 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Ruas", "Marcelo", ""], ["Street", "Alexandre", ""], ["Fernandes", "Cristiano", ""]]}, {"id": "2003.10268", "submitter": "Dominika Mik\\v{s}ov\\'a", "authors": "Dominika Mik\\v{s}ov\\'a, Christopher Rieser, Peter Filzmoser, Simon M.\n  Thaarup, Jeremie Melleton", "title": "A method to identify geochemical mineralization on linear transect", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mineral exploration in biogeochemistry is related to the detection of\nanomalies in soil, which is driven by many factors and thus a complex problem.\nMik\\v{s}ov\\'a, Rieser, and Filzmoser (2019) have introduced a method for the\nidentification of spatial patterns with increased element concentrations in\nsamples along a linear sampling transect. This procedure is based on fitting\nGeneralized Additive Models (GAMs) to the concentration data, and computing a\ncurvature measure from the pairwise log-ratios of these fits. The higher the\ncurvature, the more likely one or both elements of the pair indicate local\nmineralization. This method is applied on two geochemical data sets which have\nbeen collected specifically for the purpose of mineral exploration. The aim is\nto test the technique for its ability to identify pathfinder elements to detect\nmineralized zones, and to verify whether the method can indicate which sampling\nmaterial is best suited for this purpose.\n  Reference: Mik\\v{s}ov\\'a D., Rieser C., Filzmoser P. (2019). \"Identification\nof mineralization in geochemistry along a transect based on the spatial\ncurvature of log-ratios.\" arXiv, (1912.02867).\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 13:24:33 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Mik\u0161ov\u00e1", "Dominika", ""], ["Rieser", "Christopher", ""], ["Filzmoser", "Peter", ""], ["Thaarup", "Simon M.", ""], ["Melleton", "Jeremie", ""]]}, {"id": "2003.10287", "submitter": "Ana Fern\\'andez del R\\'io", "authors": "Ana Fern\\'andez del R\\'io, Anna Guitart and \\'Africa Peri\\'a\\~nez", "title": "A Time Series Approach To Player Churn and Conversion in Videogames", "comments": "Accepted for publication in IOS Press Intelligent Data Analysis", "journal-ref": "Intelligent Data Analysis, vol. 25, no. 1, pp. 177-203, 2021", "doi": "10.3233/IDA-194940", "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Players of a free-to-play game are divided into three main groups: non-paying\nactive users, paying active users and inactive users. A State Space time series\napproach is then used to model the daily conversion rates between the different\ngroups, i.e., the probability of transitioning from one group to another. This\nallows, not only for predictions on how these rates are to evolve, but also for\na deeper understanding of the impact that in-game planning and calendar effects\nhave. It is also used in this work for the detection of marketing and promotion\ncampaigns about which no information is available. In particular, two different\nState Space formulations are considered and compared: an Autoregressive\nIntegrated Moving Average process and an Unobserved Components approach, in\nboth cases with a linear regression to explanatory variables. Both yield very\nclose estimations for covariate parameters, producing forecasts with similar\nperformances for most transition rates. While the Unobserved Components\napproach is more robust and needs less human intervention in regards to model\ndefinition, it produces significantly worse forecasts for non-paying user\nabandonment probability. More critically, it also fails to detect a plausible\nmarketing and promotion campaign scenario.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 20:16:52 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["del R\u00edo", "Ana Fern\u00e1ndez", ""], ["Guitart", "Anna", ""], ["Peri\u00e1\u00f1ez", "\u00c1frica", ""]]}, {"id": "2003.10303", "submitter": "David Conal Higgins", "authors": "David Higgins and Vince I. Madai", "title": "From Bit To Bedside: A Practical Framework For Artificial Intelligence\n  Product Development In Healthcare", "comments": "30 pages, 4 figures", "journal-ref": "Advanced Intelligent Systems, 2020, 2000052", "doi": "10.1002/aisy.202000052", "report-no": null, "categories": "cs.CY cs.AI cs.HC stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Artificial Intelligence (AI) in healthcare holds great potential to expand\naccess to high-quality medical care, whilst reducing overall systemic costs.\nDespite hitting the headlines regularly and many publications of\nproofs-of-concept, certified products are failing to breakthrough to the\nclinic. AI in healthcare is a multi-party process with deep knowledge required\nin multiple individual domains. The lack of understanding of the specific\nchallenges in the domain is, therefore, the major contributor to the failure to\ndeliver on the big promises. Thus, we present a decision perspective framework,\nfor the development of AI-driven biomedical products, from conception to market\nlaunch. Our framework highlights the risks, objectives and key results which\nare typically required to proceed through a three-phase process to the market\nlaunch of a validated medical AI product. We focus on issues related to\nClinical validation, Regulatory affairs, Data strategy and Algorithmic\ndevelopment. The development process we propose for AI in healthcare software\nstrongly diverges from modern consumer software development processes. We\nhighlight the key time points to guide founders, investors and key stakeholders\nthroughout their relevant part of the process. Our framework should be seen as\na template for innovation frameworks, which can be used to coordinate team\ncommunications and responsibilities towards a reasonable product development\nroadmap, thus unlocking the potential of AI in medicine.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 14:42:18 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Higgins", "David", ""], ["Madai", "Vince I.", ""]]}, {"id": "2003.10336", "submitter": "Paul Bastide", "authors": "Paul Bastide and Lam Si Tung Ho and Guy Baele and Philippe Lemey and\n  Marc A Suchard", "title": "Efficient Bayesian Inference of General Gaussian Models on Large\n  Phylogenetic Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phylogenetic comparative methods correct for shared evolutionary history\namong a set of non-independent organisms by modeling sample traits as arising\nfrom a diffusion process along on the branches of a possibly unknown history.\nTo incorporate such uncertainty, we present a scalable Bayesian inference\nframework under a general Gaussian trait evolution model that exploits\nHamiltonian Monte Carlo (HMC). HMC enables efficient sampling of the\nconstrained model parameters and takes advantage of the tree structure for fast\nlikelihood and gradient computations, yielding algorithmic complexity linear in\nthe number of observations. This approach encompasses a wide family of\nstochastic processes, including the general Ornstein-Uhlenbeck (OU) process,\nwith possible missing data and measurement errors. We implement inference tools\nfor a biologically relevant subset of all these models into the BEAST\nphylogenetic software package and develop model comparison through marginal\nlikelihood estimation. We apply our approach to study the morphological\nevolution in the superfamilly of Musteloidea (including weasels and allies) as\nwell as the heritability of HIV virulence. This second problem furnishes a new\nmeasure of evolutionary heritability that demonstrates its utility through a\ntargeted simulation study.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 15:44:29 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 09:22:46 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Bastide", "Paul", ""], ["Ho", "Lam Si Tung", ""], ["Baele", "Guy", ""], ["Lemey", "Philippe", ""], ["Suchard", "Marc A", ""]]}, {"id": "2003.10442", "submitter": "Shannon Gallagher", "authors": "Shannon Gallagher, Andersen Chang, William F. Eddy", "title": "Exploring the nuances of R0: Eight estimates and application to 2009\n  pandemic influenza", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For nearly a century, the initial reproduction number (R0) has been used as a\none number summary to compare outbreaks of infectious disease, yet there is no\n`standard' estimator for R0. Difficulties in estimating R0 arise both from how\na disease transmits through a population as well as from differences in\nstatistical estimation method. We describe eight methods used to estimate R0\nand provide a thorough simulation study of how these estimates change in the\npresence of different disease parameters. As motivation, we analyze the 2009\noutbreak of the H1N1 pandemic influenza in the USA and compare the results from\nour eight methods to a previous study. We discuss the most important aspects\nfrom our results which effect the estimation of R0, which include the\npopulation size, time period used, and the initial percent of infectious\nindividuals. Additionally, we discuss how pre-processing incidence counts may\neffect estimates of R0. Finally, we provide guidelines for estimating point\nestimates and confidence intervals to create reliable, comparable estimates of\nR0.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 16:09:08 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Gallagher", "Shannon", ""], ["Chang", "Andersen", ""], ["Eddy", "William F.", ""]]}, {"id": "2003.10484", "submitter": "Haim Bar", "authors": "Haim Bar and Kangyan Liu", "title": "Large-P Variable Selection in Two-Stage Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model selection in the large-P small-N scenario is discussed in the framework\nof two-stage models. Two specific models are considered, namely, two-stage\nleast squares (TSLS) involving instrumental variables (IVs), and mediation\nmodels. In both cases, the number of putative variables (e.g. instruments or\nmediators) is large, but only a small subset should be included in the\ntwo-stage model. We use two variable selection methods which are designed for\nhigh-dimensional settings, and compare their performance in terms of their\nability to find the true IVs or mediators. Our approach is demonstrated via\nsimulations and case studies.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 18:29:29 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Bar", "Haim", ""], ["Liu", "Kangyan", ""]]}, {"id": "2003.10525", "submitter": "Costanza Tort\\`u", "authors": "C. Tort\\`u, I. Crimaldi, F. Mealli, L. Forastiere", "title": "Modelling Network Interference with Multi-valued Treatments: the Causal\n  Effect of Immigration Policy on Crime Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policy evaluation studies, which intend to assess the effect of an\nintervention, face some statistical challenges: in real-world settings\ntreatments are not randomly assigned and the analysis might be further\ncomplicated by the presence of interference between units. Researchers have\nstarted to develop novel methods that allow to manage spillover mechanisms in\nobservational studies; recent works focus primarily on binary treatments.\nHowever, many policy evaluation studies deal with more complex interventions.\nFor instance, in political science, evaluating the impact of policies\nimplemented by administrative entities often implies a multivariate approach,\nas a policy towards a specific issue operates at many different levels and can\nbe defined along a number of dimensions. In this work, we extend the\nstatistical framework about causal inference under network interference in\nobservational studies, allowing for a multi-valued individual treatment and an\ninterference structure shaped by a weighted network. The estimation strategy is\nbased on a joint multiple generalized propensity score and allows one to\nestimate direct effects, controlling for both individual and network\ncovariates. We follow the proposed methodology to analyze the impact of the\nnational immigration policy on the crime rate. We define a multi-valued\ncharacterization of political attitudes towards migrants and we assume that the\nextent to which each country can be influenced by another country is modeled by\nan appropriate indicator, summarizing their cultural and geographical\nproximity. Results suggest that implementing a highly restrictive immigration\npolicy leads to an increase of the crime rate and the estimated effects is\nlarger if we take into account interference from other countries.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 11:17:00 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 08:42:31 GMT"}, {"version": "v3", "created": "Tue, 23 Jun 2020 19:00:01 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Tort\u00f9", "C.", ""], ["Crimaldi", "I.", ""], ["Mealli", "F.", ""], ["Forastiere", "L.", ""]]}, {"id": "2003.10528", "submitter": "Yue Wei", "authors": "Yue Wei, Jason C. Hsu, Wei Chen, Emily Y. Chew, Ying Ding", "title": "A Simultaneous Inference Procedure to Identify Subgroups from RCTs with\n  Survival Outcomes: Application to Analysis of AMD Progression Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the uptake of targeted therapies, instead of the \"one-fits-all\"\napproach, modern randomized clinical trials (RCTs) often aim to develop\ntreatments that target a subgroup of patients. Motivated by analyzing the\nAge-Related Eye Disease Study (AREDS) data, a large RCT to study the efficacy\nof nutritional supplements in delaying the progression of an eye disease,\nage-related macular degeneration (AMD), we develop a simultaneous inference\nprocedure to identify and infer subgroups with differential treatment efficacy\nin RCTs with survival outcome. Specifically, we formulate the multiple testing\nproblem through contrasts and construct their simultaneous confidence\nintervals, which control both within- and across- marker multiplicity\nappropriately. Realistic simulations are conducted using real genotype data to\nevaluate the method performance under various scenarios. The method is then\napplied to AREDS to assess the efficacy of antioxidants and zinc combination in\ndelaying AMD progression. Multiple gene regions including ESRRB-VASH1 on\nchromosome 14 have been identified with subgroups showing differential\nefficacy. We further validate our findings in an independent subsequent RCT,\nAREDS2, by discovering consistent differential treatment responses in the\ntargeted and non-targeted subgroups been identified from AREDS. This\nsimultaneous inference approach provides a step forward to confidently identify\nand infer subgroups in modern drug development.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 20:16:42 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Wei", "Yue", ""], ["Hsu", "Jason C.", ""], ["Chen", "Wei", ""], ["Chew", "Emily Y.", ""], ["Ding", "Ying", ""]]}, {"id": "2003.10548", "submitter": "Renato Panaro Sr.", "authors": "Renato Valladares Panaro", "title": "spsurv: An R package for semi-parametric survival analysis", "comments": "140 pages, 25 figures, 21 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software development innovations and advances in computing have enabled more\ncomplex and less costly computations in medical research (survival analysis),\nengineering studies (reliability analysis), and social sciences event analysis\n(historical analysis). As a result, many semi-parametric modeling efforts\nemerged when it comes to time-to-event data analysis. In this context, this\nwork presents a flexible Bernstein polynomial (BP) based framework for survival\ndata modeling. This innovative approach is applied to existing families of\nmodels such as proportional hazards (PH), proportional odds (PO), and\naccelerated failure time (AFT) models to estimate unknown baseline functions.\nAlong with this contribution, this work also presents new automated routines in\nR, taking advantage of algorithms available in Stan. The proposed computation\nroutines are tested and explored through simulation studies based on artificial\ndatasets. The tools implemented to fit the proposed statistical models are\ncombined and organized in an R package. Also, the BP based proportional hazards\n(BPPH), proportional odds (BPPO), and accelerated failure time (BPAFT) models\nare illustrated in real applications related to cancer trial data using maximum\nlikelihood (ML) estimation and Markov chain Monte Carlo (MCMC) methods.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 21:04:43 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Panaro", "Renato Valladares", ""]]}, {"id": "2003.10643", "submitter": "Yoichi Matsuo", "authors": "Yoichi Matsuo, Tatsuaki Kimura and Ken Nishimatsu", "title": "DeepSIP: A System for Predicting Service Impact of Network Failure by\n  Temporal Multimodal CNN", "comments": "to appear in IEEE/IFIP International Workshop on Analytics for\n  Network and Service Management (AnNet 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.SI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a failure occurs in a network, network operators need to recognize\nservice impact, since service impact is essential information for handling\nfailures. In this paper, we propose Deep learning based Service Impact\nPrediction (DeepSIP), a system to predict the time to recovery from the failure\nand the loss of traffic volume due to the failure in a network element using a\ntemporal multimodal convolutional neural network (CNN). Since the time to\nrecovery is useful information for a service level agreement (SLA) and the loss\nof traffic volume is directly related to the severity of the failures, we\nregard these as the service impact. The service impact is challenging to\npredict, since a network element does not explicitly contain any information\nabout the service impact. Thus, we aim to predict the service impact from\nsyslog messages and traffic volume by extracting hidden information about\nfailures. To extract useful features for prediction from syslog messages and\ntraffic volume which are multimodal and strongly correlated, and have temporal\ndependencies, we use temporal multimodal CNN. We experimentally evaluated\nDeepSIP and DeepSIP reduced prediction error by approximately 50% in comparison\nwith other NN-based methods with a synthetic dataset.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 03:47:54 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Matsuo", "Yoichi", ""], ["Kimura", "Tatsuaki", ""], ["Nishimatsu", "Ken", ""]]}, {"id": "2003.10655", "submitter": "Soudeep Deb", "authors": "Soudeep Deb and Manidipa Majumdar", "title": "A time series method to analyze incidence pattern and estimate\n  reproduction number of COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ongoing pandemic of Coronavirus disease (COVID-19) emerged in Wuhan,\nChina in the end of 2019. It has already affected more than 300,000 people,\nwith the number of deaths nearing 13000 across the world. As it has been posing\na huge threat to global public health, it is of utmost importance to identify\nthe rate at which the disease is spreading. In this study, we propose a time\nseries model to analyze the trend pattern of the incidence of COVID-19\noutbreak. We also incorporate information on total or partial lockdown,\nwherever available, into the model. The model is concise in structure, and\nusing appropriate diagnostic measures, we showed that a time-dependent\nquadratic trend successfully captures the incidence pattern of the disease. We\nalso estimate the basic reproduction number across different countries, and\nfind that it is consistent except for the United States of America. The above\nstatistical analysis is able to shed light on understanding the trends of the\noutbreak, and gives insight on what epidemiological stage a region is in. This\nhas the potential to help in prompting policies to address COVID-19 pandemic in\ndifferent countries.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 04:42:43 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Deb", "Soudeep", ""], ["Majumdar", "Manidipa", ""]]}, {"id": "2003.10720", "submitter": "Lionel Roques", "authors": "Lionel Roques (BioSP), Etienne Klein (BioSP), Julien Papax (BioSP),\n  Antoine Sar, Samuel Soubeyrand (BioSP)", "title": "Using early data to estimate the actual infection fatality ratio from\n  COVID-19 in France (Running title: Infection fatality ratio from COVID-19)", "comments": null, "journal-ref": "MDPI Biology 2020, 9(5), 97", "doi": "10.3390/biology9050097", "report-no": null, "categories": "q-bio.PE math.DS stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The first cases of COVID-19 in France were detected on January 24, 2020. The\nnumber of screening tests carried out and the methodology used to target the\npatients tested do not allow for a direct computation of the real number of\ncases and the mortality rate.In this report, we develop a\n'mechanistic-statistical' approach coupling a SIR ODE model describing the\nunobserved epidemiological dynamics, a probabilistic model describing the data\nacquisition process and a statistical inference method. The objective of this\nmodel is not to make forecasts but to estimate the real number of people\ninfected with COVID-19 during the observation window in France and to deduce\nthe mortality rate associated with the epidemic.Main results. The actual number\nof infected cases in France is probably much higher than the observations: we\nfind here a factor x 15 (95%-CI: 4-33), which leads to a 5.2/1000 mortality\nrate (95%-CI: 1.5 / 1000-11.7/ 1000) at the end of the observation period. We\nfind a R0 of 4.8, a high value which may be linked to the long viral shedding\nperiod of 20 days.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 08:58:46 GMT"}, {"version": "v2", "created": "Wed, 25 Mar 2020 09:52:00 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2020 13:34:01 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Roques", "Lionel", "", "BioSP"], ["Klein", "Etienne", "", "BioSP"], ["Papax", "Julien", "", "BioSP"], ["Sar", "Antoine", "", "BioSP"], ["Soubeyrand", "Samuel", "", "BioSP"]]}, {"id": "2003.10726", "submitter": "Yue Su", "authors": "Yue Su and Patrick Kandege Mwanakatwe", "title": "Model selection criteria of the standard censored regression model based\n  on the bootstrap sample augmentation mechanism", "comments": "21 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical regression technique is an extraordinarily essential data\nfitting tool to explore the potential possible generation mechanism of the\nrandom phenomenon. Therefore, the model selection or the variable selection is\nbecoming extremely important so as to identify the most appropriate model with\nthe most optimal explanation effect on the interesting response. In this paper,\nwe discuss and compare the bootstrap-based model selection criteria on the\nstandard censored regression model (Tobit regression model) under the\ncircumstance of limited observation information. The Monte Carlo numerical\nevidence demonstrates that the performances of the model selection criteria\nbased on the bootstrap sample augmentation strategy will become more\ncompetitive than their alternative ones, such as the Akaike Information\nCriterion (AIC) and the Bayesian Information Criterion (BIC) etc. under the\ncircumstance of the inadequate observation information. Meanwhile, the\nnumerical simulation experiments further demonstrate that the model\nidentification risk due to the deficiency of the data information, such as the\nhigh censoring rate and rather limited number of observations, can be\nadequately compensated by increasing the scientific computation cost in terms\nof the bootstrap sample augmentation strategies. We also apply the recommended\nbootstrap-based model selection criterion on the Tobit regression model to fit\nthe real fidelity dataset.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 09:19:27 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Su", "Yue", ""], ["Mwanakatwe", "Patrick Kandege", ""]]}, {"id": "2003.10776", "submitter": "Nilanjan Dey", "authors": "Simon James Fong, Gloria Li, Nilanjan Dey, Rub\\'en Gonz\\'alez Crespo,\n  Enrique Herrera-Viedma", "title": "Finding an Accurate Early Forecasting Model from Small Dataset: A Case\n  of 2019-nCoV Novel Coronavirus Outbreak", "comments": "9 pages", "journal-ref": "International Journal of Interactive Multimedia and Artificial\n  Intelligence 6.1 (2020): 132-40", "doi": "10.9781/ijimai.2020.02.002", "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Epidemic is a rapid and wide spread of infectious disease threatening many\nlives and economy damages. It is important to fore-tell the epidemic lifetime\nso to decide on timely and remedic actions. These measures include closing\nborders, schools, suspending community services and commuters. Resuming such\ncurfews depends on the momentum of the outbreak and its rate of decay. Being\nable to accurately forecast the fate of an epidemic is an extremely important\nbut difficult task. Due to limited knowledge of the novel disease, the high\nuncertainty involved and the complex societal-political factors that influence\nthe widespread of the new virus, any forecast is anything but reliable. Another\nfactor is the insufficient amount of available data. Data samples are often\nscarce when an epidemic just started. With only few training samples on hand,\nfinding a forecasting model which offers forecast at the best efforts is a big\nchallenge in machine learning. In the past, three popular methods have been\nproposed, they include 1) augmenting the existing little data, 2) using a panel\nselection to pick the best forecasting model from several models, and 3)\nfine-tuning the parameters of an individual forecastingmodel for the highest\npossible accuracy. In this paper, a methodology that embraces these three\nvirtues of data mining from a small dataset is proposed...\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 11:23:26 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Fong", "Simon James", ""], ["Li", "Gloria", ""], ["Dey", "Nilanjan", ""], ["Crespo", "Rub\u00e9n Gonz\u00e1lez", ""], ["Herrera-Viedma", "Enrique", ""]]}, {"id": "2003.10783", "submitter": "Kengo Tajiri", "authors": "Kengo Tajiri and Yasuhiro Ikeda and Yuusuke Nakano and Keishiro\n  Watanabe", "title": "Dividing Deep Learning Model for Continuous Anomaly Detection of\n  Inconsistent ICT Systems", "comments": "Accepted for IEEE/IFIP Network Operations and Management Symposium\n  2020 (NOMS2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Health monitoring is important for maintaining reliable information and\ncommunications technology (ICT) systems. Anomaly detection methods based on\nmachine learning, which train a model for describing \"normality\" are promising\nfor monitoring the state of ICT systems. However, these methods cannot be used\nwhen the type of monitored log data changes from that of training data due to\nthe replacement of certain equipment. Therefore, such methods may dismiss an\nanomaly that appears when log data changes. To solve this problem, we propose\nan ICT-systems-monitoring method with deep learning models divided based on the\ncorrelation of log data. We also propose an algorithm for extracting the\ncorrelations of log data from a deep learning model and separating log data\nbased on the correlation. When some of the log data changes, our method can\ncontinue health monitoring with the divided models which are not affected by\nchanges in the log data. We present the results from experiments involving\nbenchmark data and real log data, which indicate that our method using divided\nmodels does not decrease anomaly detection accuracy and a model for anomaly\ndetection can be divided to continue monitoring a network state even if some\nthe log data change.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 11:32:00 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Tajiri", "Kengo", ""], ["Ikeda", "Yasuhiro", ""], ["Nakano", "Yuusuke", ""], ["Watanabe", "Keishiro", ""]]}, {"id": "2003.10784", "submitter": "Hiroki Ikeuchi", "authors": "Hiroki Ikeuchi, Akio Watanabe, Tsutomu Hirao, Makoto Morishita,\n  Masaaki Nishino, Yoichi Matsuo, Keishiro Watanabe", "title": "Recovery command generation towards automatic recovery in ICT systems by\n  Seq2Seq learning", "comments": "accepted for IEEE/IFIP Network Operations and Management Symposium\n  2020 (NOMS2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increase in scale and complexity of ICT systems, their operation\nincreasingly requires automatic recovery from failures. Although it has become\npossible to automatically detect anomalies and analyze root causes of failures\nwith current methods, making decisions on what commands should be executed to\nrecover from failures still depends on manual operation, which is quite\ntime-consuming. Toward automatic recovery, we propose a method of estimating\nrecovery commands by using Seq2Seq, a neural network model. This model learns\ncomplex relationships between logs obtained from equipment and recovery\ncommands that operators executed in the past. When a new failure occurs, our\nmethod estimates plausible commands that recover from the failure on the basis\nof collected logs. We conducted experiments using a synthetic dataset and\nrealistic OpenStack dataset, demonstrating that our method can estimate\nrecovery commands with high accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 11:34:10 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Ikeuchi", "Hiroki", ""], ["Watanabe", "Akio", ""], ["Hirao", "Tsutomu", ""], ["Morishita", "Makoto", ""], ["Nishino", "Masaaki", ""], ["Matsuo", "Yoichi", ""], ["Watanabe", "Keishiro", ""]]}, {"id": "2003.10791", "submitter": "Marius \\\"Otting", "authors": "Marius \\\"Otting", "title": "Predicting play calls in the National Football League using hidden\n  Markov models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, data-driven approaches have become a popular tool in a\nvariety of sports to gain an advantage by, e.g., analysing potential strategies\nof opponents. Whereas the availability of play-by-play or player tracking data\nin sports such as basketball and baseball has led to an increase of sports\nanalytics studies, equivalent datasets for the National Football League (NFL)\nwere not freely available for a long time. In this contribution, we consider a\ncomprehensive play-by-play NFL dataset provided by www.kaggle.com, comprising\n289,191 observations in total, to predict play calls in the NFL using hidden\nMarkov models. The resulting out-of-sample prediction accuracy for the 2018 NFL\nseason is 71.5%, which is substantially higher compared to similar studies on\nplay call predictions in the NFL.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 11:48:30 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["\u00d6tting", "Marius", ""]]}, {"id": "2003.10868", "submitter": "Bo Tranberg", "authors": "Neeraj Bokde, Bo Tranberg, Gorm Bruun Andresen", "title": "Short-term CO2 emissions forecasting based on decomposition approaches\n  and its impact on electricity market scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The world is facing major challenges related to global warming and emissions\nof greenhouse gases is a major causing factor. In 2017, energy industries\naccounted for 46% of all CO2 emissions globally, which shows a large potential\nfor reduction. This paper proposes a novel short-term CO2 emissions forecast to\nenable intelligent scheduling of flexible electricity consumption to minimize\nthe resulting CO2 emissions. Two proposed time series decomposition methods are\ndeveloped for short-term forecasting of the CO2 emissions of electricity. These\nare in turn bench-marked against a set of state-of-the-art models. The result\nis a new forecasting method with a 48-hour horizon targeted the day-ahead\nelectricity market. Forecasting benchmarks for France show that the new method\nhas a mean absolute percentage error that is 25% lower than the best performing\nstate-of-the-art model. Further, application of the forecast for scheduling\nflexible electricity consumption is studied for five European countries.\nScheduling a flexible block of 4 hours of electricity consumption in a 24 hour\ninterval can on average reduce the resulting CO2 emissions by 25% in France,\n17% in Germany, 69% in Norway, 20% in Denmark, and just 3% in Poland when\ncompared to consuming at random intervals during the day.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 14:19:24 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 06:50:42 GMT"}, {"version": "v3", "created": "Sun, 18 Oct 2020 07:52:32 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Bokde", "Neeraj", ""], ["Tranberg", "Bo", ""], ["Andresen", "Gorm Bruun", ""]]}, {"id": "2003.10874", "submitter": "Shahab Boumi", "authors": "Shahab Boumi, Adan Vela, Jacquelyn Chini", "title": "Quantifying the relationship between student enrollment patterns and\n  student performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ed-ph cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simplified categorizations have often led to college students being labeled\nas full-time or part-time students. However, at many universities student\nenrollment patterns can be much more complicated, as it is not uncommon for\nstudents to alternate between full-time and part-time enrollment each semester\nbased on finances, scheduling, or family needs. While prior research has\nestablished full-time students maintain better outcomes then their part-time\ncounterparts, limited study has examined the impact of enrollment patterns or\nstrategies on academic outcomes. In this paper, we applying a Hidden Markov\nModel to identify and cluster students' enrollment strategies into three\ndifferent categorizes: full-time, part-time, and mixed-enrollment strategies.\nBased the enrollment strategies we investigate and compare the academic\nperformance outcomes of each group, taking into account differences between\nfirst-time-in-college students and transfer students. Analysis of data\ncollected from the University of Central Florida from 2008 to 2017 indicates\nthat first-time-in-college students that apply a mixed enrollment strategy are\ncloser in performance to full-time students, as compared to part-time students.\nMore importantly, during their part-time semesters, mixed-enrollment students\nsignificantly outperform part-time students. Similarly, analysis of transfer\nstudents shows that a mixed-enrollment strategy is correlated a similar\ngraduation rates as the full-time enrollment strategy, and more than double the\ngraduation rate associated with part-time enrollment. Such a finding suggests\nthat increased engagement through the occasional full-time enrollment leads to\nbetter overall outcomes.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 02:29:30 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 13:28:29 GMT"}, {"version": "v3", "created": "Mon, 6 Jul 2020 14:17:16 GMT"}, {"version": "v4", "created": "Sun, 8 Nov 2020 00:33:55 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Boumi", "Shahab", ""], ["Vela", "Adan", ""], ["Chini", "Jacquelyn", ""]]}, {"id": "2003.10965", "submitter": "Changchuan Yin Dr.", "authors": "Changchuan Yin", "title": "Genotyping coronavirus SARS-CoV-2: methods and implications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emerging global infectious COVID-19 coronavirus disease by novel Severe\nAcute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2) presents critical threats\nto global public health and the economy since it was identified in late\nDecember 2019 in China. The virus has gone through various pathways of\nevolution. For understanding the evolution and transmission of SARS-CoV-2,\ngenotyping of virus isolates is of great importance. We present an accurate\nmethod for effectively genotyping SARS-CoV-2 viruses using complete genomes.\nThe method employs the multiple sequence alignments of the genome isolates with\nthe SARS-CoV-2 reference genome. The SNP genotypes are then measured by Jaccard\ndistances to track the relationship of virus isolates. The genotyping analysis\nof SARS-CoV-2 isolates from the globe reveals that specific multiple mutations\nare the predominated mutation type during the current epidemic. Our method\nserves a promising tool for monitoring and tracking the epidemic of pathogenic\nviruses in their gradual and local genetic variations. The genotyping analysis\nshows that the genes encoding the S proteins and RNA polymerase, RNA primase,\nand nucleoprotein, undergo frequent mutations. These mutations are critical for\nvaccine development in disease control.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 16:41:06 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Yin", "Changchuan", ""]]}, {"id": "2003.11356", "submitter": "Sebastien Perez Vasseur Mr", "authors": "Sebasti\\'an P\\'erez Vasseur and Jos\\'e L. Aznarte", "title": "Probabilistic forecasting approaches for extreme NO$_2$ episodes: a\n  comparison of models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  High concentration episodes for NO$_2$ are increasingly dealt with by\nauthorities through traffic restrictions which are activated when air quality\ndeteriorates beyond certain thresholds. Foreseeing the probability that\npollutant concentrations reach those thresholds becomes thus a necessity.\nProbabilistic forecasting is a family of techniques that allow for the\nprediction of the expected distribution function instead of a single value. In\nthe case of NO$_2$, it allows for the calculation of future chances of\nexceeding thresholds and to detect pollution peaks. We thoroughly compared 10\nstate of the art probabilistic predictive models, using them to predict the\ndistribution of NO$_2$ concentrations in a urban location for a set of\nforecasting horizons (up to 60 hours). Quantile gradient boosted trees shows\nthe best performance, yielding the best results for both the expected value and\nthe forecast full distribution. Furthermore, we show how this approach can be\nused to detect pollution peaks.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 13:45:58 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Vasseur", "Sebasti\u00e1n P\u00e9rez", ""], ["Aznarte", "Jos\u00e9 L.", ""]]}, {"id": "2003.11383", "submitter": "Denis Allard", "authors": "Denis Allard, Paolo Fabbri, Carlo Gaetan", "title": "Modeling and simulating depositional sequences using latent Gaussian\n  random fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulating a depositional (or stratigraphic) sequence conditionally on\nborehole data is a long-standing problem in hydrogeology and in petroleum\ngeostatistics. This paper presents a new rule-based approach for simulating\ndepositional sequences of surfaces conditionally on lithofacies thickness data.\nThe thickness of each layer is modeled by a transformed latent Gaussian random\nfield allowing for null thickness thanks to a truncation process. Layers are\nsequentially stacked above each other following the regional stratigraphic\nsequence. By choosing adequately the variograms of these random fields, the\nsimulated surfaces separating two layers can be continuous and smooth. Borehole\ninformation is often incomplete in the sense that it does not provide direct\ninformation as to the exact layer some observed thickness belongs to. The\nlatent Gaussian model proposed in this paper offers a natural solution to this\nproblem by means of a Bayesian setting with a Markov Chain Monte Carlo (MCMC)\nalgorithm that can explore all possible configurations compatible with the\ndata. The model and the associated MCMC algorithm are validated on synthetic\ndata and then applied to a subsoil in the Venetian Plain with a moderately\ndense network of cored boreholes.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 13:16:18 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Allard", "Denis", ""], ["Fabbri", "Paolo", ""], ["Gaetan", "Carlo", ""]]}, {"id": "2003.11401", "submitter": "Weijie Zhou", "authors": "Weijie Zhou, Jiao Pan, Song Ding, Xiaoli Wu", "title": "A novel discrete grey seasonal model and its applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to accurately describe real systems with seasonal disturbances,\nwhich normally appear monthly or quarterly cycles, a novel discrete grey\nseasonal model, abbreviated as , is put forward by incorporating the seasonal\ndummy variables into the conventional model. Moreover, the mechanism and\nproperties of this proposed model are discussed in depth, revealing the\ninherent differences from the existing seasonal grey models. For validation and\nexplanation purposes, the proposed model is implemented to describe three\nactual cases with monthly and quarterly seasonal fluctuations (quarterly wind\npower production, quarterly PM10, and monthly natural gas consumption), in\ncomparison with five competing models involving grey prediction models ,\nconventional econometric technology , and artificial intelligences .\nExperimental results from the cases consistently demonstrated that the proposed\nmodel significantly outperforms the other benchmark models in terms of several\nerror criteria. Moreover, further discussions about the influences of different\nsequence lengths on the forecasting performance reveal that the proposed model\nstill performs the best with strong robustness and high reliability in\naddressing seasonal sequences. In general, the new model is validated to be a\npowerful and promising methodology for handling sequences with seasonal\nfluctuations.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 13:41:03 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Zhou", "Weijie", ""], ["Pan", "Jiao", ""], ["Ding", "Song", ""], ["Wu", "Xiaoli", ""]]}, {"id": "2003.11451", "submitter": "Aristides Moustakas", "authors": "Konstantinos Konstantopoulos, Aristides Moustakas, and Ioannis N.\n  Vogiatzakis", "title": "A spatially explicit impact assessment of road characteristics,\n  road-induced fragmentation and noise on bird species in Cyprus", "comments": null, "journal-ref": null, "doi": "10.1080/14888386.2020.1736154", "report-no": null, "categories": "q-bio.QM q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid increase of transportation infrastructure during the recent decades\nhas caused a number of effects on bird species, including collision mortality,\nhabitat loss, fragmentation and noise. This paper investigates the effects of\ntraffic noise and road-induced fragmentation on breeding bird richness in\nCyprus. Cyprus, situated along one of the main migratory routes for birds, has\na rich and diverse avifauna threatened by an ever-expanding road network and a\nroad density among the highest in Europe. In this first island-wide study we\nused data from 102 breeding birds recorded in 10 km x 10 km grid cells. Within\nevery cell we calculated road traffic noise and eight road-related properties.\nMost of the grid cells are subject to intense fragmentation and traffic noise\nwith combined impact hotspots located even within protected areas (such as Cape\nGreco, and the Troodos Massif). Results from variance partitioning indicated\nthat road-related properties (total road extent and road length) accounted for\na combined 59% of variation in species richness, followed by\nfragmentation-related properties and noise properties. The study posits the\nneed for further in-depth research on the effects of road networks on birds,\nand road construction, particularly in protected areas within Mediterranean\nislands.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 15:42:40 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Konstantopoulos", "Konstantinos", ""], ["Moustakas", "Aristides", ""], ["Vogiatzakis", "Ioannis N.", ""]]}, {"id": "2003.11474", "submitter": "Gal Levy-Fix", "authors": "Gal Levy-Fix, Jason Zucker, Konstantin Stojanovic, and No\\'emie\n  Elhadad", "title": "Towards Patient Record Summarization Through Joint Phenotype Learning in\n  HIV Patients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying a patient's key problems over time is a common task for providers\nat the point care, yet a complex and time-consuming activity given current\nelectric health records. To enable a problem-oriented summarizer to identify a\npatient's comprehensive list of problems and their salience, we propose an\nunsupervised phenotyping approach that jointly learns a large number of\nphenotypes/problems across structured and unstructured data. To identify the\nappropriate granularity of the learned phenotypes, the model is trained on a\ntarget patient population of the same clinic. To enable the content\norganization of a problem-oriented summarizer, the model identifies phenotype\nrelatedness as well. The model leverages a correlated-mixed membership approach\nwith variational inference applied to heterogenous clinical data. In this\npaper, we focus our experiments on assessing the learned phenotypes and their\nrelatedness as learned from a specific patient population. We ground our\nexperiments in phenotyping patients from an HIV clinic in a large urban care\ninstitution (n=7,523), where patients have voluminous, longitudinal\ndocumentation, and where providers would benefit from summaries of these\npatient's medical histories, whether about their HIV or any comorbidities. We\nfind that the learned phenotypes and their relatedness are clinically valid\nwhen assessed qualitatively by clinical experts, and that the model surpasses\nbaseline in inferring phenotype-relatedness when comparing to existing\nexpert-curated condition groupings.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 15:41:58 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Levy-Fix", "Gal", ""], ["Zucker", "Jason", ""], ["Stojanovic", "Konstantin", ""], ["Elhadad", "No\u00e9mie", ""]]}, {"id": "2003.11591", "submitter": "Yuki Ohnishi", "authors": "Yuki Ohnishi and Shinsuke Sugaya", "title": "Applying Bayesian Hierarchical Probit Model to Interview Grade\n  Evaluation", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Job interviews are a fundamental activity for most corporations to acquire\npotential candidates, and for job seekers to get well-rewarded and fulfilling\ncareer opportunities. In many cases, interviews are conducted in multiple\nprocesses such as telephone interviews and several face-to-face interviews. At\neach stage, candidates are evaluated in various aspects. Among them, grade\nevaluation, such as a rating on a 1-4 scale, might be used as a reasonable\nmethod to evaluate candidates. However, because each evaluation is based on a\nsubjective judgment of interviewers, the aggregated evaluations can be biased\nbecause the difference in toughness of interviewers is not examined.\nAdditionally, it is noteworthy that the toughness of interviewers might vary\ndepending on the interview round. As described herein, we propose an analytical\nframework of simultaneous estimation for both the true potential of candidates\nand toughness of interviewers' judgment considering job interview rounds, with\nalgorithms to extract unseen knowledge of the true potential of candidates and\ntoughness of interviewers as latent variables through analyzing grade data of\njob interviews. We apply a Bayesian Hierarchical Ordered Probit Model to the\ngrade data from HRMOS, a cloud-based Applicant Tracking System (ATS) operated\nby BizReach, Inc., an IT start-up particularly addressing human-resource needs\nin Japan. Our model successfully quantifies the candidate potential and the\ninterviewers' toughness. An interpretation and applications of the model are\ngiven along with a discussion of its place within hiring processes in\nreal-world settings. The parameters are estimated by Markov Chain Monte Carlo\n(MCMC). A discussion of uncertainty, which is given by the posterior\ndistribution of the parameters, is also provided along with the analysis.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 19:20:55 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Ohnishi", "Yuki", ""], ["Sugaya", "Shinsuke", ""]]}, {"id": "2003.11606", "submitter": "Yuki Ohnishi", "authors": "Yuki Ohnishi and Shinsuke Sugaya", "title": "Bayesian Hierarchical Bernoulli-Weibull Mixture Model for Extremely Rare\n  Events", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the duration of user behavior is a central concern for most\ninternet companies. Survival analysis is a promising method for analyzing the\nexpected duration of events and usually assumes the same survival function for\nall subjects and the event will occur in the long run. However, such\nassumptions are inappropriate when the users behave differently or some events\nnever occur for some users, i.e., the conversion period on web services of the\nlight users with no intention of behaving actively on the service. Especially,\nif the proportion of inactive users is high, this assumption can lead to\nundesirable results. To address these challenges, this paper proposes a mixture\nmodel that separately addresses active and inactive individuals with a latent\nvariable. First, we define this specific problem setting and show the\nlimitations of conventional survival analysis in addressing this problem. We\ndemonstrate how naturally our Bernoulli-Weibull model can accommodate the\nchallenge. The proposed model was extended further to a Bayesian hierarchical\nmodel to incorporate each subject's parameter, offering substantial\nimprovements over conventional, non-hierarchical models in terms of WAIC and\nWBIC. Second, an experiment and extensive analysis were conducted using\nreal-world data from the Japanese job search website, CareerTrek, offered by\nBizReach, Inc. In the analysis, some research questions are raised, such as the\ndifference in activation rate and conversion rate between user categories, and\nhow instantaneously the rate of event occurrence changes as time passes.\nQuantitative answers and interpretations are assigned to them. Furthermore, the\nmodel is inferred in a Bayesian manner, which enables us to represent the\nuncertainty with a credible interval of the parameters and predictive\nquantities.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 20:07:44 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Ohnishi", "Yuki", ""], ["Sugaya", "Shinsuke", ""]]}, {"id": "2003.11862", "submitter": "Federico Ricciardi", "authors": "Federico Ricciardi, Silvia Liverani and Gianluca Baio", "title": "Dirichlet Process Mixture Models for Regression Discontinuity Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Regression Discontinuity Design (RDD) is a quasi-experimental design that\nestimates the causal effect of a treatment when its assignment is defined by a\nthreshold value for a continuous assignment variable. The RDD assumes that\nsubjects with measurements within a bandwidth around the threshold belong to a\ncommon population, so that the threshold can be seen as a randomising device\nassigning treatment to those falling just above the threshold and withholding\nit from those who fall just below.\n  Bandwidth selection represents a compelling decision for the RDD analysis as\nthe results may be highly sensitive to its choice. A number of methods to\nselect the optimal bandwidth, mainly originating from the econometric\nliterature, have been proposed. However, their use in practice is limited.\n  We propose a methodology that, tackling the problem from an applied point of\nview, consider units' exchangeability, i.e., their similarity with respect to\nmeasured covariates, as the main criteria to select subjects for the analysis,\nirrespectively of their distance from the threshold. We carry out clustering on\nthe sample using a Dirichlet process mixture model to identify balanced and\nhomogeneous clusters. Our proposal exploits the posterior similarity matrix,\nwhich contains the pairwise probabilities that two observations are allocated\nto the same cluster in the MCMC sample. Thus we include in the RDD analysis\nonly those clusters for which we have stronger evidence of exchangeability.\n  We illustrate the validity of our methodology with both a simulated\nexperiment and a motivating example on the effect of statins to lower\ncholesterol level, using UK primary care data.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 12:22:45 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Ricciardi", "Federico", ""], ["Liverani", "Silvia", ""], ["Baio", "Gianluca", ""]]}, {"id": "2003.11869", "submitter": "Fr\u00e9d\u00e9ric Pro\u00efa", "authors": "Eunice Okome Obiang, Pascal J\\'ez\\'equel, Fr\\'ed\\'eric Pro\\\"ia", "title": "A partial graphical model with a structural prior on the direct links\n  between predictors and responses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is devoted to the estimation of a partial graphical model with a\nstructural Bayesian penalization. Precisely, we are interested in the linear\nregression setting where the estimation is made through the direct links\nbetween potentially high-dimensional predictors and multiple responses, since\nit is known that Gaussian graphical models enable to exhibit direct links only,\nwhereas coefficients in linear regressions contain both direct and indirect\nrelations (due \\textit{e.g.} to strong correlations among the variables). A\nsmooth penalty reflecting a generalized Gaussian Bayesian prior on the\ncovariates is added, either enforcing patterns (like row structures) in the\ndirect links or regulating the joint influence of predictors. We give a\ntheoretical guarantee for our method, taking the form of an upper bound on the\nestimation error arising with high probability, provided that the model is\nsuitably regularized. Empirical studies on synthetic data and a real dataset\nare conducted.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 12:43:13 GMT"}, {"version": "v2", "created": "Sat, 22 May 2021 11:08:34 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Obiang", "Eunice Okome", ""], ["J\u00e9z\u00e9quel", "Pascal", ""], ["Pro\u00efa", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "2003.11915", "submitter": "Sebastiaan H\\\"oppner", "authors": "Bart Baesens, Sebastiaan H\\\"oppner, Irene Ortner, and Tim Verdonck", "title": "robROSE: A robust approach for dealing with imbalanced data in fraud\n  detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge when trying to detect fraud is that the fraudulent\nactivities form a minority class which make up a very small proportion of the\ndata set. In most data sets, fraud occurs in typically less than 0.5% of the\ncases. Detecting fraud in such a highly imbalanced data set typically leads to\npredictions that favor the majority group, causing fraud to remain undetected.\nWe discuss some popular oversampling techniques that solve the problem of\nimbalanced data by creating synthetic samples that mimic the minority class. A\nfrequent problem when analyzing real data is the presence of anomalies or\noutliers. When such atypical observations are present in the data, most\noversampling techniques are prone to create synthetic samples that distort the\ndetection algorithm and spoil the resulting analysis. A useful tool for anomaly\ndetection is robust statistics, which aims to find the outliers by first\nfitting the majority of the data and then flagging data observations that\ndeviate from it. In this paper, we present a robust version of ROSE, called\nrobROSE, which combines several promising approaches to cope simultaneously\nwith the problem of imbalanced data and the presence of outliers. The proposed\nmethod achieves to enhance the presence of the fraud cases while ignoring\nanomalies. The good performance of our new sampling technique is illustrated on\nsimulated and real data sets and it is shown that robROSE can provide better\ninsight in the structure of the data. The source code of the robROSE algorithm\nis made freely available.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 16:11:07 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Baesens", "Bart", ""], ["H\u00f6ppner", "Sebastiaan", ""], ["Ortner", "Irene", ""], ["Verdonck", "Tim", ""]]}, {"id": "2003.12012", "submitter": "Kaiping Zheng", "authors": "Kaiping Zheng, Shaofeng Cai, Horng Ruey Chua, Wei Wang, Kee Yuan\n  Ngiam, Beng Chin Ooi", "title": "TRACER: A Framework for Facilitating Accurate and Interpretable\n  Analytics for High Stakes Applications", "comments": "A version of this preprint will appear in ACM SIGMOD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AI cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high stakes applications such as healthcare and finance analytics, the\ninterpretability of predictive models is required and necessary for domain\npractitioners to trust the predictions. Traditional machine learning models,\ne.g., logistic regression (LR), are easy to interpret in nature. However, many\nof these models aggregate time-series data without considering the temporal\ncorrelations and variations. Therefore, their performance cannot match up to\nrecurrent neural network (RNN) based models, which are nonetheless difficult to\ninterpret. In this paper, we propose a general framework TRACER to facilitate\naccurate and interpretable predictions, with a novel model TITV devised for\nhealthcare analytics and other high stakes applications such as financial\ninvestment and risk management. Different from LR and other existing RNN-based\nmodels, TITV is designed to capture both the time-invariant and the\ntime-variant feature importance using a feature-wise transformation subnetwork\nand a self-attention subnetwork, for the feature influence shared over the\nentire time series and the time-related importance respectively. Healthcare\nanalytics is adopted as a driving use case, and we note that the proposed\nTRACER is also applicable to other domains, e.g., fintech. We evaluate the\naccuracy of TRACER extensively in two real-world hospital datasets, and our\ndoctors/clinicians further validate the interpretability of TRACER in both the\npatient level and the feature level. Besides, TRACER is also validated in a\nhigh stakes financial application and a critical temperature forecasting\napplication. The experimental results confirm that TRACER facilitates both\naccurate and interpretable analytics for high stakes applications.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 15:06:05 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Zheng", "Kaiping", ""], ["Cai", "Shaofeng", ""], ["Chua", "Horng Ruey", ""], ["Wang", "Wei", ""], ["Ngiam", "Kee Yuan", ""], ["Ooi", "Beng Chin", ""]]}, {"id": "2003.12178", "submitter": "Cornelius Fritz", "authors": "Cornelius Fritz, Paul W. Thurner, G\\\"oran Kauermann", "title": "Separable and Semiparametric Network-based Counting Processes applied to\n  the International Combat Aircraft Trades", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel tie-oriented model for longitudinal event network data.\nThe generating mechanism is assumed to be a multivariate Poisson process that\ngoverns the onset and repetition of yearly observed events with two separate\nintensity functions. We apply the model to a network obtained from the number\nof international deliveries of combat aircraft trades between 1950 and 2017.\nBased on a modified trade gravity approach we identify economic and political\nfactors impeding or lightening the number of transfers. Extensive dynamics as\nwell as country heterogeneity require the specification of semiparametric\ntime-varying effects as well as random effects.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 22:36:57 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 18:01:07 GMT"}, {"version": "v3", "created": "Sun, 18 Apr 2021 19:42:50 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Fritz", "Cornelius", ""], ["Thurner", "Paul W.", ""], ["Kauermann", "G\u00f6ran", ""]]}, {"id": "2003.12405", "submitter": "Dominik Reinhard", "authors": "Dominik Reinhard and Michael Fau{\\ss} and Abdelhak M. Zoubir", "title": "Bayesian Sequential Joint Detection and Estimation under Multiple\n  Hypotheses", "comments": "27 pages, 2 figures, submitted to Sequential Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of jointly testing multiple hypotheses and estimating\na random parameter of the underlying distribution. This problem is investigated\nin a sequential setup under mild assumptions on the underlying random process.\nThe optimal method minimizes the expected number of samples while ensuring that\nthe average detection/estimation errors do not exceed a certain level. After\nconverting the constrained problem to an unconstrained one, we characterize the\ngeneral solution by a non-linear Bellman equation, which is parametrized by a\nset of cost coefficients. A strong connection between the derivatives of the\ncost function with respect to the coefficients and the detection/estimation\nerrors of the sequential procedure is derived. Based on this fundamental\nproperty, we further show that for suitably chosen cost coefficients the\nsolutions of the constrained and the unconstrained problem coincide. We present\ntwo approaches to finding the optimal coefficients. For the first approach, the\nfinal optimization problem is converted into a linear program, whereas the\nsecond approach solves it with a projected gradient ascent. To illustrate the\ntheoretical results, we consider two problems for which the optimal schemes are\ndesigned numerically. Using Monte Carlo simulations, it is validated that the\nnumerical results agree with the theory.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 13:17:30 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 14:55:55 GMT"}, {"version": "v3", "created": "Thu, 6 May 2021 15:13:56 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Reinhard", "Dominik", ""], ["Fau\u00df", "Michael", ""], ["Zoubir", "Abdelhak M.", ""]]}, {"id": "2003.12447", "submitter": "Yuzhong Huang", "authors": "Yuzhong Huang, Andres Abeliuk, Fred Morstatter, Pavel Atanasov, Aram\n  Galstyan", "title": "Anchor Attention for Hybrid Crowd Forecasts Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.MA", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Forecasting the future is a notoriously difficult task. To overcome this\nchallenge, state-of-the-art forecasting platforms are \"hybridized\", they gather\nforecasts from a crowd of humans, as well as one or more machine models.\nHowever, an open challenge remains in how to optimally combine forecasts from\nthese pools into a single forecast. We proposed anchor attention for this type\nof sequence summary problem. Each forecast is represented by a trainable\nembedding vector, and use computed anchor attention score as the combined\nweight. We evaluate our approach using data from real-world forecasting\ntournaments, and show that our method outperforms the current state-of-the-art\naggregation approaches.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 23:39:02 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Huang", "Yuzhong", ""], ["Abeliuk", "Andres", ""], ["Morstatter", "Fred", ""], ["Atanasov", "Pavel", ""], ["Galstyan", "Aram", ""]]}, {"id": "2003.12540", "submitter": "Ning Hao", "authors": "Ning Hao, Yue Selena Niu, Feifei Xiao, and Heping Zhang", "title": "A super scalable algorithm for short segment detection", "comments": "To be published in Statistics in Biosciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications such as copy number variant (CNV) detection, the goal is\nto identify short segments on which the observations have different means or\nmedians from the background. Those segments are usually short and hidden in a\nlong sequence, and hence are very challenging to find. We study a super\nscalable short segment (4S) detection algorithm in this paper. This\nnonparametric method clusters the locations where the observations exceed a\nthreshold for segment detection. It is computationally efficient and does not\nrely on Gaussian noise assumption. Moreover, we develop a framework to assign\nsignificance levels for detected segments. We demonstrate the advantages of our\nproposed method by theoretical, simulation, and real data studies.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 17:08:22 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Hao", "Ning", ""], ["Niu", "Yue Selena", ""], ["Xiao", "Feifei", ""], ["Zhang", "Heping", ""]]}, {"id": "2003.12643", "submitter": "Anderson Ara", "authors": "Anderson Ara, Mateus Maia, Samuel Mac\\^edo and Francisco Louzada", "title": "Random Machines Regression Approach: an ensemble support vector\n  regression model with free kernel choice", "comments": "arXiv admin note: text overlap with arXiv:1911.09411", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning techniques always aim to reduce the generalized prediction\nerror. In order to reduce it, ensemble methods present a good approach\ncombining several models that results in a greater forecasting capacity. The\nRandom Machines already have been demonstrated as strong technique, i.e: high\npredictive power, to classification tasks, in this article we propose an\nprocedure to use the bagged-weighted support vector model to regression\nproblems. Simulation studies were realized over artificial datasets, and over\nreal data benchmarks. The results exhibited a good performance of Regression\nRandom Machines through lower generalization error without needing to choose\nthe best kernel function during tuning process.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 21:30:59 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Ara", "Anderson", ""], ["Maia", "Mateus", ""], ["Mac\u00eado", "Samuel", ""], ["Louzada", "Francisco", ""]]}, {"id": "2003.12767", "submitter": "\\'Angel F. Garc\\'ia-Fern\\'andez", "authors": "\\'Angel F. Garc\\'ia-Fern\\'andez, Lennart Svensson, Jason L. Williams,\n  Yuxuan Xia, Karl Granstr\\\"om", "title": "Trajectory Poisson multi-Bernoulli filters", "comments": "Matlab code is provided at https://github.com/Agarciafernandez/MTT", "journal-ref": "in IEEE Transactions on Signal Processing, vol. 68, pp. 4933-4945,\n  2020", "doi": "10.1109/TSP.2020.3017046", "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents two trajectory Poisson multi-Bernoulli (TPMB) filters for\nmulti-target tracking: one to estimate the set of alive trajectories at each\ntime step and another to estimate the set of all trajectories, which includes\nalive and dead trajectories, at each time step. The filters are based on\npropagating a Poisson multi-Bernoulli (PMB) density on the corresponding set of\ntrajectories through the filtering recursion. After the update step, the\nposterior is a PMB mixture (PMBM) so, in order to obtain a PMB density, a\nKullback-Leibler divergence minimisation on an augmented space is performed.\nThe developed filters are computationally lighter alternatives to the\ntrajectory PMBM filters, which provide the closed-form recursion for sets of\ntrajectories with Poisson birth model, and are shown to outperform previous\nmulti-target tracking algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 11:04:45 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 11:07:32 GMT"}, {"version": "v3", "created": "Thu, 17 Sep 2020 12:59:16 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Garc\u00eda-Fern\u00e1ndez", "\u00c1ngel F.", ""], ["Svensson", "Lennart", ""], ["Williams", "Jason L.", ""], ["Xia", "Yuxuan", ""], ["Granstr\u00f6m", "Karl", ""]]}, {"id": "2003.12816", "submitter": "Adam Walder", "authors": "Adam Walder, Ephraim M. Hanks, Aleksandra Slavkovi\\'c", "title": "Privacy for Spatial Point Process Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we develop methods for privatizing spatial location data, such\nas spatial locations of individual disease cases. We propose two novel Bayesian\nmethods for generating synthetic location data based on log-Gaussian Cox\nprocesses (LGCPs). We show that conditional predictive ordinate (CPO) estimates\ncan easily be obtained for point process data. We construct a novel risk metric\nthat utilizes CPO estimates to evaluate individual disclosure risks. We adapt\nthe propensity mean square error (pMSE) data utility metric for LGCPs. We\ndemonstrate that our synthesis methods offer an improved risk vs. utility\nbalance in comparison to radial synthesis with a case study of Dr. John Snow's\ncholera outbreak data.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 15:23:21 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 20:11:21 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Walder", "Adam", ""], ["Hanks", "Ephraim M.", ""], ["Slavkovi\u0107", "Aleksandra", ""]]}, {"id": "2003.12844", "submitter": "Jonathan Boss", "authors": "Jonathan Boss, Alexander Rix, Yin-Hsiu Chen, Naveen N. Narisetty,\n  Zhenke Wu, Kelly K. Ferguson, Thomas F. McElrath, John D. Meeker, Bhramar\n  Mukherjee", "title": "A Hierarchical Integrative Group LASSO (HiGLASSO) Framework for\n  Analyzing Environmental Mixtures", "comments": "29 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Environmental health studies are increasingly measuring multiple pollutants\nto characterize the joint health effects attributable to exposure mixtures.\nHowever, the underlying dose-response relationship between toxicants and health\noutcomes of interest may be highly nonlinear, with possible nonlinear\ninteraction effects. Existing penalized regression methods that account for\nexposure interactions either cannot accommodate nonlinear interactions while\nmaintaining strong heredity or are computationally unstable in applications\nwith limited sample size. In this paper, we propose a general shrinkage and\nselection framework to identify noteworthy nonlinear main and interaction\neffects among a set of exposures. We design hierarchical integrative group\nLASSO (HiGLASSO) to (a) impose strong heredity constraints on two-way\ninteraction effects (hierarchical), (b) incorporate adaptive weights without\nnecessitating initial coefficient estimates (integrative), and (c) induce\nsparsity for variable selection while respecting group structure (group LASSO).\nWe prove sparsistency of the proposed method and apply HiGLASSO to an\nenvironmental toxicants dataset from the LIFECODES birth cohort, where the\ninvestigators are interested in understanding the joint effects of 21 urinary\ntoxicant biomarkers on urinary 8-isoprostane, a measure of oxidative stress. An\nimplementation of HiGLASSO is available in the higlasso R package, accessible\nthrough the Comprehensive R Archive Network.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 17:12:29 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Boss", "Jonathan", ""], ["Rix", "Alexander", ""], ["Chen", "Yin-Hsiu", ""], ["Narisetty", "Naveen N.", ""], ["Wu", "Zhenke", ""], ["Ferguson", "Kelly K.", ""], ["McElrath", "Thomas F.", ""], ["Meeker", "John D.", ""], ["Mukherjee", "Bhramar", ""]]}, {"id": "2003.12859", "submitter": "Juan B\\'ogalo Rom\\'an", "authors": "Juan B\\'ogalo, Pilar Poncela, Eva Senra", "title": "Circulant Singular Spectrum Analysis: A new automated procedure for\n  signal extraction", "comments": null, "journal-ref": null, "doi": "10.1016/j.sigpro.2020.107824", "report-no": null, "categories": "eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sometimes, it is of interest to single out the fluctuations associated to a\ngiven frequency. We propose a new variant of SSA, Circulant SSA (CiSSA), that\nallows to extract the signal associated to any frequency specified beforehand.\nThis is a novelty when compared with other procedures that need to identify\nex-post the frequencies associated to extracted signals. We prove that CiSSA is\nasymptotically equivalent to these alternative procedures although with the\nadvantage of avoiding the need of the subsequent frequency identification. We\ncheck its good performance and compare it to alternative SSA methods through\nseveral simulations for linear and nonlinear time series. We also prove its\nvalidity in the nonstationary case. To show how it works with real data, we\napply CiSSA to extract the business cycle and deseasonalize the Industrial\nProduction Index of six countries. Economists follow this indicator in order to\nassess the state of the economy in real time. We find that the estimated cycles\nmatch the dated recessions from the OECD showing its reliability for business\ncycle analysis. Finally, we analyze the strong separability of the estimated\ncomponents. In particular, we check that the deseasonalized time series do not\nshow any evidence of residual seasonality.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 17:59:59 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["B\u00f3galo", "Juan", ""], ["Poncela", "Pilar", ""], ["Senra", "Eva", ""]]}, {"id": "2003.12890", "submitter": "Vojtech Kejzlar", "authors": "Vojtech Kejzlar and Tapabrata Maiti", "title": "Variational Inference with Vine Copulas: An efficient Approach for\n  Bayesian Computer Model Calibration", "comments": "Submitted to the Statistics and Computing Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advancements of computer architectures, the use of computational\nmodels proliferates to solve complex problems in many scientific applications\nsuch as nuclear physics and climate research. However, the potential of such\nmodels is often hindered because they tend to be computationally expensive and\nconsequently ill-fitting for uncertainty quantification. Furthermore, they are\nusually not calibrated with real-time observations. We develop a\ncomputationally efficient algorithm based on variational Bayes inference (VBI)\nfor calibration of computer models with Gaussian processes. Unfortunately, the\nspeed and scalability of VBI diminishes when applied to the calibration\nframework with dependent data. To preserve the efficiency of VBI, we adopt a\npairwise decomposition of the data likelihood using vine copulas that separate\nthe information on dependence structure in data from their marginal\ndistributions. We provide both theoretical and empirical evidence for the\ncomputational scalability of our methodology and describe all the necessary\ndetails for an efficient implementation of the proposed algorithm. We also\ndemonstrate the opportunities given by our method for practitioners on a real\ndata example through calibration of the Liquid Drop Model of nuclear binding\nenergies.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 21:05:16 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 02:49:21 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Kejzlar", "Vojtech", ""], ["Maiti", "Tapabrata", ""]]}, {"id": "2003.12936", "submitter": "Yevgeniy Kovchegov", "authors": "Evgenia Chunikhina, Paul Logan, Yevgeniy Kovchegov, Anatoly\n  Yambartsev, Debashis Mondal, Andrey Morgun", "title": "The covariance shift (C-SHIFT) algorithm for normalizing biological data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN q-bio.QM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Omics technologies are powerful tools for analyzing patterns in gene\nexpression data for thousands of genes. Due to a number of systematic\nvariations in experiments, the raw gene expression data is often obfuscated by\nundesirable technical noises. Various normalization techniques were designed in\nan attempt to remove these non-biological errors prior to any statistical\nanalysis. One of the reasons for normalizing data is the need for recovering\nthe covariance matrix used in gene network analysis. In this paper, we\nintroduce a novel normalization technique, called the covariance shift\n(C-SHIFT) method. This normalization algorithm uses optimization techniques\ntogether with the blessing of dimensionality philosophy and energy minimization\nhypothesis for covariance matrix recovery under additive noise (in biology,\nknown as the bias). Thus, it is perfectly suited for the analysis of\nlogarithmic gene expression data. Numerical experiments on synthetic data\ndemonstrate the method's advantage over the classical normalization techniques.\nNamely, the comparison is made with rank, quantile, cyclic LOESS (locally\nestimated scatterplot smoothing), and MAD (median absolute deviation)\nnormalization methods.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 03:24:51 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Chunikhina", "Evgenia", ""], ["Logan", "Paul", ""], ["Kovchegov", "Yevgeniy", ""], ["Yambartsev", "Anatoly", ""], ["Mondal", "Debashis", ""], ["Morgun", "Andrey", ""]]}, {"id": "2003.13111", "submitter": "Mar\\'ia Xos\\'e Rodr\\'iguez-\\'Alvarez", "authors": "Maria Xose Rodriguez-Alvarez and Vanda Inacio", "title": "ROCnReg: An R Package for Receiver Operating Characteristic Curve\n  Inference with and without Covariate Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The receiver operating characteristic (ROC) curve is the most popular tool\nused to evaluate the discriminatory capability of diagnostic tests/biomarkers\nmeasured on a continuous scale when distinguishing between two alternative\ndisease states (e.g, diseased and nondiseased). In some circumstances, the\ntest's performance and its discriminatory ability may vary according to\nsubject-specific characteristics or different test settings. In such cases,\ninformation-specific accuracy measures, such as the covariate-specific and the\ncovariate-adjusted ROC curve are needed, as ignoring covariate information may\nlead to biased or erroneous results. This paper introduces the R package\nROCnReg that allows estimating the pooled (unadjusted) ROC curve, the\ncovariate-specific ROC curve, and the covariate-adjusted ROC curve by different\nmethods, both from (semi) parametric and nonparametric perspectives and within\nBayesian and frequentist paradigms. From the estimated ROC curve (pooled,\ncovariate-specific or covariate-adjusted), several summary measures of\naccuracy, such as the (partial) area under the ROC curve and the Youden index,\ncan be obtained. The package also provides functions to obtain ROC-based\noptimal threshold values using several criteria, namely, the Youden Index\ncriterion and the criterion that sets a target value for the false positive\nfraction. For the Bayesian methods, we provide tools for assessing model fit\nvia posterior predictive checks, while model choice can be carried out via\nseveral information criteria. Numerical and graphical outputs are provided for\nall methods. The package is illustrated through the analyses of data from an\nendocrine study where the aim is to assess the capability of the body mass\nindex to detect the presence or absence of cardiovascular disease risk factors.\nThe package is available from CRAN at\nhttps://CRAN.R-project.org/package=ROCnReg.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 19:04:42 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 17:55:33 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2020 09:53:14 GMT"}, {"version": "v4", "created": "Mon, 11 Jan 2021 18:00:15 GMT"}, {"version": "v5", "created": "Fri, 19 Mar 2021 17:04:27 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Rodriguez-Alvarez", "Maria Xose", ""], ["Inacio", "Vanda", ""]]}, {"id": "2003.13161", "submitter": "Mei Dong", "authors": "Konstantin Shestopaloff, Mei Dong, Fan Gao, Wei Xu", "title": "DCMD: Distance-based Classification Using Mixture Distributions on\n  Microbiome Data", "comments": "27 pages, 3 figures", "journal-ref": null, "doi": "10.1371/journal.pcbi.1008799", "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current advances in next generation sequencing techniques have allowed\nresearchers to conduct comprehensive research on microbiome and human diseases,\nwith recent studies identifying associations between human microbiome and\nhealth outcomes for a number of chronic conditions. However, microbiome data\nstructure, characterized by sparsity and skewness, presents challenges to\nbuilding effective classifiers. To address this, we present an innovative\napproach for distance-based classification using mixture distributions (DCMD).\nThe method aims to improve classification performance when using microbiome\ncommunity data, where the predictors are composed of sparse and heterogeneous\ncount data. This approach models the inherent uncertainty in sparse counts by\nestimating a mixture distribution for the sample data, and representing each\nobservation as a distribution, conditional on observed counts and the estimated\nmixture, which are then used as inputs for distance-based classification. The\nmethod is implemented into a k-means and k-nearest neighbours framework and we\nidentify two distance metrics that produce optimal results. The performance of\nthe model is assessed using simulations and applied to a human microbiome\nstudy, with results compared against a number of existing machine learning and\ndistance-based approaches. The proposed method is competitive when compared to\nthe machine learning approaches and showed a clear improvement over commonly\nused distance-based classifiers. The range of applicability and robustness make\nthe proposed method a viable alternative for classification using sparse\nmicrobiome count data.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 23:30:20 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Shestopaloff", "Konstantin", ""], ["Dong", "Mei", ""], ["Gao", "Fan", ""], ["Xu", "Wei", ""]]}, {"id": "2003.13304", "submitter": "Jannis Walk", "authors": "Jannis Walk, Robin Hirt, Niklas K\\\"uhl and Erik R. Hersl{\\o}v", "title": "Half-empty or half-full? A Hybrid Approach to Predict Recycling Behavior\n  of Consumers to Increase Reverse Vending Machine Uptime", "comments": "Exploring Service Science : 10th International Conference on\n  Exploring Service Science, IESS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Reverse Vending Machines (RVMs) are a proven instrument for facilitating\nclosed-loop plastic packaging recycling. A good customer experience at the RVM\nis crucial for a further proliferation of this technology. Bin full events are\nthe major reason for Reverse Vending Machine (RVM) downtime at the world leader\nin the RVM market. The paper at hand develops and evaluates an approach based\non machine learning and statistical approximation to foresee bin full events\nand, thus increase uptime of RVMs. Our approach relies on forecasting the\nhourly time series of returned beverage containers at a given RVM. We\ncontribute by developing and evaluating an approach for hourly forecasts in a\nretail setting - this combination of application domain and forecast\ngranularity is novel. A trace-driven simulation confirms that the\nforecasting-based approach leads to less downtime and costs than naive emptying\nstrategies.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 09:48:53 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Walk", "Jannis", ""], ["Hirt", "Robin", ""], ["K\u00fchl", "Niklas", ""], ["Hersl\u00f8v", "Erik R.", ""]]}, {"id": "2003.13385", "submitter": "Ahmet Yucekaya", "authors": "Ergun Yukseltan, Ahmet Yucekaya, Ayse Humeyra Bilge, Esra Agca Aktunc", "title": "Forecasting Models for Daily Natural Gas Consumption Considering\n  Periodic Variations and Demand Segregation", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to expensive infrastructure and the difficulties in storage, supply\nconditions of natural gas are different from those of other traditional energy\nsources like petroleum or coal. To overcome these challenges, supplier\ncountries require take-or-pay agreements for requested natural gas quantities.\nThese contracts have many pre-clauses; if they are not met due to low/high\nconsumption or other external factors, buyers must completely fulfill them. A\nsimilar contract is then imposed on distributors and wholesale consumers. It is\nthus important for all parties to forecast their daily, monthly, and annual\nnatural gas demand to minimize their risk. In this paper, a model consisting of\na modulated expansion in Fourier series, supplemented by deviations from\ncomfortable temperatures as a regressor is proposed for the forecast of monthly\nand weekly consumption over a one-year horizon. This model is supplemented by a\nday-ahead feedback mechanism for the forecast of daily consumption. The method\nis applied to the study of natural gas consumption for major residential areas\nin Turkey, on a yearly, monthly, weekly, and daily basis. It is shown that\nresidential heating dominates winter consumption and masks all other\nvariations. On the other hand, weekend and holiday effects are visible in\nsummer consumption and provide an estimate for residential and industrial use.\nThe advantage of the proposed method is the capability of long term projections\nand to outperform time series methods.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 20:48:14 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Yukseltan", "Ergun", ""], ["Yucekaya", "Ahmet", ""], ["Bilge", "Ayse Humeyra", ""], ["Aktunc", "Esra Agca", ""]]}, {"id": "2003.13462", "submitter": "Zachary Pisano", "authors": "Zachary M. Pisano, Joshua S. Agterberg, Carey E. Priebe, and Daniel Q.\n  Naiman", "title": "Spectral graph clustering via the Expectation-Solution algorithm", "comments": "31 pages, intended for submission to J. Comp. and Graph. Stat", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stochastic blockmodel (SBM) models the connectivity within and between\ndisjoint subsets of nodes in networks. Prior work demonstrated that the rows of\nan SBM's adjacency spectral embedding (ASE) and Laplacian spectral embedding\n(LSE) both converge in law to Gaussian mixtures where the components are curved\nexponential families. Maximum likelihood estimation via the\nExpectation-Maximization (EM) algorithm for a full Gaussian mixture model (GMM)\ncan then perform the task of clustering graph nodes, albeit without appealing\nto the components' curvature. Noting that EM is a special case of the\nExpectation-Solution (ES) algorithm, we propose two ES algorithms that allow us\nto take full advantage of these curved structures. After presenting the ES\nalgorithm for the general curved-Gaussian mixture, we develop those\ncorresponding to the ASE and LSE limiting distributions. Simulating from\nartificial SBMs and a brain connectome SBM reveals that clustering graph nodes\nvia our ES algorithms improves upon that of EM for a full GMM.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 13:21:46 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Pisano", "Zachary M.", ""], ["Agterberg", "Joshua S.", ""], ["Priebe", "Carey E.", ""], ["Naiman", "Daniel Q.", ""]]}, {"id": "2003.13478", "submitter": "Andrii Babii", "authors": "Andrii Babii", "title": "High-dimensional mixed-frequency IV regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a high-dimensional linear IV regression for the data\nsampled at mixed frequencies. We show that the high-dimensional slope parameter\nof a high-frequency covariate can be identified and accurately estimated\nleveraging on a low-frequency instrumental variable. The distinguishing feature\nof the model is that it allows handing high-dimensional datasets without\nimposing the approximate sparsity restrictions. We propose a\nTikhonov-regularized estimator and derive the convergence rate of its\nmean-integrated squared error for time series data. The estimator has a\nclosed-form expression that is easy to compute and demonstrates excellent\nperformance in our Monte Carlo experiments. We estimate the real-time price\nelasticity of supply on the Australian electricity spot market. Our estimates\nsuggest that the supply is relatively inelastic and that its elasticity is\nheterogeneous throughout the day.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 13:41:02 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Babii", "Andrii", ""]]}, {"id": "2003.13687", "submitter": "Melissa Humphries Dr", "authors": "Rabiah A. Rahmat, Melissa A. Humphries, Jeremy J. Austin, Adrian M. T.\n  Linacre, Mark Raven, Peter Self", "title": "Integrating spectrophotometric and XRD analyses in the investigation of\n  burned dental remains", "comments": null, "journal-ref": "Forensic Science International (May 2020), 310, 110236", "doi": "10.1016/j.forsciint.2020.110236", "report-no": null, "categories": "cond-mat.mtrl-sci physics.med-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heat alters colour and crystallinity of teeth by destruction of the organic\ncontent and inducing hydroxyapatite crystal growth. The colour and crystallite\nchanges can be quantified using spectrophotometric and x-ray diffraction\nanalyses, however these analyses are not commonly used in combination to\nevaluate burned dental remains. In this study, thirty-nine teeth were\nincinerated at 300-1000$^\\circ$C for 15 and 30 minutes and then measured using\na spectrophotometer and an x-ray diffractometer. Response variables used were\nlightness, L$^\\ast$, and chromaticity a$^\\ast$ and b$^\\ast$ and luminance\n(whiteness and yellowness) for colour, and crystal size for crystallinity.\nStatistical analysis to determine the attribution of these variables revealed\nyellowness and crystal size were significantly affected by temperature ($p <\n0.05$), whilst duration of heat-exposure showed no significant effect. This\nstudy suggests the inclusion of both spectrophotometric and x-ray diffraction\nin investigating thermal-heated teeth is useful to accurately estimate the\ntemperature teeth are exposed to.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 00:09:02 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Rahmat", "Rabiah A.", ""], ["Humphries", "Melissa A.", ""], ["Austin", "Jeremy J.", ""], ["Linacre", "Adrian M. T.", ""], ["Raven", "Mark", ""], ["Self", "Peter", ""]]}, {"id": "2003.13754", "submitter": "Connor Coley", "authors": "Connor W. Coley, Natalie S. Eyke, Klavs F. Jensen", "title": "Autonomous discovery in the chemical sciences part I: Progress", "comments": "Revised version available at 10.1002/anie.201909987", "journal-ref": null, "doi": "10.1002/anie.201909987", "report-no": null, "categories": "q-bio.QM cs.AI cs.RO stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This two-part review examines how automation has contributed to different\naspects of discovery in the chemical sciences. In this first part, we describe\na classification for discoveries of physical matter (molecules, materials,\ndevices), processes, and models and how they are unified as search problems. We\nthen introduce a set of questions and considerations relevant to assessing the\nextent of autonomy. Finally, we describe many case studies of discoveries\naccelerated by or resulting from computer assistance and automation from the\ndomains of synthetic chemistry, drug discovery, inorganic chemistry, and\nmaterials science. These illustrate how rapid advancements in hardware\nautomation and machine learning continue to transform the nature of\nexperimentation and modelling.\n  Part two reflects on these case studies and identifies a set of open\nchallenges for the field.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 19:11:31 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Coley", "Connor W.", ""], ["Eyke", "Natalie S.", ""], ["Jensen", "Klavs F.", ""]]}, {"id": "2003.13755", "submitter": "Connor Coley", "authors": "Connor W. Coley, Natalie S. Eyke, Klavs F. Jensen", "title": "Autonomous discovery in the chemical sciences part II: Outlook", "comments": "Revised version available at 10.1002/anie.201909989", "journal-ref": null, "doi": "10.1002/anie.201909989", "report-no": null, "categories": "q-bio.QM cs.AI cs.RO stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This two-part review examines how automation has contributed to different\naspects of discovery in the chemical sciences. In this second part, we reflect\non a selection of exemplary studies. It is increasingly important to articulate\nwhat the role of automation and computation has been in the scientific process\nand how that has or has not accelerated discovery. One can argue that even the\nbest automated systems have yet to ``discover'' despite being incredibly useful\nas laboratory assistants. We must carefully consider how they have been and can\nbe applied to future problems of chemical discovery in order to effectively\ndesign and interact with future autonomous platforms.\n  The majority of this article defines a large set of open research directions,\nincluding improving our ability to work with complex data, build empirical\nmodels, automate both physical and computational experiments for validation,\nselect experiments, and evaluate whether we are making progress toward the\nultimate goal of autonomous discovery. Addressing these practical and\nmethodological challenges will greatly advance the extent to which autonomous\nsystems can make meaningful discoveries.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 19:11:35 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Coley", "Connor W.", ""], ["Eyke", "Natalie S.", ""], ["Jensen", "Klavs F.", ""]]}, {"id": "2003.13822", "submitter": "Avleen S. Bijral", "authors": "Stefan Wojcik, Avleen Bijral, Richard Johnston, Juan Miguel Lavista,\n  Gary King, Ryan Kennedy, Alessandro Vespignani and David Lazer", "title": "Survey Data and Human Computation for Improved Flu Tracking", "comments": null, "journal-ref": null, "doi": "10.1038/s41467-020-20206-z", "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While digital trace data from sources like search engines hold enormous\npotential for tracking and understanding human behavior, these streams of data\nlack information about the actual experiences of those individuals generating\nthe data. Moreover, most current methods ignore or under-utilize human\nprocessing capabilities that allow humans to solve problems not yet solvable by\ncomputers (human computation). We demonstrate how behavioral research, linking\ndigital and real-world behavior, along with human computation, can be utilized\nto improve the performance of studies using digital data streams. This study\nlooks at the use of search data to track prevalence of Influenza-Like Illness\n(ILI). We build a behavioral model of flu search based on survey data linked to\nusers online browsing data. We then utilize human computation for classifying\nsearch strings. Leveraging these resources, we construct a tracking model of\nILI prevalence that outperforms strong historical benchmarks using only a\nlimited stream of search data and lends itself to tracking ILI in smaller\ngeographic units. While this paper only addresses searches related to ILI, the\nmethod we describe has potential for tracking a broad set of phenomena in near\nreal-time.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 21:11:29 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Wojcik", "Stefan", ""], ["Bijral", "Avleen", ""], ["Johnston", "Richard", ""], ["Lavista", "Juan Miguel", ""], ["King", "Gary", ""], ["Kennedy", "Ryan", ""], ["Vespignani", "Alessandro", ""], ["Lazer", "David", ""]]}, {"id": "2003.13888", "submitter": "Alan Xian", "authors": "Benjamin Avanzi, Greg Taylor, Bernard Wong and Alan Xian", "title": "Modelling and understanding count processes through a Markov-modulated\n  non-homogeneous Poisson process framework", "comments": "For simulated data sets and code, please go to\n  https://github.com/agi-lab/reserving-MMNPP", "journal-ref": null, "doi": "10.1016/j.ejor.2020.07.022", "report-no": null, "categories": "q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Markov-modulated Poisson process is utilised for count modelling in a\nvariety of areas such as queueing, reliability, network and insurance claims\nanalysis. In this paper, we extend the Markov-modulated Poisson process\nframework through the introduction of a flexible frequency perturbation\nmeasure. This contribution enables known information of observed event arrivals\nto be naturally incorporated in a tractable manner, while the hidden Markov\nchain captures the effect of unobservable drivers of the data. In addition to\nincreases in accuracy and interpretability, this method supplements analysis of\nthe latent factors. Further, this procedure naturally incorporates data\nfeatures such as over-dispersion and autocorrelation. Additional insights can\nbe generated to assist analysis, including a procedure for iterative model\nimprovement.\n  Implementation difficulties are also addressed with a focus on dealing with\nlarge data sets, where latent models are especially advantageous due the large\nnumber of observations facilitating identification of hidden factors. Namely,\ncomputational issues such as numerical underflow and high processing cost arise\nin this context and in this paper, we produce procedures to overcome these\nproblems.\n  This modelling framework is demonstrated using a large insurance data set to\nillustrate theoretical, practical and computational contributions and an\nempirical comparison to other count models highlight the advantages of the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 00:50:54 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 12:05:47 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Avanzi", "Benjamin", ""], ["Taylor", "Greg", ""], ["Wong", "Bernard", ""], ["Xian", "Alan", ""]]}, {"id": "2003.14110", "submitter": "Avishek Bhandari", "authors": "Avishek Bhandari", "title": "A wavelet analysis of inter-dependence, contagion and long memory among\n  global equity markets", "comments": "PhD Thesis", "journal-ref": null, "doi": null, "report-no": "8168-12SEPH10", "categories": "econ.EM nlin.CD stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study attempts to investigate into the structure and features of global\nequity markets from a time-frequency perspective. An analysis grounded on this\nframework allows one to capture information from a different dimension, as\nopposed to the traditional time domain analyses, where multiscale structures of\nfinancial markets are clearly extracted. In financial time series, multiscale\nfeatures manifest themselves due to presence of multiple time horizons. The\nexistence of multiple time horizons necessitates a careful investigation of\neach time horizon separately as market structures are not homogenous across\ndifferent time horizons. The presence of multiple time horizons, with varying\nlevels of complexity, requires one to investigate financial time series from a\nheterogeneous market perspective where market players are said to operate at\ndifferent investment horizons. This thesis extends the application of\ntime-frequency based wavelet techniques to: i) analyse the interdependence of\nglobal equity markets from a heterogeneous investor perspective with a special\nfocus on the Indian stock market, ii) investigate the contagion effect, if any,\nof financial crises on Indian stock market, and iii) to study fractality and\nscaling properties of global equity markets and analyse the efficiency of\nIndian stock markets using wavelet based long memory methods.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 11:28:20 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Bhandari", "Avishek", ""]]}, {"id": "2003.14262", "submitter": "Jouni Takalo", "authors": "Jouni J. Takalo", "title": "Comparison of Latitude Distribution and Evolution of Even and Odd\n  Sunspot Cycles", "comments": "11 pages, 5 figures", "journal-ref": "Sol Phys 295, 49 (2020)", "doi": "10.1007/s11207-020-01615-1", "report-no": null, "categories": "astro-ph.SR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the latitudinal distribution and evolution of sunspot areas from\nSolar Cycle 12 to Solar Cycle 23 (SC12-SC23) and sunspot-groups of from Solar\nCycle 8 to Solar Cycle 23 (SC8-SC23) for even and odd cycles. The Rician\ndistribution is the best-fit function for both even and odd sunspots group\nlatitudinal occurrence. The mean and variance for even northern/southern\nbutterfly wing sunspots are 14.94/14.76 and 58.62/56.08, respectively, and the\nmean and variance for odd northern/southern wing sunspots are 15.52/15.58 and\n61.77/58.00, respectively. Sunspot groups of even cycle wings are thus at\nsomewhat lower latitudes on the average than sunspot groups of the odd cycle\nwings, i.e., about 0.6 degrees for northern hemisphere wings and 0.8 degrees\nfor southern hemisphere wings. The spatial analysis of sunspot areas between\nSC12-SC23 shows that the small sunspots are at lower solar latitudes of the sun\nthan the large sunspots for both odd and even cycles, and also for both\nhemispheres. Temporal evolution of sunspot areas shows a lack of large sunspots\nafter four years (exactly between 4.2-4.5 years), i.e., about 40\\% after the\nstart of the cycle, especially for even cycles. This is related to the\nGnevyshev gap and is occurring at the time when the evolution of the average\nsunspot latitudes cross about 15 degrees. The gap is, however, clearer for even\ncycles than odd ones. Gnevyshev gap divides the cycle into two disparate parts:\nthe ascending phase/cycle maximum and the declining phase of the sunspot cycle.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 14:47:28 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Takalo", "Jouni J.", ""]]}, {"id": "2003.14276", "submitter": "Philippe Goulet Coulombe", "authors": "Francis X. Diebold, Maximilian G\\\"obel, Philippe Goulet Coulombe,\n  Glenn D. Rudebusch, Boyuan Zhang", "title": "Optimal Combination of Arctic Sea Ice Extent Measures: A Dynamic Factor\n  Modeling Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The diminishing extent of Arctic sea ice is a key indicator of climate change\nas well as an accelerant for future global warming. Since 1978, Arctic sea ice\nhas been measured using satellite-based microwave sensing; however, different\nmeasures of Arctic sea ice extent have been made available based on differing\nalgorithmic transformations of the raw satellite data. We propose and estimate\na dynamic factor model that combines four of these measures in an optimal way\nthat accounts for their differing volatility and cross-correlations. We then\nuse the Kalman smoother to extract an optimal combined measure of Arctic sea\nice extent. It turns out that almost all weight is put on the NSIDC Sea Ice\nIndex, confirming and enhancing confidence in the Sea Ice Index and the NASA\nTeam algorithm on which it is based.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 15:02:27 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 02:01:46 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Diebold", "Francis X.", ""], ["G\u00f6bel", "Maximilian", ""], ["Coulombe", "Philippe Goulet", ""], ["Rudebusch", "Glenn D.", ""], ["Zhang", "Boyuan", ""]]}, {"id": "2003.14310", "submitter": "Abhinandan Dalal", "authors": "Arindam Roy Chowdhury, Abhinandan Dalal and Shubhajit Sen", "title": "Accelerography: Feasibility of Gesture Typing using Accelerometer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim to look into the feasibility of constructing alphabets\nusing gestures. The main idea is to construct gestures, that are easy to\nremember, not cumbersome to reproduce and easily identifiable. We construct\ngestures for the entire English alphabet and provide an algorithm to identify\nthe gestures, even when they are constructed continuously. We tackle the\nproblem statistically, taking into account the problem of randomness in the\nhand movement gestures of users, and achieve an average accuracy of 97.33% with\nthe entire English alphabet.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 20:12:46 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Chowdhury", "Arindam Roy", ""], ["Dalal", "Abhinandan", ""], ["Sen", "Shubhajit", ""]]}, {"id": "2003.14382", "submitter": "Vladim\\'ir Hol\\'y", "authors": "Petra Tomanov\\'a and Vladim\\'ir Hol\\'y", "title": "Clustering of Arrivals in Queueing Systems: Autoregressive Conditional\n  Duration Approach", "comments": null, "journal-ref": "Tomanov\\'a, P. & Hol\\'y, V. (2021). Clustering of Arrivals in\n  Queueing Systems: Autoregressive Conditional Duration Approach. Central\n  European Journal of Operations Research, 29(3), 859-874", "doi": "10.1007/s10100-021-00744-7", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arrivals in queueing systems are typically assumed to be independent and\nexponentially distributed. Our analysis of an online bookshop, however, shows\nthat there is an autocorrelation structure present. First, we adjust the\ninter-arrival times for diurnal and seasonal patterns. Second, we model\nadjusted inter-arrival times by the generalized autoregressive score (GAS)\nmodel based on the generalized gamma distribution in the spirit of the\nautoregressive conditional duration (ACD) models. Third, in a simulation study,\nwe investigate the effects of the dynamic arrival model on the number of\ncustomers, the busy period, and the response time in queueing systems with\nsingle and multiple servers. We find that ignoring the autocorrelation\nstructure leads to significantly underestimated performance measures and\nconsequently suboptimal decisions. The proposed approach serves as a general\nmethodology for the treatment of arrivals clustering in practice.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 17:27:51 GMT"}, {"version": "v2", "created": "Sat, 19 Sep 2020 10:10:00 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Tomanov\u00e1", "Petra", ""], ["Hol\u00fd", "Vladim\u00edr", ""]]}]