[{"id": "1507.01404", "submitter": "J\\'er\\'emy Magnanensi", "authors": "J\\'er\\'emy Magnanensi, Fr\\'ed\\'eric Bertrand, Myriam Maumy-Bertrand,\n  Nicolas Meyer", "title": "A new Universal Resample Stable Bootstrap-based Stopping Criterion in\n  PLS Components Construction", "comments": "31 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": "1507.01404", "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new robust stopping criterion in Partial Least Squares\nRegressions (PLSR) components construction characterised by a high level of\nstability. This new criterion is defined as a universal one since it is\nsuitable both for PLSR and its extension to Generalized Linear Regressions\n(PLSGLR). This criterion is based on a non-parametric bootstrap process and has\nto be computed algorithmically. It allows to test each successive components on\na preset significant level alpha. In order to assess its performances and\nrobustness with respect to different noise levels, we perform intensive\ndatasets simulations, with a preset and known number of components to extract,\nboth in the case n>p (n being the number of subjects and p the number of\noriginal predictors), and for datasets with n<p. We then use t-tests to compare\nthe performance of our approach to some others classical criteria. The property\nof robustness is particularly tested through resampling processes on a real\nallelotyping dataset. Our conclusion is that our criterion presents also better\nglobal predictive performances, both in the PLSR and PLSGLR (Logistic and\nPoisson) frameworks.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2015 11:56:18 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["Magnanensi", "J\u00e9r\u00e9my", ""], ["Bertrand", "Fr\u00e9d\u00e9ric", ""], ["Maumy-Bertrand", "Myriam", ""], ["Meyer", "Nicolas", ""]]}, {"id": "1507.02537", "submitter": "Thomas Opitz", "authors": "Thomas Opitz", "title": "Modeling asymptotically independent spatial extremes based on Laplace\n  random fields", "comments": null, "journal-ref": null, "doi": "10.1016/j.spasta.2016.01.001", "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the modeling of threshold exceedances in asymptotically independent\nstochastic processes by constructions based on Laplace random fields. These are\ndefined as Gaussian random fields scaled with a stochastic variable following\nan exponential distribution. This framework yields useful asymptotic properties\nwhile remaining statistically convenient. Univariate distribution tails are of\nthe half exponential type and are part of the limiting generalized Pareto\ndistributions for threshold exceedances. After normalizing marginal tail\ndistributions in data, a standard Laplace field can be used to capture spatial\ndependence among extremes. Asymptotic properties of Laplace fields are explored\nand compared to the classical framework of asymptotic dependence. Multivariate\njoint tail decay rates for Laplace fields are slower than for Gaussian fields\nwith the same covariance structure; hence they provide more conservative\nestimates of very extreme joint risks while maintaining asymptotic\nindependence. Statistical inference is illustrated on extreme wind gusts in the\nNetherlands where a comparison to the Gaussian dependence model shows a better\ngoodness-of-fit in terms of Akaike's criterion. In this specific application we\nfit the well-adapted Weibull distribution as univariate tail model, such that\nthe normalization of univariate tail distributions can be done through a simple\npower transformation of data.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 14:52:02 GMT"}, {"version": "v2", "created": "Tue, 8 Mar 2016 10:12:28 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Opitz", "Thomas", ""]]}, {"id": "1507.05057", "submitter": "Enrique Guerra-Pujol", "authors": "Enrique Guerra-Pujol", "title": "Visualizing Probabilistic Proof", "comments": "37 pages, 7 diagrams", "journal-ref": "Washington University Jurisprudence Review, vol. 7, no. 1 (2014),\n  pp. 39-75", "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The author revisits the Blue Bus Problem, a famous thought-experiment in law\ninvolving probabilistic proof, and presents simple Bayesian solutions to\ndifferent versions of the blue bus hypothetical. In addition, the author\nexpresses his solutions in standard and visual formats, i.e. in terms of\nprobabilities and natural frequencies.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2015 09:57:33 GMT"}], "update_date": "2015-07-20", "authors_parsed": [["Guerra-Pujol", "Enrique", ""]]}, {"id": "1507.05346", "submitter": "George Cobb", "authors": "George W. Cobb (Mount Holyoke College)", "title": "Mere Renovation is Too Little Too Late: We Need to Rethink Our\n  Undergraduate Curriculum from the Ground Up", "comments": "33 pages. To be published in The American Statistician", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last half-dozen years have seen The American Statistician publish\nwell-argued and provocative calls to change our thinking about statistics and\nhow we teach it, among them Brown and Kass (2009), Nolan and Temple-Lang\n(2010), and Legler et al. (2010). Within this past year, the ASA has issued a\nnew and comprehensive set of guidelines for undergraduate programs (ASA 2014).\nAccepting (and applauding) all this as background, the current article argues\nthe need to rethink our curriculum from the ground up, and offers five\nprinciples and two caveats intended to help us along the path toward a new\nsynthesis. These principles and caveats rest on my sense of three parallel\nevolutions: the convergence of trends in the roles of mathematics, computation,\nand context within statistics education. These ongoing changes, together with\nthe articles cited above and the seminal provocation by Leo Breiman (2001) call\nfor a deep rethinking of what we teach to undergraduates. In particular,\nfollowing Brown and Kass, we should put priority on two goals, to make\nfundamental concepts accessible and to minimize prerequisites to research.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2015 21:54:44 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Cobb", "George W.", "", "Mount Holyoke College"]]}, {"id": "1507.05982", "submitter": "Christopher Wild", "authors": "Chris J. Wild", "title": "On locating statistics in the world of finding out", "comments": "9 pages. The paper is an extended discussion of \"Enhancing statistics\n  education by including qualitative research\" by Irena Ograjen\\v{s}ek and Iddo\n  Gal that will appear, in some form, in the International Statistical Review\n  together with the parent paper. It can however be read as a stand-alone paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper attempts to situate statistics in relation to qualitative research\nmethods and other means of \"finding out\". It compares and contrasts aspects of\nqualitative research methods and statistical inquiry and attempts to answer the\nquestion of whether and how elements of qualitative research methods should be\nincluded in statistics teaching.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 20:39:39 GMT"}], "update_date": "2015-07-23", "authors_parsed": [["Wild", "Chris J.", ""]]}, {"id": "1507.06411", "submitter": "Olivier Francois", "authors": "Olivier Francois", "title": "Arbitrariness of peer review: A Bayesian analysis of the NIPS experiment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT cs.DL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The principle of peer review is central to the evaluation of research, by\nensuring that only high-quality items are funded or published. But peer review\nhas also received criticism, as the selection of reviewers may introduce biases\nin the system. In 2014, the organizers of the ``Neural Information Processing\nSystems\\rq\\rq{} conference conducted an experiment in which $10\\%$ of submitted\nmanuscripts (166 items) went through the review process twice. Arbitrariness\nwas measured as the conditional probability for an accepted submission to get\nrejected if examined by the second committee. This number was equal to $60\\%$,\nfor a total acceptance rate equal to $22.5\\%$. Here we present a Bayesian\nanalysis of those two numbers, by introducing a hidden parameter which measures\nthe probability that a submission meets basic quality criteria. The standard\nquality criteria usually include novelty, clarity, reproducibility, correctness\nand no form of misconduct, and are met by a large proportions of submitted\nitems. The Bayesian estimate for the hidden parameter was equal to $56\\%$\n($95\\%$CI: $ I = (0.34, 0.83)$), and had a clear interpretation. The result\nsuggested the total acceptance rate should be increased in order to decrease\narbitrariness estimates in future review processes.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2015 08:39:34 GMT"}], "update_date": "2015-07-24", "authors_parsed": [["Francois", "Olivier", ""]]}, {"id": "1507.07244", "submitter": "William Briggs", "authors": "William M. Briggs", "title": "The Crisis Of Evidence: Why Probability And Statistics Cannot Discover\n  Cause", "comments": "Corrected typos; 22 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probability models are only useful at explaining the uncertainty of what we\ndo not know, and should never be used to say what we already know. Probability\nand statistical models are useless at discerning cause. Classical statistical\nprocedures, in both their frequentist and Bayesian implementations are, falsely\nimply they can speak about cause. No hypothesis test, or Bayes factor, should\never be used again. Even assuming we know the cause or partial cause for some\nset of observations, reporting via relative risk exagerates the certainty we\nhave in the future, often by a lot. This over-certainty is made much worse when\nparametetric and not predictive methods are used. Unfortunately, predictive\nmethods are rarely used; and even when they are, cause must still be an\nassumption, meaning (again) certainty in our scientific pronouncements is too\nhigh.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2015 20:11:24 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2015 16:42:15 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Briggs", "William M.", ""]]}, {"id": "1507.07902", "submitter": "Junmo Song", "authors": "Junmo Song, Dong-hyun Oh, and Jiwon Kang", "title": "Robust Estimation in Stochastic Frontier Models", "comments": "43 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study proposes a robust estimator for stochastic frontier models by\nintegrating the idea of Basu et al. [1998, Biometrika 85, 549-559] into such\nmodels. We verify that the suggested estimator is strongly consistent and\nasymptotic normal under regularity conditions and investigate robust\nproperties. We use a simulation study to demonstrate that the estimator has\nstrong robust properties with little loss in asymptotic efficiency relative to\nthe maximum likelihood estimator. A real data analysis is performed for\nillustrating the use of the estimator.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2015 19:10:07 GMT"}], "update_date": "2015-07-29", "authors_parsed": [["Song", "Junmo", ""], ["Oh", "Dong-hyun", ""], ["Kang", "Jiwon", ""]]}, {"id": "1507.07905", "submitter": "Florian Wagner", "authors": "Florian Wagner", "title": "The XL-mHG Test For Enrichment: A Technical Report", "comments": "corrected terminology (\"threshold\" => \"cutoff\"); added section 5,\n  \"Quantifying the strength of enrichment\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The minimum hypergeometric test (mHG) is a powerful nonparametric hypothesis\ntest to detect enrichment in ranked binary lists. Here, I provide a detailed\nreview of its definition, as well as the algorithms used in its implementation,\nwhich enable the efficient computation of an exact p-value. I then introduce a\ngeneralization of the mHG, termed XL-mHG, which provides additional control\nover the type of enrichment tested, and describe the precise algorithmic\nmodifications necessary to compute its test statistic and p-value. The XL-mHG\nalgorithm is a building block of GO-PCA, a recently proposed method for the\nexploratory analysis of gene expression data using prior knowledge.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2015 19:19:54 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2015 19:53:13 GMT"}], "update_date": "2015-09-25", "authors_parsed": [["Wagner", "Florian", ""]]}, {"id": "1507.08500", "submitter": "Zhaohai Li", "authors": "Gang Zheng, Zhaohai Li, Nancy L. Geller", "title": "A Conversation with Robert C. Elston", "comments": "Published at http://dx.doi.org/10.1214/14-STS497 in the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2015, Vol. 30, No. 2, 258-267", "doi": "10.1214/14-STS497", "report-no": "IMS-STS-STS497", "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robert C. Elston was born on February 4, 1932, in London, England. He went to\nCambridge University to study natural science from 1952-1956 and obtained B.A.,\nM.A. and Diploma in Agriculture (Dip Ag). He came to the US at age 24 to study\nanimal breeding at Cornell University and received his Ph.D. in 1959. From\n1959-1960, he was a post-doctoral fellow in biostatistics at University of\nNorth Carolina (UNC), Chapel Hill, where he studied mathematical statistics. He\nthen rose through the academic ranks in the department of biostatistics at UNC,\nbecoming a full professor in 1969. From 1979-1995, he was a professor and head\nof the Department of Biometry and Genetics at Louisiana State University\nMedical Center in New Orleans. In 1995, he moved to Case Western Reserve\nUniversity where he is a professor of epidemiology and biostatistics and served\nas chairman from 2008 to 2014. Between 1966 and 2013, he directed 42 Ph.D.\nstudents and mentored over 40 post-doctoral fellows. If one regards him as a\nfounder of a pedigree in research in genetic epidemiology, it was estimated in\n2007 that there were more than 500 progeny. Among his many honors are a NIH\nResearch Career Development Award (1966-1976), the Leadership Award from\nInternational Society of Human Genetics (1995), William Allan Award from\nAmerican Society of Human Genetics (1996), NIH MERIT Award (1998) and the\nMarvin Zelen Leadership Award, Harvard University (2004). He is a Fellow of the\nAmerican Statistical Association and the Institute of Mathematical Statistics\nas well as a Fellow of the Ohio Academy of Science. A leader in research in\ngenetic epidemiology for over 40 years, he has published over 600 research\narticles in biostatistics, genetic epidemiology and applications. He has also\ncoauthored and edited 9 books in biostatistics, population genetics and methods\nfor the analysis of genetic data.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 09:05:14 GMT"}], "update_date": "2015-07-31", "authors_parsed": [["Zheng", "Gang", ""], ["Li", "Zhaohai", ""], ["Geller", "Nancy L.", ""]]}, {"id": "1507.08502", "submitter": "N. I. Fisher", "authors": "N. I. Fisher", "title": "A Conversation with Jerry Friedman", "comments": "Published at http://dx.doi.org/10.1214/14-STS509 in the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2015, Vol. 30, No. 2, 268-295", "doi": "10.1214/14-STS509", "report-no": "IMS-STS-STS509", "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Jerome H. Friedman was born in Yreka, California, USA, on December 29, 1939.\nHe received his high school education at Yreka High School, then spent two\nyears at Chico State College before transferring to the University of\nCalifornia at Berkeley in 1959. He completed an undergraduate degree in physics\nin 1962 and a Ph.D. in high-energy particle physics in 1968 and was a\npost-doctoral research physicist at the Lawrence Berkeley Laboratory during\n1968-1972. In 1972, he moved to Stanford Linear Accelerator Center (SLAC) as\nhead of the Computation Research Group, retaining this position until 2006. In\n1981, he was appointed half time as Professor in the Department of Statistics,\nStanford University, remaining half time with his SLAC appointment. He has held\nvisiting appointments at CSIRO in Sydney, CERN and the Department of Statistics\nat Berkeley, and has had a very active career as a commercial consultant. Jerry\nbecame Professor Emeritus in the Department of Statistics in 2007. Apart from\nsome 30 publications in high-energy physics early in his career, Jerry has\npublished over 70 research articles and books in statistics and computer\nscience, including co-authoring the pioneering books Classification and\nRegression Trees and The Elements of Statistical Learning. Many of his\npublications have hundreds if not thousands of citations (e.g., the CART book\nhas over 21,000). Much of his software is incorporated in commercial products,\nincluding at least one popular search engine. Many of his methods and\nalgorithms are essential inclusions in modern statistical and data mining\npackages. Honors include the following: the Rietz Lecture (1999) and the Wald\nLectures (2009); election to the American Academy of Arts and Sciences (2005)\nand the US National Academy of Sciences (2010); a Fellow of the American\nStatistical Association; Paper of the Year (JASA 1980, 1985; Technometrics\n1998, 1992); Statistician of the Year (ASA, Chicago Chapter, 1999); ACM Data\nMining Lifetime Innovation Award (2002), Emanuel & Carol Parzen Award for\nStatistical Innovation (2004); Noether Senior Lecturer (American Statistical\nAssociation, 2010); and the IEEE Computer Society Data Mining Research\nContribution Award (2012).\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 09:39:28 GMT"}], "update_date": "2015-07-31", "authors_parsed": [["Fisher", "N. I.", ""]]}, {"id": "1507.08934", "submitter": "Scott Grimshaw", "authors": "Scott D. Grimshaw", "title": "A Framework for Infusing Authentic Data Experiences Within Statistics\n  Courses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Working with complex data is one of the important updates to the 2014 ASA\nCurriculum Guidelines for Undergraduate Programs in Statistical Science.\nInfusing 'authentic data experiences' within courses allow students\nopportunities to learn and practice data skills as they prepare a dataset for\nanalysis. While more modest in scope than a senior-level culminating\nexperience, authentic data experiences provide an opportunity to demonstrate\nconnections between data skills and statistical skills. The result is more\npractice of data skills for undergraduate statisticians.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2015 16:28:30 GMT"}], "update_date": "2015-08-03", "authors_parsed": [["Grimshaw", "Scott D.", ""]]}]