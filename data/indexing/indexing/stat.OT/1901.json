[{"id": "1901.01336", "submitter": "Max Robinson", "authors": "Max Robinson", "title": "Projective Decomposition and Matrix Equivalence up to Scale", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT cs.CV math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A data matrix may be seen simply as a means of organizing observations into\nrows ( e.g., by measured object) and into columns ( e.g., by measured variable)\nso that the observations can be analyzed with mathematical tools. As a\nmathematical object, a matrix defines a linear mapping between points\nrepresenting weighted combinations of its rows (the row vector space) and\npoints representing weighted combinations of its columns (the column vector\nspace). From this perspective, a data matrix defines a relationship between the\ninformation that labels its rows and the information that labels its columns,\nand numerical methods are used to analyze this relationship. A first step is to\nnormalize the data, transforming each observation from scales convenient for\nmeasurement to a common scale, on which addition and multiplication can\nmeaningfully combine the different observations. For example, z-transformation\nrescales every variable to the same scale, standardized variation from an\nexpected value, but ignores scale differences between measured objects. Here we\ndevelop the concepts and properties of projective decomposition, which applies\nthe same normalization strategy to both rows and columns by separating the\nmatrix into row- and column-scaling factors and a scale-normalized matrix. We\nshow that different scalings of the same scale-normalized matrix form an\nequivalence class, and call the scale-normalized, canonical member of the class\nits scale-invariant form that preserves all pairwise relative ratios.\nProjective decomposition therefore provides a means of normalizing the broad\nclass of ratio-scale data, in which relative ratios are of primary interest,\nonto a common scale without altering the ratios of interest, and simultaneously\naccounting for scale effects for both organizations of the matrix values. Both\nof these properties distinguish it from z-transformation.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 22:47:42 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Robinson", "Max", ""]]}, {"id": "1901.02714", "submitter": "Avishek Choudhury", "authors": "Avishek Choudhury", "title": "Hourly Forecasting of Emergency Department Arrivals : Time Series\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: The stochastic behavior of patient arrival at an emergency\ndepartment (ED) complicates the management of an ED. More than 50% of hospitals\nED capacity tends to operate beyond its normal capacity and eventually fails to\ndeliver high-quality care. To address the concern of stochastics ED arrivals,\nmany types of research has been done using yearly, monthly and weekly time\nseries forecasting. Aim: Our research team believes that hourly time-series\nforecasting of the load can improve ED management by predicting the arrivals of\nfuture patients, and thus, can support strategic decisions in terms of quality\nenhancement. Methods: Our research does not involve any human subject, only ED\nadmission data from January 2014 to August 2017 retrieved from the UnityPoint\nHealth database. Autoregressive integrated moving average (ARIMA), Holt\nWinters, TBATS, and neural network methods were implemented to forecast hourly\nED patient arrival. Findings: ARIMA (3,0,0) (2,1,0) was selected as the best\nfit model with minimum Akaike information criterion and Schwartz Bayesian\ncriterion. The model was stationary and qualified the Box Ljung correlation\ntest and the Jarque Bera test for normality. The mean error (ME) and root mean\nsquare error (RMSE) were selected as performance measures. An ME of 1.001 and\nan RMSE of 1.55 was obtained. Conclusions: ARIMA can be used to provide hourly\nforecasts for ED arrivals and can be utilized as a decision support system in\nthe healthcare industry. Application: This technique can be implemented in\nhospitals worldwide to predict ED patient arrival.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jan 2019 03:59:53 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Choudhury", "Avishek", ""]]}, {"id": "1901.03696", "submitter": "Hideo Hirose", "authors": "Hideo Hirose", "title": "Key Factor Not to Drop Out is to Attend the Lecture", "comments": "arXiv admin note: text overlap with arXiv:1901.02056", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In addition to the learning check testing results performed at each lectures,\nwe have extended the factors to find the key dropping out factors. Among them\nare, the number of successes in the learning check testing, the number of\nattendances to the follow-up program classes, and etc. Then, we have found key\nfactors strongly related to the students at risk. They are the following. 1)\nBadly failed students (score range is 0-39 in the final examination) tend to be\nabsent for the regular classes and fail in the learning check testing even if\nthey attended, and they are very reluctant to attend the follow-up program\nclasses. 2) Successful students (score range is 60-100 in the final\nexamination) attend classes and get good scores in every learning check\ntesting. 3) Failed students but not so badly (score range is 40-59 in the final\nexamination) reveal both sides of features appeared in score range of 0-39 and\nscore range of 60-100. Therefore, it is crucial to attend the lectures in order\nnot to drop out. Students who failed in learning check testing more than half\nout of all testing times almost absolutely failed in the final examination,\nwhich could cause the drop out. Also, students who were successful to learning\ncheck testing more than two third out of all testing times took better score in\nthe final examination.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 03:06:40 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Hirose", "Hideo", ""]]}, {"id": "1901.03701", "submitter": "Gejza Dohnal", "authors": "Gejza Dohnal", "title": "Robust Adaptive Control Charts", "comments": "8 pages, 3 figures. The paper was presented as a contribution on\n  Quality and Productivity Research Conference, Long Beach, CA, June 4 - 7 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In statistical process control, procedures are applied that require\nrelatively strict conditions for their use. If such assumptions are violated,\nthese methods become inefficient, leading to increased incidence of false\nsignals. Therefore, a robust version of control charts is sought to be less\nsensitive with respect to a breach of normality and independence in\nmeasurements. Robust control charts, however, usually increase the delay in the\ndetection of assignable causes. This negative effect can, to some extent, be\nremoved with the aid of an adaptive approach.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 10:50:51 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Dohnal", "Gejza", ""]]}, {"id": "1901.03876", "submitter": "Paolo Rocchi", "authors": "Paolo Rocchi", "title": "Four Fundamental Questions in Probability Theory and Statistics", "comments": null, "journal-ref": "Physics Essays, 30(3), 2017", "doi": null, "report-no": null, "categories": "math.HO stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study has the purpose of addressing four questions that lie at the base\nof the probability theory and statistics, and includes two main steps. As\nfirst, we conduct the textual analysis of the most significant works written by\neminent probability theorists. The textual analysis turns out to be a rather\ninnovative method of study in this domain, and shows how the sampled writers,\nno matter he is a frequentist or a subjectivist, share a similar approach. Each\nauthor argues on the multifold aspects of probability then he establishes the\nmathematical theory on the basis of his intellectual conclusions. It may be\nsaid that mathematics ranks second. Hilbert foresees an approach far different\nfrom that used by the sampled authors. He proposes to axiomatize the\nprobability calculus notably to describe the probability concepts using purely\nmathematical criteria. In the second stage of the present research we address\nthe four issues of the probability theory and statistics following the\nrecommendations of Hilbert. Specifically, we use two theorems that prove how\nthe frequentist and the subjectivist models are not incompatible as many\nbelieve. Probability has distinct meanings under different hypotheses, and in\nturn classical statistics and Bayesian statistics are available for adoption in\ndifferent circumstances. Subsequently, these results are commented upon,\nfollowed by our conclusions\n", "versions": [{"version": "v1", "created": "Sat, 12 Jan 2019 16:13:06 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Rocchi", "Paolo", ""]]}, {"id": "1901.04689", "submitter": "Yiying Zhang", "authors": "Jan Dhaene, Roger J. A. Laeven, Yiying Zhang", "title": "Systemic Risk: Conditional Distortion Risk Measures", "comments": "41 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the rich classes of conditional distortion (CoD)\nrisk measures and distortion risk contribution ($\\Delta$CoD) measures as\nmeasures of systemic risk and analyze their properties and representations. The\nclasses include the well-known conditional Value-at-Risk, conditional Expected\nShortfall, and risk contribution measures in terms of the VaR and ES as special\ncases. Sufficient conditions are presented for two random vectors to be ordered\nby the proposed CoD-risk measures and distortion risk contribution measures.\nThese conditions are expressed using the conventional stochastic dominance,\nincreasing convex/concave, dispersive, and excess wealth orders of the\nmarginals and canonical positive/negative stochastic dependence notions.\nNumerical examples are provided to illustrate our theoretical findings. This\npaper is the second in a triplet of papers on systemic risk by the same\nauthors. In \\cite{DLZorder2018a}, we introduce and analyze some new stochastic\norders related to systemic risk. In a third (forthcoming) paper, we attribute\nsystemic risk to the different participants in a given risky environment.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 07:29:36 GMT"}, {"version": "v2", "created": "Mon, 28 Jan 2019 14:27:33 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Dhaene", "Jan", ""], ["Laeven", "Roger J. A.", ""], ["Zhang", "Yiying", ""]]}, {"id": "1901.04824", "submitter": "Detlef Steuer", "authors": "Ursula Garzcarek and Detlef Steuer", "title": "Approaching Ethical Guidelines for Data Scientists", "comments": "18 pages, submitted Nov 12th 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT cs.AI cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this article is to inspire data scientists to participate in the\ndebate on the impact that their professional work has on society, and to become\nactive in public debates on the digital world as data science professionals.\nHow do ethical principles (e.g., fairness, justice, beneficence, and\nnon-maleficence) relate to our professional lives? What lies in our\nresponsibility as professionals by our expertise in the field? More\nspecifically this article makes an appeal to statisticians to join that debate,\nand to be part of the community that establishes data science as a proper\nprofession in the sense of Airaksinen, a philosopher working on professional\nethics. As we will argue, data science has one of its roots in statistics and\nextends beyond it. To shape the future of statistics, and to take\nresponsibility for the statistical contributions to data science, statisticians\nshould actively engage in the discussions. First the term data science is\ndefined, and the technical changes that have led to a strong influence of data\nscience on society are outlined. Next the systematic approach from CNIL is\nintroduced. Prominent examples are given for ethical issues arising from the\nwork of data scientists. Further we provide reasons why data scientists should\nengage in shaping morality around and to formulate codes of conduct and codes\nof practice for data science. Next we present established ethical guidelines\nfor the related fields of statistics and computing machinery. Thereafter\nnecessary steps in the community to develop professional ethics for data\nscience are described. Finally we give our starting statement for the debate:\nData science is in the focal point of current societal development. Without\nbecoming a profession with professional ethics, data science will fail in\nbuilding trust in its interaction with and its much needed contributions to\nsociety!\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 16:13:27 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Garzcarek", "Ursula", ""], ["Steuer", "Detlef", ""]]}, {"id": "1901.06708", "submitter": "Benyamin Ghojogh", "authors": "Benyamin Ghojogh, Aydin Ghojogh, Mark Crowley, Fakhri Karray", "title": "Fitting A Mixture Distribution to Data: Tutorial", "comments": "12 pages, 9 figures, 1 table. Some typos are corrected in this\n  version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a step-by-step tutorial for fitting a mixture distribution to\ndata. It merely assumes the reader has the background of calculus and linear\nalgebra. Other required background is briefly reviewed before explaining the\nmain algorithm. In explaining the main algorithm, first, fitting a mixture of\ntwo distributions is detailed and examples of fitting two Gaussians and\nPoissons, respectively for continuous and discrete cases, are introduced.\nThereafter, fitting several distributions in general case is explained and\nexamples of several Gaussians (Gaussian Mixture Model) and Poissons are again\nprovided. Model-based clustering, as one of the applications of mixture\ndistributions, is also introduced. Numerical simulations are also provided for\nboth Gaussian and Poisson examples for the sake of better clarification.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jan 2019 18:17:40 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 20:49:26 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Ghojogh", "Benyamin", ""], ["Ghojogh", "Aydin", ""], ["Crowley", "Mark", ""], ["Karray", "Fakhri", ""]]}, {"id": "1901.08589", "submitter": "Russell Bowater", "authors": "Russell J. Bowater", "title": "Organic fiducial inference", "comments": "Final version with corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A substantial generalisation is put forward of the theory of subjective\nfiducial inference as it was outlined in earlier papers. In particular, this\ntheory is extended to deal with cases where the data are discrete or\ncategorical rather than continuous, and cases where there was important\npre-data knowledge about some or all of the model parameters. The system for\ndirectly expressing and then handling this pre-data knowledge, which is via\nwhat are referred to as global and local pre-data functions for the parameters\nconcerned, is distinct from that which involves attempting to directly\nrepresent this knowledge in the form of a prior distribution function over\nthese parameters, and then using Bayes' theorem. In this regard, the individual\nattributes of what are identified as three separate types of fiducial argument,\nnamely the strong, moderate and weak fiducial arguments, form an integral part\nof the theory that is developed. Various practical examples of the application\nof this theory are presented, including examples involving binomial, Poisson\nand multinomial data. The fiducial distribution functions for the parameters of\nthe models in these examples are interpreted in terms of a generalised\ndefinition of subjective probability that was set out previously.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 19:35:29 GMT"}, {"version": "v2", "created": "Thu, 1 Aug 2019 16:28:29 GMT"}, {"version": "v3", "created": "Mon, 18 Jan 2021 17:38:28 GMT"}, {"version": "v4", "created": "Thu, 8 Apr 2021 16:48:25 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Bowater", "Russell J.", ""]]}, {"id": "1901.09686", "submitter": "Sanne Willems", "authors": "Sanne J.W. Willems, Casper J. Albers and Ionica Smeets", "title": "Variability in the interpretation of Dutch probability phrases - a risk\n  for miscommunication", "comments": "22 pages, 7 figures (including appendices)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Verbal probability phrases are often used to express estimated risk. In this\nstudy, focus was on the numerical interpretation of 29 Dutch probability and\nfrequency phrases, including several complementary phrases to test (a)symmetry\nin their interpretation. Many of these phrases had not been studied before. The\nphrases were presented in the context of ordinary situations. The survey was\ndistributed among both statisticians and non-statisticians with Dutch as their\nnative language.\n  The responses from 881 participants showed a large variability in the\ninterpretation of Dutch phrases, and the neutral contexts seemed to have no\nstructural influence. Furthermore, the results demonstrated an asymmetry in the\ninterpretation of Dutch complementary phrases. The large variability of\ninterpretations was found among both statisticians and non-statisticians, and\namong males and females, however, no structural differences were found between\nthe groups.\n  Concluding, there is a large variability in the interpretation of verbal\nprobability phrases, even within sub-populations. Therefore, verbal probability\nexpressions may be a risk for miscommunication.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 14:29:56 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Willems", "Sanne J. W.", ""], ["Albers", "Casper J.", ""], ["Smeets", "Ionica", ""]]}, {"id": "1901.09779", "submitter": "Shovan Chowdhury", "authors": "Asok K Nanda and Shovan Chowdhury", "title": "Shannon's entropy and its Generalizations towards Statistics,\n  Reliability and Information Science during 1948-2018", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Starting from the pioneering works of Shannon and Weiner in 1948, a plethora\nof works have been reported on entropy in different directions. Entropy-related\nreview work in the direction of statistics, reliability and information\nscience, to the best of our knowledge, has not been reported so far. Here we\nhave tried to collect all possible works in this direction during the period\n1948-2018 so that people interested in entropy, specially the new researchers,\nget benefited.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 16:27:20 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Nanda", "Asok K", ""], ["Chowdhury", "Shovan", ""]]}, {"id": "1901.10875", "submitter": "Sacha Servan-Schreiber", "authors": "Sacha Servan-Schreiber, Olga Ohrimenko, Tim Kraska, Emanuel Zgraggen", "title": "STAR: Statistical Tests with Auditable Results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present STAR: a novel system aimed at solving the complex issue of\n\"p-hacking\" and false discoveries in scientific studies. STAR provides a\nconcrete way for ensuring the application of false discovery control procedures\nin hypothesis testing, using mathematically provable guarantees, with the goal\nof reducing the risk of data dredging. STAR generates an efficiently auditable\ncertificate which attests to the validity of each statistical test performed on\na dataset. STAR achieves this by using several cryptographic techniques which\nare combined specifically for this purpose. Under-the-hood, STAR uses a\ndecentralized set of authorities (e.g., research institutions), secure\ncomputation techniques, and an append-only ledger which together enable\nauditing of scientific claims by 3rd parties and matches real world trust\nassumptions. We implement and evaluate a construction of STAR using the\nMicrosoft SEAL encryption library and SPDZ multi-party computation protocol.\nOur experimental evaluation demonstrates the practicality of STAR in multiple\nreal world scenarios as a system for certifying scientific discoveries in a\ntamper-proof way.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jan 2019 20:04:48 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 15:54:54 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Servan-Schreiber", "Sacha", ""], ["Ohrimenko", "Olga", ""], ["Kraska", "Tim", ""], ["Zgraggen", "Emanuel", ""]]}]