[{"id": "1903.00049", "submitter": "Maciej  Skorski", "authors": "Maciej Skorski", "title": "Bounds on Bayes Factors for Binomial A/B Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayes factors, in many cases, have been proven to bridge the classic -value\nbased significance testing and bayesian analysis of posterior odds. This paper\ndiscusses this phenomena within the binomial A/B testing setup (applicable for\nexample to conversion testing). It is shown that the bayes factor is controlled\nby the \\emph{Jensen-Shannon divergence} of success ratios in two tested groups,\nwhich can be further bounded by the Welch statistic. As a result, bayesian\nsample bounds almost match frequentionist's sample bounds. The link between\nJensen-Shannon divergence and Welch's test as well as the derivation are an\nelegant application of tools from information geometry.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 20:06:02 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Skorski", "Maciej", ""]]}, {"id": "1903.01706", "submitter": "Jonathan Levy", "authors": "Jonathan Levy", "title": "Tutorial: Deriving The Efficient Influence Curve for Large Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to provide a tutorial for upper level undergraduate and\ngraduate students in statistics, biostatistics and epidemiology on deriving\ninfluence functions for non-parametric and semi-parametric models. The author\nwill build on previously known efficiency theory and provide a useful identity\nand formulaic technique only relying on the basics of integration which, are\nself-contained in this tutorial and can be used in most any setting one might\nencounter in practice. The paper provides many examples of such derivations for\nwell-known influence functions as well as for new parameters of interest. The\ninfluence function remains a central object for constructing efficient\nestimators for large models, such as the one-step estimator and the targeted\nmaximum likelihood estimator. We will not touch upon these estimators at all\nbut readers familiar with these estimators might find this tutorial of\nparticular use.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 07:38:17 GMT"}, {"version": "v2", "created": "Thu, 7 Mar 2019 09:52:51 GMT"}, {"version": "v3", "created": "Sat, 9 Mar 2019 07:20:46 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Levy", "Jonathan", ""]]}, {"id": "1903.01829", "submitter": "Leslie Myint", "authors": "Leslie Myint, Aboozar Hadavand, Leah Jager, Jeffrey Leek", "title": "Comparison of plotting system outputs in beginner analysts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The R programming language is built on an ecosystem of packages, some that\nallow analysts to accomplish the same tasks. For example, there are at least\ntwo clear workflows for creating data visualizations in R: using the base\ngraphics package (referred to as \"base R\") and the ggplot2 add-on package based\non the grammar of graphics. Here we perform an empirical study of the quality\nof scientific graphics produced by beginning R users. In our experiment,\nlearners taking a data science course on the Coursera platform were randomized\nto complete identical plotting exercises in either the base R or the ggplot2\nsystem. Learners were then asked to evaluate their peers in terms of visual\ncharacteristics key to scientific cognition. We observed that graphics created\nwith the two systems rated similarly on many characteristics. However, ggplot2\ngraphics were generally judged to be more visually pleasing and, in the case of\nfaceted scientific plots, easier to understand. Our results suggest that while\nboth graphic systems are useful in the hands of beginning users, ggplot2's\nnatural faceting system may be easier to use by beginning users for displaying\nmore complex relationships.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2019 02:55:26 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Myint", "Leslie", ""], ["Hadavand", "Aboozar", ""], ["Jager", "Leah", ""], ["Leek", "Jeffrey", ""]]}, {"id": "1903.01868", "submitter": "Giovanni Parmigiani", "authors": "Giovanni Parmigiani", "title": "The Fuzzy ROC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fuzzy ROC extends Receiver Operating Curve (ROC) visualization to the\nsituation where some data points, falling in an indeterminacy region, are not\nclassified. It addresses two challenges: definition of sensitivity and\nspecificity bounds under indeterminacy; and visual summarization of the large\nnumber of possibilities arising from different choices of indeterminacy zones.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 16:32:10 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Parmigiani", "Giovanni", ""]]}, {"id": "1903.05514", "submitter": "Guannan Hu", "authors": "Guannan Hu, Tam\\'as B\\'odai, and Valerio Lucarini", "title": "Effects of Stochastic Parametrization on Extreme Value Statistics", "comments": "21 pages and 17 figures", "journal-ref": null, "doi": "10.1063/1.5095756", "report-no": null, "categories": "nlin.CD physics.data-an stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extreme geophysical events are of crucial relevance to our daily life: they\nthreaten human lives and cause property damage. To assess the risk and reduce\nlosses, we need to model and probabilistically predict these events.\nParametrizations are computational tools used in Earth system models, which are\naimed at reproducing the impact of unresolved scales on resolved scales. The\nperformance of parametrizations has usually been examined on typical events\nrather than on extreme events. In this paper we consider a modified version of\nthe two-level Lorenz'96 model and investigate how two parametrizations of the\nfast degrees of freedom perform in terms of the representation of extreme\nevents. One parametrization is constructed following Wilks (2005) and is\nconstructed through an empirical fitting procedure; the other parametrization\nis constructed through the statistical mechanical approach proposed by Wouters\nand Lucarini (2012, 2013). The two strategies show different advantages and\ndisadvantages. We discover that the agreement between parametrized models and\ntrue model is in general worse when looking at extremes rather than at the bulk\nof the statistics. The results suggest that stochastic parametrizations should\nbe accurately and specifically tested against their performance on extreme\nevents, as usual optimization procedures might neglect them.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 14:43:58 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2019 13:27:25 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Hu", "Guannan", ""], ["B\u00f3dai", "Tam\u00e1s", ""], ["Lucarini", "Valerio", ""]]}, {"id": "1903.06568", "submitter": "Lukas Koch", "authors": "Lukas Koch", "title": "A response-matrix-centred approach to presenting cross-section\n  measurements", "comments": "26 pages, added reference to Phystat-nu", "journal-ref": null, "doi": "10.1088/1748-0221/14/09/P09013", "report-no": null, "categories": "stat.CO stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current canonical approach to publishing cross-section data is to unfold\nthe reconstructed distributions. Detector effects like efficiency and smearing\nare undone mathematically, yielding distributions in true event properties.\nThis is an ill-posed problem, as even small statistical variations in the\nreconstructed data can lead to large changes in the unfolded spectra.\n  This work presents an alternative or complementary approach: the\nresponse-matrix-centred forward-folding approach. It offers a convenient way to\nforward-fold model expectations in truth space to reconstructed quantities.\nThese can then be compared to the data directly, similar to what is usually\ndone with full detector simulations within the experimental collaborations. For\nthis, the detector response (efficiency and smearing) is parametrised as a\nmatrix. The effects of the detector on the measurement of a given model is\nsimulated by simply multiplying the binned truth expectation values by this\nresponse matrix.\n  Systematic uncertainties in the detector response are handled by providing a\nset of matrices according to the prior distribution of the detector properties\nand marginalising over them. Background events can be included in the\nlikelihood calculation by giving background events their own bins in truth\nspace.\n  To facilitate a straight-forward use of response matrices, a new software\nframework has been developed: the Response Matrix Utilities (ReMU). ReMU is a\nPython package distributed via the Python Package Index. It only uses widely\navailable, standard scientific Python libraries and does not depend on any\ncustom experiment-specific software. It offers all methods needed to build\nresponse matrices from Monte Carlo data sets, use the response matrix to\nforward-fold truth-level model predictions, and compare the predictions to real\ndata using Bayesian or frequentist statistical inference.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 14:19:32 GMT"}, {"version": "v2", "created": "Thu, 21 Mar 2019 13:05:54 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Koch", "Lukas", ""]]}, {"id": "1903.06625", "submitter": "Thomas Schnake", "authors": "Thomas Schnake, David Bauer", "title": "Synthesis of High-Resolution Load Profiles with Minimal Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the estimation of a new energy supply system it is an important to have\nhigh-resolution energy load profile. Such a profile is in general either not\npresent or very costly to obtain. We will therefore present a method which\nsynthesizes load profiles from minimal given data, but with maximal resolution.\nThe general initial data setting includes month integrals and load profiles a\nfew days. The resulting time series features all important properties to\nrepresent a real energy profile.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 09:36:31 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Schnake", "Thomas", ""], ["Bauer", "David", ""]]}, {"id": "1903.08880", "submitter": "John Galati", "authors": "John C. Galati", "title": "Three issues impeding communication of statistical methodology for\n  incomplete data", "comments": "6 pages, no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We identify three issues permeating the literature on statistical methodology\nfor incomplete data written for non-specialist statisticians and other\ninvestigators. The first is a mathematical defect in the notation Yobs, Ymis\nused to partition the data into observed and missing components. The second are\nissues concerning the notation `P(R|Yobs, Ymis)=P(R|Yobs)' used for\ncommunicating the definition of missing at random (MAR). And the third is the\nframing of ignorability by emulating complete-data methods exactly, rather than\ntreating the question of ignorability on its own merits. These issues have been\npresent in the literature for a long time, and have simple remedies. The\npurpose of this paper is to raise awareness of these issues, and to explain how\nthey can be remedied.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 08:56:57 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 09:23:01 GMT"}, {"version": "v3", "created": "Thu, 28 Mar 2019 22:20:19 GMT"}, {"version": "v4", "created": "Tue, 16 Apr 2019 03:09:33 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Galati", "John C.", ""]]}, {"id": "1903.10816", "submitter": "Thomas Pitschel", "authors": "Thomas Pitschel", "title": "Deterministic bootstrapping for a class of bootstrap methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An algorithm is described that enables efficient deterministic approximate\ncomputation of the bootstrap distribution for any linear bootstrap method\n$T_n^*$, alleviating the need for repeated resampling from observations (resp.\ninput-derived data). In essence, the algorithm computes the distribution\nfunction from a linear mixture of independent random variables each having a\nfinite discrete distribution. The algorithm is applicable to elementary\nbootstrap scenarios (targetting the mean as parameter of interest), for block\nbootstrap, as well as for certain residual bootstrap scenarios. Moreover, the\nalgorithm promises a much broader applicability, in non-bootstrapped hypothesis\ntesting.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 11:53:37 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 17:14:00 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Pitschel", "Thomas", ""]]}, {"id": "1903.11565", "submitter": "Christos Skiadas H", "authors": "Christos H Skiadas and Charilaos Skiadas", "title": "Modeling the Health Expenditure in Japan, 2011. A Healthy Life Years\n  Lost Methodology", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Healthy Life Years Lost Methodology (HLYL) is introduced to model and\nestimate the Health Expenditure in Japan in 2011. The HLYL theory and\nestimation methods are presented in our books in the Springer Series on\nDemographic Methods and Population Analysis vol. 45 and 46 titled: Exploring\nthe Health State of a Population by Dynamic Modeling Methods and Demography and\nHealth Issues: Population Aging, Mortality and Data Analysis. Special\napplications appear in Chapters of these books as in The Health-Mortality\nApproach in Estimating the Healthy Life Years Lost Compared to the Global\nBurden of Disease Studies and Applications in World, USA and Japan and in\nEstimation of the Healthy Life Expectancy in Italy Through a Simple Model Based\non Mortality Rate by Skiadas and Arezzo. Here further to present the main part\nof the methodology with more details and illustrations, we develop and extend a\nlife table important to estimate the healthy life years lost along with the\nfitting to the health expenditure in the related case. The application results\nare quite promising and important to support decision makers and health\nagencies with a powerful tool to improve the health expenditure allocation and\nthe future predictions.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 03:33:05 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Skiadas", "Christos H", ""], ["Skiadas", "Charilaos", ""]]}, {"id": "1903.12423", "submitter": "Helene Flourent", "authors": "H\\'el\\`ene Flourent (LMBA), Emmanuel Fr\\'enod (LMBA), Vincent\n  Sincholle", "title": "An innovating Statistical Learning Tool based on Partial Differential\n  Equations, intending livestock Data Assimilation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The realistic modeling intended to quantify precisely some biological\nmechanisms is a task requiering a lot of a priori knowledge and generally\nleading to heavy mathematical models. On the other hand, the structure of the\nclassical Machine Learning algorithms, such as Neural Networks, limits their\nflexibility and the possibility to take into account the existence of complex\nunderlying phenomena, such as delay, saturation and accumulation. The aim of\nthis paper is to reach a compromise between precision, parsimony and\nflexibility to design an efficient biomimetic predictive tool extracting\nknowledge from livestock data. To achieve this, we build a Mathematical Model\nbased on Partial Differential Equations (PDE) embarking the mathematical\nexpression of biological determinants. We made the hypothesis that all the\nphysico-chemical phenomena occurring in animal body can be summarized by the\nevolution of a global information. Therefore the developed PDE system describes\nthe evolution and the action of an information circulating in an Avatar of the\nReal Animal. This Avatar outlines the dynamics of the biological reactions of\nanimal body in the framework of a specific problem. Each PDE contains\nparameters corresponding to biological-like factors which can be learnt from\ndata by the developed Statistical Learning Tool.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 09:55:11 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 07:23:15 GMT"}, {"version": "v3", "created": "Wed, 8 Jan 2020 11:03:52 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Flourent", "H\u00e9l\u00e8ne", "", "LMBA"], ["Fr\u00e9nod", "Emmanuel", "", "LMBA"], ["Sincholle", "Vincent", ""]]}]