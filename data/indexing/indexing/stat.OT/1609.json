[{"id": "1609.00494", "submitter": "Carl Bergstrom", "authors": "Silas B. Nissen, Tali Magidson, Kevin Gross, and Carl T. Bergstrom", "title": "Publication bias and the canonization of false facts", "comments": null, "journal-ref": "eLife (2016), 5, e21451", "doi": "10.7554/eLife.21451", "report-no": null, "categories": "physics.soc-ph stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the process of scientific inquiry, certain claims accumulate enough\nsupport to be established as facts. Unfortunately, not every claim accorded the\nstatus of fact turns out to be true. In this paper, we model the dynamic\nprocess by which claims are canonized as fact through repeated experimental\nconfirmation. The community's confidence in a claim constitutes a Markov\nprocess: each successive published result shifts the degree of belief, until\nsufficient evidence accumulates to accept the claim as fact or to reject it as\nfalse. In our model, publication bias --- in which positive results are\npublished preferentially over negative ones --- influences the distribution of\npublished results. We find that when readers do not know the degree of\npublication bias and thus cannot condition on it, false claims often can be\ncanonized as facts. Unless a sufficient fraction of negative results are\npublished, the scientific process will do a poor job at discriminating false\nfrom true claims. This problem is exacerbated when scientists engage in\np-hacking, data dredging, and other behaviors that increase the rate at which\nfalse positives are published. If negative results become easier to publish as\na claim approaches acceptance as a fact, however, true and false claims can be\nmore readily distinguished. To the degree that the model accurately represents\ncurrent scholarly practice, there will be serious concern about the validity of\npurported facts in some areas of scientific research.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2016 08:02:43 GMT"}, {"version": "v2", "created": "Sun, 20 Nov 2016 23:46:18 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Nissen", "Silas B.", ""], ["Magidson", "Tali", ""], ["Gross", "Kevin", ""], ["Bergstrom", "Carl T.", ""]]}, {"id": "1609.00711", "submitter": "Philipp Otto", "authors": "Philipp Otto and Wolfgang Schmid and Robert Garthoff", "title": "Generalized Spatial and Spatiotemporal Autoregressive Conditional\n  Heteroscedasticity", "comments": null, "journal-ref": null, "doi": "10.1016/j.spasta.2018.07.005", "report-no": null, "categories": "math.ST stat.AP stat.ME stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new spatial model that incorporates\nheteroscedastic variance depending on neighboring locations. The proposed\nprocess is regarded as the spatial equivalent to the temporal autoregressive\nconditional heteroscedasticity (ARCH) model. We show additionally how the\nintroduced spatial ARCH model can be used in spatiotemporal settings. In\ncontrast to the temporal ARCH model, in which the distribution is known given\nthe full information set of the prior periods, the distribution is not\nstraightforward in the spatial and spatiotemporal setting. However, it is\npossible to estimate the parameters of the model using the maximum-likelihood\napproach. Via Monte Carlo simulations, we demonstrate the performance of the\nestimator for a specific spatial weighting matrix. Moreover, we combine the\nknown spatial autoregressive model with the spatial ARCH model assuming\nheteroscedastic errors. Eventually, the proposed autoregressive process is\nillustrated using an empirical example. Specifically, we model lung cancer\nmortality in 3108 U.S. counties and compare the introduced model with two\nbenchmark approaches.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2016 19:38:23 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Otto", "Philipp", ""], ["Schmid", "Wolfgang", ""], ["Garthoff", "Robert", ""]]}, {"id": "1609.04383", "submitter": "David Sharrow", "authors": "David J. Sharrow, Jessica Godwin, Yanjun He, Samuel J. Clark, Adrian\n  E. Raftery", "title": "Probabilistic Population Projections for Countries with Generalized\n  HIV/AIDS Epidemics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The United Nations (UN) issued official probabilistic population projections\nfor all countries to 2100 in July 2015. This was done by simulating future\nlevels of total fertility and life expectancy from Bayesian hierarchical\nmodels, and combining the results using a standard cohort-component projection\nmethod. The 40 countries with generalized HIV/AIDS epidemics were treated\ndifferently from others, in that the projections used the highly multistate\nSpectrum/EPP model, a complex 15-compartment model that was designed for\nshort-term projections of quantities relevant to policy for the epidemic. Here\nwe propose a simpler approach that is more compatible with the existing UN\nprobabilistic projection methodology for other countries. Changes in life\nexpectancy are projected probabilistically using a simple time series\nregression model on current life expectancy, HIV prevalence and ART coverage.\nThese are then converted to age- and sex-specific mortality rates using a new\nfamily of model life tables designed for countries with HIV/AIDS epidemics that\nreproduces the characteristic hump in middle adult mortality. These are then\ninput to the standard cohort-component method, as for other countries. The\nmethod performed well in an out-of-sample cross-validation experiment. It gives\nsimilar population projections to Spectrum/EPP in the short run, while being\nsimpler and avoiding multistate modeling.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 19:11:03 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Sharrow", "David J.", ""], ["Godwin", "Jessica", ""], ["He", "Yanjun", ""], ["Clark", "Samuel J.", ""], ["Raftery", "Adrian E.", ""]]}, {"id": "1609.04478", "submitter": "Yaakov Malinovsky", "authors": "Yaakov Malinovsky", "title": "Sterrett Procedure for the Generalized Group Testing Problem", "comments": "Submitted for publication. Revised", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group testing is a useful method that has broad applications in medicine,\nengineering, and even in airport security control. Consider a finite population\nof $N$ items, where item $i$ has a probability $p_i$ to be defective. The goal\nis to identify all items by means of group testing. This is the generalized\ngroup testing problem. The optimum procedure, with respect to the expected\ntotal number of tests, is unknown even in case when all $p_i$ are equal.\n\\cite{H1975} proved that an ordered partition (with respect to $p_i$) is the\noptimal for the Dorfman procedure (procedure $D$), and obtained an optimum\nsolution (i.e., found an optimal partition) by dynamic programming. In this\npaper, we investigate the Sterrett procedure (procedure $S$). We provide close\nform expression for the expected total number of tests, which allows us to find\nthe optimum arrangement of the items in the particular group. We also show that\nan ordered partition is not optimal for the procedure $S$ or even for a\nslightly modified Dorfman procedure (procedure $D^{\\prime}$). This discovery\nimplies that finding an optimal procedure $S$ appears to be a hard\ncomputational problem. However, by using an optimal ordered partition for all\nprocedures, we show that procedure $D^{\\prime}$ is uniformly better than\nprocedure $D$, and based on numerical comparisons, procedure $S$ is uniformly\nand significantly better than procedures $D$ and $D^{\\prime}$.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 00:15:02 GMT"}, {"version": "v2", "created": "Tue, 20 Sep 2016 14:56:26 GMT"}, {"version": "v3", "created": "Thu, 13 Apr 2017 19:18:38 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Malinovsky", "Yaakov", ""]]}, {"id": "1609.05551", "submitter": "Johannes Lederer", "authors": "Rui Zhuang, Noah Simon, and Johannes Lederer", "title": "Graphical Models for Discrete and Continuous Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a general framework for undirected graphical models. It\ngeneralizes Gaussian graphical models to a wide range of continuous, discrete,\nand combinations of different types of data. The models in the framework,\ncalled exponential trace models, are amenable to estimation based on maximum\nlikelihood. We introduce a sampling-based approximation algorithm for computing\nthe maximum likelihood estimator, and we apply this pipeline to learn\nsimultaneous neural activities from spike data.\n", "versions": [{"version": "v1", "created": "Sun, 18 Sep 2016 21:21:39 GMT"}, {"version": "v2", "created": "Sun, 27 Aug 2017 18:48:37 GMT"}, {"version": "v3", "created": "Sat, 15 Jun 2019 12:24:57 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Zhuang", "Rui", ""], ["Simon", "Noah", ""], ["Lederer", "Johannes", ""]]}]