[{"id": "2008.00315", "submitter": "Mine \\c{C}etinkaya-Rundel", "authors": "Mine \\c{C}etinkaya-Rundel and Victoria Ellison", "title": "A fresh look at introductory data science", "comments": null, "journal-ref": null, "doi": "10.1080/10691898.2020.1804497", "report-no": null, "categories": "stat.OT cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The proliferation of vast quantities of available datasets that are large and\ncomplex in nature has challenged universities to keep up with the demand for\ngraduates trained in both the statistical and the computational set of skills\nrequired to effectively plan, acquire, manage, analyze, and communicate the\nfindings of such data. To keep up with this demand, attracting students early\non to data science as well as providing them a solid foray into the field\nbecomes increasingly important. We present a case study of an introductory\nundergraduate course in data science that is designed to address these needs.\nOffered at Duke University, this course has no pre-requisites and serves a wide\naudience of aspiring statistics and data science majors as well as humanities,\nsocial sciences, and natural sciences students. We discuss the unique set of\nchallenges posed by offering such a course and in light of these challenges, we\npresent a detailed discussion into the pedagogical design elements, content,\nstructure, computational infrastructure, and the assessment methodology of the\ncourse. We also offer a repository containing all teaching materials that are\nopen-source, along with supplemental materials and the R code for reproducing\nthe figures found in the paper.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 18:39:34 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["\u00c7etinkaya-Rundel", "Mine", ""], ["Ellison", "Victoria", ""]]}, {"id": "2008.02048", "submitter": "Lolian Shtembari", "authors": "Lolian Shtembari and Allen Caldwell", "title": "On the sum of ordered spacings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide the analytic forms of the distributions for the sum of ordered\nspacings. We do this both for the case where the boundaries are included in the\ncalculation of the spacings and the case where they are excluded. Both the\nprobability densities as well as their cumulatives are provided. These results\nwill have useful applications in the physical sciences and possibly elsewhere.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 11:12:06 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Shtembari", "Lolian", ""], ["Caldwell", "Allen", ""]]}, {"id": "2008.03122", "submitter": "Fabio Priuli", "authors": "Fabio S. Priuli, Claudia Violante", "title": "Sulla decifratura di Enigma -- Come un reverendo del XVIII secolo\n  contribu\\`i alla sconfitta degli U-boot tedeschi durante la Seconda Guerra\n  Mondiale", "comments": "39 pages, 10 figures, in Italian", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GL cs.CR stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article, written in Italian language, explores the contribution given by\nBayes' rule and by subjective probability in the work at Bletchley Park towards\ncracking Enigma cyphered messages during WWII.\n  --\n  In questo articolo, scritto in Italiano, esploriamo il contributo dato dal\nteorema di Bayes e dalle idee della probabilit\\`a soggettiva nel lavoro\ncompiuto a Bletchley Park che ha portato a decifrare i messaggi cifrati con\nmacchine Enigma durante la Seconda Guerra Mondiale.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 05:04:37 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Priuli", "Fabio S.", ""], ["Violante", "Claudia", ""]]}, {"id": "2008.03256", "submitter": "Alexis Comber", "authors": "Chris Brunsdon and Alexis Comber", "title": "Opening practice: supporting Reproducibility and Critical spatial data\n  science", "comments": "19 pages, 1 figure", "journal-ref": "Journal of Geographical Science, 2020", "doi": null, "report-no": null, "categories": "stat.OT cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper reflects on a number of trends towards a more open and\nreproducible approach to geographic and spatial data science over recent years.\nIn particular it considers trends towards Big Data, and the impacts this is\nhaving on spatial data analysis and modelling. It identifies a turn in academia\ntowards coding as a core analytic tool, and away from proprietary software\ntools offering 'black boxes' where the internal workings of the analysis are\nnot revealed. It is argued that this closed form software is problematic, and\nconsiders a number of ways in which issues identified in spatial data analysis\n(such as the MAUP) could be overlooked when working with closed tools, leading\nto problems of interpretation and possibly inappropriate actions and policies\nbased on these. In addition, this paper and considers the role that\nreproducible and open spatial science may play in such an approach, taking into\naccount the issues raised. It highlights the dangers of failing to account for\nthe geographical properties of data, now that all data are spatial (they are\ncollected somewhere), the problems of a desire for n=all observations in data\nscience and it identifies the need for a critical approach. This is one in\nwhich openness, transparency, sharing and reproducibility provide a mantra for\ndefensible and robust spatial data science.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 07:50:08 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Brunsdon", "Chris", ""], ["Comber", "Alexis", ""]]}, {"id": "2008.04475", "submitter": "Mar\\'ia Fernanda Gil-Leyva Villa", "authors": "Mar\\'ia F. Gil-Leyva, Rams\\'es H. Mena", "title": "Stick-breaking processes with exchangeable length variables", "comments": "Accepted for publication by the Journal of the American Statistical\n  Association. 44 pages, 11 figures, supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our object of study is the general class of stick-breaking processes with\nexchangeable length variables. These generalize well-known Bayesian\nnon-parametric priors in an unexplored direction. We give conditions to assure\nthe respective species sampling process is proper and the corresponding prior\nhas full support. For a rich sub-class we explain how, by tuning a single\n$[0,1]$-valued parameter, the stochastic ordering of the weights can be\nmodulated, and Dirichlet and Geometric priors can be recovered. A general\nformula for the distribution of the latent allocation variables is derived and\nan MCMC algorithm is proposed for density estimation purposes.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 02:00:04 GMT"}, {"version": "v2", "created": "Sun, 18 Jul 2021 07:09:07 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Gil-Leyva", "Mar\u00eda F.", ""], ["Mena", "Rams\u00e9s H.", ""]]}, {"id": "2008.05109", "submitter": "Xingchen Yu", "authors": "Xingchen Yu, Abel Rodriguez", "title": "A Bayesian Approach to Spherical Factor Analysis for Binary Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factor models are widely used across diverse areas of application for\npurposes that include dimensionality reduction, covariance estimation, and\nfeature engineering. Traditional factor models can be seen as an instance of\nlinear embedding methods that project multivariate observations onto a lower\ndimensional Euclidean latent space. This paper discusses a new class of\ngeometric embedding models for multivariate binary data in which the embedding\nspace correspond to a spherical manifold, with potentially unknown dimension.\nThe resulting models include traditional factor models as a special case, but\nprovide additional flexibility. Furthermore, unlike other techniques for\ngeometric embedding, the models are easy to interpret, and the uncertainty\nassociated with the latent features can be properly quantified. These\nadvantages are illustrated using both simulation studies and real data on\nvoting records from the U.S. Senate.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 04:55:18 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Yu", "Xingchen", ""], ["Rodriguez", "Abel", ""]]}, {"id": "2008.06709", "submitter": "Julio Stern", "authors": "Julio Michael Stern, Marcos Antonio Simplicio, Marcos Vinicius M.\n  Silva, Roberto A. Castellanos Pfeiffer", "title": "Randomization and Fair Judgment in Law and Science", "comments": "p.399-418 in Jose Acacio de Barros and Decio Krause, eds. (2020). A\n  True Polymath: A Tribute to Francisco Antonio Doria. Rickmansworth, UK:\n  College Publications. ISBN: 978-1-84890-351-7", "journal-ref": null, "doi": null, "report-no": "ISBN: 978-1-84890-351-7", "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomization procedures are used in legal and statistical applications,\naiming to shield important decisions from spurious influences. This article\ngives an intuitive introduction to randomization and examines some intended\nconsequences of its use related to truthful statistical inference and fair\nlegal judgment. This article also presents an open-code Java implementation for\na cryptographically secure, statistically reliable, transparent, traceable, and\nfully auditable randomization tool.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2020 12:00:38 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 23:24:39 GMT"}, {"version": "v3", "created": "Tue, 24 Nov 2020 12:43:20 GMT"}, {"version": "v4", "created": "Fri, 18 Dec 2020 20:08:54 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Stern", "Julio Michael", ""], ["Simplicio", "Marcos Antonio", ""], ["Silva", "Marcos Vinicius M.", ""], ["Pfeiffer", "Roberto A. Castellanos", ""]]}, {"id": "2008.06755", "submitter": "David Leslie", "authors": "David Leslie", "title": "Tackling COVID-19 through Responsible AI Innovation: Five Steps in the\n  Right Direction", "comments": "Harvard Data Science Review (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Innovations in data science and AI/ML have a central role to play in\nsupporting global efforts to combat COVID-19. The versatility of AI/ML\ntechnologies enables scientists and technologists to address an impressively\nbroad range of biomedical, epidemiological, and socioeconomic challenges. This\nwide-reaching scientific capacity, however, also raises a diverse array of\nethical challenges. The need for researchers to act quickly and globally in\ntackling SARS-CoV-2 demands unprecedented practices of open research and\nresponsible data sharing at a time when innovation ecosystems are hobbled by\nproprietary protectionism, inequality, and a lack of public trust. Moreover,\nsocietally impactful interventions like digital contact tracing are raising\nfears of surveillance creep and are challenging widely held commitments to\nprivacy, autonomy, and civil liberties. Prepandemic concerns that data-driven\ninnovations may function to reinforce entrenched dynamics of societal inequity\nhave likewise intensified given the disparate impact of the virus on vulnerable\nsocial groups and the life-and-death consequences of biased and discriminatory\npublic health outcomes. To address these concerns, I offer five steps that need\nto be taken to encourage responsible research and innovation. These provide a\npractice-based path to responsible AI/ML design and discovery centered on open,\naccountable, equitable, and democratically governed processes and products.\nWhen taken from the start, these steps will not only enhance the capacity of\ninnovators to tackle COVID-19 responsibly, they will, more broadly, help to\nbetter equip the data science and AI/ML community to cope with future pandemics\nand to support a more humane, rational, and just society.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2020 17:26:48 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Leslie", "David", ""]]}, {"id": "2008.06869", "submitter": "Ralph Foorthuis", "authors": "Ralph Foorthuis", "title": "SECODA: Segmentation- and Combination-Based Detection of Anomalies", "comments": "12 pages (including DSAA conference poster), 9 figures, 3 tables.\n  Presented at DSAA 2017, the IEEE International Conference on Data Science and\n  Advanced Analytics", "journal-ref": null, "doi": "10.1109/DSAA.2017.35", "report-no": null, "categories": "cs.DB cs.AI cs.LG stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study introduces SECODA, a novel general-purpose unsupervised\nnon-parametric anomaly detection algorithm for datasets containing continuous\nand categorical attributes. The method is guaranteed to identify cases with\nunique or sparse combinations of attribute values. Continuous attributes are\ndiscretized repeatedly in order to correctly determine the frequency of such\nvalue combinations. The concept of constellations, exponentially increasing\nweights and discretization cut points, as well as a pruning heuristic are used\nto detect anomalies with an optimal number of iterations. Moreover, the\nalgorithm has a low memory imprint and its runtime performance scales linearly\nwith the size of the dataset. An evaluation with simulated and real-life\ndatasets shows that this algorithm is able to identify many different types of\nanomalies, including complex multidimensional instances. An evaluation in terms\nof a data quality use case with a real dataset demonstrates that SECODA can\nbring relevant and practical value to real-world settings.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 10:03:14 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Foorthuis", "Ralph", ""]]}, {"id": "2008.07478", "submitter": "Akisato Suzuki Dr", "authors": "Akisato Suzuki", "title": "Presenting the Probabilities of Different Effect Sizes: Towards a Better\n  Understanding and Communication of Statistical Uncertainty", "comments": "working paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How should social scientists understand and communicate the uncertainty of\nstatistically estimated causal effects? It is well-known that the conventional\nsignificance-vs.-insignificance approach is associated with misunderstandings\nand misuses. Behavioral research suggests people understand uncertainty more\nappropriately in a numerical, continuous scale than in a verbal, discrete\nscale. Motivated by these backgrounds, I propose presenting the probabilities\nof different effect sizes. Probability as an intuitive continuous measure of\nuncertainty allows researchers to better understand and communicate the\nuncertainty of the statistically estimated effects. In addition, my approach\nneeds no decision threshold for an uncertainty measure or effect size, unlike\nthe conventional approaches, allowing researchers to be agnostic about a\ndecision threshold such as p<5% and a justification for that. I apply my\napproach to a previous social scientific study, showing it enables richer\ninference than the significance-vs.-insignificance approach taken by the\noriginal study. The accompanying R package makes my approach easy to implement.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 17:09:53 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Suzuki", "Akisato", ""]]}, {"id": "2008.07840", "submitter": "Marcos Matabuena", "authors": "Marcos Matabuena, Alexander Petersen, Juan C.Vidal and Francisco Gude", "title": "Glucodensities: a new representation of glucose profiles using\n  distributional data analysis", "comments": null, "journal-ref": null, "doi": "10.1177/0962280221998064", "report-no": null, "categories": "stat.AP q-bio.QM stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Biosensor data has the potential ability to improve disease control and\ndetection. However, the analysis of these data under free-living conditions is\nnot feasible with current statistical techniques. To address this challenge, we\nintroduce a new functional representation of biosensor data, termed the\nglucodensity, together with a data analysis framework based on distances\nbetween them. The new data analysis procedure is illustrated through an\napplication in diabetes with continuous-time glucose monitoring (CGM) data. In\nthis domain, we show marked improvement with respect to state of the art\nanalysis methods. In particular, our findings demonstrate that i) the\nglucodensity possesses an extraordinary clinical sensitivity to capture the\ntypical biomarkers used in the standard clinical practice in diabetes, ii)\nprevious biomarkers cannot accurately predict glucodensity, so that the latter\nis a richer source of information, and iii) the glucodensity is a natural\ngeneralization of the time in range metric, this being the gold standard in the\nhandling of CGM data. Furthermore, the new method overcomes many of the\ndrawbacks of time in range metrics, and provides deeper insight into assessing\nglucose metabolism.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 10:26:15 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Matabuena", "Marcos", ""], ["Petersen", "Alexander", ""], ["Vidal", "Juan C.", ""], ["Gude", "Francisco", ""]]}, {"id": "2008.11813", "submitter": "Chris Dent", "authors": "Chris J Dent, Michael Goldstein, Andrew Wright, Henry P. Wynn", "title": "The use of multiple models within an organisation", "comments": "49 pages. White paper arising from Alan Turing Institute project", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Organisations, whether in government, industry or commerce, are required to\nmake decisions in a complex and uncertain environment. The way models are used\nis intimately connected to the way organisations make decisions and the context\nin which they make them. Typically, in a complex organisation, multiple related\nmodels will often be used in support of a decision. For example, engineering\nmodels might be combined with financial models and macro-economic models in\norder to decide whether to invest in new production capability. Different parts\nof a complex organisation might operate their own related models which might\nthen be presented to a central decision maker. Yet in practice, there is little\nawareness of the practical challenges of using models in a robust way to\nsupport decision making. There is significant scope to improve decision making\nthough an enhanced understanding of the role and limitations of modelling and\nthrough the application of cutting edge methodologies and organisational best\npractice. This report is in the form of a 'white paper', whose purpose is to\nidentify key issues for consideration whist postulating tentative approaches to\nthese issues that might be worthy of further exploration, focussing on both\ntechnical and organisational aspects. It begins with a framework for\nconsideration of how model-based decisions are made in organisations. It then\nlooks more closely at the questions of uncertainty and multiple models. It then\npostulates some technical statistical and organisational approaches for\nmanaging some of these issues. Finally, it considers the way forward, and the\npossible focus for further work.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 16:54:54 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Dent", "Chris J", ""], ["Goldstein", "Michael", ""], ["Wright", "Andrew", ""], ["Wynn", "Henry P.", ""]]}, {"id": "2008.13637", "submitter": "Andrew Tzer-Yeu Chen", "authors": "Matthias Kaiser, Andrew Tzer-Yeu Chen, Peter Gluckman", "title": "Should policy makers trust composite indices? A commentary on the\n  pitfalls of inappropriate indices for policy formation", "comments": "19 pages, 2 tables, accepted for publication by Health Research\n  Policy and Systems", "journal-ref": null, "doi": "10.1186/s12961-021-00702-4", "report-no": null, "categories": "cs.CY stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper critically discusses the use and merits of global indices, in\nparticular, the Global Health Security Index or GHSI (Cameron et 2019) in times\nof an imminent crisis, like the current pandemic. The index ranked 195\ncountries according to their expected preparedness in case of a pandemic or\nother biological threat. The Covid-19 pandemic provides the background to\ncompare each country's predicted performance from the GHSI with the actual\nperformance. In general, there is an inverted relation between predicted versus\nactual performance, i.e. the predicted top performers are among those that are\nthe worst hit. Obviously, this reflects poorly on the potential policy uses of\nthe index in imminent crisis management. The paper also uses two different data\nsets, one from the Worldmeter on the spread of the Covid-19 pandemics, and the\nother one from the INGSA policy tracker, to make comparisons between the actual\nintroduction of pandemic response policies and the corresponding death rate in\n29 selected countries.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 14:18:45 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 19:31:46 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Kaiser", "Matthias", ""], ["Chen", "Andrew Tzer-Yeu", ""], ["Gluckman", "Peter", ""]]}]