[{"id": "2010.02205", "submitter": "Yudi Pawitan", "authors": "Yudi Pawitan and Arvid Sj\\\"olander", "title": "Dealing with multiple testing: To adjust or not to adjust", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple testing problems arise naturally in scientific studies because of\nthe need to capture or convey more information with more variables. The\nliterature is enormous, but the emphasis is primarily methodological, providing\nnumerous methods with their mathematical justification and practical\nimplementation. Our aim is to highlight the logical issues involved in the\napplication of multiple testing adjustment.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 10:53:09 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Pawitan", "Yudi", ""], ["Sj\u00f6lander", "Arvid", ""]]}, {"id": "2010.02211", "submitter": "Yudi Pawitan", "authors": "Yudi Pawitan", "title": "Likelihood-based solution to the Monty Hall puzzle and a related\n  3-prisoner paradox", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Monty Hall puzzle has been solved and dissected in many ways, but always\nusing probabilistic arguments, so it is considered a probability puzzle. In\nthis paper the puzzle is set up as an orthodox statistical problem involving an\nunknown parameter, a probability model and an observation. This means we can\ncompute a likelihood function, and the decision to switch corresponds to\nchoosing the maximum likelihood solution. One advantage of the likelihood-based\nsolution is that the reasoning applies to a single game, unaffected by the\nfuture plan of the host. I also describe an earlier version of the puzzle in\nterms of three prisoners: two to be executed and one released. Unlike the goats\nand the car, these prisoners have consciousness, so they can think about\nexchanging punishments. When two of them do that, however, we have a paradox,\nwhere it is advantageous for both to exchange their punishment with each other.\nOverall, the puzzle and the paradox are useful examples of statistical\nthinking, so they are excellent teaching topics.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 10:30:14 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Pawitan", "Yudi", ""]]}, {"id": "2010.03827", "submitter": "Maria D. Ruiz-Medina", "authors": "Torres-Signes, M.P. Fr\\'ias, J. Mateu and M.D. Ruiz-Medina", "title": "A spatial functional count model for heterogeneity analysis in time", "comments": "37 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A spatial curve dynamical model framework is adopted for functional\nprediction of counts in a spatiotemporal log-Gaussian Cox process model. Our\nspatial functional estimation approach handles both wavelet-based heterogeneity\nanalysis in time, and spectral analysis in space. Specifically, model fitting\nis achieved by minimising the information divergence or relative entropy\nbetween the multiscale model underlying the data and the corresponding\ncandidates in the spatial spectral domain. A simulation study is carried out\nwithin the family of log-Gaussian Spatial Autoregressive l2-valued processes\n(SARl2 processes) to illustrate the asymptotic properties of the proposed\nspatial functional estimators. We apply our modelling strategy to\nspatiotemporal prediction of respiratory disease mortality.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 08:03:33 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Torres-Signes", "", ""], ["Fr\u00edas", "M. P.", ""], ["Mateu", "J.", ""], ["Ruiz-Medina", "M. D.", ""]]}, {"id": "2010.06698", "submitter": "Joshua Hunte", "authors": "Joshua Hunte, Martin Neil, Norman Fenton", "title": "Product risk assessment: a Bayesian network approach", "comments": "32 pages, 12 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT cs.AI stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Product risk assessment is the overall process of determining whether a\nproduct, which could be anything from a type of washing machine to a type of\nteddy bear, is judged safe for consumers to use. There are several methods used\nfor product risk assessment, including RAPEX, which is the primary method used\nby regulators in the UK and EU. However, despite its widespread use, we\nidentify several limitations of RAPEX including a limited approach to handling\nuncertainty and the inability to incorporate causal explanations for using and\ninterpreting test data. In contrast, Bayesian Networks (BNs) are a rigorous,\nnormative method for modelling uncertainty and causality which are already used\nfor risk assessment in domains such as medicine and finance, as well as\ncritical systems generally. This article proposes a BN model that provides an\nimproved systematic method for product risk assessment that resolves the\nidentified limitations with RAPEX. We use our proposed method to demonstrate\nrisk assessments for a teddy bear and a new uncertified kettle for which there\nis no testing data and the number of product instances is unknown. We show\nthat, while we can replicate the results of the RAPEX method, the BN approach\nis more powerful and flexible.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 16:40:03 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Hunte", "Joshua", ""], ["Neil", "Martin", ""], ["Fenton", "Norman", ""]]}, {"id": "2010.07017", "submitter": "Christopher Wild", "authors": "Wesley Burr, Fanny Chevalier, Christopher Collins, Alison L Gibbs,\n  Raymond Ng, Chris Wild", "title": "Computational Skills by Stealth in Secondary School Data Science", "comments": "38 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The unprecedented growth in the availability of data of all types and\nqualities and the emergence of the field of data science has provided an\nimpetus to finally realizing the implementation of the full breadth of the\nNolan and Temple Lang proposed integration of computing concepts into\nstatistics curricula at all levels in statistics and new data science programs\nand courses. Moreover, data science, implemented carefully, opens accessible\npathways to stem for students for whom neither mathematics nor computer science\nare natural affinities, and who would traditionally be excluded. We discuss a\nproposal for the stealth development of computational skills in students' first\nexposure to data science through careful, scaffolded exposure to computation\nand its power. The intent of this approach is to support students, regardless\nof interest and self-efficacy in coding, in becoming data-driven learners, who\nare capable of asking complex questions about the world around them, and then\nanswering those questions through the use of data-driven inquiry. This\ndiscussion is presented in the context of the International Data Science in\nSchools Project which recently published computer science and statistics\nconsensus curriculum frameworks for a two-year secondary school data science\nprogram, designed to make data science accessible to all.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 09:11:51 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Burr", "Wesley", ""], ["Chevalier", "Fanny", ""], ["Collins", "Christopher", ""], ["Gibbs", "Alison L", ""], ["Ng", "Raymond", ""], ["Wild", "Chris", ""]]}, {"id": "2010.08104", "submitter": "Michael Nelson", "authors": "Michael C. Nelson", "title": "On statistical deficiency: Why the test statistic of the matching method\n  is hopelessly underpowered and uniquely informative", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The random variate m is, in combinatorics, a basis for comparing\npermutations, as well as the solution to a centuries-old riddle involving the\nmishandling of hats. In statistics, m is the test statistic for a disused null\nhypothesis statistical test (NHST) of association, the matching method. In this\npaper, I show that the matching method has an absolute and relatively low limit\non its statistical power. I do so first by reinterpreting Rae's theorem, which\ndescribes the joint distributions of m with several rank correlation statistics\nunder a true null. I then derive this property solely from m's unconditional\nsampling distribution, on which basis I develop the concept of a deficient\nstatistic: a statistic that is insufficient and inconsistent and inefficient\nwith respect to its parameter. Finally, I demonstrate an application for m that\nmakes use of its deficiency to qualify the sampling error in a jointly\nestimated sample correlation.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 02:21:15 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Nelson", "Michael C.", ""]]}, {"id": "2010.08317", "submitter": "Matheus Saldanha", "authors": "Matheus Henrique Junqueira Saldanha and Adriano Kamimura Suzuki", "title": "Methods to Deal with Unknown Populational Minima during Parameter\n  Inference", "comments": "Submitted to Springer's Statistics and Computing. 14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a myriad of phenomena that are better modelled with semi-infinite\ndistribution families, many of which are studied in survival analysis. When\nperforming inference, lack of knowledge of the populational minimum becomes a\nproblem, which can be dealt with by making a good guess thereof, or by\nhandcrafting a grid of initial parameters that will be useful for that\nparticular problem. These solutions are fine when analyzing a single set of\nsamples, but it becomes unfeasible when there are multiple datasets and a\ncase-by-case analysis would be too time consuming. In this paper we propose\nmethods to deal with the populational minimum in algorithmic, efficient and/or\nsimple ways. Six methods are presented and analyzed, two of which have full\ntheoretical support, but lack simplicity. The other four are simple and have\nsome theoretical grounds in non-parametric results such as the law of iterated\nlogarithm, and they exhibited very good results when it comes to maximizing\nlikelihood and being able to recycle the grid of initial parameters among the\ndatasets. With our results, we hope to ease the inference process for\npractitioners, and expect that these methods will eventually be included in\nsoftware packages themselves.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 11:15:18 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Saldanha", "Matheus Henrique Junqueira", ""], ["Suzuki", "Adriano Kamimura", ""]]}, {"id": "2010.09563", "submitter": "Andreas Markoulidakis Mr", "authors": "Andreas Markoulidakis, Khadijeh Taiyari, Peter Holmans, Philip\n  Pallmann, Monica Busse, Mark D. Godley, Beth Ann Griffin", "title": "A tutorial comparing different covariate balancing methods with an\n  application evaluating the causal effects of substance use treatment programs\n  for adolescents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized controlled trials are the gold standard for measuring causal\neffects. However, they are often not always feasible, and causal treatment\neffects must be estimated from observational data. Observational studies do not\nallow robust conclusions about causal relationships unless statistical\ntechniques account for the imbalance of pretreatment confounders across groups\nwhile key assumptions hold. Propensity score and balance weighting (PSBW) are\nuseful techniques that aim to reduce the imbalances between treatment groups by\nweighting the groups to look alike on the observed confounders. There are many\nmethods available to estimate PSBW. However, it is unclear a priori which will\nachieve the best trade-off between covariate balance and effective sample size.\nMoreover, it is critical to assess the validity of key assumptions required for\nrobust estimation of the needed treatment effects, including the overlap and no\nunmeasured confounding assumptions. We present a step-by-step guide to\ncovariate balancing strategies, including how to evaluate overlap, obtain\nestimates of PSBW, check for covariate balance, and assess sensitivity to\nunobserved confounding. We compare the performance of several estimation\nmethods using a case study examining the relative effectiveness of substance\nuse treatment programs and provide a user-friendly web application that can\nimplement the proposed steps.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 14:47:28 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 09:41:34 GMT"}, {"version": "v3", "created": "Sat, 15 May 2021 14:58:57 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Markoulidakis", "Andreas", ""], ["Taiyari", "Khadijeh", ""], ["Holmans", "Peter", ""], ["Pallmann", "Philip", ""], ["Busse", "Monica", ""], ["Godley", "Mark D.", ""], ["Griffin", "Beth Ann", ""]]}, {"id": "2010.10513", "submitter": "Mark Rubin", "authors": "Mark Rubin", "title": "Does preregistration improve the credibility of research findings?", "comments": null, "journal-ref": "The Quantitative Methods in Psychology, 16(4), 376-390 (2020)", "doi": "10.20982/tqmp.16.4.p376", "report-no": null, "categories": "stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Preregistration entails researchers registering their planned research\nhypotheses, methods, and analyses in a time-stamped document before they\nundertake their data collection and analyses. This document is then made\navailable with the published research report to allow readers to identify\ndiscrepancies between what the researchers originally planned to do and what\nthey actually ended up doing. This historical transparency is supposed to\nfacilitate judgments about the credibility of the research findings. The\npresent article provides a critical review of 17 of the reasons behind this\nargument. The article covers issues such as HARKing, multiple testing,\np-hacking, forking paths, optional stopping, researchers' biases, selective\nreporting, test severity, publication bias, and replication rates. It is\nconcluded that preregistration's historical transparency does not facilitate\njudgments about the credibility of research findings when researchers provide\ncontemporary transparency in the form of (a) clear rationales for current\nhypotheses and analytical approaches, (b) public access to research data,\nmaterials, and code, and (c) demonstrations of the robustness of research\nconclusions to alternative interpretations and analytical approaches.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 05:23:53 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Rubin", "Mark", ""]]}, {"id": "2010.12095", "submitter": "Farzan Shenavarmasouleh", "authors": "Farzan Shenavarmasouleh, Hamid R. Arabnia", "title": "Causes of Misleading Statistics and Research Results Irreproducibility:\n  A Concise Review", "comments": null, "journal-ref": "2019 International Conference on Computational Science and\n  Computational Intelligence (CSCI), (2019), 465-470, ISBN:978-1-7281-5585-2", "doi": "10.1109/CSCI49370.2019.00090", "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bad statistics make research papers unreproducible and misleading. For the\nmost part, the reasons for such misusage of numerical data have been found and\naddressed years ago by experts and proper practical solutions have been\npresented instead. Yet, we still see numerous instances of statistical\nfallacies in modern researches which without a doubt play a significant role in\nthe research reproducibility crisis. In this paper, we review different bad\npractices that impact the research process from its beginning to its very end.\nAdditionally, we briefly propose open science as a universal methodology that\ncan facilitate the entire research life cycle.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 22:39:42 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Shenavarmasouleh", "Farzan", ""], ["Arabnia", "Hamid R.", ""]]}, {"id": "2010.14555", "submitter": "Peng Ding", "authors": "Anqi Zhao, Peng Ding", "title": "Covariate-adjusted Fisher randomization tests for the average treatment\n  effect", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fisher's randomization test (FRT) delivers exact $p$-values under the strong\nnull hypothesis of no treatment effect on any units whatsoever and allows for\nflexible covariate adjustment to improve the power. Of interest is whether the\nprocedure could also be valid for testing the weak null hypothesis of zero\naverage treatment effect. Towards this end, we evaluate two general strategies\nfor FRT with covariate-adjusted test statistics: that based on the residuals\nfrom an outcome model with only the covariates, and that based on the output\nfrom an outcome model with both the treatment and the covariates. Based on\ntheory and simulation, we recommend using the ordinary least squares (OLS) fit\nof the observed outcome on the treatment, centered covariates, and their\ninteractions for covariate adjustment, and conducting FRT with the robust\n$t$-value of the treatment as the test statistic. The resulting FRT is\nfinite-sample exact for the strong null hypothesis, asymptotically valid for\nthe weak null hypothesis, and more powerful than the unadjusted analog under\nalternatives, all irrespective of whether the linear model is correctly\nspecified or not. We develop the theory for complete randomization, cluster\nrandomization, stratified randomization, and rerandomization, respectively, and\ngive a recommendation for the test procedure and test statistic under each\ndesign. We first focus on the finite-population perspective and then extend the\nresult to the super-population perspective, highlighting the difference in\nstandard errors. Motivated by the similarity in procedure, we also evaluate the\ndesign-based properties of five existing permutation tests originally for\nlinear models and show the superiority of the proposed FRT for testing the\ntreatment effects.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 18:51:43 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 20:41:13 GMT"}, {"version": "v3", "created": "Thu, 29 Apr 2021 22:39:57 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Zhao", "Anqi", ""], ["Ding", "Peng", ""]]}, {"id": "2010.16025", "submitter": "Rushani Wijesuriya", "authors": "Rushani Wijesuriya, Margarita Moreno-Betancur, John B. Carlin, Anurika\n  P. De Silva and Katherine J. Lee", "title": "Evaluation of approaches for accommodating interactions and non-linear\n  terms in multiple imputation of incomplete three-level data", "comments": "34 pages, 5 tables and 9 figures (without additional files)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three-level data structures arising from repeated measures on individuals\nclustered within larger units are common in health research studies. Missing\ndata are prominent in such studies and are often handled via multiple\nimputation (MI). Although several MI approaches can be used to account for the\nthree-level structure, including adaptations to single- and two-level\napproaches, when the substantive analysis model includes interactions or\nquadratic effects these too need to be accommodated in the imputation model. In\nsuch analyses, substantive model compatible (SMC) MI has shown great promise in\nthe context of single-level data. While there have been recent developments in\nmultilevel SMC MI, to date only one approach that explicitly handles incomplete\nthree-level data is available. Alternatively, researchers can use pragmatic\nadaptations to single- and two-level MI approaches, or two-level SMC-MI\napproaches. We describe the available approaches and evaluate them via\nsimulation in the context of a three three-level random effects analysis models\ninvolving an interaction between the incomplete time-varying exposure and time,\nan interaction between the time-varying exposure and an incomplete time-fixed\nconfounder, or a quadratic effect of the exposure. Results showed that all\napproaches considered performed well in terms of bias and precision when the\ntarget analysis involved an interaction with time, but the three-level SMC MI\napproach performed best when the target analysis involved an interaction\nbetween the time-varying exposure and an incomplete time-fixed confounder, or a\nquadratic effect of the exposure. We illustrate the methods using data from the\nChildhood to Adolescence Transition Study.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 02:23:12 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Wijesuriya", "Rushani", ""], ["Moreno-Betancur", "Margarita", ""], ["Carlin", "John B.", ""], ["De Silva", "Anurika P.", ""], ["Lee", "Katherine J.", ""]]}]