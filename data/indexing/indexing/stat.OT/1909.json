[{"id": "1909.00061", "submitter": "John Sun", "authors": "John Sun, Christopher S. Wang, Ellie S. Krossa", "title": "Investigating Sprawl using AIC and Recursive Partitioning Trees: A\n  Machine Learning Approach to Assessing the Association between Poverty and\n  Commute Time", "comments": "35 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sprawl, according to Glaeser and Kahn, is the 21st century phenomenon that\nsome people are not dependent on city-living due to automobiles and therefore\ncan live outside public transportation spheres and cities. This is usually seen\nas pleasant and accompanied by improved qualities of life, but as they\naddressed, the problem remains that sprawl causes loss of jobs for those who\ncannot afford luxurious alternatives but only inferior substitutes (Glaeser and\nKahn 2004). Therefore, through our question, we hope to suggest that sprawl has\noccurred in the U.S. and poverty is one of the consequences.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 20:35:44 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 20:09:24 GMT"}, {"version": "v3", "created": "Thu, 5 Dec 2019 16:29:33 GMT"}, {"version": "v4", "created": "Thu, 13 Feb 2020 13:31:02 GMT"}, {"version": "v5", "created": "Sat, 27 Feb 2021 17:51:42 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Sun", "John", ""], ["Wang", "Christopher S.", ""], ["Krossa", "Ellie S.", ""]]}, {"id": "1909.00225", "submitter": "Hanshen Xiao", "authors": "Hanshen Xiao, Nan Du, Zhikang T. Wang and Guoqiang Xiao", "title": "Statistical Robust Chinese Remainder Theorem for Multiple Numbers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized Chinese Remainder Theorem (CRT) is a well-known approach to solve\nambiguity resolution related problems. In this paper, we study the robust CRT\nreconstruction for multiple numbers from a view of statistics. To the best of\nour knowledge, it is the first rigorous analysis on the underlying statistical\nmodel of CRT-based multiple parameter estimation. To address the problem, two\nnovel approaches are established. One is to directly calculate a conditional\nmaximum a posteriori probability (MAP) estimation of the residue clustering,\nand the other is based on a generalized wrapped Gaussian mixture model to\niteratively search for MAP of both estimands and clustering. Residue error\ncorrecting codes are introduced to improve the robustness further. Experimental\nresults show that the statistical schemes achieve much stronger robustness\ncompared to state-of-the-art deterministic schemes, especially in heavy-noise\nscenarios.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 14:31:45 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Xiao", "Hanshen", ""], ["Du", "Nan", ""], ["Wang", "Zhikang T.", ""], ["Xiao", "Guoqiang", ""]]}, {"id": "1909.01219", "submitter": "Simone Camosso M.S.", "authors": "Simone Camosso", "title": "The maximum likelihood degree of a chemical reaction at the equilibrium", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complexity of a maximum likelihood estimation is measured by its maximum\nlikelihood degree ($ML$ degree). In this paper we study the maximum likelihood\nproblem associated to chemical networks composed by one single chemical\nreaction under the equilibrium assumption.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 08:26:30 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Camosso", "Simone", ""]]}, {"id": "1909.02282", "submitter": "Flavio Santi", "authors": "Giuseppe Arbia, Maria Michela Dickson, Giuseppe Espa, Diego Giuliani,\n  Flavio Santi", "title": "Reduced-bias estimation of spatial econometric models with incompletely\n  geocoded data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of state-of-the-art spatial econometric models requires that\nthe information about the spatial coordinates of statistical units is\ncompletely accurate, which is usually the case in the context of areal data.\nWith micro-geographic point-level data, however, such information is inevitably\naffected by locational errors, that can be generated intentionally by the data\nproducer for privacy protection or can be due to inaccuracy of the geocoding\nprocedures. This unfortunate circumstance can potentially limit the use of the\nspatial econometric modelling framework for the analysis of micro data. Indeed,\nsome recent contributions (see e.g. Arbia, Espa and Giuliani 2016) have shown\nthat the presence of locational errors may have a non-negligible impact on the\nresults. In particular, wrong spatial coordinates can lead to downward bias and\nincreased variance in the estimation of model parameters. This contribution\naims at developing a strategy to reduce the bias and produce more reliable\ninference for spatial econometrics models with location errors. The validity of\nthe proposed approach is assessed by means of a Monte Carlo simulation study\nunder different real-case scenarios. The study results show that the method is\npromising and can make the spatial econometric modelling of micro-geographic\ndata possible.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 09:33:34 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Arbia", "Giuseppe", ""], ["Dickson", "Maria Michela", ""], ["Espa", "Giuseppe", ""], ["Giuliani", "Diego", ""], ["Santi", "Flavio", ""]]}, {"id": "1909.02989", "submitter": "Luca Rossini", "authors": "Luciana Dalla Valle, Fabrizio Leisen, Luca Rossini, Weixuan Zhu", "title": "A P\\'olya-Gamma Sampler for a Generalized Logistic Regression", "comments": "Revised Version of the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a novel Bayesian data augmentation approach for\nestimating the parameters of the generalised logistic regression model. We\npropose a P\\'olya-Gamma sampler algorithm that allows us to sample from the\nexact posterior distribution, rather than relying on approximations. A\nsimulation study illustrates the flexibility and accuracy of the proposed\napproach to capture heavy and light tails in binary response data of different\ndimensions. The methodology is applied to two different real datasets, where we\ndemonstrate that the P\\'olya-Gamma sampler provides more precise estimates than\nthe empirical likelihood method, outperforming approximate approaches.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 16:07:07 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 08:23:43 GMT"}, {"version": "v3", "created": "Mon, 21 Dec 2020 07:43:22 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Valle", "Luciana Dalla", ""], ["Leisen", "Fabrizio", ""], ["Rossini", "Luca", ""], ["Zhu", "Weixuan", ""]]}, {"id": "1909.03433", "submitter": "Xialiang Dou", "authors": "Xialiang Dou, Mihai Anitescu", "title": "Distributionally Robust Optimization with Correlated Data from Vector\n  Autoregressive Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.CO stat.ML stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a distributionally robust formulation of a stochastic optimization\nproblem for non-i.i.d vector autoregressive data. We use the Wasserstein\ndistance to define robustness in the space of distributions and we show, using\nduality theory, that the problem is equivalent to a finite convex-concave\nsaddle point problem. The performance of the method is demonstrated on both\nsynthetic and real data.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 11:30:04 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Dou", "Xialiang", ""], ["Anitescu", "Mihai", ""]]}, {"id": "1909.03540", "submitter": "Hamid Eftekhari", "authors": "Hamid Eftekhari, Moulinath Banerjee, Ya'acov Ritov", "title": "Inference In High-dimensional Single-Index Models Under Symmetric\n  Designs", "comments": "The current version was published in the Journal of Machine Learning\n  Research, Volume 22. It is available online at\n  https://jmlr.org/papers/v22/19-744.html", "journal-ref": "Journal of Machine Learning Research, 22(27), 1-63 (2021)", "doi": null, "report-no": null, "categories": "math.ST stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of statistical inference for regression coefficients in a\nhigh-dimensional single-index model is considered. Under elliptical symmetry,\nthe single index model can be reformulated as a proxy linear model whose\nregression parameter is identifiable. We construct estimates of the regression\ncoefficients of interest that are similar to the debiased lasso estimates in\nthe standard linear model and exhibit similar properties: root-n-consistency\nand asymptotic normality. The procedure completely bypasses the estimation of\nthe unknown link function, which can be extremely challenging depending on the\nunderlying structure of the problem. Furthermore, under Gaussianity, we propose\nmore efficient estimates of the coefficients by expanding the link function in\nthe Hermite polynomial basis. Finally, we illustrate our approach via carefully\ndesigned simulation experiments.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 20:22:08 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 20:44:08 GMT"}, {"version": "v3", "created": "Sat, 27 Feb 2021 18:39:39 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Eftekhari", "Hamid", ""], ["Banerjee", "Moulinath", ""], ["Ritov", "Ya'acov", ""]]}, {"id": "1909.03784", "submitter": "Mahendra Saha", "authors": "Harsh Tripathi and Mahendra Saha", "title": "A new approach of chain sampling inspection plan", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To develop decision rules regarding acceptance or rejection of production\nlots based on sample data is the purpose of acceptance sampling inspection\nplan. Dependent sampling procedures cumulate results from several preceding\nproduction lots when testing is expensive or destructive. This chaining of past\nlots reduce the sizes of the required samples, essential for acceptance or\nrejection of production lots. In this article, a new approach for chaining the\npast lot(s) results proposed, named as modified chain group acceptance sampling\ninspection plan, requires a smaller sample size than the commonly used sampling\ninspection plan, such as group acceptance sampling inspection plan and single\nacceptance sampling inspection plan. A comparison study has been done between\nthe proposed and group acceptance sampling inspection plan as well as single\nacceptance sampling inspection plan. A example has been given to illustrate the\nproposed plan in a good manner.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 12:00:13 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Tripathi", "Harsh", ""], ["Saha", "Mahendra", ""]]}, {"id": "1909.03813", "submitter": "Alessandro Gasparini", "authors": "Alessandro Gasparini and Tim P. Morris and Michael J. Crowther", "title": "INTEREST: INteractive Tool for Exploring REsults from Simulation sTudies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation studies allow us to explore the properties of statistical methods.\nThey provide a powerful tool with a multiplicity of aims; among others:\nevaluating and comparing new or existing statistical methods, assessing\nviolations of modelling assumptions, helping with the understanding of\nstatistical concepts, and supporting the design of clinical trials. The\nincreased availability of powerful computational tools and usable software has\ncontributed to the rise of simulation studies in the current literature.\nHowever, simulation studies involve increasingly complex designs, making it\ndifficult to provide all relevant results clearly. Dissemination of results\nplays a focal role in simulation studies: it can drive applied analysts to use\nmethods that have been shown to perform well in their settings, guide\nresearchers to develop new methods in a promising direction, and provide\ninsights into less established methods. It is crucial that we can digest\nrelevant results of simulation studies. Therefore, we developed INTEREST: an\nINteractive Tool for Exploring REsults from Simulation sTudies. The tool has\nbeen developed using the Shiny framework in R and is available as a web app or\nas a standalone package. It requires uploading a tidy format dataset with the\nresults of a simulation study in R, Stata, SAS, SPSS, or comma-separated\nformat. A variety of performance measures are estimated automatically along\nwith Monte Carlo standard errors; results and performance summaries are\ndisplayed both in tabular and graphical fashion, with a wide variety of\navailable plots. Consequently, the reader can focus on simulation parameters\nand estimands of most interest. In conclusion, INTEREST can facilitate the\ninvestigation of results from simulation studies and supplement the reporting\nof results, allowing researchers to share detailed results from their\nsimulations and readers to explore them freely.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 12:47:43 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 13:24:30 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Gasparini", "Alessandro", ""], ["Morris", "Tim P.", ""], ["Crowther", "Michael J.", ""]]}, {"id": "1909.04323", "submitter": "Hao Lin", "authors": "Qi Zhou and Hao Lin", "title": "Investigating the completeness and omission roads of OpenStreetMap data\n  in Hubei, China by comparing with Street Map and Street View", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OpenStreetMap (OSM) is a free map of the world which can be edited by global\nvolunteers. Existing studies have showed that completeness of OSM road data in\nsome developing countries (e.g. China) is much lower, resulting in concern in\nutilizing the data in various applications. But very few have focused on\ninvestigating what types of road are still poorly mapped. This study aims not\nonly to investigate the completeness of OSM road datasets in China but also to\ninvestigate what types of road (called omission roads) have not been mapped,\nwhich is achieved by referring to both Street Map and Street View. 16\nprefecture-level divisions in the urban areas of Hubei (China) were used as\nstudy areas. Results showed that: (1) the completeness for most\nprefecture-level divisions was at a low-to-medium level; most roads (in the\nStreet Map), however, with traffic conditions had already been mapped well. (2)\nMost of the omission OSM roads were either private roads, or public roads not\nhaving yet been named and with only one single lane, indicating their lack of\nimportance in the urban road network. We argue that although the OSM road\ndatasets in China are incomplete, they may still be used for several\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 07:03:34 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Zhou", "Qi", ""], ["Lin", "Hao", ""]]}, {"id": "1909.04486", "submitter": "Yovaninna Alarc\\'on-Soto", "authors": "Yovaninna Alarc\\'on-Soto, Jenifer Espasand\\'in-Dom\\'inguez, Ipek\n  Guler, Mercedes Conde-Amboage, Francisco Gude-Sampedro, Klaus Langohr, Carmen\n  Cadarso-Su\\'arez, Guadalupe G\\'omez-Melis", "title": "Data Science in Biomedicine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We highlight the role of Data Science in Biomedicine. Our manuscript goes\nfrom the general to the particular, presenting a global definition of Data\nScience and showing the trend for this discipline together with the terms of\ncloud computing and big data. In addition, since Data Science is mostly related\nto areas like economy or business, we describe its importance in biomedicine.\nBiomedical Data Science (BDS) presents the challenge of dealing with data\ncoming from a range of biological and medical research, focusing on\nmethodologies to advance the biomedical science discoveries, in an\ninterdisciplinary context.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 11:31:40 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Alarc\u00f3n-Soto", "Yovaninna", ""], ["Espasand\u00edn-Dom\u00ednguez", "Jenifer", ""], ["Guler", "Ipek", ""], ["Conde-Amboage", "Mercedes", ""], ["Gude-Sampedro", "Francisco", ""], ["Langohr", "Klaus", ""], ["Cadarso-Su\u00e1rez", "Carmen", ""], ["G\u00f3mez-Melis", "Guadalupe", ""]]}, {"id": "1909.06523", "submitter": "Olav Benjamin Vassend", "authors": "Olav Benjamin Vassend", "title": "Justifying the Norms of Inductive Inference", "comments": "This is a draft version of the paper \"Justifying the Norms of\n  Inductive Inference,\" which has been accepted for publication by the British\n  Journal for the Philosophy of Science. Please cite the final published\n  version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference is limited in scope because it cannot be applied in\nidealized contexts where none of the hypotheses under consideration is true and\nbecause it is committed to always using the likelihood as a measure of\nevidential favoring, even when that is inappropriate. The purpose of this paper\nis to study inductive inference in a very general setting where finding the\ntruth is not necessarily the goal and where the measure of evidential favoring\nis not necessarily the likelihood. I use an accuracy argument to argue for\nprobabilism and I develop a new kind of argument to argue for two general\nupdating rules, both of which are reasonable in different contexts. One of the\nupdating rules has standard Bayesian updating, Bissiri et al's (2016) general\nBayesian updating, Douven's (2016) IBE-based updating, and Vassend's (2019a)\nquasi-Bayesian updating as special cases. The other updating rule is novel.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 03:47:36 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Vassend", "Olav Benjamin", ""]]}, {"id": "1909.06990", "submitter": "Pieter Breur", "authors": "P.A. Breur, J.C.P.Y. Nobelen, L. Baudis, A. Brown, A.P. Colijn, R.\n  Dressler, R.F. Lang, A. Massafferri, C. Pumar, C. Reuter, D. Schumann, M.\n  Schumann, S. Towers, R. Perci", "title": "Testing claims of the GW170817 binary neutron star inspiral affecting\n  $\\beta$-decay rates", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": "10.1016/j.astropartphys.2020.102431", "report-no": null, "categories": "nucl-ex stat.OT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  On August 17, 2017, the first gravitational wave signal from a binary neutron\nstar inspiral (GW170817) was detected by Advanced LIGO and Advanced VIRGO. Here\nwe present radioactive $\\beta$-decay rates of three independent sources\n$^{44}$Ti, $^{60}$Co and $^{137}$Cs, monitored during the same period by a\nprecision experiment designed to investigate the decay of long-lived\nradioactive sources. We do not find any significant correlations between decay\nrates in a 5\\,h time interval following the GW170817 observation. This\ncontradicts a previous claim published in this journal of an observed\n2.5$\\sigma$ Pearson Correlation between fluctuations in the number of observed\ndecays from two $\\beta$-decaying isotopes ($^{32}$Si and $^{36}$Cl) in the same\ntime interval. By correcting for the choice of an arbitrary time interval, we\nfind no evidence of a correlation above 1.5$\\sigma$ confidence. In addition, we\nargue that such analyses on correlations in arbitrary time intervals should\nalways correct for the so-called Look-Elsewhere effect by quoting the global\nsignificance.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 05:16:00 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Breur", "P. A.", ""], ["Nobelen", "J. C. P. Y.", ""], ["Baudis", "L.", ""], ["Brown", "A.", ""], ["Colijn", "A. P.", ""], ["Dressler", "R.", ""], ["Lang", "R. F.", ""], ["Massafferri", "A.", ""], ["Pumar", "C.", ""], ["Reuter", "C.", ""], ["Schumann", "D.", ""], ["Schumann", "M.", ""], ["Towers", "S.", ""], ["Perci", "R.", ""]]}, {"id": "1909.07026", "submitter": "Julyan Arbel", "authors": "Julyan Arbel, Olivier Marchal, Bernardo Nipoti", "title": "On the Hurwitz zeta function with an application to the beta-exponential\n  distribution", "comments": "8 pages, 1 figure", "journal-ref": "Journal of Inequalities and Applications, 2020", "doi": null, "report-no": null, "categories": "math.ST stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove a monotonicity property of the Hurwitz zeta function which, in turn,\ntranslates into a chain of inequalities for polygamma functions of different\norders. We provide a probabilistic interpretation of our result by exploiting a\nconnection between Hurwitz zeta function and the cumulants of the\nbeta-exponential distribution.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 07:13:05 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 15:05:28 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Arbel", "Julyan", ""], ["Marchal", "Olivier", ""], ["Nipoti", "Bernardo", ""]]}, {"id": "1909.09138", "submitter": "Jennie Brand", "authors": "Jennie E. Brand, Jiahui Xu, Bernard Koch, and Pablo Geraldo", "title": "Uncovering Sociological Effect Heterogeneity using Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Individuals do not respond uniformly to treatments, events, or interventions.\nSociologists routinely partition samples into subgroups to explore how the\neffects of treatments vary by covariates like race, gender, and socioeconomic\nstatus. In so doing, analysts determine the key subpopulations based on\ntheoretical priors. Data-driven discoveries are also routine, yet the analyses\nby which sociologists typically go about them are problematic and seldom move\nus beyond our expectations, and biases, to explore new meaningful subgroups.\nEmerging machine learning methods allow researchers to explore sources of\nvariation that they may not have previously considered, or envisaged. In this\npaper, we use causal trees to recursively partition the sample and uncover\nsources of treatment effect heterogeneity. We use honest estimation, splitting\nthe sample into a training sample to grow the tree and an estimation sample to\nestimate leaf-specific effects. Assessing a central topic in the social\ninequality literature, college effects on wages, we compare what we learn from\nconventional approaches for exploring variation in effects to causal trees.\nGiven our use of observational data, we use leaf-specific matching and\nsensitivity analyses to address confounding and offer interpretations of\neffects based on observed and unobserved heterogeneity. We encourage\nresearchers to follow similar practices in their work on variation in\nsociological effects.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 19:09:17 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Brand", "Jennie E.", ""], ["Xu", "Jiahui", ""], ["Koch", "Bernard", ""], ["Geraldo", "Pablo", ""]]}, {"id": "1909.09155", "submitter": "Rodrigo Labouriau", "authors": "Gauss M. Cordeiro, Rodrigo Labouriau, Denise A. Botter", "title": "An introduction to Bent Jorgensen's ideas", "comments": "26 pages, submitted and accepted to the Brazilian Journal of\n  Probability and Statistics (\n  https://www.imstat.org/journals-and-publications/brazilian-journal-of-probability-and-statistics/\n  ). In the version v3 we corrected some typos and small errors in the section\n  on the construction of DMs", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We briefly expose some key aspects of the theory and use of dispersion\nmodels, for which Bent Jorgensen played a crucial role as a driving force and\nan inspiration source. Starting with the general notion of dispersion models,\nbuilt using minimalistic mathematical assumptions, we specialize in two classes\nof families of distributions with different statistical flavors: exponential\ndispersion and proper dispersion models. The construction of dispersion models\ninvolves the solution of integral equations that are, in general, untractable.\nThese difficulties disappear when a more mathematical structure is assumed: it\nreduces to the calculation of a moment generating function or of a\nRiemann-Stieltjes integral for the exponential dispersion and the proper\ndispersion models, respectively. A new technique for constructing dispersion\nmodels based on characteristic functions is introduced turning the integral\nequations above into a tractable convolution equation and yielding examples of\ndispersion models that are neither proper dispersion nor exponential dispersion\nmodels. A corollary is that the cardinality of regular and non-regular\ndispersion models are both large.\n  Some selected applications are discussed including exponential families\nnon-linear models (for which generalized linear models are particular cases)\nand several models for clustered and dependent data based on a latent Levy\nprocess.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 13:23:54 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 13:54:45 GMT"}, {"version": "v3", "created": "Tue, 25 Aug 2020 10:50:53 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Cordeiro", "Gauss M.", ""], ["Labouriau", "Rodrigo", ""], ["Botter", "Denise A.", ""]]}, {"id": "1909.12313", "submitter": "Joshua Speagle", "authors": "Joshua S. Speagle", "title": "A Conceptual Introduction to Markov Chain Monte Carlo Methods", "comments": "54 pages, 15 figures. Comments and feedback always appreciated", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT astro-ph.IM physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Chain Monte Carlo (MCMC) methods have become a cornerstone of many\nmodern scientific analyses by providing a straightforward approach to\nnumerically estimate uncertainties in the parameters of a model using a\nsequence of random samples. This article provides a basic introduction to MCMC\nmethods by establishing a strong conceptual understanding of what problems MCMC\nmethods are trying to solve, why we want to use them, and how they work in\ntheory and in practice. To develop these concepts, I outline the foundations of\nBayesian inference, discuss how posterior distributions are used in practice,\nexplore basic approaches to estimate posterior-based quantities, and derive\ntheir link to Monte Carlo sampling and MCMC. Using a simple toy problem, I then\ndemonstrate how these concepts can be used to understand the benefits and\ndrawbacks of various MCMC approaches. Exercises designed to highlight various\nconcepts are also included throughout the article.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 18:01:14 GMT"}, {"version": "v2", "created": "Sat, 7 Mar 2020 21:41:35 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Speagle", "Joshua S.", ""]]}, {"id": "1909.13186", "submitter": "S{\\o}ren Wengel Mogensen", "authors": "S{\\o}ren Wengel Mogensen", "title": "Causal screening for dynamical systems", "comments": "13 pages, 3 figures", "journal-ref": "Proceedings of the 36th Conference on Uncertainty in Artificial\n  Intelligence (UAI), PMLR volume 124, 2020", "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many classical algorithms output graphical representations of causal\nstructures by testing conditional independence among a set of random variables.\nIn dynamical systems, local independence can be used analogously as a testable\nimplication of the underlying data-generating process. We suggest some\ninexpensive methods for causal screening which provide output with a sound\ncausal interpretation under the assumption of ancestral faithfulness. The\npopular model class of linear Hawkes processes is used to provide an example of\na dynamical causal model. We argue that for sparse causal graphs the output\nwill often be close to complete. We give examples of this framework and apply\nit to a challenging biological system.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 02:00:54 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2020 08:24:03 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Mogensen", "S\u00f8ren Wengel", ""]]}]