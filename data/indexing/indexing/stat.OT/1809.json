[{"id": "1809.00377", "submitter": "Fariba Khoshnasib-Zeinabad", "authors": "Fariba Khoshnasib-Zeinabad and Mohammadhossein Mehrabi", "title": "On Some Integral Means", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Harmonic, Geometric, Arithmetic, Heronian and Contraharmonic means have been\nstudied by many mathematicians. In 2003, H. Evens studied these means from\ngeometrical point of view and established some of the inequalities between them\nin using a circle and its radius. In 1961, E. Beckenback and R. Bellman\nintroduced several inequalities corresponding to means. In this paper, we will\nintroduce the concept of mean functions and integral means and give bounds on\nsome of these mean functions and integral means.\n", "versions": [{"version": "v1", "created": "Sun, 2 Sep 2018 19:31:05 GMT"}, {"version": "v2", "created": "Fri, 3 Jan 2020 05:25:41 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Khoshnasib-Zeinabad", "Fariba", ""], ["Mehrabi", "Mohammadhossein", ""]]}, {"id": "1809.01489", "submitter": "Thiago dos Santos Rezende", "authors": "T. R. Santos", "title": "A Bayesian GED-Gamma stochastic volatility model for return data: a\n  marginal likelihood approach", "comments": "26 pages, 5 figures and 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several studies explore inferences based on stochastic volatility (SV)\nmodels, taking into account the stylized facts of return data. The common\nproblem is that the latent parameters of many volatility models are\nhigh-dimensional and analytically intractable, which means inferences require\napproximations using, for example, the Markov Chain Monte Carlo or Laplace\nmethods. Some SV models are expressed as a linear Gaussian state-space model\nthat leads to a marginal likelihood, reducing the dimensionality of the\nproblem. Others are not linearized, and the latent parameters are integrated\nout. However, these present a quite restrictive evolution equation. Thus, we\npropose a Bayesian GED-Gamma SV model with a direct marginal likelihood that is\na product of the generalized Student's t-distributions in which the latent\nstates are related across time through a stationary Gaussian evolution\nequation. Then, an approximation is made for the prior distribution of\nlog-precision/volatility, without the need for model linearization. This also\nallows for the computation of the marginal likelihood function, where the\nhigh-dimensional latent states are integrated out and easily sampled in blocks\nusing a smoothing procedure. In addition, extensions of our GED-Gamma model are\neasily made to incorporate skew heavy-tailed distributions. We use the Bayesian\nestimator for the inference of static parameters, and perform a simulation\nstudy on several properties of the estimator. Our results show that the\nproposed model can be reasonably estimated. Furthermore, we provide case\nstudies of a Brazilian asset and the pound/dollar exchange rate to show the\nperformance of our approach in terms of fit and prediction.\n  Keywords: SV model, New sequential and smoothing procedures, Generalized\nStudent's t-distribution, Non-Gaussian errors, Heavy tails, Skewness\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 16:59:09 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Santos", "T. R.", ""]]}, {"id": "1809.01501", "submitter": "Arthur Tarso Rego", "authors": "Arthur T. Rego and Thiago R. dos Santos", "title": "Non-Gaussian Stochastic Volatility Model with Jumps via Gibbs Sampler", "comments": "27 pages, 12 figures, 5 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST q-fin.CP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a model for estimating volatility from financial\ntime series, extending the non-Gaussian family of space-state models with exact\nmarginal likelihood proposed by Gamerman, Santos and Franco (2013). On the\nliterature there are models focused on estimating financial assets risk,\nhowever, most of them rely on MCMC methods based on Metropolis algorithms,\nsince full conditional posterior distributions are not known. We present an\nalternative model capable of estimating the volatility, in an automatic way,\nsince all full conditional posterior distributions are known, and it is\npossible to obtain an exact sample of parameters via Gibbs Sampler. The\nincorporation of jumps in returns allows the model to capture speculative\nmovements of the data, so that their influence does not propagate to\nvolatility. We evaluate the performance of the algorithm using synthetic and\nreal data time series.\n  Keywords: Financial time series, Stochastic volatility, Gibbs Sampler,\nDynamic linear models.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 14:53:28 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2018 21:39:23 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Rego", "Arthur T.", ""], ["Santos", "Thiago R. dos", ""]]}, {"id": "1809.02952", "submitter": "Nicholas Horton", "authors": "Sarah McDonald and Nicholas Jon Horton", "title": "Data scraping, ingestation, and modeling: bringing data from cars.com\n  into the intro stats class", "comments": "in press, CHANCE", "journal-ref": null, "doi": "10.1080/09332480.2019.1695443", "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New tools have made it much easier for students to develop skills to work\nwith interesting data sets as they begin to extract meaning from data. To fully\nappreciate the statistical analysis cycle, students benefit from repeated\nexperiences collecting, ingesting, wrangling, analyzing data and communicating\nresults. How can we bring such opportunities into the classroom? We describe a\nclassroom activity, originally developed by Danny Kaplan (Macalester College),\nin which students can expand upon statistical problem solving by hand-scraping\ndata from cars.com, ingesting these data into R, then carrying out analyses of\nthe relationships between price, mileage, and model year for a selected type of\ncar.\n", "versions": [{"version": "v1", "created": "Sun, 9 Sep 2018 11:01:46 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["McDonald", "Sarah", ""], ["Horton", "Nicholas Jon", ""]]}, {"id": "1809.03561", "submitter": "Florian Ziel", "authors": "Florian Ziel", "title": "Quantile Regression for Qualifying Match of GEFCom2017 Probabilistic\n  Load Forecasting", "comments": "accepted for International Journal of Forecasting", "journal-ref": "International Journal of Forecasting, 35.4 (2019) 1400-1408", "doi": "10.1016/j.ijforecast.2018.07.004", "report-no": null, "categories": "stat.AP stat.CO stat.ME stat.ML stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple quantile regression-based forecasting method that was\napplied in a probabilistic load forecasting framework of the Global Energy\nForecasting Competition 2017 (GEFCom2017). The hourly load data is log\ntransformed and split into a long-term trend component and a remainder term.\nThe key forecasting element is the quantile regression approach for the\nremainder term that takes into account weekly and annual seasonalities such as\ntheir interactions. Temperature information is only used to stabilize the\nforecast of the long-term trend component. Public holidays information is\nignored. Still, the forecasting method placed second in the open data track and\nfourth in the definite data track with our forecasting method, which is\nremarkable given simplicity of the model. The method also outperforms the\nVanilla benchmark consistently.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 19:31:15 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Ziel", "Florian", ""]]}, {"id": "1809.04338", "submitter": "Sam Doerken", "authors": "Sam Doerken and Martin Schumacher and Franz Baumdicker", "title": "Game time: statistical contests in the classroom", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a contest in variable selection which was part of a statistics\ncourse for graduate students. In particular, the possibility to create a\ncontest themselves offered an additional challenge for more advanced students.\nSince working with data is becoming more important in teaching statistics, we\ngreatly encourage other instructors to try the same.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 10:04:57 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Doerken", "Sam", ""], ["Schumacher", "Martin", ""], ["Baumdicker", "Franz", ""]]}, {"id": "1809.04721", "submitter": "Naomi Brownstein", "authors": "Naomi C Brownstein", "title": "Perspective from the Literature on the Role of Expert Judgment in\n  Scientific and Statistical Research and Practice", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article, produced as a result of the Symposium on Statistical Inference,\nis an introduction to the literature on the function of expertise, judgment,\nand choice in the practice of statistics and scientific research. In\nparticular, expert judgment plays a critical role in conducting Frequentist\nhypothesis tests and Bayesian models, especially in selection of appropriate\nprior distributions for model parameters. The subtlety of interpreting results\nis also discussed. Finally, external recommendations are collected for how to\nmore effectively encourage proper use of judgment in statistics. The paper\nsynthesizes the literature for the purpose of creating a single reference and\ninciting more productive discussions on how to improve the future of statistics\nand science.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 00:27:47 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Brownstein", "Naomi C", ""]]}, {"id": "1809.05731", "submitter": "Daniel Arenas", "authors": "Daniel J. Arenas", "title": "Inter-Rater: Software for analysis of inter-rater reliability by\n  permutating pairs of multiple users", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inter-Rater quantifies the reliability between multiple raters who evaluate a\ngroup of subjects. It calculates the group quantity, Fleiss kappa, and it\nimproves on existing software by keeping information about each user and\nquantifying how each user agreed with the rest of the group. This is\naccomplished through permutations of user pairs. The software was written in\nPython, can be run in Linux, and the code is deposited in Zenodo and GitHub.\nThis software can be used for evaluation of inter-rater reliability in\nsystematic reviews, medical diagnosis algorithms, education applications, and\nothers.\n", "versions": [{"version": "v1", "created": "Sat, 15 Sep 2018 15:32:36 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Arenas", "Daniel J.", ""]]}, {"id": "1809.06592", "submitter": "Daniela Escobar", "authors": "Daniela Escobar and Georg Pflug", "title": "The distortion principle for insurance pricing: properties,\n  identification and robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distortion (Denneberg 1990) is a well known premium calculation principle for\ninsurance contracts. In this paper, we study sensitivity properties of\ndistortion functionals w.r.t. the assumptions for risk aversion as well as\nrobustness w.r.t. ambiguity of the loss distribution. Ambiguity is measured by\nthe Wasserstein distance. We study variances of distances for probability\nmodels and identify some worst case distributions. In addition to the direct\nproblem we also investigate the inverse problem, that is how to identify the\ndistortion density on the basis of observations of insurance premia.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 08:36:19 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Escobar", "Daniela", ""], ["Pflug", "Georg", ""]]}, {"id": "1809.07401", "submitter": "David Dias", "authors": "Helder Rojas, David Dias", "title": "Transmission of Macroeconomic Shocks to Risk Parameters: Their uses in\n  Stress Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we are interested in evaluating the resilience of financial\nportfolios under extreme economic conditions. Therefore, we use empirical\nmeasures to characterize the transmission process of macroeconomic shocks to\nrisk parameters. We propose the use of an extensive family of models, called\nGeneral Transfer Function Models, which condense well the characteristics of\nthe transmission described by the impact measures. The procedure for estimating\nthe parameters of these models is described employing the Bayesian approach and\nusing the prior information provided by the impact measures. In addition, we\nillustrate the use of the estimated models from the credit risk data of a\nportfolio.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 20:36:02 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2018 10:45:09 GMT"}, {"version": "v3", "created": "Fri, 17 May 2019 18:51:58 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Rojas", "Helder", ""], ["Dias", "David", ""]]}, {"id": "1809.08771", "submitter": "{\\L}ukasz Kidzi\\'nski", "authors": "{\\L}ukasz Kidzi\\'nski, Trevor Hastie", "title": "Longitudinal data analysis using matrix completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In clinical practice and biomedical research, measurements are often\ncollected sparsely and irregularly in time while the data acquisition is\nexpensive and inconvenient. Examples include measurements of spine bone mineral\ndensity, cancer growth through mammography or biopsy, a progression of defect\nof vision, or assessment of gait in patients with neurological disorders. Since\nthe data collection is often costly and inconvenient, estimation of progression\nfrom sparse observations is of great interest for practitioners.\n  From the statistical standpoint, such data is often analyzed in the context\nof a mixed-effect model where time is treated as both random and fixed effect.\nAlternatively, researchers analyze Gaussian processes or functional data where\nobservations are assumed to be drawn from a certain distribution of processes.\nThese models are flexible but rely on probabilistic assumptions and require\nvery careful implementation.\n  In this study, we propose an alternative elementary framework for analyzing\nlongitudinal data, relying on matrix completion. Our method yields point\nestimates of progression curves by iterative application of the SVD. Our\nframework covers multivariate longitudinal data, regression and can be easily\nextended to other settings.\n  We apply our methods to understand trends of progression of motor impairment\nin children with Cerebral Palsy. Our model approximates individual progression\ncurves and explains 30% of the variability. Low-rank representation of\nprogression trends enables discovering that subtypes of Cerebral Palsy exhibit\ndifferent progression trends.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 06:18:13 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Kidzi\u0144ski", "\u0141ukasz", ""], ["Hastie", "Trevor", ""]]}, {"id": "1809.09527", "submitter": "Tymon Sloczynski", "authors": "Arun Advani, Toru Kitagawa and Tymon S{\\l}oczy\\'nski", "title": "Mostly Harmless Simulations? Using Monte Carlo Studies for Estimator\n  Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two recent suggestions for how to perform an empirically\nmotivated Monte Carlo study to help select a treatment effect estimator under\nunconfoundedness. We show theoretically that neither is likely to be\ninformative except under restrictive conditions that are unlikely to be\nsatisfied in many contexts. To test empirical relevance, we also apply the\napproaches to a real-world setting where estimator performance is known. Both\napproaches are worse than random at selecting estimators which minimise\nabsolute bias. They are better when selecting estimators that minimise mean\nsquared error. However, using a simple bootstrap is at least as good and often\nbetter. For now researchers would be best advised to use a range of estimators\nand compare estimates for robustness.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 14:54:11 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 15:32:29 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Advani", "Arun", ""], ["Kitagawa", "Toru", ""], ["S\u0142oczy\u0144ski", "Tymon", ""]]}, {"id": "1809.10496", "submitter": "Christian Hennig", "authors": "Iven Van Mechelen, Anne-Laure Boulesteix, Rainer Dangl, Nema Dean,\n  Isabelle Guyon, Christian Hennig, Friedrich Leisch, and Douglas Steinley", "title": "Benchmarking in cluster analysis: A white paper", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To achieve scientific progress in terms of building a cumulative body of\nknowledge, careful attention to benchmarking is of the utmost importance. This\nmeans that proposals of new methods of data pre-processing, new data-analytic\ntechniques, and new methods of output post-processing, should be extensively\nand carefully compared with existing alternatives, and that existing methods\nshould be subjected to neutral comparison studies. To date, benchmarking and\nrecommendations for benchmarking have been frequently seen in the context of\nsupervised learning. Unfortunately, there has been a dearth of guidelines for\nbenchmarking in an unsupervised setting, with the area of clustering as an\nimportant subdomain. To address this problem, discussion is given to the\ntheoretical conceptual underpinnings of benchmarking in the field of cluster\nanalysis by means of simulated as well as empirical data. Subsequently, the\npracticalities of how to address benchmarking questions in clustering are dealt\nwith, and foundational recommendations are made.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 12:50:27 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2018 15:08:49 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Van Mechelen", "Iven", ""], ["Boulesteix", "Anne-Laure", ""], ["Dangl", "Rainer", ""], ["Dean", "Nema", ""], ["Guyon", "Isabelle", ""], ["Hennig", "Christian", ""], ["Leisch", "Friedrich", ""], ["Steinley", "Douglas", ""]]}]