[{"id": "1905.00352", "submitter": "Bo-Qiang Ma", "authors": "Mingshu Cong, Congqiao Li, Bo-Qiang Ma", "title": "First digit law from Laplace transform", "comments": "10 latex pages, 8 figures, final version in journal publication", "journal-ref": "Phys. Lett. A 383 (2019) 1836-1844", "doi": "10.1016/j.physleta.2019.03.017", "report-no": null, "categories": "stat.OT math.ST physics.data-an stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The occurrence of digits 1 through 9 as the leftmost nonzero digit of numbers\nfrom real-world sources is distributed unevenly according to an empirical law,\nknown as Benford's law or the first digit law. It remains obscure why a variety\nof data sets generated from quite different dynamics obey this particular law.\nWe perform a study of Benford's law from the application of the Laplace\ntransform, and find that the logarithmic Laplace spectrum of the digital\nindicator function can be approximately taken as a constant. This particular\nconstant, being exactly the Benford term, explains the prevalence of Benford's\nlaw. The slight variation from the Benford term leads to deviations from\nBenford's law for distributions which oscillate violently in the inverse\nLaplace space. We prove that the whole family of completely monotonic\ndistributions can satisfy Benford's law within a small bound. Our study\nsuggests that Benford's law originates from the way that we write numbers, thus\nshould be taken as a basic mathematical knowledge.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 16:40:24 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Cong", "Mingshu", ""], ["Li", "Congqiao", ""], ["Ma", "Bo-Qiang", ""]]}, {"id": "1905.03121", "submitter": "Donghui Yan", "authors": "Donghui Yan and Gary E. Davis", "title": "A First Course in Data Science", "comments": "25 pages, 5 figures", "journal-ref": "Journal of Statistics Education, 2019", "doi": "10.1080/10691898.2019.1623136", "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data science is a discipline that provides principles, methodology and\nguidelines for the analysis of data for tools, values, or insights. Driven by a\nhuge workforce demand, many academic institutions have started to offer degrees\nin data science, with many at the graduate, and a few at the undergraduate\nlevel. Curricula may differ at different institutions, because of varying\nlevels of faculty expertise, and different disciplines (such as Math, computer\nscience, and business etc) in developing the curriculum. The University of\nMassachusetts Dartmouth started offering degree programs in data science from\nFall 2015, at both the undergraduate and the graduate level. Quite a few\narticles have been published that deal with graduate data science courses, much\nless so dealing with undergraduate ones. Our discussion will focus on\nundergraduate course structure and function, and specifically, a first course\nin data science. Our design of this course centers around a concept called the\ndata science life cycle. That is, we view tasks or steps in the practice of\ndata science as forming a process, consisting of states that indicate how it\ncomes into life, how different tasks in data science depend on or interact with\nothers until the birth of a data product or the reach of a conclusion.\nNaturally, different pieces of the data science life cycle then form individual\nparts of the course. Details of each piece are filled up by concepts,\ntechniques, or skills that are popular in industry. Consequently, the design of\nour course is both \"principled\" and practical. A significant feature of our\ncourse philosophy is that, in line with activity theory, the course is based on\nthe use of tools to transform real data in order to answer strongly motivated\nquestions related to the data.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 14:54:14 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Yan", "Donghui", ""], ["Davis", "Gary E.", ""]]}, {"id": "1905.06318", "submitter": "Martin Tveten", "authors": "Martin Tveten", "title": "Which principal components are most sensitive to distributional changes?", "comments": "11 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.OT stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  PCA is often used in anomaly detection and statistical process control tasks.\nFor bivariate data, we prove that the minor projection (the least varying\nprojection) of the PCA-rotated data is the most sensitive to distributional\nchanges, where sensitivity is defined by the Hellinger distance between\ndistributions before and after a change. In particular, this is almost always\nthe case if only one parameter of the bivariate normal distribution changes,\ni.e., the change is sparse. Simulations indicate that the minor projections are\nthe most sensitive for a large range of changes and pre-change settings in\nhigher dimensions as well. This motivates using the minor projections for\ndetecting sparse distributional changes in high-dimensional data.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 17:50:22 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Tveten", "Martin", ""]]}, {"id": "1905.08122", "submitter": "Konul Mustafayeva", "authors": "Konul Mustafayeva and Weining Wang", "title": "Non-Parametric Estimation of Spot Covariance Matrix with High-Frequency\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating spot covariance is an important issue to study, especially with\nthe increasing availability of high-frequency financial data. We study the\nestimation of spot covariance using a kernel method for high-frequency data. In\nparticular, we consider first the kernel weighted version of realized\ncovariance estimator for the price process governed by a continuous\nmultivariate semimartingale. Next, we extend it to the threshold kernel\nestimator of the spot covariances when the underlying price process is a\ndiscontinuous multivariate semimartingale with finite activity jumps. We derive\nthe asymptotic distribution of the estimators for both fixed and shrinking\nbandwidth. The estimator in a setting with jumps has the same rate of\nconvergence as the estimator for diffusion processes without jumps. A\nsimulation study examines the finite sample properties of the estimators. In\naddition, we study an application of the estimator in the context of covariance\nforecasting. We discover that the forecasting model with our estimator\noutperforms a benchmark model in the literature.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 14:01:22 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Mustafayeva", "Konul", ""], ["Wang", "Weining", ""]]}, {"id": "1905.08338", "submitter": "David Colquhoun FRS", "authors": "David Colquhoun", "title": "A response to critiques of \"The reproducibility of research and the\n  misinterpretation of p-values\"", "comments": "10 pages 0 figures. Accepted by Royal Society Open Science", "journal-ref": "R. Soc. open sci. 6: 190819 (2019)", "doi": "10.1098/rsos.190819", "report-no": null, "categories": "stat.OT", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  I proposed (8, 1, 3) that p values should be supplemented by an estimate of\nthe false positive risk (FPR). FPR was defined as the probability that, if you\nclaim that there is a real effect on the basis of p value from a single\nunbiased experiment, that you will be mistaken and the result has occurred by\nchance. This is a Bayesian quantity and that means that there is an infinitude\nof ways to calculate it. My choice of a way to estimate FPR was, therefore,\narbitrary. I maintain that it is a reasonable way, and has the advantage of\nbeing mathematically simpler than other proposals and easier to understand than\nother methods. This might make it more easily accepted by users. As always, not\nevery statistician agrees. This paper is a response to a critique of my 2017\npaper (1) by Arandjelovic (2)\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 20:31:18 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2019 10:16:40 GMT"}, {"version": "v3", "created": "Mon, 28 Oct 2019 17:12:37 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Colquhoun", "David", ""]]}, {"id": "1905.08381", "submitter": "James Hodges", "authors": "James S. Hodges", "title": "Statistical methods research done as science rather than mathematics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is about how we study statistical methods. As an example, it uses\nthe random regressions model, in which the intercept and slope of\ncluster-specific regression lines are modeled as a bivariate random effect.\nMaximizing this model's restricted likelihood often gives a boundary value for\nthe random effect correlation or variances. We argue that this is a problem;\nthat it is a problem because our discipline has little understanding of how\ncontemporary models and methods map data to inferential summaries; that we lack\nsuch understanding, even for models as simple as this, because of a\nnear-exclusive reliance on mathematics as a means of understanding; and that\nmath alone is no longer sufficient. We then argue that as a discipline, we can\nand should break open our black-box methods by mimicking the five steps that\nmolecular biologists commonly use to break open Nature's black boxes: design a\nsimple model system, formulate hypotheses using that system, test them in\nexperiments on that system, iterate as needed to reformulate and test\nhypotheses, and finally test the results in an \"in vivo\" system. We demonstrate\nthis by identifying conditions under which the random-regressions restricted\nlikelihood is likely to be maximized at a boundary value. Resistance to this\napproach seems to arise from a view that it lacks the certainty or intellectual\nheft of mathematics, perhaps because simulation experiments in our literature\nrarely do more than measure a new method's operating characteristics in a small\nrange of situations. We argue that such work can make useful contributions\nincluding, as in molecular biology, the findings themselves and sometimes the\ndesigns used in the five steps; that these contributions have as much practical\nvalue as mathematical results; and that therefore they merit publication as\nmuch as the mathematical results our discipline esteems so highly.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 23:29:46 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Hodges", "James S.", ""]]}, {"id": "1905.08876", "submitter": "Andrew Gelman", "authors": "Andrew Gelman, Brian Haig, Christian Hennig, Art Owen, Robert Cousins,\n  Stan Young, Christian Robert, Corey Yanofsky, E. J. Wagenmakers, Ron Kenett,\n  Daniel Lakeland", "title": "Many perspectives on Deborah Mayo's \"Statistical Inference as Severe\n  Testing: How to Get Beyond the Statistics Wars\"", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The new book by philosopher Deborah Mayo is relevant to data science for\ntopical reasons, as she takes various controversial positions regarding\nhypothesis testing and statistical practice, and also as an entry point to\nthinking about the philosophy of statistics. The present article is a slightly\nexpanded version of a series of informal reviews and comments on Mayo's book.\nWe hope this discussion will introduce people to Mayo's ideas along with other\nperspectives on the topics she addresses.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 21:28:31 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 20:53:00 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Gelman", "Andrew", ""], ["Haig", "Brian", ""], ["Hennig", "Christian", ""], ["Owen", "Art", ""], ["Cousins", "Robert", ""], ["Young", "Stan", ""], ["Robert", "Christian", ""], ["Yanofsky", "Corey", ""], ["Wagenmakers", "E. J.", ""], ["Kenett", "Ron", ""], ["Lakeland", "Daniel", ""]]}, {"id": "1905.09509", "submitter": "Mert Ozer", "authors": "Mehmet Yigit Yildirim, Mert Ozer, Hasan Davulcu", "title": "Leveraging Uncertainty in Deep Learning for Selective Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The wide and rapid adoption of deep learning by practitioners brought\nunintended consequences in many situations such as in the infamous case of\nGoogle Photos' racist image recognition algorithm; thus, necessitated the\nutilization of the quantified uncertainty for each prediction. There have been\nrecent efforts towards quantifying uncertainty in conventional deep learning\nmethods (e.g., dropout as Bayesian approximation); however, their optimal use\nin decision making is often overlooked and understudied. In this study, we\npropose a mixed-integer programming framework for classification with reject\noption (also known as selective classification), that investigates and combines\nmodel uncertainty and predictive mean to identify optimal classification and\nrejection regions. Our results indicate superior performance of our framework\nboth in non-rejected accuracy and rejection quality on several publicly\navailable datasets. Moreover, we extend our framework to cost-sensitive\nsettings and show that our approach outperforms industry standard methods\nsignificantly for online fraud management in real-world settings.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 07:28:36 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Yildirim", "Mehmet Yigit", ""], ["Ozer", "Mert", ""], ["Davulcu", "Hasan", ""]]}, {"id": "1905.09515", "submitter": "P. Richard Hahn", "authors": "P. Richard Hahn, Vincent Dorie, and Jared S. Murray", "title": "Atlantic Causal Inference Conference (ACIC) Data Analysis Challenge 2017", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This brief note documents the data generating processes used in the 2017 Data\nAnalysis Challenge associated with the Atlantic Causal Inference Conference\n(ACIC). The focus of the challenge was estimation and inference for conditional\naverage treatment effects (CATEs) in the presence of targeted selection, which\nleads to strong confounding. The associated data files and further plots can be\nfound on the first author's web page.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 07:49:48 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Hahn", "P. Richard", ""], ["Dorie", "Vincent", ""], ["Murray", "Jared S.", ""]]}, {"id": "1905.09715", "submitter": "P. Richard Hahn", "authors": "P. Richard Hahn", "title": "An illustration of the risk of borrowing information via a shared\n  likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A concrete, stylized example illustrates that inferences may be degraded,\nrather than improved, by incorporating supplementary data via a joint\nlikelihood. In the example, the likelihood is assumed to be correctly\nspecified, as is the prior over the parameter of interest; all that is\nnecessary for the joint modeling approach to suffer is misspecification of the\nprior over a nuisance parameter.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 15:21:34 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Hahn", "P. Richard", ""]]}, {"id": "1905.09929", "submitter": "Gunnar Taraldsen", "authors": "G. Taraldsen and B.H. Lindqvist", "title": "Discussion of \"Nonparametric generalized fiducial inference for survival\n  functions under censoring\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The following discussion is inspired by the paper Nonparametric generalized\nfiducial inference for survival functions under censoring by Cui and Hannig.\nThe discussion consists of comments on the results, but also indicates it's\nimportance more generally in the context of fiducial inference. A two page\nintroduction to fiducial inference is given to provide a context.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 12:55:17 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Taraldsen", "G.", ""], ["Lindqvist", "B. H.", ""]]}, {"id": "1905.10209", "submitter": "{\\L}ukasz Rajkowski", "authors": "John Noble, {\\L}ukasz Rajkowski", "title": "A score function for Bayesian cluster analysis", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a score function for Bayesian clustering. The function is\nparameter free and captures the interplay between the within cluster variance\nand the between cluster entropy of a clustering. It can be used to choose the\nnumber of clusters in well-established clustering methods such as hierarchical\nclustering or $K$-means algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 12:58:33 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Noble", "John", ""], ["Rajkowski", "\u0141ukasz", ""]]}, {"id": "1905.11497", "submitter": "Nathaniel Corder", "authors": "Nathan Corder and Shu Yang", "title": "Estimating Average Treatment Effects Utilizing Fractional Imputation\n  when Confounders are Subject to Missingness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of missingness in observational data is ubiquitous. When the\nconfounders are missing at random, multiple imputation is commonly used;\nhowever, the method requires congeniality conditions for valid inferences,\nwhich may not be satisfied when estimating average causal treatment effects.\nAlternatively, fractional imputation, proposed by Kim 2011, has been\nimplemented to handling missing values in regression context. In this article,\nwe develop fractional imputation methods for estimating the average treatment\neffects with confounders missing at random. We show that the fractional\nimputation estimator of the average treatment effect is asymptotically normal,\nwhich permits a consistent variance estimate. Via simulation study, we compare\nfractional imputation's accuracy and precision with that of multiple\nimputation.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 20:41:34 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 20:05:40 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Corder", "Nathan", ""], ["Yang", "Shu", ""]]}, {"id": "1905.12081", "submitter": "Julius von K\\\"ugelgen", "authors": "Julius von K\\\"ugelgen, Alexander Mey, Marco Loog, Bernhard Sch\\\"olkopf", "title": "Semi-Supervised Learning, Causality and the Conditional Cluster\n  Assumption", "comments": "36th Conference on Uncertainty in Artificial Intelligence (2020)\n  (Previously presented at the NeurIPS 2019 workshop \"Do the right thing\":\n  machine learning and causal inference for improved decision making,\n  Vancouver, Canada.)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the success of semi-supervised learning (SSL) is still not fully\nunderstood, Sch\\\"olkopf et al. (2012) have established a link to the principle\nof independent causal mechanisms. They conclude that SSL should be impossible\nwhen predicting a target variable from its causes, but possible when predicting\nit from its effects. Since both these cases are somewhat restrictive, we extend\ntheir work by considering classification using cause and effect features at the\nsame time, such as predicting disease from both risk factors and symptoms.\nWhile standard SSL exploits information contained in the marginal distribution\nof all inputs (to improve the estimate of the conditional distribution of the\ntarget given inputs), we argue that in our more general setting we should use\ninformation in the conditional distribution of effect features given causal\nfeatures. We explore how this insight generalises the previous understanding,\nand how it relates to and can be exploited algorithmically for SSL.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 20:53:56 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 13:35:22 GMT"}, {"version": "v3", "created": "Fri, 15 May 2020 09:32:52 GMT"}, {"version": "v4", "created": "Wed, 24 Jun 2020 10:40:15 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["von K\u00fcgelgen", "Julius", ""], ["Mey", "Alexander", ""], ["Loog", "Marco", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}]