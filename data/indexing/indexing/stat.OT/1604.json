[{"id": "1604.01455", "submitter": "Juan Pablo Vielma", "authors": "David Scott Hunter, Juan Pablo Vielma, Tauhid Zaman", "title": "Picking Winners in Daily Fantasy Sports Using Integer Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of selecting a portfolio of entries of fixed\ncardinality for contests with top-heavy payoff structures, i.e. most of the\nwinnings go to the top-ranked entries. This framework is general and can be\nused to model a variety of problems, such as movie studios selecting movies to\nproduce, venture capital firms picking start-up companies to invest in, or\nindividuals selecting lineups for daily fantasy sports contests, which is the\nexample we focus on here. We model the portfolio selection task as a\ncombinatorial optimization problem with a submodular objective function, which\nis given by the probability of at least one entry winning. We then show that\nthis probability can be approximated using only pairwise marginal probabilities\nof the entries winning when there is a certain structure on their joint\ndistribution. We consider a model where the entries are jointly Gaussian random\nvariables and present a closed form approximation to the objective function.\nBuilding on this, we then consider a scenario where the entries are given by\nsums of constrained resources and present an integer programming formulation to\nconstruct the entries. Our formulation uses principles based on our theoretical\nanalysis to construct entries: we maximize the expected score of an entry\nsubject to a lower bound on its variance and an upper bound on its correlation\nwith previously constructed entries. To demonstrate the effectiveness of our\ninteger programming approach, we apply it to daily fantasy sports contests that\nhave top-heavy payoff structures. We find that our approach performs well in\npractice. Using our integer programming approach, we are able to rank in the\ntop-ten multiple times in hockey and baseball contests with thousands of\ncompeting entries. Our approach can easily be extended to other problems with\nconstrained resources and a top-heavy payoff structure.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 00:31:26 GMT"}, {"version": "v2", "created": "Wed, 6 Jul 2016 13:50:00 GMT"}, {"version": "v3", "created": "Wed, 23 Jan 2019 15:23:09 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Hunter", "David Scott", ""], ["Vielma", "Juan Pablo", ""], ["Zaman", "Tauhid", ""]]}, {"id": "1604.01844", "submitter": "Jose D. Perezgonzalez PhD", "authors": "Jose D. Perezgonzalez", "title": "Statistical sensitiveness for science", "comments": "25 pages, 1 figure, 2 tables, 3 supplemental materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research often necessitates of samples, yet obtaining large enough samples is\nnot always possible. When it is, the researcher may use one of two methods for\ndeciding upon the required sample size: rules-of-thumb, quick yet uncertain,\nand estimations for power, mathematically precise yet with the potential to\noverestimate or underestimate sample sizes when effect sizes are unknown.\nMisestimated sample sizes have negative repercussions in the form of increased\ncosts, abandoned projects or abandoned publication of non-significant results.\nHere I describe a procedure for estimating sample sizes adequate for the\ntesting approach which is most common in the behavioural, social, and\nbiomedical sciences, that of tests of significance developed by Fisher. The\nprocedure focuses on a desired minimum effect size for the research at hand and\nfinds the minimum sample size required for capturing such effect size as a\nstatistically significant result. In a similar fashion than power analyses,\nsensitiveness analyses can also be extended to finding the minimum effect for a\ngiven sample size a priori as well as to calculating sensitiveness a\nposteriori. The article provides a full tutorial for carrying out a\nsensitiveness analysis, as well as empirical support via simulation\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 01:28:28 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Perezgonzalez", "Jose D.", ""]]}, {"id": "1604.02221", "submitter": "Giovana Fumes", "authors": "Silvia L. P. Ferrari and Giovana Fumes", "title": "Box-Cox symmetric distributions and applications to nutritional data", "comments": "25 pages, 4 figures", "journal-ref": "The final publication is available at Springer (2017)", "doi": "10.1007/s10182-017-0291-6", "report-no": null, "categories": "stat.OT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Box-Cox symmetric class of distributions, which is useful\nfor modeling positively skewed, possibly heavy-tailed, data. The new class of\ndistributions includes the Box-Cox t, Box-Cox Cole-Gree, Box-Cox power\nexponential distributions, and the class of the log-symmetric distributions as\nspecial cases. It provides easy parameter interpretation, which makes it\nconvenient for regression modeling purposes. Additionally, it provides enough\nflexibility to handle outliers. The usefulness of the Box-Cox symmetric models\nis illustrated in applications to nutritional data.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2016 04:00:49 GMT"}, {"version": "v2", "created": "Tue, 11 Oct 2016 01:16:07 GMT"}, {"version": "v3", "created": "Wed, 8 Mar 2017 01:13:13 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Ferrari", "Silvia L. P.", ""], ["Fumes", "Giovana", ""]]}, {"id": "1604.05224", "submitter": "Jingjing Yang", "authors": "Jingjing Yang, Peng Ren", "title": "BFDA: A Matlab Toolbox for Bayesian Functional Data Analysis", "comments": "A tool paper submitted to the Journal of Statistical Software", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a MATLAB toolbox, BFDA, that implements a Bayesian hierarchical\nmodel to smooth multiple functional data with the assumptions of the same\nunderlying Gaussian process distribution, a Gaussian process prior for the mean\nfunction, and an Inverse-Wishart process prior for the covariance function.\nThis model-based approach can borrow strength from all functional data to\nincrease the smoothing accuracy, as well as estimate the mean-covariance\nfunctions simultaneously. An option of approximating the Bayesian inference\nprocess using cubic B-spline basis functions is integrated in BFDA, which\nallows for efficiently dealing with high-dimensional functional data. Examples\nof using BFDA in various scenarios and conducting follow-up functional\nregression are provided. The advantages of BFDA include: (1) Simultaneously\nsmooths multiple functional data and estimates the mean-covariance functions in\na nonparametric way; (2) flexibly deals with sparse and high-dimensional\nfunctional data with stationary and nonstationary covariance functions, and\nwithout the requirement of common observation grids; (3) provides accurately\nsmoothed functional data for follow-up analysis.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 16:06:22 GMT"}, {"version": "v2", "created": "Fri, 3 Feb 2017 16:47:31 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Yang", "Jingjing", ""], ["Ren", "Peng", ""]]}, {"id": "1604.05418", "submitter": "Alexander Nussbaum", "authors": "Alexander Nussbaum, Richard Seides", "title": "Its All on the Square- The Importance of the Sum of Squares and Making\n  the General Linear Model Simple", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistics is one of the most valuable of disciplines. Science is based on\nproof and it alone produces results, other approaches are not, and do not.\nStatistics is the only acceptable language of proof in science. Yet statistics\nis difficult to understand for a large percentage of those who will be\nevaluating and even doing research. Reasons for this difficulty may be that\nstatistics operates counter to the way people think, as well as the widespread\nphobia of numeracy. Adding to the difficulty is that undergraduate textbooks\ntend to make statistical tests seem to be an unorganized conglomeration of\nunrelated procedures, and this leads to a failure of students to understand\nthat all of the parametric procedures they are studying in an introductory\ncourse are ultimately doing the same thing and stem from common sources. In\nstatistics, precisely because the material is complex, the presentation must be\nsimple! This article endeavors to do just that.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 03:33:28 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Nussbaum", "Alexander", ""], ["Seides", "Richard", ""]]}, {"id": "1604.07391", "submitter": "James L Friar", "authors": "J. L. Friar, T. Goldman, J. Perez-Mercader", "title": "Ubiquity of Benfords law and emergence of the reciprocal distribution", "comments": "8 pages", "journal-ref": "Physics Letters A 380, 1895 (2016)", "doi": "10.1016/j.physleta.2016.03.045", "report-no": "LA-UR-13-20486", "categories": "physics.data-an stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply the Law of Total Probability to the construction of scale-invariant\nprobability distribution functions (pdfs), and require that probability\nmeasures be dimensionless and unitless under a continuous change of scales. If\nthe scale-change distribution function is scale invariant then the constructed\ndistribution will also be scale invariant. Repeated application of this\nconstruction on an arbitrary set of (normalizable) pdfs results again in\nscale-invariant distributions. The invariant function of this procedure is\ngiven uniquely by the reciprocal distribution, suggesting a kind of\nuniversality. We separately demonstrate that the reciprocal distribution\nresults uniquely from requiring maximum entropy for size-class distributions\nwith uniform bin sizes.\n", "versions": [{"version": "v1", "created": "Sat, 23 Apr 2016 16:33:08 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Friar", "J. L.", ""], ["Goldman", "T.", ""], ["Perez-Mercader", "J.", ""]]}, {"id": "1604.07397", "submitter": "Edward J. Kim", "authors": "Robert J. Brunner and Edward J. Kim", "title": "Teaching Data Science", "comments": "10 pages, 4 figures, International Conference on Computational\n  Science (ICCS 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT cs.CY physics.ed-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an introductory data science course, entitled Introduction to\nData Science, offered at the University of Illinois at Urbana-Champaign. The\ncourse introduced general programming concepts by using the Python programming\nlanguage with an emphasis on data preparation, processing, and presentation.\nThe course had no prerequisites, and students were not expected to have any\nprogramming experience. This introductory course was designed to cover a wide\nrange of topics, from the nature of data, to storage, to visualization, to\nprobability and statistical analysis, to cloud and high performance computing,\nwithout becoming overly focused on any one subject. We conclude this article\nwith a discussion of lessons learned and our plans to develop new data science\ncourses.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 18:26:51 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Brunner", "Robert J.", ""], ["Kim", "Edward J.", ""]]}]