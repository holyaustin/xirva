[{"id": "1803.00609", "submitter": "Alberto Abadie", "authors": "Alberto Abadie", "title": "On Statistical Non-Significance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Significance tests are probably the most extended form of inference in\nempirical research, and significance is often interpreted as providing greater\ninformational content than non-significance. In this article we show, however,\nthat rejection of a point null often carries very little information, while\nfailure to reject may be highly informative. This is particularly true in\nempirical contexts where data sets are large and where there are rarely reasons\nto put substantial prior probability on a point null. Our results challenge the\nusual practice of conferring point null rejections a higher level of scientific\nsignificance than non-rejections. In consequence, we advocate a visible\nreporting and discussion of non-significant results in empirical practice.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2018 20:15:28 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Abadie", "Alberto", ""]]}, {"id": "1803.00613", "submitter": "Robert B. Gramacy", "authors": "Robert B. Gramacy", "title": "A shiny update to an old experiment game", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Games can be a powerful tool for learning about statistical methodology.\nEffective game design involves a fine balance between caricature and realism,\nto simultaneously illustrate salient concepts in a controlled setting and serve\nas a testament to real-world applicability. Striking that balance is\nparticularly challenging in response surface and design domains, where\nreal-world scenarios often play out over long time scales, during which\ntheories are revised, model and inferential techniques are improved, and\nknowledge is updated. Here I present a game, borrowing liberally from one first\nplayed over forty years ago, that attempts to achieve that balance while\nreinforcing a cascade of topics in modern nonparametric response surfaces,\nsequential design and optimization. The game embeds a blackbox simulation\nwithin a shiny app whose interface is designed to simulate a realistic\ninformation-availability setting, while offering a stimulating, competitive\nenvironment wherein students can try out new methodology, and ultimately\nappreciate its power and limitations. Interface, rules, timing with course\nmaterial, and evaluation are described, along with a \"case study\" involving a\ncohort of students at Virginia Tech.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2018 20:22:53 GMT"}, {"version": "v2", "created": "Mon, 14 May 2018 15:41:11 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Gramacy", "Robert B.", ""]]}, {"id": "1803.01156", "submitter": "Mohieddine Rahmouni", "authors": "Mohieddine Rahmouni, Ayman Orabi", "title": "A Generalization of the Exponential-Logarithmic Distribution for\n  Reliability and Life Data Analysis", "comments": "25 pages, 2 figures", "journal-ref": null, "doi": "10.1007/s41872-018-0049-5", "report-no": null, "categories": "math.ST stat.OT stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we introduce a new two-parameter lifetime distribution, called\nthe exponential-generalized truncated logarithmic (EGTL) distribution, by\ncompounding the exponential and generalized truncated logarithmic\ndistributions. Our procedure generalizes the exponential-logarithmic (EL)\ndistribution modelling the reliability of systems by the use of first-order\nconcepts, where the minimum lifetime is considered (Tahmasbi 2008). In our\napproach, we assume that a system fails if a given number k of the components\nfails and then, we consider the kth-smallest value of lifetime instead of the\nminimum lifetime. The reliability and failure rate functions as well as their\nproperties are presented for some special cases. The estimation of the\nparameters is attained by the maximum likelihood, the expectation maximization\nalgorithm, the method of moments and the Bayesian approach, with a simulation\nstudy performed to illustrate the different methods of estimation. The\napplication study is illustrated based on two real data sets used in many\napplications of reliability.\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2018 12:53:19 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Rahmouni", "Mohieddine", ""], ["Orabi", "Ayman", ""]]}, {"id": "1803.01221", "submitter": "Bhavya Kailkhura", "authors": "Bhavya Kailkhura, Priyadip Ray, Deepak Rajan, Anton Yen, Peter Barnes,\n  Ryan Goldhahn", "title": "Byzantine-Resilient Locally Optimum Detection Using Collaborative\n  Autonomous Networks", "comments": "Proceedings of the 2017 IEEE International Workshop on Computational\n  Advances in Multi-Sensor Adaptive Processing (CAMSAP 2017), 10.-13. December\n  2017, Curacao, Dutch Antilles", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a locally optimum detection (LOD) scheme for\ndetecting a weak radioactive source buried in background clutter. We develop a\ndecentralized algorithm, based on alternating direction method of multipliers\n(ADMM), for implementing the proposed scheme in autonomous sensor networks.\nResults show that algorithm performance approaches the centralized clairvoyant\ndetection algorithm in the low SNR regime, and exhibits excellent convergence\nrate and scaling behavior (w.r.t. number of nodes). We also devise a\nlow-overhead, robust ADMM algorithm for Byzantine-resilient detection, and\ndemonstrate its robustness to data falsification attacks.\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2018 19:34:29 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Kailkhura", "Bhavya", ""], ["Ray", "Priyadip", ""], ["Rajan", "Deepak", ""], ["Yen", "Anton", ""], ["Barnes", "Peter", ""], ["Goldhahn", "Ryan", ""]]}, {"id": "1803.03667", "submitter": "Irina Legchenkova", "authors": "Evgeny Shulzinger, Irina Legchenkova and Edward Bormashenko", "title": "Co-occurrence of the Benford-like and Zipf Laws Arising from the Texts\n  Representing Human and Artificial Languages", "comments": "23 pages, 8 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL physics.soc-ph stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate that large texts, representing human (English, Russian,\nUkrainian) and artificial (C++, Java) languages, display quantitative patterns\ncharacterized by the Benford-like and Zipf laws. The frequency of a word\nfollowing the Zipf law is inversely proportional to its rank, whereas the total\nnumbers of a certain word appearing in the text generate the uneven\nBenford-like distribution of leading numbers. Excluding the most popular words\nessentially improves the correlation of actual textual data with the Zipfian\ndistribution, whereas the Benford distribution of leading numbers (arising from\nthe overall amount of a certain word) is insensitive to the same elimination\nprocedure. The calculated values of the moduli of slopes of double\nlogarithmical plots for artificial languages (C++, Java) are markedly larger\nthan those for human ones.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 12:24:42 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Shulzinger", "Evgeny", ""], ["Legchenkova", "Irina", ""], ["Bormashenko", "Edward", ""]]}, {"id": "1803.05555", "submitter": "Grace Wahba", "authors": "Grace Wahba", "title": "Emanuel Parzen: A Memorial, and a Model With the Two Kernels That He\n  Championed", "comments": "22 pages, 4 photos", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manny Parzen passed away in February 2016, and this article is written partly\nas a memorial and appreciation. Manny made important contributions to several\nareas, but the two that influenced me most were his contributions to kernel\ndensity estimation and to Reproducing Kernel Hilbert Spaces, the two kernels of\nthe title. Some fond memories of Manny as a PhD advisor begin this memorial,\nfollowed by a discussion of Manny's influence on density estimation and RKHS\nmethods. A picture gallery of trips comes next, followed by the technical part\nof the article. Here our goal is to show how risk models can be built using\nRKHS penalized likelihood methods where subjects have personal (sample)\ndensities which can be used as {\\it attributes} in such models.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 00:58:26 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Wahba", "Grace", ""]]}, {"id": "1803.06214", "submitter": "Michael Wood", "authors": "Michael Wood", "title": "How sure are we? Two approaches to statistical inference", "comments": "39 pages, 2 linked spreadsheets", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose you are told that taking a statin will reduce your risk of a heart\nattack or stroke by 3% in the next ten years, or that women have better\nemotional intelligence than men. You may wonder how accurate the 3% is, or how\nconfident we should be about the assertion about women's emotional\nintelligence, bearing in mind that these conclusions are only based on samples\nof data? My aim here is to present two statistical approaches to questions like\nthese. Approach 1 is often called null hypothesis testing but I prefer the\nphrase \"baseline hypothesis\": this is the standard approach in many areas of\ninquiry but is fraught with problems. Approach 2 can be viewed as a\ngeneralisation of the idea of confidence intervals, or as the application of\nBayes' theorem. Unlike Approach 1, Approach 2 provides a tentative estimate of\nthe probability of hypotheses of interest. For both approaches, I explain, from\nfirst principles, building only on \"common sense\" statistical concepts like\naverages and randomness, both how to derive answers, and the rationale behind\nthe answers. This is achieved by using computer simulation methods (resampling\nand bootstrapping using a spreadsheet available on the web) which avoid the use\nof probability distributions (t, normal, etc). Such a minimalist, but\nreasonably rigorous, analysis is particularly useful in a discipline like\nstatistics which is widely used by people who are not specialists. My intended\naudience includes both statisticians, and users of statistical methods who are\nnot statistical experts.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 10:55:23 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Wood", "Michael", ""]]}, {"id": "1803.07141", "submitter": "Zehang Li", "authors": "Samuel J. Clark and Zehang Li and Tyler H. McCormick", "title": "Quantifying the Contributions of Training Data and Algorithm Logic to\n  the Performance of Automated Cause-assignment Algorithms for Verbal Autopsy", "comments": "This version implements Tariff with an additional normalization step\n  that was previously ignored in the package", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A verbal autopsy (VA) consists of a survey with a relative or close contact\nof a person who has recently died. VA surveys are commonly used to infer likely\ncauses of death for individuals when deaths happen outside of hospitals or\nhealthcare facilities. Several statistical and algorithmic methods are\navailable to assign cause of death using VA surveys. Each of these methods\nrequire as inputs some information about the joint distribution of symptoms and\ncauses. In this note, we examine the generalizability of this symptom-cause\ninformation by comparing different automated coding methods using various\ncombinations of inputs and evaluation data. VA algorithm performance is\naffected by both the specific SCI themselves and the logic of a given\nalgorithm. Using a variety of performance metrics for all existing VA\nalgorithms, we demonstrate that in general the adequacy of the information\nabout the joint distribution between symptoms and cause affects performance at\nleast as much or more than algorithm logic.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 19:07:53 GMT"}, {"version": "v2", "created": "Fri, 30 Mar 2018 21:19:48 GMT"}, {"version": "v3", "created": "Wed, 5 Sep 2018 01:02:53 GMT"}, {"version": "v4", "created": "Thu, 15 Nov 2018 14:10:01 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Clark", "Samuel J.", ""], ["Li", "Zehang", ""], ["McCormick", "Tyler H.", ""]]}, {"id": "1803.07172", "submitter": "Alessandro Lomi", "authors": "Tom A.B.Snijders and Alessandro Lomi", "title": "Beyond Homophily: Incorporating Actor Variables in Actor-oriented\n  Network Models", "comments": "33 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the specification of effects of numerical actor attributes in\nstatistical models for directed social networks. A fundamental mechanism is\nhomophily or assortativity, where actors have a higher likelihood to be tied\nwith others having similar values of the variable under study. But there are\nother mechanisms that may also play a role in how the attribute values of two\nactors influence the likelihood of a tie. We discuss three additional\nmechanisms: aspiration to send ties to others having high values; conformity in\nthe sense of sending more ties to others whose values are close to what may be\nconsidered the `social norm'; and sociability, where those having higher values\nwill tend to send more ties generally. These mechanisms may operate jointly,\nand then their effects will be confounded. We present a specification\nrepresenting these effects simultaneously by a four-parameter quadratic\nfunction of the values of sender and receiver. Greater flexibility can be\nobtained by a five-parameter extension. We argue that empirical researchers\noften overlook the possibility that homophily may be confounded with these\nother mechanisms, and that for actor attributes that have important effects on\ndirected networks, these specifications may provide an improvement. An\nillustration is given of the dependence of advice ties on academic grades in a\nnetwork of MBA students, analyzed by the Stochastic Actor-oriented Model.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 21:35:21 GMT"}, {"version": "v2", "created": "Tue, 11 Sep 2018 06:39:13 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Snijders", "Tom A. B.", ""], ["Lomi", "Alessandro", ""]]}, {"id": "1803.10118", "submitter": "Luis Gustavo Nardin", "authors": "Berna Devezer, Luis G. Nardin, Bert Baumgaertner, Erkan Buzbas", "title": "Scientific discovery in a model-centric framework: Reproducibility,\n  innovation, and epistemic diversity", "comments": "EDITS: New title, corrected typos and errors, extended model and\n  results description", "journal-ref": null, "doi": "10.1371/journal.pone.0216125", "report-no": null, "categories": "stat.OT", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Consistent confirmations obtained independently of each other lend\ncredibility to a scientific result. We refer to results satisfying this\nconsistency as reproducible and assume that reproducibility is a desirable\nproperty of scientific discovery. Yet seemingly science also progresses despite\nirreproducible results, indicating that the relationship between\nreproducibility and other desirable properties of scientific discovery is not\nwell understood. These properties include early discovery of truth, persistence\non truth once it is discovered, and time spent on truth in a long-term\nscientific inquiry. We build a mathematical model of scientific discovery that\npresents a viable framework to study its desirable properties including\nreproducibility. In this framework, we assume that scientists adopt a\nmodel-centric approach to discover the true model generating data in a\nstochastic process of scientific discovery. We analyze the properties of this\nprocess using Markov chain theory, Monte Carlo methods, and agent-based\nmodeling. We show that the scientific process may not converge to truth even if\nscientific results are reproducible and that irreproducible results do not\nnecessarily imply untrue results. The proportion of different research\nstrategies represented in the scientific population, scientists' choice of\nmethodology, the complexity of truth, and the strength of signal contribute to\nthis counter-intuitive finding. Important insights include that innovative\nresearch speeds up the discovery of scientific truth by facilitating the\nexploration of model space and epistemic diversity optimizes across desirable\nproperties of scientific discovery.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 14:55:02 GMT"}, {"version": "v2", "created": "Wed, 4 Apr 2018 16:47:22 GMT"}, {"version": "v3", "created": "Mon, 18 Mar 2019 09:08:01 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Devezer", "Berna", ""], ["Nardin", "Luis G.", ""], ["Baumgaertner", "Bert", ""], ["Buzbas", "Erkan", ""]]}, {"id": "1803.10121", "submitter": "Jessie Hendricks", "authors": "J. H. Hendricks, C. Neumann, C. P. Saunders", "title": "Quantification of the weight of fingerprint evidence using a ROC-based\n  Approximate Bayesian Computation algorithm for model selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For more than a century, fingerprints have been used with considerable\nsuccess to identify criminals or verify the identity of individuals. The\ncategorical conclusion scheme used by fingerprint examiners, and more generally\nthe inference process followed by forensic scientists, have been heavily\ncriticised in the scientific and legal literature. Instead, scholars have\nproposed to characterise the weight of forensic evidence using the Bayes factor\nas the key element of the inference process. In forensic science, quantifying\nthe magnitude of support is equally as important as determining which model is\nsupported. Unfortunately, the complexity of fingerprint patterns render\nlikelihood-based inference impossible. In this paper, we use an Approximate\nBayesian Computation model selection algorithm to quantify the weight of\nfingerprint evidence. We supplement the ABC algorithm using a Receiver\nOperating Characteristic curve to mitigate the effect of the curse of\ndimensionality. Our modified algorithm is computationally efficient and makes\nit easier to monitor convergence as the number of simulations increase. We use\nour method to quantify the weight of fingerprint evidence in forensic science,\nbut we note that it can be applied to any other forensic pattern evidence.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 15:04:08 GMT"}, {"version": "v2", "created": "Fri, 8 Feb 2019 20:38:05 GMT"}, {"version": "v3", "created": "Mon, 13 Jan 2020 18:11:27 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Hendricks", "J. H.", ""], ["Neumann", "C.", ""], ["Saunders", "C. P.", ""]]}]