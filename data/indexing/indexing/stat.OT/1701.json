[{"id": "1701.02383", "submitter": "Shannon Gallagher", "authors": "Shannon Gallagher, Lee Richardson, Samuel L. Ventura, William F. Eddy", "title": "SPEW: Synthetic Populations and Ecosystems of the World", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT physics.soc-ph q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Agent-based models (ABMs) simulate interactions between autonomous agents in\nconstrained environments over time. ABMs are often used for modeling the spread\nof infectious diseases. In order to simulate disease outbreaks or other\nphenomena, ABMs rely on \"synthetic ecosystems,\" or information about agents and\ntheir environments that is representative of the real world. Previous\napproaches for generating synthetic ecosystems have some limitations: they are\nnot open-source, cannot be adapted to new or updated input data sources, and do\nnot allow for alternative methods for sampling agent characteristics and\nlocations. We introduce a general framework for generating Synthetic\nPopulations and Ecosystems of the World (SPEW), implemented as an open-source R\npackage. SPEW allows researchers to choose from a variety of sampling methods\nfor agent characteristics and locations when generating synthetic ecosystems\nfor any geographic region. SPEW can produce synthetic ecosystems for any agent\n(e.g. humans, mosquitoes, etc), provided that appropriate data is available. We\nanalyze the accuracy and computational efficiency of SPEW given different\nsampling methods for agent characteristics and locations and provide a suite of\ndiagnostics to screen our synthetic ecosystems. SPEW has generated over five\nbillion human agents across approximately 100,000 geographic regions in about\n70 countries, available online.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 22:58:05 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Gallagher", "Shannon", ""], ["Richardson", "Lee", ""], ["Ventura", "Samuel L.", ""], ["Eddy", "William F.", ""]]}, {"id": "1701.04583", "submitter": "Dave Zachariah", "authors": "Dave Zachariah, Petre Stoica, Magnus Jansson", "title": "PUMA criterion = MODE criterion", "comments": null, "journal-ref": "IEEE Transactions on Signal Processing, vol. 65, no. 22, 2017", "doi": "10.1109/TSP.2017.2742982", "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the recently proposed (enhanced) PUMA estimator for array\nprocessing minimizes the same criterion function as the well-established MODE\nestimator. (PUMA = principal-singular-vector utilization for modal analysis,\nMODE = method of direction estimation.)\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 09:14:38 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Zachariah", "Dave", ""], ["Stoica", "Petre", ""], ["Jansson", "Magnus", ""]]}, {"id": "1701.08290", "submitter": "Jeremy Manning Jeremy Manning", "authors": "Andrew C. Heusser and Kirsten Ziman and Lucy L. W. Owen and Jeremy R.\n  Manning", "title": "HyperTools: A Python toolbox for visualizing and manipulating\n  high-dimensional data", "comments": "22 pages, 11 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data visualizations can reveal trends and patterns that are not otherwise\nobvious from the raw data or summary statistics. While visualizing\nlow-dimensional data is relatively straightforward (for example, plotting the\nchange in a variable over time as (x,y) coordinates on a graph), it is not\nalways obvious how to visualize high-dimensional datasets in a similarly\nintuitive way. Here we present HypeTools, a Python toolbox for visualizing and\nmanipulating large, high-dimensional datasets. Our primary approach is to use\ndimensionality reduction techniques (Pearson, 1901; Tipping & Bishop, 1999) to\nembed high-dimensional datasets in a lower-dimensional space, and plot the data\nusing a simple (yet powerful) API with many options for data manipulation [e.g.\nhyperalignment (Haxby et al., 2011), clustering, normalizing, etc.] and plot\nstyling. The toolbox is designed around the notion of data trajectories and\npoint clouds. Just as the position of an object moving through space can be\nvisualized as a 3D trajectory, HyperTools uses dimensionality reduction\nalgorithms to create similar 2D and 3D trajectories for time series of\nhigh-dimensional observations. The trajectories may be plotted as interactive\nstatic plots or visualized as animations. These same dimensionality reduction\nand alignment algorithms can also reveal structure in static datasets (e.g.\ncollections of observations or attributes). We present several examples\nshowcasing how using our toolbox to explore data through trajectories and\nlow-dimensional embeddings can reveal deep insights into datasets across a wide\nvariety of domains.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jan 2017 13:37:56 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Heusser", "Andrew C.", ""], ["Ziman", "Kirsten", ""], ["Owen", "Lucy L. W.", ""], ["Manning", "Jeremy R.", ""]]}, {"id": "1701.08366", "submitter": "Kayvan Sadeghi", "authors": "Kayvan Sadeghi", "title": "Faithfulness of Probability Distributions and Graphs", "comments": "29 pages, 3 figures", "journal-ref": "Journal of Machine Learning Research, 18(148), 1--29, 2017", "doi": null, "report-no": null, "categories": "math.ST stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A main question in graphical models and causal inference is whether, given a\nprobability distribution $P$ (which is usually an underlying distribution of\ndata), there is a graph (or graphs) to which $P$ is faithful. The main goal of\nthis paper is to provide a theoretical answer to this problem. We work with\ngeneral independence models, which contain probabilistic independence models as\na special case. We exploit a generalization of ordering, called preordering, of\nthe nodes of (mixed) graphs. This allows us to provide sufficient conditions\nfor a given independence model to be Markov to a graph with the minimum\npossible number of edges, and more importantly, necessary and sufficient\nconditions for a given probability distribution to be faithful to a graph. We\npresent our results for the general case of mixed graphs, but specialize the\ndefinitions and results to the better-known subclasses of undirected\n(concentration) and bidirected (covariance) graphs as well as directed acyclic\ngraphs.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jan 2017 11:49:54 GMT"}, {"version": "v2", "created": "Thu, 2 Nov 2017 17:11:08 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Sadeghi", "Kayvan", ""]]}, {"id": "1701.08438", "submitter": "Nicholas Horton", "authors": "Azka Javaid and Xiaofei Wang and Nicholas J Horton", "title": "Using a \"Study of Studies\" to help statistics students assess research\n  findings", "comments": "in press, CHANCE", "journal-ref": null, "doi": "10.1080/09332480.2017.1406762", "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One learning goal of the introductory statistics course is to develop the\nability to make sense of research findings in published papers. The Atlantic\nmagazine regularly publishes a feature called \"Study of Studies\" that\nsummarizes multiple articles published in a particular domain. We describe a\nclassroom activity to develop this capacity using the \"Study of Studies.\" In\nthis activity, students read capsule summaries of twelve research papers\nrelated to restaurants and dining that was published in April 2015. The\nselected papers report on topics such as how seating arrangement, server\nposture, plate color and size, and the use of background music relate to\nrevenue, ambiance, and perceived food quality. The students are assigned one of\nthe twelve papers to read and critique as part of a small group. Their group\ncritiques are shared with the class and the instructor.\n  A pilot study was conducted during the 2015-2016 academic year at Amherst\nCollege. Students noted that key details were not included in the published\nsummary. They were generally skeptical of the published conclusions. The\nstudents often provided additional summarization of information from the\njournal articles that better describe the results. By independently assessing\nand comparing the original study conclusions with the capsule summary in the\n\"Study of Studies,\" students can practice developing judgment and assessing the\nvalidity of statistical results.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jan 2017 21:52:57 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Javaid", "Azka", ""], ["Wang", "Xiaofei", ""], ["Horton", "Nicholas J", ""]]}, {"id": "1701.08452", "submitter": "Nicholas Horton", "authors": "Xiaofei Wang and Nicholas G. Reich and Nicholas J. Horton", "title": "Enriching students' conceptual understanding of confidence intervals: An\n  interactive trivia-based classroom activity", "comments": "18 pages; in press at the American Statistician", "journal-ref": null, "doi": "10.1080/00031305.2017.1305294", "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Confidence intervals provide a way to determine plausible values for a\npopulation parameter. They are omnipresent in research articles involving\nstatistical analyses. Appropriately, a key statistical literacy learning\nobjective is the ability to interpret and understand confidence intervals in a\nwide range of settings. As instructors, we devote a considerable amount of time\nand effort to ensure that students master this topic in introductory courses\nand beyond. Yet, studies continue to find that confidence intervals are\ncommonly misinterpreted and that even experts have trouble calibrating their\nindividual confidence levels. In this article, we present a ten-minute trivia\ngame-based activity that addresses these misconceptions by exposing students to\nconfidence intervals from a personal perspective. We describe how the activity\ncan be integrated into a statistics course as a one-time activity or with\nrepetition at intervals throughout a course, discuss results of using the\nactivity in class, and present possible extensions.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jan 2017 23:52:51 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Wang", "Xiaofei", ""], ["Reich", "Nicholas G.", ""], ["Horton", "Nicholas J.", ""]]}]