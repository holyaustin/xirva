[{"id": "1810.01729", "submitter": "Jean Michel Loubes", "authors": "Philippe Besse, Celine Castets-Renard, Aurelien Garivier, Jean-Michel\n  Loubes", "title": "Can everyday AI be ethical. Fairness of Machine Learning Algorithms", "comments": "in French. L'IA du quotidien peut-elle \\^etre \\'ethique. Loyaut\\'e\n  des Algorithmes d'apprentissage automatique", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining big data and machine learning algorithms, the power of automatic\ndecision tools induces as much hope as fear. Many recently enacted European\nlegislation (GDPR) and French laws attempt to regulate the use of these tools.\nLeaving aside the well-identified problems of data confidentiality and\nimpediments to competition, we focus on the risks of discrimination, the\nproblems of transparency and the quality of algorithmic decisions. The detailed\nperspective of the legal texts, faced with the complexity and opacity of the\nlearning algorithms, reveals the need for important technological disruptions\nfor the detection or reduction of the discrimination risk, and for addressing\nthe right to obtain an explanation of the auto- matic decision. Since trust of\nthe developers and above all of the users (citizens, litigants, customers) is\nessential, algorithms exploiting personal data must be deployed in a strict\nethical framework. In conclusion, to answer this need, we list some ways of\ncontrols to be developed: institutional control, ethical charter, external\naudit attached to the issue of a label.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 13:30:47 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Besse", "Philippe", ""], ["Castets-Renard", "Celine", ""], ["Garivier", "Aurelien", ""], ["Loubes", "Jean-Michel", ""]]}, {"id": "1810.02294", "submitter": "Kayvan Sadeghi", "authors": "Kayvan Sadeghi and Alessandro Rinaldo", "title": "Markov Properties of Discrete Determinantal Point Processes", "comments": "9 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determinantal point processes (DPPs) are probabilistic models for repulsion.\nWhen used to represent the occurrence of random subsets of a finite base set,\nDPPs allow to model global negative associations in a mathematically elegant\nand direct way. Discrete DPPs have become popular and computationally tractable\nmodels for solving several machine learning tasks that require the selection of\ndiverse objects, and have been successfully applied in numerous real-life\nproblems. Despite their popularity, the statistical properties of such models\nhave not been adequately explored. In this note, we derive the Markov\nproperties of discrete DPPs and show how they can be expressed using graphical\nmodels.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 16:18:59 GMT"}, {"version": "v2", "created": "Mon, 28 Jan 2019 00:56:41 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Sadeghi", "Kayvan", ""], ["Rinaldo", "Alessandro", ""]]}, {"id": "1810.05118", "submitter": "Simon Demers", "authors": "Simon Demers", "title": "Canadian Crime Rates in the Penalty Box", "comments": "7 pages, 3 figures, supporting data and R script enclosed as\n  supplementary files", "journal-ref": "Journal of Community Safety and Well-Being (2018), 3(3):105-109", "doi": "10.35502/jcswb.82", "report-no": null, "categories": "stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the 1962 to 2016 period, the Canadian violent crime rate has remained\nstrongly correlated with National Hockey League (NHL) penalties. The Canadian\nproperty crime rate was similarly correlated with stolen base attempts in the\nMajor League Baseball (MLB). Of course, correlation does not imply causation or\nprove association. It is simply presented here as an observation. Curious\nreaders might be tempted to conduct additional research and ask questions in\norder to enhance the conversation, transition away from a state of confusion,\nclarify the situation, prevent false attribution, and possibly solve a problem\nthat economists call identification.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 16:59:25 GMT"}, {"version": "v2", "created": "Fri, 26 Oct 2018 23:16:56 GMT"}, {"version": "v3", "created": "Wed, 11 Dec 2019 20:17:45 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Demers", "Simon", ""]]}, {"id": "1810.06387", "submitter": "Morgan Kain", "authors": "Jonathan Dushoff, Morgan P. Kain, Benjamin M. Bolker", "title": "I can see clearly now: reinterpreting statistical significance", "comments": "11 Pages, 1 Table, 0 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Null hypothesis significance testing remains popular despite decades of\nconcern about misuse and misinterpretation. We believe that much of the problem\nis due to language: significance testing has little to do with other meanings\nof the word \"significance\". Despite the limitations of null-hypothesis tests,\nwe argue here that they remain useful in many contexts as a guide to whether a\ncertain effect can be seen clearly in that context (e.g. whether we can clearly\nsee that a correlation or between-group difference is positive or negative). We\ntherefore suggest that researchers describe the conclusions of null-hypothesis\ntests in terms of statistical \"clarity\" rather than statistical \"significance\".\nThis simple semantic change could substantially enhance clarity in statistical\ncommunication.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 14:34:24 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Dushoff", "Jonathan", ""], ["Kain", "Morgan P.", ""], ["Bolker", "Benjamin M.", ""]]}, {"id": "1810.10859", "submitter": "John Klein", "authors": "John Klein", "title": "Complementary Lipschitz continuity results for the distribution of\n  intersections or unions of independent random sets in finite discrete spaces", "comments": null, "journal-ref": null, "doi": "10.1016/j.ijar.2019.04.007", "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that intersections and unions of independent random sets in finite\nspaces achieve a form of Lipschitz continuity. More precisely, given the\ndistribution of a random set $\\Xi$, the function mapping any random set\ndistribution to the distribution of its intersection (under independence\nassumption) with $\\Xi$ is Lipschitz continuous with unit Lipschitz constant if\nthe space of random set distributions is endowed with a metric defined as the\n$L_k$ norm distance between inclusion functionals also known as commonalities.\nMoreover, the function mapping any random set distribution to the distribution\nof its union (under independence assumption) with $\\Xi$ is Lipschitz continuous\nwith unit Lipschitz constant if the space of random set distributions is\nendowed with a metric defined as the $L_k$ norm distance between hitting\nfunctionals also known as plausibilities.\n  Using the epistemic random set interpretation of belief functions, we also\ndiscuss the ability of these distances to yield conflict measures. All the\nproofs in this paper are derived in the framework of Dempster-Shafer belief\nfunctions. Let alone the discussion on conflict measures, it is straightforward\nto transcribe the proofs into the general (non necessarily epistemic) random\nset terminology.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 08:04:14 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2019 07:39:02 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Klein", "John", ""]]}, {"id": "1810.10946", "submitter": "Mai Alfahad", "authors": "Mai F Alfahad, John T Kent, Kanti V Mardia", "title": "Helix modelling through the Mardia-Holmes model framework and an\n  extension of the Mardia-Holmes model", "comments": "8 pager, no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For noisy two-dimensional data, which are approximately uniformly distributed\nnear the circumference of an ellipse, Mardia and Holmes (1980) developed a\nmodel to fit the ellipse. In this paper we adapt their methodology to the\nanalysis of helix data in three dimensions. If the helix axis is known, then\nthe Mardia-Holmes model for the circular case can be fitted after projecting\nthe helix data onto the plane normal to the helix axis. If the axis is unknown,\nan iterative algorithm has been developed to estimate the axis. The methodology\nis illustrated using simulated protein alpha-helices. We also give a\nmultivariate version of the Mardia-Holmes model which will be applicable for\nfitting an ellipsoid and in particular a cylinder.\n", "versions": [{"version": "v1", "created": "Sun, 21 Oct 2018 06:57:09 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Alfahad", "Mai F", ""], ["Kent", "John T", ""], ["Mardia", "Kanti V", ""]]}, {"id": "1810.10972", "submitter": "Russell Bowater", "authors": "Russell J. Bowater", "title": "On a generalised form of subjective probability", "comments": "Corrected and rewritten. Authorship changed by mutual consent of the\n  original authors. *Final version*", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is motivated by the questions of how to give the concept of\nprobability an adequate real-world meaning, and how to explain a certain type\nof phenomenon that can be found, for instance, in Ellsberg's paradox. It\nattempts to answer these questions by constructing an alternative theory to one\nthat was proposed in earlier papers on the basis of various important\ncriticisms that were raised against this earlier theory. The conceptual\nprinciples of the corresponding definition of probability are laid out and\nexplained in detail. In particular, what is required to fully specify a\nprobability distribution under this definition is not just the distribution\nfunction of the variable concerned, but also an assessment of the internal\nand/or the external strength of this function relative to other distribution\nfunctions of interest. This way of defining probability is applied to various\nexamples and problems including, perhaps most notably, to a long-running\ncontroversy concerning the distinction between Bayesian and fiducial inference.\nThe characteristics of this definition of probability are carefully evaluated\nin terms of the issues that it sets out to address.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 16:48:28 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 18:25:27 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Bowater", "Russell J.", ""]]}, {"id": "1810.12430", "submitter": "Alberto Baccini", "authors": "Alberto Baccini, Lucio Barabesi, Giuseppe De Nicolao", "title": "On the agreement between bibliometrics and peer review: evidence from\n  the Italian research assessment exercises", "comments": "28 pages, 6 tables, 4 Figures. This version contains identical\n  results and maths. It adds an extended literature review, a deeper discussion\n  of findings, and 4 new figures illustrating results", "journal-ref": null, "doi": "10.1371/journal.pone.0242520", "report-no": null, "categories": "stat.AP physics.soc-ph stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper appraises the concordance between bibliometrics and peer review,\nby drawing evidence from the data of two experiments realized by the Italian\ngovernmental agency for research evaluation. The experiments were performed for\nvalidating the dual system of evaluation, consisting in the interchangeable use\nof bibliometyrics and peer review, adopted by the agency in the research\nassessment exercises. The two experiments were based on stratified random\nsamples of journal articles. Each article was scored by bibliometrics and by\npeer review. The degree of concordance between the two evaluations is then\ncomputed. The correct setting of the experiments is defined by developing the\ndesign-based estimation of the Cohen's kappa coefficient and some testing\nprocedures for assessing the homogeneity of missing proportions between strata.\nThe results of both experiments show that for each research areas of hard\nsciences, engineering and life sciences, the degree of agreement between\nbibliometrics and peer review is -- at most -- weak at an individual article\nlevel. Thus, the outcome of the experiments does not validate the use of the\ndual system of evaluation in the Italian research assessments. More in general,\nthe very weak concordance indicates that metrics should not replace peer review\nat the level of individual article. Hence, the use of the dual system of\nevaluation for reducing costs might introduce unknown biases in a research\nassessment exercise.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 22:06:43 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 06:38:48 GMT"}, {"version": "v3", "created": "Wed, 1 Apr 2020 14:32:39 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Baccini", "Alberto", ""], ["Barabesi", "Lucio", ""], ["De Nicolao", "Giuseppe", ""]]}]