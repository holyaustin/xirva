[{"id": "2101.00548", "submitter": "Xiaogang Duan", "authors": "X.G. Duan", "title": "Better understanding of the multivariate hypergeometric distribution\n  with implications in design-based survey sampling", "comments": "9 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate hypergeometric distribution arises frequently in elementary\nstatistics and probability courses, for simultaneously studying the occurence\nlaw of specified events, when sampling without replacement from a finite\npopulation with fixed number of classification. Covariance matrix of this\ndistribution is well known to be identical to its multinomial counterpart\nmultiplied by 1-(n-1)/(N-1), with N and n being population and sample sizes,\nrespectively. It appears to however, have been less discussed in the literature\nabout the meaning of this relationship, especially regarding the specific form\nof the multiplier. Based on an augmenting argument together with probabilistic\nsymmetry, we present a more transparent understanding for the covariance\nstructure of the multivariate hypergeometric distribution. We discuss\nimplications of these combined techniques and provide a unified description\nabout the relative efficiency for estimating population mean based on simple\nrandom sampling, probability proportional-to-size sampling and adaptive cluster\nsampling, with versus without replacement. We also provide insight into the\nclassic random group method for variance estimation.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 03:10:52 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Duan", "X. G.", ""]]}, {"id": "2101.03491", "submitter": "Narumasa Tsutsumida", "authors": "Joseph Emile Honour Percival, Narumasa Tsutsumida, Daisuke Murakami,\n  Takahiro Yoshida, Tomoki Nakaya", "title": "gwpcorMapper: an interactive mapping tool for exploring geographically\n  weighted correlation and partial correlation in high-dimensional geospatial\n  datasets", "comments": "18 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Exploratory spatial data analysis (ESDA) plays a key role in research that\nincludes geographic data. In ESDA, analysts often want to be able to visualize\nobservations and local relationships on a map. However, software dedicated to\nvisualizing local spatial relations be-tween multiple variables in high\ndimensional datasets remains undeveloped. This paper introduces gwpcorMapper, a\nnewly developed software application for mapping geographically weighted\ncorrelation and partial correlation in large multivariate datasets.\ngwpcorMap-per facilitates ESDA by giving researchers the ability to interact\nwith map components that describe local correlative relationships. We built\ngwpcorMapper using the R Shiny framework. The software inherits its core\nalgorithm from GWpcor, an R library for calculating the geographically weighted\ncorrelation and partial correlation statistics. We demonstrate the application\nof gwpcorMapper by using it to explore census data in order to find meaningful\nrelationships that describe the work-life environment in the 23 special wards\nof Tokyo, Japan. We show that gwpcorMapper is useful in both variable selection\nand parameter tuning for geographically weighted statistics. gwpcorMapper\nhighlights that there are strong statistically clear local variations in the\nrelationship between the number of commuters and the total number of hours\nworked when considering the total population in each district across the 23\nspecial wards of Tokyo. Our application demonstrates that the ESDA process with\nhigh-dimensional geospatial data using gwpcorMapper has applications across\nmultiple fields.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 07:16:29 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Percival", "Joseph Emile Honour", ""], ["Tsutsumida", "Narumasa", ""], ["Murakami", "Daisuke", ""], ["Yoshida", "Takahiro", ""], ["Nakaya", "Tomoki", ""]]}, {"id": "2101.04611", "submitter": "Tiandong Wang", "authors": "Tiandong Wang and Panpan Zhang", "title": "Directed Hybrid Random Networks Mixing Preferential Attachment with\n  Uniform Attachment Mechanisms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI math.PR stat.OT", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Motivated by the complexity of network data, we propose a directed hybrid\nrandom network that mixes preferential attachment (PA) rules with uniform\nattachment (UA) rules. When a new edge is created, with probability $p\\in\n[0,1]$, it follows the PA rule. Otherwise, this new edge is added between two\nuniformly chosen nodes. Such mixture makes the in- and out-degrees of a fixed\nnode grow at a slower rate, compared to the pure PA case, thus leading to\nlighter distributional tails. Useful inference methods for the proposed hybrid\nmodel are then provided and applied to both synthetic and real datasets. We see\nthat with extra flexibility given by the parameter $p$, the hybrid random\nnetwork provides a better fit to real-world scenarios, where lighter tails from\nin- and out-degrees are observed.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 16:43:52 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 22:26:56 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Wang", "Tiandong", ""], ["Zhang", "Panpan", ""]]}, {"id": "2101.05677", "submitter": "Ashwin Misra", "authors": "Ashwin Misra, Ankit Mittal, Vihaan Misra and Deepanshu Pandey", "title": "Improving non-deterministic uncertainty modelling in Industry 4.0\n  scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT cs.AI stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The latest Industrial revolution has helped industries in achieving very high\nrates of productivity and efficiency. It has introduced data aggregation and\ncyber-physical systems to optimize planning and scheduling. Although,\nuncertainty in the environment and the imprecise nature of human operators are\nnot accurately considered for into the decision making process. This leads to\ndelays in consignments and imprecise budget estimations. This widespread\npractice in the industrial models is flawed and requires rectification. Various\nother articles have approached to solve this problem through stochastic or\nfuzzy set model methods. This paper presents a comprehensive method to\nlogically and realistically quantify the non-deterministic uncertainty through\nprobabilistic uncertainty modelling. This method is applicable on virtually all\nIndustrial data sets, as the model is self adjusting and uses\nepsilon-contamination to cater to limited or incomplete data sets. The results\nare numerically validated through an Industrial data set in Flanders, Belgium.\nThe data driven results achieved through this robust scheduling method\nillustrate the improvement in performance.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 23:17:55 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Misra", "Ashwin", ""], ["Mittal", "Ankit", ""], ["Misra", "Vihaan", ""], ["Pandey", "Deepanshu", ""]]}, {"id": "2101.05744", "submitter": "L\\'aszl\\'o Csat\\'o", "authors": "L\\'aszl\\'o Csat\\'o", "title": "A comparative study of scoring systems by simulations", "comments": "14 pages, 4 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT cs.GT econ.GN q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scoring rules aggregate individual rankings by assigning some points to each\nposition in each ranking such that the total sum of points provides the overall\nranking of the alternatives. They are widely used in sports competitions\nconsisting of multiple contests. We study the tradeoff between two risks in\nthis setting: (1) the threat of early clinch when the title has been clinched\nbefore the last contest(s) of the competition take place; (2) the danger of\nwinning the competition without finishing first in any contest. In particular,\nfour historical points scoring systems of the Formula One World Championship\nare compared with the family of geometric scoring rules that have favourable\naxiomatic properties. The formers are found to be competitive or even better.\nThe current scheme seems to be a reasonable compromise in optimising the above\ngoals. Our results shed more light on the evolution of the Formula One points\nscoring systems and contribute to the issue of choosing the set of point\nvalues.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 09:57:47 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 06:38:42 GMT"}, {"version": "v3", "created": "Thu, 4 Mar 2021 14:47:40 GMT"}, {"version": "v4", "created": "Thu, 17 Jun 2021 10:51:06 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Csat\u00f3", "L\u00e1szl\u00f3", ""]]}, {"id": "2101.06854", "submitter": "Xinyu Song", "authors": "Xinyu Song and Yazhen Wang and Shang Wu and Donggyu Kim", "title": "Statistical Analysis of Quantum Annealing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Quantum computers use quantum resources to carry out computational tasks and\nmay outperform classical computers in solving certain computational problems.\nSpecial-purpose quantum computers such as quantum annealers employ quantum\nadiabatic theorem to solve combinatorial optimization problems. In this paper,\nwe compare classical annealings such as simulated annealing and quantum\nannealings that are done by the D-Wave machines both theoretically and\nnumerically. We show that if the classical and quantum annealing are\ncharacterized by equivalent Ising models, then solving an optimization problem,\ni.e., finding the minimal energy of each Ising model, by the two annealing\nprocedures, are mathematically identical. For quantum annealing, we also derive\nthe probability lower-bound on successfully solving an optimization problem by\nmeasuring the system at the end of the annealing procedure. Moreover, we\npresent the Markov chain Monte Carlo (MCMC) method to realize quantum annealing\nby classical computers and investigate its statistical properties. In the\nnumerical section, we discuss the discrepancies between the MCMC based\nannealing approaches and the quantum annealing approach in solving optimization\nproblems.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 02:57:36 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Song", "Xinyu", ""], ["Wang", "Yazhen", ""], ["Wu", "Shang", ""], ["Kim", "Donggyu", ""]]}, {"id": "2101.07097", "submitter": "Ian Silver", "authors": "Ian A Silver", "title": "The Violating Assumptions Series: Simulated demonstrations to illustrate\n  how assumptions can affect statistical estimates", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.13339.69921", "report-no": null, "categories": "stat.ME stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When teaching and discussing statistical assumptions, our focus is oftentimes\nplaced on how to test and address potential violations rather than the effects\nof violating assumptions on the estimates produced by our statistical models.\nThe latter represents a potential avenue to help us better understand the\nimpact of researcher degrees of freedom on the statistical estimates we\nproduce. The Violating Assumptions Series is an endeavor I have undertaken to\ndemonstrate the effects of violating assumptions on the estimates produced\nacross various statistical models. The series will review assumptions\nassociated with estimating causal associations, as well as more complicated\nstatistical models including, but not limited to, multilevel models, path\nmodels, structural equation models, and Bayesian models. In addition to the\nprimary goal, the series of posts is designed to illustrate how simulations can\nbe used to develop a comprehensive understanding of applied statistics.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 14:42:35 GMT"}, {"version": "v2", "created": "Sun, 7 Feb 2021 16:10:06 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 18:27:11 GMT"}, {"version": "v4", "created": "Mon, 5 Apr 2021 01:11:55 GMT"}, {"version": "v5", "created": "Wed, 28 Apr 2021 15:25:23 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Silver", "Ian A", ""]]}, {"id": "2101.07710", "submitter": "Mohammad Fayaz", "authors": "Mohammad Fayaz, Alireza Abadi, Soheila Khodakarim", "title": "The effect of Hybrid Principal Components Analysis on the Signal\n  Compression Functional Regression: With EEG-fMRI Application", "comments": "It has 11 pages with 3 tables and 3 figures. It presented at \"The\n  13th International Conference of the ERCIM WG on Computational and\n  Methodological Statistics (CMStatistics 2020) (Virtual), 19-21 December 2020\"\n  (http://www.cmstatistics.org/CMStatistics2020/index.php). We plan to publish\n  it in statistical journals, especially in the conference's recommended\n  journals", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.OT", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Objective: In some situations that exist both scalar and functional data,\ncalled mixed and hybrid data, the hybrid PCA (HPCA) was introduced. Among the\nregression models for the hybrid data, we can count covariate-adjusted HPCA,\nthe Semi-functional partial linear regression, function-on-function (FOF)\nregression with signal compression, and functional additive regression, models.\nIn this article, we study the effects of HPCA decomposition of hybrid data on\nthe prediction accuracy of the FOF regression with signal compressions. Method:\nWe stated a two-step procedure for incorporating the HPCA in the functional\nregressions. The first step is reconstructing the data based on the HPCAs and\nthe second step is merging data on the other dimensions and calculate the\npoint-wise average of the desired functional dimension. We also choose the\nnumber of HPCA based on Mean Squared Perdition Error (MSPE). Result: In the two\nsimulations, we show that the regression models with the first HPCA have the\nbest accuracy prediction and model fit summaries among no HPCA and all HPCAs\nwith a training/testing approach. Finally, we applied our methodology to the\nEEG-fMRI dataset. Conclusions: We conclude that our methodology improves the\nprediction of the experiments with the EEG datasets. And we recommend that\ninstead of using the functional PCA on the desired dimension, reconstruct the\ndata with HPCA and average it on the other two dimensions for functional\nregression models.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 16:26:47 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Fayaz", "Mohammad", ""], ["Abadi", "Alireza", ""], ["Khodakarim", "Soheila", ""]]}, {"id": "2101.08162", "submitter": "Steven Miller", "authors": "George Clark, Alex Gonye and Steven J Miller", "title": "Lessons from the German Tank Problem", "comments": "Version 2.1, 17 pages, 9 figures, to appear in the Mathematical\n  Intelligencer, fixed two typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  During World War II the German army used tanks to devastating advantage. The\nAllies needed accurate estimates of their tank production and deployment. They\nused two approaches to find these values: spies, and statistics. This note\ndescribes the statistical approach. Assuming the tanks are labeled\nconsecutively starting at 1, if we observe $k$ serial numbers from an unknown\nnumber $N$ of tanks, with the maximum observed value $m$, then the best\nestimate for $N$ is $m(1 + 1/k) - 1$. This is now known as the German Tank\nProblem, and is a terrific example of the applicability of mathematics and\nstatistics in the real world. The first part of the paper reproduces known\nresults, specifically deriving this estimate and comparing its effectiveness to\nthat of the spies. The second part presents a result we have not found in print\nelsewhere, the generalization to the case where the smallest value is not\nnecessarily 1. We emphasize in detail why we are able to obtain such clean,\nclosed-form expressions for the estimates, and conclude with an appendix\nhighlighting how to use this problem to teach regression and how statistics can\nhelp us find functional relationships.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 18:52:02 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2021 11:50:37 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Clark", "George", ""], ["Gonye", "Alex", ""], ["Miller", "Steven J", ""]]}, {"id": "2101.11470", "submitter": "Peter Aronow", "authors": "J. Sophia Wang and Peter M. Aronow", "title": "Listwise Deletion in High Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the properties of listwise deletion when both $n$ and the number\nof variables grow large. We show that when (i) all data has some idiosyncratic\nmissingness and (ii) the number of variables grows superlogarithmically in $n$,\nthen, for large $n$, listwise deletion will drop all rows with probability 1.\nUsing two canonical datasets from the study of comparative politics and\ninternational relations, we provide numerical illustration that these problems\nmay emerge in real world settings. These results suggest, in practice, using\nlistwise deletion may mean using few of the variables available to the\nresearcher.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 03:10:39 GMT"}, {"version": "v2", "created": "Sun, 18 Jul 2021 19:49:28 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Wang", "J. Sophia", ""], ["Aronow", "Peter M.", ""]]}, {"id": "2101.11857", "submitter": "Beth Ann Griffin PhD", "authors": "Ricardo Sanchez, Beth Ann Griffin, Daniel McCaffrey", "title": "Best Practices in Scientific Computing", "comments": "22 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The world is becoming increasingly complex, both in terms of the rich sources\nof data we have access to as well as in terms of the statistical and\ncomputational methods we can use on those data. These factors create an\never-increasing risk for errors in our code and sensitivity in our findings to\ndata preparation and execution of complex statistical and computing methods.\nThe consequences of coding and data mistakes can be substantial. Openness\n(e.g., providing others with data code) and transparency (e.g., requiring that\ndata processing and code follow standards) are two key solutions to help\nalleviate concerns about replicability and errors. In this paper, we describe\nthe key steps for implementing a code quality assurance (QA) process for\nresearchers to follow to improve their coding practices throughout a project to\nassure the quality of the final data, code, analyses and ultimately the\nresults. These steps include: (i) adherence to principles for code writing and\nstyle that follow best practices, (ii) clear written documentation that\ndescribes code, workflow and key analytic decisions; (iii) careful version\ncontrol, (iv) good data management; and (iv) regular testing and review.\nFollowing all these steps will greatly improve the ability of a study to assure\nresults are accurate and reproducible. The responsibility for code QA falls not\nonly on individual researchers but institutions, journals, and funding agencies\nas well.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 08:02:51 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 18:13:33 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Sanchez", "Ricardo", ""], ["Griffin", "Beth Ann", ""], ["McCaffrey", "Daniel", ""]]}, {"id": "2101.12150", "submitter": "Marco Panza", "authors": "Domenico Napoletani, Marco Panza, Daniele Struppa", "title": "The Agnostic Structure of Data Science Methods", "comments": "24 pp., no figures, a new abstract and new key-words added", "journal-ref": "Lato Sensu. Revue de la Soci\\'{e}t\\'{e} de philosophie des\n  sciences, 2021", "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper we argue that data science is a coherent and novel approach to\nempirical problems that, in its most general form, does not build understanding\nabout phenomena. Within the new type of mathematization at work in data\nscience, mathematical methods are not selected because of any relevance for a\nproblem at hand; mathematical methods are applied to a specific problem only by\n'forcing', i.e. on the basis of their ability to reorganize the data for\nfurther analysis and the intrinsic richness of their mathematical structure. In\nparticular, we argue that deep learning neural networks are best understood\nwithin the context of forcing optimization methods. We finally explore the\nbroader question of the appropriateness of data science methods in solving\nproblems. We argue that this question should not be interpreted as a search for\na correspondence between phenomena and specific solutions found by data science\nmethods; rather, it is the internal structure of data science methods that is\nopen to precise forms of understanding.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 02:32:35 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 19:59:47 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Napoletani", "Domenico", ""], ["Panza", "Marco", ""], ["Struppa", "Daniele", ""]]}]