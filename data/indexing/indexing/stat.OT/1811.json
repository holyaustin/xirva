[{"id": "1811.01469", "submitter": "Xudong Zhang", "authors": "Xudong Zhang", "title": "Monte Carlo Simulations on robustness of functional location estimator\n  based on several functional depth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional data analysis has been a growing field of study in recent decades,\nand one fundamental task in functional data analysis is estimating the sample\nlocation. A notion called statistical depth has been extended from multivariate\ndata to functional data, and it can provide a center-outward order for each\nobservation within a sample of functional curves. Making use of this intuitive\nnature of depth methods, a depth-based trimmed mean where curves with lower\ndepth values are excluded can be used as a robust location estimator for the\nsample. In this project, we first introduced several state-of-the-art depth\napproaches for functional data. These depths were half region depth, functional\nmajority depth, band depth, modified band depth and functional spatial depth.\nThen we described a robust location estimator based on functional depth, and\nstudied performances of these estimators based on different functional depth\napproaches via simulation tests. Finally, the test results showed that\nestimators based on functional spatial depth and modified band depth exhibited\nsuperior performances.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 01:05:02 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Zhang", "Xudong", ""]]}, {"id": "1811.01821", "submitter": "Lincoln Colling", "authors": "Lincoln J Colling and Denes Szucs", "title": "Statistical reform and the replication crisis", "comments": "17 Pages, 3 Figures", "journal-ref": null, "doi": "10.1007/s13164-018-0421-4", "report-no": null, "categories": "stat.ME stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The replication crisis has prompted many to call for statistical reform\nwithin the psychological sciences. Here we examine issues within Frequentist\nstatistics that may have led to the replication crisis, and we examine the\nalternative---Bayesian statistics---that many have suggested as a replacement.\nThe Frequentist approach and the Bayesian approach offer radically different\nperspectives on evidence and inference with the Frequentist approach\nprioritising error control and the Bayesian approach offering a formal method\nfor quantifying the relative strength of evidence for hypotheses. We suggest\nthat rather than mere statistical reform, what is needed is a better\nunderstanding of the different modes of statistical inference and a better\nunderstanding of how statistical inference relates to scientific inference.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 16:09:15 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Colling", "Lincoln J", ""], ["Szucs", "Denes", ""]]}, {"id": "1811.02021", "submitter": "Jacob Fiksel", "authors": "Jacob Fiksel, Johanna S. Hardin, Leah R. Jager, and Margaret A. Taub", "title": "Using GitHub Classroom To Teach Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Git and GitHub are common tools for keeping track of multiple versions of\ndata analytic content, which allow for more than one person to simultaneously\nwork on a project. GitHub Classroom aims to provide a way for students to work\non and submit their assignments via Git and GitHub, giving teachers an\nopportunity to teach these version control tools as part of their course. In\nthe Fall 2017 semester, we implemented GitHub Classroom in two educational\nsettings--an introductory computational statistics lab and a more advanced\ncomputational statistics course. We found many educational benefits of\nimplementing GitHub Classroom, such as easily providing coding feedback during\nassignments and making students more confident in their ability to collaborate\nand use version control tools for future data science work. To encourage and\nease the transition into using GitHub Classroom, we provide free and publicly\navailable resources--both for students to begin using Git/GitHub and for\nteachers to use GitHub Classroom for their own courses.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 20:30:49 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Fiksel", "Jacob", ""], ["Hardin", "Johanna S.", ""], ["Jager", "Leah R.", ""], ["Taub", "Margaret A.", ""]]}, {"id": "1811.02201", "submitter": "William Leeb", "authors": "William Leeb and Elad Romanov", "title": "Optimal spectral shrinkage and PCA with heteroscedastic noise", "comments": null, "journal-ref": null, "doi": "10.1109/TIT.2021.3055075", "report-no": null, "categories": "stat.OT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the related problems of prediction, covariance estimation,\nand principal component analysis for the spiked covariance model with\nheteroscedastic noise. We consider an estimator of the principal components\nbased on whitening the noise, and we derive optimal singular value and\neigenvalue shrinkers for use with these estimated principal components.\nUnderlying these methods are new asymptotic results for the high-dimensional\nspiked model with heteroscedastic noise, and consistent estimators for the\nrelevant population parameters. We extend previous analysis on out-of-sample\nprediction to the setting of predictors with whitening. We demonstrate certain\nadvantages of noise whitening. Specifically, we show that in a certain\nasymptotic regime, optimal singular value shrinkage with whitening converges to\nthe best linear predictor, whereas without whitening it converges to a\nsuboptimal linear predictor. We prove that for generic signals, whitening\nimproves estimation of the principal components, and increases a natural\nsignal-to-noise ratio of the observations. We also show that for rank one\nsignals, our estimated principal components achieve the asymptotic minimax\nrate.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 07:22:50 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2019 21:04:09 GMT"}, {"version": "v3", "created": "Wed, 13 Nov 2019 16:25:37 GMT"}, {"version": "v4", "created": "Wed, 16 Sep 2020 22:09:34 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Leeb", "William", ""], ["Romanov", "Elad", ""]]}, {"id": "1811.03558", "submitter": "Darrick Lee", "authors": "Chad Giusti and Darrick Lee", "title": "Iterated Integrals and Population Time Series Analysis", "comments": "24 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the core advantages topological methods for data analysis provide is\nthat the language of (co)chains can be mapped onto the semantics of the data,\nproviding a natural avenue for human understanding of the results. Here, we\ndescribe such a semantic structure on Chen's classical iterated integral\ncochain model for paths in Euclidean space. Specifically, in the context of\npopulation time series data, we observe that iterated integrals provide a\nmodel-free measure of pairwise influence that can be used for causality\ninference. Along the way, we survey recent results and applications, review the\ncurrent standard methods for causality inference, and briefly provide our\noutlook on generalizations to go beyond time series data.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 00:50:33 GMT"}, {"version": "v2", "created": "Sat, 27 Apr 2019 18:34:18 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Giusti", "Chad", ""], ["Lee", "Darrick", ""]]}, {"id": "1811.03578", "submitter": "Eric Vance", "authors": "Eric A. Vance and Heather S. Smith", "title": "The ASCCR Frame for Learning Essential Collaboration Skills", "comments": "12 pages, 1 figure. Updated to this Version 5 by adding a few more\n  references, discussing how to teach ASCCR in the classroom, calling on others\n  to add to research supporting the use of the ASCCR Frame, and adding\n  discussion of ethics and reproducible research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistics and data science are especially collaborative disciplines that\ntypically require practitioners to interact with many different people or\ngroups. Consequently, interdisciplinary collaboration skills are part of the\npersonal and professional skills essential for success as an applied\nstatistician or data scientist. These skills are learnable and teachable, and\nlearning and improving collaboration skills provides a way to enhance one's\npractice of statistics and data science. To help individuals learn these skills\nand organizations to teach them, we have developed a framework covering five\nessential components of statistical collaboration: Attitude, Structure,\nContent, Communication, and Relationship. We call this the ASCCR Frame. This\nframework can be incorporated into formal training programs in the classroom or\non the job and can also be used by individuals through self-study. We show how\nthis framework can be applied specifically to statisticians and data scientists\nto improve their collaboration skills and their interdisciplinary impact. We\nbelieve that the ASCCR Frame can help organize and stimulate research and\nteaching in interdisciplinary collaboration and call on individuals and\norganizations to begin generating evidence regarding its effectiveness.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 17:59:50 GMT"}, {"version": "v2", "created": "Thu, 14 Mar 2019 21:47:26 GMT"}, {"version": "v3", "created": "Thu, 28 Mar 2019 00:29:26 GMT"}, {"version": "v4", "created": "Fri, 9 Aug 2019 17:34:04 GMT"}, {"version": "v5", "created": "Fri, 30 Aug 2019 17:16:05 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Vance", "Eric A.", ""], ["Smith", "Heather S.", ""]]}, {"id": "1811.04079", "submitter": "Soumaya Azzi", "authors": "Soumaya Azzi, Yuanyuan Huang, Bruno Sudret, Joe Wiart", "title": "Surrogate Modeling of Stochastic Functions - Application to\n  computational Electromagnetic Dosimetry", "comments": "24 pages, 7 figures, submitted to the 'International Journal for\n  Uncertainty Quantification'", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metamodeling of complex numerical systems has recently attracted the interest\nof the mathematical programming community. Despite the progress in high\nperformance computing, simulations remain costly, as a matter of fact, the\nassessment of the exposure to radio frequency electromagnetic fields is\ncomputationally prohibitive since one simulation can require hours. Moreover,\nin many engineering problems, carrying out deterministic numerical operations\nwithout considering uncertainties can lead to unreliable designs. In this paper\nwe focus on the surrogate modeling of a particular type of computational models\ncalled stochastic simulators. In contrast to deterministic simulators which\nyield a unique output for each set of input parameters, stochastic simulators\ninherently contain some sources of randomness and the output at a given point\nis a probability density function. Characterizing the stochastic simulators is\neven more time consuming. This paper represents stochastic simulators as a\nstochastic process and describes a metamodeling approach based on the\nKarhunen-Lo\\`eve spectral decomposition.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 15:27:10 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Azzi", "Soumaya", ""], ["Huang", "Yuanyuan", ""], ["Sudret", "Bruno", ""], ["Wiart", "Joe", ""]]}, {"id": "1811.04525", "submitter": "Berna Devezer", "authors": "Bert Baumgaertner, Berna Devezer, Erkan O. Buzbas, Luis G. Nardin", "title": "Openness and Reproducibility: Insights from a Model-Centric Approach", "comments": "22 pages, 2 figures. All authors contributed equally. Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the conceptual relationship between openness and\nreproducibility using a model-centric approach, heavily informed by probability\ntheory and statistics. We first clarify the concepts of reliability,\nauditability, replicability, and reproducibility--each of which denotes a\npotential scientific objective. Then we advance a conceptual analysis to\ndelineate the relationship between open scientific practices and these\nobjectives. Using the notion of an idealized experiment, we identify which\ncomponents of an experiment need to be reported and which need to be repeated\nto achieve the relevant objective. The model-centric framework we propose aims\nto contribute precision and clarity to the discussions surrounding the\nso-called reproducibility crisis.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 01:36:23 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 19:21:31 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Baumgaertner", "Bert", ""], ["Devezer", "Berna", ""], ["Buzbas", "Erkan O.", ""], ["Nardin", "Luis G.", ""]]}, {"id": "1811.06980", "submitter": "Antonio Irpino PhD", "authors": "Antonio Irpino, Francisco De Carvalho, Rosanna Verde, Antonio\n  Balzanella", "title": "Batch Self Organizing maps for distributional data using adaptive\n  distances", "comments": "41 pages, 9 figures, some prelimiary results presented at SIS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The paper deals with a Batch Self Organizing Map algorithm (DBSOM) for data\ndescribed by distributional-valued variables. This kind of variables is\ncharacterized to take as values one-dimensional probability or frequency\ndistributions on a numeric support. The objective function optimized in the\nalgorithm depends on the choice of the distance measure. According to the\nnature of the date, the $L_2$ Wasserstein distance is proposed as one of the\nmost suitable metrics to compare distributions. It is widely used in several\ncontexts of analysis of distributional data. Conventional batch SOM algorithms\nconsider that all variables are equally important for the training of the SOM.\nHowever, it is well known that some variables are less relevant than others for\nthis task. In order to take into account the different contribution of the\nvariables we propose an adaptive version of the DBSOM algorithm that tackles\nthis problem with an additional step: a relevance weight is automatically\nlearned for each distributional-valued variable. Moreover, since the $L_2$\nWasserstein distance allows a decomposition into two components: one related to\nthe means and one related to the size and shape of the distributions, also\nrelevance weights are automatically learned for each of the measurement\ncomponents to emphasize the importance of the different estimated parameters of\nthe distributions. Examples of real and synthetic datasets of distributional\ndata illustrate the usefulness of the proposed DBSOM algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 16:37:49 GMT"}, {"version": "v2", "created": "Thu, 10 Jan 2019 13:12:25 GMT"}, {"version": "v3", "created": "Fri, 29 Mar 2019 17:06:49 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Irpino", "Antonio", ""], ["De Carvalho", "Francisco", ""], ["Verde", "Rosanna", ""], ["Balzanella", "Antonio", ""]]}, {"id": "1811.08566", "submitter": "Bradley Eck", "authors": "Bei Chen and Bradley Eck and Francesco Fusco and Robert Gormally and\n  Mark Purcell and Mathieu Sinn and Seshu Tirupathi", "title": "Castor: Contextual IoT Time Series Data and Model Management at Scale", "comments": "6 pages, 6 figures, ICDM 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate Castor, a cloud-based system for contextual IoT time series\ndata and model management at scale. Castor is designed to assist Data\nScientists in (a) exploring and retrieving all relevant time series and\ncontextual information that is required for their predictive modelling tasks;\n(b) seamlessly storing and deploying their predictive models in a cloud\nproduction environment; (c) monitoring the performance of all predictive models\nin production and (semi-)automatically retraining them in case of performance\ndeterioration. The main features of Castor are: (1) an efficient pipeline for\ningesting IoT time series data in real time; (2) a scalable, hybrid data\nmanagement service for both time series and contextual data; (3) a versatile\nsemantic model for contextual information which can be easily adopted to\ndifferent application domains; (4) an abstract framework for developing and\nstoring predictive models in R or Python; (5) deployment services which\nautomatically train and/or score predictive models upon user-defined\nconditions. We demonstrate Castor for a real-world Smart Grid use case and\ndiscuss how it can be adopted to other application domains such as Smart\nBuildings, Telecommunication, Retail or Manufacturing.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 11:26:16 GMT"}, {"version": "v2", "created": "Fri, 23 Nov 2018 14:04:23 GMT"}, {"version": "v3", "created": "Fri, 8 Feb 2019 13:26:19 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Chen", "Bei", ""], ["Eck", "Bradley", ""], ["Fusco", "Francesco", ""], ["Gormally", "Robert", ""], ["Purcell", "Mark", ""], ["Sinn", "Mathieu", ""], ["Tirupathi", "Seshu", ""]]}, {"id": "1811.10375", "submitter": "Ruaridh Macdonald", "authors": "Ruaridh. R Macdonald and R. Scott Kemp", "title": "Quantifying Privacy in Nuclear Warhead Authentication Protocols", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  International verification of nuclear warheads is a practical problem in\nwhich the protection of secret warhead information is of paramount importance.\nWe propose a measure that would enable a weapon owner to evaluate the privacy\nof a proposed protocol in a technology-neutral fashion. We show the problem is\nreducible to `natural' and `corrective' learning. The natural learning can be\ncomputed without assumptions about the inspector, while the corrective learning\naccounts for the inspector's prior knowledge. The natural learning provides the\nwarhead owner a useful lower bound on the information leaked by the proposed\nprotocol. Using numerical examples, we demonstrate that the proposed measure\ncorrelates better with the accuracy of a maximum a posteriori probability\nestimate than alternative measures.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 15:55:46 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Macdonald", "Ruaridh. R", ""], ["Kemp", "R. Scott", ""]]}, {"id": "1811.10603", "submitter": "Gane Samb Lo", "authors": "Cherif Mamadou Moctar Traor\\'e, Moumouni Diallo, Gane Samb Lo,\n  Mouhamad Ahsanullah, Okereke Lois Chinwendu", "title": "On some properties of the new Sine-skewed Cardioid Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The new Sine Skewed Cardioid (ssc) distribution been just introduced and\ncharacterized by Ahsanullah (2018). Here, we study the asymptotic properties of\nits tails by determining its extreme value domain, the characteristic function,\nthe moments and likelihood estimators of the two parameters, the asymptotic\nnormality of the moments estimators and the random generation of data from the\n\\textit{ssc} distribution. Finally, we proceed to a simulation study to show\nthe performance of the random generation method and the quality of the moments\nestimation of the parameters.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 22:34:57 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Traor\u00e9", "Cherif Mamadou Moctar", ""], ["Diallo", "Moumouni", ""], ["Lo", "Gane Samb", ""], ["Ahsanullah", "Mouhamad", ""], ["Chinwendu", "Okereke Lois", ""]]}, {"id": "1811.11301", "submitter": "Matthew Norton", "authors": "Matthew Norton, Valentyn Khokhlov, Stan Uryasev", "title": "Calculating CVaR and bPOE for Common Probability Distributions With\n  Application to Portfolio Optimization and Density Estimation", "comments": "Fixed typo in Proposition 5 (changed - to +) and added reference", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM math.OC math.PR stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional Value-at-Risk (CVaR) and Value-at-Risk (VaR), also called the\nsuperquantile and quantile, are frequently used to characterize the tails of\nprobability distribution's and are popular measures of risk. Buffered\nProbability of Exceedance (bPOE) is a recently introduced characterization of\nthe tail which is the inverse of CVaR, much like the CDF is the inverse of the\nquantile. These quantities can prove very useful as the basis for a variety of\nrisk-averse parametric engineering approaches. Their use, however, is often\nmade difficult by the lack of well-known closed-form equations for calculating\nthese quantities for commonly used probability distribution's. In this paper,\nwe derive formulas for the superquantile and bPOE for a variety of common\nunivariate probability distribution's. Besides providing a useful collection\nwithin a single reference, we use these formulas to incorporate the\nsuperquantile and bPOE into parametric procedures. In particular, we consider\ntwo: portfolio optimization and density estimation. First, when portfolio\nreturns are assumed to follow particular distribution families, we show that\nfinding the optimal portfolio via minimization of bPOE has advantages over\nsuperquantile minimization. We show that, given a fixed threshold, a single\nportfolio is the minimal bPOE portfolio for an entire class of distribution's\nsimultaneously. Second, we apply our formulas to parametric density estimation\nand propose the method of superquantile's (MOS), a simple variation of the\nmethod of moment's (MM) where moment's are replaced by superquantile's at\ndifferent confidence levels. With the freedom to select various combinations of\nconfidence levels, MOS allows the user to focus the fitting procedure on\ndifferent portions of the distribution, such as the tail when fitting\nheavy-tailed asymmetric data.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 23:00:06 GMT"}, {"version": "v2", "created": "Sun, 17 Feb 2019 18:44:33 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Norton", "Matthew", ""], ["Khokhlov", "Valentyn", ""], ["Uryasev", "Stan", ""]]}]