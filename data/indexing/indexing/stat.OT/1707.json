[{"id": "1707.00944", "submitter": "Sergio Lopes", "authors": "G. Corso, T. L. Prado, G. Z. dos S. Lima, and S. R. Lopes", "title": "A novel entropy recurrence quantification analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing study of time series, especially those related to nonlinear\nsystems, has challenged the methodologies to characterize and classify\ndynamical structures of a signal. Here we conceive a new diagnostic tool for\ntime series based on the concept of information entropy, in which the\nprobabilities are associated to microstates defined from the recurrence phase\nspace. Recurrence properties can properly be studied using recurrence plots, a\nmethodology based on binary matrices where trajec- tories in phase space of\ndynamical systems are evaluated against other embedded trajectory. Our novel\nentropy methodology has several advantages compared to the traditional\nrecurrence entropy defined in the literature, namely, the correct evaluation of\nthe chaoticity level of the signal, the weak dependence on parameters, correct\nevaluation of periodic time series properties and more sensitivity to noise\nlevel of time series. Furthermore, the new entropy quantifier developed in this\nmanuscript also fixes inconsistent results of the traditional recurrence\nentropy concept, reproducing classical results with novel insights.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 12:46:15 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Corso", "G.", ""], ["Prado", "T. L.", ""], ["Lima", "G. Z. dos S.", ""], ["Lopes", "S. R.", ""]]}, {"id": "1707.01031", "submitter": "Mohammad S. Jalali", "authors": "M. S. Jalali", "title": "Decision-Making and Biases in Cybersecurity Capability Development:\n  Evidence from a Simulation Game Experiment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.HC math.DS stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We developed a simulation game to study the effectiveness of decision-makers\nin overcoming two complexities in building cybersecurity capabilities:\npotential delays in capability development; and uncertainties in predicting\ncyber incidents. Analyzing 1,479 simulation runs, we compared the performances\nof a group of experienced professionals with those of an inexperienced control\ngroup. Experienced subjects did not understand the mechanisms of delays any\nbetter than inexperienced subjects; however, experienced subjects were better\nable to learn the need for proactive decision-making through an iterative\nprocess. Both groups exhibited similar errors when dealing with the uncertainty\nof cyber incidents. Our findings highlight the importance of training for\ndecision-makers with a focus on systems thinking skills, and lay the groundwork\nfor future research on uncovering mental biases about the complexities of\ncybersecurity.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 15:17:38 GMT"}, {"version": "v2", "created": "Tue, 15 Aug 2017 19:23:07 GMT"}, {"version": "v3", "created": "Mon, 8 Jan 2018 22:14:17 GMT"}, {"version": "v4", "created": "Tue, 3 Jul 2018 00:06:29 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Jalali", "M. S.", ""]]}, {"id": "1707.02070", "submitter": "Manuele Leonelli", "authors": "Manuele Leonelli, Eva Riccomagno, Jim Q. Smith", "title": "Coherent combination of probabilistic outputs for group decision making:\n  an algebraic approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current decision support systems address domains that are heterogeneous in\nnature and becoming progressively larger. Such systems often require the input\nof expert judgement about a variety of different fields and an intensive\ncomputational power to produce the scores necessary to rank the available\npolicies. Recently, integrating decision support systems have been introduced\nto enable a formal Bayesian multi-agent decision analysis to be distributed and\nconsequently efficient. In such systems, where different panels of experts\noversee disjoint but correlated vectors of variables, each expert group needs\nto deliver only certain summaries of the variables under their jurisdiction to\nproperly derive an overall score for the available policies. Here we present an\nalgebraic approach that makes this methodology feasible for a wide range of\nmodelling contexts and that enables us to identify the summaries needed for\nsuch a combination of judgements. We are also able to demonstrate that\ncoherence, in a sense we formalize here, is still guaranteed when panels only\nshare a partial specification of their model with other panel members. We\nillustrate this algebraic approach by applying it to a specific class of\nBayesian networks and demonstrate how we can use it to derive closed form\nformulae for the computations of the joint moments of variables that determine\nthe score of different policies.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 08:16:52 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Leonelli", "Manuele", ""], ["Riccomagno", "Eva", ""], ["Smith", "Jim Q.", ""]]}, {"id": "1707.02748", "submitter": "Marco Gazel", "authors": "Louis L\\'evy-Garboua (CIRANO, PSE, CES), Muniza Askari (CES), Marco\n  Gazel (PSE, CES)", "title": "Confidence biases and learning among intuitive Bayesians", "comments": "Theory and Decision, Springer Verlag, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design a double-or-quits game to compare the speed of learning one's\nspecific ability with the speed of rising confidence as the task gets\nincreasingly difficult. We find that people on average learn to be\noverconfident faster than they learn their true ability and we present an\nintuitive-Bayesian model of confidence which integrates confidence biases and\nlearning. Uncertainty about one's true ability to perform a task in isolation\ncan be responsible for large and stable confidence biases, namely limited\ndiscrimination, the hard--easy effect, the Dunning--Kruger effect, conservative\nlearning from experience and the overprecision phenomenon (without\nunderprecision) if subjects act as Bayesian learners who rely only on\nsequentially perceived performance cues and contrarian illusory signals induced\nby doubt. Moreover, these biases are likely to persist since the Bayesian\naggregation of past information consolidates the accumulation of errors and the\nperception of contrarian illusory signals generates conservatism and\nunder-reaction to events. Taken together, these two features may explain why\nintuitive Bayesians make systematically wrong predictions of their own\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 08:50:43 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["L\u00e9vy-Garboua", "Louis", "", "CIRANO, PSE, CES"], ["Askari", "Muniza", "", "CES"], ["Gazel", "Marco", "", "PSE, CES"]]}, {"id": "1707.05769", "submitter": "Mohammad Kazemi", "authors": "Mohammad Kazemi and Mina Azizpour", "title": "Estimation of Inverse Weibull Distribution Under Type-I Hybrid Censoring", "comments": "This paper is under review in the Austrian Journal of Statistics and\n  will likely be published there", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The hybrid censoring is a mixture of Type I and Type II censoring schemes.\nThis paper presents the statistical inferences of the Inverse Weibull\ndistribution when the data are Type-I hybrid censored. First we consider the\nmaximum likelihood estimators of the unknown parameters. It is observed that\nthe maximum likelihood estimators can not be obtained in closed form. We\nfurther obtain the Bayes estimators and the corresponding highest posterior\ndensity credible intervals of the unknown parameters under the assumption of\nindependent gamma priors using the importance sampling procedure. We also\ncompute the approximate Bayes estimators using Lindley's approximation\ntechnique. We have performed a simulation study and a real data analysis in\norder to compare the proposed Bayes estimators with the maximum likelihood\nestimators.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 17:51:04 GMT"}, {"version": "v2", "created": "Sat, 4 Apr 2020 10:30:34 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Kazemi", "Mohammad", ""], ["Azizpour", "Mina", ""]]}, {"id": "1707.05804", "submitter": "Mohammad Kazemi", "authors": "Akbar Asgharzadeh, Mohammad Kazemi, Debasis Kundu", "title": "Estimation of P(X > Y ) for Weibull distribution based on hybrid\n  censored samples", "comments": null, "journal-ref": null, "doi": "10.1007/s13198-015-0390-2", "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Hybrid censoring scheme is mixture of Type-I and Type-II censoring schemes.\nBased on hybrid censored samples, this paper deals with the in- ference on R =\nP(X > Y ), when X and Y are two independent Weibull distributions with\ndifferent scale parameters, but having the same shape pa- rameter. The maximum\nlikelihood estimator (MLE), and the approximate MLE (AMLE) of R are obtained.\nThe asymptotic distribution of the maxi- mum likelihood estimator of R is\nobtained. Based on the asymptotic distribu- tion, the confidence interval of R\ncan be derived. Two bootstrap confidence intervals are also proposed. We\nconsider the Bayesian estimate of R, and propose the corresponding credible\ninterval for R. Monte Carlo simulations are performed to compare the different\nproposed methods. Analysis of a real data set has also been presented for\nillustrative purposes.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 18:16:31 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Asgharzadeh", "Akbar", ""], ["Kazemi", "Mohammad", ""], ["Kundu", "Debasis", ""]]}, {"id": "1707.06537", "submitter": "Matthew Beckman", "authors": "Matthew D. Beckman, Robert C. delMas, Joan Garfield", "title": "Cognitive Transfer Outcomes for a Simulation-Based Introductory\n  Statistics Curriculum", "comments": null, "journal-ref": "Statistics Education Research Journal, 16(2), 419-440", "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cognitive transfer is the ability to apply learned skills and knowledge to\nnew applications and contexts. This investigation evaluates cognitive transfer\noutcomes for a tertiary-level introductory statistics course using the CATALST\ncurriculum, which exclusively used simulation-based methods to develop\nfoundations of statistical inference. A common assessment instrument\nadministered at the end of each course measured learning outcomes for students.\nCATALST students showed evidence of both near and far transfer outcomes while\nscoring as high, or higher on the assessed learning objectives, when compared\nwith peers enrolled in similar courses that emphasized parametric inferential\nmethods (e.g. the t-test).\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 14:26:39 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Beckman", "Matthew D.", ""], ["delMas", "Robert C.", ""], ["Garfield", "Joan", ""]]}, {"id": "1707.07607", "submitter": "Arthur Charpentier", "authors": "Arthur Charpentier and Baptiste Coulmont", "title": "We are not alone ! (at least, most of us). Homonymy in large scale\n  social groups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article brings forward an estimation of the proportion of homonyms in\nlarge scale groups based on the distribution of first names and last names in a\nsubset of these groups. The estimation is based on the generalization of the\n\"birthday paradox problem\". The main results is that, in societies such as\nFrance or the United States, identity collisions (based on first + last names)\nare frequent. The large majority of the population has at least one homonym.\nBut in smaller settings, it is much less frequent : even if small groups of a\nfew thousand people have at least one couple of homonyms, only a few\nindividuals have an homonym.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 15:30:05 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Charpentier", "Arthur", ""], ["Coulmont", "Baptiste", ""]]}, {"id": "1707.07625", "submitter": "Olga Goulko", "authors": "Olga Goulko, Nikolay Prokof'ev, Boris Svistunov", "title": "Restoring a smooth function from its noisy integrals", "comments": "11 pages, 9 figures, published version", "journal-ref": "Phys. Rev. E 97, 053305 (2018)", "doi": "10.1103/PhysRevE.97.053305", "report-no": null, "categories": "stat.OT cond-mat.other eess.SP physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerical (and experimental) data analysis often requires the restoration of\na smooth function from a set of sampled integrals over finite bins. We present\nthe bin hierarchy method that efficiently computes the maximally smooth\nfunction from the sampled integrals using essentially all the information\ncontained in the data. We perform extensive tests with different classes of\nfunctions and levels of data quality, including Monte Carlo data suffering from\na severe sign problem and physical data for the Green's function of the\nFr\\\"ohlich polaron.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 15:24:06 GMT"}, {"version": "v2", "created": "Fri, 11 May 2018 15:25:45 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Goulko", "Olga", ""], ["Prokof'ev", "Nikolay", ""], ["Svistunov", "Boris", ""]]}, {"id": "1707.09076", "submitter": "Maya Mathur", "authors": "Maya B. Mathur and Tyler J. VanderWeele", "title": "Sensitivity Analysis for Unmeasured Confounding in Meta-Analyses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random-effects meta-analyses of observational studies can produce biased\nestimates if the synthesized studies are subject to unmeasured confounding. We\npropose sensitivity analyses quantifying the extent to which unmeasured\nconfounding of specified magnitude could reduce to below a certain threshold\nthe proportion of true effect sizes that are scientifically meaningful. We also\ndevelop converse methods to estimate the strength of confounding capable of\nreducing the proportion of scientifically meaningful true effects to below a\nchosen threshold. These methods apply when a \"bias factor\" is assumed to be\nnormally distributed across studies or is assessed across a range of fixed\nvalues. Our estimators are derived using recently proposed sharp bounds on\nconfounding bias within a single study that do not make assumptions regarding\nthe unmeasured confounders themselves or the functional form of their\nrelationships to the exposure and outcome of interest. We provide an R package,\nConfoundedMeta, and a freely available online graphical user interface that\ncompute point estimates and inference and produce plots for conducting such\nsensitivity analyses. These methods facilitate principled use of random-effects\nmeta-analyses of observational studies to assess the strength of causal\nevidence for a hypothesis.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 00:06:44 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Mathur", "Maya B.", ""], ["VanderWeele", "Tyler J.", ""]]}, {"id": "1707.09319", "submitter": "Hrushikesh Mhaskar", "authors": "Charles K. Chui, Hrushikesh N. Mhaskar", "title": "A Fourier-invariant method for locating point-masses and computing their\n  attributes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT cs.LG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the interest of observing the growth of cancer cells among\nnormal living cells and exploring how galaxies and stars are truly formed, the\nobjective of this paper is to introduce a rigorous and effective method for\ncounting point-masses, determining their spatial locations, and computing their\nattributes. Based on computation of Hermite moments that are Fourier-invariant,\nour approach facilitates the processing of both spatial and Fourier data in any\ndimension.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 20:24:41 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Chui", "Charles K.", ""], ["Mhaskar", "Hrushikesh N.", ""]]}]