[{"id": "1611.03072", "submitter": "Fergus Simpson", "authors": "Fergus Simpson", "title": "Apocalypse Now? Reviving the Doomsday Argument", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT physics.pop-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whether the fate of our species can be forecast from its past has been the\ntopic of considerable controversy. One refutation of the so-called Doomsday\nArgument is based on the premise that we are more likely to exist in a universe\ncontaining a greater number of observers. Here we present a Bayesian\nreformulation of the Doomsday Argument which is immune to this effect. By\nmarginalising over the spatial configuration of observers, we find that any\npreference for a larger total number of observers has no impact on the inferred\nlocal number. Our results remain unchanged when we adopt either the\nSelf-Indexing Assumption (SIA) or the Self-Sampling Assumption (SSA).\nFurthermore the median value of our posterior distribution is found to be in\nagreement with the frequentist forecast. Humanity's prognosis for the coming\ncentury is well approximated by a global catastrophic risk of 0.2% per year.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 13:22:12 GMT"}], "update_date": "2016-11-10", "authors_parsed": [["Simpson", "Fergus", ""]]}, {"id": "1611.03073", "submitter": "Edda Klipp", "authors": "Andrea Auconi, Andrea Giansanti, and Edda Klipp", "title": "Causal influence in linear response models", "comments": "9 pages, 9 figures", "journal-ref": "Phys. Rev. E 95, 042315 (2017)", "doi": "10.1103/PhysRevE.95.042315", "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The intuition of causation is so fundamental that almost every research study\nin life sciences refers to this concept. However a widely accepted formal\ndefinition of causal influence between observables is still missing. In the\nframework of linear Langevin networks without feedbacks (linear response\nmodels) we developed a measure of causal influence based on a decomposition of\ninformation flows over time. We discuss its main properties and compare it with\nother information measures like the Transfer Entropy. Finally we outline some\ndifficulties of the extension to a general definition of causal influence for\ncomplex systems.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 11:41:42 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Auconi", "Andrea", ""], ["Giansanti", "Andrea", ""], ["Klipp", "Edda", ""]]}, {"id": "1611.03974", "submitter": "Hien Nguyen", "authors": "Hien D. Nguyen, Geoffrey J. McLachlan", "title": "On approximations via convolution-defined mixture models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An often-cited fact regarding mixing or mixture distributions is that their\ndensity functions are able to approximate the density function of any unknown\ndistribution to arbitrary degrees of accuracy, provided that the mixing or\nmixture distribution is sufficiently complex. This fact is often not made\nconcrete. We investigate and review theorems that provide approximation bounds\nfor mixing distributions. Connections between the approximation bounds of\nmixing distributions and estimation bounds for the maximum likelihood estimator\nof finite mixtures of location- scale distributions are reviewed.\n", "versions": [{"version": "v1", "created": "Sat, 12 Nov 2016 09:13:12 GMT"}, {"version": "v2", "created": "Wed, 28 Dec 2016 22:29:54 GMT"}, {"version": "v3", "created": "Fri, 2 Mar 2018 04:53:04 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Nguyen", "Hien D.", ""], ["McLachlan", "Geoffrey J.", ""]]}, {"id": "1611.06168", "submitter": "Patrick Laurie Davies Mr", "authors": "Laurie Davies", "title": "On $p$-values", "comments": "20 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models are consistently treated as approximations and all procedures are\nconsistent with this. They do not treat the model as being true. In this\ncontext $p$-values are one measure of approximation, a small $p$-value\nindicating a poor approximation. Approximation regions are defined and\ndistinguished from confidence regions.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 17:32:40 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Davies", "Laurie", ""]]}, {"id": "1611.06545", "submitter": "Andr\\'e C. R. Martins", "authors": "Andr\\'e C. R. Martins", "title": "Stop the tests: Opinion bias and statistical tests", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When statisticians quarrel about hypothesis testing, the debate usually focus\non which method is the correct one. The fundamental question of whether we\nshould test hypothesis at all tends to be forgotten. This lack of debate has\nits roots on our desire to have ideas we believe and defend. But cognitive\nexperiments have been showing that, when we do choose ideas, we become prey to\na large number of biases. Several of our biases can be grouped together in a\nsingle description, an opinion bias. This opinion bias is nothing more than our\ndesire to believe in something and to defend it. Also, despite our feelings,\nbelieving has no solid logical or philosophical grounds. In this paper, I will\nshow that if we combine the fact that even logic can never prove an idea right\nor wrong and the problems our brains cause when we pick ideas, hypothesis\ntesting and its terminology are a recipe for disaster. Testing should have no\nplace when we are thinking about hypothesis.\n", "versions": [{"version": "v1", "created": "Sun, 20 Nov 2016 16:58:34 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Martins", "Andr\u00e9 C. R.", ""]]}, {"id": "1611.08118", "submitter": "Gonzalo  Garc\\'ia-Donato", "authors": "Gonzalo Garcia-Donato and Anabel Forte", "title": "BayesVarSel: Bayesian Testing, Variable Selection and model averaging in\n  Linear Models using R", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the R package BayesVarSel which implements objective\nBayesian methodology for hypothesis testing and variable selection in linear\nmodels. The package computes posterior probabilities of the competing\nhypotheses/models and provides a suite of tools, specifically proposed in the\nliterature, to properly summarize the results. Additionally, \\ourpack\\ is armed\nwith functions to compute several types of model averaging estimations and\npredictions with weights given by the posterior probabilities. BayesVarSel\ncontains exact algorithms to perform fast computations in problems of small to\nmoderate size and heuristic sampling methods to solve large problems. The\nsoftware is intended to appeal to a broad spectrum of users, so the interface\nhas been carefully designed to be highly intuititive and is inspired by the\nwell-known lm function. The issue of prior inputs is carefully addressed. In\nthe default usage (fully automatic for the user)BayesVarSel implements the\ncriteria-based priors proposed by Bayarri et al (2012), but the advanced user\nhas the possibility of using several other popular priors in the literature.\nThe package is available through the Comprehensive R Archive Network, CRAN. We\nillustrate the use of BayesVarSel with several data examples.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 09:46:13 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Garcia-Donato", "Gonzalo", ""], ["Forte", "Anabel", ""]]}, {"id": "1611.08942", "submitter": "Roberto Rossi", "authors": "Roberto Rossi, \\\"Ozg\\\"ur Akg\\\"un, Steven Prestwich, Armagan Tarim", "title": "The BIN_COUNTS Constraint: Filtering and Applications", "comments": "20 pages, working draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI math.PR stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the BIN_COUNTS constraint, which deals with the problem of\ncounting the number of decision variables in a set which are assigned values\nthat lie in given bins. We illustrate a decomposition and a filtering algorithm\nthat achieves generalised arc consistency. We contrast the filtering power of\nthese two approaches and we discuss a number of applications. We show that\nBIN_COUNTS can be employed to develop a decomposition for the $\\chi^2$ test\nconstraint, a new statistical constraint that we introduce in this work. We\nalso show how this new constraint can be employed in the context of the\nBalanced Academic Curriculum Problem and of the Balanced Nursing Workload\nProblem. For both these problems we carry out numerical studies involving our\nreformulations. Finally, we present a further application of the $\\chi^2$ test\nconstraint in the context of confidence interval analysis.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 00:23:46 GMT"}, {"version": "v2", "created": "Wed, 7 Dec 2016 17:08:35 GMT"}, {"version": "v3", "created": "Thu, 8 Dec 2016 02:02:10 GMT"}, {"version": "v4", "created": "Sat, 10 Dec 2016 15:49:03 GMT"}, {"version": "v5", "created": "Wed, 14 Dec 2016 21:26:10 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Rossi", "Roberto", ""], ["Akg\u00fcn", "\u00d6zg\u00fcr", ""], ["Prestwich", "Steven", ""], ["Tarim", "Armagan", ""]]}]