[{"id": "1912.00568", "submitter": "Ziyan Zhang", "authors": "Ziyan Zhang", "title": "Inference for Synthetic Control Methods with Multiple Treated Units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the Synthetic Control Method (SCM) is now widely applied, its most\ncommonly-used inference method, placebo test, is often problematic, especially\nwhen the treatment is not uniquely assigned. This paper discuss the problems\nwith the placebo test under multivariate treatment case. And, to improve the\npower of inferences, I further propose an Andrews-type procedure as it\npotentially solve some drawbacks of placebo test. Simulations are conducted to\nshow the Andrews' test is often valid and powerful, compared with the placebo\ntest.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 03:13:08 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Zhang", "Ziyan", ""]]}, {"id": "1912.02819", "submitter": "Zhiqiang Hou", "authors": "Dandan Jiang, Jiang Hu, Zhiqiang Hou", "title": "The limits of the sample spiked eigenvalues for a high-dimensional\n  generalized Fisher matrix and its applications", "comments": "21 pages, 36 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A generalized spiked Fisher matrix is considered in this paper. We establish\na criterion for the description of the support of the limiting spectral\ndistribution of high-dimensional generalized Fisher matrix and study the almost\nsure limits of the sample spiked eigenvalues where the population covariance\nmatrices are arbitrary which successively removed an unrealistic condition\nposed in the previous works, that is, the covariance matrices are assumed to be\ndiagonal or diagonal block-wise structure. In addition, we also give a\nconsistent estimator of the population spiked eigenvalues. A series of\nsimulations are conducted that support the theoretical results and illustrate\nthe accuracy of our estimators.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 01:40:22 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Jiang", "Dandan", ""], ["Hu", "Jiang", ""], ["Hou", "Zhiqiang", ""]]}, {"id": "1912.04432", "submitter": "Megha Mehrotra", "authors": "Megha L. Mehrotra, M. Maria Glymour, Elvin Geng, Daniel Westreich,\n  David V. Glidden", "title": "Variable selection for transportability", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transportability provides a principled framework to address the problem of\napplying study results to new populations. Here, we consider the problem of\nselecting variables to include in transport estimators. We provide a brief\noverview of the transportability framework and illustrate that while selection\ndiagrams are a vital first step in variable selection, these graphs alone\nidentify a sufficient but not strictly necessary set of variables for\ngenerating an unbiased transport estimate. Next, we conduct a simulation\nexperiment assessing the impact of including unnecessary variables on the\nperformance of the parametric g-computation transport estimator. Our results\nhighlight that the types of variables included can affect the bias, variance,\nand mean squared error of the estimates. We find that addition of variables\nthat are not causes of the outcome but whose distributions differ between the\nsource and target populations can increase the variance and mean squared error\nof the transported estimates. On the other hand, inclusion of variables that\nare causes of the outcome (regardless of whether they modify the causal\ncontrast of interest or differ in distribution between the populations) reduces\nthe variance of the estimates without increasing the bias. Finally, exclusion\nof variables that cause the outcome but do not modify the causal contrast of\ninterest does not increase bias. These findings suggest that variable selection\napproaches for transport should prioritize identifying and including all causes\nof the outcome in the study population rather than focusing on variables whose\ndistribution may differ between the study sample and target population.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 00:46:35 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Mehrotra", "Megha L.", ""], ["Glymour", "M. Maria", ""], ["Geng", "Elvin", ""], ["Westreich", "Daniel", ""], ["Glidden", "David V.", ""]]}, {"id": "1912.05588", "submitter": "Haiming Zhou", "authors": "Haiming Zhou and Xianzheng Huang", "title": "Parametric mode regression for bounded responses", "comments": "To appear in Biometrical Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose new parametric frameworks of regression analysis with the\nconditional mode of a bounded response as the focal point of interest.\nCovariate effects estimation and prediction based on the maximum likelihood\nmethod under two new classes of regression models are demonstrated. We also\ndevelop graphical and numerical diagnostic tools to detect various sources of\nmodel misspecification. Predictions based on different central tendency\nmeasures inferred using various regression models are compared using synthetic\ndata in simulations. Finally, we conduct regression analysis for data from the\nAlzheimer's Disease Neuroimaging Initiative to demonstrate practical\nimplementation of the proposed methods. Supplementary materials that contain\ntechnical details, and additional simulation and data analysis results are\navailable online.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 19:25:26 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 16:18:33 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Zhou", "Haiming", ""], ["Huang", "Xianzheng", ""]]}, {"id": "1912.07442", "submitter": "Samuel Henry", "authors": "Samuel Henry", "title": "Time-based analysis of the NBA hot hand fallacy", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The debate surrounding the hot hand in the NBA has been ongoing for many\nyears. However, many of the previous works on this theme has focused on only\nthe very next sequential shot attempt, often on very select players. This work\nlooks in more detail the effect of a made or missed shot on the next series of\nshots over a two-year span, with time between shots shown to be a critical\nfactor in the analysis. Also, multi-year streakiness is analyzed, and all\nindications are that players cannot really sustain their good (or bad) fortune\nfrom year to year.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 18:53:14 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Henry", "Samuel", ""]]}, {"id": "1912.08200", "submitter": "Athanasia Mowinckel", "authors": "Athanasia M. Mowinckel and Didac Vidal-Pi\\~neiro", "title": "Visualisation of Brain Statistics with R-packages ggseg and ggseg3d", "comments": "17 pages, 10 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  There is an increased emphasis on visualizing neuroimaging results in more\nintuitive ways. Common statistical tools for dissemination, such as bar charts,\nlack the spatial dimension that is inherent in neuroimaging data. Here we\npresent two packages for the statistical software R, ggseg and ggseg3d, that\nintegrate this spatial component. The ggseg and ggseg3d packages visualize\npre-defined brain segmentations as both 2D polygons and 3D meshes,\nrespectively. Both packages are integrated with other well-established\nR-packages, allowing great flexibility. In this tutorial, we present the main\ndata and functions in the ggseg and ggseg3d packages for brain atlas\nvisualization. The main highlighted functions are able to display brain\nsegmentation plots in R. Further, the accompanying ggsegExtra-package includes\na wider collection of atlases, and is intended for community-based efforts to\ndevelop more compatible atlases to ggseg and ggseg3d. Overall, the\nggseg-packages facilitate parcellation-based visualizations in R, improve and\nease the dissemination of the results, and increase the efficiency of the\nworkflows.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 12:54:53 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Mowinckel", "Athanasia M.", ""], ["Vidal-Pi\u00f1eiro", "Didac", ""]]}, {"id": "1912.08378", "submitter": "Andriy Olenko", "authors": "Philip Broadbridge, Alexander D. Kolesnik, Nikolai Leonenko, Andriy\n  Olenko, Dareen Omari", "title": "Spherically Restricted Random Hyperbolic Diffusion", "comments": "25 pages, 12 figures", "journal-ref": null, "doi": "10.3390/e22020217", "report-no": null, "categories": "math.PR math.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates solutions of hyperbolic diffusion equations in\n$\\mathbb{R}^3$ with random initial conditions. The solutions are given as\nspatial-temporal random fields. Their restrictions to the unit sphere $S^2$ are\nstudied. All assumptions are formulated in terms of the angular power spectrum\nor the spectral measure of the random initial conditions. Approximations to the\nexact solutions are given. Upper bounds for the mean-square convergence rates\nof the approximation fields are obtained. The smoothness properties of the\nexact solution and its approximation are also investigated. It is demonstrated\nthat the H\\\"{o}lder-type continuity of the solution depends on the decay of the\nangular power spectrum. Conditions on the spectral measure of initial\nconditions that guarantee short or long-range dependence of the solutions are\ngiven. Numerical studies are presented to verify the theoretical findings.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 04:53:17 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Broadbridge", "Philip", ""], ["Kolesnik", "Alexander D.", ""], ["Leonenko", "Nikolai", ""], ["Olenko", "Andriy", ""], ["Omari", "Dareen", ""]]}, {"id": "1912.10866", "submitter": "Andrea Macrina", "authors": "Holly Brannelly, Andrea Macrina, Gareth W. Peters", "title": "Quantile Diffusions", "comments": "36 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR q-fin.RM stat.OT", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose a novel approach for the construction of quantile processes\ngoverning the stochastic dynamics of quantiles in continuous time. Two classes\nof quantile diffusions are identified: The first, which we largely focus on,\nfeatures a random quantile level and allows for direct interpretation of model\nparameters such as skewness and kurtosis. The second type are function-valued\nquantile diffusions and are driven by stochastic parameter processes, which\ndetermine the entire quantile function at each point in time. The quantile\nprocesses are obtained by transforming the marginals of a diffusion process\nunder a composite map consisting of a distribution and a quantile function.\nSuch maps, analogous to rank transmutation maps, produce the marginals of the\nresulting quantile process. We discuss the relationship and differences between\nour approach and existing methods and characterisations of quantile processes\nin discrete and continuous time. As an example of an application of quantile\ndiffusions, we show how measure distortions, often found in financial\nmathematics and actuarial science, may be induced.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 14:27:51 GMT"}, {"version": "v2", "created": "Sun, 29 Nov 2020 12:09:04 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Brannelly", "Holly", ""], ["Macrina", "Andrea", ""], ["Peters", "Gareth W.", ""]]}, {"id": "1912.11369", "submitter": "Eduardo M. Vasconcelos", "authors": "Eduardo Vasconcelos, Adriano Souza, Kelvin Dias", "title": "AVaN Pack: An Analytical/Numerical Solution for Variance-Based\n  Sensitivity Analysis", "comments": "13 pages, 1 Figure, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensitivity analysis is an important concept to analyze the influences of\nparameters in a system, an equation or a collection of data. The methods used\nfor sensitivity analysis are divided into deterministic and statistical\ntechniques. Generally, deterministic techniques analyze fixed points of a model\nwhilst stochastic techniques analyze a range of values. Deterministic methods\nfail in analyze the entire range of input values and stochastic methods\ngenerate outcomes with random errors. In this manuscript, we are interested in\nstochastic methods, mainly in variance-based techniques such as Variance and\nSobol indices, since this class of techniques is largely used on literature.\nThe objective of this manuscript is to present an analytical solution for\nvariance based sensitive analysis. As a result of this research, two small\nprograms were developed in Javascript named as AVaN Pack (Analysis of Variance\nthrough Numerical solution). These programs allow users to find the\ncontribution of each individual parameter in any function by means of a\nmathematical solution, instead of sampling-based ones.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 01:33:32 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Vasconcelos", "Eduardo", ""], ["Souza", "Adriano", ""], ["Dias", "Kelvin", ""]]}, {"id": "1912.12527", "submitter": "Lorenzo Trapani", "authors": "Mike Tsionas, Marwan Izzeldin, Lorenzo Trapani", "title": "Bayesian estimation of large dimensional time varying VARs using copulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper provides a simple, yet reliable, alternative to the (Bayesian)\nestimation of large multivariate VARs with time variation in the conditional\nmean equations and/or in the covariance structure. With our new methodology,\nthe original multivariate, n dimensional model is treated as a set of n\nunivariate estimation problems, and cross-dependence is handled through the use\nof a copula. Thus, only univariate distribution functions are needed when\nestimating the individual equations, which are often available in closed form,\nand easy to handle with MCMC (or other techniques). Estimation is carried out\nin parallel for the individual equations. Thereafter, the individual posteriors\nare combined with the copula, so obtaining a joint posterior which can be\neasily resampled. We illustrate our approach by applying it to a large\ntime-varying parameter VAR with 25 macroeconomic variables.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 21:44:16 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Tsionas", "Mike", ""], ["Izzeldin", "Marwan", ""], ["Trapani", "Lorenzo", ""]]}, {"id": "1912.13076", "submitter": "Alex Reinhart", "authors": "Alex Reinhart and Christopher R. Genovese", "title": "Expanding the scope of statistical computing: Training statisticians to\n  be software engineers", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traditionally, statistical computing courses have taught the syntax of a\nparticular programming language or specific statistical computation methods.\nSince the publication of Nolan and Temple Lang (2010), we have seen a greater\nemphasis on data wrangling, reproducible research, and visualization. This\nshift better prepares students for careers working with complex datasets and\nproducing analyses for multiple audiences. But, we argue, statisticians are now\noften called upon to develop statistical software, not just analyses, such as R\npackages implementing new analysis methods or machine learning systems\nintegrated into commercial products. This demands different skills.\n  We describe a graduate course that we developed to meet this need by focusing\non four themes: programming practices; software design; important algorithms\nand data structures; and essential tools and methods. Through code review and\nrevision, and a semester-long software project, students practice all the\nskills of software engineering. The course allows students to expand their\nunderstanding of computing as applied to statistical problems while building\nexpertise in the kind of software development that is increasingly the province\nof the working statistician. We see this as a model for the future evolution of\nthe computing curriculum in statistics and data science.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 20:23:51 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 13:57:39 GMT"}, {"version": "v3", "created": "Thu, 29 Oct 2020 00:53:54 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Reinhart", "Alex", ""], ["Genovese", "Christopher R.", ""]]}]