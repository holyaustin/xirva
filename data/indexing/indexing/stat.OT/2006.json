[{"id": "2006.00741", "submitter": "Edgar Santos-Fernandez", "authors": "Edgar Santos-Fernandez, Erin E. Peterson, Julie Vercelloni, Em\n  Rushworth, Kerrie Mengersen", "title": "Correcting misclassification errors in crowdsourced ecological data: A\n  Bayesian perspective", "comments": "18 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many research domains use data elicited from \"citizen scientists\" when a\ndirect measure of a process is expensive or infeasible. However, participants\nmay report incorrect estimates or classifications due to their lack of skill.\nWe demonstrate how Bayesian hierarchical models can be used to learn about\nlatent variables of interest, while accounting for the participants' abilities.\nThe model is described in the context of an ecological application that\ninvolves crowdsourced classifications of georeferenced coral-reef images from\nthe Great Barrier Reef, Australia. The latent variable of interest is the\nproportion of coral cover, which is a common indicator of coral reef health.\nThe participants' abilities are expressed in terms of sensitivity and\nspecificity of a correctly classified set of points on the images. The model\nalso incorporates a spatial component, which allows prediction of the latent\nvariable in locations that have not been surveyed. We show that the model\noutperforms traditional weighted-regression approaches used to account for\nuncertainty in citizen science data. Our approach produces more accurate\nregression coefficients and provides a better characterization of the latent\nprocess of interest. This new method is implemented in the probabilistic\nprogramming language Stan and can be applied to a wide number of problems that\nrely on uncertain citizen science data.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 06:41:24 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Santos-Fernandez", "Edgar", ""], ["Peterson", "Erin E.", ""], ["Vercelloni", "Julie", ""], ["Rushworth", "Em", ""], ["Mengersen", "Kerrie", ""]]}, {"id": "2006.02956", "submitter": "Julio Stern", "authors": "Marcos Vinicius M. Silva, Marcos Antonio Simplicio Jr., Roberto\n  Augusto Castellanos Pfeiffer, Julio Michael Stern", "title": "A Fair, Traceable, Auditable and Participatory Randomization Tool for\n  Legal Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world scenarios require the random selection of one or more\nindividuals from a pool of eligible candidates. One example of especial social\nrelevance refers to the legal system, in which the jurors and judges are\ncommonly picked according to some probability distribution aiming to avoid\nbiased decisions. In this scenario, ensuring auditability of the random drawing\nprocedure is imperative to promote confidence in its fairness. With this goal\nin mind, this article describes a protocol for random drawings specially\ndesigned for use in legal systems. The proposed design combines the following\nproperties: security by design, ensuring the fairness of the random draw as\nlong as at least one participant behaves honestly; auditability by any\ninterested party, even those having no technical background, using only public\ninformation; and statistical robustness, supporting drawings where candidates\nmay have distinct probability distributions. Moreover, it is capable of\ninviting and engaging as participating stakeholders the main interested parties\nof a legal process, in a way that promotes process transparency, public trust\nand institutional resilience. An open-source implementation is also provided as\nsupplementary material.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 15:38:15 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Silva", "Marcos Vinicius M.", ""], ["Simplicio", "Marcos Antonio", "Jr."], ["Pfeiffer", "Roberto Augusto Castellanos", ""], ["Stern", "Julio Michael", ""]]}, {"id": "2006.03351", "submitter": "Trivik Verma", "authors": "Trivik Verma, Mikhail Sirenko, Itto Kornecki, Scott Cunningham, Nuno\n  AM Ara\\'ujo", "title": "Extracting Spatiotemporal Demand for Public Transit from Mobility Data", "comments": "12 pages, 5 figures, submitted for peer review, SI included", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.LG stat.OT", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  With people constantly migrating to different urban areas, our mobility needs\nfor work, services and leisure are transforming rapidly. The changing urban\ndemographics pose several challenges for the efficient management of transit\nservices. To forecast transit demand, planners often resort to sociological\ninvestigations or modelling that are either difficult to obtain, inaccurate or\noutdated. How can we then estimate the variegated demand for mobility? We\npropose a simple method to identify the spatiotemporal demand for public\ntransit in a city. Using a Gaussian mixture model, we decompose empirical\nridership data into a set of temporal demand profiles representative of\nridership over any given day. A case of approximately 4.6 million daily transit\ntraces from the Greater London region reveals distinct demand profiles. We find\nthat a weighted mixture of these profiles can generate any station traffic\nremarkably well, uncovering spatially concentric clusters of mobility needs.\nOur method of analysing the spatiotemporal geography of a city can be extended\nto other urban regions with different modes of public transit.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 10:21:31 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Verma", "Trivik", ""], ["Sirenko", "Mikhail", ""], ["Kornecki", "Itto", ""], ["Cunningham", "Scott", ""], ["Ara\u00fajo", "Nuno AM", ""]]}, {"id": "2006.03722", "submitter": "Michael Fauss", "authors": "Michael Fau{\\ss}, Alex Dysto, H. Vincent Poor", "title": "MMSE Bounds Under Kullback-Leibler Divergence Constraints on the Joint\n  Input-Output Distribution", "comments": "Submitted for publication in the IEEE Transactions on Signal\n  Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new family of lower and upper bounds on the minimum\nmean squared error (MMSE). The key idea is to minimize/maximize the MMSE\nsubject to the constraint that the joint distribution of the input-output\nstatistics lies in a Kullback-Leibler divergence ball centered at some Gaussian\nreference distribution. Both bounds are tight and are attained by Gaussian\ndistributions whose mean is identical to that of the reference distribution and\nwhose covariance matrix is determined by a scalar parameter that can be\nobtained by finding the root of a monotonic function. The upper bound\ncorresponds to a minimax optimal estimator and provides performance guarantees\nunder distributional uncertainty. The lower bound provides an alternative to\nwell-known inequalities in estimation theory, such as the Cram\\'er-Rao bound,\nthat is potentially tighter and defined for a larger class of distributions.\nExamples of applications in signal processing and information theory illustrate\nthe usefulness of the proposed bounds in practice.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 22:30:59 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Fau\u00df", "Michael", ""], ["Dysto", "Alex", ""], ["Poor", "H. Vincent", ""]]}, {"id": "2006.04565", "submitter": "Farzana Jahan Mrs", "authors": "Farzana Jahan, Insha Ullah, Kerrie L Mengersen", "title": "A Survey of Bayesian Statistical Approaches for Big Data", "comments": null, "journal-ref": "In Mengersen K., Pudlo P., Robert C. (2020) Case Studies in\n  Applied Bayesian Data Science. Lecture Notes in Mathematics, vol 2259. (pp.\n  17-44) Springer, Cham", "doi": "10.1007/978-3-030-42553-1", "report-no": null, "categories": "stat.CO stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The modern era is characterised as an era of information or Big Data. This\nhas motivated a huge literature on new methods for extracting information and\ninsights from these data. A natural question is how these approaches differ\nfrom those that were available prior to the advent of Big Data. We present a\nreview of published studies that present Bayesian statistical approaches\nspecifically for Big Data and discuss the reported and perceived benefits of\nthese approaches. We conclude by addressing the question of whether focusing\nonly on improving computational algorithms and infrastructure will be enough to\nface the challenges of Big Data.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 13:16:02 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Jahan", "Farzana", ""], ["Ullah", "Insha", ""], ["Mengersen", "Kerrie L", ""]]}, {"id": "2006.05748", "submitter": "Lizanne Raubenheimer", "authors": "Andr\\'ehette Verster (1) and Lizanne Raubenheimer (2) ((1) Department\n  of Mathematical Statistics and Actuarial Science, University of the Free\n  State, Bloemfontein, South Africa, (2) School of Mathematical and Statistical\n  Sciences, North-West University, Potchefstroom, South Africa)", "title": "A different approach for choosing a threshold in peaks over threshold", "comments": "18 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstract In Extreme Value methodology the choice of threshold plays an\nimportant role in efficient modelling of observations exceeding the threshold.\nThe threshold must be chosen high enough to ensure an unbiased extreme value\nindex but choosing the threshold too high results in uncontrolled variances.\nThis paper investigates a generalized model that can assist in the choice of\noptimal threshold values in the \\gamma positive domain. A Bayesian approach is\nconsidered by deriving a posterior distribution for the unknown generalized\nparameter. Using the properties of the posterior distribution allows for a\nmethod to choose an optimal threshold without visual inspection.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 09:34:56 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Verster", "Andr\u00e9hette", ""], ["Raubenheimer", "Lizanne", ""]]}, {"id": "2006.07136", "submitter": "Frank Benford", "authors": "Frank Benford", "title": "Fourier Analysis and Benford Random Variables", "comments": "23 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper has several major purposes. The central purpose is to describe the\n\"Benford analysis\" of a positive random variable and to summarize some results\nfrom investigations into base dependence of Benford random variables. The\nprincipal tools used to derive these results are Fourier series and Fourier\ntransforms, and a second major purpose of this paper is to present an\nintroductory exposition about these tools. My motivation for writing this paper\nis twofold. First, I think the theory of Benford random variables and the\nBenford analysis of a positive random variable are interesting and deserve to\nbe better known. Second, I think that Benford analysis provides a really\nexcellent illustration of the utility of Fourier series and transforms, and\nreveals certain interconnections between series and transforms that are not\nobvious from the usual way these subjects are introduced.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 18:43:02 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2020 23:04:42 GMT"}, {"version": "v3", "created": "Tue, 1 Dec 2020 21:15:47 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Benford", "Frank", ""]]}, {"id": "2006.09059", "submitter": "Fr\\'ed\\'eric Ouimet", "authors": "Fr\\'ed\\'eric Ouimet", "title": "Explicit formulas for the joint third and fourth central moments of the\n  multinomial distribution", "comments": "7 pages, 0 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the first explicit formulas for the joint third and fourth central\nmoments of the multinomial distribution, by differentiating the moment\ngenerating function. A general formula for the joint factorial moments was\npreviously given in Mosimann (1962).\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 10:36:06 GMT"}, {"version": "v2", "created": "Fri, 25 Dec 2020 08:33:40 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Ouimet", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "2006.11345", "submitter": "Adam Loy", "authors": "Adam Loy", "title": "Bringing Visual Inference to the Classroom", "comments": "22 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In the classroom, we traditionally visualize inferential concepts using\nstatic graphics or interactive apps. For example, there is a long history of\nusing apps to visualize sampling distributions. Recent developments in\nstatistical graphics have created an opportunity to bring additional\nvisualizations into the classroom to hone student understanding. Specifically,\nthe lineup protocol for visual inference provides a framework for students see\nthe difference between signal and noise by embedding a plot of observed data in\na field of null (noise) plots. Lineups have proven valuable in visualizing\nrandomization/permutation tests, diagnosing models, and even conducting valid\ninference when distributional assumptions break down. This paper provides an\noverview of how the lineup protocol for visual inference can be used to hone\nunderstanding of key statistical topics throughout the statistics curricula.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 19:49:55 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Loy", "Adam", ""]]}, {"id": "2006.11570", "submitter": "Liubov Tupikina", "authors": "Liubov Tupikina", "title": "Continuous limits of Heterogeneous Continuous Time Random Walk model", "comments": "This is the current version of the article. We are still working on\n  it and plan to submit the edited version later. 2 figures, 9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.stat-mech physics.data-an stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous time random Walk model has been versatile analytical formalism for\nstudying and modeling diffusion processes in heterogeneous structures, such as\ndisordered or porous media.\n  We are studying the continuous limits of Heterogeneous Continuous Time Random\nWalk model, when a random walk is making jumps on a graph within different\ntime-length.\n  We apply the concept of a generalized master equation to study heterogeneous\ncontinuous-time random walks on networks.\n  Depending on the interpretations of the waiting time distributions the\ngeneralized master equation gives different forms of continuous equations.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 13:14:21 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Tupikina", "Liubov", ""]]}, {"id": "2006.14004", "submitter": "Sebastian Rosengren", "authors": "Sebastian Rosengren", "title": "Predicting First Passage Percolation Shapes Using Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many random growth models have the property that the set of discovered sites,\nscaled properly, converges to some deterministic set as time grows. Such\nresults are known as shape theorems. Typically, not much is known about the\nshapes. For first passage percolation on $\\mathbb{Z}^d$ we only know that the\nshape is convex, compact, and inherits all the symmetries of $\\mathbb{Z}^d$.\nUsing simulated data we construct and fit a neural network able to adequately\npredict the shape of the set of discovered sites from the mean, standard\ndeviation, and percentiles of the distribution of the passage times. The\npurpose of the note is two-fold. The main purpose is to give researchers a new\ntool for \\textit{quickly} getting an impression of the shape from the\ndistribution of the passage times -- instead of having to wait some time for\nthe simulations to run, as is the only available way today. The second purpose\nof the note is simply to introduce modern machine learning methods into this\narea of discrete probability, and a hope that it stimulates further research.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 19:10:21 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Rosengren", "Sebastian", ""]]}, {"id": "2006.14737", "submitter": "Nokuthaba Sibanda", "authors": "Doaa Ayad and Nokuthaba Sibanda", "title": "Monitoring of process and risk-adjusted medical outcomes using a\n  multi-stage MEWMA chart", "comments": "17 pages, 3 figures Submitted to Statistical Methods in Medical\n  Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most statistical process control programmes in healthcare focus on\nsurveillance of outcomes at the final stage of a procedure, such as mortality\nor failure rates. Such an approach ignores the multi-stage nature of these\nprocedures, in which a patient progresses through several stages prior to the\nfinal stage. In this paper, we develop a multi-stage control chart based on a\nmultivariate exponentially weighted moving average (EWMA) test statistic\nderived from score equations. This allows simultaneous monitoring of all\nintermediate and final stage outcomes of a healthcare process, with adjustment\nfor underlying patient risk factors and dependence between outcome variables.\nUse of the EWMA test statistics allows quick detection of small gradual changes\nin any part of the process. Three advantages of the approach are: better\nunderstanding of how outcomes at different stages relate to each other,\nexplicit monitoring of upstream stage outcomes may help curtail trends that\nlead to poorer end-stage outcomes and understanding the impact of each stage\ncan help determine the most effective allocation of quality improvement\nresources. Simulations are performed to test the control charts under various\ntypes of hypothesised shifts, and the results are summarised using\nout-of-control average run lengths.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 00:31:04 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Ayad", "Doaa", ""], ["Sibanda", "Nokuthaba", ""]]}, {"id": "2006.16051", "submitter": "Antonio Calcagn\\`i", "authors": "Antonio Calcagn\\`i and Luigi Lombardi", "title": "Modeling random and non-random decision uncertainty in ratings data: A\n  fuzzy beta model", "comments": "29 pages, 4 figures, 7 tables", "journal-ref": "AStA Advances in Statistical Analysis, Springer, 2021", "doi": "10.1007/s10182-021-00407-7", "report-no": null, "categories": "stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling human ratings data subject to raters' decision uncertainty is an\nattractive problem in applied statistics. In view of the complex interplay\nbetween emotion and decision making in rating processes, final raters' choices\nseldom reflect the true underlying raters' responses. Rather, they are\nimprecisely observed in the sense that they are subject to a non-random\ncomponent of uncertainty, namely the decision uncertainty. The purpose of this\narticle is to illustrate a statistical approach to analyse ratings data which\nintegrates both random and non-random components of the rating process. In\nparticular, beta fuzzy numbers are used to model raters' non-random decision\nuncertainty and a variable dispersion beta linear model is instead adopted to\nmodel the random counterpart of rating responses. The main idea is to quantify\ncharacteristics of latent and non-fuzzy rating responses by means of random\nobservations subject to fuzziness. To do so, a fuzzy version of the\nExpectation-Maximization algorithm is adopted to both estimate model's\nparameters and compute their standard errors. Finally, the characteristics of\nthe proposed fuzzy beta model are investigated by means of a simulation study\nas well as two case studies from behavioral and social contexts.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 13:41:28 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 06:46:19 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Calcagn\u00ec", "Antonio", ""], ["Lombardi", "Luigi", ""]]}, {"id": "2006.16333", "submitter": "Luca Rossini", "authors": "Florian Huber and Luca Rossini", "title": "Inference in Bayesian Additive Vector Autoregressive Tree Models", "comments": "30 pages, 5 Figures, 1 Tables; JEL Codes C11, C32, C53", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ML stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vector autoregressive (VAR) models assume linearity between the endogenous\nvariables and their lags. This assumption might be overly restrictive and could\nhave a deleterious impact on forecasting accuracy. As a solution, we propose\ncombining VAR with Bayesian additive regression tree (BART) models. The\nresulting Bayesian additive vector autoregressive tree (BAVART) model is\ncapable of capturing arbitrary non-linear relations between the endogenous\nvariables and the covariates without much input from the researcher. Since\ncontrolling for heteroscedasticity is key for producing precise density\nforecasts, our model allows for stochastic volatility in the errors. We apply\nour model to two datasets. The first application shows that the BAVART model\nyields highly competitive forecasts of the US term structure of interest rates.\nIn a second application, we estimate our model using a moderately sized\nEurozone dataset to investigate the dynamic effects of uncertainty on the\neconomy.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 19:37:09 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 12:29:13 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Huber", "Florian", ""], ["Rossini", "Luca", ""]]}]