[{"id": "1704.00609", "submitter": "Ricardo Almeida Dr", "authors": "Ricardo Almeida", "title": "What is the best fractional derivative to fit data?", "comments": "This is a preprint of a paper whose final and definite form is with\n  \"Applicable Analysis and Discrete Mathematics\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this work is to show, based on concrete data observation, that the\nchoice of the fractional derivative when modelling a problem is relevant for\nthe accuracy of a method. Using the least squares fitting technique, we\ndetermine the order of the fractional differential equation that better\ndescribes the experimental data, for different types of fractional derivatives.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 09:07:41 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Almeida", "Ricardo", ""]]}, {"id": "1704.00674", "submitter": "Srinjoy Das", "authors": "Srinjoy Das, Dimitris N. Politis", "title": "Nonparametric estimation of the conditional distribution at regression\n  boundary points", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric regression is a standard statistical tool with increased\nimportance in the Big Data era. Boundary points pose additional difficulties\nbut local polynomial regression can be used to alleviate them. Local linear\nregression, for example, is easy to implement and performs quite well both at\ninterior as well as boundary points. Estimating the conditional distribution\nfunction and/or the quantile function at a given regressor point is immediate\nvia standard kernel methods but problems ensue if local linear methods are to\nbe used. In particular, the distribution function estimator is not guaranteed\nto be monotone increasing, and the quantile curves can \"cross\". In the paper at\nhand, a simple method of correcting the local linear distribution estimator for\nmonotonicity is proposed, and its good performance is demonstrated via\nsimulations and real data examples.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 16:43:24 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Das", "Srinjoy", ""], ["Politis", "Dimitris N.", ""]]}, {"id": "1704.01171", "submitter": "Harry Crane", "authors": "Harry Crane and Ryan Martin", "title": "Rethinking probabilistic prediction in the wake of the 2016 U.S.\n  presidential election", "comments": "19 pages; 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To many statisticians and citizens, the outcome of the most recent U.S.\npresidential election represents a failure of data-driven methods on the\ngrandest scale. This impression has led to much debate and discussion about how\nthe election predictions went awry -- Were the polls inaccurate? Were the\nmodels wrong? Did we misinterpret the probabilities? -- and how they went right\n-- Perhaps the analyses were correct even though the predictions were wrong,\nthat's just the nature of probabilistic forecasting. With this in mind, we\nanalyze the election outcome with respect to a core set of effectiveness\nprinciples. Regardless of whether and how the election predictions were right\nor wrong, we argue that they were ineffective in conveying the extent to which\nthe data was informative of the outcome and the level of uncertainty in making\nthese assessments. Among other things, our analysis sheds light on the\nshortcomings of the classical interpretations of probability and its\ncommunication to consumers in the form of predictions. We present here an\nalternative approach, based on a notion of validity, which offers two immediate\ninsights for predictive inference. First, the predictions are more\nconservative, arguably more realistic, and come with certain guarantees on the\nprobability of an erroneous prediction. Second, our approach easily and\nnaturally reflects the (possibly substantial) uncertainty about the model by\noutputting plausibilities instead of probabilities. Had these simple steps been\ntaken by the popular prediction outlets, the election outcome may not have been\nso shocking.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 20:14:00 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Crane", "Harry", ""], ["Martin", "Ryan", ""]]}, {"id": "1704.01297", "submitter": "Sawon  Pratiher", "authors": "S Pratiher, S Chatterjee, R Bose", "title": "Automated Diagnosis of Epilepsy Employing Multifractal Detrended\n  Fluctuation Analysis Based Features", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV nlin.AO nlin.CD q-bio.QM stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This contribution reports an application of MultiFractal Detrended\nFluctuation Analysis, MFDFA based novel feature extraction technique for\nautomated detection of epilepsy. In fractal geometry, Multifractal Detrended\nFluctuation Analysis MFDFA is a popular technique to examine the\nself-similarity of a nonlinear, chaotic and noisy time series. In the present\nresearch work, EEG signals representing healthy, interictal (seizure free) and\nictal activities (seizure) are acquired from an existing available database.\nThe acquired EEG signals of different states are at first analyzed using MFDFA.\nTo requisite the time series singularity quantification at local and global\nscales, a novel set of fourteen different features. Suitable feature ranking\nemploying students t-test has been done to select the most statistically\nsignificant features which are henceforth being used as inputs to a support\nvector machines (SVM) classifier for the classification of different EEG\nsignals. Eight different classification problems have been presented in this\npaper and it has been observed that the overall classification accuracy using\nMFDFA based features are reasonably satisfactory for all classification\nproblems. The performance of the proposed method are also found to be quite\ncommensurable and in some cases even better when compared with the results\npublished in existing literature studied on the similar data set.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 08:00:14 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Pratiher", "S", ""], ["Chatterjee", "S", ""], ["Bose", "R", ""]]}, {"id": "1704.01732", "submitter": "Yiping Cheng", "authors": "Yiping Cheng", "title": "A Mathematically Sensible Explanation of the Concept of Statistical\n  Population", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In statistics education, the concept of population is widely felt hard to\ngrasp, as a result of vague explanations in textbooks. Some textbook authors\ntherefore chose not to mention it. This paper offers a new explanation by\nproposing a new theoretical framework of population and sampling, which aims to\nachieve high mathematical sensibleness. In the explanation, the term population\nis given clear definition, and the relationship between simple random sampling\nand iid random variables are examined mathematically.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 07:42:17 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Cheng", "Yiping", ""]]}, {"id": "1704.03812", "submitter": "Xiaoming Ye", "authors": "Huisheng Shi, Xiaoming Ye, Cheng Xing, and Shijun Ding", "title": "A New Theoretical Interpretation of Measurement Error and Its\n  Uncertainty", "comments": "20 pages, 7 figures", "journal-ref": "Discrete Dynamics in Nature and Society(2020)", "doi": "10.1155/2020/3864578", "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The traditional measurement theory interprets the variance as the dispersion\nof a measured value, which is actually contrary to a general mathematical\nconcept that the variance of a constant is 0. This paper will fully demonstrate\nthat the variance in measurement theory is actually the evaluation of\nprobability interval of an error instead of the dispersion of a measured value,\npoint out the key point of mistake in the traditional interpretation, and fully\ninterpret a series of changes in conceptual logic and processing method brought\nabout by this new concept.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 16:09:00 GMT"}, {"version": "v10", "created": "Thu, 31 Jan 2019 16:00:51 GMT"}, {"version": "v11", "created": "Thu, 14 Feb 2019 23:59:24 GMT"}, {"version": "v12", "created": "Thu, 21 Nov 2019 15:05:30 GMT"}, {"version": "v13", "created": "Mon, 6 Jan 2020 08:50:02 GMT"}, {"version": "v14", "created": "Mon, 3 Feb 2020 09:21:51 GMT"}, {"version": "v15", "created": "Fri, 18 Sep 2020 23:04:21 GMT"}, {"version": "v2", "created": "Mon, 17 Apr 2017 14:41:40 GMT"}, {"version": "v3", "created": "Sat, 22 Apr 2017 23:45:13 GMT"}, {"version": "v4", "created": "Tue, 16 May 2017 14:36:08 GMT"}, {"version": "v5", "created": "Wed, 24 May 2017 05:02:29 GMT"}, {"version": "v6", "created": "Sat, 27 May 2017 02:41:21 GMT"}, {"version": "v7", "created": "Tue, 6 Jun 2017 07:34:23 GMT"}, {"version": "v8", "created": "Tue, 27 Jun 2017 09:47:59 GMT"}, {"version": "v9", "created": "Tue, 25 Sep 2018 01:42:24 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Shi", "Huisheng", ""], ["Ye", "Xiaoming", ""], ["Xing", "Cheng", ""], ["Ding", "Shijun", ""]]}, {"id": "1704.03924", "submitter": "Yen-Chi Chen", "authors": "Yen-Chi Chen", "title": "A Tutorial on Kernel Density Estimation and Recent Advances", "comments": "A tutorial paper; accepted to Biostatistics & Epidemiology. Main\n  article: 26 pages, 8 figures. R implementations: 11 pages, generated by\n  Rmarkdown", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This tutorial provides a gentle introduction to kernel density estimation\n(KDE) and recent advances regarding confidence bands and geometric/topological\nfeatures. We begin with a discussion of basic properties of KDE: the\nconvergence rate under various metrics, density derivative estimation, and\nbandwidth selection. Then, we introduce common approaches to the construction\nof confidence intervals/bands, and we discuss how to handle bias. Next, we talk\nabout recent advances in the inference of geometric and topological features of\na density function using KDE. Finally, we illustrate how one can use KDE to\nestimate a cumulative distribution function and a receiver operating\ncharacteristic curve. We provide R implementations related to this tutorial at\nthe end.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 20:45:38 GMT"}, {"version": "v2", "created": "Tue, 12 Sep 2017 13:40:54 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Chen", "Yen-Chi", ""]]}, {"id": "1704.05630", "submitter": "Javier \\'Alvarez-Li\\'ebana", "authors": "M. Dolores Ruiz-Medina and J. \\'Alvarez-Li\\'ebana", "title": "Classical and bayesian componentwise predictors for non-compact\n  correlated ARH(1) processes", "comments": "33 pages: 6 figures are included. In press, accepted manuscript", "journal-ref": "REVSTAT 2017", "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.OT stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A special class of standard Gaussian Autoregressive Hilbertian processes of\norder one (Gaussian ARH(1) processes), with bounded linear autocorrelation\noperator, which does not satisfy the usual Hilbert-Schmidt assumption, is\nconsidered. To compensate the slow decay of the diagonal coefficients of the\nautocorrelation operator, a faster decay velocity of the eigenvalues of the\ntrace autocovariance operator of the innovation process is assumed. As usual,\nthe eigenvectors of the autocovariance operator of the ARH(1) process are\nconsidered for projection, since, here, they are assumed to be known. Diagonal\ncomponentwise classical and bayesian estimation of the autocorrelation operator\nis studied for prediction. The asymptotic efficiency and equivalence of both\nestimators is proved, as well as of their associated componentwise ARH(1)\nplugin predictors. A simulation study is undertaken to illustrate the\ntheoretical results derived.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 06:56:54 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2018 10:59:40 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Ruiz-Medina", "M. Dolores", ""], ["\u00c1lvarez-Li\u00e9bana", "J.", ""]]}, {"id": "1704.06292", "submitter": "Rajesh Sharma", "authors": "R. Sharma", "title": "Remark On Variance Bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is shown that the formula for the variance of combined series yields\nsurprisingly simple proofs of some well known variance bounds.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 14:54:43 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Sharma", "R.", ""]]}, {"id": "1704.07512", "submitter": "Grey Nearing", "authors": "Grey Nearing and Hoshin Gupta", "title": "Information vs. Uncertainty as the Foundation for a Science of\n  Environmental Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information accounting provides a better foundation for hypothesis testing\nthan does uncertainty quantification. A quantitative account of science is\nderived under this perspective that alleviates the need for epistemic bridge\nprinciples, solves the problem of ad hoc falsification criteria, and deals with\nverisimilitude by facilitating a general approach to process-level diagnostics.\nOur argument is that the well-known inconsistencies of both Bayesian and\nclassical statistical hypothesis tests are due to the fact that probability\ntheory is an insufficient logic of science. Information theory, as an extension\nof probability theory, is required to provide a complete logic on which to base\nquantitative theories of empirical learning. The organizing question in this\ncase becomes not whether our theories or models are more or less true, or about\nhow much uncertainty is associated with a particular model, but instead whether\nthere is any information available from experimental data that might allow us\nto improve the model. This becomes a formal hypothesis test, provides a theory\nof model diagnostics, and suggests a new approach to building dynamical systems\nmodels.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 01:55:24 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Nearing", "Grey", ""], ["Gupta", "Hoshin", ""]]}, {"id": "1704.08248", "submitter": "Sarit Agami", "authors": "Robert J. Adler, Sarit Agami, and Pratyush Pranav", "title": "Modeling and replicating statistical topology, and evidence for CMB\n  non-homogeneity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under the banner of `Big Data', the detection and classification of structure\nin extremely large, high dimensional, data sets, is, one of the central\nstatistical challenges of our times. Among the most intriguing approaches to\nthis challenge is `TDA', or `Topological Data Analysis', one of the primary\naims of which is providing non-metric, but topologically informative,\npre-analyses of data sets which make later, more quantitative analyses\nfeasible. While TDA rests on strong mathematical foundations from Topology, in\napplications it has faced challenges due to an inability to handle issues of\nstatistical reliability and robustness and, most importantly, in an inability\nto make scientific claims with verifiable levels of statistical confidence. We\npropose a methodology for the parametric representation, estimation, and\nreplication of persistence diagrams, the main diagnostic tool of TDA. The power\nof the methodology lies in the fact that even if only one persistence diagram\nis available for analysis -- the typical case for big data applications --\nreplications can be generated to allow for conventional statistical hypothesis\ntesting. The methodology is conceptually simple and computationally practical,\nand provides a broadly effective statistical procedure for persistence diagram\nTDA analysis. We demonstrate the basic ideas on a toy example, and the power of\nthe approach in a novel and revealing analysis of CMB non-homogeneity.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 10:57:42 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Adler", "Robert J.", ""], ["Agami", "Sarit", ""], ["Pranav", "Pratyush", ""]]}]