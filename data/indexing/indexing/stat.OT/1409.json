[{"id": "1409.3803", "submitter": "Ioannis Mariolis", "authors": "Ioannis Mariolis", "title": "Revealing the Beauty behind the Sleeping Beauty Problem", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large number of essays address the Sleeping Beauty problem, which\nundermines the validity of Bayesian inference and Bas Van Fraassen's\n'Reflection Principle'. In this study a straightforward analysis of the problem\nbased on probability theory is presented. The key difference from previous\nworks is that apart from the random experiment imposed by the problem's\ndescription, a different one is also considered, in order to negate the\nconfusion on the involved conditional probabilities. The results of the\nanalysis indicate that no inconsistency takes place, whereas both Bayesian\ninference and 'Reflection Principle' are valid.\n", "versions": [{"version": "v1", "created": "Wed, 10 Sep 2014 12:32:26 GMT"}], "update_date": "2014-09-15", "authors_parsed": [["Mariolis", "Ioannis", ""]]}, {"id": "1409.4696", "submitter": "Vishesh Karwa", "authors": "Vishesh Karwa, Aleksandra B. Slavkovi\\'c, Pavel Krivitsky", "title": "Differentially Private Exponential Random Graphs", "comments": "minor edits", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT cs.CR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose methods to release and analyze synthetic graphs in order to\nprotect privacy of individual relationships captured by the social network.\nProposed techniques aim at fitting and estimating a wide class of exponential\nrandom graph models (ERGMs) in a differentially private manner, and thus offer\nrigorous privacy guarantees. More specifically, we use the randomized response\nmechanism to release networks under $\\epsilon$-edge differential privacy. To\nmaintain utility for statistical inference, treating the original graph as\nmissing, we propose a way to use likelihood based inference and Markov chain\nMonte Carlo (MCMC) techniques to fit ERGMs to the produced synthetic networks.\nWe demonstrate the usefulness of the proposed techniques on a real data\nexample.\n", "versions": [{"version": "v1", "created": "Tue, 16 Sep 2014 16:47:47 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2015 04:10:44 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Karwa", "Vishesh", ""], ["Slavkovi\u0107", "Aleksandra B.", ""], ["Krivitsky", "Pavel", ""]]}, {"id": "1409.5196", "submitter": "Steven Frank", "authors": "Steven A. Frank", "title": "How to read probability distributions as statements about process", "comments": "v2: added table of contents, adjusted section numbers v3: minor\n  editing, updated reference", "journal-ref": "Entropy 16:6059-6098 (2014)", "doi": "10.3390/e16116059", "report-no": null, "categories": "stat.OT cond-mat.stat-mech math.PR physics.data-an q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probability distributions can be read as simple expressions of information.\nEach continuous probability distribution describes how information changes with\nmagnitude. Once one learns to read a probability distribution as a measurement\nscale of information, opportunities arise to understand the processes that\ngenerate the commonly observed patterns. Probability expressions may be parsed\ninto four components: the dissipation of all information, except the\npreservation of average values, taken over the measurement scale that relates\nchanges in observed values to changes in information, and the transformation\nfrom the underlying scale on which information dissipates to alternative scales\non which probability pattern may be expressed. Information invariances set the\ncommonly observed measurement scales and the relations between them. In\nparticular, a measurement scale for information is defined by its invariance to\nspecific transformations of underlying values into measurable outputs.\nEssentially all common distributions can be understood within this simple\nframework of information invariance and measurement scale.\n", "versions": [{"version": "v1", "created": "Thu, 18 Sep 2014 05:40:20 GMT"}, {"version": "v2", "created": "Sat, 4 Oct 2014 09:00:21 GMT"}, {"version": "v3", "created": "Tue, 18 Nov 2014 15:35:33 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Frank", "Steven A.", ""]]}, {"id": "1409.5501", "submitter": "Sean Skwerer", "authors": "Sean Skwerer", "title": "Tree Oriented Data Analysis", "comments": "PhD thesis, University of North Carolina, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex data objects arise in many areas of modern science including\nevolutionary biology, nueroscience, dynamics of gene expression and medical\nimaging. Object oriented data analysis (OODA) is the statistical analysis of\ndatasets of complex objects. Data analysis of tree data objects is an exciting\nresearch area with interesting questions and challenging problems. This thesis\nfocuses on tree oriented statistical methodologies, and algorithms for solving\nrelated mathematical optimization problems.\n  This research is motivated by the goal of analyzing a data set of images of\nhuman brain arteries. The approach we take here is to use a novel\nrepresentation of brain artery systems as points in phylogenetic treespace. The\ntreespace property of unique global geodesics leads to a notion of geometric\ncenter called a Fr\\'echet mean. For a sample of data points, the Fr\\'echet\nfunction is the sum of squared distances from a point to the data points, and\nthe Fr\\'echet mean is the minimizer of the Fr\\'echet function.\n  In this thesis we use properties of the Fr\\'echet function to develop an\nalgorithmic system for computing Fr\\'echet means. Properties of the Fr\\'echet\nfunction are also used to show a sticky law of large numbers which describes a\nsurprising stability of the topological tree structure of sample Fr\\'echet\nmeans at that of the population Fr\\'echet mean. We also introduce\nnon-parametric regression of brain artery tree structure as a response variable\nto age based on weighted Fr\\'echet means.\n", "versions": [{"version": "v1", "created": "Fri, 19 Sep 2014 02:15:43 GMT"}, {"version": "v2", "created": "Mon, 22 Sep 2014 23:53:14 GMT"}], "update_date": "2014-11-12", "authors_parsed": [["Skwerer", "Sean", ""]]}, {"id": "1409.7419", "submitter": "Cl(\\'a)udia Silvestre", "authors": "Cl\\'audia Silvestre, Margarida G. M. S. Cardoso and M\\'ario A. T.\n  Figueiredo", "title": "Identifying the number of clusters in discrete mixture models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on cluster analysis for categorical data continues to develop, with\nnew clustering algorithms being proposed. However, in this context, the\ndetermination of the number of clusters is rarely addressed. In this paper, we\npropose a new approach in which clustering of categorical data and the\nestimation of the number of clusters is carried out simultaneously. Assuming\nthat the data originate from a finite mixture of multinomial distributions, we\ndevelop a method to select the number of mixture components based on a minimum\nmessage length (MML) criterion and implement a new expectation-maximization\n(EM) algorithm to estimate all the model parameters. The proposed EM-MML\napproach, rather than selecting one among a set of pre-estimated candidate\nmodels (which requires running EM several times), seamlessly integrates\nestimation and model selection in a single algorithm. The performance of the\nproposed approach is compared with other well-known criteria (such as the\nBayesian information criterion-BIC), resorting to synthetic data and to two\nreal applications from the European Social Survey. The EM-MML computation time\nis a clear advantage of the proposed method. Also, the real data solutions are\nmuch more parsimonious than the solutions provided by competing methods, which\nreduces the risk of model order overestimation and increases interpretability.\n", "versions": [{"version": "v1", "created": "Thu, 25 Sep 2014 21:03:00 GMT"}], "update_date": "2014-09-29", "authors_parsed": [["Silvestre", "Cl\u00e1udia", ""], ["Cardoso", "Margarida G. M. S.", ""], ["Figueiredo", "M\u00e1rio A. T.", ""]]}, {"id": "1409.7933", "submitter": "Lorenzo Mercuri", "authors": "Lorenzo Mercuri, Edit Rroji", "title": "Parametric Risk Parity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.PM stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Any optimization algorithm based on the risk parity approach requires the\nformulation of portfolio total risk in terms of marginal contributions. In this\npaper we use the independence of the underlying factors in the market to derive\nthe centered moments required in the risk decomposition process when the\nmodified versions of Value at Risk and Expected Shortfall are considered.\n  The choice of the Mixed Tempered Stable distribution seems adequate for\nfitting skewed and heavy tailed distributions. The ensuing detailed description\nof the optimization procedure is due to the existence of analytical higher\norder moments. Better results are achieved in terms of out of sample\nperformance and greater diversification.\n", "versions": [{"version": "v1", "created": "Sun, 28 Sep 2014 17:10:20 GMT"}], "update_date": "2014-09-30", "authors_parsed": [["Mercuri", "Lorenzo", ""], ["Rroji", "Edit", ""]]}]