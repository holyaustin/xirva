[{"id": "2011.00992", "submitter": "Chenguang Lu", "authors": "Chenguang Lu", "title": "The P-T Probability Framework for Semantic Communication, Falsification,\n  Confirmation, and Bayesian Reasoning", "comments": "36 pages; 10 Figures", "journal-ref": "Philosophies 2020, 5(4), 25", "doi": "10.3390/philosophies5040025", "report-no": null, "categories": "stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many researchers want to unify probability and logic by defining logical\nprobability or probabilistic logic reasonably. This paper tries to unify\nstatistics and logic so that we can use both statistical probability and\nlogical probability at the same time. For this purpose, this paper proposes the\nP-T probability framework, which is assembled with Shannon's statistical\nprobability framework for communication, Kolmogorov's probability axioms for\nlogical probability, and Zadeh's membership functions used as truth functions.\nTwo kinds of probabilities are connected by an extended Bayes' theorem, with\nwhich we can convert a likelihood function and a truth function from one to\nanother. Hence, we can train truth functions (in logic) by sampling\ndistributions (in statistics). This probability framework was developed in the\nauthor's long-term studies on semantic information, statistical learning, and\ncolor vision. This paper first proposes the P-T probability framework and\nexplains different probabilities in it by its applications to semantic\ninformation theory. Then, this framework and the semantic information methods\nare applied to statistical learning, statistical mechanics, hypothesis\nevaluation (including falsification), confirmation, and Bayesian reasoning.\nTheoretical applications illustrate the reasonability and practicability of\nthis framework. This framework is helpful for interpretable AI. To interpret\nneural networks, we need further study.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 00:48:55 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Lu", "Chenguang", ""]]}, {"id": "2011.01041", "submitter": "Jan Schneider", "authors": "Jan Schneider", "title": "New definitions (measures) of skewness, mean and dispersion of fuzzy\n  numbers -- by way of a new representation as parameterized curves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a geometrically motivated measure of skewness, define a mean value\ntriangle number, and dispersion (in that order) of a fuzzy number without\nreference or seeking analogy to the namesake but parallel concepts in\nprobability theory. These measures come about by way of a new representation of\nfuzzy numbers as parameterized curves respectively their associated tangent\nbundle. Importantly skewness and dispersion are given as functions of $\\alpha$\n(the degree of membership) and such may be given separately and pointwise at\neach $\\alpha$-level, as well as overall. This allows for e.g., when a\nmathematical model is formulated in fuzzy numbers, to run optimization programs\nlevel-wise thereby encapsuling with deliberate accuracy the involved membership\nfunctions' characteristics while increasing the computational complexity by\nonly a multiplicative factor compared to the same program formulated in real\nvariables and parameters. As an example the work offers a contribution to the\nrecently very popular fuzzy mean-variance-skewness portfolio optimization.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 02:59:38 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Schneider", "Jan", ""]]}, {"id": "2011.02677", "submitter": "Sander Greenland", "authors": "Sander Greenland", "title": "The causal foundations of applied probability and statistics", "comments": "22 pages; in press for Dechter, R., Halpern, J., and Geffner, H.,\n  eds. Probabilistic and Causal Inference: The Works of Judea Pearl. ACM books", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical science (as opposed to mathematical statistics) involves far more\nthan probability theory, for it requires realistic causal models of data\ngenerators - even for purely descriptive goals. Statistical decision theory\nrequires more causality: Rational decisions are actions taken to minimize costs\nwhile maximizing benefits, and thus require explication of causes of loss and\ngain. Competent statistical practice thus integrates logic, context, and\nprobability into scientific inference and decision using narratives filled with\ncausality. This reality was seen and accounted for intuitively by the founders\nof modern statistics, but was not well recognized in the ensuing statistical\ntheory (which focused instead on the causally inert properties of probability\nmeasures). Nonetheless, both statistical foundations and basic statistics can\nand should be taught using formal causal models. The causal view of statistical\nscience fits within a broader information-processing framework which\nilluminates and unifies frequentist, Bayesian, and related probability-based\nfoundations of statistics. Causality theory can thus be seen as a key component\nconnecting computation to contextual information, not extra-statistical but\ninstead essential for sound statistical training and applications.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 06:26:31 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 20:13:02 GMT"}, {"version": "v3", "created": "Thu, 29 Apr 2021 16:25:59 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Greenland", "Sander", ""]]}, {"id": "2011.06689", "submitter": "Ezekiel Adebayo Ogundepo", "authors": "Ezekiel Ogundepo, Sakinat Folorunso, Olubayo Adekanmbi, Olalekan\n  Akinsande, Oluwatobi Banjo, Emeka Ogbuju, Francisca Oladipo, Olawale\n  Abimbola, Ehizokhale Oseghale, and Oluwatobi Babajide", "title": "An exploratory assessment of a multidimensional healthcare and economic\n  data on COVID-19 in Nigeria", "comments": null, "journal-ref": "Volume 33, December 2020, 106424", "doi": "10.1016/j.dib.2020.106424", "report-no": null, "categories": "stat.AP stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The coronavirus disease of 2019 (COVID-19) is a pandemic that is ravaging\nNigeria and the world at large. This data article provides a dataset of daily\nupdates of COVID-19 as reported online by the Nigeria Centre for Disease\nControl (NCDC) from February 27, 2020 to September 29, 2020. The data were\nobtained through web scraping from different sources and it includes some\neconomic variables such as the Nigeria budget for each state in 2020,\npopulation estimate, healthcare facilities, and the COVID-19 laboratories in\nNigeria. The dataset has been processed using the standard of the FAIR data\nprinciple which encourages its findability, accessibility, interoperability,\nand reusability and will be relevant to researchers in different fields such as\nData Science, Epidemiology, Earth Modelling, and Health Informatics.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 23:50:14 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Ogundepo", "Ezekiel", ""], ["Folorunso", "Sakinat", ""], ["Adekanmbi", "Olubayo", ""], ["Akinsande", "Olalekan", ""], ["Banjo", "Oluwatobi", ""], ["Ogbuju", "Emeka", ""], ["Oladipo", "Francisca", ""], ["Abimbola", "Olawale", ""], ["Oseghale", "Ehizokhale", ""], ["Babajide", "Oluwatobi", ""]]}, {"id": "2011.07559", "submitter": "Mehrdad Naderi Dr", "authors": "Mehrdad Naderi, Elham Mirfarah, Matthew Bernhardt and Ding-Geng Chen", "title": "Semiparametric inference for the scale-mixture of normal partial linear\n  regression model with censored data", "comments": "17 page, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the framework of censored data modeling, the classical linear regression\nmodel that assumes normally distributed random errors has received increasing\nattention in recent years, mainly for mathematical and computational\nconvenience. However, practical studies have often criticized this linear\nregression model due to its sensitivity to departure from the normality and\nfrom the partial nonlinearity. This paper proposes to solve these potential\nissues simultaneously in the context of the partial linear regression model by\nassuming that the random errors follow a scale-mixture of normal (SMN) family\nof distributions. The proposed method allows us to model data with great\nflexibility, accommodating heavy tails, and outliers. By implementing the\nB-spline function and using the convenient hierarchical representation of the\nSMN distributions, a computationally analytical EM-type algorithm is developed\nto perform maximum likelihood inference of the model parameters. Various\nsimulation studies are conducted to investigate the finite sample properties as\nwell as the robustness of the model in dealing with the heavy-tails distributed\ndatasets. Real-word data examples are finally analyzed for illustrating the\nusefulness of the proposed methodology.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 15:38:49 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Naderi", "Mehrdad", ""], ["Mirfarah", "Elham", ""], ["Bernhardt", "Matthew", ""], ["Chen", "Ding-Geng", ""]]}, {"id": "2011.09851", "submitter": "Daniel Oberski", "authors": "Laura Boeschoten and Jef Ausloos and Judith Moeller and Theo Araujo\n  and Daniel L. Oberski", "title": "Digital trace data collection through data donation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A potentially powerful method of social-scientific data collection and\ninvestigation has been created by an unexpected institution: the law. Article\n15 of the EU's 2018 General Data Protection Regulation (GDPR) mandates that\nindividuals have electronic access to a copy of their personal data, and all\nmajor digital platforms now comply with this law by providing users with \"data\ndownload packages\" (DDPs). Through voluntary donation of DDPs, all data\ncollected by public and private entities during the course of citizens' digital\nlife can be obtained and analyzed to answer social-scientific questions - with\nconsent. Thus, consented DDPs open the way for vast new research opportunities.\nHowever, while this entirely new method of data collection will undoubtedly\ngain popularity in the coming years, it also comes with its own questions of\nrepresentativeness and measurement quality, which are often evaluated\nsystematically by means of an error framework. Therefore, in this paper we\nprovide a blueprint for digital trace data collection using DDPs, and devise a\n\"total error framework\" for such projects. Our error framework for digital\ntrace data collection through data donation is intended to facilitate high\nquality social-scientific investigations using DDPs while critically reflecting\nits unique methodological challenges and sources of error. In addition, we\nprovide a quality control checklist to guide researchers in leveraging the vast\nopportunities afforded by this new mode of investigation.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 11:19:25 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Boeschoten", "Laura", ""], ["Ausloos", "Jef", ""], ["Moeller", "Judith", ""], ["Araujo", "Theo", ""], ["Oberski", "Daniel L.", ""]]}, {"id": "2011.10589", "submitter": "Tony Pourmohamad", "authors": "Tony Pourmohamad", "title": "CompModels: A suite of computer model test functions for Bayesian\n  optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The CompModels package for R provides a suite of computer model test\nfunctions that can be used for computer model prediction/emulation, uncertainty\nquantification, and calibration, but in particular, the sequential optimization\nof computer models. The package is a mix of real-world physics problems, known\nmathematical functions, and black-box functions that have been converted into\ncomputer models with the goal of Bayesian (i.e., sequential) optimization in\nmind. Likewise, the package contains computer models that represent either the\nconstrained or unconstrained optimization case, each with varying levels of\ndifficulty. In this paper, we illustrate the use of the package with both\nreal-world examples and black-box functions by solving constrained optimization\nproblems via Bayesian optimization. Ultimately, the package is shown to provide\nusers with a source of computer model test functions that are reproducible,\nshareable, and that can be used for benchmarking of novel optimization methods.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 19:00:30 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 16:16:54 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Pourmohamad", "Tony", ""]]}, {"id": "2011.11177", "submitter": "Paul Roediger", "authors": "Paul A. Roediger", "title": "Gonogo: An R Implementation of Test Methods to Perform, Analyze and\n  Simulate Sensitivity Experiments", "comments": "This documentation is 58 pages in length and contains 31 figures, 40\n  tables and 2 flow diagrams. The subject of much of the paper, the gonogo.R\n  file, contains 118 functions plus 2 constants and is available online", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME stat.ML stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work provides documentation for a suite of R functions contained in\ngonogo.R. The functions provide sensitivity testing practitioners and\nresearchers with an ability to conduct, analyze and simulate various\nsensitivity experiments involving binary responses and a single stimulus level\n(e.g., drug dosage, drop height, velocity, etc.). Included are the modern Neyer\nand 3pod adaptive procedures, as well as the Bruceton and Langlie. The latter\ntwo benchmark procedures are capable of being performed according to\ngeneralized up-down transformed-response rules. Each procedure is designated\nphase-one of a three-phase experiment. The goal of phase-one is to achieve\noverlapping data. The two additional (and optional) refinement phases utilize\nthe D-optimal criteria and the Robbins-Monro-Joseph procedure. The goals of the\ntwo refinement phases are to situate testing in the vicinity of the median and\ntails of the latent response distribution, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 02:28:29 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Roediger", "Paul A.", ""]]}, {"id": "2011.13758", "submitter": "Ludwig Hothorn", "authors": "Ludwig A. Hothorn", "title": "Comparisons of proportions in k dose groups against a negative control\n  assuming order restriction: Williams-type test vs. closed test procedures", "comments": "2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.OT", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The comparison of proportions is considered in the asymptotic generalized\nlinear model with the odds ratio as effect size. When several doses are\ncompared with a control assuming an order restriction, a Williams-type trend\ntest can be used. As an alternative, two variants of the closed testing\napproach are considered, one using global Williams-tests in the partition\nhypotheses, one with pairwise contrasts. Their advantages in terms of power and\nsimplicity are demonstrated. Related R-code is provided.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 14:44:45 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Hothorn", "Ludwig A.", ""]]}]