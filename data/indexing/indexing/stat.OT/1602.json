[{"id": "1602.01347", "submitter": "Antony Overstall", "authors": "Antony Overstall, David Woods, Kieran Martin", "title": "Bayesian prediction for physical models with application to the\n  optimization of the synthesis of pharmaceutical products using chemical\n  kinetics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quality control in industrial processes is increasingly making use of prior\nscientific knowledge, often encoded in physical models that require numerical\napproximation. Statistical prediction, and subsequent optimization, is key to\nensuring the process output meets a specification target. However, the\nnumerical expense of approximating the models poses computational challenges to\nthe identification of combinations of the process factors where there is\nconfidence in the quality of the response. Recent work in Bayesian computation\nand statistical approximation (emulation) of expensive computational models is\nexploited to develop a novel strategy for optimizing the posterior probability\nof a process meeting specification. The ensuing methodology is motivated by,\nand demonstrated on, a chemical synthesis process to manufacture a\npharmaceutical product, within which an initial set of substances evolve\naccording to chemical reactions, under certain process conditions, into a\nseries of new substances. One of these substances is a target pharmaceutical\nproduct and two are unwanted by-products. The aim is to determine the\ncombinations of process conditions and amounts of initial substances that\nmaximize the probability of obtaining sufficient target pharmaceutical product\nwhilst ensuring unwanted by-products do not exceed a given level. The\nrelationship between the factors and amounts of substances of interest is\ntheoretically described by the solution to a system of ordinary differential\nequations incorporating temperature dependence. Using data from a small\nexperiment, it is shown how the methodology can approximate the multivariate\nposterior predictive distribution of the pharmaceutical target and by-products,\nand therefore identify suitable operating values. Materials to replicate the\nanalysis can be found at www.github.com/amo105/chemicalkinetics.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 15:52:24 GMT"}, {"version": "v2", "created": "Thu, 16 Nov 2017 15:44:11 GMT"}, {"version": "v3", "created": "Wed, 27 Jun 2018 09:50:06 GMT"}, {"version": "v4", "created": "Mon, 22 Oct 2018 14:10:06 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Overstall", "Antony", ""], ["Woods", "David", ""], ["Martin", "Kieran", ""]]}, {"id": "1602.01370", "submitter": "Igor Barahona Dr", "authors": "Edis Mauricio Sanmiguel JaimesRelated, Igor Barahona Torres and\n  H\\'ector Hugo P\\'erez-Villarreal", "title": "Sensory evaluation of commercial coffee brands in Colombia", "comments": "This paper is a revised and expanded version of a paper entitled.\n  Evaluacion sensorial de marcas comerciales de cafe en Colombia. Presented at\n  the Sexto Coloquio Interdisciplinario de Doctorado. Universidad Popular\n  Autonoma del Estado de Puebla, Puebla City, Mexico, 25 June 2014", "journal-ref": "International Journal of Business and Systems Research (2015)", "doi": "10.1504/IJBSR.2015.071831", "report-no": null, "categories": "stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colombian coffee farmers have traditionally focused their efforts on\nactivities including seeding, planting and drying. Strategic issues to\nsuccessfully compete in the industry, such as branding, marketing and consumer\nresearch, have been neglected. In this research, we apply a type of sensory\nanalysis, based on several statistical techniques used to investigate the key\nfeatures of ten different brands of Colombian coffee. A panel composed of 32\njudges investigated nine different attributes related to flavour, fragrance,\nsweetness and acidity, among others. The last section presents the conclusions\nreached regarding customer preference and brands profiles.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 17:19:13 GMT"}], "update_date": "2016-02-08", "authors_parsed": [["JaimesRelated", "Edis Mauricio Sanmiguel", ""], ["Torres", "Igor Barahona", ""], ["P\u00e9rez-Villarreal", "H\u00e9ctor Hugo", ""]]}, {"id": "1602.01815", "submitter": "Michael Schmuker", "authors": "Michael Schmuker, Viktor Bahr, Ram\\'on Huerta", "title": "Exploiting plume structure to decode gas source distance using\n  metal-oxide gas sensors", "comments": "41 pages, 20 figures", "journal-ref": "Sensors and Actuators B: Chemical 235:636-646 (2016)", "doi": "10.1016/j.snb.2016.05.098", "report-no": null, "categories": "q-bio.NC physics.data-an stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the distance of a gas source is important in many applications of\nchemical sensing, like e.g. environmental monitoring, or chemically-guided\nrobot navigation. If an estimation of the gas concentration at the source is\navailable, source proximity can be estimated from the time-averaged gas\nconcentration at the sensing site. However, in turbulent environments, where\nfast concentration fluctuations dominate, comparably long measurements are\nrequired to obtain a reliable estimate. A lesser known feature that can be\nexploited for distance estimation in a turbulent environment lies in the\nrelationship between source proximity and the temporal variance of the local\ngas concentration - the farther the source, the more intermittent are gas\nencounters. However, exploiting this feature requires measurement of changes in\ngas concentration on a comparably fast time scale, that have up to now only\nbeen achieved using photo-ionisation detectors. Here, we demonstrate that by\nappropriate signal processing, off-the-shelf metal-oxide sensors are capable of\nextracting rapidly fluctuating features of gas plumes that strongly correlate\nwith source distance. We show that with a straightforward analysis method it is\npossible to decode events of large, consistent changes in the measured signal,\nso-called 'bouts'. The frequency of these bouts predicts the distance of a gas\nsource in wind-tunnel experiments with good accuracy. In addition, we found\nthat the variance of bout counts indicates cross-wind offset to the centreline\nof the gas plume. Our results offer an alternative approach to estimating gas\nsource proximity that is largely independent of gas concentration, using\noff-the-shelf metal-oxide sensors. The analysis method we employ demands very\nfew computational resources and is suitable for low-power microcontrollers.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 13:28:08 GMT"}, {"version": "v2", "created": "Wed, 18 May 2016 10:20:09 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Schmuker", "Michael", ""], ["Bahr", "Viktor", ""], ["Huerta", "Ram\u00f3n", ""]]}, {"id": "1602.03434", "submitter": "Guy Harling", "authors": "Guy Harling, Jukka-Pekka Onnela", "title": "Impact of degree truncation on the spread of a contagious process on\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.SI stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding how person-to-person contagious processes spread through a\npopulation requires accurate information on connections between population\nmembers. However, such connectivity data, when collected via interview, is\noften incomplete due to partial recall, respondent fatigue or study design,\ne.g., fixed choice designs (FCD) truncate out-degree by limiting the number of\ncontacts each respondent can report. Past research has shown how FCD truncation\naffects network properties, but its implications for predicted speed and size\nof spreading processes remain largely unexplored. To study the impact of degree\ntruncation on spreading processes, we generated collections of synthetic\nnetworks containing specific properties (degree distribution,\ndegree-assortativity, clustering), and also used empirical social network data\nfrom 75 villages in Karnataka, India. We simulated FCD using various truncation\nthresholds and ran a susceptible-infectious-recovered (SIR) process on each\nnetwork. We found that spreading processes propagated on truncated networks\nresulted in slower and smaller epidemics, with a sudden decrease in prediction\naccuracy at a level of truncation that varied by network type. Our results have\nimplications beyond FCD to truncation due to any limited sampling from a larger\nnetwork. We conclude that knowledge of network structure is important for\nunderstanding the accuracy of predictions of process spread on degree truncated\nnetworks.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 16:33:33 GMT"}], "update_date": "2016-02-11", "authors_parsed": [["Harling", "Guy", ""], ["Onnela", "Jukka-Pekka", ""]]}, {"id": "1602.03926", "submitter": "Igor Barahona Dr", "authors": "Igor Barahona, Judith Cavazos, and Jian-Bo Yang", "title": "Modelling the level of adoption of analytical tools; An implementation\n  of multi-criteria evidential reasoning", "comments": "Keywords: MCDA methods; evidential reasoning; analytical tools;\n  multiple source data", "journal-ref": "International Journal of Supply and Operations Management. (2014)\n  Vol.1, Issue 2, pp 129-151", "doi": null, "report-no": null, "categories": "stat.AP cs.CY stat.OT", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In the future, competitive advantages will be given to organisations that can\nextract valuable information from massive data and make better decisions. In\nmost cases, this data comes from multiple sources. Therefore, the challenge is\nto aggregate them into a common framework in order to make them meaningful and\nuseful. This paper will first review the most important multi-criteria decision\nanalysis methods (MCDA) existing in current literature. We will offer a novel,\npractical and consistent methodology based on a type of MCDA, to aggregate data\nfrom two different sources into a common framework. Two datasets that are\ndifferent in nature but related to the same topic are aggregated to a common\nscale by implementing a set of transformation rules. This allows us to generate\nappropriate evidence for assessing and finally prioritising the level of\nadoption of analytical tools in four types of companies. A numerical example is\nprovided to clarify the form for implementing this methodology. A six-step\nprocess is offered as a guideline to assist engineers, researchers or\npractitioners interested in replicating this methodology in any situation where\nthere is a need to aggregate and transform multiple source data.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 23:02:10 GMT"}], "update_date": "2016-05-11", "authors_parsed": [["Barahona", "Igor", ""], ["Cavazos", "Judith", ""], ["Yang", "Jian-Bo", ""]]}, {"id": "1602.04091", "submitter": "Julia Wrobel", "authors": "Julia Wrobel, So Young Park, Ana Maria Staicu and Jeff Goldsmith", "title": "Interactive graphics for functional data analyses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although there are established graphics that accompany the most common\nfunctional data analyses, generating these graphics for each dataset and\nanalysis can be cumbersome and time consuming. Often, the barriers to\nvisualization inhibit useful exploratory data analyses and prevent the\ndevelopment of intuition for a method and its application to a particular\ndataset. The refund.shiny package was developed to address these issues for\nseveral of the most common functional data analyses. After conducting an\nanalysis, the plot_shiny() function is used to generate an interactive\nvisualization environment that contains several distinct graphics, many of\nwhich are updated in response to user input. These visualizations reduce the\nburden of exploratory analyses and can serve as a useful tool for the\ncommunication of results to non-statisticians.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 15:57:27 GMT"}], "update_date": "2016-02-15", "authors_parsed": [["Wrobel", "Julia", ""], ["Park", "So Young", ""], ["Staicu", "Ana Maria", ""], ["Goldsmith", "Jeff", ""]]}, {"id": "1602.06696", "submitter": "Simon Wood", "authors": "Natalya Pya and Simon N Wood", "title": "A note on basis dimension selection in generalized additive modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two new approaches for checking the dimension of the basis functions when\nusing penalized regression smoothers are presented. The first approach is a\ntest for adequacy of the basis dimension based on an estimate of the residual\nvariance calculated by differencing residuals that are neighbours according to\nthe smooth covariates. The second approach is based on estimated degrees of\nfreedom for a smooth of the model residuals with respect to the model\ncovariates. In comparison with basis dimension selection algorithms based on\nsmoothness selection criterion (GCV, AIC, REML) the above procedures are\ncomputationally efficient enough for routine use as part of model checking.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 09:30:12 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Pya", "Natalya", ""], ["Wood", "Simon N", ""]]}, {"id": "1602.07836", "submitter": "Vesa Palonen Dr", "authors": "V. Palonen", "title": "A Bayesian baseline for belief in uncommon events", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The plausibility of uncommon events and miracles based on testimony of such\nan event has been much discussed. When analyzing the probabilities involved, it\nhas mostly been assumed that the common events can be taken as data in the\ncalculations. However, we usually have only testimonies for the common events.\nWhile this difference does not have a significant effect on the inductive part\nof the inference, it has a large influence on how one should view the\nreliability of testimonies. In this work, a full Bayesian solution is given for\nthe more realistic case, where one has a large number of testimonies for a\ncommon event and one testimony for an uncommon event. It is seen that, in order\nfor there to be a large amount of testimonies for a common event, the\ntestimonies will probably be quite reliable. For this reason, because the\ntestimonies are quite reliable based on the testimonies for the common events,\nthe probability for the uncommon event, given a testimony for it, is also\nhigher. Hence, one should be more open-minded when considering the plausibility\nof uncommon events.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 07:57:07 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2016 17:27:49 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Palonen", "V.", ""]]}]