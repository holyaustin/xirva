[{"id": "2007.00238", "submitter": "Youngjo Lee", "authors": "Youngjo Lee and Yudi Pawitan", "title": "Popper's falsification and corroboration from the statistical\n  perspectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The role of probability appears unchallenged as the key measure of\nuncertainty, used among other things for practical induction in the empirical\nsciences. Yet, Popper was emphatic in his rejection of inductive probability\nand of the logical probability of hypotheses; furthermore, for him, the degree\nof corroboration cannot be a probability. Instead he proposed a deductive\nmethod of testing. In many ways this dialectic tension has many parallels in\nstatistics, with the Bayesians on logico-inductive side vs the non-Bayesians or\nthe frequentists on the other side. Simplistically Popper seems to be on the\nfrequentist side, but recent synthesis on the non-Bayesian side might direct\nthe Popperian views to a more nuanced destination. Logical probability seems\nperfectly suited to measure partial evidence or support, so what can we use if\nwe are to reject it? For the past 100 years, statisticians have also developed\na related concept called likelihood, which has played a central role in\nstatistical modelling and inference. Remarkably, this Fisherian concept of\nuncertainty is largely unknown or at least severely under-appreciated in\nnon-statistical literature. As a measure of corroboration, the likelihood\nsatisfies the Popperian requirement that it is not a probability. Our aim is to\nintroduce the likelihood and its recent extension via a discussion of two\nwell-known logical fallacies in order to highlight that its lack of recognition\nmay have led to unnecessary confusion in our discourse about falsification and\ncorroboration of hypotheses. We highlight the 100 years of development of\nlikelihood concepts. The year 2021 will mark the 100-year anniversary of the\nlikelihood, so with this paper we wish it a long life and increased\nappreciation in non-statistical literature.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 04:43:38 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Lee", "Youngjo", ""], ["Pawitan", "Yudi", ""]]}, {"id": "2007.00765", "submitter": "Fang Liu", "authors": "Fang Liu", "title": "A Statistical Overview on Data Privacy", "comments": "To appear in Notre Dame Journal of Law, Ethics & Public Policy\n  (2020), Volume 34 Issue 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The eruption of big data with the increasing collection and processing of\nvast volumes and variety of data have led to breakthrough discoveries and\ninnovation in science, engineering, medicine, commerce, criminal justice, and\nnational security that would not have been possible in the past. While there\nare many benefits to the collection and usage of big data, there are also\ngrowing concerns among the general public on what personal information is\ncollected and how it is used. In addition to legal policies and regulations,\ntechnological tools and statistical strategies also exist to promote and\nsafeguard individual privacy, while releasing and sharing useful\npopulation-level information. In this overview, I introduce some of these\napproaches, as well as the existing challenges and opportunities in statistical\ndata privacy research and applications to better meet the practical needs of\nprivacy protection and information sharing.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 21:21:20 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Liu", "Fang", ""]]}, {"id": "2007.01360", "submitter": "Connor Dowd", "authors": "Connor Dowd", "title": "A New ECDF Two-Sample Test Statistic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical cumulative distribution functions (ECDFs) have been used to test\nthe hypothesis that two samples come from the same distribution since the\nseminal contribution by Kolmogorov and Smirnov. This paper describes a\nstatistic which is usable under the same conditions as Kolmogorov-Smirnov, but\nprovides more power than other extant tests in that vein. I demonstrate a valid\n(conservative) procedure for producing finite-sample p-values. I outline the\nclose relationship between this statistic and its two main predecessors. I also\nprovide a public R package (CRAN: twosamples [2018]) implementing the testing\nprocedure in $O(N\\log(N))$ time with $O(N)$ memory. Using the package's\nfunctions, I perform several simulation studies showing the power improvements.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 19:55:37 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Dowd", "Connor", ""]]}, {"id": "2007.01539", "submitter": "Edgar Bueno", "authors": "Edgar Bueno and Dan Hedlin", "title": "A method to find an efficient and robust sampling strategy under model\n  uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of deciding on sampling strategy, in particular\nsampling design. We propose a risk measure, whose minimizing value guides the\nchoice. The method makes use of a superpopulation model and takes into account\nuncertainty about its parameters. The method is illustrated with a real\ndataset, yielding satisfactory results. As a baseline, we use the strategy that\ncouples probability proportional-to-size sampling with the difference\nestimator, as it is known to be optimal when the superpopulation model is fully\nknown. We show that, even under moderate misspecifications of the model, this\nstrategy is not robust and can be outperformed by some alternatives\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 08:01:23 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Bueno", "Edgar", ""], ["Hedlin", "Dan", ""]]}, {"id": "2007.03611", "submitter": "Joshua Vogelstein", "authors": "Joshua T. Vogelstein", "title": "P-Values in a Post-Truth World", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The role of statisticians in society is to provide tools, techniques, and\nguidance with regards to how much to trust data. This role is increasingly more\nimportant with more data and more misinformation than ever before. The American\nStatistical Association recently released two statements on p-values, and\nprovided four guiding principles. We evaluate their claims using these\nprinciples and find that they failed to adhere to them. In this age of\ndistrust, we have an opportunity to be role models of trustworthiness, and\nresponsibility to take it.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 03:18:54 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Vogelstein", "Joshua T.", ""]]}, {"id": "2007.04180", "submitter": "Jim Albert", "authors": "Jim Albert", "title": "A Bayesian Redesign of the First Probability/Statistics Course", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The traditional calculus-based introduction to statistical inference consists\nof a semester of probability followed by a semester of frequentist inference.\nCobb (2015) challenges the statistical education community to rethink the\nundergraduate statistics curriculum. In particular, he suggests that we should\nfocus on two goals: making fundamental concepts accessible and minimizing\nprerequisites to research. Using five underlying principles of Cobb, we\ndescribe a new calculus-based introduction to statistics based on\nsimulation-based Bayesian computation.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 15:12:53 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Albert", "Jim", ""]]}, {"id": "2007.04758", "submitter": "Rosy Oh", "authors": "Jiwook Jang and Rosy Oh", "title": "A Bivariate Compound Dynamic Contagion Process for Cyber Insurance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As corporates and governments become more digital, they become vulnerable to\nvarious forms of cyber attack. Cyber insurance products have been used as risk\nmanagement tools, yet their pricing does not reflect actual risk, including\nthat of multiple, catastrophic and contagious losses. For the modelling of\naggregate losses from cyber events, in this paper we introduce a bivariate\ncompound dynamic contagion process, where the bivariate dynamic contagion\nprocess is a point process that includes both externally excited joint jumps,\nwhich are distributed according to a shot noise Cox process and two separate\nself-excited jumps, which are distributed according to the branching structure\nof a Hawkes process with an exponential fertility rate, respectively. We\nanalyse the theoretical distributional properties for these processes\nsystematically, based on the piecewise deterministic Markov process developed\nby Davis (1984) and the univariate dynamic contagion process theory developed\nby Dassios and Zhao (2011). The analytic expression of the Laplace transform of\nthe compound process and its moments are presented, which have the potential to\nbe applicable to a variety of problems in credit, insurance, market and other\noperational risks. As an application of this process, we provide insurance\npremium calculations based on its moments. Numerical examples show that this\ncompound process can be used for the modelling of aggregate losses from cyber\nevents. We also provide the simulation algorithm for statistical analysis,\nfurther business applications and research.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 04:40:18 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Jang", "Jiwook", ""], ["Oh", "Rosy", ""]]}, {"id": "2007.05236", "submitter": "Tim Sullivan", "authors": "L. Bonnet and J.-L. Akian and \\'E. Savin and T. J. Sullivan", "title": "Adaptive reconstruction of imperfectly-observed monotone functions, with\n  applications to uncertainty quantification", "comments": "19 pages, 10 figures. This is a preprint version of an article to\n  appear in Algorithms and differs from the publisher's final version in layout\n  and typographical detail", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA math.OC stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the desire to numerically calculate rigorous upper and lower\nbounds on deviation probabilities over large classes of probability\ndistributions, we present an adaptive algorithm for the reconstruction of\nincreasing real-valued functions. While this problem is similar to the\nclassical statistical problem of isotonic regression, the optimisation setting\nalters several characteristics of the problem and opens natural algorithmic\npossibilities. We present our algorithm, establish sufficient conditions for\nconvergence of the reconstruction to the ground truth, and apply the method to\nsynthetic test cases and a real-world example of uncertainty quantification for\naerodynamic design.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 08:20:53 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 15:45:53 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Bonnet", "L.", ""], ["Akian", "J. -L.", ""], ["Savin", "\u00c9.", ""], ["Sullivan", "T. J.", ""]]}, {"id": "2007.05748", "submitter": "Christian Hennig", "authors": "Christian Hennig", "title": "Frequentism-as-model", "comments": "34 pages no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most statisticians are aware that probability models interpreted in a\nfrequentist manner are not really true in objective reality, but only\nidealisations. I argue that this is often ignored when actually applying\nfrequentist methods and interpreting the results, and that keeping up the\nawareness for the essential difference between reality and models can lead to a\nmore appropriate use and interpretation of frequentist models and methods,\ncalled frequentism-as-model. This is elaborated showing connections to existing\nwork, appreciating the special role of i.i.d. models and subject matter\nknowledge, giving an account of how and under what conditions models that are\nnot true can be useful, giving detailed interpretations of tests and confidence\nintervals, confronting their implicit compatibility logic with the inverse\nprobability logic of Bayesian inference, re-interpreting the role of model\nassumptions, appreciating robustness, and the role of ``interpretative\nequivalence'' of models. Epistemic (often referred to as Bayesian) probability\nshares the issue that its models are only idealisations and not really true for\nmodelling reasoning about uncertainty, meaning that it does not have an\nessential advantage over frequentism, as is often claimed. Bayesian statistics\ncan be combined with frequentism-as-model, leading to what Gelman and Hennig\n(2017) call ``falsificationist Bayes''.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 11:22:16 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Hennig", "Christian", ""]]}, {"id": "2007.05857", "submitter": "Lourens  Waldorp", "authors": "Lourens Waldorp and Maarten Marsman and Denny Borsboom", "title": "Reliability of decisions based on tests: Fourier analysis of Boolean\n  decision functions", "comments": "41 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Items in a test are often used as a basis for making decisions and such tests\nare therefore required to have good psychometric properties, like\nunidimensionality. In many cases the sum score is used in combination with a\nthreshold to decide between pass or fail, for instance. Here we consider\nwhether such a decision function is appropriate, without a latent variable\nmodel, and which properties of a decision function are desirable. We consider\nreliability (stability) of the decision function, i.e., does the decision\nchange upon perturbations, or changes in a fraction of the outcomes of the\nitems (measurement error). We are concerned with questions of whether the sum\nscore is the best way to aggregate the items, and if so why. We use ideas from\ntest theory, social choice theory, graphical models, computer science and\nprobability theory to answer these questions. We conclude that a weighted sum\nscore has desirable properties that (i) fit with test theory and is observable\n(similar to a condition like conditional association), (ii) has the property\nthat a decision is stable (reliable), and (iii) satisfies Rousseau's criterion\nthat the input should match the decision. We use Fourier analysis of Boolean\nfunctions to investigate whether a decision function is stable and to figure\nout which (set of) items has proportionally too large an influence on the\ndecision. To apply these techniques we invoke ideas from graphical models and\nuse a pseudo-likelihood factorisation of the probability distribution.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 21:24:48 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Waldorp", "Lourens", ""], ["Marsman", "Maarten", ""], ["Borsboom", "Denny", ""]]}, {"id": "2007.06065", "submitter": "Shane Jensen", "authors": "Jesse Cui and Shane T. Jensen", "title": "The Effects of Vacant Lot Greening and the Impact of Land Use and\n  Business Vibrancy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the ongoing Philadelphia LandCare (PLC) vacant lot greening\ninitiative and evaluate the association between this built environment\nintervention and changes in crime incidence. We develop a propensity score\nmatching analysis that estimates the effect of vacant lot greening on different\ntypes of crime while accounting for substantial differences between greened and\nungreened lots in terms of their surrounding demographic, economic, land use\nand business vibrancy characteristics. Within these matched pairs of greened\nvs. ungreened vacant lots, we estimate larger and more significant beneficial\neffects of greening for reducing violent, non-violent and total crime compared\nto comparisons of greened vs. ungreened lots without matching. We also\ninvestigate the impact of land use zoning and business vibrancy and find that\nthe effect of vacant lot greening on total crime is substantially affected by\nparticular types of surrounding land use zoning and the presence of certain\nbusiness types.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 18:44:40 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Cui", "Jesse", ""], ["Jensen", "Shane T.", ""]]}, {"id": "2007.06543", "submitter": "Dmitri Koroliouk", "authors": "M.L. Bertotti, S.O. Dovgyi, D. Koroliouk", "title": "Dynamics of ternary statistical experiments with equilibrium state", "comments": "7 pages, 2 figures", "journal-ref": "Journal of Computational & Applied Mathematics, Kiev, 2015, No.2\n  (119), 3-7", "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the scenarios of the dynamics of ternary statistical experiments,\nmodeled employing difference equations. The important features are a balance\ncondition and the existence of a steady-state (equilibrium). We give a\nclassification of scenarios of the model evolution which are significantly\ndifferent between them, depending on the domain of the values of the model\nbasic parameters.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 19:36:43 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Bertotti", "M. L.", ""], ["Dovgyi", "S. O.", ""], ["Koroliouk", "D.", ""]]}, {"id": "2007.06701", "submitter": "Romain Legrand Phd", "authors": "Romain Legrand", "title": "Incertitudes et mesures", "comments": "in French", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Educational guide focused on the statistical treatment of measurement\nuncertainties. The conditions of application of current practices are detailed\nand precised: mean values, central limit theorem, linear regression. The last\ntwo chapters are devoted to an introduction to the Bayesian inference and a\nseries of application cases: machine failure date, elimination of a background\nnoise, linear adjustment with elimination of outliers.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 07:46:23 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Legrand", "Romain", ""]]}, {"id": "2007.08160", "submitter": "PierGianLuca Porta Mana", "authors": "P.G.L. Porta Mana", "title": "The rule of conditional probability is valid in quantum theory [Comment\n  on Gelman & Yao's \"Holes in Bayesian Statistics\"]", "comments": "V2: 14 pages, corrected typos, added references and some colour. V3:\n  added and updated references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT quant-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In a recent manuscript, Gelman & Yao (2020) claim that \"the usual rules of\nconditional probability fail in the quantum realm\" and that \"probability theory\nisn't true (quantum physics)\" and purport to support these statements with the\nexample of a quantum double-slit experiment. The present comment recalls some\nrelevant literature in quantum theory and shows that (i) Gelman & Yao's\nstatements are false; in fact, the quantum example confirms the rules of\nprobability theory; (ii) the particular inequality found in the quantum example\ncan be shown to appear also in very non-quantum examples, such as drawing from\nan urn; thus there is nothing peculiar to quantum theory in this matter. A\ncouple of wrong or imprecise statements about quantum theory in the cited\nmanuscript are also corrected.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 07:45:11 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 08:46:38 GMT"}, {"version": "v3", "created": "Fri, 31 Jul 2020 13:55:11 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Mana", "P. G. L. Porta", ""]]}, {"id": "2007.09996", "submitter": "Etienne Boursier", "authors": "Etienne Boursier and Vianney Perchet and Marco Scarsini", "title": "Social Learning from Reviews in Non-Stationary Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Potential buyers of a product or service tend to read reviews from previous\nconsumers before making their decisions. This behavior is modeled by a market\nof Bayesian consumers with heterogeneous preferences, who sequentially decide\nwhether to buy an item of unknown quality, based on previous buyers' reviews.\nThe quality is multi-dimensional and the reviews can assume one of different\nforms and can also be multi-dimensional. The belief about the item's quality in\nsimple uni-dimensional settings is known to converge to its true value. Our\npaper extends this result to the more general case of a multidimensional\nquality, possibly in a continuous space, and provides anytime convergence\nrates. In practice, the quality of an item may vary over time, due to some\nchange in the production process or the need to keep up with the competition.\nThis paper also studies the learning dynamic when the unknown quality changes\nat random times and shows that the cost of learning is rather small\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 10:46:58 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 11:03:34 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Boursier", "Etienne", ""], ["Perchet", "Vianney", ""], ["Scarsini", "Marco", ""]]}, {"id": "2007.10121", "submitter": "Shiva H C Prasad Prof", "authors": "Dilip U Shenoy, Vinay Sharma, Shiva HC Prasad", "title": "Strategic Evaluation in Optimizing the Internal Supply Chain Using\n  TOPSIS: Evidence In A Coil Winding Machine Manufacturer", "comments": "04 pages, 03 figure, 07 tables", "journal-ref": "Acta Mechanica Malaysia, 4(1) : 01-04 (2020)", "doi": null, "report-no": null, "categories": "cs.AI stat.OT", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Most of the manufacturing firm aims to optimize their Supply Chain in terms\nof improved profitability of its products through value Addition. This study\ntakes a critical look into the factors that affect the Performance of internal\nsupply chain with respect to specific criteria. Accordingly, ranking these\nfactors to get the critical dimensions of supply chain performance in the\nmanufacturing industry. A semi-structured interview with the pre-defined set of\nquestions used to collect the responses from decision makers of the firm. Multi\ncriteria decision-making tool called TOPSIS is used to evaluate the responses\nand rank the factors. The results of this indicate that supplier relationship\nand inventory planning were most principal factors positively influencing\non-time delivery of the product, production flexibility, cost savings,\nadditional costs. This study helps to identify and optimize the process\nparameters using objective and subjective evaluation approach. The combined\ninfluence of the thought process of the manager to optimize the internal supply\nchain is extracted in this work.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 12:46:42 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Shenoy", "Dilip U", ""], ["Sharma", "Vinay", ""], ["Prasad", "Shiva HC", ""]]}, {"id": "2007.12210", "submitter": "Roger Peng", "authors": "Roger D. Peng and Stephanie C. Hicks", "title": "Reproducible Research: A Retrospective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid advances in computing technology over the past few decades have spurred\ntwo extraordinary phenomena in science: large-scale and high-throughput data\ncollection coupled with the creation and implementation of complex statistical\nalgorithms for data analysis. Together, these two phenomena have brought about\ntremendous advances in scientific discovery but have also raised two serious\nconcerns, one relatively new and one quite familiar. The complexity of modern\ndata analyses raises questions about the reproducibility of the analyses,\nmeaning the ability of independent analysts to re-create the results claimed by\nthe original authors using the original data and analysis techniques. While\nseemingly a straightforward concept, reproducibility of analyses is typically\nthwarted by the lack of availability of the data and computer code that were\nused in the analyses. A much more general concern is the replicability of\nscientific findings, which concerns the frequency with which scientific claims\nare confirmed by completely independent investigations. While the concepts of\nreproduciblity and replicability are related, it is worth noting that they are\nfocused on quite different goals and address different aspects of scientific\nprogress. In this review, we will discuss the origins of reproducible research,\ncharacterize the current status of reproduciblity in public health research,\nand connect reproduciblity to current concerns about replicability of\nscientific findings. Finally, we describe a path forward for improving both the\nreproducibility and replicability of public health research in the future.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 18:38:12 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Peng", "Roger D.", ""], ["Hicks", "Stephanie C.", ""]]}, {"id": "2007.12631", "submitter": "Michael Kane", "authors": "Michael J. Kane and Simon Urbanek", "title": "On the Programmatic Generation of Reproducible Documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.AI stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reproducible document standards, like R Markdown, facilitate the programmatic\ncreation of documents whose content is itself programmatically generated. While\nthese documents are generally not complete in the sense that they will not\ninclude prose content, generated by an author to provide context, a narrative,\netc., programmatic generation can provide substantial efficiencies for\nstructuring and constructing documents. This paper explores the programmatic\ngeneration of reproducible by distinguishing components than can be created by\ncomputational means from those requiring human-generated prose, providing\nguidelines for the generation of these documents, and identifying a use case in\nclinical trial reporting. These concepts and use case are illustrated through\nthe listdown package for the R programming environment, which is is currently\navailable on the Comprehensive R Archive Network (CRAN).\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 22:32:58 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Kane", "Michael J.", ""], ["Urbanek", "Simon", ""]]}, {"id": "2007.13646", "submitter": "Riffat Jabeen Dr.", "authors": "Azam Zaka (1), Ahmad Saeed Akhter (1) and Riffat Jabeen (2) ((1)\n  College of Statistical and Actuarial Sciences, University of the Punjab,\n  Lahore, Pakistan, (2) COMSATS University Islamabad Lahore Campus, Lahore,\n  Pakistan)", "title": "Asymmetry approach to study for chemotherapy treatment and devices\n  failure times data using modified Power function distribution with some\n  modified estimators", "comments": "No.of Pages: 10, Figures:5, No. of tables:5", "journal-ref": "International Journal of Computing Science and Mathematics (2020)", "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to improve the already existing models that are used extensively in\nbio sciences and applied sciences research, a new class of Weighted Power\nfunction distribution (WPFD) has been proposed with its various properties and\ndifferent modifications to be more applicable in real life. We have provided\nthe mathematical derivations for the new distribution including moments,\nincomplete moments, conditional moments, inverse moments, mean residual\nfunction, vitality function, order statistics, mills ratio, information\nfunction, Shannon entropy, Bonferroni and Lorenz curves and quantile function.\nWe have also characterized the WPFD, based on doubly truncated mean. The aim of\nthe study is to increase the application of the Power function distribution.\nThe main feature of the proposed distribution is that there is no induction of\nparameters as compare to the other generalization of the distributions, which\nare complexed having many parameters. We have used R programming to estimate\nthe parameters of the new class of WPFD using Maximum Likelihood Method (MLM),\nPercentile Estimators (P.E) and their modified estimators. After analyzing the\ndata, we conclude that the proposed model WPFD performs better in the data sets\nwhile compared to different competitor models.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 10:58:52 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Zaka", "Azam", ""], ["Akhter", "Ahmad Saeed", ""], ["Jabeen", "Riffat", ""]]}, {"id": "2007.15583", "submitter": "E. Lins", "authors": "Gianfranco de M. Stieven, Edilma P. Oliveira, Erb F. Lins", "title": "Performance Analysis of Metaheuristic Optimization Algorithms in\n  Estimating the Interfacial Heat Transfer Coefficient on Directional\n  Solidification", "comments": "27 pages, 7 figures, 4 tables, 67 references cited, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.OT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper is proposed an evaluation of ten metaheuristic optimization\nalgorithms applied on the inverse optimization of the Interfacial Heat Transfer\nCoefficient (IHTC) coupled on the solidification phenomenon. It was considered\nan upward directional solidification system for Al-7wt.% Si alloy and, for IHTC\nmodel, a exponential time function. All thermophysical properties of the alloy\nwere considered constant. Scheil Rule was used as segregation model ahead\nphase-transformation interface. Optimization results from Markov Chain Monte\nCarlo method (MCMC) were considered as reference. Based on average, quantiles\n95% and 5%, kurtosis, average iterations and absolute errors of the\nmetaheuristic methods, in relation to MCMC results, the Flower Pollination\nAlgorithm (FPA) and Moth-Flame Optimization (MFO) presented the most\nappropriate results, outperforming the other methods in this particular\nphenomenon, based on these metrics. The regions with the most probable values\nfor parameters in IHTC time function were also determined.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 01:13:23 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Stieven", "Gianfranco de M.", ""], ["Oliveira", "Edilma P.", ""], ["Lins", "Erb F.", ""]]}, {"id": "2007.15634", "submitter": "Ralph Foorthuis", "authors": "Ralph Foorthuis", "title": "On the Nature and Types of Anomalies: A Review of Deviations in Data", "comments": "39 pages (30 pages content), 10 figures and 3 tables. Preprint;\n  comments will be appreciated. Improvements in version 3: Added new anomaly\n  subtypes; Tightening of definitions; Additional examples from new literature;\n  Various minor additions and improvements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LG stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomalies are occurrences in a dataset that are in some way unusual and do\nnot fit the general patterns. The concept of the anomaly is usually ill-defined\nand perceived as vague and domain-dependent. Moreover, despite some 250 years\nof publications on the topic, no comprehensive and concrete overviews of the\ndifferent types of anomalies have hitherto been published. By means of an\nextensive literature review this study therefore offers the first theoretically\nprincipled and domain-independent typology of data anomalies, and presents a\nfull overview of anomaly types and subtypes. To concretely define the concept\nof the anomaly and its different manifestations, the typology employs five\ndimensions: data type, cardinality of relationship, anomaly level, data\nstructure, and data distribution. These fundamental and data-centric dimensions\nnaturally yield 3 broad groups, 9 basic types and 63 subtypes of anomalies. The\ntypology facilitates the evaluation of the functional capabilities of anomaly\ndetection algorithms, contributes to explainable data science, and provides\ninsights into relevant topics such as local versus global anomalies.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 17:55:11 GMT"}, {"version": "v2", "created": "Sun, 27 Dec 2020 22:15:50 GMT"}, {"version": "v3", "created": "Sat, 15 May 2021 11:45:58 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Foorthuis", "Ralph", ""]]}]