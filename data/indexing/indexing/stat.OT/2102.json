[{"id": "2102.00757", "submitter": "Raya Muttarak", "authors": "Raya Muttarak", "title": "Demographic perspectives in research on global environmental change", "comments": "32 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": "WP-21-001", "categories": "stat.OT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Human population is at the centre of research on global environmental change.\nOn the one hand, population dynamics influence the environment and the global\nclimate system through consumption-based carbon emissions. On the other hand,\nhealth and wellbeing of the population is already being affected by climate\nchange. The knowledge on population dynamics and population heterogeneity thus\nis fundamental in improving our understanding of how population size,\ncomposition and distribution influence global environmental change and how\nthese changes affect subgroups of population differentially by demographic\ncharacteristics and spatial distribution. Existing theoretical concepts and\nmethodological tools in demography can be readily applied to the study of\npopulation and global environmental change. In the past couple of decades,\ndemographic research has enriched climate change research both in the analysis\nof the impact of population dynamics on the global climate system as well as\nthe impact of climate change on human population. What is missing in the\nliterature is the study that investigates how global environmental change\naffect current and future demographic processes and consequently population\ntrends. If global environmental change does influence fertility, mortality and\nmigration, the three key demographic components underlying population change,\npopulation estimates and forecast need to adjust from the climate feedback in\npopulation projections. Indisputably, this is the new area of research that\ndirectly requires expertise in population science and contribution from\ndemographers.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 10:38:54 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Muttarak", "Raya", ""]]}, {"id": "2102.01647", "submitter": "Cyrille Feudjio", "authors": "Cyrille Feudjio, Victoire Djimna Noyum, Younous Perieukeu Mofendjou,\n  Rockefeller, Ernest Fokou\\'e", "title": "A Novel Use of Discrete Wavelet Transform Features in the Prediction of\n  Epileptic Seizures from EEG Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG eess.SP stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper demonstrates the predictive superiority of discrete wavelet\ntransform (DWT) over previously used methods of feature extraction in the\ndiagnosis of epileptic seizures from EEG data. Classification accuracy,\nspecificity, and sensitivity are used as evaluation metrics. We specifically\nshow the immense potential of 2 combinations (DWT-db4 combined with SVM and\nDWT-db2 combined with RF) as compared to others when it comes to diagnosing\nepileptic seizures either in the balanced or the imbalanced dataset. The\nresults also highlight that MFCC performs less than all the DWT used in this\nstudy and that, The mean-differences are statistically significant respectively\nin the imbalanced and balanced dataset. Finally, either in the balanced or the\nimbalanced dataset, the feature extraction techniques, the models, and the\ninteraction between them have a statistically significant effect on the\nclassification accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 17:21:10 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Feudjio", "Cyrille", ""], ["Noyum", "Victoire Djimna", ""], ["Mofendjou", "Younous Perieukeu", ""], ["Rockefeller", "", ""], ["Fokou\u00e9", "Ernest", ""]]}, {"id": "2102.01892", "submitter": "Noel Cressie", "authors": "Noel Cressie", "title": "A few statistical principles for data science", "comments": "19 pages; written for a special issue (festschrift) of the Australian\n  and New Zealand Journal of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In any other circumstance, it might make sense to define the extent of the\nterrain (Data Science) first, and then locate and describe the landmarks\n(Principles). But this data revolution we are experiencing defies a cadastral\nsurvey. Areas are continually being annexed into Data Science. For example,\nbiometrics was traditionally statistics for agriculture in all its forms but\nnow, in Data Science, it means the study of characteristics that can be used to\nidentify an individual. Examples of non-intrusive measurements include height,\nweight, fingerprints, retina scan, voice, photograph/video (facial landmarks\nand facial expressions), and gait. A multivariate analysis of such data would\nbe a complex project for a statistician, but a software engineer might appear\nto have no trouble with it at all. In any applied-statistics project, the\nstatistician worries about uncertainty and quantifies it by modelling data as\nrealisations generated from a probability space. Another approach to\nuncertainty quantification is to find similar data sets, and then use the\nvariability of results between these data sets to capture the uncertainty. Both\napproaches allow 'error bars' to be put on estimates obtained from the original\ndata set, although the interpretations are different. A third approach, that\nconcentrates on giving a single answer and gives up on uncertainty\nquantification, could be considered as Data Engineering, although it has staked\na claim in the Data Science terrain. This article presents a few (actually\nnine) statistical principles for data scientists that have helped me, and\ncontinue to help me, when I work on complex interdisciplinary projects.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 06:28:03 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Cressie", "Noel", ""]]}, {"id": "2102.03106", "submitter": "Valeria Policastro", "authors": "Valeria Policastro, Dario Righelli, Annamaria Carissimo, Luisa Cutillo\n  and Italia De Feis", "title": "ROBustness In Network (robin): an R package for Comparison and\n  Validation of communities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In network analysis, many community detection algorithms have been developed,\nhowever, their implementation leaves unaddressed the question of the\nstatistical validation of the results. Here we present robin(ROBustness In\nNetwork), an R package to assess the robustness of the community structure of a\nnetwork found by one or more methods to give indications about their\nreliability. The procedure initially detects if the community structure found\nby a set of algorithms is statistically significant and then compares two\nselected detection algorithms on the same graph to choose the one that better\nfits the network of interest. We demonstrate the use of our package on the\nAmerican College Football benchmark dataset.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 11:12:18 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Policastro", "Valeria", ""], ["Righelli", "Dario", ""], ["Carissimo", "Annamaria", ""], ["Cutillo", "Luisa", ""], ["De Feis", "Italia", ""]]}, {"id": "2102.03240", "submitter": "Zhu Liu", "authors": "Zhu Liu, Biqing Zhu, Philippe Ciais, Steven J. Davis, Chenxi Lu,\n  Haiwang Zhong, Piyu Ke, Yanan Cui, Zhu Deng, Duo Cui, Taochun Sun, Xinyu Dou,\n  Jianguang Tan, Rui Guo, Bo Zheng, Katsumasa Tanaka, Wenli Zhao, Pierre\n  Gentine", "title": "De-carbonization of global energy use during the COVID-19 pandemic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ao-ph econ.GN q-fin.EC stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The COVID-19 pandemic has disrupted human activities, leading to\nunprecedented decreases in both global energy demand and GHG emissions. Yet a\nlittle known that there is also a low carbon shift of the global energy system\nin 2020. Here, using the near-real-time data on energy-related GHG emissions\nfrom 30 countries (about 70% of global power generation), we show that the\npandemic caused an unprecedented de-carbonization of global power system,\nrepresenting by a dramatic decrease in the carbon intensity of power sector\nthat reached a historical low of 414.9 tCO2eq/GWh in 2020. Moreover, the share\nof energy derived from renewable and low-carbon sources (nuclear, hydro-energy,\nwind, solar, geothermal, and biomass) exceeded that from coal and oil for the\nfirst time in history in May of 2020. The decrease in global net energy demand\n(-1.3% in the first half of 2020 relative to the average of the period in\n2016-2019) masks a large down-regulation of fossil-fuel-burning power plants\nsupply (-6.1%) coincident with a surge of low-carbon sources (+6.2%).\nConcomitant changes in the diurnal cycle of electricity demand also favored\nlow-carbon generators, including a flattening of the morning ramp, a lower\nmidday peak, and delays in both the morning and midday load peaks in most\ncountries. However, emission intensities in the power sector have since\nrebounded in many countries, and a key question for climate mitigation is thus\nto what extent countries can achieve and maintain lower, pandemic-level carbon\nintensities of electricity as part of a green recovery.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 15:37:57 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Liu", "Zhu", ""], ["Zhu", "Biqing", ""], ["Ciais", "Philippe", ""], ["Davis", "Steven J.", ""], ["Lu", "Chenxi", ""], ["Zhong", "Haiwang", ""], ["Ke", "Piyu", ""], ["Cui", "Yanan", ""], ["Deng", "Zhu", ""], ["Cui", "Duo", ""], ["Sun", "Taochun", ""], ["Dou", "Xinyu", ""], ["Tan", "Jianguang", ""], ["Guo", "Rui", ""], ["Zheng", "Bo", ""], ["Tanaka", "Katsumasa", ""], ["Zhao", "Wenli", ""], ["Gentine", "Pierre", ""]]}, {"id": "2102.03667", "submitter": "Mohammed Alser", "authors": "Mohammed Alser, Jeremie S. Kim, Nour Almadhoun Alserr, Stefan W. Tell,\n  Onur Mutlu", "title": "COVIDHunter: An Accurate, Flexible, and Environment-Aware Open-Source\n  COVID-19 Outbreak Simulation Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.LG cs.SI stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivation: Early detection and isolation of COVID-19 patients are essential\nfor successful implementation of mitigation strategies and eventually curbing\nthe disease spread. With a limited number of daily COVID19 tests performed in\nevery country, simulating the COVID-19 spread along with the potential effect\nof each mitigation strategy currently remains one of the most effective ways in\nmanaging the healthcare system and guiding policy-makers. We introduce\nCOVIDHunter, a flexible and accurate COVID-19 outbreak simulation model that\nevaluates the current mitigation measures that are applied to a region and\nprovides suggestions on what strength the upcoming mitigation measure should\nbe. The key idea of COVIDHunter is to quantify the spread of COVID-19 in a\ngeographical region by simulating the average number of new infections caused\nby an infected person considering the effect of external factors, such as\nenvironmental conditions (e.g., climate, temperature, humidity) and mitigation\nmeasures.\n  Results: Using Switzerland as a case study, COVIDHunter estimates that the\npolicy-makers need to keep the current mitigation measures for at least 30 days\nto prevent demand from quickly exceeding existing hospital capacity. Relaxing\nthe mitigation measures by 50% for 30 days increases both the daily capacity\nneed for hospital beds and daily number of deaths exponentially by an average\nof 23.8x, who may occupy ICU beds and ventilators for a period of time. Unlike\nexisting models, the COVIDHunter model accurately monitors and predicts the\ndaily number of cases, hospitalizations, and deaths due to COVID-19. Our model\nis flexible to configure and simple to modify for modeling different scenarios\nunder different environmental conditions and mitigation measures.\n  Availability: https://github.com/CMU-SAFARI/COVIDHunter\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 21:01:56 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Alser", "Mohammed", ""], ["Kim", "Jeremie S.", ""], ["Alserr", "Nour Almadhoun", ""], ["Tell", "Stefan W.", ""], ["Mutlu", "Onur", ""]]}, {"id": "2102.04421", "submitter": "Younous Mofenjou Peuriekeu", "authors": "Younous Mofenjou Peuriekeu, Victoire Djimna Noyum, Cyrille Feudjio,\n  Alkan Goktug and Ernest Fokoue", "title": "A Text Mining Discovery of Similarities and Dissimilarities Among Sacred\n  Scriptures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The careful examination of sacred texts gives valuable insights into human\npsychology, different ideas regarding the organization of societies as well as\ninto terms like truth and God. To improve and deepen our understanding of\nsacred texts, their comparison, and their separation is crucial. For this\npurpose, we use our data set has nine sacred scriptures. This work deals with\nthe separation of the Quran, the Asian scriptures Tao-Te-Ching, the Buddhism,\nthe Yogasutras, and the Upanishads as well as the four books from the Bible,\nnamely the Book of Proverbs, the Book of Ecclesiastes, the Book of\nEcclesiasticus, and the Book of Wisdom. These scriptures are analyzed based on\nthe natural language processing NLP creating the mathematical representation of\nthe corpus in terms of frequencies called document term matrix (DTM). After\nthis analysis, machine learning methods like supervised and unsupervised\nlearning are applied to perform classification. Here we use the Multinomial\nNaive Bayes (MNB), the Super Vector Machine (SVM), the Random Forest (RF), and\nthe K-nearest Neighbors (KNN). We obtain that among these methods MNB is able\nto predict the class of a sacred text with an accuracy of about 85.84 %.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 18:32:50 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Peuriekeu", "Younous Mofenjou", ""], ["Noyum", "Victoire Djimna", ""], ["Feudjio", "Cyrille", ""], ["Goktug", "Alkan", ""], ["Fokoue", "Ernest", ""]]}, {"id": "2102.08087", "submitter": "Etienne Boursier", "authors": "Etienne Boursier and Tristan Garrec and Vianney Perchet and Marco\n  Scarsini", "title": "Making the most of your day: online learning for optimal allocation of\n  time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study online learning for optimal allocation when the resource to be\nallocated is time. Examples of possible applications include a driver filling a\nday with rides, a landlord renting an estate, etc. Following our initial\nmotivation, a driver receives ride proposals sequentially according to a\nPoisson process and can either accept or reject a proposed ride. If she accepts\nthe proposal, she is busy for the duration of the ride and obtains a reward\nthat depends on the ride duration. If she rejects it, she remains on hold until\na new ride proposal arrives. We study the regret incurred by the driver first\nwhen she knows her reward function but does not know the distribution of the\nride duration, and then when she does not know her reward function, either.\nFaster rates are finally obtained by adding structural assumptions on the\ndistribution of rides or on the reward function. This natural setting bears\nsimilarities with contextual (one-armed) bandits, but with the crucial\ndifference that the normalized reward associated to a context depends on the\nwhole distribution of contexts.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 11:19:51 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Boursier", "Etienne", ""], ["Garrec", "Tristan", ""], ["Perchet", "Vianney", ""], ["Scarsini", "Marco", ""]]}, {"id": "2102.08847", "submitter": "Joerg Drechsler", "authors": "Joerg Drechsler", "title": "Differential Privacy for Government Agencies -- Are We There Yet?", "comments": "41 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR stat.OT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Government agencies always need to carefully consider potential risks of\ndisclosure whenever they publish statistics based on their data or give\nexternal researchers access to the collected data. For this reason, research on\ndisclosure avoiding techniques has a long tradition at statistical agencies. In\nthis context, the promise of formal privacy guarantees offered by concepts such\nas differential privacy seem to be the panacea enabling the agencies to exactly\nquantify and control the privacy loss incurred by any data release. Still,\ndespite the excitement in academia and industry, most agencies-with the\nprominent exception of the U.S. Census Bureau-have been reluctant to even\nconsider the concept for their data release strategy.\n  This paper aims to shed some light on potential reasons for this. We argue\nthat the requirements when implementing differential privacy approaches at\ngovernment agencies are often fundamentally different from the requirements in\nindustry. This raises many challenging problems and open questions that still\nneed to be addressed before the concept might be used as an overarching\nprinciple when sharing data with the public. The paper will not offer any\nsolutions to these challenges. Instead, we hope to stimulate some collaborative\nresearch efforts, as we believe that many of the problems can only be addressed\nby inter-disciplinary collaborations.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 16:13:09 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Drechsler", "Joerg", ""]]}, {"id": "2102.08994", "submitter": "Martin Spindler", "authors": "Barbara Felderer, Jannis Kueck, Martin Spindler", "title": "Big Data meets Causal Survey Research: Understanding Nonresponse in the\n  Recruitment of a Mixed-mode Online Panel", "comments": "33 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Survey scientists increasingly face the problem of high-dimensionality in\ntheir research as digitization makes it much easier to construct\nhigh-dimensional (or \"big\") data sets through tools such as online surveys and\nmobile applications. Machine learning methods are able to handle such data, and\nthey have been successfully applied to solve \\emph{predictive} problems.\nHowever, in many situations, survey statisticians want to learn about\n\\emph{causal} relationships to draw conclusions and be able to transfer the\nfindings of one survey to another. Standard machine learning methods provide\nbiased estimates of such relationships. We introduce into survey statistics the\ndouble machine learning approach, which gives approximately unbiased estimators\nof causal parameters, and show how it can be used to analyze survey nonresponse\nin a high-dimensional panel setting.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 19:37:53 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Felderer", "Barbara", ""], ["Kueck", "Jannis", ""], ["Spindler", "Martin", ""]]}, {"id": "2102.10429", "submitter": "Yifan Yang", "authors": "Yifan Yang and Xiaoyu Zhou", "title": "A Note on Taylor's Expansion and Mean Value Theorem With Respect to a\n  Random Variable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a stochastic version of Taylor's expansion and Mean Value\nTheorem, originally proved by Aliprantis and Border (1999), and extend them to\na multivariate case. For a univariate case, the theorem asserts that \"suppose a\nreal-valued function $f$ has a continuous derivative $f'$ on a closed interval\n$I$ and $X$ is a random variable on a probability space $(\\Omega, \\mathcal{F},\nP)$. Fix $a \\in I$, there exists a \\textit{random variable} $\\xi$ such that\n$\\xi(\\omega) \\in I$ for every $\\omega \\in \\Omega$ and $f(X(\\omega)) = f(a) +\nf'(\\xi(\\omega))(X(\\omega) - a)$.\" The proof is not trivial. By applying these\nresults in statistics, one may simplify some details in the proofs of the Delta\nmethod or the asymptotic properties for a maximum likelihood estimator. In\nparticular, when mentioning \"there exists $\\theta ^ *$ between $\\hat{\\theta}$\n(a maximum likelihood estimator) and $\\theta_0$ (the true value)\", a stochastic\nversion of Mean Value Theorem guarantees $\\theta ^ *$ is a random variable (or\na random vector).\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 20:02:30 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Yang", "Yifan", ""], ["Zhou", "Xiaoyu", ""]]}, {"id": "2102.10909", "submitter": "Semyon Malamud", "authors": "Semyon Malamud, Anna Cieslak, and Andreas Schrimpf", "title": "Optimal Transport of Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN math.OC q-fin.EC stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the general problem of Bayesian persuasion (optimal information\ndesign) with continuous actions and continuous state space in arbitrary\ndimensions. First, we show that with a finite signal space, the optimal\ninformation design is always given by a partition. Second, we take the limit of\nan infinite signal space and characterize the solution in terms of a\nMonge-Kantorovich optimal transport problem with an endogenous information\ntransport cost. We use our novel approach to: 1. Derive necessary and\nsufficient conditions for optimality based on Bregman divergences for\nnon-convex functions. 2. Compute exact bounds for the Hausdorff dimension of\nthe support of an optimal policy. 3. Derive a non-linear, second-order partial\ndifferential equation whose solutions correspond to regular optimal policies.\nWe illustrate the power of our approach by providing explicit solutions to\nseveral non-linear, multidimensional Bayesian persuasion problems.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 11:28:36 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2021 07:52:10 GMT"}, {"version": "v3", "created": "Mon, 8 Mar 2021 08:41:59 GMT"}, {"version": "v4", "created": "Tue, 9 Mar 2021 13:43:29 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Malamud", "Semyon", ""], ["Cieslak", "Anna", ""], ["Schrimpf", "Andreas", ""]]}, {"id": "2102.12225", "submitter": "Shunichiro Orihara", "authors": "Shunichiro Orihara", "title": "Valid Instrumental Variables Selection Methods using Auxiliary Variable\n  and Constructing Efficient Estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In observational studies, we are usually interested in estimating causal\neffects between treatments and outcomes. When some covariates are not observed,\nan unbiased estimator usually cannot be obtained. In this paper, we focus on\ninstrumental variable (IV) methods. By using IVs, an unbiased estimator for\ncausal effects can be estimated even if there exists some unmeasured\ncovariates. Constructing a linear combination of IVs solves weak IV problems,\nhowever, there are risks estimating biased causal effects by including some\ninvalid IVs. In this paper, we use Negative Control Outcomes as auxiliary\nvariables to select valid IVs. By using NCOs, there are no necessity to specify\nnot only the set of valid IVs but also invalid one in advance: this point is\ndifferent from previous methods. We prove that the estimated causal effects has\nthe same asymptotic variance as the estimator using Generalized Method of\nMoments that has the semiparametric efficiency. Also, we confirm properties of\nour method and previous methods through simulations.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 11:34:25 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Orihara", "Shunichiro", ""]]}, {"id": "2102.12290", "submitter": "Anass El Yaagoubi Bourakna", "authors": "Anass El Yaagoubi Bourakna, Marco Pinto, Norbert Fortin, Hernando\n  Ombao", "title": "Smooth Online Parameter Estimation for time varying VAR models with\n  application to rat's LFP data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate time series data appear often as realizations of non-stationary\nprocesses where the covariance matrix or spectral matrix smoothly evolve over\ntime. Most of the current approaches estimate the time-varying spectral\nproperties only retrospectively - that is, after the entire data has been\nobserved. Retrospective estimation is a major limitation in many adaptive\ncontrol applications where it is important to estimate these properties and\ndetect changes in the system as they happen in real-time. One major obstacle in\nonline estimation is the computational cost due to the high-dimensionality of\nthe parameters. Existing methods such as the Kalman filter or local least\nsquares are feasible. However, they are not always suitable because they\nprovide noisy estimates and can become prohibitively costly as the dimension of\nthe time series increases. In our brain signal application, it is critical to\ndevelop a robust method that can estimate, in real-time, the properties of the\nunderlying stochastic process, in particular, the spectral brain connectivity\nmeasures. For these reasons we propose a new smooth online parameter estimation\napproach (SOPE) that has the ability to control for the smoothness of the\nestimates with a reasonable computational complexity. Consequently, the models\nare fit in real-time even for high dimensional time series. We demonstrate that\nour proposed SOPE approach is as good as the Kalman filter in terms of\nmean-squared error for small dimensions. However, unlike the Kalman filter, the\nSOPE has lower computational cost and hence scalable for higher dimensions.\nFinally, we apply the SOPE method to a rat's local field potential data during\na hippocampus-dependent sequence-memory task. As demonstrated in the video, the\nproposed SOPE method is able to capture the dynamics of the connectivity as the\nrat performs the sequence of non-spatial working memory tasks.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 13:55:30 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 16:52:12 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Bourakna", "Anass El Yaagoubi", ""], ["Pinto", "Marco", ""], ["Fortin", "Norbert", ""], ["Ombao", "Hernando", ""]]}, {"id": "2102.12328", "submitter": "Giles Hooker", "authors": "Lucas Mentch and Giles Hooker", "title": "Bridging Breiman's Brook: From Algorithmic Modeling to Statistical\n  Learning", "comments": "In response to the Journal of Observational Studies reprinting Leo\n  Breiman's paper \"Statistical Modeling: The Two Cultures\" on its 20th\n  anniversary", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In 2001, Leo Breiman wrote of a divide between \"data modeling\" and\n\"algorithmic modeling\" cultures. Twenty years later this division feels far\nmore ephemeral, both in terms of assigning individuals to camps, and in terms\nof intellectual boundaries. We argue that this is largely due to the \"data\nmodelers\" incorporating algorithmic methods into their toolbox, particularly\ndriven by recent developments in the statistical understanding of Breiman's own\nRandom Forest methods. While this can be simplistically described as \"Breiman\nwon\", these same developments also expose the limitations of the\nprediction-first philosophy that he espoused, making careful statistical\nanalysis all the more important. This paper outlines these exciting recent\ndevelopments in the random forest literature which, in our view, occurred as a\nresult of a necessary blending of the two ways of thinking Breiman originally\ndescribed. We also ask what areas statistics and statisticians might currently\noverlook.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 03:38:41 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Mentch", "Lucas", ""], ["Hooker", "Giles", ""]]}, {"id": "2102.12600", "submitter": "Aref Darzi", "authors": "Aref Darzi, Vanessa Frias-Martinez, Sepehr Ghader, Hannah Younes, Lei\n  Zhang", "title": "Constructing Evacuation Evolution Patterns and Decisions Using Mobile\n  Device Location Data: A Case Study of Hurricane Irma", "comments": "22 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding individuals' behavior during hurricane evacuation is of\nparamount importance for local, state, and government agencies hoping to be\nprepared for natural disasters. Complexities involved with human\ndecision-making procedures and lack of data for such disasters are the main\nreasons that make hurricane evacuation studies challenging. In this paper, we\nutilized a large mobile phone Location-Based Services (LBS) data to construct\nthe evacuation pattern during the landfall of Hurricane Irma. By employing our\nproposed framework on more than 11 billion mobile phone location sightings, we\nwere able to capture the evacuation decision of 807,623 smartphone users who\nwere living within the state of Florida. We studied users' evacuation\ndecisions, departure and reentry date distribution, and destination choice. In\naddition to these decisions, we empirically examined the influence of\nevacuation order and low-lying residential areas on individuals' evacuation\ndecisions. Our analysis revealed that 57.92% of people living in mandatory\nevacuation zones evacuated their residences while this ratio was 32.98% and\n33.68% for people living in areas with no evacuation order and voluntary\nevacuation order, respectively. Moreover, our analysis revealed the importance\nof the individuals' mobility behavior in modeling the evacuation decision\nchoice. Historical mobility behavior information such as number of trips taken\nby each individual and the spatial area covered by individuals' location\ntrajectory estimated significant in our choice model and improve the overall\naccuracy of the model significantly.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 23:24:10 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Darzi", "Aref", ""], ["Frias-Martinez", "Vanessa", ""], ["Ghader", "Sepehr", ""], ["Younes", "Hannah", ""], ["Zhang", "Lei", ""]]}]