[{"id": "1910.00006", "submitter": "Behnaz Pirzamanbein PhD", "authors": "Behnaz Pirzamanbein", "title": "Spatial methods and their applications to environmental and climate data", "comments": "Book (this report/review of methodology and applications was written\n  as an introductory paper for PhD study in Lund University)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Environmental and climate processes are often distributed over large\nspace-time domains. Their complexity and the amount of available data make\nmodelling and analysis a challenging task. Statistical modelling of environment\nand climate data can have several different motivations including\ninterpretation or characterisation of the data. Results from statistical\nanalysis are often used as a integral part of larger environmental studies.\nSpatial statistics is an active and modern statistical field, concerned with\nthe quantitative analysis of spatial data; their dependencies and\nuncertainties. Spatio-temporal statistics extends spatial statistics through\nthe addition of time to the, two or three, spatial dimensions. The focus of\nthis introductory paper is to provide an overview of spatial methods and their\napplication to environmental and climate data. This paper also gives an\noverview of several important topics including large data sets and\nnon-stationary covariance structures. Further, it is discussed how Bayesian\nhierarchical models can provide a flexible way of constructing models.\nHierarchical models may seem to be a good solution, but they have challenges of\ntheir own such as, parameter estimation. Finally, the application of\nspatio-temporal models to the LANDCLIM data (LAND cover - CLIMate interactions\nin NW Europe during the Holocene) will be discussed.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 11:31:11 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Pirzamanbein", "Behnaz", ""]]}, {"id": "1910.00256", "submitter": "Tom Palmer", "authors": "Elinor Jones and Tom Palmer", "title": "A review of problem- and team-based methods for teaching statistics in\n  Higher Education", "comments": "Jones E and Palmer T. A review of problem- and team-based methods for\n  teaching statistics in Higher Education. Teaching Mathematics and its\n  Applications: An International Journal of the IMA. Published online\n  09-03-2021", "journal-ref": null, "doi": "10.1093/teamat/hrab002", "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The teaching of statistics in higher education in the UK is still largely\nlecture-based. This is despite recommendations such as those given by the\nAmerican Statistical Association's GAISE report that more emphasis should be\nplaced on active learning strategies where students take more responsibility\nfor their own learning. One possible model is that of collaborative learning,\nwhere students learn in groups through carefully crafted `problems', which has\nlong been suggested as a strategy for teaching statistics.\n  In this article, we review two specific approaches that fall under the\ncollaborative learning model: problem- and team-based learning. We consider the\nevidence for changing to this model of teaching in statistics, as well as give\npractical suggestions on how this could be implemented in typical statistics\nclasses in Higher Education.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 08:41:18 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 12:38:06 GMT"}, {"version": "v3", "created": "Wed, 10 Mar 2021 10:30:46 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Jones", "Elinor", ""], ["Palmer", "Tom", ""]]}, {"id": "1910.02042", "submitter": "Michael J Lew", "authors": "Michael J. Lew", "title": "A reckless guide to P-values: local evidence, global errors", "comments": "Chapter 13 in the book Good Research Practice in Experimental\n  Pharmacology, editors A. Bespalov, MC Michel, and T Steckler, to be published\n  by Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This chapter demystifies P-values, hypothesis tests and significance tests,\nand introduces the concepts of local evidence and global error rates. The local\nevidence is embodied in \\textit{this} data and concerns the hypotheses of\ninterest for \\textit{this} experiment, whereas the global error rate is a\nproperty of the statistical analysis and sampling procedure. It is shown using\nsimple examples that local evidence and global error rates can be, and should\nbe, considered together when making inferences. Power analysis for experimental\ndesign for hypothesis testing are explained, along with the more locally\nfocussed expected P-values. Issues relating to multiple testing, HARKing, and\nP-hacking are explained, and it is shown that, in many situation, their effects\non local evidence and global error rates are in conflict, a conflict that can\nalways be overcome by a fresh dataset from replication of key experiments.\nStatistics is complicated, and so is science. There is no singular right way to\ndo either, and universally acceptable compromises may not exist. Statistics\noffers a wide array of tools for assisting with scientific inference by\ncalibrating uncertainty, but statistical inference is not a substitute for\nscientific inference. P-values are useful indices of evidence and deserve their\nplace in the statistical toolbox of basic pharmacologists.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 01:20:41 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Lew", "Michael J.", ""]]}, {"id": "1910.02381", "submitter": "Kurt Riedel", "authors": "P. Yushmanov, T. Takizuka, K. Riedel, O. Kardaun, J. Cordey, S. Kaye,\n  D. Post", "title": "Scalings for Tokamak Energy Confinement", "comments": null, "journal-ref": "Nucl. Fusion 1990", "doi": null, "report-no": null, "categories": "physics.plasm-ph cs.SY eess.SY stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On the basis of an analysis of the ITER L-mode energy confinement database,\ntwo new scaling expressions for tokamak L-mode energy confinement are proposed,\nnamely a power law scaling and an offset-linear scaling. The analysis indicates\nthat the present multiplicity of scaling expressions for the energy confinement\ntime TE in tokamaks (Goldston, Kaye, Odajima-Shimomura, Rebut-Lallia, etc.) is\ndue both to the lack of variation of a key parameter combination in the\ndatabase, fs = 0.32 R a^.75 k^ 5 ~ A a^.25 k^.5, and to variations in the\ndependence of rE on the physical parameters among the different tokamaks in the\ndatabase. By combining multiples of fs and another factor, fq = 1.56 a^2 kB/R\nIp = qeng/3.2, which partially reflects the tokamak to tokamak variation of the\ndependence of TE on q and therefore implicitly the dependence of TE on Ip and\nn,., the two proposed confinement scaling expressions can be transformed to\nforms very close to most of the common scaling expressions. To reduce the\nmultiplicity of the scalings for energy confinement, the database must be\nimproved by adding new data with significant variations in fs, and the physical\nreasons for the tokamak to tokamak variation of some of the dependences of the\nenergy confinement time on tokamak parameters must be clarified\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 05:39:57 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Yushmanov", "P.", ""], ["Takizuka", "T.", ""], ["Riedel", "K.", ""], ["Kardaun", "O.", ""], ["Cordey", "J.", ""], ["Kaye", "S.", ""], ["Post", "D.", ""]]}, {"id": "1910.03368", "submitter": "Natalia R. Kunst", "authors": "Natalia R. Kunst, Edward Wilson, Fernando Alarid-Escudero, Gianluca\n  Baio, Alan Brennan, Michael Fairley, David Glynn, Jeremy D.\n  Goldhaber-Fiebert, Chris Jackson, Hawre Jalal, Nicolas A. Menzies, Mark\n  Strong, Howard Thom, Anna Heath (on behalf of the Collaborative Network for\n  Value of Information (ConVOI))", "title": "Computing the Expected Value of Sample Information Efficiently:\n  Expertise and Skills Required for Four Model-Based Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objectives: Value of information (VOI) analyses can help policy-makers make\ninformed decisions about whether to conduct and how to design future studies.\nHistorically, a computationally expensive method to compute the Expected Value\nof Sample Information (EVSI) restricted the use of VOI to simple decision\nmodels and study designs. Recently, four EVSI approximation methods have made\nsuch analyses more feasible and accessible. We provide practical\nrecommendations for analysts computing EVSI by evaluating these novel methods.\nMethods: Members of the Collaborative Network for Value of Information (ConVOI)\ncompared the inputs, analyst's expertise and skills, and software required for\nfour recently developed approximation methods. Information was also collected\non the strengths and limitations of each approximation method. Results: All\nfour EVSI methods require a decision-analytic model's probabilistic sensitivity\nanalysis (PSA) output. One of the methods also requires the model to be re-run\nto obtain new PSA outputs for each EVSI estimation. To compute EVSI, analysts\nmust be familiar with at least one of the following skills: advanced regression\nmodeling, likelihood specification, and Bayesian modeling. All methods have\ndifferent strengths and limitations, e.g., some methods handle evaluation of\nstudy designs with more outcomes more efficiently while others quantify\nuncertainty in EVSI estimates. All methods are programmed in the statistical\nlanguage R and two of the methods provide online applications. Conclusion: Our\npaper helps to inform the choice between four efficient EVSI estimation\nmethods, enabling analysts to assess the methods' strengths and limitations and\nselect the most appropriate EVSI method given their situation and skills.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 12:40:05 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Kunst", "Natalia R.", "", "on behalf of the Collaborative Network for\n  Value of Information"], ["Wilson", "Edward", "", "on behalf of the Collaborative Network for\n  Value of Information"], ["Alarid-Escudero", "Fernando", "", "on behalf of the Collaborative Network for\n  Value of Information"], ["Baio", "Gianluca", "", "on behalf of the Collaborative Network for\n  Value of Information"], ["Brennan", "Alan", "", "on behalf of the Collaborative Network for\n  Value of Information"], ["Fairley", "Michael", "", "on behalf of the Collaborative Network for\n  Value of Information"], ["Glynn", "David", "", "on behalf of the Collaborative Network for\n  Value of Information"], ["Goldhaber-Fiebert", "Jeremy D.", "", "on behalf of the Collaborative Network for\n  Value of Information"], ["Jackson", "Chris", "", "on behalf of the Collaborative Network for\n  Value of Information"], ["Jalal", "Hawre", "", "on behalf of the Collaborative Network for\n  Value of Information"], ["Menzies", "Nicolas A.", "", "on behalf of the Collaborative Network for\n  Value of Information"], ["Strong", "Mark", "", "on behalf of the Collaborative Network for\n  Value of Information"], ["Thom", "Howard", "", "on behalf of the Collaborative Network for\n  Value of Information"], ["Heath", "Anna", "", "on behalf of the Collaborative Network for\n  Value of Information"]]}, {"id": "1910.03558", "submitter": "Hamed Masnadi-Shirazi", "authors": "Hamed Masnadi-Shirazi, Alireza Masnadi-Shirazi, Mohammad-Amir\n  Dastgheib", "title": "A Step by Step Mathematical Derivation and Tutorial on Kalman Filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a step by step mathematical derivation of the Kalman filter using\ntwo different approaches. First, we consider the orthogonal projection method\nby means of vector-space optimization. Second, we derive the Kalman filter\nusing Bayesian optimal filtering. We provide detailed proofs for both methods\nand each equation is expanded in detail.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 17:37:52 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Masnadi-Shirazi", "Hamed", ""], ["Masnadi-Shirazi", "Alireza", ""], ["Dastgheib", "Mohammad-Amir", ""]]}, {"id": "1910.04118", "submitter": "Giovanni Iacobello", "authors": "Giovanni Iacobello, Massimo Marro, Luca Ridolfi, Pietro Salizzoni,\n  Stefania Scarsoglio", "title": "Experimental investigation of vertical turbulent transport of a passive\n  scalar in a boundary layer: statistics and visibility graph analysis", "comments": null, "journal-ref": "Phys. Rev. Fluids (2019) 4, 104501", "doi": "10.1103/PhysRevFluids.4.104501", "report-no": null, "categories": "physics.flu-dyn stat.OT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The dynamics of a passive scalar plume in a turbulent boundary layer is\nexperimentally investigated via vertical turbulent transport time-series. Data\nare acquired in a rough-wall turbulent boundary layer that develops in a\nrecirculating wind tunnel set-up. Two source sizes in an elevated position are\nconsidered in order to investigate the influence of the emission conditions on\nthe plume dynamics. The analysis is focused on the effects of the meandering\nmotion and the relative dispersion. First, classical statistics are\ninvestigated. We found that (in accordance with previous studies) the\nmeandering motion is the main responsible for differences in the variance and\nintermittency, as well as the kurtosis and power spectral density, between the\ntwo source sizes. On the contrary, the mean and the skewness are slightly\naffected by the emission conditions. To characterize the temporal structure of\nthe turbulent transport series, the visibility algorithm is exploited to carry\nout a complex network-based analysis. Two network metrics -- the average peak\noccurrence and the assortativity coefficient -- are analysed, as they can\ncapture the temporal occurrence of extreme events and their relative intensity\nin the series. The effects of the meandering motion and the relative dispersion\nof the plume are discussed in the view of the network metrics, revealing that a\nstronger meandering motion is associated with higher values of both the average\npeak occurrence and the assortativity coefficient. The network-based analysis\nadvances the level of information of classical statistics, by characterizing\nthe impact of the emission conditions on the temporal structure of the signals\nin terms of extreme events and their relative intensity. In this way, complex\nnetworks provide -- through the evaluation of network metrics -- an effective\ntool for time-series analysis of experimental data.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 16:58:22 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Iacobello", "Giovanni", ""], ["Marro", "Massimo", ""], ["Ridolfi", "Luca", ""], ["Salizzoni", "Pietro", ""], ["Scarsoglio", "Stefania", ""]]}, {"id": "1910.05271", "submitter": "Elena Kalinina", "authors": "Elena Kalinina, Fabian Pedregosa, Vittorio Iacovella, Emanuele\n  Olivetti, Paolo Avesani", "title": "A Test for Shared Patterns in Cross-modal Brain Activation Analysis", "comments": "5 figures, tables after References (as required by SciRep template)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG stat.ML stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining the extent to which different cognitive modalities (understood\nhere as the set of cognitive processes underlying the elaboration of a stimulus\nby the brain) rely on overlapping neural representations is a fundamental issue\nin cognitive neuroscience. In the last decade, the identification of shared\nactivity patterns has been mostly framed as a supervised learning problem. For\ninstance, a classifier is trained to discriminate categories (e.g. faces vs.\nhouses) in modality I (e.g. perception) and tested on the same categories in\nmodality II (e.g. imagery). This type of analysis is often referred to as\ncross-modal decoding. In this paper we take a different approach and instead\nformulate the problem of assessing shared patterns across modalities within the\nframework of statistical hypothesis testing. We propose both an appropriate\ntest statistic and a scheme based on permutation testing to compute the\nsignificance of this test while making only minimal distributional assumption.\nWe denote this test cross-modal permutation test (CMPT). We also provide\nempirical evidence on synthetic datasets that our approach has greater\nstatistical power than the cross-modal decoding method while maintaining low\nType I errors (rejecting a true null hypothesis). We compare both approaches on\nan fMRI dataset with three different cognitive modalities (perception, imagery,\nvisual search). Finally, we show how CMPT can be combined with Searchlight\nanalysis to explore spatial distribution of shared activity patterns.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 19:33:49 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Kalinina", "Elena", ""], ["Pedregosa", "Fabian", ""], ["Iacovella", "Vittorio", ""], ["Olivetti", "Emanuele", ""], ["Avesani", "Paolo", ""]]}, {"id": "1910.05818", "submitter": "Jingchen Hu", "authors": "Jingchen Hu", "title": "A Bayesian Statistics Course for Undergraduates: Bayesian Thinking,\n  Computing, and Research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a semester-long Bayesian statistics course for undergraduate\nstudents with calculus and probability background. We cultivate students'\nBayesian thinking with Bayesian methods applied to real data problems. We\nleverage modern Bayesian computing techniques not only for implementing\nBayesian methods, but also to deepen students' understanding of the methods.\nCollaborative case studies further enrich students' learning and provide\nexperience to solve open-ended applied problems. The course has an emphasis on\nundergraduate research, where accessible academic journal articles are read,\ndiscussed, and critiqued in class. With increased confidence and familiarity,\nstudents take the challenge of reading, implementing, and sometimes extending\nmethods in journal articles for their course projects.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 19:40:28 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 14:50:29 GMT"}, {"version": "v3", "created": "Fri, 28 Aug 2020 15:23:54 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Hu", "Jingchen", ""]]}, {"id": "1910.06964", "submitter": "Charles Gray", "authors": "Charles T. Gray", "title": "\\texttt{code::proof}: Prepare for \\emph{most} weather conditions", "comments": "This manuscript was presented by invitation at The Research School on\n  Statistics and Data Science 2019 (RSSDS2019)\n  [https://sites.google.com/view/rssds2019/home] and will be published with the\n  workshop proceedings in Springer Communications in Computer and Information\n  Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computational tools for data analysis are being released daily on\nrepositories such as the Comprehensive R Archive Network. How we integrate\nthese tools to solve a problem in research is increasingly complex and\nrequiring frequent updates. To mitigate these \\emph{Kafkaesque} computational\nchallenges in research, this manuscript proposes \\emph{toolchain walkthrough},\nan opinionated documentation of a scientific workflow. As a practical\ncomplement to our proof-based argument~(Gray and Marwick, arXiv, 2019) for\nreproducible data analysis, here we focus on the practicality of setting up a\nreproducible research compendia, with unit tests, as a measure of\n\\texttt{code::proof}, confidence in computational algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 22:13:52 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Gray", "Charles T.", ""]]}, {"id": "1910.07325", "submitter": "Florian Ziel", "authors": "Florian Ziel, Kevin Berk", "title": "Multivariate Forecasting Evaluation: On Sensitive and Strictly Proper\n  Scoring Rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.ML stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, probabilistic forecasting is an emerging topic, which is why\nthere is a growing need of suitable methods for the evaluation of multivariate\npredictions. We analyze the sensitivity of the most common scoring rules,\nespecially regarding quality of the forecasted dependency structures.\nAdditionally, we propose scoring rules based on the copula, which uniquely\ndescribes the dependency structure for every probability distribution with\ncontinuous marginal distributions. Efficient estimation of the considered\nscoring rules and evaluation methods such as the Diebold-Mariano test are\ndiscussed. In detailed simulation studies, we compare the performance of the\nrenowned scoring rules and the ones we propose. Besides extended synthetic\nstudies based on recently published results we also consider a real data\nexample. We find that the energy score, which is probably the most widely used\nmultivariate scoring rule, performs comparably well in detecting forecast\nerrors, also regarding dependencies. This contradicts other studies. The\nresults also show that a proposed copula score provides very strong distinction\nbetween models with correct and incorrect dependency structure. We close with a\ncomprehensive discussion on the proposed methodology.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 12:57:00 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Ziel", "Florian", ""], ["Berk", "Kevin", ""]]}, {"id": "1910.08880", "submitter": "Antoine Dedieu", "authors": "Antoine Dedieu", "title": "Sparse (group) learning with Lipschitz loss functions: a unified\n  analysis", "comments": "arXiv admin note: text overlap with arXiv:1810.03081", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a family of sparse estimators defined as minimizers of some\nempirical Lipschitz loss function---which include hinge, logistic and quantile\nregression losses---with a convex, sparse or group-sparse regularization. In\nparticular, we consider the L1-norm on the coefficients, its sorted Slope\nversion, and the Group L1-L2 extension. First, we propose a theoretical\nframework which simultaneously derives new L2 estimation upper bounds for all\nthree regularization schemes. For L1 and Slope regularizations, our bounds\nscale as $(k^*/n) \\log(p/k^*)$---$n\\times p$ is the size of the design matrix\nand $k^*$ the dimension of the theoretical loss minimizer $\\beta^*$---matching\nthe optimal minimax rate achieved for the least-squares case. For Group L1-L2\nregularization, our bounds scale as $(s^*/n) \\log\\left( G / s^* \\right) + m^* /\nn$---$G$ is the total number of groups and $m^*$ the number of coefficients in\nthe $s^*$ groups which contain $\\beta^*$---and improve over the least-squares\ncase. We additionally show that when the signal is strongly group-sparse Group\nL1-L2 is superior to L1 and Slope. Our bounds are achieved both in probability\nand in expectation, under common assumptions in the literature. Second, we\npropose an accelerated proximal algorithm which computes the convex estimators\nstudied when the number of variables is of the order of $100,000$. We\nadditionally compare their statistical performance of our estimators against\nstandard baselines for settings where the signal is either sparse or\ngroup-sparse. Our experiments findings reveal (i) the good empirical\nperformance of L1 and Slope regularizations for sparse binary classification\nproblems, (ii) the superiority of Group L1-L2 regularization for group-sparse\nclassification problems and (iii) the appealing properties of sparse quantile\nregression estimators for sparse regression problems with heteroscedastic\nnoise.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 02:56:37 GMT"}, {"version": "v2", "created": "Sat, 2 Nov 2019 04:02:41 GMT"}, {"version": "v3", "created": "Fri, 15 Nov 2019 18:54:50 GMT"}, {"version": "v4", "created": "Wed, 4 Dec 2019 19:15:25 GMT"}, {"version": "v5", "created": "Wed, 11 Dec 2019 18:04:59 GMT"}, {"version": "v6", "created": "Fri, 13 Dec 2019 23:49:00 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Dedieu", "Antoine", ""]]}, {"id": "1910.09407", "submitter": "Michael Betancourt", "authors": "Michael Betancourt", "title": "Incomplete Reparameterizations and Equivalent Metrics", "comments": "34 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT math-ph math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reparameterizing a probabilisitic system is common advice for improving the\nperformance of a statistical algorithm like Markov chain Monte Carlo, even\nthough in theory such reparameterizations should leave the system, and the\nperformance of any algorithm, invariant. In this paper I show how the\nreparameterizations common in practice are only incomplete reparameterizations\nwhich result in different interactions between a target probabilistic system\nand a given algorithm. I then consider how these changing interactions manifest\nin the context of Markov chain Monte Carlo algorithms defined on Riemannian\nmanifolds. In particular I show how any incomplete reparameterization is\nequivalent to modifying the metric geometry directly.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 02:10:51 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Betancourt", "Michael", ""]]}, {"id": "1910.11616", "submitter": "Maximilian Linde", "authors": "Maximilian Linde and Don van Ravenzwaaij", "title": "baymedr: An R Package for the Calculation of Bayes Factors for\n  Equivalence, Non-Inferiority, and Superiority Designs", "comments": "33 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical trials often seek to determine the equivalence, non-inferiority, or\nsuperiority of an experimental condition (e.g., a new drug) compared to a\ncontrol condition (e.g., a placebo or an already existing drug). The use of\nfrequentist statistical methods to analyze data for these types of designs is\nubiquitous. Importantly, however, frequentist inference has several\nlimitations. Bayesian inference remedies these shortcomings and allows for\nintuitive interpretations. In this article, we outline the frequentist\nconceptualization of equivalence, non-inferiority, and superiority designs and\ndiscuss its disadvantages. Subsequently, we explain how Bayes factors can be\nused to compare the relative plausibility of competing hypotheses. We present\nbaymedr, an R package that provides user-friendly tools for the computation of\nBayes factors for equivalence, non-inferiority, and superiority designs.\nDetailed instructions on how to use baymedr are provided and an example\nillustrates how already existing results can be reanalyzed with baymedr.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 10:55:22 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 10:34:34 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Linde", "Maximilian", ""], ["van Ravenzwaaij", "Don", ""]]}, {"id": "1910.13347", "submitter": "Laura Albert", "authors": "Laura A. Albert and John N. Angelis", "title": "Jump balls, rating falls, and elite status: A sensitivity analysis of\n  three quarterback rating statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Quarterback performance can be difficult to rank, and much effort has been\nspent in creating new rating systems. However, the input statistics for such\nratings are subject to randomness and factors outside the quarterback's\ncontrol. To investigate this variance, we perform a sensitivity analysis of\nthree quarterback rating statistics: the Traditional 1971 rating by Smith, the\nBurke, and the Wages of Wins ratings. The comparisons are made at the team\nlevel for the 32 NFL teams from 2002-2015, thus giving each case an even 16\ngames. We compute quarterback ratings for each offense with 1-5 additional\ntouchdowns, 1-5 fewer interceptions, 1-5 additional sacks, and a 1-5 percent\nincrease in the passing completion rate. Our sensitivity analysis provides\ninsight into whether an elite passing team could seem mediocre or vice versa\nbased on random outcomes. The results indicate that the Traditional rating is\nthe most sensitive statistic with respect to touchdowns, interceptions, and\ncompletions, whereas the Burke rating is most sensitive to sacks. The analysis\nsuggests that team passing offense rankings are highly sensitive to aspects of\nfootball that are out of the quarterback's hands (e.g., deflected passes that\nlead to interceptions). Thus, on the margins, we show arguments about whether a\nspecific quarterback has entered the elite or remains mediocre are irrelevant.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 18:19:45 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Albert", "Laura A.", ""], ["Angelis", "John N.", ""]]}, {"id": "1910.14018", "submitter": "Mariusz Tarnopolski", "authors": "Mariusz Tarnopolski", "title": "Analytical representation of Gaussian processes in the\n  $\\mathcal{A}-\\mathcal{T}$ plane", "comments": "14 pages, 12 figures, 1 table", "journal-ref": "Physical Review E, 100, 062144 (2019)", "doi": "10.1103/PhysRevE.100.062144", "report-no": null, "categories": "physics.data-an math.PR nlin.AO stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Closed-form expressions, parametrized by the Hurst exponent $H$ and the\nlength $n$ of a time series, are derived for paths of fractional Brownian\nmotion (fBm) and fractional Gaussian noise (fGn) in the\n$\\mathcal{A}-\\mathcal{T}$ plane, composed of the fraction of turning points\n$\\mathcal{T}$ and the Abbe value $\\mathcal{A}$. The exact formula for\n$\\mathcal{A}_{\\rm fBm}$ is expressed via Riemann $\\zeta$ and Hurwitz $\\zeta$\nfunctions. A very accurate approximation, yielding a simple exponential form,\nis obtained. Finite-size effects, introduced by the deviation of fGn's variance\nfrom unity, and asymptotic cases are discussed. Expressions for $\\mathcal{T}$\nfor fBm, fGn, and differentiated fGn are also presented. The same methodology,\nvalid for any Gaussian process, is applied to autoregressive moving average\nprocesses, for which regions of availability of the $\\mathcal{A}-\\mathcal{T}$\nplane are derived and given in analytic form. Locations in the\n$\\mathcal{A}-\\mathcal{T}$ plane of some real-world examples as well as\ngenerated data are discussed for illustration.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 17:57:39 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 10:54:44 GMT"}, {"version": "v3", "created": "Mon, 30 Dec 2019 17:55:34 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Tarnopolski", "Mariusz", ""]]}]