[{"id": "2002.00152", "submitter": "Waqas Ahmed", "authors": "Waqas Ahmed, Sheikh Muhamad Hizam, Ilham Sentosa, Habiba Akter, Eiad\n  Yafi, Jawad Ali", "title": "Predicting IoT Service Adoption towards Smart Mobility in Malaysia:\n  SEM-Neural Hybrid Pilot Study", "comments": "12 pages, 08 figures, 05 tables", "journal-ref": "International Journal of Advanced Computer Science and\n  Applications (IJACSA), Vol. 11, No. 1, 2020", "doi": "10.14569/IJACSA.2020.0110165", "report-no": null, "categories": "cs.HC stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Smart city is synchronized with digital environment and its transportation\nsystem is vitalized with RFID sensors, Internet of Things (IoT) and Artificial\nIntelligence. However, without user's behavioral assessment of technology, the\nultimate usefulness of smart mobility cannot be achieved. This paper aims to\nformulate the research framework for prediction of antecedents of smart\nmobility by using SEM-Neural hybrid approach towards preliminary data analysis.\nThis research undertook smart mobility services adoption in Malaysia as study\nperspective and applied the Technology Acceptance Model (TAM) as theoretical\nbasis. An extended TAM model was hypothesized with five external factors\n(digital dexterity, IoT service quality, intrusiveness concerns, social\nelectronic word of mouth and subjective norm). The data was collected through a\npilot survey in Klang Valley, Malaysia. Then responses were analyzed for\nreliability, validity and accuracy of model. Finally, the causal relationship\nwas explained by Structural Equation Modeling (SEM) and Artificial Neural\nNetworking (ANN). The paper will share better understanding of road technology\nacceptance to all stakeholders to refine, revise and update their policies. The\nproposed framework will suggest a broader approach to individual level\ntechnology acceptance.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2020 06:34:58 GMT"}, {"version": "v2", "created": "Sat, 14 Mar 2020 06:11:27 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Ahmed", "Waqas", ""], ["Hizam", "Sheikh Muhamad", ""], ["Sentosa", "Ilham", ""], ["Akter", "Habiba", ""], ["Yafi", "Eiad", ""], ["Ali", "Jawad", ""]]}, {"id": "2002.02832", "submitter": "Rakesh Sarma", "authors": "Rakesh Sarma, Mandar Chandorkar, Irina Zhelavskaya, Yuri Shprits,\n  Alexander Drozdov, Enrico Camporeale", "title": "Bayesian inference of quasi-linear radial diffusion parameters using Van\n  Allen Probes", "comments": null, "journal-ref": null, "doi": "10.1029/2019JA027618", "report-no": null, "categories": "physics.space-ph stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Van Allen radiation belts in the magnetosphere have been extensively\nstudied using models based on radial diffusion theory, which is based on a\nquasi-linear approach with prescribed inner and outer boundary conditions. The\n1-d diffusion model requires the knowledge of a diffusion coefficient and an\nelectron loss timescale, which are typically parameterized in terms of various\nquantities such as the spatial ($L$) coordinate or a geomagnetic index (for\nexample, $Kp$). These terms are empirically derived, not directly measurable,\nand hence are not known precisely, due to the inherent non-linearity of the\nprocess and the variable boundary conditions. In this work, we demonstrate a\nprobabilistic approach by inferring the values of the diffusion and loss term\nparameters, along with their uncertainty, in a Bayesian framework, where\nidentification is obtained using the Van Allen Probe measurements. Our results\nshow that the probabilistic approach statistically improves the performance of\nthe model, compared to the parameterization employed in the literature.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 15:07:53 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Sarma", "Rakesh", ""], ["Chandorkar", "Mandar", ""], ["Zhelavskaya", "Irina", ""], ["Shprits", "Yuri", ""], ["Drozdov", "Alexander", ""], ["Camporeale", "Enrico", ""]]}, {"id": "2002.04384", "submitter": "Yosep Dwi Kristanto", "authors": "Geovani Debby Setyani and Yosep Dwi Kristanto", "title": "A Case Study of Promoting Informal Inferential Reasoning in Learning\n  Sampling Distribution for High School Students", "comments": "14 pages, 2 figures, 26 references", "journal-ref": "SJME (Supremum Journal of Mathematics Education) 4 (2020) 64-77", "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Drawing inference from data is an important skill for students to understand\ntheir everyday life, so that the sampling distribution as a central topic in\nstatistical inference is necessary to be learned by the students. However,\nlittle is known about how to teach the topic for high school students,\nespecially in Indonesian context. Therefore, the present study provides a\nteaching experiment to support the students' informal inferential reasoning in\nunderstanding the sampling distribution, as well as the students' perceptions\ntoward the teaching experiment. The subjects in the present study were three\n11th-grader of one private school in Yogyakarta majoring in mathematics and\nnatural science. The method of data collection was direct observation of\nsampling distribution learning process, interviews, and documentation. The\npresent study found that that informal inferential reasoning with problem-based\nlearning using contextual problems and real data could support the students to\nunderstand the sampling distribution, and they also gave positive responses\nabout their learning experience.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 15:04:40 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Setyani", "Geovani Debby", ""], ["Kristanto", "Yosep Dwi", ""]]}, {"id": "2002.04663", "submitter": "Lijing Wang", "authors": "Lijing Wang, Jiangzhuo Chen, and Madhav Marathe", "title": "TDEFSI: Theory Guided Deep Learning Based Epidemic Forecasting with\n  Synthetic Information", "comments": "This article has been accepted by ACM TSAS journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Influenza-like illness (ILI) places a heavy social and economic burden on our\nsociety. Traditionally, ILI surveillance data is updated weekly and provided at\na spatially coarse resolution. Producing timely and reliable high-resolution\nspatiotemporal forecasts for ILI is crucial for local preparedness and optimal\ninterventions. We present TDEFSI (Theory Guided Deep Learning Based Epidemic\nForecasting with Synthetic Information), an epidemic forecasting framework that\nintegrates the strengths of deep neural networks and high-resolution\nsimulations of epidemic processes over networks. TDEFSI yields accurate\nhigh-resolution spatiotemporal forecasts using low-resolution time series data.\nDuring the training phase, TDEFSI uses high-resolution simulations of epidemics\nthat explicitly model spatial and social heterogeneity inherent in urban\nregions as one component of training data. We train a two-branch recurrent\nneural network model to take both within-season and between-season\nlow-resolution observations as features, and output high-resolution detailed\nforecasts. The resulting forecasts are not just driven by observed data but\nalso capture the intricate social, demographic and geographic attributes of\nspecific urban regions and mathematical theories of disease propagation over\nnetworks. We focus on forecasting the incidence of ILI and evaluate TDEFSI's\nperformance using synthetic and real-world testing datasets at the state and\ncounty levels in the USA. The results show that, at the state level, our method\nachieves comparable/better performance than several state-of-the-art methods.\nAt the county level, TDEFSI outperforms the other methods. The proposed method\ncan be applied to other infectious diseases as well.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 20:51:49 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Wang", "Lijing", ""], ["Chen", "Jiangzhuo", ""], ["Marathe", "Madhav", ""]]}, {"id": "2002.04916", "submitter": "Yosep Dwi Kristanto", "authors": "Yosep Dwi Kristanto", "title": "Technology-enhanced pre-instructional peer assessment: Exploring\n  students' perceptions in a Statistical Methods course", "comments": "12 pages, 1 figure, 3 tables, 51 references", "journal-ref": "REiD (Research and Evaluation in Education) 4 (2018) 105-116", "doi": "10.21831/reid.v4i2.20951", "report-no": null, "categories": "physics.ed-ph stat.OT", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  There has been strong interest among higher education institution in\nimplementing technology-enhanced peer assessment as a tool for enhancing\nstudents' learning. However, little is known on how to use the peer assessment\nsystem in pre-instructional activities. This study aims to explore how\ntechnology-enhanced peer assessment can be embedded into pre-instructional\nactivities to enhance students' learning. Therefore, the present study was an\nexplorative descriptive study that used the qualitative approach to attain the\nresearch aim. This study used a questionnaire, students' reflections, and\ninterview in collecting student's perceptions toward the interventions. The\nresults suggest that the technology-enhanced pre-instructional peer assessment\nhelps students to prepare the new content acquisition and become a source of\nstudents' motivation in improving their learning performance for the following\nmain body of the lesson. A set of practical suggestions is also proposed for\ndesigning and implementing technology-enhanced pre-instructional peer\nassessment.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 11:13:02 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Kristanto", "Yosep Dwi", ""]]}, {"id": "2002.05222", "submitter": "Hong-Li Zeng", "authors": "Hong-Li Zeng and Erik Aurell", "title": "Inverse Ising techniques to infer underlying mechanisms from data", "comments": "41 pages, 11 figures, to be submitted to Chinese Physics B", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT physics.app-ph physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a problem in data science the inverse Ising (or Potts) problem is to infer\nthe parameters of a Gibbs-Boltzmann distributions of an Ising (or Potts) model\nfrom samples drawn from that distribution. The algorithmic and computational\ninterest stems from the fact that this inference task cannot be done\nefficiently by the maximum likelihood criterion, since the normalizing constant\nof the distribution (the partition function) can not be calculated exactly and\nefficiently. The practical interest on the other hand flows from several\noutstanding applications, of which the most well known has been predicting\nspatial contacts in protein structures from tables of homologous protein\nsequences. Most applications to date have been to data that has been produced\nby a dynamical process which, as far as it is known, cannot be expected to\nsatisfy detailed balance. There is therefore no a priori reason to expect the\ndistribution to be of the Gibbs-Boltzmann type, and no a priori reason to\nexpect that inverse Ising (or Potts) techniques should yield useful\ninformation. In this review we discuss two types of problems where progress\nnevertheless can be made. We find that depending on model parameters there are\nphases where, in fact, the distribution is close to Gibbs-Boltzmann\ndistribution, a non-equilibrium nature of the under-lying dynamics\nnotwithstanding. We also discuss the relation between inferred Ising model\nparameters and parameters of the underlying dynamics.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 14:17:37 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Zeng", "Hong-Li", ""], ["Aurell", "Erik", ""]]}, {"id": "2002.07671", "submitter": "Jouni Helske", "authors": "Jouni Helske, Satu Helske, Matthew Cooper, Anders Ynnerman, and Lonni\n  Besan\\c{c}on", "title": "Can visualization alleviate dichotomous thinking? Effects of visual\n  representations on the cliff effect", "comments": null, "journal-ref": "IEEE Transactions on Visualization and Computer Graphics. 2021;\n  27(8)", "doi": "10.1109/TVCG.2021.3073466", "report-no": null, "categories": "stat.OT cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Common reporting styles for statistical results in scientific articles, such\nas p-values and confidence intervals (CI), have been reported to be prone to\ndichotomous interpretations, especially with respect to the null hypothesis\nsignificance testing framework. For example when the p-value is small enough or\nthe CIs of the mean effects of a studied drug and a placebo are not\noverlapping, scientists tend to claim significant differences while often\ndisregarding the magnitudes and absolute differences in the effect sizes. This\ntype of reasoning has been shown to be potentially harmful to science.\nTechniques relying on the visual estimation of the strength of evidence have\nbeen recommended to reduce such dichotomous interpretations but their\neffectiveness has also been challenged. We ran two experiments on researchers\nwith expertise in statistical analysis to compare several alternative\nrepresentations of confidence intervals and used Bayesian multilevel models to\nestimate the effects of the representation styles on differences in\nresearchers' subjective confidence in the results. We also asked the\nrespondents' opinions and preferences in representation styles. Our results\nsuggest that adding visual information to classic CI representation can\ndecrease the tendency towards dichotomous interpretations - measured as the\n`cliff effect': the sudden drop in confidence around p-value 0.05 - compared\nwith classic CI visualization and textual representation of the CI with\np-values. All data and analyses are publicly available at\nhttps://github.com/helske/statvis.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 17:21:35 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 07:46:04 GMT"}, {"version": "v3", "created": "Tue, 13 Apr 2021 12:15:10 GMT"}, {"version": "v4", "created": "Fri, 28 May 2021 06:26:22 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Helske", "Jouni", ""], ["Helske", "Satu", ""], ["Cooper", "Matthew", ""], ["Ynnerman", "Anders", ""], ["Besan\u00e7on", "Lonni", ""]]}, {"id": "2002.07966", "submitter": "Russell Bowater", "authors": "Russell J. Bowater", "title": "Integrated organic inference (IOI): A reconciliation of statistical\n  paradigms", "comments": "Final version with corrections. arXiv admin note: text overlap with\n  arXiv:1901.08589", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is recognised that the Bayesian approach to inference can not adequately\ncope with all the types of pre-data beliefs about population quantities of\ninterest that are commonly held in practice. In particular, it generally\nencounters difficulty when there is a lack of such beliefs over some or all the\nparameters of a model, or within certain partitions of the parameter space\nconcerned. To address this issue, a fairly comprehensive theory of inference is\nput forward called integrated organic inference that is based on a fusion of\nFisherian and Bayesian reasoning. Depending on the pre-data knowledge that is\nheld about any given model parameter, inferences are made about the parameter\nconditional on all other parameters using one of three methods of inference,\nnamely organic fiducial inference, bispatial inference and Bayesian inference.\nThe full conditional post-data densities that result from doing this are then\ncombined using a framework that allows a joint post-data density for all the\nparameters to be sensibly formed without requiring these full conditional\ndensities to be compatible. Various examples of the application of this theory\nare presented. Finally, the theory is defended against possible criticisms\npartially in terms of what was previously defined as generalised subjective\nprobability.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 02:37:29 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 16:48:26 GMT"}, {"version": "v3", "created": "Thu, 15 Apr 2021 16:50:25 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Bowater", "Russell J.", ""]]}, {"id": "2002.08465", "submitter": "Georgios Giasemidis Dr", "authors": "Georgios Giasemidis", "title": "Descriptive and Predictive Analysis of Euroleague Basketball Games and\n  the Wisdom of Basketball Crowds", "comments": "24 pages, several figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study we focus on the prediction of basketball games in the\nEuroleague competition using machine learning modelling. The prediction is a\nbinary classification problem, predicting whether a match finishes 1 (home win)\nor 2 (away win). Data is collected from the Euroleague's official website for\nthe seasons 2016-2017, 2017-2018 and 2018-2019, i.e. in the new format era.\nFeatures are extracted from matches' data and off-the-shelf supervised machine\nlearning techniques are applied. We calibrate and validate our models. We find\nthat simple machine learning models give accuracy not greater than 67% on the\ntest set, worse than some sophisticated benchmark models. Additionally, the\nimportance of this study lies in the \"wisdom of the basketball crowd\" and we\ndemonstrate how the predicting power of a collective group of basketball\nenthusiasts can outperform machine learning models discussed in this study. We\nargue why the accuracy level of this group of \"experts\" should be set as the\nbenchmark for future studies in the prediction of (European) basketball games\nusing machine learning.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 22:04:29 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Giasemidis", "Georgios", ""]]}, {"id": "2002.09700", "submitter": "Jim Albert", "authors": "Jim Albert, Mine Cetinkaya-Rundel and Jingchen Hu", "title": "Online Statistics Teaching and Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For statistics courses at all levels, teaching and learning online poses\nchallenges in different aspects. Particular online challenges include how to\neffectively and interactively conduct exploratory data analyses, how to\nincorporate statistical programming, how to include individual or team\nprojects, and how to present mathematical derivations efficiently and\neffectively.\n  This article draws from the authors' experience with seven different online\nstatistics courses to address some of the aforementioned challenges. One course\nis an online exploratory data analysis course taught at Bowling Green State\nUniversity. A second course is an upper level Bayesian statistics course taught\nat Vassar College and shared among 10 liberal arts colleges through a hybrid\nmodel. We alo describes a five-course MOOC specialization on Coursera, offered\nby Duke University.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 13:44:19 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Albert", "Jim", ""], ["Cetinkaya-Rundel", "Mine", ""], ["Hu", "Jingchen", ""]]}, {"id": "2002.09713", "submitter": "Robert Cousins", "authors": "Robert D. Cousins", "title": "Connections between statistical practice in elementary particle physics\n  and the severity concept as discussed in Mayo's Statistical Inference as\n  Severe Testing", "comments": "My implementation of the connection between severity and p-CL was\n  incorrect, as explained to me by Samuel C. Fletcher", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT hep-ex physics.data-an", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For many years, philosopher-of-statistics Deborah Mayo has been advocating\nthe concept of severe testing as a key part of hypothesis testing. Her recent\nbook, Statistical Inference as Severe Testing, is a comprehensive exposition of\nher arguments in the context of a historical study of many threads of\nstatistical inference, both frequentist and Bayesian. Her foundational point of\nview is called error statistics, emphasizing frequentist evaluation of the\nerrors called Type I and Type II in the Neyman-Pearson theory of frequentist\nhypothesis testing. Since the field of elementary particle physics (also known\nas high energy physics) has strong traditions in frequentist inference, one\nmight expect that something like the severity concept was independently\ndeveloped in the field. Indeed, I find that, at least operationally\n(numerically), we high-energy physicists have long interpreted data in ways\nthat map directly onto severity. Whether or not we subscribe to Mayo's\nphilosophical interpretations of severity is a more complicated story that I do\nnot address here.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 14:39:28 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 20:44:51 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Cousins", "Robert D.", ""]]}, {"id": "2002.11610", "submitter": "Bruce Hoadley", "authors": "Bruce Hoadley", "title": "Liquid Scorecards", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional credit scorecards are generalized additive models (GAMs) with\nstep functions as the component functions. The shapes of the step functions may\nbe constrained in order to satisfy the PILE (Palatability, Interpretability,\nLegal, Explain-ability) constraints. Before 2003, FICO used Linear Programming\nto find the traditional scorecard that approximately maximizes divergence\nsubject to the PILE constraints. In this paper, I introduce the Liquid\nScorecard, that allows the component functions to be, at least partially,\nsmooth curves. I use Quadratic Programming and B-Spline theory to find the\nLiquid Scorecard that exactly maximizes divergence subject to the PILE\nconstraints. FICO uses aspects of this technology to develop the famous FICO\nCredit Score.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 18:19:02 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Hoadley", "Bruce", ""]]}, {"id": "2002.11767", "submitter": "Johanna Hardin", "authors": "Albert Y. Kim and Johanna Hardin", "title": "\"Playing the whole game\": A data collection and analysis exercise with\n  Google Calendar", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We provide a computational exercise suitable for early introduction in an\nundergraduate statistics or data science course that allows students to 'play\nthe whole game' of data science: performing both data collection and data\nanalysis. While many teaching resources exist for data analysis, such resources\nare not as abundant for data collection given the inherent difficulty of the\ntask. Our proposed exercise centers around student use of Google Calendar to\ncollect data with the goal of answering the question 'How do I spend my time?'\nOn the one hand, the exercise involves answering a question with near universal\nappeal, but on the other hand, the data collection mechanism is not beyond the\nreach of a typical undergraduate student. A further benefit of the exercise is\nthat it provides an opportunity for discussions on ethical questions and\nconsiderations that data providers and data analysts face in today's age of\nlarge-scale internet-based data collection.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 05:27:26 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 19:14:16 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Kim", "Albert Y.", ""], ["Hardin", "Johanna", ""]]}, {"id": "2002.12690", "submitter": "Andrzej Odrzywolek", "authors": "Andrzej Odrzywolek", "title": "Criteria for the numerical constant recognition", "comments": "20 pages + Supplemental Material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.SC stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for recognition/approximation of functions in terms of elementary\nfunctions/operations emerges in many areas of experimental mathematics,\nnumerical analysis, computer algebra systems, model building, machine learning,\napproximation and data compression. One of the most underestimated methods is\nthe symbolic regression. In the article, reductionist approach is applied,\nreducing full problem to constant functions, i.e, pure numbers (decimal,\nfloating-point). However, existing solutions are plagued by lack of solid\ncriteria distinguishing between random formula, matching approximately or\nliterally decimal expansion and probable ''exact'' (the best) expression match\nin the sense of Occam's razor. In particular, convincing STOP criteria for\nsearch were never developed. In the article, such a criteria, working in\nstatistical sense, are provided. Recognition process can be viewed as (1)\nenumeration of all formulas in order of increasing Kolmogorov complexity K (2)\nrandom process with appropriate statistical distribution (3) compression of a\ndecimal string. All three approaches are remarkably consistent, and provide\nessentially the same limit for practical depth of search. Tested unique\nformulas count must not exceed 1/sigma, where sigma is relative numerical error\nof the target constant. Beyond that, further search is pointless, because, in\nthe view of approach (1), number of equivalent expressions within error bounds\ngrows exponentially; in view of (2), probability of random match approaches 1;\nin view of (3) compression ratio much smaller than 1.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 13:01:21 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 09:12:09 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Odrzywolek", "Andrzej", ""]]}]