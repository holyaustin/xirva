[{"id": "1505.06354", "submitter": "Elizabeth Schifano", "authors": "Elizabeth D. Schifano, Jing Wu, Chun Wang, Jun Yan and Ming-Hui Chen", "title": "Online Updating of Statistical Inference in the Big Data Setting", "comments": "Submitted to Technometrics", "journal-ref": "Technometrics 58 (2016) 393-403", "doi": "10.1080/00401706.2016.1142900", "report-no": null, "categories": "stat.CO stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present statistical methods for big data arising from online analytical\nprocessing, where large amounts of data arrive in streams and require fast\nanalysis without storage/access to the historical data. In particular, we\ndevelop iterative estimating algorithms and statistical inferences for linear\nmodels and estimating equations that update as new data arrive. These\nalgorithms are computationally efficient, minimally storage-intensive, and\nallow for possible rank deficiencies in the subset design matrices due to\nrare-event covariates. Within the linear model setting, the proposed\nonline-updating framework leads to predictive residual tests that can be used\nto assess the goodness-of-fit of the hypothesized model. We also propose a new\nonline-updating estimator under the estimating equation setting. Theoretical\nproperties of the goodness-of-fit tests and proposed estimators are examined in\ndetail. In simulation studies and real data applications, our estimator\ncompares favorably with competing approaches under the estimating equation\nsetting.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2015 17:34:15 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Schifano", "Elizabeth D.", ""], ["Wu", "Jing", ""], ["Wang", "Chun", ""], ["Yan", "Jun", ""], ["Chen", "Ming-Hui", ""]]}, {"id": "1505.07087", "submitter": "Jens Braband", "authors": "Jens Braband and Hendrik Sch\\\"abe", "title": "Propagation of Uncertainty in Risk Analysis and Safety Integrity Level\n  Composition", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many risk analyses the results are only given as mean values and often the\ninput data are also mean values. However the required accuracy of the result is\noften an interval of values e. g. for the derivation of a Safety Integrity\nLevel (SIL). In this paper we reason what should be the accuracy of the input\ndata of risk analyses if a particular certainty of the result is demanded. Also\nthe backside of the coin, the SIL composition is discussed. The results show\nthat common methods for risk analysis are faulty and that SIL allocation by a\nkind of SIL calculus seems infeasible without additional requirements on the\ncomposed components. A justification of a common practice for parameter scaling\nin well-constructed semi-quantitative risk analysis is also provided.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 19:39:38 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Braband", "Jens", ""], ["Sch\u00e4be", "Hendrik", ""]]}]