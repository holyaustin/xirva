[{"id": "1812.03940", "submitter": "Jarrod Olson", "authors": "Jarrod Olson, Amir Rahimi, Po Hsu Allen Chen, J. Elizabeth Jackson,\n  Tyler Coy, Adrienne Cocci, Nancy McMillan and Jeff Geppert", "title": "Rapid Prototyping Model for Healthcare Alternative Payment Models:\n  Replicating the Federally Qualified Health Center Advanced Primary Care\n  Practice Demonstration", "comments": "Working paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Innovation in healthcare payment and service delivery utilizes high cost,\nhigh risk pilots paired with traditional program evaluations. Decision-makers\nare unable to reliably forecast the impacts of pilot interventions in this\ncomplex system, complicating the feasibility assessment of proposed healthcare\nmodels. We developed and validated a Discrete Event Simulation (DES) model of\nprimary care for patients with Diabetes to allow rapid prototyping and\nassessment of models before pilot implementation. We replicated four outcomes\nfrom the Centers for Medicare and Medicaid Services Federally Qualified Health\nCenter Advanced Primary Care Practice pilot. The DES model simulates a\nsynthetic population's healthcare experience, including symptom onset,\nappointment scheduling, screening, and treatment, as well as the impact of\nphysician training. A network of detailed event modules was developed from\npeer-reviewed literature. Synthetic patients' attributes modify the probability\ndistributions for event outputs and direct them through an episode of care;\nattributes are in turn modified by patients' experiences. Our model replicates\nthe direction of the effect of physician training on the selected outcomes, and\nthe strength of the effect increases with the number of trainings. The\nsimulated effect strength replicates the pilot results for eye exams and\nnephropathy screening, but over-estimates results for HbA1c and LDL screening.\nOur model will improve decision-makers' abilities to assess the feasibility of\npilot success, with reproducible, literature-based systems models. Our model\nidentifies intervention and healthcare system components to which outcomes are\nsensitive, so these aspects can be monitored and controlled during pilot\nimplementation. More work is needed to improve replication of HbA1c and LDL\nscreening, and to elaborate sub-models related to intervention components.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 17:48:26 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Olson", "Jarrod", ""], ["Rahimi", "Amir", ""], ["Chen", "Po Hsu Allen", ""], ["Jackson", "J. Elizabeth", ""], ["Coy", "Tyler", ""], ["Cocci", "Adrienne", ""], ["McMillan", "Nancy", ""], ["Geppert", "Jeff", ""]]}, {"id": "1812.06491", "submitter": "Mikael Vejdemo-Johansson", "authors": "Mikael Vejdemo-Johansson and Sayan Mukherjee", "title": "Multiple testing with persistent homology", "comments": "18 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG math.AT stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a computationally efficient multiple hypothesis\ntesting procedure for persistence homology. Most current approaches to\nhypothesis testing using persistence homology is based on resampling or\npermutation procedures, this is because there exists little distribution theory\nthat provides a natural null model for persistence diagrams. In this paper we\npropose a null model based based approach to testing for acyclicity, coupled\nwith a Family-Wise Error Rate (FWER) control method that does not suffer from\nthe computational costs associated to resampling and permutation based\napproaches. We adapt standard False Discovery Rate (FDR) control procedures to\nthe topological setting, and use our null model for hypothesis testing using\npersistent homology. A key idea in our paper is that one can compute an\nempirical distribution under a null hypothesis model that can be used for\nhypothesis testing in the same way that a normal or t-distribution is used in\nclassical statistics. A argument for the use of this empirical null is based on\nsimulations and limit theorems for persistent homology for point processes.\n", "versions": [{"version": "v1", "created": "Sun, 16 Dec 2018 15:54:13 GMT"}, {"version": "v2", "created": "Thu, 7 Nov 2019 01:15:22 GMT"}, {"version": "v3", "created": "Sat, 19 Dec 2020 20:50:56 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Vejdemo-Johansson", "Mikael", ""], ["Mukherjee", "Sayan", ""]]}, {"id": "1812.07153", "submitter": "Abbas Zaidi", "authors": "Abbas Zaidi, Sayan Mukherjee", "title": "Gaussian Process Mixtures for Estimating Heterogeneous Treatment Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a Gaussian-process mixture model for heterogeneous treatment\neffect estimation that leverages the use of transformed outcomes. The approach\nwe will present attempts to improve point estimation and uncertainty\nquantification relative to past work that has used transformed variable related\nmethods as well as traditional outcome modeling. Earlier work on modeling\ntreatment effect heterogeneity using transformed outcomes has relied on tree\nbased methods such as single regression trees and random forests. Under the\numbrella of non-parametric models, outcome modeling has been performed using\nBayesian additive regression trees and various flavors of weighted single\ntrees. These approaches work well when large samples are available, but suffer\nin smaller samples where results are more sensitive to model misspecification -\nour method attempts to garner improvements in inference quality via a correctly\nspecified model rooted in Bayesian non-parametrics. Furthermore, while we begin\nwith a model that assumes that the treatment assignment mechanism is known, an\nextension where it is learnt from the data is presented for applications to\nobservational studies. Our approach is applied to simulated and real data to\ndemonstrate our theorized improvements in inference with respect to two causal\nestimands: the conditional average treatment effect and the average treatment\neffect. By leveraging our correctly specified model, we are able to more\naccurately estimate the treatment effects while reducing their variance.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 03:43:41 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Zaidi", "Abbas", ""], ["Mukherjee", "Sayan", ""]]}, {"id": "1812.07271", "submitter": "Luca Rossini", "authors": "Fabrizio Leisen, Rams\\'es H. Mena, Freddy Palma Mancilla, Luca Rossini", "title": "On a flexible construction of a negative binomial model", "comments": "Forthcoming in \"Statistics & Probability Letters\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a construction of stationary Markov models with\nnegative-binomial marginal distributions. A simple closed form expression for\nthe corresponding transition probabilities is given, linking the proposal to\nwell-known classes of birth and death processes and thus revealing interesting\ncharacterizations. The advantage of having such closed form expressions is\ntested on simulated and real data.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 10:12:35 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 05:58:56 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Leisen", "Fabrizio", ""], ["Mena", "Rams\u00e9s H.", ""], ["Mancilla", "Freddy Palma", ""], ["Rossini", "Luca", ""]]}, {"id": "1812.09998", "submitter": "Rafael Stern", "authors": "Luis G. Esteves, Rafael Izbicki, Rafael B. Stern, Julio M. Stern", "title": "Pragmatic hypotheses in the evolution of science", "comments": null, "journal-ref": null, "doi": "10.3390/e21090883", "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces pragmatic hypotheses and relates this concept to the\nspiral of scientific evolution. Previous works determined a characterization of\nlogically consistent statistical hypothesis tests and showed that the modal\noperators obtained from this test can be represented in the hexagon of\noppositions. However, despite the importance of precise hypothesis in science,\nthey cannot be accepted by logically consistent tests. Here, we show that this\ndilemma can be overcome by the use of pragmatic versions of precise hypotheses.\nThese pragmatic versions allow a level of imprecision in the hypothesis that is\nsmall relative to other experimental conditions. The introduction of pragmatic\nhypotheses allows the evolution of scientific theories based on statistical\nhypothesis testing to be interpreted using the narratological structure of\nhexagonal spirals, as defined by Pierre Gallais.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2018 00:19:46 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Esteves", "Luis G.", ""], ["Izbicki", "Rafael", ""], ["Stern", "Rafael B.", ""], ["Stern", "Julio M.", ""]]}, {"id": "1812.10551", "submitter": "Shiqing Yu", "authors": "Shiqing Yu, Mathias Drton, Ali Shojaie", "title": "Generalized Score Matching for Non-Negative Data", "comments": "70 pages, 76 figures", "journal-ref": "Journal of Machine Learning Research, 20(76):1-70, 2019", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common challenge in estimating parameters of probability density functions\nis the intractability of the normalizing constant. While in such cases maximum\nlikelihood estimation may be implemented using numerical integration, the\napproach becomes computationally intensive. The score matching method of\nHyv\\\"arinen [2005] avoids direct calculation of the normalizing constant and\nyields closed-form estimates for exponential families of continuous\ndistributions over $\\mathbb{R}^m$. Hyv\\\"arinen [2007] extended the approach to\ndistributions supported on the non-negative orthant, $\\mathbb{R}_+^m$. In this\npaper, we give a generalized form of score matching for non-negative data that\nimproves estimation efficiency. As an example, we consider a general class of\npairwise interaction models. Addressing an overlooked inexistence problem, we\ngeneralize the regularized score matching method of Lin et al. [2016] and\nimprove its theoretical guarantees for non-negative Gaussian graphical models.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 21:50:40 GMT"}, {"version": "v2", "created": "Sun, 19 May 2019 00:28:04 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Yu", "Shiqing", ""], ["Drton", "Mathias", ""], ["Shojaie", "Ali", ""]]}, {"id": "1812.10800", "submitter": "Nicholas Seewald", "authors": "Nicholas J. Seewald, Shawna N. Smith, Andy Jinseok Lee, Predrag\n  Klasnja, Susan A. Murphy", "title": "Practical Considerations for Data Collection and Management in Mobile\n  Health Micro-randomized Trials", "comments": "Author accepted manuscript", "journal-ref": null, "doi": "10.1007/s12561-018-09228-w", "report-no": null, "categories": "stat.OT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing interest in leveraging the prevalence of mobile technology\nto improve health by delivering momentary, contextualized interventions to\nindividuals' smartphones. A just-in-time adaptive intervention (JITAI) adjusts\nto an individual's changing state and/or context to provide the right\ntreatment, at the right time, in the right place. Micro-randomized trials\n(MRTs) allow for the collection of data which aid in the construction of an\noptimized JITAI by sequentially randomizing participants to different treatment\noptions at each of many decision points throughout the study. Often, this data\nis collected passively using a mobile phone. To assess the causal effect of\ntreatment on a near-term outcome, care must be taken when designing the data\ncollection system to ensure it is of appropriately high quality. Here, we make\nseveral recommendations for collecting and managing data from an MRT. We\nprovide advice on selecting which features to collect and when, choosing\nbetween \"agents\" to implement randomization, identifying sources of missing\ndata, and overcoming other novel challenges. The recommendations are informed\nby our experience with HeartSteps, an MRT designed to test the effects of an\nintervention aimed at increasing physical activity in sedentary adults. We also\nprovide a checklist which can be used in designing a data collection system so\nthat scientists can focus more on their questions of interest, and less on\ncleaning data.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 19:23:23 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Seewald", "Nicholas J.", ""], ["Smith", "Shawna N.", ""], ["Lee", "Andy Jinseok", ""], ["Klasnja", "Predrag", ""], ["Murphy", "Susan A.", ""]]}, {"id": "1812.11132", "submitter": "Ece Mutlu", "authors": "Burak Alakent and Ece C. Mutlu", "title": "Application of Robust Estimators in Shewhart S-Charts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maintaining the quality of manufactured products at a desired level is known\nto increase customer satisfaction and profitability. Shewhart control chart is\nthe most widely used in statistical process control (SPC) technique to monitor\nthe quality of products and control process variability. Based on the\nassumption of independent and normally distributed data sets, sample mean and\nstandard deviation statistics are known to be the most efficient conventional\nestimators to determine the process location and scale, respectively. On the\nother hand, there is not guarantee that the real-world process data would be\nnormally distributed: outliers may exist, and/or sampled population may be\ncontaminated. In such cases, efficiency of the conventional estimators is\nsignificantly reduced, and power of the Shewhart charts may be undesirably low,\ne.g. occasional outliers in the rational subgroups (Phase I dataset) may\ndrastically affect the sample mean and standard deviation, resulting a serious\ndelay in detection of inferior products (Phase II procedure). For more\nefficient analyses, it is required to use robust estimators against\ncontaminations. Consequently, it is determined that robust estimators are more\nefficient both against diffuse localized and symmetric-asymmetric\ncontaminations, and have higher power in detecting disturbances, compared to\nconventional methods.\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2018 21:01:36 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Alakent", "Burak", ""], ["Mutlu", "Ece C.", ""]]}]