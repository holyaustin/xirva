[{"id": "1504.01361", "submitter": "Giulio D'Agostini", "authors": "Giulio D'Agostini", "title": "Bertrand `paradox' reloaded (with details on transformations of\n  variables, an introduction to Monte Carlo simulation and an inferential\n  variation of the problem)", "comments": "55 pages, 31 figures. Note based on lectures to PhD students in Rome.\n  Android app available at http://www.roma1.infn.it/~dagos/prob+stat.html#bpr", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an math.HO stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note is mainly to point out, if needed, that uncertainty about models\nand their parameters has little to do with a `paradox'. The proposed `solution'\nis to formulate practical questions instead of seeking refuge into abstract\nprinciples. (And, in order to be concrete, some details on how to calculate the\nprobability density functions of the chord lengths are provided, together with\nsome comments on simulations and an appendix on the inferential aspects of the\nproblem.)\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2015 19:14:54 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["D'Agostini", "Giulio", ""]]}, {"id": "1504.01950", "submitter": "David R. Bellhouse", "authors": "David R. Bellhouse, Nicolas Fillion", "title": "Le Her and Other Problems in Probability Discussed by Bernoulli,\n  Montmort and Waldegrave", "comments": "Published at http://dx.doi.org/10.1214/14-STS469 in the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2015, Vol. 30, No. 1, 26-39", "doi": "10.1214/14-STS469", "report-no": "IMS-STS-STS469", "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Part V of the second edition of Pierre R\\'{e}mond de Montmort's Essay\nd'analyse sur les jeux de hazard published in 1713 contains correspondence on\nprobability problems between Montmort and Nicolaus Bernoulli. This\ncorrespondence begins in 1710. The last published letter, dated November 15,\n1713, is from Montmort to Nicolaus Bernoulli. There is some discussion of the\nstrategy of play in the card game Le Her and a bit of news that Montmort's\nfriend Waldegrave in Paris was going to take care of the printing of the book.\nFrom earlier correspondence between Bernoulli and Montmort, it is apparent that\nWaldegrave had also analyzed Le Her and had come up with a mixed strategy as a\nsolution. He had also suggested working on the \"problem of the pool,\" or what\nis often called Waldegrave's problem. The Universit\\\"{a}tsbibliothek Basel\ncontains an additional forty-two letters between Bernoulli and Montmort written\nafter 1713, as well as two letters between Bernoulli and Waldegrave. The\nletters are all in French, and here we provide translations of key passages.\nThe trio continued to discuss probability problems, particularly Le Her which\nwas still under discussion when the Essay d'analyse went to print. We describe\nthe probability content of this body of correspondence and put it in its\nhistorical context. We also provide a proper identification of Waldegrave based\non manuscripts in the Archives nationales de France in Paris.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 13:17:43 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Bellhouse", "David R.", ""], ["Fillion", "Nicolas", ""]]}, {"id": "1504.02057", "submitter": "Samuel Clark", "authors": "Samuel J. Clark", "title": "A Singular Value Decomposition-based Factorization and Parsimonious\n  Component Model of Demographic Quantities Correlated by Age: Predicting\n  Complete Demographic Age Schedules with Few Parameters", "comments": "65 pages, 40 figures, University of Washington Center for Statistics\n  and the Social Sciences (CSSS) Working Paper No. 143", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BACKGROUND. Formal demography has a long history of building simple models of\nage schedules of demographic quantities, e.g. mortality and fertility rates.\nThese are widely used in demographic methods to manipulate whole age schedules\nusing few parameters.\n  OBJECTIVE. The Singular Value Decomposition (SVD) factorizes a matrix into\nthree matrices with useful properties including the ability to reconstruct the\noriginal matrix using many fewer, simple matrices. This work demonstrates how\nthese properties can be exploited to build parsimonious models of whole age\nschedules of demographic quantities that can be further parameterized in terms\nof arbitrary covariates.\n  METHODS. The SVD is presented and explained in detail with attention to\ndeveloping an intuitive understanding. The SVD is used to construct a general,\ncomponent model of demographic age schedules, and that model is demonstrated\nwith age-specific mortality and fertility rates. Finally, the model is used (1)\nto predict age-specific mortality using HIV indicators and summary measures of\nage-specific mortality, and (2) to predict age-specific fertility using the\ntotal fertility rate (TFR).\n  RESULTS. The component model of age-specific mortality and fertility rates\nsucceeds in reproducing the data with two inputs, and acting through those two\ninputs, various covariates are able to accurately predict full age schedules.\n  CONCLUSIONS. The SVD is potentially useful as a way to summarize, smooth and\nmodel age-specific demographic quantities. The component model is a general\nmethod of relating covariates to whole age schedules.\n  COMMENTS. The focus of this work is the SVD and the component model. The\napplications are for illustrative purposes only.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 18:08:57 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Clark", "Samuel J.", ""]]}, {"id": "1504.02124", "submitter": "Samuel Clark", "authors": "Samuel J. Clark, Jon Wakefield, Tyler McCormick, Michelle Ross", "title": "Hyak Mortality Monitoring System: Innovative Sampling and Estimation\n  Methods - Proof of Concept by Simulation", "comments": "Updated version including new simulation study with two-stage cluster\n  and optimum allocation sampling", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally health statistics are derived from civil and/or vital\nregistration. Civil registration in low-income countries varies from partial\ncoverage to essentially nothing at all. Consequently the state of the art for\npublic health information in low-income countries is efforts to combine or\ntriangulate data from different sources to produce a more complete picture\nacross both time and space - data amalgamation. Data sources amenable to this\napproach include sample surveys, sample registration systems, health and\ndemographic surveillance systems, administrative records, census records,\nhealth facility records and others.\n  We propose a new statistical framework for gathering health and population\ndata - Hyak - that leverages the benefits of sampling and longitudinal,\nprospective surveillance to create a cheap, accurate, sustainable monitoring\nplatform. Hyak has three fundamental components:\n  1) Data Amalgamation: a sampling and surveillance component that organizes\ntwo or more data collection systems to work together: a) data from HDSS with\nfrequent, intense, linked, prospective follow-up and b) data from sample\nsurveys conducted in large areas surrounding the Health and Demographic\nSurveillance System sites using informed sampling so as to capture as many\nevents as possible;\n  2) Cause of Death: verbal autopsy to characterize the distribution of deaths\nby cause at the population level; and\n  3) SES: measurement of socioeconomic status in order to characterize poverty\nand wealth.\n  We conduct a simulation study of the informed sampling component of Hyak\nbased on the Agincourt HDSS site in South Africa. Compared to traditional\ncluster sampling, Hyak's informed sampling captures more deaths, and when\ncombined with an estimation model that includes spatial smoothing, produces\nestimates mortality that have lower variance and small bias.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 20:47:12 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 01:37:58 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Clark", "Samuel J.", ""], ["Wakefield", "Jon", ""], ["McCormick", "Tyler", ""], ["Ross", "Michelle", ""]]}, {"id": "1504.02129", "submitter": "Samuel Clark", "authors": "Samuel J. Clark, Tyler McCormick, Zehang Li, Jon Wakefield", "title": "InSilicoVA: A Method to Automate Cause of Death Assignment for Verbal\n  Autopsy", "comments": "20 pages, 4 figures, Center for Statistics and the Social Sciences\n  (CSSS), University of Washington Working Paper No. 133", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Verbal autopsies (VA) are widely used to provide cause-specific mortality\nestimates in developing world settings where vital registration does not\nfunction well. VAs assign cause(s) to a death by using information describing\nthe events leading up to the death, provided by care givers. Typically\nphysicians read VA interviews and assign causes using their expert knowledge.\nPhysician coding is often slow, and individual physicians bring bias to the\ncoding process that results in non-comparable cause assignments. These problems\nsignificantly limit the utility of physician-coded VAs. A solution to both is\nto use an algorithmic approach that formalizes the cause-assignment process.\nThis ensures that assigned causes are comparable and requires many fewer\nperson-hours so that cause assignment can be conducted quickly without\ndisrupting the normal work of physicians. Peter Byass' InterVA method is the\nmost widely used algorithmic approach to VA coding and is aligned with the WHO\n2012 standard VA questionnaire.\n  The statistical model underpinning InterVA can be improved; uncertainty needs\nto be quantified, and the link between the population-level CSMFs and the\nindividual-level cause assignments needs to be statistically rigorous.\nAddressing these theoretical concerns provides an opportunity to create new\nsoftware using modern languages that can run on multiple platforms and will be\nwidely shared. Building on the overall framework pioneered by InterVA, our work\ncreates a statistical model for automated VA cause assignment.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 21:01:53 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["Clark", "Samuel J.", ""], ["McCormick", "Tyler", ""], ["Li", "Zehang", ""], ["Wakefield", "Jon", ""]]}, {"id": "1504.03089", "submitter": "William F. Rosenberger", "authors": "William F. Rosenberger", "title": "A Conversation with Nancy Flournoy", "comments": "Published at http://dx.doi.org/10.1214/14-STS495 in the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2015, Vol. 30, No. 1, 133-146", "doi": "10.1214/14-STS495", "report-no": "IMS-STS-STS495", "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nancy Flournoy was born in Long Beach, California, on May 4, 1947. After\ngraduating from Polytechnic School in Pasadena in 1965, she earned a B.S.\n(1969) and M.S. (1971) in biostatistics from UCLA. Between her bachelors and\nmasters degrees, she worked as a Statistician I for Regional Medical Programs\nat UCLA. After receiving her master's degree, she spend three years at the\nSouthwest Laboratory for Education Research and Development in Seal Beach,\nCalifornia. Flournoy joined the Seattle team pioneering bone marrow\ntransplantation in 1973. She moved with the transplant team into the newly\nformed Fred Hutchinson Cancer Research Center in 1975 as Director of Clinical\nStatistics, where she supervised a group responsible for the design and\nanalysis of about 80 simultaneous clinical trials. To support the Clinical\nDivision, she supervised the development of an interdisciplinary shared data\nsoftware system. She recruited Leonard B. Hearne to create this database\nmanagement system in 1975 (and married him in 1978). While at the Cancer\nCenter, she was also at the University of Washington, where she received her\ndoctorate in biomathematics in 1982. She became the first female director of\nthe program in statistics at the National Science Foundation (NSF) in 1986. She\nreceived service awards from the NSF in 1988 and the National Institute of\nStatistical Science in 2006 for facilitating interdisciplinary research.\nFlournoy joined the Department of Mathematics and Statistics at American\nUniversity in 1988. She moved as department chair to the University of Missouri\nin 2002, where she became Curators' Distinguished Professor in 2012.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2015 07:59:59 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Rosenberger", "William F.", ""]]}, {"id": "1504.03131", "submitter": "John A. Rice", "authors": "John A. Rice", "title": "A Conversation with Richard A. Olshen", "comments": "Published at http://dx.doi.org/10.1214/14-STS492 in the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2015, Vol. 30, No. 1, 118-132", "doi": "10.1214/14-STS492", "report-no": "IMS-STS-STS492", "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Richard Olshen was born in Portland, Oregon, on May 17, 1942. Richard spent\nhis early years in Chevy Chase, Maryland, but has lived most of his life in\nCalifornia. He received an A.B. in Statistics at the University of California,\nBerkeley, in 1963, and a Ph.D. in Statistics from Yale University in 1966,\nwriting his dissertation under the direction of Jimmie Savage and Frank\nAnscombe. He served as Research Staff Statistician and Lecturer at Yale in\n1966-1967. Richard accepted a faculty appointment at Stanford University in\n1967, and has held tenured faculty positions at the University of Michigan\n(1972-1975), the University of California, San Diego (1975-1989), and Stanford\nUniversity (since 1989). At Stanford, he is Professor of Health Research and\nPolicy (Biostatistics), Chief of the Division of Biostatistics (since 1998) and\nProfessor (by courtesy) of Electrical Engineering and of Statistics. At various\ntimes, he has had visiting faculty positions at Columbia, Harvard, MIT,\nStanford and the Hebrew University. Richard's research interests are in\nstatistics and mathematics and their applications to medicine and biology. Much\nof his work has concerned binary tree-structured algorithms for classification,\nregression, survival analysis and clustering. Those for classification and\nsurvival analysis have been used with success in computer-aided diagnosis and\nprognosis, especially in cardiology, oncology and toxicology. He coauthored the\n1984 book Classification and Regression Trees (with Leo Brieman, Jerome\nFriedman and Charles Stone) which gives motivation, algorithms, various\nexamples and mathematical theory for what have come to be known as CART\nalgorithms. The approaches to tree-structured clustering have been applied to\nproblems in digital radiography (with Stanford EE Professor Robert Gray) and to\nHIV genetics, the latter work including studies on single nucleotide\npolymorphisms, which has helped to shed light on the presence of hypertension\nin certain subpopulations of women.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2015 11:14:57 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Rice", "John A.", ""]]}, {"id": "1504.07336", "submitter": "Mohammad Jafari Jozani", "authors": "Armin Hatefi and Mohammad Jafari Jozani", "title": "Information content of partially rank-ordered set samples", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partially rank-ordered set (PROS) sampling is a generalization of ranked set\nsampling in which rankers are not required to fully rank the sampling units in\neach set, hence having more flexibility to perform the necessary judgemental\nranking process. The PROS sampling has a wide range of applications in\ndifferent fields ranging from environmental and ecological studies to medical\nresearch and it has been shown to be superior over ranked set sampling and\nsimple random sampling for estimating the population mean. In this paper, we\nstudy the Fisher information content and uncertainty structure of the PROS\nsamples and compare them with those of simple random sample (SRS) and ranked\nset sample (RSS) counterparts of the same size from the underlying population.\nWe study the uncertainty structure in terms of the Shannon entropy, Renyi\nentropy and Kullback-Leibler (KL) discrimination measures. Several examples\nincluding the FI of PROS samples from the location-scale family of\ndistributions as well as a regression model are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 03:09:52 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Hatefi", "Armin", ""], ["Jozani", "Mohammad Jafari", ""]]}]