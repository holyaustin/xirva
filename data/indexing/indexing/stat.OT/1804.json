[{"id": "1804.02229", "submitter": "Robin de Regt", "authors": "Robin de Regt and Ravinder Kumar", "title": "Theory of Cricket: Target Scores and Predictability", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a model for recalculating the target score in rain affected\nmatches based on empirical data. During the development of the current stage of\nthe Cricket, different methods have been introduced to recalculate the target\nscores in interpreted games. Currently, the International Cricket Council (ICC)\nuses the Duckworth-Lewis method and have in the past strongly considered\nchanging to the VJD method. Here, we introduce a simple approach to calculate\ntarget scores in interrupted games by considering the area under a run rate\ncurve. To calculate the target we have analysed over a decades worth of\nempirical data using various statistical methods. As in the case of Duckworth-\nLewis method, we also have two parameters in our model, that is overs and\nwickets in combination. We also found that in the one day international cricket\n(ODI) wickets play a crucial role whereas in T20 cricket they do not effect the\nrun rate of the games to the same degree. Using empirical and mathematical\narguments we show that the run scoring distributions are independent of the\ninnings.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 12:26:06 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["de Regt", "Robin", ""], ["Kumar", "Ravinder", ""]]}, {"id": "1804.02747", "submitter": "Krzysztof Chalupka", "authors": "Krzysztof Chalupka, Pietro Perona, Frederick Eberhardt", "title": "Fast Conditional Independence Test for Vector Variables with Large\n  Sample Sizes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and evaluate the Fast (conditional) Independence Test (FIT) -- a\nnonparametric conditional independence test. The test is based on the idea that\nwhen $P(X \\mid Y, Z) = P(X \\mid Y)$, $Z$ is not useful as a feature to predict\n$X$, as long as $Y$ is also a regressor. On the contrary, if $P(X \\mid Y, Z)\n\\neq P(X \\mid Y)$, $Z$ might improve prediction results. FIT applies to\nthousand-dimensional random variables with a hundred thousand samples in a\nfraction of the time required by alternative methods. We provide an extensive\nevaluation that compares FIT to six extant nonparametric independence tests.\nThe evaluation shows that FIT has low probability of making both Type I and\nType II errors compared to other tests, especially as the number of available\nsamples grows. Our implementation of FIT is publicly available.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 20:03:07 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Chalupka", "Krzysztof", ""], ["Perona", "Pietro", ""], ["Eberhardt", "Frederick", ""]]}, {"id": "1804.03732", "submitter": "Kevin Gross", "authors": "Kevin Gross and Carl T. Bergstrom", "title": "Contest models highlight inherent inefficiencies of scientific funding\n  competitions", "comments": "Main text: 9 pages, 4 figures. SI: 6 pages, 9 figures. Version 3\n  includes expanded discussion, and is the final published version", "journal-ref": "PLoS Biology 17(1): e3000065", "doi": "10.1371/journal.pbio.3000065", "report-no": null, "categories": "physics.soc-ph stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific research funding is allocated largely through a system of\nsoliciting and ranking competitive grant proposals. In these competitions, the\nproposals themselves are not the deliverables that the funder seeks, but\ninstead are used by the funder to screen for the most promising research ideas.\nConsequently, some of the funding program's impact on science is squandered\nbecause applying researchers must spend time writing proposals instead of doing\nscience. To what extent does the community's aggregate investment in proposal\npreparation negate the scientific impact of the funding program? Are there\nalternative mechanisms for awarding funds that advance science more\nefficiently? We use the economic theory of contests to analyze how efficiently\ngrant proposal competitions advance science, and compare them with recently\nproposed, partially randomized alternatives such as lotteries. We find that the\neffort researchers waste in writing proposals may be comparable to the total\nscientific value of the research that the funding supports, especially when\nonly a few proposals can be funded. Moreover, when professional pressures\nmotivate investigators to seek funding for reasons that extend beyond the value\nof the proposed science (e.g., promotion, prestige), the entire program can\nactually hamper scientific progress when the number of awards is small. We\nsuggest that lost efficiency may be restored either by partial lotteries for\nfunding, or by funding researchers based on past scientific success instead of\nproposals for future work.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 21:53:36 GMT"}, {"version": "v2", "created": "Thu, 14 Jun 2018 05:49:55 GMT"}, {"version": "v3", "created": "Wed, 2 Jan 2019 20:54:36 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Gross", "Kevin", ""], ["Bergstrom", "Carl T.", ""]]}, {"id": "1804.04587", "submitter": "William Artman", "authors": "William J. Artman, Inbal Nahum-Shani, Tianshuang Wu, James R. McKay,\n  Ashkan Ertefaie", "title": "Power Analysis in a SMART Design: Sample Size Estimation for Determining\n  the Best Dynamic Treatment Regime", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential, multiple assignment, randomized trial (SMART) designs have become\nincreasingly popular in the field of precision medicine by providing a means\nfor comparing sequences of treatments tailored to the individual patient, i.e.,\ndynamic treatment regime (DTR). The construction of evidence-based DTRs\npromises a replacement to adhoc one-size-fits-all decisions pervasive in\npatient care. However, there are substantial statistical challenges in sizing\nSMART designs due to the complex correlation structure between the DTRs\nembedded in the design. Since the primary goal of SMARTs is the construction of\nan optimal DTR, investigators are interested in sizing SMARTs based on the\nability to screen out DTRs inferior to the optimal DTR by a given amount which\ncannot be done using existing methods. In this paper, we fill this gap by\ndeveloping a rigorous power analysis framework that leverages multiple\ncomparisons with the best methodology. Our method employs Monte Carlo\nsimulation in order to compute the minimum number of individuals to enroll in\nan arbitrary SMART. We will evaluate our method through extensive simulation\nstudies. We will illustrate our method by retrospectively computing the power\nin the Extending Treatment Effectiveness of Naltrexone SMART study.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2018 18:17:06 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Artman", "William J.", ""], ["Nahum-Shani", "Inbal", ""], ["Wu", "Tianshuang", ""], ["McKay", "James R.", ""], ["Ertefaie", "Ashkan", ""]]}, {"id": "1804.04628", "submitter": "F.Thomas Bruss Prof.", "authors": "F. Thomas Bruss", "title": "A Mathematical Approach to Comply with Ethical Constraints in\n  Compassionate Use Treatments", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Patients who are seriously ill may ask doctors to treat them with unapproved\nmedication, about which not much is known, or else with known medication in a\nhigh dosage. Apart from strict legal constraints such cases may involve\ndifficult ethical questions as e.g. how long a series of treatments of\ndifferent patients should be continued. Similar questions also arise in less\nserious situations. A physician trusts that a certain combination of freely\navailable drugs are efficient against a specific disease and tries to help\npatients and to follow at the same time the primum-non-nocere principle.\n  The objective of this paper is to contribute to the research on such\nquestions in the form of mathematical models. Arguing in a step-to-step\napproach, we will show that certain sequential optimisation problems comply in\na natural way with the true spirit of major ethical principles in medicine. We\nthen suggest protocols and associate algorithms to find optimal, or\napproximately optimal, treatment strategies. Although the contribution may\nsometimes be difficult to apply in medical practice, the author thinks that the\nrational behind the approach offers a valuable alternative for finding decision\nsupport and should attract attention.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 09:04:13 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Bruss", "F. Thomas", ""]]}, {"id": "1804.04948", "submitter": "Torsten Ensslin", "authors": "Torsten En{\\ss}lin and Margret Westerkamp", "title": "The rationality of irrationality in the Monty Hall problem", "comments": "4 pages, no figures, revised article", "journal-ref": null, "doi": "10.1002/andp.201800128", "report-no": null, "categories": "stat.OT math.HO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rational solution of the Monty Hall problem unsettles many people. Most\npeople, including the authors, think it feels wrong to switch the initial\nchoice of one of the three doors, despite having fully accepted the\nmathematical proof for its superiority. Many people, if given the choice to\nswitch, think the chances are fifty-fifty between their options, but still\nstrongly prefer to stay with their initial choice. Is there some sense behind\nthese irrational feelings?\n  We entertain the possibility that intuition solves the problem of how to\nbehave in a real game show, not in the abstract textbook version of the Monty\nHall problem. A real showmaster sometimes plays evil, either to make the show\nmore interesting, to save money, or because he is in a bad mood. A moody\nshowmaster erases any information advantage the guest could extract by him\nopening other doors which drives the chance of the car being behind the chosen\ndoor towards fifty percent. Furthermore, the showmaster could try to read or\nmanipulate the guest's strategy to the guest's disadvantage. Given this, the\npreference to stay with the initial choice turns out to be a very rational\ndefense strategy of the show's guest against the threat of being manipulated by\nits host. Thus, the intuitive feelings most people have about the Monty Hall\nproblem coincide with what would be a rational strategy for a real-world game\nshow. Although these investigations are mainly intended to be an entertaining\nmathematical commentary on an information-theoretic puzzle, they touch on\ninteresting psychological questions.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 18:00:06 GMT"}, {"version": "v2", "created": "Fri, 27 Apr 2018 10:12:20 GMT"}, {"version": "v3", "created": "Sat, 7 Jul 2018 17:18:20 GMT"}, {"version": "v4", "created": "Mon, 22 Oct 2018 19:01:29 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["En\u00dflin", "Torsten", ""], ["Westerkamp", "Margret", ""]]}, {"id": "1804.06285", "submitter": "Zhe Sha", "authors": "Zhe Sha, Jonathan Rougier, Maike Schumacher and Jonathan Bamber", "title": "Bayesian model-data synthesis with an application to global\n  Glacio-Isostatic Adjustment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a framework for updating large scale geospatial processes using\na model-data synthesis method based on Bayesian hierarchical modelling. Two\nmajor challenges come from updating large-scale Gaussian process and modelling\nnon-stationarity. To address the first, we adopt the SPDE approach that uses a\nsparse Gaussian Markov random fields (GMRF) approximation to reduce the\ncomputational cost and implement the Bayesian inference by using the INLA\nmethod. For non-stationary global processes, we propose two general models that\naccommodate commonly-seen geospatial problems. Finally, we show an example of\nupdating an estimate of global glacial isostatic adjustment (GIA) using GPS\nmeasurements.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 14:24:23 GMT"}, {"version": "v2", "created": "Mon, 30 Apr 2018 12:43:15 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Sha", "Zhe", ""], ["Rougier", "Jonathan", ""], ["Schumacher", "Maike", ""], ["Bamber", "Jonathan", ""]]}, {"id": "1804.07192", "submitter": "Antonio Irpino PhD", "authors": "Rosanna Verde and Antonio Irpino", "title": "Multiple factor analysis of distributional data", "comments": "Accepted from STATSTICA APPLICATA: Italian Journal of Applied\n  Statistics on 12/2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the framework of Symbolic Data Analysis (SDA), distribution-variables are\na particular case of multi-valued variables: each unit is represented by a set\nof distributions (e.g. histograms, density functions or quantile functions),\none for each variable. Factor analysis (FA) methods are primary exploratory\ntools for dimension reduction and visualization. In the present work, we use\nMultiple Factor Analysis (MFA) approach for the analysis of data described by\ndistributional variables. Each distributional variable induces a set new\nnumeric variable related to the quantiles of each distribution. We call these\nnew variables as \\textit{quantile variables} and the set of quantile variables\nrelated to a distributional one is a block in the MFA approach. Thus, MFA is\nperformed on juxtaposed tables of quantile variables. \\\\ We show that the\ncriterion decomposed in the analysis is an approximation of the variability\nbased on a suitable metrics between distributions: the squared $L_2$\nWasserstein distance. \\\\ Applications on simulated and real distributional data\ncorroborate the method. The interpretation of the results on the factorial\nplanes is performed by new interpretative tools that are related to the several\ncharacteristics of the distributions (location, scale and shape).\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 14:29:21 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Verde", "Rosanna", ""], ["Irpino", "Antonio", ""]]}, {"id": "1804.07923", "submitter": "Priyantha Wijayatunga", "authors": "Priyantha Wijayatunga", "title": "Resolving the Lord's Paradox", "comments": "4 pages, The 32nd International Workshop on Statistical Modelling\n  (IWSM), Johann Bernoulli Institute, Rijksuniversiteit Groningen, Netherlands,\n  3-7 July 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An explanation to Lord's paradox using ordinary least square regression\nmodels is given. It is not a paradox at all, if the regression parameters are\ninterpreted as predictive or as causal with stricter conditions and be aware of\nlaws of averages. We use derivation of a super-model from a given sub-model,\nwhen its residuals can be modelled with other potential predictors as a\nsolution.\n", "versions": [{"version": "v1", "created": "Sat, 21 Apr 2018 09:14:30 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Wijayatunga", "Priyantha", ""]]}, {"id": "1804.07940", "submitter": "Priyantha Wijayatunga", "authors": "Priyantha Wijayatunga", "title": "Viewing Simpson's Paradox", "comments": "11 pages", "journal-ref": "Statistica & Applicazioni - Vol. XII, n. 2, 2014, pp. 225-235", "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Well known Simpson's paradox is puzzling and surprising for many, especially\nfor the empirical researchers and users of statistics. However there is no\nsurprise as far as mathematical details are concerned. A lot more is written\nabout the paradox but most of them are beyond the grasp of such users. This\nshort article is about explaining the phenomenon in an easy way to grasp using\nsimple algebra and geometry. The mathematical conditions under which the\nparadox can occur are made explicit and a simple geometrical illustrations is\nused to describe it. We consider the reversal of the association between two\nbinary variables, say, $X$ and $Y$ by a third binary variable, say, $Z$. We\nshow that it is always possible to define $Z$ algebraically for non-extreme\ndependence between $X$ and $Y$, therefore occurrence of the paradox depends on\nidentifying it with a practical meaning for it in a given context of interest,\nthat is up to the subject domain expert. And finally we discuss the paradox in\npredictive contexts since in literature it is argued that the paradox is\nresolved using causal reasoning.\n", "versions": [{"version": "v1", "created": "Sat, 21 Apr 2018 10:36:02 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Wijayatunga", "Priyantha", ""]]}, {"id": "1804.10939", "submitter": "Georgios Papageorgiou", "authors": "Georgios Papageorgiou", "title": "BNSP: an R Package for Fitting Bayesian Semiparametric Regression Models\n  and Variable Selection", "comments": "28 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The R package BNSP provides a unified framework for semiparametric\nlocation-scale regression and stochastic search variable selection. The\nstatistical methodology that the package is built upon utilizes basis function\nexpansions to represent semiparametric covariate effects in the mean and\nvariance functions, and spike-slab priors to perform selection and\nregularization of the estimated effects. In addition to the main function that\nperforms posterior sampling, the package includes functions for assessing\nconvergence of the sampler, summarizing model fits, visualizing covariate\neffects and obtaining predictions for new responses or their means given\nfeature/covariate vectors.\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2018 14:28:46 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 23:21:38 GMT"}, {"version": "v3", "created": "Wed, 20 Jun 2018 09:19:23 GMT"}, {"version": "v4", "created": "Mon, 8 Oct 2018 13:45:12 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Papageorgiou", "Georgios", ""]]}]