[{"id": "2106.00006", "submitter": "Georgios Giasemidis Dr", "authors": "Stephen Haben, Siddharth Arora, Georgios Giasemidis, Marcus Voss,\n  Danica Vukadinovic Greetham", "title": "Review of Low-Voltage Load Forecasting: Methods, Applications, and\n  Recommendations", "comments": "33 pages, 5 figures, review paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increased digitalisation and monitoring of the energy system opens up\nnumerous opportunities % and solutions which can help to decarbonise the energy\nsystem. Applications on low voltage (LV), localised networks, such as community\nenergy markets and smart storage will facilitate decarbonisation, but they will\nrequire advanced control and management. Reliable forecasting will be a\nnecessary component of many of these systems to anticipate key features and\nuncertainties. Despite this urgent need, there has not yet been an extensive\ninvestigation into the current state-of-the-art of low voltage level forecasts,\nother than at the smart meter level. This paper aims to provide a comprehensive\noverview of the landscape, current approaches, core applications, challenges\nand recommendations. Another aim of this paper is to facilitate the continued\nimprovement and advancement in this area. To this end, the paper also surveys\nsome of the most relevant and promising trends. It establishes an open,\ncommunity-driven list of the known LV level open datasets to encourage further\nresearch and development.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 14:18:43 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Haben", "Stephen", ""], ["Arora", "Siddharth", ""], ["Giasemidis", "Georgios", ""], ["Voss", "Marcus", ""], ["Greetham", "Danica Vukadinovic", ""]]}, {"id": "2106.00758", "submitter": "Richard D. Gill", "authors": "Norman Fenton, Richard D. Gill, David Lagnado, Martin Neil", "title": "Statistical issues in Serial Killer Nurse cases", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss statistical issues in cases of serial killer nurses, focussing on\nthe Dutch case of the nurse Lucia de Berk, arrested under suspicion of murder\nin 2001, convicted to life imprisonment, but declared innocent in 2010; and the\ncase of the English nurse Ben Geen, arrested in 2004, also given a life\nsentence. At the trial of Ben Geen, a statistical expert was refused permission\nto present evidence on statistical biases concerning the way suspicious cases\nwere identified by a hospital team of investigators. The judge ruled that the\nexpert's written evidence was merely common sense. An application to the CCRC\nto review the case was turned down, since the application only presented\nstatistical evidence but did not re-address the medical evidence presented at\nthe original trials. This rejection has been successfully challenged in court,\nand the CCRC has withdrawn it. The paper includes some striking new statistical\nfindings on the Ben Geen case as well as giving advice to statisticians\ninvolved in future cases, which are not infrequent. Statisticians need to be\nwarned of the pitfalls which await them.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 13:01:41 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Fenton", "Norman", ""], ["Gill", "Richard D.", ""], ["Lagnado", "David", ""], ["Neil", "Martin", ""]]}, {"id": "2106.03178", "submitter": "Heyang Gong", "authors": "Heyang Gong, Ke Zhu", "title": "Path-specific Effects Based on Information Accounts of Causality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Path-specific effects in mediation analysis provide a useful tool for\nfairness analysis, which is mostly based on nested counterfactuals. However,\nthe dictum ``no causation without manipulation'' implies that path-specific\neffects might be induced by certain interventions. This paper proposes a new\npath intervention inspired by information accounts of causality, and develops\nthe corresponding intervention diagrams and $\\pi$-formula. Compared with the\ninterventionist approach of Robins et al.(2020) based on nested\ncounterfactuals, our proposed path intervention method explicitly describes the\nmanipulation in structural causal model with a simple information transferring\ninterpretation, and does not require the non-existence of recanting witness to\nidentify path-specific effects. Hence, it could serve useful communications and\ntheoretical focus for mediation analysis.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 16:50:06 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Gong", "Heyang", ""], ["Zhu", "Ke", ""]]}, {"id": "2106.04253", "submitter": "Yi Zhou", "authors": "Yi Zhou, Ao Huang and Satoshi Hattori", "title": "A likelihood based sensitivity analysis for publication bias on summary\n  ROC in meta-analysis of diagnostic test accuracy", "comments": "32 pages, 4 figure, 5 tables in main text, 5 tables in supplementary\n  file", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In meta-analysis of diagnostic test accuracy, summary receiver operating\ncharacteristic (SROC) is a recommended method to summarize the discriminant\ncapacity of a diagnostic test in the presence of study-specific cutoff values\nand the area under the SROC (SAUC) gives the aggregate measure of test\naccuracy. SROC or SAUC can be estimated by bivariate modelling of pairs of\nsensitivity and specificity over the primary diagnostic studies. However,\npublication bias is a major threat to the validity of estimates in\nmeta-analysis. To address this issue, we propose to adopt sensitivity analysis\nto make an objective inference for the impact of publication bias on SROC or\nSAUC. We extend Copas likelihood based sensitivity analysis to the bivariate\nnormal model used for meta-analysis of diagnostic test accuracy to evaluate how\nmuch SROC or SAUC would change with different selection probabilities under\nseveral selective publication mechanisms dependent on sensitivity and/or\nspecificity. The selection probability is modelled by a selection function on\n$t$-type statistic for the linear combination of logit-transformed sensitivity\nand specificity, allowing the selective publication of each study to be\ninfluenced by the cutoff-dependent $p$-value for sensitivity, specificity, or\ndiagnostic odds ratio. By embedding the selection function into the bivariate\nnormal model, the conditional likelihood is proposed and the bias-corrected\nSROC or SAUC can be estimated by maximizing the likelihood. We illustrate the\nproposed sensitivity analysis by reanalyzing a meta-analysis of test accuracy\nfor intravascular device related infection. Simulation studies are conducted to\ninvestigate the performance of proposed methods.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 11:23:46 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Zhou", "Yi", ""], ["Huang", "Ao", ""], ["Hattori", "Satoshi", ""]]}, {"id": "2106.04433", "submitter": "Alexander Wimbush", "authors": "Alexander Wimbush, Nicholas Gray, Scott Ferson", "title": "Singhing with Confidence: Visualising the Performance of Confidence\n  Structures", "comments": "12 Pages Textx, 9 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Confidence intervals are an established means of portraying uncertainty about\nan inferred parameter and can be generated through the use of confidence\ndistributions. For a confidence distribution to be ideal, it must maintain\nfrequentist coverage of the true parameter. This can be represented for a\nprecise distribution by adherence to a cumulative unit uniform distribution,\nreferred to here as a Singh plot. This manuscript extends this to imprecise\nconfidence structures with bounds around the uniform distribution, and\ndescribes how deviations convey information regarding the characteristics of\nconfidence structures designed for inference and prediction. This quick visual\nrepresentation, in a manner similar to ROC curves, aids the development of\nrobust structures and methods that make use of confidence. A demonstration of\nthe utility of Singh plots is provided with an assessment of the coverage of\nthe ProUCL Chebyshev upper confidence limit estimator for the mean of an\nunknown distribution.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 15:09:57 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Wimbush", "Alexander", ""], ["Gray", "Nicholas", ""], ["Ferson", "Scott", ""]]}, {"id": "2106.04997", "submitter": "Pavel  Krivitsky", "authors": "Pavel N. Krivitsky (1), David R. Hunter (2), Martina Morris (3), Chad\n  Klumb (3) ((1) University of New South Wales, (2) Penn State University, (3)\n  University of Washington)", "title": "ergm 4.0: New features and improvements", "comments": "52 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ergm package supports the statistical analysis and simulation of network\ndata. It anchors the statnet suite of packages for network analysis in R\nintroduced in a special issue in Journal of Statistical Software in 2008. This\narticle provides an overview of the functionality and performance improvements\nin the 2021 ergm 4.0 release. These include more flexible handling of nodal\ncovariates, operator terms that extend and simplify model specification, new\nmodels for networks with valued edges, improved handling of constraints on the\nsample space of networks, performance enhancements to the Markov chain Monte\nCarlo and maximum likelihood estimation algorithms, broader and faster\nsearching for networks with certain target statistics using simulated\nannealing, and estimation with missing edge data. We also identify the new\npackages in the statnet suite that extend ergm's functionality to other network\ndata types and structural features, and the robust set of online resources that\nsupport the statnet development process and applications.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 11:33:49 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Krivitsky", "Pavel N.", ""], ["Hunter", "David R.", ""], ["Morris", "Martina", ""], ["Klumb", "Chad", ""]]}, {"id": "2106.07053", "submitter": "Qingyun Sun", "authors": "Qingyun Sun and David Donoho", "title": "Convex Sparse Blind Deconvolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI cs.SY eess.SY math.IT math.ST stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the blind deconvolution problem, we observe the convolution of an unknown\nfilter and unknown signal and attempt to reconstruct the filter and signal. The\nproblem seems impossible in general, since there are seemingly many more\nunknowns than knowns . Nevertheless, this problem arises in many application\nfields; and empirically, some of these fields have had success using heuristic\nmethods -- even economically very important ones, in wireless communications\nand oil exploration. Today's fashionable heuristic formulations pose non-convex\noptimization problems which are then attacked heuristically as well. The fact\nthat blind deconvolution can be solved under some repeatable and\nnaturally-occurring circumstances poses a theoretical puzzle.\n  To bridge the gulf between reported successes and theory's limited\nunderstanding, we exhibit a convex optimization problem that -- assuming signal\nsparsity -- can convert a crude approximation to the true filter into a\nhigh-accuracy recovery of the true filter. Our proposed formulation is based on\nL1 minimization of inverse filter outputs. We give sharp guarantees on\nperformance of the minimizer assuming sparsity of signal, showing that our\nproposal precisely recovers the true inverse filter, up to shift and rescaling.\nThere is a sparsity/initial accuracy tradeoff: the less accurate the initial\napproximation, the greater we rely on sparsity to enable exact recovery. To our\nknowledge this is the first reported tradeoff of this kind. We consider it\nsurprising that this tradeoff is independent of dimension.\n  We also develop finite-$N$ guarantees, for highly accurate reconstruction\nunder $N\\geq O(k \\log(k) )$ with high probability. We further show stable\napproximation when the true inverse filter is infinitely long and extend our\nguarantees to the case where the observations are contaminated by stochastic or\nadversarial noise.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 17:39:26 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Sun", "Qingyun", ""], ["Donoho", "David", ""]]}, {"id": "2106.07587", "submitter": "Shunichiro Orihara", "authors": "Shunichiro Orihara", "title": "Quasi-Maximum Likelihood based Model Selection Procedures for Binary\n  Outcomes", "comments": "Keywords: Causal inference, Unmeasured covariates, Quasi-maximum\n  lilkelihood, Two-stage residual inclusion, Model selection, Consistency", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, I propose two model selection procedures based on a\nquasi-maximum likelihood estimator when there exist unmeasured covariates. I\nprove that a proposed BIC-type model selection procedure has model selection\nconsistency, and confirm these property through simulation datasets.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 16:45:15 GMT"}, {"version": "v2", "created": "Sun, 25 Jul 2021 05:33:30 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Orihara", "Shunichiro", ""]]}, {"id": "2106.11209", "submitter": "Nicholas Horton", "authors": "Chelsey Legacy and Andrew Zieffler and Benjamin S. Baumer and Valerie\n  Barr and Nicholas J. Horton", "title": "Facilitating team-based data science: lessons learned from the DSC-WAV\n  project", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While coursework provides undergraduate data science students with some\nrelevant analytic skills, many are not given the rich experiences with data and\ncomputing they need to be successful in the workplace. Additionally, students\noften have lmited exposure to team-based data science and the principles and\ntools of collaboration that are encountered outside of school.\n  In this paper, we describe the DSC-WAV program, an NSF-funded data science\nworkforce development project in which teams of undergraduate sophomores and\njuniors work with a local non-profit organization on a data-focused problem. To\nhelp students develop a sense of agency and improve confidence in their\ntechnical and non-technical data science skills, the project promoted a\nteam-based approach to data science, adopting several processes and tools\nintended to facilitate this collaboration.\n  Evidence from the project evaluation, including participant survey and\ninterview data, is presented to document the degree to which the project was\nsuccessful in engaging students in team-based data science, and how the project\nchanged the students' perceptions of their technical and non-technical skills.\nWe also examine opportunities for improvement and offer insight to provide\nadvice for other data science educators who want to implement something similar\nat their own institutions.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 15:51:34 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 12:15:06 GMT"}, {"version": "v3", "created": "Fri, 25 Jun 2021 17:36:42 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Legacy", "Chelsey", ""], ["Zieffler", "Andrew", ""], ["Baumer", "Benjamin S.", ""], ["Barr", "Valerie", ""], ["Horton", "Nicholas J.", ""]]}, {"id": "2106.12180", "submitter": "Xueheng Shi", "authors": "Xueheng Shi, Claudie Beaulieu, Rebecca Killick, Robert Lund", "title": "Changepoint Detection: An Analysis of the Central England Temperature\n  Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a statistical analysis of structural changes in the\nCentral England temperature series. This series contains one of the longest\nsurface temperature records available and a changepoint analysis of it reveals\nseveral interesting aspects. Regression functions with structural breaks,\nincluding mean and trend shifts, are fitted to the series and compared via two\ncommonly used multiple changepoint penalized likelihood criteria. In the end,\nthe optimal model is judged to be one containing three location and trend\nshifts, with a transition to a rapidly warming regime circa 1989. The\nvariability of the series is not found to be significantly changing, and shift\nfeatures are judged to be more plausible than short- or long-memory\nautocorrelations. The analysis serves as a walk-through tutorial of different\nchangepoint techniques, illustrating what can statistically be inferred from\ndifferent models.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 06:10:21 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Shi", "Xueheng", ""], ["Beaulieu", "Claudie", ""], ["Killick", "Rebecca", ""], ["Lund", "Robert", ""]]}, {"id": "2106.13564", "submitter": "Valentin Courgeau", "authors": "Valentin Courgeau and Almut E.D. Veraart", "title": "Extreme event propagation using counterfactual theory and vine copulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Understanding multivariate extreme events play a crucial role in managing the\nrisks of complex systems since extremes are governed by their own mechanisms.\nConditional on a given variable exceeding a high threshold (e.g.\\ traffic\nintensity), knowing which high-impact quantities (e.g\\ air pollutant levels)\nare the most likely to be extreme in the future is key. This article\ninvestigates the contribution of marginal extreme events on future extreme\nevents of related quantities. We propose an Extreme Event Propagation framework\nto maximise counterfactual causation probabilities between a known cause and\nfuture high-impact quantities. Extreme value theory provides a tool for\nmodelling upper tails whilst vine copulas are a flexible device for capturing a\nlarge variety of joint extremal behaviours. We optimise for the probabilities\nof causation and apply our framework to a London road traffic and air\npollutants dataset. We replicate documented atmospheric mechanisms beyond\nlinear relationships. This provides a new tool for quantifying the propagation\nof extremes in a large variety of applications.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 11:22:17 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Courgeau", "Valentin", ""], ["Veraart", "Almut E. D.", ""]]}]