[{"id": "1403.0240", "submitter": "Ivo Sbalzarini", "authors": "Ivo F. Sbalzarini, Sophie Schneider, Janick Cardinale", "title": "Particle methods enable fast and simple approximation of Sobolev\n  gradients in image segmentation", "comments": "21 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CE cs.NA q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bio-image analysis is challenging due to inhomogeneous intensity\ndistributions and high levels of noise in the images. Bayesian inference\nprovides a principled way for regularizing the problem using prior knowledge. A\nfundamental choice is how one measures \"distances\" between shapes in an image.\nIt has been shown that the straightforward geometric L2 distance is degenerate\nand leads to pathological situations. This is avoided when using Sobolev\ngradients, rendering the segmentation problem less ill-posed. The high\ncomputational cost and implementation overhead of Sobolev gradients, however,\nhave hampered practical applications. We show how particle methods as applied\nto image segmentation allow for a simple and computationally efficient\nimplementation of Sobolev gradients. We show that the evaluation of Sobolev\ngradients amounts to particle-particle interactions along the contour in an\nimage. We extend an existing particle-based segmentation algorithm to using\nSobolev gradients. Using synthetic and real-world images, we benchmark the\nresults for both 2D and 3D images using piecewise smooth and piecewise constant\nregion models. The present particle approximation of Sobolev gradients is 2.8\nto 10 times faster than the previous reference implementation, but retains the\nknown favorable properties of Sobolev gradients. This speedup is achieved by\nusing local particle-particle interactions instead of solving a global Poisson\nequation at each iteration. The computational time per iteration is higher for\nSobolev gradients than for L2 gradients. Since Sobolev gradients precondition\nthe optimization problem, however, a smaller number of overall iterations may\nbe necessary for the algorithm to converge, which can in some cases amortize\nthe higher per-iteration cost.\n", "versions": [{"version": "v1", "created": "Sun, 2 Mar 2014 16:58:29 GMT"}], "update_date": "2014-03-04", "authors_parsed": [["Sbalzarini", "Ivo F.", ""], ["Schneider", "Sophie", ""], ["Cardinale", "Janick", ""]]}, {"id": "1403.0296", "submitter": "Vrutangkumar Shah", "authors": "Vrutangkumar Shah, Sachin Goyal, Harish Palanthandalam-Madapusi", "title": "Clinical Facts Along With a Feedback Control Perspective Suggest That\n  Increased Response Time Might be the Cause of Parkinsonian Rest Tremor", "comments": null, "journal-ref": "Journal of Computational and Nonlinear Dynamics 12, no. 1 (2017):\n  011007", "doi": "10.1115/1.4034050", "report-no": null, "categories": "q-bio.QM q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parkinson's disease (PD) is a neurodegenerative disorder characterized by\nincreased response times leading to a variety of biomechanical symptoms such as\ntremors, stooping and gait instability. Although the deterioration in\nbiomechanical control can intuitively be related to sluggish response times,\nhow the delay leads to such biomechanical symptoms as tremor is not yet\nunderstood. Only recently has it been explained from the perspective of\nfeedback control theory that delay beyond a threshold can be the cause of\nParkinsonian tremor [1]. This paper correlates several observations from this\nperspective to clinical facts and reinforces them with simple numerical and\nexperimental examples. This work provides a framework towards developing a\ndeeper conceptual understanding of the mechanism behind PD symptoms.\nFurthermore, it lays a foundation for developing tools for diagnosis and\nprogress tracking of the disease by identifying some key trends.\n", "versions": [{"version": "v1", "created": "Mon, 3 Mar 2014 03:03:13 GMT"}, {"version": "v2", "created": "Sun, 13 Dec 2015 13:01:03 GMT"}, {"version": "v3", "created": "Wed, 26 Jul 2017 11:54:08 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Shah", "Vrutangkumar", ""], ["Goyal", "Sachin", ""], ["Palanthandalam-Madapusi", "Harish", ""]]}, {"id": "1403.0630", "submitter": "Christopher Eastaugh PhD", "authors": "C.S. Eastaugh, C. Thurnher, H. Hasenauer, J.K. Vanclay", "title": "Stephenson et al.'s ecological fallacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After more than a century of research the typical growth pattern of a tree\nwas thought to be fairly well understood. Following germination height growth\naccelerates for some time, then increment peaks and the added height each year\nbecomes less and less. The cross sectional area (basal area) of the tree\nfollows a similar pattern, but the maximum basal area increment occurs at some\ntime after the maximum height increment. An increase in basal area in a tall\ntree will add more volume to the stem than the same increase in a short tree,\nso the increment in stem volume (or mass) peaks very late. Stephenson et al.\nchallenge this paradigm, and suggest that mass increment increases\ncontinuously. Their analysis methods however are a textbook example of the\necological fallacy, and their conclusions therefore unsupported.\n", "versions": [{"version": "v1", "created": "Mon, 3 Mar 2014 23:11:28 GMT"}], "update_date": "2014-03-05", "authors_parsed": [["Eastaugh", "C. S.", ""], ["Thurnher", "C.", ""], ["Hasenauer", "H.", ""], ["Vanclay", "J. K.", ""]]}, {"id": "1403.0914", "submitter": "Donald Chang", "authors": "C. F. Hazlewood, D. C. Chang, D. Medina, G. Cleveland, and B. L.\n  Nichols", "title": "Distinction between the Preneoplastic and Neoplastic State of Murine\n  Mammary Glands by spin-echo NMR", "comments": "5 pages, no figure", "journal-ref": "Proc. Nat. Acad. Sci. USA, Vol. 69, No. 6, pp. 1478-1480, 1972", "doi": "10.1073/pnas.69.6.1478", "report-no": null, "categories": "q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have, using spin-echo nuclear magnetic resonance spectroscopy, measured\nthe relaxation times and diffusion coefficient of water protons in primary\nmammary adenocarcinomas of mice. In our biological model, three morphological\nstages were defined: (a) mammary gland tissue from pregnant mice, (b)\npreneoplastic nodules, and (c) neoplastic tissue. It was found that neoplastic\ntissues could be distinguished from normal and prenoeplastic tissue. Spin-spin\nand spin-lattice relaxation times and the diffusion coefficient of water\nprotons are increased in the neoplastic tissue relative to mammary gland tissue\nfrom pregnant mice and preneoplastic nodule tissue. These results suggested\nthat one can use a pulsed NMR method to detect and even predict breast cancer.\n", "versions": [{"version": "v1", "created": "Sun, 2 Mar 2014 04:43:00 GMT"}], "update_date": "2014-03-05", "authors_parsed": [["Hazlewood", "C. F.", ""], ["Chang", "D. C.", ""], ["Medina", "D.", ""], ["Cleveland", "G.", ""], ["Nichols", "B. L.", ""]]}, {"id": "1403.1347", "submitter": "Jian Zhou Zhou", "authors": "Jian Zhou and Olga G. Troyanskaya", "title": "Deep Supervised and Convolutional Generative Stochastic Network for\n  Protein Secondary Structure Prediction", "comments": "Accepted by ICML 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting protein secondary structure is a fundamental problem in protein\nstructure prediction. Here we present a new supervised generative stochastic\nnetwork (GSN) based method to predict local secondary structure with deep\nhierarchical representations. GSN is a recently proposed deep learning\ntechnique (Bengio & Thibodeau-Laufer, 2013) to globally train deep generative\nmodel. We present the supervised extension of GSN, which learns a Markov chain\nto sample from a conditional distribution, and applied it to protein structure\nprediction. To scale the model to full-sized, high-dimensional data, like\nprotein sequences with hundreds of amino acids, we introduce a convolutional\narchitecture, which allows efficient learning across multiple layers of\nhierarchical representations. Our architecture uniquely focuses on predicting\nstructured low-level labels informed with both low and high-level\nrepresentations learned by the model. In our application this corresponds to\nlabeling the secondary structure state of each amino-acid residue. We trained\nand tested the model on separate sets of non-homologous proteins sharing less\nthan 30% sequence identity. Our model achieves 66.4% Q8 accuracy on the CB513\ndataset, better than the previously reported best performance 64.9% (Wang et\nal., 2011) for this challenging secondary structure prediction problem.\n", "versions": [{"version": "v1", "created": "Thu, 6 Mar 2014 05:18:26 GMT"}], "update_date": "2014-03-07", "authors_parsed": [["Zhou", "Jian", ""], ["Troyanskaya", "Olga G.", ""]]}, {"id": "1403.1417", "submitter": "Eva Balsa-Canto", "authors": "Oana-Teodora Chis, Julio R. Banga and Eva Balsa-Canto", "title": "Sloppy models can be identifiable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.MN q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic models of biochemical networks typically consist of sets of\nnon-linear ordinary differential equations involving states (concentrations or\namounts of the components of the network) and parameters describing the\nreaction kinetics. Unfortunately, in most cases the parameters are completely\nunknown or only rough estimates of their values are available. Therefore, their\nvalues must be estimated from time-series experimental data.\n  In recent years, it has been suggested that dynamic systems biology models\nare universally sloppy so their parameters cannot be uniquely estimated. In\nthis work, we re-examine this concept, establishing links with the notions of\nidentifiability and experimental design. Further, considering a set of\nexamples, we address the following fundamental questions: i) is sloppiness\ninherent to model structure?; ii) is sloppiness influenced by experimental data\nor noise?; iii) does sloppiness mean that parameters cannot be identified?, and\niv) can sloppiness be modified by experimental design?\n  Our results indicate that sloppiness is not equivalent to lack of structural\nor practical identifiability (although they can be related), so sloppy models\ncan be identifiable. Therefore, drawing conclusions about the possibility of\nestimating unique parameter values by sloppiness analysis can be misleading.\nChecking structural and practical identifiability analyses is a better approach\nto asses the uniqueness and confidence in parameter estimation.\n", "versions": [{"version": "v1", "created": "Thu, 6 Mar 2014 11:45:59 GMT"}], "update_date": "2014-03-07", "authors_parsed": [["Chis", "Oana-Teodora", ""], ["Banga", "Julio R.", ""], ["Balsa-Canto", "Eva", ""]]}, {"id": "1403.1706", "submitter": "Johannes K\\\"oster", "authors": "Johannes K\\\"oster, Sven Rahmann", "title": "Massively parallel read mapping on GPUs with PEANUT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present PEANUT (ParallEl AligNment UTility), a highly parallel GPU-based\nread mapper with several distinguishing features, including a novel q-gram\nindex (called the q-group index) with small memory footprint built on-the-fly\nover the reads and the possibility to output both the best hits or all hits of\na read. Designing the algorithm particularly for the GPU architecture, we were\nable to reach maximum core occupancy for several key steps. Our benchmarks show\nthat PEANUT outperforms other state-of- the-art mappers in terms of speed and\nsensitivity. The software is available at http://peanut.readthedocs.org.\n", "versions": [{"version": "v1", "created": "Fri, 7 Mar 2014 10:10:07 GMT"}], "update_date": "2014-03-10", "authors_parsed": [["K\u00f6ster", "Johannes", ""], ["Rahmann", "Sven", ""]]}, {"id": "1403.1876", "submitter": "Vonn Walter", "authors": "Vonn Walter, Fred A. Wright, Andrew B. Nobel", "title": "Consistent Testing for Recurrent Genomic Aberrations", "comments": "35 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST q-bio.QM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genomic aberrations, such as somatic copy number alterations, are frequently\nobserved in tumor tissue. Recurrent aberrations, occurring in the same region\nacross multiple subjects, are of interest because they may highlight genes\nassociated with tumor development or progression. A number of tools have been\nproposed to assess the statistical significance of recurrent DNA copy number\naberrations, but their statistical properties have not been carefully studied.\nCyclic shift testing, a permutation procedure using independent random shifts\nof genomic marker observations on the genome, has been proposed to identify\nrecurrent aberrations, and is potentially useful for a wider variety of\npurposes, including identifying regions with methylation aberrations or\noverrepresented in disease association studies. For data following a\ncountable-state Markov model, we prove the asymptotic validity of cyclic shift\n$p$-values under a fixed sample size regime as the number of observed markers\ntends to infinity. We illustrate cyclic shift testing for a variety of data\ntypes, producing biologically relevant findings for three publicly available\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 7 Mar 2014 21:03:08 GMT"}], "update_date": "2014-03-11", "authors_parsed": [["Walter", "Vonn", ""], ["Wright", "Fred A.", ""], ["Nobel", "Andrew B.", ""]]}, {"id": "1403.2197", "submitter": "Markus Pagitz Dr", "authors": "Markus Pagitz and Remco I. Leine", "title": "Shape Optimization of Compliant Pressure Actuated Cellular Structures", "comments": "15 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biologically inspired pressure actuated cellular structures can alter their\nshape through pressure variations. Previous work introduced a computational\nframework for pressure actuated cellular structures which was limited to two\ncell rows and central cell corner hinges. This article rigorously extends these\nresults by taking into account an arbitrary number of cell rows, a more\ncomplicated cell kinematics that includes hinge eccentricities and varying side\nlengths as well as rotational and axial cell side springs. The nonlinear\neffects of arbitrary cell deformations are fully considered. Furthermore, the\noptimization is considerably improved by using a second-order approach. The\npresented framework enables the design of compliant pressure actuated cellular\nstructures that can change their form from one shape to another within a set of\none-dimensional C1 continuous functions.\n", "versions": [{"version": "v1", "created": "Mon, 10 Mar 2014 09:40:47 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2015 17:50:20 GMT"}, {"version": "v3", "created": "Tue, 19 Jan 2016 08:47:50 GMT"}, {"version": "v4", "created": "Fri, 24 Jun 2016 12:32:37 GMT"}, {"version": "v5", "created": "Fri, 7 Apr 2017 08:24:00 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Pagitz", "Markus", ""], ["Leine", "Remco I.", ""]]}, {"id": "1403.2877", "submitter": "Carlos Oscar Sorzano S.", "authors": "C.O.S. Sorzano, J. Vargas, A. Pascual Montano", "title": "A survey of dimensionality reduction techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experimental life sciences like biology or chemistry have seen in the recent\ndecades an explosion of the data available from experiments. Laboratory\ninstruments become more and more complex and report hundreds or thousands\nmeasurements for a single experiment and therefore the statistical methods face\nchallenging tasks when dealing with such high dimensional data. However, much\nof the data is highly redundant and can be efficiently brought down to a much\nsmaller number of variables without a significant loss of information. The\nmathematical procedures making possible this reduction are called\ndimensionality reduction techniques; they have widely been developed by fields\nlike Statistics or Machine Learning, and are currently a hot research topic. In\nthis review we categorize the plethora of dimension reduction techniques\navailable and give the mathematical insight behind them.\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2014 10:35:15 GMT"}], "update_date": "2014-03-13", "authors_parsed": [["Sorzano", "C. O. S.", ""], ["Vargas", "J.", ""], ["Montano", "A. Pascual", ""]]}, {"id": "1403.2933", "submitter": "Daniel Larremore", "authors": "Daniel B. Larremore, Aaron Clauset, Abigail Z. Jacobs", "title": "Efficiently inferring community structure in bipartite networks", "comments": "12 pages, 9 figures", "journal-ref": "Physical Review E 90(1): 012805 (2014)", "doi": "10.1103/PhysRevE.90.012805", "report-no": null, "categories": "cs.SI physics.data-an physics.soc-ph q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bipartite networks are a common type of network data in which there are two\ntypes of vertices, and only vertices of different types can be connected. While\nbipartite networks exhibit community structure like their unipartite\ncounterparts, existing approaches to bipartite community detection have\ndrawbacks, including implicit parameter choices, loss of information through\none-mode projections, and lack of interpretability. Here we solve the community\ndetection problem for bipartite networks by formulating a bipartite stochastic\nblock model, which explicitly includes vertex type information and may be\ntrivially extended to $k$-partite networks. This bipartite stochastic block\nmodel yields a projection-free and statistically principled method for\ncommunity detection that makes clear assumptions and parameter choices and\nyields interpretable results. We demonstrate this model's ability to\nefficiently and accurately find community structure in synthetic bipartite\nnetworks with known structure and in real-world bipartite networks with unknown\nstructure, and we characterize its performance in practical contexts.\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2014 13:54:06 GMT"}, {"version": "v2", "created": "Thu, 10 Jul 2014 21:38:16 GMT"}], "update_date": "2014-07-14", "authors_parsed": [["Larremore", "Daniel B.", ""], ["Clauset", "Aaron", ""], ["Jacobs", "Abigail Z.", ""]]}, {"id": "1403.3028", "submitter": "Gunnar Stefansson", "authors": "Sigrun Helga Lund, Asgeir Sigurdsson, Sigurjon Axel Gudjonsson, Julius\n  Gudmundsson, Daniel Fannar Gudbjartsson, Thorunn Rafnar, Kari Stefansson and\n  Gunnar Stefansson", "title": "Estimating robustness of the tileShuffle method with repeated probes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the TileShuffle method is evaluated as a search method for\ncandidate lncRNAs at 8q24.2. The method is run on three microarrays.\nMicroarrays which all contained the same sample and repeated copies of tiled\nprobes. This allows the coherence of the selection method within and between\nmicroarrays to be estimated by Monte Carlo simulations on the repeated probes.\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2014 16:57:14 GMT"}], "update_date": "2014-03-13", "authors_parsed": [["Lund", "Sigrun Helga", ""], ["Sigurdsson", "Asgeir", ""], ["Gudjonsson", "Sigurjon Axel", ""], ["Gudmundsson", "Julius", ""], ["Gudbjartsson", "Daniel Fannar", ""], ["Rafnar", "Thorunn", ""], ["Stefansson", "Kari", ""], ["Stefansson", "Gunnar", ""]]}, {"id": "1403.3127", "submitter": "David Anderson", "authors": "David F. Anderson and Masanori Koyama", "title": "An asymptotic relationship between coupling methods for stochastically\n  modeled population processes", "comments": "Edited Section 4.2", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA math.PR q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with elucidating a relationship between two common\ncoupling methods for the continuous time Markov chain models utilized in the\ncell biology literature. The couplings considered here are primarily used in a\ncomputational framework by providing reductions in variance for different Monte\nCarlo estimators, thereby allowing for significantly more accurate results for\na fixed amount of computational time. Common applications of the couplings\ninclude the estimation of parametric sensitivities via finite difference\nmethods and the estimation of expectations via multi-level Monte Carlo\nalgorithms. While a number of coupling strategies have been proposed for the\nmodels considered here, and a number of articles have experimentally compared\nthe different strategies, to date there has been no mathematical analysis\ndescribing the connections between them. Such analyses are critical in order to\ndetermine the best use for each. In the current paper, we show a connection\nbetween the common reaction path (CRP) method and the split coupling (SC)\nmethod, which is termed coupled finite differences (CFD) in the parametric\nsensitivities literature. In particular, we show that the two couplings are\nboth limits of a third coupling strategy we call the \"local-CRP\" coupling, with\nthe split coupling method arising as a key parameter goes to infinity, and the\ncommon reaction path coupling arising as the same parameter goes to zero. The\nanalysis helps explain why the split coupling method often provides a lower\nvariance than does the common reaction path method, a fact previously shown\nexperimentally.\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2014 22:40:44 GMT"}, {"version": "v2", "created": "Fri, 1 Aug 2014 21:18:45 GMT"}], "update_date": "2014-08-05", "authors_parsed": [["Anderson", "David F.", ""], ["Koyama", "Masanori", ""]]}, {"id": "1403.3724", "submitter": "William Gray Roncal", "authors": "William Gray Roncal, Michael Pekala, Verena Kaynig-Fittkau, Dean M.\n  Kleissas, Joshua T. Vogelstein, Hanspeter Pfister, Randal Burns, R. Jacob\n  Vogelstein, Mark A. Chevillet, Gregory D. Hager", "title": "VESICLE: Volumetric Evaluation of Synaptic Interfaces using Computer\n  vision at Large Scale", "comments": "v4: added clarifying figures and updates for readability. v3: fixed\n  metadata. 11 pp v2: Added CNN classifier, significant changes to improve\n  performance and generalization", "journal-ref": "Proceedings of the British Machine Vision Conference (BMVC), pages\n  81.1-81.13. BMVA Press, September 2015", "doi": null, "report-no": null, "categories": "cs.CV cs.CE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An open challenge problem at the forefront of modern neuroscience is to\nobtain a comprehensive mapping of the neural pathways that underlie human brain\nfunction; an enhanced understanding of the wiring diagram of the brain promises\nto lead to new breakthroughs in diagnosing and treating neurological disorders.\nInferring brain structure from image data, such as that obtained via electron\nmicroscopy (EM), entails solving the problem of identifying biological\nstructures in large data volumes. Synapses, which are a key communication\nstructure in the brain, are particularly difficult to detect due to their small\nsize and limited contrast. Prior work in automated synapse detection has relied\nupon time-intensive biological preparations (post-staining, isotropic slice\nthicknesses) in order to simplify the problem.\n  This paper presents VESICLE, the first known approach designed for mammalian\nsynapse detection in anisotropic, non-post-stained data. Our methods explicitly\nleverage biological context, and the results exceed existing synapse detection\nmethods in terms of accuracy and scalability. We provide two different\napproaches - one a deep learning classifier (VESICLE-CNN) and one a lightweight\nRandom Forest approach (VESICLE-RF) to offer alternatives in the\nperformance-scalability space. Addressing this synapse detection challenge\nenables the analysis of high-throughput imaging data soon expected to reach\npetabytes of data, and provide tools for more rapid estimation of brain-graphs.\nFinally, to facilitate community efforts, we developed tools for large-scale\nobject detection, and demonstrated this framework to find $\\approx$ 50,000\nsynapses in 60,000 $\\mu m ^3$ (220 GB on disk) of electron microscopy data.\n", "versions": [{"version": "v1", "created": "Fri, 14 Mar 2014 23:16:36 GMT"}, {"version": "v2", "created": "Wed, 13 May 2015 16:53:05 GMT"}, {"version": "v3", "created": "Thu, 14 May 2015 01:01:16 GMT"}, {"version": "v4", "created": "Mon, 7 Sep 2015 21:41:20 GMT"}], "update_date": "2015-09-09", "authors_parsed": [["Roncal", "William Gray", ""], ["Pekala", "Michael", ""], ["Kaynig-Fittkau", "Verena", ""], ["Kleissas", "Dean M.", ""], ["Vogelstein", "Joshua T.", ""], ["Pfister", "Hanspeter", ""], ["Burns", "Randal", ""], ["Vogelstein", "R. Jacob", ""], ["Chevillet", "Mark A.", ""], ["Hager", "Gregory D.", ""]]}, {"id": "1403.3780", "submitter": "Conrad Sanderson", "authors": "Arnold Wiliem, Conrad Sanderson, Yongkang Wong, Peter Hobson, Rodney\n  F. Minchin, Brian C. Lovell", "title": "Automatic Classification of Human Epithelial Type 2 Cell Indirect\n  Immunofluorescence Images using Cell Pyramid Matching", "comments": "arXiv admin note: substantial text overlap with arXiv:1304.1262", "journal-ref": "Pattern Recognition, Vol. 47, No. 7, pp. 2315-2324, 2014", "doi": "10.1016/j.patcog.2013.10.014", "report-no": null, "categories": "q-bio.CB cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a novel system for automatic classification of images\nobtained from Anti-Nuclear Antibody (ANA) pathology tests on Human Epithelial\ntype 2 (HEp-2) cells using the Indirect Immunofluorescence (IIF) protocol. The\nIIF protocol on HEp-2 cells has been the hallmark method to identify the\npresence of ANAs, due to its high sensitivity and the large range of antigens\nthat can be detected. However, it suffers from numerous shortcomings, such as\nbeing subjective as well as time and labour intensive. Computer Aided\nDiagnostic (CAD) systems have been developed to address these problems, which\nautomatically classify a HEp-2 cell image into one of its known patterns (eg.\nspeckled, homogeneous). Most of the existing CAD systems use handpicked\nfeatures to represent a HEp-2 cell image, which may only work in limited\nscenarios. We propose a novel automatic cell image classification method termed\nCell Pyramid Matching (CPM), which is comprised of regional histograms of\nvisual words coupled with the Multiple Kernel Learning framework. We present a\nstudy of several variations of generating histograms and show the efficacy of\nthe system on two publicly available datasets: the ICPR HEp-2 cell\nclassification contest dataset and the SNPHEp-2 dataset.\n", "versions": [{"version": "v1", "created": "Sat, 15 Mar 2014 10:15:25 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Wiliem", "Arnold", ""], ["Sanderson", "Conrad", ""], ["Wong", "Yongkang", ""], ["Hobson", "Peter", ""], ["Minchin", "Rodney F.", ""], ["Lovell", "Brian C.", ""]]}, {"id": "1403.3910", "submitter": "Ralf Metzler", "authors": "Aljaz Godec, Maximilian Bauer, and Ralf Metzler", "title": "Collective dynamics effect transient subdiffusion of inert tracers in\n  gel networks", "comments": "5 pages, 4 figures, RevTeX", "journal-ref": "New J. Phys. 16 092002 (2014)", "doi": "10.1088/1367-2630/16/9/092002", "report-no": null, "categories": "cond-mat.soft physics.bio-ph q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on extensive Brownian dynamics simulations we study the thermally\ndriven motion of a tracer bead in a cross-linked, dynamic gel network in the\nlimit when the tracer bead's size is of the same size or even larger than the\nequilibrium mesh size of the gel. The analysis of long individual trajectories\nof the tracer bead demonstrates the existence of pronounced transient anomalous\ndiffusion, accompanied by a drastic slow-down of the gel-bead relaxation\ndynamics. From the time averaged mean squared displacement and the van Hove\ncross-correlation function we elucidate the many-body origin of the\nnon-Brownian tracer bead dynamics. Our results shed new light on the ongoing\ndebate over the physical origin of sterical tracer interactions with structured\nenvironments.\n", "versions": [{"version": "v1", "created": "Sun, 16 Mar 2014 12:49:22 GMT"}], "update_date": "2014-12-24", "authors_parsed": [["Godec", "Aljaz", ""], ["Bauer", "Maximilian", ""], ["Metzler", "Ralf", ""]]}, {"id": "1403.4033", "submitter": "Markus Pagitz Dr", "authors": "Markus Pagitz and Remco I. Leine", "title": "Continuum Model for Pressure Actuated Cellular Structures", "comments": "11 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cond-mat.soft physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous work introduced a lower-dimensional numerical model for the\ngeometric nonlinear simulation and optimization of compliant pressure actuated\ncellular structures. This model takes into account hinge eccentricities as well\nas rotational and axial cell side springs. The aim of this article is twofold.\nFirst, previous work is extended by introducing an associated continuum model.\nThis model is an exact geometric representation of a cellular structure and the\nbasis for the spring stiffnesses and eccentricities of the numerical model.\nSecond, the state variables of the continuum and numerical model are linked via\ndiscontinuous stress constraints on the one hand and spring stiffness, hinge\neccentricities on the other hand. An efficient optimization algorithm that\nfully couples both sets of variables is presented. The performance of the\nproposed approach is demonstrated with the help of an examples.\n", "versions": [{"version": "v1", "created": "Mon, 17 Mar 2014 09:16:37 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2015 16:15:57 GMT"}, {"version": "v3", "created": "Wed, 5 Aug 2015 12:58:31 GMT"}, {"version": "v4", "created": "Fri, 28 Jul 2017 14:07:13 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Pagitz", "Markus", ""], ["Leine", "Remco I.", ""]]}, {"id": "1403.4086", "submitter": "Hande Topa", "authors": "Hande Topa, \\'Agnes J\\'on\\'as, Robert Kofler, Carolin Kosiol, Antti\n  Honkela", "title": "Gaussian process test for high-throughput sequencing time series:\n  application to experimental evolution", "comments": "41 pages, 29 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE q-bio.GN q-bio.QM stat.AP", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Motivation: Recent advances in high-throughput sequencing (HTS) have made it\npossible to monitor genomes in great detail. New experiments not only use HTS\nto measure genomic features at one time point but to monitor them changing over\ntime with the aim of identifying significant changes in their abundance. In\npopulation genetics, for example, allele frequencies are monitored over time to\ndetect significant frequency changes that indicate selection pressures.\nPrevious attempts at analysing data from HTS experiments have been limited as\nthey could not simultaneously include data at intermediate time points,\nreplicate experiments and sources of uncertainty specific to HTS such as\nsequencing depth.\n  Results: We present the beta-binomial Gaussian process (BBGP) model for\nranking features with significant non-random variation in abundance over time.\nThe features are assumed to represent proportions, such as proportion of an\nalternative allele in a population. We use the beta-binomial model to capture\nthe uncertainty arising from finite sequencing depth and combine it with a\nGaussian process model over the time series. In simulations that mimic the\nfeatures of experimental evolution data, the proposed method clearly\noutperforms classical testing in average precision of finding selected alleles.\nWe also present simulations exploring different experimental design choices and\nresults on real data from Drosophila experimental evolution experiment in\ntemperature adaptation.\n  Availability: R software implementing the test is available at\nhttps://github.com/handetopa/BBGP\n", "versions": [{"version": "v1", "created": "Mon, 17 Mar 2014 13:08:06 GMT"}, {"version": "v2", "created": "Tue, 18 Mar 2014 14:14:25 GMT"}, {"version": "v3", "created": "Thu, 18 Sep 2014 07:59:09 GMT"}], "update_date": "2014-09-19", "authors_parsed": [["Topa", "Hande", ""], ["J\u00f3n\u00e1s", "\u00c1gnes", ""], ["Kofler", "Robert", ""], ["Kosiol", "Carolin", ""], ["Honkela", "Antti", ""]]}, {"id": "1403.4264", "submitter": "Adam Auton", "authors": "Adam Auton, Simon Myers, Gil McVean", "title": "Identifying recombination hotspots using population genetic data", "comments": "3 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Recombination rates vary considerably at the fine scale within\nmammalian genomes, with the majority of recombination occurring within hotspots\nof ~2 kb in width. We present a method for inferring the location of\nrecombination hotspots from patterns of linkage disequilibrium within samples\nof population genetic data. Results: Using simulations, we show that our method\nhas hotspot detection power of approximately 50-60%, but depending on the\nmagnitude of the hotspot. The false positive rate is between 0.24 and 0.56\nfalse positives per Mb for data typical of humans. Availability:\nhttp://github.com/auton1/LDhot\n", "versions": [{"version": "v1", "created": "Mon, 17 Mar 2014 20:17:15 GMT"}], "update_date": "2014-03-19", "authors_parsed": [["Auton", "Adam", ""], ["Myers", "Simon", ""], ["McVean", "Gil", ""]]}, {"id": "1403.4732", "submitter": "Tommi Suvitaival", "authors": "Tommi Suvitaival, Simon Rogers, Samuel Kaski", "title": "Stronger findings from mass spectral data through multi-peak modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.BM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mass spectrometry-based metabolomic analysis depends upon the identification\nof spectral peaks by their mass and retention time. Statistical analysis that\nfollows the identification currently relies on one main peak of each compound.\nHowever, a compound present in the sample typically produces several spectral\npeaks due to its isotopic properties and the ionization process of the mass\nspectrometer device. In this work, we investigate the extent to which these\nadditional peaks can be used to increase the statistical strength of\ndifferential analysis.\n  We present a Bayesian approach for integrating data of multiple detected\npeaks that come from one compound. We demonstrate the approach through a\nsimulated experiment and validate it on ultra performance liquid\nchromatography-mass spectrometry (UPLC-MS) experiments for metabolomics and\nlipidomics. Peaks that are likely to be associated with one compound can be\nclustered by the similarity of their chromatographic shape. Changes of\nconcentration between sample groups can be inferred more accurately when\nmultiple peaks are available.\n  When the sample-size is limited, the proposed multi-peak approach improves\nthe accuracy at inferring covariate effects. An R implementation, data and the\nsupplementary material are available at\nhttp://research.ics.aalto.fi/mi/software/peakANOVA/ .\n", "versions": [{"version": "v1", "created": "Wed, 19 Mar 2014 08:44:18 GMT"}], "update_date": "2014-03-20", "authors_parsed": [["Suvitaival", "Tommi", ""], ["Rogers", "Simon", ""], ["Kaski", "Samuel", ""]]}, {"id": "1403.4987", "submitter": "Pier Francesco Palamara", "authors": "Pier Francesco Palamara", "title": "Population genetics of identity by descent", "comments": "Ph.D. thesis", "journal-ref": null, "doi": "10.7916/D8V122XT", "report-no": null, "categories": "q-bio.PE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent improvements in high-throughput genotyping and sequencing technologies\nhave afforded the collection of massive, genome-wide datasets of DNA\ninformation from hundreds of thousands of individuals. These datasets, in turn,\nprovide unprecedented opportunities to reconstruct the history of human\npopulations and detect genotype-phenotype association. Recently developed\ncomputational methods can identify long-range chromosomal segments that are\nidentical across samples, and have been transmitted from common ancestors that\nlived tens to hundreds of generations in the past. These segments reveal\ngenealogical relationships that are typically unknown to the carrying\nindividuals. In this work, we demonstrate that such identical-by-descent (IBD)\nsegments are informative about a number of relevant population genetics\nfeatures: they enable the inference of details about past population size\nfluctuations, migration events, and they carry the genomic signature of natural\nselection. We derive a mathematical model, based on coalescent theory, that\nallows for a quantitative description of IBD sharing across purportedly\nunrelated individuals, and develop inference procedures for the reconstruction\nof recent demographic events, where classical methodologies are statistically\nunderpowered. We analyze IBD sharing in several contemporary human populations,\nincluding representative communities of the Jewish Diaspora, Kenyan Maasai\nsamples, and individuals from several Dutch provinces, in all cases retrieving\nevidence of fine-scale demographic events from recent history. Finally, we\nexpand the presented model to describe distributions for those sites in IBD\nshared segments that harbor mutation events, showing how these may be used for\nthe inference of mutation rates in humans and other species.\n", "versions": [{"version": "v1", "created": "Wed, 19 Mar 2014 21:37:03 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Palamara", "Pier Francesco", ""]]}, {"id": "1403.5148", "submitter": "H Frost", "authors": "H. Robert Frost, Zhigang Li and Jason H. Moore", "title": "Principal component gene set enrichment (PCGSE)", "comments": null, "journal-ref": "BioData Mining 2015, 8:25", "doi": "10.1186/s13040-015-0059-z", "report-no": null, "categories": "q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Although principal component analysis (PCA) is widely used for\nthe dimensional reduction of biomedical data, interpretation of PCA results\nremains daunting. Most existing methods attempt to explain each principal\ncomponent (PC) in terms of a small number of variables by generating\napproximate PCs with few non-zero loadings. Although useful when just a few\nvariables dominate the population PCs, these methods are often inadequate for\ncharacterizing the PCs of high-dimensional genomic data. For genomic data,\nreproducible and biologically meaningful PC interpretation requires methods\nbased on the combined signal of functionally related sets of genes. While gene\nset testing methods have been widely used in supervised settings to quantify\nthe association of groups of genes with clinical outcomes, these methods have\nseen only limited application for testing the enrichment of gene sets relative\nto sample PCs. Results: We describe a novel approach, principal component gene\nset enrichment (PCGSE), for computing the statistical association between gene\nsets and the PCs of genomic data. The PCGSE method performs a two-stage\ncompetitive gene set test using the correlation between each gene and each PC\nas the gene-level test statistic with flexible choice of both the gene set test\nstatistic and the method used to compute the null distribution of the gene set\nstatistic. Using simulated data with simulated gene sets and real gene\nexpression data with curated gene sets, we demonstrate that biologically\nmeaningful and computationally efficient results can be obtained from a simple\nparametric version of the PCGSE method that performs a correlation-adjusted\ntwo-sample t-test between the gene-level test statistics for gene set members\nand genes not in the set. Availability:\nhttp://cran.r-project.org/web/packages/PCGSE/index.html Contact:\nrob.frost@dartmouth.edu or jason.h.moore@dartmouth.edu\n", "versions": [{"version": "v1", "created": "Thu, 20 Mar 2014 14:37:22 GMT"}], "update_date": "2015-08-24", "authors_parsed": [["Frost", "H. Robert", ""], ["Li", "Zhigang", ""], ["Moore", "Jason H.", ""]]}, {"id": "1403.5156", "submitter": "Daniele Marinazzo", "authors": "Sebastiano Stramaglia, Jesus M. Cortes, Daniele Marinazzo", "title": "Synergy and redundancy in the Granger causal analysis of dynamical\n  networks", "comments": null, "journal-ref": null, "doi": "10.1088/1367-2630/16/10/105003", "report-no": null, "categories": "q-bio.QM cs.IT math.IT physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze by means of Granger causality the effect of synergy and redundancy\nin the inference (from time series data) of the information flow between\nsubsystems of a complex network. Whilst we show that fully conditioned Granger\ncausality is not affected by synergy, the pairwise analysis fails to put in\nevidence synergetic effects.\n  In cases when the number of samples is low, thus making the fully conditioned\napproach unfeasible, we show that partially conditioned Granger causality is an\neffective approach if the set of conditioning variables is properly chosen. We\nconsider here two different strategies (based either on informational content\nfor the candidate driver or on selecting the variables with highest pairwise\ninfluences) for partially conditioned Granger causality and show that depending\non the data structure either one or the other might be valid. On the other\nhand, we observe that fully conditioned approaches do not work well in presence\nof redundancy, thus suggesting the strategy of separating the pairwise links in\ntwo subsets: those corresponding to indirect connections of the fully\nconditioned Granger causality (which should thus be excluded) and links that\ncan be ascribed to redundancy effects and, together with the results from the\nfully connected approach, provide a better description of the causality pattern\nin presence of redundancy. We finally apply these methods to two different real\ndatasets. First, analyzing electrophysiological data from an epileptic brain,\nwe show that synergetic effects are dominant just before seizure occurrences.\nSecond, our analysis applied to gene expression time series from HeLa culture\nshows that the underlying regulatory networks are characterized by both\nredundancy and synergy.\n", "versions": [{"version": "v1", "created": "Thu, 20 Mar 2014 14:49:27 GMT"}, {"version": "v2", "created": "Thu, 31 Jul 2014 22:38:24 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Stramaglia", "Sebastiano", ""], ["Cortes", "Jesus M.", ""], ["Marinazzo", "Daniele", ""]]}, {"id": "1403.5971", "submitter": "James Anderson", "authors": "Aivar Sootla and James Anderson", "title": "On Projection-Based Model Reduction of Biochemical Networks-- Part II:\n  The Stochastic Case", "comments": "Submitted to the 53rd CDC", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.SY q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of model order reduction of stochastic\nbiochemical networks. In particular, we reduce the order of (the number of\nequations in) the Linear Noise Approximation of the Chemical Master Equation,\nwhich is often used to describe biochemical networks. In contrast to other\nbiochemical network reduction methods, the presented one is projection-based.\nProjection-based methods are powerful tools, but the cost of their use is the\nloss of physical interpretation of the nodes in the network. In order alleviate\nthis drawback, we employ structured projectors, which means that some nodes in\nthe network will keep their physical interpretation. For many models in\nengineering, finding structured projectors is not always feasible; however, in\nthe context of biochemical networks it is much more likely as the networks are\noften (almost) monotonic. To summarise, the method can serve as a trade-off\nbetween approximation quality and physical interpretation, which is illustrated\non numerical examples.\n", "versions": [{"version": "v1", "created": "Mon, 24 Mar 2014 14:42:25 GMT"}], "update_date": "2014-03-25", "authors_parsed": [["Sootla", "Aivar", ""], ["Anderson", "James", ""]]}, {"id": "1403.6171", "submitter": "Timothy Phan", "authors": "Timothy S. Phan, John K-J. Li", "title": "Propagation of Uncertainty and Analysis of Signal-to-Noise in Nonlinear\n  Compliance Estimations of an Arterial System Model", "comments": "Conference on Information Sciences and Systems (CISS) 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The arterial system dynamically loads the heart through changes in arterial\ncompliance. The pressure-volume relation of arteries is known to be nonlinear,\nbut arterial compliance is often modeled as a constant value, due to ease of\nestimation and interpretation. Incorporating nonlinear arterial compliance\naffords insight into the continuous variations of arterial compliance in a\ncardiac cycle and its effects on the heart, as the arterial system is coupled\nwith the left ventricle. We recently proposed a method for estimating nonlinear\ncompliance parameters that yielded good results under various vasoactive\nstates. This study examines the performance of the proposed method by\nquantifying the uncertainty of the method in the presence of noise and\npropagating the uncertainty through the system model to analyze its effects on\nmodel predictions. Kernel density estimation used within a bootstrap Monte\nCarlo simulation showed the method to be stable for various vasoactive states.\n", "versions": [{"version": "v1", "created": "Mon, 24 Mar 2014 22:24:57 GMT"}], "update_date": "2014-03-26", "authors_parsed": [["Phan", "Timothy S.", ""], ["Li", "John K-J.", ""]]}, {"id": "1403.6320", "submitter": "Marco Zoli", "authors": "Marco Zoli", "title": "Twisting and Bending Stress in DNA Minicircles", "comments": "Soft Matter (2014), in press", "journal-ref": "Soft Matter vol.10, 4304 (2014)", "doi": "10.1039/C3SM52953C", "report-no": null, "categories": "cond-mat.soft cond-mat.stat-mech q-bio.BM q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interplay between bending of the molecule axis and appearance of\ndisruptions in circular DNA molecules, with $\\sim 100$ base pairs, is\naddressed. Three minicircles with different radii and almost equal content of\nAT and GC pairs are investigated. The DNA sequences are modeled by a mesoscopic\nHamiltonian which describes the essential interactions in the helix at the\nlevel of the base pair and incorporates twisting and bending degrees of\nfreedom. Helix unwinding and bubble formation patterns are consistently\ncomputed by a path integral method that sums over a large number of molecule\nconfigurations compatible with the model potential. The path ensembles are\ndetermined, as a function of temperature, by minimizing the free energy of the\nsystem. Fluctuational openings appear along the helix to release the stress due\nto the bending of the molecule backbone. In agreement with the experimental\nfindings, base pair disruptions are found with larger probability in the\nsmallest minicircle of \\textit{66-bps} whose bending angle is $\\sim 6^{o} $.\nFor this minicircle, a sizeable untwisting is obtained with the helical repeat\nshowing a step-like increase at $\\tau =\\,315K$. The method can be generalized\nto determine the bubble probability profiles of open ends linear sequences.\n", "versions": [{"version": "v1", "created": "Tue, 25 Mar 2014 12:27:20 GMT"}], "update_date": "2014-06-03", "authors_parsed": [["Zoli", "Marco", ""]]}, {"id": "1403.6358", "submitter": "Ariful Azad", "authors": "Ariful Azad, Bartek Rajwa, Alex Pothen", "title": "Immunophenotypes of Acute Myeloid Leukemia From Flow Cytometry Data\n  Using Templates", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CE", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Motivation: We investigate whether a template-based classification pipeline\ncould be used to identify immunophenotypes in (and thereby classify) a\nheterogeneous disease with many subtypes. The disease we consider here is Acute\nMyeloid Leukemia, which is heterogeneous at the morphologic, cytogenetic and\nmolecular levels, with several known subtypes. The prognosis and treatment for\nAML depends on the subtype.\n  Results: We apply flowMatch, an algorithmic pipeline for flow cytometry data\ncreated in earlier work, to compute templates succinctly summarizing classes of\nAML and healthy samples. We develop a scoring function that accounts for\nfeatures of the AML data such as heterogeneity to identify immunophenotypes\ncorresponding to various AML subtypes, including APL. All of the AML samples in\nthe test set are classified correctly with high confidence.\n  Availability: flowMatch is available at\nwww.bioconductor.org/packages/devel/bioc/html/flowMatch.html; programs specific\nto immunophenotyping AML are at www.cs.purdue.edu/homes/aazad/software.html.\n", "versions": [{"version": "v1", "created": "Sat, 22 Mar 2014 02:23:28 GMT"}], "update_date": "2014-03-26", "authors_parsed": [["Azad", "Ariful", ""], ["Rajwa", "Bartek", ""], ["Pothen", "Alex", ""]]}, {"id": "1403.6839", "submitter": "Brian Williams Dr", "authors": "Brian Williams, Eleanor Gouws, David Ginsburg", "title": "Ending AIDS in Gabon: How long will it take? How much will it cost?", "comments": "arXiv admin note: substantial text overlap with arXiv:1311.1815,\n  arXiv:1401.6430", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prevalence of HIV in West Africa is lower than elsewhere in Africa but\nGabon has one of the highest rates of HIV in that region. Gabon has a small\npopulation and a high per capita gross domestic product making it an ideal\nplace to carry out a programme of early treatment for HIV. The effectiveness,\navailability and affordability of triple combination therapy make it possible\nto contemplate ending AIDS deaths and HIV transmission in the short term and\nHIV prevalence in the long term. Here we consider what would have happened in\nGabon without the development of potent anti-retroviral therapy (ART), the\nimpact that the current roll-out of ART has had on HIV, and what might be\npossible if early treatment with ART becomes available to all. We fit a dynamic\ntransmission model to trends in the adult prevalence of HIV and infer trends in\nincidence, mortality and the impact of ART. The availability of ART has reduced\nthe prevalence of HIV among adults not on ART from 4.2% to 2.9%, annual\nincidence from 0.43% to 0.27%, and the proportion of adults dying from AIDS\nillnesses each year from 0.36% to 0.13% saving the lives of 2.3 thousand people\nin 2013 alone. The provision of ART has been highly cost effective saving the\ncountry at least $18 million up to 2013.\n", "versions": [{"version": "v1", "created": "Wed, 26 Mar 2014 20:02:43 GMT"}], "update_date": "2014-03-28", "authors_parsed": [["Williams", "Brian", ""], ["Gouws", "Eleanor", ""], ["Ginsburg", "David", ""]]}, {"id": "1403.7072", "submitter": "Jack Dekker", "authors": "Milton J. Haar and Jack Dekker", "title": "Weedy Adaptation in Setaria spp.: VII. Seed Germination Heteroblasty in\n  Setaria faberi", "comments": "19 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dormancy status of S. faberi seed at abscission was assessed with\nreference to tiller and panicle development. Seed from a single genetic line\nwere grown under field, greenhouse and controlled environment growth chamber\nconditions. At abscission, a small fraction (<10%) of S. faberi seed germinated\nunder favorable conditions. Seed were dissected and germination of caryopses\nand embryos also tested. Removal of seed structures exterior to the embryo\nincreased percentage germination. As the seed rain progressed mean percentage\ngermination and variation among samples increased. Changes in germination were\ncorrelated with tiller development and relative time of seed maturity within a\npanicle. Seed produced on tillers that developed earlier were more likely to be\ndormant than seed from later-developing tillers. Seed that matured later on a\npanicle were more likely to germinate than seed that matured earlier on the\nsame panicle. A consistent trend toward later maturing seed having less\ndormancy was found for seed grown under different environments which implies an\ninherent or parental source for variation in giant foxtail seed dormancy. The\nvariation in percentage germination at abscission and following stratification\ntreatments indicates that the S. faberi seed rain consists of individual seeds,\npossibly each with a different degree of dormancy.\n", "versions": [{"version": "v1", "created": "Thu, 27 Mar 2014 15:13:21 GMT"}], "update_date": "2014-03-28", "authors_parsed": [["Haar", "Milton J.", ""], ["Dekker", "Jack", ""]]}, {"id": "1403.7096", "submitter": "Jack Dekker", "authors": "Milt Haar, Adriaan van Aelst and Jack Dekker", "title": "Weedy Adaptation in Setaria spp.: VIII. Structure of Setaria faberi\n  Seed, Caryopsis and Embryo Germination", "comments": "16 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Giant foxtail (Setaria faberi) seeds differ in requirements for germination.\nVariable germinability arises during seed development under the influence of\ngenotype, environment and parent plant. Giant foxtail seed germination has been\nshown to be regulated by independent asynchronous or dependent synchronous\naction of seed structures. To gain better insight into the process, germination\nwas divided into axis specific embryo growth categories or states. Three states\nwere defined for each embryonic axis. The degree of embryo growth (germination\nstate) after eight to twelve days under germination conditions is believed to\nreveal the germinability state (potential for germination) possessed by the\nseed before germination. The embryo axes behave independently, which allows any\ncombination of germination states to occur. In general, the greater the\ndifference in germination between the axes, the less likely the combination of\nstates will occur. Photographic evidence of each germination state is shown for\ncaryopses and seed. Seed with a variety of germinability states is a strategy\nfor surviving variable environments.\n", "versions": [{"version": "v1", "created": "Thu, 27 Mar 2014 15:46:34 GMT"}], "update_date": "2014-03-28", "authors_parsed": [["Haar", "Milt", ""], ["van Aelst", "Adriaan", ""], ["Dekker", "Jack", ""]]}, {"id": "1403.7104", "submitter": "Brian Williams Dr", "authors": "Brian Williams", "title": "Elimination of HIV in South Africa through expanded access to\n  antiretroviral therapy: Cautions, caveats and the importance of parsimony", "comments": "Two pages. One figure embedded in text", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent article Hontelez and colleagues investigate the prospects for\nelimination of HIV in South Africa through expanded access to antiretroviral\ntherapy (ART) using STDSIM, a micro-simulation model. One of the first\npublished models to suggest that expanded access to ART could lead to the\nelimination of HIV, referred to by the authors as the Granich Model, was\ndeveloped and implemented by the present author. The notion that expanded\naccess to ART could lead to the end of the AIDS epidemic gave rise to\nconsiderable interest and debate and remains contentious. In considering this\nnotion Hontelez et al. start by stripping down STDSIM to a simple model that is\nequivalent to the model developed by the present author3 but is a stochastic\nevent driven model. Hontelez and colleagues then reintroduce levels of\ncomplexity to explore ways in which the model structure affects the results. In\ncontrast to our earlier conclusions Hontelez and colleagues conclude that\nuniversal voluntary counselling and testing with immediate ART at 90% coverage\nshould result in the elimination of HIV but would take three times longer than\npredicted by the model developed by the present author. Hontelez et al. suggest\nthat the current scale-up of ART at CD4 cell counts less than 350 cells/microL\nwill lead to elimination of HIV in 30 years. I disagree with both claims and\nbelieve that their more complex models rely on unwarranted and unsubstantiated\nassumptions.\n", "versions": [{"version": "v1", "created": "Wed, 26 Mar 2014 04:45:57 GMT"}], "update_date": "2014-03-28", "authors_parsed": [["Williams", "Brian", ""]]}, {"id": "1403.7105", "submitter": "Dur-e-Zehta Baig", "authors": "Dur-e-Zehra Baig", "title": "Physiological Control of Human Heart Rate and Oxygen Consumption during\n  Rhythmic Exercises", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physical exercise has significant benefits for humans in improving the health\nand quality of their lives, by improving the functional performance of their\ncardiovascular and respiratory systems. However, it is very important to\ncontrol the workload, e.g. the frequency of body movements, within the\ncapability of the individual to maximise the efficiency of the exercise. The\nworkload is generally represented in terms of heart rate (HR) and oxygen\nconsumption VO2. We focus particularly on the control of HR and VO2 using the\nworkload of an individual body movement, also known as the exercise rate (ER),\nin this research. The first part of this report deals with the modelling and\ncontrol of HR during an unknown type of rhythmic exercise. A novel feature of\nthe developed system is to control HR via manipulating ER as a control input.\nThe relation between ER and HR is modelled using a simple autoregressive model\nwith unknown parameters. The parameters of the model are estimated using a\nKalman filter and an indirect adaptive H1 controller is designed. The\nperformance of the system is tested and validated on six subjects during rowing\nand cycling exercise. The results demonstrate that the designed control system\ncan regulate HR to a predefined profile. The second part of this report deals\nwith the problem of estimating VO2 during rhythmic exercise, as the direct\nmeasurement of VO2 is not realisable in these environments. Therefore,\nnon-invasive sensors are used to measure HR, RespR, and ER to estimate VO2. The\ndeveloped approach for cycling and rowing exercise predicts the percentage\nchange in maximum VO2 from the resting to the exercising phases, using a\nHammerstein model.. Results show that the average quality of fit in both\nexercises is improved as the intensity of exercise is increased.\n", "versions": [{"version": "v1", "created": "Sat, 15 Mar 2014 05:53:19 GMT"}], "update_date": "2014-03-28", "authors_parsed": [["Baig", "Dur-e-Zehra", ""]]}, {"id": "1403.7481", "submitter": "Sebastian Deorowicz", "authors": "Agnieszka Danek, Sebastian Deorowicz, Szymon Grabowski", "title": "Indexing large genome collections on a PC", "comments": null, "journal-ref": "PLOS ONE, Article no.0109384 (2014)", "doi": "10.1371/journal.pone.0109384", "report-no": null, "categories": "cs.CE q-bio.GN q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: The availability of thousands of invidual genomes of one species\nshould boost rapid progress in personalized medicine or understanding of the\ninteraction between genotype and phenotype, to name a few applications. A key\noperation useful in such analyses is aligning sequencing reads against a\ncollection of genomes, which is costly with the use of existing algorithms due\nto their large memory requirements.\n  Results: We present MuGI, Multiple Genome Index, which reports all\noccurrences of a given pattern, in exact and approximate matching model,\nagainst a collection of thousand(s) genomes. Its unique feature is the small\nindex size fitting in a standard computer with 16--32\\,GB, or even 8\\,GB, of\nRAM, for the 1000GP collection of 1092 diploid human genomes. The solution is\nalso fast. For example, the exact matching queries are handled in average time\nof 39\\,$\\mu$s and with up to 3 mismatches in 373\\,$\\mu$s on the test PC with\nthe index size of 13.4\\,GB. For a smaller index, occupying 7.4\\,GB in memory,\nthe respective times grow to 76\\,$\\mu$s and 917\\,$\\mu$s.\n  Availability: Software and Suuplementary material:\n\\url{http://sun.aei.polsl.pl/mugi}.\n", "versions": [{"version": "v1", "created": "Fri, 28 Mar 2014 18:41:39 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Danek", "Agnieszka", ""], ["Deorowicz", "Sebastian", ""], ["Grabowski", "Szymon", ""]]}, {"id": "1403.7660", "submitter": "Ion Udroiu", "authors": "Ion Udroiu", "title": "Estimation of erythrocyte surface area in mammals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measures of erythrocytes volume and surface are helpful in several\nphysiological studies, both for zoologists and veterinarians. Whilst diameter\nand volume are assessed with ease from observations of blood smears and\ncomplete blood count, respectively, thickness and surface area, instead, are\nmuch more difficult to be obtained. The accurate description of the erythrocyte\ngeometry is given by the equation of the oval of Cassini, but the formulas\nderiving from it are very complex, comprising elliptic integrals. In this\narticle three solids are proposed as models approximating the erythrocyte:\nsphere, cylinder and a spheroid with concave caps. Volumes and Surface Areas\nobtained with these models are compared to those effectively measured.\n", "versions": [{"version": "v1", "created": "Sat, 29 Mar 2014 18:40:43 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Udroiu", "Ion", ""]]}, {"id": "1403.7668", "submitter": "William David Pearse", "authors": "William D. Pearse, Andy Purvis, David B. Roy, and Alexandros\n  Stamatakis", "title": "Modelling ecological communities as if they were DNA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ecologists are interested in understanding and predicting how ecological\ncommunities change through time. While it might seem natural to measure this\nthrough changes in species' abundances, computational limitations mean\ntransitions between community types are often modelled instead. We present an\napproach inspired by DNA substitution models that attempts to estimate historic\ninteractions between species, and thus estimate turnover rates in ecological\ncommunities. Although our simulations show that the method has some\nlimitations, our application to butterfly community data shows the method can\ndetect signal in real data. Open source C++ code implementing the method is\navailable at http://www.github.com/willpearse/lotto.\n", "versions": [{"version": "v1", "created": "Sat, 29 Mar 2014 20:48:19 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Pearse", "William D.", ""], ["Purvis", "Andy", ""], ["Roy", "David B.", ""], ["Stamatakis", "Alexandros", ""]]}, {"id": "1403.8057", "submitter": "Iain Johnston", "authors": "Iain G. Johnston", "title": "Efficient parametric inference for stochastic biological systems with\n  measured variability", "comments": "11 pages, 4 figs", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic systems in biology often exhibit substantial variability within\nand between cells. This variability, as well as having dramatic functional\nconsequences, provides information about the underlying details of the system's\nbehaviour. It is often desirable to infer properties of the parameters\ngoverning such systems given experimental observations of the mean and variance\nof observed quantities. In some circumstances, analytic forms for the\nlikelihood of these observations allow very efficient inference: we present\nthese forms and demonstrate their usage. When likelihood functions are\nunavailable or difficult to calculate, we show that an implementation of\napproximate Bayesian computation (ABC) is a powerful tool for parametric\ninference in these systems. However, the calculations required to apply ABC to\nthese systems can also be computationally expensive, relying on repeated\nstochastic simulations. We propose an ABC approach that cheaply eliminates\nunimportant regions of parameter space, by addressing computationally simple\nmean behaviour before explicitly simulating the more computationally demanding\nvariance behaviour. We show that this approach leads to a substantial increase\nin speed when applied to synthetic and experimental datasets.\n", "versions": [{"version": "v1", "created": "Mon, 31 Mar 2014 15:42:01 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2015 15:01:50 GMT"}], "update_date": "2015-11-09", "authors_parsed": [["Johnston", "Iain G.", ""]]}]