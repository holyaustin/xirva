[{"id": "1102.0075", "submitter": "Amit Singer", "authors": "Amit Singer and Hau-tieng Wu", "title": "Vector Diffusion Maps and the Connection Laplacian", "comments": "64 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce {\\em vector diffusion maps} (VDM), a new mathematical framework\nfor organizing and analyzing massive high dimensional data sets, images and\nshapes. VDM is a mathematical and algorithmic generalization of diffusion maps\nand other non-linear dimensionality reduction methods, such as LLE, ISOMAP and\nLaplacian eigenmaps. While existing methods are either directly or indirectly\nrelated to the heat kernel for functions over the data, VDM is based on the\nheat kernel for vector fields. VDM provides tools for organizing complex data\nsets, embedding them in a low dimensional space, and interpolating and\nregressing vector fields over the data. In particular, it equips the data with\na metric, which we refer to as the {\\em vector diffusion distance}. In the\nmanifold learning setup, where the data set is distributed on (or near) a low\ndimensional manifold $\\MM^d$ embedded in $\\RR^{p}$, we prove the relation\nbetween VDM and the connection-Laplacian operator for vector fields over the\nmanifold.\n", "versions": [{"version": "v1", "created": "Tue, 1 Feb 2011 04:29:30 GMT"}], "update_date": "2011-02-02", "authors_parsed": [["Singer", "Amit", ""], ["Wu", "Hau-tieng", ""]]}, {"id": "1102.0844", "submitter": "Ernie Esser", "authors": "Ernie Esser, Michael M\\\"oller, Stanley Osher, Guillermo Sapiro, Jack\n  Xin", "title": "A convex model for non-negative matrix factorization and dimensionality\n  reduction on physical space", "comments": "14 pages, 9 figures. EE and JX were supported by NSF grants\n  {DMS-0911277}, {PRISM-0948247}, MM by the German Academic Exchange Service\n  (DAAD), SO and MM by NSF grants {DMS-0835863}, {DMS-0914561}, {DMS-0914856}\n  and ONR grant {N00014-08-1119}, and GS was supported by NSF, NGA, ONR, ARO,\n  DARPA, and {NSSEFF.}", "journal-ref": null, "doi": "10.1109/TIP.2012.2190081", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A collaborative convex framework for factoring a data matrix $X$ into a\nnon-negative product $AS$, with a sparse coefficient matrix $S$, is proposed.\nWe restrict the columns of the dictionary matrix $A$ to coincide with certain\ncolumns of the data matrix $X$, thereby guaranteeing a physically meaningful\ndictionary and dimensionality reduction. We use $l_{1,\\infty}$ regularization\nto select the dictionary from the data and show this leads to an exact convex\nrelaxation of $l_0$ in the case of distinct noise free data. We also show how\nto relax the restriction-to-$X$ constraint by initializing an alternating\nminimization approach with the solution of the convex model, obtaining a\ndictionary close to but not necessarily in $X$. We focus on applications of the\nproposed framework to hyperspectral endmember and abundances identification and\nalso show an application to blind source separation of NMR data.\n", "versions": [{"version": "v1", "created": "Fri, 4 Feb 2011 07:16:10 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Esser", "Ernie", ""], ["M\u00f6ller", "Michael", ""], ["Osher", "Stanley", ""], ["Sapiro", "Guillermo", ""], ["Xin", "Jack", ""]]}, {"id": "1102.1204", "submitter": "Bala Rajaratnam", "authors": "Alfred O. Hero and Bala Rajaratnam", "title": "Large Scale Correlation Screening", "comments": "33 pages, 7 figures; Changes in version 2: There are no changes in\n  the technical material in this revised version. The only changes are\n  correcting typographical errors and referencing related work in the area.\n  There is also material in the introduction where more context to the\n  correlation screening problem is given (especially in terms of relationships\n  to other testing methods)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper treats the problem of screening for variables with high\ncorrelations in high dimensional data in which there can be many fewer samples\nthan variables. We focus on threshold-based correlation screening methods for\nthree related applications: screening for variables with large correlations\nwithin a single treatment (autocorrelation screening); screening for variables\nwith large cross-correlations over two treatments (cross-correlation\nscreening); screening for variables that have persistently large\nauto-correlations over two treatments (persistent-correlation screening). The\nnovelty of correlation screening is that it identifies a smaller number of\nvariables which are highly correlated with others, as compared to identifying a\nnumber of correlation parameters. Correlation screening suffers from a phase\ntransition phenomenon: as the correlation threshold decreases the number of\ndiscoveries increases abruptly. We obtain asymptotic expressions for the mean\nnumber of discoveries and the phase transition thresholds as a function of the\nnumber of samples, the number of variables, and the joint sample distribution.\nWe also show that under a weak dependency condition the number of discoveries\nis dominated by a Poisson random variable giving an asymptotic expression for\nthe false positive rate. The correlation screening approach bears tremendous\ndividends in terms of the type and strength of the asymptotic results that can\nbe obtained. It also overcomes some of the major hurdles faced by existing\nmethods in the literature as correlation screening is naturally scalable to\nhigh dimension. Numerical results strongly validate the theory that is\npresented in this paper. We illustrate the application of the correlation\nscreening methodology on a large scale gene-expression dataset, revealing a few\ninfluential variables that exhibit a significant amount of correlation over\nmultiple treatments.\n", "versions": [{"version": "v1", "created": "Sun, 6 Feb 2011 21:35:37 GMT"}, {"version": "v2", "created": "Sun, 26 Jun 2011 23:55:30 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Hero", "Alfred O.", ""], ["Rajaratnam", "Bala", ""]]}, {"id": "1102.1465", "submitter": "Adrian Barbu", "authors": "Adrian Barbu, Nathan Lay", "title": "An Introduction to Artificial Prediction Markets for Classification", "comments": "29 pages, 8 figures", "journal-ref": "Journal of Machine Learning Research, 13, 2177-2204, 2012", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction markets are used in real life to predict outcomes of interest such\nas presidential elections. This paper presents a mathematical theory of\nartificial prediction markets for supervised learning of conditional\nprobability estimators. The artificial prediction market is a novel method for\nfusing the prediction information of features or trained classifiers, where the\nfusion result is the contract price on the possible outcomes. The market can be\ntrained online by updating the participants' budgets using training examples.\nInspired by the real prediction markets, the equations that govern the market\nare derived from simple and reasonable assumptions. Efficient numerical\nalgorithms are presented for solving these equations. The obtained artificial\nprediction market is shown to be a maximum likelihood estimator. It generalizes\nlinear aggregation, existent in boosting and random forest, as well as logistic\nregression and some kernel methods. Furthermore, the market mechanism allows\nthe aggregation of specialized classifiers that participate only on specific\ninstances. Experimental comparisons show that the artificial prediction markets\noften outperform random forest and implicit online learning on synthetic data\nand real UCI datasets. Moreover, an extensive evaluation for pelvic and\nabdominal lymph node detection in CT data shows that the prediction market\nimproves adaboost's detection rate from 79.6% to 81.2% at 3 false\npositives/volume.\n", "versions": [{"version": "v1", "created": "Mon, 7 Feb 2011 23:25:47 GMT"}, {"version": "v2", "created": "Wed, 9 Feb 2011 15:48:12 GMT"}, {"version": "v3", "created": "Mon, 14 Feb 2011 21:02:49 GMT"}, {"version": "v4", "created": "Thu, 22 Sep 2011 20:23:30 GMT"}, {"version": "v5", "created": "Sun, 26 Feb 2012 21:54:27 GMT"}, {"version": "v6", "created": "Mon, 9 Jul 2012 19:24:19 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Barbu", "Adrian", ""], ["Lay", "Nathan", ""]]}, {"id": "1102.1492", "submitter": "Jasper Snoek", "authors": "Jasper Snoek and Ryan Prescott Adams and Hugo Larochelle", "title": "On Nonparametric Guidance for Learning Autoencoder Representations", "comments": "9 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised discovery of latent representations, in addition to being useful\nfor density modeling, visualisation and exploratory data analysis, is also\nincreasingly important for learning features relevant to discriminative tasks.\nAutoencoders, in particular, have proven to be an effective way to learn latent\ncodes that reflect meaningful variations in data. A continuing challenge,\nhowever, is guiding an autoencoder toward representations that are useful for\nparticular tasks. A complementary challenge is to find codes that are invariant\nto irrelevant transformations of the data. The most common way of introducing\nsuch problem-specific guidance in autoencoders has been through the\nincorporation of a parametric component that ties the latent representation to\nthe label information. In this work, we argue that a preferable approach relies\ninstead on a nonparametric guidance mechanism. Conceptually, it ensures that\nthere exists a function that can predict the label information, without\nexplicitly instantiating that function. The superiority of this guidance\nmechanism is confirmed on two datasets. In particular, this approach is able to\nincorporate invariance information (lighting, elevation, etc.) from the small\nNORB object recognition dataset and yields state-of-the-art performance for a\nsingle layer, non-convolutional network.\n", "versions": [{"version": "v1", "created": "Tue, 8 Feb 2011 02:33:30 GMT"}, {"version": "v2", "created": "Wed, 9 Feb 2011 19:40:46 GMT"}, {"version": "v3", "created": "Sun, 5 Jun 2011 19:29:12 GMT"}, {"version": "v4", "created": "Wed, 26 Oct 2011 00:08:04 GMT"}], "update_date": "2011-10-27", "authors_parsed": [["Snoek", "Jasper", ""], ["Adams", "Ryan Prescott", ""], ["Larochelle", "Hugo", ""]]}, {"id": "1102.1753", "submitter": "Omar Lizardo A", "authors": "Troy Raeder, Omar Lizardo, David Hachen, Nitesh V. Chawla", "title": "Predictors of short-term decay of cell phone contacts in a large scale\n  communication network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under what conditions is an edge present in a social network at time t likely\nto decay or persist by some future time t + Delta(t)? Previous research\naddressing this issue suggests that the network range of the people involved in\nthe edge, the extent to which the edge is embedded in a surrounding structure,\nand the age of the edge all play a role in edge decay. This paper uses weighted\ndata from a large-scale social network built from cell-phone calls in an 8-week\nperiod to determine the importance of edge weight for the decay/persistence\nprocess. In particular, we study the relative predictive power of directed\nweight, embeddedness, newness, and range (measured as outdegree) with respect\nto edge decay and assess the effectiveness with which a simple decision tree\nand logistic regression classifier can accurately predict whether an edge that\nwas active in one time period continues to be so in a future time period. We\nfind that directed edge weight, weighted reciprocity and time-dependent\nmeasures of edge longevity are highly predictive of whether we classify an edge\nas persistent or decayed, relative to the other types of factors at the dyad\nand neighborhood level.\n", "versions": [{"version": "v1", "created": "Wed, 9 Feb 2011 00:11:28 GMT"}, {"version": "v2", "created": "Fri, 11 Feb 2011 14:12:30 GMT"}], "update_date": "2011-02-14", "authors_parsed": [["Raeder", "Troy", ""], ["Lizardo", "Omar", ""], ["Hachen", "David", ""], ["Chawla", "Nitesh V.", ""]]}, {"id": "1102.2041", "submitter": "Gabor Bartok", "authors": "Andr\\'as Antos, G\\'abor Bart\\'ok, D\\'avid P\\'al, Csaba Szepesv\\'ari", "title": "Toward a Classification of Finite Partial-Monitoring Games", "comments": "Submitted for review to Theoretical Computer Science (Special Issue\n  of the conference Algorithmic Learning Theory 2010)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial-monitoring games constitute a mathematical framework for sequential\ndecision making problems with imperfect feedback: The learner repeatedly\nchooses an action, opponent responds with an outcome, and then the learner\nsuffers a loss and receives a feedback signal, both of which are fixed\nfunctions of the action and the outcome. The goal of the learner is to minimize\nhis total cumulative loss. We make progress towards the classification of these\ngames based on their minimax expected regret. Namely, we classify almost all\ngames with two outcomes and finite number of actions: We show that their\nminimax expected regret is either zero, $\\widetilde{\\Theta}(\\sqrt{T})$,\n$\\Theta(T^{2/3})$, or $\\Theta(T)$ and we give a simple and efficiently\ncomputable classification of these four classes of games. Our hope is that the\nresult can serve as a stepping stone toward classifying all finite\npartial-monitoring games.\n", "versions": [{"version": "v1", "created": "Thu, 10 Feb 2011 06:24:19 GMT"}, {"version": "v2", "created": "Thu, 10 Mar 2011 23:43:21 GMT"}, {"version": "v3", "created": "Tue, 11 Oct 2011 23:52:20 GMT"}], "update_date": "2011-10-13", "authors_parsed": [["Antos", "Andr\u00e1s", ""], ["Bart\u00f3k", "G\u00e1bor", ""], ["P\u00e1l", "D\u00e1vid", ""], ["Szepesv\u00e1ri", "Csaba", ""]]}, {"id": "1102.2075", "submitter": "Ulrike von Luxburg", "authors": "Markus Maier and Ulrike von Luxburg and Matthias Hein", "title": "How the result of graph clustering methods depends on the construction\n  of the graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the scenario of graph-based clustering algorithms such as spectral\nclustering. Given a set of data points, one first has to construct a graph on\nthe data points and then apply a graph clustering algorithm to find a suitable\npartition of the graph. Our main question is if and how the construction of the\ngraph (choice of the graph, choice of parameters, choice of weights) influences\nthe outcome of the final clustering result. To this end we study the\nconvergence of cluster quality measures such as the normalized cut or the\nCheeger cut on various kinds of random geometric graphs as the sample size\ntends to infinity. It turns out that the limit values of the same objective\nfunction are systematically different on different types of graphs. This\nimplies that clustering results systematically depend on the graph and can be\nvery different for different types of graph. We provide examples to illustrate\nthe implications on spectral clustering.\n", "versions": [{"version": "v1", "created": "Thu, 10 Feb 2011 10:44:42 GMT"}], "update_date": "2011-02-11", "authors_parsed": [["Maier", "Markus", ""], ["von Luxburg", "Ulrike", ""], ["Hein", "Matthias", ""]]}, {"id": "1102.2254", "submitter": "Yudong Chen", "authors": "Yudong Chen and Huan Xu and Constantine Caramanis and Sujay Sanghavi", "title": "Matrix completion with column manipulation: Near-optimal\n  sample-robustness-rank tradeoffs", "comments": "This is the journal version of the paper with additional results", "journal-ref": "IEEE Transactions on Information Theory, vol. 62, no. 1, pp.\n  503-526, 2016", "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of matrix completion when some number of the\ncolumns are completely and arbitrarily corrupted, potentially by a malicious\nadversary. It is well-known that standard algorithms for matrix completion can\nreturn arbitrarily poor results, if even a single column is corrupted. One\ndirect application comes from robust collaborative filtering. Here, some number\nof users are so-called manipulators who try to skew the predictions of the\nalgorithm by calibrating their inputs to the system. In this paper, we develop\nan efficient algorithm for this problem based on a combination of a trimming\nprocedure and a convex program that minimizes the nuclear norm and the\n$\\ell_{1,2}$ norm. Our theoretical results show that given a vanishing fraction\nof observed entries, it is nevertheless possible to complete the underlying\nmatrix even when the number of corrupted columns grows. Significantly, our\nresults hold without any assumptions on the locations or values of the observed\nentries of the manipulated columns. Moreover, we show by an\ninformation-theoretic argument that our guarantees are nearly optimal in terms\nof the fraction of sampled entries on the authentic columns, the fraction of\ncorrupted columns, and the rank of the underlying matrix. Our results therefore\nsharply characterize the tradeoffs between sample, robustness and rank in\nmatrix completion.\n", "versions": [{"version": "v1", "created": "Thu, 10 Feb 2011 22:57:47 GMT"}, {"version": "v2", "created": "Sun, 24 Apr 2016 06:12:00 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Chen", "Yudong", ""], ["Xu", "Huan", ""], ["Caramanis", "Constantine", ""], ["Sanghavi", "Sujay", ""]]}, {"id": "1102.2878", "submitter": "Dongryeol Lee", "authors": "Dongryeol Lee, Alexander G. Gray, and Andrew W. Moore", "title": "Dual-Tree Fast Gauss Transforms", "comments": "Extended version of a conference paper. Submitted to a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel density estimation (KDE) is a popular statistical technique for\nestimating the underlying density distribution with minimal assumptions.\nAlthough they can be shown to achieve asymptotic estimation optimality for any\ninput distribution, cross-validating for an optimal parameter requires\nsignificant computation dominated by kernel summations. In this paper we\npresent an improvement to the dual-tree algorithm, the first practical kernel\nsummation algorithm for general dimension. Our extension is based on the\nseries-expansion for the Gaussian kernel used by fast Gauss transform. First,\nwe derive two additional analytical machinery for extending the original\nalgorithm to utilize a hierarchical data structure, demonstrating the first\ntruly hierarchical fast Gauss transform. Second, we show how to integrate the\nseries-expansion approximation within the dual-tree approach to compute kernel\nsummations with a user-controllable relative error bound. We evaluate our\nalgorithm on real-world datasets in the context of optimal bandwidth selection\nin kernel density estimation. Our results demonstrate that our new algorithm is\nthe only one that guarantees a hard relative error bound and offers fast\nperformance across a wide range of bandwidths evaluated in cross validation\nprocedures.\n", "versions": [{"version": "v1", "created": "Mon, 14 Feb 2011 20:24:01 GMT"}], "update_date": "2011-02-15", "authors_parsed": [["Lee", "Dongryeol", ""], ["Gray", "Alexander G.", ""], ["Moore", "Andrew W.", ""]]}, {"id": "1102.3074", "submitter": "Genevera Allen", "authors": "Genevera I. Allen and Logan Grosenick and Jonathan Taylor", "title": "A Generalized Least Squares Matrix Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variables in many massive high-dimensional data sets are structured, arising\nfor example from measurements on a regular grid as in imaging and time series\nor from spatial-temporal measurements as in climate studies. Classical\nmultivariate techniques ignore these structural relationships often resulting\nin poor performance. We propose a generalization of the singular value\ndecomposition (SVD) and principal components analysis (PCA) that is appropriate\nfor massive data sets with structured variables or known two-way dependencies.\nBy finding the best low rank approximation of the data with respect to a\ntransposable quadratic norm, our decomposition, entitled the Generalized least\nsquares Matrix Decomposition (GMD), directly accounts for structural\nrelationships. As many variables in high-dimensional settings are often\nirrelevant or noisy, we also regularize our matrix decomposition by adding\ntwo-way penalties to encourage sparsity or smoothness. We develop fast\ncomputational algorithms using our methods to perform generalized PCA (GPCA),\nsparse GPCA, and functional GPCA on massive data sets. Through simulations and\na whole brain functional MRI example we demonstrate the utility of our\nmethodology for dimension reduction, signal recovery, and feature selection\nwith high-dimensional structured data.\n", "versions": [{"version": "v1", "created": "Tue, 15 Feb 2011 14:02:47 GMT"}, {"version": "v2", "created": "Wed, 9 Mar 2011 16:52:50 GMT"}, {"version": "v3", "created": "Tue, 13 Mar 2012 17:59:55 GMT"}], "update_date": "2012-03-14", "authors_parsed": [["Allen", "Genevera I.", ""], ["Grosenick", "Logan", ""], ["Taylor", "Jonathan", ""]]}, {"id": "1102.3176", "submitter": "Mario Frank", "authors": "Mario Frank and Joachim M. Buhmann", "title": "Selecting the rank of truncated SVD by Maximum Approximation Capacity", "comments": "7 pages, 5 figures; Will be presented at the IEEE International\n  Symposium on Information Theory (ISIT) 2011. The conference version has only\n  5 pages. This version has an extended appendix", "journal-ref": "Information Theory Proceedings (ISIT), 2011 IEEE International\n  Symposium on, 2011, pages 1036-1040", "doi": "10.1109/ISIT.2011.6033687", "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Truncated Singular Value Decomposition (SVD) calculates the closest rank-$k$\napproximation of a given input matrix. Selecting the appropriate rank $k$\ndefines a critical model order choice in most applications of SVD. To obtain a\nprincipled cut-off criterion for the spectrum, we convert the underlying\noptimization problem into a noisy channel coding problem. The optimal\napproximation capacity of this channel controls the appropriate strength of\nregularization to suppress noise. In simulation experiments, this information\ntheoretic method to determine the optimal rank competes with state-of-the art\nmodel selection techniques.\n", "versions": [{"version": "v1", "created": "Tue, 15 Feb 2011 20:49:37 GMT"}, {"version": "v2", "created": "Wed, 9 Mar 2011 13:18:10 GMT"}, {"version": "v3", "created": "Wed, 8 Jun 2011 10:08:51 GMT"}], "update_date": "2013-01-08", "authors_parsed": [["Frank", "Mario", ""], ["Buhmann", "Joachim M.", ""]]}, {"id": "1102.3887", "submitter": "Brian Eriksson", "authors": "Brian Eriksson, Gautam Dasarathy, Aarti Singh, Robert Nowak", "title": "Active Clustering: Robust and Efficient Hierarchical Clustering using\n  Adaptively Selected Similarities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical clustering based on pairwise similarities is a common tool used\nin a broad range of scientific applications. However, in many problems it may\nbe expensive to obtain or compute similarities between the items to be\nclustered. This paper investigates the hierarchical clustering of N items based\non a small subset of pairwise similarities, significantly less than the\ncomplete set of N(N-1)/2 similarities. First, we show that if the intracluster\nsimilarities exceed intercluster similarities, then it is possible to correctly\ndetermine the hierarchical clustering from as few as 3N log N similarities. We\ndemonstrate this order of magnitude savings in the number of pairwise\nsimilarities necessitates sequentially selecting which similarities to obtain\nin an adaptive fashion, rather than picking them at random. We then propose an\nactive clustering method that is robust to a limited fraction of anomalous\nsimilarities, and show how even in the presence of these noisy similarity\nvalues we can resolve the hierarchical clustering using only O(N log^2 N)\npairwise similarities.\n", "versions": [{"version": "v1", "created": "Fri, 18 Feb 2011 19:05:49 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Eriksson", "Brian", ""], ["Dasarathy", "Gautam", ""], ["Singh", "Aarti", ""], ["Nowak", "Robert", ""]]}, {"id": "1102.3923", "submitter": "Rina Foygel", "authors": "Rina Foygel, Nathan Srebro", "title": "Concentration-Based Guarantees for Low-Rank Matrix Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of approximately reconstructing a partially-observed,\napproximately low-rank matrix. This problem has received much attention lately,\nmostly using the trace-norm as a surrogate to the rank. Here we study low-rank\nmatrix reconstruction using both the trace-norm, as well as the less-studied\nmax-norm, and present reconstruction guarantees based on existing analysis on\nthe Rademacher complexity of the unit balls of these norms. We show how these\nare superior in several ways to recently published guarantees based on\nspecialized analysis.\n", "versions": [{"version": "v1", "created": "Fri, 18 Feb 2011 21:26:16 GMT"}, {"version": "v2", "created": "Thu, 26 May 2011 19:26:27 GMT"}], "update_date": "2011-05-27", "authors_parsed": [["Foygel", "Rina", ""], ["Srebro", "Nathan", ""]]}, {"id": "1102.3949", "submitter": "Zhilin Zhang", "authors": "Zhilin Zhang and Bhaskar D. Rao", "title": "Sparse Signal Recovery with Temporally Correlated Source Vectors Using\n  Sparse Bayesian Learning", "comments": "The final version with some typos corrected. Codes can be downloaded\n  at: http://dsp.ucsd.edu/~zhilin/TSBL_code.zip", "journal-ref": "IEEE Journal of Selected Topics in Signal Processing, vol.5, no.\n  5, pp. 912-926, 2011", "doi": "10.1109/JSTSP.2011.2159773", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the sparse signal recovery problem in the context of multiple\nmeasurement vectors (MMV) when elements in each nonzero row of the solution\nmatrix are temporally correlated. Existing algorithms do not consider such\ntemporal correlations and thus their performance degrades significantly with\nthe correlations. In this work, we propose a block sparse Bayesian learning\nframework which models the temporal correlations. In this framework we derive\ntwo sparse Bayesian learning (SBL) algorithms, which have superior recovery\nperformance compared to existing algorithms, especially in the presence of high\ntemporal correlations. Furthermore, our algorithms are better at handling\nhighly underdetermined problems and require less row-sparsity on the solution\nmatrix. We also provide analysis of the global and local minima of their cost\nfunction, and show that the SBL cost function has the very desirable property\nthat the global minimum is at the sparsest solution to the MMV problem.\nExtensive experiments also provide some interesting results that motivate\nfuture theoretical research on the MMV model.\n", "versions": [{"version": "v1", "created": "Sat, 19 Feb 2011 01:41:35 GMT"}, {"version": "v2", "created": "Wed, 17 Aug 2011 00:03:36 GMT"}], "update_date": "2011-08-18", "authors_parsed": [["Zhang", "Zhilin", ""], ["Rao", "Bhaskar D.", ""]]}, {"id": "1102.3975", "submitter": "Abhimanyu Das", "authors": "Abhimanyu Das and David Kempe", "title": "Submodular meets Spectral: Greedy Algorithms for Subset Selection,\n  Sparse Approximation and Dictionary Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of selecting a subset of k random variables from a large\nset, in order to obtain the best linear prediction of another variable of\ninterest. This problem can be viewed in the context of both feature selection\nand sparse approximation. We analyze the performance of widely used greedy\nheuristics, using insights from the maximization of submodular functions and\nspectral analysis. We introduce the submodularity ratio as a key quantity to\nhelp understand why greedy algorithms perform well even when the variables are\nhighly correlated. Using our techniques, we obtain the strongest known\napproximation guarantees for this problem, both in terms of the submodularity\nratio and the smallest k-sparse eigenvalue of the covariance matrix. We further\ndemonstrate the wide applicability of our techniques by analyzing greedy\nalgorithms for the dictionary selection problem, and significantly improve the\npreviously known guarantees. Our theoretical analysis is complemented by\nexperiments on real-world and synthetic data sets; the experiments show that\nthe submodularity ratio is a stronger predictor of the performance of greedy\nalgorithms than other spectral parameters.\n", "versions": [{"version": "v1", "created": "Sat, 19 Feb 2011 07:25:00 GMT"}, {"version": "v2", "created": "Fri, 25 Feb 2011 01:12:26 GMT"}], "update_date": "2011-02-28", "authors_parsed": [["Das", "Abhimanyu", ""], ["Kempe", "David", ""]]}, {"id": "1102.4110", "submitter": "Eric F. Lock", "authors": "Eric F. Lock, Katherine A. Hoadley, J. S. Marron, Andrew B. Nobel", "title": "Joint and individual variation explained (JIVE) for integrated analysis\n  of multiple data types", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS597 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 1, 523-542", "doi": "10.1214/12-AOAS597", "report-no": "IMS-AOAS-AOAS597", "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in several fields now requires the analysis of data sets in which\nmultiple high-dimensional types of data are available for a common set of\nobjects. In particular, The Cancer Genome Atlas (TCGA) includes data from\nseveral diverse genomic technologies on the same cancerous tumor samples. In\nthis paper we introduce Joint and Individual Variation Explained (JIVE), a\ngeneral decomposition of variation for the integrated analysis of such data\nsets. The decomposition consists of three terms: a low-rank approximation\ncapturing joint variation across data types, low-rank approximations for\nstructured variation individual to each data type, and residual noise. JIVE\nquantifies the amount of joint variation between data types, reduces the\ndimensionality of the data and provides new directions for the visual\nexploration of joint and individual structures. The proposed method represents\nan extension of Principal Component Analysis and has clear advantages over\npopular two-block methods such as Canonical Correlation Analysis and Partial\nLeast Squares. A JIVE analysis of gene expression and miRNA data on\nGlioblastoma Multiforme tumor samples reveals gene-miRNA associations and\nprovides better characterization of tumor types. Data and software are\navailable at https://genome.unc.edu/jive/\n", "versions": [{"version": "v1", "created": "Sun, 20 Feb 2011 23:52:20 GMT"}, {"version": "v2", "created": "Tue, 28 May 2013 06:15:04 GMT"}], "update_date": "2013-05-29", "authors_parsed": [["Lock", "Eric F.", ""], ["Hoadley", "Katherine A.", ""], ["Marron", "J. S.", ""], ["Nobel", "Andrew B.", ""]]}, {"id": "1102.4311", "submitter": "Ray Maleh", "authors": "Ray Maleh", "title": "Improved RIP Analysis of Orthogonal Matching Pursuit", "comments": "Submitted to ACHA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.FA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Orthogonal Matching Pursuit (OMP) has long been considered a powerful\nheuristic for attacking compressive sensing problems; however, its theoretical\ndevelopment is, unfortunately, somewhat lacking. This paper presents an\nimproved Restricted Isometry Property (RIP) based performance guarantee for\nT-sparse signal reconstruction that asymptotically approaches the conjectured\nlower bound given in Davenport et al. We also further extend the\nstate-of-the-art by deriving reconstruction error bounds for the case of\ngeneral non-sparse signals subjected to measurement noise. We then generalize\nour results to the case of K-fold Orthogonal Matching Pursuit (KOMP). We finish\nby presenting an empirical analysis suggesting that OMP and KOMP outperform\nother compressive sensing algorithms in average case scenarios. This turns out\nto be quite surprising since RIP analysis (i.e. worst case scenario) suggests\nthat these matching pursuits should perform roughly T^0.5 times worse than\nconvex optimization, CoSAMP, and Iterative Thresholding.\n", "versions": [{"version": "v1", "created": "Mon, 21 Feb 2011 19:19:45 GMT"}], "update_date": "2011-02-22", "authors_parsed": [["Maleh", "Ray", ""]]}, {"id": "1102.4399", "submitter": "Shuichi Kawano", "authors": "Shuichi Kawano and Sadanori Konishi", "title": "Semi-supervised logistic discrimination for functional data", "comments": "21 pages, 7 figures", "journal-ref": "Bulletin of Informatics and Cybernetics 44 (2012) 1-15", "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-class classification methods based on both labeled and unlabeled\nfunctional data sets are discussed. We present a semi-supervised logistic model\nfor classification in the context of functional data analysis. Unknown\nparameters in our proposed model are estimated by regularization with the help\nof EM algorithm. A crucial point in the modeling procedure is the choice of a\nregularization parameter involved in the semi-supervised functional logistic\nmodel. In order to select the adjusted parameter, we introduce model selection\ncriteria from information-theoretic and Bayesian viewpoints. Monte Carlo\nsimulations and a real data analysis are given to examine the effectiveness of\nour proposed modeling strategy.\n", "versions": [{"version": "v1", "created": "Tue, 22 Feb 2011 03:43:24 GMT"}, {"version": "v2", "created": "Tue, 4 Oct 2011 06:31:40 GMT"}, {"version": "v3", "created": "Mon, 28 May 2012 11:05:51 GMT"}], "update_date": "2013-02-15", "authors_parsed": [["Kawano", "Shuichi", ""], ["Konishi", "Sadanori", ""]]}, {"id": "1102.4548", "submitter": "Ricardo Henao", "authors": "Ricardo Henao and Ole Winther", "title": "Predictive Active Set Selection Methods for Gaussian Processes", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an active set selection framework for Gaussian process\nclassification for cases when the dataset is large enough to render its\ninference prohibitive. Our scheme consists of a two step alternating procedure\nof active set update rules and hyperparameter optimization based upon marginal\nlikelihood maximization. The active set update rules rely on the ability of the\npredictive distributions of a Gaussian process classifier to estimate the\nrelative contribution of a datapoint when being either included or removed from\nthe model. This means that we can use it to include points with potentially\nhigh impact to the classifier decision process while removing those that are\nless relevant. We introduce two active set rules based on different criteria,\nthe first one prefers a model with interpretable active set parameters whereas\nthe second puts computational complexity first, thus a model with active set\nparameters that directly control its complexity. We also provide both\ntheoretical and empirical support for our active set selection strategy being a\ngood approximation of a full Gaussian process classifier. Our extensive\nexperiments show that our approach can compete with state-of-the-art\nclassification techniques with reasonable time complexity. Source code publicly\navailable at http://cogsys.imm.dtu.dk/passgp.\n", "versions": [{"version": "v1", "created": "Tue, 22 Feb 2011 16:33:24 GMT"}, {"version": "v2", "created": "Thu, 23 Jun 2011 08:53:36 GMT"}], "update_date": "2011-06-24", "authors_parsed": [["Henao", "Ricardo", ""], ["Winther", "Ole", ""]]}, {"id": "1102.4807", "submitter": "Alekh Agarwal", "authors": "Alekh Agarwal and Sahand N. Negahban and Martin J. Wainwright", "title": "Noisy matrix decomposition via convex relaxation: Optimal rates in high\n  dimensions", "comments": "41 pages, 2 figures", "journal-ref": "Annals of Statistics 2012, Vol. 40, No. 2, 1171-1197", "doi": "10.1214/12-AOS1000", "report-no": "IMS-AOS-AOS1000", "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze a class of estimators based on convex relaxation for solving\nhigh-dimensional matrix decomposition problems. The observations are noisy\nrealizations of a linear transformation $\\mathfrak{X}$ of the sum of an\napproximately) low rank matrix $\\Theta^\\star$ with a second matrix\n$\\Gamma^\\star$ endowed with a complementary form of low-dimensional structure;\nthis set-up includes many statistical models of interest, including factor\nanalysis, multi-task regression, and robust covariance estimation. We derive a\ngeneral theorem that bounds the Frobenius norm error for an estimate of the\npair $(\\Theta^\\star, \\Gamma^\\star)$ obtained by solving a convex optimization\nproblem that combines the nuclear norm with a general decomposable regularizer.\nOur results utilize a \"spikiness\" condition that is related to but milder than\nsingular vector incoherence. We specialize our general result to two cases that\nhave been studied in past work: low rank plus an entrywise sparse matrix, and\nlow rank plus a columnwise sparse matrix. For both models, our theory yields\nnon-asymptotic Frobenius error bounds for both deterministic and stochastic\nnoise matrices, and applies to matrices $\\Theta^\\star$ that can be exactly or\napproximately low rank, and matrices $\\Gamma^\\star$ that can be exactly or\napproximately sparse. Moreover, for the case of stochastic noise matrices and\nthe identity observation operator, we establish matching lower bounds on the\nminimax error. The sharpness of our predictions is confirmed by numerical\nsimulations.\n", "versions": [{"version": "v1", "created": "Wed, 23 Feb 2011 18:02:53 GMT"}, {"version": "v2", "created": "Mon, 18 Jul 2011 02:25:47 GMT"}, {"version": "v3", "created": "Tue, 6 Mar 2012 06:59:59 GMT"}], "update_date": "2012-08-09", "authors_parsed": [["Agarwal", "Alekh", ""], ["Negahban", "Sahand N.", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1102.4816", "submitter": "Mikhail Langovoy", "authors": "Mikhail A. Langovoy and Olaf Wittich", "title": "Computationally efficient algorithms for statistical image processing.\n  Implementation in R", "comments": "This paper initially appeared in 2010 as EURANDOM Report 2010-053.\n  Link to EURANDOM repository:\n  http://www.eurandom.tue.nl/reports/2010/053-report.pdf Link to the abstract\n  at EURANDOM repository:\n  http://www.eurandom.tue.nl/reports/2010/053-abstract.pdf", "journal-ref": null, "doi": null, "report-no": "EURANDOM Report 2010-053", "categories": "stat.CO cs.CV stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the series of our earlier papers on the subject, we proposed a novel\nstatistical hypothesis testing method for detection of objects in noisy images.\nThe method uses results from percolation theory and random graph theory. We\ndeveloped algorithms that allowed to detect objects of unknown shapes in the\npresence of nonparametric noise of unknown level and of unknown distribution.\nNo boundary shape constraints were imposed on the objects, only a weak bulk\ncondition for the object's interior was required. Our algorithms have linear\ncomplexity and exponential accuracy. In the present paper, we describe an\nimplementation of our nonparametric hypothesis testing method. We provide a\nprogram that can be used for statistical experiments in image processing. This\nprogram is written in the statistical programming language R.\n", "versions": [{"version": "v1", "created": "Wed, 23 Feb 2011 18:46:56 GMT"}], "update_date": "2011-02-24", "authors_parsed": [["Langovoy", "Mikhail A.", ""], ["Wittich", "Olaf", ""]]}, {"id": "1102.4820", "submitter": "Mikhail Langovoy", "authors": "Mikhail A. Langovoy and Olaf Wittich", "title": "Multiple testing, uncertainty and realistic pictures", "comments": "This paper initially appeared in January 2011 as EURANDOM Report\n  2011-004. Link to the abstract at EURANDOM Repository:\n  http://www.eurandom.tue.nl/reports/2011/004-abstract.pdf Link to the paper at\n  EURANDOM Repository: http://www.eurandom.tue.nl/reports/2011/004-report.pdf", "journal-ref": null, "doi": null, "report-no": "EURANDOM Report 2011-004", "categories": "math.ST math.PR stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study statistical detection of grayscale objects in noisy images. The\nobject of interest is of unknown shape and has an unknown intensity, that can\nbe varying over the object and can be negative. No boundary shape constraints\nare imposed on the object, only a weak bulk condition for the object's interior\nis required. We propose an algorithm that can be used to detect grayscale\nobjects of unknown shapes in the presence of nonparametric noise of unknown\nlevel. Our algorithm is based on a nonparametric multiple testing procedure. We\nestablish the limit of applicability of our method via an explicit,\nclosed-form, non-asymptotic and nonparametric consistency bound. This bound is\nvalid for a wide class of nonparametric noise distributions. We achieve this by\nproving an uncertainty principle for percolation on finite lattices.\n", "versions": [{"version": "v1", "created": "Wed, 23 Feb 2011 19:05:16 GMT"}], "update_date": "2011-02-24", "authors_parsed": [["Langovoy", "Mikhail A.", ""], ["Wittich", "Olaf", ""]]}, {"id": "1102.5014", "submitter": "Mikhail Langovoy", "authors": "Mikhail A. Langovoy and Olaf Wittich", "title": "Randomized algorithms for statistical image analysis and site\n  percolation on square lattices", "comments": "Submitted for publication on December 11, 2009", "journal-ref": null, "doi": "10.1111/stan.12010", "report-no": null, "categories": "math.ST math.PR stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel probabilistic method for detection of objects in noisy\nimages. The method uses results from percolation and random graph theories. We\npresent an algorithm that allows to detect objects of unknown shapes in the\npresence of random noise. The algorithm has linear complexity and exponential\naccuracy and is appropriate for real-time systems. We prove results on\nconsistency and algorithmic complexity of our procedure.\n", "versions": [{"version": "v1", "created": "Thu, 24 Feb 2011 15:20:41 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Langovoy", "Mikhail A.", ""], ["Wittich", "Olaf", ""]]}, {"id": "1102.5019", "submitter": "Mikhail Langovoy", "authors": "Mikhail A. Langovoy and Olaf Wittich and Patrick Laurie Davies", "title": "Unsupervised nonparametric detection of unknown objects in noisy images\n  based on percolation theory", "comments": "Added references, updated terminology, changed the order of the\n  authors and the title. arXiv admin note: substantial text overlap with\n  arXiv:1102.4803, arXiv:1102.5014", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an unsupervised, nonparametric, and scalable statistical learning\nmethod for detection of unknown objects in noisy images. The method uses\nresults from percolation theory and random graph theory. We present an\nalgorithm that allows to detect objects of unknown shapes and sizes in the\npresence of nonparametric noise of unknown level. The noise density is assumed\nto be unknown and can be very irregular. The algorithm has linear complexity\nand exponential accuracy and is appropriate for real-time systems. We prove\nstrong consistency and scalability of our method in this setup with minimal\nassumptions.\n", "versions": [{"version": "v1", "created": "Thu, 24 Feb 2011 15:37:09 GMT"}, {"version": "v2", "created": "Thu, 12 Jul 2018 13:51:22 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Langovoy", "Mikhail A.", ""], ["Wittich", "Olaf", ""], ["Davies", "Patrick Laurie", ""]]}, {"id": "1102.5171", "submitter": "Remi Monasson", "authors": "Remi Monasson (LPTENS), Simona Cocco (LPS)", "title": "Fast Inference of Interactions in Assemblies of Stochastic\n  Integrate-and-Fire Neurons from Spike Recordings", "comments": "Accepted for publication in J. Comput. Neurosci. (dec 2010)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two Bayesian procedures to infer the interactions and external\ncurrents in an assembly of stochastic integrate-and-fire neurons from the\nrecording of their spiking activity. The first procedure is based on the exact\ncalculation of the most likely time courses of the neuron membrane potentials\nconditioned by the recorded spikes, and is exact for a vanishing noise variance\nand for an instantaneous synaptic integration. The second procedure takes into\naccount the presence of fluctuations around the most likely time courses of the\npotentials, and can deal with moderate noise levels. The running time of both\nprocedures is proportional to the number S of spikes multiplied by the squared\nnumber N of neurons. The algorithms are validated on synthetic data generated\nby networks with known couplings and currents. We also reanalyze previously\npublished recordings of the activity of the salamander retina (including from\n32 to 40 neurons, and from 65,000 to 170,000 spikes). We study the dependence\nof the inferred interactions on the membrane leaking time; the differences and\nsimilarities with the classical cross-correlation analysis are discussed.\n", "versions": [{"version": "v1", "created": "Fri, 25 Feb 2011 07:11:13 GMT"}], "update_date": "2011-02-28", "authors_parsed": [["Monasson", "Remi", "", "LPTENS"], ["Cocco", "Simona", "", "LPS"]]}, {"id": "1102.5288", "submitter": "Derin Babacan", "authors": "S. Derin Babacan, Martin Luessi, Rafael Molina, Aggelos K. Katsaggelos", "title": "Sparse Bayesian Methods for Low-Rank Matrix Estimation", "comments": "This paper has been withdrawn by the author due to significant\n  revisions in the paper. The new version will be uploaded soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SY math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovery of low-rank matrices has recently seen significant activity in many\nareas of science and engineering, motivated by recent theoretical results for\nexact reconstruction guarantees and interesting practical applications. A\nnumber of methods have been developed for this recovery problem. However, a\nprincipled method for choosing the unknown target rank is generally not\nprovided. In this paper, we present novel recovery algorithms for estimating\nlow-rank matrices in matrix completion and robust principal component analysis\nbased on sparse Bayesian learning (SBL) principles. Starting from a matrix\nfactorization formulation and enforcing the low-rank constraint in the\nestimates as a sparsity constraint, we develop an approach that is very\neffective in determining the correct rank while providing high recovery\nperformance. We provide connections with existing methods in other similar\nproblems and empirical results and comparisons with current state-of-the-art\nmethods that illustrate the effectiveness of this approach.\n", "versions": [{"version": "v1", "created": "Fri, 25 Feb 2011 17:13:00 GMT"}, {"version": "v2", "created": "Fri, 9 Sep 2011 19:06:10 GMT"}], "update_date": "2011-09-12", "authors_parsed": [["Babacan", "S. Derin", ""], ["Luessi", "Martin", ""], ["Molina", "Rafael", ""], ["Katsaggelos", "Aggelos K.", ""]]}, {"id": "1102.5509", "submitter": "Leo Lahti", "authors": "Leo Lahti", "title": "Probabilistic analysis of the human transcriptome with side information", "comments": "Doctoral thesis. 103 pages, 11 figures", "journal-ref": "TKK Dissertations in Information and Computer Science TKK-ICS-D19.\n  Aalto University School of Science and Technology, Department of Information\n  and Computer Science, Espoo, Finland, 2010", "doi": null, "report-no": "TKK-ICS-D19", "categories": "stat.ML cs.CE q-bio.GN q-bio.MN q-bio.QM stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Understanding functional organization of genetic information is a major\nchallenge in modern biology. Following the initial publication of the human\ngenome sequence in 2001, advances in high-throughput measurement technologies\nand efficient sharing of research material through community databases have\nopened up new views to the study of living organisms and the structure of life.\nIn this thesis, novel computational strategies have been developed to\ninvestigate a key functional layer of genetic information, the human\ntranscriptome, which regulates the function of living cells through protein\nsynthesis. The key contributions of the thesis are general exploratory tools\nfor high-throughput data analysis that have provided new insights to\ncell-biological networks, cancer mechanisms and other aspects of genome\nfunction.\n  A central challenge in functional genomics is that high-dimensional genomic\nobservations are associated with high levels of complex and largely unknown\nsources of variation. By combining statistical evidence across multiple\nmeasurement sources and the wealth of background information in genomic data\nrepositories it has been possible to solve some the uncertainties associated\nwith individual observations and to identify functional mechanisms that could\nnot be detected based on individual measurement sources. Statistical learning\nand probabilistic models provide a natural framework for such modeling tasks.\nOpen source implementations of the key methodological contributions have been\nreleased to facilitate further adoption of the developed methods by the\nresearch community.\n", "versions": [{"version": "v1", "created": "Sun, 27 Feb 2011 14:11:30 GMT"}], "update_date": "2011-03-01", "authors_parsed": [["Lahti", "Leo", ""]]}, {"id": "1102.5549", "submitter": "Gagan Sidhu", "authors": "Gagan Sidhu", "title": "Instant Replay: Investigating statistical Analysis in Sports", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.AI physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technology has had an unquestionable impact on the way people watch sports.\nAlong with this technological evolution has come a higher standard to ensure a\ngood viewing experience for the casual sports fan. It can be argued that the\npervasion of statistical analysis in sports serves to satiate the fan's desire\nfor detailed sports statistics. The goal of statistical analysis in sports is a\nsimple one: to eliminate subjective analysis. In this paper, we review previous\nwork that attempts to analyze various aspects in sports by using ideas from\nMarkov Chains, Bayesian Inference and Markov Chain Monte Carlo (MCMC) methods.\nThe unifying goal of these works is to achieve an accurate representation of\nthe player's ability, the sport, or the environmental effects on the player's\nperformance. With the prevalence of cheap computation, it is possible that\nusing techniques in Artificial Intelligence could improve the result of\nstatistical analysis in sport. This is best illustrated when evaluating\nfootball using Neuro Dynamic Programming, a Control Theory paradigm heavily\nbased on theory in Stochastic processes. The results from this method suggest\nthat statistical analysis in sports may benefit from using ideas from the area\nof Control Theory or Machine Learning\n", "versions": [{"version": "v1", "created": "Sun, 27 Feb 2011 21:16:56 GMT"}, {"version": "v2", "created": "Wed, 28 Sep 2011 20:40:42 GMT"}, {"version": "v3", "created": "Fri, 30 Sep 2011 16:56:19 GMT"}, {"version": "v4", "created": "Tue, 11 Oct 2011 04:20:03 GMT"}], "update_date": "2011-10-12", "authors_parsed": [["Sidhu", "Gagan", ""]]}, {"id": "1102.5750", "submitter": "Philippe Rigollet", "authors": "Philippe Rigollet and Xin Tong", "title": "Neyman-Pearson classification, convexity and stochastic constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by problems of anomaly detection, this paper implements the\nNeyman-Pearson paradigm to deal with asymmetric errors in binary classification\nwith a convex loss. Given a finite collection of classifiers, we combine them\nand obtain a new classifier that satisfies simultaneously the two following\nproperties with high probability: (i) its probability of type I error is below\na pre-specified level and (ii), it has probability of type II error close to\nthe minimum possible. The proposed classifier is obtained by solving an\noptimization problem with an empirical objective and an empirical constraint.\nNew techniques to handle such problems are developed and have consequences on\nchance constrained programming.\n", "versions": [{"version": "v1", "created": "Mon, 28 Feb 2011 19:31:41 GMT"}], "update_date": "2011-03-01", "authors_parsed": [["Rigollet", "Philippe", ""], ["Tong", "Xin", ""]]}]