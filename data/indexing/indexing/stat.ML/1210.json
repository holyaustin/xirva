[{"id": "1210.0066", "submitter": "Zhaosong Lu", "authors": "Zhaosong Lu", "title": "Iterative Reweighted Minimization Methods for $l_p$ Regularized\n  Unconstrained Nonlinear Programming", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study general $l_p$ regularized unconstrained minimization\nproblems. In particular, we derive lower bounds for nonzero entries of first-\nand second-order stationary points, and hence also of local minimizers of the\n$l_p$ minimization problems. We extend some existing iterative reweighted $l_1$\n(IRL1) and $l_2$ (IRL2) minimization methods to solve these problems and\nproposed new variants for them in which each subproblem has a closed form\nsolution. Also, we provide a unified convergence analysis for these methods. In\naddition, we propose a novel Lipschitz continuous $\\epsilon$-approximation to\n$\\|x\\|^p_p$. Using this result, we develop new IRL1 methods for the $l_p$\nminimization problems and showed that any accumulation point of the sequence\ngenerated by these methods is a first-order stationary point, provided that the\napproximation parameter $\\epsilon$ is below a computable threshold value. This\nis a remarkable result since all existing iterative reweighted minimization\nmethods require that $\\epsilon$ be dynamically updated and approach zero. Our\ncomputational results demonstrate that the new IRL1 method is generally more\nstable than the existing IRL1 methods [21,18] in terms of objective function\nvalue and CPU time.\n", "versions": [{"version": "v1", "created": "Sat, 29 Sep 2012 01:42:57 GMT"}], "update_date": "2012-10-02", "authors_parsed": [["Lu", "Zhaosong", ""]]}, {"id": "1210.0193", "submitter": "Ahmed Farhan Hanif", "authors": "A.F. Hanif, H. Tembine, M. Assaad, D. Zeghlache", "title": "On The Convergence of a Nash Seeking Algorithm with Stochastic State\n  Dependent Payoff", "comments": "20 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.DS math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed strategic learning has been getting attention in recent years. As\nsystems become distributed finding Nash equilibria in a distributed fashion is\nbecoming more important for various applications. In this paper, we develop a\ndistributed strategic learning framework for seeking Nash equilibria under\nstochastic state-dependent payoff functions. We extend the work of Krstic\net.al. in [1] to the case of stochastic state dependent payoff functions. We\ndevelop an iterative distributed algorithm for Nash seeking and examine its\nconvergence to a limiting trajectory defined by an Ordinary Differential\nEquation (ODE). We show convergence of our proposed algorithm for vanishing\nstep size and provide an error bound for fixed step size. Finally, we conduct a\nstability analysis and apply the proposed scheme in a generic wireless\nnetworks. We also present numerical results which corroborate our claim.\n", "versions": [{"version": "v1", "created": "Sun, 30 Sep 2012 13:05:31 GMT"}], "update_date": "2012-10-02", "authors_parsed": [["Hanif", "A. F.", ""], ["Tembine", "H.", ""], ["Assaad", "M.", ""], ["Zeghlache", "D.", ""]]}, {"id": "1210.0563", "submitter": "Tao Hu", "authors": "Tao Hu and Dmitri B. Chklovskii", "title": "Sparse LMS via Online Linearized Bregman Iteration", "comments": "11 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a version of least-mean-square (LMS) algorithm for sparse system\nidentification. Our algorithm called online linearized Bregman iteration (OLBI)\nis derived from minimizing the cumulative prediction error squared along with\nan l1-l2 norm regularizer. By systematically treating the non-differentiable\nregularizer we arrive at a simple two-step iteration. We demonstrate that OLBI\nis bias free and compare its operation with existing sparse LMS algorithms by\nrederiving them in the online convex optimization framework. We perform\nconvergence analysis of OLBI for white input signals and derive theoretical\nexpressions for both the steady state and instantaneous mean square deviations\n(MSD). We demonstrate numerically that OLBI improves the performance of LMS\ntype algorithms for signals generated from sparse tap weights.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2012 20:28:09 GMT"}], "update_date": "2012-10-03", "authors_parsed": [["Hu", "Tao", ""], ["Chklovskii", "Dmitri B.", ""]]}, {"id": "1210.0564", "submitter": "Tao Hu", "authors": "Tao Hu, Juan Nunez-Iglesias, Shiv Vitaladevuni, Lou Scheffer, Shan Xu,\n  Mehdi Bolorizadeh, Harald Hess, Richard Fetter and Dmitri Chklovskii", "title": "Super-resolution using Sparse Representations over Learned Dictionaries:\n  Reconstruction of Brain Structure using Electron Microscopy", "comments": "12 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central problem in neuroscience is reconstructing neuronal circuits on the\nsynapse level. Due to a wide range of scales in brain architecture such\nreconstruction requires imaging that is both high-resolution and\nhigh-throughput. Existing electron microscopy (EM) techniques possess required\nresolution in the lateral plane and either high-throughput or high depth\nresolution but not both. Here, we exploit recent advances in unsupervised\nlearning and signal processing to obtain high depth-resolution EM images\ncomputationally without sacrificing throughput. First, we show that the brain\ntissue can be represented as a sparse linear combination of localized basis\nfunctions that are learned using high-resolution datasets. We then develop\ncompressive sensing-inspired techniques that can reconstruct the brain tissue\nfrom very few (typically 5) tomographic views of each section. This enables\ntracing of neuronal processes and, hence, high throughput reconstruction of\nneural circuits on the level of individual synapses.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2012 20:30:36 GMT"}], "update_date": "2012-10-03", "authors_parsed": [["Hu", "Tao", ""], ["Nunez-Iglesias", "Juan", ""], ["Vitaladevuni", "Shiv", ""], ["Scheffer", "Lou", ""], ["Xu", "Shan", ""], ["Bolorizadeh", "Mehdi", ""], ["Hess", "Harald", ""], ["Fetter", "Richard", ""], ["Chklovskii", "Dmitri", ""]]}, {"id": "1210.0645", "submitter": "Yingzhen Yang", "authors": "Yingzhen Yang, Thomas S. Huang", "title": "Nonparametric Unsupervised Classification", "comments": "Submitted to ALT 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised classification methods learn a discriminative classifier from\nunlabeled data, which has been proven to be an effective way of simultaneously\nclustering the data and training a classifier from the data. Various\nunsupervised classification methods obtain appealing results by the classifiers\nlearned in an unsupervised manner. However, existing methods do not consider\nthe misclassification error of the unsupervised classifiers except unsupervised\nSVM, so the performance of the unsupervised classifiers is not fully evaluated.\nIn this work, we study the misclassification error of two popular classifiers,\ni.e. the nearest neighbor classifier (NN) and the plug-in classifier, in the\nsetting of unsupervised classification.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2012 04:22:50 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2012 00:21:57 GMT"}, {"version": "v3", "created": "Wed, 14 Nov 2012 03:53:04 GMT"}, {"version": "v4", "created": "Mon, 19 Nov 2012 00:40:43 GMT"}, {"version": "v5", "created": "Mon, 20 May 2013 22:11:10 GMT"}], "update_date": "2013-05-23", "authors_parsed": [["Yang", "Yingzhen", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1210.0685", "submitter": "Rodolphe Jenatton", "authors": "Rodolphe Jenatton (CMAP), R\\'emi Gribonval (INRIA - IRISA), Francis\n  Bach (LIENS, INRIA Paris - Rocquencourt)", "title": "Local stability and robustness of sparse dictionary learning in the\n  presence of noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular approach within the signal processing and machine learning\ncommunities consists in modelling signals as sparse linear combinations of\natoms selected from a learned dictionary. While this paradigm has led to\nnumerous empirical successes in various fields ranging from image to audio\nprocessing, there have only been a few theoretical arguments supporting these\nevidences. In particular, sparse coding, or sparse dictionary learning, relies\non a non-convex procedure whose local minima have not been fully analyzed yet.\nIn this paper, we consider a probabilistic model of sparse signals, and show\nthat, with high probability, sparse coding admits a local minimum around the\nreference dictionary generating the signals. Our study takes into account the\ncase of over-complete dictionaries and noisy signals, thus extending previous\nwork limited to noiseless settings and/or under-complete dictionaries. The\nanalysis we conduct is non-asymptotic and makes it possible to understand how\nthe key quantities of the problem, such as the coherence or the level of noise,\ncan scale with respect to the dimension of the signals, the number of atoms,\nthe sparsity and the number of observations.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2012 07:48:08 GMT"}], "update_date": "2012-10-03", "authors_parsed": [["Jenatton", "Rodolphe", "", "CMAP"], ["Gribonval", "R\u00e9mi", "", "INRIA - IRISA"], ["Bach", "Francis", "", "LIENS, INRIA Paris - Rocquencourt"]]}, {"id": "1210.0734", "submitter": "Artemy Kolchinsky", "authors": "Artemy Kolchinsky, An\\'alia Louren\\c{c}o, Lang Li, Luis M. Rocha", "title": "Evaluation of linear classifiers on articles containing pharmacokinetic\n  evidence of drug-drug interactions", "comments": "Pacific Symposium on Biocomputing, 2013", "journal-ref": "Pac Symp Biocomput. 2013:409-20", "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background. Drug-drug interaction (DDI) is a major cause of morbidity and\nmortality. [...] Biomedical literature mining can aid DDI research by\nextracting relevant DDI signals from either the published literature or large\nclinical databases. However, though drug interaction is an ideal area for\ntranslational research, the inclusion of literature mining methodologies in DDI\nworkflows is still very preliminary. One area that can benefit from literature\nmining is the automatic identification of a large number of potential DDIs,\nwhose pharmacological mechanisms and clinical significance can then be studied\nvia in vitro pharmacology and in populo pharmaco-epidemiology. Experiments. We\nimplemented a set of classifiers for identifying published articles relevant to\nexperimental pharmacokinetic DDI evidence. These documents are important for\nidentifying causal mechanisms behind putative drug-drug interactions, an\nimportant step in the extraction of large numbers of potential DDIs. We\nevaluate performance of several linear classifiers on PubMed abstracts, under\ndifferent feature transformation and dimensionality reduction methods. In\naddition, we investigate the performance benefits of including various\npublicly-available named entity recognition features, as well as a set of\ninternally-developed pharmacokinetic dictionaries. Results. We found that\nseveral classifiers performed well in distinguishing relevant and irrelevant\nabstracts. We found that the combination of unigram and bigram textual features\ngave better performance than unigram features alone, and also that\nnormalization transforms that adjusted for feature frequency and document\nlength improved classification. For some classifiers, such as linear\ndiscriminant analysis (LDA), proper dimensionality reduction had a large impact\non performance. Finally, the inclusion of NER features and dictionaries was\nfound not to help classification.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2012 11:34:57 GMT"}], "update_date": "2013-03-05", "authors_parsed": [["Kolchinsky", "Artemy", ""], ["Louren\u00e7o", "An\u00e1lia", ""], ["Li", "Lang", ""], ["Rocha", "Luis M.", ""]]}, {"id": "1210.0758", "submitter": "Daniele Cerra", "authors": "Daniele Cerra and Mihai Datcu", "title": "A fast compression-based similarity measure with applications to\n  content-based image retrieval", "comments": "Pre-print", "journal-ref": "Journal of Visual Communication and Image Representation, vol. 23,\n  no. 2, pp. 293-302, 2012", "doi": "10.1016/j.jvcir.2011.10.009", "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compression-based similarity measures are effectively employed in\napplications on diverse data types with a basically parameter-free approach.\nNevertheless, there are problems in applying these techniques to\nmedium-to-large datasets which have been seldom addressed. This paper proposes\na similarity measure based on compression with dictionaries, the Fast\nCompression Distance (FCD), which reduces the complexity of these methods,\nwithout degradations in performance. On its basis a content-based color image\nretrieval system is defined, which can be compared to state-of-the-art methods\nbased on invariant color features. Through the FCD a better understanding of\ncompression-based techniques is achieved, by performing experiments on datasets\nwhich are larger than the ones analyzed so far in literature.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2012 13:04:49 GMT"}], "update_date": "2012-10-03", "authors_parsed": [["Cerra", "Daniele", ""], ["Datcu", "Mihai", ""]]}, {"id": "1210.0762", "submitter": "Fabrice Rossi", "authors": "Mohamed Khalil El Mahrsi (LTCI), Fabrice Rossi (SAMM)", "title": "Graph-Based Approaches to Clustering Network-Constrained Trajectory Data", "comments": null, "journal-ref": "Workshop on New Frontiers in Mining Complex Patterns (NFMCP 2012),\n  held at ECML-PKDD 2012, Bristol : United Kingdom (2012)", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though clustering trajectory data attracted considerable attention in\nthe last few years, most of prior work assumed that moving objects can move\nfreely in an euclidean space and did not consider the eventual presence of an\nunderlying road network and its influence on evaluating the similarity between\ntrajectories. In this paper, we present two approaches to clustering\nnetwork-constrained trajectory data. The first approach discovers clusters of\ntrajectories that traveled along the same parts of the road network. The second\napproach is segment-oriented and aims to group together road segments based on\ntrajectories that they have in common. Both approaches use a graph model to\ndepict the interactions between observations w.r.t. their similarity and\ncluster this similarity graph using a community detection algorithm. We also\npresent experimental results obtained on synthetic data to showcase our\npropositions.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2012 13:17:33 GMT"}], "update_date": "2012-10-04", "authors_parsed": [["Mahrsi", "Mohamed Khalil El", "", "LTCI"], ["Rossi", "Fabrice", "", "SAMM"]]}, {"id": "1210.0805", "submitter": "Clemens Hage", "authors": "Clemens Hage and Martin Kleinsteuber", "title": "Robust PCA and subspace tracking from incomplete observations using\n  L0-surrogates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications in data analysis rely on the decomposition of a data matrix\ninto a low-rank and a sparse component. Existing methods that tackle this task\nuse the nuclear norm and L1-cost functions as convex relaxations of the rank\nconstraint and the sparsity measure, respectively, or employ thresholding\ntechniques. We propose a method that allows for reconstructing and tracking a\nsubspace of upper-bounded dimension from incomplete and corrupted observations.\nIt does not require any a priori information about the number of outliers. The\ncore of our algorithm is an intrinsic Conjugate Gradient method on the set of\northogonal projection matrices, the so-called Grassmannian. Non-convex sparsity\nmeasures are used for outlier detection, which leads to improved performance in\nterms of robustly recovering and tracking the low-rank matrix. In particular,\nour approach can cope with more outliers and with an underlying matrix of\nhigher rank than other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2012 15:26:15 GMT"}, {"version": "v2", "created": "Thu, 17 Jan 2013 14:20:38 GMT"}], "update_date": "2013-01-18", "authors_parsed": [["Hage", "Clemens", ""], ["Kleinsteuber", "Martin", ""]]}, {"id": "1210.0824", "submitter": "Zoltan Szabo", "authors": "Zoltan Szabo, Andras Lorincz", "title": "Distributed High Dimensional Information Theoretical Image Registration\n  via Random Projections", "comments": null, "journal-ref": "Zoltan Szabo, Andras Lorincz: Distributed High Dimensional\n  Information Theoretical Image Registration via Random Projections. Digital\n  Signal Processing, 22(6):894-902, 2012", "doi": "10.1016/j.dsp.2012.04.018", "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information theoretical measures, such as entropy, mutual information, and\nvarious divergences, exhibit robust characteristics in image registration\napplications. However, the estimation of these quantities is computationally\nintensive in high dimensions. On the other hand, consistent estimation from\npairwise distances of the sample points is possible, which suits random\nprojection (RP) based low dimensional embeddings. We adapt the RP technique to\nthis task by means of a simple ensemble method. To the best of our knowledge,\nthis is the first distributed, RP based information theoretical image\nregistration approach. The efficiency of the method is demonstrated through\nnumerical examples.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2012 16:08:53 GMT"}], "update_date": "2012-10-03", "authors_parsed": [["Szabo", "Zoltan", ""], ["Lorincz", "Andras", ""]]}, {"id": "1210.1048", "submitter": "Roger Guimera", "authors": "Roger Guimera, Alejandro Llorente, Esteban Moro, Marta Sales-Pardo", "title": "Predicting human preferences using the block structure of complex social\n  networks", "comments": null, "journal-ref": "PLOS ONE 7(9): e44620 (2012)", "doi": "10.1371/journal.pone.0044620", "report-no": null, "categories": "physics.soc-ph cs.SI physics.data-an stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  With ever-increasing available data, predicting individuals' preferences and\nhelping them locate the most relevant information has become a pressing need.\nUnderstanding and predicting preferences is also important from a fundamental\npoint of view, as part of what has been called a \"new\" computational social\nscience. Here, we propose a novel approach based on stochastic block models,\nwhich have been developed by sociologists as plausible models of complex\nnetworks of social interactions. Our model is in the spirit of predicting\nindividuals' preferences based on the preferences of others but, rather than\nfitting a particular model, we rely on a Bayesian approach that samples over\nthe ensemble of all possible models. We show that our approach is considerably\nmore accurate than leading recommender algorithms, with major relative\nimprovements between 38% and 99% over industry-level algorithms. Besides, our\napproach sheds light on decision-making processes by identifying groups of\nindividuals that have consistently similar preferences, and enabling the\nanalysis of the characteristics of those groups.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2012 09:40:36 GMT"}], "update_date": "2012-10-05", "authors_parsed": [["Guimera", "Roger", ""], ["Llorente", "Alejandro", ""], ["Moro", "Esteban", ""], ["Sales-Pardo", "Marta", ""]]}, {"id": "1210.1121", "submitter": "Krishnakumar Balasubramanian", "authors": "Krishnakumar Balasubramanian, Kai Yu, Guy Lebanon", "title": "Smooth Sparse Coding via Marginal Regression for Learning Sparse\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and analyze a novel framework for learning sparse representations,\nbased on two statistical techniques: kernel smoothing and marginal regression.\nThe proposed approach provides a flexible framework for incorporating feature\nsimilarity or temporal information present in data sets, via non-parametric\nkernel smoothing. We provide generalization bounds for dictionary learning\nusing smooth sparse coding and show how the sample complexity depends on the L1\nnorm of kernel function used. Furthermore, we propose using marginal regression\nfor obtaining sparse codes, which significantly improves the speed and allows\none to scale to large dictionary sizes easily. We demonstrate the advantages of\nthe proposed approach, both in terms of accuracy and speed by extensive\nexperimentation on several real data sets. In addition, we demonstrate how the\nproposed approach could be used for improving semi-supervised sparse coding.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2012 14:26:59 GMT"}], "update_date": "2012-10-04", "authors_parsed": [["Balasubramanian", "Krishnakumar", ""], ["Yu", "Kai", ""], ["Lebanon", "Guy", ""]]}, {"id": "1210.1190", "submitter": "Abhishek Kumar", "authors": "Abhishek Kumar, Vikas Sindhwani, Prabhanjan Kambadur", "title": "Fast Conical Hull Algorithms for Near-separable Non-negative Matrix\n  Factorization", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The separability assumption (Donoho & Stodden, 2003; Arora et al., 2012)\nturns non-negative matrix factorization (NMF) into a tractable problem.\nRecently, a new class of provably-correct NMF algorithms have emerged under\nthis assumption. In this paper, we reformulate the separable NMF problem as\nthat of finding the extreme rays of the conical hull of a finite set of\nvectors. From this geometric perspective, we derive new separable NMF\nalgorithms that are highly scalable and empirically noise robust, and have\nseveral other favorable properties in relation to existing methods. A parallel\nimplementation of our algorithm demonstrates high scalability on shared- and\ndistributed-memory machines.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2012 18:37:47 GMT"}], "update_date": "2012-10-04", "authors_parsed": [["Kumar", "Abhishek", ""], ["Sindhwani", "Vikas", ""], ["Kambadur", "Prabhanjan", ""]]}, {"id": "1210.1258", "submitter": "Mariya Ishteva", "authors": "Mariya Ishteva, Haesun Park, Le Song", "title": "Unfolding Latent Tree Structures using 4th Order Tensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering the latent structure from many observed variables is an important\nyet challenging learning task. Existing approaches for discovering latent\nstructures often require the unknown number of hidden states as an input. In\nthis paper, we propose a quartet based approach which is \\emph{agnostic} to\nthis number. The key contribution is a novel rank characterization of the\ntensor associated with the marginal distribution of a quartet. This\ncharacterization allows us to design a \\emph{nuclear norm} based test for\nresolving quartet relations. We then use the quartet test as a subroutine in a\ndivide-and-conquer algorithm for recovering the latent tree structure. Under\nmild conditions, the algorithm is consistent and its error probability decays\nexponentially with increasing sample size. We demonstrate that the proposed\napproach compares favorably to alternatives. In a real world stock dataset, it\nalso discovers meaningful groupings of variables, and produces a model that\nfits the data better.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2012 23:30:24 GMT"}], "update_date": "2012-10-05", "authors_parsed": [["Ishteva", "Mariya", ""], ["Park", "Haesun", ""], ["Song", "Le", ""]]}, {"id": "1210.1461", "submitter": "Shusen Wang", "authors": "Shusen Wang, Zhihua Zhang, Jian Li", "title": "A Scalable CUR Matrix Decomposition Algorithm: Lower Time Complexity and\n  Tighter Bound", "comments": "accepted by NIPS 2012", "journal-ref": "Shusen Wang and Zhihua Zhang. A Scalable CUR Matrix Decomposition\n  Algorithm: Lower Time Complexity and Tighter Bound. In Advances in Neural\n  Information Processing Systems 25, 2012", "doi": null, "report-no": null, "categories": "cs.LG cs.DM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The CUR matrix decomposition is an important extension of Nystr\\\"{o}m\napproximation to a general matrix. It approximates any data matrix in terms of\na small number of its columns and rows. In this paper we propose a novel\nrandomized CUR algorithm with an expected relative-error bound. The proposed\nalgorithm has the advantages over the existing relative-error CUR algorithms\nthat it possesses tighter theoretical bound and lower time complexity, and that\nit can avoid maintaining the whole data matrix in main memory. Finally,\nexperiments on several real-world datasets demonstrate significant improvement\nover the existing relative-error algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2012 14:23:34 GMT"}], "update_date": "2012-10-05", "authors_parsed": [["Wang", "Shusen", ""], ["Zhang", "Zhihua", ""], ["Li", "Jian", ""]]}, {"id": "1210.1766", "submitter": "Jun Zhu", "authors": "Jun Zhu, Ning Chen, and Eric P. Xing", "title": "Bayesian Inference with Posterior Regularization and applications to\n  Infinite Latent SVMs", "comments": "49 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing Bayesian models, especially nonparametric Bayesian methods, rely on\nspecially conceived priors to incorporate domain knowledge for discovering\nimproved latent representations. While priors can affect posterior\ndistributions through Bayes' rule, imposing posterior regularization is\narguably more direct and in some cases more natural and general. In this paper,\nwe present regularized Bayesian inference (RegBayes), a novel computational\nframework that performs posterior inference with a regularization term on the\ndesired post-data posterior distribution under an information theoretical\nformulation. RegBayes is more flexible than the procedure that elicits expert\nknowledge via priors, and it covers both directed Bayesian networks and\nundirected Markov networks whose Bayesian formulation results in hybrid chain\ngraph models. When the regularization is induced from a linear operator on the\nposterior distributions, such as the expectation operator, we present a general\nconvex-analysis theorem to characterize the solution of RegBayes. Furthermore,\nwe present two concrete examples of RegBayes, infinite latent support vector\nmachines (iLSVM) and multi-task infinite latent support vector machines\n(MT-iLSVM), which explore the large-margin idea in combination with a\nnonparametric Bayesian model for discovering predictive latent features for\nclassification and multi-task learning, respectively. We present efficient\ninference methods and report empirical studies on several benchmark datasets,\nwhich appear to demonstrate the merits inherited from both large-margin\nlearning and Bayesian nonparametrics. Such results were not available until\nnow, and contribute to push forward the interface between these two important\nsubfields, which have been largely treated as isolated in the community.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2012 14:10:20 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2013 09:33:44 GMT"}, {"version": "v3", "created": "Wed, 12 Feb 2014 06:31:12 GMT"}], "update_date": "2014-02-13", "authors_parsed": [["Zhu", "Jun", ""], ["Chen", "Ning", ""], ["Xing", "Eric P.", ""]]}, {"id": "1210.1928", "submitter": "Shrihari Vasudevan", "authors": "Shrihari Vasudevan and Arman Melkumyan and Steven Scheding", "title": "Information fusion in multi-task Gaussian processes", "comments": "53 pages, 33 figures; improved presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper evaluates heterogeneous information fusion using multi-task\nGaussian processes in the context of geological resource modeling.\nSpecifically, it empirically demonstrates that information integration across\nheterogeneous information sources leads to superior estimates of all the\nquantities being modeled, compared to modeling them individually. Multi-task\nGaussian processes provide a powerful approach for simultaneous modeling of\nmultiple quantities of interest while taking correlations between these\nquantities into consideration. Experiments are performed on large scale real\nsensor data.\n", "versions": [{"version": "v1", "created": "Sat, 6 Oct 2012 08:11:01 GMT"}, {"version": "v2", "created": "Mon, 15 Oct 2012 04:25:31 GMT"}, {"version": "v3", "created": "Thu, 5 Sep 2013 03:42:50 GMT"}], "update_date": "2013-09-06", "authors_parsed": [["Vasudevan", "Shrihari", ""], ["Melkumyan", "Arman", ""], ["Scheding", "Steven", ""]]}, {"id": "1210.1960", "submitter": "Wittawat Jitkrittum", "authors": "Wittawat Jitkrittum, Hirotaka Hachiya, Masashi Sugiyama", "title": "Feature Selection via L1-Penalized Squared-Loss Mutual Information", "comments": "25 pages", "journal-ref": null, "doi": "10.1587/transinf.E96.D.1513", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection is a technique to screen out less important features. Many\nexisting supervised feature selection algorithms use redundancy and relevancy\nas the main criteria to select features. However, feature interaction,\npotentially a key characteristic in real-world problems, has not received much\nattention. As an attempt to take feature interaction into account, we propose\nL1-LSMI, an L1-regularization based algorithm that maximizes a squared-loss\nvariant of mutual information between selected features and outputs. Numerical\nresults show that L1-LSMI performs well in handling redundancy, detecting\nnon-linear dependency, and considering feature interaction.\n", "versions": [{"version": "v1", "created": "Sat, 6 Oct 2012 14:16:33 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Jitkrittum", "Wittawat", ""], ["Hachiya", "Hirotaka", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1210.2022", "submitter": "Bruno Scarpa", "authors": "Daniele Durante, Bruno Scarpa and David B. Dunson", "title": "Locally adaptive factor processes for multivariate time series", "comments": null, "journal-ref": "Journal of Machine Learning Research (2014), 15: 1493-1522.\n  http://jmlr.org/papers/v15/durante14a.html", "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modeling multivariate time series, it is important to allow time-varying\nsmoothness in the mean and covariance process. In particular, there may be\ncertain time intervals exhibiting rapid changes and others in which changes are\nslow. If such time-varying smoothness is not accounted for, one can obtain\nmisleading inferences and predictions, with over-smoothing across erratic time\nintervals and under-smoothing across times exhibiting slow variation. This can\nlead to mis-calibration of predictive intervals, which can be substantially too\nnarrow or wide depending on the time. We propose a locally adaptive factor\nprocess for characterizing multivariate mean-covariance changes in continuous\ntime, allowing locally varying smoothness in both the mean and covariance\nmatrix. This process is constructed utilizing latent dictionary functions\nevolving in time through nested Gaussian processes and linearly related to the\nobserved data with a sparse mapping. Using a differential equation\nrepresentation, we bypass usual computational bottlenecks in obtaining MCMC and\nonline algorithms for approximate Bayesian inference. The performance is\nassessed in simulations and illustrated in a financial application.\n", "versions": [{"version": "v1", "created": "Sun, 7 Oct 2012 05:40:25 GMT"}, {"version": "v2", "created": "Fri, 21 Jun 2013 07:39:35 GMT"}], "update_date": "2014-06-02", "authors_parsed": [["Durante", "Daniele", ""], ["Scarpa", "Bruno", ""], ["Dunson", "David B.", ""]]}, {"id": "1210.2077", "submitter": "Julien  Chiquet Dr.", "authors": "Yves Grandvalet, Julien Chiquet and Christophe Ambroise", "title": "Sparsity by Worst-Case Penalties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new interpretation of sparse penalties such as the\nelastic-net and the group-lasso. Beyond providing a new viewpoint on these\npenalization schemes, our approach results in a unified optimization strategy.\nOur experiments demonstrate that this strategy, implemented on the elastic-net,\nis computationally extremely efficient for small to medium size problems. Our\naccompanying software solves problems very accurately, at machine precision, in\nthe time required to get a rough estimate with competing state-of-the-art\nalgorithms. We illustrate on real and artificial datasets that this accuracy is\nrequired to for the correctness of the support of the solution, which is an\nimportant element for the interpretability of sparsity-inducing penalties.\n", "versions": [{"version": "v1", "created": "Sun, 7 Oct 2012 17:06:45 GMT"}, {"version": "v2", "created": "Wed, 19 Jul 2017 16:24:52 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Grandvalet", "Yves", ""], ["Chiquet", "Julien", ""], ["Ambroise", "Christophe", ""]]}, {"id": "1210.2085", "submitter": "John Duchi", "authors": "John C. Duchi and Michael I. Jordan and Martin J. Wainwright", "title": "Privacy Aware Learning", "comments": "60 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study statistical risk minimization problems under a privacy model in\nwhich the data is kept confidential even from the learner. In this local\nprivacy framework, we establish sharp upper and lower bounds on the convergence\nrates of statistical estimation procedures. As a consequence, we exhibit a\nprecise tradeoff between the amount of privacy the data preserves and the\nutility, as measured by convergence rate, of any statistical estimator or\nlearning procedure.\n", "versions": [{"version": "v1", "created": "Sun, 7 Oct 2012 18:27:03 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2013 17:53:36 GMT"}], "update_date": "2013-10-11", "authors_parsed": [["Duchi", "John C.", ""], ["Jordan", "Michael I.", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1210.2289", "submitter": "Annie I-An Chen", "authors": "Annie I. Chen and Asuman Ozdaglar", "title": "A Fast Distributed Proximal-Gradient Method", "comments": "10 pages (including 2-page appendix); 1 figure; submitted to Allerton\n  2012 on July 10, 2012; accepted by Allerton 2012, October 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a distributed proximal-gradient method for optimizing the average\nof convex functions, each of which is the private local objective of an agent\nin a network with time-varying topology. The local objectives have distinct\ndifferentiable components, but they share a common nondifferentiable component,\nwhich has a favorable structure suitable for effective computation of the\nproximal operator. In our method, each agent iteratively updates its estimate\nof the global minimum by optimizing its local objective function, and\nexchanging estimates with others via communication in the network. Using\nNesterov-type acceleration techniques and multiple communication steps per\niteration, we show that this method converges at the rate 1/k (where k is the\nnumber of communication rounds between the agents), which is faster than the\nconvergence rate of the existing distributed methods for solving this problem.\nThe superior convergence rate of our method is also verified by numerical\nexperiments.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2012 14:14:13 GMT"}], "update_date": "2012-10-09", "authors_parsed": [["Chen", "Annie I.", ""], ["Ozdaglar", "Asuman", ""]]}, {"id": "1210.2294", "submitter": "Ccsd", "authors": "Guillaume Allain (MM), Fabrice Gamboa (IMT), Philippe Goudal (MM),\n  Jean-No\\\"el Kien (MM, IMT), Jean-Michel Loubes (IMT)", "title": "Modeling Weather Conditions Consequences on Road Trafficking Behaviors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a model to understand how adverse weather conditions modify\ntraffic flow dynamic. We first prove that the microscopic Free Flow Speed of\nthe vehicles is changed and then provide a rule to model this change. For this,\nwe consider a thresholded linear model, corresponding to an application of a\nMARS model to road trafficking. This model adapts itself locally to the whole\nroad network and provides accurate unbiased forecasted speed using live or\nshort term forecasted weather data information.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2012 14:25:16 GMT"}], "update_date": "2012-10-09", "authors_parsed": [["Allain", "Guillaume", "", "MM"], ["Gamboa", "Fabrice", "", "IMT"], ["Goudal", "Philippe", "", "MM"], ["Kien", "Jean-No\u00ebl", "", "MM, IMT"], ["Loubes", "Jean-Michel", "", "IMT"]]}, {"id": "1210.2429", "submitter": "Mario Frank", "authors": "Mario Frank, Ben Dong, Adrienne Porter Felt, Dawn Song", "title": "Mining Permission Request Patterns from Android and Facebook\n  Applications (extended author version)", "comments": "To be presented at the IEEE International Conference on Data Mining\n  (ICDM) in Brussels, Belgium. This extended author version contains additional\n  analysis of the dataset(price distribution, rating distribution), more\n  details about model-order selection, and more experiments. Please download\n  the dataset from http://www.mariofrank.net/andrApps/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Android and Facebook provide third-party applications with access to users'\nprivate data and the ability to perform potentially sensitive operations (e.g.,\npost to a user's wall or place phone calls). As a security measure, these\nplatforms restrict applications' privileges with permission systems: users must\napprove the permissions requested by applications before the applications can\nmake privacy- or security-relevant API calls. However, recent studies have\nshown that users often do not understand permission requests and lack a notion\nof typicality of requests. As a first step towards simplifying permission\nsystems, we cluster a corpus of 188,389 Android applications and 27,029\nFacebook applications to find patterns in permission requests. Using a method\nfor Boolean matrix factorization for finding overlapping clusters, we find that\nFacebook permission requests follow a clear structure that exhibits high\nstability when fitted with only five clusters, whereas Android applications\ndemonstrate more complex permission requests. We also find that low-reputation\napplications often deviate from the permission request patterns that we\nidentified for high-reputation applications suggesting that permission request\npatterns are indicative for user satisfaction or application quality.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2012 21:54:22 GMT"}], "update_date": "2012-10-10", "authors_parsed": [["Frank", "Mario", ""], ["Dong", "Ben", ""], ["Felt", "Adrienne Porter", ""], ["Song", "Dawn", ""]]}, {"id": "1210.2440", "submitter": "Waheed Bajwa", "authors": "Waheed U. Bajwa and Dustin G. Mixon", "title": "Group Model Selection Using Marginal Correlations: The Good, the Bad and\n  the Ugly", "comments": "Accepted for publication in Proc. 50th Annu. Allerton Conf.\n  Communication, Control, and Computing, Monticello, IL, Oct. 1-5, 2012; 8\n  pages and 4 figures", "journal-ref": "Proc. 50th Annu. Allerton Conf. Communication, Control, and\n  Computing, Monticello, IL, Oct. 1-5, 2012, pp. 494-501", "doi": "10.1109/Allerton.2012.6483259", "report-no": null, "categories": "math.ST cs.IT math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group model selection is the problem of determining a small subset of groups\nof predictors (e.g., the expression data of genes) that are responsible for\nmajority of the variation in a response variable (e.g., the malignancy of a\ntumor). This paper focuses on group model selection in high-dimensional linear\nmodels, in which the number of predictors far exceeds the number of samples of\nthe response variable. Existing works on high-dimensional group model selection\neither require the number of samples of the response variable to be\nsignificantly larger than the total number of predictors contributing to the\nresponse or impose restrictive statistical priors on the predictors and/or\nnonzero regression coefficients. This paper provides comprehensive\nunderstanding of a low-complexity approach to group model selection that avoids\nsome of these limitations. The proposed approach, termed Group Thresholding\n(GroTh), is based on thresholding of marginal correlations of groups of\npredictors with the response variable and is reminiscent of existing\nthresholding-based approaches in the literature. The most important\ncontribution of the paper in this regard is relating the performance of GroTh\nto a polynomial-time verifiable property of the predictors for the general case\nof arbitrary (random or deterministic) predictors and arbitrary nonzero\nregression coefficients.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2012 23:02:14 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Bajwa", "Waheed U.", ""], ["Mixon", "Dustin G.", ""]]}, {"id": "1210.2474", "submitter": "Akshay Soni", "authors": "Akshay Soni and Jarvis Haupt", "title": "Level Set Estimation from Compressive Measurements using Box Constrained\n  Total Variation Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the level set of a signal from measurements is a task that arises\nin a variety of fields, including medical imaging, astronomy, and digital\nelevation mapping. Motivated by scenarios where accurate and complete\nmeasurements of the signal may not available, we examine here a simple\nprocedure for estimating the level set of a signal from highly incomplete\nmeasurements, which may additionally be corrupted by additive noise. The\nproposed procedure is based on box-constrained Total Variation (TV)\nregularization. We demonstrate the performance of our approach, relative to\nexisting state-of-the-art techniques for level set estimation from compressive\nmeasurements, via several simulation examples.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2012 02:57:12 GMT"}], "update_date": "2012-10-10", "authors_parsed": [["Soni", "Akshay", ""], ["Haupt", "Jarvis", ""]]}, {"id": "1210.2503", "submitter": "Antti Honkela", "authors": "Hande Topa and Antti Honkela", "title": "Gaussian process modelling of multiple short time series", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present techniques for effective Gaussian process (GP) modelling of\nmultiple short time series. These problems are common when applying GP models\nindependently to each gene in a gene expression time series data set. Such sets\ntypically contain very few time points. Naive application of common GP\nmodelling techniques can lead to severe over-fitting or under-fitting in a\nsignificant fraction of the fitted models, depending on the details of the data\nset. We propose avoiding over-fitting by constraining the GP length-scale to\nvalues that focus most of the energy spectrum to frequencies below the Nyquist\nfrequency corresponding to the sampling frequency in the data set.\nUnder-fitting can be avoided by more informative priors on observation noise.\nCombining these methods allows applying GP methods reliably automatically to\nlarge numbers of independent instances of short time series. This is\nillustrated with experiments with both synthetic data and real gene expression\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2012 06:36:09 GMT"}], "update_date": "2012-10-10", "authors_parsed": [["Topa", "Hande", ""], ["Honkela", "Antti", ""]]}, {"id": "1210.2748", "submitter": "Jakob Runge", "authors": "Jakob Runge, Jobst Heitzig, Norbert Marwan, and J\\\"urgen Kurths", "title": "Quantifying Causal Coupling Strength: A Lag-specific Measure For\n  Multivariate Time Series Related To Transfer Entropy", "comments": "15 pages, 6 figures; accepted for publication in Physical Review E", "journal-ref": "Physical Review E, 86, 061121 (2012)", "doi": "10.1103/PhysRevE.86.061121", "report-no": null, "categories": "physics.data-an cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While it is an important problem to identify the existence of causal\nassociations between two components of a multivariate time series, a topic\naddressed in Runge et al. (2012), it is even more important to assess the\nstrength of their association in a meaningful way. In the present article we\nfocus on the problem of defining a meaningful coupling strength using\ninformation theoretic measures and demonstrate the short-comings of the\nwell-known mutual information and transfer entropy. Instead, we propose a\ncertain time-delayed conditional mutual information, the momentary information\ntransfer (MIT), as a measure of association that is general, causal and\nlag-specific, reflects a well interpretable notion of coupling strength and is\npractically computable. MIT is based on the fundamental concept of source\nentropy, which we utilize to yield a notion of coupling strength that is,\ncompared to mutual information and transfer entropy, well interpretable, in\nthat for many cases it solely depends on the interaction of the two components\nat a certain lag. In particular, MIT is thus in many cases able to exclude the\nmisleading influence of autodependency within a process in an\ninformation-theoretic way. We formalize and prove this idea analytically and\nnumerically for a general class of nonlinear stochastic processes and\nillustrate the potential of MIT on climatological data.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2012 20:33:30 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2012 16:32:20 GMT"}], "update_date": "2015-07-15", "authors_parsed": [["Runge", "Jakob", ""], ["Heitzig", "Jobst", ""], ["Marwan", "Norbert", ""], ["Kurths", "J\u00fcrgen", ""]]}, {"id": "1210.2771", "submitter": "Zhixiang Eddie Xu", "authors": "Zhixiang Xu, Matt J. Kusner, Kilian Q. Weinberger, Minmin Chen", "title": "Cost-Sensitive Tree of Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, machine learning algorithms have successfully entered large-scale\nreal-world industrial applications (e.g. search engines and email spam\nfilters). Here, the CPU cost during test time must be budgeted and accounted\nfor. In this paper, we address the challenge of balancing the test-time cost\nand the classifier accuracy in a principled fashion. The test-time cost of a\nclassifier is often dominated by the computation required for feature\nextraction-which can vary drastically across eatures. We decrease this\nextraction time by constructing a tree of classifiers, through which test\ninputs traverse along individual paths. Each path extracts different features\nand is optimized for a specific sub-partition of the input space. By only\ncomputing features for inputs that benefit from them the most, our cost\nsensitive tree of classifiers can match the high accuracies of the current\nstate-of-the-art at a small fraction of the computational cost.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2012 22:17:42 GMT"}, {"version": "v2", "created": "Wed, 14 Nov 2012 21:45:56 GMT"}, {"version": "v3", "created": "Mon, 22 Apr 2013 17:56:54 GMT"}], "update_date": "2013-04-23", "authors_parsed": [["Xu", "Zhixiang", ""], ["Kusner", "Matt J.", ""], ["Weinberger", "Kilian Q.", ""], ["Chen", "Minmin", ""]]}, {"id": "1210.3039", "submitter": "Zhaosong Lu", "authors": "Zhaosong Lu", "title": "Sequential Convex Programming Methods for A Class of Structured\n  Nonlinear Programming", "comments": "This paper has been withdrawn by the author due to major revision and\n  corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study a broad class of structured nonlinear programming\n(SNLP) problems. In particular, we first establish the first-order optimality\nconditions for them. Then we propose sequential convex programming (SCP)\nmethods for solving them in which each iteration is obtained by solving a\nconvex programming problem exactly or inexactly. Under some suitable\nassumptions, we establish that any accumulation point of the sequence generated\nby the methods is a KKT point of the SNLP problems. In addition, we propose a\nvariant of the exact SCP method for SNLP in which nonmonotone scheme and\n\"local\" Lipschitz constants of the associated functions are used. And a similar\nconvergence result as mentioned above is established.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2012 20:01:29 GMT"}, {"version": "v2", "created": "Mon, 31 Oct 2016 17:31:12 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Lu", "Zhaosong", ""]]}, {"id": "1210.3288", "submitter": "Willie Neiswanger", "authors": "Willie Neiswanger, Frank Wood", "title": "Unsupervised Detection and Tracking of Arbitrary Objects with Dependent\n  Dirichlet Process Mixtures", "comments": "21 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a technique for the unsupervised detection and tracking\nof arbitrary objects in videos. It is intended to reduce the need for detection\nand localization methods tailored to specific object types and serve as a\ngeneral framework applicable to videos with varied objects, backgrounds, and\nimage qualities. The technique uses a dependent Dirichlet process mixture\n(DDPM) known as the Generalized Polya Urn (GPUDDPM) to model image pixel data\nthat can be easily and efficiently extracted from the regions in a video that\nrepresent objects. This paper describes a specific implementation of the model\nusing spatial and color pixel data extracted via frame differencing and gives\ntwo algorithms for performing inference in the model to accomplish detection\nand tracking. This technique is demonstrated on multiple synthetic and\nbenchmark video datasets that illustrate its ability to, without modification,\ndetect and track objects with diverse physical characteristics moving over\nnon-uniform backgrounds and through occlusion.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2012 16:30:15 GMT"}], "update_date": "2012-10-12", "authors_parsed": [["Neiswanger", "Willie", ""], ["Wood", "Frank", ""]]}, {"id": "1210.3335", "submitter": "Yudong Chen", "authors": "Yudong Chen, Sujay Sanghavi, Huan Xu", "title": "Improved Graph Clustering", "comments": "This is the final version published in IEEE Transactions on\n  Information Theory. An earlier version of this work appeared under the title\n  \"Clustering Sparse Graphs\" at the Neural Information Processing Systems\n  Conference (NIPS), 2012", "journal-ref": "IEEE Transactions on Information Theory, vol. 60, no. 10, pp.\n  6440-6455, 2014", "doi": "10.1109/TIT.2014.2346205", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph clustering involves the task of dividing nodes into clusters, so that\nthe edge density is higher within clusters as opposed to across clusters. A\nnatural, classic and popular statistical setting for evaluating solutions to\nthis problem is the stochastic block model, also referred to as the planted\npartition model.\n  In this paper we present a new algorithm--a convexified version of Maximum\nLikelihood--for graph clustering. We show that, in the classic stochastic block\nmodel setting, it outperforms existing methods by polynomial factors when the\ncluster size is allowed to have general scalings. In fact, it is within\nlogarithmic factors of known lower bounds for spectral methods, and there is\nevidence suggesting that no polynomial time algorithm would do significantly\nbetter.\n  We then show that this guarantee carries over to a more general extension of\nthe stochastic block model. Our method can handle the settings of semi-random\ngraphs, heterogeneous degree distributions, unequal cluster sizes, unaffiliated\nnodes, partially observed graphs and planted clique/coloring etc. In\nparticular, our results provide the best exact recovery guarantees to date for\nthe planted partition, planted k-disjoint-cliques and planted noisy coloring\nmodels with general cluster sizes; in other settings, we match the best\nexisting results up to logarithmic factors.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2012 19:27:28 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2013 04:55:29 GMT"}, {"version": "v3", "created": "Tue, 6 Jan 2015 02:26:53 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Chen", "Yudong", ""], ["Sanghavi", "Sujay", ""], ["Xu", "Huan", ""]]}, {"id": "1210.3384", "submitter": "Shankar Vembu", "authors": "Wei Jiao, Shankar Vembu, Amit G. Deshwar, Lincoln Stein, Quaid Morris", "title": "Inferring clonal evolution of tumors from single nucleotide somatic\n  mutations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.PE q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-throughput sequencing allows the detection and quantification of\nfrequencies of somatic single nucleotide variants (SNV) in heterogeneous tumor\ncell populations. In some cases, the evolutionary history and population\nfrequency of the subclonal lineages of tumor cells present in the sample can be\nreconstructed from these SNV frequency measurements. However, automated methods\nto do this reconstruction are not available and the conditions under which\nreconstruction is possible have not been described.\n  We describe the conditions under which the evolutionary history can be\nuniquely reconstructed from SNV frequencies from single or multiple samples\nfrom the tumor population and we introduce a new statistical model, PhyloSub,\nthat infers the phylogeny and genotype of the major subclonal lineages\nrepresented in the population of cancer cells. It uses a Bayesian nonparametric\nprior over trees that groups SNVs into major subclonal lineages and\nautomatically estimates the number of lineages and their ancestry. We sample\nfrom the joint posterior distribution over trees to identify evolutionary\nhistories and cell population frequencies that have the highest probability of\ngenerating the observed SNV frequency data. When multiple phylogenies are\nconsistent with a given set of SNV frequencies, PhyloSub represents the\nuncertainty in the tumor phylogeny using a partial order plot. Experiments on a\nsimulated dataset and two real datasets comprising tumor samples from acute\nmyeloid leukemia and chronic lymphocytic leukemia patients demonstrate that\nPhyloSub can infer both linear (or chain) and branching lineages and its\ninferences are in good agreement with ground truth, where it is available.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2012 22:20:33 GMT"}, {"version": "v2", "created": "Mon, 15 Oct 2012 18:41:13 GMT"}, {"version": "v3", "created": "Sun, 16 Jun 2013 18:35:00 GMT"}, {"version": "v4", "created": "Sat, 2 Nov 2013 21:38:34 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Jiao", "Wei", ""], ["Vembu", "Shankar", ""], ["Deshwar", "Amit G.", ""], ["Stein", "Lincoln", ""], ["Morris", "Quaid", ""]]}, {"id": "1210.3456", "submitter": "Mingjun Zhong", "authors": "Mingjun Zhong, Rong Liu, Bo Liu", "title": "Bayesian Analysis for miRNA and mRNA Interactions Using Expression Data", "comments": "21 pages, 11 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG q-bio.GN q-bio.MN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MicroRNAs (miRNAs) are small RNA molecules composed of 19-22 nt, which play\nimportant regulatory roles in post-transcriptional gene regulation by\ninhibiting the translation of the mRNA into proteins or otherwise cleaving the\ntarget mRNA. Inferring miRNA targets provides useful information for\nunderstanding the roles of miRNA in biological processes that are potentially\ninvolved in complex diseases. Statistical methodologies for point estimation,\nsuch as the Least Absolute Shrinkage and Selection Operator (LASSO) algorithm,\nhave been proposed to identify the interactions of miRNA and mRNA based on\nsequence and expression data. In this paper, we propose using the Bayesian\nLASSO (BLASSO) and the non-negative Bayesian LASSO (nBLASSO) to analyse the\ninteractions between miRNA and mRNA using expression data. The proposed\nBayesian methods explore the posterior distributions for those parameters\nrequired to model the miRNA-mRNA interactions. These approaches can be used to\nobserve the inferred effects of the miRNAs on the targets by plotting the\nposterior distributions of those parameters. For comparison purposes, the Least\nSquares Regression (LSR), Ridge Regression (RR), LASSO, non-negative LASSO\n(nLASSO), and the proposed Bayesian approaches were applied to four public\ndatasets. We concluded that nLASSO and nBLASSO perform best in terms of\nsensitivity and specificity. Compared to the point estimate algorithms, which\nonly provide single estimates for those parameters, the Bayesian methods are\nmore meaningful and provide credible intervals, which take into account the\nuncertainty of the inferred interactions of the miRNA and mRNA. Furthermore,\nBayesian methods naturally provide statistical significance to select\nconvincing inferred interactions, while point estimate algorithms require a\nmanually chosen threshold, which is less meaningful, to choose the possible\ninteractions.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2012 09:03:14 GMT"}, {"version": "v2", "created": "Mon, 30 Jun 2014 10:16:51 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Zhong", "Mingjun", ""], ["Liu", "Rong", ""], ["Liu", "Bo", ""]]}, {"id": "1210.3709", "submitter": "Weimin Miao", "authors": "Weimin Miao, Shaohua Pan and Defeng Sun", "title": "A Rank-Corrected Procedure for Matrix Completion with Fixed Basis\n  Coefficients", "comments": "51 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.IT cs.NA math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the problems of low-rank matrix completion, the efficiency of the\nwidely-used nuclear norm technique may be challenged under many circumstances,\nespecially when certain basis coefficients are fixed, for example, the low-rank\ncorrelation matrix completion in various fields such as the financial market\nand the low-rank density matrix completion from the quantum state tomography.\nTo seek a solution of high recovery quality beyond the reach of the nuclear\nnorm, in this paper, we propose a rank-corrected procedure using a nuclear\nsemi-norm to generate a new estimator. For this new estimator, we establish a\nnon-asymptotic recovery error bound. More importantly, we quantify the\nreduction of the recovery error bound for this rank-corrected procedure.\nCompared with the one obtained for the nuclear norm penalized least squares\nestimator, this reduction can be substantial (around 50%). We also provide\nnecessary and sufficient conditions for rank consistency in the sense of Bach\n(2008). Very interestingly, these conditions are highly related to the concept\nof constraint nondegeneracy in matrix optimization. As a byproduct, our results\nprovide a theoretical foundation for the majorized penalty method of Gao and\nSun (2010) and Gao (2010) for structured low-rank matrix optimization problems.\nExtensive numerical experiments demonstrate that our proposed rank-corrected\nprocedure can simultaneously achieve a high recovery accuracy and capture the\nlow-rank structure.\n", "versions": [{"version": "v1", "created": "Sat, 13 Oct 2012 14:22:27 GMT"}, {"version": "v2", "created": "Thu, 10 Apr 2014 21:44:16 GMT"}, {"version": "v3", "created": "Mon, 22 Jun 2015 17:14:03 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Miao", "Weimin", ""], ["Pan", "Shaohua", ""], ["Sun", "Defeng", ""]]}, {"id": "1210.3831", "submitter": "Marco Scutari", "authors": "Marco Scutari", "title": "Graphical Modelling in Genetics and Systems Biology", "comments": "Workshop on Foundations of Biomedical Knowledge Representation,\n  October 29 - November 2, 2012 Leiden, The Netherlands", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.MN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical modelling has a long history in statistics as a tool for the\nanalysis of multivariate data, starting from Wright's path analysis and Gibbs'\napplications to statistical physics at the beginning of the last century. In\nits modern form, it was pioneered by Lauritzen and Wermuth and Pearl in the\n1980s, and has since found applications in fields as diverse as bioinformatics,\ncustomer satisfaction surveys and weather forecasts.\n  Genetics and systems biology are unique among these fields in the dimension\nof the data sets they study, which often contain several hundreds of variables\nand only a few tens or hundreds of observations. This raises problems in both\ncomputational complexity and the statistical significance of the resulting\nnetworks, collectively known as the \"curse of dimensionality\". Furthermore, the\ndata themselves are difficult to model correctly due to the limited\nunderstanding of the underlying mechanisms. In the following, we will\nillustrate how such challenges affect practical graphical modelling and some\npossible solutions.\n", "versions": [{"version": "v1", "created": "Sun, 14 Oct 2012 20:15:08 GMT"}, {"version": "v2", "created": "Sun, 12 Jan 2014 17:40:14 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Scutari", "Marco", ""]]}, {"id": "1210.4006", "submitter": "Maayan Harel Maayan Harel", "authors": "Maayan Harel and Shie Mannor", "title": "The Perturbed Variation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new discrepancy score between two distributions that gives an\nindication on their similarity. While much research has been done to determine\nif two samples come from exactly the same distribution, much less research\nconsidered the problem of determining if two finite samples come from similar\ndistributions. The new score gives an intuitive interpretation of similarity;\nit optimally perturbs the distributions so that they best fit each other. The\nscore is defined between distributions, and can be efficiently estimated from\nsamples. We provide convergence bounds of the estimated score, and develop\nhypothesis testing procedures that test if two data sets come from similar\ndistributions. The statistical power of this procedures is presented in\nsimulations. We also compare the score's capacity to detect similarity with\nthat of other known measures on real data.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2012 12:43:03 GMT"}], "update_date": "2012-10-16", "authors_parsed": [["Harel", "Maayan", ""], ["Mannor", "Shie", ""]]}, {"id": "1210.4184", "submitter": "Sotirios Chatzis", "authors": "Sotirios P. Chatzis and Dimitrios Korkinof and Yiannis Demiris", "title": "The Kernel Pitman-Yor Process", "comments": "This is a Technical Report summarizing our ongoing work on the Kernel\n  Pitman-Yor Process. Experiments will be added by D. Korkinof prior to journal\n  or conference submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose the kernel Pitman-Yor process (KPYP) for\nnonparametric clustering of data with general spatial or temporal\ninterdependencies. The KPYP is constructed by first introducing an infinite\nsequence of random locations. Then, based on the stick-breaking construction of\nthe Pitman-Yor process, we define a predictor-dependent random probability\nmeasure by considering that the discount hyperparameters of the\nBeta-distributed random weights (stick variables) of the process are not\nuniform among the weights, but controlled by a kernel function expressing the\nproximity between the location assigned to each weight and the given\npredictors.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2012 20:14:23 GMT"}], "update_date": "2012-10-17", "authors_parsed": [["Chatzis", "Sotirios P.", ""], ["Korkinof", "Dimitrios", ""], ["Demiris", "Yiannis", ""]]}, {"id": "1210.4276", "submitter": "Bertrand Lebichot", "authors": "Bertrand Lebichot, Ilkka Kivim\\\"aki, Kevin Fran\\c{c}oisse and Marco\n  Saerens", "title": "Semi-Supervised Classification Through the Bag-of-Paths Group\n  Betweenness", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel, well-founded, betweenness measure, called the\nBag-of-Paths (BoP) betweenness, as well as its extension, the BoP group\nbetweenness, to tackle semisupervised classification problems on weighted\ndirected graphs. The objective of semi-supervised classification is to assign a\nlabel to unlabeled nodes using the whole topology of the graph and the labeled\nnodes at our disposal. The BoP betweenness relies on a bag-of-paths framework\nassigning a Boltzmann distribution on the set of all possible paths through the\nnetwork such that long (high-cost) paths have a low probability of being picked\nfrom the bag, while short (low-cost) paths have a high probability of being\npicked. Within that context, the BoP betweenness of node j is defined as the\nsum of the a posteriori probabilities that node j lies in-between two arbitrary\nnodes i, k, when picking a path starting in i and ending in k. Intuitively, a\nnode typically receives a high betweenness if it has a large probability of\nappearing on paths connecting two arbitrary nodes of the network. This quantity\ncan be computed in closed form by inverting a n x n matrix where n is the\nnumber of nodes. For the group betweenness, the paths are constrained to start\nand end in nodes within the same class, therefore defining a group betweenness\nfor each class. Unlabeled nodes are then classified according to the class\nshowing the highest group betweenness. Experiments on various real-world data\nsets show that BoP group betweenness outperforms all the tested state\nof-the-art methods. The benefit of the BoP betweenness is particularly\nnoticeable when only a few labeled nodes are available.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 07:31:59 GMT"}], "update_date": "2012-10-17", "authors_parsed": [["Lebichot", "Bertrand", ""], ["Kivim\u00e4ki", "Ilkka", ""], ["Fran\u00e7oisse", "Kevin", ""], ["Saerens", "Marco", ""]]}, {"id": "1210.4347", "submitter": "Krikamol Muandet", "authors": "Krikamol Muandet", "title": "Hilbert Space Embedding for Dirichlet Process Mixtures", "comments": "NIPS 2012 Workshop in confluence between kernel methods and graphical\n  models", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a Hilbert space embedding for Dirichlet Process mixture\nmodels via a stick-breaking construction of Sethuraman. Although Bayesian\nnonparametrics offers a powerful approach to construct a prior that avoids the\nneed to specify the model size/complexity explicitly, an exact inference is\noften intractable. On the other hand, frequentist approaches such as kernel\nmachines, which suffer from the model selection/comparison problems, often\nbenefit from efficient learning algorithms. This paper discusses the\npossibility to combine the best of both worlds by using the Dirichlet Process\nmixture model as a case study.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 10:26:29 GMT"}], "update_date": "2012-10-17", "authors_parsed": [["Muandet", "Krikamol", ""]]}, {"id": "1210.4460", "submitter": "Yaman Aksu Ph.D.", "authors": "Yaman Aksu", "title": "Fast SVM-based Feature Elimination Utilizing Data Radius, Hard-Margin,\n  Soft-Margin", "comments": "Incomplete but good, again. To Apr 28 version, made few misc text and\n  notation improvements including typo corrections, probably mostly in\n  Appendix, but probably best to read in whole again. New results for one of\n  the datasets (Leukemia gene dataset)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Margin maximization in the hard-margin sense, proposed as feature elimination\ncriterion by the MFE-LO method, is combined here with data radius utilization\nto further aim to lower generalization error, as several published bounds and\nbound-related formulations pertaining to lowering misclassification risk (or\nerror) pertain to radius e.g. product of squared radius and weight vector\nsquared norm. Additionally, we propose additional novel feature elimination\ncriteria that, while instead being in the soft-margin sense, too can utilize\ndata radius, utilizing previously published bound-related formulations for\napproaching radius for the soft-margin sense, whereby e.g. a focus was on the\nprinciple stated therein as \"finding a bound whose minima are in a region with\nsmall leave-one-out values may be more important than its tightness\". These\nadditional criteria we propose combine radius utilization with a novel and\ncomputationally low-cost soft-margin light classifier retraining approach we\ndevise named QP1; QP1 is the soft-margin alternative to the hard-margin LO. We\ncorrect an error in the MFE-LO description, find MFE-LO achieves the highest\ngeneralization accuracy among the previously published margin-based feature\nelimination (MFE) methods, discuss some limitations of MFE-LO, and find our\nnovel methods herein outperform MFE-LO, attain lower test set classification\nerror rate. On several datasets that each both have a large number of features\nand fall into the `large features few samples' dataset category, and on\ndatasets with lower (low-to-intermediate) number of features, our novel methods\ngive promising results. Especially, among our methods the tunable ones, that do\nnot employ (the non-tunable) LO approach, can be tuned more aggressively in the\nfuture than herein, to aim to demonstrate for them even higher performance than\nherein.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 15:54:36 GMT"}, {"version": "v2", "created": "Wed, 9 Jan 2013 16:28:17 GMT"}, {"version": "v3", "created": "Mon, 28 Apr 2014 21:15:46 GMT"}, {"version": "v4", "created": "Sun, 11 May 2014 11:47:07 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Aksu", "Yaman", ""]]}, {"id": "1210.4657", "submitter": "Hamidou Tembine", "authors": "Hamidou Tembine, Raul Tempone and Pedro Vilanova", "title": "Mean-Field Learning: a Survey", "comments": "36 pages. 5 figures. survey style", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT cs.MA math.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study iterative procedures for stationary equilibria in\ngames with large number of players. Most of learning algorithms for games with\ncontinuous action spaces are limited to strict contraction best reply maps in\nwhich the Banach-Picard iteration converges with geometrical convergence rate.\nWhen the best reply map is not a contraction, Ishikawa-based learning is\nproposed. The algorithm is shown to behave well for Lipschitz continuous and\npseudo-contractive maps. However, the convergence rate is still unsatisfactory.\nSeveral acceleration techniques are presented. We explain how cognitive users\ncan improve the convergence rate based only on few number of measurements. The\nmethodology provides nice properties in mean field games where the payoff\nfunction depends only on own-action and the mean of the mean-field (first\nmoment mean-field games). A learning framework that exploits the structure of\nsuch games, called, mean-field learning, is proposed. The proposed mean-field\nlearning framework is suitable not only for games but also for non-convex\nglobal optimization problems. Then, we introduce mean-field learning without\nfeedback and examine the convergence to equilibria in beauty contest games,\nwhich have interesting applications in financial markets. Finally, we provide a\nfully distributed mean-field learning and its speedup versions for satisfactory\nsolution in wireless networks. We illustrate the convergence rate improvement\nwith numerical examples.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2012 07:51:56 GMT"}], "update_date": "2012-10-18", "authors_parsed": [["Tembine", "Hamidou", ""], ["Tempone", "Raul", ""], ["Vilanova", "Pedro", ""]]}, {"id": "1210.4762", "submitter": "Stephane Chretien", "authors": "St\\'ephane Chr\\'etien", "title": "Mixture model for designs in high dimensional regression and the LASSO", "comments": "Draft. Simulations to be included soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The LASSO is a recent technique for variable selection in the regression\nmodel \\bean y & = & X\\beta +\\epsilon, \\eean where $X\\in \\R^{n\\times p}$ and\n$\\epsilon$ is a centered gaussian i.i.d. noise vector $\\mathcal\nN(0,\\sigma^2I)$. The LASSO has been proved to perform exact support recovery\nfor regression vectors when the design matrix satisfies certain algebraic\nconditions and $\\beta$ is sufficiently sparse. Estimation of the vector\n$X\\beta$ has also extensively been studied for the purpose of prediction under\nthe same algebraic conditions on $X$ and under sufficient sparsity of $\\beta$.\nAmong many other, the coherence is an index which can be used to study these\nnice properties of the LASSO. More precisely, a small coherence implies that\nmost sparse vectors, with less nonzero components than the order $n/\\log(p)$,\ncan be recovered with high probability if its nonzero components are larger\nthan the order $\\sigma \\sqrt{\\log(p)}$. However, many matrices occuring in\npractice do not have a small coherence and thus, most results which have\nappeared in the litterature cannot be applied. The goal of this paper is to\nstudy a model for which precise results can be obtained. In the proposed model,\nthe columns of the design matrix are drawn from a Gaussian mixture model and\nthe coherence condition is imposed on the much smaller matrix whose columns are\nthe mixture's centers, instead of on $X$ itself. Our main theorem states that\n$X\\beta$ is as well estimated as in the case of small coherence up to a\ncorrection parametrized by the maximal variance in the mixture model.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2012 15:10:39 GMT"}], "update_date": "2012-10-18", "authors_parsed": [["Chr\u00e9tien", "St\u00e9phane", ""]]}, {"id": "1210.4768", "submitter": "Roger Guimera", "authors": "Roger Guimera and Marta Sales-Pardo", "title": "Justice blocks and predictability of US Supreme Court votes", "comments": null, "journal-ref": "PLOS ONE 6 (11), e27188 (2011)", "doi": "10.1371/journal.pone.0027188", "report-no": null, "categories": "physics.soc-ph physics.data-an stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Successful attempts to predict judges' votes shed light into how legal\ndecisions are made and, ultimately, into the behavior and evolution of the\njudiciary. Here, we investigate to what extent it is possible to make\npredictions of a justice's vote based on the other justices' votes in the same\ncase. For our predictions, we use models and methods that have been developed\nto uncover hidden associations between actors in complex social networks. We\nshow that these methods are more accurate at predicting justice's votes than\nforecasts made by legal experts and by algorithms that take into consideration\nthe content of the cases. We argue that, within our framework, high\npredictability is a quantitative proxy for stable justice (and case) blocks,\nwhich probably reflect stable a priori attitudes toward the law. We find that\nU. S. Supreme Court justice votes are more predictable than one would expect\nfrom an ideal court composed of perfectly independent justices. Deviations from\nideal behavior are most apparent in divided 5-4 decisions, where justice blocks\nseem to be most stable. Moreover, we find evidence that justice predictability\ndecreased during the 50-year period spanning from the Warren Court to the\nRehnquist Court, and that aggregate court predictability has been significantly\nlower during Democratic presidencies. More broadly, our results show that it is\npossible to use methods developed for the analysis of complex social networks\nto quantitatively investigate historical questions related to political\ndecision-making.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2012 15:28:36 GMT"}], "update_date": "2012-10-18", "authors_parsed": [["Guimera", "Roger", ""], ["Sales-Pardo", "Marta", ""]]}, {"id": "1210.4792", "submitter": "Vikas Sindhwani", "authors": "Vikas Sindhwani and Minh Ha Quang and Aurelie C. Lozano", "title": "Scalable Matrix-valued Kernel Learning for High-dimensional Nonlinear\n  Multivariate Regression and Granger Causality", "comments": "22 pages. Presentation changes; Corrections made to Theorem 2\n  (section 6.2) in this version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general matrix-valued multiple kernel learning framework for\nhigh-dimensional nonlinear multivariate regression problems. This framework\nallows a broad class of mixed norm regularizers, including those that induce\nsparsity, to be imposed on a dictionary of vector-valued Reproducing Kernel\nHilbert Spaces. We develop a highly scalable and eigendecomposition-free\nalgorithm that orchestrates two inexact solvers for simultaneously learning\nboth the input and output components of separable matrix-valued kernels. As a\nkey application enabled by our framework, we show how high-dimensional causal\ninference tasks can be naturally cast as sparse function estimation problems,\nleading to novel nonlinear extensions of a class of Graphical Granger Causality\ntechniques. Our algorithmic developments and extensive empirical studies are\ncomplemented by theoretical analyses in terms of Rademacher generalization\nbounds.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2012 16:57:48 GMT"}, {"version": "v2", "created": "Fri, 8 Mar 2013 02:19:07 GMT"}], "update_date": "2013-03-11", "authors_parsed": [["Sindhwani", "Vikas", ""], ["Quang", "Minh Ha", ""], ["Lozano", "Aurelie C.", ""]]}, {"id": "1210.4839", "submitter": "Stephane Caron", "authors": "Stephane Caron, Branislav Kveton, Marc Lelarge, Smriti Bhagat", "title": "Leveraging Side Observations in Stochastic Bandits", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-142-151", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers stochastic bandits with side observations, a model that\naccounts for both the exploration/exploitation dilemma and relationships\nbetween arms. In this setting, after pulling an arm i, the decision maker also\nobserves the rewards for some other actions related to i. We will see that this\nmodel is suited to content recommendation in social networks, where users'\nreactions may be endorsed or not by their friends. We provide efficient\nalgorithms based on upper confidence bounds (UCBs) to leverage this additional\ninformation and derive new bounds improving on standard regret guarantees. We\nalso evaluate these policies in the context of movie recommendation in social\nnetworks: experiments on real datasets show substantial learning rate speedups\nranging from 2.2x to 14x on dense networks.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:32:09 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Caron", "Stephane", ""], ["Kveton", "Branislav", ""], ["Lelarge", "Marc", ""], ["Bhagat", "Smriti", ""]]}, {"id": "1210.4841", "submitter": "Dhruv Batra", "authors": "Dhruv Batra", "title": "An Efficient Message-Passing Algorithm for the M-Best MAP Problem", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-121-130", "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much effort has been directed at algorithms for obtaining the highest\nprobability configuration in a probabilistic random field model known as the\nmaximum a posteriori (MAP) inference problem. In many situations, one could\nbenefit from having not just a single solution, but the top M most probable\nsolutions known as the M-Best MAP problem. In this paper, we propose an\nefficient message-passing based algorithm for solving the M-Best MAP problem.\nSpecifically, our algorithm solves the recently proposed Linear Programming\n(LP) formulation of M-Best MAP [7], while being orders of magnitude faster than\na generic LP-solver. Our approach relies on studying a particular partial\nLagrangian relaxation of the M-Best MAP LP which exposes a natural\ncombinatorial structure of the problem that we exploit.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:32:34 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Batra", "Dhruv", ""]]}, {"id": "1210.4846", "submitter": "Saeed Amizadeh", "authors": "Saeed Amizadeh, Bo Thiesson, Milos Hauskrecht", "title": "Variational Dual-Tree Framework for Large-Scale Transition Matrix\n  Approximation", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-64-73", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, non-parametric methods utilizing random walks on graphs have\nbeen used to solve a wide range of machine learning problems, but in their\nsimplest form they do not scale well due to the quadratic complexity. In this\npaper, a new dual-tree based variational approach for approximating the\ntransition matrix and efficiently performing the random walk is proposed. The\napproach exploits a connection between kernel density estimation, mixture\nmodeling, and random walk on graphs in an optimization of the transition matrix\nfor the data graph that ties together edge transitions probabilities that are\nsimilar. Compared to the de facto standard approximation method based on\nk-nearestneighbors, we demonstrate order of magnitudes speedup without\nsacrificing accuracy for Label Propagation tasks on benchmark data sets in\nsemi-supervised learning.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:34:45 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Amizadeh", "Saeed", ""], ["Thiesson", "Bo", ""], ["Hauskrecht", "Milos", ""]]}, {"id": "1210.4850", "submitter": "Raja Hafiz Affandi", "authors": "Raja Hafiz Affandi, Alex Kulesza, Emily B. Fox", "title": "Markov Determinantal Point Processes", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-26-35", "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A determinantal point process (DPP) is a random process useful for modeling\nthe combinatorial problem of subset selection. In particular, DPPs encourage a\nrandom subset Y to contain a diverse set of items selected from a base set Y.\nFor example, we might use a DPP to display a set of news headlines that are\nrelevant to a user's interests while covering a variety of topics. Suppose,\nhowever, that we are asked to sequentially select multiple diverse sets of\nitems, for example, displaying new headlines day-by-day. We might want these\nsets to be diverse not just individually but also through time, offering\nheadlines today that are unlike the ones shown yesterday. In this paper, we\nconstruct a Markov DPP (M-DPP) that models a sequence of random sets {Yt}. The\nproposed M-DPP defines a stationary process that maintains DPP margins.\nCrucially, the induced union process Zt = Yt u Yt-1 is also marginally\nDPP-distributed. Jointly, these properties imply that the sequence of random\nsets are encouraged to be diverse both at a given time step as well as across\ntime steps. We describe an exact, efficient sampling procedure, and a method\nfor incrementally learning a quality measure over items in the base set Y based\non external preferences. We apply the M-DPP to the task of sequentially\ndisplaying diverse and relevant news articles to a user with topic preferences.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:35:39 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Affandi", "Raja Hafiz", ""], ["Kulesza", "Alex", ""], ["Fox", "Emily B.", ""]]}, {"id": "1210.4851", "submitter": "Sreangsu Acharyya", "authors": "Sreangsu Acharyya, Oluwasanmi Koyejo, Joydeep Ghosh", "title": "Learning to Rank With Bregman Divergences and Monotone Retargeting", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-15-25", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel approach for learning to rank (LETOR) based on\nthe notion of monotone retargeting. It involves minimizing a divergence between\nall monotonic increasing transformations of the training scores and a\nparameterized prediction function. The minimization is both over the\ntransformations as well as over the parameters. It is applied to Bregman\ndivergences, a large class of \"distance like\" functions that were recently\nshown to be the unique class that is statistically consistent with the\nnormalized discounted gain (NDCG) criterion [19]. The algorithm uses\nalternating projection style updates, in which one set of simultaneous\nprojections can be computed independent of the Bregman divergence and the other\nreduces to parameter estimation of a generalized linear model. This results in\neasily implemented, efficiently parallelizable algorithm for the LETOR task\nthat enjoys global optimum guarantees under mild conditions. We present\nempirical results on benchmark datasets showing that this approach can\noutperform the state of the art NDCG consistent techniques.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:35:52 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Acharyya", "Sreangsu", ""], ["Koyejo", "Oluwasanmi", ""], ["Ghosh", "Joydeep", ""]]}, {"id": "1210.4855", "submitter": "Sunil Kumar Gupta", "authors": "Sunil Kumar Gupta, Dinh Q. Phung, Svetha Venkatesh", "title": "A Slice Sampler for Restricted Hierarchical Beta Process with\n  Applications to Shared Subspace Learning", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-316-325", "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical beta process has found interesting applications in recent years.\nIn this paper we present a modified hierarchical beta process prior with\napplications to hierarchical modeling of multiple data sources. The novel use\nof the prior over a hierarchical factor model allows factors to be shared\nacross different sources. We derive a slice sampler for this model, enabling\ntractable inference even when the likelihood and the prior over parameters are\nnon-conjugate. This allows the application of the model in much wider contexts\nwithout restrictions. We present two different data generative models a linear\nGaussianGaussian model for real valued data and a linear Poisson-gamma model\nfor count data. Encouraging transfer learning results are shown for two real\nworld applications text modeling and content based image retrieval.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:37:29 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Gupta", "Sunil Kumar", ""], ["Phung", "Dinh Q.", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1210.4856", "submitter": "Roger Grosse", "authors": "Roger Grosse, Ruslan R Salakhutdinov, William T. Freeman, Joshua B.\n  Tenenbaum", "title": "Exploiting compositionality to explore a large space of model structures", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-306-315", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent proliferation of richly structured probabilistic models raises the\nquestion of how to automatically determine an appropriate model for a dataset.\nWe investigate this question for a space of matrix decomposition models which\ncan express a variety of widely used models from unsupervised learning. To\nenable model selection, we organize these models into a context-free grammar\nwhich generates a wide variety of structures through the compositional\napplication of a few simple rules. We use our grammar to generically and\nefficiently infer latent components and estimate predictive likelihood for\nnearly 2500 structures using a small toolbox of reusable algorithms. Using a\ngreedy search over our grammar, we automatically choose the decomposition\nstructure from raw data by evaluating only a small fraction of all models. The\nproposed method typically finds the correct structure for synthetic data and\nbacks off gracefully to simpler models under heavy noise. It learns sensible\nstructures for datasets as diverse as image patches, motion capture, 20\nQuestions, and U.S. Senate votes, all using exactly the same code.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:37:41 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Grosse", "Roger", ""], ["Salakhutdinov", "Ruslan R", ""], ["Freeman", "William T.", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "1210.4859", "submitter": "Dinesh Garg", "authors": "Dinesh Garg, Sourangshu Bhattacharya, S. Sundararajan, Shirish Shevade", "title": "Mechanism Design for Cost Optimal PAC Learning in the Presence of\n  Strategic Noisy Annotators", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-275-285", "categories": "cs.LG cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of Probably Approximate Correct (PAC) learning of a\nbinary classifier from noisy labeled examples acquired from multiple annotators\n(each characterized by a respective classification noise rate). First, we\nconsider the complete information scenario, where the learner knows the noise\nrates of all the annotators. For this scenario, we derive sample complexity\nbound for the Minimum Disagreement Algorithm (MDA) on the number of labeled\nexamples to be obtained from each annotator. Next, we consider the incomplete\ninformation scenario, where each annotator is strategic and holds the\nrespective noise rate as a private information. For this scenario, we design a\ncost optimal procurement auction mechanism along the lines of Myerson's optimal\nauction design framework in a non-trivial manner. This mechanism satisfies\nincentive compatibility property, thereby facilitating the learner to elicit\ntrue noise rates of all the annotators.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:38:13 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Garg", "Dinesh", ""], ["Bhattacharya", "Sourangshu", ""], ["Sundararajan", "S.", ""], ["Shevade", "Shirish", ""]]}, {"id": "1210.4860", "submitter": "Antonino Freno", "authors": "Antonino Freno, Mikaela Keller, Gemma C. Garriga, Marc Tommasi", "title": "Spectral Estimation of Conditional Random Graph Models for Large-Scale\n  Network Data", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-265-274", "categories": "cs.SI cs.LG physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models for graphs have been typically committed to strong prior\nassumptions concerning the form of the modeled distributions. Moreover, the\nvast majority of currently available models are either only suitable for\ncharacterizing some particular network properties (such as degree distribution\nor clustering coefficient), or they are aimed at estimating joint probability\ndistributions, which is often intractable in large-scale networks. In this\npaper, we first propose a novel network statistic, based on the Laplacian\nspectrum of graphs, which allows to dispense with any parametric assumption\nconcerning the modeled network properties. Second, we use the defined statistic\nto develop the Fiedler random graph model, switching the focus from the\nestimation of joint probability distributions to a more tractable conditional\nestimation setting. After analyzing the dependence structure characterizing\nFiedler random graphs, we evaluate them experimentally in edge prediction over\nseveral real-world networks, showing that they allow to reach a much higher\nprediction accuracy than various alternative statistical models.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:38:22 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Freno", "Antonino", ""], ["Keller", "Mikaela", ""], ["Garriga", "Gemma C.", ""], ["Tommasi", "Marc", ""]]}, {"id": "1210.4862", "submitter": "Miroslav Dudik", "authors": "Miroslav Dudik, Dumitru Erhan, John Langford, Lihong Li", "title": "Sample-efficient Nonstationary Policy Evaluation for Contextual Bandits", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-247-254", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and prove properties of a new offline policy evaluator for an\nexploration learning setting which is superior to previous evaluators. In\nparticular, it simultaneously and correctly incorporates techniques from\nimportance weighting, doubly robust evaluation, and nonstationary policy\nevaluation approaches. In addition, our approach allows generating longer\nhistories by careful control of a bias-variance tradeoff, and further decreases\nvariance by incorporating information about randomness of the target policy.\nEmpirical evidence from synthetic and realworld exploration learning problems\nshows the new evaluator successfully unifies previous approaches and uses\ninformation an order of magnitude more efficiently.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:38:45 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Dudik", "Miroslav", ""], ["Erhan", "Dumitru", ""], ["Langford", "John", ""], ["Li", "Lihong", ""]]}, {"id": "1210.4867", "submitter": "Jaesik Choi", "authors": "Jaesik Choi, Eyal Amir", "title": "Lifted Relational Variational Inference", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-196-206", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hybrid continuous-discrete models naturally represent many real-world\napplications in robotics, finance, and environmental engineering. Inference\nwith large-scale models is challenging because relational structures\ndeteriorate rapidly during inference with observations. The main contribution\nof this paper is an efficient relational variational inference algorithm that\nfactors largescale probability models into simpler variational models, composed\nof mixtures of iid (Bernoulli) random variables. The algorithm takes\nprobability relational models of largescale hybrid systems and converts them to\na close-to-optimal variational models. Then, it efficiently calculates marginal\nprobabilities on the variational models by using a latent (or lifted) variable\nelimination or a lifted stochastic sampling. This inference is unique because\nit maintains the relational structure upon individual observations and during\ninference steps.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:39:37 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Choi", "Jaesik", ""], ["Amir", "Eyal", ""]]}, {"id": "1210.4869", "submitter": "Guang Ling", "authors": "Guang Ling, Haiqin Yang, Michael R. Lyu, Irwin King", "title": "Response Aware Model-Based Collaborative Filtering", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-501-510", "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous work on recommender systems mainly focus on fitting the ratings\nprovided by users. However, the response patterns, i.e., some items are rated\nwhile others not, are generally ignored. We argue that failing to observe such\nresponse patterns can lead to biased parameter estimation and sub-optimal model\nperformance. Although several pieces of work have tried to model users'\nresponse patterns, they miss the effectiveness and interpretability of the\nsuccessful matrix factorization collaborative filtering approaches. To bridge\nthe gap, in this paper, we unify explicit response models and PMF to establish\nthe Response Aware Probabilistic Matrix Factorization (RAPMF) framework. We\nshow that RAPMF subsumes PMF as a special case. Empirically we demonstrate the\nmerits of RAPMF from various aspects.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:40:52 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Ling", "Guang", ""], ["Yang", "Haiqin", ""], ["Lyu", "Michael R.", ""], ["King", "Irwin", ""]]}, {"id": "1210.4871", "submitter": "Hui Lin", "authors": "Hui Lin, Jeff A. Bilmes", "title": "Learning Mixtures of Submodular Shells with Application to Document\n  Summarization", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-479-490", "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method to learn a mixture of submodular \"shells\" in a\nlarge-margin setting. A submodular shell is an abstract submodular function\nthat can be instantiated with a ground set and a set of parameters to produce a\nsubmodular function. A mixture of such shells can then also be so instantiated\nto produce a more complex submodular function. What our algorithm learns are\nthe mixture weights over such shells. We provide a risk bound guarantee when\nlearning in a large-margin structured-prediction setting using a projected\nsubgradient method when only approximate submodular optimization is possible\n(such as with submodular function maximization). We apply this method to the\nproblem of multi-document summarization and produce the best results reported\nso far on the widely used NIST DUC-05 through DUC-07 document summarization\ncorpora.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:41:30 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Lin", "Hui", ""], ["Bilmes", "Jeff A.", ""]]}, {"id": "1210.4872", "submitter": "Lingbo Li", "authors": "Lingbo Li, XianXing Zhang, Mingyuan Zhou, Lawrence Carin", "title": "Nested Dictionary Learning for Hierarchical Organization of Imagery and\n  Text", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-469-478", "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A tree-based dictionary learning model is developed for joint analysis of\nimagery and associated text. The dictionary learning may be applied directly to\nthe imagery from patches, or to general feature vectors extracted from patches\nor superpixels (using any existing method for image feature extraction). Each\nimage is associated with a path through the tree (from root to a leaf), and\neach of the multiple patches in a given image is associated with one node in\nthat path. Nodes near the tree root are shared between multiple paths,\nrepresenting image characteristics that are common among different types of\nimages. Moving toward the leaves, nodes become specialized, representing\ndetails in image classes. If available, words (text) are also jointly modeled,\nwith a path-dependent probability over words. The tree structure is inferred\nvia a nested Dirichlet process, and a retrospective stick-breaking sampler is\nused to infer the tree depth and width.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:41:42 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Li", "Lingbo", ""], ["Zhang", "XianXing", ""], ["Zhou", "Mingyuan", ""], ["Carin", "Lawrence", ""]]}, {"id": "1210.4876", "submitter": "Kshitij Judah", "authors": "Kshitij Judah, Alan Fern, Thomas G. Dietterich", "title": "Active Imitation Learning via Reduction to I.I.D. Active Learning", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-428-437", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In standard passive imitation learning, the goal is to learn a target policy\nby passively observing full execution trajectories of it. Unfortunately,\ngenerating such trajectories can require substantial expert effort and be\nimpractical in some cases. In this paper, we consider active imitation learning\nwith the goal of reducing this effort by querying the expert about the desired\naction at individual states, which are selected based on answers to past\nqueries and the learner's interactions with an environment simulator. We\nintroduce a new approach based on reducing active imitation learning to i.i.d.\nactive learning, which can leverage progress in the i.i.d. setting. Our first\ncontribution, is to analyze reductions for both non-stationary and stationary\npolicies, showing that the label complexity (number of queries) of active\nimitation learning can be substantially less than passive learning. Our second\ncontribution, is to introduce a practical algorithm inspired by the reductions,\nwhich is shown to be highly effective in four test domains compared to a number\nof alternatives.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:43:04 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Judah", "Kshitij", ""], ["Fern", "Alan", ""], ["Dietterich", "Thomas G.", ""]]}, {"id": "1210.4879", "submitter": "Antti Hyttinen", "authors": "Antti Hyttinen, Frederick Eberhardt, Patrik O. Hoyer", "title": "Causal Discovery of Linear Cyclic Models from Multiple Experimental Data\n  Sets with Overlapping Variables", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-387-396", "categories": "stat.ME cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of scientific data is collected as randomized experiments intervening on\nsome and observing other variables of interest. Quite often, a given phenomenon\nis investigated in several studies, and different sets of variables are\ninvolved in each study. In this article we consider the problem of integrating\nsuch knowledge, inferring as much as possible concerning the underlying causal\nstructure with respect to the union of observed variables from such\nexperimental or passive observational overlapping data sets. We do not assume\nacyclicity or joint causal sufficiency of the underlying data generating model,\nbut we do restrict the causal relationships to be linear and use only second\norder statistics of the data. We derive conditions for full model\nidentifiability in the most generic case, and provide novel techniques for\nincorporating an assumption of faithfulness to aid in inference. In each case\nwe seek to establish what is and what is not determined by the data at hand.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:43:35 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Hyttinen", "Antti", ""], ["Eberhardt", "Frederick", ""], ["Hoyer", "Patrik O.", ""]]}, {"id": "1210.4881", "submitter": "Tamir Hazan", "authors": "Tamir Hazan, Jian Peng, Amnon Shashua", "title": "Tightening Fractional Covering Upper Bounds on the Partition Function\n  for High-Order Region Graphs", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-356-366", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new approach for tightening upper bounds on the\npartition function. Our upper bounds are based on fractional covering bounds on\nthe entropy function, and result in a concave program to compute these bounds\nand a convex program to tighten them. To solve these programs effectively for\ngeneral region graphs we utilize the entropy barrier method, thus decomposing\nthe original programs by their dual programs and solve them with dual block\noptimization scheme. The entropy barrier method provides an elegant framework\nto generalize the message-passing scheme to high-order region graph, as well as\nto solve the block dual steps in closed-form. This is a key for computational\nrelevancy for large problems with thousands of regions.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:43:59 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Hazan", "Tamir", ""], ["Peng", "Jian", ""], ["Shashua", "Amnon", ""]]}, {"id": "1210.4883", "submitter": "Leonard K. M. Poon", "authors": "Leonard K. M. Poon, April H. Liu, Tengfei Liu, Nevin Lianwen Zhang", "title": "A Model-Based Approach to Rounding in Spectral Clustering", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-685-694", "categories": "cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spectral clustering, one defines a similarity matrix for a collection of\ndata points, transforms the matrix to get the Laplacian matrix, finds the\neigenvectors of the Laplacian matrix, and obtains a partition of the data using\nthe leading eigenvectors. The last step is sometimes referred to as rounding,\nwhere one needs to decide how many leading eigenvectors to use, to determine\nthe number of clusters, and to partition the data points. In this paper, we\npropose a novel method for rounding. The method differs from previous methods\nin three ways. First, we relax the assumption that the number of clusters\nequals the number of eigenvectors used. Second, when deciding the number of\nleading eigenvectors to use, we not only rely on information contained in the\nleading eigenvectors themselves, but also use subsequent eigenvectors. Third,\nour method is model-based and solves all the three subproblems of rounding\nusing a class of graphical models called latent tree models. We evaluate our\nmethod on both synthetic and real-world data. The results show that our method\nworks correctly in the ideal case where between-clusters similarity is 0, and\ndegrades gracefully as one moves away from the ideal case.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:45:11 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Poon", "Leonard K. M.", ""], ["Liu", "April H.", ""], ["Liu", "Tengfei", ""], ["Zhang", "Nevin Lianwen", ""]]}, {"id": "1210.4884", "submitter": "Ankur P. Parikh", "authors": "Ankur P. Parikh, Le Song, Mariya Ishteva, Gabi Teodoru, Eric P. Xing", "title": "A Spectral Algorithm for Latent Junction Trees", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-675-684", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent variable models are an elegant framework for capturing rich\nprobabilistic dependencies in many applications. However, current approaches\ntypically parametrize these models using conditional probability tables, and\nlearning relies predominantly on local search heuristics such as Expectation\nMaximization. Using tensor algebra, we propose an alternative parameterization\nof latent variable models (where the model structures are junction trees) that\nstill allows for computation of marginals among observed variables. While this\nnovel representation leads to a moderate increase in the number of parameters\nfor junction trees of low treewidth, it lets us design a local-minimum-free\nalgorithm for learning this parameterization. The main computation of the\nalgorithm involves only tensor operations and SVDs which can be orders of\nmagnitude faster than EM algorithms for large datasets. To our knowledge, this\nis the first provably consistent parameter learning technique for a large class\nof low-treewidth latent graphical models beyond trees. We demonstrate the\nadvantages of our method on synthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:45:30 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Parikh", "Ankur P.", ""], ["Song", "Le", ""], ["Ishteva", "Mariya", ""], ["Teodoru", "Gabi", ""], ["Xing", "Eric P.", ""]]}, {"id": "1210.4887", "submitter": "Yu Nishiyama", "authors": "Yu Nishiyama, Abdeslam Boularias, Arthur Gretton, Kenji Fukumizu", "title": "Hilbert Space Embeddings of POMDPs", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-644-653", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A nonparametric approach for policy learning for POMDPs is proposed. The\napproach represents distributions over the states, observations, and actions as\nembeddings in feature spaces, which are reproducing kernel Hilbert spaces.\nDistributions over states given the observations are obtained by applying the\nkernel Bayes' rule to these distribution embeddings. Policies and value\nfunctions are defined on the feature space over states, which leads to a\nfeature space expression for the Bellman equation. Value iteration may then be\nused to estimate the optimal value function and associated policy. Experimental\nresults confirm that the correct policy is learned using the feature space\nrepresentation.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:46:07 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Nishiyama", "Yu", ""], ["Boularias", "Abdeslam", ""], ["Gretton", "Arthur", ""], ["Fukumizu", "Kenji", ""]]}, {"id": "1210.4888", "submitter": "Teppo Niinimaki", "authors": "Teppo Niinimaki, Pekka Parviainen", "title": "Local Structure Discovery in Bayesian Networks", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-634-643", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a Bayesian network structure from data is an NP-hard problem and\nthus exact algorithms are feasible only for small data sets. Therefore, network\nstructures for larger networks are usually learned with various heuristics.\nAnother approach to scaling up the structure learning is local learning. In\nlocal learning, the modeler has one or more target variables that are of\nspecial interest; he wants to learn the structure near the target variables and\nis not interested in the rest of the variables. In this paper, we present a\nscore-based local learning algorithm called SLL. We conjecture that our\nalgorithm is theoretically sound in the sense that it is optimal in the limit\nof large sample size. Empirical results suggest that SLL is competitive when\ncompared to the constraint-based HITON algorithm. We also study the prospects\nof constructing the network structure for the whole node set based on local\nresults by presenting two algorithms and comparing them to several heuristics.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:46:17 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Niinimaki", "Teppo", ""], ["Parviainen", "Pekka", ""]]}, {"id": "1210.4889", "submitter": "Kira Mourao", "authors": "Kira Mourao, Luke S. Zettlemoyer, Ronald P. A. Petrick, Mark Steedman", "title": "Learning STRIPS Operators from Noisy and Incomplete Observations", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-614-623", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Agents learning to act autonomously in real-world domains must acquire a\nmodel of the dynamics of the domain in which they operate. Learning domain\ndynamics can be challenging, especially where an agent only has partial access\nto the world state, and/or noisy external sensors. Even in standard STRIPS\ndomains, existing approaches cannot learn from noisy, incomplete observations\ntypical of real-world domains. We propose a method which learns STRIPS action\nmodels in such domains, by decomposing the problem into first learning a\ntransition function between states in the form of a set of classifiers, and\nthen deriving explicit STRIPS rules from the classifiers' parameters. We\nevaluate our approach on simulated standard planning domains from the\nInternational Planning Competition, and show that it learns useful domain\ndescriptions from noisy, incomplete observations.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:46:26 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Mourao", "Kira", ""], ["Zettlemoyer", "Luke S.", ""], ["Petrick", "Ronald P. A.", ""], ["Steedman", "Mark", ""]]}, {"id": "1210.4892", "submitter": "Marwan A. Mattar", "authors": "Marwan A. Mattar, Allen R. Hanson, Erik G. Learned-Miller", "title": "Unsupervised Joint Alignment and Clustering using Bayesian\n  Nonparametrics", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-584-593", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint alignment of a collection of functions is the process of independently\ntransforming the functions so that they appear more similar to each other.\nTypically, such unsupervised alignment algorithms fail when presented with\ncomplex data sets arising from multiple modalities or make restrictive\nassumptions about the form of the functions or transformations, limiting their\ngenerality. We present a transformed Bayesian infinite mixture model that can\nsimultaneously align and cluster a data set. Our model and associated learning\nscheme offer two key advantages: the optimal number of clusters is determined\nin a data-driven fashion through the use of a Dirichlet process prior, and it\ncan accommodate any transformation function parameterized by a continuous\nparameter vector. As a result, it is applicable to a wide range of data types,\nand transformation functions. We present positive results on synthetic\ntwo-dimensional data, on a set of one-dimensional curves, and on various image\ndata sets, showing large improvements over previous work. We discuss several\nvariations of the model and conclude with directions for future work.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:47:18 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Mattar", "Marwan A.", ""], ["Hanson", "Allen R.", ""], ["Learned-Miller", "Erik G.", ""]]}, {"id": "1210.4893", "submitter": "Sridhar Mahadevan", "authors": "Sridhar Mahadevan, Bo Liu", "title": "Sparse Q-learning with Mirror Descent", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-564-573", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores a new framework for reinforcement learning based on\nonline convex optimization, in particular mirror descent and related\nalgorithms. Mirror descent can be viewed as an enhanced gradient method,\nparticularly suited to minimization of convex functions in highdimensional\nspaces. Unlike traditional gradient methods, mirror descent undertakes gradient\nupdates of weights in both the dual space and primal space, which are linked\ntogether using a Legendre transform. Mirror descent can be viewed as a proximal\nalgorithm where the distance generating function used is a Bregman divergence.\nA new class of proximal-gradient based temporal-difference (TD) methods are\npresented based on different Bregman divergences, which are more powerful than\nregular TD learning. Examples of Bregman divergences that are studied include\np-norm functions, and Mahalanobis distance based on the covariance of sample\ngradients. A new family of sparse mirror-descent reinforcement learning methods\nare proposed, which are able to find sparse fixed points of an l1-regularized\nBellman equation at significantly less computational cost than previous methods\nbased on second-order matrix methods. An experimental study of mirror-descent\nreinforcement learning is presented using discrete and continuous Markov\ndecision processes.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:47:32 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Mahadevan", "Sridhar", ""], ["Liu", "Bo", ""]]}, {"id": "1210.4896", "submitter": "Daniel Lowd", "authors": "Daniel Lowd", "title": "Closed-Form Learning of Markov Networks from Dependency Networks", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-533-542", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov networks (MNs) are a powerful way to compactly represent a joint\nprobability distribution, but most MN structure learning methods are very slow,\ndue to the high cost of evaluating candidates structures. Dependency networks\n(DNs) represent a probability distribution as a set of conditional probability\ndistributions. DNs are very fast to learn, but the conditional distributions\nmay be inconsistent with each other and few inference algorithms support DNs.\nIn this paper, we present a closed-form method for converting a DN into an MN,\nallowing us to enjoy both the efficiency of DN learning and the convenience of\nthe MN representation. When the DN is consistent, this conversion is exact. For\ninconsistent DNs, we present averaging methods that significantly improve the\napproximation. In experiments on 12 standard datasets, our methods are orders\nof magnitude faster than and often more accurate than combining conditional\ndistributions using weight learning.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:48:08 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Lowd", "Daniel", ""]]}, {"id": "1210.4898", "submitter": "Gavin Taylor", "authors": "Gavin Taylor, Ron Parr", "title": "Value Function Approximation in Noisy Environments Using Locally\n  Smoothed Regularized Approximate Linear Programs", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-835-842", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Petrik et al. demonstrated that L1Regularized Approximate Linear\nProgramming (RALP) could produce value functions and policies which compared\nfavorably to established linear value function approximation techniques like\nLSPI. RALP's success primarily stems from the ability to solve the feature\nselection and value function approximation steps simultaneously. RALP's\nperformance guarantees become looser if sampled next states are used. For very\nnoisy domains, RALP requires an accurate model rather than samples, which can\nbe unrealistic in some practical scenarios. In this paper, we demonstrate this\nweakness, and then introduce Locally Smoothed L1-Regularized Approximate Linear\nProgramming (LS-RALP). We demonstrate that LS-RALP mitigates inaccuracies\nstemming from noise even without an accurate model. We show that, given some\nsmoothness assumptions, as the number of samples increases, error from noise\napproaches zero, and provide experimental examples of LS-RALP's success on\ncommon reinforcement learning benchmark problems.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:50:15 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Taylor", "Gavin", ""], ["Parr", "Ron", ""]]}, {"id": "1210.4899", "submitter": "Daniel Tarlow", "authors": "Daniel Tarlow, Kevin Swersky, Richard S. Zemel, Ryan Prescott Adams,\n  Brendan J. Frey", "title": "Fast Exact Inference for Recursive Cardinality Models", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-825-834", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardinality potentials are a generally useful class of high order potential\nthat affect probabilities based on how many of D binary variables are active.\nMaximum a posteriori (MAP) inference for cardinality potential models is\nwell-understood, with efficient computations taking O(DlogD) time. Yet\nefficient marginalization and sampling have not been addressed as thoroughly in\nthe machine learning community. We show that there exists a simple algorithm\nfor computing marginal probabilities and drawing exact joint samples that runs\nin O(Dlog2 D) time, and we show how to frame the algorithm as efficient belief\npropagation in a low order tree-structured model that includes additional\nauxiliary variables. We then develop a new, more general class of models,\ntermed Recursive Cardinality models, which take advantage of this efficiency.\nFinally, we show how to do efficient exact inference in models composed of a\ntree structure and a cardinality potential. We explore the expressive power of\nRecursive Cardinality models and empirically demonstrate their utility.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:50:25 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Tarlow", "Daniel", ""], ["Swersky", "Kevin", ""], ["Zemel", "Richard S.", ""], ["Adams", "Ryan Prescott", ""], ["Frey", "Brendan J.", ""]]}, {"id": "1210.4902", "submitter": "David Sontag", "authors": "David Sontag, Do Kook Choe, Yitao Li", "title": "Efficiently Searching for Frustrated Cycles in MAP Inference", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-795-804", "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dual decomposition provides a tractable framework for designing algorithms\nfor finding the most probable (MAP) configuration in graphical models. However,\nfor many real-world inference problems, the typical decomposition has a large\nintegrality gap, due to frustrated cycles. One way to tighten the relaxation is\nto introduce additional constraints that explicitly enforce cycle consistency.\nEarlier work showed that cluster-pursuit algorithms, which iteratively\nintroduce cycle and other higherorder consistency constraints, allows one to\nexactly solve many hard inference problems. However, these algorithms\nexplicitly enumerate a candidate set of clusters, limiting them to triplets or\nother short cycles. We solve the search problem for cycle constraints, giving a\nnearly linear time algorithm for finding the most frustrated cycle of arbitrary\nlength. We show how to use this search algorithm together with the dual\ndecomposition framework and clusterpursuit. The new algorithm exactly solves\nMAP inference problems arising from relational classification and stereo\nvision.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:51:21 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Sontag", "David", ""], ["Choe", "Do Kook", ""], ["Li", "Yitao", ""]]}, {"id": "1210.4905", "submitter": "Ricardo Silva", "authors": "Ricardo Silva", "title": "Latent Composite Likelihood Learning for the Structured Canonical\n  Correlation Model", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-765-774", "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent variable models are used to estimate variables of interest quantities\nwhich are observable only up to some measurement error. In many studies, such\nvariables are known but not precisely quantifiable (such as \"job satisfaction\"\nin social sciences and marketing, \"analytical ability\" in educational testing,\nor \"inflation\" in economics). This leads to the development of measurement\ninstruments to record noisy indirect evidence for such unobserved variables\nsuch as surveys, tests and price indexes. In such problems, there are\npostulated latent variables and a given measurement model. At the same time,\nother unantecipated latent variables can add further unmeasured confounding to\nthe observed variables. The problem is how to deal with unantecipated latents\nvariables. In this paper, we provide a method loosely inspired by canonical\ncorrelation that makes use of background information concerning the \"known\"\nlatent variables. Given a partially specified structure, it provides a\nstructure learning approach to detect \"unknown unknowns,\" the confounding\neffect of potentially infinitely many other latent variables. This is done\nwithout explicitly modeling such extra latent factors. Because of the special\nstructure of the problem, we are able to exploit a new variation of composite\nlikelihood fitting to efficiently learn this structure. Validation is provided\nwith experiments in synthetic data and the analysis of a large survey done with\na sample of over 100,000 staff members of the National Health Service of the\nUnited Kingdom.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:51:50 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Silva", "Ricardo", ""]]}, {"id": "1210.4909", "submitter": "Jens Roeder", "authors": "Jens Roeder, Boaz Nadler, Kevin Kunzmann, Fred A. Hamprecht", "title": "Active Learning with Distributional Estimates", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-715-725", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active Learning (AL) is increasingly important in a broad range of\napplications. Two main AL principles to obtain accurate classification with few\nlabeled data are refinement of the current decision boundary and exploration of\npoorly sampled regions. In this paper we derive a novel AL scheme that balances\nthese two principles in a natural way. In contrast to many AL strategies, which\nare based on an estimated class conditional probability ^p(y|x), a key\ncomponent of our approach is to view this quantity as a random variable, hence\nexplicitly considering the uncertainty in its estimated value. Our main\ncontribution is a novel mathematical framework for uncertainty-based AL, and a\ncorresponding AL scheme, where the uncertainty in ^p(y|x) is modeled by a\nsecond-order distribution. On the practical side, we show how to approximate\nsuch second-order distributions for kernel density classification. Finally, we\nfind that over a large number of UCI, USPS and Caltech4 datasets, our AL scheme\nachieves significantly better learning curves than popular AL methods such as\nuncertainty sampling and error reduction sampling, when all use the same kernel\ndensity classifier.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:53:17 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Roeder", "Jens", ""], ["Nadler", "Boaz", ""], ["Kunzmann", "Kevin", ""], ["Hamprecht", "Fred A.", ""]]}, {"id": "1210.4910", "submitter": "Khaled S. Refaat", "authors": "Khaled S. Refaat, Arthur Choi, Adnan Darwiche", "title": "New Advances and Theoretical Insights into EDML", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-705-714", "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  EDML is a recently proposed algorithm for learning MAP parameters in Bayesian\nnetworks. In this paper, we present a number of new advances and insights on\nthe EDML algorithm. First, we provide the multivalued extension of EDML,\noriginally proposed for Bayesian networks over binary variables. Next, we\nidentify a simplified characterization of EDML that further implies a simple\nfixed-point algorithm for the convex optimization problem that underlies it.\nThis characterization further reveals a connection between EDML and EM: a fixed\npoint of EDML is a fixed point of EM, and vice versa. We thus identify also a\nnew characterization of EM fixed points, but in the semantics of EDML. Finally,\nwe propose a hybrid EDML/EM algorithm that takes advantage of the improved\nempirical convergence behavior of EDML, while maintaining the monotonic\nimprovement property of EM.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:53:29 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Refaat", "Khaled S.", ""], ["Choi", "Arthur", ""], ["Darwiche", "Adnan", ""]]}, {"id": "1210.4913", "submitter": "Changhe Yuan", "authors": "Changhe Yuan, Brandon Malone", "title": "An Improved Admissible Heuristic for Learning Optimal Bayesian Networks", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-924-933", "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently two search algorithms, A* and breadth-first branch and bound\n(BFBnB), were developed based on a simple admissible heuristic for learning\nBayesian network structures that optimize a scoring function. The heuristic\nrepresents a relaxation of the learning problem such that each variable chooses\noptimal parents independently. As a result, the heuristic may contain many\ndirected cycles and result in a loose bound. This paper introduces an improved\nadmissible heuristic that tries to avoid directed cycles within small groups of\nvariables. A sparse representation is also introduced to store only the unique\noptimal parent choices. Empirical results show that the new techniques\nsignificantly improved the efficiency and scalability of A* and BFBnB on most\nof datasets tested in this paper.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:55:57 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Yuan", "Changhe", ""], ["Malone", "Brandon", ""]]}, {"id": "1210.4914", "submitter": "Jason Weston", "authors": "Jason Weston, John Blitzer", "title": "Latent Structured Ranking", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-903-913", "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many latent (factorized) models have been proposed for recommendation tasks\nlike collaborative filtering and for ranking tasks like document or image\nretrieval and annotation. Common to all those methods is that during inference\nthe items are scored independently by their similarity to the query in the\nlatent embedding space. The structure of the ranked list (i.e. considering the\nset of items returned as a whole) is not taken into account. This can be a\nproblem because the set of top predictions can be either too diverse (contain\nresults that contradict each other) or are not diverse enough. In this paper we\nintroduce a method for learning latent structured rankings that improves over\nexisting methods by providing the right blend of predictions at the top of the\nranked list. Particular emphasis is put on making this method scalable.\nEmpirical results on large scale image annotation and music recommendation\ntasks show improvements over existing approaches.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:56:08 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Weston", "Jason", ""], ["Blitzer", "John", ""]]}, {"id": "1210.4917", "submitter": "Jun Wang", "authors": "Jun Wang, Yinglong Xia", "title": "Fast Graph Construction Using Auction Algorithm", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-873-882", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In practical machine learning systems, graph based data representation has\nbeen widely used in various learning paradigms, ranging from unsupervised\nclustering to supervised classification. Besides those applications with\nnatural graph or network structure data, such as social network analysis and\nrelational learning, many other applications often involve a critical step in\nconverting data vectors to an adjacency graph. In particular, a sparse subgraph\nextracted from the original graph is often required due to both theoretic and\npractical needs. Previous study clearly shows that the performance of different\nlearning algorithms, e.g., clustering and classification, benefits from such\nsparse subgraphs with balanced node connectivity. However, the existing graph\nconstruction methods are either computationally expensive or with\nunsatisfactory performance. In this paper, we utilize a scalable method called\nauction algorithm and its parallel extension to recover a sparse yet nearly\nbalanced subgraph with significantly reduced computational cost. Empirical\nstudy and comparison with the state-ofart approaches clearly demonstrate the\nsuperiority of the proposed method in both efficiency and accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:56:43 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Wang", "Jun", ""], ["Xia", "Yinglong", ""]]}, {"id": "1210.4918", "submitter": "Thomas J. Walsh", "authors": "Thomas J. Walsh, Sergiu Goschin", "title": "Dynamic Teaching in Sequential Decision Making Environments", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-863-872", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe theoretical bounds and a practical algorithm for teaching a model\nby demonstration in a sequential decision making environment. Unlike previous\nefforts that have optimized learners that watch a teacher demonstrate a static\npolicy, we focus on the teacher as a decision maker who can dynamically choose\ndifferent policies to teach different parts of the environment. We develop\nseveral teaching frameworks based on previously defined supervised protocols,\nsuch as Teaching Dimension, extending them to handle noise and sequences of\ninputs encountered in an MDP.We provide theoretical bounds on the learnability\nof several important model classes in this setting and suggest a practical\nalgorithm for dynamic teaching.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:56:54 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Walsh", "Thomas J.", ""], ["Goschin", "Sergiu", ""]]}, {"id": "1210.4919", "submitter": "Mirwaes Wahabzada", "authors": "Mirwaes Wahabzada, Kristian Kersting, Christian Bauckhage, Christoph\n  Roemer, Agim Ballvora, Francisco Pinto, Uwe Rascher, Jens Leon, Lutz Ploemer", "title": "Latent Dirichlet Allocation Uncovers Spectral Characteristics of Drought\n  Stressed Plants", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-852-862", "categories": "cs.LG cs.CE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the adaptation process of plants to drought stress is essential\nin improving management practices, breeding strategies as well as engineering\nviable crops for a sustainable agriculture in the coming decades.\nHyper-spectral imaging provides a particularly promising approach to gain such\nunderstanding since it allows to discover non-destructively spectral\ncharacteristics of plants governed primarily by scattering and absorption\ncharacteristics of the leaf internal structure and biochemical constituents.\nSeveral drought stress indices have been derived using hyper-spectral imaging.\nHowever, they are typically based on few hyper-spectral images only, rely on\ninterpretations of experts, and consider few wavelengths only. In this study,\nwe present the first data-driven approach to discovering spectral drought\nstress indices, treating it as an unsupervised labeling problem at massive\nscale. To make use of short range dependencies of spectral wavelengths, we\ndevelop an online variational Bayes algorithm for latent Dirichlet allocation\nwith convolved Dirichlet regularizer. This approach scales to massive datasets\nand, hence, provides a more objective complement to plant physiological\npractices. The spectral topics found conform to plant physiological knowledge\nand can be computed in a fraction of the time compared to existing LDA\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:57:06 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Wahabzada", "Mirwaes", ""], ["Kersting", "Kristian", ""], ["Bauckhage", "Christian", ""], ["Roemer", "Christoph", ""], ["Ballvora", "Agim", ""], ["Pinto", "Francisco", ""], ["Rascher", "Uwe", ""], ["Leon", "Jens", ""], ["Ploemer", "Lutz", ""]]}, {"id": "1210.4920", "submitter": "Seppo Virtanen", "authors": "Seppo Virtanen, Yangqing Jia, Arto Klami, Trevor Darrell", "title": "Factorized Multi-Modal Topic Model", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-843-851", "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-modal data collections, such as corpora of paired images and text\nsnippets, require analysis methods beyond single-view component and topic\nmodels. For continuous observations the current dominant approach is based on\nextensions of canonical correlation analysis, factorizing the variation into\ncomponents shared by the different modalities and those private to each of\nthem. For count data, multiple variants of topic models attempting to tie the\nmodalities together have been presented. All of these, however, lack the\nability to learn components private to one modality, and consequently will try\nto force dependencies even between minimally correlating modalities. In this\nwork we combine the two approaches by presenting a novel HDP-based topic model\nthat automatically learns both shared and private topics. The model is shown to\nbe especially useful for querying the contents of one domain given samples of\nthe other.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:57:22 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Virtanen", "Seppo", ""], ["Jia", "Yangqing", ""], ["Klami", "Arto", ""], ["Darrell", "Trevor", ""]]}, {"id": "1210.5135", "submitter": "Yang Lu", "authors": "Yang Lu, Mengying Wang, Menglu Li, Qili Zhu, Bo Yuan", "title": "LSBN: A Large-Scale Bayesian Structure Learning Framework for Model\n  Averaging", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The motivation for this paper is to apply Bayesian structure learning using\nModel Averaging in large-scale networks. Currently, Bayesian model averaging\nalgorithm is applicable to networks with only tens of variables, restrained by\nits super-exponential complexity. We present a novel framework, called\nLSBN(Large-Scale Bayesian Network), making it possible to handle networks with\ninfinite size by following the principle of divide-and-conquer. The method of\nLSBN comprises three steps. In general, LSBN first performs the partition by\nusing a second-order partition strategy, which achieves more robust results.\nLSBN conducts sampling and structure learning within each overlapping community\nafter the community is isolated from other variables by Markov Blanket. Finally\nLSBN employs an efficient algorithm, to merge structures of overlapping\ncommunities into a whole. In comparison with other four state-of-art\nlarge-scale network structure learning algorithms such as ARACNE, PC, Greedy\nSearch and MMHC, LSBN shows comparable results in five common benchmark\ndatasets, evaluated by precision, recall and f-score. What's more, LSBN makes\nit possible to learn large-scale Bayesian structure by Model Averaging which\nused to be intractable. In summary, LSBN provides an scalable and parallel\nframework for the reconstruction of network structures. Besides, the complete\ninformation of overlapping communities serves as the byproduct, which could be\nused to mine meaningful clusters in biological networks, such as\nprotein-protein-interaction network or gene regulatory network, as well as in\nsocial network.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2012 14:15:40 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Lu", "Yang", ""], ["Wang", "Mengying", ""], ["Li", "Menglu", ""], ["Zhu", "Qili", ""], ["Yuan", "Bo", ""]]}, {"id": "1210.5196", "submitter": "Rina Foygel", "authors": "Rina Foygel, Nathan Srebro, Ruslan Salakhutdinov", "title": "Matrix reconstruction with the local max norm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new family of matrix norms, the \"local max\" norms,\ngeneralizing existing methods such as the max norm, the trace norm (nuclear\nnorm), and the weighted or smoothed weighted trace norms, which have been\nextensively used in the literature as regularizers for matrix reconstruction\nproblems. We show that this new family can be used to interpolate between the\n(weighted or unweighted) trace norm and the more conservative max norm. We test\nthis interpolation on simulated data and on the large-scale Netflix and\nMovieLens ratings data, and find improved accuracy relative to the existing\nmatrix norms. We also provide theoretical results showing learning guarantees\nfor some of the new norms.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2012 17:30:43 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Foygel", "Rina", ""], ["Srebro", "Nathan", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1210.5338", "submitter": "Cyril Furtlehner", "authors": "Cyril Furtlehner, Yufei Han, Jean-Marc Lasgouttes and Victorin Martin", "title": "Pairwise MRF Calibration by Perturbation of the Bethe Reference Point", "comments": "54 pages, 8 figure. section 5 and refs added in V2", "journal-ref": null, "doi": null, "report-no": "Inria RR-8059", "categories": "cond-mat.dis-nn cond-mat.stat-mech cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate different ways of generating approximate solutions to the\npairwise Markov random field (MRF) selection problem. We focus mainly on the\ninverse Ising problem, but discuss also the somewhat related inverse Gaussian\nproblem because both types of MRF are suitable for inference tasks with the\nbelief propagation algorithm (BP) under certain conditions. Our approach\nconsists in to take a Bethe mean-field solution obtained with a maximum\nspanning tree (MST) of pairwise mutual information, referred to as the\n\\emph{Bethe reference point}, for further perturbation procedures. We consider\nthree different ways following this idea: in the first one, we select and\ncalibrate iteratively the optimal links to be added starting from the Bethe\nreference point; the second one is based on the observation that the natural\ngradient can be computed analytically at the Bethe point; in the third one,\nassuming no local field and using low temperature expansion we develop a dual\nloop joint model based on a well chosen fundamental cycle basis. We indeed\nidentify a subclass of planar models, which we refer to as \\emph{Bethe-dual\ngraph models}, having possibly many loops, but characterized by a singly\nconnected dual factor graph, for which the partition function and the linear\nresponse can be computed exactly in respectively O(N) and $O(N^2)$ operations,\nthanks to a dual weight propagation (DWP) message passing procedure that we set\nup. When restricted to this subclass of models, the inverse Ising problem being\nconvex, becomes tractable at any temperature. Experimental tests on various\ndatasets with refined $L_0$ or $L_1$ regularization procedures indicate that\nthese approaches may be competitive and useful alternatives to existing ones.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 08:08:55 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2013 17:32:44 GMT"}], "update_date": "2013-02-04", "authors_parsed": [["Furtlehner", "Cyril", ""], ["Han", "Yufei", ""], ["Lasgouttes", "Jean-Marc", ""], ["Martin", "Victorin", ""]]}, {"id": "1210.5345", "submitter": "Alexandra Carpentier", "authors": "Alexandra Carpentier and R\\'emi Munos", "title": "Adaptive Stratified Sampling for Monte-Carlo integration of\n  Differentiable functions", "comments": "23 pages, 3 figures, to appear in NIPS 2012 conference proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of adaptive stratified sampling for Monte Carlo\nintegration of a differentiable function given a finite number of evaluations\nto the function. We construct a sampling scheme that samples more often in\nregions where the function oscillates more, while allocating the samples such\nthat they are well spread on the domain (this notion shares similitude with low\ndiscrepancy). We prove that the estimate returned by the algorithm is almost\nsimilarly accurate as the estimate that an optimal oracle strategy (that would\nknow the variations of the function everywhere) would return, and provide a\nfinite-sample analysis.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 09:03:24 GMT"}], "update_date": "2012-10-22", "authors_parsed": [["Carpentier", "Alexandra", ""], ["Munos", "R\u00e9mi", ""]]}, {"id": "1210.5474", "submitter": "Guillaume Desjardins", "authors": "Guillaume Desjardins and Aaron Courville and Yoshua Bengio", "title": "Disentangling Factors of Variation via Generative Entangling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we propose a novel model family with the objective of learning to\ndisentangle the factors of variation in data. Our approach is based on the\nspike-and-slab restricted Boltzmann machine which we generalize to include\nhigher-order interactions among multiple latent variables. Seen from a\ngenerative perspective, the multiplicative interactions emulates the entangling\nof factors of variation. Inference in the model can be seen as disentangling\nthese generative factors. Unlike previous attempts at disentangling latent\nfactors, the proposed model is trained using no supervised information\nregarding the latent factors. We apply our model to the task of facial\nexpression classification.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 17:16:48 GMT"}], "update_date": "2012-10-22", "authors_parsed": [["Desjardins", "Guillaume", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1210.5631", "submitter": "Mu Zhu", "authors": "Jennifer Nguyen, Mu Zhu", "title": "Content-boosted Matrix Factorization Techniques for Recommender Systems", "comments": null, "journal-ref": "Statistical Analysis and Data Mining, Vol. 6, pp. 286 - 301,\n  August 2013", "doi": "10.1002/sam.11184", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many businesses are using recommender systems for marketing outreach.\nRecommendation algorithms can be either based on content or driven by\ncollaborative filtering. We study different ways to incorporate content\ninformation directly into the matrix factorization approach of collaborative\nfiltering. These content-boosted matrix factorization algorithms not only\nimprove recommendation accuracy, but also provide useful insights about the\ncontents, as well as make recommendations more easily interpretable.\n", "versions": [{"version": "v1", "created": "Sat, 20 Oct 2012 14:39:39 GMT"}, {"version": "v2", "created": "Fri, 4 Jan 2013 22:52:39 GMT"}], "update_date": "2013-08-09", "authors_parsed": [["Nguyen", "Jennifer", ""], ["Zhu", "Mu", ""]]}, {"id": "1210.5806", "submitter": "Pinghua Gong", "authors": "Pinghua Gong, Jieping Ye, Changshui Zhang", "title": "Multi-Stage Multi-Task Feature Learning", "comments": "The short version appears in NIPS 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task sparse feature learning aims to improve the generalization\nperformance by exploiting the shared features among tasks. It has been\nsuccessfully applied to many applications including computer vision and\nbiomedical informatics. Most of the existing multi-task sparse feature learning\nalgorithms are formulated as a convex sparse regularization problem, which is\nusually suboptimal, due to its looseness for approximating an $\\ell_0$-type\nregularizer. In this paper, we propose a non-convex formulation for multi-task\nsparse feature learning based on a novel non-convex regularizer. To solve the\nnon-convex optimization problem, we propose a Multi-Stage Multi-Task Feature\nLearning (MSMTFL) algorithm; we also provide intuitive interpretations,\ndetailed convergence and reproducibility analysis for the proposed algorithm.\nMoreover, we present a detailed theoretical analysis showing that MSMTFL\nachieves a better parameter estimation error bound than the convex formulation.\nEmpirical studies on both synthetic and real-world data sets demonstrate the\neffectiveness of MSMTFL in comparison with the state of the art multi-task\nsparse feature learning algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2012 05:41:29 GMT"}], "update_date": "2012-10-23", "authors_parsed": [["Gong", "Pinghua", ""], ["Ye", "Jieping", ""], ["Zhang", "Changshui", ""]]}, {"id": "1210.5840", "submitter": "Purushottam Kar", "authors": "Purushottam Kar and Prateek Jain", "title": "Supervised Learning with Similarity Functions", "comments": "To appear in the proceedings of NIPS 2012, 30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of general supervised learning when data can only be\naccessed through an (indefinite) similarity function between data points.\nExisting work on learning with indefinite kernels has concentrated solely on\nbinary/multi-class classification problems. We propose a model that is generic\nenough to handle any supervised learning task and also subsumes the model\npreviously proposed for classification. We give a \"goodness\" criterion for\nsimilarity functions w.r.t. a given supervised learning task and then adapt a\nwell-known landmarking technique to provide efficient algorithms for supervised\nlearning using \"good\" similarity functions. We demonstrate the effectiveness of\nour model on three important super-vised learning problems: a) real-valued\nregression, b) ordinal regression and c) ranking where we show that our method\nguarantees bounded generalization error. Furthermore, for the case of\nreal-valued regression, we give a natural goodness definition that, when used\nin conjunction with a recent result in sparse vector recovery, guarantees a\nsparse predictor with bounded generalization error. Finally, we report results\nof our learning algorithms on regression and ordinal regression tasks using\nnon-PSD similarity functions and demonstrate the effectiveness of our\nalgorithms, especially that of the sparse landmark selection algorithm that\nachieves significantly higher accuracies than the baseline methods while\noffering reduced computational costs.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2012 08:55:13 GMT"}], "update_date": "2012-10-23", "authors_parsed": [["Kar", "Purushottam", ""], ["Jain", "Prateek", ""]]}, {"id": "1210.5873", "submitter": "Ayodeji Akinduko Mr", "authors": "A. A. Akinduko and E. M. Mirkes", "title": "Initialization of Self-Organizing Maps: Principal Components Versus\n  Random Initialization. A Case Study", "comments": "18 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The performance of the Self-Organizing Map (SOM) algorithm is dependent on\nthe initial weights of the map. The different initialization methods can\nbroadly be classified into random and data analysis based initialization\napproach. In this paper, the performance of random initialization (RI) approach\nis compared to that of principal component initialization (PCI) in which the\ninitial map weights are chosen from the space of the principal component.\nPerformance is evaluated by the fraction of variance unexplained (FVU).\nDatasets were classified into quasi-linear and non-linear and it was observed\nthat RI performed better for non-linear datasets; however the performance of\nPCI approach remains inconclusive for quasi-linear datasets.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2012 11:17:31 GMT"}], "update_date": "2012-10-23", "authors_parsed": [["Akinduko", "A. A.", ""], ["Mirkes", "E. M.", ""]]}, {"id": "1210.5992", "submitter": "Jianqing Fan", "authors": "Jianqing Fan, Lingzhou Xue, Hui Zou", "title": "Strong oracle optimality of folded concave penalized estimation", "comments": "Published in at http://dx.doi.org/10.1214/13-AOS1198 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org). With Corrections", "journal-ref": "Annals of Statistics 2014, Vol. 42, No. 3, 819-849", "doi": "10.1214/13-AOS1198", "report-no": "IMS-AOS-AOS1198", "categories": "math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Folded concave penalization methods have been shown to enjoy the strong\noracle property for high-dimensional sparse estimation. However, a folded\nconcave penalization problem usually has multiple local solutions and the\noracle property is established only for one of the unknown local solutions. A\nchallenging fundamental issue still remains that it is not clear whether the\nlocal optimum computed by a given optimization algorithm possesses those nice\ntheoretical properties. To close this important theoretical gap in over a\ndecade, we provide a unified theory to show explicitly how to obtain the oracle\nsolution via the local linear approximation algorithm. For a folded concave\npenalized estimation problem, we show that as long as the problem is\nlocalizable and the oracle estimator is well behaved, we can obtain the oracle\nestimator by using the one-step local linear approximation. In addition, once\nthe oracle estimator is obtained, the local linear approximation algorithm\nconverges, namely it produces the same estimator in the next iteration. The\ngeneral theory is demonstrated by using four classical sparse estimation\nproblems, that is, sparse linear regression, sparse logistic regression, sparse\nprecision matrix estimation and sparse quantile regression.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2012 18:39:03 GMT"}, {"version": "v2", "created": "Fri, 19 Jul 2013 03:16:12 GMT"}, {"version": "v3", "created": "Tue, 27 May 2014 05:30:35 GMT"}, {"version": "v4", "created": "Thu, 2 Apr 2015 11:36:04 GMT"}], "update_date": "2015-04-03", "authors_parsed": [["Fan", "Jianqing", ""], ["Xue", "Lingzhou", ""], ["Zou", "Hui", ""]]}, {"id": "1210.6001", "submitter": "Daniil Ryabko", "authors": "Daniil Ryabko, J\\'er\\'emie Mary", "title": "Reducing statistical time-series problems to binary classification", "comments": "In proceedings of NIPS 2012, pp. 2069-2077", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how binary classification methods developed to work on i.i.d. data\ncan be used for solving statistical problems that are seemingly unrelated to\nclassification and concern highly-dependent time series. Specifically, the\nproblems of time-series clustering, homogeneity testing and the three-sample\nproblem are addressed. The algorithms that we construct for solving these\nproblems are based on a new metric between time-series distributions, which can\nbe evaluated using binary classification methods. Universal consistency of the\nproposed algorithms is proven under most general assumptions. The theoretical\nresults are illustrated with experiments on synthetic and real-world data.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2012 19:02:21 GMT"}, {"version": "v2", "created": "Mon, 28 Jan 2013 10:25:38 GMT"}, {"version": "v3", "created": "Fri, 7 Jun 2013 09:45:45 GMT"}], "update_date": "2013-06-10", "authors_parsed": [["Ryabko", "Daniil", ""], ["Mary", "J\u00e9r\u00e9mie", ""]]}, {"id": "1210.6170", "submitter": "Minh Ha Quang", "authors": "Minh Ha Quang", "title": "Further properties of Gaussian Reproducing Kernel Hilbert Spaces", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalize the orthonormal basis for the Gaussian RKHS described in\n\\cite{MinhGaussian2010} to an infinite, continuously parametrized, family of\northonormal bases, along with some implications. The proofs are direct\ngeneralizations of those in \\cite{MinhGaussian2010}.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2012 09:05:45 GMT"}], "update_date": "2012-10-24", "authors_parsed": [["Quang", "Minh Ha", ""]]}, {"id": "1210.6317", "submitter": "Shivakumar Viswanathan", "authors": "Shivakumar Viswanathan, Matthew Cieslak and Scott T. Grafton", "title": "On the geometric structure of fMRI searchlight-based information maps", "comments": "15 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC q-bio.QM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information mapping is a popular application of Multivoxel Pattern Analysis\n(MVPA) to fMRI. Information maps are constructed using the so called\nsearchlight method, where the spherical multivoxel neighborhood of every voxel\n(i.e., a searchlight) in the brain is evaluated for the presence of\ntask-relevant response patterns. Despite their widespread use, information maps\npresent several challenges for interpretation. One such challenge has to do\nwith inferring the size and shape of a multivoxel pattern from its signature on\nthe information map. To address this issue, we formally examined the geometric\nbasis of this mapping relationship. Based on geometric considerations, we show\nhow and why small patterns (i.e., having smaller spatial extents) can produce a\nlarger signature on the information map as compared to large patterns,\nindependent of the size of the searchlight radius. Furthermore, we show that\nthe number of informative searchlights over the brain increase as a function of\nsearchlight radius, even in the complete absence of any multivariate response\npatterns. These properties are unrelated to the statistical capabilities of the\npattern-analysis algorithms used but are obligatory geometric properties\narising from using the searchlight procedure.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2012 18:21:16 GMT"}], "update_date": "2012-10-24", "authors_parsed": [["Viswanathan", "Shivakumar", ""], ["Cieslak", "Matthew", ""], ["Grafton", "Scott T.", ""]]}, {"id": "1210.6321", "submitter": "Ryohei Hisano", "authors": "Ryohei Hisano, Didier Sornette, Takayuki Mizuno, Takaaki Ohnishi,\n  Tsutomu Watanabe", "title": "High quality topic extraction from business news explains abnormal\n  financial market volatility", "comments": "The previous version of this article included an error. This is a\n  revised version", "journal-ref": null, "doi": "10.1371/journal.pone.0064846", "report-no": null, "categories": "stat.ML cs.LG cs.SI physics.soc-ph q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the mutual relationships between information flows and social\nactivity in society today is one of the cornerstones of the social sciences. In\nfinancial economics, the key issue in this regard is understanding and\nquantifying how news of all possible types (geopolitical, environmental,\nsocial, financial, economic, etc.) affect trading and the pricing of firms in\norganized stock markets. In this article, we seek to address this issue by\nperforming an analysis of more than 24 million news records provided by\nThompson Reuters and of their relationship with trading activity for 206 major\nstocks in the S&P US stock index. We show that the whole landscape of news that\naffect stock price movements can be automatically summarized via simple\nregularized regressions between trading activity and news information pieces\ndecomposed, with the help of simple topic modeling techniques, into their\n\"thematic\" features. Using these methods, we are able to estimate and quantify\nthe impacts of news on trading. We introduce network-based visualization\ntechniques to represent the whole landscape of news information associated with\na basket of stocks. The examination of the words that are representative of the\ntopic distributions confirms that our method is able to extract the significant\npieces of information influencing the stock market. Our results show that one\nof the most puzzling stylized fact in financial economies, namely that at\ncertain times trading volumes appear to be \"abnormally large,\" can be partially\nexplained by the flow of news. In this sense, our results prove that there is\nno \"excess trading,\" when restricting to times when news are genuinely novel\nand provide relevant financial information.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2012 18:31:46 GMT"}, {"version": "v2", "created": "Wed, 24 Oct 2012 17:16:47 GMT"}, {"version": "v3", "created": "Tue, 19 Mar 2013 10:29:11 GMT"}, {"version": "v4", "created": "Sat, 23 Mar 2013 14:34:36 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Hisano", "Ryohei", ""], ["Sornette", "Didier", ""], ["Mizuno", "Takayuki", ""], ["Ohnishi", "Takaaki", ""], ["Watanabe", "Tsutomu", ""]]}, {"id": "1210.6511", "submitter": "Fabrice Rossi", "authors": "Marie Cottrell (SAMM), Madalina Olteanu (SAMM), Fabrice Rossi (SAMM),\n  Joseph Rynkiewicz (SAMM), Nathalie Villa-Vialaneix (SAMM)", "title": "Neural Networks for Complex Data", "comments": null, "journal-ref": "K\\\"unstliche Intelligenz 26, 4 (2012) 373-380", "doi": "10.1007/s13218-012-0207-2", "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks are simple and efficient machine learning tools.\nDefined originally in the traditional setting of simple vector data, neural\nnetwork models have evolved to address more and more difficulties of complex\nreal world problems, ranging from time evolving data to sophisticated data\nstructures such as graphs and functions. This paper summarizes advances on\nthose themes from the last decade, with a focus on results obtained by members\nof the SAMM team of Universit\\'e Paris 1\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2012 12:37:53 GMT"}], "update_date": "2012-10-26", "authors_parsed": [["Cottrell", "Marie", "", "SAMM"], ["Olteanu", "Madalina", "", "SAMM"], ["Rossi", "Fabrice", "", "SAMM"], ["Rynkiewicz", "Joseph", "", "SAMM"], ["Villa-Vialaneix", "Nathalie", "", "SAMM"]]}, {"id": "1210.6707", "submitter": "Emanuele Coviello", "authors": "Emanuele Coviello and Antoni B. Chan and Gert R.G. Lanckriet", "title": "Clustering hidden Markov models with variational HEM", "comments": "44 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The hidden Markov model (HMM) is a widely-used generative model that copes\nwith sequential data, assuming that each observation is conditioned on the\nstate of a hidden Markov chain. In this paper, we derive a novel algorithm to\ncluster HMMs based on the hierarchical EM (HEM) algorithm. The proposed\nalgorithm i) clusters a given collection of HMMs into groups of HMMs that are\nsimilar, in terms of the distributions they represent, and ii) characterizes\neach group by a \"cluster center\", i.e., a novel HMM that is representative for\nthe group, in a manner that is consistent with the underlying generative model\nof the HMM. To cope with intractable inference in the E-step, the HEM algorithm\nis formulated as a variational optimization problem, and efficiently solved for\nthe HMM case by leveraging an appropriate variational approximation. The\nbenefits of the proposed algorithm, which we call variational HEM (VHEM), are\ndemonstrated on several tasks involving time-series data, such as hierarchical\nclustering of motion capture sequences, and automatic annotation and retrieval\nof music and of online hand-writing data, showing improvements over current\nmethods. In particular, our variational HEM algorithm effectively leverages\nlarge amounts of data when learning annotation models by using an efficient\nhierarchical estimation procedure, which reduces learning times and memory\nrequirements, while improving model robustness through better regularization.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2012 23:57:35 GMT"}], "update_date": "2012-10-26", "authors_parsed": [["Coviello", "Emanuele", ""], ["Chan", "Antoni B.", ""], ["Lanckriet", "Gert R. G.", ""]]}, {"id": "1210.6738", "submitter": "John Paisley", "authors": "John Paisley, Chong Wang, David M. Blei and Michael I. Jordan", "title": "Nested Hierarchical Dirichlet Processes", "comments": "To appear in IEEE Transactions on Pattern Analysis and Machine\n  Intelligence, Special Issue on Bayesian Nonparametrics", "journal-ref": null, "doi": "10.1109/TPAMI.2014.2318728", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a nested hierarchical Dirichlet process (nHDP) for hierarchical\ntopic modeling. The nHDP is a generalization of the nested Chinese restaurant\nprocess (nCRP) that allows each word to follow its own path to a topic node\naccording to a document-specific distribution on a shared tree. This alleviates\nthe rigid, single-path formulation of the nCRP, allowing a document to more\neasily express thematic borrowings as a random effect. We derive a stochastic\nvariational inference algorithm for the model, in addition to a greedy subtree\nselection method for each document, which allows for efficient inference using\nmassive collections of text documents. We demonstrate our algorithm on 1.8\nmillion documents from The New York Times and 3.3 million documents from\nWikipedia.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2012 04:25:00 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2012 16:03:19 GMT"}, {"version": "v3", "created": "Wed, 9 Oct 2013 19:46:20 GMT"}, {"version": "v4", "created": "Fri, 2 May 2014 16:36:57 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Paisley", "John", ""], ["Wang", "Chong", ""], ["Blei", "David M.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1210.6911", "submitter": "Fredrik Lindsten", "authors": "Fredrik Lindsten, Michael I. Jordan and Thomas B. Sch\\\"on", "title": "Ancestor Sampling for Particle Gibbs", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems (NIPS) 25,\n  (2012) 2600-2608", "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method in the family of particle MCMC methods that we\nrefer to as particle Gibbs with ancestor sampling (PG-AS). Similarly to the\nexisting PG with backward simulation (PG-BS) procedure, we use backward\nsampling to (considerably) improve the mixing of the PG kernel. Instead of\nusing separate forward and backward sweeps as in PG-BS, however, we achieve the\nsame effect in a single forward sweep. We apply the PG-AS framework to the\nchallenging class of non-Markovian state-space models. We develop a truncation\nstrategy of these models that is applicable in principle to any\nbackward-simulation-based method, but which is particularly well suited to the\nPG-AS framework. In particular, as we show in a simulation study, PG-AS can\nyield an order-of-magnitude improved accuracy relative to PG-BS due to its\nrobustness to the truncation error. Several application examples are discussed,\nincluding Rao-Blackwellized particle smoothing and inference in degenerate\nstate-space models.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2012 17:12:36 GMT"}], "update_date": "2014-09-17", "authors_parsed": [["Lindsten", "Fredrik", ""], ["Jordan", "Michael I.", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1210.6912", "submitter": "Gaurav Pandey", "authors": "Gaurav Pandey and Sahil Manocha and Gowtham Atluri and Vipin Kumar", "title": "Enhancing the functional content of protein interaction networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.MN cs.CE cs.LG q-bio.GN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Protein interaction networks are a promising type of data for studying\ncomplex biological systems. However, despite the rich information embedded in\nthese networks, they face important data quality challenges of noise and\nincompleteness that adversely affect the results obtained from their analysis.\nHere, we explore the use of the concept of common neighborhood similarity\n(CNS), which is a form of local structure in networks, to address these issues.\nAlthough several CNS measures have been proposed in the literature, an\nunderstanding of their relative efficacies for the analysis of interaction\nnetworks has been lacking. We follow the framework of graph transformation to\nconvert the given interaction network into a transformed network corresponding\nto a variety of CNS measures evaluated. The effectiveness of each measure is\nthen estimated by comparing the quality of protein function predictions\nobtained from its corresponding transformed network with those from the\noriginal network. Using a large set of S. cerevisiae interactions, and a set of\n136 GO terms, we find that several of the transformed networks produce more\naccurate predictions than those obtained from the original network. In\nparticular, the $HC.cont$ measure proposed here performs particularly well for\nthis task. Further investigation reveals that the two major factors\ncontributing to this improvement are the abilities of CNS measures, especially\n$HC.cont$, to prune out noisy edges and introduce new links between\nfunctionally related proteins.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2012 17:13:57 GMT"}], "update_date": "2012-10-29", "authors_parsed": [["Pandey", "Gaurav", ""], ["Manocha", "Sahil", ""], ["Atluri", "Gowtham", ""], ["Kumar", "Vipin", ""]]}, {"id": "1210.7053", "submitter": "Khoat Than", "authors": "Khoat Than and Tu Bao Ho", "title": "Managing sparsity, time, and quality of inference in topic models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference is an integral part of probabilistic topic models, but is often\nnon-trivial to derive an efficient algorithm for a specific model. It is even\nmuch more challenging when we want to find a fast inference algorithm which\nalways yields sparse latent representations of documents. In this article, we\nintroduce a simple framework for inference in probabilistic topic models,\ndenoted by FW. This framework is general and flexible enough to be easily\nadapted to mixture models. It has a linear convergence rate, offers an easy way\nto incorporate prior knowledge, and provides us an easy way to directly trade\noff sparsity against quality and time. We demonstrate the goodness and\nflexibility of FW over existing inference methods by a number of tasks.\nFinally, we show how inference in topic models with nonconjugate priors can be\ndone efficiently.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2012 05:23:25 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2013 00:09:46 GMT"}], "update_date": "2013-04-16", "authors_parsed": [["Than", "Khoat", ""], ["Ho", "Tu Bao", ""]]}, {"id": "1210.7054", "submitter": "Youwei Zhang", "authors": "Youwei Zhang, Laurent El Ghaoui", "title": "Large-Scale Sparse Principal Component Analysis with Application to Text\n  Data", "comments": "Appeared in the proceedings of NIPS 2011; The Neural Information\n  Processing Systems Conference (NIPS), Granada, Spain, December 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse PCA provides a linear combination of small number of features that\nmaximizes variance across data. Although Sparse PCA has apparent advantages\ncompared to PCA, such as better interpretability, it is generally thought to be\ncomputationally much more expensive. In this paper, we demonstrate the\nsurprising fact that sparse PCA can be easier than PCA in practice, and that it\ncan be reliably applied to very large data sets. This comes from a rigorous\nfeature elimination pre-processing result, coupled with the favorable fact that\nfeatures in real-life data typically have exponentially decreasing variances,\nwhich allows for many features to be eliminated. We introduce a fast block\ncoordinate ascent algorithm with much better computational complexity than the\nexisting first-order ones. We provide experimental results obtained on text\ncorpora involving millions of documents and hundreds of thousands of features.\nThese results illustrate how Sparse PCA can help organize a large corpus of\ntext data in a user-interpretable way, providing an attractive alternative\napproach to topic models.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2012 05:35:26 GMT"}], "update_date": "2012-10-29", "authors_parsed": [["Zhang", "Youwei", ""], ["Ghaoui", "Laurent El", ""]]}, {"id": "1210.7056", "submitter": "Zhongqi Lu", "authors": "Zhongqi Lu and Erheng Zhong and Lili Zhao and Wei Xiang and Weike Pan\n  and Qiang Yang", "title": "Selective Transfer Learning for Cross Domain Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering (CF) aims to predict users' ratings on items\naccording to historical user-item preference data. In many real-world\napplications, preference data are usually sparse, which would make models\noverfit and fail to give accurate predictions. Recently, several research works\nshow that by transferring knowledge from some manually selected source domains,\nthe data sparseness problem could be mitigated. However for most cases, parts\nof source domain data are not consistent with the observations in the target\ndomain, which may misguide the target domain model building. In this paper, we\npropose a novel criterion based on empirical prediction error and its variance\nto better capture the consistency across domains in CF settings. Consequently,\nwe embed this criterion into a boosting framework to perform selective\nknowledge transfer. Comparing to several state-of-the-art methods, we show that\nour proposed selective transfer learning framework can significantly improve\nthe accuracy of rating prediction tasks on several real-world recommendation\ntasks.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2012 05:36:57 GMT"}], "update_date": "2012-10-29", "authors_parsed": [["Lu", "Zhongqi", ""], ["Zhong", "Erheng", ""], ["Zhao", "Lili", ""], ["Xiang", "Wei", ""], ["Pan", "Weike", ""], ["Yang", "Qiang", ""]]}, {"id": "1210.7070", "submitter": "Shai Bagon", "authors": "Shai Bagon and Meirav Galun", "title": "A Multiscale Framework for Challenging Discrete Optimization", "comments": "5 pages, 1 figure, To appear in NIPS Workshop on Optimization for\n  Machine Learning (December 2012). Camera-ready version. Fixed typos,\n  acknowledgements added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG math.OC stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Current state-of-the-art discrete optimization methods struggle behind when\nit comes to challenging contrast-enhancing discrete energies (i.e., favoring\ndifferent labels for neighboring variables). This work suggests a multiscale\napproach for these challenging problems. Deriving an algebraic representation\nallows us to coarsen any pair-wise energy using any interpolation in a\nprincipled algebraic manner. Furthermore, we propose an energy-aware\ninterpolation operator that efficiently exposes the multiscale landscape of the\nenergy yielding an effective coarse-to-fine optimization scheme. Results on\nchallenging contrast-enhancing energies show significant improvement over\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2012 09:08:55 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2012 13:26:02 GMT"}, {"version": "v3", "created": "Fri, 2 Nov 2012 10:11:10 GMT"}], "update_date": "2012-11-05", "authors_parsed": [["Bagon", "Shai", ""], ["Galun", "Meirav", ""]]}, {"id": "1210.7362", "submitter": "Shai Bagon", "authors": "Shai Bagon", "title": "Discrete Energy Minimization, beyond Submodularity: Applications and\n  Approximations", "comments": "Doctoral dissertation, Weizmann Institute of Science. Under the\n  supervision of Prof. Michal Irani and Dr Meirav Galun Corrected typos.\n  Citation added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG math.OC stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this thesis I explore challenging discrete energy minimization problems\nthat arise mainly in the context of computer vision tasks. This work motivates\nthe use of such \"hard-to-optimize\" non-submodular functionals, and proposes\nmethods and algorithms to cope with the NP-hardness of their optimization.\nConsequently, this thesis revolves around two axes: applications and\napproximations. The applications axis motivates the use of such\n\"hard-to-optimize\" energies by introducing new tasks. As the energies become\nless constrained and structured one gains more expressive power for the\nobjective function achieving more accurate models. Results show how\nchallenging, hard-to-optimize, energies are more adequate for certain computer\nvision applications. To overcome the resulting challenging optimization tasks\nthe second axis of this thesis proposes approximation algorithms to cope with\nthe NP-hardness of the optimization. Experiments show that these new methods\nyield good results for representative challenging problems.\n", "versions": [{"version": "v1", "created": "Sat, 27 Oct 2012 19:12:49 GMT"}, {"version": "v2", "created": "Wed, 7 Nov 2012 21:09:53 GMT"}], "update_date": "2012-11-09", "authors_parsed": [["Bagon", "Shai", ""]]}, {"id": "1210.7461", "submitter": "Cesar Roberto de Souza", "authors": "C\\'esar Roberto de Souza, Ednaldo Brigante Pizzolato, Mauro dos Santos\n  Anjo", "title": "Recognizing Static Signs from the Brazilian Sign Language: Comparing\n  Large-Margin Decision Directed Acyclic Graphs, Voting Support Vector Machines\n  and Artificial Neural Networks", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore and detail our experiments in a\nhigh-dimensionality, multi-class image classification problem often found in\nthe automatic recognition of Sign Languages. Here, our efforts are directed\ntowards comparing the characteristics, advantages and drawbacks of creating and\ntraining Support Vector Machines disposed in a Directed Acyclic Graph and\nArtificial Neural Networks to classify signs from the Brazilian Sign Language\n(LIBRAS). We explore how the different heuristics, hyperparameters and\nmulti-class decision schemes affect the performance, efficiency and ease of use\nfor each classifier. We provide hyperparameter surface maps capturing accuracy\nand efficiency, comparisons between DDAGs and 1-vs-1 SVMs, and effects of\nheuristics when training ANNs with Resilient Backpropagation. We report\nstatistically significant results using Cohen's Kappa statistic for contingency\ntables.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2012 13:55:07 GMT"}], "update_date": "2012-10-30", "authors_parsed": [["de Souza", "C\u00e9sar Roberto", ""], ["Pizzolato", "Ednaldo Brigante", ""], ["Anjo", "Mauro dos Santos", ""]]}, {"id": "1210.7477", "submitter": "Robert Nishihara", "authors": "Robert Nishihara, Iain Murray, Ryan P. Adams", "title": "Parallel MCMC with Generalized Elliptical Slice Sampling", "comments": "19 pages, 8 figures, 3 algorithms", "journal-ref": "Journal of Machine Learning Research 15:2087-2112, 2014", "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic models are conceptually powerful tools for finding structure in\ndata, but their practical effectiveness is often limited by our ability to\nperform inference in them. Exact inference is frequently intractable, so\napproximate inference is often performed using Markov chain Monte Carlo (MCMC).\nTo achieve the best possible results from MCMC, we want to efficiently simulate\nmany steps of a rapidly mixing Markov chain which leaves the target\ndistribution invariant. Of particular interest in this regard is how to take\nadvantage of multi-core computing to speed up MCMC-based inference, both to\nimprove mixing and to distribute the computational load. In this paper, we\npresent a parallelizable Markov chain Monte Carlo algorithm for efficiently\nsampling from continuous probability distributions that can take advantage of\nhundreds of cores. This method shares information between parallel Markov\nchains to build a scale-mixture of Gaussians approximation to the density\nfunction of the target distribution. We combine this approximation with a\nrecent method known as elliptical slice sampling to create a Markov chain with\nno step-size parameters that can mix rapidly without requiring gradient or\ncurvature computations.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2012 17:10:29 GMT"}, {"version": "v2", "created": "Thu, 24 Jul 2014 01:05:43 GMT"}], "update_date": "2014-07-25", "authors_parsed": [["Nishihara", "Robert", ""], ["Murray", "Iain", ""], ["Adams", "Ryan P.", ""]]}, {"id": "1210.7559", "submitter": "Daniel Hsu", "authors": "Anima Anandkumar and Rong Ge and Daniel Hsu and Sham M. Kakade and\n  Matus Telgarsky", "title": "Tensor decompositions for learning latent variable models", "comments": null, "journal-ref": "Journal of Machine Learning Research, 15(Aug):2773-2832, 2014", "doi": null, "report-no": null, "categories": "cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work considers a computationally and statistically efficient parameter\nestimation method for a wide class of latent variable models---including\nGaussian mixture models, hidden Markov models, and latent Dirichlet\nallocation---which exploits a certain tensor structure in their low-order\nobservable moments (typically, of second- and third-order). Specifically,\nparameter estimation is reduced to the problem of extracting a certain\n(orthogonal) decomposition of a symmetric tensor derived from the moments; this\ndecomposition can be viewed as a natural generalization of the singular value\ndecomposition for matrices. Although tensor decompositions are generally\nintractable to compute, the decomposition of these specially structured tensors\ncan be efficiently obtained by a variety of approaches, including power\niterations and maximization approaches (similar to the case of matrices). A\ndetailed analysis of a robust tensor power method is provided, establishing an\nanalogue of Wedin's perturbation theorem for the singular vectors of matrices.\nThis implies a robust and computationally tractable estimation approach for\nseveral popular latent variable models.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2012 04:38:41 GMT"}, {"version": "v2", "created": "Sun, 9 Dec 2012 00:59:17 GMT"}, {"version": "v3", "created": "Sat, 1 Mar 2014 19:06:31 GMT"}, {"version": "v4", "created": "Thu, 13 Nov 2014 22:43:15 GMT"}], "update_date": "2014-11-17", "authors_parsed": [["Anandkumar", "Anima", ""], ["Ge", "Rong", ""], ["Hsu", "Daniel", ""], ["Kakade", "Sham M.", ""], ["Telgarsky", "Matus", ""]]}, {"id": "1210.7665", "submitter": "Mladen Kolar", "authors": "Mladen Kolar, Han Liu, Eric P. Xing", "title": "Graph Estimation From Multi-attribute Data", "comments": "Extended simulation study. Added an application to a new data set", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real world network problems often concern multivariate nodal attributes\nsuch as image, textual, and multi-view feature vectors on nodes, rather than\nsimple univariate nodal attributes. The existing graph estimation methods built\non Gaussian graphical models and covariance selection algorithms can not handle\nsuch data, neither can the theories developed around such methods be directly\napplied. In this paper, we propose a new principled framework for estimating\ngraphs from multi-attribute data. Instead of estimating the partial correlation\nas in current literature, our method estimates the partial canonical\ncorrelations that naturally accommodate complex nodal features.\nComputationally, we provide an efficient algorithm which utilizes the\nmulti-attribute structure. Theoretically, we provide sufficient conditions\nwhich guarantee consistent graph recovery. Extensive simulation studies\ndemonstrate performance of our method under various conditions. Furthermore, we\nprovide illustrative applications to uncovering gene regulatory networks from\ngene and protein profiles, and uncovering brain connectivity graph from\nfunctional magnetic resonance imaging data.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2012 13:54:36 GMT"}, {"version": "v2", "created": "Sat, 20 Apr 2013 16:31:08 GMT"}], "update_date": "2013-04-23", "authors_parsed": [["Kolar", "Mladen", ""], ["Liu", "Han", ""], ["Xing", "Eric P.", ""]]}, {"id": "1210.8353", "submitter": "Alex Susemihl", "authors": "Chris H\\\"ausler, Alex Susemihl", "title": "Temporal Autoencoding Restricted Boltzmann Machine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much work has been done refining and characterizing the receptive fields\nlearned by deep learning algorithms. A lot of this work has focused on the\ndevelopment of Gabor-like filters learned when enforcing sparsity constraints\non a natural image dataset. Little work however has investigated how these\nfilters might expand to the temporal domain, namely through training on natural\nmovies. Here we investigate exactly this problem in established temporal deep\nlearning algorithms as well as a new learning paradigm suggested here, the\nTemporal Autoencoding Restricted Boltzmann Machine (TARBM).\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2012 14:55:50 GMT"}], "update_date": "2012-11-01", "authors_parsed": [["H\u00e4usler", "Chris", ""], ["Susemihl", "Alex", ""]]}, {"id": "1210.8429", "submitter": "Youngser Park", "authors": "Youngser Park, Carey E. Priebe, Abdou Youssef", "title": "Anomaly Detection in Time Series of Graphs using Fusion of Graph\n  Invariants", "comments": null, "journal-ref": null, "doi": "10.1109/JSTSP.2012.2233712", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a time series of graphs G(t) = (V, E(t)), t = 1, 2, ..., where the\nfixed vertex set V represents \"actors\" and an edge between vertex u and vertex\nv at time t (uv \\in E(t)) represents the existence of a communications event\nbetween actors u and v during the tth time period, we wish to detect anomalies\nand/or change points. We consider a collection of graph features, or\ninvariants, and demonstrate that adaptive fusion provides superior inferential\nefficacy compared to naive equal weighting for a certain class of anomaly\ndetection problems. Simulation results using a latent process model for time\nseries of graphs, as well as illustrative experimental results for a time\nseries of graphs derived from the Enron email data, show that a fusion\nstatistic can provide superior inference compared to individual invariants\nalone. These results also demonstrate that an adaptive weighting scheme for\nfusion of invariants performs better than naive equal weighting.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2012 18:24:10 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Park", "Youngser", ""], ["Priebe", "Carey E.", ""], ["Youssef", "Abdou", ""]]}, {"id": "1210.8442", "submitter": "Louis Shao", "authors": "Louis Yuanlong Shao", "title": "Linear-Nonlinear-Poisson Neuron Networks Perform Bayesian Inference On\n  Boltzmann Machines", "comments": "Submitted to International Conference of Learning Representation\n  (ICLR) 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One conjecture in both deep learning and classical connectionist viewpoint is\nthat the biological brain implements certain kinds of deep networks as its\nback-end. However, to our knowledge, a detailed correspondence has not yet been\nset up, which is important if we want to bridge between neuroscience and\nmachine learning. Recent researches emphasized the biological plausibility of\nLinear-Nonlinear-Poisson (LNP) neuron model. We show that with neurally\nplausible settings, the whole network is capable of representing any Boltzmann\nmachine and performing a semi-stochastic Bayesian inference algorithm lying\nbetween Gibbs sampling and variational inference.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2012 19:14:41 GMT"}, {"version": "v2", "created": "Tue, 15 Jan 2013 01:23:04 GMT"}, {"version": "v3", "created": "Sun, 27 Jan 2013 05:30:35 GMT"}], "update_date": "2013-01-29", "authors_parsed": [["Shao", "Louis Yuanlong", ""]]}]