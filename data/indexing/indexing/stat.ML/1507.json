[{"id": "1507.00039", "submitter": "Jason Lee", "authors": "Jason D. Lee", "title": "Selective Inference and Learning Mixed Graphical Models", "comments": "Jason D. Lee PhD Dissertation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis studies two problems in modern statistics. First, we study\nselective inference, or inference for hypothesis that are chosen after looking\nat the data. The motiving application is inference for regression coefficients\nselected by the lasso. We present the Condition-on-Selection method that allows\nfor valid selective inference, and study its application to the lasso, and\nseveral other selection algorithms.\n  In the second part, we consider the problem of learning the structure of a\npairwise graphical model over continuous and discrete variables. We present a\nnew pairwise model for graphical models with both continuous and discrete\nvariables that is amenable to structure learning. In previous work, authors\nhave considered structure learning of Gaussian graphical models and structure\nlearning of discrete models. Our approach is a natural generalization of these\ntwo lines of work to the mixed case. The penalization scheme involves a novel\nsymmetric use of the group-lasso norm and follows naturally from a particular\nparametrization of the model. We provide conditions under which our estimator\nis model selection consistent in the high-dimensional regime.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 21:10:18 GMT"}], "update_date": "2015-07-02", "authors_parsed": [["Lee", "Jason D.", ""]]}, {"id": "1507.00043", "submitter": "Athanasios N. Nikolakopoulos", "authors": "Athanasios N. Nikolakopoulos and John D. Garofalakis", "title": "Top-N recommendations in the presence of sparsity: An NCD-based approach", "comments": "To appear in the Web Intelligence Journal as a regular paper", "journal-ref": null, "doi": "10.3233/WEB-150324", "report-no": null, "categories": "cs.IR cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Making recommendations in the presence of sparsity is known to present one of\nthe most challenging problems faced by collaborative filtering methods. In this\nwork we tackle this problem by exploiting the innately hierarchical structure\nof the item space following an approach inspired by the theory of\nDecomposability. We view the itemspace as a Nearly Decomposable system and we\ndefine blocks of closely related elements and corresponding indirect proximity\ncomponents. We study the theoretical properties of the decomposition and we\nderive sufficient conditions that guarantee full item space coverage even in\ncold-start recommendation scenarios. A comprehensive set of experiments on the\nMovieLens and the Yahoo!R2Music datasets, using several widely applied\nperformance metrics, support our model's theoretically predicted properties and\nverify that NCDREC outperforms several state-of-the-art algorithms, in terms of\nrecommendation accuracy, diversity and sparseness insensitivity.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 21:34:53 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2015 13:55:35 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Nikolakopoulos", "Athanasios N.", ""], ["Garofalakis", "John D.", ""]]}, {"id": "1507.00052", "submitter": "Cuong Tran", "authors": "Cuong Tran, Vladimir Pavlovic, Robert Kopp", "title": "Gaussian Process for Noisy Inputs with Ordering Constraints", "comments": "submitted to NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Gaussian Process regression model in the context of training\ndata with noise in both input and output. The presence of two sources of noise\nmakes the task of learning accurate predictive models extremely challenging.\nHowever, in some instances additional constraints may be available that can\nreduce the uncertainty in the resulting predictive models. In particular, we\nconsider the case of monotonically ordered latent input, which occurs in many\napplication domains that deal with temporal data. We present a novel inference\nand learning approach based on non-parametric Gaussian variational\napproximation to learn the GP model while taking into account the new\nconstraints. The resulting strategy allows one to gain access to posterior\nestimates of both the input and the output and results in improved predictive\nperformance. We compare our proposed models to state-of-the-art Noisy Input\nGaussian Process (NIGP) and other competing approaches on synthetic and real\nsea-level rise data. Experimental results suggest that the proposed approach\nconsistently outperforms selected methods while, at the same time, reducing the\ncomputational costs of learning and inference.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 22:28:06 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2015 04:39:55 GMT"}], "update_date": "2015-07-03", "authors_parsed": [["Tran", "Cuong", ""], ["Pavlovic", "Vladimir", ""], ["Kopp", "Robert", ""]]}, {"id": "1507.00066", "submitter": "Pooria Joulani", "authors": "Pooria Joulani, Andr\\'as Gy\\\"orgy, Csaba Szepesv\\'ari", "title": "Fast Cross-Validation for Incremental Learning", "comments": "Appearing in the International Joint Conference on Artificial\n  Intelligence (IJCAI-2015), Buenos Aires, Argentina, July 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-validation (CV) is one of the main tools for performance estimation and\nparameter tuning in machine learning. The general recipe for computing CV\nestimate is to run a learning algorithm separately for each CV fold, a\ncomputationally expensive process. In this paper, we propose a new approach to\nreduce the computational burden of CV-based performance estimation. As opposed\nto all previous attempts, which are specific to a particular learning model or\nproblem domain, we propose a general method applicable to a large class of\nincremental learning algorithms, which are uniquely fitted to big data\nproblems. In particular, our method applies to a wide range of supervised and\nunsupervised learning tasks with different performance criteria, as long as the\nbase learning algorithm is incremental. We show that the running time of the\nalgorithm scales logarithmically, rather than linearly, in the number of CV\nfolds. Furthermore, the algorithm has favorable properties for parallel and\ndistributed implementation. Experiments with state-of-the-art incremental\nlearning algorithms confirm the practicality of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 23:30:28 GMT"}], "update_date": "2015-07-02", "authors_parsed": [["Joulani", "Pooria", ""], ["Gy\u00f6rgy", "Andr\u00e1s", ""], ["Szepesv\u00e1ri", "Csaba", ""]]}, {"id": "1507.00210", "submitter": "Guillaume Desjardins", "authors": "Guillaume Desjardins, Karen Simonyan, Razvan Pascanu, Koray\n  Kavukcuoglu", "title": "Natural Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Natural Neural Networks, a novel family of algorithms that speed\nup convergence by adapting their internal representation during training to\nimprove conditioning of the Fisher matrix. In particular, we show a specific\nexample that employs a simple and efficient reparametrization of the neural\nnetwork weights by implicitly whitening the representation obtained at each\nlayer, while preserving the feed-forward computation of the network. Such\nnetworks can be trained efficiently via the proposed Projected Natural Gradient\nDescent algorithm (PRONG), which amortizes the cost of these reparametrizations\nover many parameter updates and is closely related to the Mirror Descent online\nlearning algorithm. We highlight the benefits of our method on both\nunsupervised and supervised learning tasks, and showcase its scalability by\ntraining on the large-scale ImageNet Challenge dataset.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2015 12:42:01 GMT"}], "update_date": "2015-07-02", "authors_parsed": [["Desjardins", "Guillaume", ""], ["Simonyan", "Karen", ""], ["Pascanu", "Razvan", ""], ["Kavukcuoglu", "Koray", ""]]}, {"id": "1507.00220", "submitter": "Alexander Cloninger", "authors": "Alexander Cloninger, Ronald R. Coifman, Nicholas Downing, Harlan M.\n  Krumholz", "title": "Bigeometric Organization of Deep Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we build an organization of high-dimensional datasets that\ncannot be cleanly embedded into a low-dimensional representation due to missing\nentries and a subset of the features being irrelevant to modeling functions of\ninterest. Our algorithm begins by defining coarse neighborhoods of the points\nand defining an expected empirical function value on these neighborhoods. We\nthen generate new non-linear features with deep net representations tuned to\nmodel the approximate function, and re-organize the geometry of the points with\nrespect to the new representation. Finally, the points are locally z-scored to\ncreate an intrinsic geometric organization which is independent of the\nparameters of the deep net, a geometry designed to assure smoothness with\nrespect to the empirical function. We examine this approach on data from the\nCenter for Medicare and Medicaid Services Hospital Quality Initiative, and\ngenerate an intrinsic low-dimensional organization of the hospitals that is\nsmooth with respect to an expert driven function of quality.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2015 13:18:53 GMT"}], "update_date": "2015-07-02", "authors_parsed": [["Cloninger", "Alexander", ""], ["Coifman", "Ronald R.", ""], ["Downing", "Nicholas", ""], ["Krumholz", "Harlan M.", ""]]}, {"id": "1507.00300", "submitter": "Ian Osband", "authors": "Ian Osband and Benjamin Van Roy", "title": "Bootstrapped Thompson Sampling and Deep Exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical note presents a new approach to carrying out the kind of\nexploration achieved by Thompson sampling, but without explicitly maintaining\nor sampling from posterior distributions. The approach is based on a bootstrap\ntechnique that uses a combination of observed and artificially generated data.\nThe latter serves to induce a prior distribution which, as we will demonstrate,\nis critical to effective exploration. We explain how the approach can be\napplied to multi-armed bandit and reinforcement learning problems and how it\nrelates to Thompson sampling. The approach is particularly well-suited for\ncontexts in which exploration is coupled with deep learning, since in these\nsettings, maintaining or generating samples from a posterior distribution\nbecomes computationally infeasible.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2015 17:47:01 GMT"}], "update_date": "2015-07-02", "authors_parsed": [["Osband", "Ian", ""], ["Van Roy", "Benjamin", ""]]}, {"id": "1507.00353", "submitter": "Harm van Seijen", "authors": "Harm van Seijen, A. Rupam Mahmood, Patrick M. Pilarski, Richard S.\n  Sutton", "title": "An Empirical Evaluation of True Online TD({\\lambda})", "comments": "European Workshop on Reinforcement Learning (EWRL) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The true online TD({\\lambda}) algorithm has recently been proposed (van\nSeijen and Sutton, 2014) as a universal replacement for the popular\nTD({\\lambda}) algorithm, in temporal-difference learning and reinforcement\nlearning. True online TD({\\lambda}) has better theoretical properties than\nconventional TD({\\lambda}), and the expectation is that it also results in\nfaster learning. In this paper, we put this hypothesis to the test.\nSpecifically, we compare the performance of true online TD({\\lambda}) with that\nof TD({\\lambda}) on challenging examples, random Markov reward processes, and a\nreal-world myoelectric prosthetic arm. We use linear function approximation\nwith tabular, binary, and non-binary features. We assess the algorithms along\nthree dimensions: computational cost, learning speed, and ease of use. Our\nresults confirm the strength of true online TD({\\lambda}): 1) for sparse\nfeature vectors, the computational overhead with respect to TD({\\lambda}) is\nminimal; for non-sparse features the computation time is at most twice that of\nTD({\\lambda}), 2) across all domains/representations the learning speed of true\nonline TD({\\lambda}) is often better, but never worse than that of\nTD({\\lambda}), and 3) true online TD({\\lambda}) is easier to use, because it\ndoes not require choosing between trace types, and it is generally more stable\nwith respect to the step-size. Overall, our results suggest that true online\nTD({\\lambda}) should be the first choice when looking for an efficient,\ngeneral-purpose TD method.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2015 20:03:49 GMT"}], "update_date": "2015-07-03", "authors_parsed": [["van Seijen", "Harm", ""], ["Mahmood", "A. Rupam", ""], ["Pilarski", "Patrick M.", ""], ["Sutton", "Richard S.", ""]]}, {"id": "1507.00421", "submitter": "Yao Xie", "authors": "Yang Cao, Yao Xie", "title": "Categorical Matrix Completion", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of completing a matrix with categorical-valued\nentries from partial observations. This is achieved by extending the\nformulation and theory of one-bit matrix completion. We recover a low-rank\nmatrix $X$ by maximizing the likelihood ratio with a constraint on the nuclear\nnorm of $X$, and the observations are mapped from entries of $X$ through\nmultiple link functions. We establish theoretical upper and lower bounds on the\nrecovery error, which meet up to a constant factor $\\mathcal{O}(K^{3/2})$ where\n$K$ is the fixed number of categories. The upper bound in our case depends on\nthe number of categories implicitly through a maximization of terms that\ninvolve the smoothness of the link functions. In contrast to one-bit matrix\ncompletion, our bounds for categorical matrix completion are optimal up to a\nfactor on the order of the square root of the number of categories, which is\nconsistent with an intuition that the problem becomes harder when the number of\ncategories increases. By comparing the performance of our method with the\nconventional matrix completion method on the MovieLens dataset, we demonstrate\nthe advantage of our method.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 03:58:47 GMT"}], "update_date": "2015-07-03", "authors_parsed": [["Cao", "Yang", ""], ["Xie", "Yao", ""]]}, {"id": "1507.00438", "submitter": "Alain Rakotomamonjy", "authors": "Alain Rakotomamonjy (LITIS), Remi Flamary (LAGRANGE, OCA), Gilles\n  Gasso (LITIS)", "title": "DC Proximal Newton for Non-Convex Optimization Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel algorithm for solving learning problems where both the\nloss function and the regularizer are non-convex but belong to the class of\ndifference of convex (DC) functions. Our contribution is a new general purpose\nproximal Newton algorithm that is able to deal with such a situation. The\nalgorithm consists in obtaining a descent direction from an approximation of\nthe loss function and then in performing a line search to ensure sufficient\ndescent. A theoretical analysis is provided showing that the iterates of the\nproposed algorithm {admit} as limit points stationary points of the DC\nobjective function. Numerical experiments show that our approach is more\nefficient than current state of the art for a problem with a convex loss\nfunctions and non-convex regularizer. We have also illustrated the benefit of\nour algorithm in high-dimensional transductive learning problem where both loss\nfunction and regularizers are non-convex.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 06:41:32 GMT"}], "update_date": "2015-07-03", "authors_parsed": [["Rakotomamonjy", "Alain", "", "LITIS"], ["Flamary", "Remi", "", "LAGRANGE, OCA"], ["Gasso", "Gilles", "", "LITIS"]]}, {"id": "1507.00473", "submitter": "Steve Hanneke", "authors": "Steve Hanneke", "title": "The Optimal Sample Complexity of PAC Learning", "comments": null, "journal-ref": "Journal of Machine Learning Research, Vol. 17 (2016), No. 38, pp.\n  1-15", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work establishes a new upper bound on the number of samples sufficient\nfor PAC learning in the realizable case. The bound matches known lower bounds\nup to numerical constant factors. This solves a long-standing open problem on\nthe sample complexity of PAC learning. The technique and analysis build on a\nrecent breakthrough by Hans Simon.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 08:43:21 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2015 12:23:27 GMT"}, {"version": "v3", "created": "Mon, 27 Jul 2015 04:08:28 GMT"}, {"version": "v4", "created": "Sun, 7 Feb 2016 16:35:39 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Hanneke", "Steve", ""]]}, {"id": "1507.00507", "submitter": "Diego Romeres", "authors": "Diego Romeres, Gianluigi Pillonetto and Alessandro Chiuso", "title": "Identification of stable models via nonparametric prediction error\n  methods", "comments": "number of pages = 6, number of figures = 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new Bayesian approach to linear system identification has been proposed in\na series of recent papers. The main idea is to frame linear system\nidentification as predictor estimation in an infinite dimensional space, with\nthe aid of regularization/Bayesian techniques. This approach guarantees the\nidentification of stable predictors based on the prediction error minimization.\nUnluckily, the stability of the predictors does not guarantee the stability of\nthe impulse response of the system. In this paper we propose and compare\nvarious techniques to address this issue. Simulations results comparing these\ntechniques will be provided.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 10:20:52 GMT"}], "update_date": "2015-07-03", "authors_parsed": [["Romeres", "Diego", ""], ["Pillonetto", "Gianluigi", ""], ["Chiuso", "Alessandro", ""]]}, {"id": "1507.00513", "submitter": "Mokhtar Zahdi Alaya", "authors": "Mokhtar Zahdi Alaya (LSTA), St\\'ephane Ga\\\"iffas (CMAP), Agathe\n  Guilloux (LSTA)", "title": "Learning the intensity of time events with change-points", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning the inhomogeneous intensity of a counting\nprocess, under a sparse segmentation assumption. We introduce a weighted\ntotal-variation penalization, using data-driven weights that correctly scale\nthe penalization along the observation interval. We prove that this leads to a\nsharp tuning of the convex relaxation of the segmentation prior, by stating\noracle inequalities with fast rates of convergence, and consistency for\nchange-points detection. This provides first theoretical guarantees for\nsegmentation with a convex proxy beyond the standard i.i.d signal + white noise\nsetting. We introduce a fast algorithm to solve this convex problem. Numerical\nexperiments illustrate our approach on simulated and on a high-frequency\ngenomics dataset.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 10:46:45 GMT"}], "update_date": "2015-07-03", "authors_parsed": [["Alaya", "Mokhtar Zahdi", "", "LSTA"], ["Ga\u00efffas", "St\u00e9phane", "", "CMAP"], ["Guilloux", "Agathe", "", "LSTA"]]}, {"id": "1507.00543", "submitter": "Diego Romeres", "authors": "D. Romeres, G. Prando, G. Pillonetto and A. Chiuso", "title": "Classical vs. Bayesian methods for linear system identification: point\n  estimators and confidence sets", "comments": "number of pages = 8, number of figures = 4", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper compares classical parametric methods with recently developed\nBayesian methods for system identification. A Full Bayes solution is considered\ntogether with one of the standard approximations based on the Empirical Bayes\nparadigm. Results regarding point estimators for the impulse response as well\nas for confidence regions are reported.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 12:28:41 GMT"}], "update_date": "2015-07-03", "authors_parsed": [["Romeres", "D.", ""], ["Prando", "G.", ""], ["Pillonetto", "G.", ""], ["Chiuso", "A.", ""]]}, {"id": "1507.00566", "submitter": "Steven Reece", "authors": "Steven Reece, Roman Garnett, Michael Osborne and Stephen Roberts", "title": "Anomaly Detection and Removal Using Non-Stationary Gaussian Processes", "comments": "9 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel Gaussian process approach to fault removal in\ntime-series data. Fault removal does not delete the faulty signal data but,\ninstead, massages the fault from the data. We assume that only one fault occurs\nat any one time and model the signal by two separate non-parametric Gaussian\nprocess models for both the physical phenomenon and the fault. In order to\nfacilitate fault removal we introduce the Markov Region Link kernel for\nhandling non-stationary Gaussian processes. This kernel is piece-wise\nstationary but guarantees that functions generated by it and their derivatives\n(when required) are everywhere continuous. We apply this kernel to the removal\nof drift and bias errors in faulty sensor data and also to the recovery of EOG\nartifact corrupted EEG signals.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 13:11:04 GMT"}], "update_date": "2015-07-03", "authors_parsed": [["Reece", "Steven", ""], ["Garnett", "Roman", ""], ["Osborne", "Michael", ""], ["Roberts", "Stephen", ""]]}, {"id": "1507.00672", "submitter": "James P. Crutchfield", "authors": "Pooneh M. Ara, Ryan G. James, and James P. Crutchfield", "title": "The Elusive Present: Hidden Past and Future Dependency and Why We Build\n  Models", "comments": "12 pages, 10 figures;\n  http://csc.ucdavis.edu/~cmg/compmech/pubs/tep.htm", "journal-ref": "Phys. Rev. E 93, 022143 (2016)", "doi": "10.1103/PhysRevE.93.022143", "report-no": null, "categories": "cond-mat.stat-mech cs.IT math.DS math.IT nlin.CD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling a temporal process as if it is Markovian assumes the present encodes\nall of the process's history. When this occurs, the present captures all of the\ndependency between past and future. We recently showed that if one randomly\nsamples in the space of structured processes, this is almost never the case.\nSo, how does the Markov failure come about? That is, how do individual\nmeasurements fail to encode the past? And, how many are needed to capture\ndependencies between the past and future? Here, we investigate how much\ninformation can be shared between the past and future, but not be reflected in\nthe present. We quantify this elusive information, give explicit calculational\nmethods, and draw out the consequences. The most important of which is that\nwhen the present hides past-future dependency we must move beyond\nsequence-based statistics and build state-based models.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 17:34:53 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Ara", "Pooneh M.", ""], ["James", "Ryan G.", ""], ["Crutchfield", "James P.", ""]]}, {"id": "1507.00677", "submitter": "Takeru Miyato", "authors": "Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, Ken Nakae, Shin Ishii", "title": "Distributional Smoothing with Virtual Adversarial Training", "comments": "Under review as a conference paper at ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose local distributional smoothness (LDS), a new notion of smoothness\nfor statistical model that can be used as a regularization term to promote the\nsmoothness of the model distribution. We named the LDS based regularization as\nvirtual adversarial training (VAT). The LDS of a model at an input datapoint is\ndefined as the KL-divergence based robustness of the model distribution against\nlocal perturbation around the datapoint. VAT resembles adversarial training,\nbut distinguishes itself in that it determines the adversarial direction from\nthe model distribution alone without using the label information, making it\napplicable to semi-supervised learning. The computational cost for VAT is\nrelatively low. For neural network, the approximated gradient of the LDS can be\ncomputed with no more than three pairs of forward and back propagations. When\nwe applied our technique to supervised and semi-supervised learning for the\nMNIST dataset, it outperformed all the training methods other than the current\nstate of the art method, which is based on a highly advanced generative model.\nWe also applied our method to SVHN and NORB, and confirmed our method's\nsuperior performance over the current state of the art semi-supervised method\napplied to these datasets.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 18:01:23 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2015 19:59:36 GMT"}, {"version": "v3", "created": "Thu, 13 Aug 2015 09:19:40 GMT"}, {"version": "v4", "created": "Fri, 25 Sep 2015 12:20:05 GMT"}, {"version": "v5", "created": "Thu, 19 Nov 2015 18:47:51 GMT"}, {"version": "v6", "created": "Wed, 25 Nov 2015 13:31:07 GMT"}, {"version": "v7", "created": "Sat, 9 Jan 2016 23:53:05 GMT"}, {"version": "v8", "created": "Mon, 29 Feb 2016 15:39:55 GMT"}, {"version": "v9", "created": "Sat, 11 Jun 2016 18:22:33 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Miyato", "Takeru", ""], ["Maeda", "Shin-ichi", ""], ["Koyama", "Masanori", ""], ["Nakae", "Ken", ""], ["Ishii", "Shin", ""]]}, {"id": "1507.00720", "submitter": "Rajesh Ranganath", "authors": "Rajesh Ranganath, David Blei", "title": "Correlated Random Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop correlated random measures, random measures where the atom weights\ncan exhibit a flexible pattern of dependence, and use them to develop powerful\nhierarchical Bayesian nonparametric models. Hierarchical Bayesian nonparametric\nmodels are usually built from completely random measures, a Poisson-process\nbased construction in which the atom weights are independent. Completely random\nmeasures imply strong independence assumptions in the corresponding\nhierarchical model, and these assumptions are often misplaced in real-world\nsettings. Correlated random measures address this limitation. They model\ncorrelation within the measure by using a Gaussian process in concert with the\nPoisson process. With correlated random measures, for example, we can develop a\nlatent feature model for which we can infer both the properties of the latent\nfeatures and their dependency pattern. We develop several other examples as\nwell. We study a correlated random measure model of pairwise count data. We\nderive an efficient variational inference algorithm and show improved\npredictive performance on large data sets of documents, web clicks, and\nelectronic health records.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 19:56:45 GMT"}, {"version": "v2", "created": "Wed, 2 Nov 2016 03:12:18 GMT"}, {"version": "v3", "created": "Wed, 9 Nov 2016 07:54:54 GMT"}], "update_date": "2016-11-10", "authors_parsed": [["Ranganath", "Rajesh", ""], ["Blei", "David", ""]]}, {"id": "1507.00803", "submitter": "Guillaume Basse", "authors": "Guillaume W. Basse, Edoardo M. Airoldi", "title": "Model-assisted design of experiments in the presence of network\n  correlated outcomes", "comments": "56 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of how to assign treatment in a randomized\nexperiment, in which the correlation among the outcomes is informed by a\nnetwork available pre-intervention. Working within the potential outcome causal\nframework, we develop a class of models that posit such a correlation structure\namong the outcomes. Then we leverage these models to develop restricted\nrandomization strategies for allocating treatment optimally, by minimizing the\nmean square error of the estimated average treatment effect. Analytical\ndecompositions of the mean square error, due both to the model and to the\nrandomization distribution, provide insights into aspects of the optimal\ndesigns. In particular, the analysis suggests new notions of balance based on\nspecific network quantities, in addition to classical covariate balance. The\nresulting balanced, optimal restricted randomization strategies are still\ndesign unbiased, in situations where the model used to derive them does not\nhold. We illustrate how the proposed treatment allocation strategies improve on\nallocations that ignore the network structure, with extensive simulations.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2015 01:44:51 GMT"}, {"version": "v2", "created": "Wed, 26 Oct 2016 21:14:41 GMT"}, {"version": "v3", "created": "Wed, 17 May 2017 15:14:40 GMT"}, {"version": "v4", "created": "Thu, 18 May 2017 14:30:09 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Basse", "Guillaume W.", ""], ["Airoldi", "Edoardo M.", ""]]}, {"id": "1507.00814", "submitter": "Bradly Stadie", "authors": "Bradly C. Stadie, Sergey Levine, Pieter Abbeel", "title": "Incentivizing Exploration In Reinforcement Learning With Deep Predictive\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Achieving efficient and scalable exploration in complex domains poses a major\nchallenge in reinforcement learning. While Bayesian and PAC-MDP approaches to\nthe exploration problem offer strong formal guarantees, they are often\nimpractical in higher dimensions due to their reliance on enumerating the\nstate-action space. Hence, exploration in complex domains is often performed\nwith simple epsilon-greedy methods. In this paper, we consider the challenging\nAtari games domain, which requires processing raw pixel inputs and delayed\nrewards. We evaluate several more sophisticated exploration strategies,\nincluding Thompson sampling and Boltzman exploration, and propose a new\nexploration method based on assigning exploration bonuses from a concurrently\nlearned model of the system dynamics. By parameterizing our learned model with\na neural network, we are able to develop a scalable and efficient approach to\nexploration bonuses that can be applied to tasks with complex, high-dimensional\nstate spaces. In the Atari domain, our method provides the most consistent\nimprovement across a range of games that pose a major challenge for prior\nmethods. In addition to raw game-scores, we also develop an AUC-100 metric for\nthe Atari Learning domain to evaluate the impact of exploration on this\nbenchmark.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2015 04:11:15 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2015 23:05:34 GMT"}, {"version": "v3", "created": "Thu, 19 Nov 2015 22:40:30 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Stadie", "Bradly C.", ""], ["Levine", "Sergey", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1507.00824", "submitter": "Behnam Babagholami-Mohamadabadi", "authors": "Behnam Babagholami-Mohamadabadi, Sejong Yoon, Vladimir Pavlovic", "title": "D-MFVI: Distributed Mean Field Variational Inference using Bregman ADMM", "comments": "19 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian models provide a framework for probabilistic modelling of complex\ndatasets. However, many of such models are computationally demanding especially\nin the presence of large datasets. On the other hand, in sensor network\napplications, statistical (Bayesian) parameter estimation usually needs\ndistributed algorithms, in which both data and computation are distributed\nacross the nodes of the network. In this paper we propose a general framework\nfor distributed Bayesian learning using Bregman Alternating Direction Method of\nMultipliers (B-ADMM). We demonstrate the utility of our framework, with Mean\nField Variational Bayes (MFVB) as the primitive for distributed Matrix\nFactorization (MF) and distributed affine structure from motion (SfM).\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2015 06:14:26 GMT"}], "update_date": "2015-07-06", "authors_parsed": [["Babagholami-Mohamadabadi", "Behnam", ""], ["Yoon", "Sejong", ""], ["Pavlovic", "Vladimir", ""]]}, {"id": "1507.00825", "submitter": "Yutaro Shigeto", "authors": "Yutaro Shigeto, Ikumi Suzuki, Kazuo Hara, Masashi Shimbo, Yuji\n  Matsumoto", "title": "Ridge Regression, Hubness, and Zero-Shot Learning", "comments": "To be presented at ECML/PKDD 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the effect of hubness in zero-shot learning, when ridge\nregression is used to find a mapping between the example space to the label\nspace. Contrary to the existing approach, which attempts to find a mapping from\nthe example space to the label space, we show that mapping labels into the\nexample space is desirable to suppress the emergence of hubs in the subsequent\nnearest neighbor search step. Assuming a simple data model, we prove that the\nproposed approach indeed reduces hubness. This was verified empirically on the\ntasks of bilingual lexicon extraction and image labeling: hubness was reduced\nwith both of these tasks and the accuracy was improved accordingly.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2015 06:18:36 GMT"}], "update_date": "2015-07-06", "authors_parsed": [["Shigeto", "Yutaro", ""], ["Suzuki", "Ikumi", ""], ["Hara", "Kazuo", ""], ["Shimbo", "Masashi", ""], ["Matsumoto", "Yuji", ""]]}, {"id": "1507.00827", "submitter": "Can Le", "authors": "Can M. Le and Elizaveta Levina", "title": "Estimating the number of communities in networks by spectral methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection is a fundamental problem in network analysis with many\nmethods available to estimate communities. Most of these methods assume that\nthe number of communities is known, which is often not the case in practice. We\nstudy a simple and very fast method for estimating the number of communities\nbased on the spectral properties of certain graph operators, such as the\nnon-backtracking matrix and the Bethe Hessian matrix. We show that the method\nperforms well under several models and a wide range of parameters, and is\nguaranteed to be consistent under several asymptotic regimes. We compare this\nmethod to several existing methods for estimating the number of communities and\nshow that it is both more accurate and more computationally efficient.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2015 06:32:12 GMT"}, {"version": "v2", "created": "Fri, 15 Nov 2019 01:46:05 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Le", "Can M.", ""], ["Levina", "Elizaveta", ""]]}, {"id": "1507.00908", "submitter": "Zhao Kang", "authors": "Zhao Kang, Chong Peng, Jie Cheng and Qiang Chen", "title": "LogDet Rank Minimization with Application to Subspace Clustering", "comments": "10 pages, 4 figures", "journal-ref": "Computational Intelligence and Neuroscience, Volume 2015, Article\n  ID 824289", "doi": "10.1155/2015/824289", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank matrix is desired in many machine learning and computer vision\nproblems. Most of the recent studies use the nuclear norm as a convex surrogate\nof the rank operator. However, all singular values are simply added together by\nthe nuclear norm, and thus the rank may not be well approximated in practical\nproblems. In this paper, we propose to use a log-determinant (LogDet) function\nas a smooth and closer, though non-convex, approximation to rank for obtaining\na low-rank representation in subspace clustering. Augmented Lagrange\nmultipliers strategy is applied to iteratively optimize the LogDet-based\nnon-convex objective function on potentially large-scale data. By making use of\nthe angular information of principal directions of the resultant low-rank\nrepresentation, an affinity graph matrix is constructed for spectral\nclustering. Experimental results on motion segmentation and face clustering\ndata demonstrate that the proposed method often outperforms state-of-the-art\nsubspace clustering algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2015 13:30:41 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Kang", "Zhao", ""], ["Peng", "Chong", ""], ["Cheng", "Jie", ""], ["Chen", "Qiang", ""]]}, {"id": "1507.00955", "submitter": "Olga Kolchyna", "authors": "Olga Kolchyna, Tharsis T. P. Souza, Philip Treleaven, Tomaso Aste", "title": "Twitter Sentiment Analysis: Lexicon Method, Machine Learning Method and\n  Their Combination", "comments": "32 pages, 5 figures", "journal-ref": "Handbook of Sentiment Analysis in Finance. Mitra, G. and Yu, X.\n  (Eds.). (2016). ISBN 1910571571", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper covers the two approaches for sentiment analysis: i) lexicon based\nmethod; ii) machine learning method. We describe several techniques to\nimplement these approaches and discuss how they can be adopted for sentiment\nclassification of Twitter messages. We present a comparative study of different\nlexicon combinations and show that enhancing sentiment lexicons with emoticons,\nabbreviations and social-media slang expressions increases the accuracy of\nlexicon-based classification for Twitter. We discuss the importance of feature\ngeneration and feature selection processes for machine learning sentiment\nclassification. To quantify the performance of the main sentiment analysis\nmethods over Twitter we run these algorithms on a benchmark Twitter dataset\nfrom the SemEval-2013 competition, task 2-B. The results show that machine\nlearning method based on SVM and Naive Bayes classifiers outperforms the\nlexicon method. We present a new ensemble method that uses a lexicon based\nsentiment score as input feature for the machine learning approach. The\ncombined method proved to produce more precise classifications. We also show\nthat employing a cost-sensitive classifier for highly unbalanced datasets\nyields an improvement of sentiment classification performance up to 7%.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2015 15:46:55 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2015 17:24:18 GMT"}, {"version": "v3", "created": "Fri, 18 Sep 2015 11:44:33 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Kolchyna", "Olga", ""], ["Souza", "Tharsis T. P.", ""], ["Treleaven", "Philip", ""], ["Aste", "Tomaso", ""]]}, {"id": "1507.00996", "submitter": "Jan-Willem van de Meent", "authors": "Frank Wood, Jan Willem van de Meent, Vikash Mansinghka", "title": "A New Approach to Probabilistic Programming Inference", "comments": "Updated version of the 2014 AISTATS paper (to reflect changes in new\n  language syntax). 10 pages, 3 figures. Proceedings of the Seventeenth\n  International Conference on Artificial Intelligence and Statistics, JMLR\n  Workshop and Conference Proceedings, Vol 33, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and demonstrate a new approach to inference in expressive\nprobabilistic programming languages based on particle Markov chain Monte Carlo.\nOur approach is simple to implement and easy to parallelize. It applies to\nTuring-complete probabilistic programming languages and supports accurate\ninference in models that make use of complex control flow, including stochastic\nrecursion. It also includes primitives from Bayesian nonparametric statistics.\nOur experiments show that this approach can be more efficient than previously\nintroduced single-site Metropolis-Hastings methods.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2015 19:52:58 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2015 10:31:26 GMT"}], "update_date": "2015-07-10", "authors_parsed": [["Wood", "Frank", ""], ["van de Meent", "Jan Willem", ""], ["Mansinghka", "Vikash", ""]]}, {"id": "1507.01059", "submitter": "Hisashi Johno", "authors": "Hisashi Johno, Kazunori Nakamoto, Tatsuhiko Saigo", "title": "Remarks on kernel Bayes' rule", "comments": "22 pages, 5 figures", "journal-ref": null, "doi": "10.1080/23311835.2018.1447220", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel Bayes' rule has been proposed as a nonparametric kernel-based method\nto realize Bayesian inference in reproducing kernel Hilbert spaces. However, we\ndemonstrate both theoretically and experimentally that the prediction result by\nkernel Bayes' rule is in some cases unnatural. We consider that this phenomenon\nis in part due to the fact that the assumptions in kernel Bayes' rule do not\nhold in general.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2015 02:03:02 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2015 08:09:07 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Johno", "Hisashi", ""], ["Nakamoto", "Kazunori", ""], ["Saigo", "Tatsuhiko", ""]]}, {"id": "1507.01073", "submitter": "Makoto Yamada", "authors": "Makoto Yamada, Wenzhao Lian, Amit Goyal, Jianhui Chen, Kishan\n  Wimalawarne, Suleiman A Khan, Samuel Kaski, Hiroshi Mamitsuka, Yi Chang", "title": "Convex Factorization Machine for Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the convex factorization machine (CFM), which is a convex variant\nof the widely used Factorization Machines (FMs). Specifically, we employ a\nlinear+quadratic model and regularize the linear term with the\n$\\ell_2$-regularizer and the quadratic term with the trace norm regularizer.\nThen, we formulate the CFM optimization as a semidefinite programming problem\nand propose an efficient optimization procedure with Hazan's algorithm. A key\nadvantage of CFM over existing FMs is that it can find a globally optimal\nsolution, while FMs may get a poor locally optimal solution since the objective\nfunction of FMs is non-convex. In addition, the proposed algorithm is simple\nyet effective and can be implemented easily. Finally, CFM is a general\nfactorization method and can also be used for other factorization problems\nincluding including multi-view matrix factorization and tensor completion\nproblems. Through synthetic and movielens datasets, we first show that the\nproposed CFM achieves results competitive to FMs. Furthermore, in a\ntoxicogenomics prediction task, we show that CFM outperforms a state-of-the-art\ntensor factorization method.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2015 05:54:29 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2015 17:17:17 GMT"}, {"version": "v3", "created": "Wed, 23 Dec 2015 08:52:42 GMT"}, {"version": "v4", "created": "Mon, 8 Aug 2016 14:55:49 GMT"}, {"version": "v5", "created": "Wed, 10 Aug 2016 01:23:56 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Yamada", "Makoto", ""], ["Lian", "Wenzhao", ""], ["Goyal", "Amit", ""], ["Chen", "Jianhui", ""], ["Wimalawarne", "Kishan", ""], ["Khan", "Suleiman A", ""], ["Kaski", "Samuel", ""], ["Mamitsuka", "Hiroshi", ""], ["Chang", "Yi", ""]]}, {"id": "1507.01154", "submitter": "R\\'emi Bardenet", "authors": "R\\'emi Bardenet, Michalis K. Titsias", "title": "Inference for determinantal point processes without spectral knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determinantal point processes (DPPs) are point process models that naturally\nencode diversity between the points of a given realization, through a positive\ndefinite kernel $K$. DPPs possess desirable properties, such as exact sampling\nor analyticity of the moments, but learning the parameters of kernel $K$\nthrough likelihood-based inference is not straightforward. First, the kernel\nthat appears in the likelihood is not $K$, but another kernel $L$ related to\n$K$ through an often intractable spectral decomposition. This issue is\ntypically bypassed in machine learning by directly parametrizing the kernel\n$L$, at the price of some interpretability of the model parameters. We follow\nthis approach here. Second, the likelihood has an intractable normalizing\nconstant, which takes the form of a large determinant in the case of a DPP over\na finite set of objects, and the form of a Fredholm determinant in the case of\na DPP over a continuous domain. Our main contribution is to derive bounds on\nthe likelihood of a DPP, both for finite and continuous domains. Unlike\nprevious work, our bounds are cheap to evaluate since they do not rely on\napproximating the spectrum of a large matrix or an operator. Through usual\narguments, these bounds thus yield cheap variational inference and moderately\nexpensive exact Markov chain Monte Carlo inference methods for DPPs.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2015 23:26:27 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Bardenet", "R\u00e9mi", ""], ["Titsias", "Michalis K.", ""]]}, {"id": "1507.01160", "submitter": "Vaibhav Srivastava", "authors": "Vaibhav Srivastava, Paul Reverdy, Naomi Ehrich Leonard", "title": "Correlated Multiarmed Bandit Problem: Bayesian Algorithms and Regret\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the correlated multiarmed bandit (MAB) problem in which the\nrewards associated with each arm are modeled by a multivariate Gaussian random\nvariable, and we investigate the influence of the assumptions in the Bayesian\nprior on the performance of the upper credible limit (UCL) algorithm and a new\ncorrelated UCL algorithm. We rigorously characterize the influence of accuracy,\nconfidence, and correlation scale in the prior on the decision-making\nperformance of the algorithms. Our results show how priors and correlation\nstructure can be leveraged to improve performance.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2015 02:16:25 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2015 22:27:35 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["Srivastava", "Vaibhav", ""], ["Reverdy", "Paul", ""], ["Leonard", "Naomi Ehrich", ""]]}, {"id": "1507.01238", "submitter": "Chong You", "authors": "Chong You, Daniel P. Robinson, Rene Vidal", "title": "Scalable Sparse Subspace Clustering by Orthogonal Matching Pursuit", "comments": "13 pages, 1 figure, 2 tables. Accepted to CVPR 2016 as an oral\n  presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace clustering methods based on $\\ell_1$, $\\ell_2$ or nuclear norm\nregularization have become very popular due to their simplicity, theoretical\nguarantees and empirical success. However, the choice of the regularizer can\ngreatly impact both theory and practice. For instance, $\\ell_1$ regularization\nis guaranteed to give a subspace-preserving affinity (i.e., there are no\nconnections between points from different subspaces) under broad conditions\n(e.g., arbitrary subspaces and corrupted data). However, it requires solving a\nlarge scale convex optimization problem. On the other hand, $\\ell_2$ and\nnuclear norm regularization provide efficient closed form solutions, but\nrequire very strong assumptions to guarantee a subspace-preserving affinity,\ne.g., independent subspaces and uncorrupted data. In this paper we study a\nsubspace clustering method based on orthogonal matching pursuit. We show that\nthe method is both computationally efficient and guaranteed to give a\nsubspace-preserving affinity under broad conditions. Experiments on synthetic\ndata verify our theoretical analysis, and applications in handwritten digit and\nface clustering show that our approach achieves the best trade off between\naccuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2015 16:29:31 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2015 04:25:27 GMT"}, {"version": "v3", "created": "Thu, 5 May 2016 20:16:54 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["You", "Chong", ""], ["Robinson", "Daniel P.", ""], ["Vidal", "Rene", ""]]}, {"id": "1507.01279", "submitter": "Shuang Li", "authors": "Shuang Li, Yao Xie, Hanjun Dai, and Le Song", "title": "Scan $B$-Statistic for Kernel Change-Point Detection", "comments": "Submitted for journal publication. Partial results appeared in NIPS\n  2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting the emergence of an abrupt change-point is a classic problem in\nstatistics and machine learning. Kernel-based nonparametric statistics have\nbeen used for this task which enjoy fewer assumptions on the distributions than\nthe parametric approach and can handle high-dimensional data. In this paper we\nfocus on the scenario when the amount of background data is large, and propose\ntwo related computationally efficient kernel-based statistics for change-point\ndetection, which are inspired by the recently developed $B$-statistics. A novel\ntheoretical result of the paper is the characterization of the tail probability\nof these statistics using the change-of-measure technique, which focuses on\ncharacterizing the tail of the detection statistics rather than obtaining its\nasymptotic distribution under the null distribution. Such approximations are\ncrucial to control the false alarm rate, which corresponds to the significance\nlevel in offline change-point detection and the average-run-length in online\nchange-point detection. Our approximations are shown to be highly accurate.\nThus, they provide a convenient way to find detection thresholds for both\noffline and online cases without the need to resort to the more expensive\nsimulations or bootstrapping. We show that our methods perform well on both\nsynthetic data and real data.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2015 21:46:03 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2016 18:41:51 GMT"}, {"version": "v3", "created": "Sun, 8 May 2016 14:36:53 GMT"}, {"version": "v4", "created": "Sun, 24 Sep 2017 04:22:51 GMT"}, {"version": "v5", "created": "Mon, 12 Nov 2018 20:57:24 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Li", "Shuang", ""], ["Xie", "Yao", ""], ["Dai", "Hanjun", ""], ["Song", "Le", ""]]}, {"id": "1507.01307", "submitter": "Chong You", "authors": "C. You, R. Vidal", "title": "Subspace-Sparse Representation", "comments": "15 pages, 3 figures, previous version published in ICML 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an overcomplete dictionary $A$ and a signal $b$ that is a linear\ncombination of a few linearly independent columns of $A$, classical sparse\nrecovery theory deals with the problem of recovering the unique sparse\nrepresentation $x$ such that $b = A x$. It is known that under certain\nconditions on $A$, $x$ can be recovered by the Basis Pursuit (BP) and the\nOrthogonal Matching Pursuit (OMP) algorithms. In this work, we consider the\nmore general case where $b$ lies in a low-dimensional subspace spanned by some\ncolumns of $A$, which are possibly linearly dependent. In this case, the\nsparsest solution $x$ is generally not unique, and we study the problem that\nthe representation $x$ identifies the subspace, i.e. the nonzero entries of $x$\ncorrespond to dictionary atoms that are in the subspace. Such a representation\n$x$ is called subspace-sparse. We present sufficient conditions for\nguaranteeing subspace-sparse recovery, which have clear geometric\ninterpretations and explain properties of subspace-sparse recovery. We also\nshow that the sufficient conditions can be satisfied under a randomized model.\nOur results are applicable to the traditional sparse recovery problem and we\nget conditions for sparse recovery that are less restrictive than the canonical\nmutual coherent condition. We also use the results to analyze the sparse\nrepresentation based classification (SRC) method, for which we get conditions\nto show its correctness.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2015 00:10:04 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["You", "C.", ""], ["Vidal", "R.", ""]]}, {"id": "1507.01497", "submitter": "Neil Rabinowitz", "authors": "Neil C. Rabinowitz and Robbe L. T. Goris and Johannes Ball\\'e and Eero\n  P. Simoncelli", "title": "A model of sensory neural responses in the presence of unknown\n  modulatory inputs", "comments": "9 pages, 5 figures. minor changes since v1: added extra references,\n  connections to previous models, links to GLMs, complexity measures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural responses are highly variable, and some portion of this variability\narises from fluctuations in modulatory factors that alter their gain, such as\nadaptation, attention, arousal, expected or actual reward, emotion, and local\nmetabolic resource availability. Regardless of their origin, fluctuations in\nthese signals can confound or bias the inferences that one derives from spiking\nresponses. Recent work demonstrates that for sensory neurons, these effects can\nbe captured by a modulated Poisson model, whose rate is the product of a\nstimulus-driven response function and an unknown modulatory signal. Here, we\nextend this model, by incorporating explicit modulatory elements that are known\n(specifically, spike-history dependence, as in previous models), and by\nconstraining the remaining latent modulatory signals to be smooth in time. We\ndevelop inference procedures for fitting the entire model, including\nhyperparameters, via evidence optimization, and apply these to simulated data,\nand to responses of ferret auditory midbrain and cortical neurons to complex\nsounds. We show that integrating out the latent modulators yields better (or\nmore readily-interpretable) receptive field estimates than a standard Poisson\nmodel. Conversely, integrating out the stimulus dependence yields estimates of\nthe slowly-varying latent modulators.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2015 15:31:20 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2015 01:28:39 GMT"}], "update_date": "2015-07-08", "authors_parsed": [["Rabinowitz", "Neil C.", ""], ["Goris", "Robbe L. T.", ""], ["Ball\u00e9", "Johannes", ""], ["Simoncelli", "Eero P.", ""]]}, {"id": "1507.01661", "submitter": "Xiao Fu", "authors": "Xiao Fu, Wing-Kin Ma, Jos\\'e Bioucas-Dias, Tsung-Han Chan", "title": "Semiblind Hyperspectral Unmixing in the Presence of Spectral Library\n  Mismatches", "comments": null, "journal-ref": null, "doi": "10.1109/TGRS.2016.2557340", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dictionary-aided sparse regression (SR) approach has recently emerged as\na promising alternative to hyperspectral unmixing (HU) in remote sensing. By\nusing an available spectral library as a dictionary, the SR approach identifies\nthe underlying materials in a given hyperspectral image by selecting a small\nsubset of spectral samples in the dictionary to represent the whole image. A\ndrawback with the current SR developments is that an actual spectral signature\nin the scene is often assumed to have zero mismatch with its corresponding\ndictionary sample, and such an assumption is considered too ideal in practice.\nIn this paper, we tackle the spectral signature mismatch problem by proposing a\ndictionary-adjusted nonconvex sparsity-encouraging regression (DANSER)\nframework. The main idea is to incorporate dictionary correcting variables in\nan SR formulation. A simple and low per-iteration complexity algorithm is\ntailor-designed for practical realization of DANSER. Using the same dictionary\ncorrecting idea, we also propose a robust subspace solution for dictionary\npruning. Extensive simulations and real-data experiments show that the proposed\nmethod is effective in mitigating the undesirable spectral signature mismatch\neffects.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2015 03:00:17 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Fu", "Xiao", ""], ["Ma", "Wing-Kin", ""], ["Bioucas-Dias", "Jos\u00e9", ""], ["Chan", "Tsung-Han", ""]]}, {"id": "1507.01784", "submitter": "Anastasia Podosinnikova", "authors": "Anastasia Podosinnikova, Francis Bach, and Simon Lacoste-Julien", "title": "Rethinking LDA: moment matching for discrete ICA", "comments": "30 pages; added plate diagrams and clarifications, changed style,\n  corrected typos, updated figures. in Proceedings of the 29-th Conference on\n  Neural Information Processing Systems (NIPS), 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider moment matching techniques for estimation in Latent Dirichlet\nAllocation (LDA). By drawing explicit links between LDA and discrete versions\nof independent component analysis (ICA), we first derive a new set of\ncumulant-based tensors, with an improved sample complexity. Moreover, we reuse\nstandard ICA techniques such as joint diagonalization of tensors to improve\nover existing methods based on the tensor power method. In an extensive set of\nexperiments on both synthetic and real datasets, we show that our new\ncombination of tensors and orthogonal joint diagonalization techniques\noutperforms existing moment matching methods.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2015 12:48:30 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2015 20:16:04 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Podosinnikova", "Anastasia", ""], ["Bach", "Francis", ""], ["Lacoste-Julien", "Simon", ""]]}, {"id": "1507.01826", "submitter": "Natalie Stanley", "authors": "Natalie Stanley, Saray Shai, Dane Taylor, Peter J. Mucha", "title": "Clustering Network Layers With the Strata Multilayer Stochastic Block\n  Model", "comments": null, "journal-ref": null, "doi": "10.1109/TNSE.2016.2537545", "report-no": null, "categories": "cs.SI physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilayer networks are a useful data structure for simultaneously capturing\nmultiple types of relationships between a set of nodes. In such networks, each\nrelational definition gives rise to a layer. While each layer provides its own\nset of information, community structure across layers can be collectively\nutilized to discover and quantify underlying relational patterns between nodes.\nTo concisely extract information from a multilayer network, we propose to\nidentify and combine sets of layers with meaningful similarities in community\nstructure. In this paper, we describe the \"strata multilayer stochastic block\nmodel'' (sMLSBM), a probabilistic model for multilayer community structure. The\ncentral extension of the model is that there exist groups of layers, called\n\"strata'', which are defined such that all layers in a given stratum have\ncommunity structure described by a common stochastic block model (SBM). That\nis, layers in a stratum exhibit similar node-to-community assignments and SBM\nprobability parameters. Fitting the sMLSBM to a multilayer network provides a\njoint clustering that yields node-to-community and layer-to-stratum\nassignments, which cooperatively aid one another during inference. We describe\nan algorithm for separating layers into their appropriate strata and an\ninference technique for estimating the SBM parameters for each stratum. We\ndemonstrate our method using synthetic networks and a multilayer network\ninferred from data collected in the Human Microbiome Project.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2015 14:36:04 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2015 14:28:58 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Stanley", "Natalie", ""], ["Shai", "Saray", ""], ["Taylor", "Dane", ""], ["Mucha", "Peter J.", ""]]}, {"id": "1507.01890", "submitter": "Zhana Kuncheva", "authors": "Zhana Kuncheva and Giovanni Montana", "title": "Community detection in multiplex networks using locally adaptive random\n  walks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiplex networks, a special type of multilayer networks, are increasingly\napplied in many domains ranging from social media analytics to biology. A\ncommon task in these applications concerns the detection of community\nstructures. Many existing algorithms for community detection in multiplexes\nattempt to detect communities which are shared by all layers. In this article\nwe propose a community detection algorithm, LART (Locally Adaptive Random\nTransitions), for the detection of communities that are shared by either some\nor all the layers in the multiplex. The algorithm is based on a random walk on\nthe multiplex, and the transition probabilities defining the random walk are\nallowed to depend on the local topological similarity between layers at any\ngiven node so as to facilitate the exploration of communities across layers.\nBased on this random walk, a node dissimilarity measure is derived and nodes\nare clustered based on this distance in a hierarchical fashion. We present\nexperimental results using networks simulated under various scenarios to\nshowcase the performance of LART in comparison to related community detection\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2015 14:24:05 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2015 14:57:50 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Kuncheva", "Zhana", ""], ["Montana", "Giovanni", ""]]}, {"id": "1507.01972", "submitter": "Gr\\'egoire Montavon", "authors": "Gr\\'egoire Montavon, Klaus-Robert M\\\"uller, Marco Cuturi", "title": "Wasserstein Training of Boltzmann Machines", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Boltzmann machine provides a useful framework to learn highly complex,\nmultimodal and multiscale data distributions that occur in the real world. The\ndefault method to learn its parameters consists of minimizing the\nKullback-Leibler (KL) divergence from training samples to the Boltzmann model.\nWe propose in this work a novel approach for Boltzmann training which assumes\nthat a meaningful metric between observations is given. This metric can be\nrepresented by the Wasserstein distance between distributions, for which we\nderive a gradient with respect to the model parameters. Minimization of this\nnew Wasserstein objective leads to generative models that are better when\nconsidering the metric and that have a cluster-like structure. We demonstrate\nthe practical potential of these models for data completion and denoising, for\nwhich the metric between observations plays a crucial role.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2015 21:30:36 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["Montavon", "Gr\u00e9goire", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Cuturi", "Marco", ""]]}, {"id": "1507.01978", "submitter": "Magda Gregorova", "authors": "Magda Gregorova, Alexandros Kalousis, St\\'ephane Marchand-Maillet", "title": "Learning Leading Indicators for Time Series Predictions", "comments": "Changed title plus minor updates in the text", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning models for forecasting multiple\ntime-series systems together with discovering the leading indicators that serve\nas good predictors for the system. We model the systems by linear vector\nautoregressive models (VAR) and link the discovery of leading indicators to\ninferring sparse graphs of Granger-causality. We propose new problem\nformulations and develop two new methods to learn such models, gradually\nincreasing the complexity of assumptions and approaches. While the first method\nassumes common structures across the whole system, our second method uncovers\nmodel clusters based on the Granger-causality and leading indicators together\nwith learning the model parameters. We study the performance of our methods on\na comprehensive set of experiments and confirm their efficacy and their\nadvantages over state-of-the-art sparse VAR and graphical Granger learning\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2015 22:18:43 GMT"}, {"version": "v2", "created": "Mon, 25 Apr 2016 12:54:19 GMT"}, {"version": "v3", "created": "Wed, 2 Nov 2016 18:52:54 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Gregorova", "Magda", ""], ["Kalousis", "Alexandros", ""], ["Marchand-Maillet", "St\u00e9phane", ""]]}, {"id": "1507.02000", "submitter": "Guanghui Lan", "authors": "Guanghui Lan, Yi Zhou", "title": "An optimal randomized incremental gradient method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a class of finite-sum convex optimization problems\nwhose objective function is given by the summation of $m$ ($\\ge 1$) smooth\ncomponents together with some other relatively simple terms. We first introduce\na deterministic primal-dual gradient (PDG) method that can achieve the optimal\nblack-box iteration complexity for solving these composite optimization\nproblems using a primal-dual termination criterion. Our major contribution is\nto develop a randomized primal-dual gradient (RPDG) method, which needs to\ncompute the gradient of only one randomly selected smooth component at each\niteration, but can possibly achieve better complexity than PDG in terms of the\ntotal number of gradient evaluations. More specifically, we show that the total\nnumber of gradient evaluations performed by RPDG can be ${\\cal O} (\\sqrt{m})$\ntimes smaller, both in expectation and with high probability, than those\nperformed by deterministic optimal first-order methods under favorable\nsituations. We also show that the complexity of the RPDG method is not\nimprovable by developing a new lower complexity bound for a general class of\nrandomized methods for solving large-scale finite-sum convex optimization\nproblems. Moreover, through the development of PDG and RPDG, we introduce a\nnovel game-theoretic interpretation for these optimal methods for convex\noptimization.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 00:49:52 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2015 11:58:29 GMT"}, {"version": "v3", "created": "Sun, 18 Oct 2015 20:59:35 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Lan", "Guanghui", ""], ["Zhou", "Yi", ""]]}, {"id": "1507.02188", "submitter": "Abhishek Thakur", "authors": "Abhishek Thakur and Artus Krohn-Grimberghe", "title": "AutoCompete: A Framework for Machine Learning Competition", "comments": "Paper at AutoML workshop in ICML, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose AutoCompete, a highly automated machine learning\nframework for tackling machine learning competitions. This framework has been\nlearned by us, validated and improved over a period of more than two years by\nparticipating in online machine learning competitions. It aims at minimizing\nhuman interference required to build a first useful predictive model and to\nassess the practical difficulty of a given machine learning challenge. The\nproposed system helps in identifying data types, choosing a machine learn- ing\nmodel, tuning hyper-parameters, avoiding over-fitting and optimization for a\nprovided evaluation metric. We also observe that the proposed system produces\nbetter (or comparable) results with less runtime as compared to other\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 15:07:39 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["Thakur", "Abhishek", ""], ["Krohn-Grimberghe", "Artus", ""]]}, {"id": "1507.02189", "submitter": "Rong Ge", "authors": "Rong Ge and James Zou", "title": "Intersecting Faces: Non-negative Matrix Factorization With New\n  Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-negative matrix factorization (NMF) is a natural model of admixture and\nis widely used in science and engineering. A plethora of algorithms have been\ndeveloped to tackle NMF, but due to the non-convex nature of the problem, there\nis little guarantee on how well these methods work. Recently a surge of\nresearch have focused on a very restricted class of NMFs, called separable NMF,\nwhere provably correct algorithms have been developed. In this paper, we\npropose the notion of subset-separable NMF, which substantially generalizes the\nproperty of separability. We show that subset-separability is a natural\nnecessary condition for the factorization to be unique or to have minimum\nvolume. We developed the Face-Intersect algorithm which provably and\nefficiently solves subset-separable NMF under natural conditions, and we prove\nthat our algorithm is robust to small noise. We explored the performance of\nFace-Intersect on simulations and discuss settings where it empirically\noutperformed the state-of-art methods. Our work is a step towards finding\nprovably correct algorithms that solve large classes of NMF problems.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 15:07:40 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["Ge", "Rong", ""], ["Zou", "James", ""]]}, {"id": "1507.02216", "submitter": "Cecile Chenot", "authors": "Cecile Chenot, Jerome Bobin and Jeremy Rapin", "title": "Robust Sparse Blind Source Separation", "comments": "Submitted to IEEE Signal Processing Letters", "journal-ref": null, "doi": "10.1109/LSP.2015.2463232", "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind Source Separation is a widely used technique to analyze multichannel\ndata. In many real-world applications, its results can be significantly\nhampered by the presence of unknown outliers. In this paper, a novel algorithm\ncoined rGMCA (robust Generalized Morphological Component Analysis) is\nintroduced to retrieve sparse sources in the presence of outliers. It\nexplicitly estimates the sources, the mixing matrix, and the outliers. It also\ntakes advantage of the estimation of the outliers to further implement a\nweighting scheme, which provides a highly robust separation procedure.\nNumerical experiments demonstrate the efficiency of rGMCA to estimate the\nmixing matrix in comparison with standard BSS techniques.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 16:50:26 GMT"}, {"version": "v2", "created": "Mon, 25 Apr 2016 12:06:29 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Chenot", "Cecile", ""], ["Bobin", "Jerome", ""], ["Rapin", "Jeremy", ""]]}, {"id": "1507.02268", "submitter": "Jelani Nelson", "authors": "Michael B. Cohen, Jelani Nelson, David P. Woodruff", "title": "Optimal approximate matrix product in terms of stable rank", "comments": "v3: minor edits; v2: fixed one step in proof of Theorem 9 which was\n  wrong by a constant factor (see the new Lemma 5 and its use; final theorem\n  unaffected)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove, using the subspace embedding guarantee in a black box way, that one\ncan achieve the spectral norm guarantee for approximate matrix multiplication\nwith a dimensionality-reducing map having $m = O(\\tilde{r}/\\varepsilon^2)$\nrows. Here $\\tilde{r}$ is the maximum stable rank, i.e. squared ratio of\nFrobenius and operator norms, of the two matrices being multiplied. This is a\nquantitative improvement over previous work of [MZ11, KVZ14], and is also\noptimal for any oblivious dimensionality-reducing map. Furthermore, due to the\nblack box reliance on the subspace embedding property in our proofs, our\ntheorem can be applied to a much more general class of sketching matrices than\nwhat was known before, in addition to achieving better bounds. For example, one\ncan apply our theorem to efficient subspace embeddings such as the Subsampled\nRandomized Hadamard Transform or sparse subspace embeddings, or even with\nsubspace embedding constructions that may be developed in the future.\n  Our main theorem, via connections with spectral error matrix multiplication\nshown in prior work, implies quantitative improvements for approximate least\nsquares regression and low rank approximation. Our main result has also already\nbeen applied to improve dimensionality reduction guarantees for $k$-means\nclustering [CEMMP14], and implies new results for nonparametric regression\n[YPW15].\n  We also separately point out that the proof of the \"BSS\" deterministic\nrow-sampling result of [BSS12] can be modified to show that for any matrices\n$A, B$ of stable rank at most $\\tilde{r}$, one can achieve the spectral norm\nguarantee for approximate matrix multiplication of $A^T B$ by deterministically\nsampling $O(\\tilde{r}/\\varepsilon^2)$ rows that can be found in polynomial\ntime. The original result of [BSS12] was for rank instead of stable rank. Our\nobservation leads to a stronger version of a main theorem of [KMST10].\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 19:45:21 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2015 22:33:26 GMT"}, {"version": "v3", "created": "Wed, 2 Mar 2016 12:58:32 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Cohen", "Michael B.", ""], ["Nelson", "Jelani", ""], ["Woodruff", "David P.", ""]]}, {"id": "1507.02284", "submitter": "Greg Ver Steeg", "authors": "Greg Ver Steeg and Aram Galstyan", "title": "The Information Sieve", "comments": "Appearing in Proceedings of the International Conference on Machine\n  Learning (ICML), 2016. Updated reference to continuous version:\n  http://arxiv.org/abs/1606.02307", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new framework for unsupervised learning of representations\nbased on a novel hierarchical decomposition of information. Intuitively, data\nis passed through a series of progressively fine-grained sieves. Each layer of\nthe sieve recovers a single latent factor that is maximally informative about\nmultivariate dependence in the data. The data is transformed after each pass so\nthat the remaining unexplained information trickles down to the next layer.\nUltimately, we are left with a set of latent factors explaining all the\ndependence in the original data and remainder information consisting of\nindependent noise. We present a practical implementation of this framework for\ndiscrete variables and apply it to a variety of fundamental tasks in\nunsupervised learning including independent component analysis, lossy and\nlossless compression, and predicting missing values in data.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 20:00:42 GMT"}, {"version": "v2", "created": "Fri, 29 Apr 2016 20:29:36 GMT"}, {"version": "v3", "created": "Thu, 9 Jun 2016 00:12:24 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Steeg", "Greg Ver", ""], ["Galstyan", "Aram", ""]]}, {"id": "1507.02293", "submitter": "Manuel Gomez Rodriguez", "authors": "Mehrdad Farajtabar and Yichen Wang and Manuel Gomez Rodriguez and\n  Shuang Li and Hongyuan Zha and Le Song", "title": "COEVOLVE: A Joint Point Process Model for Information Diffusion and\n  Network Co-evolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information diffusion in online social networks is affected by the underlying\nnetwork topology, but it also has the power to change it. Online users are\nconstantly creating new links when exposed to new information sources, and in\nturn these links are alternating the way information spreads. However, these\ntwo highly intertwined stochastic processes, information diffusion and network\nevolution, have been predominantly studied separately, ignoring their\nco-evolutionary dynamics.\n  We propose a temporal point process model, COEVOLVE, for such joint dynamics,\nallowing the intensity of one process to be modulated by that of the other.\nThis model allows us to efficiently simulate interleaved diffusion and network\nevents, and generate traces obeying common diffusion and network patterns\nobserved in real-world networks. Furthermore, we also develop a convex\noptimization framework to learn the parameters of the model from historical\ndiffusion and network evolution traces. We experimented with both synthetic\ndata and data gathered from Twitter, and show that our model provides a good\nfit to the data as well as more accurate predictions than alternatives.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 20:01:32 GMT"}, {"version": "v2", "created": "Fri, 1 Apr 2016 13:51:32 GMT"}], "update_date": "2016-04-04", "authors_parsed": [["Farajtabar", "Mehrdad", ""], ["Wang", "Yichen", ""], ["Rodriguez", "Manuel Gomez", ""], ["Li", "Shuang", ""], ["Zha", "Hongyuan", ""], ["Song", "Le", ""]]}, {"id": "1507.02323", "submitter": "Naman Agarwal", "authors": "Naman Agarwal and Afonso S. Bandeira and Konstantinos Koiliaris and\n  Alexandra Kolla", "title": "Multisection in the Stochastic Block Model using Semidefinite\n  Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of identifying underlying community-like structures\nin graphs. Towards this end we study the Stochastic Block Model (SBM) on\n$k$-clusters: a random model on $n=km$ vertices, partitioned in $k$ equal sized\nclusters, with edges sampled independently across clusters with probability $q$\nand within clusters with probability $p$, $p>q$. The goal is to recover the\ninitial \"hidden\" partition of $[n]$. We study semidefinite programming (SDP)\nbased algorithms in this context. In the regime $p = \\frac{\\alpha \\log(m)}{m}$\nand $q = \\frac{\\beta \\log(m)}{m}$ we show that a certain natural SDP based\nalgorithm solves the problem of {\\em exact recovery} in the $k$-community SBM,\nwith high probability, whenever $\\sqrt{\\alpha} - \\sqrt{\\beta} > \\sqrt{1}$, as\nlong as $k=o(\\log n)$. This threshold is known to be the information\ntheoretically optimal. We also study the case when $k=\\theta(\\log(n))$. In this\ncase however we achieve recovery guarantees that no longer match the optimal\ncondition $\\sqrt{\\alpha} - \\sqrt{\\beta} > \\sqrt{1}$, thus leaving achieving\noptimality for this range an open question.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 22:02:45 GMT"}], "update_date": "2015-07-10", "authors_parsed": [["Agarwal", "Naman", ""], ["Bandeira", "Afonso S.", ""], ["Koiliaris", "Konstantinos", ""], ["Kolla", "Alexandra", ""]]}, {"id": "1507.02356", "submitter": "Chintan Dalal", "authors": "Chintan A. Dalal, Vladimir Pavlovic, Robert E. Kopp", "title": "Intrinsic Non-stationary Covariance Function for Climate Modeling", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing a covariance function that represents the underlying correlation is\na crucial step in modeling complex natural systems, such as climate models.\nGeospatial datasets at a global scale usually suffer from non-stationarity and\nnon-uniformly smooth spatial boundaries. A Gaussian process regression using a\nnon-stationary covariance function has shown promise for this task, as this\ncovariance function adapts to the variable correlation structure of the\nunderlying distribution. In this paper, we generalize the non-stationary\ncovariance function to address the aforementioned global scale geospatial\nissues. We define this generalized covariance function as an intrinsic\nnon-stationary covariance function, because it uses intrinsic statistics of the\nsymmetric positive definite matrices to represent the characteristic length\nscale and, thereby, models the local stochastic process. Experiments on a\nsynthetic and real dataset of relative sea level changes across the world\ndemonstrate improvements in the error metrics for the regression estimates\nusing our newly proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 02:52:19 GMT"}], "update_date": "2015-07-10", "authors_parsed": [["Dalal", "Chintan A.", ""], ["Pavlovic", "Vladimir", ""], ["Kopp", "Robert E.", ""]]}, {"id": "1507.02592", "submitter": "Nishant Mehta", "authors": "Tim van Erven and Peter D. Gr\\\"unwald and Nishant A. Mehta and Mark D.\n  Reid and Robert C. Williamson", "title": "Fast rates in statistical and online learning", "comments": "69 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The speed with which a learning algorithm converges as it is presented with\nmore data is a central problem in machine learning --- a fast rate of\nconvergence means less data is needed for the same level of performance. The\npursuit of fast rates in online and statistical learning has led to the\ndiscovery of many conditions in learning theory under which fast learning is\npossible. We show that most of these conditions are special cases of a single,\nunifying condition, that comes in two forms: the central condition for 'proper'\nlearning algorithms that always output a hypothesis in the given model, and\nstochastic mixability for online algorithms that may make predictions outside\nof the model. We show that under surprisingly weak assumptions both conditions\nare, in a certain sense, equivalent. The central condition has a\nre-interpretation in terms of convexity of a set of pseudoprobabilities,\nlinking it to density estimation under misspecification. For bounded losses, we\nshow how the central condition enables a direct proof of fast rates and we\nprove its equivalence to the Bernstein condition, itself a generalization of\nthe Tsybakov margin condition, both of which have played a central role in\nobtaining fast rates in statistical learning. Yet, while the Bernstein\ncondition is two-sided, the central condition is one-sided, making it more\nsuitable to deal with unbounded losses. In its stochastic mixability form, our\ncondition generalizes both a stochastic exp-concavity condition identified by\nJuditsky, Rigollet and Tsybakov and Vovk's notion of mixability. Our unifying\nconditions thus provide a substantial step towards a characterization of fast\nrates in statistical learning, similar to how classical mixability\ncharacterizes constant regret in the sequential prediction with expert advice\nsetting.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 16:53:30 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2015 09:38:07 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["van Erven", "Tim", ""], ["Gr\u00fcnwald", "Peter D.", ""], ["Mehta", "Nishant A.", ""], ["Reid", "Mark D.", ""], ["Williamson", "Robert C.", ""]]}, {"id": "1507.02646", "submitter": "Aki Vehtari", "authors": "Aki Vehtari, Daniel Simpson, Andrew Gelman, Yuling Yao, Jonah Gabry", "title": "Pareto Smoothed Importance Sampling", "comments": "Minor revision: fixed some typos and updated references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Importance weighting is a general way to adjust Monte Carlo integration to\naccount for draws from the wrong distribution, but the resulting estimate can\nbe noisy when the importance ratios have a heavy right tail. This routinely\noccurs when there are aspects of the target distribution that are not well\ncaptured by the approximating distribution, in which case more stable estimates\ncan be obtained by modifying extreme importance ratios. We present a new method\nfor stabilizing importance weights using a generalized Pareto distribution fit\nto the upper tail of the distribution of the simulated importance ratios. The\nmethod, which empirically performs better than existing methods for stabilizing\nimportance sampling estimates, includes stabilized effective sample size\nestimates, Monte Carlo error estimates and convergence diagnostics.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 18:43:28 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2015 18:37:17 GMT"}, {"version": "v3", "created": "Fri, 23 Sep 2016 11:30:37 GMT"}, {"version": "v4", "created": "Mon, 26 Sep 2016 08:34:49 GMT"}, {"version": "v5", "created": "Sat, 21 Oct 2017 08:37:46 GMT"}, {"version": "v6", "created": "Tue, 2 Jul 2019 13:56:16 GMT"}, {"version": "v7", "created": "Tue, 23 Feb 2021 10:07:05 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Vehtari", "Aki", ""], ["Simpson", "Daniel", ""], ["Gelman", "Andrew", ""], ["Yao", "Yuling", ""], ["Gabry", "Jonah", ""]]}, {"id": "1507.02672", "submitter": "Mathias Berglund", "authors": "Antti Rasmus and Harri Valpola and Mikko Honkala and Mathias Berglund\n  and Tapani Raiko", "title": "Semi-Supervised Learning with Ladder Networks", "comments": "Revised denoising function, updated results, fixed typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We combine supervised learning with unsupervised learning in deep neural\nnetworks. The proposed model is trained to simultaneously minimize the sum of\nsupervised and unsupervised cost functions by backpropagation, avoiding the\nneed for layer-wise pre-training. Our work builds on the Ladder network\nproposed by Valpola (2015), which we extend by combining the model with\nsupervision. We show that the resulting model reaches state-of-the-art\nperformance in semi-supervised MNIST and CIFAR-10 classification, in addition\nto permutation-invariant MNIST classification with all labels.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 19:52:19 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2015 09:22:23 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Rasmus", "Antti", ""], ["Valpola", "Harri", ""], ["Honkala", "Mikko", ""], ["Berglund", "Mathias", ""], ["Raiko", "Tapani", ""]]}, {"id": "1507.02743", "submitter": "Prateek Jain", "authors": "Kush Bhatia and Himanshu Jain and Purushottam Kar and Prateek Jain and\n  Manik Varma", "title": "Locally Non-linear Embeddings for Extreme Multi-label Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective in extreme multi-label learning is to train a classifier that\ncan automatically tag a novel data point with the most relevant subset of\nlabels from an extremely large label set. Embedding based approaches make\ntraining and prediction tractable by assuming that the training label matrix is\nlow-rank and hence the effective number of labels can be reduced by projecting\nthe high dimensional label vectors onto a low dimensional linear subspace.\nStill, leading embedding approaches have been unable to deliver high prediction\naccuracies or scale to large problems as the low rank assumption is violated in\nmost real world applications.\n  This paper develops the X-One classifier to address both limitations. The\nmain technical contribution in X-One is a formulation for learning a small\nensemble of local distance preserving embeddings which can accurately predict\ninfrequently occurring (tail) labels. This allows X-One to break free of the\ntraditional low-rank assumption and boost classification accuracy by learning\nembeddings which preserve pairwise distances between only the nearest label\nvectors.\n  We conducted extensive experiments on several real-world as well as benchmark\ndata sets and compared our method against state-of-the-art methods for extreme\nmulti-label classification. Experiments reveal that X-One can make\nsignificantly more accurate predictions then the state-of-the-art methods\nincluding both embeddings (by as much as 35%) as well as trees (by as much as\n6%). X-One can also scale efficiently to data sets with a million labels which\nare beyond the pale of leading embedding methods.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 23:29:10 GMT"}], "update_date": "2015-07-13", "authors_parsed": [["Bhatia", "Kush", ""], ["Jain", "Himanshu", ""], ["Kar", "Purushottam", ""], ["Jain", "Prateek", ""], ["Varma", "Manik", ""]]}, {"id": "1507.02801", "submitter": "Heysem Kaya Dr", "authors": "Heysem Kaya and Albert Ali Salah", "title": "Adaptive Mixtures of Factor Analyzers", "comments": "Pre-print has 30 pages including the appendix and references. A\n  MATLAB tool of the proposed method is available (see the conclusions section)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mixture of factor analyzers is a semi-parametric density estimator that\ngeneralizes the well-known mixtures of Gaussians model by allowing each\nGaussian in the mixture to be represented in a different lower-dimensional\nmanifold. This paper presents a robust and parsimonious model selection\nalgorithm for training a mixture of factor analyzers, carrying out simultaneous\nclustering and locally linear, globally nonlinear dimensionality reduction.\nPermitting different number of factors per mixture component, the algorithm\nadapts the model complexity to the data complexity. We compare the proposed\nalgorithm with related automatic model selection algorithms on a number of\nbenchmarks. The results indicate the effectiveness of this fast and robust\napproach in clustering, manifold learning and class-conditional modeling.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2015 08:13:02 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2015 19:50:02 GMT"}], "update_date": "2015-10-23", "authors_parsed": [["Kaya", "Heysem", ""], ["Salah", "Albert Ali", ""]]}, {"id": "1507.02925", "submitter": "Tue Herlau Mr", "authors": "Tue Herlau, Mikkel N. Schmidt, Morten M{\\o}rup", "title": "Completely random measures for modelling block-structured networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many statistical methods for network data parameterize the edge-probability\nby attributing latent traits to the vertices such as block structure and assume\nexchangeability in the sense of the Aldous-Hoover representation theorem.\nEmpirical studies of networks indicate that many real-world networks have a\npower-law distribution of the vertices which in turn implies the number of\nedges scale slower than quadratically in the number of vertices. These\nassumptions are fundamentally irreconcilable as the Aldous-Hoover theorem\nimplies quadratic scaling of the number of edges. Recently Caron and Fox (2014)\nproposed the use of a different notion of exchangeability due to Kallenberg\n(2009) and obtained a network model which admits power-law behaviour while\nretaining desirable statistical properties, however this model does not capture\nlatent vertex traits such as block-structure. In this work we re-introduce the\nuse of block-structure for network models obeying Kallenberg's notion of\nexchangeability and thereby obtain a model which admits the inference of\nblock-structure and edge inhomogeneity. We derive a simple expression for the\nlikelihood and an efficient sampling method. The obtained model is not\nsignificantly more difficult to implement than existing approaches to\nblock-modelling and performs well on real network datasets.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2015 14:49:52 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2015 08:48:17 GMT"}, {"version": "v3", "created": "Fri, 4 Dec 2015 11:30:37 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Herlau", "Tue", ""], ["Schmidt", "Mikkel N.", ""], ["M\u00f8rup", "Morten", ""]]}, {"id": "1507.02971", "submitter": "Matias Quiroz", "authors": "Matias Quiroz, Mattias Villani and Robert Kohn", "title": "Scalable MCMC for Large Data Problems using Data Subsampling and the\n  Difference Estimator", "comments": "The content in this paper is now in arXiv:1404.4178, as a result of a\n  major revision of that paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a generic Markov Chain Monte Carlo (MCMC) algorithm to speed up\ncomputations for datasets with many observations. A key feature of our approach\nis the use of the highly efficient difference estimator from the survey\nsampling literature to estimate the log-likelihood accurately using only a\nsmall fraction of the data. Our algorithm improves on the $O(n)$ complexity of\nregular MCMC by operating over local data clusters instead of the full sample\nwhen computing the likelihood. The likelihood estimate is used in a\nPseudo-marginal framework to sample from a perturbed posterior which is within\n$O(m^{-1/2})$ of the true posterior, where $m$ is the subsample size. The\nmethod is applied to a logistic regression model to predict firm bankruptcy for\na large data set. We document a significant speed up in comparison to the\nstandard MCMC on the full dataset.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2015 17:10:33 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2015 11:49:19 GMT"}, {"version": "v3", "created": "Wed, 2 Aug 2017 00:33:01 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Quiroz", "Matias", ""], ["Villani", "Mattias", ""], ["Kohn", "Robert", ""]]}, {"id": "1507.03003", "submitter": "Edgar Dobriban", "authors": "Edgar Dobriban and Stefan Wager", "title": "High-Dimensional Asymptotics of Prediction: Ridge Regression and\n  Classification", "comments": "Added a section on prediction versus estimation for ridge regression.\n  Rewrote introduction. Other results unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a unified analysis of the predictive risk of ridge regression and\nregularized discriminant analysis in a dense random effects model. We work in a\nhigh-dimensional asymptotic regime where $p, n \\to \\infty$ and $p/n \\to \\gamma\n\\in (0, \\, \\infty)$, and allow for arbitrary covariance among the features. For\nboth methods, we provide an explicit and efficiently computable expression for\nthe limiting predictive risk, which depends only on the spectrum of the\nfeature-covariance matrix, the signal strength, and the aspect ratio $\\gamma$.\nEspecially in the case of regularized discriminant analysis, we find that\npredictive accuracy has a nuanced dependence on the eigenvalue distribution of\nthe covariance matrix, suggesting that analyses based on the operator norm of\nthe covariance matrix may not be sharp. Our results also uncover several\nqualitative insights about both methods: for example, with ridge regression,\nthere is an exact inverse relation between the limiting predictive risk and the\nlimiting estimation risk given a fixed signal strength. Our analysis builds on\nrecent advances in random matrix theory.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2015 19:48:07 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2015 20:38:45 GMT"}], "update_date": "2015-11-05", "authors_parsed": [["Dobriban", "Edgar", ""], ["Wager", "Stefan", ""]]}, {"id": "1507.03040", "submitter": "Yury Maximov", "authors": "Yury Maximov, Daria Reshetova", "title": "Tight Risk Bounds for Multi-Class Margin Classifiers", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a problem of risk estimation for large-margin multi-class\nclassifiers. We propose a novel risk bound for the multi-class classification\nproblem. The bound involves the marginal distribution of the classifier and the\nRademacher complexity of the hypothesis class. We prove that our bound is tight\nin the number of classes. Finally, we compare our bound with the related ones\nand provide a simplified version of the bound for the multi-class\nclassification with kernel based hypotheses.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2015 22:19:17 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2015 15:22:50 GMT"}, {"version": "v3", "created": "Sat, 2 Jul 2016 22:07:43 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Maximov", "Yury", ""], ["Reshetova", "Daria", ""]]}, {"id": "1507.03092", "submitter": "Matthias Schmid", "authors": "Matthias Schmid, Marvin Wright, Andreas Ziegler", "title": "On the use of Harrell's C for clinical risk prediction via random\n  survival forests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random survival forests (RSF) are a powerful method for risk prediction of\nright-censored outcomes in biomedical research. RSF use the log-rank split\ncriterion to form an ensemble of survival trees. The most common approach to\nevaluate the prediction accuracy of a RSF model is Harrell's concordance index\nfor survival data ('C index'). Conceptually, this strategy implies that the\nsplit criterion in RSF is different from the evaluation criterion of interest.\nThis discrepancy can be overcome by using Harrell's C for both node splitting\nand evaluation. We compare the difference between the two split criteria\nanalytically and in simulation studies with respect to the preference of more\nunbalanced splits, termed end-cut preference (ECP). Specifically, we show that\nthe log-rank statistic has a stronger ECP compared to the C index. In\nsimulation studies and with the help of two medical data sets we demonstrate\nthat the accuracy of RSF predictions, as measured by Harrell's C, can be\nimproved if the log-rank statistic is replaced by the C index for node\nsplitting. This is especially true in situations where the censoring rate or\nthe fraction of informative continuous predictor variables is high. Conversely,\nlog-rank splitting is preferable in noisy scenarios. Both C-based and log-rank\nsplitting are implemented in the R~package ranger. We recommend Harrell's C as\nsplit criterion for use in smaller scale clinical studies and the log-rank\nsplit criterion for use in large-scale 'omics' studies.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2015 10:31:50 GMT"}, {"version": "v2", "created": "Mon, 18 Jul 2016 20:13:45 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Schmid", "Matthias", ""], ["Wright", "Marvin", ""], ["Ziegler", "Andreas", ""]]}, {"id": "1507.03111", "submitter": "Boumediene Hamzi", "authors": "Fritz Colonius and Boumediene Hamzi", "title": "Kernel Methods for Linear Discrete-Time Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods from learning theory are used in the state space of linear dynamical\nand control systems in order to estimate the system matrices. An application to\nstabilization via algebraic Riccati equations is included. The approach is\nillustrated via a series of numerical examples.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2015 14:49:12 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2015 20:12:37 GMT"}], "update_date": "2015-08-12", "authors_parsed": [["Colonius", "Fritz", ""], ["Hamzi", "Boumediene", ""]]}, {"id": "1507.03130", "submitter": "Yun Yang", "authors": "Yun Yang and Surya Tokdar", "title": "Joint estimation of quantile planes over arbitrary predictor spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spite of the recent surge of interest in quantile regression, joint\nestimation of linear quantile planes remains a great challenge in statistics\nand econometrics. We propose a novel parametrization that characterizes any\ncollection of non-crossing quantile planes over arbitrarily shaped convex\npredictor domains in any dimension by means of unconstrained scalar, vector and\nfunction valued parameters. Statistical models based on this parametrization\ninherit a fast computation of the likelihood function, enabling penalized\nlikelihood or Bayesian approaches to model fitting. We introduce a complete\nBayesian methodology by using Gaussian process prior distributions on the\nfunction valued parameters and develop a robust and efficient Markov chain\nMonte Carlo parameter estimation. The resulting method is shown to offer\nposterior consistency under mild tail and regularity conditions. We present\nseveral illustrative examples where the new method is compared against existing\napproaches and is found to offer better accuracy, coverage and model fit.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2015 17:30:04 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Yang", "Yun", ""], ["Tokdar", "Surya", ""]]}, {"id": "1507.03133", "submitter": "Rahul Mazumder", "authors": "Dimitris Bertsimas, Angela King and Rahul Mazumder", "title": "Best Subset Selection via a Modern Optimization Lens", "comments": "This is a revised version (May, 2015) of the first submission in June\n  2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last twenty-five years (1990-2014), algorithmic advances in integer\noptimization combined with hardware improvements have resulted in an\nastonishing 200 billion factor speedup in solving Mixed Integer Optimization\n(MIO) problems. We present a MIO approach for solving the classical best subset\nselection problem of choosing $k$ out of $p$ features in linear regression\ngiven $n$ observations. We develop a discrete extension of modern first order\ncontinuous optimization methods to find high quality feasible solutions that we\nuse as warm starts to a MIO solver that finds provably optimal solutions. The\nresulting algorithm (a) provides a solution with a guarantee on its\nsuboptimality even if we terminate the algorithm early, (b) can accommodate\nside constraints on the coefficients of the linear regression and (c) extends\nto finding best subset solutions for the least absolute deviation loss\nfunction. Using a wide variety of synthetic and real datasets, we demonstrate\nthat our approach solves problems with $n$ in the 1000s and $p$ in the 100s in\nminutes to provable optimality, and finds near optimal solutions for $n$ in the\n100s and $p$ in the 1000s in minutes. We also establish via numerical\nexperiments that the MIO approach performs better than {\\texttt {Lasso}} and\nother popularly used sparse learning procedures, in terms of achieving sparse\nsolutions with good predictive power.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2015 18:19:27 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Bertsimas", "Dimitris", ""], ["King", "Angela", ""], ["Mazumder", "Rahul", ""]]}, {"id": "1507.03176", "submitter": "Junyu Xuan", "authors": "Junyu Xuan and Jie Lu and Guangquan Zhang and Richard Yi Da Xu and\n  Xiangfeng Luo", "title": "Dependent Indian Buffet Process-based Sparse Nonparametric Nonnegative\n  Matrix Factorization", "comments": "14 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative Matrix Factorization (NMF) aims to factorize a matrix into two\noptimized nonnegative matrices appropriate for the intended applications. The\nmethod has been widely used for unsupervised learning tasks, including\nrecommender systems (rating matrix of users by items) and document clustering\n(weighting matrix of papers by keywords). However, traditional NMF methods\ntypically assume the number of latent factors (i.e., dimensionality of the\nloading matrices) to be fixed. This assumption makes them inflexible for many\napplications. In this paper, we propose a nonparametric NMF framework to\nmitigate this issue by using dependent Indian Buffet Processes (dIBP). In a\nnutshell, we apply a correlation function for the generation of two stick\nweights associated with each pair of columns of loading matrices, while still\nmaintaining their respective marginal distribution specified by IBP. As a\nconsequence, the generation of two loading matrices will be column-wise\n(indirectly) correlated. Under this same framework, two classes of correlation\nfunction are proposed (1) using Bivariate beta distribution and (2) using\nCopula function. Both methods allow us to adopt our work for various\napplications by flexibly choosing an appropriate parameter settings. Compared\nwith the other state-of-the art approaches in this area, such as using Gaussian\nProcess (GP)-based dIBP, our work is seen to be much more flexible in terms of\nallowing the two corresponding binary matrix columns to have greater variations\nin their non-zero entries. Our experiments on the real-world and synthetic\ndatasets show that three proposed models perform well on the document\nclustering task comparing standard NMF without predefining the dimension for\nthe factor matrices, and the Bivariate beta distribution-based and Copula-based\nmodels have better flexibility than the GP-based model.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2015 01:41:12 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Xuan", "Junyu", ""], ["Lu", "Jie", ""], ["Zhang", "Guangquan", ""], ["Da Xu", "Richard Yi", ""], ["Luo", "Xiangfeng", ""]]}, {"id": "1507.03194", "submitter": "Ali Caner T\\\"urkmen", "authors": "Ali Caner T\\\"urkmen", "title": "A Review of Nonnegative Matrix Factorization Methods for Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative Matrix Factorization (NMF) was first introduced as a low-rank\nmatrix approximation technique, and has enjoyed a wide area of applications.\nAlthough NMF does not seem related to the clustering problem at first, it was\nshown that they are closely linked. In this report, we provide a gentle\nintroduction to clustering and NMF before reviewing the theoretical\nrelationship between them. We then explore several NMF variants, namely Sparse\nNMF, Projective NMF, Nonnegative Spectral Clustering and Cluster-NMF, along\nwith their clustering interpretations.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2015 07:14:16 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2015 13:43:28 GMT"}], "update_date": "2015-08-31", "authors_parsed": [["T\u00fcrkmen", "Ali Caner", ""]]}, {"id": "1507.03228", "submitter": "Scott Linderman", "authors": "Scott W. Linderman and Ryan P. Adams", "title": "Scalable Bayesian Inference for Excitatory Point Process Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks capture our intuition about relationships in the world. They\ndescribe the friendships between Facebook users, interactions in financial\nmarkets, and synapses connecting neurons in the brain. These networks are\nrichly structured with cliques of friends, sectors of stocks, and a smorgasbord\nof cell types that govern how neurons connect. Some networks, like social\nnetwork friendships, can be directly observed, but in many cases we only have\nan indirect view of the network through the actions of its constituents and an\nunderstanding of how the network mediates that activity. In this work, we focus\non the problem of latent network discovery in the case where the observable\nactivity takes the form of a mutually-excitatory point process known as a\nHawkes process. We build on previous work that has taken a Bayesian approach to\nthis problem, specifying prior distributions over the latent network structure\nand a likelihood of observed activity given this network. We extend this work\nby proposing a discrete-time formulation and developing a computationally\nefficient stochastic variational inference (SVI) algorithm that allows us to\nscale the approach to long sequences of observations. We demonstrate our\nalgorithm on the calcium imaging data used in the Chalearn neural connectomics\nchallenge.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2015 12:59:28 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Linderman", "Scott W.", ""], ["Adams", "Ryan P.", ""]]}, {"id": "1507.03229", "submitter": "Ichiro Takeuchi Prof.", "authors": "Shinya Suzumura, Kohei Ogawa, Masashi Sugiyama, Masayuki Karasuyama,\n  Ichiro Takeuchi", "title": "Homotopy Continuation Approaches for Robust SV Classification and\n  Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In support vector machine (SVM) applications with unreliable data that\ncontains a portion of outliers, non-robustness of SVMs often causes\nconsiderable performance deterioration. Although many approaches for improving\nthe robustness of SVMs have been studied, two major challenges remain in robust\nSVM learning. First, robust learning algorithms are essentially formulated as\nnon-convex optimization problems. It is thus important to develop a non-convex\noptimization method for robust SVM that can find a good local optimal solution.\nThe second practical issue is how one can tune the hyperparameter that controls\nthe balance between robustness and efficiency. Unfortunately, due to the\nnon-convexity, robust SVM solutions with slightly different hyper-parameter\nvalues can be significantly different, which makes model selection highly\nunstable. In this paper, we address these two issues simultaneously by\nintroducing a novel homotopy approach to non-convex robust SVM learning. Our\nbasic idea is to introduce parametrized formulations of robust SVM which bridge\nthe standard SVM and fully robust SVM via the parameter that represents the\ninfluence of outliers. We characterize the necessary and sufficient conditions\nof the local optimal solutions of robust SVM, and develop an algorithm that can\ntrace a path of local optimal solutions when the influence of outliers is\ngradually decreased. An advantage of our homotopy approach is that it can be\ninterpreted as simulated annealing, a common approach for finding a good local\noptimal solution in non-convex optimization problems. In addition, our homotopy\nmethod allows stable and efficient model selection based on the path of local\noptimal solutions. Empirical performances of the proposed approach are\ndemonstrated through intensive numerical experiments both on robust\nclassification and regression problems.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2015 13:07:26 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Suzumura", "Shinya", ""], ["Ogawa", "Kohei", ""], ["Sugiyama", "Masashi", ""], ["Karasuyama", "Masayuki", ""], ["Takeuchi", "Ichiro", ""]]}, {"id": "1507.03269", "submitter": "David Steurer", "authors": "Samuel B. Hopkins and Jonathan Shi and David Steurer", "title": "Tensor principal component analysis via sum-of-squares proofs", "comments": "published in Conference on Learning Theory (COLT) 2015 (submitted\n  February 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a statistical model for the tensor principal component analysis\nproblem introduced by Montanari and Richard: Given a order-$3$ tensor $T$ of\nthe form $T = \\tau \\cdot v_0^{\\otimes 3} + A$, where $\\tau \\geq 0$ is a\nsignal-to-noise ratio, $v_0$ is a unit vector, and $A$ is a random noise\ntensor, the goal is to recover the planted vector $v_0$. For the case that $A$\nhas iid standard Gaussian entries, we give an efficient algorithm to recover\n$v_0$ whenever $\\tau \\geq \\omega(n^{3/4} \\log(n)^{1/4})$, and certify that the\nrecovered vector is close to a maximum likelihood estimator, all with high\nprobability over the random choice of $A$. The previous best algorithms with\nprovable guarantees required $\\tau \\geq \\Omega(n)$.\n  In the regime $\\tau \\leq o(n)$, natural tensor-unfolding-based spectral\nrelaxations for the underlying optimization problem break down (in the sense\nthat their integrality gap is large). To go beyond this barrier, we use convex\nrelaxations based on the sum-of-squares method. Our recovery algorithm proceeds\nby rounding a degree-$4$ sum-of-squares relaxations of the\nmaximum-likelihood-estimation problem for the statistical model. To complement\nour algorithmic results, we show that degree-$4$ sum-of-squares relaxations\nbreak down for $\\tau \\leq O(n^{3/4}/\\log(n)^{1/4})$, which demonstrates that\nimproving our current guarantees (by more than logarithmic factors) would\nrequire new techniques or might even be intractable.\n  Finally, we show how to exploit additional problem structure in order to\nsolve our sum-of-squares relaxations, up to some approximation, very\nefficiently. Our fastest algorithm runs in nearly-linear time using shifted\n(matrix) power iteration and has similar guarantees as above. The analysis of\nthis algorithm also confirms a variant of a conjecture of Montanari and Richard\nabout singular vectors of tensor unfoldings.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2015 20:30:09 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Hopkins", "Samuel B.", ""], ["Shi", "Jonathan", ""], ["Steurer", "David", ""]]}, {"id": "1507.03285", "submitter": "Michael Kane", "authors": "Michael J. Kane, Bryan Lewis, Sekhar Tatikonda, Simon Urbanek", "title": "Scatter Matrix Concordance: A Diagnostic for Regressions on Subsets of\n  Data", "comments": null, "journal-ref": null, "doi": "10.1002/sam.11283", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear regression models depend directly on the design matrix and its\nproperties. Techniques that efficiently estimate model coefficients by\npartitioning rows of the design matrix are increasingly popular for large-scale\nproblems because they fit well with modern parallel computing architectures. We\npropose a simple measure of {\\em concordance} between a design matrix and a\nsubset of its rows that estimates how well a subset captures the\nvariance-covariance structure of a larger data set. We illustrate the use of\nthis measure in a heuristic method for selecting row partition sizes that\nbalance statistical and computational efficiency goals in real-world problems.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2015 22:51:07 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Kane", "Michael J.", ""], ["Lewis", "Bryan", ""], ["Tatikonda", "Sekhar", ""], ["Urbanek", "Simon", ""]]}, {"id": "1507.03482", "submitter": "Francisco Hernando-Gallego", "authors": "Francisco Hernando-Gallego and Antonio Art\\'es-Rodr\\'iguez", "title": "Individual performance calibration using physiological stress signals", "comments": "5 pages, 12 figures, Workshop of Shimmer sensors, IEEE Body Sensor\n  Networks Conference 2015 MIT Lab Boston (USA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The relation between performance and stress is described by the Yerkes-Dodson\nLaw but varies significantly between individuals. This paper describes a method\nfor determining the individual optimal performance as a function of\nphysiological signals. The method is based on attention and reasoning tests of\nincreasing complexity under monitoring of three physiological signals: Galvanic\nSkin Response (GSR), Heart Rate (HR), and Electromyogram (EMG). Based on the\ntest results with 15 different individuals, we first show that two of the\nsignals, GSR and HR, have enough discriminative power to distinguish between\nrelax and stress periods. We then show a positive correlation between the\ncomplexity level of the tests and the GSR and HR signals, and we finally\ndetermine the optimal performance point as the signal level just before a\nperformance decrease. We also discuss the differences among signals depending\non the type of test.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2015 14:52:02 GMT"}], "update_date": "2015-07-15", "authors_parsed": [["Hernando-Gallego", "Francisco", ""], ["Art\u00e9s-Rodr\u00edguez", "Antonio", ""]]}, {"id": "1507.03496", "submitter": "Jos\\'e Luis Torrecilla", "authors": "Jos\\'e R. Berrendero, Antonio Cuevas and Jos\\'e L. Torrecilla", "title": "The mRMR variable selection method: a comparative study for functional\n  data", "comments": null, "journal-ref": "Journal of Statistical Computation and Simulation Volume 86, Issue\n  5, 2016", "doi": "10.1080/00949655.2015.1042378", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of variable selection methods is particularly appealing in\nstatistical problems with functional data. The obvious general criterion for\nvariable selection is to choose the `most representative' or `most relevant'\nvariables. However, it is also clear that a purely relevance-oriented criterion\ncould lead to select many redundant variables. The mRMR (minimum Redundance\nMaximum Relevance) procedure, proposed by Ding and Peng (2005) and Peng et al.\n(2005) is an algorithm to systematically perform variable selection, achieving\na reasonable trade-off between relevance and redundancy. In its original form,\nthis procedure is based on the use of the so-called mutual information\ncriterion to assess relevance and redundancy. Keeping the focus on functional\ndata problems, we propose here a modified version of the mRMR method, obtained\nby replacing the mutual information by the new association measure (called\ndistance correlation) suggested by Sz\\'ekely et al. (2007). We have also\nperformed an extensive simulation study, including 1600 functional experiments\n(100 functional models $\\times$ 4 sample sizes $\\times$ 4 classifiers) and\nthree real-data examples aimed at comparing the different versions of the mRMR\nmethodology. The results are quite conclusive in favor of the new proposed\nalternative.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2015 15:31:26 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Berrendero", "Jos\u00e9 R.", ""], ["Cuevas", "Antonio", ""], ["Torrecilla", "Jos\u00e9 L.", ""]]}, {"id": "1507.03538", "submitter": "Giri Gopalan", "authors": "Giri Gopalan, Saeqa Dil Vrtilek, and Luke Bornn", "title": "Classifying X-ray Binaries: A Probabilistic Approach", "comments": "Editing of figure captions and correction of y-axis labels for bar\n  charts in figures 6 and 7", "journal-ref": "2015 ApJ 809 40", "doi": "10.1088/0004-637X/809/1/40", "report-no": null, "categories": "astro-ph.HE stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In X-ray binary star systems consisting of a compact object that accretes\nmaterial from an orbiting secondary star, there is no straightforward means to\ndecide if the compact object is a black hole or a neutron star. To assist this\nclassification, we develop a Bayesian statistical model that makes use of the\nfact that X-ray binary systems appear to cluster based on their compact object\ntype when viewed from a 3-dimensional coordinate system derived from X-ray\nspectral data. The first coordinate of this data is the ratio of counts in mid\nto low energy band (color 1), the second coordinate is the ratio of counts in\nhigh to low energy band (color 2), and the third coordinate is the sum of\ncounts in all three bands. We use this model to estimate the probabilities that\nan X-ray binary system contains a black hole, non-pulsing neutron star, or\npulsing neutron star. In particular, we utilize a latent variable model in\nwhich the latent variables follow a Gaussian process prior distribution, and\nhence we are able to induce the spatial correlation we believe exists between\nsystems of the same type. The utility of this approach is evidenced by the\naccurate prediction of system types using Rossi X-ray Timing Explorer All Sky\nMonitor data, but it is not flawless. In particular, non-pulsing neutron\nsystems containing \"bursters\" that are close to the boundary demarcating\nsystems containing black holes tend to be classified as black hole systems. As\na byproduct of our analyses, we provide the astronomer with public R code that\ncan be used to predict the compact object type of X-ray binaries given training\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2015 18:24:16 GMT"}, {"version": "v2", "created": "Thu, 28 Dec 2017 07:01:28 GMT"}, {"version": "v3", "created": "Fri, 17 Aug 2018 13:12:02 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Gopalan", "Giri", ""], ["Vrtilek", "Saeqa Dil", ""], ["Bornn", "Luke", ""]]}, {"id": "1507.03734", "submitter": "Quoc Tran-Dinh", "authors": "Quoc Tran-Dinh and Volkan Cevher", "title": "Smooth Alternating Direction Methods for Nonsmooth Constrained Convex\n  Optimization", "comments": "35 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": "UNC-STOR 2016", "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two new alternating direction methods to solve \"fully\" nonsmooth\nconstrained convex problems. Our algorithms have the best known worst-case\niteration-complexity guarantee under mild assumptions for both the objective\nresidual and feasibility gap. Through theoretical analysis, we show how to\nupdate all the algorithmic parameters automatically with clear impact on the\nconvergence performance. We also provide a representative numerical example\nshowing the advantages of our methods over the classical alternating direction\nmethods using a well-known feasibility problem.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2015 06:39:05 GMT"}, {"version": "v2", "created": "Thu, 22 Dec 2016 21:39:24 GMT"}, {"version": "v3", "created": "Mon, 15 Jan 2018 16:26:46 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Tran-Dinh", "Quoc", ""], ["Cevher", "Volkan", ""]]}, {"id": "1507.03857", "submitter": "Lenka Zdeborova", "authors": "Thibault Lesieur, Florent Krzakala and Lenka Zdeborov\\'a", "title": "MMSE of probabilistic low-rank matrix estimation: Universality with\n  respect to the output channel", "comments": "10 pages, Allerton Conference on Communication, Control, and\n  Computing 2015", "journal-ref": "2015 53rd Annual Allerton Conference on Communication, Control,\n  and Computing, page 680 - 687, IEEE", "doi": "10.1109/ALLERTON.2015.7447070", "report-no": null, "categories": "cs.IT cond-mat.stat-mech math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers probabilistic estimation of a low-rank matrix from\nnon-linear element-wise measurements of its elements. We derive the\ncorresponding approximate message passing (AMP) algorithm and its state\nevolution. Relying on non-rigorous but standard assumptions motivated by\nstatistical physics, we characterize the minimum mean squared error (MMSE)\nachievable information theoretically and with the AMP algorithm. Unlike in\nrelated problems of linear estimation, in the present setting the MMSE depends\non the output channel only trough a single parameter - its Fisher information.\nWe illustrate this striking finding by analysis of submatrix localization, and\nof detection of communities hidden in a dense stochastic block model. For this\nexample we locate the computational and statistical boundaries that are not\nequal for rank larger than four.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2015 14:17:42 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2016 15:44:24 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Lesieur", "Thibault", ""], ["Krzakala", "Florent", ""], ["Zdeborov\u00e1", "Lenka", ""]]}, {"id": "1507.03867", "submitter": "Rong Ge", "authors": "Rong Ge and James Zou", "title": "Rich Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many settings, we have multiple data sets (also called views) that capture\ndifferent and overlapping aspects of the same phenomenon. We are often\ninterested in finding patterns that are unique to one or to a subset of the\nviews. For example, we might have one set of molecular observations and one set\nof physiological observations on the same group of individuals, and we want to\nquantify molecular patterns that are uncorrelated with physiology. Despite\nbeing a common problem, this is highly challenging when the correlations come\nfrom complex distributions. In this paper, we develop the general framework of\nRich Component Analysis (RCA) to model settings where the observations from\ndifferent views are driven by different sets of latent components, and each\ncomponent can be a complex, high-dimensional distribution. We introduce\nalgorithms based on cumulant extraction that provably learn each of the\ncomponents without having to model the other components. We show how to\nintegrate RCA with stochastic gradient descent into a meta-algorithm for\nlearning general models, and demonstrate substantial improvement in accuracy on\nseveral synthetic and real datasets in both supervised and unsupervised tasks.\nOur method makes it possible to learn latent variable models when we don't have\nsamples from the true model but only samples after complex perturbations.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2015 14:38:23 GMT"}], "update_date": "2015-07-15", "authors_parsed": [["Ge", "Rong", ""], ["Zou", "James", ""]]}, {"id": "1507.03887", "submitter": "Muhammad Farooq", "authors": "Muhammad Farooq, Ingo Steinwart", "title": "An SVM-like Approach for Expectile Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expectile regression is a nice tool for investigating conditional\ndistributions beyond the conditional mean. It is well-known that expectiles can\nbe described with the help of the asymmetric least square loss function, and\nthis link makes it possible to estimate expectiles in a non-parametric\nframework by a support vector machine like approach. In this work we develop an\nefficient sequential-minimal-optimization-based solver for the underlying\noptimization problem. The behavior of the solver is investigated by conducting\nvarious experiments and the results are compared with the recent R-package\nER-Boost.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2015 15:24:52 GMT"}], "update_date": "2015-07-15", "authors_parsed": [["Farooq", "Muhammad", ""], ["Steinwart", "Ingo", ""]]}, {"id": "1507.04001", "submitter": "Aaron Clauset", "authors": "M. E. J. Newman and Aaron Clauset", "title": "Structure and inference in annotated networks", "comments": "16 pages, 7 figures, 1 table", "journal-ref": "Nature Communications 7, 11863 (2016)", "doi": "10.1038/ncomms11863", "report-no": null, "categories": "cs.SI physics.data-an physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many networks of scientific interest we know both the connections of the\nnetwork and information about the network nodes, such as the age or gender of\nindividuals in a social network, geographic location of nodes in the Internet,\nor cellular function of nodes in a gene regulatory network. Here we demonstrate\nhow this \"metadata\" can be used to improve our analysis and understanding of\nnetwork structure. We focus in particular on the problem of community detection\nin networks and develop a mathematically principled approach that combines a\nnetwork and its metadata to detect communities more accurately than can be done\nwith either alone. Crucially, the method does not assume that the metadata are\ncorrelated with the communities we are trying to find. Instead the method\nlearns whether a correlation exists and correctly uses or ignores the metadata\ndepending on whether they contain useful information. The learned correlations\nare also of interest in their own right, allowing us to make predictions about\nthe community membership of nodes whose network connections are unknown. We\ndemonstrate our method on synthetic networks with known structure and on\nreal-world networks, large and small, drawn from social, biological, and\ntechnological domains.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2015 20:01:46 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Newman", "M. E. J.", ""], ["Clauset", "Aaron", ""]]}, {"id": "1507.04155", "submitter": "Cem Orhan", "authors": "Cem Orhan and \\\"Oznur Ta\\c{s}tan", "title": "ALEVS: Active Learning by Statistical Leverage Sampling", "comments": "4 pages, presented as contributed talk in ICML 2015 Active Learning\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning aims to obtain a classifier of high accuracy by using fewer\nlabel requests in comparison to passive learning by selecting effective\nqueries. Many active learning methods have been developed in the past two\ndecades, which sample queries based on informativeness or representativeness of\nunlabeled data points. In this work, we explore a novel querying criterion\nbased on statistical leverage scores. The statistical leverage scores of a row\nin a matrix are the squared row-norms of the matrix containing its (top) left\nsingular vectors and is a measure of influence of the row on the matrix.\nLeverage scores have been used for detecting high influential points in\nregression diagnostics and have been recently shown to be useful for data\nanalysis and randomized low-rank matrix approximation algorithms. We explore\nhow sampling data instances with high statistical leverage scores perform in\nactive learning. Our empirical comparison on several binary classification\ndatasets indicate that querying high leverage points is an effective strategy.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 10:31:00 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Orhan", "Cem", ""], ["Ta\u015ftan", "\u00d6znur", ""]]}, {"id": "1507.04201", "submitter": "Nicos Pavlidis", "authors": "Nicos G. Pavlidis, David P. Hofmeyr, Sotiris K. Tasoulis", "title": "Minimum Density Hyperplanes", "comments": null, "journal-ref": "Journal of Machine Learning Research, 17(156): 1-33, 2016", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Associating distinct groups of objects (clusters) with contiguous regions of\nhigh probability density (high-density clusters), is central to many\nstatistical and machine learning approaches to the classification of unlabelled\ndata. We propose a novel hyperplane classifier for clustering and\nsemi-supervised classification which is motivated by this objective. The\nproposed minimum density hyperplane minimises the integral of the empirical\nprobability density function along it, thereby avoiding intersection with high\ndensity clusters. We show that the minimum density and the maximum margin\nhyperplanes are asymptotically equivalent, thus linking this approach to\nmaximum margin clustering and semi-supervised support vector classifiers. We\npropose a projection pursuit formulation of the associated optimisation problem\nwhich allows us to find minimum density hyperplanes efficiently in practice,\nand evaluate its performance on a range of benchmark datasets. The proposed\napproach is found to be very competitive with state of the art methods for\nclustering and semi-supervised classification.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 13:08:11 GMT"}, {"version": "v2", "created": "Wed, 9 Mar 2016 14:11:27 GMT"}, {"version": "v3", "created": "Wed, 28 Sep 2016 17:19:48 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Pavlidis", "Nicos G.", ""], ["Hofmeyr", "David P.", ""], ["Tasoulis", "Sotiris K.", ""]]}, {"id": "1507.04208", "submitter": "Branislav Kveton", "authors": "Branislav Kveton, Zheng Wen, Azin Ashkan, and Csaba Szepesvari", "title": "Combinatorial Cascading Bandits", "comments": "Advances in Neural Information Processing Systems 28", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose combinatorial cascading bandits, a class of partial monitoring\nproblems where at each step a learning agent chooses a tuple of ground items\nsubject to constraints and receives a reward if and only if the weights of all\nchosen items are one. The weights of the items are binary, stochastic, and\ndrawn independently of each other. The agent observes the index of the first\nchosen item whose weight is zero. This observation model arises in network\nrouting, for instance, where the learning agent may only observe the first link\nin the routing path which is down, and blocks the path. We propose a UCB-like\nalgorithm for solving our problems, CombCascade; and prove gap-dependent and\ngap-free upper bounds on its $n$-step regret. Our proofs build on recent work\nin stochastic combinatorial semi-bandits but also address two novel challenges\nof our setting, a non-linear reward function and partial observability. We\nevaluate CombCascade on two real-world problems and show that it performs well\neven when our modeling assumptions are violated. We also demonstrate that our\nsetting requires a new learning algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 13:30:46 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2015 19:34:21 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2015 20:27:44 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Kveton", "Branislav", ""], ["Wen", "Zheng", ""], ["Ashkan", "Azin", ""], ["Szepesvari", "Csaba", ""]]}, {"id": "1507.04230", "submitter": "Jiaji Huang", "authors": "Jiaji Huang and Qiang Qiu and Robert Calderbank", "title": "The Role of Principal Angles in Subspace Classification", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2015.2500889", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace models play an important role in a wide range of signal processing\ntasks, and this paper explores how the pairwise geometry of subspaces\ninfluences the probability of misclassification. When the mismatch between the\nsignal and the model is vanishingly small, the probability of misclassification\nis determined by the product of the sines of the principal angles between\nsubspaces. When the mismatch is more significant, the probability of\nmisclassification is determined by the sum of the squares of the sines of the\nprincipal angles. Reliability of classification is derived in terms of the\ndistribution of signal energy across principal vectors. Larger principal angles\nlead to smaller classification error, motivating a linear transform that\noptimizes principal angles. The transform presented here (TRAIT) preserves some\nspecific characteristic of each individual class, and this approach is shown to\nbe complementary to a previously developed transform (LRT) that enlarges\ninter-class distance while suppressing intra-class dispersion. Theoretical\nresults are supported by demonstration of superior classification accuracy on\nsynthetic and measured data even in the presence of significant model mismatch.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 14:24:24 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Huang", "Jiaji", ""], ["Qiu", "Qiang", ""], ["Calderbank", "Robert", ""]]}, {"id": "1507.04396", "submitter": "Risi Kondor", "authors": "Risi Kondor, Nedelina Teneva, Pramod K. Mudrakarta", "title": "Parallel MMF: a Multiresolution Approach to Matrix Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiresolution Matrix Factorization (MMF) was recently introduced as a\nmethod for finding multiscale structure and defining wavelets on\ngraphs/matrices. In this paper we derive pMMF, a parallel algorithm for\ncomputing the MMF factorization. Empirically, the running time of pMMF scales\nlinearly in the dimension for sparse matrices. We argue that this makes pMMF a\nvaluable new computational primitive in its own right, and present experiments\non using pMMF for two distinct purposes: compressing matrices and\npreconditioning large sparse linear systems.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 21:19:25 GMT"}], "update_date": "2015-07-17", "authors_parsed": [["Kondor", "Risi", ""], ["Teneva", "Nedelina", ""], ["Mudrakarta", "Pramod K.", ""]]}, {"id": "1507.04436", "submitter": "Xiao Fu", "authors": "Xiao Fu and Kejun Huang and Wing-Kin Ma and Nicholas D. Sidiropoulos\n  and Rasmus Bro", "title": "Joint Tensor Factorization and Outlying Slab Suppression with\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider factoring low-rank tensors in the presence of outlying slabs.\nThis problem is important in practice, because data collected in many\nreal-world applications, such as speech, fluorescence, and some social network\ndata, fit this paradigm. Prior work tackles this problem by iteratively\nselecting a fixed number of slabs and fitting, a procedure which may not\nconverge. We formulate this problem from a group-sparsity promoting point of\nview, and propose an alternating optimization framework to handle the\ncorresponding $\\ell_p$ ($0<p\\leq 1$) minimization-based low-rank tensor\nfactorization problem. The proposed algorithm features a similar per-iteration\ncomplexity as the plain trilinear alternating least squares (TALS) algorithm.\nConvergence of the proposed algorithm is also easy to analyze under the\nframework of alternating optimization and its variants. In addition,\nregularization and constraints can be easily incorporated to make use of\n\\emph{a priori} information on the latent loading factors. Simulations and real\ndata experiments on blind speech separation, fluorescence data analysis, and\nsocial network mining are used to showcase the effectiveness of the proposed\nalgorithm.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 02:47:58 GMT"}], "update_date": "2015-07-17", "authors_parsed": [["Fu", "Xiao", ""], ["Huang", "Kejun", ""], ["Ma", "Wing-Kin", ""], ["Sidiropoulos", "Nicholas D.", ""], ["Bro", "Rasmus", ""]]}, {"id": "1507.04457", "submitter": "Dohyung Park", "authors": "Dohyung Park, Joe Neeman, Jin Zhang, Sujay Sanghavi, Inderjit S.\n  Dhillon", "title": "Preference Completion: Large-scale Collaborative Ranking from Pairwise\n  Comparisons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the collaborative ranking setting: a pool of users\neach provides a small number of pairwise preferences between $d$ possible\nitems; from these we need to predict preferences of the users for items they\nhave not yet seen. We do so by fitting a rank $r$ score matrix to the pairwise\ndata, and provide two main contributions: (a) we show that an algorithm based\non convex optimization provides good generalization guarantees once each user\nprovides as few as $O(r\\log^2 d)$ pairwise comparisons -- essentially matching\nthe sample complexity required in the related matrix completion setting (which\nuses actual numerical as opposed to pairwise information), and (b) we develop a\nlarge-scale non-convex implementation, which we call AltSVM, that trains a\nfactored form of the matrix via alternating minimization (which we show reduces\nto alternating SVM problems), and scales and parallelizes very well to large\nproblem settings. It also outperforms common baselines on many moderately large\npopular collaborative filtering datasets in both NDCG and in other measures of\nranking performance.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 06:00:51 GMT"}], "update_date": "2015-07-17", "authors_parsed": [["Park", "Dohyung", ""], ["Neeman", "Joe", ""], ["Zhang", "Jin", ""], ["Sanghavi", "Sujay", ""], ["Dhillon", "Inderjit S.", ""]]}, {"id": "1507.04505", "submitter": "Ulrich Paquet", "authors": "Ulrich Paquet", "title": "On the Convergence of Stochastic Variational Inference in Bayesian\n  Networks", "comments": "NIPS 2014 Workshop on Advances in Variational Inference. Montreal,\n  Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We highlight a pitfall when applying stochastic variational inference to\ngeneral Bayesian networks. For global random variables approximated by an\nexponential family distribution, natural gradient steps, commonly starting from\na unit length step size, are averaged to convergence. This useful insight into\nthe scaling of initial step sizes is lost when the approximation factorizes\nacross a general Bayesian network, and care must be taken to ensure practical\nconvergence. We experimentally investigate how much of the baby (well-scaled\nsteps) is thrown out with the bath water (exact gradients).\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 09:37:32 GMT"}], "update_date": "2015-07-17", "authors_parsed": [["Paquet", "Ulrich", ""]]}, {"id": "1507.04513", "submitter": "Daniel Hern\\'andez-Lobato", "authors": "Daniel Hern\\'andez-Lobato and Jos\\'e Miguel Hern\\'andez-Lobato", "title": "Scalable Gaussian Process Classification via Expectation Propagation", "comments": "9 pages, some figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational methods have been recently considered for scaling the training\nprocess of Gaussian process classifiers to large datasets. As an alternative,\nwe describe here how to train these classifiers efficiently using expectation\npropagation. The proposed method allows for handling datasets with millions of\ndata instances. More precisely, it can be used for (i) training in a\ndistributed fashion where the data instances are sent to different nodes in\nwhich the required computations are carried out, and for (ii) maximizing an\nestimate of the marginal likelihood using a stochastic approximation of the\ngradient. Several experiments indicate that the method described is competitive\nwith the variational approach.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 10:11:44 GMT"}], "update_date": "2015-07-17", "authors_parsed": [["Hern\u00e1ndez-Lobato", "Daniel", ""], ["Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", ""]]}, {"id": "1507.04540", "submitter": "Tianpei Xie", "authors": "Tianpei Xie, Nasser M. Nasrabadi and Alfred O. Hero", "title": "Learning to classify with possible sensor failures", "comments": "13 pages, submitted to IEEE Transaction of Signal Processing, Feb\n  2016", "journal-ref": null, "doi": "10.1109/ICASSP.2014.6854029", "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a general framework to learn a robust large-margin\nbinary classifier when corrupt measurements, called anomalies, caused by sensor\nfailure might be present in the training set. The goal is to minimize the\ngeneralization error of the classifier on non-corrupted measurements while\ncontrolling the false alarm rate associated with anomalous samples. By\nincorporating a non-parametric regularizer based on an empirical entropy\nestimator, we propose a Geometric-Entropy-Minimization regularized Maximum\nEntropy Discrimination (GEM-MED) method to learn to classify and detect\nanomalies in a joint manner. We demonstrate using simulated data and a real\nmultimodal data set. Our GEM-MED method can yield improved performance over\nprevious robust classification methods in terms of both classification accuracy\nand anomaly detection rate.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 12:16:02 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2015 02:19:49 GMT"}, {"version": "v3", "created": "Mon, 22 Feb 2016 18:18:15 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Xie", "Tianpei", ""], ["Nasrabadi", "Nasser M.", ""], ["Hero", "Alfred O.", ""]]}, {"id": "1507.04564", "submitter": "Sandeep Juneja", "authors": "Peter Glynn and Sandeep Juneja", "title": "Selecting the best system and multi-armed bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the problem of finding a population or a probability distribution\namongst many with the largest mean when these means are unknown but population\nsamples can be simulated or otherwise generated. Typically, by selecting\nlargest sample mean population, it can be shown that false selection\nprobability decays at an exponential rate. Lately, researchers have sought\nalgorithms that guarantee that this probability is restricted to a small\n$\\delta$ in order $\\log(1/\\delta)$ computational time by estimating the\nassociated large deviations rate function via simulation. We show that such\nguarantees are misleading when populations have unbounded support even when\nthese may be light-tailed. Specifically, we show that any policy that\nidentifies the correct population with probability at least $1-\\delta$ for each\nproblem instance requires infinite number of samples in expectation in making\nsuch a determination in any problem instance. This suggests that some\nrestrictions are essential on populations to devise $O(\\log(1/\\delta))$\nalgorithms with $1 - \\delta$ correctness guarantees. We note that under\nrestriction on population moments, such methods are easily designed, and that\nsequential methods from stochastic multi-armed bandit literature can be adapted\nto devise such algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 13:33:13 GMT"}, {"version": "v2", "created": "Fri, 2 Feb 2018 14:54:56 GMT"}, {"version": "v3", "created": "Mon, 10 Sep 2018 16:39:12 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Glynn", "Peter", ""], ["Juneja", "Sandeep", ""]]}, {"id": "1507.04635", "submitter": "Jan-Willem van de Meent", "authors": "Jan-Willem van de Meent, Brooks Paige, David Tolpin, Frank Wood", "title": "Black-Box Policy Search with Probabilistic Programs", "comments": null, "journal-ref": "Proceedings of the 19th International Conference on Artificial\n  Intelligence and Statistics (2016) 1195-1204", "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we explore how probabilistic programs can be used to represent\npolicies in sequential decision problems. In this formulation, a probabilistic\nprogram is a black-box stochastic simulator for both the problem domain and the\nagent. We relate classic policy gradient techniques to recently introduced\nblack-box variational methods which generalize to probabilistic program\ninference. We present case studies in the Canadian traveler problem, Rock\nSample, and a benchmark for optimal diagnosis inspired by Guess Who. Each study\nillustrates how programs can efficiently represent policies using moderate\nnumbers of parameters.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 16:18:44 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2015 17:49:52 GMT"}, {"version": "v3", "created": "Mon, 21 Dec 2015 21:32:03 GMT"}, {"version": "v4", "created": "Thu, 4 Aug 2016 10:56:37 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["van de Meent", "Jan-Willem", ""], ["Paige", "Brooks", ""], ["Tolpin", "David", ""], ["Wood", "Frank", ""]]}, {"id": "1507.04717", "submitter": "Alessandro Rudi", "authors": "Alessandro Rudi, Raffaello Camoriano, Lorenzo Rosasco", "title": "Less is More: Nystr\\\"om Computational Regularization", "comments": "updated version of NIPS 2015 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study Nystr\\\"om type subsampling approaches to large scale kernel methods,\nand prove learning bounds in the statistical learning setting, where random\nsampling and high probability estimates are considered. In particular, we prove\nthat these approaches can achieve optimal learning bounds, provided the\nsubsampling level is suitably chosen. These results suggest a simple\nincremental variant of Nystr\\\"om Kernel Regularized Least Squares, where the\nsubsampling level implements a form of computational regularization, in the\nsense that it controls at the same time regularization and computations.\nExtensive experimental analysis shows that the considered approach achieves\nstate of the art performances on benchmark large scale datasets.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 19:26:27 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2015 15:37:29 GMT"}, {"version": "v3", "created": "Mon, 5 Oct 2015 21:34:59 GMT"}, {"version": "v4", "created": "Thu, 5 Nov 2015 15:16:59 GMT"}, {"version": "v5", "created": "Mon, 7 Mar 2016 17:34:28 GMT"}, {"version": "v6", "created": "Thu, 17 Mar 2016 16:27:36 GMT"}], "update_date": "2016-03-18", "authors_parsed": [["Rudi", "Alessandro", ""], ["Camoriano", "Raffaello", ""], ["Rosasco", "Lorenzo", ""]]}, {"id": "1507.04734", "submitter": "Amin Jalali", "authors": "Amin Jalali, Maryam Fazel, Lin Xiao", "title": "Variational Gram Functions: Convex Analysis and Optimization", "comments": "26 pages, 5 figures, additional revisions to text, under revision in\n  SIOPT, An earlier version of this work has appeared as Chapter 3 in reference\n  [21]", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new class of convex penalty functions, called \\emph{variational\nGram functions} (VGFs), that can promote pairwise relations, such as\northogonality, among a set of vectors in a vector space. These functions can\nserve as regularizers in convex optimization problems arising from hierarchical\nclassification, multitask learning, and estimating vectors with disjoint\nsupports, among other applications. We study convexity for VGFs, and give\nefficient characterizations for their convex conjugates, subdifferentials, and\nproximal operators. We discuss efficient optimization algorithms for\nregularized loss minimization problems where the loss admits a common, yet\nsimple, variational representation and the regularizer is a VGF. These\nalgorithms enjoy a simple kernel trick, an efficient line search, as well as\ncomputational advantages over first order methods based on the subdifferential\nor proximal maps. We also establish a general representer theorem for such\nlearning problems. Lastly, numerical experiments on a hierarchical\nclassification problem are presented to demonstrate the effectiveness of VGFs\nand the associated optimization algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 19:51:39 GMT"}, {"version": "v2", "created": "Wed, 6 Jul 2016 18:08:03 GMT"}, {"version": "v3", "created": "Wed, 12 Apr 2017 03:45:29 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Jalali", "Amin", ""], ["Fazel", "Maryam", ""], ["Xiao", "Lin", ""]]}, {"id": "1507.04777", "submitter": "Stephan Mandt", "authors": "Stephan Mandt, Florian Wenzel, Shinichi Nakajima, John P. Cunningham,\n  Christoph Lippert, and Marius Kloft", "title": "Sparse Probit Linear Mixed Model", "comments": "Published version, 21 pages, 6 figures", "journal-ref": "Machine Learning, 106(9), 1621-1642 (2017)", "doi": "10.1007/s10994-017-5652-6", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear Mixed Models (LMMs) are important tools in statistical genetics. When\nused for feature selection, they allow to find a sparse set of genetic traits\nthat best predict a continuous phenotype of interest, while simultaneously\ncorrecting for various confounding factors such as age, ethnicity and\npopulation structure. Formulated as models for linear regression, LMMs have\nbeen restricted to continuous phenotypes. We introduce the Sparse Probit Linear\nMixed Model (Probit-LMM), where we generalize the LMM modeling paradigm to\nbinary phenotypes. As a technical challenge, the model no longer possesses a\nclosed-form likelihood function. In this paper, we present a scalable\napproximate inference algorithm that lets us fit the model to high-dimensional\ndata sets. We show on three real-world examples from different domains that in\nthe setup of binary labels, our algorithm leads to better prediction accuracies\nand also selects features which show less correlation with the confounding\nfactors.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 21:33:48 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2016 16:01:49 GMT"}, {"version": "v3", "created": "Sat, 11 Feb 2017 23:12:43 GMT"}, {"version": "v4", "created": "Mon, 17 Jul 2017 21:10:27 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Mandt", "Stephan", ""], ["Wenzel", "Florian", ""], ["Nakajima", "Shinichi", ""], ["Cunningham", "John P.", ""], ["Lippert", "Christoph", ""], ["Kloft", "Marius", ""]]}, {"id": "1507.04886", "submitter": "Mariusz Tarnopolski", "authors": "Mariusz Tarnopolski", "title": "Distinguishing short and long $Fermi$ gamma-ray bursts", "comments": "9 pages, 6 figures; matches the before-proof version accepted for\n  publication in MNRAS", "journal-ref": "MNRAS, 454, 1132 (2015)", "doi": "10.1093/mnras/stv2061", "report-no": null, "categories": "astro-ph.HE astro-ph.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two classes of gamma-ray bursts (GRBs), short and long, have been determined\nwithout any doubts, and are usually ascribed to different progenitors, yet\nthese classes overlap for a variety of descriptive parameters. A subsample of\n46 long and 22 short $Fermi$ GRBs with estimated Hurst Exponents (HEs),\ncomplemented by minimum variability time-scales (MVTS) and durations ($T_{90}$)\nis used to perform a supervised Machine Learning (ML) and Monte Carlo (MC)\nsimulation using a Support Vector Machine (SVM) algorithm. It is found that\nwhile $T_{90}$ itself performs very well in distinguishing short and long GRBs,\nthe overall success ratio is higher when the training set is complemented by\nMVTS and HE. These results may allow to introduce a new (non-linear) parameter\nthat might provide less ambiguous classification of GRBs.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 09:13:32 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2015 00:35:21 GMT"}, {"version": "v3", "created": "Thu, 3 Sep 2015 16:34:16 GMT"}, {"version": "v4", "created": "Fri, 2 Oct 2015 12:42:34 GMT"}], "update_date": "2015-10-05", "authors_parsed": [["Tarnopolski", "Mariusz", ""]]}, {"id": "1507.04997", "submitter": "Ismael Rodr\\'iguez-Fdez M.Sc", "authors": "I. Rodr\\'iguez-Fdez, M. Mucientes, A. Bugar\\'in", "title": "FRULER: Fuzzy Rule Learning through Evolution for Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In regression problems, the use of TSK fuzzy systems is widely extended due\nto the precision of the obtained models. Moreover, the use of simple linear TSK\nmodels is a good choice in many real problems due to the easy understanding of\nthe relationship between the output and input variables. In this paper we\npresent FRULER, a new genetic fuzzy system for automatically learning accurate\nand simple linguistic TSK fuzzy rule bases for regression problems. In order to\nreduce the complexity of the learned models while keeping a high accuracy, the\nalgorithm consists of three stages: instance selection, multi-granularity fuzzy\ndiscretization of the input variables, and the evolutionary learning of the\nrule base that uses the Elastic Net regularization to obtain the consequents of\nthe rules. Each stage was validated using 28 real-world datasets and FRULER was\ncompared with three state of the art enetic fuzzy systems. Experimental results\nshow that FRULER achieves the most accurate and simple models compared even\nwith approximative approaches.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 15:26:06 GMT"}], "update_date": "2015-07-20", "authors_parsed": [["Rodr\u00edguez-Fdez", "I.", ""], ["Mucientes", "M.", ""], ["Bugar\u00edn", "A.", ""]]}, {"id": "1507.05016", "submitter": "Cedric Archambeau", "authors": "Cedric Archambeau and Beyza Ermis", "title": "Incremental Variational Inference for Latent Dirichlet Allocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce incremental variational inference and apply it to latent\nDirichlet allocation (LDA). Incremental variational inference is inspired by\nincremental EM and provides an alternative to stochastic variational inference.\nIncremental LDA can process massive document collections, does not require to\nset a learning rate, converges faster to a local optimum of the variational\nbound and enjoys the attractive property of monotonically increasing it. We\nstudy the performance of incremental LDA on large benchmark data sets. We\nfurther introduce a stochastic approximation of incremental variational\ninference which extends to the asynchronous distributed setting. The resulting\ndistributed algorithm achieves comparable performance as single host\nincremental variational inference, but with a significant speed-up.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 16:14:54 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2015 15:57:35 GMT"}], "update_date": "2015-07-23", "authors_parsed": [["Archambeau", "Cedric", ""], ["Ermis", "Beyza", ""]]}, {"id": "1507.05073", "submitter": "Michael Stephanou", "authors": "Michael Stephanou, Melvin Varughese and Iain Macdonald", "title": "Sequential Quantiles via Hermite Series Density Estimation", "comments": "43 pages, 9 figures. Improved version incorporating referee comments,\n  as appears in Electronic Journal of Statistics", "journal-ref": "Electron. J. Statist. 11 (2017), no. 1, 570--607", "doi": "10.1214/17-EJS1245", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential quantile estimation refers to incorporating observations into\nquantile estimates in an incremental fashion thus furnishing an online estimate\nof one or more quantiles at any given point in time. Sequential quantile\nestimation is also known as online quantile estimation. This area is relevant\nto the analysis of data streams and to the one-pass analysis of massive data\nsets. Applications include network traffic and latency analysis, real time\nfraud detection and high frequency trading. We introduce new techniques for\nonline quantile estimation based on Hermite series estimators in the settings\nof static quantile estimation and dynamic quantile estimation. In the static\nquantile estimation setting we apply the existing Gauss-Hermite expansion in a\nnovel manner. In particular, we exploit the fact that Gauss-Hermite\ncoefficients can be updated in a sequential manner. To treat dynamic quantile\nestimation we introduce a novel expansion with an exponentially weighted\nestimator for the Gauss-Hermite coefficients which we term the Exponentially\nWeighted Gauss-Hermite (EWGH) expansion. These algorithms go beyond existing\nsequential quantile estimation algorithms in that they allow arbitrary\nquantiles (as opposed to pre-specified quantiles) to be estimated at any point\nin time. In doing so we provide a solution to online distribution function and\nonline quantile function estimation on data streams. In particular we derive an\nanalytical expression for the CDF and prove consistency results for the CDF\nunder certain conditions. In addition we analyse the associated quantile\nestimator. Simulation studies and tests on real data reveal the Gauss-Hermite\nbased algorithms to be competitive with a leading existing algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 19:10:03 GMT"}, {"version": "v2", "created": "Sat, 4 Mar 2017 11:30:39 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Stephanou", "Michael", ""], ["Varughese", "Melvin", ""], ["Macdonald", "Iain", ""]]}, {"id": "1507.05086", "submitter": "Dimitris S. Papailiopoulos", "authors": "Xinghao Pan, Dimitris Papailiopoulos, Samet Oymak, Benjamin Recht,\n  Kannan Ramchandran, Michael I. Jordan", "title": "Parallel Correlation Clustering on Big Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a similarity graph between items, correlation clustering (CC) groups\nsimilar items together and dissimilar ones apart. One of the most popular CC\nalgorithms is KwikCluster: an algorithm that serially clusters neighborhoods of\nvertices, and obtains a 3-approximation ratio. Unfortunately, KwikCluster in\npractice requires a large number of clustering rounds, a potential bottleneck\nfor large graphs.\n  We present C4 and ClusterWild!, two algorithms for parallel correlation\nclustering that run in a polylogarithmic number of rounds and achieve nearly\nlinear speedups, provably. C4 uses concurrency control to enforce\nserializability of a parallel clustering process, and guarantees a\n3-approximation ratio. ClusterWild! is a coordination free algorithm that\nabandons consistency for the benefit of better scaling; this leads to a\nprovably small loss in the 3-approximation ratio.\n  We provide extensive experimental results for both algorithms, where we\noutperform the state of the art, both in terms of clustering accuracy and\nrunning time. We show that our algorithms can cluster billion-edge graphs in\nunder 5 seconds on 32 cores, while achieving a 15x speedup.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 19:48:32 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2015 18:36:02 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Pan", "Xinghao", ""], ["Papailiopoulos", "Dimitris", ""], ["Oymak", "Samet", ""], ["Recht", "Benjamin", ""], ["Ramchandran", "Kannan", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1507.05087", "submitter": "Ritwik Giri", "authors": "Ritwik Giri, Bhaskar D. Rao", "title": "Type I and Type II Bayesian Methods for Sparse Signal Recovery using\n  Scale Mixtures", "comments": "Under Review", "journal-ref": null, "doi": "10.1109/TSP.2016.2546231", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a generalized scale mixture family of\ndistributions, namely the Power Exponential Scale Mixture (PESM) family, to\nmodel the sparsity inducing priors currently in use for sparse signal recovery\n(SSR). We show that the successful and popular methods such as LASSO,\nReweighted $\\ell_1$ and Reweighted $\\ell_2$ methods can be formulated in an\nunified manner in a maximum a posteriori (MAP) or Type I Bayesian framework\nusing an appropriate member of the PESM family as the sparsity inducing prior.\nIn addition, exploiting the natural hierarchical framework induced by the PESM\nfamily, we utilize these priors in a Type II framework and develop the\ncorresponding EM based estimation algorithms. Some insight into the differences\nbetween Type I and Type II methods is provided and of particular interest in\nthe algorithmic development is the Type II variant of the popular and\nsuccessful reweighted $\\ell_1$ method. Extensive empirical results are provided\nand they show that the Type II methods exhibit better support recovery than the\ncorresponding Type I methods.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 19:57:38 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Giri", "Ritwik", ""], ["Rao", "Bhaskar D.", ""]]}, {"id": "1507.05117", "submitter": "Sanmitra Ghosh", "authors": "Sanmitra Ghosh, Srinandan Dasmahapatra, Koushik Maharatna", "title": "Fast Approximate Bayesian Computation for Estimating Parameters in\n  Differential Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) using a sequential Monte Carlo method\nprovides a comprehensive platform for parameter estimation, model selection and\nsensitivity analysis in differential equations. However, this method, like\nother Monte Carlo methods, incurs a significant computational cost as it\nrequires explicit numerical integration of differential equations to carry out\ninference. In this paper we propose a novel method for circumventing the\nrequirement of explicit integration by using derivatives of Gaussian processes\nto smooth the observations from which parameters are estimated. We evaluate our\nmethods using synthetic data generated from model biological systems described\nby ordinary and delay differential equations. Upon comparing the performance of\nour method to existing ABC techniques, we demonstrate that it produces\ncomparably reliable parameter estimates at a significantly reduced execution\ntime.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 21:03:53 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Ghosh", "Sanmitra", ""], ["Dasmahapatra", "Srinandan", ""], ["Maharatna", "Koushik", ""]]}, {"id": "1507.05131", "submitter": "Dong Xia", "authors": "Vladimir Koltchinskii and Dong Xia", "title": "Optimal Estimation of Low Rank Density Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The density matrices are positively semi-definite Hermitian matrices of unit\ntrace that describe the state of a quantum system. The goal of the paper is to\ndevelop minimax lower bounds on error rates of estimation of low rank density\nmatrices in trace regression models used in quantum state tomography (in\nparticular, in the case of Pauli measurements) with explicit dependence of the\nbounds on the rank and other complexity parameters. Such bounds are established\nfor several statistically relevant distances, including quantum versions of\nKullback-Leibler divergence (relative entropy distance) and of Hellinger\ndistance (so called Bures distance), and Schatten $p$-norm distances. Sharp\nupper bounds and oracle inequalities for least squares estimator with von\nNeumann entropy penalization are obtained showing that minimax lower bounds are\nattained (up to logarithmic factors) for these distances.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 23:05:40 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2015 20:11:24 GMT"}, {"version": "v3", "created": "Sun, 13 Sep 2015 15:42:28 GMT"}, {"version": "v4", "created": "Mon, 18 Apr 2016 01:02:55 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Koltchinskii", "Vladimir", ""], ["Xia", "Dong", ""]]}, {"id": "1507.05181", "submitter": "Matej Balog", "authors": "Matej Balog and Yee Whye Teh", "title": "The Mondrian Process for Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report is concerned with the Mondrian process and its applications in\nmachine learning. The Mondrian process is a guillotine-partition-valued\nstochastic process that possesses an elegant self-consistency property. The\nfirst part of the report uses simple concepts from applied probability to\ndefine the Mondrian process and explore its properties.\n  The Mondrian process has been used as the main building block of a clever\nonline random forest classification algorithm that turns out to be equivalent\nto its batch counterpart. We outline a slight adaptation of this algorithm to\nregression, as the remainder of the report uses regression as a case study of\nhow Mondrian processes can be utilized in machine learning. In particular, the\nMondrian process will be used to construct a fast approximation to the\ncomputationally expensive kernel ridge regression problem with a Laplace\nkernel.\n  The complexity of random guillotine partitions generated by a Mondrian\nprocess and hence the complexity of the resulting regression models is\ncontrolled by a lifetime hyperparameter. It turns out that these models can be\nefficiently trained and evaluated for all lifetimes in a given range at once,\nwithout needing to retrain them from scratch for each lifetime value. This\nleads to an efficient procedure for determining the right model complexity for\na dataset at hand.\n  The limitation of having a single lifetime hyperparameter will motivate the\nfinal Mondrian grid model, in which each input dimension is endowed with its\nown lifetime parameter. In this model we preserve the property that its\nhyperparameters can be tweaked without needing to retrain the modified model\nfrom scratch.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2015 12:58:11 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Balog", "Matej", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1507.05185", "submitter": "Tianbao Yang", "authors": "Tianbao Yang, Lijun Zhang, Qihang Lin, Rong Jin", "title": "Fast Sparse Least-Squares Regression with Non-Asymptotic Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CC stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a fast approximation method for {\\it large-scale\nhigh-dimensional} sparse least-squares regression problem by exploiting the\nJohnson-Lindenstrauss (JL) transforms, which embed a set of high-dimensional\nvectors into a low-dimensional space. In particular, we propose to apply the JL\ntransforms to the data matrix and the target vector and then to solve a sparse\nleast-squares problem on the compressed data with a {\\it slightly larger\nregularization parameter}. Theoretically, we establish the optimization error\nbound of the learned model for two different sparsity-inducing regularizers,\ni.e., the elastic net and the $\\ell_1$ norm. Compared with previous relevant\nwork, our analysis is {\\it non-asymptotic and exhibits more insights} on the\nbound, the sample complexity and the regularization. As an illustration, we\nalso provide an error bound of the {\\it Dantzig selector} under JL transforms.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2015 13:16:09 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Yang", "Tianbao", ""], ["Zhang", "Lijun", ""], ["Lin", "Qihang", ""], ["Jin", "Rong", ""]]}, {"id": "1507.05253", "submitter": "James McInerney", "authors": "James McInerney, Rajesh Ranganath, David M. Blei", "title": "The Population Posterior and Bayesian Inference on Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern data analysis problems involve inferences from streaming data.\nHowever, streaming data is not easily amenable to the standard probabilistic\nmodeling approaches, which assume that we condition on finite data. We develop\npopulation variational Bayes, a new approach for using Bayesian modeling to\nanalyze streams of data. It approximates a new type of distribution, the\npopulation posterior, which combines the notion of a population distribution of\nthe data with Bayesian inference in a probabilistic model. We study our method\nwith latent Dirichlet allocation and Dirichlet process mixtures on several\nlarge-scale data sets.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2015 07:19:22 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2015 20:41:38 GMT"}], "update_date": "2015-07-23", "authors_parsed": [["McInerney", "James", ""], ["Ranganath", "Rajesh", ""], ["Blei", "David M.", ""]]}, {"id": "1507.05259", "submitter": "Muhammad Bilal Zafar", "authors": "Muhammad Bilal Zafar and Isabel Valera and Manuel Gomez Rodriguez and\n  Krishna P. Gummadi", "title": "Fairness Constraints: Mechanisms for Fair Classification", "comments": "To appear in Proceedings of the 20th International Conference on\n  Artificial Intelligence and Statistics (AISTATS). Open-source code\n  implementation of our scheme is available at:\n  https://github.com/mbilalzafar/fair-classification", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithmic decision making systems are ubiquitous across a wide variety of\nonline as well as offline services. These systems rely on complex learning\nmethods and vast amounts of data to optimize the service functionality,\nsatisfaction of the end user and profitability. However, there is a growing\nconcern that these automated decisions can lead, even in the absence of intent,\nto a lack of fairness, i.e., their outcomes can disproportionately hurt (or,\nbenefit) particular groups of people sharing one or more sensitive attributes\n(e.g., race, sex). In this paper, we introduce a flexible mechanism to design\nfair classifiers by leveraging a novel intuitive measure of decision boundary\n(un)fairness. We instantiate this mechanism with two well-known classifiers,\nlogistic regression and support vector machines, and show on real-world data\nthat our mechanism allows for a fine-grained control on the degree of fairness,\noften at a small cost in terms of accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2015 07:34:25 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2015 16:20:40 GMT"}, {"version": "v3", "created": "Fri, 18 Mar 2016 16:56:26 GMT"}, {"version": "v4", "created": "Mon, 23 May 2016 16:21:12 GMT"}, {"version": "v5", "created": "Thu, 23 Mar 2017 18:10:34 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["Zafar", "Muhammad Bilal", ""], ["Valera", "Isabel", ""], ["Rodriguez", "Manuel Gomez", ""], ["Gummadi", "Krishna P.", ""]]}, {"id": "1507.05331", "submitter": "Justin Bayer", "authors": "Justin Bayer and Maximilian Karl and Daniela Korhammer and Patrick van\n  der Smagt", "title": "Fast Adaptive Weight Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Marginalising out uncertain quantities within the internal representations or\nparameters of neural networks is of central importance for a wide range of\nlearning techniques, such as empirical, variational or full Bayesian methods.\nWe set out to generalise fast dropout (Wang & Manning, 2013) to cover a wider\nvariety of noise processes in neural networks. This leads to an efficient\ncalculation of the marginal likelihood and predictive distribution which evades\nsampling and the consequential increase in training time due to highly variant\ngradient estimates. This allows us to approximate variational Bayes for the\nparameters of feed-forward neural networks. Inspired by the minimum description\nlength principle, we also propose and experimentally verify the direct\noptimisation of the regularised predictive distribution. The methods yield\nresults competitive with previous neural network based approaches and Gaussian\nprocesses on a wide range of regression tasks.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2015 20:30:10 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Bayer", "Justin", ""], ["Karl", "Maximilian", ""], ["Korhammer", "Daniela", ""], ["van der Smagt", "Patrick", ""]]}, {"id": "1507.05333", "submitter": "Mateo Rojas-Carulla Mr", "authors": "Mateo Rojas-Carulla, Bernhard Sch\\\"olkopf, Richard Turner, Jonas\n  Peters", "title": "Invariant Models for Causal Transfer Learning", "comments": null, "journal-ref": "Journal of Machine Learning Research. 19 (2018)", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods of transfer learning try to combine knowledge from several related\ntasks (or domains) to improve performance on a test task. Inspired by causal\nmethodology, we relax the usual covariate shift assumption and assume that it\nholds true for a subset of predictor variables: the conditional distribution of\nthe target variable given this subset of predictors is invariant over all\ntasks. We show how this assumption can be motivated from ideas in the field of\ncausality. We focus on the problem of Domain Generalization, in which no\nexamples from the test task are observed. We prove that in an adversarial\nsetting using this subset for prediction is optimal in Domain Generalization;\nwe further provide examples, in which the tasks are sufficiently diverse and\nthe estimator therefore outperforms pooling the data, even on average. If\nexamples from the test task are available, we also provide a method to transfer\nknowledge from the training tasks and exploit all available features for\nprediction. However, we provide no guarantees for this method. We introduce a\npractical method which allows for automatic inference of the above subset and\nprovide corresponding code. We present results on synthetic data sets and a\ngene deletion data set.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2015 20:36:10 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2016 11:51:45 GMT"}, {"version": "v3", "created": "Wed, 3 Aug 2016 16:19:29 GMT"}, {"version": "v4", "created": "Mon, 24 Sep 2018 14:18:52 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Rojas-Carulla", "Mateo", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Turner", "Richard", ""], ["Peters", "Jonas", ""]]}, {"id": "1507.05367", "submitter": "Anastasios Kyrillidis", "authors": "Anastasios Kyrillidis, Luca Baldassarre, Marwa El-Halabi, Quoc\n  Tran-Dinh and Volkan Cevher", "title": "Structured Sparsity: Discrete and Convex approaches", "comments": "30 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressive sensing (CS) exploits sparsity to recover sparse or compressible\nsignals from dimensionality reducing, non-adaptive sensing mechanisms. Sparsity\nis also used to enhance interpretability in machine learning and statistics\napplications: While the ambient dimension is vast in modern data analysis\nproblems, the relevant information therein typically resides in a much lower\ndimensional space. However, many solutions proposed nowadays do not leverage\nthe true underlying structure. Recent results in CS extend the simple sparsity\nidea to more sophisticated {\\em structured} sparsity models, which describe the\ninterdependency between the nonzero components of a signal, allowing to\nincrease the interpretability of the results and lead to better recovery\nperformance. In order to better understand the impact of structured sparsity,\nin this chapter we analyze the connections between the discrete models and\ntheir convex relaxations, highlighting their relative advantages. We start with\nthe general group sparse model and then elaborate on two important special\ncases: the dispersive and the hierarchical models. For each, we present the\nmodels in their discrete nature, discuss how to solve the ensuing discrete\nproblems and then describe convex relaxations. We also consider more general\nstructures as defined by set functions and present their convex proxies.\nFurther, we discuss efficient optimization solutions for structured sparsity\nproblems and illustrate structured sparsity in action via three applications.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2015 02:19:27 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Kyrillidis", "Anastasios", ""], ["Baldassarre", "Luca", ""], ["El-Halabi", "Marwa", ""], ["Tran-Dinh", "Quoc", ""], ["Cevher", "Volkan", ""]]}, {"id": "1507.05370", "submitter": "Anastasios Kyrillidis", "authors": "Volkan Cevher, Sina Jafarpour, Anastasios Kyrillidis", "title": "Linear Inverse Problems with Norm and Sparsity Constraints", "comments": "21 pages, authors in alphabetical order", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe two nonconventional algorithms for linear regression, called GAME\nand CLASH. The salient characteristics of these approaches is that they exploit\nthe convex $\\ell_1$-ball and non-convex $\\ell_0$-sparsity constraints jointly\nin sparse recovery. To establish the theoretical approximation guarantees of\nGAME and CLASH, we cover an interesting range of topics from game theory,\nconvex and combinatorial optimization. We illustrate that these approaches lead\nto improved theoretical guarantees and empirical performance beyond convex and\nnon-convex solvers alone.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2015 02:30:00 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Cevher", "Volkan", ""], ["Jafarpour", "Sina", ""], ["Kyrillidis", "Anastasios", ""]]}, {"id": "1507.05371", "submitter": "Luis F Voloch", "authors": "Guy Bresler, Devavrat Shah, and Luis F. Voloch", "title": "Regret Guarantees for Item-Item Collaborative Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is much empirical evidence that item-item collaborative filtering works\nwell in practice. Motivated to understand this, we provide a framework to\ndesign and analyze various recommendation algorithms. The setup amounts to\nonline binary matrix completion, where at each time a random user requests a\nrecommendation and the algorithm chooses an entry to reveal in the user's row.\nThe goal is to minimize regret, or equivalently to maximize the number of +1\nentries revealed at any time. We analyze an item-item collaborative filtering\nalgorithm that can achieve fundamentally better performance compared to\nuser-user collaborative filtering. The algorithm achieves good \"cold-start\"\nperformance (appropriately defined) by quickly making good recommendations to\nnew users about whom there is little information.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2015 02:45:01 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2016 15:28:40 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Bresler", "Guy", ""], ["Shah", "Devavrat", ""], ["Voloch", "Luis F.", ""]]}, {"id": "1507.05444", "submitter": "Tom Rainforth", "authors": "Tom Rainforth and Frank Wood", "title": "Canonical Correlation Forests", "comments": "Substantial update: longer journal format version which now covers\n  regression and multiple output prediction", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce canonical correlation forests (CCFs), a new decision tree\nensemble method for classification and regression. Individual canonical\ncorrelation trees are binary decision trees with hyperplane splits based on\nlocal canonical correlation coefficients calculated during training. Unlike\naxis-aligned alternatives, the decision surfaces of CCFs are not restricted to\nthe coordinate system of the inputs features and therefore more naturally\nrepresent data with correlated inputs. CCFs naturally accommodate multiple\noutputs, provide a similar computational complexity to random forests, and\ninherit their impressive robustness to the choice of input parameters. As part\nof the CCF training algorithm, we also introduce projection bootstrapping, a\nnovel alternative to bagging for oblique decision tree ensembles which\nmaintains use of the full dataset in selecting split points, often leading to\nimprovements in predictive accuracy. Our experiments show that, even without\nparameter tuning, CCFs out-perform axis-aligned random forests and other\nstate-of-the-art tree ensemble methods on both classification and regression\nproblems, delivering both improved predictive accuracy and faster training\ntimes. We further show that they outperform all of the 179 classifiers\nconsidered in a recent extensive survey.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2015 10:51:02 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2015 09:59:49 GMT"}, {"version": "v3", "created": "Mon, 27 Jul 2015 15:17:25 GMT"}, {"version": "v4", "created": "Thu, 13 Aug 2015 10:54:55 GMT"}, {"version": "v5", "created": "Sat, 5 Dec 2015 18:48:19 GMT"}, {"version": "v6", "created": "Wed, 9 Aug 2017 16:55:56 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Rainforth", "Tom", ""], ["Wood", "Frank", ""]]}, {"id": "1507.05498", "submitter": "Alexander Jung", "authors": "Alexander Jung, Yonina C. Eldar, Norbert G\\\"ortz", "title": "On the Minimax Risk of Dictionary Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning a dictionary matrix from a number of\nobserved signals, which are assumed to be generated via a linear model with a\ncommon underlying dictionary. In particular, we derive lower bounds on the\nminimum achievable worst case mean squared error (MSE), regardless of\ncomputational complexity of the dictionary learning (DL) schemes. By casting DL\nas a classical (or frequentist) estimation problem, the lower bounds on the\nworst case MSE are derived by following an established information-theoretic\napproach to minimax estimation. The main conceptual contribution of this paper\nis the adaption of the information-theoretic approach to minimax estimation for\nthe DL problem in order to derive lower bounds on the worst case MSE of any DL\nscheme. We derive three different lower bounds applying to different generative\nmodels for the observed signals. The first bound applies to a wide range of\nmodels, it only requires the existence of a covariance matrix of the (unknown)\nunderlying coefficient vector. By specializing this bound to the case of sparse\ncoefficient distributions, and assuming the true dictionary satisfies the\nrestricted isometry property, we obtain a lower bound on the worst case MSE of\nDL schemes in terms of a signal to noise ratio (SNR). The third bound applies\nto a more restrictive subclass of coefficient distributions by requiring the\nnon-zero coefficients to be Gaussian. While, compared with the previous two\nbounds, the applicability of this final bound is the most limited it is the\ntightest of the three bounds in the low SNR regime.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2015 13:58:49 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Jung", "Alexander", ""], ["Eldar", "Yonina C.", ""], ["G\u00f6rtz", "Norbert", ""]]}, {"id": "1507.05605", "submitter": "Alexander Wein", "authors": "Amelia Perry and Alexander S. Wein", "title": "A semidefinite program for unbalanced multisection in the stochastic\n  block model", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a semidefinite programming (SDP) algorithm for community detection\nin the stochastic block model, a popular model for networks with latent\ncommunity structure. We prove that our algorithm achieves exact recovery of the\nlatent communities, up to the information-theoretic limits determined by Abbe\nand Sandon (2015). Our result extends prior SDP approaches by allowing for many\ncommunities of different sizes. By virtue of a semidefinite approach, our\nalgorithms succeed against a semirandom variant of the stochastic block model,\nguaranteeing a form of robustness and generalization. We further explore how\nsemirandom models can lend insight into both the strengths and limitations of\nSDPs in this setting.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2015 19:58:52 GMT"}, {"version": "v2", "created": "Fri, 2 Dec 2016 17:59:55 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Perry", "Amelia", ""], ["Wein", "Alexander S.", ""]]}, {"id": "1507.05720", "submitter": "David Budden", "authors": "David M. Budden and Edmund J. Crampin", "title": "Gene expression modelling across multiple cell-lines with MapReduce", "comments": "10 pages, 3 figures", "journal-ref": "BMC Bioinformatics 2016 17:446", "doi": "10.1186/s12859-016-1313-1", "report-no": null, "categories": "q-bio.QM cs.DC q-bio.GN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the wealth of high-throughput sequencing data generated by recent\nlarge-scale consortia, predictive gene expression modelling has become an\nimportant tool for integrative analysis of transcriptomic and epigenetic data.\nHowever, sequencing data-sets are characteristically large, and previously\nmodelling frameworks are typically inefficient and unable to leverage\nmulti-core or distributed processing architectures. In this study, we detail an\nefficient and parallelised MapReduce implementation of gene expression\nmodelling. We leverage the computational efficiency of this framework to\nprovide an integrative analysis of over fifty histone modification data-sets\nacross a variety of cancerous and non-cancerous cell-lines. Our results\ndemonstrate that the genome-wide relationships between histone modifications\nand mRNA transcription are lineage, tissue and karyotype-invariant, and that\nmodels trained on matched epigenetic/transcriptomic data from non-cancerous\ncell-lines are able to predict cancerous expression with equivalent genome-wide\nfidelity.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 06:37:35 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Budden", "David M.", ""], ["Crampin", "Edmund J.", ""]]}, {"id": "1507.05781", "submitter": "Ingmar Schuster", "authors": "Ingmar Schuster", "title": "Gradient Importance Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive Monte Carlo schemes developed over the last years usually seek to\nensure ergodicity of the sampling process in line with MCMC tradition. This\nposes constraints on what is possible in terms of adaptation. In the general\ncase ergodicity can only be guaranteed if adaptation is diminished at a certain\nrate. Importance Sampling approaches offer a way to circumvent this limitation\nand design sampling algorithms that keep adapting. Here I present a gradient\ninformed variant of SMC (and its special case Population Monte Carlo) for\nstatic problems.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 10:51:49 GMT"}], "update_date": "2015-07-22", "authors_parsed": [["Schuster", "Ingmar", ""]]}, {"id": "1507.05869", "submitter": "Ali Faisal", "authors": "Ali Faisal, Anni Nora, Jaeho Seol, Hanna Renvall, Riitta Salmelin", "title": "Kernel convolution model for decoding sounds from time-varying neural\n  responses", "comments": "4 pages, Accepted at IEEE International Workshop on Pattern\n  Recognition in Neuroimaging, Stanford, June 2015", "journal-ref": null, "doi": "10.1109/PRNI.2015.10", "report-no": null, "categories": "stat.ML q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study we present a kernel based convolution model to characterize\nneural responses to natural sounds by decoding their time-varying acoustic\nfeatures. The model allows to decode natural sounds from high-dimensional\nneural recordings, such as magnetoencephalography (MEG), that track timing and\nlocation of human cortical signalling noninvasively across multiple channels.\nWe used the MEG responses recorded from subjects listening to acoustically\ndifferent environmental sounds. By decoding the stimulus frequencies from the\nresponses, our model was able to accurately distinguish between two different\nsounds that it had never encountered before with 70% accuracy. Convolution\nmodels typically decode frequencies that appear at a certain time point in the\nsound signal by using neural responses from that time point until a certain\nfixed duration of the response. Using our model, we evaluated several fixed\ndurations (time-lags) of the neural responses and observed auditory MEG\nresponses to be most sensitive to spectral content of the sounds at time-lags\nof 250 ms to 500 ms. The proposed model should be useful for determining what\naspects of natural sounds are represented by high-dimensional neural responses\nand may reveal novel properties of neural signals.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 15:25:37 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Faisal", "Ali", ""], ["Nora", "Anni", ""], ["Seol", "Jaeho", ""], ["Renvall", "Hanna", ""], ["Salmelin", "Riitta", ""]]}, {"id": "1507.05870", "submitter": "Siheng Chen", "authors": "Siheng Chen and Rohan Varma and Aarti Singh and Jelena Kova\\v{c}evi\\'c", "title": "A statistical perspective of sampling scores for linear regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a statistical problem of learning a linear model\nfrom noisy samples. Existing work has focused on approximating the least\nsquares solution by using leverage-based scores as an importance sampling\ndistribution. However, no finite sample statistical guarantees and no\ncomputationally efficient optimal sampling strategies have been proposed. To\nevaluate the statistical properties of different sampling strategies, we\npropose a simple yet effective estimator, which is easy for theoretical\nanalysis and is useful in multitask linear regression. We derive the exact mean\nsquare error of the proposed estimator for any given sampling scores. Based on\nminimizing the mean square error, we propose the optimal sampling scores for\nboth estimator and predictor, and show that they are influenced by the\nnoise-to-signal ratio. Numerical simulations match the theoretical analysis\nwell.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 15:25:49 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2016 04:24:27 GMT"}], "update_date": "2016-02-11", "authors_parsed": [["Chen", "Siheng", ""], ["Varma", "Rohan", ""], ["Singh", "Aarti", ""], ["Kova\u010devi\u0107", "Jelena", ""]]}, {"id": "1507.05899", "submitter": "Nicolas Goix", "authors": "Nicolas Goix (LTCI), Anne Sabourin (LTCI), St\\'ephan Cl\\'emen\\c{c}on\n  (LTCI)", "title": "Sparsity in Multivariate Extremes with Applications to Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capturing the dependence structure of multivariate extreme events is a major\nconcern in many fields involving the management of risks stemming from multiple\nsources, e.g. portfolio monitoring, insurance, environmental risk management\nand anomaly detection. One convenient (non-parametric) characterization of\nextremal dependence in the framework of multivariate Extreme Value Theory (EVT)\nis the angular measure, which provides direct information about the probable\n'directions' of extremes, that is, the relative contribution of each\nfeature/coordinate of the 'largest' observations. Modeling the angular measure\nin high dimensional problems is a major challenge for the multivariate analysis\nof rare events. The present paper proposes a novel methodology aiming at\nexhibiting a sparsity pattern within the dependence structure of extremes. This\nis done by estimating the amount of mass spread by the angular measure on\nrepresentative sets of directions, corresponding to specific sub-cones of\n$R^d\\_+$. This dimension reduction technique paves the way towards scaling up\nexisting multivariate EVT methods. Beyond a non-asymptotic study providing a\ntheoretical validity framework for our method, we propose as a direct\napplication a --first-- anomaly detection algorithm based on multivariate EVT.\nThis algorithm builds a sparse 'normal profile' of extreme behaviours, to be\nconfronted with new (possibly abnormal) extreme observations. Illustrative\nexperimental results provide strong empirical evidence of the relevance of our\napproach.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 16:27:08 GMT"}, {"version": "v2", "created": "Mon, 14 Mar 2016 19:35:44 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Goix", "Nicolas", "", "LTCI"], ["Sabourin", "Anne", "", "LTCI"], ["Cl\u00e9men\u00e7on", "St\u00e9phan", "", "LTCI"]]}, {"id": "1507.05910", "submitter": "Sarath Chandar", "authors": "Alex Auvolat, Sarath Chandar, Pascal Vincent, Hugo Larochelle, Yoshua\n  Bengio", "title": "Clustering is Efficient for Approximate Maximum Inner Product Search", "comments": "10 pages, Under review at ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient Maximum Inner Product Search (MIPS) is an important task that has a\nwide applicability in recommendation systems and classification with a large\nnumber of classes. Solutions based on locality-sensitive hashing (LSH) as well\nas tree-based solutions have been investigated in the recent literature, to\nperform approximate MIPS in sublinear time. In this paper, we compare these to\nanother extremely simple approach for solving approximate MIPS, based on\nvariants of the k-means clustering algorithm. Specifically, we propose to train\na spherical k-means, after having reduced the MIPS problem to a Maximum Cosine\nSimilarity Search (MCSS). Experiments on two standard recommendation system\nbenchmarks as well as on large vocabulary word embeddings, show that this\nsimple approach yields much higher speedups, for the same retrieval precision,\nthan current state-of-the-art hashing-based and tree-based methods. This simple\nmethod also yields more robust retrievals when the query is corrupted by noise.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 16:53:12 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2015 16:36:09 GMT"}, {"version": "v3", "created": "Mon, 30 Nov 2015 02:26:44 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Auvolat", "Alex", ""], ["Chandar", "Sarath", ""], ["Vincent", "Pascal", ""], ["Larochelle", "Hugo", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1507.05950", "submitter": "Dimitris S. Papailiopoulos", "authors": "Siu On Chan, Dimitris Papailiopoulos, Aviad Rubinstein", "title": "On the Worst-Case Approximability of Sparse PCA", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that Sparse PCA (Sparse Principal Component Analysis) is\nNP-hard to solve exactly on worst-case instances. What is the complexity of\nsolving Sparse PCA approximately? Our contributions include: 1) a simple and\nefficient algorithm that achieves an $n^{-1/3}$-approximation; 2) NP-hardness\nof approximation to within $(1-\\varepsilon)$, for some small constant\n$\\varepsilon > 0$; 3) SSE-hardness of approximation to within any constant\nfactor; and 4) an $\\exp\\exp\\left(\\Omega\\left(\\sqrt{\\log \\log n}\\right)\\right)$\n(\"quasi-quasi-polynomial\") gap for the standard semidefinite program.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 19:34:32 GMT"}], "update_date": "2015-07-22", "authors_parsed": [["Chan", "Siu On", ""], ["Papailiopoulos", "Dimitris", ""], ["Rubinstein", "Aviad", ""]]}, {"id": "1507.06032", "submitter": "Hongshuai Dai", "authors": "Chunhong Li, Dengxiang Huang, Hongshuai Dai, Xinxing Wei", "title": "Elastic Net Procedure for Partially Linear Models", "comments": "arXiv admin note: text overlap with arXiv:0908.1836 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection plays an important role in the high-dimensional data\nanalysis. However the high-dimensional data often induces the strongly\ncorrelated variables problem. In this paper, we propose Elastic Net procedure\nfor partially linear models and prove the group effect of its estimate. By a\nsimulation study, we show that the strongly correlated variables problem can be\nbetter handled by the Elastic Net procedure than Lasso, ALasso and Ridge. Based\non an empirical analysis, we can get that the Elastic Net procedure is\nparticularly useful when the number of predictors $p$ is much bigger than the\nsample size $n$.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 02:08:23 GMT"}], "update_date": "2015-07-23", "authors_parsed": [["Li", "Chunhong", ""], ["Huang", "Dengxiang", ""], ["Dai", "Hongshuai", ""], ["Wei", "Xinxing", ""]]}, {"id": "1507.06065", "submitter": "Reshad Hosseini", "authors": "Reshad Hosseini and Mohamadreza Mash'al", "title": "MixEst: An Estimation Toolbox for Mixture Models", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture models are powerful statistical models used in many applications\nranging from density estimation to clustering and classification. When dealing\nwith mixture models, there are many issues that the experimenter should be\naware of and needs to solve. The MixEst toolbox is a powerful and user-friendly\npackage for MATLAB that implements several state-of-the-art approaches to\naddress these problems. Additionally, MixEst gives the possibility of using\nmanifold optimization for fitting the density model, a feature specific to this\ntoolbox. MixEst simplifies using and integration of mixture models in\nstatistical models and applications. For developing mixture models of new\ndensities, the user just needs to provide a few functions for that statistical\ndistribution and the toolbox takes care of all the issues regarding mixture\nmodels. MixEst is available at visionlab.ut.ac.ir/mixest and is fully\ndocumented and is licensed under GPL.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 05:23:14 GMT"}], "update_date": "2015-07-23", "authors_parsed": [["Hosseini", "Reshad", ""], ["Mash'al", "Mohamadreza", ""]]}, {"id": "1507.06105", "submitter": "Jianyuan Sun", "authors": "Jianyuan Sun and Guoqiang Zhong and Junyu Dong and Yajuan Cai", "title": "Banzhaf Random Forests", "comments": "arXiv admin note: text overlap with arXiv:1302.4853 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random forests are a type of ensemble method which makes predictions by\ncombining the results of several independent trees. However, the theory of\nrandom forests has long been outpaced by their application. In this paper, we\npropose a novel random forests algorithm based on cooperative game theory.\nBanzhaf power index is employed to evaluate the power of each feature by\ntraversing possible feature coalitions. Unlike the previously used information\ngain rate of information theory, which simply chooses the most informative\nfeature, the Banzhaf power index can be considered as a metric of the\nimportance of each feature on the dependency among a group of features. More\nimportantly, we have proved the consistency of the proposed algorithm, named\nBanzhaf random forests (BRF). This theoretical analysis takes a step towards\nnarrowing the gap between the theory and practice of random forests for\nclassification problems. Experiments on several UCI benchmark data sets show\nthat BRF is competitive with state-of-the-art classifiers and dramatically\noutperforms previous consistent random forests. Particularly, it is much more\nefficient than previous consistent random forests.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 09:10:15 GMT"}], "update_date": "2015-07-23", "authors_parsed": [["Sun", "Jianyuan", ""], ["Zhong", "Guoqiang", ""], ["Dong", "Junyu", ""], ["Cai", "Yajuan", ""]]}, {"id": "1507.06145", "submitter": "Adam Charles", "authors": "Adam Charles, Aurele Balavoine, Christopher Rozell", "title": "Dynamic Filtering of Time-Varying Sparse Signals via l1 Minimization", "comments": "26 pages, 8 figures. arXiv admin note: substantial text overlap with\n  arXiv:1208.0325", "journal-ref": null, "doi": "10.1109/TSP.2016.2586745", "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the importance of sparsity signal models and the increasing\nprevalence of high-dimensional streaming data, there are relatively few\nalgorithms for dynamic filtering of time-varying sparse signals. Of the\nexisting algorithms, fewer still provide strong performance guarantees. This\npaper examines two algorithms for dynamic filtering of sparse signals that are\nbased on efficient l1 optimization methods. We first present an analysis for\none simple algorithm (BPDN-DF) that works well when the system dynamics are\nknown exactly. We then introduce a novel second algorithm (RWL1-DF) that is\nmore computationally complex than BPDN-DF but performs better in practice,\nespecially in the case where the system dynamics model is inaccurate.\nRobustness to model inaccuracy is achieved by using a hierarchical\nprobabilistic data model and propagating higher-order statistics from the\nprevious estimate (akin to Kalman filtering) in the sparse inference process.\nWe demonstrate the properties of these algorithms on both simulated data as\nwell as natural video sequences. Taken together, the algorithms presented in\nthis paper represent the first strong performance analysis of dynamic filtering\nalgorithms for time-varying sparse signals as well as state-of-the-art\nperformance in this emerging application.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 12:10:16 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2016 16:20:48 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Charles", "Adam", ""], ["Balavoine", "Aurele", ""], ["Rozell", "Christopher", ""]]}, {"id": "1507.06217", "submitter": "Sofya Chepushtanova", "authors": "Henry Adams, Sofya Chepushtanova, Tegan Emerson, Eric Hanson, Michael\n  Kirby, Francis Motta, Rachel Neville, Chris Peterson, Patrick Shipman, Lori\n  Ziegelmeier", "title": "Persistence Images: A Stable Vector Representation of Persistent\n  Homology", "comments": "Version 3 contains updated theoretical results supporting\n  methodology; expanded discussion of related works; extended list of\n  references; extended applications section; additional experimental results\n  and new figures", "journal-ref": "Journal of Machine Learning Research 18 (2017), Number 8, 1-35", "doi": null, "report-no": null, "categories": "cs.CG math.AT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many datasets can be viewed as a noisy sampling of an underlying space, and\ntools from topological data analysis can characterize this structure for the\npurpose of knowledge discovery. One such tool is persistent homology, which\nprovides a multiscale description of the homological features within a dataset.\nA useful representation of this homological information is a persistence\ndiagram (PD). Efforts have been made to map PDs into spaces with additional\nstructure valuable to machine learning tasks. We convert a PD to a\nfinite-dimensional vector representation which we call a persistence image\n(PI), and prove the stability of this transformation with respect to small\nperturbations in the inputs. The discriminatory power of PIs is compared\nagainst existing methods, showing significant performance gains. We explore the\nuse of PIs with vector-based machine learning tools, such as linear sparse\nsupport vector machines, which identify features containing discriminating\ntopological information. Finally, high accuracy inference of parameter values\nfrom the dynamic output of a discrete dynamical system (the linked twist map)\nand a partial differential equation (the anisotropic Kuramoto-Sivashinsky\nequation) provide a novel application of the discriminatory power of PIs.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 14:59:02 GMT"}, {"version": "v2", "created": "Sun, 24 Jan 2016 01:18:01 GMT"}, {"version": "v3", "created": "Mon, 11 Jul 2016 14:52:14 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Adams", "Henry", ""], ["Chepushtanova", "Sofya", ""], ["Emerson", "Tegan", ""], ["Hanson", "Eric", ""], ["Kirby", "Michael", ""], ["Motta", "Francis", ""], ["Neville", "Rachel", ""], ["Peterson", "Chris", ""], ["Shipman", "Patrick", ""], ["Ziegelmeier", "Lori", ""]]}, {"id": "1507.06346", "submitter": "Robert Mattila", "authors": "Robert Mattila, Cristian R. Rojas, Bo Wahlberg", "title": "Evaluation of Spectral Learning for the Identification of Hidden Markov\n  Models", "comments": "This paper is accepted and will be published in The Proceedings of\n  the 17th IFAC Symposium on System Identification (SYSID 2015), Beijing,\n  China, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hidden Markov models have successfully been applied as models of discrete\ntime series in many fields. Often, when applied in practice, the parameters of\nthese models have to be estimated. The currently predominating identification\nmethods, such as maximum-likelihood estimation and especially\nexpectation-maximization, are iterative and prone to have problems with local\nminima. A non-iterative method employing a spectral subspace-like approach has\nrecently been proposed in the machine learning literature. This paper evaluates\nthe performance of this algorithm, and compares it to the performance of the\nexpectation-maximization algorithm, on a number of numerical examples. We find\nthat the performance is mixed; it successfully identifies some systems with\nrelatively few available observations, but fails completely for some systems\neven when a large amount of observations is available. An open question is how\nthis discrepancy can be explained. We provide some indications that it could be\nrelated to how well-conditioned some system parameters are.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 21:49:19 GMT"}], "update_date": "2015-07-24", "authors_parsed": [["Mattila", "Robert", ""], ["Rojas", "Cristian R.", ""], ["Wahlberg", "Bo", ""]]}, {"id": "1507.06350", "submitter": "Giri Gopalan", "authors": "Giri Gopalan", "title": "Admissibility of a posterior predictive decision rule", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent decades have seen an interest in prediction problems for which\nBayesian methodology has been used ubiquitously. Sampling from or approximating\nthe posterior predictive distribution in a Bayesian model allows one to make\ninferential statements about potentially observable random quantities given\nobserved data. The purpose of this note is to use statistical decision theory\nas a basis to justify the use of a posterior predictive distribution for making\na point prediction.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 22:29:30 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2015 18:27:30 GMT"}, {"version": "v3", "created": "Tue, 15 Dec 2015 23:59:39 GMT"}, {"version": "v4", "created": "Sat, 25 Jun 2016 05:20:12 GMT"}, {"version": "v5", "created": "Sun, 11 Dec 2016 04:05:27 GMT"}, {"version": "v6", "created": "Fri, 24 Mar 2017 12:43:49 GMT"}, {"version": "v7", "created": "Sat, 9 Sep 2017 21:55:46 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Gopalan", "Giri", ""]]}, {"id": "1507.06370", "submitter": "Tengyu Ma", "authors": "Tengyu Ma, Avi Wigderson", "title": "Sum-of-Squares Lower Bounds for Sparse PCA", "comments": "to appear at NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper establishes a statistical versus computational trade-off for\nsolving a basic high-dimensional machine learning problem via a basic convex\nrelaxation method. Specifically, we consider the {\\em Sparse Principal\nComponent Analysis} (Sparse PCA) problem, and the family of {\\em\nSum-of-Squares} (SoS, aka Lasserre/Parillo) convex relaxations. It was well\nknown that in large dimension $p$, a planted $k$-sparse unit vector can be {\\em\nin principle} detected using only $n \\approx k\\log p$ (Gaussian or Bernoulli)\nsamples, but all {\\em efficient} (polynomial time) algorithms known require $n\n\\approx k^2$ samples. It was also known that this quadratic gap cannot be\nimproved by the the most basic {\\em semi-definite} (SDP, aka spectral)\nrelaxation, equivalent to a degree-2 SoS algorithms. Here we prove that also\ndegree-4 SoS algorithms cannot improve this quadratic gap. This average-case\nlower bound adds to the small collection of hardness results in machine\nlearning for this powerful family of convex relaxation algorithms. Moreover,\nour design of moments (or \"pseudo-expectations\") for this lower bound is quite\ndifferent than previous lower bounds. Establishing lower bounds for higher\ndegree SoS algorithms for remains a challenging problem.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2015 01:50:43 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2015 05:50:16 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Ma", "Tengyu", ""], ["Wigderson", "Avi", ""]]}, {"id": "1507.06411", "submitter": "Olivier Francois", "authors": "Olivier Francois", "title": "Arbitrariness of peer review: A Bayesian analysis of the NIPS experiment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT cs.DL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The principle of peer review is central to the evaluation of research, by\nensuring that only high-quality items are funded or published. But peer review\nhas also received criticism, as the selection of reviewers may introduce biases\nin the system. In 2014, the organizers of the ``Neural Information Processing\nSystems\\rq\\rq{} conference conducted an experiment in which $10\\%$ of submitted\nmanuscripts (166 items) went through the review process twice. Arbitrariness\nwas measured as the conditional probability for an accepted submission to get\nrejected if examined by the second committee. This number was equal to $60\\%$,\nfor a total acceptance rate equal to $22.5\\%$. Here we present a Bayesian\nanalysis of those two numbers, by introducing a hidden parameter which measures\nthe probability that a submission meets basic quality criteria. The standard\nquality criteria usually include novelty, clarity, reproducibility, correctness\nand no form of misconduct, and are met by a large proportions of submitted\nitems. The Bayesian estimate for the hidden parameter was equal to $56\\%$\n($95\\%$CI: $ I = (0.34, 0.83)$), and had a clear interpretation. The result\nsuggested the total acceptance rate should be increased in order to decrease\narbitrariness estimates in future review processes.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2015 08:39:34 GMT"}], "update_date": "2015-07-24", "authors_parsed": [["Francois", "Olivier", ""]]}, {"id": "1507.06452", "submitter": "Amin Mantrach", "authors": "Robin Devooght and Nicolas Kourtellis and Amin Mantrach", "title": "Dynamic Matrix Factorization with Priors on Unknown Values", "comments": "in the Proceedings of 21st ACM SIGKDD Conference on Knowledge\n  Discovery and Data Mining 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advanced and effective collaborative filtering methods based on explicit\nfeedback assume that unknown ratings do not follow the same model as the\nobserved ones (\\emph{not missing at random}). In this work, we build on this\nassumption, and introduce a novel dynamic matrix factorization framework that\nallows to set an explicit prior on unknown values. When new ratings, users, or\nitems enter the system, we can update the factorization in time independent of\nthe size of data (number of users, items and ratings). Hence, we can quickly\nrecommend items even to very recent users. We test our methods on three large\ndatasets, including two very sparse ones, in static and dynamic conditions. In\neach case, we outrank state-of-the-art matrix factorization methods that do not\nuse a prior on unknown ratings.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2015 11:39:58 GMT"}], "update_date": "2015-07-24", "authors_parsed": [["Devooght", "Robin", ""], ["Kourtellis", "Nicolas", ""], ["Mantrach", "Amin", ""]]}, {"id": "1507.06535", "submitter": "Alhussein Fawzi", "authors": "Alhussein Fawzi, Pascal Frossard", "title": "Manitest: Are classifiers really invariant?", "comments": "BMVC 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Invariance to geometric transformations is a highly desirable property of\nautomatic classifiers in many image recognition tasks. Nevertheless, it is\nunclear to which extent state-of-the-art classifiers are invariant to basic\ntransformations such as rotations and translations. This is mainly due to the\nlack of general methods that properly measure such an invariance. In this\npaper, we propose a rigorous and systematic approach for quantifying the\ninvariance to geometric transformations of any classifier. Our key idea is to\ncast the problem of assessing a classifier's invariance as the computation of\ngeodesics along the manifold of transformed images. We propose the Manitest\nmethod, built on the efficient Fast Marching algorithm to compute the\ninvariance of classifiers. Our new method quantifies in particular the\nimportance of data augmentation for learning invariance from data, and the\nincreased invariance of convolutional neural networks with depth. We foresee\nthat the proposed generic tool for measuring invariance to a large class of\ngeometric transformations and arbitrary classifiers will have many applications\nfor evaluating and comparing classifiers based on their invariance, and help\nimproving the invariance of existing classifiers.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2015 15:36:50 GMT"}], "update_date": "2015-07-24", "authors_parsed": [["Fawzi", "Alhussein", ""], ["Frossard", "Pascal", ""]]}, {"id": "1507.06580", "submitter": "Ronen Eldan", "authors": "S\\'ebastien Bubeck and Ronen Eldan", "title": "Multi-scale exploration of convex functions and bandit convex\n  optimization", "comments": "Preliminary version; 22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.MG cs.LG math.OC math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct a new map from a convex function to a distribution on its\ndomain, with the property that this distribution is a multi-scale exploration\nof the function. We use this map to solve a decade-old open problem in\nadversarial bandit convex optimization by showing that the minimax regret for\nthis problem is $\\tilde{O}(\\mathrm{poly}(n) \\sqrt{T})$, where $n$ is the\ndimension and $T$ the number of rounds. This bound is obtained by studying the\ndual Bayesian maximin regret via the information ratio analysis of Russo and\nVan Roy, and then using the multi-scale exploration to solve the Bayesian\nproblem.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2015 17:32:49 GMT"}], "update_date": "2015-07-24", "authors_parsed": [["Bubeck", "S\u00e9bastien", ""], ["Eldan", "Ronen", ""]]}, {"id": "1507.06615", "submitter": "Ingo Steinwart", "authors": "Mona Eberts and Ingo Steinwart", "title": "Optimal Learning Rates for Localized SVMs", "comments": "68 pages, 20 figures, and 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the limiting factors of using support vector machines (SVMs) in large\nscale applications are their super-linear computational requirements in terms\nof the number of training samples. To address this issue, several approaches\nthat train SVMs on many small chunks of large data sets separately have been\nproposed in the literature. So far, however, almost all these approaches have\nonly been empirically investigated. In addition, their motivation was always\nbased on computational requirements. In this work, we consider a localized SVM\napproach based upon a partition of the input space. For this local SVM, we\nderive a general oracle inequality. Then we apply this oracle inequality to\nleast squares regression using Gaussian kernels and deduce local learning rates\nthat are essentially minimax optimal under some standard smoothness assumptions\non the regression function. This gives the first motivation for using local\nSVMs that is not based on computational requirements but on theoretical\npredictions on the generalization performance. We further introduce a\ndata-dependent parameter selection method for our local SVM approach and show\nthat this method achieves the same learning rates as before. Finally, we\npresent some larger scale experiments for our localized SVM showing that it\nachieves essentially the same test performance as a global SVM for a fraction\nof the computational requirements. In addition, it turns out that the\ncomputational requirements for the local SVMs are similar to those of a vanilla\nrandom chunk approach, while the achieved test errors are significantly better.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2015 19:03:48 GMT"}], "update_date": "2015-07-24", "authors_parsed": [["Eberts", "Mona", ""], ["Steinwart", "Ingo", ""]]}, {"id": "1507.06682", "submitter": "Pin-Yu Chen", "authors": "Pin-Yu Chen, Chia-Wei Lien, Fu-Jen Chu, Pai-Shun Ting, Shin-Ming Cheng", "title": "Supervised Collective Classification for Crowdsourcing", "comments": "to appear in IEEE Global Communications Conference (GLOBECOM)\n  Workshop on Networking and Collaboration Issues for the Internet of\n  Everything", "journal-ref": null, "doi": "10.1109/GLOCOMW.2015.7414077", "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing utilizes the wisdom of crowds for collective classification via\ninformation (e.g., labels of an item) provided by labelers. Current\ncrowdsourcing algorithms are mainly unsupervised methods that are unaware of\nthe quality of crowdsourced data. In this paper, we propose a supervised\ncollective classification algorithm that aims to identify reliable labelers\nfrom the training data (e.g., items with known labels). The reliability (i.e.,\nweighting factor) of each labeler is determined via a saddle point algorithm.\nThe results on several crowdsourced data show that supervised methods can\nachieve better classification accuracy than unsupervised methods, and our\nproposed method outperforms other algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2015 21:02:33 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2015 03:13:46 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Chen", "Pin-Yu", ""], ["Lien", "Chia-Wei", ""], ["Chu", "Fu-Jen", ""], ["Ting", "Pai-Shun", ""], ["Cheng", "Shin-Ming", ""]]}, {"id": "1507.06683", "submitter": "Nata\\v{s}a Kej\\v{z}ar", "authors": "Vladimir Batagelj and Nata\\v{s}a Kej\\v{z}ar and Simona\n  Korenjak-\\v{C}erne", "title": "Clustering of Modal Valued Symbolic Data", "comments": "26 pages, 6 figures. Adv Data Anal Classif (2020)", "journal-ref": null, "doi": "10.1007/s11634-020-00425-4", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symbolic Data Analysis is based on special descriptions of data - symbolic\nobjects (SO). Such descriptions preserve more detailed information about units\nand their clusters than the usual representations with mean values. A special\nkind of symbolic object is a representation with frequency or probability\ndistributions (modal values). This representation enables us to consider in the\nclustering process the variables of all measurement types at the same time. In\nthe paper a clustering criterion function for SOs is proposed such that the\nrepresentative of each cluster is again composed of distributions of variables'\nvalues over the cluster. The corresponding leaders clustering method is based\non this result. It is also shown that for the corresponding agglomerative\nhierarchical method a generalized Ward's formula holds. Both methods are\ncompatible - they are solving the same clustering optimization problem. The\nleaders method efficiently solves clustering problems with large number of\nunits; while the agglomerative method can be applied alone on the smaller data\nset, or it could be applied on leaders, obtained with compatible\nnonhierarchical clustering method. Such a combination of two compatible methods\nenables us to decide upon the right number of clusters on the basis of the\ncorresponding dendrogram. The proposed methods were applied on different data\nsets. In the paper, some results of clustering of ESS data are presented.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2015 21:07:11 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Batagelj", "Vladimir", ""], ["Kej\u017ear", "Nata\u0161a", ""], ["Korenjak-\u010cerne", "Simona", ""]]}, {"id": "1507.06738", "submitter": "Shipra Agrawal", "authors": "Shipra Agrawal and Nikhil R. Devanur", "title": "Linear Contextual Bandits with Knapsacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the linear contextual bandit problem with resource consumption,\nin addition to reward generation. In each round, the outcome of pulling an arm\nis a reward as well as a vector of resource consumptions. The expected values\nof these outcomes depend linearly on the context of that arm. The\nbudget/capacity constraints require that the total consumption doesn't exceed\nthe budget for each resource. The objective is once again to maximize the total\nreward. This problem turns out to be a common generalization of classic linear\ncontextual bandits (linContextual), bandits with knapsacks (BwK), and the\nonline stochastic packing problem (OSPP). We present algorithms with\nnear-optimal regret bounds for this problem. Our bounds compare favorably to\nresults on the unstructured version of the problem where the relation between\nthe contexts and the outcomes could be arbitrary, but the algorithm only\ncompetes against a fixed set of policies accessible through an optimization\noracle. We combine techniques from the work on linContextual, BwK, and OSPP in\na nontrivial manner while also tackling new difficulties that are not present\nin any of these special cases.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 04:24:22 GMT"}, {"version": "v2", "created": "Sat, 9 Jul 2016 06:29:22 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Agrawal", "Shipra", ""], ["Devanur", "Nikhil R.", ""]]}, {"id": "1507.06759", "submitter": "Phaedon-Stelios Koutsourelakis", "authors": "Phaedon-Stelios Koutsourelakis", "title": "Variational Bayesian strategies for high-dimensional, stochastic design\n  problems", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2015.12.031", "report-no": null, "categories": "stat.CO math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with a lesser-studied problem in the context of\nmodel-based, uncertainty quantification (UQ), that of\noptimization/design/control under uncertainty. The solution of such problems is\nhindered not only by the usual difficulties encountered in UQ tasks (e.g. the\nhigh computational cost of each forward simulation, the large number of random\nvariables) but also by the need to solve a nonlinear optimization problem\ninvolving large numbers of design variables and potentially constraints. We\npropose a framework that is suitable for a large class of such problems and is\nbased on the idea of recasting them as probabilistic inference tasks. To that\nend, we propose a Variational Bayesian (VB) formulation and an iterative\nVB-Expectation-Maximization scheme that is also capable of identifying a\nlow-dimensional set of directions in the design space, along which, the\nobjective exhibits the largest sensitivity.\n  We demonstrate the validity of the proposed approach in the context of two\nnumerical examples involving $\\mathcal{O}(10^3)$ random and design variables.\nIn all cases considered the cost of the computations in terms of calls to the\nforward model was of the order $\\mathcal{O}(10^2)$. The accuracy of the\napproximations provided is assessed by appropriate information-theoretic\nmetrics.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 07:00:32 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2015 10:56:46 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Koutsourelakis", "Phaedon-Stelios", ""]]}, {"id": "1507.06763", "submitter": "Rina Okada", "authors": "Rina Okada, Kazuto Fukuchi, Kazuya Kakizaki and Jun Sakuma", "title": "Differentially Private Analysis of Outliers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates differentially private analysis of distance-based\noutliers. The problem of outlier detection is to find a small number of\ninstances that are apparently distant from the remaining instances. On the\nother hand, the objective of differential privacy is to conceal presence (or\nabsence) of any particular instance. Outlier detection and privacy protection\nare thus intrinsically conflicting tasks. In this paper, instead of reporting\noutliers detected, we present two types of differentially private queries that\nhelp to understand behavior of outliers. One is the query to count outliers,\nwhich reports the number of outliers that appear in a given subspace. Our\nformal analysis on the exact global sensitivity of outlier counts reveals that\nregular global sensitivity based method can make the outputs too noisy,\nparticularly when the dimensionality of the given subspace is high. Noting that\nthe counts of outliers are typically expected to be relatively small compared\nto the number of data, we introduce a mechanism based on the smooth upper bound\nof the local sensitivity. The other is the query to discovery top-$h$ subspaces\ncontaining a large number of outliers. This task can be naively achieved by\nissuing count queries to each subspace in turn. However, the variation of\nsubspaces can grow exponentially in the data dimensionality. This can cause\nserious consumption of the privacy budget. For this task, we propose an\nexponential mechanism with a customized score function for subspace discovery.\nTo the best of our knowledge, this study is the first trial to ensure\ndifferential privacy for distance-based outlier analysis. We demonstrated our\nmethods with synthesized datasets and real datasets. The experimental results\nshow that out method achieve better utility compared to the global sensitivity\nbased methods.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 07:30:49 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2015 02:19:15 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Okada", "Rina", ""], ["Fukuchi", "Kazuto", ""], ["Kakizaki", "Kazuya", ""], ["Sakuma", "Jun", ""]]}, {"id": "1507.06802", "submitter": "Jesse Krijthe", "authors": "Jesse H. Krijthe and Marco Loog", "title": "Implicitly Constrained Semi-Supervised Least Squares Classification", "comments": "12 pages, 2 figures, 1 table. The Fourteenth International Symposium\n  on Intelligent Data Analysis (2015), Saint-Etienne, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel semi-supervised version of the least squares classifier.\nThis implicitly constrained least squares (ICLS) classifier minimizes the\nsquared loss on the labeled data among the set of parameters implied by all\npossible labelings of the unlabeled data. Unlike other discriminative\nsemi-supervised methods, our approach does not introduce explicit additional\nassumptions into the objective function, but leverages implicit assumptions\nalready present in the choice of the supervised least squares classifier. We\nshow this approach can be formulated as a quadratic programming problem and its\nsolution can be found using a simple gradient descent procedure. We prove that,\nin a certain way, our method never leads to performance worse than the\nsupervised classifier. Experimental results corroborate this theoretical result\nin the multidimensional case on benchmark datasets, also in terms of the error\nrate.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 10:39:44 GMT"}], "update_date": "2015-07-27", "authors_parsed": [["Krijthe", "Jesse H.", ""], ["Loog", "Marco", ""]]}, {"id": "1507.06947", "submitter": "Hasim Sak", "authors": "Ha\\c{s}im Sak, Andrew Senior, Kanishka Rao, Fran\\c{c}oise Beaufays", "title": "Fast and Accurate Recurrent Neural Network Acoustic Models for Speech\n  Recognition", "comments": "To be published in the INTERSPEECH 2015 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have recently shown that deep Long Short-Term Memory (LSTM) recurrent\nneural networks (RNNs) outperform feed forward deep neural networks (DNNs) as\nacoustic models for speech recognition. More recently, we have shown that the\nperformance of sequence trained context dependent (CD) hidden Markov model\n(HMM) acoustic models using such LSTM RNNs can be equaled by sequence trained\nphone models initialized with connectionist temporal classification (CTC). In\nthis paper, we present techniques that further improve performance of LSTM RNN\nacoustic models for large vocabulary speech recognition. We show that frame\nstacking and reduced frame rate lead to more accurate models and faster\ndecoding. CD phone modeling leads to further improvements. We also present\ninitial results for LSTM RNN models outputting words directly.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 18:28:32 GMT"}], "update_date": "2015-07-27", "authors_parsed": [["Sak", "Ha\u015fim", ""], ["Senior", "Andrew", ""], ["Rao", "Kanishka", ""], ["Beaufays", "Fran\u00e7oise", ""]]}, {"id": "1507.06970", "submitter": "Horia Mania", "authors": "Horia Mania, Xinghao Pan, Dimitris Papailiopoulos, Benjamin Recht,\n  Kannan Ramchandran, Michael I. Jordan", "title": "Perturbed Iterate Analysis for Asynchronous Stochastic Optimization", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.DS cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and analyze stochastic optimization methods where the input to\neach gradient update is perturbed by bounded noise. We show that this framework\nforms the basis of a unified approach to analyze asynchronous implementations\nof stochastic optimization algorithms.In this framework, asynchronous\nstochastic optimization algorithms can be thought of as serial methods\noperating on noisy inputs. Using our perturbed iterate framework, we provide\nnew analyses of the Hogwild! algorithm and asynchronous stochastic coordinate\ndescent, that are simpler than earlier analyses, remove many assumptions of\nprevious models, and in some cases yield improved upper bounds on the\nconvergence rates. We proceed to apply our framework to develop and analyze\nKroMagnon: a novel, parallel, sparse stochastic variance-reduced gradient\n(SVRG) algorithm. We demonstrate experimentally on a 16-core machine that the\nsparse and parallel version of SVRG is in some cases more than four orders of\nmagnitude faster than the standard SVRG algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 19:36:13 GMT"}, {"version": "v2", "created": "Fri, 25 Mar 2016 20:00:45 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Mania", "Horia", ""], ["Pan", "Xinghao", ""], ["Papailiopoulos", "Dimitris", ""], ["Recht", "Benjamin", ""], ["Ramchandran", "Kannan", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1507.06977", "submitter": "Yves-Laurent Kom Samo", "authors": "Yves-Laurent Kom Samo and Stephen Roberts", "title": "String and Membrane Gaussian Processes", "comments": "To appear in the Journal of Machine Learning Research (JMLR), Volume\n  17", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a novel framework for making exact nonparametric\nBayesian inference on latent functions, that is particularly suitable for Big\nData tasks. Firstly, we introduce a class of stochastic processes we refer to\nas string Gaussian processes (string GPs), which are not to be mistaken for\nGaussian processes operating on text. We construct string GPs so that their\nfinite-dimensional marginals exhibit suitable local conditional independence\nstructures, which allow for scalable, distributed, and flexible nonparametric\nBayesian inference, without resorting to approximations, and while ensuring\nsome mild global regularity constraints. Furthermore, string GP priors\nnaturally cope with heterogeneous input data, and the gradient of the learned\nlatent function is readily available for explanatory analysis. Secondly, we\nprovide some theoretical results relating our approach to the standard GP\nparadigm. In particular, we prove that some string GPs are Gaussian processes,\nwhich provides a complementary global perspective on our framework. Finally, we\nderive a scalable and distributed MCMC scheme for supervised learning tasks\nunder string GP priors. The proposed MCMC scheme has computational time\ncomplexity $\\mathcal{O}(N)$ and memory requirement $\\mathcal{O}(dN)$, where $N$\nis the data size and $d$ the dimension of the input space. We illustrate the\nefficacy of the proposed approach on several synthetic and real-world datasets,\nincluding a dataset with $6$ millions input points and $8$ attributes.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 19:53:47 GMT"}, {"version": "v2", "created": "Fri, 13 May 2016 11:07:14 GMT"}, {"version": "v3", "created": "Mon, 16 May 2016 12:51:33 GMT"}, {"version": "v4", "created": "Fri, 19 Aug 2016 08:25:17 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Samo", "Yves-Laurent Kom", ""], ["Roberts", "Stephen", ""]]}, {"id": "1507.07094", "submitter": "Miles Lopes", "authors": "Miles E. Lopes", "title": "Unknown sparsity in compressed sensing: Denoising and inference", "comments": "The title of the previous tech report has been updated so that it\n  matches the published version. The published version contains additional\n  material", "journal-ref": "IEEE Transactions on Information Theory 62.9 (2016): 5145-5166", "doi": "10.1109/TIT.2016.2587772", "report-no": null, "categories": "cs.IT math.IT math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theory of Compressed Sensing (CS) asserts that an unknown signal\n$x\\in\\mathbb{R}^p$ can be accurately recovered from an underdetermined set of\n$n$ linear measurements with $n\\ll p$, provided that $x$ is sufficiently\nsparse. However, in applications, the degree of sparsity $\\|x\\|_0$ is typically\nunknown, and the problem of directly estimating $\\|x\\|_0$ has been a\nlongstanding gap between theory and practice. A closely related issue is that\n$\\|x\\|_0$ is a highly idealized measure of sparsity, and for real signals with\nentries not equal to 0, the value $\\|x\\|_0=p$ is not a useful description of\ncompressibility. In our previous conference paper [Lop13] that examined these\nproblems, we considered an alternative measure of \"soft\" sparsity,\n$\\|x\\|_1^2/\\|x\\|_2^2$, and designed a procedure to estimate\n$\\|x\\|_1^2/\\|x\\|_2^2$ that does not rely on sparsity assumptions.\n  The present work offers a new deconvolution-based method for estimating\nunknown sparsity, which has wider applicability and sharper theoretical\nguarantees. In particular, we introduce a family of entropy-based sparsity\nmeasures $s_q(x):=\\big(\\frac{\\|x\\|_q}{\\|x\\|_1}\\big)^{\\frac{q}{1-q}}$\nparameterized by $q\\in[0,\\infty]$. This family interpolates between\n$\\|x\\|_0=s_0(x)$ and $\\|x\\|_1^2/\\|x\\|_2^2=s_2(x)$ as $q$ ranges over $[0,2]$.\nFor any $q\\in (0,2]\\setminus\\{1\\}$, we propose an estimator $\\hat{s}_q(x)$\nwhose relative error converges at the dimension-free rate of $1/\\sqrt{n}$, even\nwhen $p/n\\to\\infty$. Our main results also describe the limiting distribution\nof $\\hat{s}_q(x)$, as well as some connections to Basis Pursuit Denosing, the\nLasso, deterministic measurement matrices, and inference problems in CS.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2015 11:17:03 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2015 23:06:31 GMT"}, {"version": "v3", "created": "Thu, 31 Aug 2017 09:52:03 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Lopes", "Miles E.", ""]]}, {"id": "1507.07105", "submitter": "Reinhard Heckel", "authors": "Reinhard Heckel, Michael Tschannen, and Helmut B\\\"olcskei", "title": "Dimensionality-reduced subspace clustering", "comments": "new results for the noisy case, additional simulation work,\n  additional discussions in the main body", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace clustering refers to the problem of clustering unlabeled\nhigh-dimensional data points into a union of low-dimensional linear subspaces,\nwhose number, orientations, and dimensions are all unknown. In practice one may\nhave access to dimensionality-reduced observations of the data only, resulting,\ne.g., from undersampling due to complexity and speed constraints on the\nacquisition device or mechanism. More pertinently, even if the high-dimensional\ndata set is available it is often desirable to first project the data points\ninto a lower-dimensional space and to perform clustering there; this reduces\nstorage requirements and computational cost. The purpose of this paper is to\nquantify the impact of dimensionality reduction through random projection on\nthe performance of three subspace clustering algorithms, all of which are based\non principles from sparse signal recovery. Specifically, we analyze the\nthresholding based subspace clustering (TSC) algorithm, the sparse subspace\nclustering (SSC) algorithm, and an orthogonal matching pursuit variant thereof\n(SSC-OMP). We find, for all three algorithms, that dimensionality reduction\ndown to the order of the subspace dimensions is possible without incurring\nsignificant performance degradation. Moreover, these results are order-wise\noptimal in the sense that reducing the dimensionality further leads to a\nfundamentally ill-posed clustering problem. Our findings carry over to the\nnoisy case as illustrated through analytical results for TSC and simulations\nfor SSC and SSC-OMP. Extensive experiments on synthetic and real data\ncomplement our theoretical findings.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2015 14:49:01 GMT"}, {"version": "v2", "created": "Sun, 13 Dec 2015 20:50:49 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Heckel", "Reinhard", ""], ["Tschannen", "Michael", ""], ["B\u00f6lcskei", "Helmut", ""]]}, {"id": "1507.07238", "submitter": "Dimitris Katselis", "authors": "Dimitrios Katselis, Cristian R. Rojas, Carolyn L. Beck", "title": "Estimator Selection: End-Performance Metric Aspects", "comments": "arXiv admin note: substantial text overlap with arXiv:1303.4289", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a framework for application-oriented optimal experiment design has\nbeen introduced. In this context, the distance of the estimated system from the\ntrue one is measured in terms of a particular end-performance metric. This\ntreatment leads to superior unknown system estimates to classical experiment\ndesigns based on usual pointwise functional distances of the estimated system\nfrom the true one. The separation of the system estimator from the experiment\ndesign is done within this new framework by choosing and fixing the estimation\nmethod to either a maximum likelihood (ML) approach or a Bayesian estimator\nsuch as the minimum mean square error (MMSE). Since the MMSE estimator delivers\na system estimate with lower mean square error (MSE) than the ML estimator for\nfinite-length experiments, it is usually considered the best choice in practice\nin signal processing and control applications. Within the application-oriented\nframework a related meaningful question is: Are there end-performance metrics\nfor which the ML estimator outperforms the MMSE when the experiment is\nfinite-length? In this paper, we affirmatively answer this question based on a\nsimple linear Gaussian regression example.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2015 19:43:36 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Katselis", "Dimitrios", ""], ["Rojas", "Cristian R.", ""], ["Beck", "Carolyn L.", ""]]}, {"id": "1507.07260", "submitter": "Hassan Kingravi", "authors": "Hassan A. Kingravi, Patricio A. Vela, Alexandar Gray", "title": "Reduced-Set Kernel Principal Components Analysis for Improving the\n  Training and Execution Speed of Kernel Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a practical, and theoretically well-founded, approach to\nimprove the speed of kernel manifold learning algorithms relying on spectral\ndecomposition. Utilizing recent insights in kernel smoothing and learning with\nintegral operators, we propose Reduced Set KPCA (RSKPCA), which also suggests\nan easy-to-implement method to remove or replace samples with minimal effect on\nthe empirical operator. A simple data point selection procedure is given to\ngenerate a substitute density for the data, with accuracy that is governed by a\nuser-tunable parameter . The effect of the approximation on the quality of the\nKPCA solution, in terms of spectral and operator errors, can be shown directly\nin terms of the density estimate error and as a function of the parameter . We\nshow in experiments that RSKPCA can improve both training and evaluation time\nof KPCA by up to an order of magnitude, and compares favorably to the\nwidely-used Nystrom and density-weighted Nystrom methods.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2015 22:28:34 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Kingravi", "Hassan A.", ""], ["Vela", "Patricio A.", ""], ["Gray", "Alexandar", ""]]}, {"id": "1507.07495", "submitter": "Asif Shakeel", "authors": "David A. Meyer and Asif Shakeel", "title": "Estimating an Activity Driven Hidden Markov Model", "comments": "13 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG cs.SI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a Hidden Markov Model (HMM) in which each hidden state has\ntime-dependent $\\textit{activity levels}$ that drive transitions and emissions,\nand show how to estimate its parameters. Our construction is motivated by the\nproblem of inferring human mobility on sub-daily time scales from, for example,\nmobile phone records.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2015 17:37:27 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Meyer", "David A.", ""], ["Shakeel", "Asif", ""]]}, {"id": "1507.07536", "submitter": "Vassilis Kekatos", "authors": "Dimitris Berberidis, Vassilis Kekatos, Georgios B. Giannakis", "title": "Online Censoring for Large-Scale Regressions with Application to\n  Streaming Big Data", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2016.2546225", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear regression is arguably the most prominent among statistical inference\nmethods, popular both for its simplicity as well as its broad applicability. On\npar with data-intensive applications, the sheer size of linear regression\nproblems creates an ever growing demand for quick and cost efficient solvers.\nFortunately, a significant percentage of the data accrued can be omitted while\nmaintaining a certain quality of statistical inference with an affordable\ncomputational budget. The present paper introduces means of identifying and\nomitting \"less informative\" observations in an online and data-adaptive\nfashion, built on principles of stochastic approximation and data censoring.\nFirst- and second-order stochastic approximation maximum likelihood-based\nalgorithms for censored observations are developed for estimating the\nregression coefficients. Online algorithms are also put forth to reduce the\noverall complexity by adaptively performing censoring along with estimation.\nThe novel algorithms entail simple closed-form updates, and have provable\n(non)asymptotic convergence guarantees. Furthermore, specific rules are\ninvestigated for tuning to desired censoring patterns and levels of\ndimensionality reduction. Simulated tests on real and synthetic datasets\ncorroborate the efficacy of the proposed data-adaptive methods compared to\ndata-agnostic random projection-based alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2015 19:25:29 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Berberidis", "Dimitris", ""], ["Kekatos", "Vassilis", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1507.07595", "submitter": "Tengyu Ma", "authors": "Jason D. Lee, Qihang Lin, Tengyu Ma, Tianbao Yang", "title": "Distributed Stochastic Variance Reduced Gradient Methods and A Lower\n  Bound for Communication Complexity", "comments": "significant addition to both theory and experimental results", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study distributed optimization algorithms for minimizing the average of\nconvex functions. The applications include empirical risk minimization problems\nin statistical machine learning where the datasets are large and have to be\nstored on different machines. We design a distributed stochastic variance\nreduced gradient algorithm that, under certain conditions on the condition\nnumber, simultaneously achieves the optimal parallel runtime, amount of\ncommunication and rounds of communication among all distributed first-order\nmethods up to constant factors. Our method and its accelerated extension also\noutperform existing distributed algorithms in terms of the rounds of\ncommunication as long as the condition number is not too large compared to the\nsize of data in each machine. We also prove a lower bound for the number of\nrounds of communication for a broad class of distributed first-order methods\nincluding the proposed algorithms in this paper. We show that our accelerated\ndistributed stochastic variance reduced gradient algorithm achieves this lower\nbound so that it uses the fewest rounds of communication among all distributed\nfirst-order algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2015 22:09:57 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2016 19:26:31 GMT"}], "update_date": "2016-01-07", "authors_parsed": [["Lee", "Jason D.", ""], ["Lin", "Qihang", ""], ["Ma", "Tengyu", ""], ["Yang", "Tianbao", ""]]}, {"id": "1507.07680", "submitter": "Yann Ollivier", "authors": "Yann Ollivier, Corentin Tallec, Guillaume Charpiat", "title": "Training recurrent networks online without backtracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the \"NoBackTrack\" algorithm to train the parameters of dynamical\nsystems such as recurrent neural networks. This algorithm works in an online,\nmemoryless setting, thus requiring no backpropagation through time, and is\nscalable, avoiding the large computational and memory cost of maintaining the\nfull gradient of the current state with respect to the parameters.\n  The algorithm essentially maintains, at each time, a single search direction\nin parameter space. The evolution of this search direction is partly stochastic\nand is constructed in such a way to provide, at every time, an unbiased random\nestimate of the gradient of the loss function with respect to the parameters.\nBecause the gradient estimate is unbiased, on average over time the parameter\nis updated as it should.\n  The resulting gradient estimate can then be fed to a lightweight Kalman-like\nfilter to yield an improved algorithm. For recurrent neural networks, the\nresulting algorithms scale linearly with the number of parameters.\n  Small-scale experiments confirm the suitability of the approach, showing that\nthe stochastic approximation of the gradient introduced in the algorithm is not\ndetrimental to learning. In particular, the Kalman-like version of NoBackTrack\nis superior to backpropagation through time (BPTT) when the time span of\ndependencies in the data is longer than the truncation span for BPTT.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2015 08:26:50 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2015 22:29:38 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Ollivier", "Yann", ""], ["Tallec", "Corentin", ""], ["Charpiat", "Guillaume", ""]]}, {"id": "1507.07813", "submitter": "Yuval Harel", "authors": "Yuval Harel, Ron Meir, Manfred Opper", "title": "An Analytically Tractable Bayesian Approximation to Optimal Point\n  Process Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The process of dynamic state estimation (filtering) based on point process\nobservations is in general intractable. Numerical sampling techniques are often\npractically useful, but lead to limited conceptual insight about optimal\nencoding/decoding strategies, which are of significant relevance to\nComputational Neuroscience. We develop an analytically tractable Bayesian\napproximation to optimal filtering based on point process observations, which\nallows us to introduce distributional assumptions about sensory cell\nproperties, that greatly facilitates the analysis of optimal encoding in\nsituations deviating from common assumptions of uniform coding. The analytic\nframework leads to insights which are difficult to obtain from numerical\nalgorithms, and is consistent with experiments about the distribution of tuning\ncurve centers. Interestingly, we find that the information gained from the\nabsence of spikes may be crucial to performance.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2015 15:35:54 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Harel", "Yuval", ""], ["Meir", "Ron", ""], ["Opper", "Manfred", ""]]}, {"id": "1507.07909", "submitter": "Luiz Gustavo Hafemann", "authors": "Luiz G. Hafemann, Robert Sabourin, Luiz S. Oliveira", "title": "Offline Handwritten Signature Verification - Literature Review", "comments": "Accepted to the International Conference on Image Processing Theory,\n  Tools and Applications (IPTA 2017)", "journal-ref": null, "doi": "10.1109/IPTA.2017.8310112", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The area of Handwritten Signature Verification has been broadly researched in\nthe last decades, but remains an open research problem. The objective of\nsignature verification systems is to discriminate if a given signature is\ngenuine (produced by the claimed individual), or a forgery (produced by an\nimpostor). This has demonstrated to be a challenging task, in particular in the\noffline (static) scenario, that uses images of scanned signatures, where the\ndynamic information about the signing process is not available. Many\nadvancements have been proposed in the literature in the last 5-10 years, most\nnotably the application of Deep Learning methods to learn feature\nrepresentations from signature images. In this paper, we present how the\nproblem has been handled in the past few decades, analyze the recent\nadvancements in the field, and the potential directions for future research.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2015 19:31:44 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2015 20:40:01 GMT"}, {"version": "v3", "created": "Wed, 2 Aug 2017 15:11:20 GMT"}, {"version": "v4", "created": "Mon, 16 Oct 2017 12:56:24 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Hafemann", "Luiz G.", ""], ["Sabourin", "Robert", ""], ["Oliveira", "Luiz S.", ""]]}, {"id": "1507.07974", "submitter": "Shuchin Aeron", "authors": "John Pothier, Josh Girson, Shuchin Aeron", "title": "An algorithm for online tensor prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for online prediction and learning of tensors\n($N$-way arrays, $N >2$) from sequential measurements. We focus on the specific\ncase of 3-D tensors and exploit a recently developed framework of structured\ntensor decompositions proposed in [1]. In this framework it is possible to\ntreat 3-D tensors as linear operators and appropriately generalize notions of\nrank and positive definiteness to tensors in a natural way. Using these notions\nwe propose a generalization of the matrix exponentiated gradient descent\nalgorithm [2] to a tensor exponentiated gradient descent algorithm using an\nextension of the notion of von-Neumann divergence to tensors. Then following a\nsimilar construction as in [3], we exploit this algorithm to propose an online\nalgorithm for learning and prediction of tensors with provable regret\nguarantees. Simulations results are presented on semi-synthetic data sets of\nratings evolving in time under local influence over a social network. The\nresult indicate superior performance compared to other (online) convex tensor\ncompletion methods.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2015 22:09:36 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Pothier", "John", ""], ["Girson", "Josh", ""], ["Aeron", "Shuchin", ""]]}, {"id": "1507.08074", "submitter": "Sergey Novoselov", "authors": "Sergey Novoselov, Alexandr Kozlov, Galina Lavrentyeva, Konstantin\n  Simonchik, Vadim Shchemelinin", "title": "STC Anti-spoofing Systems for the ASVspoof 2015 Challenge", "comments": "5 pages, 8 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the Speech Technology Center (STC) systems submitted to\nAutomatic Speaker Verification Spoofing and Countermeasures (ASVspoof)\nChallenge 2015. In this work we investigate different acoustic feature spaces\nto determine reliable and robust countermeasures against spoofing attacks. In\naddition to the commonly used front-end MFCC features we explored features\nderived from phase spectrum and features based on applying the multiresolution\nwavelet transform. Similar to state-of-the-art ASV systems, we used the\nstandard TV-JFA approach for probability modelling in spoofing detection\nsystems. Experiments performed on the development and evaluation datasets of\nthe Challenge demonstrate that the use of phase-related and wavelet-based\nfeatures provides a substantial input into the efficiency of the resulting STC\nsystems. In our research we also focused on the comparison of the linear (SVM)\nand nonlinear (DBN) classifiers.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 09:22:58 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Novoselov", "Sergey", ""], ["Kozlov", "Alexandr", ""], ["Lavrentyeva", "Galina", ""], ["Simonchik", "Konstantin", ""], ["Shchemelinin", "Vadim", ""]]}, {"id": "1507.08155", "submitter": "Teng Qiu", "authors": "Teng Qiu, Yongjie Li", "title": "IT-Dendrogram: A New Member of the In-Tree (IT) Clustering Family", "comments": "13 pages, 6 figures. IT-Dendrogram: An Effective Method to Visualize\n  the In-Tree structure by Dendrogram", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Previously, we proposed a physically-inspired method to construct data points\ninto an effective in-tree (IT) structure, in which the underlying cluster\nstructure in the dataset is well revealed. Although there are some edges in the\nIT structure requiring to be removed, such undesired edges are generally\ndistinguishable from other edges and thus are easy to be determined. For\ninstance, when the IT structures for the 2-dimensional (2D) datasets are\ngraphically presented, those undesired edges can be easily spotted and\ninteractively determined. However, in practice, there are many datasets that do\nnot lie in the 2D Euclidean space, thus their IT structures cannot be\ngraphically presented. But if we can effectively map those IT structures into a\nvisualized space in which the salient features of those undesired edges are\npreserved, then the undesired edges in the IT structures can still be visually\ndetermined in a visualization environment. Previously, this purpose was reached\nby our method called IT-map. The outstanding advantage of IT-map is that\nclusters can still be found even with the so-called crowding problem in the\nembedding.\n  In this paper, we propose another method, called IT-Dendrogram, to achieve\nthe same goal through an effective combination of the IT structure and the\nsingle link hierarchical clustering (SLHC) method. Like IT-map, IT-Dendrogram\ncan also effectively represent the IT structures in a visualization\nenvironment, whereas using another form, called the Dendrogram. IT-Dendrogram\ncan serve as another visualization method to determine the undesired edges in\nthe IT structures and thus benefit the IT-based clustering analysis. This was\ndemonstrated on several datasets with different shapes, dimensions, and\nattributes. Unlike IT-map, IT-Dendrogram can always avoid the crowding problem,\nwhich could help users make more reliable cluster analysis in certain problems.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 14:22:13 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Qiu", "Teng", ""], ["Li", "Yongjie", ""]]}, {"id": "1507.08271", "submitter": "Guy Lever Dr", "authors": "Thomas Furmston and Guy Lever", "title": "A Gauss-Newton Method for Markov Decision Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Newton methods are a standard optimization tool which aim to\nmaintain the benefits of Newton's method, such as a fast rate of convergence,\nwhilst alleviating its drawbacks, such as computationally expensive calculation\nor estimation of the inverse Hessian. In this work we investigate approximate\nNewton methods for policy optimization in Markov Decision Processes (MDPs). We\nfirst analyse the structure of the Hessian of the objective function for MDPs.\nWe show that, like the gradient, the Hessian exhibits useful structure in the\ncontext of MDPs and we use this analysis to motivate two Gauss-Newton Methods\nfor MDPs. Like the Gauss-Newton method for non-linear least squares, these\nmethods involve approximating the Hessian by ignoring certain terms in the\nHessian which are difficult to estimate. The approximate Hessians possess\ndesirable properties, such as negative definiteness, and we demonstrate several\nimportant performance guarantees including guaranteed ascent directions,\ninvariance to affine transformation of the parameter space, and convergence\nguarantees. We finally provide a unifying perspective of key policy search\nalgorithms, demonstrating that our second Gauss-Newton algorithm is closely\nrelated to both the EM-algorithm and natural gradient ascent applied to MDPs,\nbut performs significantly better in practice on a range of challenging\ndomains.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 19:37:24 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2015 19:08:37 GMT"}, {"version": "v3", "created": "Tue, 4 Aug 2015 17:33:39 GMT"}, {"version": "v4", "created": "Thu, 6 Aug 2015 14:02:01 GMT"}], "update_date": "2015-08-07", "authors_parsed": [["Furmston", "Thomas", ""], ["Lever", "Guy", ""]]}, {"id": "1507.08272", "submitter": "Serafeim Perdikis", "authors": "Serafeim Perdikis and Robert Leeb and Ricardo Chavarriaga and Jos\\'e\n  del R. Mill\\'an", "title": "Context-aware learning for generative models", "comments": "Final version published in IEEE Transactions on Neural Networks and\n  Learning systems", "journal-ref": null, "doi": "10.1109/TNNLS.2020.3011671", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies the class of algorithms for learning with side-information\nthat emerge by extending generative models with embedded context-related\nvariables. Using finite mixture models (FMM) as the prototypical Bayesian\nnetwork, we show that maximum-likelihood estimation (MLE) of parameters through\nexpectation-maximization (EM) improves over the regular unsupervised case and\ncan approach the performances of supervised learning, despite the absence of\nany explicit ground truth data labeling. By direct application of the missing\ninformation principle (MIP), the algorithms' performances are proven to range\nbetween the conventional supervised and unsupervised MLE extremities\nproportionally to the information content of the contextual assistance\nprovided. The acquired benefits regard higher estimation precision, smaller\nstandard errors, faster convergence rates and improved classification accuracy\nor regression fitness shown in various scenarios, while also highlighting\nimportant properties and differences among the outlined situations.\nApplicability is showcased with three real-world unsupervised classification\nscenarios employing Gaussian Mixture Models. Importantly, we exemplify the\nnatural extension of this methodology to any type of generative model by\nderiving an equivalent context-aware algorithm for variational autoencoders\n(VAs), thus broadening the spectrum of applicability to unsupervised deep\nlearning with artificial neural networks. The latter is contrasted with a\nneural-symbolic algorithm exploiting side-information.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 19:54:54 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 16:38:52 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Perdikis", "Serafeim", ""], ["Leeb", "Robert", ""], ["Chavarriaga", "Ricardo", ""], ["Mill\u00e1n", "Jos\u00e9 del R.", ""]]}, {"id": "1507.08396", "submitter": "Shuangyin Li", "authors": "Shuangyin Li, Jiefei Li, Guan Huang, Ruiyang Tan, and Rong Pan", "title": "Tag-Weighted Topic Model For Large-scale Semi-Structured Documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To date, there have been massive Semi-Structured Documents (SSDs) during the\nevolution of the Internet. These SSDs contain both unstructured features (e.g.,\nplain text) and metadata (e.g., tags). Most previous works focused on modeling\nthe unstructured text, and recently, some other methods have been proposed to\nmodel the unstructured text with specific tags. To build a general model for\nSSDs remains an important problem in terms of both model fitness and\nefficiency. We propose a novel method to model the SSDs by a so-called\nTag-Weighted Topic Model (TWTM). TWTM is a framework that leverages both the\ntags and words information, not only to learn the document-topic and topic-word\ndistributions, but also to infer the tag-topic distributions for text mining\ntasks. We present an efficient variational inference method with an EM\nalgorithm for estimating the model parameters. Meanwhile, we propose three\nlarge-scale solutions for our model under the MapReduce distributed computing\nplatform for modeling large-scale SSDs. The experimental results show the\neffectiveness, efficiency and the robustness by comparing our model with the\nstate-of-the-art methods in document modeling, tags prediction and text\nclassification. We also show the performance of the three distributed solutions\nin terms of time and accuracy on document modeling.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 06:44:37 GMT"}], "update_date": "2015-07-31", "authors_parsed": [["Li", "Shuangyin", ""], ["Li", "Jiefei", ""], ["Huang", "Guan", ""], ["Tan", "Ruiyang", ""], ["Pan", "Rong", ""]]}, {"id": "1507.08566", "submitter": "Vinay Chakravarthi Gogineni", "authors": "Vinay Chakravarthi Gogineni and Mrityunjoy Chakraborty", "title": "Diffusion Adaptation Over Clustered Multitask Networks Based on the\n  Affine Projection Algorithm", "comments": "Under Communication. arXiv admin note: substantial text overlap with\n  arXiv:1311.4894 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SY math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed adaptive networks achieve better estimation performance by\nexploiting temporal and as well spatial diversity while consuming few\nresources. Recent works have studied the single task distributed estimation\nproblem, in which the nodes estimate a single optimum parameter vector\ncollaboratively. However, there are many important applications where the\nmultiple vectors have to estimated simultaneously, in a collaborative manner.\nThis paper presents multi-task diffusion strategies based on the Affine\nProjection Algorithm (APA), usage of APA makes the algorithm robust against the\ncorrelated input. The performance analysis of the proposed multi-task diffusion\nAPA algorithm is studied in mean and mean square sense. And also a modified\nmulti-task diffusion strategy is proposed that improves the performance in\nterms of convergence rate and steady state EMSE as well. Simulations are\nconducted to verify the analytical results.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 09:27:38 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2015 13:33:33 GMT"}, {"version": "v3", "created": "Wed, 23 Sep 2015 15:12:47 GMT"}, {"version": "v4", "created": "Thu, 1 Oct 2015 09:23:55 GMT"}], "update_date": "2015-10-02", "authors_parsed": [["Gogineni", "Vinay Chakravarthi", ""], ["Chakraborty", "Mrityunjoy", ""]]}, {"id": "1507.08577", "submitter": "Luca Martino", "authors": "L. Martino, V. Elvira, D. Luengo, J. Corander, F. Louzada", "title": "Orthogonal parallel MCMC methods for sampling and optimization", "comments": null, "journal-ref": "Digital Signal Processing Volume 58, Pages: 64-84, 2016", "doi": "10.1016/j.dsp.2016.07.013", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo (MC) methods are widely used for Bayesian inference and\noptimization in statistics, signal processing and machine learning. A\nwell-known class of MC methods are Markov Chain Monte Carlo (MCMC) algorithms.\nIn order to foster better exploration of the state space, specially in\nhigh-dimensional applications, several schemes employing multiple parallel MCMC\nchains have been recently introduced. In this work, we describe a novel\nparallel interacting MCMC scheme, called {\\it orthogonal MCMC} (O-MCMC), where\na set of \"vertical\" parallel MCMC chains share information using some\n\"horizontal\" MCMC techniques working on the entire population of current\nstates. More specifically, the vertical chains are led by random-walk\nproposals, whereas the horizontal MCMC techniques employ independent proposals,\nthus allowing an efficient combination of global exploration and local\napproximation. The interaction is contained in these horizontal iterations.\nWithin the analysis of different implementations of O-MCMC, novel schemes in\norder to reduce the overall computational cost of parallel multiple try\nMetropolis (MTM) chains are also presented. Furthermore, a modified version of\nO-MCMC for optimization is provided by considering parallel simulated annealing\n(SA) algorithms. Numerical results show the advantages of the proposed sampling\nscheme in terms of efficiency in the estimation, as well as robustness in terms\nof independence with respect to initial values and the choice of the\nparameters.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 16:47:33 GMT"}, {"version": "v2", "created": "Sun, 25 Sep 2016 11:37:28 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Martino", "L.", ""], ["Elvira", "V.", ""], ["Luengo", "D.", ""], ["Corander", "J.", ""], ["Louzada", "F.", ""]]}, {"id": "1507.08726", "submitter": "Jelena Bradic", "authors": "Jelena Bradic", "title": "Robustness in sparse linear models: relative efficiency based on robust\n  approximate message passing", "comments": "49 pages, 10 figures", "journal-ref": "Electronic Journal of Statistics, Volume 10, Number 2 (2016),\n  3894-3944", "doi": null, "report-no": null, "categories": "math.ST cs.AI cs.LG stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding efficiency in high dimensional linear models is a longstanding\nproblem of interest. Classical work with smaller dimensional problems dating\nback to Huber and Bickel has illustrated the benefits of efficient loss\nfunctions. When the number of parameters $p$ is of the same order as the sample\nsize $n$, $p \\approx n$, an efficiency pattern different from the one of Huber\nwas recently established. In this work, we consider the effects of model\nselection on the estimation efficiency of penalized methods. In particular, we\nexplore whether sparsity, results in new efficiency patterns when $p > n$. In\nthe interest of deriving the asymptotic mean squared error for regularized\nM-estimators, we use the powerful framework of approximate message passing. We\npropose a novel, robust and sparse approximate message passing algorithm\n(RAMP), that is adaptive to the error distribution. Our algorithm includes many\nnon-quadratic and non-differentiable loss functions. We derive its asymptotic\nmean squared error and show its convergence, while allowing $p, n, s \\to\n\\infty$, with $n/p \\in (0,1)$ and $n/s \\in (1,\\infty)$. We identify new\npatterns of relative efficiency regarding a number of penalized $M$ estimators,\nwhen $p$ is much larger than $n$. We show that the classical information bound\nis no longer reachable, even for light--tailed error distributions. We show\nthat the penalized least absolute deviation estimator dominates the penalized\nleast square estimator, in cases of heavy--tailed distributions. We observe\nthis pattern for all choices of the number of non-zero parameters $s$, both $s\n\\leq n$ and $s \\approx n$. In non-penalized problems where $s =p \\approx n$,\nthe opposite regime holds. Therefore, we discover that the presence of model\nselection significantly changes the efficiency patterns.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2015 01:31:24 GMT"}, {"version": "v2", "created": "Sat, 24 Dec 2016 00:27:46 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Bradic", "Jelena", ""]]}, {"id": "1507.08752", "submitter": "Ohad Shamir", "authors": "Ohad Shamir", "title": "An Optimal Algorithm for Bandit and Zero-Order Convex Optimization with\n  Two-Point Feedback", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the closely related problems of bandit convex optimization with\ntwo-point feedback, and zero-order stochastic convex optimization with two\nfunction evaluations per round. We provide a simple algorithm and analysis\nwhich is optimal for convex Lipschitz functions. This improves on\n\\cite{dujww13}, which only provides an optimal result for smooth functions;\nMoreover, the algorithm and analysis are simpler, and readily extend to\nnon-Euclidean problems. The algorithm is based on a small but surprisingly\npowerful modification of the gradient estimator.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2015 04:48:58 GMT"}], "update_date": "2015-08-03", "authors_parsed": [["Shamir", "Ohad", ""]]}, {"id": "1507.08788", "submitter": "Ohad Shamir", "authors": "Ohad Shamir", "title": "Fast Stochastic Algorithms for SVD and PCA: Convergence Properties and\n  Convexity", "comments": "35 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the convergence properties of the VR-PCA algorithm introduced by\n\\cite{shamir2015stochastic} for fast computation of leading singular vectors.\nWe prove several new results, including a formal analysis of a block version of\nthe algorithm, and convergence from random initialization. We also make a few\nobservations of independent interest, such as how pre-initializing with just a\nsingle exact power iteration can significantly improve the runtime of\nstochastic methods, and what are the convexity and non-convexity properties of\nthe underlying optimization problem.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2015 07:57:18 GMT"}], "update_date": "2015-08-03", "authors_parsed": [["Shamir", "Ohad", ""]]}]