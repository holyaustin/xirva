[{"id": "1602.00061", "submitter": "Weihao Kong", "authors": "Weihao Kong and Gregory Valiant", "title": "Spectrum Estimation from Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of approximating the set of eigenvalues of the\ncovariance matrix of a multivariate distribution (equivalently, the problem of\napproximating the \"population spectrum\"), given access to samples drawn from\nthe distribution. The eigenvalues of the covariance of a distribution contain\nbasic information about the distribution, including the presence or lack of\nstructure in the distribution, the effective dimensionality of the\ndistribution, and the applicability of higher-level machine learning and\nmultivariate statistical tools. We consider this fundamental recovery problem\nin the regime where the number of samples is comparable, or even sublinear in\nthe dimensionality of the distribution in question. First, we propose a\ntheoretically optimal and computationally efficient algorithm for recovering\nthe moments of the eigenvalues of the population covariance matrix. We then\nleverage this accurate moment recovery, via a Wasserstein distance argument, to\nshow that the vector of eigenvalues can be accurately recovered. We provide\nfinite--sample bounds on the expected error of the recovered eigenvalues, which\nimply that our estimator is asymptotically consistent as the dimensionality of\nthe distribution and sample size tend towards infinity, even in the sublinear\nsample regime where the ratio of the sample size to the dimensionality tends to\nzero. In addition to our theoretical results, we show that our approach\nperforms well in practice for a broad range of distributions and sample sizes.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2016 02:28:39 GMT"}, {"version": "v2", "created": "Fri, 27 May 2016 18:43:05 GMT"}, {"version": "v3", "created": "Tue, 15 Nov 2016 19:10:15 GMT"}, {"version": "v4", "created": "Wed, 21 Dec 2016 17:46:25 GMT"}, {"version": "v5", "created": "Sun, 16 Jul 2017 04:50:25 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Kong", "Weihao", ""], ["Valiant", "Gregory", ""]]}, {"id": "1602.00078", "submitter": "Hau-tieng Wu", "authors": "Ronen Talmon and Hau-tieng Wu", "title": "Latent common manifold learning with alternating diffusion: analysis and\n  applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cs.DS math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of data sets arising from multiple sensors has drawn significant\nresearch attention over the years. Traditional methods, including kernel-based\nmethods, are typically incapable of capturing nonlinear geometric structures.\nWe introduce a latent common manifold model underlying multiple sensor\nobservations for the purpose of multimodal data fusion. A method based on\nalternating diffusion is presented and analyzed; we provide theoretical\nanalysis of the method under the latent common manifold model. To exemplify the\npower of the proposed framework, experimental results in several applications\nare reported.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2016 06:14:24 GMT"}, {"version": "v2", "created": "Thu, 3 Aug 2017 00:12:13 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Talmon", "Ronen", ""], ["Wu", "Hau-tieng", ""]]}, {"id": "1602.00133", "submitter": "Zhao Shen-Yi", "authors": "Shen-Yi Zhao, Ru Xiang, Ying-Hao Shi, Peng Gao, Wu-Jun Li", "title": "SCOPE: Scalable Composite Optimization for Learning on Spark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning models, such as logistic regression~(LR) and support\nvector machine~(SVM), can be formulated as composite optimization problems.\nRecently, many distributed stochastic optimization~(DSO) methods have been\nproposed to solve the large-scale composite optimization problems, which have\nshown better performance than traditional batch methods. However, most of these\nDSO methods are not scalable enough. In this paper, we propose a novel DSO\nmethod, called \\underline{s}calable \\underline{c}omposite\n\\underline{op}timization for l\\underline{e}arning~({SCOPE}), and implement it\non the fault-tolerant distributed platform \\mbox{Spark}. SCOPE is both\ncomputation-efficient and communication-efficient. Theoretical analysis shows\nthat SCOPE is convergent with linear convergence rate when the objective\nfunction is convex. Furthermore, empirical results on real datasets show that\nSCOPE can outperform other state-of-the-art distributed learning methods on\nSpark, including both batch learning methods and DSO methods.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2016 16:11:53 GMT"}, {"version": "v2", "created": "Sun, 7 Feb 2016 07:07:56 GMT"}, {"version": "v3", "created": "Wed, 1 Jun 2016 07:50:39 GMT"}, {"version": "v4", "created": "Thu, 2 Jun 2016 07:01:25 GMT"}, {"version": "v5", "created": "Sun, 11 Dec 2016 16:10:37 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Zhao", "Shen-Yi", ""], ["Xiang", "Ru", ""], ["Shi", "Ying-Hao", ""], ["Gao", "Peng", ""], ["Li", "Wu-Jun", ""]]}, {"id": "1602.00203", "submitter": "Angshul Majumdar Dr.", "authors": "Snigdha Tariyal, Angshul Majumdar, Richa Singh and Mayank Vatsa", "title": "Greedy Deep Dictionary Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a new deep learning tool called deep dictionary\nlearning. Multi-level dictionaries are learnt in a greedy fashion, one layer at\na time. This requires solving a simple (shallow) dictionary learning problem,\nthe solution to this is well known. We apply the proposed technique on some\nbenchmark deep learning datasets. We compare our results with other deep\nlearning tools like stacked autoencoder and deep belief network; and state of\nthe art supervised dictionary learning tools like discriminative KSVD and label\nconsistent KSVD. Our method yields better results than all.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2016 06:12:58 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Tariyal", "Snigdha", ""], ["Majumdar", "Angshul", ""], ["Singh", "Richa", ""], ["Vatsa", "Mayank", ""]]}, {"id": "1602.00214", "submitter": "Gustau Camps-Valls", "authors": "Valero Laparra, Jesus Malo, Gustau Camps-Valls", "title": "Dimensionality Reduction via Regression in Hyperspectral Imagery", "comments": "12 pages, 6 figures, 62 references", "journal-ref": "J. Sel. Topics Signal Processing 9(6): 1026-1036 (2015)", "doi": "10.1109/JSTSP.2015.2417833", "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new unsupervised method for dimensionality reduction\nvia regression (DRR). The algorithm belongs to the family of invertible\ntransforms that generalize Principal Component Analysis (PCA) by using\ncurvilinear instead of linear features. DRR identifies the nonlinear features\nthrough multivariate regression to ensure the reduction in redundancy between\nhe PCA coefficients, the reduction of the variance of the scores, and the\nreduction in the reconstruction error. More importantly, unlike other nonlinear\ndimensionality reduction methods, the invertibility, volume-preservation, and\nstraightforward out-of-sample extension, makes DRR interpretable and easy to\napply. The properties of DRR enable learning a more broader class of data\nmanifolds than the recently proposed Non-linear Principal Components Analysis\n(NLPCA) and Principal Polynomial Analysis (PPA). We illustrate the performance\nof the representation in reducing the dimensionality of remote sensing data. In\nparticular, we tackle two common problems: processing very high dimensional\nspectral information such as in hyperspectral image sounding data, and dealing\nwith spatial-spectral image patches of multispectral images. Both settings pose\ncollinearity and ill-determination problems. Evaluation of the expressive power\nof the features is assessed in terms of truncation error, estimating\natmospheric variables, and surface land cover classification error. Results\nshow that DRR outperforms linear PCA and recently proposed invertible\nextensions based on neural networks (NLPCA) and univariate regressions (PPA).\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2016 09:34:58 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Laparra", "Valero", ""], ["Malo", "Jesus", ""], ["Camps-Valls", "Gustau", ""]]}, {"id": "1602.00216", "submitter": "Jean Golay", "authors": "Jean Golay, Michael Leuenberger, Mikhail Kanevski", "title": "Feature Selection for Regression Problems Based on the Morisita\n  Estimator of Intrinsic Dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data acquisition, storage and management have been improved, while the key\nfactors of many phenomena are not well known. Consequently, irrelevant and\nredundant features artificially increase the size of datasets, which\ncomplicates learning tasks, such as regression. To address this problem,\nfeature selection methods have been proposed. This paper introduces a new\nsupervised filter based on the Morisita estimator of intrinsic dimension. It\ncan identify relevant features and distinguish between redundant and irrelevant\ninformation. Besides, it offers a clear graphical representation of the\nresults, and it can be easily implemented in different programming languages.\nComprehensive numerical experiments are conducted using simulated datasets\ncharacterized by different levels of complexity, sample size and noise. The\nsuggested algorithm is also successfully tested on a selection of real world\napplications and compared with RReliefF using extreme learning machine. In\naddition, a new measure of feature relevance is presented and discussed.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2016 09:59:27 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2016 17:03:26 GMT"}, {"version": "v3", "created": "Mon, 7 Mar 2016 20:40:06 GMT"}, {"version": "v4", "created": "Fri, 11 Mar 2016 14:39:24 GMT"}, {"version": "v5", "created": "Fri, 8 Apr 2016 18:37:17 GMT"}, {"version": "v6", "created": "Tue, 4 Apr 2017 13:28:48 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Golay", "Jean", ""], ["Leuenberger", "Michael", ""], ["Kanevski", "Mikhail", ""]]}, {"id": "1602.00217", "submitter": "Gustau Camps-Valls", "authors": "Valero Laparra, Juan Guti\\'errez, Gustavo Camps-Valls, Jes\\'us Malo", "title": "Image Denoising with Kernels based on Natural Image Relations", "comments": null, "journal-ref": "Journal of Machine Learning Research 11: 873-903 (2010)", "doi": "10.1145/1756006.1756035", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A successful class of image denoising methods is based on Bayesian approaches\nworking in wavelet representations. However, analytical estimates can be\nobtained only for particular combinations of analytical models of signal and\nnoise, thus precluding its straightforward extension to deal with other\narbitrary noise sources. In this paper, we propose an alternative non-explicit\nway to take into account the relations among natural image wavelet coefficients\nfor denoising: we use support vector regression (SVR) in the wavelet domain to\nenforce these relations in the estimated signal. Since relations among the\ncoefficients are specific to the signal, the regularization property of SVR is\nexploited to remove the noise, which does not share this feature. The specific\nsignal relations are encoded in an anisotropic kernel obtained from mutual\ninformation measures computed on a representative image database. Training\nconsiders minimizing the Kullback-Leibler divergence (KLD) between the\nestimated and actual probability functions of signal and noise in order to\nenforce similarity. Due to its non-parametric nature, the method can eventually\ncope with different noise sources without the need of an explicit\nre-formulation, as it is strictly necessary under parametric Bayesian\nformalisms. Results under several noise levels and noise sources show that: (1)\nthe proposed method outperforms conventional wavelet methods that assume\ncoefficient independence, (2) it is similar to state-of-the-art methods that do\nexplicitly include these relations when the noise source is Gaussian, and (3)\nit gives better numerical and visual performance when more complex, realistic\nnoise sources are considered. Therefore, the proposed machine learning approach\ncan be seen as a more flexible (model-free) alternative to the explicit\ndescription of wavelet coefficient relations for image denoising.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2016 10:02:14 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Laparra", "Valero", ""], ["Guti\u00e9rrez", "Juan", ""], ["Camps-Valls", "Gustavo", ""], ["Malo", "Jes\u00fas", ""]]}, {"id": "1602.00221", "submitter": "Gustau Camps-Valls", "authors": "Valero Laparra, Sandra Jim\\'enez, Devis Tuia, Gustau Camps-Valls,\n  Jes\\'us Malo", "title": "Principal Polynomial Analysis", "comments": null, "journal-ref": "Int. J. Neural Syst. 24(7) (2014)", "doi": "10.1142/S0129065714400073", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new framework for manifold learning based on a sequence\nof principal polynomials that capture the possibly nonlinear nature of the\ndata. The proposed Principal Polynomial Analysis (PPA) generalizes PCA by\nmodeling the directions of maximal variance by means of curves, instead of\nstraight lines. Contrarily to previous approaches, PPA reduces to performing\nsimple univariate regressions, which makes it computationally feasible and\nrobust. Moreover, PPA shows a number of interesting analytical properties.\nFirst, PPA is a volume-preserving map, which in turn guarantees the existence\nof the inverse. Second, such an inverse can be obtained in closed form.\nInvertibility is an important advantage over other learning methods, because it\npermits to understand the identified features in the input domain where the\ndata has physical meaning. Moreover, it allows to evaluate the performance of\ndimensionality reduction in sensible (input-domain) units. Volume preservation\nalso allows an easy computation of information theoretic quantities, such as\nthe reduction in multi-information after the transform. Third, the analytical\nnature of PPA leads to a clear geometrical interpretation of the manifold: it\nallows the computation of Frenet-Serret frames (local features) and of\ngeneralized curvatures at any point of the space. And fourth, the analytical\nJacobian allows the computation of the metric induced by the data, thus\ngeneralizing the Mahalanobis distance. These properties are demonstrated\ntheoretically and illustrated experimentally. The performance of PPA is\nevaluated in dimensionality and redundancy reduction, in both synthetic and\nreal datasets from the UCI repository.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2016 10:46:44 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Laparra", "Valero", ""], ["Jim\u00e9nez", "Sandra", ""], ["Tuia", "Devis", ""], ["Camps-Valls", "Gustau", ""], ["Malo", "Jes\u00fas", ""]]}, {"id": "1602.00223", "submitter": "Luo Luo", "authors": "Luo Luo, Zihao Chen, Zhihua Zhang, Wu-Jun Li", "title": "A Proximal Stochastic Quasi-Newton Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we discuss the problem of minimizing the sum of two convex\nfunctions: a smooth function plus a non-smooth function. Further, the smooth\npart can be expressed by the average of a large number of smooth component\nfunctions, and the non-smooth part is equipped with a simple proximal mapping.\nWe propose a proximal stochastic second-order method, which is efficient and\nscalable. It incorporates the Hessian in the smooth part of the function and\nexploits multistage scheme to reduce the variance of the stochastic gradient.\nWe prove that our method can achieve linear rate of convergence.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2016 10:56:29 GMT"}, {"version": "v2", "created": "Wed, 16 Nov 2016 05:03:14 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Luo", "Luo", ""], ["Chen", "Zihao", ""], ["Zhang", "Zhihua", ""], ["Li", "Wu-Jun", ""]]}, {"id": "1602.00229", "submitter": "Gustau Camps-Valls", "authors": "Valero Laparra, Gustavo Camps-Valls, Jes\\'us Malo", "title": "Iterative Gaussianization: from ICA to Random Rotations", "comments": null, "journal-ref": "Neural Networks, IEEE Transactions on, vol.22, no.4, pp.537-549,\n  April 2011", "doi": "10.1109/TNN.2011.2106511", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most signal processing problems involve the challenging task of\nmultidimensional probability density function (PDF) estimation. In this work,\nwe propose a solution to this problem by using a family of Rotation-based\nIterative Gaussianization (RBIG) transforms. The general framework consists of\nthe sequential application of a univariate marginal Gaussianization transform\nfollowed by an orthonormal transform. The proposed procedure looks for\ndifferentiable transforms to a known PDF so that the unknown PDF can be\nestimated at any point of the original domain. In particular, we aim at a zero\nmean unit covariance Gaussian for convenience. RBIG is formally similar to\nclassical iterative Projection Pursuit (PP) algorithms. However, we show that,\nunlike in PP methods, the particular class of rotations used has no special\nqualitative relevance in this context, since looking for interestingness is not\na critical issue for PDF estimation. The key difference is that our approach\nfocuses on the univariate part (marginal Gaussianization) of the problem rather\nthan on the multivariate part (rotation). This difference implies that one may\nselect the most convenient rotation suited to each practical application. The\ndifferentiability, invertibility and convergence of RBIG are theoretically and\nexperimentally analyzed. Relation to other methods, such as Radial\nGaussianization (RG), one-class support vector domain description (SVDD), and\ndeep neural networks (DNN) is also pointed out. The practical performance of\nRBIG is successfully illustrated in a number of multidimensional problems such\nas image synthesis, classification, denoising, and multi-information\nestimation.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2016 11:30:50 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Laparra", "Valero", ""], ["Camps-Valls", "Gustavo", ""], ["Malo", "Jes\u00fas", ""]]}, {"id": "1602.00236", "submitter": "Gustau Camps-Valls", "authors": "Valero Laparra, Sandra Jim\\'enez, Gustavo Camps-Valls, Jes\\'us Malo", "title": "Nonlinearities and Adaptation of Color Vision from Sequential Principal\n  Curves Analysis", "comments": null, "journal-ref": "Neural Comput. 2012 Oct;24(10):2751-88", "doi": "10.1162/NECO_a_00342", "report-no": null, "categories": "stat.ML q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mechanisms of human color vision are characterized by two phenomenological\naspects: the system is nonlinear and adaptive to changing environments.\nConventional attempts to derive these features from statistics use separate\narguments for each aspect. The few statistical approaches that do consider both\nphenomena simultaneously follow parametric formulations based on empirical\nmodels. Therefore, it may be argued that the behavior does not come directly\nfrom the color statistics but from the convenient functional form adopted. In\naddition, many times the whole statistical analysis is based on simplified\ndatabases that disregard relevant physical effects in the input signal, as for\ninstance by assuming flat Lambertian surfaces. Here we address the simultaneous\nstatistical explanation of (i) the nonlinear behavior of achromatic and\nchromatic mechanisms in a fixed adaptation state, and (ii) the change of such\nbehavior. Both phenomena emerge directly from the samples through a single\ndata-driven method: the Sequential Principal Curves Analysis (SPCA) with local\nmetric. SPCA is a new manifold learning technique to derive a set of sensors\nadapted to the manifold using different optimality criteria. A new database of\ncolorimetrically calibrated images of natural objects under these illuminants\nwas collected. The results obtained by applying SPCA show that the\npsychophysical behavior on color discrimination thresholds, discount of the\nilluminant and corresponding pairs in asymmetric color matching, emerge\ndirectly from realistic data regularities assuming no a priori functional form.\nThese results provide stronger evidence for the hypothesis of a statistically\ndriven organization of color sensors. Moreover, the obtained results suggest\nthat color perception at this low abstraction level may be guided by an error\nminimization strategy rather than by the information maximization principle.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2016 12:33:18 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Laparra", "Valero", ""], ["Jim\u00e9nez", "Sandra", ""], ["Camps-Valls", "Gustavo", ""], ["Malo", "Jes\u00fas", ""]]}, {"id": "1602.00260", "submitter": "M{\\aa}ns Magnusson", "authors": "M{\\aa}ns Magnusson, Leif Jonsson and Mattias Villani", "title": "DOLDA - a regularized supervised topic model for high-dimensional\n  multi-class regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating user interpretable multi-class predictions in data rich\nenvironments with many classes and explanatory covariates is a daunting task.\nWe introduce Diagonal Orthant Latent Dirichlet Allocation (DOLDA), a supervised\ntopic model for multi-class classification that can handle both many classes as\nwell as many covariates. To handle many classes we use the recently proposed\nDiagonal Orthant (DO) probit model (Johndrow et al., 2013) together with an\nefficient Horseshoe prior for variable selection/shrinkage (Carvalho et al.,\n2010). We propose a computationally efficient parallel Gibbs sampler for the\nnew model. An important advantage of DOLDA is that learned topics are directly\nconnected to individual classes without the need for a reference class. We\nevaluate the model's predictive accuracy on two datasets and demonstrate\nDOLDA's advantage in interpreting the generated predictions.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2016 15:33:10 GMT"}, {"version": "v2", "created": "Thu, 20 Oct 2016 14:52:53 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Magnusson", "M\u00e5ns", ""], ["Jonsson", "Leif", ""], ["Villani", "Mattias", ""]]}, {"id": "1602.00287", "submitter": "Kirthevasan Kandasamy", "authors": "Kirthevasan Kandasamy, Yaoliang Yu", "title": "Additive Approximations in High Dimensional Nonparametric Regression via\n  the SALSA", "comments": "International Conference on Machine Learning (ICML) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High dimensional nonparametric regression is an inherently difficult problem\nwith known lower bounds depending exponentially in dimension. A popular\nstrategy to alleviate this curse of dimensionality has been to use additive\nmodels of \\emph{first order}, which model the regression function as a sum of\nindependent functions on each dimension. Though useful in controlling the\nvariance of the estimate, such models are often too restrictive in practical\nsettings. Between non-additive models which often have large variance and first\norder additive models which have large bias, there has been little work to\nexploit the trade-off in the middle via additive models of intermediate order.\nIn this work, we propose SALSA, which bridges this gap by allowing interactions\nbetween variables, but controls model capacity by limiting the order of\ninteractions. SALSA minimises the residual sum of squares with squared RKHS\nnorm penalties. Algorithmically, it can be viewed as Kernel Ridge Regression\nwith an additive kernel. When the regression function is additive, the excess\nrisk is only polynomial in dimension. Using the Girard-Newton formulae, we\nefficiently sum over a combinatorial number of terms in the additive expansion.\nVia a comparison on $15$ real datasets, we show that our method is competitive\nagainst $21$ other alternatives.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2016 17:32:51 GMT"}, {"version": "v2", "created": "Sun, 20 Mar 2016 23:11:13 GMT"}, {"version": "v3", "created": "Tue, 24 May 2016 23:15:24 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Kandasamy", "Kirthevasan", ""], ["Yu", "Yaoliang", ""]]}, {"id": "1602.00354", "submitter": "Gautam Dasarathy", "authors": "Gautam Dasarathy, Aarti Singh, Maria-Florina Balcan, Jong Hyuk Park", "title": "Active Learning Algorithms for Graphical Model Selection", "comments": "26 pages, 3 figures. Preliminary version to appear in AI & Statistics\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of learning the structure of a high dimensional graphical model\nfrom data has received considerable attention in recent years. In many\napplications such as sensor networks and proteomics it is often expensive to\nobtain samples from all the variables involved simultaneously. For instance,\nthis might involve the synchronization of a large number of sensors or the\ntagging of a large number of proteins. To address this important issue, we\ninitiate the study of a novel graphical model selection problem, where the goal\nis to optimize the total number of scalar samples obtained by allowing the\ncollection of samples from only subsets of the variables. We propose a general\nparadigm for graphical model selection where feedback is used to guide the\nsampling to high degree vertices, while obtaining only few samples from the\nones with the low degrees. We instantiate this framework with two specific\nactive learning algorithms, one of which makes mild assumptions but is\ncomputationally expensive, while the other is more computationally efficient\nbut requires stronger (nevertheless standard) assumptions. Whereas the sample\ncomplexity of passive algorithms is typically a function of the maximum degree\nof the graph, we show that the sample complexity of our algorithms is provable\nsmaller and that it depends on a novel local complexity measure that is akin to\nthe average degree of the graph. We finally demonstrate the efficacy of our\nframework via simulations.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 01:04:34 GMT"}, {"version": "v2", "created": "Thu, 7 Apr 2016 05:49:56 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Dasarathy", "Gautam", ""], ["Singh", "Aarti", ""], ["Balcan", "Maria-Florina", ""], ["Park", "Jong Hyuk", ""]]}, {"id": "1602.00355", "submitter": "Rafael Izbicki Rafael Izbicki", "authors": "Ann B. Lee, Rafael Izbicki", "title": "A Spectral Series Approach to High-Dimensional Nonparametric Regression", "comments": null, "journal-ref": "Electron. J. Statist. Volume 10, Number 1 (2016), 423-463", "doi": "10.1214/16-EJS1112", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key question in modern statistics is how to make fast and reliable\ninferences for complex, high-dimensional data. While there has been much\ninterest in sparse techniques, current methods do not generalize well to data\nwith nonlinear structure. In this work, we present an orthogonal series\nestimator for predictors that are complex aggregate objects, such as natural\nimages, galaxy spectra, trajectories, and movies. Our series approach ties\ntogether ideas from kernel machine learning, and Fourier methods. We expand the\nunknown regression on the data in terms of the eigenfunctions of a kernel-based\noperator, and we take advantage of orthogonality of the basis with respect to\nthe underlying data distribution, P, to speed up computations and tuning of\nparameters. If the kernel is appropriately chosen, then the eigenfunctions\nadapt to the intrinsic geometry and dimension of the data. We provide\ntheoretical guarantees for a radial kernel with varying bandwidth, and we\nrelate smoothness of the regression function with respect to P to sparsity in\nthe eigenbasis. Finally, using simulated and real-world data, we systematically\ncompare the performance of the spectral series approach with classical kernel\nsmoothing, k-nearest neighbors regression, kernel ridge regression, and\nstate-of-the-art manifold and local regression methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 01:20:29 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Lee", "Ann B.", ""], ["Izbicki", "Rafael", ""]]}, {"id": "1602.00357", "submitter": "Trang Pham", "authors": "Trang Pham, Truyen Tran, Dinh Phung and Svetha Venkatesh", "title": "DeepCare: A Deep Dynamic Memory Model for Predictive Medicine", "comments": "Accepted at JBI under the new name: \"Predicting healthcare\n  trajectories from medical records: A deep learning approach\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized predictive medicine necessitates the modeling of patient illness\nand care processes, which inherently have long-term temporal dependencies.\nHealthcare observations, recorded in electronic medical records, are episodic\nand irregular in time. We introduce DeepCare, an end-to-end deep dynamic neural\nnetwork that reads medical records, stores previous illness history, infers\ncurrent illness states and predicts future medical outcomes. At the data level,\nDeepCare represents care episodes as vectors in space, models patient health\nstate trajectories through explicit memory of historical records. Built on Long\nShort-Term Memory (LSTM), DeepCare introduces time parameterizations to handle\nirregular timed events by moderating the forgetting and consolidation of memory\ncells. DeepCare also incorporates medical interventions that change the course\nof illness and shape future medical risk. Moving up to the health state level,\nhistorical and present health states are then aggregated through multiscale\ntemporal pooling, before passing through a neural network that estimates future\noutcomes. We demonstrate the efficacy of DeepCare for disease progression\nmodeling, intervention recommendation, and future risk prediction. On two\nimportant cohorts with heavy social and economic burden -- diabetes and mental\nhealth -- the results show improved modeling and risk prediction accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 01:47:00 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 22:54:01 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Pham", "Trang", ""], ["Tran", "Truyen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1602.00360", "submitter": "Jordan Yoder", "authors": "Jordan Yoder and Carey E. Priebe", "title": "Semi-supervised K-means++", "comments": "16 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, practitioners initialize the {\\tt k-means} algorithm with\ncenters chosen uniformly at random. Randomized initialization with uneven\nweights ({\\tt k-means++}) has recently been used to improve the performance\nover this strategy in cost and run-time. We consider the k-means problem with\nsemi-supervised information, where some of the data are pre-labeled, and we\nseek to label the rest according to the minimum cost solution. By extending the\n{\\tt k-means++} algorithm and analysis to account for the labels, we derive an\nimproved theoretical bound on expected cost and observe improved performance in\nsimulated and real data examples. This analysis provides theoretical\njustification for a roughly linear semi-supervised clustering algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 01:49:23 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Yoder", "Jordan", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1602.00522", "submitter": "Benjamin Guedj", "authors": "Le Li and Benjamin Guedj and S\\'ebastien Loustau", "title": "A Quasi-Bayesian Perspective to Online Clustering", "comments": null, "journal-ref": "Electronic Journal of Statistics (2018), vol. 12(2), 3071--3113", "doi": "10.1214/18-EJS1479", "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  When faced with high frequency streams of data, clustering raises theoretical\nand algorithmic pitfalls. We introduce a new and adaptive online clustering\nalgorithm relying on a quasi-Bayesian approach, with a dynamic (i.e.,\ntime-dependent) estimation of the (unknown and changing) number of clusters. We\nprove that our approach is supported by minimax regret bounds. We also provide\nan RJMCMC-flavored implementation (called PACBO, see\nhttps://cran.r-project.org/web/packages/PACBO/index.html) for which we give a\nconvergence guarantee. Finally, numerical experiments illustrate the potential\nof our procedure.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 13:49:06 GMT"}, {"version": "v2", "created": "Sat, 8 Apr 2017 08:35:09 GMT"}, {"version": "v3", "created": "Fri, 25 May 2018 15:51:09 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Li", "Le", ""], ["Guedj", "Benjamin", ""], ["Loustau", "S\u00e9bastien", ""]]}, {"id": "1602.00542", "submitter": "Ramji Venkataramanan", "authors": "K. Pavan Srinath and Ramji Venkataramanan", "title": "Cluster-Seeking James-Stein Estimators", "comments": "Appeared in IEEE Transactions on Information Theory", "journal-ref": "IEEE Transactions on Information Theory, vol. 64, no. 2, pp.\n  853-874, February 2018", "doi": "10.1109/TIT.2017.2783543", "report-no": null, "categories": "cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of estimating a high-dimensional vector of\nparameters $\\boldsymbol{\\theta} \\in \\mathbb{R}^n$ from a noisy observation. The\nnoise vector is i.i.d. Gaussian with known variance. For a squared-error loss\nfunction, the James-Stein (JS) estimator is known to dominate the simple\nmaximum-likelihood (ML) estimator when the dimension $n$ exceeds two. The\nJS-estimator shrinks the observed vector towards the origin, and the risk\nreduction over the ML-estimator is greatest for $\\boldsymbol{\\theta}$ that lie\nclose to the origin. JS-estimators can be generalized to shrink the data\ntowards any target subspace. Such estimators also dominate the ML-estimator,\nbut the risk reduction is significant only when $\\boldsymbol{\\theta}$ lies\nclose to the subspace. This leads to the question: in the absence of prior\ninformation about $\\boldsymbol{\\theta}$, how do we design estimators that give\nsignificant risk reduction over the ML-estimator for a wide range of\n$\\boldsymbol{\\theta}$?\n  In this paper, we propose shrinkage estimators that attempt to infer the\nstructure of $\\boldsymbol{\\theta}$ from the observed data in order to construct\na good attracting subspace. In particular, the components of the observed\nvector are separated into clusters, and the elements in each cluster shrunk\ntowards a common attractor. The number of clusters and the attractor for each\ncluster are determined from the observed vector. We provide concentration\nresults for the squared-error loss and convergence results for the risk of the\nproposed estimators. The results show that the estimators give significant risk\nreduction over the ML-estimator for a wide range of $\\boldsymbol{\\theta}$,\nparticularly for large $n$. Simulation results are provided to support the\ntheoretical claims.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 14:37:20 GMT"}, {"version": "v2", "created": "Tue, 1 Mar 2016 14:09:44 GMT"}, {"version": "v3", "created": "Mon, 24 Jul 2017 17:19:46 GMT"}, {"version": "v4", "created": "Fri, 16 Mar 2018 10:52:10 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Srinath", "K. Pavan", ""], ["Venkataramanan", "Ramji", ""]]}, {"id": "1602.00734", "submitter": "Yen-Huan  Li", "authors": "Yen-Huan Li and Volkan Cevher", "title": "Learning Data Triage: Linear Decoding Works for Compressive MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard approach to compressive sampling considers recovering an unknown\ndeterministic signal with certain known structure, and designing the\nsub-sampling pattern and recovery algorithm based on the known structure. This\napproach requires looking for a good representation that reveals the signal\nstructure, and solving a non-smooth convex minimization problem (e.g., basis\npursuit). In this paper, another approach is considered: We learn a good\nsub-sampling pattern based on available training signals, without knowing the\nsignal structure in advance, and reconstruct an accordingly sub-sampled signal\nby computationally much cheaper linear reconstruction. We provide a theoretical\nguarantee on the recovery error, and show via experiments on real-world MRI\ndata the effectiveness of the proposed compressive MRI scheme.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 22:43:55 GMT"}], "update_date": "2016-02-03", "authors_parsed": [["Li", "Yen-Huan", ""], ["Cevher", "Volkan", ""]]}, {"id": "1602.00853", "submitter": "Le Riche", "authors": "Hossein Mohammadi (LIMOS, DEMO-ENSMSE), Rodolphe Le Riche (LIMOS,\n  DEMO-ENSMSE), Nicolas Durrande (DEMO-ENSMSE, LIMOS), Eric Touboul (LIMOS,\n  DEMO-ENSMSE), Xavier Bay (LIMOS, DEMO-ENSMSE)", "title": "An analytic comparison of regularization methods for Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian Processes (GPs) are a popular approach to predict the output of a\nparameterized experiment. They have many applications in the field of Computer\nExperiments, in particular to perform sensitivity analysis, adaptive design of\nexperiments and global optimization. Nearly all of the applications of GPs\nrequire the inversion of a covariance matrix that, in practice, is often\nill-conditioned. Regularization methodologies are then employed with\nconsequences on the GPs that need to be better understood.The two principal\nmethods to deal with ill-conditioned covariance matrices are i) pseudoinverse\nand ii) adding a positive constant to the diagonal (the so-called nugget\nregularization).The first part of this paper provides an algebraic comparison\nof PI and nugget regularizations. Redundant points, responsible for covariance\nmatrix singularity, are defined. It is proven that pseudoinverse\nregularization, contrarily to nugget regularization, averages the output values\nand makes the variance zero at redundant points. However, pseudoinverse and\nnugget regularizations become equivalent as the nugget value vanishes. A\nmeasure for data-model discrepancy is proposed which serves for choosing a\nregularization technique.In the second part of the paper, a distribution-wise\nGP is introduced that interpolates Gaussian distributions instead of data\npoints. Distribution-wise GP can be seen as an improved regularization method\nfor GPs.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 09:46:44 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 08:30:20 GMT"}, {"version": "v3", "created": "Fri, 5 May 2017 12:29:59 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Mohammadi", "Hossein", "", "LIMOS, DEMO-ENSMSE"], ["Riche", "Rodolphe Le", "", "LIMOS,\n  DEMO-ENSMSE"], ["Durrande", "Nicolas", "", "DEMO-ENSMSE, LIMOS"], ["Touboul", "Eric", "", "LIMOS,\n  DEMO-ENSMSE"], ["Bay", "Xavier", "", "LIMOS, DEMO-ENSMSE"]]}, {"id": "1602.00877", "submitter": "Jonathan Scarlett", "authors": "Jonathan Scarlett and Volkan Cevher", "title": "Partial Recovery Bounds for the Sparse Stochastic Block Model", "comments": "Accepted to ISIT 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.SI math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the information-theoretic limits of community\ndetection in the symmetric two-community stochastic block model, with\nintra-community and inter-community edge probabilities $\\frac{a}{n}$ and\n$\\frac{b}{n}$ respectively. We consider the sparse setting, in which $a$ and\n$b$ do not scale with $n$, and provide upper and lower bounds on the proportion\nof community labels recovered on average. We provide a numerical example for\nwhich the bounds are near-matching for moderate values of $a - b$, and matching\nin the limit as $a-b$ grows large.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 11:00:10 GMT"}, {"version": "v2", "created": "Mon, 4 Apr 2016 16:59:26 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Scarlett", "Jonathan", ""], ["Cevher", "Volkan", ""]]}, {"id": "1602.00904", "submitter": "Vangelis Oikonomou", "authors": "Vangelis P. Oikonomou, Georgios Liaros, Kostantinos Georgiadis,\n  Elisavet Chatzilari, Katerina Adam, Spiros Nikolopoulos and Ioannis\n  Kompatsiaris", "title": "Comparative evaluation of state-of-the-art algorithms for SSVEP-based\n  BCIs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain-computer interfaces (BCIs) have been gaining momentum in making\nhuman-computer interaction more natural, especially for people with\nneuro-muscular disabilities. Among the existing solutions the systems relying\non electroencephalograms (EEG) occupy the most prominent place due to their\nnon-invasiveness. However, the process of translating EEG signals into computer\ncommands is far from trivial, since it requires the optimization of many\ndifferent parameters that need to be tuned jointly. In this report, we focus on\nthe category of EEG-based BCIs that rely on Steady-State-Visual-Evoked\nPotentials (SSVEPs) and perform a comparative evaluation of the most promising\nalgorithms existing in the literature. More specifically, we define a set of\nalgorithms for each of the various different parameters composing a BCI system\n(i.e. filtering, artifact removal, feature extraction, feature selection and\nclassification) and study each parameter independently by keeping all other\nparameters fixed. The results obtained from this evaluation process are\nprovided together with a dataset consisting of the 256-channel, EEG signals of\n11 subjects, as well as a processing toolbox for reproducing the results and\nsupporting further experimentation. In this way, we manage to make available\nfor the community a state-of-the-art baseline for SSVEP-based BCIs that can be\nused as a basis for introducing novel methods and approaches.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 12:31:48 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2016 09:59:44 GMT"}], "update_date": "2016-02-04", "authors_parsed": [["Oikonomou", "Vangelis P.", ""], ["Liaros", "Georgios", ""], ["Georgiadis", "Kostantinos", ""], ["Chatzilari", "Elisavet", ""], ["Adam", "Katerina", ""], ["Nikolopoulos", "Spiros", ""], ["Kompatsiaris", "Ioannis", ""]]}, {"id": "1602.01052", "submitter": "Eric Schulz", "authors": "Eric Schulz, Quentin J. M. Huys, Dominik R. Bach, Maarten\n  Speekenbrink, Andreas Krause", "title": "Better safe than sorry: Risky function exploitation through safe\n  optimization", "comments": "6 pages, submitted to Cognitive Science Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploration-exploitation of functions, that is learning and optimizing a\nmapping between inputs and expected outputs, is ubiquitous to many real world\nsituations. These situations sometimes require us to avoid certain outcomes at\nall cost, for example because they are poisonous, harmful, or otherwise\ndangerous. We test participants' behavior in scenarios in which they have to\nfind the optimum of a function while at the same time avoid outputs below a\ncertain threshold. In two experiments, we find that Safe-Optimization, a\nGaussian Process-based exploration-exploitation algorithm, describes\nparticipants' behavior well and that participants seem to care firstly whether\na point is safe and then try to pick the optimal point from all such safe\npoints. This means that their trade-off between exploration and exploitation\ncan be seen as an intelligent, approximate, and homeostasis-driven strategy.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 19:27:54 GMT"}, {"version": "v2", "created": "Sat, 14 May 2016 15:42:05 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Schulz", "Eric", ""], ["Huys", "Quentin J. M.", ""], ["Bach", "Dominik R.", ""], ["Speekenbrink", "Maarten", ""], ["Krause", "Andreas", ""]]}, {"id": "1602.01064", "submitter": "Jan Hendrik Metzen", "authors": "Jan Hendrik Metzen", "title": "Minimum Regret Search for Single- and Multi-Task Optimization", "comments": "Final version for ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG cs.RO math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose minimum regret search (MRS), a novel acquisition function for\nBayesian optimization. MRS bears similarities with information-theoretic\napproaches such as entropy search (ES). However, while ES aims in each query at\nmaximizing the information gain with respect to the global maximum, MRS aims at\nminimizing the expected simple regret of its ultimate recommendation for the\noptimum. While empirically ES and MRS perform similar in most of the cases, MRS\nproduces fewer outliers with high simple regret than ES. We provide empirical\nresults both for a synthetic single-task optimization problem as well as for a\nsimulated multi-task robotic control problem.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 19:58:11 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2016 18:58:45 GMT"}, {"version": "v3", "created": "Tue, 24 May 2016 06:57:30 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Metzen", "Jan Hendrik", ""]]}, {"id": "1602.01107", "submitter": "Justin Cheng", "authors": "Justin Cheng, Lada A Adamic, Jon Kleinberg, Jure Leskovec", "title": "Do Cascades Recur?", "comments": "WWW 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cascades of information-sharing are a primary mechanism by which content\nreaches its audience on social media, and an active line of research has\nstudied how such cascades, which form as content is reshared from person to\nperson, develop and subside. In this paper, we perform a large-scale analysis\nof cascades on Facebook over significantly longer time scales, and find that a\nmore complex picture emerges, in which many large cascades recur, exhibiting\nmultiple bursts of popularity with periods of quiescence in between. We\ncharacterize recurrence by measuring the time elapsed between bursts, their\noverlap and proximity in the social network, and the diversity in the\ndemographics of individuals participating in each peak. We discover that\ncontent virality, as revealed by its initial popularity, is a main driver of\nrecurrence, with the availability of multiple copies of that content helping to\nspark new bursts. Still, beyond a certain popularity of content, the rate of\nrecurrence drops as cascades start exhausting the population of interested\nindividuals. We reproduce these observed patterns in a simple model of content\nrecurrence simulated on a real social network. Using only characteristics of a\ncascade's initial burst, we demonstrate strong performance in predicting\nwhether it will recur in the future.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 21:01:04 GMT"}], "update_date": "2016-02-04", "authors_parsed": [["Cheng", "Justin", ""], ["Adamic", "Lada A", ""], ["Kleinberg", "Jon", ""], ["Leskovec", "Jure", ""]]}, {"id": "1602.01120", "submitter": "Daniel McDonald", "authors": "Darren Homrighausen and Daniel J. McDonald", "title": "On the Nystr\\\"om and Column-Sampling Methods for the Approximate\n  Principal Components Analysis of Large Data Sets", "comments": "20 pages", "journal-ref": "Journal of Computational and Graphical Statistics, 25(2), 2016", "doi": "10.1080/10618600.2014.995799", "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we analyze approximate methods for undertaking a principal\ncomponents analysis (PCA) on large data sets. PCA is a classical dimension\nreduction method that involves the projection of the data onto the subspace\nspanned by the leading eigenvectors of the covariance matrix. This projection\ncan be used either for exploratory purposes or as an input for further\nanalysis, e.g. regression. If the data have billions of entries or more, the\ncomputational and storage requirements for saving and manipulating the design\nmatrix in fast memory is prohibitive. Recently, the Nystr\\\"om and\ncolumn-sampling methods have appeared in the numerical linear algebra community\nfor the randomized approximation of the singular value decomposition of large\nmatrices. However, their utility for statistical applications remains unclear.\nWe compare these approximations theoretically by bounding the distance between\nthe induced subspaces and the desired, but computationally infeasible, PCA\nsubspace. Additionally we show empirically, through simulations and a real data\nexample involving a corpus of emails, the trade-off of approximation accuracy\nand computational complexity.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 21:26:48 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Homrighausen", "Darren", ""], ["McDonald", "Daniel J.", ""]]}, {"id": "1602.01130", "submitter": "Christopher Harshaw", "authors": "Christopher R. Harshaw, Robert A. Bridges, Michael D. Iannacone, Joel\n  W. Reed, John R. Goodall", "title": "GraphPrints: Towards a Graph Analytic Method for Network Anomaly\n  Detection", "comments": "4 pages submitted to Cyber & Information Security Research Conference\n  2016, ACM", "journal-ref": null, "doi": "10.1145/1235", "report-no": null, "categories": "cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel graph-analytic approach for detecting anomalies\nin network flow data called GraphPrints. Building on foundational\nnetwork-mining techniques, our method represents time slices of traffic as a\ngraph, then counts graphlets -- small induced subgraphs that describe local\ntopology. By performing outlier detection on the sequence of graphlet counts,\nanomalous intervals of traffic are identified, and furthermore, individual IPs\nexperiencing abnormal behavior are singled-out. Initial testing of GraphPrints\nis performed on real network data with an implanted anomaly. Evaluation shows\nfalse positive rates bounded by 2.84% at the time-interval level, and 0.05% at\nthe IP-level with 100% true positive rates at both.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 22:04:22 GMT"}], "update_date": "2016-02-04", "authors_parsed": [["Harshaw", "Christopher R.", ""], ["Bridges", "Robert A.", ""], ["Iannacone", "Michael D.", ""], ["Reed", "Joel W.", ""], ["Goodall", "John R.", ""]]}, {"id": "1602.01132", "submitter": "Sivan Sabato", "authors": "Sivan Sabato and Tom Hess", "title": "Interactive algorithms: from pool to stream", "comments": "Appearing in COLT 2016", "journal-ref": "S. Sabato and T. Hess, \"Interactive Algorithms: from Pool to\n  Stream\", Proceedings of the 29th Annual Conference on Learning Theory (COLT),\n  JMLR Workshop and Conference Proceedings 49:1419-1439, 2016", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider interactive algorithms in the pool-based setting, and in the\nstream-based setting. Interactive algorithms observe suggested elements\n(representing actions or queries), and interactively select some of them and\nreceive responses. Pool-based algorithms can select elements at any order,\nwhile stream-based algorithms observe elements in sequence, and can only select\nelements immediately after observing them. We assume that the suggested\nelements are generated independently from some source distribution, and ask\nwhat is the stream size required for emulating a pool algorithm with a given\npool size. We provide algorithms and matching lower bounds for general pool\nalgorithms, and for utility-based pool algorithms. We further show that a\nmaximal gap between the two settings exists also in the special case of active\nlearning for binary classification.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 22:06:02 GMT"}, {"version": "v2", "created": "Sun, 14 Feb 2016 09:27:44 GMT"}, {"version": "v3", "created": "Thu, 16 Jun 2016 13:40:06 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Sabato", "Sivan", ""], ["Hess", "Tom", ""]]}, {"id": "1602.01164", "submitter": "Conrado Miranda", "authors": "Conrado S. Miranda and Fernando J. Von Zuben", "title": "Single-Solution Hypervolume Maximization and its use for Improving\n  Generalization of Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the hypervolume maximization with a single solution as\nan alternative to the mean loss minimization. The relationship between the two\nproblems is proved through bounds on the cost function when an optimal solution\nto one of the problems is evaluated on the other, with a hyperparameter to\ncontrol the similarity between the two problems. This same hyperparameter\nallows higher weight to be placed on samples with higher loss when computing\nthe hypervolume's gradient, whose normalized version can range from the mean\nloss to the max loss. An experiment on MNIST with a neural network is used to\nvalidate the theory developed, showing that the hypervolume maximization can\nbehave similarly to the mean loss minimization and can also provide better\nperformance, resulting on a 20% reduction of the classification error on the\ntest set.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 01:32:01 GMT"}], "update_date": "2016-02-04", "authors_parsed": [["Miranda", "Conrado S.", ""], ["Von Zuben", "Fernando J.", ""]]}, {"id": "1602.01168", "submitter": "Zhuolin Jiang", "authors": "Zhuolin Jiang, Yaming Wang, Larry Davis, Walt Andrews, Viktor Rozgic", "title": "Learning Discriminative Features via Label Consistent Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (CNN) enforces supervised information only\nat the output layer, and hidden layers are trained by back propagating the\nprediction error from the output layer without explicit supervision. We propose\na supervised feature learning approach, Label Consistent Neural Network, which\nenforces direct supervision in late hidden layers. We associate each neuron in\na hidden layer with a particular class label and encourage it to be activated\nfor input signals from the same class. More specifically, we introduce a label\nconsistency regularization called \"discriminative representation error\" loss\nfor late hidden layers and combine it with classification error loss to build\nour overall objective function. This label consistency constraint alleviates\nthe common problem of gradient vanishing and tends to faster convergence; it\nalso makes the features derived from late hidden layers discriminative enough\nfor classification even using a simple $k$-NN classifier, since input signals\nfrom the same class will have very similar representations. Experimental\nresults demonstrate that our approach achieves state-of-the-art performances on\nseveral public benchmarks for action and object category recognition.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 02:41:33 GMT"}, {"version": "v2", "created": "Sun, 5 Jun 2016 02:45:35 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Jiang", "Zhuolin", ""], ["Wang", "Yaming", ""], ["Davis", "Larry", ""], ["Andrews", "Walt", ""], ["Rozgic", "Viktor", ""]]}, {"id": "1602.01182", "submitter": "John Ramey", "authors": "John A. Ramey, Caleb K. Stein, Phil D. Young, Dean M. Young", "title": "High-Dimensional Regularized Discriminant Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularized discriminant analysis (RDA), proposed by Friedman (1989), is a\nwidely popular classifier that lacks interpretability and is impractical for\nhigh-dimensional data sets. Here, we present an interpretable and\ncomputationally efficient classifier called high-dimensional RDA (HDRDA),\ndesigned for the small-sample, high-dimensional setting. For HDRDA, we show\nthat each training observation, regardless of class, contributes to the class\ncovariance matrix, resulting in an interpretable estimator that borrows from\nthe pooled sample covariance matrix. Moreover, we show that HDRDA is equivalent\nto a classifier in a reduced-feature space with dimension approximately equal\nto the training sample size. As a result, the matrix operations employed by\nHDRDA are computationally linear in the number of features, making the\nclassifier well-suited for high-dimensional classification in practice. We\ndemonstrate that HDRDA is often superior to several sparse and regularized\nclassifiers in terms of classification accuracy with three artificial and six\nreal high-dimensional data sets. Also, timing comparisons between our HDRDA\nimplementation in the sparsediscrim R package and the standard RDA formulation\nin the klaR R package demonstrate that as the number of features increases, the\ncomputational runtime of HDRDA is drastically smaller than that of RDA.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 03:56:08 GMT"}, {"version": "v2", "created": "Sun, 5 Feb 2017 20:03:06 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Ramey", "John A.", ""], ["Stein", "Caleb K.", ""], ["Young", "Phil D.", ""], ["Young", "Dean M.", ""]]}, {"id": "1602.01345", "submitter": "Thijs van de Laar MSc.", "authors": "Thijs van de Laar and Bert de Vries", "title": "A Probabilistic Modeling Approach to Hearing Loss Compensation", "comments": null, "journal-ref": null, "doi": "10.1109/TASLP.2016.2599275", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hearing Aid (HA) algorithms need to be tuned (\"fitted\") to match the\nimpairment of each specific patient. The lack of a fundamental HA fitting\ntheory is a strong contributing factor to an unsatisfying sound experience for\nabout 20% of hearing aid patients. This paper proposes a probabilistic modeling\napproach to the design of HA algorithms. The proposed method relies on a\ngenerative probabilistic model for the hearing loss problem and provides for\nautomated inference of the corresponding (1) signal processing algorithm, (2)\nthe fitting solution as well as a principled (3) performance evaluation metric.\nAll three tasks are realized as message passing algorithms in a factor graph\nrepresentation of the generative model, which in principle allows for fast\nimplementation on hearing aid or mobile device hardware. The methods are\ntheoretically worked out and simulated with a custom-built factor graph toolbox\nfor a specific hearing loss model.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 15:45:47 GMT"}, {"version": "v2", "created": "Tue, 6 Sep 2016 09:56:03 GMT"}], "update_date": "2016-09-07", "authors_parsed": [["van de Laar", "Thijs", ""], ["de Vries", "Bert", ""]]}, {"id": "1602.01407", "submitter": "Roger Grosse", "authors": "Roger Grosse and James Martens", "title": "A Kronecker-factored approximate Fisher matrix for convolution layers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Second-order optimization methods such as natural gradient descent have the\npotential to speed up training of neural networks by correcting for the\ncurvature of the loss function. Unfortunately, the exact natural gradient is\nimpractical to compute for large models, and most approximations either require\nan expensive iterative procedure or make crude approximations to the curvature.\nWe present Kronecker Factors for Convolution (KFC), a tractable approximation\nto the Fisher matrix for convolutional networks based on a structured\nprobabilistic model for the distribution over backpropagated derivatives.\nSimilarly to the recently proposed Kronecker-Factored Approximate Curvature\n(K-FAC), each block of the approximate Fisher matrix decomposes as the\nKronecker product of small matrices, allowing for efficient inversion. KFC\ncaptures important curvature information while still yielding comparably\nefficient updates to stochastic gradient descent (SGD). We show that the\nupdates are invariant to commonly used reparameterizations, such as centering\nof the activations. In our experiments, approximate natural gradient descent\nwith KFC was able to train convolutional networks several times faster than\ncarefully tuned SGD. Furthermore, it was able to train the networks in 10-20\ntimes fewer iterations than SGD, suggesting its potential applicability in a\ndistributed setting.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 18:45:07 GMT"}, {"version": "v2", "created": "Mon, 23 May 2016 22:44:56 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Grosse", "Roger", ""], ["Martens", "James", ""]]}, {"id": "1602.01410", "submitter": "Miguel Sim\\~oes", "authors": "Miguel Sim\\~oes, Luis B. Almeida, Jos\\'e Bioucas-Dias, Jocelyn\n  Chanussot", "title": "A Framework for Fast Image Deconvolution with Incomplete Observations", "comments": "IEEE Trans. Image Process., to be published. 15 pages, 11 figures.\n  MATLAB code available at\n  https://github.com/alfaiate/DeconvolutionIncompleteObs", "journal-ref": null, "doi": "10.1109/TIP.2016.2603920", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In image deconvolution problems, the diagonalization of the underlying\noperators by means of the FFT usually yields very large speedups. When there\nare incomplete observations (e.g., in the case of unknown boundaries), standard\ndeconvolution techniques normally involve non-diagonalizable operators,\nresulting in rather slow methods, or, otherwise, use inexact convolution\nmodels, resulting in the occurrence of artifacts in the enhanced images. In\nthis paper, we propose a new deconvolution framework for images with incomplete\nobservations that allows us to work with diagonalized convolution operators,\nand therefore is very fast. We iteratively alternate the estimation of the\nunknown pixels and of the deconvolved image, using, e.g., an FFT-based\ndeconvolution method. This framework is an efficient, high-quality alternative\nto existing methods of dealing with the image boundaries, such as edge\ntapering. It can be used with any fast deconvolution method. We give an example\nin which a state-of-the-art method that assumes periodic boundary conditions is\nextended, through the use of this framework, to unknown boundary conditions.\nFurthermore, we propose a specific implementation of this framework, based on\nthe alternating direction method of multipliers (ADMM). We provide a proof of\nconvergence for the resulting algorithm, which can be seen as a \"partial\" ADMM,\nin which not all variables are dualized. We report experimental comparisons\nwith other primal-dual methods, where the proposed one performed at the level\nof the state of the art. Four different kinds of applications were tested in\nthe experiments: deconvolution, deconvolution with inpainting, superresolution,\nand demosaicing, all with unknown boundaries.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 18:57:02 GMT"}, {"version": "v2", "created": "Tue, 30 Aug 2016 17:40:01 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Sim\u00f5es", "Miguel", ""], ["Almeida", "Luis B.", ""], ["Bioucas-Dias", "Jos\u00e9", ""], ["Chanussot", "Jocelyn", ""]]}, {"id": "1602.01522", "submitter": "Daniel McDonald", "authors": "Darren Homrighausen and Daniel J. McDonald", "title": "A study on tuning parameter selection for the high-dimensional lasso", "comments": "64 pages, 11 figures", "journal-ref": "Journal of Statistical Computation and Simulation (2018), vol. 88,\n  pp. 2865-2892", "doi": "10.1080/00949655.2018.1491575", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional predictive models, those with more measurements than\nobservations, require regularization to be well defined, perform well\nempirically, and possess theoretical guarantees. The amount of regularization,\noften determined by tuning parameters, is integral to achieving good\nperformance. One can choose the tuning parameter in a variety of ways, such as\nthrough resampling methods or generalized information criteria. However, the\ntheory supporting many regularized procedures relies on an estimate for the\nvariance parameter, which is complicated in high dimensions. We develop a suite\nof information criteria for choosing the tuning parameter in lasso regression\nby leveraging the literature on high-dimensional variance estimation. We derive\nintuition showing that existing information-theoretic approaches work poorly in\nthis setting. We compare our risk estimators to existing methods with an\nextensive simulation and derive some theoretical justification. We find that\nour new estimators perform well across a wide range of simulation conditions\nand evaluation criteria.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 01:06:09 GMT"}, {"version": "v2", "created": "Fri, 12 Jul 2019 18:34:39 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Homrighausen", "Darren", ""], ["McDonald", "Daniel J.", ""]]}, {"id": "1602.01557", "submitter": "Miguel \\'A. Carreira-Perpi\\~n\\'an", "authors": "Miguel \\'A. Carreira-Perpi\\~n\\'an and Ramin Raziperchikolaei", "title": "An ensemble diversity approach to supervised binary hashing", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary hashing is a well-known approach for fast approximate nearest-neighbor\nsearch in information retrieval. Much work has focused on affinity-based\nobjective functions involving the hash functions or binary codes. These\nobjective functions encode neighborhood information between data points and are\noften inspired by manifold learning algorithms. They ensure that the hash\nfunctions differ from each other through constraints or penalty terms that\nencourage codes to be orthogonal or dissimilar across bits, but this couples\nthe binary variables and complicates the already difficult optimization. We\npropose a much simpler approach: we train each hash function (or bit)\nindependently from each other, but introduce diversity among them using\ntechniques from classifier ensembles. Surprisingly, we find that not only is\nthis faster and trivially parallelizable, but it also improves over the more\ncomplex, coupled objective function, and achieves state-of-the-art precision\nand recall in experiments with image retrieval.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 04:59:54 GMT"}], "update_date": "2016-02-05", "authors_parsed": [["Carreira-Perpi\u00f1\u00e1n", "Miguel \u00c1.", ""], ["Raziperchikolaei", "Ramin", ""]]}, {"id": "1602.01729", "submitter": "Paul Honeine", "authors": "Fei Zhu, Abderrahim Halimi, Paul Honeine, Badong Chen, Nanning Zheng", "title": "Correntropy Maximization via ADMM - Application to Robust Hyperspectral\n  Unmixing", "comments": "23 pages", "journal-ref": null, "doi": "10.1109/TGRS.2017.2696262", "report-no": null, "categories": "stat.ML cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In hyperspectral images, some spectral bands suffer from low signal-to-noise\nratio due to noisy acquisition and atmospheric effects, thus requiring robust\ntechniques for the unmixing problem. This paper presents a robust supervised\nspectral unmixing approach for hyperspectral images. The robustness is achieved\nby writing the unmixing problem as the maximization of the correntropy\ncriterion subject to the most commonly used constraints. Two unmixing problems\nare derived: the first problem considers the fully-constrained unmixing, with\nboth the non-negativity and sum-to-one constraints, while the second one deals\nwith the non-negativity and the sparsity-promoting of the abundances. The\ncorresponding optimization problems are solved efficiently using an alternating\ndirection method of multipliers (ADMM) approach. Experiments on synthetic and\nreal hyperspectral images validate the performance of the proposed algorithms\nfor different scenarios, demonstrating that the correntropy-based unmixing is\nrobust to outlier bands.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 16:21:09 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Zhu", "Fei", ""], ["Halimi", "Abderrahim", ""], ["Honeine", "Paul", ""], ["Chen", "Badong", ""], ["Zheng", "Nanning", ""]]}, {"id": "1602.01818", "submitter": "Alexander Wong", "authors": "A. G. Chung, M. J. Shafiee, and A. Wong", "title": "Random Feature Maps via a Layered Random Projection (LaRP) Framework for\n  Object Classification", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The approximation of nonlinear kernels via linear feature maps has recently\ngained interest due to their applications in reducing the training and testing\ntime of kernel-based learning algorithms. Current random projection methods\navoid the curse of dimensionality by embedding the nonlinear feature space into\na low dimensional Euclidean space to create nonlinear kernels. We introduce a\nLayered Random Projection (LaRP) framework, where we model the linear kernels\nand nonlinearity separately for increased training efficiency. The proposed\nLaRP framework was assessed using the MNIST hand-written digits database and\nthe COIL-100 object database, and showed notable improvement in object\nclassification performance relative to other state-of-the-art random projection\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 20:31:44 GMT"}], "update_date": "2016-02-05", "authors_parsed": [["Chung", "A. G.", ""], ["Shafiee", "M. J.", ""], ["Wong", "A.", ""]]}, {"id": "1602.01889", "submitter": "Furong Huang", "authors": "Furong Huang, Animashree Anandkumar, Christian Borgs, Jennifer Chayes,\n  Ernest Fraenkel, Michael Hawrylycz, Ed Lein, Alessandro Ingrosso, Srinivas\n  Turaga", "title": "Discovering Neuronal Cell Types and Their Gene Expression Profiles Using\n  a Spatial Point Process Mixture Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cataloging the neuronal cell types that comprise circuitry of individual\nbrain regions is a major goal of modern neuroscience and the BRAIN initiative.\nSingle-cell RNA sequencing can now be used to measure the gene expression\nprofiles of individual neurons and to categorize neurons based on their gene\nexpression profiles. While the single-cell techniques are extremely powerful\nand hold great promise, they are currently still labor intensive, have a high\ncost per cell, and, most importantly, do not provide information on spatial\ndistribution of cell types in specific regions of the brain. We propose a\ncomplementary approach that uses computational methods to infer the cell types\nand their gene expression profiles through analysis of brain-wide single-cell\nresolution in situ hybridization (ISH) imagery contained in the Allen Brain\nAtlas (ABA). We measure the spatial distribution of neurons labeled in the ISH\nimage for each gene and model it as a spatial point process mixture, whose\nmixture weights are given by the cell types which express that gene. By fitting\na point process mixture model jointly to the ISH images, we infer both the\nspatial point process distribution for each cell type and their gene expression\nprofile. We validate our predictions of cell type-specific gene expression\nprofiles using single cell RNA sequencing data, recently published for the\nmouse somatosensory cortex. Jointly with the gene expression profiles, cell\nfeatures such as cell size, orientation, intensity and local density level are\ninferred per cell type.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 23:52:18 GMT"}, {"version": "v2", "created": "Sat, 11 Jun 2016 01:45:12 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Huang", "Furong", ""], ["Anandkumar", "Animashree", ""], ["Borgs", "Christian", ""], ["Chayes", "Jennifer", ""], ["Fraenkel", "Ernest", ""], ["Hawrylycz", "Michael", ""], ["Lein", "Ed", ""], ["Ingrosso", "Alessandro", ""], ["Turaga", "Srinivas", ""]]}, {"id": "1602.02018", "submitter": "Nicolas Tremblay", "authors": "Nicolas Tremblay, Gilles Puy, Remi Gribonval, Pierre Vandergheynst", "title": "Compressive Spectral Clustering", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral clustering has become a popular technique due to its high\nperformance in many contexts. It comprises three main steps: create a\nsimilarity graph between N objects to cluster, compute the first k eigenvectors\nof its Laplacian matrix to define a feature vector for each object, and run\nk-means on these features to separate objects into k classes. Each of these\nthree steps becomes computationally intensive for large N and/or k. We propose\nto speed up the last two steps based on recent results in the emerging field of\ngraph signal processing: graph filtering of random signals, and random sampling\nof bandlimited graph signals. We prove that our method, with a gain in\ncomputation time that can reach several orders of magnitude, is in fact an\napproximation of spectral clustering, for which we are able to control the\nerror. We test the performance of our method on artificial and real-world\nnetwork data.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 13:42:27 GMT"}, {"version": "v2", "created": "Mon, 23 May 2016 13:21:56 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Tremblay", "Nicolas", ""], ["Puy", "Gilles", ""], ["Gribonval", "Remi", ""], ["Vandergheynst", "Pierre", ""]]}, {"id": "1602.02068", "submitter": "Ram\\'on Fernandez Astudillo", "authors": "Andr\\'e F. T. Martins and Ram\\'on Fernandez Astudillo", "title": "From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label\n  Classification", "comments": "Minor corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose sparsemax, a new activation function similar to the traditional\nsoftmax, but able to output sparse probabilities. After deriving its\nproperties, we show how its Jacobian can be efficiently computed, enabling its\nuse in a network trained with backpropagation. Then, we propose a new smooth\nand convex loss function which is the sparsemax analogue of the logistic loss.\nWe reveal an unexpected connection between this new loss and the Huber\nclassification loss. We obtain promising empirical results in multi-label\nclassification problems and in attention-based neural networks for natural\nlanguage inference. For the latter, we achieve a similar performance as the\ntraditional softmax, but with a selective, more compact, attention focus.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 15:49:02 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2016 09:41:36 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Martins", "Andr\u00e9 F. T.", ""], ["Astudillo", "Ram\u00f3n Fernandez", ""]]}, {"id": "1602.02114", "submitter": "Xenia Miscouridou", "authors": "Adrien Todeschini, Xenia Miscouridou and Fran\\c{c}ois Caron", "title": "Exchangeable Random Measures for Sparse and Modular Graphs with\n  Overlapping Communities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel statistical model for sparse networks with overlapping\ncommunity structure. The model is based on representing the graph as an\nexchangeable point process, and naturally generalizes existing probabilistic\nmodels with overlapping block-structure to the sparse regime. Our construction\nbuilds on vectors of completely random measures, and has interpretable\nparameters, each node being assigned a vector representing its level of\naffiliation to some latent communities. We develop methods for simulating this\nclass of random graphs, as well as to perform posterior inference. We show that\nthe proposed approach can recover interpretable structure from two real-world\nnetworks and can handle graphs with thousands of nodes and tens of thousands of\nedges.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 18:22:14 GMT"}, {"version": "v2", "created": "Wed, 23 Aug 2017 10:37:49 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Todeschini", "Adrien", ""], ["Miscouridou", "Xenia", ""], ["Caron", "Fran\u00e7ois", ""]]}, {"id": "1602.02136", "submitter": "Jialei Wang", "authors": "Jialei Wang, Hai Wang, Nathan Srebro", "title": "Reducing Runtime by Recycling Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrary to the situation with stochastic gradient descent, we argue that\nwhen using stochastic methods with variance reduction, such as SDCA, SAG or\nSVRG, as well as their variants, it could be beneficial to reuse previously\nused samples instead of fresh samples, even when fresh samples are available.\nWe demonstrate this empirically for SDCA, SAG and SVRG, studying the optimal\nsample size one should use, and also uncover be-havior that suggests running\nSDCA for an integer number of epochs could be wasteful.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 19:46:23 GMT"}], "update_date": "2016-02-08", "authors_parsed": [["Wang", "Jialei", ""], ["Wang", "Hai", ""], ["Srebro", "Nathan", ""]]}, {"id": "1602.02151", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Yang Yuan, Karthik Sridharan", "title": "Exploiting the Structure: Stochastic Gradient Methods Using Raw Clusters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amount of data available in the world is growing faster than our ability\nto deal with it. However, if we take advantage of the internal\n\\emph{structure}, data may become much smaller for machine learning purposes.\nIn this paper we focus on one of the fundamental machine learning tasks,\nempirical risk minimization (ERM), and provide faster algorithms with the help\nfrom the clustering structure of the data.\n  We introduce a simple notion of raw clustering that can be efficiently\ncomputed from the data, and propose two algorithms based on clustering\ninformation. Our accelerated algorithm ClusterACDM is built on a novel Haar\ntransformation applied to the dual space of the ERM problem, and our\nvariance-reduction based algorithm ClusterSVRG introduces a new gradient\nestimator using clustering. Our algorithms outperform their classical\ncounterparts ACDM and SVRG respectively.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 20:58:18 GMT"}, {"version": "v2", "created": "Fri, 4 Nov 2016 17:10:04 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Yuan", "Yang", ""], ["Sridharan", "Karthik", ""]]}, {"id": "1602.02164", "submitter": "David Gamarnik", "authors": "David Gamarnik and Sidhant Misra", "title": "A Note on Alternating Minimization Algorithm for the Matrix Completion\n  Problem", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": "10.1109/LSP.2016.2576979", "report-no": null, "categories": "stat.ML cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of reconstructing a low rank matrix from a subset of\nits entries and analyze two variants of the so-called Alternating Minimization\nalgorithm, which has been proposed in the past. We establish that when the\nunderlying matrix has rank $r=1$, has positive bounded entries, and the graph\n$\\mathcal{G}$ underlying the revealed entries has bounded degree and diameter\nwhich is at most logarithmic in the size of the matrix, both algorithms succeed\nin reconstructing the matrix approximately in polynomial time starting from an\narbitrary initialization. We further provide simulation results which suggest\nthat the second algorithm which is based on the message passing type updates,\nperforms significantly better.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 21:07:16 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Gamarnik", "David", ""], ["Misra", "Sidhant", ""]]}, {"id": "1602.02172", "submitter": "Weiran Wang", "authors": "Weiran Wang", "title": "On Column Selection in Approximate Kernel Canonical Correlation Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of column selection in large-scale kernel canonical\ncorrelation analysis (KCCA) using the Nystr\\\"om approximation, where one\napproximates two positive semi-definite kernel matrices using \"landmark\" points\nfrom the training set. When building low-rank kernel approximations in KCCA,\nprevious work mostly samples the landmarks uniformly at random from the\ntraining set. We propose novel strategies for sampling the landmarks\nnon-uniformly based on a version of statistical leverage scores recently\ndeveloped for kernel ridge regression. We study the approximation accuracy of\nthe proposed non-uniform sampling strategy, develop an incremental algorithm\nthat explores the path of approximation ranks and facilitates efficient model\nselection, and derive the kernel stability of out-of-sample mapping for our\nmethod. Experimental results on both synthetic and real-world datasets\ndemonstrate the promise of our method.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 21:51:41 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Wang", "Weiran", ""]]}, {"id": "1602.02181", "submitter": "Paul Mineiro", "authors": "He He, Paul Mineiro, Nikos Karampatziakis", "title": "Active Information Acquisition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general framework for sequential and dynamic acquisition of\nuseful information in order to solve a particular task. While our goal could in\nprinciple be tackled by general reinforcement learning, our particular setting\nis constrained enough to allow more efficient algorithms. In this paper, we\nwork under the Learning to Search framework and show how to formulate the goal\nof finding a dynamic information acquisition policy in that framework. We apply\nour formulation on two tasks, sentiment analysis and image recognition, and\nshow that the learned policies exhibit good statistical performance. As an\nemergent byproduct, the learned policies show a tendency to focus on the most\nprominent parts of each instance and give harder instances more attention\nwithout explicitly being trained to do so.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 22:32:50 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["He", "He", ""], ["Mineiro", "Paul", ""], ["Karampatziakis", "Nikos", ""]]}, {"id": "1602.02191", "submitter": "Mohammad Gheshlaghi Azar", "authors": "Mohammad Gheshlaghi Azar, Eva Dyer, Konrad Kording", "title": "Convex Relaxation Regression: Black-Box Optimization of Smooth Functions\n  by Learning Their Convex Envelopes", "comments": null, "journal-ref": "Proc. of the Conference on Uncertainty in Artificial Intelligence,\n  pg. 22-31, 2016", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding efficient and provable methods to solve non-convex optimization\nproblems is an outstanding challenge in machine learning and optimization\ntheory. A popular approach used to tackle non-convex problems is to use convex\nrelaxation techniques to find a convex surrogate for the problem.\nUnfortunately, convex relaxations typically must be found on a\nproblem-by-problem basis. Thus, providing a general-purpose strategy to\nestimate a convex relaxation would have a wide reaching impact. Here, we\nintroduce Convex Relaxation Regression (CoRR), an approach for learning convex\nrelaxations for a class of smooth functions. The main idea behind our approach\nis to estimate the convex envelope of a function $f$ by evaluating $f$ at a set\nof $T$ random points and then fitting a convex function to these function\nevaluations. We prove that with probability greater than $1-\\delta$, the\nsolution of our algorithm converges to the global optimizer of $f$ with error\n$\\mathcal{O} \\Big( \\big(\\frac{\\log(1/\\delta) }{T} \\big)^{\\alpha} \\Big)$ for\nsome $\\alpha> 0$. Our approach enables the use of convex optimization tools to\nsolve a class of non-convex optimization problems.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 23:23:32 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2016 01:36:29 GMT"}, {"version": "v3", "created": "Thu, 3 Mar 2016 20:09:26 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Azar", "Mohammad Gheshlaghi", ""], ["Dyer", "Eva", ""], ["Kording", "Konrad", ""]]}, {"id": "1602.02196", "submitter": "Alexander Rakhlin", "authors": "Alexander Rakhlin and Karthik Sridharan", "title": "BISTRO: An Efficient Relaxation-Based Method for Contextual Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present efficient algorithms for the problem of contextual bandits with\ni.i.d. covariates, an arbitrary sequence of rewards, and an arbitrary class of\npolicies. Our algorithm BISTRO requires d calls to the empirical risk\nminimization (ERM) oracle per round, where d is the number of actions. The\nmethod uses unlabeled data to make the problem computationally simple. When the\nERM problem itself is computationally hard, we extend the approach by employing\nmultiplicative approximation algorithms for the ERM. The integrality gap of the\nrelaxation only enters in the regret bound rather than the benchmark. Finally,\nwe show that the adversarial version of the contextual bandit problem is\nlearnable (and efficient) whenever the full-information supervised online\nlearning problem has a non-trivial regret guarantee (and efficient).\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2016 00:34:59 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Rakhlin", "Alexander", ""], ["Sridharan", "Karthik", ""]]}, {"id": "1602.02210", "submitter": "Aaditya Ramdas", "authors": "Ilmun Kim, Aaditya Ramdas, Aarti Singh, Larry Wasserman", "title": "Classification accuracy as a proxy for two sample testing", "comments": "71 pages, 4 figures. Accepted for publication at the Annals of\n  Statistics (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When data analysts train a classifier and check if its accuracy is\nsignificantly different from chance, they are implicitly performing a\ntwo-sample test. We investigate the statistical properties of this flexible\napproach in the high-dimensional setting. We prove two results that hold for\nall classifiers in any dimensions: if its true error remains $\\epsilon$-better\nthan chance for some $\\epsilon>0$ as $d,n \\to \\infty$, then (a) the\npermutation-based test is consistent (has power approaching to one), (b) a\ncomputationally efficient test based on a Gaussian approximation of the null\ndistribution is also consistent. To get a finer understanding of the rates of\nconsistency, we study a specialized setting of distinguishing Gaussians with\nmean-difference $\\delta$ and common (known or unknown) covariance $\\Sigma$,\nwhen $d/n \\to c \\in (0,\\infty)$. We study variants of Fisher's linear\ndiscriminant analysis (LDA) such as \"naive Bayes\" in a nontrivial regime when\n$\\epsilon \\to 0$ (the Bayes classifier has true accuracy approaching 1/2), and\ncontrast their power with corresponding variants of Hotelling's test.\nSurprisingly, the expressions for their power match exactly in terms of\n$n,d,\\delta,\\Sigma$, and the LDA approach is only worse by a constant factor,\nachieving an asymptotic relative efficiency (ARE) of $1/\\sqrt{\\pi}$ for\nbalanced samples. We also extend our results to high-dimensional elliptical\ndistributions with finite kurtosis. Other results of independent interest\ninclude minimax lower bounds, and the optimality of Hotelling's test when\n$d=o(n)$. Simulation results validate our theory, and we present practical\ntakeaway messages along with natural open problems.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2016 03:48:04 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 22:36:47 GMT"}, {"version": "v3", "created": "Fri, 17 Jan 2020 21:29:08 GMT"}, {"version": "v4", "created": "Mon, 17 Feb 2020 17:56:24 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Kim", "Ilmun", ""], ["Ramdas", "Aaditya", ""], ["Singh", "Aarti", ""], ["Wasserman", "Larry", ""]]}, {"id": "1602.02219", "submitter": "Cheng Zhang", "authors": "Cheng Zhang, Babak Shahbaba, Hongkai Zhao", "title": "Variational Hamiltonian Monte Carlo via Score Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, the field of computational Bayesian statistics has been\ndivided into two main subfields: variational methods and Markov chain Monte\nCarlo (MCMC). In recent years, however, several methods have been proposed\nbased on combining variational Bayesian inference and MCMC simulation in order\nto improve their overall accuracy and computational efficiency. This marriage\nof fast evaluation and flexible approximation provides a promising means of\ndesigning scalable Bayesian inference methods. In this paper, we explore the\npossibility of incorporating variational approximation into a state-of-the-art\nMCMC method, Hamiltonian Monte Carlo (HMC), to reduce the required gradient\ncomputation in the simulation of Hamiltonian flow, which is the bottleneck for\nmany applications of HMC in big data problems. To this end, we use a {\\it\nfree-form} approximation induced by a fast and flexible surrogate function\nbased on single-hidden layer feedforward neural networks. The surrogate\nprovides sufficiently accurate approximation while allowing for fast\nexploration of parameter space, resulting in an efficient approximate inference\nalgorithm. We demonstrate the advantages of our method on both synthetic and\nreal data problems.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2016 05:36:02 GMT"}, {"version": "v2", "created": "Mon, 17 Apr 2017 23:28:24 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Zhang", "Cheng", ""], ["Shahbaba", "Babak", ""], ["Zhao", "Hongkai", ""]]}, {"id": "1602.02220", "submitter": "Tianbao Yang", "authors": "Zhe Li, Boqing Gong, Tianbao Yang", "title": "Improved Dropout for Shallow and Deep Learning", "comments": "In NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dropout has been witnessed with great success in training deep neural\nnetworks by independently zeroing out the outputs of neurons at random. It has\nalso received a surge of interest for shallow learning, e.g., logistic\nregression. However, the independent sampling for dropout could be suboptimal\nfor the sake of convergence. In this paper, we propose to use multinomial\nsampling for dropout, i.e., sampling features or neurons according to a\nmultinomial distribution with different probabilities for different\nfeatures/neurons. To exhibit the optimal dropout probabilities, we analyze the\nshallow learning with multinomial dropout and establish the risk bound for\nstochastic optimization. By minimizing a sampling dependent factor in the risk\nbound, we obtain a distribution-dependent dropout with sampling probabilities\ndependent on the second order statistics of the data distribution. To tackle\nthe issue of evolving distribution of neurons in deep learning, we propose an\nefficient adaptive dropout (named \\textbf{evolutional dropout}) that computes\nthe sampling probabilities on-the-fly from a mini-batch of examples. Empirical\nstudies on several benchmark datasets demonstrate that the proposed dropouts\nachieve not only much faster convergence and but also a smaller testing error\nthan the standard dropout. For example, on the CIFAR-100 data, the evolutional\ndropout achieves relative improvements over 10\\% on the prediction performance\nand over 50\\% on the convergence speed compared to the standard dropout.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2016 05:41:57 GMT"}, {"version": "v2", "created": "Sun, 4 Dec 2016 05:31:19 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Li", "Zhe", ""], ["Gong", "Boqing", ""], ["Yang", "Tianbao", ""]]}, {"id": "1602.02256", "submitter": "Kohei Hayashi", "authors": "Kohei Hayashi, Takuya Konishi, Tatsuro Kawamoto", "title": "A Tractable Fully Bayesian Method for the Stochastic Block Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stochastic block model (SBM) is a generative model revealing macroscopic\nstructures in graphs. Bayesian methods are used for (i) cluster assignment\ninference and (ii) model selection for the number of clusters. In this paper,\nwe study the behavior of Bayesian inference in the SBM in the large sample\nlimit. Combining variational approximation and Laplace's method, a consistent\ncriterion of the fully marginalized log-likelihood is established. Based on\nthat, we derive a tractable algorithm that solves tasks (i) and (ii)\nconcurrently, obviating the need for an outer loop to check all model\ncandidates. Our empirical and theoretical results demonstrate that our method\nis scalable in computation, accurate in approximation, and concise in model\nselection.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2016 13:47:34 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Hayashi", "Kohei", ""], ["Konishi", "Takuya", ""], ["Kawamoto", "Tatsuro", ""]]}, {"id": "1602.02262", "submitter": "Yingyu Liang", "authors": "Yuanzhi Li, Yingyu Liang, Andrej Risteski", "title": "Recovery guarantee of weighted low-rank approximation via alternating\n  minimization", "comments": "40 pages. Updated with the ICML 2016 camera ready version, together\n  with an additional algorithm which needs less assumptions in Appendix C", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications require recovering a ground truth low-rank matrix from\nnoisy observations of the entries, which in practice is typically formulated as\na weighted low-rank approximation problem and solved by non-convex optimization\nheuristics such as alternating minimization. In this paper, we provide provable\nrecovery guarantee of weighted low-rank via a simple alternating minimization\nalgorithm. In particular, for a natural class of matrices and weights and\nwithout any assumption on the noise, we bound the spectral norm of the\ndifference between the recovered matrix and the ground truth, by the spectral\nnorm of the weighted noise plus an additive error that decreases exponentially\nwith the number of rounds of alternating minimization, from either\ninitialization by SVD or, more importantly, random initialization. These\nprovide the first theoretical results for weighted low-rank via alternating\nminimization with non-binary deterministic weights, significantly generalizing\nthose for matrix completion, the special case with binary weights, since our\nassumptions are similar or weaker than those made in existing works.\nFurthermore, this is achieved by a very simple algorithm that improves the\nvanilla alternating minimization with a simple clipping step.\n  The key technical challenge is that under non-binary deterministic weights,\nna\\\"ive alternating steps will destroy the incoherence and spectral properties\nof the intermediate solutions, which are needed for making progress towards the\nground truth. We show that the properties only need to hold in an average sense\nand can be achieved by the clipping step.\n  We further provide an alternating algorithm that uses a whitening step that\nkeeps the properties via SDP and Rademacher rounding and thus requires weaker\nassumptions. This technique can potentially be applied in some other\napplications and is of independent interest.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2016 14:55:12 GMT"}, {"version": "v2", "created": "Thu, 8 Dec 2016 17:05:41 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Li", "Yuanzhi", ""], ["Liang", "Yingyu", ""], ["Risteski", "Andrej", ""]]}, {"id": "1602.02263", "submitter": "Andreas Tillmann", "authors": "Andreas M. Tillmann, Yonina C. Eldar, Julien Mairal", "title": "DOLPHIn - Dictionary Learning for Phase Retrieval", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2016.2607180", "report-no": null, "categories": "math.OC cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm to learn a dictionary for reconstructing and\nsparsely encoding signals from measurements without phase. Specifically, we\nconsider the task of estimating a two-dimensional image from squared-magnitude\nmeasurements of a complex-valued linear transformation of the original image.\nSeveral recent phase retrieval algorithms exploit underlying sparsity of the\nunknown signal in order to improve recovery performance. In this work, we\nconsider such a sparse signal prior in the context of phase retrieval, when the\nsparsifying dictionary is not known in advance. Our algorithm jointly\nreconstructs the unknown signal - possibly corrupted by noise - and learns a\ndictionary such that each patch of the estimated image can be sparsely\nrepresented. Numerical experiments demonstrate that our approach can obtain\nsignificantly better reconstructions for phase retrieval problems with noise\nthan methods that cannot exploit such \"hidden\" sparsity. Moreover, on the\ntheoretical side, we provide a convergence result for our method.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2016 15:03:26 GMT"}, {"version": "v2", "created": "Wed, 3 Aug 2016 09:53:47 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Tillmann", "Andreas M.", ""], ["Eldar", "Yonina C.", ""], ["Mairal", "Julien", ""]]}, {"id": "1602.02282", "submitter": "Casper Kaae S{\\o}nderby", "authors": "Casper Kaae S{\\o}nderby, Tapani Raiko, Lars Maal{\\o}e, S{\\o}ren Kaae\n  S{\\o}nderby, Ole Winther", "title": "Ladder Variational Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational Autoencoders are powerful models for unsupervised learning.\nHowever deep models with several layers of dependent stochastic variables are\ndifficult to train which limits the improvements obtained using these highly\nexpressive models. We propose a new inference model, the Ladder Variational\nAutoencoder, that recursively corrects the generative distribution by a data\ndependent approximate likelihood in a process resembling the recently proposed\nLadder Network. We show that this model provides state of the art predictive\nlog-likelihood and tighter log-likelihood lower bound compared to the purely\nbottom-up inference in layered Variational Autoencoders and other generative\nmodels. We provide a detailed analysis of the learned hierarchical latent\nrepresentation and show that our new inference model is qualitatively different\nand utilizes a deeper more distributed hierarchy of latent variables. Finally,\nwe observe that batch normalization and deterministic warm-up (gradually\nturning on the KL-term) are crucial for training variational models with many\nstochastic layers.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2016 17:32:48 GMT"}, {"version": "v2", "created": "Tue, 24 May 2016 10:41:45 GMT"}, {"version": "v3", "created": "Fri, 27 May 2016 09:05:10 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["S\u00f8nderby", "Casper Kaae", ""], ["Raiko", "Tapani", ""], ["Maal\u00f8e", "Lars", ""], ["S\u00f8nderby", "S\u00f8ren Kaae", ""], ["Winther", "Ole", ""]]}, {"id": "1602.02283", "submitter": "Dominik Csiba", "authors": "Dominik Csiba and Peter Richt\\'arik", "title": "Importance Sampling for Minibatches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minibatching is a very well studied and highly popular technique in\nsupervised learning, used by practitioners due to its ability to accelerate\ntraining through better utilization of parallel processing power and reduction\nof stochastic variance. Another popular technique is importance sampling -- a\nstrategy for preferential sampling of more important examples also capable of\naccelerating the training process. However, despite considerable effort by the\ncommunity in these areas, and due to the inherent technical difficulty of the\nproblem, there is no existing work combining the power of importance sampling\nwith the strength of minibatching. In this paper we propose the first {\\em\nimportance sampling for minibatches} and give simple and rigorous complexity\nanalysis of its performance. We illustrate on synthetic problems that for\ntraining data of certain properties, our sampling can lead to several orders of\nmagnitude improvement in training time. We then test the new sampling on\nseveral popular datasets, and show that the improvement can reach an order of\nmagnitude.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2016 17:35:53 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Csiba", "Dominik", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "1602.02285", "submitter": "Uri Shaham", "authors": "Uri Shaham, Xiuyuan Cheng, Omer Dror, Ariel Jaffe, Boaz Nadler, Joseph\n  Chang, Yuval Kluger", "title": "A Deep Learning Approach to Unsupervised Ensemble Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": "PMLR 48:30-39", "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how deep learning methods can be applied in the context of\ncrowdsourcing and unsupervised ensemble learning. First, we prove that the\npopular model of Dawid and Skene, which assumes that all classifiers are\nconditionally independent, is {\\em equivalent} to a Restricted Boltzmann\nMachine (RBM) with a single hidden node. Hence, under this model, the posterior\nprobabilities of the true labels can be instead estimated via a trained RBM.\nNext, to address the more general case, where classifiers may strongly violate\nthe conditional independence assumption, we propose to apply RBM-based Deep\nNeural Net (DNN). Experimental results on various simulated and real-world\ndatasets demonstrate that our proposed DNN approach outperforms other\nstate-of-the-art methods, in particular when the data violates the conditional\nindependence assumption.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2016 17:56:59 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Shaham", "Uri", ""], ["Cheng", "Xiuyuan", ""], ["Dror", "Omer", ""], ["Jaffe", "Ariel", ""], ["Nadler", "Boaz", ""], ["Chang", "Joseph", ""], ["Kluger", "Yuval", ""]]}, {"id": "1602.02311", "submitter": "Yingzhen Li", "authors": "Yingzhen Li, Richard E. Turner", "title": "R\\'enyi Divergence Variational Inference", "comments": "NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the variational R\\'enyi bound (VR) that extends\ntraditional variational inference to R\\'enyi's alpha-divergences. This new\nfamily of variational methods unifies a number of existing approaches, and\nenables a smooth interpolation from the evidence lower-bound to the log\n(marginal) likelihood that is controlled by the value of alpha that\nparametrises the divergence. The reparameterization trick, Monte Carlo\napproximation and stochastic optimisation methods are deployed to obtain a\ntractable and unified framework for optimisation. We further consider negative\nalpha values and propose a novel variational inference method as a new special\ncase in the proposed framework. Experiments on Bayesian neural networks and\nvariational auto-encoders demonstrate the wide applicability of the VR bound.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2016 21:35:23 GMT"}, {"version": "v2", "created": "Fri, 20 May 2016 08:35:54 GMT"}, {"version": "v3", "created": "Fri, 28 Oct 2016 18:16:29 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Li", "Yingzhen", ""], ["Turner", "Richard E.", ""]]}, {"id": "1602.02338", "submitter": "Saul Toscano-Palmerin", "authors": "Saul Toscano-Palmerin and Peter I. Frazier", "title": "Stratified Bayesian Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider derivative-free black-box global optimization of expensive noisy\nfunctions, when most of the randomness in the objective is produced by a few\ninfluential scalar random inputs. We present a new Bayesian global optimization\nalgorithm, called Stratified Bayesian Optimization (SBO), which uses this\nstrong dependence to improve performance. Our algorithm is similar in spirit to\nstratification, a technique from simulation, which uses strong dependence on a\ncategorical representation of the random input to reduce variance. We\ndemonstrate in numerical experiments that SBO outperforms state-of-the-art\nBayesian optimization benchmarks that do not leverage this dependence.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2016 03:53:16 GMT"}, {"version": "v2", "created": "Sat, 20 Feb 2016 10:08:18 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Toscano-Palmerin", "Saul", ""], ["Frazier", "Peter I.", ""]]}, {"id": "1602.02355", "submitter": "Fabian Pedregosa", "authors": "Fabian Pedregosa", "title": "Hyperparameter optimization with approximate gradient", "comments": "Proceedings of the International conference on Machine Learning\n  (ICML)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most models in machine learning contain at least one hyperparameter to\ncontrol for model complexity. Choosing an appropriate set of hyperparameters is\nboth crucial in terms of model accuracy and computationally challenging. In\nthis work we propose an algorithm for the optimization of continuous\nhyperparameters using inexact gradient information. An advantage of this method\nis that hyperparameters can be updated before model parameters have fully\nconverged. We also give sufficient conditions for the global convergence of\nthis method, based on regularity conditions of the involved functions and\nsummability of errors. Finally, we validate the empirical performance of this\nmethod on the estimation of regularization constants of L2-regularized logistic\nregression and kernel Ridge regression. Empirical benchmarks indicate that our\napproach is highly competitive with respect to state of the art methods.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2016 10:37:13 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2016 11:24:08 GMT"}, {"version": "v3", "created": "Fri, 27 May 2016 09:50:52 GMT"}, {"version": "v4", "created": "Thu, 9 Jun 2016 15:42:04 GMT"}, {"version": "v5", "created": "Sun, 26 Jun 2016 01:02:54 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Pedregosa", "Fabian", ""]]}, {"id": "1602.02373", "submitter": "Rie Johnson", "authors": "Rie Johnson, Tong Zhang", "title": "Supervised and Semi-Supervised Text Categorization using LSTM for Region\n  Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-hot CNN (convolutional neural network) has been shown to be effective for\ntext categorization (Johnson & Zhang, 2015). We view it as a special case of a\ngeneral framework which jointly trains a linear model with a non-linear feature\ngenerator consisting of `text region embedding + pooling'. Under this\nframework, we explore a more sophisticated region embedding method using Long\nShort-Term Memory (LSTM). LSTM can embed text regions of variable (and possibly\nlarge) sizes, whereas the region size needs to be fixed in a CNN. We seek\neffective and efficient use of LSTM for this purpose in the supervised and\nsemi-supervised settings. The best results were obtained by combining region\nembeddings in the form of LSTM and convolution layers trained on unlabeled\ndata. The results indicate that on this task, embeddings of text regions, which\ncan convey complex concepts, are more useful than embeddings of single words in\nisolation. We report performances exceeding the previous best results on four\nbenchmark datasets.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2016 14:05:58 GMT"}, {"version": "v2", "created": "Thu, 26 May 2016 15:26:34 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Johnson", "Rie", ""], ["Zhang", "Tong", ""]]}, {"id": "1602.02386", "submitter": "Qingming Tang", "authors": "Qingming Tang, Lifu Tu, Weiran Wang and Jinbo Xu", "title": "Network Inference by Learned Node-Specific Degree Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for network inference from partially observed edges\nusing a node-specific degree prior. The degree prior is derived from observed\nedges in the network to be inferred, and its hyper-parameters are determined by\ncross validation. Then we formulate network inference as a matrix completion\nproblem regularized by our degree prior. Our theoretical analysis indicates\nthat this prior favors a network following the learned degree distribution, and\nmay lead to improved network recovery error bound than previous work.\nExperimental results on both simulated and real biological networks demonstrate\nthe superior performance of our method in various settings.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2016 16:11:18 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Tang", "Qingming", ""], ["Tu", "Lifu", ""], ["Wang", "Weiran", ""], ["Xu", "Jinbo", ""]]}, {"id": "1602.02389", "submitter": "Tom Zahavy", "authors": "Tom Zahavy, Bingyi Kang, Alex Sivak, Jiashi Feng, Huan Xu, Shie Mannor", "title": "Ensemble Robustness and Generalization of Stochastic Deep Learning\n  Algorithms", "comments": "16 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The question why deep learning algorithms generalize so well has attracted\nincreasing research interest. However, most of the well-established approaches,\nsuch as hypothesis capacity, stability or sparseness, have not provided\ncomplete explanations (Zhang et al., 2016; Kawaguchi et al., 2017). In this\nwork, we focus on the robustness approach (Xu & Mannor, 2012), i.e., if the\nerror of a hypothesis will not change much due to perturbations of its training\nexamples, then it will also generalize well. As most deep learning algorithms\nare stochastic (e.g., Stochastic Gradient Descent, Dropout, and\nBayes-by-backprop), we revisit the robustness arguments of Xu & Mannor, and\nintroduce a new approach, ensemble robustness, that concerns the robustness of\na population of hypotheses. Through the lens of ensemble robustness, we reveal\nthat a stochastic learning algorithm can generalize well as long as its\nsensitiveness to adversarial perturbations is bounded in average over training\nexamples. Moreover, an algorithm may be sensitive to some adversarial examples\n(Goodfellow et al., 2015) but still generalize well. To support our claims, we\nprovide extensive simulations for different deep learning algorithms and\ndifferent network architectures exhibiting a strong correlation between\nensemble robustness and the ability to generalize.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2016 16:50:14 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2016 01:59:39 GMT"}, {"version": "v3", "created": "Wed, 29 Jun 2016 21:02:34 GMT"}, {"version": "v4", "created": "Sun, 5 Nov 2017 12:18:24 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Zahavy", "Tom", ""], ["Kang", "Bingyi", ""], ["Sivak", "Alex", ""], ["Feng", "Jiashi", ""], ["Xu", "Huan", ""], ["Mannor", "Shie", ""]]}, {"id": "1602.02442", "submitter": "Aaron Defazio Dr", "authors": "Aaron Defazio", "title": "A Simple Practical Accelerated Method for Finite Sums", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We describe a novel optimization method for finite sums (such as empirical\nrisk minimization problems) building on the recently introduced SAGA method.\nOur method achieves an accelerated convergence rate on strongly convex smooth\nproblems. Our method has only one parameter (a step size), and is radically\nsimpler than other accelerated methods for finite sums. Additionally it can be\napplied when the terms are non-smooth, yielding a method applicable in many\nareas where operator splitting methods would traditionally be applied.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 00:24:01 GMT"}, {"version": "v2", "created": "Thu, 27 Oct 2016 23:18:05 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Defazio", "Aaron", ""]]}, {"id": "1602.02450", "submitter": "Giorgio Patrini", "authors": "Giorgio Patrini, Frank Nielsen, Richard Nock, Marcello Carioni", "title": "Loss factorization, weakly supervised learning and label noise\n  robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that the empirical risk of most well-known loss functions factors\ninto a linear term aggregating all labels with a term that is label free, and\ncan further be expressed by sums of the loss. This holds true even for\nnon-smooth, non-convex losses and in any RKHS. The first term is a (kernel)\nmean operator --the focal quantity of this work-- which we characterize as the\nsufficient statistic for the labels. The result tightens known generalization\nbounds and sheds new light on their interpretation.\n  Factorization has a direct application on weakly supervised learning. In\nparticular, we demonstrate that algorithms like SGD and proximal methods can be\nadapted with minimal effort to handle weak supervision, once the mean operator\nhas been estimated. We apply this idea to learning with asymmetric noisy\nlabels, connecting and extending prior work. Furthermore, we show that most\nlosses enjoy a data-dependent (by the mean operator) form of noise robustness,\nin contrast with known negative results.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 01:50:43 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2016 23:10:23 GMT"}], "update_date": "2016-02-11", "authors_parsed": [["Patrini", "Giorgio", ""], ["Nielsen", "Frank", ""], ["Nock", "Richard", ""], ["Carioni", "Marcello", ""]]}, {"id": "1602.02485", "submitter": "Ichiro Takeuchi Prof.", "authors": "Atsushi Shibagaki, Masayuki Karasuyama, Kohei Hatano, Ichiro Takeuchi", "title": "Simultaneous Safe Screening of Features and Samples in Doubly Sparse\n  Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of learning a sparse model is conceptually interpreted as the\nprocess of identifying active features/samples and then optimizing the model\nover them. Recently introduced safe screening allows us to identify a part of\nnon-active features/samples. So far, safe screening has been individually\nstudied either for feature screening or for sample screening. In this paper, we\nintroduce a new approach for safely screening features and samples\nsimultaneously by alternatively iterating feature and sample screening steps. A\nsignificant advantage of considering them simultaneously rather than\nindividually is that they have a synergy effect in the sense that the results\nof the previous safe feature screening can be exploited for improving the next\nsafe sample screening performances, and vice-versa. We first theoretically\ninvestigate the synergy effect, and then illustrate the practical advantage\nthrough intensive numerical experiments for problems with large numbers of\nfeatures and samples.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 07:48:38 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Shibagaki", "Atsushi", ""], ["Karasuyama", "Masayuki", ""], ["Hatano", "Kohei", ""], ["Takeuchi", "Ichiro", ""]]}, {"id": "1602.02499", "submitter": "David van Leeuwen", "authors": "David A. van Leeuwen and Rosemary Orr", "title": "The \"Sprekend Nederland\" project and its application to accent location", "comments": "Accepted to Speaker and Language Recognition Odyssey 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the data collection effort that is part of the project\nSprekend Nederland (The Netherlands Talking), and discusses its potential use\nin Automatic Accent Location. We define Automatic Accent Location as the task\nto describe the accent of a speaker in terms of the location of the speaker and\nits history. We discuss possible ways of describing accent location, the\nconsequence these have for the task of automatic accent location, and potential\nevaluation metrics.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 09:23:19 GMT"}, {"version": "v2", "created": "Fri, 8 Apr 2016 13:17:20 GMT"}], "update_date": "2016-04-11", "authors_parsed": [["van Leeuwen", "David A.", ""], ["Orr", "Rosemary", ""]]}, {"id": "1602.02514", "submitter": "James Newling", "authors": "James Newling and Fran\\c{c}ois Fleuret", "title": "Fast K-Means with Accurate Bounds", "comments": "8 pages + supplementary material v2: mlpack installed with\n  optimisation (previously installed in DEBUG) v3: Annulus -> Annular v4:\n  Author affiliation update v5: Synced with version at ICML, now including\n  Suppl. Mat", "journal-ref": "Proceedings of the International Conference on Machine Learning\n  (ICML) pp. 936-944, 2016", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel accelerated exact k-means algorithm, which performs better\nthan the current state-of-the-art low-dimensional algorithm in 18 of 22\nexperiments, running up to 3 times faster. We also propose a general\nimprovement of existing state-of-the-art accelerated exact k-means algorithms\nthrough better estimates of the distance bounds used to reduce the number of\ndistance calculations, and get a speedup in 36 of 44 experiments, up to 1.8\ntimes faster.\n  We have conducted experiments with our own implementations of existing\nmethods to ensure homogeneous evaluation of performance, and we show that our\nimplementations perform as well or better than existing available\nimplementations. Finally, we propose simplified variants of standard approaches\nand show that they are faster than their fully-fledged counterparts in 59 of 62\nexperiments.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 10:19:09 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2016 18:51:11 GMT"}, {"version": "v3", "created": "Thu, 25 Feb 2016 15:18:03 GMT"}, {"version": "v4", "created": "Mon, 4 Apr 2016 09:12:21 GMT"}, {"version": "v5", "created": "Thu, 28 Apr 2016 12:11:49 GMT"}, {"version": "v6", "created": "Sun, 11 Sep 2016 14:57:29 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Newling", "James", ""], ["Fleuret", "Fran\u00e7ois", ""]]}, {"id": "1602.02518", "submitter": "Sahely Bhadra", "authors": "Sahely Bhadra, Samuel Kaski and Juho Rousu", "title": "Multi-view Kernel Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the first method that (1) can complete kernel\nmatrices with completely missing rows and columns as opposed to individual\nmissing kernel values, (2) does not require any of the kernels to be complete a\npriori, and (3) can tackle non-linear kernels. These aspects are necessary in\npractical applications such as integrating legacy data sets, learning under\nsensor failures and learning when measurements are costly for some of the\nviews. The proposed approach predicts missing rows by modelling both\nwithin-view and between-view relationships among kernel values. We show, both\non simulated data and real world data, that the proposed method outperforms\nexisting techniques in the restricted settings where they are available, and\nextends applicability to new settings.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 10:29:13 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Bhadra", "Sahely", ""], ["Kaski", "Samuel", ""], ["Rousu", "Juho", ""]]}, {"id": "1602.02523", "submitter": "Rowan McAllister", "authors": "Rowan McAllister, Carl Edward Rasmussen", "title": "Data-Efficient Reinforcement Learning in Continuous-State POMDPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a data-efficient reinforcement learning algorithm resistant to\nobservation noise. Our method extends the highly data-efficient PILCO algorithm\n(Deisenroth & Rasmussen, 2011) into partially observed Markov decision\nprocesses (POMDPs) by considering the filtering process during policy\nevaluation. PILCO conducts policy search, evaluating each policy by first\npredicting an analytic distribution of possible system trajectories. We\nadditionally predict trajectories w.r.t. a filtering process, achieving\nsignificantly higher performance than combining a filter with a policy\noptimised by the original (unfiltered) framework. Our test setup is the\ncartpole swing-up task with sensor noise, which involves nonlinear dynamics and\nrequires nonlinear control.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 11:08:49 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["McAllister", "Rowan", ""], ["Rasmussen", "Carl Edward", ""]]}, {"id": "1602.02575", "submitter": "Xiangyu Wang", "authors": "Xiangyu Wang, David Dunson, Chenlei Leng", "title": "DECOrrelated feature space partitioning for distributed sparse\n  regression", "comments": "Correct legend errors in Figure 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fitting statistical models is computationally challenging when the sample\nsize or the dimension of the dataset is huge. An attractive approach for\ndown-scaling the problem size is to first partition the dataset into subsets\nand then fit using distributed algorithms. The dataset can be partitioned\neither horizontally (in the sample space) or vertically (in the feature space).\nWhile the majority of the literature focuses on sample space partitioning,\nfeature space partitioning is more effective when $p\\gg n$. Existing methods\nfor partitioning features, however, are either vulnerable to high correlations\nor inefficient in reducing the model dimension. In this paper, we solve these\nproblems through a new embarrassingly parallel framework named DECO for\ndistributed variable selection and parameter estimation. In DECO, variables are\nfirst partitioned and allocated to $m$ distributed workers. The decorrelated\nsubset data within each worker are then fitted via any algorithm designed for\nhigh-dimensional problems. We show that by incorporating the decorrelation\nstep, DECO can achieve consistent variable selection and parameter estimation\non each subset with (almost) no assumptions. In addition, the convergence rate\nis nearly minimax optimal for both sparse and weakly sparse models and does NOT\ndepend on the partition number $m$. Extensive numerical experiments are\nprovided to illustrate the performance of the new framework.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 14:17:38 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2016 13:18:57 GMT"}], "update_date": "2016-02-15", "authors_parsed": [["Wang", "Xiangyu", ""], ["Dunson", "David", ""], ["Leng", "Chenlei", ""]]}, {"id": "1602.02616", "submitter": "Thomas Bonis", "authors": "Thomas Bonis", "title": "Guarantees in Wasserstein Distance for the Langevin Monte Carlo\n  Algorithm", "comments": "Updated and merged with arXiv article 1506.06966", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of sampling from a distribution $\\target$ using the\nLangevin Monte Carlo algorithm and provide rate of convergences for this\nalgorithm in terms of Wasserstein distance of order $2$. Our result holds as\nlong as the continuous diffusion process associated with the algorithm\nconverges exponentially fast to the target distribution along with some\ntechnical assumptions. While such an exponential convergence holds for example\nin the log-concave measure case, it also holds for the more general case of\nasymptoticaly log-concave measures. Our results thus extends the known rates of\nconvergence in total variation and Wasserstein distances which have only been\nobtained in the log-concave case. Moreover, using a sharper approximation bound\nof the continuous process, we obtain better asymptotic rates than traditional\nresults. We also look into variations of the Langevin Monte Carlo algorithm\nusing other discretization schemes. In a first time, we look into the use of\nthe Ozaki's discretization but are unable to obtain any significative\nimprovement in terms of convergence rates compared to the Euler's scheme. We\nthen provide a (sub-optimal) way to study more general schemes, however our\napproach only holds for the log-concave case.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 15:49:07 GMT"}, {"version": "v2", "created": "Fri, 1 Jul 2016 11:19:23 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Bonis", "Thomas", ""]]}, {"id": "1602.02666", "submitter": "Stephan Mandt", "authors": "Stephan Mandt, Matthew D. Hoffman, and David M. Blei", "title": "A Variational Analysis of Stochastic Gradient Algorithms", "comments": "8 pages, 3 figures", "journal-ref": "International Conference on Machine Learning (ICML 2016), p.\n  354--363", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Gradient Descent (SGD) is an important algorithm in machine\nlearning. With constant learning rates, it is a stochastic process that, after\nan initial phase of convergence, generates samples from a stationary\ndistribution. We show that SGD with constant rates can be effectively used as\nan approximate posterior inference algorithm for probabilistic modeling.\nSpecifically, we show how to adjust the tuning parameters of SGD such as to\nmatch the resulting stationary distribution to the posterior. This analysis\nrests on interpreting SGD as a continuous-time stochastic process and then\nminimizing the Kullback-Leibler divergence between its stationary distribution\nand the target posterior. (This is in the spirit of variational inference.) In\nmore detail, we model SGD as a multivariate Ornstein-Uhlenbeck process and then\nuse properties of this process to derive the optimal parameters. This\ntheoretical framework also connects SGD to modern scalable inference\nalgorithms; we analyze the recently proposed stochastic gradient Fisher scoring\nunder this perspective. We demonstrate that SGD with properly chosen constant\nrates gives a new way to optimize hyperparameters in probabilistic models.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 17:46:18 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Mandt", "Stephan", ""], ["Hoffman", "Matthew D.", ""], ["Blei", "David M.", ""]]}, {"id": "1602.02701", "submitter": "Arthur Mensch", "authors": "Arthur Mensch (PARIETAL), Ga\\\"el Varoquaux (PARIETAL), Bertrand\n  Thirion (PARIETAL)", "title": "Compressed Online Dictionary Learning for Fast fMRI Decomposition", "comments": null, "journal-ref": "IEEE International Symposium on Biomedical Imaging, 2016", "doi": "10.1109/ISBI.2016.7493501", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for fast resting-state fMRI spatial decomposi-tions of\nvery large datasets, based on the reduction of the temporal dimension before\napplying dictionary learning on concatenated individual records from groups of\nsubjects. Introducing a measure of correspondence between spatial\ndecompositions of rest fMRI, we demonstrates that time-reduced dictionary\nlearning produces result as reliable as non-reduced decompositions. We also\nshow that this reduction significantly improves computational scalability.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 19:19:08 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Mensch", "Arthur", "", "PARIETAL"], ["Varoquaux", "Ga\u00ebl", "", "PARIETAL"], ["Thirion", "Bertrand", "", "PARIETAL"]]}, {"id": "1602.02722", "submitter": "Akshay Krishnamurthy", "authors": "Akshay Krishnamurthy, Alekh Agarwal, John Langford", "title": "PAC Reinforcement Learning with Rich Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and study a new model for reinforcement learning with rich\nobservations, generalizing contextual bandits to sequential decision making.\nThese models require an agent to take actions based on observations (features)\nwith the goal of achieving long-term performance competitive with a large set\nof policies. To avoid barriers to sample-efficient learning associated with\nlarge observation spaces and general POMDPs, we focus on problems that can be\nsummarized by a small number of hidden states and have long-term rewards that\nare predictable by a reactive function class. In this setting, we design and\nanalyze a new reinforcement learning algorithm, Least Squares Value Elimination\nby Exploration. We prove that the algorithm learns near optimal behavior after\na number of episodes that is polynomial in all relevant parameters, logarithmic\nin the number of policies, and independent of the size of the observation\nspace. Our result provides theoretical justification for reinforcement learning\nwith function approximation.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 20:12:50 GMT"}, {"version": "v2", "created": "Tue, 1 Mar 2016 15:16:12 GMT"}, {"version": "v3", "created": "Tue, 24 May 2016 13:20:29 GMT"}, {"version": "v4", "created": "Fri, 28 Oct 2016 15:37:17 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Krishnamurthy", "Akshay", ""], ["Agarwal", "Alekh", ""], ["Langford", "John", ""]]}, {"id": "1602.02823", "submitter": "Mark Tygert", "authors": "Mark Tygert", "title": "Poor starting points in machine learning", "comments": "11 pages, 3 figures, 1 table; this initial version is literally\n  identical to that circulated among a restricted audience over a month ago", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Poor (even random) starting points for learning/training/optimization are\ncommon in machine learning. In many settings, the method of Robbins and Monro\n(online stochastic gradient descent) is known to be optimal for good starting\npoints, but may not be optimal for poor starting points -- indeed, for poor\nstarting points Nesterov acceleration can help during the initial iterations,\neven though Nesterov methods not designed for stochastic approximation could\nhurt during later iterations. The common practice of training with nontrivial\nminibatches enhances the advantage of Nesterov acceleration.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 00:14:03 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Tygert", "Mark", ""]]}, {"id": "1602.02842", "submitter": "Truyen Tran", "authors": "Truyen Tran, Dinh Phung and Svetha Venkatesh", "title": "Collaborative filtering via sparse Markov random fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems play a central role in providing individualized access to\ninformation and services. This paper focuses on collaborative filtering, an\napproach that exploits the shared structure among mind-liked users and similar\nitems. In particular, we focus on a formal probabilistic framework known as\nMarkov random fields (MRF). We address the open problem of structure learning\nand introduce a sparsity-inducing algorithm to automatically estimate the\ninteraction structures between users and between items. Item-item and user-user\ncorrelation networks are obtained as a by-product. Large-scale experiments on\nmovie recommendation and date matching datasets demonstrate the power of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 02:30:27 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Tran", "Truyen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1602.02845", "submitter": "Carlos Riquelme Ruiz", "authors": "Carlos Riquelme, Ramesh Johari, Baosen Zhang", "title": "Online Active Linear Regression via Thresholding", "comments": "Published in AAAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of online active learning to collect data for\nregression modeling. Specifically, we consider a decision maker with a limited\nexperimentation budget who must efficiently learn an underlying linear\npopulation model. Our main contribution is a novel threshold-based algorithm\nfor selection of most informative observations; we characterize its performance\nand fundamental lower bounds. We extend the algorithm and its guarantees to\nsparse linear regression in high-dimensional settings. Simulations suggest the\nalgorithm is remarkably robust: it provides significant benefits over passive\nrandom sampling in real-world datasets that exhibit high nonlinearity and high\ndimensionality --- significantly reducing both the mean and variance of the\nsquared error.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 02:51:12 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2016 17:53:33 GMT"}, {"version": "v3", "created": "Thu, 23 Jun 2016 18:36:58 GMT"}, {"version": "v4", "created": "Wed, 21 Dec 2016 13:36:50 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Riquelme", "Carlos", ""], ["Johari", "Ramesh", ""], ["Zhang", "Baosen", ""]]}, {"id": "1602.02850", "submitter": "Bo Tang", "authors": "Bo Tang, Steven Kay, and Haibo He", "title": "Toward Optimal Feature Selection in Naive Bayes for Text Categorization", "comments": "This paper has been submitted to the IEEE Trans. Knowledge and Data\n  Engineering. 14 pages, 5 figures", "journal-ref": null, "doi": "10.1109/TKDE.2016.2563436", "report-no": null, "categories": "stat.ML cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated feature selection is important for text categorization to reduce\nthe feature size and to speed up the learning process of classifiers. In this\npaper, we present a novel and efficient feature selection framework based on\nthe Information Theory, which aims to rank the features with their\ndiscriminative capacity for classification. We first revisit two information\nmeasures: Kullback-Leibler divergence and Jeffreys divergence for binary\nhypothesis testing, and analyze their asymptotic properties relating to type I\nand type II errors of a Bayesian classifier. We then introduce a new divergence\nmeasure, called Jeffreys-Multi-Hypothesis (JMH) divergence, to measure\nmulti-distribution divergence for multi-class classification. Based on the\nJMH-divergence, we develop two efficient feature selection methods, termed\nmaximum discrimination ($MD$) and $MD-\\chi^2$ methods, for text categorization.\nThe promising results of extensive experiments demonstrate the effectiveness of\nthe proposed approaches.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 03:43:21 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Tang", "Bo", ""], ["Kay", "Steven", ""], ["He", "Haibo", ""]]}, {"id": "1602.02852", "submitter": "Nicol\\'as Della Penna", "authors": "Nicol\\'as Della Penna, Mark D. Reid, David Balduzzi", "title": "Compliance-Aware Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by clinical trials, we study bandits with observable\nnon-compliance. At each step, the learner chooses an arm, after, instead of\nobserving only the reward, it also observes the action that took place. We show\nthat such noncompliance can be helpful or hurtful to the learner in general.\nUnfortunately, naively incorporating compliance information into bandit\nalgorithms loses guarantees on sublinear regret. We present hybrid algorithms\nthat maintain regret bounds up to a multiplicative factor and can incorporate\ncompliance information. Simulations based on real data from the International\nStoke Trial show the practical potential of these algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 04:00:32 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Della Penna", "Nicol\u00e1s", ""], ["Reid", "Mark D.", ""], ["Balduzzi", "David", ""]]}, {"id": "1602.02867", "submitter": "Aviv Tamar", "authors": "Aviv Tamar, Yi Wu, Garrett Thomas, Sergey Levine, Pieter Abbeel", "title": "Value Iteration Networks", "comments": "Fixed missing table values", "journal-ref": "Advances in Neural Information Processing Systems 29 pages\n  2154--2162, 2016", "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the value iteration network (VIN): a fully differentiable neural\nnetwork with a `planning module' embedded within. VINs can learn to plan, and\nare suitable for predicting outcomes that involve planning-based reasoning,\nsuch as policies for reinforcement learning. Key to our approach is a novel\ndifferentiable approximation of the value-iteration algorithm, which can be\nrepresented as a convolutional neural network, and trained end-to-end using\nstandard backpropagation. We evaluate VIN based policies on discrete and\ncontinuous path-planning domains, and on a natural-language based search task.\nWe show that by learning an explicit planning computation, VIN policies\ngeneralize better to new, unseen domains.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 05:44:36 GMT"}, {"version": "v2", "created": "Sun, 29 May 2016 18:33:04 GMT"}, {"version": "v3", "created": "Sun, 5 Feb 2017 20:06:14 GMT"}, {"version": "v4", "created": "Mon, 20 Mar 2017 21:41:51 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Tamar", "Aviv", ""], ["Wu", "Yi", ""], ["Thomas", "Garrett", ""], ["Levine", "Sergey", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1602.02915", "submitter": "Ting Kei Pong", "authors": "Guoyin Li and Ting Kei Pong", "title": "Calculus of the exponent of Kurdyka-{\\L}ojasiewicz inequality and its\n  applications to linear convergence of first-order methods", "comments": "The paper is accepted for publication in Foundations of Computational\n  Mathematics: https://link.springer.com/article/10.1007/s10208-017-9366-8. In\n  this update, we fill in more details to the proof of Theorem 4.1 concerning\n  the nonemptiness of the projection onto the set of stationary points", "journal-ref": null, "doi": "10.1007/s10208-017-9366-8", "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the Kurdyka-{\\L}ojasiewicz (KL) exponent, an\nimportant quantity for analyzing the convergence rate of first-order methods.\nSpecifically, we develop various calculus rules to deduce the KL exponent of\nnew (possibly nonconvex and nonsmooth) functions formed from functions with\nknown KL exponents. In addition, we show that the well-studied Luo-Tseng error\nbound together with a mild assumption on the separation of stationary values\nimplies that the KL exponent is $\\frac12$. The Luo-Tseng error bound is known\nto hold for a large class of concrete structured optimization problems, and\nthus we deduce the KL exponent of a large class of functions whose exponents\nwere previously unknown. Building upon this and the calculus rules, we are then\nable to show that for many convex or nonconvex optimization models for\napplications such as sparse recovery, their objective function's KL exponent is\n$\\frac12$. This includes the least squares problem with smoothly clipped\nabsolute deviation (SCAD) regularization or minimax concave penalty (MCP)\nregularization and the logistic regression problem with $\\ell_1$\nregularization. Since many existing local convergence rate analysis for\nfirst-order methods in the nonconvex scenario relies on the KL exponent, our\nresults enable us to obtain explicit convergence rate for various first-order\nmethods when they are applied to a large variety of practical optimization\nmodels. Finally, we further illustrate how our results can be applied to\nestablishing local linear convergence of the proximal gradient algorithm and\nthe inertial proximal algorithm with constant step-sizes for some specific\nmodels that arise in sparse recovery.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 09:50:00 GMT"}, {"version": "v2", "created": "Sat, 3 Sep 2016 06:21:17 GMT"}, {"version": "v3", "created": "Thu, 25 May 2017 05:28:35 GMT"}, {"version": "v4", "created": "Thu, 17 Aug 2017 13:57:45 GMT"}, {"version": "v5", "created": "Fri, 19 Jan 2018 03:40:29 GMT"}], "update_date": "2018-01-22", "authors_parsed": [["Li", "Guoyin", ""], ["Pong", "Ting Kei", ""]]}, {"id": "1602.02934", "submitter": "James Newling", "authors": "James Newling and Fran\\c{c}ois Fleuret", "title": "Nested Mini-Batch K-Means", "comments": "8 pages + Supplementary Material. Version 2 : new experiments added.\n  Version 3 : Add acknowledgments, upper case in title. Version 4 : Correct\n  spelling of Acknowledgements, change title. Version 5: camera ready NIPS", "journal-ref": "Nested Mini-Batch K-Means, Proceedings of the International\n  Conference on Neural Information Processing Systems (NIPS), 2016", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new algorithm is proposed which accelerates the mini-batch k-means\nalgorithm of Sculley (2010) by using the distance bounding approach of Elkan\n(2003). We argue that, when incorporating distance bounds into a mini-batch\nalgorithm, already used data should preferentially be reused. To this end we\npropose using nested mini-batches, whereby data in a mini-batch at iteration t\nis automatically reused at iteration t+1.\n  Using nested mini-batches presents two difficulties. The first is that\nunbalanced use of data can bias estimates, which we resolve by ensuring that\neach data sample contributes exactly once to centroids. The second is in\nchoosing mini-batch sizes, which we address by balancing premature fine-tuning\nof centroids with redundancy induced slow-down. Experiments show that the\nresulting nmbatch algorithm is very effective, often arriving within 1% of the\nempirical minimum 100 times earlier than the standard mini-batch algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 11:05:42 GMT"}, {"version": "v2", "created": "Sun, 22 May 2016 12:04:17 GMT"}, {"version": "v3", "created": "Mon, 30 May 2016 07:40:13 GMT"}, {"version": "v4", "created": "Wed, 29 Jun 2016 06:17:32 GMT"}, {"version": "v5", "created": "Mon, 12 Sep 2016 22:25:20 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Newling", "James", ""], ["Fleuret", "Fran\u00e7ois", ""]]}, {"id": "1602.02964", "submitter": "Kacper Chwialkowski", "authors": "Kacper Chwialkowski, Heiko Strathmann and Arthur Gretton", "title": "A Kernel Test of Goodness of Fit", "comments": "14 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a nonparametric statistical test for goodness-of-fit: given a set\nof samples, the test determines how likely it is that these were generated from\na target density function. The measure of goodness-of-fit is a divergence\nconstructed via Stein's method using functions from a Reproducing Kernel\nHilbert Space. Our test statistic is based on an empirical estimate of this\ndivergence, taking the form of a V-statistic in terms of the log gradients of\nthe target density and the kernel. We derive a statistical test, both for\ni.i.d. and non-i.i.d. samples, where we estimate the null distribution\nquantiles using a wild bootstrap procedure. We apply our test to quantifying\nconvergence of approximate Markov Chain Monte Carlo methods, statistical model\ncriticism, and evaluating quality of fit vs model complexity in nonparametric\ndensity estimation.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 12:54:16 GMT"}, {"version": "v2", "created": "Wed, 23 Mar 2016 16:42:36 GMT"}, {"version": "v3", "created": "Thu, 24 Mar 2016 09:43:27 GMT"}, {"version": "v4", "created": "Tue, 27 Sep 2016 12:42:49 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Chwialkowski", "Kacper", ""], ["Strathmann", "Heiko", ""], ["Gretton", "Arthur", ""]]}, {"id": "1602.03014", "submitter": "Yutian Chen", "authors": "Yutian Chen and Max Welling", "title": "Herding as a Learning System with Edge-of-Chaos Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Herding defines a deterministic dynamical system at the edge of chaos. It\ngenerates a sequence of model states and parameters by alternating parameter\nperturbations with state maximizations, where the sequence of states can be\ninterpreted as \"samples\" from an associated MRF model. Herding differs from\nmaximum likelihood estimation in that the sequence of parameters does not\nconverge to a fixed point and differs from an MCMC posterior sampling approach\nin that the sequence of states is generated deterministically. Herding may be\ninterpreted as a\"perturb and map\" method where the parameter perturbations are\ngenerated using a deterministic nonlinear dynamical system rather than randomly\nfrom a Gumbel distribution. This chapter studies the distinct statistical\ncharacteristics of the herding algorithm and shows that the fast convergence\nrate of the controlled moments may be attributed to edge of chaos dynamics. The\nherding algorithm can also be generalized to models with latent variables and\nto a discriminative learning setting. The perceptron cycling theorem ensures\nthat the fast moment matching property is preserved in the more general\nframework.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 14:59:45 GMT"}, {"version": "v2", "created": "Tue, 1 Mar 2016 09:37:28 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Chen", "Yutian", ""], ["Welling", "Max", ""]]}, {"id": "1602.03027", "submitter": "Ilya Tolstikhin", "authors": "Ilya Tolstikhin and David Lopez-Paz", "title": "Minimax Lower Bounds for Realizable Transductive Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transductive learning considers a training set of $m$ labeled samples and a\ntest set of $u$ unlabeled samples, with the goal of best labeling that\nparticular test set. Conversely, inductive learning considers a training set of\n$m$ labeled samples drawn iid from $P(X,Y)$, with the goal of best labeling any\nfuture samples drawn iid from $P(X)$. This comparison suggests that\ntransduction is a much easier type of inference than induction, but is this\nreally the case? This paper provides a negative answer to this question, by\nproving the first known minimax lower bounds for transductive, realizable,\nbinary classification. Our lower bounds show that $m$ should be at least\n$\\Omega(d/\\epsilon + \\log(1/\\delta)/\\epsilon)$ when $\\epsilon$-learning a\nconcept class $\\mathcal{H}$ of finite VC-dimension $d<\\infty$ with confidence\n$1-\\delta$, for all $m \\leq u$. This result draws three important conclusions.\nFirst, general transduction is as hard as general induction, since both\nproblems have $\\Omega(d/m)$ minimax values. Second, the use of unlabeled data\ndoes not help general transduction, since supervised learning algorithms such\nas ERM and (Hanneke, 2015) match our transductive lower bounds while ignoring\nthe unlabeled test set. Third, our transductive lower bounds imply lower bounds\nfor semi-supervised learning, which add to the important discussion about the\nrole of unlabeled data in machine learning.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 15:17:24 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Tolstikhin", "Ilya", ""], ["Lopez-Paz", "David", ""]]}, {"id": "1602.03048", "submitter": "Richard Yi Da Xu Dr", "authors": "Richard Yi Da Xu, Francois Caron, Arnaud Doucet", "title": "Bayesian nonparametric image segmentation using a generalized\n  Swendsen-Wang algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised image segmentation aims at clustering the set of pixels of an\nimage into spatially homogeneous regions. We introduce here a class of Bayesian\nnonparametric models to address this problem. These models are based on a\ncombination of a Potts-like spatial smoothness component and a prior on\npartitions which is used to control both the number and size of clusters. This\nclass of models is flexible enough to include the standard Potts model and the\nmore recent Potts-Dirichlet Process model \\cite{Orbanz2008}. More importantly,\nany prior on partitions can be introduced to control the global clustering\nstructure so that it is possible to penalize small or large clusters if\nnecessary. Bayesian computation is carried out using an original generalized\nSwendsen-Wang algorithm. Experiments demonstrate that our method is competitive\nin terms of RAND\\ index compared to popular image segmentation methods, such as\nmean-shift, and recent alternative Bayesian nonparametric models.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 16:02:00 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Da Xu", "Richard Yi", ""], ["Caron", "Francois", ""], ["Doucet", "Arnaud", ""]]}, {"id": "1602.03105", "submitter": "Branislav Kveton", "authors": "Branislav Kveton, Hung Bui, Mohammad Ghavamzadeh, Georgios\n  Theocharous, S. Muthukrishnan, and Siqi Sun", "title": "Graphical Model Sketch", "comments": "Proceedings of the European Conference on Machine Learning and\n  Knowledge Discovery in Databases", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured high-cardinality data arises in many domains, and poses a major\nchallenge for both modeling and inference. Graphical models are a popular\napproach to modeling structured data but they are unsuitable for\nhigh-cardinality variables. The count-min (CM) sketch is a popular approach to\nestimating probabilities in high-cardinality data but it does not scale well\nbeyond a few variables. In this work, we bring together the ideas of graphical\nmodels and count sketches; and propose and analyze several approaches to\nestimating probabilities in structured high-cardinality streams of data. The\nkey idea of our approximations is to use the structure of a graphical model and\napproximately estimate its factors by \"sketches\", which hash high-cardinality\nvariables using random projections. Our approximations are computationally\nefficient and their space complexity is independent of the cardinality of\nvariables. Our error bounds are multiplicative and significantly improve upon\nthose of the CM sketch, a state-of-the-art approach to estimating probabilities\nin streams. We evaluate our approximations on synthetic and real-world\nproblems, and report an order of magnitude improvements over the CM sketch.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 18:07:51 GMT"}, {"version": "v2", "created": "Mon, 18 Jul 2016 05:48:03 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Kveton", "Branislav", ""], ["Bui", "Hung", ""], ["Ghavamzadeh", "Mohammad", ""], ["Theocharous", "Georgios", ""], ["Muthukrishnan", "S.", ""], ["Sun", "Siqi", ""]]}, {"id": "1602.03131", "submitter": "Kinjal Basu", "authors": "Kinjal Basu, Ankan Saha, Shaunak Chatterjee", "title": "Large scale multi-objective optimization: Theoretical and practical\n  challenges", "comments": "10 pages, 2 figures, KDD'16 Submitted Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-objective optimization (MOO) is a well-studied problem for several\nimportant recommendation problems. While multiple approaches have been\nproposed, in this work, we focus on using constrained optimization formulations\n(e.g., quadratic and linear programs) to formulate and solve MOO problems. This\napproach can be used to pick desired operating points on the trade-off curve\nbetween multiple objectives. It also works well for internet applications which\nserve large volumes of online traffic, by working with Lagrangian duality\nformulation to connect dual solutions (computed offline) with the primal\nsolutions (computed online).\n  We identify some key limitations of this approach -- namely the inability to\nhandle user and item level constraints, scalability considerations and variance\nof dual estimates introduced by sampling processes. We propose solutions for\neach of the problems and demonstrate how through these solutions we\nsignificantly advance the state-of-the-art in this realm. Our proposed methods\ncan exactly handle user and item (and other such local) constraints, achieve a\n$100\\times$ scalability boost over existing packages in R and reduce variance\nof dual estimates by two orders of magnitude.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 19:43:45 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2016 08:42:01 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Basu", "Kinjal", ""], ["Saha", "Ankan", ""], ["Chatterjee", "Shaunak", ""]]}, {"id": "1602.03146", "submitter": "Sumeet Katariya", "authors": "Sumeet Katariya, Branislav Kveton, Csaba Szepesv\\'ari, Zheng Wen", "title": "DCM Bandits: Learning to Rank with Multiple Clicks", "comments": "Proceedings of the 33rd International Conference on Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A search engine recommends to the user a list of web pages. The user examines\nthis list, from the first page to the last, and clicks on all attractive pages\nuntil the user is satisfied. This behavior of the user can be described by the\ndependent click model (DCM). We propose DCM bandits, an online learning variant\nof the DCM where the goal is to maximize the probability of recommending\nsatisfactory items, such as web pages. The main challenge of our learning\nproblem is that we do not observe which attractive item is satisfactory. We\npropose a computationally-efficient learning algorithm for solving our problem,\ndcmKL-UCB; derive gap-dependent upper bounds on its regret under reasonable\nassumptions; and also prove a matching lower bound up to logarithmic factors.\nWe evaluate our algorithm on synthetic and real-world problems, and show that\nit performs well even when our model is misspecified. This work presents the\nfirst practical and regret-optimal online algorithm for learning to rank with\nmultiple clicks in a cascade-like click model.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 20:03:30 GMT"}, {"version": "v2", "created": "Tue, 31 May 2016 20:52:17 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Katariya", "Sumeet", ""], ["Kveton", "Branislav", ""], ["Szepesv\u00e1ri", "Csaba", ""], ["Wen", "Zheng", ""]]}, {"id": "1602.03220", "submitter": "Alex Lamb", "authors": "Alex Lamb, Vincent Dumoulin and Aaron Courville", "title": "Discriminative Regularization for Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the question of whether the representations learned by classifiers\ncan be used to enhance the quality of generative models. Our conjecture is that\nlabels correspond to characteristics of natural data which are most salient to\nhumans: identity in faces, objects in images, and utterances in speech. We\npropose to take advantage of this by using the representations from\ndiscriminative classifiers to augment the objective function corresponding to a\ngenerative model. In particular we enhance the objective function of the\nvariational autoencoder, a popular generative model, with a discriminative\nregularization term. We show that enhancing the objective function in this way\nleads to samples that are clearer and have higher visual quality than the\nsamples from the standard variational autoencoders.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 23:35:18 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2016 01:24:48 GMT"}, {"version": "v3", "created": "Fri, 12 Feb 2016 06:19:34 GMT"}, {"version": "v4", "created": "Mon, 15 Feb 2016 17:38:37 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Lamb", "Alex", ""], ["Dumoulin", "Vincent", ""], ["Courville", "Aaron", ""]]}, {"id": "1602.03253", "submitter": "Qiang Liu", "authors": "Qiang Liu and Jason D. Lee and Michael I. Jordan", "title": "A Kernelized Stein Discrepancy for Goodness-of-fit Tests and Model\n  Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive a new discrepancy statistic for measuring differences between two\nprobability distributions based on combining Stein's identity with the\nreproducing kernel Hilbert space theory. We apply our result to test how well a\nprobabilistic model fits a set of observations, and derive a new class of\npowerful goodness-of-fit tests that are widely applicable for complex and high\ndimensional distributions, even for those with computationally intractable\nnormalization constants. Both theoretical and empirical properties of our\nmethods are studied thoroughly.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 03:38:52 GMT"}, {"version": "v2", "created": "Fri, 1 Jul 2016 18:52:21 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Liu", "Qiang", ""], ["Lee", "Jason D.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1602.03264", "submitter": "Yang Lu", "authors": "Jianwen Xie, Yang Lu, Song-Chun Zhu, Ying Nian Wu", "title": "A Theory of Generative ConvNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that a generative random field model, which we call generative\nConvNet, can be derived from the commonly used discriminative ConvNet, by\nassuming a ConvNet for multi-category classification and assuming one of the\ncategories is a base category generated by a reference distribution. If we\nfurther assume that the non-linearity in the ConvNet is Rectified Linear Unit\n(ReLU) and the reference distribution is Gaussian white noise, then we obtain a\ngenerative ConvNet model that is unique among energy-based models: The model is\npiecewise Gaussian, and the means of the Gaussian pieces are defined by an\nauto-encoder, where the filters in the bottom-up encoding become the basis\nfunctions in the top-down decoding, and the binary activation variables\ndetected by the filters in the bottom-up convolution process become the\ncoefficients of the basis functions in the top-down deconvolution process. The\nLangevin dynamics for sampling the generative ConvNet is driven by the\nreconstruction error of this auto-encoder. The contrastive divergence learning\nof the generative ConvNet reconstructs the training images by the auto-encoder.\nThe maximum likelihood learning algorithm can synthesize realistic natural\nimage patterns.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 04:46:45 GMT"}, {"version": "v2", "created": "Sun, 29 May 2016 05:52:10 GMT"}, {"version": "v3", "created": "Tue, 31 May 2016 06:02:19 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Xie", "Jianwen", ""], ["Lu", "Yang", ""], ["Zhu", "Song-Chun", ""], ["Wu", "Ying Nian", ""]]}, {"id": "1602.03351", "submitter": "Daniel J Mankowitz", "authors": "Daniel J. Mankowitz, Timothy A. Mann, Shie Mannor", "title": "Adaptive Skills, Adaptive Partitions (ASAP)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Adaptive Skills, Adaptive Partitions (ASAP) framework that\n(1) learns skills (i.e., temporally extended actions or options) as well as (2)\nwhere to apply them. We believe that both (1) and (2) are necessary for a truly\ngeneral skill learning framework, which is a key building block needed to scale\nup to lifelong learning agents. The ASAP framework can also solve related new\ntasks simply by adapting where it applies its existing learned skills. We prove\nthat ASAP converges to a local optimum under natural conditions. Finally, our\nexperimental results, which include a RoboCup domain, demonstrate the ability\nof ASAP to learn where to reuse skills as well as solve multiple tasks with\nconsiderably less experience than solving each task from scratch.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 12:35:37 GMT"}, {"version": "v2", "created": "Tue, 7 Jun 2016 19:50:33 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Mankowitz", "Daniel J.", ""], ["Mann", "Timothy A.", ""], ["Mannor", "Shie", ""]]}, {"id": "1602.03368", "submitter": "Aydin Demircioglu", "authors": "Aydin Demircioglu, Daniel Horn, Tobias Glasmachers, Bernd Bischl,\n  Claus Weihs", "title": "Fast model selection by limiting SVM training times", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernelized Support Vector Machines (SVMs) are among the best performing\nsupervised learning methods. But for optimal predictive performance,\ntime-consuming parameter tuning is crucial, which impedes application. To\ntackle this problem, the classic model selection procedure based on grid-search\nand cross-validation was refined, e.g. by data subsampling and direct search\nheuristics. Here we focus on a different aspect, the stopping criterion for SVM\ntraining. We show that by limiting the training time given to the SVM solver\nduring parameter tuning we can reduce model selection times by an order of\nmagnitude.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 13:34:30 GMT"}], "update_date": "2016-02-11", "authors_parsed": [["Demircioglu", "Aydin", ""], ["Horn", "Daniel", ""], ["Glasmachers", "Tobias", ""], ["Bischl", "Bernd", ""], ["Weihs", "Claus", ""]]}, {"id": "1602.03442", "submitter": "Umut \\c{S}im\\c{s}ekli", "authors": "Umut \\c{S}im\\c{s}ekli, Roland Badeau, A. Taylan Cemgil, Ga\\\"el Richard", "title": "Stochastic Quasi-Newton Langevin Monte Carlo", "comments": "Published in ICML 2016, International Conference on Machine Learning\n  2016, New York, NY, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Stochastic Gradient Markov Chain Monte Carlo (SG-MCMC) methods have\nbeen proposed for scaling up Monte Carlo computations to large data problems.\nWhilst these approaches have proven useful in many applications, vanilla\nSG-MCMC might suffer from poor mixing rates when random variables exhibit\nstrong couplings under the target densities or big scale differences. In this\nstudy, we propose a novel SG-MCMC method that takes the local geometry into\naccount by using ideas from Quasi-Newton optimization methods. These second\norder methods directly approximate the inverse Hessian by using a limited\nhistory of samples and their gradients. Our method uses dense approximations of\nthe inverse Hessian while keeping the time and memory complexities linear with\nthe dimension of the problem. We provide a formal theoretical analysis where we\nshow that the proposed method is asymptotically unbiased and consistent with\nthe posterior expectations. We illustrate the effectiveness of the approach on\nboth synthetic and real datasets. Our experiments on two challenging\napplications show that our method achieves fast convergence rates similar to\nRiemannian approaches while at the same time having low computational\nrequirements similar to diagonal preconditioning approaches.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 16:53:36 GMT"}, {"version": "v2", "created": "Mon, 12 Dec 2016 16:06:31 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["\u015eim\u015fekli", "Umut", ""], ["Badeau", "Roland", ""], ["Cemgil", "A. Taylan", ""], ["Richard", "Ga\u00ebl", ""]]}, {"id": "1602.03476", "submitter": "Sewoong Oh", "authors": "Weihao Gao, Sreeram Kannan, Sewoong Oh, Pramod Viswanath", "title": "Conditional Dependence via Shannon Capacity: Axioms, Estimators and\n  Applications", "comments": "43 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We conduct an axiomatic study of the problem of estimating the strength of a\nknown causal relationship between a pair of variables. We propose that an\nestimate of causal strength should be based on the conditional distribution of\nthe effect given the cause (and not on the driving distribution of the cause),\nand study dependence measures on conditional distributions. Shannon capacity,\nappropriately regularized, emerges as a natural measure under these axioms. We\nexamine the problem of calculating Shannon capacity from the observed samples\nand propose a novel fixed-$k$ nearest neighbor estimator, and demonstrate its\nconsistency. Finally, we demonstrate an application to single-cell\nflow-cytometry, where the proposed estimators significantly reduce sample\ncomplexity.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 18:27:04 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2016 19:32:09 GMT"}, {"version": "v3", "created": "Thu, 2 Jun 2016 15:55:46 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Gao", "Weihao", ""], ["Kannan", "Sreeram", ""], ["Oh", "Sewoong", ""], ["Viswanath", "Pramod", ""]]}, {"id": "1602.03481", "submitter": "Sewoong Oh", "authors": "Ashish Khetan and Sewoong Oh", "title": "Achieving Budget-optimality with Adaptive Schemes in Crowdsourcing", "comments": "32 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing platforms provide marketplaces where task requesters can pay to\nget labels on their data. Such markets have emerged recently as popular venues\nfor collecting annotations that are crucial in training machine learning models\nin various applications. However, as jobs are tedious and payments are low,\nerrors are common in such crowdsourced labels. A common strategy to overcome\nsuch noise in the answers is to add redundancy by getting multiple answers for\neach task and aggregating them using some methods such as majority voting. For\nsuch a system, there is a fundamental question of interest: how can we maximize\nthe accuracy given a fixed budget on how many responses we can collect on the\ncrowdsourcing system. We characterize this fundamental trade-off between the\nbudget (how many answers the requester can collect in total) and the accuracy\nin the estimated labels. In particular, we ask whether adaptive task assignment\nschemes lead to a more efficient trade-off between the accuracy and the budget.\n  Adaptive schemes, where tasks are assigned adaptively based on the data\ncollected thus far, are widely used in practical crowdsourcing systems to\nefficiently use a given fixed budget. However, existing theoretical analyses of\ncrowdsourcing systems suggest that the gain of adaptive task assignments is\nminimal. To bridge this gap, we investigate this question under a strictly more\ngeneral probabilistic model, which has been recently introduced to model\npractical crowdsourced annotations. Under this generalized Dawid-Skene model,\nwe characterize the fundamental trade-off between budget and accuracy. We\nintroduce a novel adaptive scheme that matches this fundamental limit. We\nfurther quantify the fundamental gap between adaptive and non-adaptive schemes,\nby comparing the trade-off with the one for non-adaptive schemes. Our analyses\nconfirm that the gap is significant.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 18:46:30 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 04:31:25 GMT"}, {"version": "v3", "created": "Fri, 25 Aug 2017 16:35:55 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Khetan", "Ashish", ""], ["Oh", "Sewoong", ""]]}, {"id": "1602.03506", "submitter": "Max Tegmark", "authors": "Stuart Russell (Berkeley), Daniel Dewey (FHI), Max Tegmark (MIT)", "title": "Research Priorities for Robust and Beneficial Artificial Intelligence", "comments": "This article gives examples of the type of research advocated by the\n  open letter for robust & beneficial AI at\n  http://futureoflife.org/ai-open-letter", "journal-ref": "AI Magazine 36:4 (2015)", "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Success in the quest for artificial intelligence has the potential to bring\nunprecedented benefits to humanity, and it is therefore worthwhile to\ninvestigate how to maximize these benefits while avoiding potential pitfalls.\nThis article gives numerous examples (which should by no means be construed as\nan exhaustive list) of such worthwhile research aimed at ensuring that AI\nremains robust and beneficial.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 20:29:25 GMT"}], "update_date": "2016-02-15", "authors_parsed": [["Russell", "Stuart", "", "Berkeley"], ["Dewey", "Daniel", "", "FHI"], ["Tegmark", "Max", "", "MIT"]]}, {"id": "1602.03534", "submitter": "Ozan Sener", "authors": "Ozan Sener, Hyun Oh Song, Ashutosh Saxena, Silvio Savarese", "title": "Unsupervised Transductive Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised learning with large scale labeled datasets and deep layered models\nhas made a paradigm shift in diverse areas in learning and recognition.\nHowever, this approach still suffers generalization issues under the presence\nof a domain shift between the training and the test data distribution. In this\nregard, unsupervised domain adaptation algorithms have been proposed to\ndirectly address the domain shift problem. In this paper, we approach the\nproblem from a transductive perspective. We incorporate the domain shift and\nthe transductive target inference into our framework by jointly solving for an\nasymmetric similarity metric and the optimal transductive target label\nassignment. We also show that our model can easily be extended for deep feature\nlearning in order to learn features which are discriminative in the target\ndomain. Our experiments show that the proposed method significantly outperforms\nstate-of-the-art algorithms in both object recognition and digit classification\nexperiments by a large margin.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 21:07:23 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2016 22:37:36 GMT"}, {"version": "v3", "created": "Fri, 25 Mar 2016 16:47:54 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Sener", "Ozan", ""], ["Song", "Hyun Oh", ""], ["Saxena", "Ashutosh", ""], ["Savarese", "Silvio", ""]]}, {"id": "1602.03571", "submitter": "Anand Sarwate", "authors": "Tamir Hazan, Francesco Orabona, Anand D. Sarwate, Subhransu Maji,\n  Tommi Jaakkola", "title": "High Dimensional Inference with Random Maximum A-Posteriori\n  Perturbations", "comments": "47 pages, 10 figures, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new approach, called perturb-max, for high-dimensional\nstatistical inference that is based on applying random perturbations followed\nby optimization. This framework injects randomness to maximum a-posteriori\n(MAP) predictors by randomly perturbing the potential function for the input. A\nclassic result from extreme value statistics asserts that perturb-max\noperations generate unbiased samples from the Gibbs distribution using\nhigh-dimensional perturbations. Unfortunately, the computational cost of\ngenerating so many high-dimensional random variables can be prohibitive.\nHowever, when the perturbations are of low dimension, sampling the perturb-max\nprediction is as efficient as MAP optimization. This paper shows that the\nexpected value of perturb-max inference with low dimensional perturbations can\nbe used sequentially to generate unbiased samples from the Gibbs distribution.\nFurthermore the expected value of the maximal perturbations is a natural bound\non the entropy of such perturb-max models. A measure concentration result for\nperturb-max values shows that the deviation of their sampled average from its\nexpectation decays exponentially in the number of samples, allowing effective\napproximation of the expectation.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 23:15:39 GMT"}, {"version": "v2", "created": "Tue, 15 Nov 2016 22:56:18 GMT"}, {"version": "v3", "created": "Tue, 30 May 2017 18:20:26 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Hazan", "Tamir", ""], ["Orabona", "Francesco", ""], ["Sarwate", "Anand D.", ""], ["Maji", "Subhransu", ""], ["Jaakkola", "Tommi", ""]]}, {"id": "1602.03600", "submitter": "Onur Atan", "authors": "Onur Atan, Mihaela van der Schaar", "title": "Data-Driven Online Decision Making with Costly Information Acquisition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most real-world settings such as recommender systems, finance, and\nhealthcare, collecting useful information is costly and requires an active\nchoice on the part of the decision maker. The decision-maker needs to learn\nsimultaneously what observations to make and what actions to take. This paper\nincorporates the information acquisition decision into an online learning\nframework. We propose two different algorithms for this dual learning problem:\nSim-OOS and Seq-OOS where observations are made simultaneously and\nsequentially, respectively. We prove that both algorithms achieve a regret that\nis sublinear in time. The developed framework and algorithms can be used in\nmany applications including medical informatics, recommender systems and\nactionable intelligence in transportation, finance, cyber-security etc., in\nwhich collecting information prior to making decisions is costly. We validate\nour algorithms in a breast cancer example setting in which we show substantial\nperformance gains for our proposed algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 01:43:22 GMT"}, {"version": "v2", "created": "Sat, 21 Oct 2017 01:15:31 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Atan", "Onur", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1602.03619", "submitter": "Jungseul Ok", "authors": "Jungseul Ok, Sewoong Oh, Jinwoo Shin and Yung Yi", "title": "Optimal Inference in Crowdsourced Classification via Belief Propagation", "comments": "This article is partially based on preliminary results published in\n  the proceeding of the 33rd International Conference on Machine Learning (ICML\n  2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing systems are popular for solving large-scale labelling tasks\nwith low-paid workers. We study the problem of recovering the true labels from\nthe possibly erroneous crowdsourced labels under the popular Dawid-Skene model.\nTo address this inference problem, several algorithms have recently been\nproposed, but the best known guarantee is still significantly larger than the\nfundamental limit. We close this gap by introducing a tighter lower bound on\nthe fundamental limit and proving that Belief Propagation (BP) exactly matches\nthis lower bound. The guaranteed optimality of BP is the strongest in the sense\nthat it is information-theoretically impossible for any other algorithm to\ncorrectly label a larger fraction of the tasks. Experimental results suggest\nthat BP is close to optimal for all regimes considered and improves upon\ncompeting state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 05:35:19 GMT"}, {"version": "v2", "created": "Thu, 10 Mar 2016 10:04:23 GMT"}, {"version": "v3", "created": "Wed, 19 Oct 2016 15:28:25 GMT"}, {"version": "v4", "created": "Thu, 12 Jan 2017 01:16:00 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Ok", "Jungseul", ""], ["Oh", "Sewoong", ""], ["Shin", "Jinwoo", ""], ["Yi", "Yung", ""]]}, {"id": "1602.03647", "submitter": "Jonathan Scarlett", "authors": "Jonathan Scarlett and Volkan Cevher", "title": "On the Difficulty of Selecting Ising Models with Approximate Recovery", "comments": "Accepted to IEEE Transactions on Signal and Information Processing\n  over Networks (TSIPN)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG cs.SI math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of estimating the underlying graph\nassociated with an Ising model given a number of independent and identically\ndistributed samples. We adopt an \\emph{approximate recovery} criterion that\nallows for a number of missed edges or incorrectly-included edges, in contrast\nwith the widely-studied exact recovery problem. Our main results provide\ninformation-theoretic lower bounds on the sample complexity for graph classes\nimposing constraints on the number of edges, maximal degree, and other\nproperties. We identify a broad range of scenarios where, either up to constant\nfactors or logarithmic factors, our lower bounds match the best known lower\nbounds for the exact recovery criterion, several of which are known to be tight\nor near-tight. Hence, in these cases, approximate recovery has a similar\ndifficulty to exact recovery in the minimax sense.\n  Our bounds are obtained via a modification of Fano's inequality for handling\nthe approximate recovery criterion, along with suitably-designed ensembles of\ngraphs that can broadly be classed into two categories: (i) Those containing\ngraphs that contain several isolated edges or cliques and are thus difficult to\ndistinguish from the empty graph; (ii) Those containing graphs for which\ncertain groups of nodes are highly correlated, thus making it difficult to\ndetermine precisely which edges connect them. We support our theoretical\nresults on these ensembles with numerical experiments.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 09:25:24 GMT"}, {"version": "v2", "created": "Fri, 8 Jul 2016 15:30:47 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Scarlett", "Jonathan", ""], ["Cevher", "Volkan", ""]]}, {"id": "1602.03670", "submitter": "Paris Giampouras", "authors": "Paris V. Giampouras, Athanasios A. Rontogiannis, Konstantinos E.\n  Themelis, Konstantinos D. Koutroumbas", "title": "Online Low-Rank Subspace Learning from Incomplete Data: A Bayesian View", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting the underlying low-dimensional space where high-dimensional\nsignals often reside has long been at the center of numerous algorithms in the\nsignal processing and machine learning literature during the past few decades.\nAt the same time, working with incomplete (partly observed) large scale\ndatasets has recently been commonplace for diverse reasons. This so called {\\it\nbig data era} we are currently living calls for devising online subspace\nlearning algorithms that can suitably handle incomplete data. Their envisaged\nobjective is to {\\it recursively} estimate the unknown subspace by processing\nstreaming data sequentially, thus reducing computational complexity, while\nobviating the need for storing the whole dataset in memory. In this paper, an\nonline variational Bayes subspace learning algorithm from partial observations\nis presented. To account for the unawareness of the true rank of the subspace,\ncommonly met in practice, low-rankness is explicitly imposed on the sought\nsubspace data matrix by exploiting sparse Bayesian learning principles.\nMoreover, sparsity, {\\it simultaneously} to low-rankness, is favored on the\nsubspace matrix by the sophisticated hierarchical Bayesian scheme that is\nadopted. In doing so, the proposed algorithm becomes adept in dealing with\napplications whereby the underlying subspace may be also sparse, as, e.g., in\nsparse dictionary learning problems. As shown, the new subspace tracking scheme\noutperforms its state-of-the-art counterparts in terms of estimation accuracy,\nin a variety of experiments conducted on simulated and real data.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 10:47:31 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2016 10:27:06 GMT"}], "update_date": "2016-02-15", "authors_parsed": [["Giampouras", "Paris V.", ""], ["Rontogiannis", "Athanasios A.", ""], ["Themelis", "Konstantinos E.", ""], ["Koutroumbas", "Konstantinos D.", ""]]}, {"id": "1602.03681", "submitter": "Tomislav Slijep\\v{c}evi\\'c", "authors": "Tomislav Slijep\\v{c}evi\\'c", "title": "Package equivalence in complex software network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The public package registry npm is one of the biggest software registry. With\nits 216 911 software packages, it forms a big network of software dependencies.\nIn this paper we evaluate various methods for finding similar packages in the\nnpm network, using only the structure of the graph. Namely, we want to find a\nway of categorizing similar packages, which would be useful for recommendation\nsystems. This size enables us to compute meaningful results, as it softened the\nparticularities of the graph. Npm is also quite famous as it is the default\npackage repository of Node.js. We believe that it will make our results\ninteresting for more people than a less used package repository. This makes it\na good subject of analysis of software networks.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 11:26:01 GMT"}], "update_date": "2016-02-12", "authors_parsed": [["Slijep\u010devi\u0107", "Tomislav", ""]]}, {"id": "1602.03683", "submitter": "Hien Nguyen", "authors": "Hien D Nguyen, Luke R Lloyd-Jones, Geoffrey J McLachlan", "title": "A Universal Approximation Theorem for Mixture of Experts Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mixture of experts (MoE) model is a popular neural network architecture\nfor nonlinear regression and classification. The class of MoE mean functions is\nknown to be uniformly convergent to any unknown target function, assuming that\nthe target function is from Sobolev space that is sufficiently differentiable\nand that the domain of estimation is a compact unit hypercube. We provide an\nalternative result, which shows that the class of MoE mean functions is dense\nin the class of all continuous functions over arbitrary compact domains of\nestimation. Our result can be viewed as a universal approximation theorem for\nMoE models.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 11:42:15 GMT"}], "update_date": "2016-02-12", "authors_parsed": [["Nguyen", "Hien D", ""], ["Lloyd-Jones", "Luke R", ""], ["McLachlan", "Geoffrey J", ""]]}, {"id": "1602.03807", "submitter": "John Ingraham", "authors": "John Ingraham, Debora Marks", "title": "Variational Inference for Sparse and Undirected Models", "comments": "34th International Conference on Machine Learning (ICML 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.dis-nn physics.data-an q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Undirected graphical models are applied in genomics, protein structure\nprediction, and neuroscience to identify sparse interactions that underlie\ndiscrete data. Although Bayesian methods for inference would be favorable in\nthese contexts, they are rarely used because they require doubly intractable\nMonte Carlo sampling. Here, we develop a framework for scalable Bayesian\ninference of discrete undirected models based on two new methods. The first is\nPersistent VI, an algorithm for variational inference of discrete undirected\nmodels that avoids doubly intractable MCMC and approximations of the partition\nfunction. The second is Fadeout, a reparameterization approach for variational\ninference under sparsity-inducing priors that captures a posteriori\ncorrelations between parameters and hyperparameters with noncentered\nparameterizations. We find that, together, these methods for variational\ninference substantially improve learning of sparse undirected graphical models\nin simulated and real problems from physics and biology.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 18:06:40 GMT"}, {"version": "v2", "created": "Wed, 14 Jun 2017 16:06:11 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Ingraham", "John", ""], ["Marks", "Debora", ""]]}, {"id": "1602.03861", "submitter": "Subhadeep Mukhopadhyay", "authors": "Subhadeep Mukhopadhyay", "title": "Unified Statistical Theory of Spectral Graph Analysis", "comments": "Major changes have been done in terms of contents and structure of\n  the paper. New set of motivations for GraField, Expanding Section 4,\n  Connections with Diffusion map and Google's PageRank method etc", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to show that there exists a simple, yet universal\nstatistical logic of spectral graph analysis by recasting it into a\nnonparametric function estimation problem. The prescribed viewpoint appears to\nbe good enough to accommodate most of the existing spectral graph techniques as\na consequence of just one single formalism and algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 20:05:38 GMT"}, {"version": "v2", "created": "Fri, 4 Mar 2016 01:24:51 GMT"}, {"version": "v3", "created": "Sun, 20 Mar 2016 13:13:13 GMT"}, {"version": "v4", "created": "Tue, 20 Sep 2016 21:35:31 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Mukhopadhyay", "Subhadeep", ""]]}, {"id": "1602.03943", "submitter": "Naman Agarwal", "authors": "Naman Agarwal, Brian Bullins, Elad Hazan", "title": "Second-Order Stochastic Optimization for Machine Learning in Linear Time", "comments": null, "journal-ref": "Journal of Machine Learning Research 18(116) (2017) 1-40", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  First-order stochastic methods are the state-of-the-art in large-scale\nmachine learning optimization owing to efficient per-iteration complexity.\nSecond-order methods, while able to provide faster convergence, have been much\nless explored due to the high cost of computing the second-order information.\nIn this paper we develop second-order stochastic methods for optimization\nproblems in machine learning that match the per-iteration cost of gradient\nbased methods, and in certain settings improve upon the overall running time\nover popular first-order methods. Furthermore, our algorithm has the desirable\nproperty of being implementable in time linear in the sparsity of the input\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 01:38:05 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2016 19:31:38 GMT"}, {"version": "v3", "created": "Tue, 15 Mar 2016 19:26:55 GMT"}, {"version": "v4", "created": "Fri, 14 Oct 2016 16:45:09 GMT"}, {"version": "v5", "created": "Thu, 30 Nov 2017 18:38:02 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Agarwal", "Naman", ""], ["Bullins", "Brian", ""], ["Hazan", "Elad", ""]]}, {"id": "1602.03950", "submitter": "Hong Zhao", "authors": "Hong Zhao", "title": "General Vector Machine", "comments": "57pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The support vector machine (SVM) is an important class of learning machines\nfor function approach, pattern recognition, and time-serious prediction, etc.\nIt maps samples into the feature space by so-called support vectors of selected\nsamples, and then feature vectors are separated by maximum margin hyperplane.\nThe present paper presents the general vector machine (GVM) to replace the SVM.\nThe support vectors are replaced by general project vectors selected from the\nusual vector space, and a Monte Carlo (MC) algorithm is developed to find the\ngeneral vectors. The general project vectors improves the feature-extraction\nability, and the MC algorithm can control the width of the separation margin of\nthe hyperplane. By controlling the separation margin, we show that the maximum\nmargin hyperplane can usually induce the overlearning, and the best learning\nmachine is achieved with a proper separation margin. Applications in function\napproach, pattern recognition, and classification indicate that the developed\nmethod is very successful, particularly for small-set training problems.\nAdditionally, our algorithm may induce some particular applications, such as\nfor the transductive inference.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 02:55:34 GMT"}], "update_date": "2016-02-15", "authors_parsed": [["Zhao", "Hong", ""]]}, {"id": "1602.03990", "submitter": "Li Ma", "authors": "Li Ma and Jacopo Soriano", "title": "Efficient functional ANOVA through wavelet-domain Markov groves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a wavelet-domain functional analysis of variance (fANOVA) method\nbased on a Bayesian hierarchical model. The factor effects are modeled through\na spike-and-slab mixture at each location-scale combination along with a\nnormal-inverse-Gamma (NIG) conjugate setup for the coefficients and errors. A\ngraphical model called the Markov grove (MG) is designed to jointly model the\nspike-and-slab statuses at all location-scale combinations, which incorporates\nthe clustering of each factor effect in the wavelet-domain thereby allowing\nborrowing of strength across location and scale. The posterior of this NIG-MG\nmodel is analytically available through a pyramid algorithm of the same\ncomputational complexity as Mallat's pyramid algorithm for discrete wavelet\ntransform, i.e., linear in both the number of observations and the number of\nlocations. Posterior probabilities of factor contributions can also be computed\nthrough pyramid recursion, and exact samples from the posterior can be drawn\nwithout MCMC. We investigate the performance of our method through extensive\nsimulation and show that it outperforms existing wavelet-domain fANOVA methods\nin a variety of common settings. We apply the method to analyzing the orthosis\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 09:31:34 GMT"}, {"version": "v2", "created": "Sun, 21 Feb 2016 03:56:08 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Ma", "Li", ""], ["Soriano", "Jacopo", ""]]}, {"id": "1602.03992", "submitter": "Konstantinos Benidis", "authors": "Konstantinos Benidis, Ying Sun, Prabhu Babu, and Daniel P. Palomar", "title": "Orthogonal Sparse PCA and Covariance Estimation via Procrustes\n  Reformulation", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2016.2605073", "report-no": null, "categories": "stat.ML cs.LG math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of estimating sparse eigenvectors of a symmetric matrix attracts\na lot of attention in many applications, especially those with high dimensional\ndata set. While classical eigenvectors can be obtained as the solution of a\nmaximization problem, existing approaches formulate this problem by adding a\npenalty term into the objective function that encourages a sparse solution.\nHowever, the resulting methods achieve sparsity at the expense of sacrificing\nthe orthogonality property. In this paper, we develop a new method to estimate\ndominant sparse eigenvectors without trading off their orthogonality. The\nproblem is highly non-convex and hard to handle. We apply the MM framework\nwhere we iteratively maximize a tight lower bound (surrogate function) of the\nobjective function over the Stiefel manifold. The inner maximization problem\nturns out to be a rectangular Procrustes problem, which has a closed form\nsolution. In addition, we propose a method to improve the covariance estimation\nproblem when its underlying eigenvectors are known to be sparse. We use the\neigenvalue decomposition of the covariance matrix to formulate an optimization\nproblem where we impose sparsity on the corresponding eigenvectors. Numerical\nexperiments show that the proposed eigenvector extraction algorithm matches or\noutperforms existing algorithms in terms of support recovery and explained\nvariance, while the covariance estimation algorithms improve significantly the\nsample covariance estimator.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 09:48:22 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Benidis", "Konstantinos", ""], ["Sun", "Ying", ""], ["Babu", "Prabhu", ""], ["Palomar", "Daniel P.", ""]]}, {"id": "1602.04129", "submitter": "Carlo Baldassi", "authors": "Carlo Baldassi, Federica Gerace, Carlo Lucibello, Luca Saglietti,\n  Riccardo Zecchina", "title": "Learning may need only a few bits of synaptic precision", "comments": "38 pages (main text: 16 pages), 5 figures;\n  http://link.aps.org/doi/10.1103/PhysRevE.93.052313", "journal-ref": "Phys. Rev. E 93, 052313 (2016)", "doi": "10.1103/PhysRevE.93.052313", "report-no": null, "categories": "cond-mat.dis-nn q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning in neural networks poses peculiar challenges when using discretized\nrather then continuous synaptic states. The choice of discrete synapses is\nmotivated by biological reasoning and experiments, and possibly by hardware\nimplementation considerations as well. In this paper we extend a previous large\ndeviations analysis which unveiled the existence of peculiar dense regions in\nthe space of synaptic states which accounts for the possibility of learning\nefficiently in networks with binary synapses. We extend the analysis to\nsynapses with multiple states and generally more plausible biological features.\nThe results clearly indicate that the overall qualitative picture is unchanged\nwith respect to the binary case, and very robust to variation of the details of\nthe model. We also provide quantitative results which suggest that the\nadvantages of increasing the synaptic precision (i.e.~the number of internal\nsynaptic states) rapidly vanish after the first few bits, and therefore that,\nfor practical applications, only few bits may be needed for near-optimal\nperformance, consistently with recent biological findings. Finally, we\ndemonstrate how the theoretical analysis can be exploited to design efficient\nalgorithmic search strategies.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 17:27:15 GMT"}, {"version": "v2", "created": "Fri, 27 May 2016 18:45:51 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Baldassi", "Carlo", ""], ["Gerace", "Federica", ""], ["Lucibello", "Carlo", ""], ["Saglietti", "Luca", ""], ["Zecchina", "Riccardo", ""]]}, {"id": "1602.04133", "submitter": "Thang Bui", "authors": "Thang D. Bui and Daniel Hern\\'andez-Lobato and Yingzhen Li and Jos\\'e\n  Miguel Hern\\'andez-Lobato and Richard E. Turner", "title": "Deep Gaussian Processes for Regression using Approximate Expectation\n  Propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Gaussian processes (DGPs) are multi-layer hierarchical generalisations\nof Gaussian processes (GPs) and are formally equivalent to neural networks with\nmultiple, infinitely wide hidden layers. DGPs are nonparametric probabilistic\nmodels and as such are arguably more flexible, have a greater capacity to\ngeneralise, and provide better calibrated uncertainty estimates than\nalternative deep models. This paper develops a new approximate Bayesian\nlearning scheme that enables DGPs to be applied to a range of medium to large\nscale regression problems for the first time. The new method uses an\napproximate Expectation Propagation procedure and a novel and efficient\nextension of the probabilistic backpropagation algorithm for learning. We\nevaluate the new method for non-linear regression on eleven real-world\ndatasets, showing that it always outperforms GP regression and is almost always\nbetter than state-of-the-art deterministic and sampling-based approximate\ninference methods for Bayesian neural networks. As a by-product, this work\nprovides a comprehensive analysis of six approximate Bayesian methods for\ntraining neural networks.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 17:32:39 GMT"}], "update_date": "2016-02-15", "authors_parsed": [["Bui", "Thang D.", ""], ["Hern\u00e1ndez-Lobato", "Daniel", ""], ["Li", "Yingzhen", ""], ["Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", ""], ["Turner", "Richard E.", ""]]}, {"id": "1602.04208", "submitter": "Martin Jaggi", "authors": "Rajiv Khanna, Michael Tschannen, Martin Jaggi", "title": "Pursuits in Structured Non-Convex Matrix Factorizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficiently representing real world data in a succinct and parsimonious\nmanner is of central importance in many fields. We present a generalized greedy\npursuit framework, allowing us to efficiently solve structured matrix\nfactorization problems, where the factors are allowed to be from arbitrary sets\nof structured vectors. Such structure may include sparsity, non-negativeness,\norder, or a combination thereof. The algorithm approximates a given matrix by a\nlinear combination of few rank-1 matrices, each factorized into an outer\nproduct of two vector atoms of the desired structure. For the non-convex\nsubproblems of obtaining good rank-1 structured matrix atoms, we employ and\nanalyze a general atomic power method. In addition to the above applications,\nwe prove linear convergence for generalized pursuit variants in Hilbert spaces\n- for the task of approximation over the linear span of arbitrary dictionaries\n- which generalizes OMP and is useful beyond matrix problems. Our experiments\non real datasets confirm both the efficiency and also the broad applicability\nof our framework in practice.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 20:57:35 GMT"}], "update_date": "2016-02-15", "authors_parsed": [["Khanna", "Rajiv", ""], ["Tschannen", "Michael", ""], ["Jaggi", "Martin", ""]]}, {"id": "1602.04227", "submitter": "Patrick Rebeschini", "authors": "Patrick Rebeschini and Sekhar Tatikonda", "title": "Scale-free network optimization: foundations and algorithms", "comments": "arXiv admin note: substantial text overlap with arXiv:1509.06246", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the fundamental principles that drive the development of\nscalable algorithms for network optimization. Despite the significant amount of\nwork on parallel and decentralized algorithms in the optimization community,\nthe methods that have been proposed typically rely on strict separability\nassumptions for objective function and constraints. Beside sparsity, these\nmethods typically do not exploit the strength of the interaction between\nvariables in the system. We propose a notion of correlation in constrained\noptimization that is based on the sensitivity of the optimal solution upon\nperturbations of the constraints. We develop a general theory of sensitivity of\noptimizers the extends beyond the infinitesimal setting. We present instances\nin network optimization where the correlation decays exponentially fast with\nrespect to the natural distance in the network, and we design algorithms that\ncan exploit this decay to yield dimension-free optimization. Our results are\nthe first of their kind, and open new possibilities in the theory of local\nalgorithms.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 21:09:56 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Rebeschini", "Patrick", ""], ["Tatikonda", "Sekhar", ""]]}, {"id": "1602.04259", "submitter": "Viktoriya Krakovna", "authors": "Viktoriya Krakovna, Moshe Looks", "title": "A Minimalistic Approach to Sum-Product Network Learning for Real\n  Applications", "comments": "Accepted to ICLR 2016 workshop track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sum-Product Networks (SPNs) are a class of expressive yet tractable\nhierarchical graphical models. LearnSPN is a structure learning algorithm for\nSPNs that uses hierarchical co-clustering to simultaneously identifying similar\nentities and similar features. The original LearnSPN algorithm assumes that all\nthe variables are discrete and there is no missing data. We introduce a\npractical, simplified version of LearnSPN, MiniSPN, that runs faster and can\nhandle missing data and heterogeneous features common in real applications. We\ndemonstrate the performance of MiniSPN on standard benchmark datasets and on\ntwo datasets from Google's Knowledge Graph exhibiting high missingness rates\nand a mix of discrete and continuous features.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 23:11:05 GMT"}, {"version": "v2", "created": "Thu, 24 Mar 2016 22:37:52 GMT"}, {"version": "v3", "created": "Sun, 24 Apr 2016 23:38:43 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Krakovna", "Viktoriya", ""], ["Looks", "Moshe", ""]]}, {"id": "1602.04265", "submitter": "Kam Chung Wong", "authors": "Kam Chung Wong, Zifan Li and Ambuj Tewari", "title": "Lasso Guarantees for Time Series Estimation Under Subgaussian Tails and\n  $ \\beta $-Mixing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many theoretical results on estimation of high dimensional time series\nrequire specifying an underlying data generating model (DGM). Instead, along\nthe footsteps of~\\cite{wong2017lasso}, this paper relies only on (strict)\nstationarity and $ \\beta $-mixing condition to establish consistency of lasso\nwhen data comes from a $\\beta$-mixing process with marginals having subgaussian\ntails. Because of the general assumptions, the data can come from DGMs\ndifferent than standard time series models such as VAR or ARCH. When the true\nDGM is not VAR, the lasso estimates correspond to those of the best linear\npredictors using the past observations. We establish non-asymptotic\ninequalities for estimation and prediction errors of the lasso estimates.\nTogether with~\\cite{wong2017lasso}, we provide lasso guarantees that cover full\nspectrum of the parameters in specifications of $ \\beta $-mixing subgaussian\ntime series. Applications of these results potentially extend to non-Gaussian,\nnon-Markovian and non-linear times series models as the examples we provide\ndemonstrate. In order to prove our results, we derive a novel Hanson-Wright\ntype concentration inequality for $\\beta$-mixing subgaussian random vectors\nthat may be of independent interest.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 23:44:53 GMT"}, {"version": "v2", "created": "Thu, 18 May 2017 18:20:17 GMT"}, {"version": "v3", "created": "Tue, 23 May 2017 17:52:08 GMT"}, {"version": "v4", "created": "Mon, 5 Feb 2018 17:42:51 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Wong", "Kam Chung", ""], ["Li", "Zifan", ""], ["Tewari", "Ambuj", ""]]}, {"id": "1602.04277", "submitter": "Renzhi Cao", "authors": "Renzhi Cao, Taeho Jo, Jianlin Cheng", "title": "Evaluation of Protein Structural Models Using Random Forests", "comments": "13 pages, 3 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.BM q-bio.QM stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Protein structure prediction has been a grand challenge problem in the\nstructure biology over the last few decades. Protein quality assessment plays a\nvery important role in protein structure prediction. In the paper, we propose a\nnew protein quality assessment method which can predict both local and global\nquality of the protein 3D structural models. Our method uses both multi and\nsingle model quality assessment method for global quality assessment, and uses\nchemical, physical, geo-metrical features, and global quality score for local\nquality assessment. CASP9 targets are used to generate the features for local\nquality assessment. We evaluate the performance of our local quality assessment\nmethod on CASP10, which is comparable with two stage-of-art QA methods based on\nthe average absolute distance between the real and predicted distance. In\naddition, we blindly tested our method on CASP11, and the good performance\nshows that combining single and multiple model quality assessment method could\nbe a good way to improve the accuracy of model quality assessment, and the\nrandom forest technique could be used to train a good local quality assessment\nmodel.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2016 03:18:23 GMT"}], "update_date": "2016-02-20", "authors_parsed": [["Cao", "Renzhi", ""], ["Jo", "Taeho", ""], ["Cheng", "Jianlin", ""]]}, {"id": "1602.04282", "submitter": "Roshan Shariff", "authors": "Yifan Wu, Roshan Shariff, Tor Lattimore and Csaba Szepesv\\'ari", "title": "Conservative Bandits", "comments": "9 pages, plus 4-page appendix, with 3 figures. Submitted to ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a novel multi-armed bandit problem that models the challenge faced\nby a company wishing to explore new strategies to maximize revenue whilst\nsimultaneously maintaining their revenue above a fixed baseline, uniformly over\ntime. While previous work addressed the problem under the weaker requirement of\nmaintaining the revenue constraint only at a given fixed time in the future,\nthe algorithms previously proposed are unsuitable due to their design under the\nmore stringent constraints. We consider both the stochastic and the adversarial\nsettings, where we propose, natural, yet novel strategies and analyze the price\nfor maintaining the constraints. Amongst other things, we prove both high\nprobability and expectation bounds on the regret, while we also consider both\nthe problem of maintaining the constraints with high probability or\nexpectation. For the adversarial setting the price of maintaining the\nconstraint appears to be higher, at least for the algorithm considered. A lower\nbound is given showing that the algorithm for the stochastic setting is almost\noptimal. Empirical results obtained in synthetic environments complement our\ntheoretical findings.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2016 03:47:11 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Wu", "Yifan", ""], ["Shariff", "Roshan", ""], ["Lattimore", "Tor", ""], ["Szepesv\u00e1ri", "Csaba", ""]]}, {"id": "1602.04283", "submitter": "Griffin Lacey", "authors": "Griffin Lacey, Graham W. Taylor, Shawki Areibi", "title": "Deep Learning on FPGAs: Past, Present, and Future", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid growth of data size and accessibility in recent years has\ninstigated a shift of philosophy in algorithm design for artificial\nintelligence. Instead of engineering algorithms by hand, the ability to learn\ncomposable systems automatically from massive amounts of data has led to\nground-breaking performance in important domains such as computer vision,\nspeech recognition, and natural language processing. The most popular class of\ntechniques used in these domains is called deep learning, and is seeing\nsignificant attention from industry. However, these models require incredible\namounts of data and compute power to train, and are limited by the need for\nbetter hardware acceleration to accommodate scaling beyond current data and\nmodel sizes. While the current solution has been to use clusters of graphics\nprocessing units (GPU) as general purpose processors (GPGPU), the use of field\nprogrammable gate arrays (FPGA) provide an interesting alternative. Current\ntrends in design tools for FPGAs have made them more compatible with the\nhigh-level software practices typically practiced in the deep learning\ncommunity, making FPGAs more accessible to those who build and deploy models.\nSince FPGA architectures are flexible, this could also allow researchers the\nability to explore model-level optimizations beyond what is possible on fixed\narchitectures such as GPUs. As well, FPGAs tend to provide high performance per\nwatt of power consumption, which is of particular importance for application\nscientists interested in large scale server-based deployment or\nresource-limited embedded applications. This review takes a look at deep\nlearning and FPGAs from a hardware acceleration perspective, identifying trends\nand innovations that make these technologies a natural fit, and motivates a\ndiscussion on how FPGAs may best serve the needs of the deep learning community\nmoving forward.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2016 03:50:37 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Lacey", "Griffin", ""], ["Taylor", "Graham W.", ""], ["Areibi", "Shawki", ""]]}, {"id": "1602.04287", "submitter": "Yu-Xiang Wang", "authors": "Yu-Xiang Wang, Jing Lei, Stephen E. Fienberg", "title": "A Minimax Theory for Adaptive Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In adaptive data analysis, the user makes a sequence of queries on the data,\nwhere at each step the choice of query may depend on the results in previous\nsteps. The releases are often randomized in order to reduce overfitting for\nsuch adaptively chosen queries. In this paper, we propose a minimax framework\nfor adaptive data analysis. Assuming Gaussianity of queries, we establish the\nfirst sharp minimax lower bound on the squared error in the order of\n$O(\\frac{\\sqrt{k}\\sigma^2}{n})$, where $k$ is the number of queries asked, and\n$\\sigma^2/n$ is the ordinary signal-to-noise ratio for a single query. Our\nlower bound is based on the construction of an approximately least favorable\nadversary who picks a sequence of queries that are most likely to be affected\nby overfitting. This approximately least favorable adversary uses only one\nlevel of adaptivity, suggesting that the minimax risk for 1-step adaptivity\nwith k-1 initial releases and that for $k$-step adaptivity are on the same\norder. The key technical component of the lower bound proof is a reduction to\nfinding the convoluting distribution that optimally obfuscates the sign of a\nGaussian signal. Our lower bound construction also reveals a transparent and\nelementary proof of the matching upper bound as an alternative approach to\nRusso and Zou (2015), who used information-theoretic tools to provide the same\nupper bound. We believe that the proposed framework opens up opportunities to\nobtain theoretical insights for many other settings of adaptive data analysis,\nwhich would extend the idea to more practical realms.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2016 04:18:03 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Wang", "Yu-Xiang", ""], ["Lei", "Jing", ""], ["Fienberg", "Stephen E.", ""]]}, {"id": "1602.04302", "submitter": "Ganzhao Yuan", "authors": "Ganzhao Yuan, Yin Yang, Zhenjie Zhang, Zhifeng Hao", "title": "Convex Optimization for Linear Query Processing under Approximate\n  Differential Privacy", "comments": "to appear in ACM SIGKDD 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy enables organizations to collect accurate aggregates\nover sensitive data with strong, rigorous guarantees on individuals' privacy.\nPrevious work has found that under differential privacy, computing multiple\ncorrelated aggregates as a batch, using an appropriate \\emph{strategy}, may\nyield higher accuracy than computing each of them independently. However,\nfinding the best strategy that maximizes result accuracy is non-trivial, as it\ninvolves solving a complex constrained optimization program that appears to be\nnon-linear and non-convex. Hence, in the past much effort has been devoted in\nsolving this non-convex optimization program. Existing approaches include\nvarious sophisticated heuristics and expensive numerical solutions. None of\nthem, however, guarantees to find the optimal solution of this optimization\nproblem.\n  This paper points out that under ($\\epsilon$, $\\delta$)-differential privacy,\nthe optimal solution of the above constrained optimization problem in search of\na suitable strategy can be found, rather surprisingly, by solving a simple and\nelegant convex optimization program. Then, we propose an efficient algorithm\nbased on Newton's method, which we prove to always converge to the optimal\nsolution with linear global convergence rate and quadratic local convergence\nrate. Empirical evaluations demonstrate the accuracy and efficiency of the\nproposed solution.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2016 08:31:14 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2016 06:51:59 GMT"}, {"version": "v3", "created": "Mon, 16 May 2016 23:20:13 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Yuan", "Ganzhao", ""], ["Yang", "Yin", ""], ["Zhang", "Zhenjie", ""], ["Hao", "Zhifeng", ""]]}, {"id": "1602.04358", "submitter": "Leonid Gugel", "authors": "Leonid Gugel, Yoel Shkolnisky, Shai Dekel", "title": "Machine olfaction using time scattering of sensor multiresolution graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we construct a learning architecture for high dimensional time\nseries sampled by sensor arrangements. Using a redundant wavelet decomposition\non a graph constructed over the sensor locations, our algorithm is able to\nconstruct discriminative features that exploit the mutual information between\nthe sensors. The algorithm then applies scattering networks to the time series\ngraphs to create the feature space. We demonstrate our method on a machine\nolfaction problem, where one needs to classify the gas type and the location\nwhere it originates from data sampled by an array of sensors. Our experimental\nresults clearly demonstrate that our method outperforms classical machine\nlearning techniques used in previous studies.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2016 17:25:03 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Gugel", "Leonid", ""], ["Shkolnisky", "Yoel", ""], ["Dekel", "Shai", ""]]}, {"id": "1602.04379", "submitter": "Gustavo Amaral Mr", "authors": "Jean Pierre von der Weid, Mario H. Souto, Joaquim D. Garcia, and\n  Gustavo C. Amaral", "title": "Adaptive Filter for Automatic Identification of Multiple Faults in a\n  Noisy OTDR Profile", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": "10.1109/JLT.2016.2570302", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel methodology able to distinguish meaningful level shifts\nfrom typical signal fluctuations. A two-stage regularization filtering can\naccurately identify the location of the significant level-shifts with an\nefficient parameter-free algorithm. The developed methodology demands low\ncomputational effort and can easily be embedded in a dedicated processing unit.\nOur case studies compare the new methodology with current available ones and\nshow that it is the most adequate technique for fast detection of multiple\nunknown level-shifts in a noisy OTDR profile.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2016 21:03:13 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["von der Weid", "Jean Pierre", ""], ["Souto", "Mario H.", ""], ["Garcia", "Joaquim D.", ""], ["Amaral", "Gustavo C.", ""]]}, {"id": "1602.04391", "submitter": "Kinjal Basu", "authors": "Kinjal Basu, Shaunak Chatterjee, Ankan Saha", "title": "Constrained Multi-Slot Optimization for Ranking Recommendations", "comments": "12 Pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranking items to be recommended to users is one of the main problems in large\nscale social media applications. This problem can be set up as a\nmulti-objective optimization problem to allow for trading off multiple,\npotentially conflicting objectives (that are driven by those items) against\neach other. Most previous approaches to this problem optimize for a single slot\nwithout considering the interaction effect of these items on one another.\n  In this paper, we develop a constrained multi-slot optimization formulation,\nwhich allows for modeling interactions among the items on the different slots.\nWe characterize the solution in terms of problem parameters and identify\nconditions under which an efficient solution is possible. The problem\nformulation results in a quadratically constrained quadratic program (QCQP). We\nprovide an algorithm that gives us an efficient solution by relaxing the\nconstraints of the QCQP minimally. Through simulated experiments, we show the\nbenefits of modeling interactions in a multi-slot ranking context, and the\nspeed and accuracy of our QCQP approximate solver against other state of the\nart methods.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2016 22:22:54 GMT"}, {"version": "v2", "created": "Tue, 16 May 2017 06:30:42 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Basu", "Kinjal", ""], ["Chatterjee", "Shaunak", ""], ["Saha", "Ankan", ""]]}, {"id": "1602.04393", "submitter": "Abhinav Maurya", "authors": "Abhinav Maurya, Kenton Murray, Yandong Liu, Chris Dyer, William W.\n  Cohen, Daniel B. Neill", "title": "Semantic Scan: Detecting Subtle, Spatially Localized Events in Text\n  Streams", "comments": "10 pages, 4 figures, KDD 2016 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early detection and precise characterization of emerging topics in text\nstreams can be highly useful in applications such as timely and targeted public\nhealth interventions and discovering evolving regional business trends. Many\nmethods have been proposed for detecting emerging events in text streams using\ntopic modeling. However, these methods have numerous shortcomings that make\nthem unsuitable for rapid detection of locally emerging events on massive text\nstreams. In this paper, we describe Semantic Scan (SS) that has been developed\nspecifically to overcome these shortcomings in detecting new spatially compact\nevents in text streams.\n  Semantic Scan integrates novel contrastive topic modeling with online\ndocument assignment and principled likelihood ratio-based spatial scanning to\nidentify emerging events with unexpected patterns of keywords hidden in text\nstreams. This enables more timely and accurate detection and characterization\nof anomalous, spatially localized emerging events. Semantic Scan does not\nrequire manual intervention or labeled training data, and is robust to noise in\nreal-world text data since it identifies anomalous text patterns that occur in\na cluster of new documents rather than an anomaly in a single new document.\n  We compare Semantic Scan to alternative state-of-the-art methods such as\nTopics over Time, Online LDA, and Labeled LDA on two real-world tasks: (i) a\ndisease surveillance task monitoring free-text Emergency Department chief\ncomplaints in Allegheny County, and (ii) an emerging business trend detection\ntask based on Yelp reviews. On both tasks, we find that Semantic Scan provides\nsignificantly better event detection and characterization accuracy than\ncompeting approaches, while providing up to an order of magnitude speedup.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2016 22:33:56 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Maurya", "Abhinav", ""], ["Murray", "Kenton", ""], ["Liu", "Yandong", ""], ["Dyer", "Chris", ""], ["Cohen", "William W.", ""], ["Neill", "Daniel B.", ""]]}, {"id": "1602.04398", "submitter": "Yanjun Li", "authors": "Yanjun Li, Yoram Bresler", "title": "Joint Dimensionality Reduction for Two Feature Vectors", "comments": "19 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning problems, especially multi-modal learning problems,\nhave two sets of distinct features (e.g., image and text features in news story\nclassification, or neuroimaging data and neurocognitive data in cognitive\nscience research). This paper addresses the joint dimensionality reduction of\ntwo feature vectors in supervised learning problems. In particular, we assume a\ndiscriminative model where low-dimensional linear embeddings of the two feature\nvectors are sufficient statistics for predicting a dependent variable. We show\nthat a simple algorithm involving singular value decomposition can accurately\nestimate the embeddings provided that certain sample complexities are\nsatisfied, without specifying the nonlinear link function (regressor or\nclassifier). The main results establish sample complexities under multiple\nsettings. Sample complexities for different link functions only differ by\nconstant factors.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2016 23:11:56 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2016 06:01:05 GMT"}, {"version": "v3", "created": "Mon, 31 Oct 2016 16:57:32 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Li", "Yanjun", ""], ["Bresler", "Yoram", ""]]}, {"id": "1602.04418", "submitter": "Gunwoong Park", "authors": "Gunwoong Park and Garvesh Raskutti", "title": "Identifiability Assumptions and Algorithm for Directed Graphical Models\n  with Feedback", "comments": "28 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Directed graphical models provide a useful framework for modeling causal or\ndirectional relationships for multivariate data. Prior work has largely focused\non identifiability and search algorithms for directed acyclic graphical (DAG)\nmodels. In many applications, feedback naturally arises and directed graphical\nmodels that permit cycles occur. In this paper we address the issue of\nidentifiability for general directed cyclic graphical (DCG) models satisfying\nthe Markov assumption. In particular, in addition to the faithfulness\nassumption which has already been introduced for cyclic models, we introduce\ntwo new identifiability assumptions, one based on selecting the model with the\nfewest edges and the other based on selecting the DCG model that entails the\nmaximum number of d-separation rules. We provide theoretical results comparing\nthese assumptions which show that: (1) selecting models with the largest number\nof d-separation rules is strictly weaker than the faithfulness assumption; (2)\nunlike for DAG models, selecting models with the fewest edges does not\nnecessarily result in a milder assumption than the faithfulness assumption. We\nalso provide connections between our two new principles and minimality\nassumptions. We use our identifiability assumptions to develop search\nalgorithms for small-scale DCG models. Our simulation study supports our\ntheoretical results, showing that the algorithms based on our two new\nprinciples generally out-perform algorithms based on the faithfulness\nassumption in terms of selecting the true skeleton for DCG models.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2016 05:15:50 GMT"}, {"version": "v2", "created": "Wed, 6 Jul 2016 04:44:52 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Park", "Gunwoong", ""], ["Raskutti", "Garvesh", ""]]}, {"id": "1602.04434", "submitter": "Andreas Loukas", "authors": "Andreas Loukas and Damien Foucard", "title": "Frequency Analysis of Temporal Graph Signals", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This letter extends the concept of graph-frequency to graph signals that\nevolve with time. Our goal is to generalize and, in fact, unify the familiar\nconcepts from time- and graph-frequency analysis. To this end, we study a joint\ntemporal and graph Fourier transform (JFT) and demonstrate its attractive\nproperties. We build on our results to create filters which act on the joint\n(temporal and graph) frequency domain, and show how these can be used to\nperform interference cancellation. The proposed algorithms are distributed,\nhave linear complexity, and can approximate any desired joint filtering\nobjective.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2016 09:48:56 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Loukas", "Andreas", ""], ["Foucard", "Damien", ""]]}, {"id": "1602.04436", "submitter": "Andreas Loukas", "authors": "Elvin Isufi and Andreas Loukas and Andrea Simonetto and Geert Leus", "title": "Autoregressive Moving Average Graph Filtering", "comments": null, "journal-ref": "IEEE Transactions on Signal Processing, vol. 67 (2), pages 274 -\n  288, 2017", "doi": "10.1109/TSP.2016.2614793", "report-no": null, "categories": "cs.LG cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the cornerstones of the field of signal processing on graphs are graph\nfilters, direct analogues of classical filters, but intended for signals\ndefined on graphs. This work brings forth new insights on the distributed graph\nfiltering problem. We design a family of autoregressive moving average (ARMA)\nrecursions, which (i) are able to approximate any desired graph frequency\nresponse, and (ii) give exact solutions for tasks such as graph signal\ndenoising and interpolation. The design philosophy, which allows us to design\nthe ARMA coefficients independently from the underlying graph, renders the ARMA\ngraph filters suitable in static and, particularly, time-varying settings. The\nlatter occur when the graph signal and/or graph are changing over time. We show\nthat in case of a time-varying graph signal our approach extends naturally to a\ntwo-dimensional filter, operating concurrently in the graph and regular time\ndomains. We also derive sufficient conditions for filter stability when the\ngraph and signal are time-varying. The analytical and numerical results\npresented in this paper illustrate that ARMA graph filters are practically\nappealing for static and time-varying settings, as predicted by theoretical\nderivations.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2016 10:14:54 GMT"}, {"version": "v2", "created": "Wed, 21 Sep 2016 14:34:07 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Isufi", "Elvin", ""], ["Loukas", "Andreas", ""], ["Simonetto", "Andrea", ""], ["Leus", "Geert", ""]]}, {"id": "1602.04474", "submitter": "Alessandro Rudi", "authors": "Alessandro Rudi and Lorenzo Rosasco", "title": "Generalization Properties of Learning with Random Features", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the generalization properties of ridge regression with random\nfeatures in the statistical learning framework. We show for the first time that\n$O(1/\\sqrt{n})$ learning bounds can be achieved with only $O(\\sqrt{n}\\log n)$\nrandom features rather than $O({n})$ as suggested by previous results. Further,\nwe prove faster learning rates and show that they might require more random\nfeatures, unless they are sampled according to a possibly problem dependent\ndistribution. Our results shed light on the statistical computational\ntrade-offs in large scale kernelized learning, showing the potential\neffectiveness of random features in reducing the computational complexity while\nkeeping optimal generalization properties.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2016 16:26:39 GMT"}, {"version": "v2", "created": "Sat, 21 May 2016 08:43:32 GMT"}, {"version": "v3", "created": "Wed, 31 May 2017 07:04:58 GMT"}, {"version": "v4", "created": "Wed, 31 Jan 2018 17:43:18 GMT"}, {"version": "v5", "created": "Thu, 15 Apr 2021 09:03:18 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Rudi", "Alessandro", ""], ["Rosasco", "Lorenzo", ""]]}, {"id": "1602.04484", "submitter": "Phil Long", "authors": "David P. Helmbold and Philip M. Long", "title": "Surprising properties of dropout in deep networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze dropout in deep networks with rectified linear units and the\nquadratic loss. Our results expose surprising differences between the behavior\nof dropout and more traditional regularizers like weight decay. For example, on\nsome simple data sets dropout training produces negative weights even though\nthe output is the sum of the inputs. This provides a counterpoint to the\nsuggestion that dropout discourages co-adaptation of weights. We also show that\nthe dropout penalty can grow exponentially in the depth of the network while\nthe weight-decay penalty remains essentially linear, and that dropout is\ninsensitive to various re-scalings of the input features, outputs, and network\nweights. This last insensitivity implies that there are no isolated local\nminima of the dropout training criterion. Our work uncovers new properties of\ndropout, extends our understanding of why dropout succeeds, and lays the\nfoundation for further progress.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2016 18:20:29 GMT"}, {"version": "v2", "created": "Sat, 5 Mar 2016 23:00:10 GMT"}, {"version": "v3", "created": "Fri, 27 May 2016 23:24:17 GMT"}, {"version": "v4", "created": "Thu, 3 Nov 2016 16:39:19 GMT"}, {"version": "v5", "created": "Wed, 19 Apr 2017 21:15:15 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Helmbold", "David P.", ""], ["Long", "Philip M.", ""]]}, {"id": "1602.04485", "submitter": "Matus Telgarsky", "authors": "Matus Telgarsky", "title": "Benefits of depth in neural networks", "comments": "To appear, COLT 2016. For a simplified version, see\n  http://arxiv.org/abs/1509.08101", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For any positive integer $k$, there exist neural networks with $\\Theta(k^3)$\nlayers, $\\Theta(1)$ nodes per layer, and $\\Theta(1)$ distinct parameters which\ncan not be approximated by networks with $\\mathcal{O}(k)$ layers unless they\nare exponentially large --- they must possess $\\Omega(2^k)$ nodes. This result\nis proved here for a class of nodes termed \"semi-algebraic gates\" which\nincludes the common choices of ReLU, maximum, indicator, and piecewise\npolynomial functions, therefore establishing benefits of depth against not just\nstandard networks with ReLU gates, but also convolutional networks with ReLU\nand maximization gates, sum-product networks, and boosted decision trees (in\nthis last case with a stronger separation: $\\Omega(2^{k^3})$ total tree nodes\nare required).\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2016 18:36:59 GMT"}, {"version": "v2", "created": "Fri, 27 May 2016 22:11:26 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Telgarsky", "Matus", ""]]}, {"id": "1602.04511", "submitter": "Hongteng Xu", "authors": "Hongteng Xu and Mehrdad Farajtabar and Hongyuan Zha", "title": "Learning Granger Causality for Hawkes Processes", "comments": "International Conference on Machine Learning, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning Granger causality for general point processes is a very challenging\ntask. In this paper, we propose an effective method, learning Granger\ncausality, for a special but significant type of point processes --- Hawkes\nprocess. We reveal the relationship between Hawkes process's impact function\nand its Granger causality graph. Specifically, our model represents impact\nfunctions using a series of basis functions and recovers the Granger causality\ngraph via group sparsity of the impact functions' coefficients. We propose an\neffective learning algorithm combining a maximum likelihood estimator (MLE)\nwith a sparse-group-lasso (SGL) regularizer. Additionally, the flexibility of\nour model allows to incorporate the clustering structure event types into\nlearning framework. We analyze our learning algorithm and propose an adaptive\nprocedure to select basis functions. Experiments on both synthetic and\nreal-world data show that our method can learn the Granger causality graph and\nthe triggering patterns of the Hawkes processes simultaneously.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2016 21:14:07 GMT"}, {"version": "v2", "created": "Sat, 11 Jun 2016 23:47:23 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Xu", "Hongteng", ""], ["Farajtabar", "Mehrdad", ""], ["Zha", "Hongyuan", ""]]}, {"id": "1602.04548", "submitter": "Ichiro Takeuchi Prof.", "authors": "Kazuya Nakagawa, Shinya Suzumura, Masayuki Karasuyama, Koji Tsuda,\n  Ichiro Takeuchi", "title": "Safe Pattern Pruning: An Efficient Approach for Predictive Pattern\n  Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study predictive pattern mining problems where the goal is\nto construct a predictive model based on a subset of predictive patterns in the\ndatabase. Our main contribution is to introduce a novel method called safe\npattern pruning (SPP) for a class of predictive pattern mining problems. The\nSPP method allows us to efficiently find a superset of all the predictive\npatterns in the database that are needed for the optimal predictive model. The\nadvantage of the SPP method over existing boosting-type method is that the\nformer can find the superset by a single search over the database, while the\nlatter requires multiple searches. The SPP method is inspired by recent\ndevelopment of safe feature screening. In order to extend the idea of safe\nfeature screening into predictive pattern mining, we derive a novel pruning\nrule called safe pattern pruning (SPP) rule that can be used for searching over\nthe tree defined among patterns in the database. The SPP rule has a property\nthat, if a node corresponding to a pattern in the database is pruned out by the\nSPP rule, then it is guaranteed that all the patterns corresponding to its\ndescendant nodes are never needed for the optimal predictive model. We apply\nthe SPP method to graph mining and item-set mining problems, and demonstrate\nits computational advantage.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2016 02:16:16 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Nakagawa", "Kazuya", ""], ["Suzumura", "Shinya", ""], ["Karasuyama", "Masayuki", ""], ["Tsuda", "Koji", ""], ["Takeuchi", "Ichiro", ""]]}, {"id": "1602.04567", "submitter": "Changho Suh", "authors": "Changho Suh, Vincent Y. F. Tan, Renbo Zhao", "title": "Adversarial Top-$K$ Ranking", "comments": "32 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the top-$K$ ranking problem where the goal is to recover the set of\ntop-$K$ ranked items out of a large collection of items based on partially\nrevealed preferences. We consider an adversarial crowdsourced setting where\nthere are two population sets, and pairwise comparison samples drawn from one\nof the populations follow the standard Bradley-Terry-Luce model (i.e., the\nchance of item $i$ beating item $j$ is proportional to the relative score of\nitem $i$ to item $j$), while in the other population, the corresponding chance\nis inversely proportional to the relative score. When the relative size of the\ntwo populations is known, we characterize the minimax limit on the sample size\nrequired (up to a constant) for reliably identifying the top-$K$ items, and\ndemonstrate how it scales with the relative size. Moreover, by leveraging a\ntensor decomposition method for disambiguating mixture distributions, we extend\nour result to the more realistic scenario in which the relative population size\nis unknown, thus establishing an upper bound on the fundamental limit of the\nsample size for recovering the top-$K$ set.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2016 06:22:59 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Suh", "Changho", ""], ["Tan", "Vincent Y. F.", ""], ["Zhao", "Renbo", ""]]}, {"id": "1602.04579", "submitter": "Ichiro Takeuchi Prof.", "authors": "Toshiyuki Takada, Hiroyuki Hanada, Yoshiji Yamada, Jun Sakuma, Ichiro\n  Takeuchi", "title": "Secure Approximation Guarantee for Cryptographically Private Empirical\n  Risk Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy concern has been increasingly important in many machine learning (ML)\nproblems. We study empirical risk minimization (ERM) problems under secure\nmulti-party computation (MPC) frameworks. Main technical tools for MPC have\nbeen developed based on cryptography. One of limitations in current\ncryptographically private ML is that it is computationally intractable to\nevaluate non-linear functions such as logarithmic functions or exponential\nfunctions. Therefore, for a class of ERM problems such as logistic regression\nin which non-linear function evaluations are required, one can only obtain\napproximate solutions. In this paper, we introduce a novel cryptographically\nprivate tool called secure approximation guarantee (SAG) method. The key\nproperty of SAG method is that, given an arbitrary approximate solution, it can\nprovide a non-probabilistic assumption-free bound on the approximation quality\nunder cryptographically secure computation framework. We demonstrate the\nbenefit of the SAG method by applying it to several problems including a\npractical privacy-preserving data analysis task on genomic and clinical\ninformation.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2016 07:22:42 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Takada", "Toshiyuki", ""], ["Hanada", "Hiroyuki", ""], ["Yamada", "Yoshiji", ""], ["Sakuma", "Jun", ""], ["Takeuchi", "Ichiro", ""]]}, {"id": "1602.04589", "submitter": "Aurelien Garivier", "authors": "Aur\\'elien Garivier (IMT), Emilie Kaufmann (CRIStAL, SEQUEL)", "title": "Optimal Best Arm Identification with Fixed Confidence", "comments": "Conference on Learning Theory (COLT), Jun 2016, New York, United\n  States", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a complete characterization of the complexity of best-arm\nidentification in one-parameter bandit problems. We prove a new, tight lower\nbound on the sample complexity. We propose the `Track-and-Stop' strategy, which\nwe prove to be asymptotically optimal. It consists in a new sampling rule\n(which tracks the optimal proportions of arm draws highlighted by the lower\nbound) and in a stopping rule named after Chernoff, for which we give a new\nanalysis.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2016 08:25:02 GMT"}, {"version": "v2", "created": "Wed, 1 Jun 2016 14:29:14 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Garivier", "Aur\u00e9lien", "", "IMT"], ["Kaufmann", "Emilie", "", "CRIStAL, SEQUEL"]]}, {"id": "1602.04601", "submitter": "Ichiro Takeuchi Prof.", "authors": "Shinya Suzumura, Kazuya Nakagawa, Mahito Sugiyama, Koji Tsuda, Ichiro\n  Takeuchi", "title": "Selective Inference Approach for Statistically Sound Predictive Pattern\n  Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering statistically significant patterns from databases is an important\nchallenging problem. The main obstacle of this problem is in the difficulty of\ntaking into account the selection bias, i.e., the bias arising from the fact\nthat patterns are selected from extremely large number of candidates in\ndatabases. In this paper, we introduce a new approach for predictive pattern\nmining problems that can address the selection bias issue. Our approach is\nbuilt on a recently popularized statistical inference framework called\nselective inference. In selective inference, statistical inferences (such as\nstatistical hypothesis testing) are conducted based on sampling distributions\nconditional on a selection event. If the selection event is characterized in a\ntractable way, statistical inferences can be made without minding selection\nbias issue. However, in pattern mining problems, it is difficult to\ncharacterize the entire selection process of mining algorithms. Our main\ncontribution in this paper is to solve this challenging problem for a class of\npredictive pattern mining problems by introducing a novel algorithmic\nframework. We demonstrate that our approach is useful for finding statistically\nsignificant patterns from databases.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2016 09:52:00 GMT"}, {"version": "v2", "created": "Wed, 9 Mar 2016 06:00:32 GMT"}], "update_date": "2016-03-10", "authors_parsed": [["Suzumura", "Shinya", ""], ["Nakagawa", "Kazuya", ""], ["Sugiyama", "Mahito", ""], ["Tsuda", "Koji", ""], ["Takeuchi", "Ichiro", ""]]}, {"id": "1602.04621", "submitter": "Ian Osband", "authors": "Ian Osband, Charles Blundell, Alexander Pritzel, Benjamin Van Roy", "title": "Deep Exploration via Bootstrapped DQN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient exploration in complex environments remains a major challenge for\nreinforcement learning. We propose bootstrapped DQN, a simple algorithm that\nexplores in a computationally and statistically efficient manner through use of\nrandomized value functions. Unlike dithering strategies such as epsilon-greedy\nexploration, bootstrapped DQN carries out temporally-extended (or deep)\nexploration; this can lead to exponentially faster learning. We demonstrate\nthese benefits in complex stochastic MDPs and in the large-scale Arcade\nLearning Environment. Bootstrapped DQN substantially improves learning times\nand performance across most Atari games.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2016 10:54:20 GMT"}, {"version": "v2", "created": "Fri, 1 Jul 2016 16:23:55 GMT"}, {"version": "v3", "created": "Mon, 4 Jul 2016 17:11:52 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Osband", "Ian", ""], ["Blundell", "Charles", ""], ["Pritzel", "Alexander", ""], ["Van Roy", "Benjamin", ""]]}, {"id": "1602.04676", "submitter": "Aurelien Garivier", "authors": "Aur\\'elien Garivier (IMT), Emilie Kaufmann (CRIStAL, SEQUEL), Wouter\n  Koolen (CWI)", "title": "Maximin Action Identification: A New Bandit Framework for Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.GT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an original problem of pure exploration in a strategic bandit model\nmotivated by Monte Carlo Tree Search. It consists in identifying the best\naction in a game, when the player may sample random outcomes of sequentially\nchosen pairs of actions. We propose two strategies for the fixed-confidence\nsetting: Maximin-LUCB, based on lower-and upper-confidence bounds; and\nMaximin-Racing, which operates by successively eliminating the sub-optimal\nactions. We discuss the sample complexity of both methods and compare their\nperformance empirically. We sketch a lower bound analysis, and possible\nconnections to an optimal algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2016 13:55:45 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Garivier", "Aur\u00e9lien", "", "IMT"], ["Kaufmann", "Emilie", "", "CRIStAL, SEQUEL"], ["Koolen", "Wouter", "", "CWI"]]}, {"id": "1602.04723", "submitter": "David Jacobs", "authors": "Ronen Basri and David Jacobs", "title": "Efficient Representation of Low-Dimensional Manifolds using Deep\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the ability of deep neural networks to represent data that lies\nnear a low-dimensional manifold in a high-dimensional space. We show that deep\nnetworks can efficiently extract the intrinsic, low-dimensional coordinates of\nsuch data. We first show that the first two layers of a deep network can\nexactly embed points lying on a monotonic chain, a special type of piecewise\nlinear manifold, mapping them to a low-dimensional Euclidean space. Remarkably,\nthe network can do this using an almost optimal number of parameters. We also\nshow that this network projects nearby points onto the manifold and then embeds\nthem with little error. We then extend these results to more general manifolds.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2016 16:16:56 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Basri", "Ronen", ""], ["Jacobs", "David", ""]]}, {"id": "1602.04799", "submitter": "Nathan Wiebe", "authors": "Nathan Wiebe, Ashish Kapoor, Krysta M Svore", "title": "Quantum Perceptron Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate how quantum computation can provide non-trivial improvements\nin the computational and statistical complexity of the perceptron model. We\ndevelop two quantum algorithms for perceptron learning. The first algorithm\nexploits quantum information processing to determine a separating hyperplane\nusing a number of steps sublinear in the number of data points $N$, namely\n$O(\\sqrt{N})$. The second algorithm illustrates how the classical mistake bound\nof $O(\\frac{1}{\\gamma^2})$ can be further improved to\n$O(\\frac{1}{\\sqrt{\\gamma}})$ through quantum means, where $\\gamma$ denotes the\nmargin. Such improvements are achieved through the application of quantum\namplitude amplification to the version space interpretation of the perceptron\nmodel.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2016 20:45:35 GMT"}], "update_date": "2016-02-20", "authors_parsed": [["Wiebe", "Nathan", ""], ["Kapoor", "Ashish", ""], ["Svore", "Krysta M", ""]]}, {"id": "1602.04805", "submitter": "Jovana Mitrovic", "authors": "Jovana Mitrovic, Dino Sejdinovic, Yee Whye Teh", "title": "DR-ABC: Approximate Bayesian Computation with Kernel-Based Distribution\n  Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing exact posterior inference in complex generative models is often\ndifficult or impossible due to an expensive to evaluate or intractable\nlikelihood function. Approximate Bayesian computation (ABC) is an inference\nframework that constructs an approximation to the true likelihood based on the\nsimilarity between the observed and simulated data as measured by a predefined\nset of summary statistics. Although the choice of appropriate problem-specific\nsummary statistics crucially influences the quality of the likelihood\napproximation and hence also the quality of the posterior sample in ABC, there\nare only few principled general-purpose approaches to the selection or\nconstruction of such summary statistics. In this paper, we develop a novel\nframework for this task using kernel-based distribution regression. We model\nthe functional relationship between data distributions and the optimal choice\n(with respect to a loss function) of summary statistics using kernel-based\ndistribution regression. We show that our approach can be implemented in a\ncomputationally and statistically efficient way using the random Fourier\nfeatures framework for large-scale kernel learning. In addition to that, our\nframework shows superior performance when compared to related methods on toy\nand real-world problems.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2016 20:55:57 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Mitrovic", "Jovana", ""], ["Sejdinovic", "Dino", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1602.04910", "submitter": "Shuichi Kawano", "authors": "Kaito Shimamura, Masao Ueki, Shuichi Kawano and Sadanori Konishi", "title": "Bayesian generalized fused lasso modeling via NEG distribution", "comments": "26 pages", "journal-ref": "Communications in Statistics - Theory and Methods 48 (2019)\n  4132-4153", "doi": "10.1080/03610926.2018.1489056", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fused lasso penalizes a loss function by the $L_1$ norm for both the\nregression coefficients and their successive differences to encourage sparsity\nof both. In this paper, we propose a Bayesian generalized fused lasso modeling\nbased on a normal-exponential-gamma (NEG) prior distribution. The NEG prior is\nassumed into the difference of successive regression coefficients. The proposed\nmethod enables us to construct a more versatile sparse model than the ordinary\nfused lasso by using a flexible regularization term. We also propose a sparse\nfused algorithm to produce exact sparse solutions. Simulation studies and real\ndata analyses show that the proposed method has superior performance to the\nordinary fused lasso.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 05:20:14 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Shimamura", "Kaito", ""], ["Ueki", "Masao", ""], ["Kawano", "Shuichi", ""], ["Konishi", "Sadanori", ""]]}, {"id": "1602.04912", "submitter": "Dionysios Kalogerias", "authors": "Dionysios S. Kalogerias, Athina P. Petropulu", "title": "Uniform {\\varepsilon}-Stability of Distributed Nonlinear Filtering over\n  DNAs: Gaussian-Finite HMMs", "comments": "30 pages, to appear in the IEEE Transactions on Signal & Information\n  Processing over Networks, in the upcoming Special Issue on Inference &\n  Learning over Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study stability of distributed filtering of Markov chains\nwith finite state space, partially observed in conditionally Gaussian noise. We\nconsider a nonlinear filtering scheme over a Distributed Network of Agents\n(DNA), which relies on the distributed evaluation of the likelihood part of the\ncentralized nonlinear filter and is based on a particular specialization of the\nAlternating Direction Method of Multipliers (ADMM) for fast average consensus.\nAssuming the same number of consensus steps between any two consecutive noisy\nmeasurements for each sensor in the network, we fully characterize a minimal\nnumber of such steps, such that the distributed filter remains uniformly stable\nwith a prescribed accuracy level, {\\varepsilon} \\in (0,1], within a finite\noperational horizon, T, and across all sensors. Stability is in the sense of\nthe \\ell_1-norm between the centralized and distributed versions of the\nposterior at each sensor, and at each time within T. Roughly speaking, our main\nresult shows that uniform {\\varepsilon}-stability of the distributed filtering\nprocess depends only loglinearly on T and (roughly) the size of the network,\nand only logarithmically on 1/{\\varepsilon}. If this total loglinear bound is\nfulfilled, any additional consensus iterations will incur a fully quantified\nfurther exponential decay in the consensus error. Our bounds are universal, in\nthe sense that they are independent of the particular structure of the Gaussian\nHidden Markov Model (HMM) under consideration.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 05:23:13 GMT"}, {"version": "v2", "created": "Mon, 4 Apr 2016 20:55:19 GMT"}, {"version": "v3", "created": "Wed, 6 Jul 2016 16:29:28 GMT"}, {"version": "v4", "created": "Mon, 26 Sep 2016 18:39:58 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Kalogerias", "Dionysios S.", ""], ["Petropulu", "Athina P.", ""]]}, {"id": "1602.04915", "submitter": "Jason Lee", "authors": "Jason D. Lee, Max Simchowitz, Michael I. Jordan, Benjamin Recht", "title": "Gradient Descent Converges to Minimizers", "comments": "Submitted to COLT 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that gradient descent converges to a local minimizer, almost surely\nwith random initialization. This is proved by applying the Stable Manifold\nTheorem from dynamical systems theory.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 05:43:31 GMT"}, {"version": "v2", "created": "Fri, 4 Mar 2016 08:04:58 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Lee", "Jason D.", ""], ["Simchowitz", "Max", ""], ["Jordan", "Michael I.", ""], ["Recht", "Benjamin", ""]]}, {"id": "1602.04938", "submitter": "Marco Tulio Ribeiro", "authors": "Marco Tulio Ribeiro and Sameer Singh and Carlos Guestrin", "title": "\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite widespread adoption, machine learning models remain mostly black\nboxes. Understanding the reasons behind predictions is, however, quite\nimportant in assessing trust, which is fundamental if one plans to take action\nbased on a prediction, or when choosing whether to deploy a new model. Such\nunderstanding also provides insights into the model, which can be used to\ntransform an untrustworthy model or prediction into a trustworthy one. In this\nwork, we propose LIME, a novel explanation technique that explains the\npredictions of any classifier in an interpretable and faithful manner, by\nlearning an interpretable model locally around the prediction. We also propose\na method to explain models by presenting representative individual predictions\nand their explanations in a non-redundant way, framing the task as a submodular\noptimization problem. We demonstrate the flexibility of these methods by\nexplaining different models for text (e.g. random forests) and image\nclassification (e.g. neural networks). We show the utility of explanations via\nnovel experiments, both simulated and with human subjects, on various scenarios\nthat require trust: deciding if one should trust a prediction, choosing between\nmodels, improving an untrustworthy classifier, and identifying why a classifier\nshould not be trusted.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 08:20:14 GMT"}, {"version": "v2", "created": "Wed, 3 Aug 2016 22:30:58 GMT"}, {"version": "v3", "created": "Tue, 9 Aug 2016 17:54:52 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Ribeiro", "Marco Tulio", ""], ["Singh", "Sameer", ""], ["Guestrin", "Carlos", ""]]}, {"id": "1602.04951", "submitter": "Anna Harutyunyan", "authors": "Anna Harutyunyan and Marc G. Bellemare and Tom Stepleton and Remi\n  Munos", "title": "Q($\\lambda$) with Off-Policy Corrections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and analyze an alternate approach to off-policy multi-step\ntemporal difference learning, in which off-policy returns are corrected with\nthe current Q-function in terms of rewards, rather than with the target policy\nin terms of transition probabilities. We prove that such approximate\ncorrections are sufficient for off-policy convergence both in policy evaluation\nand control, provided certain conditions. These conditions relate the distance\nbetween the target and behavior policies, the eligibility trace parameter and\nthe discount factor, and formalize an underlying tradeoff in off-policy\nTD($\\lambda$). We illustrate this theoretical relationship empirically on a\ncontinuous-state control task.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 09:09:56 GMT"}, {"version": "v2", "created": "Thu, 11 Aug 2016 09:40:12 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Harutyunyan", "Anna", ""], ["Bellemare", "Marc G.", ""], ["Stepleton", "Tom", ""], ["Munos", "Remi", ""]]}, {"id": "1602.04976", "submitter": "Emile Contal", "authors": "Emile Contal and Nicolas Vayatis", "title": "Stochastic Process Bandits: Upper Confidence Bounds Algorithms via\n  Generic Chaining", "comments": "preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper considers the problem of global optimization in the setup of\nstochastic process bandits. We introduce an UCB algorithm which builds a\ncascade of discretization trees based on generic chaining in order to render\npossible his operability over a continuous domain. The theoretical framework\napplies to functions under weak probabilistic smoothness assumptions and also\nextends significantly the spectrum of application of UCB strategies. Moreover\ngeneric regret bounds are derived which are then specialized to Gaussian\nprocesses indexed on infinite-dimensional spaces as well as to quadratic forms\nof Gaussian processes. Lower bounds are also proved in the case of Gaussian\nprocesses to assess the optimality of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 10:48:28 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Contal", "Emile", ""], ["Vayatis", "Nicolas", ""]]}, {"id": "1602.05003", "submitter": "Alexandre Navarro", "authors": "Alexandre K. W. Navarro, Jes Frellsen and Richard E. Turner", "title": "The Multivariate Generalised von Mises distribution: Inference and\n  applications", "comments": "16 pages, 8 figures. Final version available at AAAI Press website:\n  https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/15020. This version\n  includes supplementary material submitted to, but not published, in the AAAI\n  proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Circular variables arise in a multitude of data-modelling contexts ranging\nfrom robotics to the social sciences, but they have been largely overlooked by\nthe machine learning community. This paper partially redresses this imbalance\nby extending some standard probabilistic modelling tools to the circular\ndomain. First we introduce a new multivariate distribution over circular\nvariables, called the multivariate Generalised von Mises (mGvM) distribution.\nThis distribution can be constructed by restricting and renormalising a general\nmultivariate Gaussian distribution to the unit hyper-torus. Previously proposed\nmultivariate circular distributions are shown to be special cases of this\nconstruction. Second, we introduce a new probabilistic model for circular\nregression, that is inspired by Gaussian Processes, and a method for\nprobabilistic principal component analysis with circular hidden variables.\nThese models can leverage standard modelling tools (e.g. covariance functions\nand methods for automatic relevance determination). Third, we show that the\nposterior distribution in these models is a mGvM distribution which enables\ndevelopment of an efficient variational free-energy scheme for performing\napproximate inference and approximate maximum-likelihood learning.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 12:50:16 GMT"}, {"version": "v2", "created": "Tue, 22 Mar 2016 15:07:43 GMT"}, {"version": "v3", "created": "Tue, 5 Apr 2016 15:30:58 GMT"}, {"version": "v4", "created": "Mon, 23 May 2016 12:16:06 GMT"}, {"version": "v5", "created": "Mon, 9 Jan 2017 16:39:49 GMT"}, {"version": "v6", "created": "Tue, 8 Aug 2017 22:19:23 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Navarro", "Alexandre K. W.", ""], ["Frellsen", "Jes", ""], ["Turner", "Richard E.", ""]]}, {"id": "1602.05012", "submitter": "Jaroslav Fowkes", "authors": "Jaroslav Fowkes and Charles Sutton", "title": "A Subsequence Interleaving Model for Sequential Pattern Mining", "comments": "10 pages in KDD 2016: Proceedings of the 22nd ACM SIGKDD\n  International Conference on Knowledge Discovery and Data Mining", "journal-ref": null, "doi": "10.1145/2939672.2939787", "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent sequential pattern mining methods have used the minimum description\nlength (MDL) principle to define an encoding scheme which describes an\nalgorithm for mining the most compressing patterns in a database. We present a\nnovel subsequence interleaving model based on a probabilistic model of the\nsequence database, which allows us to search for the most compressing set of\npatterns without designing a specific encoding scheme. Our proposed algorithm\nis able to efficiently mine the most relevant sequential patterns and rank them\nusing an associated measure of interestingness. The efficient inference in our\nmodel is a direct result of our use of a structural expectation-maximization\nframework, in which the expectation-step takes the form of a submodular\noptimization problem subject to a coverage constraint. We show on both\nsynthetic and real world datasets that our model mines a set of sequential\npatterns with low spuriousness and redundancy, high interpretability and\nusefulness in real-world applications. Furthermore, we demonstrate that the\nquality of the patterns from our approach is comparable to, if not better than,\nexisting state of the art sequential pattern mining algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 13:30:10 GMT"}, {"version": "v2", "created": "Fri, 11 Nov 2016 10:43:36 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Fowkes", "Jaroslav", ""], ["Sutton", "Charles", ""]]}, {"id": "1602.05128", "submitter": "Tom Rainforth", "authors": "Tom Rainforth, Christian A. Naesseth, Fredrik Lindsten, Brooks Paige,\n  Jan-Willem van de Meent, Arnaud Doucet, Frank Wood", "title": "Interacting Particle Markov Chain Monte Carlo", "comments": null, "journal-ref": "JMLR W&CP 48 : 2616-2625, 2016", "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce interacting particle Markov chain Monte Carlo (iPMCMC), a PMCMC\nmethod based on an interacting pool of standard and conditional sequential\nMonte Carlo samplers. Like related methods, iPMCMC is a Markov chain Monte\nCarlo sampler on an extended space. We present empirical results that show\nsignificant improvements in mixing rates relative to both non-interacting PMCMC\nsamplers, and a single PMCMC sampler with an equivalent memory and\ncomputational budget. An additional advantage of the iPMCMC method is that it\nis suitable for distributed and multi-core architectures.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 18:36:19 GMT"}, {"version": "v2", "created": "Wed, 10 Aug 2016 18:05:09 GMT"}, {"version": "v3", "created": "Wed, 12 Apr 2017 14:10:58 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Rainforth", "Tom", ""], ["Naesseth", "Christian A.", ""], ["Lindsten", "Fredrik", ""], ["Paige", "Brooks", ""], ["van de Meent", "Jan-Willem", ""], ["Doucet", "Arnaud", ""], ["Wood", "Frank", ""]]}, {"id": "1602.05149", "submitter": "Jialei Wang", "authors": "Jialei Wang, Scott C. Clark, Eric Liu, and Peter I. Frazier", "title": "Parallel Bayesian Global Optimization of Expensive Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider parallel global optimization of derivative-free\nexpensive-to-evaluate functions, and propose an efficient method based on\nstochastic approximation for implementing a conceptual Bayesian optimization\nalgorithm proposed by Ginsbourger et al. (2007). At the heart of this algorithm\nis maximizing the information criterion called the \"multi-points expected\nimprovement'', or the q-EI. To accomplish this, we use infinitessimal\nperturbation analysis (IPA) to construct a stochastic gradient estimator and\nshow that this estimator is unbiased. We also show that the stochastic gradient\nascent algorithm using the constructed gradient estimator converges to a\nstationary point of the q-EI surface, and therefore, as the number of multiple\nstarts of the gradient ascent algorithm and the number of steps for each start\ngrow large, the one-step Bayes optimal set of points is recovered. We show in\nnumerical experiments that our method for maximizing the q-EI is faster than\nmethods based on closed-form evaluation using high-dimensional integration,\nwhen considering many parallel function evaluations, and is comparable in speed\nwhen considering few. We also show that the resulting one-step Bayes optimal\nalgorithm for parallel global optimization finds high-quality solutions with\nfewer evaluations than a heuristic based on approximately maximizing the q-EI.\nA high-quality open source implementation of this algorithm is available in the\nopen source Metrics Optimization Engine (MOE).\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 19:40:15 GMT"}, {"version": "v2", "created": "Wed, 2 Mar 2016 01:16:44 GMT"}, {"version": "v3", "created": "Wed, 1 Nov 2017 05:52:53 GMT"}, {"version": "v4", "created": "Sun, 5 May 2019 06:01:21 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Wang", "Jialei", ""], ["Clark", "Scott C.", ""], ["Liu", "Eric", ""], ["Frazier", "Peter I.", ""]]}, {"id": "1602.05221", "submitter": "Matthew Johnson", "authors": "Elaine Angelino, Matthew James Johnson, Ryan P. Adams", "title": "Patterns of Scalable Bayesian Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Datasets are growing not just in size but in complexity, creating a demand\nfor rich models and quantification of uncertainty. Bayesian methods are an\nexcellent fit for this demand, but scaling Bayesian inference is a challenge.\nIn response to this challenge, there has been considerable recent work based on\nvarying assumptions about model structure, underlying computational resources,\nand the importance of asymptotic correctness. As a result, there is a zoo of\nideas with few clear overarching principles.\n  In this paper, we seek to identify unifying principles, patterns, and\nintuitions for scaling Bayesian inference. We review existing work on utilizing\nmodern computing resources with both MCMC and variational approximation\ntechniques. From this taxonomy of ideas, we characterize the general principles\nthat have proven successful for designing scalable inference procedures and\ncomment on the path forward.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 22:13:00 GMT"}, {"version": "v2", "created": "Tue, 22 Mar 2016 14:55:29 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Angelino", "Elaine", ""], ["Johnson", "Matthew James", ""], ["Adams", "Ryan P.", ""]]}, {"id": "1602.05236", "submitter": "Linjun Zhang", "authors": "T.Tony Cai, Linjun Zhang", "title": "A Sparse PCA Approach to Clustering", "comments": "This paper is part of a discussion of the paper \"Important feature\n  PCA for high dimensional clustering\" by Jiashun Jin and Wanjie Wang to appear\n  in The Annals of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss a clustering method for Gaussian mixture model based on the sparse\nprincipal component analysis (SPCA) method and compare it with the IF-PCA\nmethod. We also discuss the dependent case where the covariance matrix $\\Sigma$\nis not necessarily diagonal.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 22:49:41 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Cai", "T. Tony", ""], ["Zhang", "Linjun", ""]]}, {"id": "1602.05257", "submitter": "Deovrat Kakde", "authors": "Deovrat Kakde, Arin Chaudhuri, Seunghyun Kong, Maria Jahja, Hansi\n  Jiang, Jorge Silva", "title": "Peak Criterion for Choosing Gaussian Kernel Bandwidth in Support Vector\n  Data Description", "comments": null, "journal-ref": null, "doi": "10.1109/ICPHM.2017.7998302", "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support Vector Data Description (SVDD) is a machine-learning technique used\nfor single class classification and outlier detection. SVDD formulation with\nkernel function provides a flexible boundary around data. The value of kernel\nfunction parameters affects the nature of the data boundary. For example, it is\nobserved that with a Gaussian kernel, as the value of kernel bandwidth is\nlowered, the data boundary changes from spherical to wiggly. The spherical data\nboundary leads to underfitting, and an extremely wiggly data boundary leads to\noverfitting. In this paper, we propose empirical criterion to obtain good\nvalues of the Gaussian kernel bandwidth parameter. This criterion provides a\nsmooth boundary that captures the essential geometric features of the data.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 00:51:18 GMT"}, {"version": "v2", "created": "Wed, 11 May 2016 21:39:53 GMT"}, {"version": "v3", "created": "Tue, 8 Aug 2017 18:00:45 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Kakde", "Deovrat", ""], ["Chaudhuri", "Arin", ""], ["Kong", "Seunghyun", ""], ["Jahja", "Maria", ""], ["Jiang", "Hansi", ""], ["Silva", "Jorge", ""]]}, {"id": "1602.05264", "submitter": "Puneet Chhabra", "authors": "Puneet S Chhabra, Andrew M Wallace and James R Hopgood", "title": "Anomaly Detection in Clutter using Spectrally Enhanced Ladar", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.optics cs.LG physics.ins-det stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete return (DR) Laser Detection and Ranging (Ladar) systems provide a\nseries of echoes that reflect from objects in a scene. These can be first, last\nor multi-echo returns. In contrast, Full-Waveform (FW)-Ladar systems measure\nthe intensity of light reflected from objects continuously over a period of\ntime. In a camouflaged scenario, e.g., objects hidden behind dense foliage, a\nFW-Ladar penetrates such foliage and returns a sequence of echoes including\nburied faint echoes. The aim of this paper is to learn local-patterns of\nco-occurring echoes characterised by their measured spectra. A deviation from\nsuch patterns defines an abnormal event in a forest/tree depth profile. As far\nas the authors know, neither DR or FW-Ladar, along with several spectral\nmeasurements, has not been applied to anomaly detection. This work presents an\nalgorithm that allows detection of spectral and temporal anomalies in FW-Multi\nSpectral Ladar (FW-MSL) data samples. An anomaly is defined as a full waveform\ntemporal and spectral signature that does not conform to a prior expectation,\nrepresented using a learnt subspace (dictionary) and set of coefficients that\ncapture co-occurring local-patterns using an overlapping temporal window. A\nmodified optimization scheme is proposed for subspace learning based on\nstochastic approximations. The objective function is augmented with a\ndiscriminative term that represents the subspace's separability properties and\nsupports anomaly characterisation. The algorithm detects several man-made\nobjects and anomalous spectra hidden in a dense clutter of vegetation and also\nallows tree species classification.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 01:39:29 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Chhabra", "Puneet S", ""], ["Wallace", "Andrew M", ""], ["Hopgood", "James R", ""]]}, {"id": "1602.05285", "submitter": "Truyen Tran", "authors": "Truyen Tran, Dinh Phung and Svetha Venkatesh", "title": "Choice by Elimination via Deep Neural Networks", "comments": "PAKDD workshop on Biologically Inspired Techniques for Data Mining\n  (BDM'16)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Neural Choice by Elimination, a new framework that integrates\ndeep neural networks into probabilistic sequential choice models for learning\nto rank. Given a set of items to chose from, the elimination strategy starts\nwith the whole item set and iteratively eliminates the least worthy item in the\nremaining subset. We prove that the choice by elimination is equivalent to\nmarginalizing out the random Gompertz latent utilities. Coupled with the choice\nmodel is the recently introduced Neural Highway Networks for approximating\narbitrarily complex rank functions. We evaluate the proposed framework on a\nlarge-scale public dataset with over 425K items, drawn from the Yahoo! learning\nto rank challenge. It is demonstrated that the proposed method is competitive\nagainst state-of-the-art learning to rank methods.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 03:17:10 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Tran", "Truyen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1602.05310", "submitter": "Stephen Tu", "authors": "Stephen Tu and Rebecca Roelofs and Shivaram Venkataraman and Benjamin\n  Recht", "title": "Large Scale Kernel Learning using Block Coordinate Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate that distributed block coordinate descent can quickly solve\nkernel regression and classification problems with millions of data points.\nArmed with this capability, we conduct a thorough comparison between the full\nkernel, the Nystr\\\"om method, and random features on three large classification\ntasks from various domains. Our results suggest that the Nystr\\\"om method\ngenerally achieves better statistical accuracy than random features, but can\nrequire significantly more iterations of optimization. Lastly, we derive new\nrates for block coordinate descent which support our experimental findings when\nspecialized to kernel methods.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 05:41:07 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Tu", "Stephen", ""], ["Roelofs", "Rebecca", ""], ["Venkataraman", "Shivaram", ""], ["Recht", "Benjamin", ""]]}, {"id": "1602.05394", "submitter": "Rodolphe Jenatton", "authors": "Rodolphe Jenatton, Jim Huang, Dominik Csiba, Cedric Archambeau", "title": "Online optimization and regret guarantees for non-additive long-term\n  constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider online optimization in the 1-lookahead setting, where the\nobjective does not decompose additively over the rounds of the online game. The\nresulting formulation enables us to deal with non-stationary and/or long-term\nconstraints , which arise, for example, in online display advertising problems.\nWe propose an on-line primal-dual algorithm for which we obtain dynamic\ncumulative regret guarantees. They depend on the convexity and the smoothness\nof the non-additive penalty, as well as terms capturing the smoothness with\nwhich the residuals of the non-stationary and long-term constraints vary over\nthe rounds. We conduct experiments on synthetic data to illustrate the benefits\nof the non-additive penalty and show vanishing regret convergence on live\ntraffic data collected by a display advertising platform in production.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 12:57:08 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 09:04:18 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Jenatton", "Rodolphe", ""], ["Huang", "Jim", ""], ["Csiba", "Dominik", ""], ["Archambeau", "Cedric", ""]]}, {"id": "1602.05419", "submitter": "Nicolas Flammarion", "authors": "Aymeric Dieuleveut (SIERRA, LIENS), Nicolas Flammarion (LIENS,\n  SIERRA), Francis Bach (SIERRA, LIENS)", "title": "Harder, Better, Faster, Stronger Convergence Rates for Least-Squares\n  Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the optimization of a quadratic objective function whose\ngradients are only accessible through a stochastic oracle that returns the\ngradient at any given point plus a zero-mean finite variance random error. We\npresent the first algorithm that achieves jointly the optimal prediction error\nrates for least-squares regression, both in terms of forgetting of initial\nconditions in O(1/n 2), and in terms of dependence on the noise and dimension d\nof the problem, as O(d/n). Our new algorithm is based on averaged accelerated\nregularized gradient descent, and may also be analyzed through finer\nassumptions on initial conditions and the Hessian matrix, leading to\ndimension-free quantities that may still be small while the \" optimal \" terms\nabove are large. In order to characterize the tightness of these new bounds, we\nconsider an application to non-parametric regression and use the known lower\nbounds on the statistical performance (without computational limits), which\nhappen to match our bounds obtained from a single pass on the data and thus\nshow optimality of our algorithm in a wide variety of particular trade-offs\nbetween bias and variance.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 14:06:34 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2016 06:37:55 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Dieuleveut", "Aymeric", "", "SIERRA, LIENS"], ["Flammarion", "Nicolas", "", "LIENS,\n  SIERRA"], ["Bach", "Francis", "", "SIERRA, LIENS"]]}, {"id": "1602.05436", "submitter": "Mike Gartrell", "authors": "Mike Gartrell, Ulrich Paquet, Noam Koenigstein", "title": "Low-Rank Factorization of Determinantal Point Processes for\n  Recommendation", "comments": "10 pages, 4 figures. Submitted to KDD 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determinantal point processes (DPPs) have garnered attention as an elegant\nprobabilistic model of set diversity. They are useful for a number of subset\nselection tasks, including product recommendation. DPPs are parametrized by a\npositive semi-definite kernel matrix. In this work we present a new method for\nlearning the DPP kernel from observed data using a low-rank factorization of\nthis kernel. We show that this low-rank factorization enables a learning\nalgorithm that is nearly an order of magnitude faster than previous approaches,\nwhile also providing for a method for computing product recommendation\npredictions that is far faster (up to 20x faster or more for large item\ncatalogs) than previous techniques that involve a full-rank DPP kernel.\nFurthermore, we show that our method provides equivalent or sometimes better\npredictive performance than prior full-rank DPP approaches, and better\nperformance than several other competing recommendation methods in many cases.\nWe conduct an extensive experimental evaluation using several real-world\ndatasets in the domain of product recommendation to demonstrate the utility of\nour method, along with its limitations.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 14:40:52 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Gartrell", "Mike", ""], ["Paquet", "Ulrich", ""], ["Koenigstein", "Noam", ""]]}, {"id": "1602.05450", "submitter": "Adrian \\v{S}o\\v{s}i\\'c", "authors": "Adrian \\v{S}o\\v{s}i\\'c, Wasiur R. KhudaBukhsh, Abdelhak M. Zoubir,\n  Heinz Koeppl", "title": "Inverse Reinforcement Learning in Swarm Systems", "comments": "9 pages, 8 figures; ### Version 2 ### version accepted at AAMAS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.MA cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse reinforcement learning (IRL) has become a useful tool for learning\nbehavioral models from demonstration data. However, IRL remains mostly\nunexplored for multi-agent systems. In this paper, we show how the principle of\nIRL can be extended to homogeneous large-scale problems, inspired by the\ncollective swarming behavior of natural systems. In particular, we make the\nfollowing contributions to the field: 1) We introduce the swarMDP framework, a\nsub-class of decentralized partially observable Markov decision processes\nendowed with a swarm characterization. 2) Exploiting the inherent homogeneity\nof this framework, we reduce the resulting multi-agent IRL problem to a\nsingle-agent one by proving that the agent-specific value functions in this\nmodel coincide. 3) To solve the corresponding control problem, we propose a\nnovel heterogeneous learning scheme that is particularly tailored to the swarm\nsetting. Results on two example systems demonstrate that our framework is able\nto produce meaningful local reward models from which we can replicate the\nobserved global system dynamics.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 15:19:56 GMT"}, {"version": "v2", "created": "Fri, 24 Mar 2017 13:06:48 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["\u0160o\u0161i\u0107", "Adrian", ""], ["KhudaBukhsh", "Wasiur R.", ""], ["Zoubir", "Abdelhak M.", ""], ["Koeppl", "Heinz", ""]]}, {"id": "1602.05473", "submitter": "Lars Maal{\\o}e", "authors": "Lars Maal{\\o}e, Casper Kaae S{\\o}nderby, S{\\o}ren Kaae S{\\o}nderby,\n  Ole Winther", "title": "Auxiliary Deep Generative Models", "comments": "Proceedings of the 33rd International Conference on Machine Learning,\n  New York, NY, USA, 2016, JMLR: Workshop and Conference Proceedings volume 48,\n  Proceedings of the 33rd International Conference on Machine Learning, New\n  York, NY, USA, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models parameterized by neural networks have recently\nachieved state-of-the-art performance in unsupervised and semi-supervised\nlearning. We extend deep generative models with auxiliary variables which\nimproves the variational approximation. The auxiliary variables leave the\ngenerative model unchanged but make the variational distribution more\nexpressive. Inspired by the structure of the auxiliary variable we also propose\na model with two stochastic layers and skip connections. Our findings suggest\nthat more expressive and properly specified deep generative models converge\nfaster with better results. We show state-of-the-art performance within\nsemi-supervised learning on MNIST, SVHN and NORB datasets.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 16:24:50 GMT"}, {"version": "v2", "created": "Thu, 26 May 2016 10:21:34 GMT"}, {"version": "v3", "created": "Fri, 3 Jun 2016 09:19:21 GMT"}, {"version": "v4", "created": "Thu, 16 Jun 2016 06:39:08 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Maal\u00f8e", "Lars", ""], ["S\u00f8nderby", "Casper Kaae", ""], ["S\u00f8nderby", "S\u00f8ren Kaae", ""], ["Winther", "Ole", ""]]}, {"id": "1602.05563", "submitter": "Md Ashad Alam PhD", "authors": "Md. Ashad Alam, Kenji Fukumizu and Yu-Ping Wang", "title": "Robust Kernel (Cross-) Covariance Operators in Reproducing Kernel\n  Hilbert Space toward Kernel Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To the best of our knowledge, there are no general well-founded robust\nmethods for statistical unsupervised learning. Most of the unsupervised methods\nexplicitly or implicitly depend on the kernel covariance operator (kernel CO)\nor kernel cross-covariance operator (kernel CCO). They are sensitive to\ncontaminated data, even when using bounded positive definite kernels. First, we\npropose robust kernel covariance operator (robust kernel CO) and robust kernel\ncrosscovariance operator (robust kernel CCO) based on a generalized loss\nfunction instead of the quadratic loss function. Second, we propose influence\nfunction of classical kernel canonical correlation analysis (classical kernel\nCCA). Third, using this influence function, we propose a visualization method\nto detect influential observations from two sets of data. Finally, we propose a\nmethod based on robust kernel CO and robust kernel CCO, called robust kernel\nCCA, which is designed for contaminated data and less sensitive to noise than\nclassical kernel CCA. The principles we describe also apply to many kernel\nmethods which must deal with the issue of kernel CO or kernel CCO. Experiments\non synthesized and imaging genetics analysis demonstrate that the proposed\nvisualization and robust kernel CCA can be applied effectively to both ideal\ndata and contaminated data. The robust methods show the superior performance\nover the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 20:37:40 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Alam", "Md. Ashad", ""], ["Fukumizu", "Kenji", ""], ["Wang", "Yu-Ping", ""]]}, {"id": "1602.05702", "submitter": "Alexander Bertrand", "authors": "Simon Van Eyndhoven, Tom Francart, and Alexander Bertrand", "title": "EEG-informed attended speaker extraction from recorded speech mixtures\n  with application in neuro-steered hearing prostheses", "comments": "This paper is published in IEEE Transactions on Biomedical\n  Engineering (2016) and is under copyright. Please cite this paper as: S. Van\n  Eyndhoven, T. Francart, and A. Bertrand, \"EEG-informed attended speaker\n  extraction from recorded speech mixtures with application in neuro-steered\n  hearing prostheses\", IEEE Transactions on Biomedical Engineering, vol. 64,\n  no. 5, pp. 1045-1056, 2017", "journal-ref": "IEEE Transactions on Biomedical Engineering, vol. 64, no. 5, pp.\n  1045-1056, 2017", "doi": "10.1109/TBME.2016.2587382", "report-no": null, "categories": "cs.SD cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OBJECTIVE: We aim to extract and denoise the attended speaker in a noisy,\ntwo-speaker acoustic scenario, relying on microphone array recordings from a\nbinaural hearing aid, which are complemented with electroencephalography (EEG)\nrecordings to infer the speaker of interest. METHODS: In this study, we propose\na modular processing flow that first extracts the two speech envelopes from the\nmicrophone recordings, then selects the attended speech envelope based on the\nEEG, and finally uses this envelope to inform a multi-channel speech separation\nand denoising algorithm. RESULTS: Strong suppression of interfering\n(unattended) speech and background noise is achieved, while the attended speech\nis preserved. Furthermore, EEG-based auditory attention detection (AAD) is\nshown to be robust to the use of noisy speech signals. CONCLUSIONS: Our results\nshow that AAD-based speaker extraction from microphone array recordings is\nfeasible and robust, even in noisy acoustic environments, and without access to\nthe clean speech signals to perform EEG-based AAD. SIGNIFICANCE: Current\nresearch on AAD always assumes the availability of the clean speech signals,\nwhich limits the applicability in real settings. We have extended this research\nto detect the attended speaker even when only microphone recordings with noisy\nspeech mixtures are available. This is an enabling ingredient for new\nbrain-computer interfaces and effective filtering schemes in neuro-steered\nhearing prostheses. Here, we provide a first proof of concept for EEG-informed\nattended speaker extraction and denoising.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 07:32:00 GMT"}, {"version": "v2", "created": "Wed, 6 Jul 2016 13:14:59 GMT"}, {"version": "v3", "created": "Thu, 14 Jul 2016 12:13:45 GMT"}, {"version": "v4", "created": "Tue, 5 Feb 2019 15:53:34 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Van Eyndhoven", "Simon", ""], ["Francart", "Tom", ""], ["Bertrand", "Alexander", ""]]}, {"id": "1602.05822", "submitter": "Alex Mendelson", "authors": "Alex F. Mendelson, Maria A. Zuluaga, Brian F. Hutton, S\\'ebastien\n  Ourselin", "title": "What is the distribution of the number of unique original items in a\n  bootstrap sample?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sampling with replacement occurs in many settings in machine learning,\nnotably in the bagging ensemble technique and the .632+ validation scheme. The\nnumber of unique original items in a bootstrap sample can have an important\nrole in the behaviour of prediction models learned on it. Indeed, there are\nuncontrived examples where duplicate items have no effect. The purpose of this\nreport is to present the distribution of the number of unique original items in\na bootstrap sample clearly and concisely, with a view to enabling other machine\nlearning researchers to understand and control this quantity in existing and\nfuture resampling techniques. We describe the key characteristics of this\ndistribution along with the generalisation for the case where items come from\ndistinct categories, as in classification. In both cases we discuss the normal\nlimit, and conduct an empirical investigation to derive a heuristic for when a\nnormal approximation is permissible.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 14:56:47 GMT"}], "update_date": "2016-02-19", "authors_parsed": [["Mendelson", "Alex F.", ""], ["Zuluaga", "Maria A.", ""], ["Hutton", "Brian F.", ""], ["Ourselin", "S\u00e9bastien", ""]]}, {"id": "1602.05875", "submitter": "Gil Keren", "authors": "Gil Keren and Bj\\\"orn Schuller", "title": "Convolutional RNN: an Enhanced Model for Extracting Features from\n  Sequential Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional convolutional layers extract features from patches of data by\napplying a non-linearity on an affine function of the input. We propose a model\nthat enhances this feature extraction process for the case of sequential data,\nby feeding patches of the data into a recurrent neural network and using the\noutputs or hidden states of the recurrent units to compute the extracted\nfeatures. By doing so, we exploit the fact that a window containing a few\nframes of the sequential data is a sequence itself and this additional\nstructure might encapsulate valuable information. In addition, we allow for\nmore steps of computation in the feature extraction process, which is\npotentially beneficial as an affine function followed by a non-linearity can\nresult in too simple features. Using our convolutional recurrent layers we\nobtain an improvement in performance in two audio classification tasks,\ncompared to traditional convolutional layers. Tensorflow code for the\nconvolutional recurrent layers is publicly available in\nhttps://github.com/cruvadom/Convolutional-RNN.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 16:55:30 GMT"}, {"version": "v2", "created": "Tue, 1 Mar 2016 10:01:25 GMT"}, {"version": "v3", "created": "Thu, 20 Jul 2017 14:03:16 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Keren", "Gil", ""], ["Schuller", "Bj\u00f6rn", ""]]}, {"id": "1602.05897", "submitter": "Amit Daniely", "authors": "Amit Daniely and Roy Frostig and Yoram Singer", "title": "Toward Deeper Understanding of Neural Networks: The Power of\n  Initialization and a Dual View on Expressivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CC cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a general duality between neural networks and compositional\nkernels, striving towards a better understanding of deep learning. We show that\ninitial representations generated by common random initializations are\nsufficiently rich to express all functions in the dual kernel space. Hence,\nthough the training objective is hard to optimize in the worst case, the\ninitial weights form a good starting point for optimization. Our dual view also\nreveals a pragmatic and aesthetic perspective of neural networks and\nunderscores their expressive power.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 18:14:19 GMT"}, {"version": "v2", "created": "Fri, 19 May 2017 18:39:00 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Daniely", "Amit", ""], ["Frostig", "Roy", ""], ["Singer", "Yoram", ""]]}, {"id": "1602.05908", "submitter": "Rong Ge", "authors": "Anima Anandkumar, Rong Ge", "title": "Efficient approaches for escaping higher order saddle points in\n  non-convex optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local search heuristics for non-convex optimizations are popular in applied\nmachine learning. However, in general it is hard to guarantee that such\nalgorithms even converge to a local minimum, due to the existence of\ncomplicated saddle point structures in high dimensions. Many functions have\ndegenerate saddle points such that the first and second order derivatives\ncannot distinguish them with local optima. In this paper we use higher order\nderivatives to escape these saddle points: we design the first efficient\nalgorithm guaranteed to converge to a third order local optimum (while existing\ntechniques are at most second order). We also show that it is NP-hard to extend\nthis further to finding fourth order local optima.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 18:52:15 GMT"}], "update_date": "2016-02-19", "authors_parsed": [["Anandkumar", "Anima", ""], ["Ge", "Rong", ""]]}, {"id": "1602.06025", "submitter": "Yong Ren", "authors": "Yong Ren, Yining Wang, Jun Zhu", "title": "Spectral Learning for Supervised Topic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Supervised topic models simultaneously model the latent topic structure of\nlarge collections of documents and a response variable associated with each\ndocument. Existing inference methods are based on variational approximation or\nMonte Carlo sampling, which often suffers from the local minimum defect.\nSpectral methods have been applied to learn unsupervised topic models, such as\nlatent Dirichlet allocation (LDA), with provable guarantees. This paper\ninvestigates the possibility of applying spectral methods to recover the\nparameters of supervised LDA (sLDA). We first present a two-stage spectral\nmethod, which recovers the parameters of LDA followed by a power update method\nto recover the regression model parameters. Then, we further present a\nsingle-phase spectral algorithm to jointly recover the topic distribution\nmatrix as well as the regression weights. Our spectral algorithms are provably\ncorrect and computationally efficient. We prove a sample complexity bound for\neach algorithm and subsequently derive a sufficient condition for the\nidentifiability of sLDA. Thorough experiments on synthetic and real-world\ndatasets verify the theory and demonstrate the practical effectiveness of the\nspectral algorithms. In fact, our results on a large-scale review rating\ndataset demonstrate that our single-phase spectral algorithm alone gets\ncomparable or even better performance than state-of-the-art methods, while\nprevious work on spectral methods has rarely reported such promising\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2016 02:07:20 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Ren", "Yong", ""], ["Wang", "Yining", ""], ["Zhu", "Jun", ""]]}, {"id": "1602.06042", "submitter": "Nikhil Rao", "authors": "Prateek Jain, Nikhil Rao, Inderjit Dhillon", "title": "Structured Sparse Regression via Greedy Hard-Thresholding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several learning applications require solving high-dimensional regression\nproblems where the relevant features belong to a small number of (overlapping)\ngroups. For very large datasets and under standard sparsity constraints, hard\nthresholding methods have proven to be extremely efficient, but such methods\nrequire NP hard projections when dealing with overlapping groups. In this\npaper, we show that such NP-hard projections can not only be avoided by\nappealing to submodular optimization, but such methods come with strong\ntheoretical guarantees even in the presence of poorly conditioned data (i.e.\nsay when two features have correlation $\\geq 0.99$), which existing analyses\ncannot handle. These methods exhibit an interesting computation-accuracy\ntrade-off and can be extended to significantly harder problems such as sparse\noverlapping groups. Experiments on both real and synthetic data validate our\nclaims and demonstrate that the proposed methods are orders of magnitude faster\nthan other greedy and convex relaxation techniques for learning with\ngroup-structured sparsity.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2016 04:28:50 GMT"}, {"version": "v2", "created": "Fri, 27 May 2016 04:47:38 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Jain", "Prateek", ""], ["Rao", "Nikhil", ""], ["Dhillon", "Inderjit", ""]]}, {"id": "1602.06049", "submitter": "Arnab Bhadury", "authors": "Arnab Bhadury, Jianfei Chen, Jun Zhu, Shixia Liu", "title": "Scaling up Dynamic Topic Models", "comments": "10 pages, 8 figures, to appear in WWW 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic topic models (DTMs) are very effective in discovering topics and\ncapturing their evolution trends in time series data. To do posterior inference\nof DTMs, existing methods are all batch algorithms that scan the full dataset\nbefore each update of the model and make inexact variational approximations\nwith mean-field assumptions. Due to a lack of a more scalable inference\nalgorithm, despite the usefulness, DTMs have not captured large topic dynamics.\n  This paper fills this research void, and presents a fast and parallelizable\ninference algorithm using Gibbs Sampling with Stochastic Gradient Langevin\nDynamics that does not make any unwarranted assumptions. We also present a\nMetropolis-Hastings based $O(1)$ sampler for topic assignments for each word\ntoken. In a distributed environment, our algorithm requires very little\ncommunication between workers during sampling (almost embarrassingly parallel)\nand scales up to large-scale applications. We are able to learn the largest\nDynamic Topic Model to our knowledge, and learned the dynamics of 1,000 topics\nfrom 2.6 million documents in less than half an hour, and our empirical results\nshow that our algorithm is not only orders of magnitude faster than the\nbaselines but also achieves lower perplexity.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2016 05:55:08 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Bhadury", "Arnab", ""], ["Chen", "Jianfei", ""], ["Zhu", "Jun", ""], ["Liu", "Shixia", ""]]}, {"id": "1602.06053", "submitter": "Hongyi Zhang", "authors": "Hongyi Zhang, Suvrit Sra", "title": "First-order Methods for Geodesically Convex Optimization", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geodesic convexity generalizes the notion of (vector space) convexity to\nnonlinear metric spaces. But unlike convex optimization, geodesically convex\n(g-convex) optimization is much less developed. In this paper we contribute to\nthe understanding of g-convex optimization by developing iteration complexity\nanalysis for several first-order algorithms on Hadamard manifolds.\nSpecifically, we prove upper bounds for the global complexity of deterministic\nand stochastic (sub)gradient methods for optimizing smooth and nonsmooth\ng-convex functions, both with and without strong g-convexity. Our analysis also\nreveals how the manifold geometry, especially \\emph{sectional curvature},\nimpacts convergence rates. To the best of our knowledge, our work is the first\nto provide global complexity analysis for first-order algorithms for general\ng-convex optimization.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2016 06:56:50 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Zhang", "Hongyi", ""], ["Sra", "Suvrit", ""]]}, {"id": "1602.06225", "submitter": "Eugene Ndiaye", "authors": "Eugene Ndiaye, Olivier Fercoq, Alexandre Gramfort, Joseph Salmon", "title": "GAP Safe Screening Rules for Sparse-Group-Lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high dimensional settings, sparse structures are crucial for efficiency,\neither in term of memory, computation or performance. In some contexts, it is\nnatural to handle more refined structures than pure sparsity, such as for\ninstance group sparsity. Sparse-Group Lasso has recently been introduced in the\ncontext of linear regression to enforce sparsity both at the feature level and\nat the group level. We adapt to the case of Sparse-Group Lasso recent safe\nscreening rules that discard early in the solver irrelevant features/groups.\nSuch rules have led to important speed-ups for a wide range of iterative\nmethods. Thanks to dual gap computations, we provide new safe screening rules\nfor Sparse-Group Lasso and show significant gains in term of computing time for\na coordinate descent implementation.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2016 17:08:34 GMT"}], "update_date": "2016-02-24", "authors_parsed": [["Ndiaye", "Eugene", ""], ["Fercoq", "Olivier", ""], ["Gramfort", "Alexandre", ""], ["Salmon", "Joseph", ""]]}, {"id": "1602.06235", "submitter": "Julian Katz-Samuels", "authors": "Julian Katz-Samuels and Clayton Scott", "title": "A Mutual Contamination Analysis of Mixed Membership and Partial Label\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning problems can be characterized by mutual contamination\nmodels. In these problems, one observes several random samples from different\nconvex combinations of a set of unknown base distributions. It is of interest\nto decontaminate mutual contamination models, i.e., to recover the base\ndistributions either exactly or up to a permutation. This paper considers the\ngeneral setting where the base distributions are defined on arbitrary\nprobability spaces. We examine the decontamination problem in two mutual\ncontamination models that describe popular machine learning tasks: recovering\nthe base distributions up to a permutation in a mixed membership model, and\nrecovering the base distributions exactly in a partial label model for\nclassification. We give necessary and sufficient conditions for identifiability\nof both mutual contamination models, algorithms for both problems in the\ninfinite and finite sample cases, and introduce novel proof techniques based on\naffine geometry.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2016 17:40:58 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Katz-Samuels", "Julian", ""], ["Scott", "Clayton", ""]]}, {"id": "1602.06276", "submitter": "Milad Kharratzadeh", "authors": "Milad Kharratzadeh, Mark Coates", "title": "Semi-parametric Order-based Generalized Multivariate Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a generalized multivariate regression problem\nwhere the responses are monotonic functions of linear transformations of\npredictors. We propose a semi-parametric algorithm based on the ordering of the\nresponses which is invariant to the functional form of the transformation\nfunction. We prove that our algorithm, which maximizes the rank correlation of\nresponses and linear transformations of predictors, is a consistent estimator\nof the true coefficient matrix. We also identify the rate of convergence and\nshow that the squared estimation error decays with a rate of $o(1/\\sqrt{n})$.\nWe then propose a greedy algorithm to maximize the highly non-smooth objective\nfunction of our model and examine its performance through extensive\nsimulations. Finally, we compare our algorithm with traditional multivariate\nregression algorithms over synthetic and real data.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2016 20:20:56 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Kharratzadeh", "Milad", ""], ["Coates", "Mark", ""]]}, {"id": "1602.06346", "submitter": "Bernardo \\'Avila Pires", "authors": "Bernardo \\'Avila Pires and Csaba Szepesv\\'ari", "title": "Policy Error Bounds for Model-Based Reinforcement Learning with Factored\n  Linear Models", "comments": "30 pages. Corrected typos. Appears in JMLR Workshop and Conference\n  Proceedings 49: Proceedings of the 29th Annual Conference on Learning Theory\n  (COLT 2016)", "journal-ref": "JMLR W&CP 49: COLT 2016 Proceedings (2016) 1-31", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study a model-based approach to calculating approximately\noptimal policies in Markovian Decision Processes. In particular, we derive\nnovel bounds on the loss of using a policy derived from a factored linear\nmodel, a class of models which generalize numerous previous models out of those\nthat come with strong computational guarantees. For the first time in the\nliterature, we derive performance bounds for model-based techniques where the\nmodel inaccuracy is measured in weighted norms. Moreover, our bounds show a\ndecreased sensitivity to the discount factor and, unlike similar bounds derived\nfor other approaches, they are insensitive to measure mismatch. Similarly to\nprevious works, our proofs are also based on contraction arguments, but with\nthe main differences that we use carefully constructed norms building on Banach\nlattices, and the contraction property is only assumed for operators acting on\n\"compressed\" spaces, thus weakening previous assumptions, while strengthening\nprevious results.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2016 23:46:11 GMT"}, {"version": "v2", "created": "Tue, 20 Sep 2016 23:36:02 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Pires", "Bernardo \u00c1vila", ""], ["Szepesv\u00e1ri", "Csaba", ""]]}, {"id": "1602.06349", "submitter": "Ardavan Saeedi", "authors": "Ardavan Saeedi, Matthew Hoffman, Matthew Johnson, Ryan Adams", "title": "The Segmented iHMM: A Simple, Efficient Hierarchical Infinite HMM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the segmented iHMM (siHMM), a hierarchical infinite hidden Markov\nmodel (iHMM) that supports a simple, efficient inference scheme. The siHMM is\nwell suited to segmentation problems, where the goal is to identify points at\nwhich a time series transitions from one relatively stable regime to a new\nregime. Conventional iHMMs often struggle with such problems, since they have\nno mechanism for distinguishing between high- and low-level dynamics.\nHierarchical HMMs (HHMMs) can do better, but they require much more complex and\nexpensive inference algorithms. The siHMM retains the simplicity and efficiency\nof the iHMM, but outperforms it on a variety of segmentation problems,\nachieving performance that matches or exceeds that of a more complicated HHMM.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2016 00:30:03 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Saeedi", "Ardavan", ""], ["Hoffman", "Matthew", ""], ["Johnson", "Matthew", ""], ["Adams", "Ryan", ""]]}, {"id": "1602.06410", "submitter": "Jiaming Xu", "authors": "Bruce Hajek and Yihong Wu and Jiaming Xu", "title": "Semidefinite Programs for Exact Recovery of a Hidden Community", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.SI math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a semidefinite programming (SDP) relaxation of the maximum\nlikelihood estimation for exactly recovering a hidden community of cardinality\n$K$ from an $n \\times n$ symmetric data matrix $A$, where for distinct indices\n$i,j$, $A_{ij} \\sim P$ if $i, j$ are both in the community and $A_{ij} \\sim Q$\notherwise, for two known probability distributions $P$ and $Q$. We identify a\nsufficient condition and a necessary condition for the success of SDP for the\ngeneral model. For both the Bernoulli case ($P={{\\rm Bern}}(p)$ and $Q={{\\rm\nBern}}(q)$ with $p>q$) and the Gaussian case ($P=\\mathcal{N}(\\mu,1)$ and\n$Q=\\mathcal{N}(0,1)$ with $\\mu>0$), which correspond to the problem of planted\ndense subgraph recovery and submatrix localization respectively, the general\nresults lead to the following findings: (1) If $K=\\omega( n /\\log n)$, SDP\nattains the information-theoretic recovery limits with sharp constants; (2) If\n$K=\\Theta(n/\\log n)$, SDP is order-wise optimal, but strictly suboptimal by a\nconstant factor; (3) If $K=o(n/\\log n)$ and $K \\to \\infty$, SDP is order-wise\nsuboptimal. The same critical scaling for $K$ is found to hold, up to constant\nfactors, for the performance of SDP on the stochastic block model of $n$\nvertices partitioned into multiple communities of equal size $K$. A key\ningredient in the proof of the necessary condition is a construction of a\nprimal feasible solution based on random perturbation of the true cluster\nmatrix.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2016 14:15:59 GMT"}, {"version": "v2", "created": "Fri, 3 Jun 2016 15:54:06 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Hajek", "Bruce", ""], ["Wu", "Yihong", ""], ["Xu", "Jiaming", ""]]}, {"id": "1602.06429", "submitter": "Jonathan Warrell", "authors": "Jonathan H. Warrell, Anca F. Savulescu, Robyn Brackin, Musa M. Mhlanga", "title": "Generalized Statistical Tests for mRNA and Protein Subcellular Spatial\n  Patterning against Complete Spatial Randomness", "comments": "17 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive generalized estimators for a number of spatial statistics that have\nbeen used in the analysis of spatially resolved omics data, such as Ripley's K,\nH and L functions, clustering index, and degree of clustering, which allow\nthese statistics to be calculated on data modelled by arbitrary random measures\n(RMs). Our estimators generalize those typically used to calculate these\nstatistics on point process data, allowing them to be calculated on RMs which\nassign continuous values to spatial regions, for instance to model protein\nintensity. The clustering index (H*) compares Ripley's H function calculated\nempirically to its distribution under complete spatial randomness (CSR),\nleading us to consider CSR null hypotheses for RMs which are not\npoint-processes when generalizing this statistic. We thus consider restricted\nclasses of completely random measures which can be simulated directly (Gamma\nprocesses and Marked Poisson Processes), as well as the general class of all\nCSR RMs, for which we derive an exact permutation-based H* estimator. We\nestablish several properties of the estimators, including bounds on the\naccuracy of our general Ripley K estimator, its relationship to a previous\nestimator for the cross-correlation measure, and the relationship of our\ngeneralized H* estimator to previous statistics. To test the ability of our\napproach to identify spatial patterning, we use Fluorescent In Situ\nHybridization (FISH) and Immunofluorescence (IF) data to probe for mRNA and\nprotein subcellular localization patterns respectively in polarizing mouse\nfibroblasts on micropattened cells. We observe correlated patterns of\nclustering over time for corresponding mRNAs and proteins, suggesting a\ndeterministic effect of mRNA localization on protein localization for several\npairs tested, including one case in which spatial patterning at the mRNA level\nhas not been previously demonstrated.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2016 16:31:18 GMT"}, {"version": "v2", "created": "Sun, 28 Feb 2016 12:20:53 GMT"}, {"version": "v3", "created": "Sun, 10 Apr 2016 12:40:27 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Warrell", "Jonathan H.", ""], ["Savulescu", "Anca F.", ""], ["Brackin", "Robyn", ""], ["Mhlanga", "Musa M.", ""]]}, {"id": "1602.06431", "submitter": "Rodrigo Alves", "authors": "Rodrigo A S Alves, Renato Assun\\c{c}\\~ao and Pedro O S Vaz de Melo", "title": "Burstiness Scale: a highly parsimonious model for characterizing random\n  series of events", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem to accurately and parsimoniously characterize random series of\nevents (RSEs) present in the Web, such as e-mail conversations or Twitter\nhashtags, is not trivial. Reports found in the literature reveal two apparent\nconflicting visions of how RSEs should be modeled. From one side, the\nPoissonian processes, of which consecutive events follow each other at a\nrelatively regular time and should not be correlated. On the other side, the\nself-exciting processes, which are able to generate bursts of correlated events\nand periods of inactivities. The existence of many and sometimes conflicting\napproaches to model RSEs is a consequence of the unpredictability of the\naggregated dynamics of our individual and routine activities, which sometimes\nshow simple patterns, but sometimes results in irregular rising and falling\ntrends. In this paper we propose a highly parsimonious way to characterize\ngeneral RSEs, namely the Burstiness Scale (BuSca) model. BuSca views each RSE\nas a mix of two independent process: a Poissonian and a self-exciting one. Here\nwe describe a fast method to extract the two parameters of BuSca that,\ntogether, gives the burstyness scale, which represents how much of the RSE is\ndue to bursty and viral effects. We validated our method in eight diverse and\nlarge datasets containing real random series of events seen in Twitter, Yelp,\ne-mail conversations, Digg, and online forums. Results showed that, even using\nonly two parameters, BuSca is able to accurately describe RSEs seen in these\ndiverse systems, what can leverage many applications.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2016 16:47:10 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Alves", "Rodrigo A S", ""], ["Assun\u00e7\u00e3o", "Renato", ""], ["de Melo", "Pedro O S Vaz", ""]]}, {"id": "1602.06516", "submitter": "Debarghya Ghoshdastidar", "authors": "Debarghya Ghoshdastidar, Ambedkar Dukkipati", "title": "Uniform Hypergraph Partitioning: Provable Tensor Methods and Sampling\n  Techniques", "comments": "To appear in Journal of Machine Learning Research (vol 18)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a series of recent works, we have generalised the consistency results in\nthe stochastic block model literature to the case of uniform and non-uniform\nhypergraphs. The present paper continues the same line of study, where we focus\non partitioning weighted uniform hypergraphs---a problem often encountered in\ncomputer vision. This work is motivated by two issues that arise when a\nhypergraph partitioning approach is used to tackle computer vision problems:\n(i) The uniform hypergraphs constructed for higher-order learning contain all\nedges, but most have negligible weights. Thus, the adjacency tensor is nearly\nsparse, and yet, not binary. (ii) A more serious concern is that standard\npartitioning algorithms need to compute all edge weights, which is\ncomputationally expensive for hypergraphs. This is usually resolved in practice\nby merging the clustering algorithm with a tensor sampling strategy---an\napproach that is yet to be analysed rigorously. We build on our earlier work on\npartitioning dense unweighted uniform hypergraphs (Ghoshdastidar and Dukkipati,\nICML, 2015), and address the aforementioned issues by proposing provable and\nefficient partitioning algorithms. Our analysis justifies the empirical success\nof practical sampling techniques. We also complement our theoretical findings\nby elaborate empirical comparison of various hypergraph partitioning schemes.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2016 10:52:42 GMT"}, {"version": "v2", "created": "Sun, 6 Mar 2016 09:10:27 GMT"}, {"version": "v3", "created": "Thu, 1 Dec 2016 17:49:56 GMT"}, {"version": "v4", "created": "Wed, 17 May 2017 07:26:18 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Ghoshdastidar", "Debarghya", ""], ["Dukkipati", "Ambedkar", ""]]}, {"id": "1602.06518", "submitter": "Anastasia Pentina", "authors": "Anastasia Pentina and Christoph H. Lampert", "title": "Multi-Task Learning with Labeled and Unlabeled Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multi-task learning, a learner is given a collection of prediction tasks\nand needs to solve all of them. In contrast to previous work, which required\nthat annotated training data is available for all tasks, we consider a new\nsetting, in which for some tasks, potentially most of them, only unlabeled\ntraining data is provided. Consequently, to solve all tasks, information must\nbe transferred between tasks with labels and tasks without labels. Focusing on\nan instance-based transfer method we analyze two variants of this setting: when\nthe set of labeled tasks is fixed, and when it can be actively selected by the\nlearner. We state and prove a generalization bound that covers both scenarios\nand derive from it an algorithm for making the choice of labeled tasks (in the\nactive case) and for transferring information between the tasks in a principled\nway. We also illustrate the effectiveness of the algorithm by experiments on\nsynthetic and real data.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2016 11:18:10 GMT"}, {"version": "v2", "created": "Wed, 30 Mar 2016 09:30:21 GMT"}, {"version": "v3", "created": "Wed, 1 Mar 2017 12:22:56 GMT"}, {"version": "v4", "created": "Thu, 8 Jun 2017 09:14:03 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Pentina", "Anastasia", ""], ["Lampert", "Christoph H.", ""]]}, {"id": "1602.06531", "submitter": "Anastasia Pentina", "authors": "Anastasia Pentina and Shai Ben-David", "title": "Multi-task and Lifelong Learning of Kernels", "comments": "Appears in Proceedings of ALT 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a problem of learning kernels for use in SVM classification in\nthe multi-task and lifelong scenarios and provide generalization bounds on the\nerror of a large margin classifier. Our results show that, under mild\nconditions on the family of kernels used for learning, solving several related\ntasks simultaneously is beneficial over single task learning. In particular, as\nthe number of observed tasks grows, assuming that in the considered family of\nkernels there exists one that yields low approximation error on all tasks, the\noverhead associated with learning such a kernel vanishes and the complexity\nconverges to that of learning when this good kernel is given to the learner.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2016 14:05:48 GMT"}, {"version": "v2", "created": "Thu, 18 Aug 2016 11:35:32 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Pentina", "Anastasia", ""], ["Ben-David", "Shai", ""]]}, {"id": "1602.06550", "submitter": "Igor Melnyk", "authors": "Igor Melnyk, Arindam Banerjee, Bryan Matthews, and Nikunj Oza", "title": "Semi-Markov Switching Vector Autoregressive Model-based Anomaly\n  Detection in Aviation Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we consider the problem of anomaly detection in heterogeneous,\nmultivariate, variable-length time series datasets. Our focus is on the\naviation safety domain, where data objects are flights and time series are\nsensor readings and pilot switches. In this context the goal is to detect\nanomalous flight segments, due to mechanical, environmental, or human factors\nin order to identifying operationally significant events and provide insights\ninto the flight operations and highlight otherwise unavailable potential safety\nrisks and precursors to accidents. For this purpose, we propose a framework\nwhich represents each flight using a semi-Markov switching vector\nautoregressive (SMS-VAR) model. Detection of anomalies is then based on\nmeasuring dissimilarities between the model's prediction and data observation.\nThe framework is scalable, due to the inherent parallel nature of most\ncomputations, and can be used to perform online anomaly detection. Extensive\nexperimental results on simulated and real datasets illustrate that the\nframework can detect various types of anomalies along with the key parameters\ninvolved.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2016 16:55:36 GMT"}, {"version": "v2", "created": "Sun, 28 Feb 2016 23:12:31 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Melnyk", "Igor", ""], ["Banerjee", "Arindam", ""], ["Matthews", "Bryan", ""], ["Oza", "Nikunj", ""]]}, {"id": "1602.06566", "submitter": "Mohammad Islam", "authors": "Dipayan Maiti and Mohammad Raihanul Islam and Scotland Leman and Naren\n  Ramakrishnan", "title": "Interactive Storytelling over Document Collections", "comments": "This paper has been submitted to a conference for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Storytelling algorithms aim to 'connect the dots' between disparate documents\nby linking starting and ending documents through a series of intermediate\ndocuments. Existing storytelling algorithms are based on notions of coherence\nand connectivity, and thus the primary way by which users can steer the story\nconstruction is via design of suitable similarity functions. We present an\nalternative approach to storytelling wherein the user can interactively and\niteratively provide 'must use' constraints to preferentially support the\nconstruction of some stories over others. The three innovations in our approach\nare distance measures based on (inferred) topic distributions, the use of\nconstraints to define sets of linear inequalities over paths, and the\nintroduction of slack and surplus variables to condition the topic distribution\nto preferentially emphasize desired terms over others. We describe experimental\nresults to illustrate the effectiveness of our interactive storytelling\napproach over multiple text datasets.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2016 18:46:35 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Maiti", "Dipayan", ""], ["Islam", "Mohammad Raihanul", ""], ["Leman", "Scotland", ""], ["Ramakrishnan", "Naren", ""]]}, {"id": "1602.06577", "submitter": "Ping Li", "authors": "Ping Li, Michael Mitzenmacher, Anshumali Shrivastava", "title": "2-Bit Random Projections, NonLinear Estimators, and Approximate Near\n  Neighbor Search", "comments": "arXiv admin note: text overlap with arXiv:1403.8144", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method of random projections has become a standard tool for machine\nlearning, data mining, and search with massive data at Web scale. The effective\nuse of random projections requires efficient coding schemes for quantizing\n(real-valued) projected data into integers. In this paper, we focus on a simple\n2-bit coding scheme. In particular, we develop accurate nonlinear estimators of\ndata similarity based on the 2-bit strategy. This work will have important\npractical applications. For example, in the task of near neighbor search, a\ncrucial step (often called re-ranking) is to compute or estimate data\nsimilarities once a set of candidate data points have been identified by hash\ntable techniques. This re-ranking step can take advantage of the proposed\ncoding scheme and estimator.\n  As a related task, in this paper, we also study a simple uniform quantization\nscheme for the purpose of building hash tables with projected data. Our\nanalysis shows that typically only a small number of bits are needed. For\nexample, when the target similarity level is high, 2 or 3 bits might be\nsufficient. When the target similarity level is not so high, it is preferable\nto use only 1 or 2 bits. Therefore, a 2-bit scheme appears to be overall a good\nchoice for the task of sublinear time approximate near neighbor search via hash\ntables.\n  Combining these results, we conclude that 2-bit random projections should be\nrecommended for approximate near neighbor search and similarity estimation.\nExtensive experimental results are provided.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2016 20:46:13 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Li", "Ping", ""], ["Mitzenmacher", "Michael", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "1602.06606", "submitter": "Igor Melnyk", "authors": "Igor Melnyk and Arindam Banerjee", "title": "Estimating Structured Vector Autoregressive Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While considerable advances have been made in estimating high-dimensional\nstructured models from independent data using Lasso-type models, limited\nprogress has been made for settings when the samples are dependent. We consider\nestimating structured VAR (vector auto-regressive models), where the structure\ncan be captured by any suitable norm, e.g., Lasso, group Lasso, order weighted\nLasso, sparse group Lasso, etc. In VAR setting with correlated noise, although\nthere is strong dependence over time and covariates, we establish bounds on the\nnon-asymptotic estimation error of structured VAR parameters. Surprisingly, the\nestimation error is of the same order as that of the corresponding Lasso-type\nestimator with independent samples, and the analysis holds for any norm. Our\nanalysis relies on results in generic chaining, sub-exponential martingales,\nand spectral representation of VAR models. Experimental results on synthetic\ndata with a variety of structures as well as real aviation data are presented,\nvalidating theoretical results.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2016 23:47:36 GMT"}, {"version": "v2", "created": "Sun, 28 Feb 2016 22:06:48 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Melnyk", "Igor", ""], ["Banerjee", "Arindam", ""]]}, {"id": "1602.06612", "submitter": "Soledad Villar", "authors": "Dustin G. Mixon, Soledad Villar, Rachel Ward", "title": "Clustering subgaussian mixtures by semidefinite programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a model-free relax-and-round algorithm for k-means clustering\nbased on a semidefinite relaxation due to Peng and Wei. The algorithm\ninterprets the SDP output as a denoised version of the original data and then\nrounds this output to a hard clustering. We provide a generic method for\nproving performance guarantees for this algorithm, and we analyze the algorithm\nin the context of subgaussian mixture models. We also study the fundamental\nlimits of estimating Gaussian centers by k-means clustering in order to compare\nour approximation guarantee to the theoretically optimal k-means clustering\nsolution.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 00:19:20 GMT"}, {"version": "v2", "created": "Tue, 10 May 2016 19:38:37 GMT"}], "update_date": "2016-05-11", "authors_parsed": [["Mixon", "Dustin G.", ""], ["Villar", "Soledad", ""], ["Ward", "Rachel", ""]]}, {"id": "1602.06632", "submitter": "Tejal Bhamre", "authors": "Tejal Bhamre, Teng Zhang, Amit Singer", "title": "Denoising and Covariance Estimation of Single Particle Cryo-EM Images", "comments": "Revision for JSB", "journal-ref": null, "doi": "10.1016/j.jsb.2016.04.013", "report-no": null, "categories": "cs.CV q-bio.BM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of image restoration in cryo-EM entails correcting for the\neffects of the Contrast Transfer Function (CTF) and noise. Popular methods for\nimage restoration include `phase flipping', which corrects only for the Fourier\nphases but not amplitudes, and Wiener filtering, which requires the spectral\nsignal to noise ratio. We propose a new image restoration method which we call\n`Covariance Wiener Filtering' (CWF). In CWF, the covariance matrix of the\nprojection images is used within the classical Wiener filtering framework for\nsolving the image restoration deconvolution problem. Our estimation procedure\nfor the covariance matrix is new and successfully corrects for the CTF. We\ndemonstrate the efficacy of CWF by applying it to restore both simulated and\nexperimental cryo-EM images. Results with experimental datasets demonstrate\nthat CWF provides a good way to evaluate the particle images and to see what\nthe dataset contains even without 2D classification and averaging.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 03:04:44 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2016 04:03:55 GMT"}, {"version": "v3", "created": "Wed, 6 Apr 2016 19:41:52 GMT"}], "update_date": "2016-04-29", "authors_parsed": [["Bhamre", "Tejal", ""], ["Zhang", "Teng", ""], ["Singer", "Amit", ""]]}, {"id": "1602.06662", "submitter": "Mikael Henaff", "authors": "Mikael Henaff, Arthur Szlam, Yann LeCun", "title": "Recurrent Orthogonal Networks and Long-Memory Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although RNNs have been shown to be powerful tools for processing sequential\ndata, finding architectures or optimization strategies that allow them to model\nvery long term dependencies is still an active area of research. In this work,\nwe carefully analyze two synthetic datasets originally outlined in (Hochreiter\nand Schmidhuber, 1997) which are used to evaluate the ability of RNNs to store\ninformation over many time steps. We explicitly construct RNN solutions to\nthese problems, and using these constructions, illuminate both the problems\nthemselves and the way in which RNNs store different types of information in\ntheir hidden states. These constructions furthermore explain the success of\nrecent methods that specify unitary initializations or constraints on the\ntransition matrices.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 06:51:25 GMT"}, {"version": "v2", "created": "Wed, 15 Mar 2017 17:45:08 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Henaff", "Mikael", ""], ["Szlam", "Arthur", ""], ["LeCun", "Yann", ""]]}, {"id": "1602.06664", "submitter": "Ju Sun", "authors": "Ju Sun, Qing Qu, John Wright", "title": "A Geometric Analysis of Phase Retrieval", "comments": "61 pages, 5 figures. A short version can be found here\n  http://sunju.org/docs/PR_G4_16.pdf . Revised according to reviewers' feedback", "journal-ref": "Foundations of Computational Mathematics, 18(5):1131--1198, 2018", "doi": "10.1007/s10208-017-9365-9", "report-no": null, "categories": "cs.IT math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can we recover a complex signal from its Fourier magnitudes? More generally,\ngiven a set of $m$ measurements, $y_k = |\\mathbf a_k^* \\mathbf x|$ for $k = 1,\n\\dots, m$, is it possible to recover $\\mathbf x \\in \\mathbb{C}^n$ (i.e.,\nlength-$n$ complex vector)? This **generalized phase retrieval** (GPR) problem\nis a fundamental task in various disciplines, and has been the subject of much\nrecent investigation. Natural nonconvex heuristics often work remarkably well\nfor GPR in practice, but lack clear theoretical explanations. In this paper, we\ntake a step towards bridging this gap. We prove that when the measurement\nvectors $\\mathbf a_k$'s are generic (i.i.d. complex Gaussian) and the number of\nmeasurements is large enough ($m \\ge C n \\log^3 n$), with high probability, a\nnatural least-squares formulation for GPR has the following benign geometric\nstructure: (1) there are no spurious local minimizers, and all global\nminimizers are equal to the target signal $\\mathbf x$, up to a global phase;\nand (2) the objective function has a negative curvature around each saddle\npoint. This structure allows a number of iterative optimization methods to\nefficiently find a global minimizer, without special initialization. To\ncorroborate the claim, we describe and analyze a second-order trust-region\nalgorithm.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 06:54:07 GMT"}, {"version": "v2", "created": "Tue, 1 Mar 2016 05:05:07 GMT"}, {"version": "v3", "created": "Sun, 1 Jan 2017 06:47:43 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Sun", "Ju", ""], ["Qu", "Qing", ""], ["Wright", "John", ""]]}, {"id": "1602.06687", "submitter": "Margareta Ackerman Margareta Ackerman", "authors": "Margareta Ackerman, Andreas Adolfsson, and Naomi Brownstein", "title": "An Effective and Efficient Approach for Clusterability Evaluation", "comments": "10 pages, 2 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is an essential data mining tool that aims to discover inherent\ncluster structure in data. As such, the study of clusterability, which\nevaluates whether data possesses such structure, is an integral part of cluster\nanalysis. Yet, despite their central role in the theory and application of\nclustering, current notions of clusterability fall short in two crucial aspects\nthat render them impractical; most are computationally infeasible and others\nfail to classify the structure of real datasets.\n  In this paper, we propose a novel approach to clusterability evaluation that\nis both computationally efficient and successfully captures the structure of\nreal data. Our method applies multimodality tests to the (one-dimensional) set\nof pairwise distances based on the original, potentially high-dimensional data.\nWe present extensive analyses of our approach for both the Dip and Silverman\nmultimodality tests on real data as well as 17,000 simulations, demonstrating\nthe success of our approach as the first practical notion of clusterability.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 09:01:10 GMT"}], "update_date": "2016-02-24", "authors_parsed": [["Ackerman", "Margareta", ""], ["Adolfsson", "Andreas", ""], ["Brownstein", "Naomi", ""]]}, {"id": "1602.06693", "submitter": "Maurizio Filippone", "authors": "Kurt Cutajar, Michael A. Osborne, John P. Cunningham, Maurizio\n  Filippone", "title": "Preconditioning Kernel Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational and storage complexity of kernel machines presents the\nprimary barrier to their scaling to large, modern, datasets. A common way to\ntackle the scalability issue is to use the conjugate gradient algorithm, which\nrelieves the constraints on both storage (the kernel matrix need not be stored)\nand computation (both stochastic gradients and parallelization can be used).\nEven so, conjugate gradient is not without its own issues: the conditioning of\nkernel matrices is often such that conjugate gradients will have poor\nconvergence in practice. Preconditioning is a common approach to alleviating\nthis issue. Here we propose preconditioned conjugate gradients for kernel\nmachines, and develop a broad range of preconditioners particularly useful for\nkernel matrices. We describe a scalable approach to both solving kernel\nmachines and learning their hyperparameters. We show this approach is exact in\nthe limit of iterations and outperforms state-of-the-art approximations for a\ngiven computational budget.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 09:21:48 GMT"}, {"version": "v2", "created": "Wed, 25 May 2016 08:05:52 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Cutajar", "Kurt", ""], ["Osborne", "Michael A.", ""], ["Cunningham", "John P.", ""], ["Filippone", "Maurizio", ""]]}, {"id": "1602.06701", "submitter": "Brooks Paige", "authors": "Brooks Paige, Frank Wood", "title": "Inference Networks for Sequential Monte Carlo in Graphical Models", "comments": "10 pages. Updated from version at ICML 2016; includes code at\n  http://github.com/tbrx/compiled-inference", "journal-ref": "Paige, B., & Wood, F. (2016). Inference Networks for Sequential\n  Monte Carlo in Graphical Models. In Proceedings of the 33rd International\n  Conference on Machine Learning, JMLR W&CP 48: 3040-3049", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new approach for amortizing inference in directed graphical\nmodels by learning heuristic approximations to stochastic inverses, designed\nspecifically for use as proposal distributions in sequential Monte Carlo\nmethods. We describe a procedure for constructing and learning a structured\nneural network which represents an inverse factorization of the graphical\nmodel, resulting in a conditional density estimator that takes as input\nparticular values of the observed random variables, and returns an\napproximation to the distribution of the latent variables. This recognition\nmodel can be learned offline, independent from any particular dataset, prior to\nperforming inference. The output of these networks can be used as\nautomatically-learned high-quality proposal distributions to accelerate\nsequential Monte Carlo across a diverse range of problem settings.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 09:39:09 GMT"}, {"version": "v2", "created": "Wed, 7 Mar 2018 19:52:52 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Paige", "Brooks", ""], ["Wood", "Frank", ""]]}, {"id": "1602.06725", "submitter": "Andriy Mnih", "authors": "Andriy Mnih, Danilo J. Rezende", "title": "Variational inference for Monte Carlo objectives", "comments": "Appears in Proceedings of the 33rd International Conference on\n  Machine Learning (ICML), New York, NY, USA, 2016. JMLR: W&CP volume 48", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in deep latent variable models has largely been driven by the\ndevelopment of flexible and scalable variational inference methods. Variational\ntraining of this type involves maximizing a lower bound on the log-likelihood,\nusing samples from the variational posterior to compute the required gradients.\nRecently, Burda et al. (2016) have derived a tighter lower bound using a\nmulti-sample importance sampling estimate of the likelihood and showed that\noptimizing it yields models that use more of their capacity and achieve higher\nlikelihoods. This development showed the importance of such multi-sample\nobjectives and explained the success of several related approaches.\n  We extend the multi-sample approach to discrete latent variables and analyze\nthe difficulty encountered when estimating the gradients involved. We then\ndevelop the first unbiased gradient estimator designed for importance-sampled\nobjectives and evaluate it at training generative and structured output\nprediction models. The resulting estimator, which is based on low-variance\nper-sample learning signals, is both simpler and more effective than the NVIL\nestimator proposed for the single-sample variational objective, and is\ncompetitive with the currently used biased estimators.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 11:06:06 GMT"}, {"version": "v2", "created": "Wed, 1 Jun 2016 16:36:06 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Mnih", "Andriy", ""], ["Rezende", "Danilo J.", ""]]}, {"id": "1602.06746", "submitter": "Bjoern Andres", "authors": "Iaroslav Shcherbatyi and Bjoern Andres", "title": "Convexification of Learning from Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularized empirical risk minimization with constrained labels (in contrast\nto fixed labels) is a remarkably general abstraction of learning. For common\nloss and regularization functions, this optimization problem assumes the form\nof a mixed integer program (MIP) whose objective function is non-convex. In\nthis form, the problem is resistant to standard optimization techniques. We\nconstruct MIPs with the same solutions whose objective functions are convex.\nSpecifically, we characterize the tightest convex extension of the objective\nfunction, given by the Legendre-Fenchel biconjugate. Computing values of this\ntightest convex extension is NP-hard. However, by applying our characterization\nto every function in an additive decomposition of the objective function, we\nobtain a class of looser convex extensions that can be computed efficiently.\nFor some decompositions, common loss and regularization functions, we derive a\nclosed form.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 12:20:50 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Shcherbatyi", "Iaroslav", ""], ["Andres", "Bjoern", ""]]}, {"id": "1602.06872", "submitter": "Christopher Musco", "authors": "Roy Frostig, Cameron Musco, Christopher Musco, Aaron Sidford", "title": "Principal Component Projection Without Principal Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to efficiently project a vector onto the top principal components\nof a matrix, without explicitly computing these components. Specifically, we\nintroduce an iterative algorithm that provably computes the projection using\nfew calls to any black-box routine for ridge regression.\n  By avoiding explicit principal component analysis (PCA), our algorithm is the\nfirst with no runtime dependence on the number of top principal components. We\nshow that it can be used to give a fast iterative method for the popular\nprincipal component regression problem, giving the first major runtime\nimprovement over the naive method of combining PCA with regression.\n  To achieve our results, we first observe that ridge regression can be used to\nobtain a \"smooth projection\" onto the top principal components. We then sharpen\nthis approximation to true projection using a low-degree polynomial\napproximation to the matrix step function. Step function approximation is a\ntopic of long-term interest in scientific computing. We extend prior theory by\nconstructing polynomials with simple iterative structure and rigorously\nanalyzing their behavior under limited precision.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 17:52:02 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 17:29:20 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Frostig", "Roy", ""], ["Musco", "Cameron", ""], ["Musco", "Christopher", ""], ["Sidford", "Aaron", ""]]}, {"id": "1602.06886", "submitter": "Akash Srivastava", "authors": "Akash Srivastava, James Zou and Charles Sutton", "title": "Clustering with a Reject Option: Interactive Clustering as Bayesian\n  Prior Elicitation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A good clustering can help a data analyst to explore and understand a data\nset, but what constitutes a good clustering may depend on domain-specific and\napplication-specific criteria. These criteria can be difficult to formalize,\neven when it is easy for an analyst to know a good clustering when she sees\none. We present a new approach to interactive clustering for data exploration,\ncalled \\ciif, based on a particularly simple feedback mechanism, in which an\nanalyst can choose to reject individual clusters and request new ones. The new\nclusters should be different from previously rejected clusters while still\nfitting the data well. We formalize this interaction in a novel Bayesian prior\nelicitation framework. In each iteration, the prior is adapted to account for\nall the previous feedback, and a new clustering is then produced from the\nposterior distribution. To achieve the computational efficiency necessary for\nan interactive setting, we propose an incremental optimization method over data\nminibatches using Lagrangian relaxation. Experiments demonstrate that \\ciif can\nproduce accurate and diverse clusterings.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 18:38:27 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 14:15:58 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Srivastava", "Akash", ""], ["Zou", "James", ""], ["Sutton", "Charles", ""]]}, {"id": "1602.06916", "submitter": "Abolfazl Hashemi", "authors": "Abolfazl Hashemi, Haris Vikalo", "title": "Sparse Linear Regression via Generalized Orthogonal Least-Squares", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse linear regression, which entails finding a sparse solution to an\nunderdetermined system of linear equations, can formally be expressed as an\n$l_0$-constrained least-squares problem. The Orthogonal Least-Squares (OLS)\nalgorithm sequentially selects the features (i.e., columns of the coefficient\nmatrix) to greedily find an approximate sparse solution. In this paper, a\ngeneralization of Orthogonal Least-Squares which relies on a recursive relation\nbetween the components of the optimal solution to select L features at each\nstep and solve the resulting overdetermined system of equations is proposed.\nSimulation results demonstrate that the generalized OLS algorithm is\ncomputationally efficient and achieves performance superior to that of existing\ngreedy algorithms broadly used in the literature.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 19:55:01 GMT"}, {"version": "v2", "created": "Thu, 28 Jul 2016 22:52:35 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Hashemi", "Abolfazl", ""], ["Vikalo", "Haris", ""]]}, {"id": "1602.06929", "submitter": "Praneeth Netrapalli", "authors": "Prateek Jain and Chi Jin and Sham M. Kakade and Praneeth Netrapalli\n  and Aaron Sidford", "title": "Streaming PCA: Matching Matrix Bernstein and Near-Optimal Finite Sample\n  Guarantees for Oja's Algorithm", "comments": "Updated title", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work provides improved guarantees for streaming principle component\nanalysis (PCA). Given $A_1, \\ldots, A_n\\in \\mathbb{R}^{d\\times d}$ sampled\nindependently from distributions satisfying $\\mathbb{E}[A_i] = \\Sigma$ for\n$\\Sigma \\succeq \\mathbf{0}$, this work provides an $O(d)$-space linear-time\nsingle-pass streaming algorithm for estimating the top eigenvector of $\\Sigma$.\nThe algorithm nearly matches (and in certain cases improves upon) the accuracy\nobtained by the standard batch method that computes top eigenvector of the\nempirical covariance $\\frac{1}{n} \\sum_{i \\in [n]} A_i$ as analyzed by the\nmatrix Bernstein inequality. Moreover, to achieve constant accuracy, our\nalgorithm improves upon the best previous known sample complexities of\nstreaming algorithms by either a multiplicative factor of $O(d)$ or\n$1/\\mathrm{gap}$ where $\\mathrm{gap}$ is the relative distance between the top\ntwo eigenvalues of $\\Sigma$.\n  These results are achieved through a novel analysis of the classic Oja's\nalgorithm, one of the oldest and most popular algorithms for streaming PCA. In\nparticular, this work shows that simply picking a random initial point $w_0$\nand applying the update rule $w_{i + 1} = w_i + \\eta_i A_i w_i$ suffices to\naccurately estimate the top eigenvector, with a suitable choice of $\\eta_i$. We\nbelieve our result sheds light on how to efficiently perform streaming PCA both\nin theory and in practice and we hope that our analysis may serve as the basis\nfor analyzing many variants and extensions of streaming PCA.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 20:30:37 GMT"}, {"version": "v2", "created": "Mon, 28 Mar 2016 17:45:51 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Jain", "Prateek", ""], ["Jin", "Chi", ""], ["Kakade", "Sham M.", ""], ["Netrapalli", "Praneeth", ""], ["Sidford", "Aaron", ""]]}, {"id": "1602.06989", "submitter": "Renato Cordeiro De Amorim", "authors": "Renato Cordeiro de Amorim and Christian Hennig", "title": "Recovering the number of clusters in data sets with noise features using\n  feature rescaling factors", "comments": null, "journal-ref": "Information Sciences 324 (2015), 126-145", "doi": "10.1016/j.ins.2015.06.039", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce three methods for re-scaling data sets aiming at\nimproving the likelihood of clustering validity indexes to return the true\nnumber of spherical Gaussian clusters with additional noise features. Our\nmethod obtains feature re-scaling factors taking into account the structure of\na given data set and the intuitive idea that different features may have\ndifferent degrees of relevance at different clusters.\n  We experiment with the Silhouette (using squared Euclidean, Manhattan, and\nthe p$^{th}$ power of the Minkowski distance), Dunn's, Calinski-Harabasz and\nHartigan indexes on data sets with spherical Gaussian clusters with and without\nnoise features. We conclude that our methods indeed increase the chances of\nestimating the true number of clusters in a data set.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 22:40:00 GMT"}], "update_date": "2016-02-24", "authors_parsed": [["de Amorim", "Renato Cordeiro", ""], ["Hennig", "Christian", ""]]}, {"id": "1602.07043", "submitter": "Suresh Venkatasubramanian", "authors": "Philip Adler, Casey Falk, Sorelle A. Friedler, Gabriel Rybeck, Carlos\n  Scheidegger, Brandon Smith and Suresh Venkatasubramanian", "title": "Auditing Black-box Models for Indirect Influence", "comments": "Final version of paper that appears in the IEEE International\n  Conference on Data Mining (ICDM), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-trained predictive models see widespread use, but for the most part they\nare used as black boxes which output a prediction or score. It is therefore\nhard to acquire a deeper understanding of model behavior, and in particular how\ndifferent features influence the model prediction. This is important when\ninterpreting the behavior of complex models, or asserting that certain\nproblematic attributes (like race or gender) are not unduly influencing\ndecisions.\n  In this paper, we present a technique for auditing black-box models, which\nlets us study the extent to which existing models take advantage of particular\nfeatures in the dataset, without knowing how the models work. Our work focuses\non the problem of indirect influence: how some features might indirectly\ninfluence outcomes via other, related features. As a result, we can find\nattribute influences even in cases where, upon further direct examination of\nthe model, the attribute is not referred to by the model at all.\n  Our approach does not require the black-box model to be retrained. This is\nimportant if (for example) the model is only accessible via an API, and\ncontrasts our work with other methods that investigate feature influence like\nfeature selection. We present experimental evidence for the effectiveness of\nour procedure using a variety of publicly available datasets and models. We\nalso validate our procedure using techniques from interpretable learning and\nfeature selection, as well as against other black-box auditing procedures.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 04:52:28 GMT"}, {"version": "v2", "created": "Wed, 30 Nov 2016 06:55:16 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Adler", "Philip", ""], ["Falk", "Casey", ""], ["Friedler", "Sorelle A.", ""], ["Rybeck", "Gabriel", ""], ["Scheidegger", "Carlos", ""], ["Smith", "Brandon", ""], ["Venkatasubramanian", "Suresh", ""]]}, {"id": "1602.07046", "submitter": "Simon Du", "authors": "Maria Florina Balcan, Simon S. Du, Yining Wang, Adams Wei Yu", "title": "An Improved Gap-Dependency Analysis of the Noisy Power Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the noisy power method algorithm, which has wide applications in\nmachine learning and statistics, especially those related to principal\ncomponent analysis (PCA) under resource (communication, memory or privacy)\nconstraints. Existing analysis of the noisy power method shows an\nunsatisfactory dependency over the \"consecutive\" spectral gap\n$(\\sigma_k-\\sigma_{k+1})$ of an input data matrix, which could be very small\nand hence limits the algorithm's applicability. In this paper, we present a new\nanalysis of the noisy power method that achieves improved gap dependency for\nboth sample complexity and noise tolerance bounds. More specifically, we\nimprove the dependency over $(\\sigma_k-\\sigma_{k+1})$ to dependency over\n$(\\sigma_k-\\sigma_{q+1})$, where $q$ is an intermediate algorithm parameter and\ncould be much larger than the target rank $k$. Our proofs are built upon a\nnovel characterization of proximity between two subspaces that differ from\ncanonical angle characterizations analyzed in previous works. Finally, we apply\nour improved bounds to distributed private PCA and memory-efficient streaming\nPCA and obtain bounds that are superior to existing results in the literature.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 05:15:08 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Balcan", "Maria Florina", ""], ["Du", "Simon S.", ""], ["Wang", "Yining", ""], ["Yu", "Adams Wei", ""]]}, {"id": "1602.07107", "submitter": "Richard Combes", "authors": "Thomas Bonald and Richard Combes", "title": "A Streaming Algorithm for Crowdsourced Data Classification", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a streaming algorithm for the binary classification of data based\non crowdsourcing. The algorithm learns the competence of each labeller by\ncomparing her labels to those of other labellers on the same tasks and uses\nthis information to minimize the prediction error rate on each task. We provide\nperformance guarantees of our algorithm for a fixed population of independent\nlabellers. In particular, we show that our algorithm is optimal in the sense\nthat the cumulative regret compared to the optimal decision with known labeller\nerror probabilities is finite, independently of the number of tasks to label.\nThe complexity of the algorithm is linear in the number of labellers and the\nnumber of tasks, up to some logarithmic factors. Numerical experiments\nillustrate the performance of our algorithm compared to existing algorithms,\nincluding simple majority voting and expectation-maximization algorithms, on\nboth synthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 10:21:58 GMT"}], "update_date": "2016-02-24", "authors_parsed": [["Bonald", "Thomas", ""], ["Combes", "Richard", ""]]}, {"id": "1602.07109", "submitter": "Maximilian Soelch", "authors": "Maximilian Soelch, Justin Bayer, Marvin Ludersdorfer, Patrick van der\n  Smagt", "title": "Variational Inference for On-line Anomaly Detection in High-Dimensional\n  Time Series", "comments": "Accepted as workshop paper at ICLR 2016; accepted as workshop paper\n  for anomaly detection workshop at ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate variational inference has shown to be a powerful tool for\nmodeling unknown complex probability distributions. Recent advances in the\nfield allow us to learn probabilistic models of sequences that actively exploit\nspatial and temporal structure. We apply a Stochastic Recurrent Network (STORN)\nto learn robot time series data. Our evaluation demonstrates that we can\nrobustly detect anomalies both off- and on-line.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 10:31:51 GMT"}, {"version": "v2", "created": "Wed, 23 Mar 2016 15:42:23 GMT"}, {"version": "v3", "created": "Tue, 5 Apr 2016 16:56:29 GMT"}, {"version": "v4", "created": "Thu, 19 May 2016 20:20:45 GMT"}, {"version": "v5", "created": "Tue, 14 Jun 2016 10:01:00 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Soelch", "Maximilian", ""], ["Bayer", "Justin", ""], ["Ludersdorfer", "Marvin", ""], ["van der Smagt", "Patrick", ""]]}, {"id": "1602.07120", "submitter": "Sivan Sabato", "authors": "Sivan Sabato", "title": "Submodular Learning and Covering with Response-Dependent Costs", "comments": null, "journal-ref": "S. Sabato, \"Submodular Learning and Covering with\n  Response-Dependent Costs \", Theoretical Computer Science, 742:98--113, 2018", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider interactive learning and covering problems, in a setting where\nactions may incur different costs, depending on the response to the action. We\npropose a natural greedy algorithm for response-dependent costs. We bound the\napproximation factor of this greedy algorithm in active learning settings as\nwell as in the general setting. We show that a different property of the cost\nfunction controls the approximation factor in each of these scenarios. We\nfurther show that in both settings, the approximation factor of this greedy\nalgorithm is near-optimal among all greedy algorithms. Experiments demonstrate\nthe advantages of the proposed algorithm in the response-dependent cost\nsetting.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 11:20:37 GMT"}, {"version": "v2", "created": "Mon, 29 Aug 2016 12:10:15 GMT"}, {"version": "v3", "created": "Sun, 18 Nov 2018 12:25:25 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Sabato", "Sivan", ""]]}, {"id": "1602.07194", "submitter": "Matth\\\"aus Kleindessner", "authors": "Matth\\\"aus Kleindessner and Ulrike von Luxburg", "title": "Lens depth function and k-relative neighborhood graph: versatile tools\n  for ordinal data analysis", "comments": null, "journal-ref": "Journal of Machine Learning Research 18(58):1-52, 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years it has become popular to study machine learning problems in a\nsetting of ordinal distance information rather than numerical distance\nmeasurements. By ordinal distance information we refer to binary answers to\ndistance comparisons such as $d(A,B)<d(C,D)$. For many problems in machine\nlearning and statistics it is unclear how to solve them in such a scenario. Up\nto now, the main approach is to explicitly construct an ordinal embedding of\nthe data points in the Euclidean space, an approach that has a number of\ndrawbacks. In this paper, we propose algorithms for the problems of medoid\nestimation, outlier identification, classification, and clustering when given\nonly ordinal data. They are based on estimating the lens depth function and the\n$k$-relative neighborhood graph on a data set. Our algorithms are simple, are\nmuch faster than an ordinal embedding approach and avoid some of its drawbacks,\nand can easily be parallelized.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 15:30:46 GMT"}, {"version": "v2", "created": "Mon, 24 Jul 2017 11:52:01 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Kleindessner", "Matth\u00e4us", ""], ["von Luxburg", "Ulrike", ""]]}, {"id": "1602.07265", "submitter": "Chicheng Zhang", "authors": "Alina Beygelzimer, Daniel Hsu, John Langford, Chicheng Zhang", "title": "Search Improves Label for Active Learning", "comments": "32 pages; NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate active learning with access to two distinct oracles: Label\n(which is standard) and Search (which is not). The Search oracle models the\nsituation where a human searches a database to seed or counterexample an\nexisting solution. Search is stronger than Label while being natural to\nimplement in many situations. We show that an algorithm using both oracles can\nprovide exponentially large problem-dependent improvements over Label alone.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 19:05:09 GMT"}, {"version": "v2", "created": "Mon, 24 Oct 2016 06:29:08 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Beygelzimer", "Alina", ""], ["Hsu", "Daniel", ""], ["Langford", "John", ""], ["Zhang", "Chicheng", ""]]}, {"id": "1602.07277", "submitter": "Xiao Pu", "authors": "Ery Arias-Castro and Xiao Pu", "title": "A Simple Approach to Sparse Clustering", "comments": null, "journal-ref": "Computational Statistics & Data Analysis. 2017 Jan 31", "doi": "10.1016/j.csda.2016.08.003", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the problem of sparse clustering, where it is assumed that only a\nsubset of the features are useful for clustering purposes. In the framework of\nthe COSA method of Friedman and Meulman, subsequently improved in the form of\nthe Sparse K-means method of Witten and Tibshirani, a natural and simpler\nhill-climbing approach is introduced. The new method is shown to be competitive\nwith these two methods and others.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 19:49:16 GMT"}, {"version": "v2", "created": "Sun, 11 Sep 2016 17:40:59 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Arias-Castro", "Ery", ""], ["Pu", "Xiao", ""]]}, {"id": "1602.07291", "submitter": "Omid Sadjadi", "authors": "Seyed Omid Sadjadi, Sriram Ganapathy, Jason W. Pelecanos", "title": "The IBM 2016 Speaker Recognition System", "comments": "Submitted to Odyssey 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe the recent advancements made in the IBM i-vector\nspeaker recognition system for conversational speech. In particular, we\nidentify key techniques that contribute to significant improvements in\nperformance of our system, and quantify their contributions. The techniques\ninclude: 1) a nearest-neighbor discriminant analysis (NDA) approach that is\nformulated to alleviate some of the limitations associated with the\nconventional linear discriminant analysis (LDA) that assumes Gaussian\nclass-conditional distributions, 2) the application of speaker- and\nchannel-adapted features, which are derived from an automatic speech\nrecognition (ASR) system, for speaker recognition, and 3) the use of a deep\nneural network (DNN) acoustic model with a large number of output units (~10k\nsenones) to compute the frame-level soft alignments required in the i-vector\nestimation process. We evaluate these techniques on the NIST 2010 speaker\nrecognition evaluation (SRE) extended core conditions involving telephone and\nmicrophone trials. Experimental results indicate that: 1) the NDA is more\neffective (up to 35% relative improvement in terms of EER) than the traditional\nparametric LDA for speaker recognition, 2) when compared to raw acoustic\nfeatures (e.g., MFCCs), the ASR speaker-adapted features provide gains in\nspeaker recognition performance, and 3) increasing the number of output units\nin the DNN acoustic model (i.e., increasing the senone set size from 2k to 10k)\nprovides consistent improvements in performance (for example from 37% to 57%\nrelative EER gains over our baseline GMM i-vector system). To our knowledge,\nresults reported in this paper represent the best performances published to\ndate on the NIST SRE 2010 extended core tasks.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 20:39:40 GMT"}], "update_date": "2016-02-24", "authors_parsed": [["Sadjadi", "Seyed Omid", ""], ["Ganapathy", "Sriram", ""], ["Pelecanos", "Jason W.", ""]]}, {"id": "1602.07349", "submitter": "Tomaso Aste", "authors": "Wolfram Barfuss, Guido Previde Massara, T. Di Matteo, Tomaso Aste", "title": "Parsimonious modeling with Information Filtering Networks", "comments": "17 pages, 10 figures, 3 tables", "journal-ref": "Phys. Rev. E 94, 062306 (2016)", "doi": "10.1103/PhysRevE.94.062306", "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a methodology to construct parsimonious probabilistic models.\nThis method makes use of Information Filtering Networks to produce a robust\nestimate of the global sparse inverse covariance from a simple sum of local\ninverse covariances computed on small sub-parts of the network. Being based on\nlocal and low-dimensional inversions, this method is computationally very\nefficient and statistically robust even for the estimation of inverse\ncovariance of high-dimensional, noisy and short time-series. Applied to\nfinancial data our method results computationally more efficient than\nstate-of-the-art methodologies such as Glasso producing, in a fraction of the\ncomputation time, models that can have equivalent or better performances but\nwith a sparser inference structure. We also discuss performances with sparse\nfactor models where we notice that relative performances decrease with the\nnumber of factors. The local nature of this approach allows us to perform\ncomputations in parallel and provides a tool for dynamical adaptation by\npartial updating when the properties of some variables change without the need\nof recomputing the whole model. This makes this approach particularly suitable\nto handle big datasets with large numbers of variables. Examples of practical\napplication for forecasting, stress testing and risk allocation in financial\nsystems are also provided.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 23:03:56 GMT"}, {"version": "v2", "created": "Thu, 30 Jun 2016 15:11:14 GMT"}, {"version": "v3", "created": "Wed, 23 Nov 2016 15:32:05 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Barfuss", "Wolfram", ""], ["Massara", "Guido Previde", ""], ["Di Matteo", "T.", ""], ["Aste", "Tomaso", ""]]}, {"id": "1602.07387", "submitter": "Peter Kairouz", "authors": "Peter Kairouz and Keith Bonawitz and Daniel Ramage", "title": "Discrete Distribution Estimation under Local Privacy", "comments": "23 pages, 12 figures, submitted to ICML 2016 (under review)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The collection and analysis of user data drives improvements in the app and\nweb ecosystems, but comes with risks to privacy. This paper examines discrete\ndistribution estimation under local privacy, a setting wherein service\nproviders can learn the distribution of a categorical statistic of interest\nwithout collecting the underlying data. We present new mechanisms, including\nhashed K-ary Randomized Response (KRR), that empirically meet or exceed the\nutility of existing mechanisms at all privacy levels. New theoretical results\ndemonstrate the order-optimality of KRR and the existing RAPPOR mechanism at\ndifferent privacy regimes.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 03:48:19 GMT"}, {"version": "v2", "created": "Wed, 2 Mar 2016 19:08:36 GMT"}, {"version": "v3", "created": "Wed, 15 Jun 2016 18:26:31 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Kairouz", "Peter", ""], ["Bonawitz", "Keith", ""], ["Ramage", "Daniel", ""]]}, {"id": "1602.07428", "submitter": "Jun Zhu", "authors": "Jun Zhu and Jiaming Song and Bei Chen", "title": "Max-Margin Nonparametric Latent Feature Models for Link Prediction", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Link prediction is a fundamental task in statistical network analysis. Recent\nadvances have been made on learning flexible nonparametric Bayesian latent\nfeature models for link prediction. In this paper, we present a max-margin\nlearning method for such nonparametric latent feature relational models. Our\napproach attempts to unite the ideas of max-margin learning and Bayesian\nnonparametrics to discover discriminative latent features for link prediction.\nIt inherits the advances of nonparametric Bayesian methods to infer the unknown\nlatent social dimension, while for discriminative link prediction, it adopts\nthe max-margin learning principle by minimizing a hinge-loss using the linear\nexpectation operator, without dealing with a highly nonlinear link likelihood\nfunction. For posterior inference, we develop an efficient stochastic\nvariational inference algorithm under a truncated mean-field assumption. Our\nmethods can scale up to large-scale real networks with millions of entities and\ntens of millions of positive links. We also provide a full Bayesian\nformulation, which can avoid tuning regularization hyper-parameters.\nExperimental results on a diverse range of real datasets demonstrate the\nbenefits inherited from max-margin learning and Bayesian nonparametric\ninference.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 08:08:05 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Zhu", "Jun", ""], ["Song", "Jiaming", ""], ["Chen", "Bei", ""]]}, {"id": "1602.07464", "submitter": "Pawe{\\l} Teisseyre", "authors": "Pawe{\\l} Teisseyre", "title": "Feature ranking for multi-label classification using Markov Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple and efficient method for ranking features in multi-label\nclassification. The method produces a ranking of features showing their\nrelevance in predicting labels, which in turn allows to choose a final subset\nof features. The procedure is based on Markov Networks and allows to model the\ndependencies between labels and features in a direct way. In the first step we\nbuild a simple network using only labels and then we test how much adding a\nsingle feature affects the initial network. More specifically, in the first\nstep we use the Ising model whereas the second step is based on the score\nstatistic, which allows to test a significance of added features very quickly.\nThe proposed approach does not require transformation of label space, gives\ninterpretable results and allows for attractive visualization of dependency\nstructure. We give a theoretical justification of the procedure by discussing\nsome theoretical properties of the Ising model and the score statistic. We also\ndiscuss feature ranking procedure based on fitting Ising model using $l_1$\nregularized logistic regressions. Numerical experiments show that the proposed\nmethods outperform the conventional approaches on the considered artificial and\nreal datasets.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 11:11:10 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Teisseyre", "Pawe\u0142", ""]]}, {"id": "1602.07466", "submitter": "Pawe{\\l} Teisseyre", "authors": "Pawe{\\l} Teisseyre", "title": "Asymptotic consistency and order specification for logistic classifier\n  chains in multi-label learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifier chains are popular and effective method to tackle a multi-label\nclassification problem. The aim of this paper is to study the asymptotic\nproperties of the chain model in which the conditional probabilities are of the\nlogistic form. In particular we find conditions on the number of labels and the\ndistribution of feature vector under which the estimated mode of the joint\ndistribution of labels converges to the true mode. Best of our knowledge, this\nimportant issue has not yet been studied in the context of multi-label\nlearning. We also investigate how the order of model building in a chain\ninfluences the estimation of the joint distribution of labels. We establish the\nlink between the problem of incorrect ordering in the chain and incorrect model\nspecification. We propose a procedure of determining the optimal ordering of\nlabels in the chain, which is based on using measures of correct specification\nand allows to find the ordering such that the consecutive logistic models are\nbest possibly specified. The other important question raised in this paper is\nhow accurately can we estimate the joint posterior probability when the\nordering of labels is wrong or the logistic models in the chain are incorrectly\nspecified. The numerical experiments illustrate the theoretical results.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 11:15:03 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Teisseyre", "Pawe\u0142", ""]]}, {"id": "1602.07576", "submitter": "Taco Cohen", "authors": "Taco S. Cohen, Max Welling", "title": "Group Equivariant Convolutional Networks", "comments": null, "journal-ref": "Proceedings of the International Conference on Machine Learning\n  (ICML), 2016", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a\nnatural generalization of convolutional neural networks that reduces sample\ncomplexity by exploiting symmetries. G-CNNs use G-convolutions, a new type of\nlayer that enjoys a substantially higher degree of weight sharing than regular\nconvolution layers. G-convolutions increase the expressive capacity of the\nnetwork without increasing the number of parameters. Group convolution layers\nare easy to use and can be implemented with negligible computational overhead\nfor discrete groups generated by translations, reflections and rotations.\nG-CNNs achieve state of the art results on CIFAR10 and rotated MNIST.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 16:17:15 GMT"}, {"version": "v2", "created": "Fri, 11 Mar 2016 18:26:26 GMT"}, {"version": "v3", "created": "Fri, 3 Jun 2016 10:54:16 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Cohen", "Taco S.", ""], ["Welling", "Max", ""]]}, {"id": "1602.07630", "submitter": "Bicheng Ying", "authors": "Bicheng Ying, Kun Yuan, Ali H. Sayed", "title": "Online Dual Coordinate Ascent Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stochastic dual coordinate-ascent (S-DCA) technique is a useful\nalternative to the traditional stochastic gradient-descent algorithm for\nsolving large-scale optimization problems due to its scalability to large data\nsets and strong theoretical guarantees. However, the available S-DCA\nformulation is limited to finite sample sizes and relies on performing multiple\npasses over the same data. This formulation is not well-suited for online\nimplementations where data keep streaming in. In this work, we develop an {\\em\nonline} dual coordinate-ascent (O-DCA) algorithm that is able to respond to\nstreaming data and does not need to revisit the past data. This feature embeds\nthe resulting construction with continuous adaptation, learning, and tracking\nabilities, which are particularly attractive for online learning scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 18:26:35 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Ying", "Bicheng", ""], ["Yuan", "Kun", ""], ["Sayed", "Ali H.", ""]]}, {"id": "1602.07714", "submitter": "Hado van Hasselt", "authors": "Hado van Hasselt and Arthur Guez and Matteo Hessel and Volodymyr Mnih\n  and David Silver", "title": "Learning values across many orders of magnitude", "comments": "Paper accepted for publication at NIPS 2016. This version includes\n  the appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most learning algorithms are not invariant to the scale of the function that\nis being approximated. We propose to adaptively normalize the targets used in\nlearning. This is useful in value-based reinforcement learning, where the\nmagnitude of appropriate value approximations can change over time when we\nupdate the policy of behavior. Our main motivation is prior work on learning to\nplay Atari games, where the rewards were all clipped to a predetermined range.\nThis clipping facilitates learning across many different games with a single\nlearning algorithm, but a clipped reward function can result in qualitatively\ndifferent behavior. Using the adaptive normalization we can remove this\ndomain-specific heuristic without diminishing overall performance.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 21:14:52 GMT"}, {"version": "v2", "created": "Tue, 16 Aug 2016 05:27:17 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["van Hasselt", "Hado", ""], ["Guez", "Arthur", ""], ["Hessel", "Matteo", ""], ["Mnih", "Volodymyr", ""], ["Silver", "David", ""]]}, {"id": "1602.07754", "submitter": "Kevin Xu", "authors": "Swayambhoo Jain, Urvashi Oswal, Kevin S. Xu, Brian Eriksson, Jarvis\n  Haupt", "title": "A Compressed Sensing Based Decomposition of Electrodermal Activity\n  Signals", "comments": "To appear in IEEE Transactions on Biomedical Engineering", "journal-ref": null, "doi": "10.1109/TBME.2016.2632523", "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The measurement and analysis of Electrodermal Activity (EDA) offers\napplications in diverse areas ranging from market research, to seizure\ndetection, to human stress analysis. Unfortunately, the analysis of EDA signals\nis made difficult by the superposition of numerous components which can obscure\nthe signal information related to a user's response to a stimulus. We show how\nsimple pre-processing followed by a novel compressed sensing based\ndecomposition can mitigate the effects of the undesired noise components and\nhelp reveal the underlying physiological signal. The proposed framework allows\nfor decomposition of EDA signals with provable bounds on the recovery of user\nresponses. We test our procedure on both synthetic and real-world EDA signals\nfrom wearable sensors and demonstrate that our approach allows for more\naccurate recovery of user responses as compared to the existing techniques.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 23:52:07 GMT"}, {"version": "v2", "created": "Thu, 26 Jan 2017 21:03:57 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Jain", "Swayambhoo", ""], ["Oswal", "Urvashi", ""], ["Xu", "Kevin S.", ""], ["Eriksson", "Brian", ""], ["Haupt", "Jarvis", ""]]}, {"id": "1602.07764", "submitter": "Kamyar Azizzadenesheli Ph.D.", "authors": "Kamyar Azizzadenesheli, Alessandro Lazaric, Animashree Anandkumar", "title": "Reinforcement Learning of POMDPs using Spectral Methods", "comments": null, "journal-ref": "29th Annual Conference on Learning Theory, PMLR 49:193-256, 2016", "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new reinforcement learning algorithm for partially observable\nMarkov decision processes (POMDP) based on spectral decomposition methods.\nWhile spectral methods have been previously employed for consistent learning of\n(passive) latent variable models such as hidden Markov models, POMDPs are more\nchallenging since the learner interacts with the environment and possibly\nchanges the future observations in the process. We devise a learning algorithm\nrunning through episodes, in each episode we employ spectral techniques to\nlearn the POMDP parameters from a trajectory generated by a fixed policy. At\nthe end of the episode, an optimization oracle returns the optimal memoryless\nplanning policy which maximizes the expected reward based on the estimated\nPOMDP model. We prove an order-optimal regret bound with respect to the optimal\nmemoryless policy and efficient scaling with respect to the dimensionality of\nobservation and action spaces.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 01:25:36 GMT"}, {"version": "v2", "created": "Sun, 29 May 2016 07:15:21 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Azizzadenesheli", "Kamyar", ""], ["Lazaric", "Alessandro", ""], ["Anandkumar", "Animashree", ""]]}, {"id": "1602.07783", "submitter": "Zhao Kang", "authors": "Zhao Kang and Qiang Cheng", "title": "Top-N Recommendation with Novel Rank Approximation", "comments": "SDM 2016. arXiv admin note: text overlap with arXiv:1601.04800", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The importance of accurate recommender systems has been widely recognized by\nacademia and industry. However, the recommendation quality is still rather low.\nRecently, a linear sparse and low-rank representation of the user-item matrix\nhas been applied to produce Top-N recommendations. This approach uses the\nnuclear norm as a convex relaxation for the rank function and has achieved\nbetter recommendation accuracy than the state-of-the-art methods. In the past\nseveral years, solving rank minimization problems by leveraging nonconvex\nrelaxations has received increasing attention. Some empirical results\ndemonstrate that it can provide a better approximation to original problems\nthan convex relaxation. In this paper, we propose a novel rank approximation to\nenhance the performance of Top-N recommendation systems, where the\napproximation error is controllable. Experimental results on real data show\nthat the proposed rank approximation improves the Top-$N$ recommendation\naccuracy substantially.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 03:33:44 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2016 15:58:56 GMT"}], "update_date": "2016-02-29", "authors_parsed": [["Kang", "Zhao", ""], ["Cheng", "Qiang", ""]]}, {"id": "1602.07795", "submitter": "Alyson K. Fletcher", "authors": "Alyson K. Fletcher, Mojtaba Sahraee-Ardakan, Sundeep Rangan and Philip\n  Schniter", "title": "Expectation Consistent Approximate Inference: Generalizations and\n  Convergence", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximations of loopy belief propagation, including expectation propagation\nand approximate message passing, have attracted considerable attention for\nprobabilistic inference problems. This paper proposes and analyzes a\ngeneralization of Opper and Winther's expectation consistent (EC) approximate\ninference method. The proposed method, called Generalized Expectation\nConsistency (GEC), can be applied to both maximum a posteriori (MAP) and\nminimum mean squared error (MMSE) estimation. Here we characterize its fixed\npoints, convergence, and performance relative to the replica prediction of\noptimality.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 05:06:47 GMT"}, {"version": "v2", "created": "Tue, 24 Jan 2017 22:40:32 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Fletcher", "Alyson K.", ""], ["Sahraee-Ardakan", "Mojtaba", ""], ["Rangan", "Sundeep", ""], ["Schniter", "Philip", ""]]}, {"id": "1602.07800", "submitter": "Yizhe Zhang", "authors": "Yizhe Zhang, Xiangyu Wang, Changyou Chen, Ricardo Henao, Kai Fan and\n  Lawrence Carin", "title": "Towards Unifying Hamiltonian Monte Carlo and Slice Sampling", "comments": "updated version", "journal-ref": "Advances in Neural Information Processing Systems, pages\n  1741--1749, year 2016", "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We unify slice sampling and Hamiltonian Monte Carlo (HMC) sampling,\ndemonstrating their connection via the Hamiltonian-Jacobi equation from\nHamiltonian mechanics. This insight enables extension of HMC and slice sampling\nto a broader family of samplers, called Monomial Gamma Samplers (MGS). We\nprovide a theoretical analysis of the mixing performance of such samplers,\nproving that in the limit of a single parameter, the MGS draws decorrelated\nsamples from the desired target distribution. We further show that as this\nparameter tends toward this limit, performance gains are achieved at a cost of\nincreasing numerical difficulty and some practical convergence issues. Our\ntheoretical results are validated with synthetic data and real-world\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 05:14:45 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2016 01:27:26 GMT"}, {"version": "v3", "created": "Mon, 29 Feb 2016 02:18:51 GMT"}, {"version": "v4", "created": "Mon, 17 Oct 2016 03:13:47 GMT"}, {"version": "v5", "created": "Wed, 10 Jan 2018 19:26:02 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Zhang", "Yizhe", ""], ["Wang", "Xiangyu", ""], ["Chen", "Changyou", ""], ["Henao", "Ricardo", ""], ["Fan", "Kai", ""], ["Carin", "Lawrence", ""]]}, {"id": "1602.07807", "submitter": "Michael Bloodgood", "authors": "Michael Bloodgood and Benjamin Strauss", "title": "Data Cleaning for XML Electronic Dictionaries via Statistical Anomaly\n  Detection", "comments": "8 pages, 4 figures, 5 tables; published in Proceedings of the 2016\n  IEEE Tenth International Conference on Semantic Computing (ICSC), Laguna\n  Hills, CA, USA, pages 79-86, February 2016", "journal-ref": "In Proceedings of the 2016 IEEE Tenth International Conference on\n  Semantic Computing (ICSC), pages 79-86, Laguna Hills, CA, USA, February 2016.\n  IEEE", "doi": "10.1109/ICSC.2016.38", "report-no": null, "categories": "cs.DB cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many important forms of data are stored digitally in XML format. Errors can\noccur in the textual content of the data in the fields of the XML. Fixing these\nerrors manually is time-consuming and expensive, especially for large amounts\nof data. There is increasing interest in the research, development, and use of\nautomated techniques for assisting with data cleaning. Electronic dictionaries\nare an important form of data frequently stored in XML format that frequently\nhave errors introduced through a mixture of manual typographical entry errors\nand optical character recognition errors. In this paper we describe methods for\nflagging statistical anomalies as likely errors in electronic dictionaries\nstored in XML format. We describe six systems based on different sources of\ninformation. The systems detect errors using various signals in the data\nincluding uncommon characters, text length, character-based language models,\nword-based language models, tied-field length ratios, and tied-field\ntransliteration models. Four of the systems detect errors based on expectations\nautomatically inferred from content within elements of a single field type. We\ncall these single-field systems. Two of the systems detect errors based on\ncorrespondence expectations automatically inferred from content within elements\nof multiple related field types. We call these tied-field systems. For each\nsystem, we provide an intuitive analysis of the type of error that it is\nsuccessful at detecting. Finally, we describe two larger-scale evaluations\nusing crowdsourcing with Amazon's Mechanical Turk platform and using the\nannotations of a domain expert. The evaluations consistently show that the\nsystems are useful for improving the efficiency with which errors in XML\nelectronic dictionaries can be detected.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 05:49:36 GMT"}, {"version": "v2", "created": "Mon, 11 Apr 2016 04:01:43 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Bloodgood", "Michael", ""], ["Strauss", "Benjamin", ""]]}, {"id": "1602.07836", "submitter": "Vesa Palonen Dr", "authors": "V. Palonen", "title": "A Bayesian baseline for belief in uncommon events", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The plausibility of uncommon events and miracles based on testimony of such\nan event has been much discussed. When analyzing the probabilities involved, it\nhas mostly been assumed that the common events can be taken as data in the\ncalculations. However, we usually have only testimonies for the common events.\nWhile this difference does not have a significant effect on the inductive part\nof the inference, it has a large influence on how one should view the\nreliability of testimonies. In this work, a full Bayesian solution is given for\nthe more realistic case, where one has a large number of testimonies for a\ncommon event and one testimony for an uncommon event. It is seen that, in order\nfor there to be a large amount of testimonies for a common event, the\ntestimonies will probably be quite reliable. For this reason, because the\ntestimonies are quite reliable based on the testimonies for the common events,\nthe probability for the uncommon event, given a testimony for it, is also\nhigher. Hence, one should be more open-minded when considering the plausibility\nof uncommon events.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 07:57:07 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2016 17:27:49 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Palonen", "V.", ""]]}, {"id": "1602.07844", "submitter": "Shuai Zheng", "authors": "Shuai Zheng and Ruiliang Zhang and James T. Kwok", "title": "Fast Nonsmooth Regularized Risk Minimization with Continuation", "comments": "AAAI-2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In regularized risk minimization, the associated optimization problem becomes\nparticularly difficult when both the loss and regularizer are nonsmooth.\nExisting approaches either have slow or unclear convergence properties, are\nrestricted to limited problem subclasses, or require careful setting of a\nsmoothing parameter. In this paper, we propose a continuation algorithm that is\napplicable to a large class of nonsmooth regularized risk minimization\nproblems, can be flexibly used with a number of existing solvers for the\nunderlying smoothed subproblem, and with convergence results on the whole\nalgorithm rather than just one of its subproblems. In particular, when\naccelerated solvers are used, the proposed algorithm achieves the fastest known\nrates of $O(1/T^2)$ on strongly convex problems, and $O(1/T)$ on general convex\nproblems. Experiments on nonsmooth classification and regression tasks\ndemonstrate that the proposed algorithm outperforms the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 08:34:59 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Zheng", "Shuai", ""], ["Zhang", "Ruiliang", ""], ["Kwok", "James T.", ""]]}, {"id": "1602.07860", "submitter": "Yash Satsangi", "authors": "Yash Satsangi, Shimon Whiteson, Frans A. Oliehoek", "title": "Probably Approximately Correct Greedy Maximization with Efficient Bounds\n  on Information Gain for Sensor Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Submodular function maximization finds application in a variety of real-world\ndecision-making problems. However, most existing methods, based on greedy\nmaximization, assume it is computationally feasible to evaluate F, the function\nbeing maximized. Unfortunately, in many realistic settings F is too expensive\nto evaluate exactly even once. We present probably approximately correct greedy\nmaximization, which requires access only to cheap anytime confidence bounds on\nF and uses them to prune elements. We show that, with high probability, our\nmethod returns an approximately optimal set. We propose novel, cheap confidence\nbounds for conditional entropy, which appears in many common choices of F and\nfor which it is difficult to find unbiased or bounded estimates. Finally,\nresults on a real-world dataset from a multi-camera tracking system in a\nshopping mall demonstrate that our approach performs comparably to existing\nmethods, but at a fraction of the computational cost.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 09:34:38 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 12:02:33 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Satsangi", "Yash", ""], ["Whiteson", "Shimon", ""], ["Oliehoek", "Frans A.", ""]]}, {"id": "1602.07863", "submitter": "Janne Lepp\\\"a-Aho", "authors": "Janne Lepp\\\"a-aho, Johan Pensar, Teemu Roos, Jukka Corander", "title": "Learning Gaussian Graphical Models With Fractional Marginal\n  Pseudo-likelihood", "comments": null, "journal-ref": null, "doi": "10.1016/j.ijar.2017.01.001", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian approximate inference method for learning the\ndependence structure of a Gaussian graphical model. Using pseudo-likelihood, we\nderive an analytical expression to approximate the marginal likelihood for an\narbitrary graph structure without invoking any assumptions about\ndecomposability. The majority of the existing methods for learning Gaussian\ngraphical models are either restricted to decomposable graphs or require\nspecification of a tuning parameter that may have a substantial impact on\nlearned structures. By combining a simple sparsity inducing prior for the graph\nstructures with a default reference prior for the model parameters, we obtain a\nfast and easily applicable scoring function that works well for even\nhigh-dimensional data. We demonstrate the favourable performance of our\napproach by large-scale comparisons against the leading methods for learning\nnon-decomposable Gaussian graphical models. A theoretical justification for our\nmethod is provided by showing that it yields a consistent estimator of the\ngraph structure.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 09:42:46 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Lepp\u00e4-aho", "Janne", ""], ["Pensar", "Johan", ""], ["Roos", "Teemu", ""], ["Corander", "Jukka", ""]]}, {"id": "1602.07865", "submitter": "Jesse Krijthe", "authors": "Jesse H. Krijthe and Marco Loog", "title": "Projected Estimators for Robust Semi-supervised Classification", "comments": "13 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For semi-supervised techniques to be applied safely in practice we at least\nwant methods to outperform their supervised counterparts. We study this\nquestion for classification using the well-known quadratic surrogate loss\nfunction. Using a projection of the supervised estimate onto a set of\nconstraints imposed by the unlabeled data, we find we can safely improve over\nthe supervised solution in terms of this quadratic loss. Unlike other\napproaches to semi-supervised learning, the procedure does not rely on\nassumptions that are not intrinsic to the classifier at hand. It is\ntheoretically demonstrated that, measured on the labeled and unlabeled training\ndata, this semi-supervised procedure never gives a lower quadratic loss than\nthe supervised alternative. To our knowledge this is the first approach that\noffers such strong, albeit conservative, guarantees for improvement over the\nsupervised solution. The characteristics of our approach are explicated using\nbenchmark datasets to further understand the similarities and differences\nbetween the quadratic loss criterion used in the theoretical results and the\nclassification accuracy often considered in practice.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 09:57:42 GMT"}], "update_date": "2016-02-26", "authors_parsed": [["Krijthe", "Jesse H.", ""], ["Loog", "Marco", ""]]}, {"id": "1602.07905", "submitter": "Jan Leike", "authors": "Jan Leike and Tor Lattimore and Laurent Orseau and Marcus Hutter", "title": "Thompson Sampling is Asymptotically Optimal in General Environments", "comments": "UAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss a variant of Thompson sampling for nonparametric reinforcement\nlearning in a countable classes of general stochastic environments. These\nenvironments can be non-Markov, non-ergodic, and partially observable. We show\nthat Thompson sampling learns the environment class in the sense that (1)\nasymptotically its value converges to the optimal value in mean and (2) given a\nrecoverability assumption regret is sublinear.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 12:37:21 GMT"}, {"version": "v2", "created": "Fri, 3 Jun 2016 10:59:36 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Leike", "Jan", ""], ["Lattimore", "Tor", ""], ["Orseau", "Laurent", ""], ["Hutter", "Marcus", ""]]}, {"id": "1602.07960", "submitter": "Ming Li", "authors": "Lijue Liu, Ming Li, Sha Wen", "title": "Measuring and Discovering Correlations in Large Data Sets", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a class of statistics named ART (the alternant recursive\ntopology statistics) is proposed to measure the properties of correlation\nbetween two variables. A wide range of bi-variable correlations both linear and\nnonlinear can be evaluated by ART efficiently and equitably even if nothing is\nknown about the specific types of those relationships. ART compensates the\ndisadvantages of Reshef's model in which no polynomial time precise algorithm\nexists and the \"local random\" phenomenon can not be identified. As a class of\nnonparametric exploration statistics, ART is applied for analyzing a dataset of\n10 American classical indexes, as a result, lots of bi-variable correlations\nare discovered.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2016 09:58:16 GMT"}], "update_date": "2016-02-26", "authors_parsed": [["Liu", "Lijue", ""], ["Li", "Ming", ""], ["Wen", "Sha", ""]]}, {"id": "1602.08007", "submitter": "Yann Ollivier", "authors": "Ga\\'etan Marceau-Caron, Yann Ollivier", "title": "Practical Riemannian Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide the first experimental results on non-synthetic datasets for the\nquasi-diagonal Riemannian gradient descents for neural networks introduced in\n[Ollivier, 2015]. These include the MNIST, SVHN, and FACE datasets as well as a\npreviously unpublished electroencephalogram dataset. The quasi-diagonal\nRiemannian algorithms consistently beat simple stochastic gradient gradient\ndescents by a varying margin. The computational overhead with respect to simple\nbackpropagation is around a factor $2$. Perhaps more interestingly, these\nmethods also reach their final performance quickly, thus requiring fewer\ntraining epochs and a smaller total computation time.\n  We also present an implementation guide to these Riemannian gradient descents\nfor neural networks, showing how the quasi-diagonal versions can be implemented\nwith minimal effort on top of existing routines which compute gradients.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 17:37:28 GMT"}], "update_date": "2016-02-26", "authors_parsed": [["Marceau-Caron", "Ga\u00e9tan", ""], ["Ollivier", "Yann", ""]]}, {"id": "1602.08017", "submitter": "Alexey Melnikov", "authors": "Adi Makmal, Alexey A. Melnikov, Vedran Dunjko, Hans J. Briegel", "title": "Meta-learning within Projective Simulation", "comments": "14 pages, 12 figures", "journal-ref": "IEEE Access 4, 2110-2122 (2016)", "doi": "10.1109/access.2016.2556579", "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning models of artificial intelligence can nowadays perform very well on\na large variety of tasks. However, in practice different task environments are\nbest handled by different learning models, rather than a single, universal,\napproach. Most non-trivial models thus require the adjustment of several to\nmany learning parameters, which is often done on a case-by-case basis by an\nexternal party. Meta-learning refers to the ability of an agent to autonomously\nand dynamically adjust its own learning parameters, or meta-parameters. In this\nwork we show how projective simulation, a recently developed model of\nartificial intelligence, can naturally be extended to account for meta-learning\nin reinforcement learning settings. The projective simulation approach is based\non a random walk process over a network of clips. The suggested meta-learning\nscheme builds upon the same design and employs clip networks to monitor the\nagent's performance and to adjust its meta-parameters \"on the fly\". We\ndistinguish between \"reflexive adaptation\" and \"adaptation through learning\",\nand show the utility of both approaches. In addition, a trade-off between\nflexibility and learning-time is addressed. The extended model is examined on\nthree different kinds of reinforcement learning tasks, in which the agent has\ndifferent optimal values of the meta-parameters, and is shown to perform well,\nreaching near-optimal to optimal success rates in all of them, without ever\nneeding to manually adjust any meta-parameter.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 18:07:53 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Makmal", "Adi", ""], ["Melnikov", "Alexey A.", ""], ["Dunjko", "Vedran", ""], ["Briegel", "Hans J.", ""]]}, {"id": "1602.08151", "submitter": "Akshay Balsubramani", "authors": "Akshay Balsubramani", "title": "Learning to Abstain from Binary Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A binary classifier capable of abstaining from making a label prediction has\ntwo goals in tension: minimizing errors, and avoiding abstaining unnecessarily\noften. In this work, we exactly characterize the best achievable tradeoff\nbetween these two goals in a general semi-supervised setting, given an ensemble\nof predictors of varying competence as well as unlabeled data on which we wish\nto predict or abstain. We give an algorithm for learning a classifier in this\nsetting which trades off its errors with abstentions in a minimax optimal\nmanner, is as efficient as linear learning and prediction, and is demonstrably\npractical. Our analysis extends to a large class of loss functions and other\nscenarios, including ensembles comprised of specialists that can themselves\nabstain.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 23:46:57 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 17:05:38 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Balsubramani", "Akshay", ""]]}, {"id": "1602.08194", "submitter": "Ryan Spring", "authors": "Ryan Spring, Anshumali Shrivastava", "title": "Scalable and Sustainable Deep Learning via Randomized Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current deep learning architectures are growing larger in order to learn from\ncomplex datasets. These architectures require giant matrix multiplication\noperations to train millions of parameters. Conversely, there is another\ngrowing trend to bring deep learning to low-power, embedded devices. The matrix\noperations, associated with both training and testing of deep networks, are\nvery expensive from a computational and energy standpoint. We present a novel\nhashing based technique to drastically reduce the amount of computation needed\nto train and test deep networks. Our approach combines recent ideas from\nadaptive dropouts and randomized hashing for maximum inner product search to\nselect the nodes with the highest activation efficiently. Our new algorithm for\ndeep learning reduces the overall computational cost of forward and\nback-propagation by operating on significantly fewer (sparse) nodes. As a\nconsequence, our algorithm uses only 5% of the total multiplications, while\nkeeping on average within 1% of the accuracy of the original model. A unique\nproperty of the proposed hashing based back-propagation is that the updates are\nalways sparse. Due to the sparse gradient updates, our algorithm is ideally\nsuited for asynchronous and parallel training leading to near linear speedup\nwith increasing number of cores. We demonstrate the scalability and\nsustainability (energy efficiency) of our proposed algorithm via rigorous\nexperimental evaluations on several real datasets.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 05:07:23 GMT"}, {"version": "v2", "created": "Mon, 5 Dec 2016 04:52:36 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Spring", "Ryan", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "1602.08207", "submitter": "Philip Schniter", "authors": "Alyson K. Fletcher and Philip Schniter", "title": "Learning and Free Energies for Vector Approximate Message Passing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vector approximate message passing (VAMP) is a computationally simple\napproach to the recovery of a signal $\\mathbf{x}$ from noisy linear\nmeasurements $\\mathbf{y}=\\mathbf{Ax}+\\mathbf{w}$. Like the AMP proposed by\nDonoho, Maleki, and Montanari in 2009, VAMP is characterized by a rigorous\nstate evolution (SE) that holds under certain large random matrices and that\nmatches the replica prediction of optimality. But while AMP's SE holds only for\nlarge i.i.d. sub-Gaussian $\\mathbf{A}$, VAMP's SE holds under the much larger\nclass: right-rotationally invariant $\\mathbf{A}$. To run VAMP, however, one\nmust specify the statistical parameters of the signal and noise. This work\ncombines VAMP with Expectation-Maximization to yield an algorithm, EM-VAMP,\nthat can jointly recover $\\mathbf{x}$ while learning those statistical\nparameters. The fixed points of the proposed EM-VAMP algorithm are shown to be\nstationary points of a certain constrained free-energy, providing a variational\ninterpretation of the algorithm. Numerical simulations show that EM-VAMP is\nrobust to highly ill-conditioned $\\mathbf{A}$ with performance nearly matching\noracle-parameter VAMP.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 06:06:13 GMT"}, {"version": "v2", "created": "Wed, 12 Oct 2016 06:37:00 GMT"}, {"version": "v3", "created": "Mon, 9 Jan 2017 17:54:12 GMT"}, {"version": "v4", "created": "Thu, 8 Mar 2018 15:55:46 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Fletcher", "Alyson K.", ""], ["Schniter", "Philip", ""]]}, {"id": "1602.08418", "submitter": "Kevin Scaman", "authors": "R\\'emi Lemonnier, Kevin Scaman and Argyris Kalogeratos", "title": "Multivariate Hawkes Processes for Large-scale Inference", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a framework for fitting multivariate Hawkes\nprocesses for large-scale problems both in the number of events in the observed\nhistory $n$ and the number of event types $d$ (i.e. dimensions). The proposed\nLow-Rank Hawkes Process (LRHP) framework introduces a low-rank approximation of\nthe kernel matrix that allows to perform the nonparametric learning of the\n$d^2$ triggering kernels using at most $O(ndr^2)$ operations, where $r$ is the\nrank of the approximation ($r \\ll d,n$). This comes as a major improvement to\nthe existing state-of-the-art inference algorithms that are in $O(nd^2)$.\nFurthermore, the low-rank approximation allows LRHP to learn representative\npatterns of interaction between event types, which may be valuable for the\nanalysis of such complex processes in real world datasets. The efficiency and\nscalability of our approach is illustrated with numerical experiments on\nsimulated as well as real datasets.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 17:56:13 GMT"}], "update_date": "2016-02-29", "authors_parsed": [["Lemonnier", "R\u00e9mi", ""], ["Scaman", "Kevin", ""], ["Kalogeratos", "Argyris", ""]]}, {"id": "1602.08425", "submitter": "Florian Bernard", "authors": "Florian Bernard, Luis Salamanca, Johan Thunberg, Alexander Tack,\n  Dennis Jentsch, Hans Lamecker, Stefan Zachow, Frank Hertel, Jorge Goncalves,\n  Peter Gemmar", "title": "Shape-aware Surface Reconstruction from Sparse 3D Point-Clouds", "comments": null, "journal-ref": null, "doi": "10.1016/j.media.2017.02.005", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reconstruction of an object's shape or surface from a set of 3D points\nplays an important role in medical image analysis, e.g. in anatomy\nreconstruction from tomographic measurements or in the process of aligning\nintra-operative navigation and preoperative planning data. In such scenarios,\none usually has to deal with sparse data, which significantly aggravates the\nproblem of reconstruction. However, medical applications often provide\ncontextual information about the 3D point data that allow to incorporate prior\nknowledge about the shape that is to be reconstructed. To this end, we propose\nthe use of a statistical shape model (SSM) as a prior for surface\nreconstruction. The SSM is represented by a point distribution model (PDM),\nwhich is associated with a surface mesh. Using the shape distribution that is\nmodelled by the PDM, we formulate the problem of surface reconstruction from a\nprobabilistic perspective based on a Gaussian Mixture Model (GMM). In order to\ndo so, the given points are interpreted as samples of the GMM. By using mixture\ncomponents with anisotropic covariances that are \"oriented\" according to the\nsurface normals at the PDM points, a surface-based fitting is accomplished.\nEstimating the parameters of the GMM in a maximum a posteriori manner yields\nthe reconstruction of the surface from the given data points. We compare our\nmethod to the extensively used Iterative Closest Points method on several\ndifferent anatomical datasets/SSMs (brain, femur, tibia, hip, liver) and\ndemonstrate superior accuracy and robustness on sparse data.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 18:30:07 GMT"}, {"version": "v2", "created": "Wed, 15 Feb 2017 11:45:36 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Bernard", "Florian", ""], ["Salamanca", "Luis", ""], ["Thunberg", "Johan", ""], ["Tack", "Alexander", ""], ["Jentsch", "Dennis", ""], ["Lamecker", "Hans", ""], ["Zachow", "Stefan", ""], ["Hertel", "Frank", ""], ["Goncalves", "Jorge", ""], ["Gemmar", "Peter", ""]]}, {"id": "1602.08712", "submitter": "Vladimir Temlyakov", "authors": "V. Temlyakov", "title": "On the entropy numbers of the mixed smoothness function classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA math.FA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Behavior of the entropy numbers of classes of multivariate functions with\nmixed smoothness is studied here. This problem has a long history and some\nfundamental problems in the area are still open. The main goal of this paper is\nto develop a new method of proving the upper bounds for the entropy numbers.\nThis method is based on recent developments of nonlinear approximation, in\nparticular, on greedy approximation. This method consists of the following two\nsteps strategy. At the first step we obtain bounds of the best m-term\napproximations with respect to a dictionary. At the second step we use general\ninequalities relating the entropy numbers to the best m-term approximations.\nFor the lower bounds we use the volume estimates method, which is a well known\npowerful method for proving the lower bounds for the entropy numbers. It was\nused in a number of previous papers.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2016 13:16:10 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Temlyakov", "V.", ""]]}, {"id": "1602.08734", "submitter": "Tim Salimans", "authors": "Tim Salimans", "title": "A Structured Variational Auto-encoder for Learning Deep Hierarchies of\n  Sparse Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we present a generative model of natural images consisting of a\ndeep hierarchy of layers of latent random variables, each of which follows a\nnew type of distribution that we call rectified Gaussian. These rectified\nGaussian units allow spike-and-slab type sparsity, while retaining the\ndifferentiability necessary for efficient stochastic gradient variational\ninference. To learn the parameters of the new model, we approximate the\nposterior of the latent variables with a variational auto-encoder. Rather than\nmaking the usual mean-field assumption however, the encoder parameterizes a new\ntype of structured variational approximation that retains the prior\ndependencies of the generative model. Using this structured posterior\napproximation, we are able to perform joint training of deep models with many\nlayers of latent random variables, without having to resort to stacking or\nother layerwise training procedures.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2016 16:10:40 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Salimans", "Tim", ""]]}, {"id": "1602.08753", "submitter": "Jonathan Warrell", "authors": "Jonathan H. Warrell, Musa M. Mhlanga", "title": "Stability and Structural Properties of Gene Regulation Networks with\n  Coregulation Rules", "comments": "21 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.MN q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coregulation of the expression of groups of genes has been extensively\ndemonstrated empirically in bacterial and eukaryotic systems. Such coregulation\ncan arise through the use of shared regulatory motifs, which allow the\ncoordinated expression of modules (and module groups) of functionally related\ngenes across the genome. Coregulation can also arise through the physical\nassociation of multi-gene complexes through chromosomal looping, which are then\ntranscribed together. We present a general formalism for modeling coregulation\nrules in the framework of Random Boolean Networks (RBN), and develop specific\nmodels for transcription factor networks with modular structure (including\nmodule groups, and multi-input modules (MIM) with autoregulation) and\nmulti-gene complexes (including hierarchical differentiation between multi-gene\ncomplex members). We develop a mean-field approach to analyse the stability of\nlarge networks incorporating coregulation, and show that autoregulated MIM and\nhierarchical gene-complex models can achieve greater stability than networks\nwithout coregulation whose rules have matching activation frequency. We provide\nfurther analysis of the stability of small networks of both kinds through\nsimulations. We also characterize several general properties of the transients\nand attractors in the hierarchical coregulation model, and show using\nsimulations that the steady-state distribution factorizes hierarchically as a\nBayesian network in a Markov Jump Process analogue of the RBN model.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2016 18:40:04 GMT"}, {"version": "v2", "created": "Sun, 10 Apr 2016 12:44:59 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Warrell", "Jonathan H.", ""], ["Mhlanga", "Musa M.", ""]]}, {"id": "1602.08761", "submitter": "Tolga Bolukbasi", "authors": "Tolga Bolukbasi, Kai-Wei Chang, Joseph Wang, Venkatesh Saligrama", "title": "Resource Constrained Structured Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of structured prediction under test-time budget\nconstraints. We propose a novel approach applicable to a wide range of\nstructured prediction problems in computer vision and natural language\nprocessing. Our approach seeks to adaptively generate computationally costly\nfeatures during test-time in order to reduce the computational cost of\nprediction while maintaining prediction performance. We show that training the\nadaptive feature generation system can be reduced to a series of structured\nlearning problems, resulting in efficient training using existing structured\nlearning algorithms. This framework provides theoretical justification for\nseveral existing heuristic approaches found in literature. We evaluate our\nproposed adaptive system on two structured prediction tasks, optical character\nrecognition (OCR) and dependency parsing and show strong performance in\nreduction of the feature costs without degrading accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2016 19:44:57 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 01:31:01 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Bolukbasi", "Tolga", ""], ["Chang", "Kai-Wei", ""], ["Wang", "Joseph", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1602.08771", "submitter": "Adam White", "authors": "Adam White, Martha White", "title": "Investigating practical linear temporal difference learning", "comments": "Autonomous Agents and Multi-agent Systems, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Off-policy reinforcement learning has many applications including: learning\nfrom demonstration, learning multiple goal seeking policies in parallel, and\nrepresenting predictive knowledge. Recently there has been an proliferation of\nnew policy-evaluation algorithms that fill a longstanding algorithmic void in\nreinforcement learning: combining robustness to off-policy sampling, function\napproximation, linear complexity, and temporal difference (TD) updates. This\npaper contains two main contributions. First, we derive two new hybrid TD\npolicy-evaluation algorithms, which fill a gap in this collection of\nalgorithms. Second, we perform an empirical comparison to elicit which of these\nnew linear TD methods should be preferred in different situations, and make\nconcrete suggestions about practical use.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2016 21:23:54 GMT"}, {"version": "v2", "created": "Thu, 31 Mar 2016 01:10:08 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["White", "Adam", ""], ["White", "Martha", ""]]}, {"id": "1602.08780", "submitter": "Dirk Tasche", "authors": "Dirk Tasche", "title": "Does quantification without adjustments work?", "comments": "20 pages, 2 figures, major update", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification is the task of predicting the class labels of objects based on\nthe observation of their features. In contrast, quantification has been defined\nas the task of determining the prevalences of the different sorts of class\nlabels in a target dataset. The simplest approach to quantification is Classify\n& Count where a classifier is optimised for classification on a training set\nand applied to the target dataset for the prediction of class labels. In the\ncase of binary quantification, the number of predicted positive labels is then\nused as an estimate of the prevalence of the positive class in the target\ndataset. Since the performance of Classify & Count for quantification is known\nto be inferior its results typically are subject to adjustments. However, some\nresearchers recently have suggested that Classify & Count might actually work\nwithout adjustments if it is based on a classifer that was specifically trained\nfor quantification. We discuss the theoretical foundation for this claim and\nexplore its potential and limitations with a numerical example based on the\nbinormal model with equal variances. In order to identify an optimal quantifier\nin the binormal setting, we introduce the concept of local Bayes optimality. As\na side remark, we present a complete proof of a theorem by Ye et al. (2012).\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2016 22:29:25 GMT"}, {"version": "v2", "created": "Fri, 12 Aug 2016 16:24:05 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Tasche", "Dirk", ""]]}, {"id": "1602.08886", "submitter": "Ravi Kolla", "authors": "Ravi Kumar Kolla, Krishna Jagannathan, Aditya Gopalan", "title": "Collaborative Learning of Stochastic Bandits over a Social Network", "comments": "14 Pages, 6 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a collaborative online learning paradigm, wherein a group of\nagents connected through a social network are engaged in playing a stochastic\nmulti-armed bandit game. Each time an agent takes an action, the corresponding\nreward is instantaneously observed by the agent, as well as its neighbours in\nthe social network. We perform a regret analysis of various policies in this\ncollaborative learning setting. A key finding of this paper is that natural\nextensions of widely-studied single agent learning policies to the network\nsetting need not perform well in terms of regret. In particular, we identify a\nclass of non-altruistic and individually consistent policies, and argue by\nderiving regret lower bounds that they are liable to suffer a large regret in\nthe networked setting. We also show that the learning performance can be\nsubstantially improved if the agents exploit the structure of the network, and\ndevelop a simple learning algorithm based on dominating sets of the network.\nSpecifically, we first consider a star network, which is a common motif in\nhierarchical social networks, and show analytically that the hub agent can be\nused as an information sink to expedite learning and improve the overall\nregret. We also derive networkwide regret bounds for the algorithm applied to\ngeneral networks. We conduct numerical experiments on a variety of networks to\ncorroborate our analytical results.\n", "versions": [{"version": "v1", "created": "Mon, 29 Feb 2016 09:53:28 GMT"}, {"version": "v2", "created": "Mon, 11 Jul 2016 06:31:57 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Kolla", "Ravi Kumar", ""], ["Jagannathan", "Krishna", ""], ["Gopalan", "Aditya", ""]]}, {"id": "1602.08927", "submitter": "Martin Spindler", "authors": "Ye Luo and Martin Spindler", "title": "High-Dimensional $L_2$Boosting: Rate of Convergence", "comments": "19 pages, 4 tables; AMS 2000 subject classifications: Primary 62J05,\n  62J07, 41A25; secondary 49M15, 68Q32", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG econ.EM math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boosting is one of the most significant developments in machine learning.\nThis paper studies the rate of convergence of $L_2$Boosting, which is tailored\nfor regression, in a high-dimensional setting. Moreover, we introduce so-called\n\\textquotedblleft post-Boosting\\textquotedblright. This is a post-selection\nestimator which applies ordinary least squares to the variables selected in the\nfirst stage by $L_2$Boosting. Another variant is \\textquotedblleft Orthogonal\nBoosting\\textquotedblright\\ where after each step an orthogonal projection is\nconducted. We show that both post-$L_2$Boosting and the orthogonal boosting\nachieve the same rate of convergence as LASSO in a sparse, high-dimensional\nsetting. We show that the rate of convergence of the classical $L_2$Boosting\ndepends on the design matrix described by a sparse eigenvalue constant. To show\nthe latter results, we derive new approximation results for the pure greedy\nalgorithm, based on analyzing the revisiting behavior of $L_2$Boosting. We also\nintroduce feasible rules for early stopping, which can be easily implemented\nand used in applied work. Our results also allow a direct comparison between\nLASSO and boosting which has been missing from the literature. Finally, we\npresent simulation studies and applications to illustrate the relevance of our\ntheoretical results and to provide insights into the practical aspects of\nboosting. In these simulation studies, post-$L_2$Boosting clearly outperforms\nLASSO.\n", "versions": [{"version": "v1", "created": "Mon, 29 Feb 2016 12:05:53 GMT"}, {"version": "v2", "created": "Sat, 5 Nov 2016 14:35:38 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Luo", "Ye", ""], ["Spindler", "Martin", ""]]}, {"id": "1602.09013", "submitter": "Anastasia Podosinnikova", "authors": "Anastasia Podosinnikova, Francis Bach, and Simon Lacoste-Julien", "title": "Beyond CCA: Moment Matching for Multi-View Models", "comments": "Appears in: Proceedings of the 33rd International Conference on\n  Machine Learning (ICML 2016). 22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce three novel semi-parametric extensions of probabilistic\ncanonical correlation analysis with identifiability guarantees. We consider\nmoment matching techniques for estimation in these models. For that, by drawing\nexplicit links between the new models and a discrete version of independent\ncomponent analysis (DICA), we first extend the DICA cumulant tensors to the new\ndiscrete version of CCA. By further using a close connection with independent\ncomponent analysis, we introduce generalized covariance matrices, which can\nreplace the cumulant tensors in the moment matching framework, and, therefore,\nimprove sample complexity and simplify derivations and algorithms\nsignificantly. As the tensor power method or orthogonal joint diagonalization\nare not applicable in the new setting, we use non-orthogonal joint\ndiagonalization techniques for matching the cumulants. We demonstrate\nperformance of the proposed models and estimation techniques on experiments\nwith both synthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Mon, 29 Feb 2016 15:51:50 GMT"}, {"version": "v2", "created": "Fri, 3 Jun 2016 14:06:23 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Podosinnikova", "Anastasia", ""], ["Bach", "Francis", ""], ["Lacoste-Julien", "Simon", ""]]}, {"id": "1602.09118", "submitter": "Joshua Achiam", "authors": "Joshua Achiam", "title": "Easy Monotonic Policy Iteration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key problem in reinforcement learning for control with general function\napproximators (such as deep neural networks and other nonlinear functions) is\nthat, for many algorithms employed in practice, updates to the policy or\n$Q$-function may fail to improve performance---or worse, actually cause the\npolicy performance to degrade. Prior work has addressed this for policy\niteration by deriving tight policy improvement bounds; by optimizing the lower\nbound on policy improvement, a better policy is guaranteed. However, existing\napproaches suffer from bounds that are hard to optimize in practice because\nthey include sup norm terms which cannot be efficiently estimated or\ndifferentiated. In this work, we derive a better policy improvement bound where\nthe sup norm of the policy divergence has been replaced with an average\ndivergence; this leads to an algorithm, Easy Monotonic Policy Iteration, that\ngenerates sequences of policies with guaranteed non-decreasing returns and is\neasy to implement in a sample-based framework.\n", "versions": [{"version": "v1", "created": "Mon, 29 Feb 2016 19:59:16 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Achiam", "Joshua", ""]]}]