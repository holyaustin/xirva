[{"id": "1609.00036", "submitter": "Amogh Gudi", "authors": "Agne Grinciunaite, Amogh Gudi, Emrah Tasli, Marten den Uyl", "title": "Human Pose Estimation in Space and Time using 3D CNN", "comments": "Accepted at ECCV 2016 Workshop on: Brave new ideas for motion\n  representations in videos", "journal-ref": null, "doi": "10.1007/978-3-319-49409-8_5", "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the capabilities of convolutional neural networks to deal\nwith a task that is easily manageable for humans: perceiving 3D pose of a human\nbody from varying angles. However, in our approach, we are restricted to using\na monocular vision system. For this purpose, we apply a convolutional neural\nnetwork approach on RGB videos and extend it to three dimensional convolutions.\nThis is done via encoding the time dimension in videos as the 3\\ts{rd}\ndimension in convolutional space, and directly regressing to human body joint\npositions in 3D coordinate space. This research shows the ability of such a\nnetwork to achieve state-of-the-art performance on the selected Human3.6M\ndataset, thus demonstrating the possibility of successfully representing\ntemporal data with an additional dimension in the convolutional operation.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 20:55:26 GMT"}, {"version": "v2", "created": "Wed, 14 Sep 2016 16:17:15 GMT"}, {"version": "v3", "created": "Wed, 19 Oct 2016 12:44:15 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Grinciunaite", "Agne", ""], ["Gudi", "Amogh", ""], ["Tasli", "Emrah", ""], ["Uyl", "Marten den", ""]]}, {"id": "1609.00048", "submitter": "Joel Tropp", "authors": "Joel A. Tropp, Alp Yurtsever, Madeleine Udell, Volkan Cevher", "title": "Practical sketching algorithms for low-rank matrix approximation", "comments": null, "journal-ref": "SIAM J. Matrix Analysis and Applications, Vol. 38, num. 4, pp.\n  1454-1485, Dec. 2017", "doi": "10.1137/17M1111590", "report-no": null, "categories": "cs.NA cs.DS math.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a suite of algorithms for constructing low-rank\napproximations of an input matrix from a random linear image of the matrix,\ncalled a sketch. These methods can preserve structural properties of the input\nmatrix, such as positive-semidefiniteness, and they can produce approximations\nwith a user-specified rank. The algorithms are simple, accurate, numerically\nstable, and provably correct. Moreover, each method is accompanied by an\ninformative error bound that allows users to select parameters a priori to\nachieve a given approximation quality. These claims are supported by numerical\nexperiments with real and synthetic data.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 21:30:26 GMT"}, {"version": "v2", "created": "Tue, 2 Jan 2018 18:13:40 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Tropp", "Joel A.", ""], ["Yurtsever", "Alp", ""], ["Udell", "Madeleine", ""], ["Cevher", "Volkan", ""]]}, {"id": "1609.00066", "submitter": "David Inouye", "authors": "David I. Inouye, Eunho Yang, Genevera I. Allen, Pradeep Ravikumar", "title": "A Review of Multivariate Distributions for Count Data Derived from the\n  Poisson Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Poisson distribution has been widely studied and used for modeling\nunivariate count-valued data. Multivariate generalizations of the Poisson\ndistribution that permit dependencies, however, have been far less popular.\nYet, real-world high-dimensional count-valued data found in word counts,\ngenomics, and crime statistics, for example, exhibit rich dependencies, and\nmotivate the need for multivariate distributions that can appropriately model\nthis data. We review multivariate distributions derived from the univariate\nPoisson, categorizing these models into three main classes: 1) where the\nmarginal distributions are Poisson, 2) where the joint distribution is a\nmixture of independent multivariate Poisson distributions, and 3) where the\nnode-conditional distributions are derived from the Poisson. We discuss the\ndevelopment of multiple instances of these classes and compare the models in\nterms of interpretability and theory. Then, we empirically compare multiple\nmodels from each class on three real-world datasets that have varying data\ncharacteristics from different domains, namely traffic accident data,\nbiological next generation sequencing data, and text data. These empirical\nexperiments develop intuition about the comparative advantages and\ndisadvantages of each class of multivariate distribution that was derived from\nthe Poisson. Finally, we suggest new research directions as explored in the\nsubsequent discussion section.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 23:08:02 GMT"}, {"version": "v2", "created": "Tue, 27 Dec 2016 13:42:25 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Inouye", "David I.", ""], ["Yang", "Eunho", ""], ["Allen", "Genevera I.", ""], ["Ravikumar", "Pradeep", ""]]}, {"id": "1609.00074", "submitter": "Junqi Jin", "authors": "Junqi Jin, Ziang Yan, Kun Fu, Nan Jiang, Changshui Zhang", "title": "Neural Network Architecture Optimization through Submodularity and\n  Supermodularity", "comments": "Withdrawn due to incompleteness and some overlaps with existing\n  literatures, I will resubmit adding further results", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models' architectures, including depth and width, are key\nfactors influencing models' performance, such as test accuracy and computation\ntime. This paper solves two problems: given computation time budget, choose an\narchitecture to maximize accuracy, and given accuracy requirement, choose an\narchitecture to minimize computation time. We convert this architecture\noptimization into a subset selection problem. With accuracy's submodularity and\ncomputation time's supermodularity, we propose efficient greedy optimization\nalgorithms. The experiments demonstrate our algorithm's ability to find more\naccurate models or faster models. By analyzing architecture evolution with\ngrowing time budget, we discuss relationships among accuracy, time and\narchitecture, and give suggestions on neural network architecture design.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2016 00:59:30 GMT"}, {"version": "v2", "created": "Sun, 19 Mar 2017 13:23:34 GMT"}, {"version": "v3", "created": "Wed, 21 Feb 2018 03:45:19 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Jin", "Junqi", ""], ["Yan", "Ziang", ""], ["Fu", "Kun", ""], ["Jiang", "Nan", ""], ["Zhang", "Changshui", ""]]}, {"id": "1609.00116", "submitter": "Martin  Biehl", "authors": "Nicholas Guttenberg, Martin Biehl, Ryota Kanai", "title": "Neural Coarse-Graining: Extracting slowly-varying latent degrees of\n  freedom with neural networks", "comments": "9 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a loss function for neural networks that encompasses an idea of\ntrivial versus non-trivial predictions, such that the network jointly\ndetermines its own prediction goals and learns to satisfy them. This permits\nthe network to choose sub-sets of a problem which are most amenable to its\nabilities to focus on solving, while discarding 'distracting' elements that\ninterfere with its learning. To do this, the network first transforms the raw\ndata into a higher-level categorical representation, and then trains a\npredictor from that new time series to its future. To prevent a trivial\nsolution of mapping the signal to zero, we introduce a measure of\nnon-triviality via a contrast between the prediction error of the learned model\nwith a naive model of the overall signal statistics. The transform can learn to\ndiscard uninformative and unpredictable components of the signal in favor of\nthe features which are both highly predictive and highly predictable. This\ncreates a coarse-grained model of the time-series dynamics, focusing on\npredicting the slowly varying latent parameters which control the statistics of\nthe time-series, rather than predicting the fast details directly. The result\nis a semi-supervised algorithm which is capable of extracting latent\nparameters, segmenting sections of time-series with differing statistics, and\nbuilding a higher-level representation of the underlying dynamics from\nunlabeled data.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2016 05:34:23 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Guttenberg", "Nicholas", ""], ["Biehl", "Martin", ""], ["Kanai", "Ryota", ""]]}, {"id": "1609.00285", "submitter": "Thomas Moreau", "authors": "Thomas Moreau and Joan Bruna", "title": "Understanding Trainable Sparse Coding via Matrix Factorization", "comments": "Published as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse coding is a core building block in many data analysis and machine\nlearning pipelines. Typically it is solved by relying on generic optimization\ntechniques, that are optimal in the class of first-order methods for\nnon-smooth, convex functions, such as the Iterative Soft Thresholding Algorithm\nand its accelerated version (ISTA, FISTA). However, these methods don't exploit\nthe particular structure of the problem at hand nor the input data\ndistribution. An acceleration using neural networks was proposed in\n\\cite{Gregor10}, coined LISTA, which showed empirically that one could achieve\nhigh quality estimates with few iterations by modifying the parameters of the\nproximal splitting appropriately.\n  In this paper we study the reasons for such acceleration. Our mathematical\nanalysis reveals that it is related to a specific matrix factorization of the\nGram kernel of the dictionary, which attempts to nearly diagonalise the kernel\nwith a basis that produces a small perturbation of the $\\ell_1$ ball. When this\nfactorization succeeds, we prove that the resulting splitting algorithm enjoys\nan improved convergence bound with respect to the non-adaptive version.\nMoreover, our analysis also shows that conditions for acceleration occur mostly\nat the beginning of the iterative process, consistent with numerical\nexperiments. We further validate our analysis by showing that on dictionaries\nwhere this factorization does not exist, adaptive acceleration fails.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2016 15:46:07 GMT"}, {"version": "v2", "created": "Fri, 2 Sep 2016 19:46:58 GMT"}, {"version": "v3", "created": "Thu, 3 Nov 2016 17:27:36 GMT"}, {"version": "v4", "created": "Mon, 29 May 2017 09:37:06 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Moreau", "Thomas", ""], ["Bruna", "Joan", ""]]}, {"id": "1609.00368", "submitter": "Emmanouil Zampetakis", "authors": "Constantinos Daskalakis, Christos Tzamos, Manolis Zampetakis", "title": "Ten Steps of EM Suffice for Mixtures of Two Gaussians", "comments": "Accepted for presentation at Conference on Learning Theory (COLT)\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Expectation-Maximization (EM) algorithm is a widely used method for\nmaximum likelihood estimation in models with latent variables. For estimating\nmixtures of Gaussians, its iteration can be viewed as a soft version of the\nk-means clustering algorithm. Despite its wide use and applications, there are\nessentially no known convergence guarantees for this method. We provide global\nconvergence guarantees for mixtures of two Gaussians with known covariance\nmatrices. We show that the population version of EM, where the algorithm is\ngiven access to infinitely many samples from the mixture, converges\ngeometrically to the correct mean vectors, and provide simple, closed-form\nexpressions for the convergence rate. As a simple illustration, we show that,\nin one dimension, ten steps of the EM algorithm initialized at infinity result\nin less than 1\\% error estimation of the means. In the finite sample regime, we\nshow that, under a random initialization, $\\tilde{O}(d/\\epsilon^2)$ samples\nsuffice to compute the unknown vectors to within $\\epsilon$ in Mahalanobis\ndistance, where $d$ is the dimension. In particular, the error rate of the EM\nbased estimator is $\\tilde{O}\\left(\\sqrt{d \\over n}\\right)$ where $n$ is the\nnumber of samples, which is optimal up to logarithmic factors.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2016 19:57:26 GMT"}, {"version": "v2", "created": "Thu, 15 Sep 2016 19:55:11 GMT"}, {"version": "v3", "created": "Wed, 12 Apr 2017 17:59:27 GMT"}, {"version": "v4", "created": "Thu, 13 Apr 2017 00:55:32 GMT"}, {"version": "v5", "created": "Mon, 5 Jun 2017 07:53:53 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Daskalakis", "Constantinos", ""], ["Tzamos", "Christos", ""], ["Zampetakis", "Manolis", ""]]}, {"id": "1609.00451", "submitter": "Mauricio Sadinle", "authors": "Mauricio Sadinle, Jing Lei, Larry Wasserman", "title": "Least Ambiguous Set-Valued Classifiers with Bounded Error Levels", "comments": "Final version to be published in the Journal of the American\n  Statistical Association at\n  https://www.tandfonline.com/doi/abs/10.1080/01621459.2017.1395341?journalCode=uasa20", "journal-ref": null, "doi": "10.1080/01621459.2017.1395341", "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most classification tasks there are observations that are ambiguous and\ntherefore difficult to correctly label. Set-valued classifiers output sets of\nplausible labels rather than a single label, thereby giving a more appropriate\nand informative treatment to the labeling of ambiguous instances. We introduce\na framework for multiclass set-valued classification, where the classifiers\nguarantee user-defined levels of coverage or confidence (the probability that\nthe true label is contained in the set) while minimizing the ambiguity (the\nexpected size of the output). We first derive oracle classifiers assuming the\ntrue distribution to be known. We show that the oracle classifiers are obtained\nfrom level sets of the functions that define the conditional probability of\neach class. Then we develop estimators with good asymptotic and finite sample\nproperties. The proposed estimators build on existing single-label classifiers.\nThe optimal classifier can sometimes output the empty set, but we provide two\nsolutions to fix this issue that are suitable for various practical needs.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2016 02:46:45 GMT"}, {"version": "v2", "created": "Sat, 22 Dec 2018 20:39:01 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Sadinle", "Mauricio", ""], ["Lei", "Jing", ""], ["Wasserman", "Larry", ""]]}, {"id": "1609.00489", "submitter": "Truyen Tran", "authors": "Morakot Choetkiertikul, Hoa Khanh Dam, Truyen Tran, Trang Pham, Aditya\n  Ghose and Tim Menzies", "title": "A deep learning model for estimating story points", "comments": "Submitted to ICSE'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although there has been substantial research in software analytics for effort\nestimation in traditional software projects, little work has been done for\nestimation in agile projects, especially estimating user stories or issues.\nStory points are the most common unit of measure used for estimating the effort\ninvolved in implementing a user story or resolving an issue. In this paper, we\noffer for the \\emph{first} time a comprehensive dataset for story points-based\nestimation that contains 23,313 issues from 16 open source projects. We also\npropose a prediction model for estimating story points based on a novel\ncombination of two powerful deep learning architectures: long short-term memory\nand recurrent highway network. Our prediction system is \\emph{end-to-end}\ntrainable from raw input data to prediction outcomes without any manual feature\nengineering. An empirical evaluation demonstrates that our approach\nconsistently outperforms three common effort estimation baselines and two\nalternatives in both Mean Absolute Error and the Standardized Accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2016 07:42:29 GMT"}, {"version": "v2", "created": "Tue, 6 Sep 2016 06:18:04 GMT"}], "update_date": "2016-09-07", "authors_parsed": [["Choetkiertikul", "Morakot", ""], ["Dam", "Hoa Khanh", ""], ["Tran", "Truyen", ""], ["Pham", "Trang", ""], ["Ghose", "Aditya", ""], ["Menzies", "Tim", ""]]}, {"id": "1609.00577", "submitter": "Edwin Bonilla", "authors": "Edwin V. Bonilla and Karl Krauth and Amir Dezfouli", "title": "Generic Inference in Latent Gaussian Process Models", "comments": "61 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an automated variational method for inference in models with\nGaussian process (GP) priors and general likelihoods. The method supports\nmultiple outputs and multiple latent functions and does not require detailed\nknowledge of the conditional likelihood, only needing its evaluation as a\nblack-box function. Using a mixture of Gaussians as the variational\ndistribution, we show that the evidence lower bound and its gradients can be\nestimated efficiently using samples from univariate Gaussian distributions.\nFurthermore, the method is scalable to large datasets which is achieved by\nusing an augmented prior via the inducing-variable approach underpinning most\nsparse GP approximations, along with parallel computation and stochastic\noptimization. We evaluate our approach quantitatively and qualitatively with\nexperiments on small datasets, medium-scale datasets and large datasets,\nshowing its competitiveness under different likelihood models and sparsity\nlevels. On the large-scale experiments involving prediction of airline delays\nand classification of handwritten digits, we show that our method is on par\nwith the state-of-the-art hard-coded approaches for scalable GP regression and\nclassification.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2016 12:43:34 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 06:05:09 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Bonilla", "Edwin V.", ""], ["Krauth", "Karl", ""], ["Dezfouli", "Amir", ""]]}, {"id": "1609.00629", "submitter": "Elad Richardson", "authors": "Elad Richardson, Rom Herskovitz, Boris Ginsburg, Michael Zibulevsky", "title": "SEBOOST - Boosting Stochastic Learning Using Subspace Optimization\n  Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SEBOOST, a technique for boosting the performance of existing\nstochastic optimization methods. SEBOOST applies a secondary optimization\nprocess in the subspace spanned by the last steps and descent directions. The\nmethod was inspired by the SESOP optimization method for large-scale problems,\nand has been adapted for the stochastic learning framework. It can be applied\non top of any existing optimization method with no need to tweak the internal\nalgorithm. We show that the method is able to boost the performance of\ndifferent algorithms, and make them more robust to changes in their\nhyper-parameters. As the boosting steps of SEBOOST are applied between large\nsets of descent steps, the additional subspace optimization hardly increases\nthe overall computational burden. We introduce two hyper-parameters that\ncontrol the balance between the baseline method and the secondary optimization\nprocess. The method was evaluated on several deep learning tasks, demonstrating\npromising results.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2016 14:48:16 GMT"}], "update_date": "2016-09-05", "authors_parsed": [["Richardson", "Elad", ""], ["Herskovitz", "Rom", ""], ["Ginsburg", "Boris", ""], ["Zibulevsky", "Michael", ""]]}, {"id": "1609.00661", "submitter": "Xiansheng Guo", "authors": "Xiansheng Guo, and Nirwan Ansari", "title": "Localization by Fusing a Group of Fingerprints via Multiple Antennas in\n  Indoor Environment", "comments": "11 pages,9 figures, submitted to IEEE Transactions on Vehicular\n  Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing fingerprints-based indoor localization approaches are based on\nsome single fingerprints, such as received signal strength (RSS), channel\nimpulse response (CIR), and signal subspace. However, the localization accuracy\nobtained by the single fingerprint approach is rather susceptible to the\nchanging environment, multi-path, and non-line-of-sight (NLOS) propagation.\nFurthermore, building the fingerprints is a very time consuming process. In\nthis paper, we propose a novel localization framework by Fusing A Group Of\nfingerprinTs (FAGOT) via multiple antennas for the indoor environment. We first\nbuild a GrOup Of Fingerprints (GOOF), which includes five different\nfingerprints, namely, RSS, covariance matrix, signal subspace, fractional low\norder moment, and fourth-order cumulant, which are obtained by different\ntransformations of the received signals from multiple antennas in the offline\nstage. Then, we design a parallel GOOF multiple classifiers based on AdaBoost\n(GOOF-AdaBoost) to train each of these fingerprints in parallel as five strong\nmultiple classifiers. In the online stage, we input the corresponding\ntransformations of the real measurements into these strong classifiers to\nobtain independent decisions. Finally, we propose an efficient combination\nfusion algorithm, namely, MUltiple Classifiers mUltiple Samples (MUCUS) fusion\nalgorithm to improve the accuracy of localization by combining the predictions\nof multiple classifiers with different samples. As compared with the single\nfingerprint approaches, the prediction probability of our proposed approach is\nimproved significantly. The process for building fingerprints can also be\nreduced drastically. We demonstrate the feasibility and performance of the\nproposed algorithm through extensive simulations as well as via real\nexperimental data using a Universal Software Radio Peripheral (USRP) platform\nwith four antennas.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2016 04:25:57 GMT"}, {"version": "v2", "created": "Wed, 20 Dec 2017 06:41:52 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Guo", "Xiansheng", ""], ["Ansari", "Nirwan", ""]]}, {"id": "1609.00672", "submitter": "Elie Wolfe", "authors": "Elie Wolfe, Robert W. Spekkens, Tobias Fritz", "title": "The Inflation Technique for Causal Inference with Latent Variables", "comments": "Minor final corrections, updated to match the published version as\n  closely as possible", "journal-ref": "J. Causal Inference 7(2), 2019", "doi": "10.1515/jci-2017-0020", "report-no": null, "categories": "quant-ph math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of causal inference is to determine if a given probability\ndistribution on observed variables is compatible with some causal structure.\nThe difficult case is when the causal structure includes latent variables. We\nhere introduce the $\\textit{inflation technique}$ for tackling this problem. An\ninflation of a causal structure is a new causal structure that can contain\nmultiple copies of each of the original variables, but where the ancestry of\neach copy mirrors that of the original. To every distribution of the observed\nvariables that is compatible with the original causal structure, we assign a\nfamily of marginal distributions on certain subsets of the copies that are\ncompatible with the inflated causal structure. It follows that compatibility\nconstraints for the inflation can be translated into compatibility constraints\nfor the original causal structure. Even if the constraints at the level of\ninflation are weak, such as observable statistical independences implied by\ndisjoint causal ancestry, the translated constraints can be strong. We apply\nthis method to derive new inequalities whose violation by a distribution\nwitnesses that distribution's incompatibility with the causal structure (of\nwhich Bell inequalities and Pearl's instrumental inequality are prominent\nexamples). We describe an algorithm for deriving all such inequalities for the\noriginal causal structure that follow from ancestral independences in the\ninflation. For three observed binary variables with pairwise common causes, it\nyields inequalities that are stronger in at least some aspects than those\nobtainable by existing methods. We also describe an algorithm that derives a\nweaker set of inequalities but is more efficient. Finally, we discuss which\ninflations are such that the inequalities one obtains from them remain valid\neven for quantum (and post-quantum) generalizations of the notion of a causal\nmodel.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2016 17:21:25 GMT"}, {"version": "v2", "created": "Mon, 12 Sep 2016 10:55:55 GMT"}, {"version": "v3", "created": "Wed, 14 Jun 2017 16:22:04 GMT"}, {"version": "v4", "created": "Fri, 10 Aug 2018 03:39:13 GMT"}, {"version": "v5", "created": "Mon, 22 Jul 2019 20:08:08 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Wolfe", "Elie", ""], ["Spekkens", "Robert W.", ""], ["Fritz", "Tobias", ""]]}, {"id": "1609.00680", "submitter": "Jinbo Xu", "authors": "Sheng Wang, Siqi Sun, Zhen Li, Renyu Zhang and Jinbo Xu", "title": "Accurate De Novo Prediction of Protein Contact Map by Ultra-Deep\n  Learning Model", "comments": null, "journal-ref": "PLoS Comput Biol 13(1): e1005324, 2017", "doi": "10.1371/journal.pcbi.1005324", "report-no": null, "categories": "q-bio.BM cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently exciting progress has been made on protein contact prediction, but\nthe predicted contacts for proteins without many sequence homologs is still of\nlow quality and not very useful for de novo structure prediction. This paper\npresents a new deep learning method that predicts contacts by integrating both\nevolutionary coupling (EC) and sequence conservation information through an\nultra-deep neural network formed by two deep residual networks. This deep\nneural network allows us to model very complex sequence-contact relationship as\nwell as long-range inter-contact correlation. Our method greatly outperforms\nexisting contact prediction methods and leads to much more accurate\ncontact-assisted protein folding. Tested on three datasets of 579 proteins, the\naverage top L long-range prediction accuracy obtained our method, the\nrepresentative EC method CCMpred and the CASP11 winner MetaPSICOV is 0.47, 0.21\nand 0.30, respectively; the average top L/10 long-range accuracy of our method,\nCCMpred and MetaPSICOV is 0.77, 0.47 and 0.59, respectively. Ab initio folding\nusing our predicted contacts as restraints can yield correct folds (i.e.,\nTMscore>0.6) for 203 test proteins, while that using MetaPSICOV- and\nCCMpred-predicted contacts can do so for only 79 and 62 proteins, respectively.\nFurther, our contact-assisted models have much better quality than\ntemplate-based models. Using our predicted contacts as restraints, we can (ab\ninitio) fold 208 of the 398 membrane proteins with TMscore>0.5. By contrast,\nwhen the training proteins of our method are used as templates, homology\nmodeling can only do so for 10 of them. One interesting finding is that even if\nwe do not train our prediction models with any membrane proteins, our method\nworks very well on membrane protein prediction. Finally, in recent blind CAMEO\nbenchmark our method successfully folded 5 test proteins with a novel fold.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2016 17:41:54 GMT"}, {"version": "v2", "created": "Mon, 5 Sep 2016 15:39:23 GMT"}, {"version": "v3", "created": "Thu, 15 Sep 2016 03:09:45 GMT"}, {"version": "v4", "created": "Fri, 16 Sep 2016 23:08:52 GMT"}, {"version": "v5", "created": "Mon, 7 Nov 2016 06:01:32 GMT"}, {"version": "v6", "created": "Sun, 27 Nov 2016 22:32:50 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Wang", "Sheng", ""], ["Sun", "Siqi", ""], ["Li", "Zhen", ""], ["Zhang", "Renyu", ""], ["Xu", "Jinbo", ""]]}, {"id": "1609.00718", "submitter": "Rie Johnson", "authors": "Rie Johnson and Tong Zhang", "title": "Convolutional Neural Networks for Text Categorization: Shallow\n  Word-level vs. Deep Character-level", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports the performances of shallow word-level convolutional\nneural networks (CNN), our earlier work (2015), on the eight datasets with\nrelatively large training data that were used for testing the very deep\ncharacter-level CNN in Conneau et al. (2016). Our findings are as follows. The\nshallow word-level CNNs achieve better error rates than the error rates\nreported in Conneau et al., though the results should be interpreted with some\nconsideration due to the unique pre-processing of Conneau et al. The shallow\nword-level CNN uses more parameters and therefore requires more storage than\nthe deep character-level CNN; however, the shallow word-level CNN computes much\nfaster.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 15:43:27 GMT"}], "update_date": "2016-09-05", "authors_parsed": [["Johnson", "Rie", ""], ["Zhang", "Tong", ""]]}, {"id": "1609.00719", "submitter": "Ziyuan Lin", "authors": "Jaakko Peltonen, Ziyuan Lin", "title": "Peacock Bundles: Bundle Coloring for Graphs with Globality-Locality\n  Trade-off", "comments": "Appears in the Proceedings of the 24th International Symposium on\n  Graph Drawing and Network Visualization (GD 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bundling of graph edges (node-to-node connections) is a common technique to\nenhance visibility of overall trends in the edge structure of a large graph\nlayout, and a large variety of bundling algorithms have been proposed. However,\nwith strong bundling, it becomes hard to identify origins and destinations of\nindividual edges. We propose a solution: we optimize edge coloring to\ndifferentiate bundled edges. We quantify strength of bundling in a flexible\npairwise fashion between edges, and among bundled edges, we quantify how\ndissimilar their colors should be by dissimilarity of their origins and\ndestinations. We solve the resulting nonlinear optimization, which is also\ninterpretable as a novel dimensionality reduction task. In large graphs the\nnecessary compromise is whether to differentiate colors sharply between locally\noccurring strongly bundled edges (\"local bundles\"), or also between the weakly\nbundled edges occurring globally over the graph (\"global bundles\"); we allow a\nuser-set global-local tradeoff. We call the technique \"peacock bundles\".\nExperiments show the coloring clearly enhances comprehensibility of graph\nlayouts with edge bundling.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2016 00:09:48 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Peltonen", "Jaakko", ""], ["Lin", "Ziyuan", ""]]}, {"id": "1609.00770", "submitter": "Ari Pakman", "authors": "Ari Pakman, Dar Gilboa, David Carlson and Liam Paninski", "title": "Stochastic Bouncy Particle Sampler", "comments": "ICML Camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel stochastic version of the non-reversible, rejection-free\nBouncy Particle Sampler (BPS), a Markov process whose sample trajectories are\npiecewise linear. The algorithm is based on simulating first arrival times in a\ndoubly stochastic Poisson process using the thinning method, and allows\nefficient sampling of Bayesian posteriors in big datasets. We prove that in the\nBPS no bias is introduced by noisy evaluations of the log-likelihood gradient.\nOn the other hand, we argue that efficiency considerations favor a small,\ncontrollable bias in the construction of the thinning proposals, in exchange\nfor faster mixing. We introduce a simple regression-based proposal intensity\nfor the thinning method that controls this trade-off. We illustrate the\nalgorithm in several examples in which it outperforms both unbiased, but slowly\nmixing stochastic versions of BPS, as well as biased stochastic gradient-based\nsamplers.\n", "versions": [{"version": "v1", "created": "Sat, 3 Sep 2016 00:13:05 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 17:00:17 GMT"}, {"version": "v3", "created": "Wed, 14 Jun 2017 01:16:51 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Pakman", "Ari", ""], ["Gilboa", "Dar", ""], ["Carlson", "David", ""], ["Paninski", "Liam", ""]]}, {"id": "1609.00845", "submitter": "Kwang-Sung Jun", "authors": "Kwang-Sung Jun and Robert Nowak", "title": "Graph-Based Active Learning: A New Look at Expected Error Minimization", "comments": "Submitted to GlobalSIP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In graph-based active learning, algorithms based on expected error\nminimization (EEM) have been popular and yield good empirical performance. The\nexact computation of EEM optimally balances exploration and exploitation. In\npractice, however, EEM-based algorithms employ various approximations due to\nthe computational hardness of exact EEM. This can result in a lack of either\nexploration or exploitation, which can negatively impact the effectiveness of\nactive learning. We propose a new algorithm TSA (Two-Step Approximation) that\nbalances between exploration and exploitation efficiently while enjoying the\nsame computational complexity as existing approximations. Finally, we\nempirically show the value of balancing between exploration and exploitation in\nboth toy and real-world datasets where our method outperforms several\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 3 Sep 2016 17:30:15 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Jun", "Kwang-Sung", ""], ["Nowak", "Robert", ""]]}, {"id": "1609.00878", "submitter": "Joao Papa", "authors": "Silas E. N. Fernandes, Danillo R. Pereira, Caio C. O. Ramos, Andre N.\n  Souza and Joao P. Papa", "title": "A Probabilistic Optimum-Path Forest Classifier for Binary Classification\n  Problems", "comments": "Submitted to Neural Processing Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic-driven classification techniques extend the role of traditional\napproaches that output labels (usually integer numbers) only. Such techniques\nare more fruitful when dealing with problems where one is not interested in\nrecognition/identification only, but also into monitoring the behavior of\nconsumers and/or machines, for instance. Therefore, by means of probability\nestimates, one can take decisions to work better in a number of scenarios. In\nthis paper, we propose a probabilistic-based Optimum Path Forest (OPF)\nclassifier to handle with binary classification problems, and we show it can be\nmore accurate than naive OPF in a number of datasets. In addition to being just\nmore accurate or not, probabilistic OPF turns to be another useful tool to the\nscientific community.\n", "versions": [{"version": "v1", "created": "Sun, 4 Sep 2016 00:12:04 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Fernandes", "Silas E. N.", ""], ["Pereira", "Danillo R.", ""], ["Ramos", "Caio C. O.", ""], ["Souza", "Andre N.", ""], ["Papa", "Joao P.", ""]]}, {"id": "1609.00904", "submitter": "Eric Holloway", "authors": "Eric Holloway and Robert Marks II", "title": "High Dimensional Human Guided Machine Learning", "comments": "3 pages, 1 figure, HCOMP 2016 submission, work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Have you ever looked at a machine learning classification model and thought,\nI could have made that? Well, that is what we test in this project, comparing\nXGBoost trained on human engineered features to training directly on data. The\nhuman engineered features do not outperform XGBoost trained di- rectly on the\ndata, but they are comparable. This project con- tributes a novel method for\nutilizing human created classifi- cation models on high dimensional datasets.\n", "versions": [{"version": "v1", "created": "Sun, 4 Sep 2016 08:45:26 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Holloway", "Eric", ""], ["Marks", "Robert", "II"]]}, {"id": "1609.00921", "submitter": "Muhammad Yousefnezhad", "authors": "Muhammad Yousefnezhad and Daoqiang Zhang", "title": "Decoding visual stimuli in human brain by using Anatomical Pattern\n  Analysis on fMRI images", "comments": "The 8th International Conference on Brain Inspired Cognitive Systems\n  (BICS'16), Beijing, China, Nov/28-30/2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A universal unanswered question in neuroscience and machine learning is\nwhether computers can decode the patterns of the human brain. Multi-Voxels\nPattern Analysis (MVPA) is a critical tool for addressing this question.\nHowever, there are two challenges in the previous MVPA methods, which include\ndecreasing sparsity and noises in the extracted features and increasing the\nperformance of prediction. In overcoming mentioned challenges, this paper\nproposes Anatomical Pattern Analysis (APA) for decoding visual stimuli in the\nhuman brain. This framework develops a novel anatomical feature extraction\nmethod and a new imbalance AdaBoost algorithm for binary classification.\nFurther, it utilizes an Error-Correcting Output Codes (ECOC) method for\nmulti-class prediction. APA can automatically detect active regions for each\ncategory of the visual stimuli. Moreover, it enables us to combine homogeneous\ndatasets for applying advanced classification. Experimental studies on 4 visual\ncategories (words, consonants, objects and scrambled photos) demonstrate that\nthe proposed approach achieves superior performance to state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 4 Sep 2016 12:01:50 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Yousefnezhad", "Muhammad", ""], ["Zhang", "Daoqiang", ""]]}, {"id": "1609.00951", "submitter": "Renbo Zhao", "authors": "Renbo Zhao, Vincent Y. F. Tan", "title": "A Unified Convergence Analysis of the Multiplicative Update Algorithm\n  for Regularized Nonnegative Matrix Factorization", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multiplicative update (MU) algorithm has been extensively used to\nestimate the basis and coefficient matrices in nonnegative matrix factorization\n(NMF) problems under a wide range of divergences and regularizers. However,\ntheoretical convergence guarantees have only been derived for a few special\ndivergences without regularization. In this work, we provide a conceptually\nsimple, self-contained, and unified proof for the convergence of the MU\nalgorithm applied on NMF with a wide range of divergences and regularizers. Our\nmain result shows the sequence of iterates (i.e., pairs of basis and\ncoefficient matrices) produced by the MU algorithm converges to the set of\nstationary points of the non-convex NMF optimization problem. Our proof\nstrategy has the potential to open up new avenues for analyzing similar\nproblems in machine learning and signal processing.\n", "versions": [{"version": "v1", "created": "Sun, 4 Sep 2016 15:47:43 GMT"}, {"version": "v2", "created": "Tue, 6 Sep 2016 11:04:24 GMT"}, {"version": "v3", "created": "Wed, 7 Jun 2017 10:44:01 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Zhao", "Renbo", ""], ["Tan", "Vincent Y. F.", ""]]}, {"id": "1609.00978", "submitter": "Chi Jin", "authors": "Chi Jin, Yuchen Zhang, Sivaraman Balakrishnan, Martin J. Wainwright,\n  Michael Jordan", "title": "Local Maxima in the Likelihood of Gaussian Mixture Models: Structural\n  Results and Algorithmic Consequences", "comments": "Neural Information Processing Systems (NIPS) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide two fundamental results on the population (infinite-sample)\nlikelihood function of Gaussian mixture models with $M \\geq 3$ components. Our\nfirst main result shows that the population likelihood function has bad local\nmaxima even in the special case of equally-weighted mixtures of well-separated\nand spherical Gaussians. We prove that the log-likelihood value of these bad\nlocal maxima can be arbitrarily worse than that of any global optimum, thereby\nresolving an open question of Srebro (2007). Our second main result shows that\nthe EM algorithm (or a first-order variant of it) with random initialization\nwill converge to bad critical points with probability at least\n$1-e^{-\\Omega(M)}$. We further establish that a first-order variant of EM will\nnot converge to strict saddle points almost surely, indicating that the poor\nperformance of the first-order method can be attributed to the existence of bad\nlocal maxima rather than bad saddle points. Overall, our results highlight the\nnecessity of careful initialization when using the EM algorithm in practice,\neven when applied in highly favorable settings.\n", "versions": [{"version": "v1", "created": "Sun, 4 Sep 2016 19:34:56 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Jin", "Chi", ""], ["Zhang", "Yuchen", ""], ["Balakrishnan", "Sivaraman", ""], ["Wainwright", "Martin J.", ""], ["Jordan", "Michael", ""]]}, {"id": "1609.01037", "submitter": "Ohad Shamir", "authors": "Ohad Shamir", "title": "Distribution-Specific Hardness of Learning Neural Networks", "comments": "Simpler and more explicit theorems in section 4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although neural networks are routinely and successfully trained in practice\nusing simple gradient-based methods, most existing theoretical results are\nnegative, showing that learning such networks is difficult, in a worst-case\nsense over all data distributions. In this paper, we take a more nuanced view,\nand consider whether specific assumptions on the \"niceness\" of the input\ndistribution, or \"niceness\" of the target function (e.g. in terms of\nsmoothness, non-degeneracy, incoherence, random choice of parameters etc.), are\nsufficient to guarantee learnability using gradient-based methods. We provide\nevidence that neither class of assumptions alone is sufficient: On the one\nhand, for any member of a class of \"nice\" target functions, there are difficult\ninput distributions. On the other hand, we identify a family of simple target\nfunctions, which are difficult to learn even if the input distribution is\n\"nice\". To prove our results, we develop some tools which may be of independent\ninterest, such as extending Fourier-based hardness techniques developed in the\ncontext of statistical queries \\cite{blum1994weakly}, from the Boolean cube to\nEuclidean space and to more general classes of functions.\n", "versions": [{"version": "v1", "created": "Mon, 5 Sep 2016 06:47:10 GMT"}, {"version": "v2", "created": "Thu, 9 Mar 2017 08:56:44 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["Shamir", "Ohad", ""]]}, {"id": "1609.01051", "submitter": "Eduardo C\\'esar Garrido Merch\\'an", "authors": "Eduardo C. Garrido-Merch\\'an and Daniel Hern\\'andez-Lobato", "title": "Predictive Entropy Search for Multi-objective Bayesian Optimization with\n  Constraints", "comments": "6 pages 2 figures", "journal-ref": "Neurocomputing, 361:50-68, 2019", "doi": "10.1016/j.neucom.2019.06.025", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents PESMOC, Predictive Entropy Search for Multi-objective\nBayesian Optimization with Constraints, an information-based strategy for the\nsimultaneous optimization of multiple expensive-to-evaluate black-box functions\nunder the presence of several constraints. PESMOC can hence be used to solve a\nwide range of optimization problems. Iteratively, PESMOC chooses an input\nlocation on which to evaluate the objective functions and the constraints so as\nto maximally reduce the entropy of the Pareto set of the corresponding\noptimization problem. The constraints considered in PESMOC are assumed to have\nsimilar properties to those of the objective functions in typical Bayesian\noptimization problems. That is, they do not have a known expression (which\nprevents gradient computation), their evaluation is considered to be very\nexpensive, and the resulting observations may be corrupted by noise. These\nconstraints arise in a plethora of expensive black-box optimization problems.\nWe carry out synthetic experiments to illustrate the effectiveness of PESMOC,\nwhere we sample both the objectives and the constraints from a Gaussian process\nprior. The results obtained show that PESMOC is able to provide better\nrecommendations with a smaller number of evaluations than a strategy based on\nrandom search.\n", "versions": [{"version": "v1", "created": "Mon, 5 Sep 2016 08:17:17 GMT"}, {"version": "v2", "created": "Mon, 26 Sep 2016 12:35:35 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Garrido-Merch\u00e1n", "Eduardo C.", ""], ["Hern\u00e1ndez-Lobato", "Daniel", ""]]}, {"id": "1609.01088", "submitter": "Evgeny Burnaev", "authors": "Mikhail Belyaev, Evgeny Burnaev, Ermek Kapushev, Maxim Panov, Pavel\n  Prikhodko, Dmitry Vetrov, Dmitry Yarotsky", "title": "GTApprox: surrogate modeling for industrial design", "comments": "31 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.CE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe GTApprox - a new tool for medium-scale surrogate modeling in\nindustrial design. Compared to existing software, GTApprox brings several\ninnovations: a few novel approximation algorithms, several advanced methods of\nautomated model selection, novel options in the form of hints. We demonstrate\nthe efficiency of GTApprox on a large collection of test problems. In addition,\nwe describe several applications of GTApprox to real engineering problems.\n", "versions": [{"version": "v1", "created": "Mon, 5 Sep 2016 10:41:14 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Belyaev", "Mikhail", ""], ["Burnaev", "Evgeny", ""], ["Kapushev", "Ermek", ""], ["Panov", "Maxim", ""], ["Prikhodko", "Pavel", ""], ["Vetrov", "Dmitry", ""], ["Yarotsky", "Dmitry", ""]]}, {"id": "1609.01226", "submitter": "Pingfan Tang", "authors": "Pingfan Tang, Jeff M. Phillips", "title": "The Robustness of Estimator Composition", "comments": "14 pages, 2 figures, 29th Conference on Neural Information Processing\n  Systems (NIPS 2016), Barcelona, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formalize notions of robustness for composite estimators via the notion of\na breakdown point. A composite estimator successively applies two (or more)\nestimators: on data decomposed into disjoint parts, it applies the first\nestimator on each part, then the second estimator on the outputs of the first\nestimator. And so on, if the composition is of more than two estimators.\nInformally, the breakdown point is the minimum fraction of data points which if\nsignificantly modified will also significantly modify the output of the\nestimator, so it is typically desirable to have a large breakdown point. Our\nmain result shows that, under mild conditions on the individual estimators, the\nbreakdown point of the composite estimator is the product of the breakdown\npoints of the individual estimators. We also demonstrate several scenarios,\nranging from regression to statistical testing, where this analysis is easy to\napply, useful in understanding worst case robustness, and sheds powerful\ninsights onto the associated data analysis.\n", "versions": [{"version": "v1", "created": "Mon, 5 Sep 2016 17:27:22 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Tang", "Pingfan", ""], ["Phillips", "Jeff M.", ""]]}, {"id": "1609.01233", "submitter": "James P. Crutchfield", "authors": "Ryan G. James and James P. Crutchfield", "title": "Multivariate Dependence Beyond Shannon Information", "comments": "10 pages, 6 figures, 3 tables;\n  http://csc.ucdavis.edu/~cmg/compmech/pubs/mdbsi.htm", "journal-ref": null, "doi": "10.3390/e19100531", "report-no": null, "categories": "cs.IT cond-mat.stat-mech math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately determining dependency structure is critical to discovering a\nsystem's causal organization. We recently showed that the transfer entropy\nfails in a key aspect of this---measuring information flow---due to its\nconflation of dyadic and polyadic relationships. We extend this observation to\ndemonstrate that this is true of all such Shannon information measures when\nused to analyze multivariate dependencies. This has broad implications,\nparticularly when employing information to express the organization and\nmechanisms embedded in complex systems, including the burgeoning efforts to\ncombine complex network theory with information theory. Here, we do not suggest\nthat any aspect of information theory is wrong. Rather, the vast majority of\nits informational measures are simply inadequate for determining the meaningful\ndependency structure within joint probability distributions. Therefore, such\ninformation measures are inadequate for discovering intrinsic causal relations.\nWe close by demonstrating that such distributions exist across an arbitrary set\nof variables.\n", "versions": [{"version": "v1", "created": "Mon, 5 Sep 2016 17:38:07 GMT"}, {"version": "v2", "created": "Thu, 8 Sep 2016 23:07:39 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["James", "Ryan G.", ""], ["Crutchfield", "James P.", ""]]}, {"id": "1609.01360", "submitter": "Alexander Wong", "authors": "Mohammad Javad Shafiee and Alexander Wong", "title": "Evolutionary Synthesis of Deep Neural Networks via Synaptic\n  Cluster-driven Genetic Encoding", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been significant recent interest towards achieving highly efficient\ndeep neural network architectures. A promising paradigm for achieving this is\nthe concept of evolutionary deep intelligence, which attempts to mimic\nbiological evolution processes to synthesize highly-efficient deep neural\nnetworks over successive generations. An important aspect of evolutionary deep\nintelligence is the genetic encoding scheme used to mimic heredity, which can\nhave a significant impact on the quality of offspring deep neural networks.\nMotivated by the neurobiological phenomenon of synaptic clustering, we\nintroduce a new genetic encoding scheme where synaptic probability is driven\ntowards the formation of a highly sparse set of synaptic clusters. Experimental\nresults for the task of image classification demonstrated that the synthesized\noffspring networks using this synaptic cluster-driven genetic encoding scheme\ncan achieve state-of-the-art performance while having network architectures\nthat are not only significantly more efficient (with a ~125-fold decrease in\nsynapses for MNIST) compared to the original ancestor network, but also\ntailored for GPU-accelerated machine learning applications.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 01:08:03 GMT"}, {"version": "v2", "created": "Tue, 22 Nov 2016 16:00:01 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Shafiee", "Mohammad Javad", ""], ["Wong", "Alexander", ""]]}, {"id": "1609.01423", "submitter": "Amicie de Pierrefeu", "authors": "Amicie de Pierrefeu, Tommy L\\\"ofstedt, Fouad Hadj-Selem, Mathieu\n  Dubois, Philippe Ciuciu, Vincent Frouin and Edouard Duchesnay", "title": "Structured Sparse Principal Components Analysis with the TV-Elastic Net\n  penalty", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is an exploratory tool widely used in data\nanalysis to uncover dominant patterns of variability within a population.\nDespite its ability to represent a data set in a low-dimensional space, the\ninterpretability of PCA remains limited. However, in neuroimaging, it is\nessential to uncover clinically interpretable phenotypic markers that would\naccount for the main variability in the brain images of a population. Recently,\nsome alternatives to the standard PCA approach, such as Sparse PCA, have been\nproposed, their aim being to limit the density of the components. Nonetheless,\nsparsity alone does not entirely solve the interpretability problem, since it\nmay yield scattered and unstable components. We hypothesized that the\nincorporation of prior information regarding the structure of the data may lead\nto improved relevance and interpretability of brain patterns. We therefore\npresent a simple extension of the popular PCA framework that adds structured\nsparsity penalties on the loading vectors in order to identify the few stable\nregions in the brain images accounting for most of the variability. Such\nstructured sparsity can be obtained by combining l1 and total variation (TV)\npenalties, where the TV regularization encodes higher order information about\nthe structure of the data. This paper presents the structured sparse PCA\n(denoted SPCA-TV) optimization framework and its resolution. We demonstrate the\nefficiency and versatility of SPCA-TV on three different data sets. The gains\nof SPCA-TV over unstructured approaches are significant,since SPCA-TV reveals\nthe variability within a data set in the form of intelligible brain patterns\nthat are easy to interpret, and are more stable across different samples.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 08:07:27 GMT"}, {"version": "v2", "created": "Mon, 12 Sep 2016 13:37:45 GMT"}, {"version": "v3", "created": "Wed, 14 Sep 2016 16:10:23 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["de Pierrefeu", "Amicie", ""], ["L\u00f6fstedt", "Tommy", ""], ["Hadj-Selem", "Fouad", ""], ["Dubois", "Mathieu", ""], ["Ciuciu", "Philippe", ""], ["Frouin", "Vincent", ""], ["Duchesnay", "Edouard", ""]]}, {"id": "1609.01468", "submitter": "Kardi Teknomo", "authors": "Wilfredo Badoy Jr. and Kardi Teknomo", "title": "Q-Learning with Basic Emotions", "comments": "7 pages, Badoy, W. and Teknomo, K. (2014) Q-Learning with Basic\n  Emotions, Proceeding of the 7th IEEE International Conference Humanoid,\n  Nanotechnology, Information Technology Communication and Control, Environment\n  and Management (HNICEM) 12-16 November 2014 Hotel Centro, Puerto Princesa,\n  Palawan, Philippines", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Q-learning is a simple and powerful tool in solving dynamic problems where\nenvironments are unknown. It uses a balance of exploration and exploitation to\nfind an optimal solution to the problem. In this paper, we propose using four\nbasic emotions: joy, sadness, fear, and anger to influence a Qlearning agent.\nSimulations show that the proposed affective agent requires lesser number of\nsteps to find the optimal path. We found when affective agent finds the optimal\npath, the ratio between exploration to exploitation gradually decreases,\nindicating lower total step count in the long run\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 10:03:27 GMT"}], "update_date": "2016-09-07", "authors_parsed": [["Badoy", "Wilfredo", "Jr."], ["Teknomo", "Kardi", ""]]}, {"id": "1609.01596", "submitter": "Arild N{\\o}kland", "authors": "Arild N{\\o}kland", "title": "Direct Feedback Alignment Provides Learning in Deep Neural Networks", "comments": "Accepted for publication at NIPS 2016. [v2] Corrected convolutional\n  results for feedback-alignment. [v3,v4,v5] Corrected theorem and proof", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks are most commonly trained with the\nback-propagation algorithm, where the gradient for learning is provided by\nback-propagating the error, layer by layer, from the output layer to the hidden\nlayers. A recently discovered method called feedback-alignment shows that the\nweights used for propagating the error backward don't have to be symmetric with\nthe weights used for propagation the activation forward. In fact, random\nfeedback weights work evenly well, because the network learns how to make the\nfeedback useful. In this work, the feedback alignment principle is used for\ntraining hidden layers more independently from the rest of the network, and\nfrom a zero initial condition. The error is propagated through fixed random\nfeedback connections directly from the output layer to each hidden layer. This\nsimple method is able to achieve zero training error even in convolutional\nnetworks and very deep networks, completely without error back-propagation. The\nmethod is a step towards biologically plausible machine learning because the\nerror signal is almost local, and no symmetric or reciprocal weights are\nrequired. Experiments show that the test performance on MNIST and CIFAR is\nalmost as good as those obtained with back-propagation for fully connected\nnetworks. If combined with dropout, the method achieves 1.45% error on the\npermutation invariant MNIST task.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 15:07:32 GMT"}, {"version": "v2", "created": "Sat, 24 Sep 2016 10:12:35 GMT"}, {"version": "v3", "created": "Thu, 6 Oct 2016 16:19:16 GMT"}, {"version": "v4", "created": "Sun, 23 Oct 2016 18:14:54 GMT"}, {"version": "v5", "created": "Wed, 21 Dec 2016 16:36:40 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["N\u00f8kland", "Arild", ""]]}, {"id": "1609.01672", "submitter": "Daniel Sussman", "authors": "Runze Tang, Michael Ketcha, Alexandra Badea, Evan D. Calabrese, Daniel\n  S. Margulies, Joshua T. Vogelstein, Carey E. Priebe, Daniel L. Sussman", "title": "Connectome Smoothing via Low-rank Approximations", "comments": "43 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In statistical connectomics, the quantitative study of brain networks,\nestimating the mean of a population of graphs based on a sample is a core\nproblem. Often, this problem is especially difficult because the sample or\ncohort size is relatively small, sometimes even a single subject. While using\nthe element-wise sample mean of the adjacency matrices is a common approach,\nthis method does not exploit any underlying structural properties of the\ngraphs. We propose using a low-rank method which incorporates tools for\ndimension selection and diagonal augmentation to smooth the estimates and\nimprove performance over the naive methodology for small sample sizes.\nTheoretical results for the stochastic blockmodel show that this method offers\nmajor improvements when there are many vertices. Similarly, we demonstrate that\nthe low-rank methods outperform the standard sample mean for a variety of\nindependent edge distributions as well as human connectome data derived from\nmagnetic resonance imaging, especially when sample sizes are small. Moreover,\nthe low-rank methods yield \"eigen-connectomes\", which correlate with the\nlobe-structure of the human brain and superstructures of the mouse brain. These\nresults indicate that low-rank methods are an important part of the tool box\nfor researchers studying populations of graphs in general, and statistical\nconnectomics in particular.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 17:53:37 GMT"}, {"version": "v2", "created": "Thu, 18 Jan 2018 19:00:18 GMT"}, {"version": "v3", "created": "Thu, 6 Dec 2018 22:23:40 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Tang", "Runze", ""], ["Ketcha", "Michael", ""], ["Badea", "Alexandra", ""], ["Calabrese", "Evan D.", ""], ["Margulies", "Daniel S.", ""], ["Vogelstein", "Joshua T.", ""], ["Priebe", "Carey E.", ""], ["Sussman", "Daniel L.", ""]]}, {"id": "1609.01840", "submitter": "Jinmeng Song", "authors": "Jinmeng Song, Chun Yuan", "title": "Learning Boltzmann Machine with EM-like Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an expectation-maximization-like(EMlike) method to train Boltzmann\nmachine with unconstrained connectivity. It adopts Monte Carlo approximation in\nthe E-step, and replaces the intractable likelihood objective with efficiently\ncomputed objectives or directly approximates the gradient of likelihood\nobjective in the M-step. The EM-like method is a modification of alternating\nminimization. We prove that EM-like method will be the exactly same with\ncontrastive divergence in restricted Boltzmann machine if the M-step of this\nmethod adopts special approximation. We also propose a new measure to assess\nthe performance of Boltzmann machine as generative models of data, and its\ncomputational complexity is O(Rmn). Finally, we demonstrate the performance of\nEM-like method using numerical experiments.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 05:17:30 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Song", "Jinmeng", ""], ["Yuan", "Chun", ""]]}, {"id": "1609.01872", "submitter": "Gabor Balazs", "authors": "G\\'abor Bal\\'azs, Andr\\'as Gy\\\"orgy, Csaba Szepesv\\'ari", "title": "Chaining Bounds for Empirical Risk Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper extends the standard chaining technique to prove excess risk upper\nbounds for empirical risk minimization with random design settings even if the\nmagnitude of the noise and the estimates is unbounded. The bound applies to\nmany loss functions besides the squared loss, and scales only with the\nsub-Gaussian or subexponential parameters without further statistical\nassumptions such as the bounded kurtosis condition over the hypothesis class. A\ndetailed analysis is provided for slope constrained and penalized linear least\nsquares regression with a sub-Gaussian setting, which often proves tight sample\ncomplexity bounds up to logartihmic factors.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 08:18:18 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Bal\u00e1zs", "G\u00e1bor", ""], ["Gy\u00f6rgy", "Andr\u00e1s", ""], ["Szepesv\u00e1ri", "Csaba", ""]]}, {"id": "1609.02020", "submitter": "Zhenyu Liao", "authors": "Zhenyu Liao, Romain Couillet", "title": "Random matrices meet machine learning: a large dimensional analysis of\n  LS-SVM", "comments": "wrong article submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes a performance analysis of kernel least squares support\nvector machines (LS-SVMs) based on a random matrix approach, in the regime\nwhere both the dimension of data $p$ and their number $n$ grow large at the\nsame rate. Under a two-class Gaussian mixture model for the input data, we\nprove that the LS-SVM decision function is asymptotically normal with means and\ncovariances shown to depend explicitly on the derivatives of the kernel\nfunction. This provides improved understanding along with new insights into the\ninternal workings of SVM-type methods for large datasets.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 15:39:24 GMT"}, {"version": "v2", "created": "Thu, 8 Sep 2016 07:26:00 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Liao", "Zhenyu", ""], ["Couillet", "Romain", ""]]}, {"id": "1609.02116", "submitter": "Trapit Bansal", "authors": "Trapit Bansal, David Belanger, Andrew McCallum", "title": "Ask the GRU: Multi-Task Learning for Deep Text Recommendations", "comments": "8 pages", "journal-ref": null, "doi": "10.1145/2959100.2959180", "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a variety of application domains the content to be recommended to users is\nassociated with text. This includes research papers, movies with associated\nplot summaries, news articles, blog posts, etc. Recommendation approaches based\non latent factor models can be extended naturally to leverage text by employing\nan explicit mapping from text to factors. This enables recommendations for new,\nunseen content, and may generalize better, since the factors for all items are\nproduced by a compactly-parametrized model. Previous work has used topic models\nor averages of word embeddings for this mapping. In this paper we present a\nmethod leveraging deep recurrent neural networks to encode the text sequence\ninto a latent vector, specifically gated recurrent units (GRUs) trained\nend-to-end on the collaborative filtering task. For the task of scientific\npaper recommendation, this yields models with significantly higher accuracy. In\ncold-start scenarios, we beat the previous state-of-the-art, all of which\nignore word order. Performance is further improved by multi-task learning,\nwhere the text encoder network is trained for a combination of content\nrecommendation and item metadata prediction. This regularizes the collaborative\nfiltering model, ameliorating the problem of sparsity of the observed rating\nmatrix.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 19:05:42 GMT"}, {"version": "v2", "created": "Fri, 9 Sep 2016 19:27:19 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["Bansal", "Trapit", ""], ["Belanger", "David", ""], ["McCallum", "Andrew", ""]]}, {"id": "1609.02200", "submitter": "Jason Rolfe", "authors": "Jason Tyler Rolfe", "title": "Discrete Variational Autoencoders", "comments": "Published as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic models with discrete latent variables naturally capture\ndatasets composed of discrete classes. However, they are difficult to train\nefficiently, since backpropagation through discrete variables is generally not\npossible. We present a novel method to train a class of probabilistic models\nwith discrete latent variables using the variational autoencoder framework,\nincluding backpropagation through the discrete latent variables. The associated\nclass of probabilistic models comprises an undirected discrete component and a\ndirected hierarchical continuous component. The discrete component captures the\ndistribution over the disconnected smooth manifolds induced by the continuous\ncomponent. As a result, this class of models efficiently learns both the class\nof objects in an image, and their specific realization in pixels, from\nunsupervised data, and outperforms state-of-the-art methods on the\npermutation-invariant MNIST, Omniglot, and Caltech-101 Silhouettes datasets.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 21:41:32 GMT"}, {"version": "v2", "created": "Sat, 22 Apr 2017 01:23:06 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Rolfe", "Jason Tyler", ""]]}, {"id": "1609.02208", "submitter": "Sewoong Oh", "authors": "Weihao Gao and Sewoong Oh and Pramod Viswanath", "title": "Breaking the Bandwidth Barrier: Geometrical Adaptive Entropy Estimation", "comments": "24 pages 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimators of information theoretic measures such as entropy and mutual\ninformation are a basic workhorse for many downstream applications in modern\ndata science. State of the art approaches have been either geometric (nearest\nneighbor (NN) based) or kernel based (with a globally chosen bandwidth). In\nthis paper, we combine both these approaches to design new estimators of\nentropy and mutual information that outperform state of the art methods. Our\nestimator uses local bandwidth choices of $k$-NN distances with a finite $k$,\nindependent of the sample size. Such a local and data dependent choice improves\nperformance in practice, but the bandwidth is vanishing at a fast rate, leading\nto a non-vanishing bias. We show that the asymptotic bias of the proposed\nestimator is universal; it is independent of the underlying distribution.\nHence, it can be pre-computed and subtracted from the estimate. As a byproduct,\nwe obtain a unified way of obtaining both kernel and NN estimators. The\ncorresponding theoretical contribution relating the asymptotic geometry of\nnearest neighbors to order statistics is of independent mathematical interest.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 22:11:39 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Gao", "Weihao", ""], ["Oh", "Sewoong", ""], ["Viswanath", "Pramod", ""]]}, {"id": "1609.02487", "submitter": "Lennart Gulikers", "authors": "Lennart Gulikers, Marc Lelarge, Laurent Massouli\\'e", "title": "Non-Backtracking Spectrum of Degree-Corrected Stochastic Block Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by community detection, we characterise the spectrum of the\nnon-backtracking matrix $B$ in the Degree-Corrected Stochastic Block Model.\n  Specifically, we consider a random graph on $n$ vertices partitioned into two\nequal-sized clusters. The vertices have i.i.d. weights $\\{ \\phi_u \\}_{u=1}^n$\nwith second moment $\\Phi^{(2)}$. The intra-cluster connection probability for\nvertices $u$ and $v$ is $\\frac{\\phi_u \\phi_v}{n}a$ and the inter-cluster\nconnection probability is $\\frac{\\phi_u \\phi_v}{n}b$.\n  We show that with high probability, the following holds: The leading\neigenvalue of the non-backtracking matrix $B$ is asymptotic to $\\rho =\n\\frac{a+b}{2} \\Phi^{(2)}$. The second eigenvalue is asymptotic to $\\mu_2 =\n\\frac{a-b}{2} \\Phi^{(2)}$ when $\\mu_2^2 > \\rho$, but asymptotically bounded by\n$\\sqrt{\\rho}$ when $\\mu_2^2 \\leq \\rho$. All the remaining eigenvalues are\nasymptotically bounded by $\\sqrt{\\rho}$. As a result, a clustering\npositively-correlated with the true communities can be obtained based on the\nsecond eigenvector of $B$ in the regime where $\\mu_2^2 > \\rho.$\n  In a previous work we obtained that detection is impossible when $\\mu_2^2 <\n\\rho,$ meaning that there occurs a phase-transition in the sparse regime of the\nDegree-Corrected Stochastic Block Model.\n  As a corollary, we obtain that Degree-Corrected Erd\\H{o}s-R\\'enyi graphs\nasymptotically satisfy the graph Riemann hypothesis, a quasi-Ramanujan\nproperty.\n  A by-product of our proof is a weak law of large numbers for\nlocal-functionals on Degree-Corrected Stochastic Block Models, which could be\nof independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2016 16:33:39 GMT"}, {"version": "v2", "created": "Thu, 18 May 2017 11:48:10 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Gulikers", "Lennart", ""], ["Lelarge", "Marc", ""], ["Massouli\u00e9", "Laurent", ""]]}, {"id": "1609.02513", "submitter": "Dan Guralnik", "authors": "Jared Culbertson, Dan P. Guralnik, Peter F. Stiller", "title": "Functorial Hierarchical Clustering with Overlaps", "comments": "Minor revisions. 24 pages, 1 figure", "journal-ref": "Discrete Applied Mathematics, Volume 236, 19 February 2018,\n  pp.108--123", "doi": "10.1016/j.dam.2017.10.015", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This work draws inspiration from three important sources of research on\ndissimilarity-based clustering and intertwines those three threads into a\nconsistent principled functorial theory of clustering. Those three are the\noverlapping clustering of Jardine and Sibson, the functorial approach of\nCarlsson and M\\'{e}moli to partition-based clustering, and the Isbell/Dress\nschool's study of injective envelopes. Carlsson and M\\'{e}moli introduce the\nidea of viewing clustering methods as functors from a category of metric spaces\nto a category of clusters, with functoriality subsuming many desirable\nproperties. Our first series of results extends their theory of functorial\nclustering schemes to methods that allow overlapping clusters in the spirit of\nJardine and Sibson. This obviates some of the unpleasant effects of chaining\nthat occur, for example with single-linkage clustering. We prove an equivalence\nbetween these general overlapping clustering functors and projections of weight\nspaces to what we term clustering domains, by focusing on the order structure\ndetermined by the morphisms. As a specific application of this machinery, we\nare able to prove that there are no functorial projections to cut metrics, or\neven to tree metrics. Finally, although we focus less on the construction of\nclustering methods (clustering domains) derived from injective envelopes, we\nlay out some preliminary results, that hopefully will give a feel for how the\nthird leg of the stool comes into play.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2016 17:52:26 GMT"}, {"version": "v2", "created": "Tue, 14 Aug 2018 18:34:04 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Culbertson", "Jared", ""], ["Guralnik", "Dan P.", ""], ["Stiller", "Peter F.", ""]]}, {"id": "1609.02521", "submitter": "Rohit Babbar", "authors": "Rohit Babbar and Bernhard Shoelkopf", "title": "DiSMEC - Distributed Sparse Machines for Extreme Multi-label\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extreme multi-label classification refers to supervised multi-label learning\ninvolving hundreds of thousands or even millions of labels. Datasets in extreme\nclassification exhibit fit to power-law distribution, i.e. a large fraction of\nlabels have very few positive instances in the data distribution. Most\nstate-of-the-art approaches for extreme multi-label classification attempt to\ncapture correlation among labels by embedding the label matrix to a\nlow-dimensional linear sub-space. However, in the presence of power-law\ndistributed extremely large and diverse label spaces, structural assumptions\nsuch as low rank can be easily violated.\n  In this work, we present DiSMEC, which is a large-scale distributed framework\nfor learning one-versus-rest linear classifiers coupled with explicit capacity\ncontrol to control model size. Unlike most state-of-the-art methods, DiSMEC\ndoes not make any low rank assumptions on the label matrix. Using double layer\nof parallelization, DiSMEC can learn classifiers for datasets consisting\nhundreds of thousands labels within few hours. The explicit capacity control\nmechanism filters out spurious parameters which keep the model compact in size,\nwithout losing prediction accuracy. We conduct extensive empirical evaluation\non publicly available real-world datasets consisting upto 670,000 labels. We\ncompare DiSMEC with recent state-of-the-art approaches, including - SLEEC which\nis a leading approach for learning sparse local embeddings, and FastXML which\nis a tree-based approach optimizing ranking based loss function. On some of the\ndatasets, DiSMEC can significantly boost prediction accuracies - 10% better\ncompared to SLECC and 15% better compared to FastXML, in absolute terms.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2016 18:17:25 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Babbar", "Rohit", ""], ["Shoelkopf", "Bernhard", ""]]}, {"id": "1609.02606", "submitter": "Shahin Shahrampour", "authors": "Shahin Shahrampour, Mohammad Noshad, Vahid Tarokh", "title": "On Sequential Elimination Algorithms for Best-Arm Identification in\n  Multi-Armed Bandits", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2017.2706192", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the best-arm identification problem in multi-armed bandits, which\nfocuses purely on exploration. A player is given a fixed budget to explore a\nfinite set of arms, and the rewards of each arm are drawn independently from a\nfixed, unknown distribution. The player aims to identify the arm with the\nlargest expected reward. We propose a general framework to unify sequential\nelimination algorithms, where the arms are dismissed iteratively until a unique\narm is left. Our analysis reveals a novel performance measure expressed in\nterms of the sampling mechanism and number of eliminated arms at each round.\nBased on this result, we develop an algorithm that divides the budget according\nto a nonlinear function of remaining arms at each round. We provide theoretical\nguarantees for the algorithm, characterizing the suitable nonlinearity for\ndifferent problem environments described by the number of competitive arms.\nMatching the theoretical results, our experiments show that the nonlinear\nalgorithm outperforms the state-of-the-art. We finally study the\nside-observation model, where pulling an arm reveals the rewards of its related\narms, and we establish improved theoretical guarantees in the pure-exploration\nsetting.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2016 21:46:37 GMT"}, {"version": "v2", "created": "Thu, 13 Apr 2017 16:02:04 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Shahrampour", "Shahin", ""], ["Noshad", "Mohammad", ""], ["Tarokh", "Vahid", ""]]}, {"id": "1609.02613", "submitter": "Wei Fu", "authors": "Wei Fu and Vivek Nair and Tim Menzies", "title": "Why is Differential Evolution Better than Grid Search for Tuning Defect\n  Predictors?", "comments": "12 pages, 8 figures, submitted to Information and Software Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context: One of the black arts of data mining is learning the magic\nparameters which control the learners. In software analytics, at least for\ndefect prediction, several methods, like grid search and differential evolution\n(DE), have been proposed to learn these parameters, which has been proved to be\nable to improve the performance scores of learners.\n  Objective: We want to evaluate which method can find better parameters in\nterms of performance score and runtime cost.\n  Methods: This paper compares grid search to differential evolution, which is\nan evolutionary algorithm that makes extensive use of stochastic jumps around\nthe search space.\n  Results: We find that the seemingly complete approach of grid search does no\nbetter, and sometimes worse, than the stochastic search. When repeated 20 times\nto check for conclusion validity, DE was over 210 times faster than grid search\nto tune Random Forests on 17 testing data sets with F-Measure\n  Conclusions: These results are puzzling: why does a quick partial search be\njust as effective as a much slower, and much more, extensive search? To answer\nthat question, we turned to the theoretical optimization literature. Bergstra\nand Bengio conjecture that grid search is not more effective than more\nrandomized searchers if the underlying search space is inherently low\ndimensional. This is significant since recent results show that defect\nprediction exhibits very low intrinsic dimensionality-- an observation that\nexplains why a fast method like DE may work as well as a seemingly more\nthorough grid search. This suggests, as a future research direction, that it\nmight be possible to peek at data sets before doing any optimization in order\nto match the optimization algorithm to the problem at hand.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2016 22:32:44 GMT"}, {"version": "v2", "created": "Tue, 13 Sep 2016 02:15:12 GMT"}, {"version": "v3", "created": "Fri, 10 Mar 2017 17:29:06 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["Fu", "Wei", ""], ["Nair", "Vivek", ""], ["Menzies", "Tim", ""]]}, {"id": "1609.02631", "submitter": "Varvara Kollia", "authors": "Varvara Kollia, Oguz H. Elibol", "title": "Distributed Processing of Biosignal-Database for Emotion Recognition\n  with Mahout", "comments": "4 pages, 5 png figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the use of distributed processing on the problem of\nemotion recognition from physiological sensors using a popular machine learning\nlibrary on distributed mode. Specifically, we run a random forests classifier\non the biosignal-data, which have been pre-processed to form exclusive groups\nin an unsupervised fashion, on a Cloudera cluster using Mahout. The use of\ndistributed processing significantly reduces the time required for the offline\ntraining of the classifier, enabling processing of large physiological datasets\nthrough many iterations.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 01:13:20 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["Kollia", "Varvara", ""], ["Elibol", "Oguz H.", ""]]}, {"id": "1609.02655", "submitter": "Nhat Ho", "authors": "Nhat Ho and XuanLong Nguyen", "title": "Singularity structures and impacts on parameter estimation in finite\n  mixtures of distributions", "comments": "87 pages. This version has improved introduction and expanded\n  discussion of related work. An abridged version is to appear on SIAM Journal\n  on Mathematics of Data Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Singularities of a statistical model are the elements of the model's\nparameter space which make the corresponding Fisher information matrix\ndegenerate. These are the points for which estimation techniques such as the\nmaximum likelihood estimator and standard Bayesian procedures do not admit the\nroot-$n$ parametric rate of convergence. We propose a general framework for the\nidentification of singularity structures of the parameter space of finite\nmixtures, and study the impacts of the singularity structures on minimax lower\nbounds and rates of convergence for the maximum likelihood estimator over a\ncompact parameter space. Our study makes explicit the deep links between model\nsingularities, parameter estimation convergence rates and minimax lower bounds,\nand the algebraic geometry of the parameter space for mixtures of continuous\ndistributions. The theory is applied to establish concrete convergence rates of\nparameter estimation for finite mixture of skew-normal distributions. This rich\nand increasingly popular mixture model is shown to exhibit a remarkably complex\nrange of asymptotic behaviors which have not been hitherto reported in the\nliterature.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 04:22:03 GMT"}, {"version": "v2", "created": "Sat, 1 Sep 2018 19:06:27 GMT"}, {"version": "v3", "created": "Thu, 18 Jul 2019 21:05:28 GMT"}, {"version": "v4", "created": "Tue, 23 Jul 2019 22:12:15 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Ho", "Nhat", ""], ["Nguyen", "XuanLong", ""]]}, {"id": "1609.02686", "submitter": "Elisabeth Waldmann", "authors": "Elisabeth Waldmann, David Taylor-Robinson, Nadja Klein, Thomas Kneib,\n  Tania Pressler, Matthias Schmid and Andreas Mayr", "title": "Boosting Joint Models for Longitudinal and Time-to-Event Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint Models for longitudinal and time-to-event data have gained a lot of\nattention in the last few years as they are a helpful technique to approach\ncommon a data structure in clinical studies where longitudinal outcomes are\nrecorded alongside event times. Those two processes are often linked and the\ntwo outcomes should thus be modeled jointly in order to prevent the potential\nbias introduced by independent modelling. Commonly, joint models are estimated\nin likelihood based expectation maximization or Bayesian approaches using\nframeworks where variable selection is problematic and which do not immediately\nwork for high-dimensional data. In this paper, we propose a boosting algorithm\ntackling these challenges by being able to simultaneously estimate predictors\nfor joint models and automatically select the most influential variables even\nin high-dimensional data situations. We analyse the performance of the new\nalgorithm in a simulation study and apply it to the Danish cystic fibrosis\nregistry which collects longitudinal lung function data on patients with cystic\nfibrosis together with data regarding the onset of pulmonary infections. This\nis the first approach to combine state-of-the art algorithms from the field of\nmachine-learning with the model class of joint models, providing a fully\ndata-driven mechanism to select variables and predictor effects in a unified\nframework of boosting joint models.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 08:17:54 GMT"}, {"version": "v2", "created": "Thu, 22 Dec 2016 17:04:04 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Waldmann", "Elisabeth", ""], ["Taylor-Robinson", "David", ""], ["Klein", "Nadja", ""], ["Kneib", "Thomas", ""], ["Pressler", "Tania", ""], ["Schmid", "Matthias", ""], ["Mayr", "Andreas", ""]]}, {"id": "1609.02700", "submitter": "Clement Chevalier", "authors": "S\\'ebastien Marmin (IMSV, I2M), Cl\\'ement Chevalier, David Ginsbourger\n  (IMSV)", "title": "Efficient batch-sequential Bayesian optimization with moments of\n  truncated Gaussian vectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We deal with the efficient parallelization of Bayesian global optimization\nalgorithms, and more specifically of those based on the expected improvement\ncriterion and its variants. A closed form formula relying on multivariate\nGaussian cumulative distribution functions is established for a generalized\nversion of the multipoint expected improvement criterion. In turn, the latter\nrelies on intermediate results that could be of independent interest concerning\nmoments of truncated Gaussian vectors. The obtained expansion of the criterion\nenables studying its differentiability with respect to point batches and\ncalculating the corresponding gradient in closed form. Furthermore , we derive\nfast numerical approximations of this gradient and propose efficient batch\noptimization strategies. Numerical experiments illustrate that the proposed\napproaches enable computational savings of between one and two order of\nmagnitudes, hence enabling derivative-based batch-sequential acquisition\nfunction maximization to become a practically implementable and efficient\nstandard.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 08:42:12 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["Marmin", "S\u00e9bastien", "", "IMSV, I2M"], ["Chevalier", "Cl\u00e9ment", "", "IMSV"], ["Ginsbourger", "David", "", "IMSV"]]}, {"id": "1609.02815", "submitter": "Felix Brockherde", "authors": "Felix Brockherde, Leslie Vogt, Li Li, Mark E. Tuckerman, Kieron Burke,\n  Klaus-Robert M\\\"uller", "title": "By-passing the Kohn-Sham equations with machine learning", "comments": null, "journal-ref": null, "doi": "10.1038/s41467-017-00839-3", "report-no": null, "categories": "physics.comp-ph cs.LG physics.chem-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Last year, at least 30,000 scientific papers used the Kohn-Sham scheme of\ndensity functional theory to solve electronic structure problems in a wide\nvariety of scientific fields, ranging from materials science to biochemistry to\nastrophysics. Machine learning holds the promise of learning the kinetic energy\nfunctional via examples, by-passing the need to solve the Kohn-Sham equations.\nThis should yield substantial savings in computer time, allowing either larger\nsystems or longer time-scales to be tackled, but attempts to machine-learn this\nfunctional have been limited by the need to find its derivative. The present\nwork overcomes this difficulty by directly learning the density-potential and\nenergy-density maps for test systems and various molecules. Both improved\naccuracy and lower computational cost with this method are demonstrated by\nreproducing DFT energies for a range of molecular geometries generated during\nmolecular dynamics simulations. Moreover, the methodology could be applied\ndirectly to quantum chemical calculations, allowing construction of density\nfunctionals of quantum-chemical accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 14:45:48 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 09:24:40 GMT"}, {"version": "v3", "created": "Thu, 15 Jun 2017 08:19:46 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Brockherde", "Felix", ""], ["Vogt", "Leslie", ""], ["Li", "Li", ""], ["Tuckerman", "Mark E.", ""], ["Burke", "Kieron", ""], ["M\u00fcller", "Klaus-Robert", ""]]}, {"id": "1609.02845", "submitter": "Shahin Shahrampour", "authors": "Shahin Shahrampour, Ali Jadbabaie", "title": "Distributed Online Optimization in Dynamic Environments Using Mirror\n  Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses decentralized online optimization in non-stationary\nenvironments. A network of agents aim to track the minimizer of a global\ntime-varying convex function. The minimizer evolves according to a known\ndynamics corrupted by an unknown, unstructured noise. At each time, the global\nfunction can be cast as a sum of a finite number of local functions, each of\nwhich is assigned to one agent in the network. Moreover, the local functions\nbecome available to agents sequentially, and agents do not have a prior\nknowledge of the future cost functions. Therefore, agents must communicate with\neach other to build an online approximation of the global function. We propose\na decentralized variation of the celebrated Mirror Descent, developed by\nNemirovksi and Yudin. Using the notion of Bregman divergence in lieu of\nEuclidean distance for projection, Mirror Descent has been shown to be a\npowerful tool in large-scale optimization. Our algorithm builds on Mirror\nDescent, while ensuring that agents perform a consensus step to follow the\nglobal function and take into account the dynamics of the global minimizer. To\nmeasure the performance of the proposed online algorithm, we compare it to its\noffline counterpart, where the global functions are available a priori. The gap\nbetween the two is called dynamic regret. We establish a regret bound that\nscales inversely in the spectral gap of the network, and more notably it\nrepresents the deviation of minimizer sequence with respect to the given\ndynamics. We then show that our results subsume a number of results in\ndistributed optimization. We demonstrate the application of our method to\ndecentralized tracking of dynamic parameters and verify the results via\nnumerical experiments.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 16:00:04 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["Shahrampour", "Shahin", ""], ["Jadbabaie", "Ali", ""]]}, {"id": "1609.02906", "submitter": "Pan Zhang", "authors": "Pan Zhang", "title": "Robust Spectral Detection of Global Structures in the Data by Learning a\n  Regularization", "comments": "13 pages, 9 figures, Neural Information Processing Systems 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral methods are popular in detecting global structures in the given data\nthat can be represented as a matrix. However when the data matrix is sparse or\nnoisy, classic spectral methods usually fail to work, due to localization of\neigenvectors (or singular vectors) induced by the sparsity or noise. In this\nwork, we propose a general method to solve the localization problem by learning\na regularization matrix from the localized eigenvectors. Using matrix\nperturbation analysis, we demonstrate that the learned regularizations suppress\ndown the eigenvalues associated with localized eigenvectors and enable us to\nrecover the informative eigenvectors representing the global structure. We show\napplications of our method in several inference problems: community detection\nin networks, clustering from pairwise similarities, rank estimation and matrix\ncompletion problems. Using extensive experiments, we illustrate that our method\nsolves the localization problem and works down to the theoretical detectability\nlimits in different kinds of synthetic data. This is in contrast with existing\nspectral algorithms based on data matrix, non-backtracking matrix, Laplacians\nand those with rank-one regularizations, which perform poorly in the sparse\ncase with noise.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 19:48:29 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["Zhang", "Pan", ""]]}, {"id": "1609.02907", "submitter": "Thomas Kipf", "authors": "Thomas N. Kipf, Max Welling", "title": "Semi-Supervised Classification with Graph Convolutional Networks", "comments": "Published as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a scalable approach for semi-supervised learning on\ngraph-structured data that is based on an efficient variant of convolutional\nneural networks which operate directly on graphs. We motivate the choice of our\nconvolutional architecture via a localized first-order approximation of\nspectral graph convolutions. Our model scales linearly in the number of graph\nedges and learns hidden layer representations that encode both local graph\nstructure and features of nodes. In a number of experiments on citation\nnetworks and on a knowledge graph dataset we demonstrate that our approach\noutperforms related methods by a significant margin.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 19:48:41 GMT"}, {"version": "v2", "created": "Mon, 24 Oct 2016 21:25:47 GMT"}, {"version": "v3", "created": "Thu, 3 Nov 2016 18:37:23 GMT"}, {"version": "v4", "created": "Wed, 22 Feb 2017 09:55:36 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Kipf", "Thomas N.", ""], ["Welling", "Max", ""]]}, {"id": "1609.02938", "submitter": "Hau-tieng Wu", "authors": "Su Li, Hau-tieng Wu", "title": "Extract fetal ECG from single-lead abdominal ECG by de-shape short time\n  Fourier transform and nonlocal median", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM physics.data-an stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multiple fundamental frequency detection problem and the source\nseparation problem from a single-channel signal containing multiple oscillatory\ncomponents and a nonstationary noise are both challenging tasks. To extract the\nfetal electrocardiogram (ECG) from a single-lead maternal abdominal ECG, we\nface both challenges. In this paper, we propose a novel method to extract the\nfetal ECG signal from the single channel maternal abdominal ECG signal, without\nany additional measurement. The algorithm is composed of three main\ningredients. First, the maternal and fetal heart rates are estimated by the\nde-shape short time Fourier transform, which is a recently proposed nonlinear\ntime-frequency analysis technique; second, the beat tracking technique is\napplied to accurately obtain the maternal and fetal R peaks; third, the\nmaternal and fetal ECG waveforms are established by the nonlocal median. The\nalgorithm is evaluated on a simulated fetal ECG signal database ({\\em fecgsyn}\ndatabase), and tested on two real databases with the annotation provided by\nexperts ({\\em adfecgdb} database and {\\em CinC2013} database). In general, the\nalgorithm could be applied to solve other detection and source separation\nproblems, and reconstruct the time-varying wave-shape function of each\noscillatory component.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 20:26:31 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Li", "Su", ""], ["Wu", "Hau-tieng", ""]]}, {"id": "1609.02943", "submitter": "Florian Tram\\`er", "authors": "Florian Tram\\`er, Fan Zhang, Ari Juels, Michael K. Reiter, Thomas\n  Ristenpart", "title": "Stealing Machine Learning Models via Prediction APIs", "comments": "19 pages, 7 figures, Proceedings of USENIX Security 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) models may be deemed confidential due to their\nsensitive training data, commercial value, or use in security applications.\nIncreasingly often, confidential ML models are being deployed with publicly\naccessible query interfaces. ML-as-a-service (\"predictive analytics\") systems\nare an example: Some allow users to train models on potentially sensitive data\nand charge others for access on a pay-per-query basis.\n  The tension between model confidentiality and public access motivates our\ninvestigation of model extraction attacks. In such attacks, an adversary with\nblack-box access, but no prior knowledge of an ML model's parameters or\ntraining data, aims to duplicate the functionality of (i.e., \"steal\") the\nmodel. Unlike in classical learning theory settings, ML-as-a-service offerings\nmay accept partial feature vectors as inputs and include confidence values with\npredictions. Given these practices, we show simple, efficient attacks that\nextract target ML models with near-perfect fidelity for popular model classes\nincluding logistic regression, neural networks, and decision trees. We\ndemonstrate these attacks against the online services of BigML and Amazon\nMachine Learning. We further show that the natural countermeasure of omitting\nconfidence values from model outputs still admits potentially harmful model\nextraction attacks. Our results highlight the need for careful ML model\ndeployment and new model extraction countermeasures.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 20:39:20 GMT"}, {"version": "v2", "created": "Mon, 3 Oct 2016 02:44:14 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Tram\u00e8r", "Florian", ""], ["Zhang", "Fan", ""], ["Juels", "Ari", ""], ["Reiter", "Michael K.", ""], ["Ristenpart", "Thomas", ""]]}, {"id": "1609.02997", "submitter": "Young Woong Park", "authors": "Young Woong Park, Diego Klabjan", "title": "Iteratively Reweighted Least Squares Algorithms for L1-Norm Principal\n  Component Analysis", "comments": null, "journal-ref": "2016 IEEE 16th International Conference on Data Mining, Barcelona,\n  Spain (2016): 430-438", "doi": "10.1109/ICDM.2016.0054", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is often used to reduce the dimension of\ndata by selecting a few orthonormal vectors that explain most of the variance\nstructure of the data. L1 PCA uses the L1 norm to measure error, whereas the\nconventional PCA uses the L2 norm. For the L1 PCA problem minimizing the\nfitting error of the reconstructed data, we propose an exact reweighted and an\napproximate algorithm based on iteratively reweighted least squares. We provide\nconvergence analyses, and compare their performance against benchmark\nalgorithms in the literature. The computational experiment shows that the\nproposed algorithms consistently perform best.\n", "versions": [{"version": "v1", "created": "Sat, 10 Sep 2016 04:06:07 GMT"}, {"version": "v2", "created": "Mon, 19 Sep 2016 21:22:38 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Park", "Young Woong", ""], ["Klabjan", "Diego", ""]]}, {"id": "1609.03126", "submitter": "Junbo Zhao", "authors": "Junbo Zhao, Michael Mathieu and Yann LeCun", "title": "Energy-based Generative Adversarial Network", "comments": "Submitted to ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the \"Energy-based Generative Adversarial Network\" model (EBGAN)\nwhich views the discriminator as an energy function that attributes low\nenergies to the regions near the data manifold and higher energies to other\nregions. Similar to the probabilistic GANs, a generator is seen as being\ntrained to produce contrastive samples with minimal energies, while the\ndiscriminator is trained to assign high energies to these generated samples.\nViewing the discriminator as an energy function allows to use a wide variety of\narchitectures and loss functionals in addition to the usual binary classifier\nwith logistic output. Among them, we show one instantiation of EBGAN framework\nas using an auto-encoder architecture, with the energy being the reconstruction\nerror, in place of the discriminator. We show that this form of EBGAN exhibits\nmore stable behavior than regular GANs during training. We also show that a\nsingle-scale architecture can be trained to generate high-resolution images.\n", "versions": [{"version": "v1", "created": "Sun, 11 Sep 2016 07:11:13 GMT"}, {"version": "v2", "created": "Wed, 14 Sep 2016 18:07:30 GMT"}, {"version": "v3", "created": "Fri, 4 Nov 2016 20:04:33 GMT"}, {"version": "v4", "created": "Mon, 6 Mar 2017 22:52:53 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Zhao", "Junbo", ""], ["Mathieu", "Michael", ""], ["LeCun", "Yann", ""]]}, {"id": "1609.03164", "submitter": "Steven Van Vaerenbergh", "authors": "Steven Van Vaerenbergh, Jesus Fernandez-Bes, V\\'ictor Elvira", "title": "On the Relationship between Online Gaussian Process Regression and\n  Kernel Least Mean Squares Algorithms", "comments": "Accepted for publication in 2016 IEEE International Workshop on\n  Machine Learning for Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the relationship between online Gaussian process (GP) regression and\nkernel least mean squares (KLMS) algorithms. While the latter have no capacity\nof storing the entire posterior distribution during online learning, we\ndiscover that their operation corresponds to the assumption of a fixed\nposterior covariance that follows a simple parametric model. Interestingly,\nseveral well-known KLMS algorithms correspond to specific cases of this model.\nThe probabilistic perspective allows us to understand how each of them handles\nuncertainty, which could explain some of their performance differences.\n", "versions": [{"version": "v1", "created": "Sun, 11 Sep 2016 14:17:33 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Van Vaerenbergh", "Steven", ""], ["Fernandez-Bes", "Jesus", ""], ["Elvira", "V\u00edctor", ""]]}, {"id": "1609.03219", "submitter": "Shinichi Nakajima", "authors": "Wikor Pronobis, Danny Panknin, Johannes Kirschnick, Vignesh\n  Srinivasan, Wojciech Samek, Volker Markl, Manohar Kaul, Klaus-Robert Mueller,\n  Shinichi Nakajima", "title": "Sharing Hash Codes for Multiple Purposes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locality sensitive hashing (LSH) is a powerful tool for sublinear-time\napproximate nearest neighbor search, and a variety of hashing schemes have been\nproposed for different dissimilarity measures. However, hash codes\nsignificantly depend on the dissimilarity, which prohibits users from adjusting\nthe dissimilarity at query time. In this paper, we propose {multiple purpose\nLSH (mp-LSH) which shares the hash codes for different dissimilarities. mp-LSH\nsupports L2, cosine, and inner product dissimilarities, and their corresponding\nweighted sums, where the weights can be adjusted at query time. It also allows\nus to modify the importance of pre-defined groups of features. Thus, mp-LSH\nenables us, for example, to retrieve similar items to a query with the user\npreference taken into account, to find a similar material to a query with some\nproperties (stability, utility, etc.) optimized, and to turn on or off a part\nof multi-modal information (brightness, color, audio, text, etc.) in\nimage/video retrieval. We theoretically and empirically analyze the performance\nof three variants of mp-LSH, and demonstrate their usefulness on real-world\ndata sets.\n", "versions": [{"version": "v1", "created": "Sun, 11 Sep 2016 21:55:07 GMT"}, {"version": "v2", "created": "Thu, 15 Sep 2016 22:40:23 GMT"}, {"version": "v3", "created": "Thu, 1 Jun 2017 14:53:12 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Pronobis", "Wikor", ""], ["Panknin", "Danny", ""], ["Kirschnick", "Johannes", ""], ["Srinivasan", "Vignesh", ""], ["Samek", "Wojciech", ""], ["Markl", "Volker", ""], ["Kaul", "Manohar", ""], ["Mueller", "Klaus-Robert", ""], ["Nakajima", "Shinichi", ""]]}, {"id": "1609.03228", "submitter": "Eric Lock", "authors": "Eric F. Lock and Gen Li", "title": "Supervised multiway factorization", "comments": "31 pages, 6 figures, 7 tables", "journal-ref": "Electronic Journal of Statistics 2018, Vol. 12, No. 1, 1150-1180", "doi": "10.1214/18-EJS1421", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a probabilistic PARAFAC/CANDECOMP (CP) factorization for multiway\n(i.e., tensor) data that incorporates auxiliary covariates, SupCP. SupCP\ngeneralizes the supervised singular value decomposition (SupSVD) for\nvector-valued observations, to allow for observations that have the form of a\nmatrix or higher-order array. Such data are increasingly encountered in\nbiomedical research and other fields. We describe a likelihood-based latent\nvariable representation of the CP factorization, in which the latent variables\nare informed by additional covariates. We give conditions for identifiability,\nand develop an EM algorithm for simultaneous estimation of all model\nparameters. SupCP can be used for dimension reduction, capturing latent\nstructures that are more accurate and interpretable due to covariate\nsupervision. Moreover, SupCP specifies a full probability distribution for a\nmultiway data observation with given covariate values, which can be used for\npredictive modeling. We conduct comprehensive simulations to evaluate the SupCP\nalgorithm. We apply it to a facial image database with facial descriptors\n(e.g., smiling / not smiling) as covariates, and to a study of amino acid\nfluorescence. Software is available at https://github.com/lockEF/SupCP .\n", "versions": [{"version": "v1", "created": "Sun, 11 Sep 2016 23:12:54 GMT"}, {"version": "v2", "created": "Sun, 1 Apr 2018 02:04:36 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Lock", "Eric F.", ""], ["Li", "Gen", ""]]}, {"id": "1609.03240", "submitter": "Anastasios Kyrillidis", "authors": "Dohyung Park, Anastasios Kyrillidis, Constantine Caramanis, Sujay\n  Sanghavi", "title": "Non-square matrix sensing without spurious local minima via the\n  Burer-Monteiro approach", "comments": "14 pages, no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the non-square matrix sensing problem, under restricted isometry\nproperty (RIP) assumptions. We focus on the non-convex formulation, where any\nrank-$r$ matrix $X \\in \\mathbb{R}^{m \\times n}$ is represented as $UV^\\top$,\nwhere $U \\in \\mathbb{R}^{m \\times r}$ and $V \\in \\mathbb{R}^{n \\times r}$. In\nthis paper, we complement recent findings on the non-convex geometry of the\nanalogous PSD setting [5], and show that matrix factorization does not\nintroduce any spurious local minima, under RIP.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 01:20:45 GMT"}, {"version": "v2", "created": "Tue, 27 Sep 2016 00:57:44 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Park", "Dohyung", ""], ["Kyrillidis", "Anastasios", ""], ["Caramanis", "Constantine", ""], ["Sanghavi", "Sujay", ""]]}, {"id": "1609.03261", "submitter": "Lihua Lei", "authors": "Lihua Lei and Michael I. Jordan", "title": "Less than a Single Pass: Stochastically Controlled Stochastic Gradient\n  Method", "comments": "Add Lemma B.4", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop and analyze a procedure for gradient-based optimization that we\nrefer to as stochastically controlled stochastic gradient (SCSG). As a member\nof the SVRG family of algorithms, SCSG makes use of gradient estimates at two\nscales, with the number of updates at the faster scale being governed by a\ngeometric random variable. Unlike most existing algorithms in this family, both\nthe computation cost and the communication cost of SCSG do not necessarily\nscale linearly with the sample size $n$; indeed, these costs are independent of\n$n$ when the target accuracy is low. An experimental evaluation on real\ndatasets confirms the effectiveness of SCSG.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 03:35:29 GMT"}, {"version": "v2", "created": "Mon, 3 Jul 2017 00:25:50 GMT"}, {"version": "v3", "created": "Thu, 16 May 2019 04:18:36 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Lei", "Lihua", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1609.03319", "submitter": "Nishant Mehta", "authors": "Nishant A. Mehta and Alistair Rendell and Anish Varghese and\n  Christfried Webers", "title": "CompAdaGrad: A Compressed, Complementary, Computationally-Efficient\n  Adaptive Gradient Method", "comments": "only updated acknowledgements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The adaptive gradient online learning method known as AdaGrad has seen\nwidespread use in the machine learning community in stochastic and adversarial\nonline learning problems and more recently in deep learning methods. The\nmethod's full-matrix incarnation offers much better theoretical guarantees and\npotentially better empirical performance than its diagonal version; however,\nthis version is computationally prohibitive and so the simpler diagonal version\noften is used in practice. We introduce a new method, CompAdaGrad, that\nnavigates the space between these two schemes and show that this method can\nyield results much better than diagonal AdaGrad while avoiding the (effectively\nintractable) $O(n^3)$ computational complexity of full-matrix AdaGrad for\ndimension $n$. CompAdaGrad essentially performs full-matrix regularization in a\nlow-dimensional subspace while performing diagonal regularization in the\ncomplementary subspace. We derive CompAdaGrad's updates for composite mirror\ndescent in case of the squared $\\ell_2$ norm and the $\\ell_1$ norm, demonstrate\nthat its complexity per iteration is linear in the dimension, and establish\nguarantees for the method independent of the choice of composite regularizer.\nFinally, we show preliminary results on several datasets.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 09:06:44 GMT"}, {"version": "v2", "created": "Tue, 4 Oct 2016 13:03:21 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Mehta", "Nishant A.", ""], ["Rendell", "Alistair", ""], ["Varghese", "Anish", ""], ["Webers", "Christfried", ""]]}, {"id": "1609.03333", "submitter": "Niek Tax", "authors": "Niek Tax, Emin Alasgarov, Natalia Sidorova, Reinder Haakma", "title": "On Generation of Time-based Label Refinements", "comments": "Accepted at CS&P workshop 2016 Overlap in preliminaries with\n  arXiv:1606.07259", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process mining is a research field focused on the analysis of event data with\nthe aim of extracting insights in processes. Applying process mining techniques\non data from smart home environments has the potential to provide valuable\ninsights in (un)healthy habits and to contribute to ambient assisted living\nsolutions. Finding the right event labels to enable application of process\nmining techniques is however far from trivial, as simply using the triggering\nsensor as the label for sensor events results in uninformative models that\nallow for too much behavior (overgeneralizing). Refinements of sensor level\nevent labels suggested by domain experts have shown to enable discovery of more\nprecise and insightful process models. However, there exist no automated\napproach to generate refinements of event labels in the context of process\nmining. In this paper we propose a framework for automated generation of label\nrefinements based on the time attribute of events. We show on a case study with\nreal life smart home event data that behaviorally more specific, and therefore\nmore insightful, process models can be found by using automatically generated\nrefined labels in process discovery.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 10:25:29 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Tax", "Niek", ""], ["Alasgarov", "Emin", ""], ["Sidorova", "Natalia", ""], ["Haakma", "Reinder", ""]]}, {"id": "1609.03344", "submitter": "Ning Xu", "authors": "Ning Xu, Jian Hong, Timothy C.G. Fisher", "title": "Finite-sample and asymptotic analysis of generalization ability with an\n  application to penalized regression", "comments": "The theoretical generalization and extension of arXiv:1606.00142", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST q-fin.EC stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the performance of extremum estimators from the\nperspective of generalization ability (GA): the ability of a model to predict\noutcomes in new samples from the same population. By adapting the classical\nconcentration inequalities, we derive upper bounds on the empirical\nout-of-sample prediction errors as a function of the in-sample errors,\nin-sample data size, heaviness in the tails of the error distribution, and\nmodel complexity. We show that the error bounds may be used for tuning key\nestimation hyper-parameters, such as the number of folds $K$ in\ncross-validation. We also show how $K$ affects the bias-variance trade-off for\ncross-validation. We demonstrate that the $\\mathcal{L}_2$-norm difference\nbetween penalized and the corresponding un-penalized regression estimates is\ndirectly explained by the GA of the estimates and the GA of empirical moment\nconditions. Lastly, we prove that all penalized regression estimates are\n$L_2$-consistent for both the $n \\geqslant p$ and the $n < p$ cases.\nSimulations are used to demonstrate key results.\n  Keywords: generalization ability, upper bound of generalization error,\npenalized regression, cross-validation, bias-variance trade-off,\n$\\mathcal{L}_2$ difference between penalized and unpenalized regression, lasso,\nhigh-dimensional data.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 11:09:50 GMT"}, {"version": "v2", "created": "Tue, 13 Sep 2016 09:34:17 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Xu", "Ning", ""], ["Hong", "Jian", ""], ["Fisher", "Timothy C. G.", ""]]}, {"id": "1609.03519", "submitter": "Yuval Harel", "authors": "Yuval Harel, Ron Meir, Manfred Opper", "title": "Optimal Encoding and Decoding for Point Process Observations: an\n  Approximate Closed-Form Filter", "comments": "arXiv admin note: text overlap with arXiv:1507.07813", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The process of dynamic state estimation (filtering) based on point process\nobservations is in general intractable. Numerical sampling techniques are often\npractically useful, but lead to limited conceptual insight about optimal\nencoding/decoding strategies, which are of significant relevance to\nComputational Neuroscience. We develop an analytically tractable Bayesian\napproximation to optimal filtering based on point process observations, which\nallows us to introduce distributional assumptions about sensor properties, that\ngreatly facilitate the analysis of optimal encoding in situations deviating\nfrom common assumptions of uniform coding. Numerical comparison with particle\nfiltering demonstrate the quality of the approximation. The analytic framework\nleads to insights which are difficult to obtain from numerical algorithms, and\nis consistent with biological observations about the distribution of sensory\ncells' tuning curve centers.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 18:33:50 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Harel", "Yuval", ""], ["Meir", "Ron", ""], ["Opper", "Manfred", ""]]}, {"id": "1609.03541", "submitter": "David Schwab", "authors": "David J. Schwab, Pankaj Mehta", "title": "Comment on \"Why does deep and cheap learning work so well?\"\n  [arXiv:1608.08225]", "comments": "Comment on arXiv:1608.08225", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.dis-nn cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent paper, \"Why does deep and cheap learning work so well?\", Lin and\nTegmark claim to show that the mapping between deep belief networks and the\nvariational renormalization group derived in [arXiv:1410.3831] is invalid, and\npresent a \"counterexample\" that claims to show that this mapping does not hold.\nIn this comment, we show that these claims are incorrect and stem from a\nmisunderstanding of the variational RG procedure proposed by Kadanoff. We also\nexplain why the \"counterexample\" of Lin and Tegmark is compatible with the\nmapping proposed in [arXiv:1410.3831].\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 19:25:27 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Schwab", "David J.", ""], ["Mehta", "Pankaj", ""]]}, {"id": "1609.03544", "submitter": "Xin Jiang", "authors": "Xin Jiang, Rebecca Willett", "title": "Online Data Thinning via Multi-Subspace Tracking", "comments": "32 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an era of ubiquitous large-scale streaming data, the availability of data\nfar exceeds the capacity of expert human analysts. In many settings, such data\nis either discarded or stored unprocessed in datacenters. This paper proposes a\nmethod of online data thinning, in which large-scale streaming datasets are\nwinnowed to preserve unique, anomalous, or salient elements for timely expert\nanalysis. At the heart of this proposed approach is an online anomaly detection\nmethod based on dynamic, low-rank Gaussian mixture models. Specifically, the\nhigh-dimensional covariances matrices associated with the Gaussian components\nare associated with low-rank models. According to this model, most observations\nlie near a union of subspaces. The low-rank modeling mitigates the curse of\ndimensionality associated with anomaly detection for high-dimensional data, and\nrecent advances in subspace clustering and subspace tracking allow the proposed\nmethod to adapt to dynamic environments. Furthermore, the proposed method\nallows subsampling, is robust to missing data, and uses a mini-batch online\noptimization approach. The resulting algorithms are scalable, efficient, and\nare capable of operating in real time. Experiments on wide-area motion imagery\nand e-mail databases illustrate the efficacy of the proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 19:34:02 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Jiang", "Xin", ""], ["Willett", "Rebecca", ""]]}, {"id": "1609.03666", "submitter": "S\\'ebastien Arnold", "authors": "S\\'ebastien Arnold", "title": "A Greedy Algorithm to Cluster Specialists", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several recent deep neural networks experiments leverage the\ngeneralist-specialist paradigm for classification. However, no formal study\ncompared the performance of different clustering algorithms for class\nassignment. In this paper we perform such a study, suggest slight modifications\nto the clustering procedures, and propose a novel algorithm designed to\noptimize the performance of of the specialist-generalist classification system.\nOur experiments on the CIFAR-10 and CIFAR-100 datasets allow us to investigate\nsituations for varying number of classes on similar data. We find that our\n\\emph{greedy pairs} clustering algorithm consistently outperforms other\nalternatives, while the choice of the confusion matrix has little impact on the\nfinal performance.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 03:26:42 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Arnold", "S\u00e9bastien", ""]]}, {"id": "1609.03677", "submitter": "Cl\\'ement Godard", "authors": "Cl\\'ement Godard, Oisin Mac Aodha and Gabriel J. Brostow", "title": "Unsupervised Monocular Depth Estimation with Left-Right Consistency", "comments": "CVPR 2017 oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning based methods have shown very promising results for the task of\ndepth estimation in single images. However, most existing approaches treat\ndepth prediction as a supervised regression problem and as a result, require\nvast quantities of corresponding ground truth depth data for training. Just\nrecording quality depth data in a range of environments is a challenging\nproblem. In this paper, we innovate beyond existing approaches, replacing the\nuse of explicit depth data during training with easier-to-obtain binocular\nstereo footage.\n  We propose a novel training objective that enables our convolutional neural\nnetwork to learn to perform single image depth estimation, despite the absence\nof ground truth depth data. Exploiting epipolar geometry constraints, we\ngenerate disparity images by training our network with an image reconstruction\nloss. We show that solving for image reconstruction alone results in poor\nquality depth images. To overcome this problem, we propose a novel training\nloss that enforces consistency between the disparities produced relative to\nboth the left and right images, leading to improved performance and robustness\ncompared to existing approaches. Our method produces state of the art results\nfor monocular depth estimation on the KITTI driving dataset, even outperforming\nsupervised methods that have been trained with ground truth depth.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 04:48:31 GMT"}, {"version": "v2", "created": "Fri, 9 Dec 2016 17:18:17 GMT"}, {"version": "v3", "created": "Wed, 12 Apr 2017 14:40:50 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Godard", "Cl\u00e9ment", ""], ["Mac Aodha", "Oisin", ""], ["Brostow", "Gabriel J.", ""]]}, {"id": "1609.03683", "submitter": "Giorgio Patrini", "authors": "Giorgio Patrini, Alessandro Rozza, Aditya Menon, Richard Nock, Lizhen\n  Qu", "title": "Making Deep Neural Networks Robust to Label Noise: a Loss Correction\n  Approach", "comments": "Oral paper at CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a theoretically grounded approach to train deep neural networks,\nincluding recurrent networks, subject to class-dependent label noise. We\npropose two procedures for loss correction that are agnostic to both\napplication domain and network architecture. They simply amount to at most a\nmatrix inversion and multiplication, provided that we know the probability of\neach class being corrupted into another. We further show how one can estimate\nthese probabilities, adapting a recent technique for noise estimation to the\nmulti-class setting, and thus providing an end-to-end framework. Extensive\nexperiments on MNIST, IMDB, CIFAR-10, CIFAR-100 and a large scale dataset of\nclothing images employing a diversity of architectures --- stacking dense,\nconvolutional, pooling, dropout, batch normalization, word embedding, LSTM and\nresidual layers --- demonstrate the noise robustness of our proposals.\nIncidentally, we also prove that, when ReLU is the only non-linearity, the loss\ncurvature is immune to class-dependent label noise.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 05:23:29 GMT"}, {"version": "v2", "created": "Wed, 22 Mar 2017 08:48:02 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Patrini", "Giorgio", ""], ["Rozza", "Alessandro", ""], ["Menon", "Aditya", ""], ["Nock", "Richard", ""], ["Qu", "Lizhen", ""]]}, {"id": "1609.03769", "submitter": "Daniele Calandriello", "authors": "Daniele Calandriello, Alessandro Lazaric and Michal Valko", "title": "Analysis of Kelner and Levin graph sparsification algorithm for a\n  streaming setting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive a new proof to show that the incremental resparsification algorithm\nproposed by Kelner and Levin (2013) produces a spectral sparsifier in high\nprobability. We rigorously take into account the dependencies across subsequent\nresparsifications using martingale inequalities, fixing a flaw in the original\nanalysis.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 11:18:03 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Calandriello", "Daniele", ""], ["Lazaric", "Alessandro", ""], ["Valko", "Michal", ""]]}, {"id": "1609.03772", "submitter": "Nguyen Tran Quang", "authors": "Nguyen Tran Quang and Alexander Jung", "title": "Learning conditional independence structure for high-dimensional\n  uncorrelated vector processes", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate and analyze a graphical model selection method for inferring the\nconditional independence graph of a high-dimensional nonstationary Gaussian\nrandom process (time series) from a finite-length observation. The observed\nprocess samples are assumed uncorrelated over time and having a time-varying\nmarginal distribution. The selection method is based on testing conditional\nvariances obtained for small subsets of process components. This allows to cope\nwith the high-dimensional regime, where the sample size can be (drastically)\nsmaller than the process dimension. We characterize the required sample size\nsuch that the proposed selection method is successful with high probability.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 11:35:12 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Quang", "Nguyen Tran", ""], ["Jung", "Alexander", ""]]}, {"id": "1609.03912", "submitter": "Kevin Moon", "authors": "Kevin R. Moon, Morteza Noshad, Salimeh Yasaei Sekeh, Alfred O. Hero\n  III", "title": "Information Theoretic Structure Learning with Confidence", "comments": "10 pages, 3 figures", "journal-ref": "IEEE International Conference on Acoustics, Speech, and Signal\n  Processing (ICASSP), pp. 6095-6099, Mar. 2017", "doi": "10.1109/ICASSP.2017.7953327", "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information theoretic measures (e.g. the Kullback Liebler divergence and\nShannon mutual information) have been used for exploring possibly nonlinear\nmultivariate dependencies in high dimension. If these dependencies are assumed\nto follow a Markov factor graph model, this exploration process is called\nstructure discovery. For discrete-valued samples, estimates of the information\ndivergence over the parametric class of multinomial models lead to structure\ndiscovery methods whose mean squared error achieves parametric convergence\nrates as the sample size grows. However, a naive application of this method to\ncontinuous nonparametric multivariate models converges much more slowly. In\nthis paper we introduce a new method for nonparametric structure discovery that\nuses weighted ensemble divergence estimators that achieve parametric\nconvergence rates and obey an asymptotic central limit theorem that facilitates\nhypothesis testing and other types of statistical validation.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 16:20:02 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Moon", "Kevin R.", ""], ["Noshad", "Morteza", ""], ["Sekeh", "Salimeh Yasaei", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1609.03932", "submitter": "Michael Mahoney", "authors": "David Lawlor and Tam\\'as Budav\\'ari and Michael W. Mahoney", "title": "Mapping the Similarities of Spectra: Global and Locally-biased\n  Approaches to SDSS Galaxy Data", "comments": "34 pages. A modified version of this paper has been accepted to The\n  Astrophysical Journal", "journal-ref": null, "doi": "10.3847/0004-637X/833/1/26", "report-no": null, "categories": "astro-ph.IM astro-ph.CO cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply a novel spectral graph technique, that of locally-biased\nsemi-supervised eigenvectors, to study the diversity of galaxies. This\ntechnique permits us to characterize empirically the natural variations in\nobserved spectra data, and we illustrate how this approach can be used in an\nexploratory manner to highlight both large-scale global as well as small-scale\nlocal structure in Sloan Digital Sky Survey (SDSS) data. We use this method in\na way that simultaneously takes into account the measurements of spectral lines\nas well as the continuum shape. Unlike Principal Component Analysis, this\nmethod does not assume that the Euclidean distance between galaxy spectra is a\ngood global measure of similarity between all spectra, but instead it only\nassumes that local difference information between similar spectra is reliable.\nMoreover, unlike other nonlinear dimensionality methods, this method can be\nused to characterize very finely both small-scale local as well as large-scale\nglobal properties of realistic noisy data. The power of the method is\ndemonstrated on the SDSS Main Galaxy Sample by illustrating that the derived\nembeddings of spectra carry an unprecedented amount of information. By using a\nstraightforward global or unsupervised variant, we observe that the main\nfeatures correlate strongly with star formation rate and that they clearly\nseparate active galactic nuclei. Computed parameters of the method can be used\nto describe line strengths and their interdependencies. By using a\nlocally-biased or semi-supervised variant, we are able to focus on typical\nvariations around specific objects of astronomical interest. We present several\nexamples illustrating that this approach can enable new discoveries in the data\nas well as a detailed understanding of very fine local structure that would\notherwise be overwhelmed by large-scale noise and global trends in the data.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 16:46:51 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Lawlor", "David", ""], ["Budav\u00e1ri", "Tam\u00e1s", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1609.03958", "submitter": "Akshay Soni", "authors": "Akshay Soni, Troy Chevalier, Swayambhoo Jain", "title": "Noisy Inductive Matrix Completion Under Sparse Factor Models", "comments": "5 pages. arXiv admin note: text overlap with arXiv:1411.0282", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inductive Matrix Completion (IMC) is an important class of matrix completion\nproblems that allows direct inclusion of available features to enhance\nestimation capabilities. These models have found applications in personalized\nrecommendation systems, multilabel learning, dictionary learning, etc. This\npaper examines a general class of noisy matrix completion tasks where the\nunderlying matrix is following an IMC model i.e., it is formed by a mixing\nmatrix (a priori unknown) sandwiched between two known feature matrices. The\nmixing matrix here is assumed to be well approximated by the product of two\nsparse matrices---referred here to as \"sparse factor models.\" We leverage the\nmain theorem of Soni:2016:NMC and extend it to provide theoretical error bounds\nfor the sparsity-regularized maximum likelihood estimators for the class of\nproblems discussed in this paper. The main result is general in the sense that\nit can be used to derive error bounds for various noise models. In this paper,\nwe instantiate our main result for the case of Gaussian noise and provide\ncorresponding error bounds in terms of squared loss.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 18:08:06 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Soni", "Akshay", ""], ["Chevalier", "Troy", ""], ["Jain", "Swayambhoo", ""]]}, {"id": "1609.03960", "submitter": "Chu Wang", "authors": "Bernard Chazelle, Chu Wang", "title": "Self-Sustaining Iterated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important result from psycholinguistics (Griffiths & Kalish, 2005) states\nthat no language can be learned iteratively by rational agents in a\nself-sustaining manner. We show how to modify the learning process slightly in\norder to achieve self-sustainability. Our work is in two parts. First, we\ncharacterize iterated learnability in geometric terms and show how a slight,\nsteady increase in the lengths of the training sessions ensures\nself-sustainability for any discrete language class. In the second part, we\ntackle the nondiscrete case and investigate self-sustainability for iterated\nlinear regression. We discuss the implications of our findings to issues of\nnon-equilibrium dynamics in natural algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 18:18:38 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Chazelle", "Bernard", ""], ["Wang", "Chu", ""]]}, {"id": "1609.04120", "submitter": "Mijung Park", "authors": "Mijung Park, James Foulds, Kamalika Chaudhuri, Max Welling", "title": "Private Topic Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a privatised stochastic variational inference method for Latent\nDirichlet Allocation (LDA). The iterative nature of stochastic variational\ninference presents challenges: multiple iterations are required to obtain\naccurate posterior distributions, yet each iteration increases the amount of\nnoise that must be added to achieve a reasonable degree of privacy. We propose\na practical algorithm that overcomes this challenge by combining: (1) an\nimproved composition method for differential privacy, called the moments\naccountant, which provides a tight bound on the privacy cost of multiple\nvariational inference iterations and thus significantly decreases the amount of\nadditive noise; and (2) privacy amplification resulting from subsampling of\nlarge-scale data. Focusing on conjugate exponential family models, in our\nprivate variational inference, all the posterior distributions will be\nprivatised by simply perturbing expected sufficient statistics. Using Wikipedia\ndata, we illustrate the effectiveness of our algorithm for large-scale data.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 03:18:36 GMT"}, {"version": "v2", "created": "Mon, 28 Nov 2016 20:56:45 GMT"}, {"version": "v3", "created": "Mon, 3 Dec 2018 19:58:47 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Park", "Mijung", ""], ["Foulds", "James", ""], ["Chaudhuri", "Kamalika", ""], ["Welling", "Max", ""]]}, {"id": "1609.04228", "submitter": "Sebastien Gadat", "authors": "S\\'ebastien Gadat and Fabien Panloup and Sofiane Saadane", "title": "Stochastic Heavy Ball", "comments": "39 pages, 3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with a natural stochastic optimization procedure derived\nfrom the so-called Heavy-ball method differential equation, which was\nintroduced by Polyak in the 1960s with his seminal contribution [Pol64]. The\nHeavy-ball method is a second-order dynamics that was investigated to minimize\nconvex functions f . The family of second-order methods recently received a\nlarge amount of attention, until the famous contribution of Nesterov [Nes83],\nleading to the explosion of large-scale optimization problems. This work\nprovides an in-depth description of the stochastic heavy-ball method, which is\nan adaptation of the deterministic one when only unbiased evalutions of the\ngradient are available and used throughout the iterations of the algorithm. We\nfirst describe some almost sure convergence results in the case of general\nnon-convex coercive functions f . We then examine the situation of convex and\nstrongly convex potentials and derive some non-asymptotic results about the\nstochastic heavy-ball method. We end our study with limit theorems on several\nrescaled algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 11:54:04 GMT"}, {"version": "v2", "created": "Fri, 21 Oct 2016 12:19:06 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Gadat", "S\u00e9bastien", ""], ["Panloup", "Fabien", ""], ["Saadane", "Sofiane", ""]]}, {"id": "1609.04289", "submitter": "Edwin Bonilla", "authors": "Pietro Galliani and Amir Dezfouli and Edwin V. Bonilla and Novi\n  Quadrianto", "title": "Gray-box inference for structured Gaussian process models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an automated variational inference method for Bayesian structured\nprediction problems with Gaussian process (GP) priors and linear-chain\nlikelihoods. Our approach does not need to know the details of the structured\nlikelihood model and can scale up to a large number of observations.\nFurthermore, we show that the required expected likelihood term and its\ngradients in the variational objective (ELBO) can be estimated efficiently by\nusing expectations over very low-dimensional Gaussian distributions.\nOptimization of the ELBO is fully parallelizable over sequences and amenable to\nstochastic optimization, which we use along with control variate techniques and\nstate-of-the-art incremental optimization to make our framework useful in\npractice. Results on a set of natural language processing tasks show that our\nmethod can be as good as (and sometimes better than) hard-coded approaches\nincluding SVM-struct and CRFs, and overcomes the scalability limitations of\nprevious inference algorithms based on sampling. Overall, this is a fundamental\nstep to developing automated inference methods for Bayesian structured\nprediction.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 14:27:32 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Galliani", "Pietro", ""], ["Dezfouli", "Amir", ""], ["Bonilla", "Edwin V.", ""], ["Quadrianto", "Novi", ""]]}, {"id": "1609.04301", "submitter": "Herv\\'e Bredin", "authors": "Herv\\'e Bredin", "title": "TristouNet: Triplet Loss for Speaker Turn Embedding", "comments": "ICASSP 2017 (42nd IEEE International Conference on Acoustics, Speech\n  and Signal Processing). Code available at\n  http://github.com/hbredin/TristouNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  TristouNet is a neural network architecture based on Long Short-Term Memory\nrecurrent networks, meant to project speech sequences into a fixed-dimensional\neuclidean space. Thanks to the triplet loss paradigm used for training, the\nresulting sequence embeddings can be compared directly with the euclidean\ndistance, for speaker comparison purposes. Experiments on short (between 500ms\nand 5s) speech turn comparison and speaker change detection show that\nTristouNet brings significant improvements over the current state-of-the-art\ntechniques for both tasks.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 14:39:36 GMT"}, {"version": "v2", "created": "Tue, 27 Sep 2016 15:48:37 GMT"}, {"version": "v3", "created": "Tue, 11 Apr 2017 15:15:20 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Bredin", "Herv\u00e9", ""]]}, {"id": "1609.04321", "submitter": "Luca Masera", "authors": "Luca Masera, Enrico Blanzieri", "title": "Very Simple Classifier: a Concept Binary Classifier toInvestigate\n  Features Based on Subsampling and Localility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Very Simple Classifier (VSC) a novel method designed to\nincorporate the concepts of subsampling and locality in the definition of\nfeatures to be used as the input of a perceptron. The rationale is that\nlocality theoretically guarantees a bound on the generalization error. Each\nfeature in VSC is a max-margin classifier built on randomly-selected pairs of\nsamples. The locality in VSC is achieved by multiplying the value of the\nfeature by a confidence measure that can be characterized in terms of the\nChebichev inequality. The output of the layer is then fed in a output layer of\nneurons. The weights of the output layer are then determined by a regularized\npseudoinverse. Extensive comparison of VSC against 9 competitors in the task of\nbinary classification is carried out. Results on 22 benchmark datasets with\nfixed parameters show that VSC is competitive with the Multi Layer Perceptron\n(MLP) and outperforms the other competitors. An exploration of the parameter\nspace shows VSC can outperform MLP.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 15:51:46 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Masera", "Luca", ""], ["Blanzieri", "Enrico", ""]]}, {"id": "1609.04388", "submitter": "Sebastian Vollmer", "authors": "Xiaoyu Lu and Valerio Perrone and Leonard Hasenclever and Yee Whye Teh\n  and Sebastian J. Vollmer", "title": "Relativistic Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hamiltonian Monte Carlo (HMC) is a popular Markov chain Monte Carlo (MCMC)\nalgorithm that generates proposals for a Metropolis-Hastings algorithm by\nsimulating the dynamics of a Hamiltonian system. However, HMC is sensitive to\nlarge time discretizations and performs poorly if there is a mismatch between\nthe spatial geometry of the target distribution and the scales of the momentum\ndistribution. In particular the mass matrix of HMC is hard to tune well. In\norder to alleviate these problems we propose relativistic Hamiltonian Monte\nCarlo, a version of HMC based on relativistic dynamics that introduce a maximum\nvelocity on particles. We also derive stochastic gradient versions of the\nalgorithm and show that the resulting algorithms bear interesting relationships\nto gradient clipping, RMSprop, Adagrad and Adam, popular optimisation methods\nin deep learning. Based on this, we develop relativistic stochastic gradient\ndescent by taking the zero-temperature limit of relativistic stochastic\ngradient Hamiltonian Monte Carlo. In experiments we show that the relativistic\nalgorithms perform better than classical Newtonian variants and Adam.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 19:48:34 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Lu", "Xiaoyu", ""], ["Perrone", "Valerio", ""], ["Hasenclever", "Leonard", ""], ["Teh", "Yee Whye", ""], ["Vollmer", "Sebastian J.", ""]]}, {"id": "1609.04436", "submitter": "Mohammad Ghavamzadeh", "authors": "Mohammad Ghavamzadeh, Shie Mannor, Joelle Pineau, and Aviv Tamar", "title": "Bayesian Reinforcement Learning: A Survey", "comments": null, "journal-ref": "Foundations and Trends in Machine Learning, Vol. 8: No. 5-6, pp\n  359-492, 2015", "doi": "10.1561/2200000049", "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian methods for machine learning have been widely investigated, yielding\nprincipled methods for incorporating prior information into inference\nalgorithms. In this survey, we provide an in-depth review of the role of\nBayesian methods for the reinforcement learning (RL) paradigm. The major\nincentives for incorporating Bayesian reasoning in RL are: 1) it provides an\nelegant approach to action-selection (exploration/exploitation) as a function\nof the uncertainty in learning; and 2) it provides a machinery to incorporate\nprior knowledge into the algorithms. We first discuss models and methods for\nBayesian inference in the simple single-step Bandit model. We then review the\nextensive recent literature on Bayesian methods for model-based RL, where prior\ninformation can be expressed on the parameters of the Markov model. We also\npresent Bayesian methods for model-free RL, where priors are expressed over the\nvalue function or policy class. The objective of the paper is to provide a\ncomprehensive survey on Bayesian RL algorithms and their theoretical and\nempirical properties.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 20:34:26 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["Ghavamzadeh", "Mohammad", ""], ["Mannor", "Shie", ""], ["Pineau", "Joelle", ""], ["Tamar", "Aviv", ""]]}, {"id": "1609.04468", "submitter": "Tom White", "authors": "Tom White", "title": "Sampling Generative Networks", "comments": "11 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce several techniques for sampling and visualizing the latent\nspaces of generative models. Replacing linear interpolation with spherical\nlinear interpolation prevents diverging from a model's prior distribution and\nproduces sharper samples. J-Diagrams and MINE grids are introduced as\nvisualizations of manifolds created by analogies and nearest neighbors. We\ndemonstrate two new techniques for deriving attribute vectors: bias-corrected\nvectors with data replication and synthetic vectors with data augmentation.\nBinary classification using attribute vectors is presented as a technique\nsupporting quantitative analysis of the latent space. Most techniques are\nintended to be independent of model type and examples are shown on both\nVariational Autoencoders and Generative Adversarial Networks.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 22:42:23 GMT"}, {"version": "v2", "created": "Fri, 16 Sep 2016 09:38:48 GMT"}, {"version": "v3", "created": "Tue, 6 Dec 2016 14:39:05 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["White", "Tom", ""]]}, {"id": "1609.04508", "submitter": "Trang Pham", "authors": "Trang Pham, Truyen Tran, Dinh Phung, Svetha Venkatesh", "title": "Column Networks for Collective Classification", "comments": "Accepted at AAAI'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational learning deals with data that are characterized by relational\nstructures. An important task is collective classification, which is to jointly\nclassify networked objects. While it holds a great promise to produce a better\naccuracy than non-collective classifiers, collective classification is\ncomputational challenging and has not leveraged on the recent breakthroughs of\ndeep learning. We present Column Network (CLN), a novel deep learning model for\ncollective classification in multi-relational domains. CLN has many desirable\ntheoretical properties: (i) it encodes multi-relations between any two\ninstances; (ii) it is deep and compact, allowing complex functions to be\napproximated at the network level with a small set of free parameters; (iii)\nlocal and relational features are learned simultaneously; (iv) long-range,\nhigher-order dependencies between instances are supported naturally; and (v)\ncrucially, learning and inference are efficient, linear in the size of the\nnetwork and the number of relations. We evaluate CLN on multiple real-world\napplications: (a) delay prediction in software projects, (b) PubMed Diabetes\npublication classification and (c) film genre classification. In all\napplications, CLN demonstrates a higher accuracy than state-of-the-art rivals.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 04:45:11 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 03:59:26 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Pham", "Trang", ""], ["Tran", "Truyen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1609.04522", "submitter": "Will Wei Sun", "authors": "Xiang Lyu, Will Wei Sun, Zhaoran Wang, Han Liu, Jian Yang, Guang Cheng", "title": "Tensor Graphical Model: Non-convex Optimization and Statistical\n  Inference", "comments": "63 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the estimation and inference of graphical models that\ncharacterize the dependency structure of high-dimensional tensor-valued data.\nTo facilitate the estimation of the precision matrix corresponding to each way\nof the tensor, we assume the data follow a tensor normal distribution whose\ncovariance has a Kronecker product structure. A critical challenge in the\nestimation and inference of this model is the fact that its penalized maximum\nlikelihood estimation involves minimizing a non-convex objective function. To\naddress it, this paper makes two contributions: (i) In spite of the\nnon-convexity of this estimation problem, we prove that an alternating\nminimization algorithm, which iteratively estimates each sparse precision\nmatrix while fixing the others, attains an estimator with an optimal\nstatistical rate of convergence. (ii) We propose a de-biased statistical\ninference procedure for testing hypotheses on the true support of the sparse\nprecision matrices, and employ it for testing a growing number of hypothesis\nwith false discovery rate (FDR) control. The asymptotic normality of our test\nstatistic and the consistency of FDR control procedure are established. Our\ntheoretical results are backed up by thorough numerical studies and our real\napplications on neuroimaging studies of Autism spectrum disorder and users'\nadvertising click analysis bring new scientific findings and business insights.\nThe proposed methods are encoded into a publicly available R package Tlasso.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 06:41:11 GMT"}, {"version": "v2", "created": "Mon, 25 Feb 2019 19:47:07 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Lyu", "Xiang", ""], ["Sun", "Will Wei", ""], ["Wang", "Zhaoran", ""], ["Liu", "Han", ""], ["Yang", "Jian", ""], ["Cheng", "Guang", ""]]}, {"id": "1609.04523", "submitter": "Will Wei Sun", "authors": "Will Wei Sun and Lexin Li", "title": "STORE: Sparse Tensor Response Regression and Neuroimaging Analysis", "comments": "42 pages. To appear in Journal of Machine Learning Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications in neuroimaging analysis, we propose a new\nregression model, Sparse TensOr REsponse regression (STORE), with a tensor\nresponse and a vector predictor. STORE embeds two key sparse structures:\nelement-wise sparsity and low-rankness. It can handle both a non-symmetric and\na symmetric tensor response, and thus is applicable to both structural and\nfunctional neuroimaging data. We formulate the parameter estimation as a\nnon-convex optimization problem, and develop an efficient alternating updating\nalgorithm. We establish a non-asymptotic estimation error bound for the actual\nestimator obtained from the proposed algorithm. This error bound reveals an\ninteresting interaction between the computational efficiency and the\nstatistical rate of convergence. When the distribution of the error tensor is\nGaussian, we further obtain a fast estimation error rate which allows the\ntensor dimension to grow exponentially with the sample size. We illustrate the\nefficacy of our model through intensive simulations and an analysis of the\nAutism spectrum disorder neuroimaging data.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 06:51:51 GMT"}, {"version": "v2", "created": "Fri, 20 Jan 2017 03:32:26 GMT"}, {"version": "v3", "created": "Fri, 24 Nov 2017 23:01:54 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Sun", "Will Wei", ""], ["Li", "Lexin", ""]]}, {"id": "1609.04541", "submitter": "Johann Bengua", "authors": "Johann A. Bengua and Ho N. Phien and Hoang D. Tuan and Minh N. Do", "title": "Matrix Product State for Higher-Order Tensor Compression and\n  Classification", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": "10.1109/TSP.2017.2703882", "report-no": null, "categories": "stat.ML cs.CV cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces matrix product state (MPS) decomposition as a new and\nsystematic method to compress multidimensional data represented by higher-order\ntensors. It solves two major bottlenecks in tensor compression: computation and\ncompression quality. Regardless of tensor order, MPS compresses tensors to\nmatrices of moderate dimension which can be used for classification. Mainly\nbased on a successive sequence of singular value decompositions (SVD), MPS is\nquite simple to implement and arrives at the global optimal matrix, bypassing\nlocal alternating optimization, which is not only computationally expensive but\ncannot yield the global solution. Benchmark results show that MPS can achieve\nbetter classification performance with favorable computation cost compared to\nother tensor compression methods.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 09:04:25 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Bengua", "Johann A.", ""], ["Phien", "Ho N.", ""], ["Tuan", "Hoang D.", ""], ["Do", "Minh N.", ""]]}, {"id": "1609.04608", "submitter": "Andres Hoyos Idrobo", "authors": "Andr\\'es Hoyos-Idrobo (PARIETAL, NEUROSPIN), Ga\\\"el Varoquaux\n  (PARIETAL, NEUROSPIN), Jonas Kahn, Bertrand Thirion (PARIETAL)", "title": "Recursive nearest agglomeration (ReNA): fast clustering for\n  approximation of structured signals", "comments": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  Institute of Electrical and Electronics Engineers, In press", "journal-ref": null, "doi": "10.1109/TPAMI.2018.2815524", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we revisit fast dimension reduction approaches, as with random\nprojections and random sampling. Our goal is to summarize the data to decrease\ncomputational costs and memory footprint of subsequent analysis. Such dimension\nreduction can be very efficient when the signals of interest have a strong\nstructure, such as with images. We focus on this setting and investigate\nfeature clustering schemes for data reductions that capture this structure. An\nimpediment to fast dimension reduction is that good clustering comes with large\nalgorithmic costs. We address it by contributing a linear-time agglomerative\nclustering scheme, Recursive Nearest Agglomeration (ReNA). Unlike existing fast\nagglomerative schemes, it avoids the creation of giant clusters. We empirically\nvalidate that it approximates the data as well as traditional\nvariance-minimizing clustering schemes that have a quadratic complexity. In\naddition, we analyze signal approximation with feature clustering and show that\nit can remove noise, improving subsequent analysis steps. As a consequence,\ndata reduction by clustering features with ReNA yields very fast and accurate\nmodels, enabling to process large datasets on budget. Our theoretical analysis\nis backed by extensive experiments on publicly-available data that illustrate\nthe computation efficiency and the denoising properties of the resulting\ndimension reduction scheme.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 12:46:52 GMT"}, {"version": "v2", "created": "Mon, 19 Mar 2018 08:44:46 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Hoyos-Idrobo", "Andr\u00e9s", "", "PARIETAL, NEUROSPIN"], ["Varoquaux", "Ga\u00ebl", "", "PARIETAL, NEUROSPIN"], ["Kahn", "Jonas", "", "PARIETAL"], ["Thirion", "Bertrand", "", "PARIETAL"]]}, {"id": "1609.04623", "submitter": "Marko Angjelichinoski", "authors": "Marko Angjelichinoski, Anna Scaglione, Petar Popovski, Cedomir\n  Stefanovic", "title": "Distributed Estimation of the Operating State of a Single-Bus DC\n  MicroGrid without an External Communication Interface", "comments": "Accepted to GlobalSIP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a decentralized Maximum Likelihood solution for estimating the\nstochastic renewable power generation and demand in single bus Direct Current\n(DC) MicroGrids (MGs), with high penetration of droop controlled power\nelectronic converters. The solution relies on the fact that the primary control\nparameters are set in accordance with the local power generation status of the\ngenerators. Therefore, the steady state voltage is inherently dependent on the\ngeneration capacities and the load, through a non-linear parametric model,\nwhich can be estimated. To have a well conditioned estimation problem, our\nsolution avoids the use of an external communication interface and utilizes\ncontrolled voltage disturbances to perform distributed training. Using this\ntool, we develop an efficient, decentralized Maximum Likelihood Estimator (MLE)\nand formulate the sufficient condition for the existence of the globally\noptimal solution. The numerical results illustrate the promising performance of\nour MLE algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 11:22:32 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["Angjelichinoski", "Marko", ""], ["Scaglione", "Anna", ""], ["Popovski", "Petar", ""], ["Stefanovic", "Cedomir", ""]]}, {"id": "1609.04699", "submitter": "Owen Richfield", "authors": "Owen Richfield, Md. Ashad Alam, Vince Calhoun, Yu-Ping Wang", "title": "Learning Schizophrenia Imaging Genetics Data Via Multiple Kernel\n  Canonical Correlation Analysis", "comments": "arXiv admin note: text overlap with arXiv:1606.00113", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel and Multiple Kernel Canonical Correlation Analysis (CCA) are employed\nto classify schizophrenic and healthy patients based on their SNPs, DNA\nMethylation and fMRI data. Kernel and Multiple Kernel CCA are popular methods\nfor finding nonlinear correlations between high-dimensional datasets. Data was\ngathered from 183 patients, 79 with schizophrenia and 104 healthy controls.\nKernel and Multiple Kernel CCA represent new avenues for studying\nschizophrenia, because, to our knowledge, these methods have not been used on\nthese data before. Classification is performed via k-means clustering on the\nkernel matrix outputs of the Kernel and Multiple Kernel CCA algorithm.\nAccuracies of the Kernel and Multiple Kernel CCA classification are compared to\nthat of the regularized linear CCA algorithm classification, and are found to\nbe significantly more accurate. Both algorithms demonstrate maximal accuracies\nwhen the combination of DNA methylation and fMRI data are used, and experience\nlower accuracies when the SNP data are incorporated.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 15:26:37 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["Richfield", "Owen", ""], ["Alam", "Md. Ashad", ""], ["Calhoun", "Vince", ""], ["Wang", "Yu-Ping", ""]]}, {"id": "1609.04721", "submitter": "Jos\\'e Enrique Chac\\'on", "authors": "Jos\\'e E. Chac\\'on", "title": "Mixture model modal clustering", "comments": "20 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The two most extended density-based approaches to clustering are surely\nmixture model clustering and modal clustering. In the mixture model approach,\nthe density is represented as a mixture and clusters are associated to the\ndifferent mixture components. In modal clustering, clusters are understood as\nregions of high density separated from each other by zones of lower density, so\nthat they are closely related to certain regions around the density modes. If\nthe true density is indeed in the assumed class of mixture densities, then\nmixture model clustering allows to scrutinize more subtle situations than modal\nclustering. However, when mixture modeling is used in a nonparametric way,\ntaking advantage of the denseness of the sieve of mixture densities to\napproximate any density, then the correspondence between clusters and mixture\ncomponents may become questionable. In this paper we introduce two methods to\nadopt a modal clustering point of view after a mixture model fit. Numerous\nexamples are provided to illustrate that mixture modeling can also be used for\nclustering in a nonparametric sense, as long as clusters are understood as the\ndomains of attraction of the density modes.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 16:24:43 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["Chac\u00f3n", "Jos\u00e9 E.", ""]]}, {"id": "1609.04789", "submitter": "Mostafa Rahmani", "authors": "Mostafa Rahmani, George Atia", "title": "Coherence Pursuit: Fast, Simple, and Robust Principal Component Analysis", "comments": null, "journal-ref": "IEEE Transactions on Signal Processing ( Volume: 65, Issue: 23,\n  Dec.1, 1 2017 )", "doi": "10.1109/TSP.2017.2749215", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a remarkably simple, yet powerful, algorithm termed\nCoherence Pursuit (CoP) to robust Principal Component Analysis (PCA). As\ninliers lie in a low dimensional subspace and are mostly correlated, an inlier\nis likely to have strong mutual coherence with a large number of data points.\nBy contrast, outliers either do not admit low dimensional structures or form\nsmall clusters. In either case, an outlier is unlikely to bear strong\nresemblance to a large number of data points. Given that, CoP sets an outlier\napart from an inlier by comparing their coherence with the rest of the data\npoints. The mutual coherences are computed by forming the Gram matrix of the\nnormalized data points. Subsequently, the sought subspace is recovered from the\nspan of the subset of the data points that exhibit strong coherence with the\nrest of the data. As CoP only involves one simple matrix multiplication, it is\nsignificantly faster than the state-of-the-art robust PCA algorithms. We derive\nanalytical performance guarantees for CoP under different models for the\ndistributions of inliers and outliers in both noise-free and noisy settings.\nCoP is the first robust PCA algorithm that is simultaneously non-iterative,\nprovably robust to both unstructured and structured outliers, and can tolerate\na large number of unstructured outliers.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 19:25:55 GMT"}, {"version": "v2", "created": "Sun, 22 Jan 2017 00:13:24 GMT"}, {"version": "v3", "created": "Thu, 13 Jul 2017 01:23:18 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Rahmani", "Mostafa", ""], ["Atia", "George", ""]]}, {"id": "1609.04802", "submitter": "Christian Ledig", "authors": "Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew\n  Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz,\n  Zehan Wang, Wenzhe Shi", "title": "Photo-Realistic Single Image Super-Resolution Using a Generative\n  Adversarial Network", "comments": "19 pages, 15 figures, 2 tables, accepted for oral presentation at\n  CVPR, main paper + some supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the breakthroughs in accuracy and speed of single image\nsuper-resolution using faster and deeper convolutional neural networks, one\ncentral problem remains largely unsolved: how do we recover the finer texture\ndetails when we super-resolve at large upscaling factors? The behavior of\noptimization-based super-resolution methods is principally driven by the choice\nof the objective function. Recent work has largely focused on minimizing the\nmean squared reconstruction error. The resulting estimates have high peak\nsignal-to-noise ratios, but they are often lacking high-frequency details and\nare perceptually unsatisfying in the sense that they fail to match the fidelity\nexpected at the higher resolution. In this paper, we present SRGAN, a\ngenerative adversarial network (GAN) for image super-resolution (SR). To our\nknowledge, it is the first framework capable of inferring photo-realistic\nnatural images for 4x upscaling factors. To achieve this, we propose a\nperceptual loss function which consists of an adversarial loss and a content\nloss. The adversarial loss pushes our solution to the natural image manifold\nusing a discriminator network that is trained to differentiate between the\nsuper-resolved images and original photo-realistic images. In addition, we use\na content loss motivated by perceptual similarity instead of similarity in\npixel space. Our deep residual network is able to recover photo-realistic\ntextures from heavily downsampled images on public benchmarks. An extensive\nmean-opinion-score (MOS) test shows hugely significant gains in perceptual\nquality using SRGAN. The MOS scores obtained with SRGAN are closer to those of\nthe original high-resolution images than to those obtained with any\nstate-of-the-art method.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 19:53:07 GMT"}, {"version": "v2", "created": "Mon, 19 Sep 2016 17:22:44 GMT"}, {"version": "v3", "created": "Mon, 21 Nov 2016 18:30:18 GMT"}, {"version": "v4", "created": "Thu, 13 Apr 2017 14:25:44 GMT"}, {"version": "v5", "created": "Thu, 25 May 2017 11:25:41 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Ledig", "Christian", ""], ["Theis", "Lucas", ""], ["Huszar", "Ferenc", ""], ["Caballero", "Jose", ""], ["Cunningham", "Andrew", ""], ["Acosta", "Alejandro", ""], ["Aitken", "Andrew", ""], ["Tejani", "Alykhan", ""], ["Totz", "Johannes", ""], ["Wang", "Zehan", ""], ["Shi", "Wenzhe", ""]]}, {"id": "1609.04849", "submitter": "Mark Harmon", "authors": "Mark Harmon, Abdolghani Ebrahimi, Patrick Lucey, Diego Klabjan", "title": "Predicting Shot Making in Basketball Learnt from Adversarial Multiagent\n  Trajectories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we predict the likelihood of a player making a shot in\nbasketball from multiagent trajectories. Previous approaches to similar\nproblems center on hand-crafting features to capture domain specific knowledge.\nAlthough intuitive, recent work in deep learning has shown this approach is\nprone to missing important predictive features. To circumvent this issue, we\npresent a convolutional neural network (CNN) approach where we initially\nrepresent the multiagent behavior as an image. To encode the adversarial nature\nof basketball, we use a multi-channel image which we then feed into a CNN.\nAdditionally, to capture the temporal aspect of the trajectories we \"fade\" the\nplayer trajectories. We find that this approach is superior to a traditional\nFFN model. By using gradient ascent to create images using an already trained\nCNN, we discover what features the CNN filters learn. Last, we find that a\ncombined CNN+FFN is the best performing network with an error rate of 39%.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 20:27:26 GMT"}, {"version": "v2", "created": "Mon, 19 Sep 2016 19:30:35 GMT"}, {"version": "v3", "created": "Fri, 24 Feb 2017 22:22:26 GMT"}, {"version": "v4", "created": "Thu, 28 Dec 2017 18:37:06 GMT"}, {"version": "v5", "created": "Fri, 15 Jan 2021 22:56:48 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Harmon", "Mark", ""], ["Ebrahimi", "Abdolghani", ""], ["Lucey", "Patrick", ""], ["Klabjan", "Diego", ""]]}, {"id": "1609.05057", "submitter": "Michael Ying Yang", "authors": "Hanno Ackermann and Michael Ying Yang and Bodo Rosenhahn", "title": "Unbiased Sparse Subspace Clustering By Selective Pursuit", "comments": "Conference on Computer and Robot Vision, 2017 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse subspace clustering (SSC) is an elegant approach for unsupervised\nsegmentation if the data points of each cluster are located in linear\nsubspaces. This model applies, for instance, in motion segmentation if some\nrestrictions on the camera model hold. SSC requires that problems based on the\n$l_1$-norm are solved to infer which points belong to the same subspace. If\nthese unknown subspaces are well-separated this algorithm is guaranteed to\nsucceed. The algorithm rests upon the assumption that points on the same\nsubspace are well spread. The question what happens if this condition is\nviolated has not yet been investigated. In this work, the effect of particular\ndistributions on the same subspace will be analyzed. It will be shown that SSC\nfails to infer correct labels if points on the same subspace fall into more\nthan one cluster.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2016 13:59:01 GMT"}, {"version": "v2", "created": "Fri, 9 Feb 2018 14:34:02 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Ackermann", "Hanno", ""], ["Yang", "Michael Ying", ""], ["Rosenhahn", "Bodo", ""]]}, {"id": "1609.05148", "submitter": "Joshua Vogelstein", "authors": "Joshua T. Vogelstein, Eric Bridgeford, Qing Wang, Carey E. Priebe,\n  Mauro Maggioni, Cencheng Shen", "title": "Discovering and Deciphering Relationships Across Disparate Data\n  Modalities", "comments": null, "journal-ref": "eLife 2019;8:e41690", "doi": "10.7554/eLife.41690", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the relationships between different properties of data, such as\nwhether a connectome or genome has information about disease status, is\nbecoming increasingly important in modern biological datasets. While existing\napproaches can test whether two properties are related, they often require\nunfeasibly large sample sizes in real data scenarios, and do not provide any\ninsight into how or why the procedure reached its decision. Our approach,\n\"Multiscale Graph Correlation\" (MGC), is a dependence test that juxtaposes\npreviously disparate data science techniques, including k-nearest neighbors,\nkernel methods (such as support vector machines), and multiscale analysis (such\nas wavelets). Other methods typically require double or triple the number\nsamples to achieve the same statistical power as MGC in a benchmark suite\nincluding high-dimensional and nonlinear relationships - spanning polynomial\n(linear, quadratic, cubic), trigonometric (sinusoidal, circular, ellipsoidal,\nspiral), geometric (square, diamond, W-shape), and other functions, with\ndimensionality ranging from 1 to 1000. Moreover, MGC uniquely provides a simple\nand elegant characterization of the potentially complex latent geometry\nunderlying the relationship, providing insight while maintaining computational\nefficiency. In several real data applications, including brain imaging and\ncancer genetics, MGC is the only method that can both detect the presence of a\ndependency and provide specific guidance for the next experiment and/or\nanalysis to conduct.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2016 17:29:01 GMT"}, {"version": "v2", "created": "Thu, 9 Mar 2017 14:53:46 GMT"}, {"version": "v3", "created": "Fri, 10 Mar 2017 03:31:20 GMT"}, {"version": "v4", "created": "Thu, 6 Jul 2017 21:00:45 GMT"}, {"version": "v5", "created": "Wed, 21 Mar 2018 22:07:33 GMT"}, {"version": "v6", "created": "Sun, 22 Apr 2018 18:26:09 GMT"}, {"version": "v7", "created": "Tue, 25 Sep 2018 16:29:18 GMT"}, {"version": "v8", "created": "Thu, 6 Dec 2018 07:02:50 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Vogelstein", "Joshua T.", ""], ["Bridgeford", "Eric", ""], ["Wang", "Qing", ""], ["Priebe", "Carey E.", ""], ["Maggioni", "Mauro", ""], ["Shen", "Cencheng", ""]]}, {"id": "1609.05158", "submitter": "Wenzhe Shi", "authors": "Wenzhe Shi, Jose Caballero, Ferenc Husz\\'ar, Johannes Totz, Andrew P.\n  Aitken, Rob Bishop, Daniel Rueckert and Zehan Wang", "title": "Real-Time Single Image and Video Super-Resolution Using an Efficient\n  Sub-Pixel Convolutional Neural Network", "comments": "CVPR 2016 paper with updated affiliations and supplemental material,\n  fixed typo in equation 4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, several models based on deep neural networks have achieved great\nsuccess in terms of both reconstruction accuracy and computational performance\nfor single image super-resolution. In these methods, the low resolution (LR)\ninput image is upscaled to the high resolution (HR) space using a single\nfilter, commonly bicubic interpolation, before reconstruction. This means that\nthe super-resolution (SR) operation is performed in HR space. We demonstrate\nthat this is sub-optimal and adds computational complexity. In this paper, we\npresent the first convolutional neural network (CNN) capable of real-time SR of\n1080p videos on a single K2 GPU. To achieve this, we propose a novel CNN\narchitecture where the feature maps are extracted in the LR space. In addition,\nwe introduce an efficient sub-pixel convolution layer which learns an array of\nupscaling filters to upscale the final LR feature maps into the HR output. By\ndoing so, we effectively replace the handcrafted bicubic filter in the SR\npipeline with more complex upscaling filters specifically trained for each\nfeature map, whilst also reducing the computational complexity of the overall\nSR operation. We evaluate the proposed approach using images and videos from\npublicly available datasets and show that it performs significantly better\n(+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster\nthan previous CNN-based methods.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2016 17:58:14 GMT"}, {"version": "v2", "created": "Fri, 23 Sep 2016 17:16:37 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Shi", "Wenzhe", ""], ["Caballero", "Jose", ""], ["Husz\u00e1r", "Ferenc", ""], ["Totz", "Johannes", ""], ["Aitken", "Andrew P.", ""], ["Bishop", "Rob", ""], ["Rueckert", "Daniel", ""], ["Wang", "Zehan", ""]]}, {"id": "1609.05191", "submitter": "Tengyu Ma", "authors": "Moritz Hardt, Tengyu Ma, Benjamin Recht", "title": "Gradient Descent Learns Linear Dynamical Systems", "comments": "updated with more experimental results and references to prior work;\n  published in JMLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that stochastic gradient descent efficiently converges to the global\noptimizer of the maximum likelihood objective of an unknown linear\ntime-invariant dynamical system from a sequence of noisy observations generated\nby the system. Even though the objective function is non-convex, we provide\npolynomial running time and sample complexity bounds under strong but natural\nassumptions. Linear systems identification has been studied for many decades,\nyet, to the best of our knowledge, these are the first polynomial guarantees\nfor the problem we consider.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2016 19:42:34 GMT"}, {"version": "v2", "created": "Mon, 11 Feb 2019 16:55:24 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Hardt", "Moritz", ""], ["Ma", "Tengyu", ""], ["Recht", "Benjamin", ""]]}, {"id": "1609.05342", "submitter": "Reza Borhani", "authors": "Reza Borhani, Jeremy Watt, Aggelos Katsaggelos", "title": "Fast and Effective Algorithms for Symmetric Nonnegative Matrix\n  Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symmetric Nonnegative Matrix Factorization (SNMF) models arise naturally as\nsimple reformulations of many standard clustering algorithms including the\npopular spectral clustering method. Recent work has demonstrated that an\nelementary instance of SNMF provides superior clustering quality compared to\nmany classic clustering algorithms on a variety of synthetic and real world\ndata sets. In this work, we present novel reformulations of this instance of\nSNMF based on the notion of variable splitting and produce two fast and\neffective algorithms for its optimization using i) the provably convergent\nAccelerated Proximal Gradient (APG) procedure and ii) a heuristic version of\nthe Alternating Direction Method of Multipliers (ADMM) framework. Our two\nalgorithms present an interesting tradeoff between computational speed and\nmathematical convergence guarantee: while the former method is provably\nconvergent it is considerably slower than the latter approach, for which we\nalso provide significant but less stringent mathematical proof regarding its\nconvergence. Through extensive experiments we show not only that the efficacy\nof these approaches is equal to that of the state of the art SNMF algorithm,\nbut also that the latter of our algorithms is extremely fast being one to two\norders of magnitude faster in terms of total computation time than the state of\nthe art approach, outperforming even spectral clustering in terms of\ncomputation time on large data sets.\n", "versions": [{"version": "v1", "created": "Sat, 17 Sep 2016 14:41:32 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Borhani", "Reza", ""], ["Watt", "Jeremy", ""], ["Katsaggelos", "Aggelos", ""]]}, {"id": "1609.05388", "submitter": "Charalampos Tsourakakis", "authors": "Jaros{\\l}aw B{\\l}asiok, Charalampos E. Tsourakakis", "title": "ADAGIO: Fast Data-aware Near-Isometric Linear Embeddings", "comments": "ICDM 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many important applications, including signal reconstruction, parameter\nestimation, and signal processing in a compressed domain, rely on a\nlow-dimensional representation of the dataset that preserves {\\em all} pairwise\ndistances between the data points and leverages the inherent geometric\nstructure that is typically present. Recently Hedge, Sankaranarayanan, Yin and\nBaraniuk \\cite{hedge2015} proposed the first data-aware near-isometric linear\nembedding which achieves the best of both worlds. However, their method NuMax\ndoes not scale to large-scale datasets.\n  Our main contribution is a simple, data-aware, near-isometric linear\ndimensionality reduction method which significantly outperforms a\nstate-of-the-art method \\cite{hedge2015} with respect to scalability while\nachieving high quality near-isometries. Furthermore, our method comes with\nstrong worst-case theoretical guarantees that allow us to guarantee the quality\nof the obtained near-isometry. We verify experimentally the efficiency of our\nmethod on numerous real-world datasets, where we find that our method ($<$10\nsecs) is more than 3\\,000$\\times$ faster than the state-of-the-art method\n\\cite{hedge2015} ($>$9 hours) on medium scale datasets with 60\\,000 data points\nin 784 dimensions. Finally, we use our method as a preprocessing step to\nincrease the computational efficiency of a classification application and for\nspeeding up approximate nearest neighbor queries.\n", "versions": [{"version": "v1", "created": "Sat, 17 Sep 2016 21:01:19 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["B\u0142asiok", "Jaros\u0142aw", ""], ["Tsourakakis", "Charalampos E.", ""]]}, {"id": "1609.05486", "submitter": "Chang Li Mr.", "authors": "Bingbing Jiang, Chang Li, Maarten de Rijke, Xin Yao and Huanhuan Chen", "title": "Probabilistic Feature Selection and Classification Vector Machine", "comments": "26 pages, 10 figures", "journal-ref": "ACM Transactions on Knowledge Discovery from Data (TKDD) April\n  2019 Article No.: 21", "doi": "10.1145/3309541", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse Bayesian learning is a state-of-the-art supervised learning algorithm\nthat can choose a subset of relevant samples from the input data and make\nreliable probabilistic predictions. However, in the presence of\nhigh-dimensional data with irrelevant features, traditional sparse Bayesian\nclassifiers suffer from performance degradation and low efficiency by failing\nto eliminate irrelevant features. To tackle this problem, we propose a novel\nsparse Bayesian embedded feature selection method that adopts truncated\nGaussian distributions as both sample and feature priors. The proposed method,\ncalled probabilistic feature selection and classification vector machine\n(PFCVMLP ), is able to simultaneously select relevant features and samples for\nclassification tasks. In order to derive the analytical solutions, Laplace\napproximation is applied to compute approximate posteriors and marginal\nlikelihoods. Finally, parameters and hyperparameters are optimized by the\ntype-II maximum likelihood method. Experiments on three datasets validate the\nperformance of PFCVMLP along two dimensions: classification performance and\neffectiveness for feature selection. Finally, we analyze the generalization\nperformance and derive a generalization error bound for PFCVMLP . By tightening\nthe bound, the importance of feature selection is demonstrated.\n", "versions": [{"version": "v1", "created": "Sun, 18 Sep 2016 14:01:04 GMT"}, {"version": "v2", "created": "Mon, 2 Jan 2017 13:36:44 GMT"}, {"version": "v3", "created": "Wed, 13 Jun 2018 10:10:48 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Jiang", "Bingbing", ""], ["Li", "Chang", ""], ["de Rijke", "Maarten", ""], ["Yao", "Xin", ""], ["Chen", "Huanhuan", ""]]}, {"id": "1609.05524", "submitter": "Roy Fox", "authors": "Roy Fox, Michal Moshkovitz and Naftali Tishby", "title": "Principled Option Learning in Markov Decision Processes", "comments": null, "journal-ref": "13th European Workshop on Reinforcement Learning (EWRL 2016)", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that options can make planning more efficient, among their\nmany benefits. Thus far, algorithms for autonomously discovering a set of\nuseful options were heuristic. Naturally, a principled way of finding a set of\nuseful options may be more promising and insightful. In this paper we suggest a\nmathematical characterization of good sets of options using tools from\ninformation theory. This characterization enables us to find conditions for a\nset of options to be optimal and an algorithm that outputs a useful set of\noptions and illustrate the proposed algorithm in simulation.\n", "versions": [{"version": "v1", "created": "Sun, 18 Sep 2016 18:19:02 GMT"}, {"version": "v2", "created": "Tue, 22 Nov 2016 22:51:15 GMT"}, {"version": "v3", "created": "Thu, 30 Mar 2017 05:04:42 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Fox", "Roy", ""], ["Moshkovitz", "Michal", ""], ["Tishby", "Naftali", ""]]}, {"id": "1609.05528", "submitter": "Shebuti Rayana", "authors": "Shebuti Rayana, Wen Zhong and Leman Akoglu", "title": "Sequential Ensemble Learning for Outlier Detection: A Bias-Variance\n  Perspective", "comments": "11 pages, 8 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensemble methods for classification and clustering have been effectively used\nfor decades, while ensemble learning for outlier detection has only been\nstudied recently. In this work, we design a new ensemble approach for outlier\ndetection in multi-dimensional point data, which provides improved accuracy by\nreducing error through both bias and variance. Although classification and\noutlier detection appear as different problems, their theoretical underpinnings\nare quite similar in terms of the bias-variance trade-off [1], where outlier\ndetection is considered as a binary classification task with unobserved labels\nbut a similar bias-variance decomposition of error.\n  In this paper, we propose a sequential ensemble approach called CARE that\nemploys a two-phase aggregation of the intermediate results in each iteration\nto reach the final outcome. Unlike existing outlier ensembles which solely\nincorporate a parallel framework by aggregating the outcomes of independent\nbase detectors to reduce variance, our ensemble incorporates both the parallel\nand sequential building blocks to reduce bias as well as variance by ($i$)\nsuccessively eliminating outliers from the original dataset to build a better\ndata model on which outlierness is estimated (sequentially), and ($ii$)\ncombining the results from individual base detectors and across iterations\n(parallelly). Through extensive experiments on sixteen real-world datasets\nmainly from the UCI machine learning repository [2], we show that CARE performs\nsignificantly better than or at least similar to the individual baselines. We\nalso compare CARE with the state-of-the-art outlier ensembles where it also\nprovides significant improvement when it is the winner and remains close\notherwise.\n", "versions": [{"version": "v1", "created": "Sun, 18 Sep 2016 18:59:42 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Rayana", "Shebuti", ""], ["Zhong", "Wen", ""], ["Akoglu", "Leman", ""]]}, {"id": "1609.05536", "submitter": "Theja Tulabandhula", "authors": "Theja Tulabandhula", "title": "Learning Personalized Optimal Control for Repeatedly Operated Systems", "comments": "This work was presented at the NIPS 2015 Workshop: Machine Learning\n  From and For Adaptive User Technologies: From Active Learning &\n  Experimentation to Optimization & Personalization (ref.\n  https://sites.google.com/site/mlaihci)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of online learning of optimal control for repeatedly\noperated systems in the presence of parametric uncertainty. During each round\nof operation, environment selects system parameters according to a fixed but\nunknown probability distribution. These parameters govern the dynamics of a\nplant. An agent chooses a control input to the plant and is then revealed the\ncost of the choice. In this setting, we design an agent that personalizes the\ncontrol input to this plant taking into account the stochasticity involved. We\ndemonstrate the effectiveness of our approach on a simulated system.\n", "versions": [{"version": "v1", "created": "Sun, 18 Sep 2016 19:58:48 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Tulabandhula", "Theja", ""]]}, {"id": "1609.05539", "submitter": "Mostafa El Gamal", "authors": "Mostafa El Gamal and Lifeng Lai", "title": "On Randomized Distributed Coordinate Descent with Quantized Updates", "comments": "Accepted at CISS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the randomized distributed coordinate descent\nalgorithm with quantized updates. In the literature, the iteration complexity\nof the randomized distributed coordinate descent algorithm has been\ncharacterized under the assumption that machines can exchange updates with an\ninfinite precision. We consider a practical scenario in which the messages\nexchange occurs over channels with finite capacity, and hence the updates have\nto be quantized. We derive sufficient conditions on the quantization error such\nthat the algorithm with quantized update still converge. We further verify our\ntheoretical results by running an experiment, where we apply the algorithm with\nquantized updates to solve a linear regression problem.\n", "versions": [{"version": "v1", "created": "Sun, 18 Sep 2016 20:17:00 GMT"}, {"version": "v2", "created": "Fri, 20 Jan 2017 17:37:44 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Gamal", "Mostafa El", ""], ["Lai", "Lifeng", ""]]}, {"id": "1609.05573", "submitter": "Alexander Wein", "authors": "Amelia Perry and Alexander S. Wein and Afonso S. Bandeira and Ankur\n  Moitra", "title": "Optimality and Sub-optimality of PCA for Spiked Random Matrices and\n  Synchronization", "comments": "58 pages, 5 figures. This version adds improved results for the\n  Wishart model", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.IT math.IT math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central problem of random matrix theory is to understand the eigenvalues of\nspiked random matrix models, in which a prominent eigenvector is planted into a\nrandom matrix. These distributions form natural statistical models for\nprincipal component analysis (PCA) problems throughout the sciences. Baik, Ben\nArous and P\\'ech\\'e showed that the spiked Wishart ensemble exhibits a sharp\nphase transition asymptotically: when the signal strength is above a critical\nthreshold, it is possible to detect the presence of a spike based on the top\neigenvalue, and below the threshold the top eigenvalue provides no information.\nSuch results form the basis of our understanding of when PCA can detect a\nlow-rank signal in the presence of noise.\n  However, not all the information about the spike is necessarily contained in\nthe spectrum. We study the fundamental limitations of statistical methods,\nincluding non-spectral ones. Our results include:\n  I) For the Gaussian Wigner ensemble, we show that PCA achieves the optimal\ndetection threshold for a variety of benign priors for the spike. We extend\nprevious work on the spherically symmetric and i.i.d. Rademacher priors through\nan elementary, unified analysis.\n  II) For any non-Gaussian Wigner ensemble, we show that PCA is always\nsuboptimal for detection. However, a variant of PCA achieves the optimal\nthreshold (for benign priors) by pre-transforming the matrix entries according\nto a carefully designed function. This approach has been stated before, and we\ngive a rigorous and general analysis.\n  III) For both the Gaussian Wishart ensemble and various synchronization\nproblems over groups, we show that inefficient procedures can work below the\nthreshold where PCA succeeds, whereas no known efficient algorithm achieves\nthis. This conjectural gap between what is statistically possible and what can\nbe done efficiently remains open.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2016 00:25:43 GMT"}, {"version": "v2", "created": "Fri, 23 Dec 2016 08:11:18 GMT"}], "update_date": "2016-12-26", "authors_parsed": [["Perry", "Amelia", ""], ["Wein", "Alexander S.", ""], ["Bandeira", "Afonso S.", ""], ["Moitra", "Ankur", ""]]}, {"id": "1609.05772", "submitter": "Christopher Adams", "authors": "Christopher Adams", "title": "Stochastic Matrix Factorization", "comments": "24 pages, 4 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a restriction to non-negative matrix factorization in\nwhich at least one matrix factor is stochastic. That is, the elements of the\nmatrix factors are non-negative and the columns of one matrix factor sum to 1.\nThis restriction includes topic models, a popular method for analyzing\nunstructured data. It also includes a method for storing and finding pictures.\nThe paper presents necessary and sufficient conditions on the observed data\nsuch that the factorization is unique. In addition, the paper characterizes\nnatural bounds on the parameters for any observed data and presents a\nconsistent least squares estimator. The results are illustrated using a topic\nmodel analysis of PhD abstracts in economics and the problem of storing and\nretrieving a set of pictures of faces.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2016 15:19:44 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Adams", "Christopher", ""]]}, {"id": "1609.05796", "submitter": "Siamak Ravanbakhsh", "authors": "Siamak Ravanbakhsh and Francois Lanusse and Rachel Mandelbaum and Jeff\n  Schneider and Barnabas Poczos", "title": "Enabling Dark Energy Science with Deep Generative Models of Galaxy\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM astro-ph.CO cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the nature of dark energy, the mysterious force driving the\naccelerated expansion of the Universe, is a major challenge of modern\ncosmology. The next generation of cosmological surveys, specifically designed\nto address this issue, rely on accurate measurements of the apparent shapes of\ndistant galaxies. However, shape measurement methods suffer from various\nunavoidable biases and therefore will rely on a precise calibration to meet the\naccuracy requirements of the science analysis. This calibration process remains\nan open challenge as it requires large sets of high quality galaxy images. To\nthis end, we study the application of deep conditional generative models in\ngenerating realistic galaxy images. In particular we consider variations on\nconditional variational autoencoder and introduce a new adversarial objective\nfor training of conditional generative networks. Our results suggest a reliable\nalternative to the acquisition of expensive high quality observations for\ngenerating the calibration data needed by the next generation of cosmological\nsurveys.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2016 15:48:03 GMT"}, {"version": "v2", "created": "Wed, 30 Nov 2016 16:57:52 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Ravanbakhsh", "Siamak", ""], ["Lanusse", "Francois", ""], ["Mandelbaum", "Rachel", ""], ["Schneider", "Jeff", ""], ["Poczos", "Barnabas", ""]]}, {"id": "1609.05807", "submitter": "Jon Kleinberg", "authors": "Jon Kleinberg, Sendhil Mullainathan, Manish Raghavan", "title": "Inherent Trade-Offs in the Fair Determination of Risk Scores", "comments": "To appear in Proceedings of Innovations in Theoretical Computer\n  Science (ITCS), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent discussion in the public sphere about algorithmic classification has\ninvolved tension between competing notions of what it means for a probabilistic\nclassification to be fair to different groups. We formalize three fairness\nconditions that lie at the heart of these debates, and we prove that except in\nhighly constrained special cases, there is no method that can satisfy these\nthree conditions simultaneously. Moreover, even satisfying all three conditions\napproximately requires that the data lie in an approximate version of one of\nthe constrained special cases identified by our theorem. These results suggest\nsome of the ways in which key notions of fairness are incompatible with each\nother, and hence provide a framework for thinking about the trade-offs between\nthem.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2016 16:08:51 GMT"}, {"version": "v2", "created": "Thu, 17 Nov 2016 16:41:21 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Kleinberg", "Jon", ""], ["Mullainathan", "Sendhil", ""], ["Raghavan", "Manish", ""]]}, {"id": "1609.05820", "submitter": "Yuxin Chen", "authors": "Yuxin Chen and Emmanuel Candes", "title": "The Projected Power Method: An Efficient Algorithm for Joint Alignment\n  from Pairwise Differences", "comments": "Accepted to Communications on Pure and Applied Mathematics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV cs.LG math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various applications involve assigning discrete label values to a collection\nof objects based on some pairwise noisy data. Due to the discrete---and hence\nnonconvex---structure of the problem, computing the optimal assignment\n(e.g.~maximum likelihood assignment) becomes intractable at first sight. This\npaper makes progress towards efficient computation by focusing on a concrete\njoint alignment problem---that is, the problem of recovering $n$ discrete\nvariables $x_i \\in \\{1,\\cdots, m\\}$, $1\\leq i\\leq n$ given noisy observations\nof their modulo differences $\\{x_i - x_j~\\mathsf{mod}~m\\}$. We propose a\nlow-complexity and model-free procedure, which operates in a lifted space by\nrepresenting distinct label values in orthogonal directions, and which attempts\nto optimize quadratic functions over hypercubes. Starting with a first guess\ncomputed via a spectral method, the algorithm successively refines the iterates\nvia projected power iterations. We prove that for a broad class of statistical\nmodels, the proposed projected power method makes no error---and hence\nconverges to the maximum likelihood estimate---in a suitable regime. Numerical\nexperiments have been carried out on both synthetic and real data to\ndemonstrate the practicality of our algorithm. We expect this algorithmic\nframework to be effective for a broad range of discrete assignment problems.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2016 16:29:46 GMT"}, {"version": "v2", "created": "Tue, 22 Nov 2016 21:59:14 GMT"}, {"version": "v3", "created": "Thu, 7 Dec 2017 20:30:54 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Chen", "Yuxin", ""], ["Candes", "Emmanuel", ""]]}, {"id": "1609.05866", "submitter": "Alexandre de Br\\'ebisson", "authors": "Alexandre de Br\\'ebisson, Pascal Vincent", "title": "A Cheap Linear Attention Mechanism with Fast Lookups and Fixed-Size\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The softmax content-based attention mechanism has proven to be very\nbeneficial in many applications of recurrent neural networks. Nevertheless it\nsuffers from two major computational limitations. First, its computations for\nan attention lookup scale linearly in the size of the attended sequence.\nSecond, it does not encode the sequence into a fixed-size representation but\ninstead requires to memorize all the hidden states. These two limitations\nrestrict the use of the softmax attention mechanism to relatively small-scale\napplications with short sequences and few lookups per sequence. In this work we\nintroduce a family of linear attention mechanisms designed to overcome the two\nlimitations listed above. We show that removing the softmax non-linearity from\nthe traditional attention formulation yields constant-time attention lookups\nand fixed-size representations of the attended sequences. These properties make\nthese linear attention mechanisms particularly suitable for large-scale\napplications with extreme query loads, real-time requirements and memory\nconstraints. Early experiments on a question answering task show that these\nlinear mechanisms yield significantly better accuracy results than no\nattention, but obviously worse than their softmax alternative.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2016 18:55:18 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["de Br\u00e9bisson", "Alexandre", ""], ["Vincent", "Pascal", ""]]}, {"id": "1609.05877", "submitter": "Cesar A. Uribe", "authors": "Angelia Nedi\\'c, Alex Olshevsky, Wei Shi, and C\\'esar A. Uribe", "title": "Geometrically Convergent Distributed Optimization with Uncoordinated\n  Step-Sizes", "comments": "arXiv admin note: text overlap with arXiv:1607.03218", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent algorithmic family for distributed optimization, DIGing's, have been\nshown to have geometric convergence over time-varying undirected/directed\ngraphs. Nevertheless, an identical step-size for all agents is needed. In this\npaper, we study the convergence rates of the Adapt-Then-Combine (ATC) variation\nof the DIGing algorithm under uncoordinated step-sizes. We show that the ATC\nvariation of DIGing algorithm converges geometrically fast even if the\nstep-sizes are different among the agents. In addition, our analysis implies\nthat the ATC structure can accelerate convergence compared to the distributed\ngradient descent (DGD) structure which has been used in the original DIGing\nalgorithm.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2016 19:22:10 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Nedi\u0107", "Angelia", ""], ["Olshevsky", "Alex", ""], ["Shi", "Wei", ""], ["Uribe", "C\u00e9sar A.", ""]]}, {"id": "1609.05881", "submitter": "Priyank Jaini", "authors": "Priyank Jaini and Pascal Poupart", "title": "Online and Distributed learning of Gaussian mixture models by Bayesian\n  Moment Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gaussian mixture model is a classic technique for clustering and data\nmodeling that is used in numerous applications. With the rise of big data,\nthere is a need for parameter estimation techniques that can handle streaming\ndata and distribute the computation over several processors. While online\nvariants of the Expectation Maximization (EM) algorithm exist, their data\nefficiency is reduced by a stochastic approximation of the E-step and it is not\nclear how to distribute the computation over multiple processors. We propose a\nBayesian learning technique that lends itself naturally to online and\ndistributed computation. Since the Bayesian posterior is not tractable, we\nproject it onto a family of tractable distributions after each observation by\nmatching a set of sufficient moments. This Bayesian moment matching technique\ncompares favorably to online EM in terms of time and accuracy on a set of data\nmodeling benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2016 19:44:06 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Jaini", "Priyank", ""], ["Poupart", "Pascal", ""]]}, {"id": "1609.05959", "submitter": "Evgeny Burnaev", "authors": "Evgeny Burnaev and Ivan Nazarov", "title": "Conformalized Kernel Ridge Regression", "comments": "8 pages, 8 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  General predictive models do not provide a measure of confidence in\npredictions without Bayesian assumptions. A way to circumvent potential\nrestrictions is to use conformal methods for constructing non-parametric\nconfidence regions, that offer guarantees regarding validity. In this paper we\nprovide a detailed description of a computationally efficient conformal\nprocedure for Kernel Ridge Regression (KRR), and conduct a comparative\nnumerical study to see how well conformal regions perform against the Bayesian\nconfidence sets. The results suggest that conformalized KRR can yield\npredictive confidence regions with specified coverage rate, which is essential\nin constructing anomaly detection systems based on predictive models.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2016 22:30:36 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Burnaev", "Evgeny", ""], ["Nazarov", "Ivan", ""]]}, {"id": "1609.06070", "submitter": "David R\\\"ugamer", "authors": "David R\\\"ugamer, Sarah Brockhaus, Kornelia Gentsch, Klaus Scherer and\n  Sonja Greven", "title": "Boosting Factor-Specific Functional Historical Models for the Detection\n  of Synchronisation in Bioelectrical Signals", "comments": "Revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The link between different psychophysiological measures during emotion\nepisodes is not well understood. To analyse the functional relationship between\nelectroencephalography (EEG) and facial electromyography (EMG), we apply\nhistorical function-on-function regression models to EEG and EMG data that were\nsimultaneously recorded from 24 participants while they were playing a\ncomputerised gambling task. Given the complexity of the data structure for this\napplication, we extend simple functional historical models to models including\nrandom historical effects, factor-specific historical effects, and\nfactor-specific random historical effects. Estimation is conducted by a\ncomponent-wise gradient boosting algorithm, which scales well to large data\nsets and complex models.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 09:42:32 GMT"}, {"version": "v2", "created": "Sat, 13 May 2017 09:31:56 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["R\u00fcgamer", "David", ""], ["Brockhaus", "Sarah", ""], ["Gentsch", "Kornelia", ""], ["Scherer", "Klaus", ""], ["Greven", "Sonja", ""]]}, {"id": "1609.06100", "submitter": "Paolo Di Lorenzo", "authors": "P. Di Lorenzo, P. Banelli, S. Barbarossa, S. Sardellitti", "title": "Distributed Adaptive Learning of Graph Signals", "comments": "To appear in IEEE Transactions on Signal Processing, 2017", "journal-ref": null, "doi": "10.1109/TSP.2017.2708035", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to propose distributed strategies for adaptive\nlearning of signals defined over graphs. Assuming the graph signal to be\nbandlimited, the method enables distributed reconstruction, with guaranteed\nperformance in terms of mean-square error, and tracking from a limited number\nof sampled observations taken from a subset of vertices. A detailed mean square\nanalysis is carried out and illustrates the role played by the sampling\nstrategy on the performance of the proposed method. Finally, some useful\nstrategies for distributed selection of the sampling set are provided. Several\nnumerical results validate our theoretical findings, and illustrate the\nperformance of the proposed method for distributed adaptive learning of signals\ndefined over graphs.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 11:12:04 GMT"}, {"version": "v2", "created": "Fri, 27 Jan 2017 14:09:39 GMT"}, {"version": "v3", "created": "Sat, 15 Apr 2017 16:20:59 GMT"}, {"version": "v4", "created": "Sat, 13 May 2017 21:06:26 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Di Lorenzo", "P.", ""], ["Banelli", "P.", ""], ["Barbarossa", "S.", ""], ["Sardellitti", "S.", ""]]}, {"id": "1609.06144", "submitter": "Tigran Nagapetyan", "authors": "Mike Giles, Tigran Nagapetyan, Lukasz Szpruch, Sebastian Vollmer,\n  Konstantinos Zygalakis", "title": "Multilevel Monte Carlo for Scalable Bayesian Computations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) algorithms are ubiquitous in Bayesian\ncomputations. However, they need to access the full data set in order to\nevaluate the posterior density at every step of the algorithm. This results in\na great computational burden in big data applications. In contrast to MCMC\nmethods, Stochastic Gradient MCMC (SGMCMC) algorithms such as the Stochastic\nGradient Langevin Dynamics (SGLD) only require access to a batch of the data\nset at every step. This drastically improves the computational performance and\nscales well to large data sets. However, the difficulty with SGMCMC algorithms\ncomes from the sensitivity to its parameters which are notoriously difficult to\ntune. Moreover, the Root Mean Square Error (RMSE) scales as\n$\\mathcal{O}(c^{-\\frac{1}{3}})$ as opposed to standard MCMC\n$\\mathcal{O}(c^{-\\frac{1}{2}})$ where $c$ is the computational cost.\n  We introduce a new class of Multilevel Stochastic Gradient Markov chain Monte\nCarlo algorithms that are able to mitigate the problem of tuning the step size\nand more importantly of recovering the $\\mathcal{O}(c^{-\\frac{1}{2}})$\nconvergence of standard Markov Chain Monte Carlo methods without the need to\nintroduce Metropolis-Hasting steps. A further advantage of this new class of\nalgorithms is that it can easily be parallelised over a heterogeneous computer\narchitecture. We illustrate our methodology using Bayesian logistic regression\nand provide numerical evidence that for a prescribed relative RMSE the\ncomputational cost is sublinear in the number of data items.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 10:36:36 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Giles", "Mike", ""], ["Nagapetyan", "Tigran", ""], ["Szpruch", "Lukasz", ""], ["Vollmer", "Sebastian", ""], ["Zygalakis", "Konstantinos", ""]]}, {"id": "1609.06369", "submitter": "Aleksandr Aravkin", "authors": "A.Y. Aravkin, J.V. Burke, L. Ljung, A. Lozano, and G. Pillonetto", "title": "Generalized Kalman Smoothing: Modeling and Algorithms", "comments": "29 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-space smoothing has found many applications in science and engineering.\nUnder linear and Gaussian assumptions, smoothed estimates can be obtained using\nefficient recursions, for example Rauch-Tung-Striebel and Mayne-Fraser\nalgorithms. Such schemes are equivalent to linear algebraic techniques that\nminimize a convex quadratic objective function with structure induced by the\ndynamic model.\n  These classical formulations fall short in many important circumstances. For\ninstance, smoothers obtained using quadratic penalties can fail when outliers\nare present in the data, and cannot track impulsive inputs and abrupt state\nchanges. Motivated by these shortcomings, generalized Kalman smoothing\nformulations have been proposed in the last few years, replacing quadratic\nmodels with more suitable, often nonsmooth, convex functions. In contrast to\nclassical models, these general estimators require use of iterated algorithms,\nand these have received increased attention from control, signal processing,\nmachine learning, and optimization communities.\n  In this survey we show that the optimization viewpoint provides the control\nand signal processing community great freedom in the development of novel\nmodeling and inference frameworks for dynamical systems. We discuss general\nstatistical models for dynamic systems, making full use of nonsmooth convex\npenalties and constraints, and providing links to important models in signal\nprocessing and machine learning. We also survey optimization techniques for\nthese formulations, paying close attention to dynamic problem structure.\nModeling concepts and algorithms are illustrated with numerical examples.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 21:58:06 GMT"}, {"version": "v2", "created": "Sun, 25 Sep 2016 12:37:20 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Aravkin", "A. Y.", ""], ["Burke", "J. V.", ""], ["Ljung", "L.", ""], ["Lozano", "A.", ""], ["Pillonetto", "G.", ""]]}, {"id": "1609.06385", "submitter": "Bernardo \\'Avila Pires", "authors": "Bernardo \\'Avila Pires and Csaba Szepesv\\'ari", "title": "Multiclass Classification Calibration Functions", "comments": "44 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we refine the process of computing calibration functions for a\nnumber of multiclass classification surrogate losses. Calibration functions are\na powerful tool for easily converting bounds for the surrogate risk (which can\nbe computed through well-known methods) into bounds for the true risk, the\nprobability of making a mistake. They are particularly suitable in\nnon-parametric settings, where the approximation error can be controlled, and\nprovide tighter bounds than the common technique of upper-bounding the 0-1 loss\nby the surrogate loss.\n  The abstract nature of the more sophisticated existing calibration function\nresults requires calibration functions to be explicitly derived on a\ncase-by-case basis, requiring repeated efforts whenever bounds for a new\nsurrogate loss are required. We devise a streamlined analysis that simplifies\nthe process of deriving calibration functions for a large number of surrogate\nlosses that have been proposed in the literature. The effort of deriving\ncalibration functions is then surmised in verifying, for a chosen surrogate\nloss, a small number of conditions that we introduce.\n  As case studies, we recover existing calibration functions for the well-known\nloss of Lee et al. (2004), and also provide novel calibration functions for\nwell-known losses, including the one-versus-all loss and the logistic\nregression loss, plus a number of other losses that have been shown to be\nclassification-calibrated in the past, but for which no calibration function\nhad been derived.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 23:41:55 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Pires", "Bernardo \u00c1vila", ""], ["Szepesv\u00e1ri", "Csaba", ""]]}, {"id": "1609.06390", "submitter": "Kirthevasan Kandasamy", "authors": "Kirthevasan Kandasamy, Maruan Al-Shedivat, Eric P. Xing", "title": "Learning HMMs with Nonparametric Emissions via Spectral Decompositions\n  of Continuous Matrices", "comments": "To appear in NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been a surge of interest in using spectral methods for\nestimating latent variable models. However, it is usually assumed that the\ndistribution of the observations conditioned on the latent variables is either\ndiscrete or belongs to a parametric family. In this paper, we study the\nestimation of an $m$-state hidden Markov model (HMM) with only smoothness\nassumptions, such as H\\\"olderian conditions, on the emission densities. By\nleveraging some recent advances in continuous linear algebra and numerical\nanalysis, we develop a computationally efficient spectral algorithm for\nlearning nonparametric HMMs. Our technique is based on computing an SVD on\nnonparametric estimates of density functions by viewing them as\n\\emph{continuous matrices}. We derive sample complexity bounds via\nconcentration results for nonparametric density estimation and novel\nperturbation theory results for continuous matrices. We implement our method\nusing Chebyshev polynomial approximations. Our method is competitive with other\nbaselines on synthetic and real problems and is also very computationally\nefficient.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 00:15:44 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Kandasamy", "Kirthevasan", ""], ["Al-Shedivat", "Maruan", ""], ["Xing", "Eric P.", ""]]}, {"id": "1609.06457", "submitter": "Pin-Yu Chen", "authors": "Pin-Yu Chen and Thibaut Gensollen and Alfred O. Hero III", "title": "AMOS: An Automated Model Order Selection Algorithm for Spectral Graph\n  Clustering", "comments": "arXiv admin note: substantial text overlap with arXiv:1604.03159", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the longstanding problems in spectral graph clustering (SGC) is the\nso-called model order selection problem: automated selection of the correct\nnumber of clusters. This is equivalent to the problem of finding the number of\nconnected components or communities in an undirected graph. In this paper, we\npropose AMOS, an automated model order selection algorithm for SGC. Based on a\nrecent analysis of clustering reliability for SGC under the random\ninterconnection model, AMOS works by incrementally increasing the number of\nclusters, estimating the quality of identified clusters, and providing a series\nof clustering reliability tests. Consequently, AMOS outputs clusters of minimal\nmodel order with statistical clustering reliability guarantees. Comparing to\nthree other automated graph clustering methods on real-world datasets, AMOS\nshows superior performance in terms of multiple external and internal\nclustering metrics.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 08:14:12 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Chen", "Pin-Yu", ""], ["Gensollen", "Thibaut", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1609.06480", "submitter": "Shihua Zhang", "authors": "Wenwen Min, Juan Liu, Shihua Zhang", "title": "Network-regularized Sparse Logistic Regression Models for Clinical Risk\n  Prediction and Biomarker Discovery", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Molecular profiling data (e.g., gene expression) has been used for clinical\nrisk prediction and biomarker discovery. However, it is necessary to integrate\nother prior knowledge like biological pathways or gene interaction networks to\nimprove the predictive ability and biological interpretability of biomarkers.\nHere, we first introduce a general regularized Logistic Regression (LR)\nframework with regularized term $\\lambda \\|\\bm{w}\\|_1 +\n\\eta\\bm{w}^T\\bm{M}\\bm{w}$, which can reduce to different penalties, including\nLasso, elastic net, and network-regularized terms with different $\\bm{M}$. This\nframework can be easily solved in a unified manner by a cyclic coordinate\ndescent algorithm which can avoid inverse matrix operation and accelerate the\ncomputing speed. However, if those estimated $\\bm{w}_i$ and $\\bm{w}_j$ have\nopposite signs, then the traditional network-regularized penalty may not\nperform well. To address it, we introduce a novel network-regularized sparse LR\nmodel with a new penalty $\\lambda \\|\\bm{w}\\|_1 + \\eta|\\bm{w}|^T\\bm{M}|\\bm{w}|$\nto consider the difference between the absolute values of the coefficients. And\nwe develop two efficient algorithms to solve it. Finally, we test our methods\nand compare them with the related ones using simulated and real data to show\ntheir efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 09:47:32 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Min", "Wenwen", ""], ["Liu", "Juan", ""], ["Zhang", "Shihua", ""]]}, {"id": "1609.06532", "submitter": "Kar Wai Lim", "authors": "Kar Wai Lim and Wray Buntine", "title": "Bibliographic Analysis on Research Publications using Authors,\n  Categorical Labels and the Citation Network", "comments": "Preprint for Journal Machine Learning", "journal-ref": "Machine Learning 103(2):185-213, 2016", "doi": "10.1007/s10994-016-5554-z", "report-no": null, "categories": "cs.DL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bibliographic analysis considers the author's research areas, the citation\nnetwork and the paper content among other things. In this paper, we combine\nthese three in a topic model that produces a bibliographic model of authors,\ntopics and documents, using a nonparametric extension of a combination of the\nPoisson mixed-topic link model and the author-topic model. This gives rise to\nthe Citation Network Topic Model (CNTM). We propose a novel and efficient\ninference algorithm for the CNTM to explore subsets of research publications\nfrom CiteSeerX. The publication datasets are organised into three corpora,\ntotalling to about 168k publications with about 62k authors. The queried\ndatasets are made available online. In three publicly available corpora in\naddition to the queried datasets, our proposed model demonstrates an improved\nperformance in both model fitting and document clustering, compared to several\nbaselines. Moreover, our model allows extraction of additional useful knowledge\nfrom the corpora, such as the visualisation of the author-topics network.\nAdditionally, we propose a simple method to incorporate supervision into topic\nmodelling to achieve further improvement on the clustering task.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 12:44:37 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Lim", "Kar Wai", ""], ["Buntine", "Wray", ""]]}, {"id": "1609.06533", "submitter": "Kajsa M{\\o}llersen", "authors": "Kajsa M{\\o}llersen, Subhra S. Dhar, Fred Godtliebsen", "title": "On Data-Independent Properties for Density-Based Dissimilarity Measures\n  in Hybrid Clustering", "comments": null, "journal-ref": "M{\\o}llersen, K., Dhar, S.S. and Godtliebsen, F. (2016) On\n  Data-Independent Properties for Density-Based Dissimilarity Measures in\n  Hybrid Clustering. Applied Mathematics, 7, 1674-1706", "doi": "10.4236/am.2016.715143", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hybrid clustering combines partitional and hierarchical clustering for\ncomputational effectiveness and versatility in cluster shape. In such\nclustering, a dissimilarity measure plays a crucial role in the hierarchical\nmerging. The dissimilarity measure has great impact on the final clustering,\nand data-independent properties are needed to choose the right dissimilarity\nmeasure for the problem at hand. Properties for distance-based dissimilarity\nmeasures have been studied for decades, but properties for density-based\ndissimilarity measures have so far received little attention. Here, we propose\nsix data-independent properties to evaluate density-based dissimilarity\nmeasures associated with hybrid clustering, regarding equality, orthogonality,\nsymmetry, outlier and noise observations, and light-tailed models for\nheavy-tailed clusters. The significance of the properties is investigated, and\nwe study some well-known dissimilarity measures based on Shannon entropy,\nmisclassification rate, Bhattacharyya distance and Kullback-Leibler divergence\nwith respect to the proposed properties. As none of them satisfy all the\nproposed properties, we introduce a new dissimilarity measure based on the\nKullback-Leibler information and show that it satisfies all proposed\nproperties. The effect of the proposed properties is also illustrated on\nseveral real and simulated data sets.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 12:46:09 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["M\u00f8llersen", "Kajsa", ""], ["Dhar", "Subhra S.", ""], ["Godtliebsen", "Fred", ""]]}, {"id": "1609.06575", "submitter": "M. Ros\\'ario Oliveira", "authors": "Cl\\'audia Pascoal, M. Ros\\'ario Oliveira, Ant\\'onio Pacheco, and Rui\n  Valadas", "title": "Theoretical Evaluation of Feature Selection Methods based on Mutual\n  Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection methods are usually evaluated by wrapping specific\nclassifiers and datasets in the evaluation process, resulting very often in\nunfair comparisons between methods. In this work, we develop a theoretical\nframework that allows obtaining the true feature ordering of two-dimensional\nsequential forward feature selection methods based on mutual information, which\nis independent of entropy or mutual information estimation methods,\nclassifiers, or datasets, and leads to an undoubtful comparison of the methods.\nMoreover, the theoretical framework unveils problems intrinsic to some methods\nthat are otherwise difficult to detect, namely inconsistencies in the\nconstruction of the objective function used to select the candidate features,\ndue to various types of indeterminations and to the possibility of the entropy\nof continuous random variables taking null and negative values.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 14:23:15 GMT"}, {"version": "v2", "created": "Fri, 7 Oct 2016 22:51:50 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Pascoal", "Cl\u00e1udia", ""], ["Oliveira", "M. Ros\u00e1rio", ""], ["Pacheco", "Ant\u00f3nio", ""], ["Valadas", "Rui", ""]]}, {"id": "1609.06764", "submitter": "Nicholas Boyd", "authors": "Nicholas Boyd, Trevor Hastie, Stephen Boyd, Benjamin Recht, Michael\n  Jordan", "title": "Saturating Splines and Feature Selection", "comments": "Adding missing references and related work", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the adaptive regression spline model by incorporating saturation,\nthe natural requirement that a function extend as a constant outside a certain\nrange. We fit saturating splines to data using a convex optimization problem\nover a space of measures, which we solve using an efficient algorithm based on\nthe conditional gradient method. Unlike many existing approaches, our algorithm\nsolves the original infinite-dimensional (for splines of degree at least two)\noptimization problem without pre-specified knot locations. We then adapt our\nalgorithm to fit generalized additive models with saturating splines as\ncoordinate functions and show that the saturation requirement allows our model\nto simultaneously perform feature selection and nonlinear function fitting.\nFinally, we briefly sketch how the method can be extended to higher order\nsplines and to different requirements on the extension outside the data range.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 21:57:25 GMT"}, {"version": "v2", "created": "Tue, 27 Sep 2016 17:10:58 GMT"}, {"version": "v3", "created": "Mon, 4 Dec 2017 18:56:30 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Boyd", "Nicholas", ""], ["Hastie", "Trevor", ""], ["Boyd", "Stephen", ""], ["Recht", "Benjamin", ""], ["Jordan", "Michael", ""]]}, {"id": "1609.06783", "submitter": "Kar Wai Lim", "authors": "Kar Wai Lim, Wray Buntine, Changyou Chen, Lan Du", "title": "Nonparametric Bayesian Topic Modelling with the Hierarchical Pitman-Yor\n  Processes", "comments": "Preprint for International Journal of Approximate Reasoning", "journal-ref": "International Journal of Approximate Reasoning, Volume 78, pp.\n  172-191. Elsevier. 2016", "doi": "10.1016/j.ijar.2016.07.007", "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Dirichlet process and its extension, the Pitman-Yor process, are\nstochastic processes that take probability distributions as a parameter. These\nprocesses can be stacked up to form a hierarchical nonparametric Bayesian\nmodel. In this article, we present efficient methods for the use of these\nprocesses in this hierarchical context, and apply them to latent variable\nmodels for text analytics. In particular, we propose a general framework for\ndesigning these Bayesian models, which are called topic models in the computer\nscience community. We then propose a specific nonparametric Bayesian topic\nmodel for modelling text from social media. We focus on tweets (posts on\nTwitter) in this article due to their ease of access. We find that our\nnonparametric model performs better than existing parametric models in both\ngoodness of fit and real world applications.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 00:10:16 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Lim", "Kar Wai", ""], ["Buntine", "Wray", ""], ["Chen", "Changyou", ""], ["Du", "Lan", ""]]}, {"id": "1609.06826", "submitter": "Kar Wai Lim", "authors": "Kar Wai Lim, Wray Buntine", "title": "Bibliographic Analysis with the Citation Network Topic Model", "comments": "A copy of ACML paper. arXiv admin note: substantial text overlap with\n  arXiv:1609.06532", "journal-ref": "Proceedings of the Sixth Asian Conference on Machine Learning\n  (ACML), pp. 142-158. JMLR. 2014", "doi": null, "report-no": null, "categories": "cs.DL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bibliographic analysis considers author's research areas, the citation\nnetwork and paper content among other things. In this paper, we combine these\nthree in a topic model that produces a bibliographic model of authors, topics\nand documents using a non-parametric extension of a combination of the Poisson\nmixed-topic link model and the author-topic model. We propose a novel and\nefficient inference algorithm for the model to explore subsets of research\npublications from CiteSeerX. Our model demonstrates improved performance in\nboth model fitting and a clustering task compared to several baselines.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 05:46:46 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Lim", "Kar Wai", ""], ["Buntine", "Wray", ""]]}, {"id": "1609.06831", "submitter": "Kar Wai Lim", "authors": "Young Lee, Kar Wai Lim, Cheng Soon Ong", "title": "Hawkes Processes with Stochastic Excitations", "comments": "Copy of ICML paper", "journal-ref": "Proceedings of The 33rd International Conference on Machine\n  Learning (ICML), pp. 79-88. JMLR. 2016", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an extension to Hawkes processes by treating the levels of\nself-excitation as a stochastic differential equation. Our new point process\nallows better approximation in application domains where events and intensities\naccelerate each other with correlated levels of contagion. We generalize a\nrecent algorithm for simulating draws from Hawkes processes whose levels of\nexcitation are stochastic processes, and propose a hybrid Markov chain Monte\nCarlo approach for model fitting. Our sampling procedure scales linearly with\nthe number of required events and does not require stationarity of the point\nprocess. A modular inference procedure consisting of a combination between\nGibbs and Metropolis Hastings steps is put forward. We recover expectation\nmaximization as a special case. Our general approach is illustrated for\ncontagion following geometric Brownian motion and exponential Langevin\ndynamics.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 06:18:20 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Lee", "Young", ""], ["Lim", "Kar Wai", ""], ["Ong", "Cheng Soon", ""]]}, {"id": "1609.06840", "submitter": "Philipp Hennig PhD", "authors": "Philipp Hennig and Roman Garnett", "title": "Exact Sampling from Determinantal Point Processes", "comments": "Fixed a nontrivial typo in Eq. 12. Many thanks to Lucy Kuncheva and\n  Joseph Courtney for pointing it out to us", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determinantal point processes (DPPs) are an important concept in random\nmatrix theory and combinatorics. They have also recently attracted interest in\nthe study of numerical methods for machine learning, as they offer an elegant\n\"missing link\" between independent Monte Carlo sampling and deterministic\nevaluation on regular grids, applicable to a general set of spaces. This is\nhelpful whenever an algorithm explores to reduce uncertainty, such as in active\nlearning, Bayesian optimization, reinforcement learning, and marginalization in\ngraphical models. To draw samples from a DPP in practice, existing literature\nfocuses on approximate schemes of low cost, or comparably inefficient exact\nalgorithms like rejection sampling. We point out that, for many settings of\nrelevance to machine learning, it is also possible to draw exact samples from\nDPPs on continuous domains. We start from an intuitive example on the real\nline, which is then generalized to multivariate real vector spaces. We also\ncompare to previously studied approximations, showing that exact sampling,\ndespite higher cost, can be preferable where precision is needed.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 07:06:28 GMT"}, {"version": "v2", "created": "Tue, 17 Apr 2018 10:21:59 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Hennig", "Philipp", ""], ["Garnett", "Roman", ""]]}, {"id": "1609.06864", "submitter": "Alessandro Magrini", "authors": "Alessandro Magrini, Davide Luciani and Federico Mattia Stefanini", "title": "A probabilistic network for the diagnosis of acute cardiopulmonary\n  diseases", "comments": "The DOI of the article published after peer review was added. A\n  technical detail was added in Section 3.2, Formula 8 (as a consequence, the\n  ID of all the subsequent formulas result augmented by 1 with respect to the\n  previous version). The prior standard deviation of the Gamma distribution in\n  Table 4 was fixed (in the previous version, the prior variance was indicated,\n  instead)", "journal-ref": "Biometrical Journal, 60(1), 174-195, January 2018", "doi": "10.1002/bimj.201600206", "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the development of a probabilistic network for the diagnosis\nof acute cardiopulmonary diseases is presented. This paper is a draft version\nof the article published after peer review in 2018\n(https://doi.org/10.1002/bimj.201600206). A panel of expert physicians\ncollaborated to specify the qualitative part, that is a directed acyclic graph\ndefining a factorization of the joint probability distribution of domain\nvariables. The quantitative part, that is the set of all conditional\nprobability distributions defined by each factor, was estimated in the Bayesian\nparadigm: we applied a special formal representation, characterized by a low\nnumber of parameters and a parameterization intelligible for physicians,\nelicited the joint prior distribution of parameters from medical experts, and\nupdated it by conditioning on a dataset of hospital patient records using\nMarkov Chain Monte Carlo simulation. Refinement was cyclically performed until\nthe probabilistic network provided satisfactory Concordance Index values for a\nselection of acute diseases and reasonable inference on six fictitious patient\ncases. The probabilistic network can be employed to perform medical diagnosis\non a total of 63 diseases (38 acute and 25 chronic) on the basis of up to 167\npatient findings.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 08:28:38 GMT"}, {"version": "v2", "created": "Sat, 12 May 2018 09:38:01 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Magrini", "Alessandro", ""], ["Luciani", "Davide", ""], ["Stefanini", "Federico Mattia", ""]]}, {"id": "1609.06942", "submitter": "Matan Sela", "authors": "Matan Sela and Ron Kimmel", "title": "Randomized Independent Component Analysis", "comments": "Accepted to ICSEE 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SY math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent component analysis (ICA) is a method for recovering statistically\nindependent signals from observations of unknown linear combinations of the\nsources. Some of the most accurate ICA decomposition methods require searching\nfor the inverse transformation which minimizes different approximations of the\nMutual Information, a measure of statistical independence of random vectors.\nTwo such approximations are the Kernel Generalized Variance or the Kernel\nCanonical Correlation which has been shown to reach the highest performance of\nICA methods. However, the computational effort necessary just for computing\nthese measures is cubic in the sample size. Hence, optimizing them becomes even\nmore computationally demanding, in terms of both space and time. Here, we\npropose a couple of alternative novel measures based on randomized features of\nthe samples - the Randomized Generalized Variance and the Randomized Canonical\nCorrelation. The computational complexity of calculating the proposed\nalternatives is linear in the sample size and provide a controllable\napproximation of their Kernel-based non-random versions. We also show that\noptimization of the proposed statistical properties yields a comparable\nseparation error at an order of magnitude faster compared to Kernel-based\nmeasures.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 12:40:58 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Sela", "Matan", ""], ["Kimmel", "Ron", ""]]}, {"id": "1609.06957", "submitter": "Michal Tadeusiak", "authors": "Robert Bogucki, Jan Lasek, Jan Kanty Milczek, Michal Tadeusiak", "title": "Early Warning System for Seismic Events in Coal Mines Using Machine\n  Learning", "comments": "Winner of AAIA'16 Data Mining Challenge: Predicting Dangerous Seismic\n  Events in Active Coal Mines", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document describes an approach to the problem of predicting dangerous\nseismic events in active coal mines up to 8 hours in advance. It was developed\nas a part of the AAIA'16 Data Mining Challenge: Predicting Dangerous Seismic\nEvents in Active Coal Mines. The solutions presented consist of ensembles of\nvarious predictive models trained on different sets of features. The best one\nachieved a winning score of 0.939 AUC.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 09:35:56 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Bogucki", "Robert", ""], ["Lasek", "Jan", ""], ["Milczek", "Jan Kanty", ""], ["Tadeusiak", "Michal", ""]]}, {"id": "1609.07042", "submitter": "Xiang Xiang", "authors": "Xiang Xiang and Trac D. Tran", "title": "Pose-Selective Max Pooling for Measuring Similarity", "comments": "The tutorial and program associated with this paper are available at\n  https://github.com/eglxiang/ytf yet for non-commercial use", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we deal with two challenges for measuring the similarity of\nthe subject identities in practical video-based face recognition - the\nvariation of the head pose in uncontrolled environments and the computational\nexpense of processing videos. Since the frame-wise feature mean is unable to\ncharacterize the pose diversity among frames, we define and preserve the\noverall pose diversity and closeness in a video. Then, identity will be the\nonly source of variation across videos since the pose varies even within a\nsingle video. Instead of simply using all the frames, we select those faces\nwhose pose point is closest to the centroid of the K-means cluster containing\nthat pose point. Then, we represent a video as a bag of frame-wise deep face\nfeatures while the number of features has been reduced from hundreds to K.\nSince the video representation can well represent the identity, now we measure\nthe subject similarity between two videos as the max correlation among all\npossible pairs in the two bags of features. On the official 5,000 video-pairs\nof the YouTube Face dataset for face verification, our algorithm achieves a\ncomparable performance with VGG-face that averages over deep features of all\nframes. Other vision tasks can also benefit from the generic idea of employing\ngeometric cues to improve the descriptiveness of deep features.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 15:59:38 GMT"}, {"version": "v2", "created": "Mon, 26 Sep 2016 18:21:05 GMT"}, {"version": "v3", "created": "Thu, 10 Nov 2016 04:05:29 GMT"}, {"version": "v4", "created": "Mon, 14 Nov 2016 04:10:09 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Xiang", "Xiang", ""], ["Tran", "Trac D.", ""]]}, {"id": "1609.07060", "submitter": "Madhu Advani", "authors": "Madhu Advani, Surya Ganguli", "title": "An equivalence between high dimensional Bayes optimal inference and\n  M-estimation", "comments": "To appear in NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.dis-nn math.ST q-bio.NC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When recovering an unknown signal from noisy measurements, the computational\ndifficulty of performing optimal Bayesian MMSE (minimum mean squared error)\ninference often necessitates the use of maximum a posteriori (MAP) inference, a\nspecial case of regularized M-estimation, as a surrogate. However, MAP is\nsuboptimal in high dimensions, when the number of unknown signal components is\nsimilar to the number of measurements. In this work we demonstrate, when the\nsignal distribution and the likelihood function associated with the noise are\nboth log-concave, that optimal MMSE performance is asymptotically achievable\nvia another M-estimation procedure. This procedure involves minimizing convex\nloss and regularizer functions that are nonlinearly smoothed versions of the\nwidely applied MAP optimization problem. Our findings provide a new heuristic\nderivation and interpretation for recent optimal M-estimators found in the\nsetting of linear measurements and additive noise, and further extend these\nresults to nonlinear measurements with non-additive noise. We numerically\ndemonstrate superior performance of our optimal M-estimators relative to MAP.\nOverall, at the heart of our work is the revelation of a remarkable equivalence\nbetween two seemingly very different computational problems: namely that of\nhigh dimensional Bayesian integration underlying MMSE inference, and high\ndimensional convex optimization underlying M-estimation. In essence we show\nthat the former difficult integral may be computed by solving the latter,\nsimpler optimization problem.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 16:46:18 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Advani", "Madhu", ""], ["Ganguli", "Surya", ""]]}, {"id": "1609.07072", "submitter": "John Herr", "authors": "Kun Yao, John E. Herr, and John Parkhill", "title": "The Many-Body Expansion Combined with Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1063/1.4973380", "report-no": null, "categories": "physics.chem-ph physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fragmentation methods such as the many-body expansion (MBE) are a common\nstrategy to model large systems by partitioning energies into a hierarchy of\ndecreasingly significant contributions. The number of fragments required for\nchemical accuracy is still prohibitively expensive for ab-initio MBE to compete\nwith force field approximations for applications beyond single-point energies.\nAlongside the MBE, empirical models of ab-initio potential energy surfaces have\nimproved, especially non-linear models based on neural networks (NN) which can\nreproduce ab-initio potential energy surfaces rapidly and accurately. Although\nthey are fast, NNs suffer from their own curse of dimensionality; they must be\ntrained on a representative sample of chemical space. In this paper we examine\nthe synergy of the MBE and NN's, and explore their complementarity. The MBE\noffers a systematic way to treat systems of arbitrary size and intelligently\nsample chemical space. NN's reduce, by a factor in excess of $10^6$ the\ncomputational overhead of the MBE and reproduce the accuracy of ab-initio\ncalculations without specialized force fields. We show they are remarkably\ngeneral, providing comparable accuracy with drastically different chemical\nembeddings. To assess this we test a new chemical embedding which can be\ninverted to predict molecules with desired properties.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 17:09:47 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Yao", "Kun", ""], ["Herr", "John E.", ""], ["Parkhill", "John", ""]]}, {"id": "1609.07087", "submitter": "L.A. Prashanth", "authors": "Xiaowei Hu, Prashanth L.A., Andr\\'as Gy\\\"orgy and Csaba Szepesv\\'ari", "title": "(Bandit) Convex Optimization with Biased Noisy Gradient Oracles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms for bandit convex optimization and online learning often rely on\nconstructing noisy gradient estimates, which are then used in appropriately\nadjusted first-order algorithms, replacing actual gradients. Depending on the\nproperties of the function to be optimized and the nature of ``noise'' in the\nbandit feedback, the bias and variance of gradient estimates exhibit various\ntradeoffs. In this paper we propose a novel framework that replaces the\nspecific gradient estimation methods with an abstract oracle. With the help of\nthe new framework we unify previous works, reproducing their results in a clean\nand concise fashion, while, perhaps more importantly, the framework also allows\nus to formally show that to achieve the optimal root-$n$ rate either the\nalgorithms that use existing gradient estimators, or the proof techniques used\nto analyze them have to go beyond what exists today.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 17:56:38 GMT"}, {"version": "v2", "created": "Sat, 4 Jul 2020 22:16:51 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Hu", "Xiaowei", ""], ["A.", "Prashanth L.", ""], ["Gy\u00f6rgy", "Andr\u00e1s", ""], ["Szepesv\u00e1ri", "Csaba", ""]]}, {"id": "1609.07093", "submitter": "Andrew Brock", "authors": "Andrew Brock, Theodore Lim, J.M. Ritchie, Nick Weston", "title": "Neural Photo Editing with Introspective Adversarial Networks", "comments": "10 pages, 7 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasingly photorealistic sample quality of generative image models\nsuggests their feasibility in applications beyond image generation. We present\nthe Neural Photo Editor, an interface that leverages the power of generative\nneural networks to make large, semantically coherent changes to existing\nimages. To tackle the challenge of achieving accurate reconstructions without\nloss of feature quality, we introduce the Introspective Adversarial Network, a\nnovel hybridization of the VAE and GAN. Our model efficiently captures\nlong-range dependencies through use of a computational block based on\nweight-shared dilated convolutions, and improves generalization performance\nwith Orthogonal Regularization, a novel weight regularization method. We\nvalidate our contributions on CelebA, SVHN, and CIFAR-100, and produce samples\nand reconstructions with high visual fidelity.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 18:07:56 GMT"}, {"version": "v2", "created": "Thu, 10 Nov 2016 13:16:21 GMT"}, {"version": "v3", "created": "Mon, 6 Feb 2017 18:46:50 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Brock", "Andrew", ""], ["Lim", "Theodore", ""], ["Ritchie", "J. M.", ""], ["Weston", "Nick", ""]]}, {"id": "1609.07165", "submitter": "Jelena Bradic", "authors": "Jelena Bradic and Jiaqi Guo", "title": "Robust Confidence Intervals in High-Dimensional Left-Censored Regression", "comments": "62 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops robust confidence intervals in high-dimensional and\nleft-censored regression. Type-I censored regression models are extremely\ncommon in practice, where a competing event makes the variable of interest\nunobservable. However, techniques developed for entirely observed data do not\ndirectly apply to the censored observations. In this paper, we develop smoothed\nestimating equations that augment the de-biasing method, such that the\nresulting estimator is adaptive to censoring and is more robust to the\nmisspecification of the error distribution. We propose a unified class of\nrobust estimators, including Mallow's, Schweppe's and Hill-Ryan's one-step\nestimator.\n  In the ultra-high-dimensional setting, where the dimensionality can grow\nexponentially with the sample size, we show that as long as the preliminary\nestimator converges faster than $n^{-1/4}$, the one-step estimator inherits\nasymptotic distribution of fully iterated version. Moreover, we show that the\nsize of the residuals of the Bahadur representation matches those of the simple\nlinear models, $s^{3/4 } (\\log (p \\vee n))^{3/4} / n^{1/4}$ -- that is, the\neffects of censoring asymptotically disappear. Simulation studies demonstrate\nthat our method is adaptive to the censoring level and asymmetry in the error\ndistribution, and does not lose efficiency when the errors are from symmetric\ndistributions. Finally, we apply the developed method to a real data set from\nthe MAQC-II repository that is related to the HIV-1 study.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 21:09:43 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Bradic", "Jelena", ""], ["Guo", "Jiaqi", ""]]}, {"id": "1609.07195", "submitter": "Johannes Lederer", "authors": "Mahsa Taheri, N\\'eh\\'emy Lim, and Johannes Lederer", "title": "Balancing Statistical and Computational Precision and Applications to\n  Penalized Linear Regression with Group Sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to technological advances, large and high-dimensional data have become\nthe rule rather than the exception. Methods that allow for feature selection\nwith such data are thus highly sought after, in particular, since standard\nmethods, such as cross-validated lasso and group-lasso, can be challenging both\ncomputationally and mathematically. In this paper, we propose a novel approach\nto feature selection and group feature selection in linear regression. It\nconsists of simple optimization steps and tests, which makes it computationally\nmore efficient than standard approaches and suitable even for very large data\nsets. Moreover, it satisfies sharp guarantees for estimation and feature\nselection in terms of oracle inequalities. We thus expect that our contribution\ncan help to leverage the increasing volume of data in Biology, Public Health,\nAstronomy, Economics, and other fields.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 00:35:23 GMT"}, {"version": "v2", "created": "Sat, 30 Nov 2019 20:31:06 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Taheri", "Mahsa", ""], ["Lim", "N\u00e9h\u00e9my", ""], ["Lederer", "Johannes", ""]]}, {"id": "1609.07200", "submitter": "Pin-Yu Chen", "authors": "Pin-Yu Chen and Alfred O. Hero III", "title": "Multilayer Spectral Graph Clustering via Convex Layer Aggregation", "comments": "To appear at IEEE GlobalSIP 2016", "journal-ref": "IEEE Tran. Signal and Information Processing over Networks, 2017\n  https://arxiv.org/abs/1708.02620", "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilayer graphs are commonly used for representing different relations\nbetween entities and handling heterogeneous data processing tasks. New\nchallenges arise in multilayer graph clustering for assigning clusters to a\ncommon multilayer node set and for combining information from each layer. This\npaper presents a theoretical framework for multilayer spectral graph clustering\nof the nodes via convex layer aggregation. Under a novel multilayer signal plus\nnoise model, we provide a phase transition analysis that establishes the\nexistence of a critical value on the noise level that permits reliable cluster\nseparation. The analysis also specifies analytical upper and lower bounds on\nthe critical value, where the bounds become exact when the clusters have\nidentical sizes. Numerical experiments on synthetic multilayer graphs are\nconducted to validate the phase transition analysis and study the effect of\nlayer weights and noise levels on clustering reliability.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 01:16:46 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Chen", "Pin-Yu", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1609.07236", "submitter": "Suresh Venkatasubramanian", "authors": "Sorelle A. Friedler and Carlos Scheidegger and Suresh\n  Venkatasubramanian", "title": "On the (im)possibility of fairness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What does it mean for an algorithm to be fair? Different papers use different\nnotions of algorithmic fairness, and although these appear internally\nconsistent, they also seem mutually incompatible. We present a mathematical\nsetting in which the distinctions in previous papers can be made formal. In\naddition to characterizing the spaces of inputs (the \"observed\" space) and\noutputs (the \"decision\" space), we introduce the notion of a construct space: a\nspace that captures unobservable, but meaningful variables for the prediction.\n  We show that in order to prove desirable properties of the entire\ndecision-making process, different mechanisms for fairness require different\nassumptions about the nature of the mapping from construct space to decision\nspace. The results in this paper imply that future treatments of algorithmic\nfairness should more explicitly state assumptions about the relationship\nbetween constructs and observations.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 05:38:20 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Friedler", "Sorelle A.", ""], ["Scheidegger", "Carlos", ""], ["Venkatasubramanian", "Suresh", ""]]}, {"id": "1609.07257", "submitter": "Tomas Pevny", "authors": "Tomas Pevny and Petr Somol", "title": "Using Neural Network Formalism to Solve Multiple-Instance Problems", "comments": "Accepted to International Symposium on Neural Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many objects in the real world are difficult to describe by a single\nnumerical vector of a fixed length, whereas describing them by a set of vectors\nis more natural. Therefore, Multiple instance learning (MIL) techniques have\nbeen constantly gaining on importance throughout last years. MIL formalism\nrepresents each object (sample) by a set (bag) of feature vectors (instances)\nof fixed length where knowledge about objects (e.g., class label) is available\non bag level but not necessarily on instance level. Many standard tools\nincluding supervised classifiers have been already adapted to MIL setting since\nthe problem got formalized in late nineties. In this work we propose a neural\nnetwork (NN) based formalism that intuitively bridges the gap between MIL\nproblem definition and the vast existing knowledge-base of standard models and\nclassifiers. We show that the proposed NN formalism is effectively optimizable\nby a modified back-propagation algorithm and can reveal unknown patterns inside\nbags. Comparison to eight types of classifiers from the prior art on a set of\n14 publicly available benchmark datasets confirms the advantages and accuracy\nof the proposed solution.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 07:40:12 GMT"}, {"version": "v2", "created": "Sun, 16 Oct 2016 11:15:43 GMT"}, {"version": "v3", "created": "Tue, 7 Mar 2017 06:38:36 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Pevny", "Tomas", ""], ["Somol", "Petr", ""]]}, {"id": "1609.07272", "submitter": "Toon Van Craenendonck", "authors": "Toon Van Craenendonck, Hendrik Blockeel", "title": "Constraint-Based Clustering Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised clustering methods incorporate a limited amount of\nsupervision into the clustering process. Typically, this supervision is\nprovided by the user in the form of pairwise constraints. Existing methods use\nsuch constraints in one of the following ways: they adapt their clustering\nprocedure, their similarity metric, or both. All of these approaches operate\nwithin the scope of individual clustering algorithms. In contrast, we propose\nto use constraints to choose between clusterings generated by very different\nunsupervised clustering algorithms, run with different parameter settings. We\nempirically show that this simple approach often outperforms existing\nsemi-supervised clustering methods.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 08:51:14 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Van Craenendonck", "Toon", ""], ["Blockeel", "Hendrik", ""]]}, {"id": "1609.07333", "submitter": "Avid Afzal", "authors": "Hamse Y. Mussa and Avid M. Afzal", "title": "Estimating Probability Distributions using \"Dirac\" Kernels (via\n  Rademacher-Walsh Polynomial Basis Functions)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications (in particular information systems, such as pattern\nrecognition, machine learning, cheminformatics, bioinformatics to name but a\nfew) the assessment of uncertainty is essential - i.e., the estimation of the\nunderlying probability distribution function. More often than not, the form of\nthis function is unknown and it becomes necessary to non-parametrically\nconstruct/estimate it from a given sample. One of the methods of choice to\nnon-parametrically estimate the unknown probability distribution function for a\ngiven random variable (defined on binary space) has been the expansion of the\nestimation function in Rademacher-Walsh Polynomial basis functions. In this\npaper we demonstrate that the expansion of the probability distribution\nfunction estimation in Rademacher-Walsh Polynomial basis functions is\nequivalent to the expansion of the function estimation in a set of \"Dirac\nkernel\" functions. The latter approach can ameliorate the computational\nbottleneck and notational awkwardness often associated with the\nRademacher-Walsh Polynomial basis functions approach, in particular when the\nbinary input space is large.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 12:11:54 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Mussa", "Hamse Y.", ""], ["Afzal", "Avid M.", ""]]}, {"id": "1609.07363", "submitter": "Paul Fearnhead", "authors": "Paul Fearnhead and Guillem Rigaill", "title": "Changepoint Detection in the Presence of Outliers", "comments": "Updated to include a proof of consistency and accuracy of estimating\n  change points using the biweight loss function", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many traditional methods for identifying changepoints can struggle in the\npresence of outliers, or when the noise is heavy-tailed. Often they will infer\nadditional changepoints in order to fit the outliers. To overcome this problem,\ndata often needs to be pre-processed to remove outliers, though this is\ndifficult for applications where the data needs to be analysed online. We\npresent an approach to changepoint detection that is robust to the presence of\noutliers. The idea is to adapt existing penalised cost approaches for detecting\nchanges so that they use loss functions that are less sensitive to outliers. We\nargue that loss functions that are bounded, such as the classical biweight\nloss, are particularly suitable -- as we show that only bounded loss functions\nare robust to arbitrarily extreme outliers. We present an efficient dynamic\nprogramming algorithm that can find the optimal segmentation under our\npenalised cost criteria. Importantly, this algorithm can be used in settings\nwhere the data needs to be analysed online. We show that we can consistently\nestimate the number of changepoints, and accurately estimate their locations,\nusing the biweight loss function. We demonstrate the usefulness of our approach\nfor applications such as analysing well-log data, detecting copy number\nvariation, and detecting tampering of wireless devices.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 13:49:23 GMT"}, {"version": "v2", "created": "Tue, 11 Jul 2017 10:56:16 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Fearnhead", "Paul", ""], ["Rigaill", "Guillem", ""]]}, {"id": "1609.07386", "submitter": "Aaron Molstad", "authors": "Aaron J. Molstad and Adam J. Rothman", "title": "A penalized likelihood method for classification with matrix-valued\n  predictors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a penalized likelihood method to fit the linear discriminant\nanalysis model when the predictor is matrix valued. We simultaneously estimate\nthe means and the precision matrix, which we assume has a Kronecker product\ndecomposition. Our penalties encourage pairs of response category mean matrices\nto have equal entries and also encourage zeros in the precision matrix. To\ncompute our estimators, we use a blockwise coordinate descent algorithm. To\nupdate the optimization variables corresponding to response category mean\nmatrices, we use an alternating minimization algorithm that takes advantage of\nthe Kronecker structure of the precision matrix. We show that our method can\noutperform relevant competitors in classification, even when our modeling\nassumptions are violated. We analyze an EEG dataset to demonstrate our method's\ninterpretability and classification accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 14:40:47 GMT"}, {"version": "v2", "created": "Fri, 28 Oct 2016 03:42:49 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Molstad", "Aaron J.", ""], ["Rothman", "Adam J.", ""]]}, {"id": "1609.07410", "submitter": "Michalis Titsias", "authors": "Michalis K. Titsias", "title": "One-vs-Each Approximation to Softmax for Scalable Estimation of\n  Probabilities", "comments": "To appear in NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The softmax representation of probabilities for categorical variables plays a\nprominent role in modern machine learning with numerous applications in areas\nsuch as large scale classification, neural language modeling and recommendation\nsystems. However, softmax estimation is very expensive for large scale\ninference because of the high cost associated with computing the normalizing\nconstant. Here, we introduce an efficient approximation to softmax\nprobabilities which takes the form of a rigorous lower bound on the exact\nprobability. This bound is expressed as a product over pairwise probabilities\nand it leads to scalable estimation based on stochastic optimization. It allows\nus to perform doubly stochastic estimation by subsampling both training\ninstances and class labels. We show that the new bound has interesting\ntheoretical properties and we demonstrate its use in classification problems.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 16:05:53 GMT"}, {"version": "v2", "created": "Sat, 29 Oct 2016 14:44:16 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Titsias", "Michalis K.", ""]]}, {"id": "1609.07478", "submitter": "Martin Jaggi", "authors": "Anant Raj, Jakob Olbrich, Bernd G\\\"artner, Bernhard Sch\\\"olkopf,\n  Martin Jaggi", "title": "Screening Rules for Convex Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework for deriving screening rules for convex\noptimization problems. Our approach covers a large class of constrained and\npenalized optimization formulations, and works in two steps. First, given any\napproximate point, the structure of the objective function and the duality gap\nis used to gather information on the optimal solution. In the second step, this\ninformation is used to produce screening rules, i.e. safely identifying\nunimportant weight variables of the optimal solution. Our general framework\nleads to a large variety of useful existing as well as new screening rules for\nmany applications. For example, we provide new screening rules for general\nsimplex and $L_1$-constrained problems, Elastic Net, squared-loss Support\nVector Machines, minimum enclosing ball, as well as structured norm regularized\nproblems, such as group lasso.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 19:59:50 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Raj", "Anant", ""], ["Olbrich", "Jakob", ""], ["G\u00e4rtner", "Bernd", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Jaggi", "Martin", ""]]}, {"id": "1609.07480", "submitter": "Stylianos Kampakis", "authors": "Stylianos Kampakis", "title": "Predictive modelling of football injuries", "comments": "PhD Thesis submitted and defended successfully at the Department of\n  Computer Science at University College London", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this thesis is to investigate the potential of predictive\nmodelling for football injuries. This work was conducted in close collaboration\nwith Tottenham Hotspurs FC (THFC), the PGA European tour and the participation\nof Wolverhampton Wanderers (WW).\n  Three investigations were conducted:\n  1. Predicting the recovery time of football injuries using the UEFA injury\nrecordings: The UEFA recordings is a common standard for recording injuries in\nprofessional football. For this investigation, three datasets of UEFA injury\nrecordings were available. Different machine learning algorithms were used in\norder to build a predictive model. The performance of the machine learning\nmodels is then improved by using feature selection conducted through\ncorrelation-based subset feature selection and random forests.\n  2. Predicting injuries in professional football using exposure records: The\nrelationship between exposure (in training hours and match hours) in\nprofessional football athletes and injury incidence was studied. A common\nproblem in football is understanding how the training schedule of an athlete\ncan affect the chance of him getting injured. The task was to predict the\nnumber of days a player can train before he gets injured.\n  3. Predicting intrinsic injury incidence using in-training GPS measurements:\nA significant percentage of football injuries can be attributed to overtraining\nand fatigue. GPS data collected during training sessions might provide\nindicators of fatigue, or might be used to detect very intense training\nsessions which can lead to overtraining. This research used GPS data gathered\nduring training sessions of the first team of THFC, in order to predict whether\nan injury would take place during a week.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 11:58:42 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Kampakis", "Stylianos", ""]]}, {"id": "1609.07521", "submitter": "Michael Hughes", "authors": "Michael C. Hughes and Erik B. Sudderth", "title": "Fast Learning of Clusters and Topics via Sparse Posteriors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture models and topic models generate each observation from a single\ncluster, but standard variational posteriors for each observation assign\npositive probability to all possible clusters. This requires dense storage and\nruntime costs that scale with the total number of clusters, even though\ntypically only a few clusters have significant posterior mass for any data\npoint. We propose a constrained family of sparse variational distributions that\nallow at most $L$ non-zero entries, where the tunable threshold $L$ trades off\nspeed for accuracy. Previous sparse approximations have used hard assignments\n($L=1$), but we find that moderate values of $L>1$ provide superior\nperformance. Our approach easily integrates with stochastic or incremental\noptimization algorithms to scale to millions of examples. Experiments training\nmixture models of image patches and topic models for news articles show that\nour approach produces better-quality models in far less time than baseline\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 21:18:31 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Hughes", "Michael C.", ""], ["Sudderth", "Erik B.", ""]]}, {"id": "1609.07537", "submitter": "Cesar A. Uribe", "authors": "Angelia Nedi\\'c, Alex Olshevsky and C\\'esar A. Uribe", "title": "A Tutorial on Distributed (Non-Bayesian) Learning: Problem, Algorithms\n  and Results", "comments": "Tutorial Presented in CDC2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.MA cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We overview some results on distributed learning with focus on a family of\nrecently proposed algorithms known as non-Bayesian social learning. We consider\ndifferent approaches to the distributed learning problem and its algorithmic\nsolutions for the case of finitely many hypotheses. The original centralized\nproblem is discussed at first, and then followed by a generalization to the\ndistributed setting. The results on convergence and convergence rate are\npresented for both asymptotic and finite time regimes. Various extensions are\ndiscussed such as those dealing with directed time-varying networks, Nesterov's\nacceleration technique and a continuum sets of hypothesis.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 23:12:06 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Nedi\u0107", "Angelia", ""], ["Olshevsky", "Alex", ""], ["Uribe", "C\u00e9sar A.", ""]]}, {"id": "1609.07560", "submitter": "Lantao Liu", "authors": "Kai-Chieh Ma, Lantao Liu, Gaurav S. Sukhatme", "title": "Informative Planning and Online Learning with Sparse Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A big challenge in environmental monitoring is the spatiotemporal variation\nof the phenomena to be observed. To enable persistent sensing and estimation in\nsuch a setting, it is beneficial to have a time-varying underlying\nenvironmental model. Here we present a planning and learning method that\nenables an autonomous marine vehicle to perform persistent ocean monitoring\ntasks by learning and refining an environmental model. To alleviate the\ncomputational bottleneck caused by large-scale data accumulated, we propose a\nframework that iterates between a planning component aimed at collecting the\nmost information-rich data, and a sparse Gaussian Process learning component\nwhere the environmental model and hyperparameters are learned online by taking\nadvantage of only a subset of data that provides the greatest contribution. Our\nsimulations with ground-truth ocean data shows that the proposed method is both\naccurate and efficient.\n", "versions": [{"version": "v1", "created": "Sat, 24 Sep 2016 02:56:25 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Ma", "Kai-Chieh", ""], ["Liu", "Lantao", ""], ["Sukhatme", "Gaurav S.", ""]]}, {"id": "1609.07574", "submitter": "Adel Javanmard", "authors": "Adel Javanmard and Hamid Nazerzadeh", "title": "Dynamic Pricing in High-dimensions", "comments": "47 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the pricing problem faced by a firm that sells a large number of\nproducts, described via a wide range of features, to customers that arrive over\ntime. Customers independently make purchasing decisions according to a general\nchoice model that includes products features and customers' characteristics,\nencoded as $d$-dimensional numerical vectors, as well as the price offered. The\nparameters of the choice model are a priori unknown to the firm, but can be\nlearned as the (binary-valued) sales data accrues over time. The firm's\nobjective is to minimize the regret, i.e., the expected revenue loss against a\nclairvoyant policy that knows the parameters of the choice model in advance,\nand always offers the revenue-maximizing price. This setting is motivated in\npart by the prevalence of online marketplaces that allow for real-time pricing.\nWe assume a structured choice model, parameters of which depend on $s_0$ out of\nthe $d$ product features. We propose a dynamic policy, called Regularized\nMaximum Likelihood Pricing (RMLP) that leverages the (sparsity) structure of\nthe high-dimensional model and obtains a logarithmic regret in $T$. More\nspecifically, the regret of our algorithm is of $O(s_0 \\log d \\cdot \\log T)$.\nFurthermore, we show that no policy can obtain regret better than $O(s_0 (\\log\nd + \\log T))$.\n", "versions": [{"version": "v1", "created": "Sat, 24 Sep 2016 06:02:24 GMT"}, {"version": "v2", "created": "Fri, 14 Oct 2016 21:11:00 GMT"}, {"version": "v3", "created": "Mon, 21 Nov 2016 02:48:54 GMT"}, {"version": "v4", "created": "Mon, 1 Jan 2018 04:00:13 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Javanmard", "Adel", ""], ["Nazerzadeh", "Hamid", ""]]}, {"id": "1609.07664", "submitter": "Wen-Xin Zhou", "authors": "Ethan X. Fang, Han Liu, Kim-Chuan Toh and Wen-Xin Zhou", "title": "Max-Norm Optimization for Robust Matrix Recovery", "comments": "32 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the matrix completion problem under arbitrary sampling\nschemes. We propose a new estimator incorporating both max-norm and\nnuclear-norm regularization, based on which we can conduct efficient low-rank\nmatrix recovery using a random subset of entries observed with additive noise\nunder general non-uniform and unknown sampling distributions. This method\nsignificantly relaxes the uniform sampling assumption imposed for the widely\nused nuclear-norm penalized approach, and makes low-rank matrix recovery\nfeasible in more practical settings. Theoretically, we prove that the proposed\nestimator achieves fast rates of convergence under different settings.\nComputationally, we propose an alternating direction method of multipliers\nalgorithm to efficiently compute the estimator, which bridges a gap between\ntheory and practice of machine learning methods with max-norm regularization.\nFurther, we provide thorough numerical studies to evaluate the proposed method\nusing both simulated and real datasets.\n", "versions": [{"version": "v1", "created": "Sat, 24 Sep 2016 19:46:58 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Fang", "Ethan X.", ""], ["Liu", "Han", ""], ["Toh", "Kim-Chuan", ""], ["Zhou", "Wen-Xin", ""]]}, {"id": "1609.07912", "submitter": "Antoine Tixier", "authors": "Antoine J.-P. Tixier, Matthew R. Hallowell, Balaji Rajagopalan", "title": "Construction Safety Risk Modeling and Simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By building on a recently introduced genetic-inspired attribute-based\nconceptual framework for safety risk analysis, we propose a novel methodology\nto compute construction univariate and bivariate construction safety risk at a\nsituational level. Our fully data-driven approach provides construction\npractitioners and academicians with an easy and automated way of extracting\nvaluable empirical insights from databases of unstructured textual injury\nreports. By applying our methodology on an attribute and outcome dataset\ndirectly obtained from 814 injury reports, we show that the frequency-magnitude\ndistribution of construction safety risk is very similar to that of natural\nphenomena such as precipitation or earthquakes. Motivated by this observation,\nand drawing on state-of-the-art techniques in hydroclimatology and insurance,\nwe introduce univariate and bivariate nonparametric stochastic safety risk\ngenerators, based on Kernel Density Estimators and Copulas. These generators\nenable the user to produce large numbers of synthetic safety risk values\nfaithfully to the original data, allowing safetyrelated decision-making under\nuncertainty to be grounded on extensive empirical evidence. Just like the\naccurate modeling and simulation of natural phenomena such as wind or\nstreamflow is indispensable to successful structure dimensioning or water\nreservoir management, we posit that improving construction safety calls for the\naccurate modeling, simulation, and assessment of safety risk. The underlying\nassumption is that like natural phenomena, construction safety may benefit from\nbeing studied in an empirical and quantitative way rather than qualitatively\nwhich is the current industry standard. Finally, a side but interesting finding\nis that attributes related to high energy levels and to human error emerge as\nstrong risk shapers on the dataset we used to illustrate our methodology.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 10:19:02 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Tixier", "Antoine J. -P.", ""], ["Hallowell", "Matthew R.", ""], ["Rajagopalan", "Balaji", ""]]}, {"id": "1609.07959", "submitter": "Benjamin Krause", "authors": "Ben Krause, Liang Lu, Iain Murray, Steve Renals", "title": "Multiplicative LSTM for sequence modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce multiplicative LSTM (mLSTM), a recurrent neural network\narchitecture for sequence modelling that combines the long short-term memory\n(LSTM) and multiplicative recurrent neural network architectures. mLSTM is\ncharacterised by its ability to have different recurrent transition functions\nfor each possible input, which we argue makes it more expressive for\nautoregressive density estimation. We demonstrate empirically that mLSTM\noutperforms standard LSTM and its deep variants for a range of character level\nlanguage modelling tasks. In this version of the paper, we regularise mLSTM to\nachieve 1.27 bits/char on text8 and 1.24 bits/char on Hutter Prize. We also\napply a purely byte-level mLSTM on the WikiText-2 dataset to achieve a\ncharacter level entropy of 1.26 bits/char, corresponding to a word level\nperplexity of 88.8, which is comparable to word level LSTMs regularised in\nsimilar ways on the same task.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 13:12:51 GMT"}, {"version": "v2", "created": "Tue, 25 Oct 2016 10:30:16 GMT"}, {"version": "v3", "created": "Thu, 12 Oct 2017 17:05:47 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Krause", "Ben", ""], ["Lu", "Liang", ""], ["Murray", "Iain", ""], ["Renals", "Steve", ""]]}, {"id": "1609.08017", "submitter": "Xuezhe Ma", "authors": "Xuezhe Ma, Yingkai Gao, Zhiting Hu, Yaoliang Yu, Yuntian Deng, Eduard\n  Hovy", "title": "Dropout with Expectation-linear Regularization", "comments": "Published as a conference paper at ICLR 2017. Camera-ready Version.\n  23 pages (paper + appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dropout, a simple and effective way to train deep neural networks, has led to\na number of impressive empirical successes and spawned many recent theoretical\ninvestigations. However, the gap between dropout's training and inference\nphases, introduced due to tractability considerations, has largely remained\nunder-appreciated. In this work, we first formulate dropout as a tractable\napproximation of some latent variable model, leading to a clean view of\nparameter sharing and enabling further theoretical analysis. Then, we introduce\n(approximate) expectation-linear dropout neural networks, whose inference gap\nwe are able to formally characterize. Algorithmically, we show that our\nproposed measure of the inference gap can be used to regularize the standard\ndropout training objective, resulting in an \\emph{explicit} control of the gap.\nOur method is as simple and efficient as standard dropout. We further prove the\nupper bounds on the loss in accuracy due to expectation-linearization, describe\nclasses of input distributions that expectation-linearize easily. Experiments\non three image classification benchmark datasets demonstrate that reducing the\ninference gap can indeed improve the performance consistently.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 15:14:05 GMT"}, {"version": "v2", "created": "Fri, 28 Oct 2016 18:04:11 GMT"}, {"version": "v3", "created": "Wed, 15 Feb 2017 19:40:29 GMT"}], "update_date": "2017-02-17", "authors_parsed": [["Ma", "Xuezhe", ""], ["Gao", "Yingkai", ""], ["Hu", "Zhiting", ""], ["Yu", "Yaoliang", ""], ["Deng", "Yuntian", ""], ["Hovy", "Eduard", ""]]}, {"id": "1609.08039", "submitter": "Evgeny Burnaev", "authors": "Evgeny Burnaev and Dmitry Smolyakov", "title": "One-Class SVM with Privileged Information and its Application to Malware\n  Detection", "comments": "8 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of important applied problems in engineering, finance and medicine\ncan be formulated as a problem of anomaly detection. A classical approach to\nthe problem is to describe a normal state using a one-class support vector\nmachine. Then to detect anomalies we quantify a distance from a new observation\nto the constructed description of the normal class. In this paper we present a\nnew approach to the one-class classification. We formulate a new problem\nstatement and a corresponding algorithm that allow taking into account a\nprivileged information during the training phase. We evaluate performance of\nthe proposed approach using a synthetic dataset, as well as the publicly\navailable Microsoft Malware Classification Challenge dataset.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 16:01:02 GMT"}, {"version": "v2", "created": "Sun, 20 Nov 2016 16:31:46 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Burnaev", "Evgeny", ""], ["Smolyakov", "Dmitry", ""]]}, {"id": "1609.08201", "submitter": "Shahriar Shariat", "authors": "Shahriar Shariat and Vladimir Pavlovic", "title": "Robust Time-Series Retrieval Using Probabilistic Adaptive Segmental\n  Alignment", "comments": null, "journal-ref": "Knowl Inf Syst (2016) 49: 91. doi:10.1007/s10115-015-0898-4", "doi": "10.1007/s10115-015-0898-4", "report-no": null, "categories": "cs.DB cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional pairwise sequence alignment is based on matching individual\nsamples from two sequences, under time monotonicity constraints. However, in\nmany application settings matching subsequences (segments) instead of\nindividual samples may bring in additional robustness to noise or local\nnon-causal perturbations. This paper presents an approach to segmental sequence\nalignment that jointly segments and aligns two sequences, generalizing the\ntraditional per-sample alignment. To accomplish this task, we introduce a\ndistance metric between segments based on average pairwise distances and then\npresent a modified pair-HMM (PHMM) that incorporates the proposed distance\nmetric to solve the joint segmentation and alignment task. We also propose a\nrelaxation to our model that improves the computational efficiency of the\ngeneric segmental PHMM. Our results demonstrate that this new measure of\nsequence similarity can lead to improved classification performance, while\nbeing resilient to noise, on a variety of sequence retrieval problems, from EEG\nto motion sequence classification.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 21:53:42 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Shariat", "Shahriar", ""], ["Pavlovic", "Vladimir", ""]]}, {"id": "1609.08203", "submitter": "Maximilian Karl", "authors": "Christopher Wolf, Maximilian Karl, Patrick van der Smagt", "title": "Variational Inference with Hamiltonian Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference lies at the core of many state-of-the-art algorithms.\nTo improve the approximation of the posterior beyond parametric families, it\nwas proposed to include MCMC steps into the variational lower bound. In this\nwork we explore this idea using steps of the Hamiltonian Monte Carlo (HMC)\nalgorithm, an efficient MCMC method. In particular, we incorporate the\nacceptance step of the HMC algorithm, guaranteeing asymptotic convergence to\nthe true posterior. Additionally, we introduce some extensions to the HMC\nalgorithm geared towards faster convergence. The theoretical advantages of\nthese modifications are reflected by performance improvements in our\nexperimental results.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 22:01:04 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Wolf", "Christopher", ""], ["Karl", "Maximilian", ""], ["van der Smagt", "Patrick", ""]]}, {"id": "1609.08209", "submitter": "Evgeny Burnaev", "authors": "Evgeny Burnaev and Ivan Koptelov and German Novikov and Timur Khanipov", "title": "Automatic Construction of a Recurrent Neural Network based Classifier\n  for Vehicle Passage Detection", "comments": "6 pages, 2 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) are extensively used for time-series\nmodeling and prediction. We propose an approach for automatic construction of a\nbinary classifier based on Long Short-Term Memory RNNs (LSTM-RNNs) for\ndetection of a vehicle passage through a checkpoint. As an input to the\nclassifier we use multidimensional signals of various sensors that are\ninstalled on the checkpoint. Obtained results demonstrate that the previous\napproach to handcrafting a classifier, consisting of a set of deterministic\nrules, can be successfully replaced by an automatic RNN training on an\nappropriately labelled data.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 22:11:05 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Burnaev", "Evgeny", ""], ["Koptelov", "Ivan", ""], ["Novikov", "German", ""], ["Khanipov", "Timur", ""]]}, {"id": "1609.08235", "submitter": "Yanning Shen", "authors": "Yanning Shen, Morteza Mardani, Georgios B. Giannakis", "title": "Online Categorical Subspace Learning for Sketching Big Data with Misses", "comments": "13 pages", "journal-ref": null, "doi": "10.1109/TSP.2017.2701333", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the scale of data growing every day, reducing the dimensionality (a.k.a.\nsketching) of high-dimensional data has emerged as a task of paramount\nimportance. Relevant issues to address in this context include the sheer volume\nof data that may consist of categorical samples, the typically streaming format\nof acquisition, and the possibly missing entries. To cope with these\nchallenges, the present paper develops a novel categorical subspace learning\napproach to unravel the latent structure for three prominent categorical\n(bilinear) models, namely, Probit, Tobit, and Logit. The deterministic Probit\nand Tobit models treat data as quantized values of an analog-valued process\nlying in a low-dimensional subspace, while the probabilistic Logit model relies\non low dimensionality of the data log-likelihood ratios. Leveraging the low\nintrinsic dimensionality of the sought models, a rank regularized\nmaximum-likelihood estimator is devised, which is then solved recursively via\nalternating majorization-minimization to sketch high-dimensional categorical\ndata `on the fly.' The resultant procedure alternates between sketching the new\nincomplete datum and refining the latent subspace, leading to lightweight\nfirst-order algorithms with highly parallelizable tasks per iteration. As an\nextra degree of freedom, the quantization thresholds are also learned jointly\nalong with the subspace to enhance the predictive power of the sought models.\nPerformance of the subspace iterates is analyzed for both infinite and finite\ndata streams, where for the former asymptotic convergence to the stationary\npoint set of the batch estimator is established, while for the latter sublinear\nregret bounds are derived for the empirical cost. Simulated tests with both\nsynthetic and real-world datasets corroborate the merits of the novel schemes\nfor real-time movie recommendation and chess-game classification.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 01:15:35 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Shen", "Yanning", ""], ["Mardani", "Morteza", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1609.08349", "submitter": "Luca Martino", "authors": "Jesse Read, Luca Martino, Jaakko Hollm\\'en", "title": "Multi-label Methods for Prediction with Sequential Data", "comments": null, "journal-ref": "Pattern Recognition, Volume 63, Pages 45-55, 2017", "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of methods available for classification of multi-label data has\nincreased rapidly over recent years, yet relatively few links have been made\nwith the related task of classification of sequential data. If labels indices\nare considered as time indices, the problems can often be seen as equivalent.\nIn this paper we detect and elaborate on connections between multi-label\nmethods and Markovian models, and study the suitability of multi-label methods\nfor prediction in sequential data. From this study we draw upon the most\nsuitable techniques from the area and develop two novel competitive approaches\nwhich can be applied to either kind of data. We carry out an empirical\nevaluation investigating performance on real-world sequential-prediction tasks:\nelectricity demand, and route prediction. As well as showing that several\npopular multi-label algorithms are in fact easily applicable to sequencing\ntasks, our novel approaches, which benefit from a unified view of these areas,\nprove very competitive against established methods.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 10:53:37 GMT"}, {"version": "v2", "created": "Thu, 29 Sep 2016 09:02:28 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Read", "Jesse", ""], ["Martino", "Luca", ""], ["Hollm\u00e9n", "Jaakko", ""]]}, {"id": "1609.08391", "submitter": "Luca Masera", "authors": "Luca Masera", "title": "Multiple protein feature prediction with statistical relational learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High throughput sequencing techniques have highly impactedon modern biology,\nwidening the gap between sequenced andannotated data. Automatic annotation\ntools are thereforeof the foremost importance to guide biologists' experiments.\nHowever, most of the state-of-the-art methods rely on annotation transfer,\noffering reliable predictions only in homology settings. In this work we\npresent a novel appraoch to protein feature prediction, which exploits the\nSemanti Based Regularization to inject prior knowledge in the learning process.\nThe experimental results conducted on the yeast genome show that the\nintroduction of the constraints positively impacts on the overall prediction\nquality.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 12:57:05 GMT"}], "update_date": "2016-10-02", "authors_parsed": [["Masera", "Luca", ""]]}, {"id": "1609.08397", "submitter": "Qi Meng", "authors": "Qi Meng, Yue Wang, Wei Chen, Taifeng Wang, Zhi-Ming Ma, and Tie-Yan\n  Liu", "title": "Generalization Error Bounds for Optimization Algorithms via Stability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning tasks can be formulated as Regularized Empirical Risk\nMinimization (R-ERM), and solved by optimization algorithms such as gradient\ndescent (GD), stochastic gradient descent (SGD), and stochastic variance\nreduction (SVRG). Conventional analysis on these optimization algorithms\nfocuses on their convergence rates during the training process, however, people\nin the machine learning community may care more about the generalization\nperformance of the learned model on unseen test data. In this paper, we\ninvestigate on this issue, by using stability as a tool. In particular, we\ndecompose the generalization error for R-ERM, and derive its upper bound for\nboth convex and non-convex cases. In convex cases, we prove that the\ngeneralization error can be bounded by the convergence rate of the optimization\nalgorithm and the stability of the R-ERM process, both in expectation (in the\norder of $\\mathcal{O}((1/n)+\\mathbb{E}\\rho(T))$, where $\\rho(T)$ is the\nconvergence error and $T$ is the number of iterations) and in high probability\n(in the order of\n$\\mathcal{O}\\left(\\frac{\\log{1/\\delta}}{\\sqrt{n}}+\\rho(T)\\right)$ with\nprobability $1-\\delta$). For non-convex cases, we can also obtain a similar\nexpected generalization error bound. Our theorems indicate that 1) along with\nthe training process, the generalization error will decrease for all the\noptimization algorithms under our investigation; 2) Comparatively speaking,\nSVRG has better generalization ability than GD and SGD. We have conducted\nexperiments on both convex and non-convex problems, and the experimental\nresults verify our theoretical findings.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 13:10:57 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Meng", "Qi", ""], ["Wang", "Yue", ""], ["Chen", "Wei", ""], ["Wang", "Taifeng", ""], ["Ma", "Zhi-Ming", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1609.08409", "submitter": "Giovanni Montana", "authors": "Savelie Cornegruta, Robert Bakewell, Samuel Withey, Giovanni Montana", "title": "Modelling Radiological Language with Bidirectional Long Short-Term\n  Memory Networks", "comments": "LOUHI 2016 conference proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the need to automate medical information extraction from\nfree-text radiological reports, we present a bi-directional long short-term\nmemory (BiLSTM) neural network architecture for modelling radiological\nlanguage. The model has been used to address two NLP tasks: medical\nnamed-entity recognition (NER) and negation detection. We investigate whether\nlearning several types of word embeddings improves BiLSTM's performance on\nthose tasks. Using a large dataset of chest x-ray reports, we compare the\nproposed model to a baseline dictionary-based NER system and a negation\ndetection system that leverages the hand-crafted rules of the NegEx algorithm\nand the grammatical relations obtained from the Stanford Dependency Parser.\nCompared to these more traditional rule-based systems, we argue that BiLSTM\noffers a strong alternative for both our tasks.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 13:25:10 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Cornegruta", "Savelie", ""], ["Bakewell", "Robert", ""], ["Withey", "Samuel", ""], ["Montana", "Giovanni", ""]]}, {"id": "1609.08502", "submitter": "Raghu Bollapragada", "authors": "Raghu Bollapragada, Richard Byrd and Jorge Nocedal", "title": "Exact and Inexact Subsampled Newton Methods for Optimization", "comments": "37 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper studies the solution of stochastic optimization problems in which\napproximations to the gradient and Hessian are obtained through subsampling. We\nfirst consider Newton-like methods that employ these approximations and discuss\nhow to coordinate the accuracy in the gradient and Hessian to yield a\nsuperlinear rate of convergence in expectation. The second part of the paper\nanalyzes an inexact Newton method that solves linear systems approximately\nusing the conjugate gradient (CG) method, and that samples the Hessian and not\nthe gradient (the gradient is assumed to be exact). We provide a complexity\nanalysis for this method based on the properties of the CG iteration and the\nquality of the Hessian approximation, and compare it with a method that employs\na stochastic gradient iteration instead of the CG method. We report preliminary\nnumerical results that illustrate the performance of inexact subsampled Newton\nmethods on machine learning applications based on logistic regression.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 15:43:31 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Bollapragada", "Raghu", ""], ["Byrd", "Richard", ""], ["Nocedal", "Jorge", ""]]}, {"id": "1609.08677", "submitter": "Zhao Kang", "authors": "Chong Peng, Zhao Kang, Qiang Chen", "title": "A Fast Factorization-based Approach to Robust PCA", "comments": "ICDM 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust principal component analysis (RPCA) has been widely used for\nrecovering low-rank matrices in many data mining and machine learning problems.\nIt separates a data matrix into a low-rank part and a sparse part. The convex\napproach has been well studied in the literature. However, state-of-the-art\nalgorithms for the convex approach usually have relatively high complexity due\nto the need of solving (partial) singular value decompositions of large\nmatrices. A non-convex approach, AltProj, has also been proposed with lighter\ncomplexity and better scalability. Given the true rank $r$ of the underlying\nlow rank matrix, AltProj has a complexity of $O(r^2dn)$, where $d\\times n$ is\nthe size of data matrix. In this paper, we propose a novel factorization-based\nmodel of RPCA, which has a complexity of $O(kdn)$, where $k$ is an upper bound\nof the true rank. Our method does not need the precise value of the true rank.\nFrom extensive experiments, we observe that AltProj can work only when $r$ is\nprecisely known in advance; however, when the needed rank parameter $r$ is\nspecified to a value different from the true rank, AltProj cannot fully\nseparate the two parts while our method succeeds. Even when both work, our\nmethod is about 4 times faster than AltProj. Our method can be used as a\nlight-weight, scalable tool for RPCA in the absence of the precise value of the\ntrue rank.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 21:32:16 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Peng", "Chong", ""], ["Kang", "Zhao", ""], ["Chen", "Qiang", ""]]}, {"id": "1609.08703", "submitter": "Franck Dernoncourt", "authors": "Franck Dernoncourt, Ji Young Lee", "title": "Optimizing Neural Network Hyperparameters with Gaussian Processes for\n  Dialog Act Classification", "comments": "Accepted as a conference paper at IEEE SLT 2016. The two authors\n  contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systems based on artificial neural networks (ANNs) have achieved\nstate-of-the-art results in many natural language processing tasks. Although\nANNs do not require manually engineered features, ANNs have many\nhyperparameters to be optimized. The choice of hyperparameters significantly\nimpacts models' performances. However, the ANN hyperparameters are typically\nchosen by manual, grid, or random search, which either requires expert\nexperiences or is computationally expensive. Recent approaches based on\nBayesian optimization using Gaussian processes (GPs) is a more systematic way\nto automatically pinpoint optimal or near-optimal machine learning\nhyperparameters. Using a previously published ANN model yielding\nstate-of-the-art results for dialog act classification, we demonstrate that\noptimizing hyperparameters using GP further improves the results, and reduces\nthe computational time by a factor of 4 compared to a random search. Therefore\nit is a useful technique for tuning ANN models to yield the best performances\nfor natural language processing tasks.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 23:10:42 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Dernoncourt", "Franck", ""], ["Lee", "Ji Young", ""]]}, {"id": "1609.08752", "submitter": "Shivapratap Gopakumar", "authors": "Shivapratap Gopakumar, Truyen Tran, Dinh Phung, Svetha Venkatesh", "title": "Stabilizing Linear Prediction Models using Autoencoder", "comments": "accepted in ADMA 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To date, the instability of prognostic predictors in a sparse high\ndimensional model, which hinders their clinical adoption, has received little\nattention. Stable prediction is often overlooked in favour of performance. Yet,\nstability prevails as key when adopting models in critical areas as healthcare.\nOur study proposes a stabilization scheme by detecting higher order feature\ncorrelations. Using a linear model as basis for prediction, we achieve feature\nstability by regularising latent correlation in features. Latent higher order\ncorrelation among features is modelled using an autoencoder network. Stability\nis enhanced by combining a recent technique that uses a feature graph, and\naugmenting external unlabelled data for training the autoencoder network. Our\nexperiments are conducted on a heart failure cohort from an Australian\nhospital. Stability was measured using Consistency index for feature subsets\nand signal-to-noise ratio for model parameters. Our methods demonstrated\nsignificant improvement in feature stability and model estimation stability\nwhen compared to baselines.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 03:25:24 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Gopakumar", "Shivapratap", ""], ["Tran", "Truyen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1609.08870", "submitter": "Vianney Perchet", "authors": "J\\'anos Flesch, Rida Laraki (LAMSADE, CNRS), Vianney Perchet", "title": "Approachability of convex sets in generalized quitting games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Blackwell approachability, a very powerful and geometric tool in\ngame theory, used for example to design strategies of the uninformed player in\nrepeated games with incomplete information. We extend this theory to\n\"generalized quitting games\" , a class of repeated stochastic games in which\neach player may have quitting actions, such as the Big-Match. We provide three\nsimple geometric and strongly related conditions for the weak approachability\nof a convex target set. The first is sufficient: it guarantees that, for any\nfixed horizon, a player has a strategy ensuring that the expected time-average\npayoff vector converges to the target set as horizon goes to infinity. The\nthird is necessary: if it is not satisfied, the opponent can weakly exclude the\ntarget set. In the special case where only the approaching player can quit the\ngame (Big-Match of type I), the three conditions are equivalent and coincide\nwith Blackwell's condition. Consequently, we obtain a full characterization and\nprove that the game is weakly determined-every convex set is either weakly\napproachable or weakly excludable. In games where only the opponent can quit\n(Big-Match of type II), none of our conditions is both sufficient and necessary\nfor weak approachability. We provide a continuous time sufficient condition\nusing techniques coming from differential games, and show its usefulness in\npractice, in the spirit of Vieille's seminal work for weak\napproachability.Finally, we study uniform approachability where the strategy\nshould not depend on the horizon and demonstrate that, in contrast with\nclassical Blackwell approacha-bility for convex sets, weak approachability does\nnot imply uniform approachability.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 11:57:47 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Flesch", "J\u00e1nos", "", "LAMSADE, CNRS"], ["Laraki", "Rida", "", "LAMSADE, CNRS"], ["Perchet", "Vianney", ""]]}, {"id": "1609.08886", "submitter": "Shuichi Kawano", "authors": "Shuichi Kawano, Hironori Fujisawa, Toyoyuki Takada, Toshihiko\n  Shiroishi", "title": "Sparse principal component regression for generalized linear models", "comments": "29 pages", "journal-ref": "Computational Statistics & Data Analysis 124 (2018) 180-196", "doi": "10.1016/j.csda.2018.03.008", "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component regression (PCR) is a widely used two-stage procedure:\nprincipal component analysis (PCA), followed by regression in which the\nselected principal components are regarded as new explanatory variables in the\nmodel. Note that PCA is based only on the explanatory variables, so the\nprincipal components are not selected using the information on the response\nvariable. In this paper, we propose a one-stage procedure for PCR in the\nframework of generalized linear models. The basic loss function is based on a\ncombination of the regression loss and PCA loss. An estimate of the regression\nparameter is obtained as the minimizer of the basic loss function with a sparse\npenalty. We call the proposed method sparse principal component regression for\ngeneralized linear models (SPCR-glm). Taking the two loss function into\nconsideration simultaneously, SPCR-glm enables us to obtain sparse principal\ncomponent loadings that are related to a response variable. However, a\ncombination of loss functions may cause a parameter identification problem, but\nthis potential problem is avoided by virtue of the sparse penalty. Thus, the\nsparse penalty plays two roles in this method. The parameter estimation\nprocedure is proposed using various update algorithms with the coordinate\ndescent algorithm. We apply SPCR-glm to two real datasets, doctor visits data\nand mouse consomic strain data. SPCR-glm provides more easily interpretable\nprincipal component (PC) scores and clearer classification on PC plots than the\nusual PCA.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 12:33:42 GMT"}, {"version": "v2", "created": "Wed, 12 Oct 2016 05:59:25 GMT"}, {"version": "v3", "created": "Thu, 13 Oct 2016 03:51:02 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Kawano", "Shuichi", ""], ["Fujisawa", "Hironori", ""], ["Takada", "Toyoyuki", ""], ["Shiroishi", "Toshihiko", ""]]}, {"id": "1609.08905", "submitter": "Giorgio Corani", "authors": "Giorgio Corani and Alessio Benavoli and Janez Dem\\v{s}ar and Francesca\n  Mangili and Marco Zaffalon", "title": "Statistical comparison of classifiers through Bayesian hierarchical\n  modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Usually one compares the accuracy of two competing classifiers via null\nhypothesis significance tests (nhst). Yet the nhst tests suffer from important\nshortcomings, which can be overcome by switching to Bayesian hypothesis\ntesting. We propose a Bayesian hierarchical model which jointly analyzes the\ncross-validation results obtained by two classifiers on multiple data sets. It\nreturns the posterior probability of the accuracies of the two classifiers\nbeing practically equivalent or significantly different. A further strength of\nthe hierarchical model is that, by jointly analyzing the results obtained on\nall data sets, it reduces the estimation error compared to the usual approach\nof averaging the cross-validation results obtained on a given data set.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 13:30:31 GMT"}, {"version": "v2", "created": "Mon, 3 Oct 2016 08:23:38 GMT"}, {"version": "v3", "created": "Tue, 22 Nov 2016 15:16:45 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Corani", "Giorgio", ""], ["Benavoli", "Alessio", ""], ["Dem\u0161ar", "Janez", ""], ["Mangili", "Francesca", ""], ["Zaffalon", "Marco", ""]]}, {"id": "1609.08913", "submitter": "George Monta\\~nez", "authors": "George D. Montanez", "title": "The Famine of Forte: Few Search Problems Greatly Favor Your Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Casting machine learning as a type of search, we demonstrate that the\nproportion of problems that are favorable for a fixed algorithm is strictly\nbounded, such that no single algorithm can perform well over a large fraction\nof them. Our results explain why we must either continue to develop new\nlearning methods year after year or move towards highly parameterized models\nthat are both flexible and sensitive to their hyperparameters. We further give\nan upper bound on the expected performance for a search algorithm as a function\nof the mutual information between the target and the information resource\n(e.g., training dataset), proving the importance of certain types of dependence\nfor machine learning. Lastly, we show that the expected per-query probability\nof success for an algorithm is mathematically equivalent to a single-query\nprobability of success under a distribution (called a search strategy), and\nprove that the proportion of favorable strategies is also strictly bounded.\nThus, whether one holds fixed the search algorithm and considers all possible\nproblems or one fixes the search problem and looks at all possible search\nstrategies, favorable matches are exceedingly rare. The forte (strength) of any\nalgorithm is quantifiably restricted.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 13:52:17 GMT"}, {"version": "v2", "created": "Wed, 19 Apr 2017 14:21:53 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Montanez", "George D.", ""]]}, {"id": "1609.08938", "submitter": "Allison Del Giorno", "authors": "Allison Del Giorno, J. Andrew Bagnell, Martial Hebert", "title": "A Discriminative Framework for Anomaly Detection in Large Videos", "comments": "14 pages without references, 16 pages with. 7 figures. Accepted to\n  ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address an anomaly detection setting in which training sequences are\nunavailable and anomalies are scored independently of temporal ordering.\nCurrent algorithms in anomaly detection are based on the classical density\nestimation approach of learning high-dimensional models and finding\nlow-probability events. These algorithms are sensitive to the order in which\nanomalies appear and require either training data or early context assumptions\nthat do not hold for longer, more complex videos. By defining anomalies as\nexamples that can be distinguished from other examples in the same video, our\ndefinition inspires a shift in approaches from classical density estimation to\nsimple discriminative learning. Our contributions include a novel framework for\nanomaly detection that is (1) independent of temporal ordering of anomalies,\nand (2) unsupervised, requiring no separate training sequences. We show that\nour algorithm can achieve state-of-the-art results even when we adjust the\nsetting by removing training sequences from standard datasets.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 14:48:32 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Del Giorno", "Allison", ""], ["Bagnell", "J. Andrew", ""], ["Hebert", "Martial", ""]]}, {"id": "1609.08976", "submitter": "Chunyuan Li", "authors": "Yunchen Pu, Zhe Gan, Ricardo Henao, Xin Yuan, Chunyuan Li, Andrew\n  Stevens, Lawrence Carin", "title": "Variational Autoencoder for Deep Learning of Images, Labels and Captions", "comments": "NIPS 2016 (To appear)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel variational autoencoder is developed to model images, as well as\nassociated labels or captions. The Deep Generative Deconvolutional Network\n(DGDN) is used as a decoder of the latent image features, and a deep\nConvolutional Neural Network (CNN) is used as an image encoder; the CNN is used\nto approximate a distribution for the latent DGDN features/code. The latent\ncode is also linked to generative models for labels (Bayesian support vector\nmachine) or captions (recurrent neural network). When predicting a\nlabel/caption for a new image at test, averaging is performed across the\ndistribution of latent codes; this is computationally efficient as a\nconsequence of the learned CNN-based encoder. Since the framework is capable of\nmodeling the image in the presence/absence of associated labels/captions, a new\nsemi-supervised setting is manifested for CNN learning with images; the\nframework even allows unsupervised CNN learning, based on images alone.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 15:56:15 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Pu", "Yunchen", ""], ["Gan", "Zhe", ""], ["Henao", "Ricardo", ""], ["Yuan", "Xin", ""], ["Li", "Chunyuan", ""], ["Stevens", "Andrew", ""], ["Carin", "Lawrence", ""]]}, {"id": "1609.09000", "submitter": "Till Sch\\\"afer", "authors": "Till Sch\\\"afer and Petra Mutzel", "title": "StruClus: Structural Clustering of Large-Scale Graph Databases", "comments": "10 pages, experimental evaluation, big data, subgraph mining,\n  clustering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a structural clustering algorithm for large-scale datasets of\nsmall labeled graphs, utilizing a frequent subgraph sampling strategy. A set of\nrepresentatives provides an intuitive description of each cluster, supports the\nclustering process, and helps to interpret the clustering results. The\nprojection-based nature of the clustering approach allows us to bypass\ndimensionality and feature extraction problems that arise in the context of\ngraph datasets reduced to pairwise distances or feature vectors. While\nachieving high quality and (human) interpretable clusterings, the runtime of\nthe algorithm only grows linearly with the number of graphs. Furthermore, the\napproach is easy to parallelize and therefore suitable for very large datasets.\nOur extensive experimental evaluation on synthetic and real world datasets\ndemonstrates the superiority of our approach over existing structural and\nsubspace clustering algorithms, both, from a runtime and quality point of view.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 16:43:12 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Sch\u00e4fer", "Till", ""], ["Mutzel", "Petra", ""]]}, {"id": "1609.09143", "submitter": "Giovanni Montana", "authors": "Petros-Pavlos Ypsilantis, Giovanni Montana", "title": "Recurrent Convolutional Networks for Pulmonary Nodule Detection in CT\n  Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computed tomography (CT) generates a stack of cross-sectional images covering\na region of the body. The visual assessment of these images for the\nidentification of potential abnormalities is a challenging and time consuming\ntask due to the large amount of information that needs to be processed. In this\narticle we propose a deep artificial neural network architecture, ReCTnet, for\nthe fully-automated detection of pulmonary nodules in CT scans. The\narchitecture learns to distinguish nodules and normal structures at the pixel\nlevel and generates three-dimensional probability maps highlighting areas that\nare likely to harbour the objects of interest. Convolutional and recurrent\nlayers are combined to learn expressive image representations exploiting the\nspatial dependencies across axial slices. We demonstrate that leveraging\nintra-slice dependencies substantially increases the sensitivity to detect\npulmonary nodules without inflating the false positive rate. On the publicly\navailable LIDC/IDRI dataset consisting of 1,018 annotated CT scans, ReCTnet\nreaches a detection sensitivity of 90.5% with an average of 4.5 false positives\nper scan. Comparisons with a competing multi-channel convolutional neural\nnetwork for multi-slice segmentation and other published methodologies using\nthe same dataset provide evidence that ReCTnet offers significant performance\ngains.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 22:32:24 GMT"}, {"version": "v2", "created": "Fri, 30 Sep 2016 12:30:14 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Ypsilantis", "Petros-Pavlos", ""], ["Montana", "Giovanni", ""]]}, {"id": "1609.09154", "submitter": "Grey Ballard", "authors": "Ramakrishnan Kannan, Grey Ballard, Haesun Park", "title": "MPI-FAUN: An MPI-Based Framework for Alternating-Updating Nonnegative\n  Matrix Factorization", "comments": "arXiv admin note: text overlap with arXiv:1509.09313", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-negative matrix factorization (NMF) is the problem of determining two\nnon-negative low rank factors $W$ and $H$, for the given input matrix $A$, such\nthat $A \\approx W H$. NMF is a useful tool for many applications in different\ndomains such as topic modeling in text mining, background separation in video\nanalysis, and community detection in social networks. Despite its popularity in\nthe data mining community, there is a lack of efficient parallel algorithms to\nsolve the problem for big data sets.\n  The main contribution of this work is a new, high-performance parallel\ncomputational framework for a broad class of NMF algorithms that iteratively\nsolves alternating non-negative least squares (NLS) subproblems for $W$ and\n$H$. It maintains the data and factor matrices in memory (distributed across\nprocessors), uses MPI for interprocessor communication, and, in the dense case,\nprovably minimizes communication costs (under mild assumptions). The framework\nis flexible and able to leverage a variety of NMF and NLS algorithms, including\nMultiplicative Update, Hierarchical Alternating Least Squares, and Block\nPrincipal Pivoting. Our implementation allows us to benchmark and compare\ndifferent algorithms on massive dense and sparse data matrices of size that\nspans for few hundreds of millions to billions. We demonstrate the scalability\nof our algorithm and compare it with baseline implementations, showing\nsignificant performance improvements. The code and the datasets used for\nconducting the experiments are available online.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 23:31:45 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Kannan", "Ramakrishnan", ""], ["Ballard", "Grey", ""], ["Park", "Haesun", ""]]}, {"id": "1609.09196", "submitter": "Davis Blalock", "authors": "Davis W. Blalock, John V. Guttag", "title": "EXTRACT: Strong Examples from Weakly-Labeled Sensor Data", "comments": "To appear in IEEE International Conference on Data Mining 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to the rise of wearable and connected devices, sensor-generated time\nseries comprise a large and growing fraction of the world's data.\nUnfortunately, extracting value from this data can be challenging, since\nsensors report low-level signals (e.g., acceleration), not the high-level\nevents that are typically of interest (e.g., gestures). We introduce a\ntechnique to bridge this gap by automatically extracting examples of real-world\nevents in low-level data, given only a rough estimate of when these events have\ntaken place.\n  By identifying sets of features that repeat in the same temporal arrangement,\nwe isolate examples of such diverse events as human actions, power consumption\npatterns, and spoken words with up to 96% precision and recall. Our method is\nfast enough to run in real time and assumes only minimal knowledge of which\nvariables are relevant or the lengths of events. Our evaluation uses numerous\npublicly available datasets and over 1 million samples of manually labeled\nsensor data.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 04:02:31 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Blalock", "Davis W.", ""], ["Guttag", "John V.", ""]]}, {"id": "1609.09353", "submitter": "Di Chen", "authors": "Di Chen, Yexiang Xue, Shuo Chen, Daniel Fink, Carla Gomes", "title": "Deep Multi-Species Embedding", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.PE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding how species are distributed across landscapes over time is a\nfundamental question in biodiversity research. Unfortunately, most species\ndistribution models only target a single species at a time, despite strong\necological evidence that species are not independently distributed. We propose\nDeep Multi-Species Embedding (DMSE), which jointly embeds vectors corresponding\nto multiple species as well as vectors representing environmental covariates\ninto a common high-dimensional feature space via a deep neural network. Applied\nto bird observational data from the citizen science project \\textit{eBird}, we\ndemonstrate how the DMSE model discovers inter-species relationships to\noutperform single-species distribution models (random forests and SVMs) as well\nas competing multi-label models. Additionally, we demonstrate the benefit of\nusing a deep neural network to extract features within the embedding and show\nhow they improve the predictive performance of species distribution modelling.\nAn important domain contribution of the DMSE model is the ability to discover\nand describe species interactions while simultaneously learning the shared\nhabitat preferences among species. As an additional contribution, we provide a\ngraphical embedding of hundreds of bird species in the Northeast US.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 00:39:47 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 22:20:18 GMT"}, {"version": "v3", "created": "Mon, 20 Feb 2017 11:12:38 GMT"}, {"version": "v4", "created": "Tue, 21 Feb 2017 15:35:11 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Chen", "Di", ""], ["Xue", "Yexiang", ""], ["Chen", "Shuo", ""], ["Fink", "Daniel", ""], ["Gomes", "Carla", ""]]}, {"id": "1609.09408", "submitter": "Yang Lu", "authors": "Jianwen Xie, Yang Lu, Ruiqi Gao, Song-Chun Zhu and Ying Nian Wu", "title": "Cooperative Training of Descriptor and Generator Networks", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the cooperative training of two generative models for\nimage modeling and synthesis. Both models are parametrized by convolutional\nneural networks (ConvNets). The first model is a deep energy-based model, whose\nenergy function is defined by a bottom-up ConvNet, which maps the observed\nimage to the energy. We call it the descriptor network. The second model is a\ngenerator network, which is a non-linear version of factor analysis. It is\ndefined by a top-down ConvNet, which maps the latent factors to the observed\nimage. The maximum likelihood learning algorithms of both models involve MCMC\nsampling such as Langevin dynamics. We observe that the two learning algorithms\ncan be seamlessly interwoven into a cooperative learning algorithm that can\ntrain both models simultaneously. Specifically, within each iteration of the\ncooperative learning algorithm, the generator model generates initial\nsynthesized examples to initialize a finite-step MCMC that samples and trains\nthe energy-based descriptor model. After that, the generator model learns from\nhow the MCMC changes its synthesized examples. That is, the descriptor model\nteaches the generator model by MCMC, so that the generator model accumulates\nthe MCMC transitions and reproduces them by direct ancestral sampling. We call\nthis scheme MCMC teaching. We show that the cooperative algorithm can learn\nhighly realistic generative models.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 16:14:45 GMT"}, {"version": "v2", "created": "Wed, 8 Feb 2017 17:32:46 GMT"}, {"version": "v3", "created": "Mon, 29 Oct 2018 16:42:24 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Xie", "Jianwen", ""], ["Lu", "Yang", ""], ["Gao", "Ruiqi", ""], ["Zhu", "Song-Chun", ""], ["Wu", "Ying Nian", ""]]}, {"id": "1609.09430", "submitter": "Shawn Hershey", "authors": "Shawn Hershey, Sourish Chaudhuri, Daniel P. W. Ellis, Jort F. Gemmeke,\n  Aren Jansen, R. Channing Moore, Manoj Plakal, Devin Platt, Rif A. Saurous,\n  Bryan Seybold, Malcolm Slaney, Ron J. Weiss, Kevin Wilson", "title": "CNN Architectures for Large-Scale Audio Classification", "comments": "Accepted for publication at ICASSP 2017 Changes: Added definitions of\n  mAP, AUC, and d-prime. Updated mAP/AUC/d-prime numbers for Audio Set based on\n  changes of latest Audio Set revision. Changed wording to fit 4 page limit\n  with new additions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have proven very effective in image\nclassification and show promise for audio. We use various CNN architectures to\nclassify the soundtracks of a dataset of 70M training videos (5.24 million\nhours) with 30,871 video-level labels. We examine fully connected Deep Neural\nNetworks (DNNs), AlexNet [1], VGG [2], Inception [3], and ResNet [4]. We\ninvestigate varying the size of both training set and label vocabulary, finding\nthat analogs of the CNNs used in image classification do well on our audio\nclassification task, and larger training and label sets help up to a point. A\nmodel using embeddings from these classifiers does much better than raw\nfeatures on the Audio Set [5] Acoustic Event Detection (AED) classification\ntask.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 17:04:50 GMT"}, {"version": "v2", "created": "Tue, 10 Jan 2017 18:06:51 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Hershey", "Shawn", ""], ["Chaudhuri", "Sourish", ""], ["Ellis", "Daniel P. W.", ""], ["Gemmeke", "Jort F.", ""], ["Jansen", "Aren", ""], ["Moore", "R. Channing", ""], ["Plakal", "Manoj", ""], ["Platt", "Devin", ""], ["Saurous", "Rif A.", ""], ["Seybold", "Bryan", ""], ["Slaney", "Malcolm", ""], ["Weiss", "Ron J.", ""], ["Wilson", "Kevin", ""]]}, {"id": "1609.09432", "submitter": "Hejia Zhang", "authors": "Hejia Zhang, Po-Hsuan Chen, Janice Chen, Xia Zhu, Javier S. Turek,\n  Theodore L. Willke, Uri Hasson, Peter J. Ramadge", "title": "A Searchlight Factor Model Approach for Locating Shared Information in\n  Multi-Subject fMRI Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing interest in joint multi-subject fMRI analysis. The\nchallenge of such analysis comes from inherent anatomical and functional\nvariability across subjects. One approach to resolving this is a shared\nresponse factor model. This assumes a shared and time synchronized stimulus\nacross subjects. Such a model can often identify shared information, but it may\nnot be able to pinpoint with high resolution the spatial location of this\ninformation. In this work, we examine a searchlight based shared response model\nto identify shared information in small contiguous regions (searchlights)\nacross the whole brain. Validation using classification tasks demonstrates that\nwe can pinpoint informative local regions.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 17:20:23 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Zhang", "Hejia", ""], ["Chen", "Po-Hsuan", ""], ["Chen", "Janice", ""], ["Zhu", "Xia", ""], ["Turek", "Javier S.", ""], ["Willke", "Theodore L.", ""], ["Hasson", "Uri", ""], ["Ramadge", "Peter J.", ""]]}, {"id": "1609.09471", "submitter": "Lovedeep Gondara", "authors": "Lovedeep Gondara", "title": "Classifier comparison using precision", "comments": "Extended version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New proposed models are often compared to state-of-the-art using statistical\nsignificance testing. Literature is scarce for classifier comparison using\nmetrics other than accuracy. We present a survey of statistical methods that\ncan be used for classifier comparison using precision, accounting for\ninter-precision correlation arising from use of same dataset. Comparisons are\nmade using per-class precision and methods presented to test global null\nhypothesis of an overall model comparison. Comparisons are extended to multiple\nmulti-class classifiers and to models using cross validation or its variants.\nPartial Bayesian update to precision is introduced when population prevalence\nof a class is known. Applications to compare deep architectures are studied.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 19:19:29 GMT"}, {"version": "v2", "created": "Wed, 16 Nov 2016 01:43:21 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Gondara", "Lovedeep", ""]]}, {"id": "1609.09481", "submitter": "Vu Dinh", "authors": "Vu Dinh, Lam Si Tung Ho, Duy Nguyen, Binh T. Nguyen", "title": "Fast learning rates with heavy-tailed losses", "comments": "Advances in Neural Information Processing Systems (NIPS 2016): 11\n  pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study fast learning rates when the losses are not necessarily bounded and\nmay have a distribution with heavy tails. To enable such analyses, we introduce\ntwo new conditions: (i) the envelope function $\\sup_{f \\in \\mathcal{F}}|\\ell\n\\circ f|$, where $\\ell$ is the loss function and $\\mathcal{F}$ is the\nhypothesis class, exists and is $L^r$-integrable, and (ii) $\\ell$ satisfies the\nmulti-scale Bernstein's condition on $\\mathcal{F}$. Under these assumptions, we\nprove that learning rate faster than $O(n^{-1/2})$ can be obtained and,\ndepending on $r$ and the multi-scale Bernstein's powers, can be arbitrarily\nclose to $O(n^{-1})$. We then verify these assumptions and derive fast learning\nrates for the problem of vector quantization by $k$-means clustering with\nheavy-tailed distributions. The analyses enable us to obtain novel learning\nrates that extend and complement existing results in the literature from both\ntheoretical and practical viewpoints.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 19:46:13 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Dinh", "Vu", ""], ["Ho", "Lam Si Tung", ""], ["Nguyen", "Duy", ""], ["Nguyen", "Binh T.", ""]]}, {"id": "1609.09519", "submitter": "James Hook", "authors": "James Hook", "title": "Max-plus statistical leverage scores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical leverage scores of a complex matrix $A\\in\\mathbb{C}^{n\\times\nd}$ record the degree of alignment between col$(A)$ and the coordinate axes in\n$\\mathbb{C}^n$. These score are used in random sampling algorithms for solving\ncertain numerical linear algebra problems. In this paper we present a max-plus\nalgebraic analogue for statistical leverage scores. We show that max-plus\nstatistical leverage scores can be used to calculate the exact asymptotic\nbehavior of the conventional statistical leverage scores of a generic matrices\nof Puiseux series and also provide a novel way to approximate the conventional\nstatistical leverage scores of a fixed or complex matrix. The advantage of\napproximating a complex matrices scores with max-plus scores is that the\nmax-plus scores can be computed very quickly. This approximation is typically\naccurate to within an order or magnitude and should be useful in practical\nproblems where the true scores are known to vary widely.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 20:31:10 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Hook", "James", ""]]}, {"id": "1609.09597", "submitter": "Xing Zhang", "authors": "Xing Zhang, Zhenglei Yi, Zhi Yan, Geyong Min, Wenbo Wang, Sabita\n  Maharjan, Yan Zhang", "title": "Social Computing for Mobile Big Data in Wireless Networks", "comments": "8 papges, 3 figures, 1 tables", "journal-ref": "Computer, vol.49, no. 9, pp. 86-90, Sept. 2016", "doi": "10.1109/MC.2016.267", "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Mobile big data contains vast statistical features in various dimensions,\nincluding spatial, temporal, and the underlying social domain. Understanding\nand exploiting the features of mobile data from a social network perspective\nwill be extremely beneficial to wireless networks, from planning, operation,\nand maintenance to optimization and marketing. In this paper, we categorize and\nanalyze the big data collected from real wireless cellular networks. Then, we\nstudy the social characteristics of mobile big data and highlight several\nresearch directions for mobile big data in the social computing areas.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2016 05:20:24 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Zhang", "Xing", ""], ["Yi", "Zhenglei", ""], ["Yan", "Zhi", ""], ["Min", "Geyong", ""], ["Wang", "Wenbo", ""], ["Maharjan", "Sabita", ""], ["Zhang", "Yan", ""]]}, {"id": "1609.09660", "submitter": "Junyang Jin", "authors": "J. Jin, Y. Yuan, W. Pan, D. L.T. Pham, C. J. Tomlin, A.Webb, J.\n  Goncalves", "title": "On Identification of Sparse Multivariable ARX Model: A Sparse Bayesian\n  Learning Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper begins with considering the identification of sparse linear\ntime-invariant networks described by multivariable ARX models. Such models\npossess relatively simple structure thus used as a benchmark to promote further\nresearch. With identifiability of the network guaranteed, this paper presents\nan identification method that infers both the Boolean structure of the network\nand the internal dynamics between nodes. Identification is performed directly\nfrom data without any prior knowledge of the system, including its order. The\nproposed method solves the identification problem using Maximum a posteriori\nestimation (MAP) but with inseparable penalties for complexity, both in terms\nof element (order of nonzero connections) and group sparsity (network\ntopology). Such an approach is widely applied in Compressive Sensing (CS) and\nknown as Sparse Bayesian Learning (SBL). We then propose a novel scheme that\ncombines sparse Bayesian and group sparse Bayesian to efficiently solve the\nproblem. The resulted algorithm has a similar form of the standard Sparse Group\nLasso (SGL) while with known noise variance, it simplifies to exact re-weighted\nSGL. The method and the developed toolbox can be applied to infer networks from\na wide range of fields, including systems biology applications such as\nsignaling and genetic regulatory networks.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2016 10:17:47 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Jin", "J.", ""], ["Yuan", "Y.", ""], ["Pan", "W.", ""], ["Pham", "D. L. T.", ""], ["Tomlin", "C. J.", ""], ["Webb", "A.", ""], ["Goncalves", "J.", ""]]}, {"id": "1609.09744", "submitter": "Antoine Deleforge", "authors": "Antoine Deleforge (PANAMA), Yann Traonmilin (PANAMA)", "title": "Phase Unmixing : Multichannel Source Separation with Magnitude\n  Constraints", "comments": "2017 IEEE International Conference on Acoustics, Speech and Signal\n  Processing (ICASSP), Mar 2017, New Orleans, United States", "journal-ref": null, "doi": null, "report-no": "hal-01372418", "categories": "cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the phases of K mixed complex signals\nfrom a multichannel observation, when the mixing matrix and signal magnitudes\nare known. This problem can be cast as a non-convex quadratically constrained\nquadratic program which is known to be NP-hard in general. We propose three\napproaches to tackle it: a heuristic method, an alternate minimization method,\nand a convex relaxation into a semi-definite program. The last two approaches\nare showed to outperform the oracle multichannel Wiener filter in\nunder-determined informed source separation tasks, using simulated and speech\nsignals. The convex relaxation approach yields best results, including the\npotential for exact source separation in under-determined settings.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2016 14:17:32 GMT"}, {"version": "v2", "created": "Mon, 20 Mar 2017 13:36:08 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Deleforge", "Antoine", "", "PANAMA"], ["Traonmilin", "Yann", "", "PANAMA"]]}, {"id": "1609.09799", "submitter": "Remi Flamary", "authors": "R\\'emi Flamary, C\\'edric F\\'evotte, Nicolas Courty, Valentin Emiya", "title": "Optimal spectral transportation with application to music transcription", "comments": "NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many spectral unmixing methods rely on the non-negative decomposition of\nspectral data onto a dictionary of spectral templates. In particular,\nstate-of-the-art music transcription systems decompose the spectrogram of the\ninput signal onto a dictionary of representative note spectra. The typical\nmeasures of fit used to quantify the adequacy of the decomposition compare the\ndata and template entries frequency-wise. As such, small displacements of\nenergy from a frequency bin to another as well as variations of timber can\ndisproportionally harm the fit. We address these issues by means of optimal\ntransportation and propose a new measure of fit that treats the frequency\ndistributions of energy holistically as opposed to frequency-wise. Building on\nthe harmonic nature of sound, the new measure is invariant to shifts of energy\nto harmonically-related frequencies, as well as to small and local\ndisplacements of energy. Equipped with this new measure of fit, the dictionary\nof note templates can be considerably simplified to a set of Dirac vectors\nlocated at the target fundamental frequencies (musical pitch values). This in\nturns gives ground to a very fast and simple decomposition algorithm that\nachieves state-of-the-art performance on real musical data.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2016 16:28:12 GMT"}, {"version": "v2", "created": "Mon, 10 Oct 2016 06:42:07 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Flamary", "R\u00e9mi", ""], ["F\u00e9votte", "C\u00e9dric", ""], ["Courty", "Nicolas", ""], ["Emiya", "Valentin", ""]]}, {"id": "1609.09869", "submitter": "Rahul Gopal Krishnan", "authors": "Rahul G. Krishnan, Uri Shalit, David Sontag", "title": "Structured Inference Networks for Nonlinear State Space Models", "comments": "To appear in the Thirty-First AAAI Conference on Artificial\n  Intelligence, February 2017, 13 pages, 11 figures with supplement, changed to\n  AAAI formatting style, added references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian state space models have been used for decades as generative models\nof sequential data. They admit an intuitive probabilistic interpretation, have\na simple functional form, and enjoy widespread adoption. We introduce a unified\nalgorithm to efficiently learn a broad class of linear and non-linear state\nspace models, including variants where the emission and transition\ndistributions are modeled by deep neural networks. Our learning algorithm\nsimultaneously learns a compiled inference network and the generative model,\nleveraging a structured variational approximation parameterized by recurrent\nneural networks to mimic the posterior distribution. We apply the learning\nalgorithm to both synthetic and real-world datasets, demonstrating its\nscalability and versatility. We find that using the structured approximation to\nthe posterior results in models with significantly higher held-out likelihood.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2016 19:53:11 GMT"}, {"version": "v2", "created": "Mon, 5 Dec 2016 19:10:10 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Krishnan", "Rahul G.", ""], ["Shalit", "Uri", ""], ["Sontag", "David", ""]]}]