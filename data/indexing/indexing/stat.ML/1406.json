[{"id": "1406.0013", "submitter": "Marina Meila", "authors": "Dominique Perrault-Joncas and Marina Meila", "title": "Estimating Vector Fields on Manifolds and the Embedding of Directed\n  Graphs", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of embedding directed graphs in Euclidean\nspace while retaining directional information. We model a directed graph as a\nfinite set of observations from a diffusion on a manifold endowed with a vector\nfield. This is the first generative model of its kind for directed graphs. We\nintroduce a graph embedding algorithm that estimates all three features of this\nmodel: the low-dimensional embedding of the manifold, the data density and the\nvector field. In the process, we also obtain new theoretical results on the\nlimits of \"Laplacian type\" matrices derived from directed graphs. The\napplication of our method to both artificially constructed and real data\nhighlights its strengths.\n", "versions": [{"version": "v1", "created": "Fri, 30 May 2014 20:45:50 GMT"}], "update_date": "2014-06-03", "authors_parsed": [["Perrault-Joncas", "Dominique", ""], ["Meila", "Marina", ""]]}, {"id": "1406.0067", "submitter": "Can Le", "authors": "Can M. Le, Elizaveta Levina, Roman Vershynin", "title": "Optimization via Low-rank Approximation for Community Detection in\n  Networks", "comments": "45 pages, 7 figures; added discussions about computational complexity\n  and extension to more than two communities", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SI math.ST physics.soc-ph stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection is one of the fundamental problems of network analysis,\nfor which a number of methods have been proposed. Most model-based or\ncriteria-based methods have to solve an optimization problem over a discrete\nset of labels to find communities, which is computationally infeasible. Some\nfast spectral algorithms have been proposed for specific methods or models, but\nonly on a case-by-case basis. Here we propose a general approach for maximizing\na function of a network adjacency matrix over discrete labels by projecting the\nset of labels onto a subspace approximating the leading eigenvectors of the\nexpected adjacency matrix. This projection onto a low-dimensional space makes\nthe feasible set of labels much smaller and the optimization problem much\neasier. We prove a general result about this method and show how to apply it to\nseveral previously proposed community detection criteria, establishing its\nconsistency for label estimation in each case and demonstrating the fundamental\nconnection between spectral properties of the network and various model-based\napproaches to community detection. Simulations and applications to real-world\ndata are included to demonstrate our method performs well for multiple problems\nover a wide range of parameters.\n", "versions": [{"version": "v1", "created": "Sat, 31 May 2014 11:02:37 GMT"}, {"version": "v2", "created": "Sun, 10 May 2015 08:31:14 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Le", "Can M.", ""], ["Levina", "Elizaveta", ""], ["Vershynin", "Roman", ""]]}, {"id": "1406.0071", "submitter": "Tue Herlau", "authors": "Tue Herlau, Morten M{\\o}rup, Yee Whye Teh, Mikkel N. Schmidt", "title": "Adaptive Reconfiguration Moves for Dirichlet Mixtures", "comments": "Manuscript is in preparation, 26 pages including figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian mixture models are widely applied for unsupervised learning and\nexploratory data analysis. Markov chain Monte Carlo based on Gibbs sampling and\nsplit-merge moves are widely used for inference in these models. However, both\nmethods are restricted to limited types of transitions and suffer from torpid\nmixing and low accept rates even for problems of modest size. We propose a\nmethod that considers a broader range of transitions that are close to\nequilibrium by exploiting multiple chains in parallel and using the past states\nadaptively to inform the proposal distribution. The method significantly\nimproves on Gibbs and split-merge sampling as quantified using convergence\ndiagnostics and acceptance rates. Adaptive MCMC methods which use past states\nto inform the proposal distribution has given rise to many ingenious sampling\nschemes for continuous problems and the present work can be seen as an\nimportant first step in bringing these benefits to partition-based problems\n", "versions": [{"version": "v1", "created": "Sat, 31 May 2014 11:50:03 GMT"}], "update_date": "2014-06-03", "authors_parsed": [["Herlau", "Tue", ""], ["M\u00f8rup", "Morten", ""], ["Teh", "Yee Whye", ""], ["Schmidt", "Mikkel N.", ""]]}, {"id": "1406.0118", "submitter": "Marina Meila", "authors": "Dominique Perrault-Joncas and Marina Meila", "title": "Improved graph Laplacian via geometric self-consistency", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of setting the kernel bandwidth used by Manifold\nLearning algorithms to construct the graph Laplacian. Exploiting the connection\nbetween manifold geometry, represented by the Riemannian metric, and the\nLaplace-Beltrami operator, we set the bandwidth by optimizing the Laplacian's\nability to preserve the geometry of the data. Experiments show that this\nprincipled approach is effective and robust.\n", "versions": [{"version": "v1", "created": "Sat, 31 May 2014 23:00:36 GMT"}], "update_date": "2014-06-03", "authors_parsed": [["Perrault-Joncas", "Dominique", ""], ["Meila", "Marina", ""]]}, {"id": "1406.0156", "submitter": "Sheng Han", "authors": "Sheng Han, Suzhen Wang, Xinyu Wu", "title": "$l_1$-regularized Outlier Isolation and Regression", "comments": "Outlier Detection, Robust Regression, Robust Rank Factorization,\n  $l_1$-regularization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposed a new regression model called $l_1$-regularized outlier\nisolation and regression (LOIRE) and a fast algorithm based on block coordinate\ndescent to solve this model. Besides, assuming outliers are gross errors\nfollowing a Bernoulli process, this paper also presented a Bernoulli estimate\nmodel which, in theory, should be very accurate and robust due to its complete\nelimination of affections caused by outliers. Though this Bernoulli estimate is\nhard to solve, it could be approximately achieved through a process which takes\nLOIRE as an important intermediate step. As a result, the approximate Bernoulli\nestimate is a good combination of Bernoulli estimate's accuracy and LOIRE\nregression's efficiency with several simulations conducted to strongly verify\nthis point. Moreover, LOIRE can be further extended to realize robust rank\nfactorization which is powerful in recovering low-rank component from massive\ncorruptions. Extensive experimental results showed that the proposed method\noutperforms state-of-the-art methods like RPCA and GoDec in the aspect of\ncomputation speed with a competitive performance.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jun 2014 11:52:19 GMT"}, {"version": "v2", "created": "Thu, 20 Nov 2014 08:58:09 GMT"}], "update_date": "2014-11-21", "authors_parsed": [["Han", "Sheng", ""], ["Wang", "Suzhen", ""], ["Wu", "Xinyu", ""]]}, {"id": "1406.0167", "submitter": "Saurabh Paul", "authors": "Saurabh Paul, Malik Magdon-Ismail and Petros Drineas", "title": "Feature Selection for Linear SVM with Provable Guarantees", "comments": "Appearing in Proceedings of 18th AISTATS, JMLR W&CP, vol 38, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give two provably accurate feature-selection techniques for the linear\nSVM. The algorithms run in deterministic and randomized time respectively. Our\nalgorithms can be used in an unsupervised or supervised setting. The supervised\napproach is based on sampling features from support vectors. We prove that the\nmargin in the feature space is preserved to within $\\epsilon$-relative error of\nthe margin in the full feature space in the worst-case. In the unsupervised\nsetting, we also provide worst-case guarantees of the radius of the minimum\nenclosing ball, thereby ensuring comparable generalization as in the full\nfeature space and resolving an open problem posed in Dasgupta et al. We present\nextensive experiments on real-world datasets to support our theory and to\ndemonstrate that our method is competitive and often better than prior\nstate-of-the-art, for which there are no known provable guarantees.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jun 2014 14:37:54 GMT"}, {"version": "v2", "created": "Mon, 20 Oct 2014 14:20:00 GMT"}, {"version": "v3", "created": "Fri, 6 Feb 2015 13:43:54 GMT"}], "update_date": "2015-02-09", "authors_parsed": [["Paul", "Saurabh", ""], ["Magdon-Ismail", "Malik", ""], ["Drineas", "Petros", ""]]}, {"id": "1406.0189", "submitter": "Nikolai Slavov", "authors": "Dmitry Malioutov and Nikolai Slavov", "title": "Convex Total Least Squares", "comments": "9 pages, 4 figures", "journal-ref": "JMLR W&CP 32 (1) :109-117, 2014", "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.GN q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the total least squares (TLS) problem that generalizes least squares\nregression by allowing measurement errors in both dependent and independent\nvariables. TLS is widely used in applied fields including computer vision,\nsystem identification and econometrics. The special case when all dependent and\nindependent variables have the same level of uncorrelated Gaussian noise, known\nas ordinary TLS, can be solved by singular value decomposition (SVD). However,\nSVD cannot solve many important practical TLS problems with realistic noise\nstructure, such as having varying measurement noise, known structure on the\nerrors, or large outliers requiring robust error-norms. To solve such problems,\nwe develop convex relaxation approaches for a general class of structured TLS\n(STLS). We show both theoretically and experimentally, that while the plain\nnuclear norm relaxation incurs large approximation errors for STLS, the\nre-weighted nuclear norm approach is very effective, and achieves better\naccuracy on challenging STLS problems than popular non-convex solvers. We\ndescribe a fast solution based on augmented Lagrangian formulation, and apply\nour approach to an important class of biological problems that use population\naverage measurements to infer cell-type and physiological-state specific\nexpression levels that are very hard to measure directly.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jun 2014 18:13:08 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Malioutov", "Dmitry", ""], ["Slavov", "Nikolai", ""]]}, {"id": "1406.0193", "submitter": "Nikolai Slavov", "authors": "Nikolai Slavov", "title": "Inference of Sparse Networks with Unobserved Variables. Application to\n  Gene Regulatory Networks", "comments": "8 pages, 5 figures", "journal-ref": "JMLR W&CP 9:757-764, 2010", "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.MN q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks are a unifying framework for modeling complex systems and network\ninference problems are frequently encountered in many fields. Here, I develop\nand apply a generative approach to network inference (RCweb) for the case when\nthe network is sparse and the latent (not observed) variables affect the\nobserved ones. From all possible factor analysis (FA) decompositions explaining\nthe variance in the data, RCweb selects the FA decomposition that is consistent\nwith a sparse underlying network. The sparsity constraint is imposed by a novel\nmethod that significantly outperforms (in terms of accuracy, robustness to\nnoise, complexity scaling, and computational efficiency) Bayesian methods and\nMLE methods using l1 norm relaxation such as K-SVD and l1--based sparse\nprinciple component analysis (PCA). Results from simulated models demonstrate\nthat RCweb recovers exactly the model structures for sparsity as low (as\nnon-sparse) as 50% and with ratio of unobserved to observed variables as high\nas 2. RCweb is robust to noise, with gradual decrease in the parameter ranges\nas the noise level increases.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jun 2014 19:09:14 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Slavov", "Nikolai", ""]]}, {"id": "1406.0214", "submitter": "Nathaniel Strawn", "authors": "Paul Bendich, Sang Chin, Jesse Clarke, Jonathan deSena, John Harer,\n  Elizabeth Munch, Andrew Newman, David Porter, David Rouse, Nate Strawn, and\n  Adam Watkins", "title": "Topological and Statistical Behavior Classifiers for Tracking\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY math.AT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the first unified theory for target tracking using Multiple\nHypothesis Tracking, Topological Data Analysis, and machine learning. Our\nstring of innovations are 1) robust topological features are used to encode\nbehavioral information, 2) statistical models are fitted to distributions over\nthese topological features, and 3) the target type classification methods of\nWigren and Bar Shalom et al. are employed to exploit the resulting likelihoods\nfor topological features inside of the tracking procedure. To demonstrate the\nefficacy of our approach, we test our procedure on synthetic vehicular data\ngenerated by the Simulation of Urban Mobility package.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jun 2014 23:07:37 GMT"}], "update_date": "2014-06-03", "authors_parsed": [["Bendich", "Paul", ""], ["Chin", "Sang", ""], ["Clarke", "Jesse", ""], ["deSena", "Jonathan", ""], ["Harer", "John", ""], ["Munch", "Elizabeth", ""], ["Newman", "Andrew", ""], ["Porter", "David", ""], ["Rouse", "David", ""], ["Strawn", "Nate", ""], ["Watkins", "Adam", ""]]}, {"id": "1406.0281", "submitter": "Veronika Cheplygina", "authors": "Veronika Cheplygina, David M. J. Tax, Marco Loog", "title": "On Classification with Bags, Groups and Sets", "comments": null, "journal-ref": "Pattern Recognition Letters Volume 59, 2015, Pages 11 - 17", "doi": "10.1016/j.patrec.2015.03.008", "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many classification problems can be difficult to formulate directly in terms\nof the traditional supervised setting, where both training and test samples are\nindividual feature vectors. There are cases in which samples are better\ndescribed by sets of feature vectors, that labels are only available for sets\nrather than individual samples, or, if individual labels are available, that\nthese are not independent. To better deal with such problems, several\nextensions of supervised learning have been proposed, where either training\nand/or test objects are sets of feature vectors. However, having been proposed\nrather independently of each other, their mutual similarities and differences\nhave hitherto not been mapped out. In this work, we provide an overview of such\nlearning scenarios, propose a taxonomy to illustrate the relationships between\nthem, and discuss directions for further research in these areas.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jun 2014 08:06:12 GMT"}, {"version": "v2", "created": "Tue, 7 Oct 2014 14:55:44 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Cheplygina", "Veronika", ""], ["Tax", "David M. J.", ""], ["Loog", "Marco", ""]]}, {"id": "1406.0304", "submitter": "Markus Schneider", "authors": "Markus Schneider and Fabio Ramos", "title": "Transductive Learning for Multi-Task Copula Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of multi-task learning with copula process.\nMultivariable prediction in spatial and spatial-temporal processes such as\nnatural resource estimation and pollution monitoring have been typically\naddressed using techniques based on Gaussian processes and co-Kriging. While\nthe Gaussian prior assumption is convenient from analytical and computational\nperspectives, nature is dominated by non-Gaussian likelihoods. Copula processes\nare an elegant and flexible solution to handle various non-Gaussian likelihoods\nby capturing the dependence structure of random variables with cumulative\ndistribution functions rather than their marginals. We show how multi-task\nlearning for copula processes can be used to improve multivariable prediction\nfor problems where the simple Gaussianity prior assumption does not hold. Then,\nwe present a transductive approximation for multi-task learning and derive\nanalytical expressions for the copula process model. The approach is evaluated\nand compared to other techniques in one artificial dataset and two publicly\navailable datasets for natural resource estimation and concrete slump\nprediction.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jun 2014 09:22:49 GMT"}], "update_date": "2014-06-03", "authors_parsed": [["Schneider", "Markus", ""], ["Ramos", "Fabio", ""]]}, {"id": "1406.0531", "submitter": "Ricardo Silva", "authors": "Ricardo Silva and Robin Evans", "title": "Causal Inference through a Witness Protection Program", "comments": "41 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most fundamental problems in causal inference is the estimation of\na causal effect when variables are confounded. This is difficult in an\nobservational study, because one has no direct evidence that all confounders\nhave been adjusted for. We introduce a novel approach for estimating causal\neffects that exploits observational conditional independencies to suggest\n\"weak\" paths in a unknown causal graph. The widely used faithfulness condition\nof Spirtes et al. is relaxed to allow for varying degrees of \"path\ncancellations\" that imply conditional independencies but do not rule out the\nexistence of confounding causal paths. The outcome is a posterior distribution\nover bounds on the average causal effect via a linear programming approach and\nBayesian inference. We claim this approach should be used in regular practice\nalong with other default tools in observational studies.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jun 2014 20:42:14 GMT"}, {"version": "v2", "created": "Thu, 30 Oct 2014 22:07:15 GMT"}], "update_date": "2014-11-03", "authors_parsed": [["Silva", "Ricardo", ""], ["Evans", "Robin", ""]]}, {"id": "1406.0597", "submitter": "Hitoshi Koyano", "authors": "Hitoshi Koyano, Morihiro Hayashida, Tatsuya Akutsu", "title": "Maximum margin classifier working in a set of strings", "comments": "This manuscript has been withdrawn because the experiments in Section\n  6 are insufficient", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numbers and numerical vectors account for a large portion of data. However,\nrecently the amount of string data generated has increased dramatically.\nConsequently, classifying string data is a common problem in many fields. The\nmost widely used approach to this problem is to convert strings into numerical\nvectors using string kernels and subsequently apply a support vector machine\nthat works in a numerical vector space. However, this non-one-to-one conversion\ninvolves a loss of information and makes it impossible to evaluate, using\nprobability theory, the generalization error of a learning machine, considering\nthat the given data to train and test the machine are strings generated\naccording to probability laws. In this study, we approach this classification\nproblem by constructing a classifier that works in a set of strings. To\nevaluate the generalization error of such a classifier theoretically,\nprobability theory for strings is required. Therefore, we first extend a limit\ntheorem on the asymptotic behavior of a consensus sequence of strings, which is\nthe counterpart of the mean of numerical vectors, as demonstrated in the\nprobability theory on a metric space of strings developed by one of the authors\nand his colleague in a previous study [18]. Using the obtained result, we then\ndemonstrate that our learning machine classifies strings in an asymptotically\noptimal manner. Furthermore, we demonstrate the usefulness of our machine in\npractical data analysis by applying it to predicting protein--protein\ninteractions using amino acid sequences.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jun 2014 06:58:33 GMT"}, {"version": "v2", "created": "Sun, 19 Oct 2014 06:59:12 GMT"}, {"version": "v3", "created": "Tue, 23 Feb 2016 04:10:03 GMT"}], "update_date": "2016-02-24", "authors_parsed": [["Koyano", "Hitoshi", ""], ["Hayashida", "Morihiro", ""], ["Akutsu", "Tatsuya", ""]]}, {"id": "1406.0764", "submitter": "Ashkan Ertefaie", "authors": "Ashkan Ertefaie", "title": "Constructing Dynamic Treatment Regimes in Infinite-Horizon Settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of existing methods for constructing optimal dynamic\ntreatment regimes is limited to cases where investigators are interested in\noptimizing a utility function over a fixed period of time (finite horizon). In\nthis manuscript, we develop an inferential procedure based on temporal\ndifference residuals for optimal dynamic treatment regimes in infinite-horizon\nsettings, where there is no a priori fixed end of follow-up point. The proposed\nmethod can be used to determine the optimal regime in chronic diseases where\npatients are monitored and treated throughout their life. We derive large\nsample results necessary for conducting inference. We also simulate a cohort of\npatients with diabetes to mimic the third wave of the National Health and\nNutrition Examination Survey, and we examine the performance of the proposed\nmethod in controlling the level of hemoglobin A1c. Supplementary materials for\nthis article are available online.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jun 2014 15:55:33 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2015 06:25:38 GMT"}], "update_date": "2015-10-22", "authors_parsed": [["Ertefaie", "Ashkan", ""]]}, {"id": "1406.0824", "submitter": "Sukru Burc Eryilmaz", "authors": "Sercan Arik, Sukru Burc Eryilmaz, Adam Goldberg", "title": "Supervised classification-based stock prediction and portfolio\n  optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.CE cs.LG q-fin.PM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the number of publicly traded companies as well as the amount of their\nfinancial data grows rapidly, it is highly desired to have tracking, analysis,\nand eventually stock selections automated. There have been few works focusing\non estimating the stock prices of individual companies. However, many of those\nhave worked with very small number of financial parameters. In this work, we\napply machine learning techniques to address automated stock picking, while\nusing a larger number of financial parameters for individual companies than the\nprevious studies. Our approaches are based on the supervision of prediction\nparameters using company fundamentals, time-series properties, and correlation\ninformation between different stocks. We examine a variety of supervised\nlearning techniques and found that using stock fundamentals is a useful\napproach for the classification problem, when combined with the high\ndimensional data handling capabilities of support vector machine. The portfolio\nour system suggests by predicting the behavior of stocks results in a 3% larger\ngrowth on average than the overall market within a 3-month time period, as the\nout-of-sample test suggests.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jun 2014 19:32:09 GMT"}], "update_date": "2014-06-04", "authors_parsed": [["Arik", "Sercan", ""], ["Eryilmaz", "Sukru Burc", ""], ["Goldberg", "Adam", ""]]}, {"id": "1406.0873", "submitter": "John Cunningham PhD", "authors": "John P. Cunningham and Zoubin Ghahramani", "title": "Linear Dimensionality Reduction: Survey, Insights, and Generalizations", "comments": "42 pages, 5 figures, 1 table", "journal-ref": "Journal of Machine Learning Research. 16(Dec): 2859-2900, 2015", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear dimensionality reduction methods are a cornerstone of analyzing high\ndimensional data, due to their simple geometric interpretations and typically\nattractive computational properties. These methods capture many data features\nof interest, such as covariance, dynamical structure, correlation between data\nsets, input-output relationships, and margin between data classes. Methods have\nbeen developed with a variety of names and motivations in many fields, and\nperhaps as a result the connections between all these methods have not been\nhighlighted. Here we survey methods from this disparate literature as\noptimization programs over matrix manifolds. We discuss principal component\nanalysis, factor analysis, linear multidimensional scaling, Fisher's linear\ndiscriminant analysis, canonical correlations analysis, maximum autocorrelation\nfactors, slow feature analysis, sufficient dimensionality reduction,\nundercomplete independent component analysis, linear regression, distance\nmetric learning, and more. This optimization framework gives insight to some\nrarely discussed shortcomings of well-known methods, such as the suboptimality\nof certain eigenvector solutions. Modern techniques for optimization over\nmatrix manifolds enable a generic linear dimensionality reduction solver, which\naccepts as input data and an objective to be optimized, and returns, as output,\nan optimal low-dimensional projection of the data. This simple optimization\nframework further allows straightforward generalizations and novel variants of\nclassical methods, which we demonstrate here by creating an\northogonal-projection canonical correlations analysis. More broadly, this\nsurvey and generic solver suggest that linear dimensionality reduction can move\ntoward becoming a blackbox, objective-agnostic numerical technology.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jun 2014 20:53:38 GMT"}, {"version": "v2", "created": "Fri, 18 Mar 2016 20:18:26 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Cunningham", "John P.", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1406.0919", "submitter": "Guanghui Lan", "authors": "Guanghui Lan", "title": "Gradient Sliding for Composite Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider in this paper a class of composite optimization problems whose\nobjective function is given by the summation of a general smooth and nonsmooth\ncomponent, together with a relatively simple nonsmooth term. We present a new\nclass of first-order methods, namely the gradient sliding algorithms, which can\nskip the computation of the gradient for the smooth component from time to\ntime. As a consequence, these algorithms require only ${\\cal\nO}(1/\\sqrt{\\epsilon})$ gradient evaluations for the smooth component in order\nto find an $\\epsilon$-solution for the composite problem, while still\nmaintaining the optimal ${\\cal O}(1/\\epsilon^2)$ bound on the total number of\nsubgradient evaluations for the nonsmooth component. We then present a\nstochastic counterpart for these algorithms and establish similar complexity\nbounds for solving an important class of stochastic composite optimization\nproblems. Moreover, if the smooth component in the composite function is\nstrongly convex, the developed gradient sliding algorithms can significantly\nreduce the number of graduate and subgradient evaluations for the smooth and\nnonsmooth component to ${\\cal O} (\\log (1/\\epsilon))$ and ${\\cal\nO}(1/\\epsilon)$, respectively. Finally, we generalize these algorithms to the\ncase when the smooth component is replaced by a nonsmooth one possessing a\ncertain bi-linear saddle point structure.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jun 2014 01:52:27 GMT"}, {"version": "v2", "created": "Thu, 12 Jun 2014 00:39:21 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Lan", "Guanghui", ""]]}, {"id": "1406.1078", "submitter": "KyungHyun Cho", "authors": "Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry\n  Bahdanau, Fethi Bougares, Holger Schwenk and Yoshua Bengio", "title": "Learning Phrase Representations using RNN Encoder-Decoder for\n  Statistical Machine Translation", "comments": "EMNLP 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel neural network model called RNN\nEncoder-Decoder that consists of two recurrent neural networks (RNN). One RNN\nencodes a sequence of symbols into a fixed-length vector representation, and\nthe other decodes the representation into another sequence of symbols. The\nencoder and decoder of the proposed model are jointly trained to maximize the\nconditional probability of a target sequence given a source sequence. The\nperformance of a statistical machine translation system is empirically found to\nimprove by using the conditional probabilities of phrase pairs computed by the\nRNN Encoder-Decoder as an additional feature in the existing log-linear model.\nQualitatively, we show that the proposed model learns a semantically and\nsyntactically meaningful representation of linguistic phrases.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jun 2014 17:47:08 GMT"}, {"version": "v2", "created": "Thu, 24 Jul 2014 20:07:13 GMT"}, {"version": "v3", "created": "Wed, 3 Sep 2014 00:25:02 GMT"}], "update_date": "2014-09-04", "authors_parsed": [["Cho", "Kyunghyun", ""], ["van Merrienboer", "Bart", ""], ["Gulcehre", "Caglar", ""], ["Bahdanau", "Dzmitry", ""], ["Bougares", "Fethi", ""], ["Schwenk", "Holger", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1406.1089", "submitter": "Stephen Becker", "authors": "Aleksandr Aravkin, Stephen Becker, Volkan Cevher, Peder Olsen", "title": "A variational approach to stable principal component pursuit", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new convex formulation for stable principal component pursuit\n(SPCP) to decompose noisy signals into low-rank and sparse representations. For\nnumerical solutions of our SPCP formulation, we first develop a convex\nvariational framework and then accelerate it with quasi-Newton methods. We\nshow, via synthetic and real data experiments, that our approach offers\nadvantages over the classical SPCP formulations in scalability and practical\nparameter selection.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jun 2014 16:10:49 GMT"}], "update_date": "2014-06-06", "authors_parsed": [["Aravkin", "Aleksandr", ""], ["Becker", "Stephen", ""], ["Cevher", "Volkan", ""], ["Olsen", "Peder", ""]]}, {"id": "1406.1102", "submitter": "Pinghua Gong", "authors": "Pinghua Gong and Jieping Ye", "title": "Linear Convergence of Variance-Reduced Stochastic Gradient without\n  Strong Convexity", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient algorithms estimate the gradient based on only one or a\nfew samples and enjoy low computational cost per iteration. They have been\nwidely used in large-scale optimization problems. However, stochastic gradient\nalgorithms are usually slow to converge and achieve sub-linear convergence\nrates, due to the inherent variance in the gradient computation. To accelerate\nthe convergence, some variance-reduced stochastic gradient algorithms, e.g.,\nproximal stochastic variance-reduced gradient (Prox-SVRG) algorithm, have\nrecently been proposed to solve strongly convex problems. Under the strongly\nconvex condition, these variance-reduced stochastic gradient algorithms achieve\na linear convergence rate. However, many machine learning problems are convex\nbut not strongly convex. In this paper, we introduce Prox-SVRG and its\nprojected variant called Variance-Reduced Projected Stochastic Gradient (VRPSG)\nto solve a class of non-strongly convex optimization problems widely used in\nmachine learning. As the main technical contribution of this paper, we show\nthat both VRPSG and Prox-SVRG achieve a linear convergence rate without strong\nconvexity. A key ingredient in our proof is a Semi-Strongly Convex (SSC)\ninequality which is the first to be rigorously proved for a class of\nnon-strongly convex problems in both constrained and regularized settings.\nMoreover, the SSC inequality is independent of algorithms and may be applied to\nanalyze other stochastic gradient algorithms besides VRPSG and Prox-SVRG, which\nmay be of independent interest. To the best of our knowledge, this is the first\nwork that establishes the linear convergence rate for the variance-reduced\nstochastic gradient algorithms on solving both constrained and regularized\nproblems without strong convexity.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jun 2014 16:37:33 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2015 14:44:37 GMT"}], "update_date": "2015-07-13", "authors_parsed": [["Gong", "Pinghua", ""], ["Ye", "Jieping", ""]]}, {"id": "1406.1222", "submitter": "Greg Ver Steeg", "authors": "Greg Ver Steeg and Aram Galstyan", "title": "Discovering Structure in High-Dimensional Data Through Correlation\n  Explanation", "comments": "15 pages, 6 figures. Includes supplementary material and link to\n  code. Published in the proceedings of the 28th Annual Conference on Neural\n  Information Processing Systems, NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method to learn a hierarchy of successively more abstract\nrepresentations of complex data based on optimizing an information-theoretic\nobjective. Intuitively, the optimization searches for a set of latent factors\nthat best explain the correlations in the data as measured by multivariate\nmutual information. The method is unsupervised, requires no model assumptions,\nand scales linearly with the number of variables which makes it an attractive\napproach for very high dimensional systems. We demonstrate that Correlation\nExplanation (CorEx) automatically discovers meaningful structure for data from\ndiverse sources including personality tests, DNA, and human language.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jun 2014 21:46:30 GMT"}, {"version": "v2", "created": "Fri, 31 Oct 2014 02:43:28 GMT"}], "update_date": "2014-11-03", "authors_parsed": [["Steeg", "Greg Ver", ""], ["Galstyan", "Aram", ""]]}, {"id": "1406.1231", "submitter": "George Dahl", "authors": "George E. Dahl and Navdeep Jaitly and Ruslan Salakhutdinov", "title": "Multi-task Neural Networks for QSAR Predictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although artificial neural networks have occasionally been used for\nQuantitative Structure-Activity/Property Relationship (QSAR/QSPR) studies in\nthe past, the literature has of late been dominated by other machine learning\ntechniques such as random forests. However, a variety of new neural net\ntechniques along with successful applications in other domains have renewed\ninterest in network approaches. In this work, inspired by the winning team's\nuse of neural networks in a recent QSAR competition, we used an artificial\nneural network to learn a function that predicts activities of compounds for\nmultiple assays at the same time. We conducted experiments leveraging recent\nmethods for dealing with overfitting in neural networks as well as other tricks\nfrom the neural networks literature. We compared our methods to alternative\nmethods reported to perform well on these tasks and found that our neural net\nmethods provided superior performance.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jun 2014 23:00:05 GMT"}], "update_date": "2014-06-06", "authors_parsed": [["Dahl", "George E.", ""], ["Jaitly", "Navdeep", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1406.1411", "submitter": "Denis Mau\\'a Dr.", "authors": "Siqi Nie, Denis Deratani Maua, Cassio Polpo de Campos, Qiang Ji", "title": "Advances in Learning Bayesian Networks of Bounded Treewidth", "comments": "23 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents novel algorithms for learning Bayesian network structures\nwith bounded treewidth. Both exact and approximate methods are developed. The\nexact method combines mixed-integer linear programming formulations for\nstructure learning and treewidth computation. The approximate method consists\nin uniformly sampling $k$-trees (maximal graphs of treewidth $k$), and\nsubsequently selecting, exactly or approximately, the best structure whose\nmoral graph is a subgraph of that $k$-tree. Some properties of these methods\nare discussed and proven. The approaches are empirically compared to each other\nand to a state-of-the-art method for learning bounded treewidth structures on a\ncollection of public data sets with up to 100 variables. The experiments show\nthat our exact algorithm outperforms the state of the art, and that the\napproximate approach is fairly accurate.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jun 2014 15:10:40 GMT"}, {"version": "v2", "created": "Fri, 6 Jun 2014 19:51:07 GMT"}], "update_date": "2014-06-09", "authors_parsed": [["Nie", "Siqi", ""], ["Maua", "Denis Deratani", ""], ["de Campos", "Cassio Polpo", ""], ["Ji", "Qiang", ""]]}, {"id": "1406.1440", "submitter": "Pierre Alquier", "authors": "Pierre Alquier, Vincent Cottet, Nicolas Chopin, Judith Rousseau", "title": "Bayesian matrix completion: prior specification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank matrix estimation from incomplete measurements recently received\nincreased attention due to the emergence of several challenging applications,\nsuch as recommender systems; see in particular the famous Netflix challenge.\nWhile the behaviour of algorithms based on nuclear norm minimization is now\nwell understood, an as yet unexplored avenue of research is the behaviour of\nBayesian algorithms in this context. In this paper, we briefly review the\npriors used in the Bayesian literature for matrix completion. A standard\napproach is to assign an inverse gamma prior to the singular values of a\ncertain singular value decomposition of the matrix of interest; this prior is\nconjugate. However, we show that two other types of priors (again for the\nsingular values) may be conjugate for this model: a gamma prior, and a discrete\nprior. Conjugacy is very convenient, as it makes it possible to implement\neither Gibbs sampling or Variational Bayes. Interestingly enough, the maximum a\nposteriori for these different priors is related to the nuclear norm\nminimization problems. We also compare all these priors on simulated datasets,\nand on the classical MovieLens and Netflix datasets.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jun 2014 16:46:46 GMT"}, {"version": "v2", "created": "Wed, 11 Jun 2014 15:32:24 GMT"}, {"version": "v3", "created": "Wed, 22 Oct 2014 15:34:33 GMT"}], "update_date": "2014-10-23", "authors_parsed": [["Alquier", "Pierre", ""], ["Cottet", "Vincent", ""], ["Chopin", "Nicolas", ""], ["Rousseau", "Judith", ""]]}, {"id": "1406.1485", "submitter": "Li Yao", "authors": "Tapani Raiko, Li Yao, Kyunghyun Cho and Yoshua Bengio", "title": "Iterative Neural Autoregressive Distribution Estimator (NADE-k)", "comments": "Accepted at Neural Information Processing Systems (NIPS) 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training of the neural autoregressive density estimator (NADE) can be viewed\nas doing one step of probabilistic inference on missing values in data. We\npropose a new model that extends this inference scheme to multiple steps,\narguing that it is easier to learn to improve a reconstruction in $k$ steps\nrather than to learn to reconstruct in a single inference step. The proposed\nmodel is an unsupervised building block for deep learning that combines the\ndesirable properties of NADE and multi-predictive training: (1) Its test\nlikelihood can be computed analytically, (2) it is easy to generate independent\nsamples from it, and (3) it uses an inference engine that is a superset of\nvariational inference for Boltzmann machines. The proposed NADE-k is\ncompetitive with the state-of-the-art in density estimation on the two datasets\ntested.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jun 2014 19:13:51 GMT"}, {"version": "v2", "created": "Wed, 11 Jun 2014 11:28:36 GMT"}, {"version": "v3", "created": "Sat, 6 Dec 2014 00:22:00 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Raiko", "Tapani", ""], ["Yao", "Li", ""], ["Cho", "Kyunghyun", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1406.1546", "submitter": "Sanjoy Dasgupta", "authors": "Kamalika Chaudhuri, Sanjoy Dasgupta, Samory Kpotufe and Ulrike von\n  Luxburg", "title": "Consistent procedures for cluster tree estimation and pruning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a density $f$ on ${\\mathbb R}^d$, a {\\it high-density cluster} is any\nconnected component of $\\{x: f(x) \\geq \\lambda\\}$, for some $\\lambda > 0$. The\nset of all high-density clusters forms a hierarchy called the {\\it cluster\ntree} of $f$. We present two procedures for estimating the cluster tree given\nsamples from $f$. The first is a robust variant of the single linkage algorithm\nfor hierarchical clustering. The second is based on the $k$-nearest neighbor\ngraph of the samples. We give finite-sample convergence rates for these\nalgorithms which also imply consistency, and we derive lower bounds on the\nsample complexity of cluster tree estimation. Finally, we study a tree pruning\nprocedure that guarantees, under milder conditions than usual, to remove\nclusters that are spurious while recovering those that are salient.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jun 2014 23:34:06 GMT"}], "update_date": "2014-06-09", "authors_parsed": [["Chaudhuri", "Kamalika", ""], ["Dasgupta", "Sanjoy", ""], ["Kpotufe", "Samory", ""], ["von Luxburg", "Ulrike", ""]]}, {"id": "1406.1621", "submitter": "Matthias Seibert", "authors": "Matthias Seibert, Julian W\\\"ormann, R\\'emi Gribonval, Martin\n  Kleinsteuber", "title": "Separable Cosparse Analysis Operator Learning", "comments": "5 pages, 3 figures, accepted at EUSIPCO 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability of having a sparse representation for a certain class of signals\nhas many applications in data analysis, image processing, and other research\nfields. Among sparse representations, the cosparse analysis model has recently\ngained increasing interest. Many signals exhibit a multidimensional structure,\ne.g. images or three-dimensional MRI scans. Most data analysis and learning\nalgorithms use vectorized signals and thereby do not account for this\nunderlying structure. The drawback of not taking the inherent structure into\naccount is a dramatic increase in computational cost. We propose an algorithm\nfor learning a cosparse Analysis Operator that adheres to the preexisting\nstructure of the data, and thus allows for a very efficient implementation.\nThis is achieved by enforcing a separable structure on the learned operator.\nOur learning algorithm is able to deal with multidimensional data of arbitrary\norder. We evaluate our method on volumetric data at the example of\nthree-dimensional MRI scans.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jun 2014 09:33:59 GMT"}], "update_date": "2014-06-09", "authors_parsed": [["Seibert", "Matthias", ""], ["W\u00f6rmann", "Julian", ""], ["Gribonval", "R\u00e9mi", ""], ["Kleinsteuber", "Martin", ""]]}, {"id": "1406.1655", "submitter": "Justin Bayer", "authors": "Justin Bayer, Christian Osendorfer", "title": "Variational inference of latent state sequences using Recurrent Networks", "comments": "This paper has been withdrawn due to a derivation/implementation\n  error and the resulting invalidation of the results", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in the estimation of deep directed graphical models and\nrecurrent networks let us contribute to the removal of a blind spot in the area\nof probabilistc modelling of time series. The proposed methods i) can infer\ndistributed latent state-space trajectories with nonlinear transitions, ii)\nscale to large data sets thanks to the use of a stochastic objective and fast,\napproximate inference, iii) enable the design of rich emission models which iv)\nwill naturally lead to structured outputs. Two different paths of introducing\nlatent state sequences are pursued, leading to the variational recurrent auto\nencoder (VRAE) and the variational one step predictor (VOSP). The use of\nindependent Wiener processes as priors on the latent state sequence is a viable\ncompromise between efficient computation of the Kullback-Leibler divergence\nfrom the variational approximation of the posterior and maintaining a\nreasonable belief in the dynamics. We verify our methods empirically, obtaining\nresults close or superior to the state of the art. We also show qualitative\nresults for denoising and missing value imputation.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jun 2014 11:53:46 GMT"}, {"version": "v2", "created": "Tue, 30 Sep 2014 08:04:58 GMT"}], "update_date": "2014-10-01", "authors_parsed": [["Bayer", "Justin", ""], ["Osendorfer", "Christian", ""]]}, {"id": "1406.1780", "submitter": "Yen-Chi Chen", "authors": "Yen-Chi Chen, Christopher R. Genovese, Larry Wasserman", "title": "A Comprehensive Approach to Mode Clustering", "comments": "34 pages, 17 figures. Accepted to the Electronic Journal of\n  Statistics. The original title is \"Enhanced Mode Clustering\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mode clustering is a nonparametric method for clustering that defines\nclusters using the basins of attraction of a density estimator's modes. We\nprovide several enhancements to mode clustering: (i) a soft variant of cluster\nassignment, (ii) a measure of connectivity between clusters, (iii) a technique\nfor choosing the bandwidth, (iv) a method for denoising small clusters, and (v)\nan approach to visualizing the clusters. Combining all these enhancements gives\nus a complete procedure for clustering in multivariate problems. We also\ncompare mode clustering to other clustering methods in several examples\n", "versions": [{"version": "v1", "created": "Fri, 6 Jun 2014 19:34:54 GMT"}, {"version": "v2", "created": "Sun, 7 Dec 2014 15:40:02 GMT"}, {"version": "v3", "created": "Sun, 14 Dec 2014 01:06:42 GMT"}, {"version": "v4", "created": "Tue, 22 Dec 2015 15:20:50 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["Chen", "Yen-Chi", ""], ["Genovese", "Christopher R.", ""], ["Wasserman", "Larry", ""]]}, {"id": "1406.1845", "submitter": "Lucas Mentch", "authors": "Lucas Mentch and Giles Hooker", "title": "Formal Hypothesis Tests for Additive Structure in Random Forests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While statistical learning methods have proved powerful tools for predictive\nmodeling, the black-box nature of the models they produce can severely limit\ntheir interpretability and the ability to conduct formal inference. However,\nthe natural structure of ensemble learners like bagged trees and random forests\nhas been shown to admit desirable asymptotic properties when base learners are\nbuilt with proper subsamples. In this work, we demonstrate that by defining an\nappropriate grid structure on the covariate space, we may carry out formal\nhypothesis tests for both variable importance and underlying additive model\nstructure. To our knowledge, these tests represent the first statistical tools\nfor investigating the underlying regression structure in a context such as\nrandom forests. We develop notions of total and partial additivity and further\ndemonstrate that testing can be carried out at no additional computational cost\nby estimating the variance within the process of constructing the ensemble.\nFurthermore, we propose a novel extension of these testing procedures utilizing\nrandom projections in order to allow for computationally efficient testing\nprocedures that retain high power even when the grid size is much larger than\nthat of the training set.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jun 2014 00:58:30 GMT"}, {"version": "v2", "created": "Tue, 11 Nov 2014 22:55:32 GMT"}, {"version": "v3", "created": "Fri, 26 Aug 2016 21:10:03 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Mentch", "Lucas", ""], ["Hooker", "Giles", ""]]}, {"id": "1406.1853", "submitter": "Ian Osband", "authors": "Ian Osband, Benjamin Van Roy", "title": "Model-based Reinforcement Learning and the Eluder Dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning to optimize an unknown Markov decision\nprocess (MDP). We show that, if the MDP can be parameterized within some known\nfunction class, we can obtain regret bounds that scale with the dimensionality,\nrather than cardinality, of the system. We characterize this dependence\nexplicitly as $\\tilde{O}(\\sqrt{d_K d_E T})$ where $T$ is time elapsed, $d_K$ is\nthe Kolmogorov dimension and $d_E$ is the \\emph{eluder dimension}. These\nrepresent the first unified regret bounds for model-based reinforcement\nlearning and provide state of the art guarantees in several important settings.\nMoreover, we present a simple and computationally efficient algorithm\n\\emph{posterior sampling for reinforcement learning} (PSRL) that satisfies\nthese bounds.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jun 2014 03:02:09 GMT"}, {"version": "v2", "created": "Fri, 31 Oct 2014 23:36:00 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Osband", "Ian", ""], ["Van Roy", "Benjamin", ""]]}, {"id": "1406.1880", "submitter": "Alaa Saade", "authors": "Alaa Saade, Florent Krzakala and Lenka Zdeborov\\'a", "title": "Spectral Clustering of Graphs with the Bethe Hessian", "comments": "8 pages, 2 figures", "journal-ref": "Advances in Neural Information Processing Systems 27 (NIPS 2014)\n  pp 406-414", "doi": null, "report-no": null, "categories": "cond-mat.dis-nn cs.SI physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral clustering is a standard approach to label nodes on a graph by\nstudying the (largest or lowest) eigenvalues of a symmetric real matrix such as\ne.g. the adjacency or the Laplacian. Recently, it has been argued that using\ninstead a more complicated, non-symmetric and higher dimensional operator,\nrelated to the non-backtracking walk on the graph, leads to improved\nperformance in detecting clusters, and even to optimal performance for the\nstochastic block model. Here, we propose to use instead a simpler object, a\nsymmetric real matrix known as the Bethe Hessian operator, or deformed\nLaplacian. We show that this approach combines the performances of the\nnon-backtracking operator, thus detecting clusters all the way down to the\ntheoretical limit in the stochastic block model, with the computational,\ntheoretical and memory advantages of real symmetric matrices.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jun 2014 10:03:12 GMT"}, {"version": "v2", "created": "Mon, 8 Sep 2014 08:49:39 GMT"}], "update_date": "2015-04-30", "authors_parsed": [["Saade", "Alaa", ""], ["Krzakala", "Florent", ""], ["Zdeborov\u00e1", "Lenka", ""]]}, {"id": "1406.1916", "submitter": "Rajarshi Guhaniyogi", "authors": "Rajarshi Guhaniyogi and David B. Dunson", "title": "Compressed Gaussian Process", "comments": "33 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric regression for massive numbers of samples (n) and features (p)\nis an increasingly important problem. In big n settings, a common strategy is\nto partition the feature space, and then separately apply simple models to each\npartition set. We propose an alternative approach, which avoids such\npartitioning and the associated sensitivity to neighborhood choice and distance\nmetrics, by using random compression combined with Gaussian process regression.\nThe proposed approach is particularly motivated by the setting in which the\nresponse is conditionally independent of the features given the projection to a\nlow dimensional manifold. Conditionally on the random compression matrix and a\nsmoothness parameter, the posterior distribution for the regression surface and\nposterior predictive distributions are available analytically. Running the\nanalysis in parallel for many random compression matrices and smoothness\nparameters, model averaging is used to combine the results. The algorithm can\nbe implemented rapidly even in very big n and p problems, has strong\ntheoretical justification, and is found to yield state of the art predictive\nperformance.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jun 2014 18:28:58 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Guhaniyogi", "Rajarshi", ""], ["Dunson", "David B.", ""]]}, {"id": "1406.1922", "submitter": "Leila Wehbe", "authors": "Aaditya Ramdas and Leila Wehbe", "title": "Nonparametric Independence Testing for Small Sample Sizes", "comments": "Equal contribution from both student authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the problem of nonparametric independence testing, a\nfundamental decision-theoretic problem that asks if two arbitrary (possibly\nmultivariate) random variables $X,Y$ are independent or not, a question that\ncomes up in many fields like causality and neuroscience. While quantities like\ncorrelation of $X,Y$ only test for (univariate) linear independence, natural\nalternatives like mutual information of $X,Y$ are hard to estimate due to a\nserious curse of dimensionality. A recent approach, avoiding both issues,\nestimates norms of an \\textit{operator} in Reproducing Kernel Hilbert Spaces\n(RKHSs). Our main contribution is strong empirical evidence that by employing\n\\textit{shrunk} operators when the sample size is small, one can attain an\nimprovement in power at low false positive rates. We analyze the effects of\nStein shrinkage on a popular test statistic called HSIC (Hilbert-Schmidt\nIndependence Criterion). Our observations provide insights into two recently\nproposed shrinkage estimators, SCOSE and FCOSE - we prove that SCOSE is\n(essentially) the optimal linear shrinkage method for \\textit{estimating} the\ntrue operator; however, the non-linearly shrunk FCOSE usually achieves greater\nimprovements in \\textit{test power}. This work is important for more powerful\nnonparametric detection of subtle nonlinear dependencies for small samples.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jun 2014 19:14:23 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2015 03:31:35 GMT"}], "update_date": "2015-09-04", "authors_parsed": [["Ramdas", "Aaditya", ""], ["Wehbe", "Leila", ""]]}, {"id": "1406.2035", "submitter": "Dani Yogatama", "authors": "Dani Yogatama and Manaal Faruqui and Chris Dyer and Noah A. Smith", "title": "Learning Word Representations with Hierarchical Sparse Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for learning word representations using hierarchical\nregularization in sparse coding inspired by the linguistic study of word\nmeanings. We show an efficient learning algorithm based on stochastic proximal\nmethods that is significantly faster than previous approaches, making it\npossible to perform hierarchical sparse coding on a corpus of billions of word\ntokens. Experiments on various benchmark tasks---word similarity ranking,\nanalogies, sentence completion, and sentiment analysis---demonstrate that the\nmethod outperforms or is competitive with state-of-the-art methods. Our word\nrepresentations are available at\n\\url{http://www.ark.cs.cmu.edu/dyogatam/wordvecs/}.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jun 2014 22:35:09 GMT"}, {"version": "v2", "created": "Thu, 6 Nov 2014 14:26:21 GMT"}], "update_date": "2014-11-07", "authors_parsed": [["Yogatama", "Dani", ""], ["Faruqui", "Manaal", ""], ["Dyer", "Chris", ""], ["Smith", "Noah A.", ""]]}, {"id": "1406.2082", "submitter": "Aaditya Ramdas", "authors": "Aaditya Ramdas and Ryan J. Tibshirani", "title": "Fast and Flexible ADMM Algorithms for Trend Filtering", "comments": "22 pages, 10 figures; published in Journal of Computational and\n  Graphical Statistics, 2015", "journal-ref": null, "doi": "10.1080/10618600.2015.1054033", "report-no": null, "categories": "stat.ML cs.LG cs.NA math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a fast and robust algorithm for trend filtering, a\nrecently developed nonparametric regression tool. It has been shown that, for\nestimating functions whose derivatives are of bounded variation, trend\nfiltering achieves the minimax optimal error rate, while other popular methods\nlike smoothing splines and kernels do not. Standing in the way of a more\nwidespread practical adoption, however, is a lack of scalable and numerically\nstable algorithms for fitting trend filtering estimates. This paper presents a\nhighly efficient, specialized ADMM routine for trend filtering. Our algorithm\nis competitive with the specialized interior point methods that are currently\nin use, and yet is far more numerically robust. Furthermore, the proposed ADMM\nimplementation is very simple, and importantly, it is flexible enough to extend\nto many interesting related problems, such as sparse trend filtering and\nisotonic trend filtering. Software for our method is freely available, in both\nthe C and R languages.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jun 2014 05:50:20 GMT"}, {"version": "v2", "created": "Mon, 27 Oct 2014 19:24:51 GMT"}, {"version": "v3", "created": "Mon, 18 May 2015 21:09:02 GMT"}, {"version": "v4", "created": "Sat, 29 Aug 2015 00:46:34 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Ramdas", "Aaditya", ""], ["Tibshirani", "Ryan J.", ""]]}, {"id": "1406.2083", "submitter": "Aaditya Ramdas", "authors": "Sashank J. Reddi, Aaditya Ramdas, Barnab\\'as P\\'oczos, Aarti Singh and\n  Larry Wasserman", "title": "On the Decreasing Power of Kernel and Distance based Nonparametric\n  Hypothesis Tests in High Dimensions", "comments": "19 pages, 9 figures, published in AAAI-15: The 29th AAAI Conference\n  on Artificial Intelligence (with author order reversed from ArXiv)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is about two related decision theoretic problems, nonparametric\ntwo-sample testing and independence testing. There is a belief that two\nrecently proposed solutions, based on kernels and distances between pairs of\npoints, behave well in high-dimensional settings. We identify different sources\nof misconception that give rise to the above belief. Specifically, we\ndifferentiate the hardness of estimation of test statistics from the hardness\nof testing whether these statistics are zero or not, and explicitly discuss a\nnotion of \"fair\" alternative hypotheses for these problems as dimension\nincreases. We then demonstrate that the power of these tests actually drops\npolynomially with increasing dimension against fair alternatives. We end with\nsome theoretical insights and shed light on the \\textit{median heuristic} for\nkernel bandwidth selection. Our work advances the current understanding of the\npower of modern nonparametric hypothesis tests in high dimensions.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jun 2014 05:59:21 GMT"}, {"version": "v2", "created": "Mon, 24 Nov 2014 00:23:35 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Reddi", "Sashank J.", ""], ["Ramdas", "Aaditya", ""], ["P\u00f3czos", "Barnab\u00e1s", ""], ["Singh", "Aarti", ""], ["Wasserman", "Larry", ""]]}, {"id": "1406.2098", "submitter": "Jie Peng", "authors": "Ru Wang and Jie Peng", "title": "Learning directed acyclic graphs via bootstrap aggregating", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic graphical models are graphical representations of probability\ndistributions. Graphical models have applications in many fields including\nbiology, social sciences, linguistic, neuroscience. In this paper, we propose\ndirected acyclic graphs (DAGs) learning via bootstrap aggregating. The proposed\nprocedure is named as DAGBag. Specifically, an ensemble of DAGs is first\nlearned based on bootstrap resamples of the data and then an aggregated DAG is\nderived by minimizing the overall distance to the entire ensemble. A family of\nmetrics based on the structural hamming distance is defined for the space of\nDAGs (of a given node set) and is used for aggregation. Under the\nhigh-dimensional-low-sample size setting, the graph learned on one data set\noften has excessive number of false positive edges due to over-fitting of the\nnoise. Aggregation overcomes over-fitting through variance reduction and thus\ngreatly reduces false positives. We also develop an efficient implementation of\nthe hill climbing search algorithm of DAG learning which makes the proposed\nmethod computationally competitive for the high-dimensional regime. The DAGBag\nprocedure is implemented in the R package dagbag.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jun 2014 07:43:22 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Wang", "Ru", ""], ["Peng", "Jie", ""]]}, {"id": "1406.2206", "submitter": "Martin Azizyan", "authors": "Martin Azizyan and Aarti Singh and Larry Wasserman", "title": "Efficient Sparse Clustering of High-Dimensional Non-spherical Gaussian\n  Mixtures", "comments": "11 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of clustering data points in high dimensions, i.e.\nwhen the number of data points may be much smaller than the number of\ndimensions. Specifically, we consider a Gaussian mixture model (GMM) with\nnon-spherical Gaussian components, where the clusters are distinguished by only\na few relevant dimensions. The method we propose is a combination of a recent\napproach for learning parameters of a Gaussian mixture model and sparse linear\ndiscriminant analysis (LDA). In addition to cluster assignments, the method\nreturns an estimate of the set of features relevant for clustering. Our results\nindicate that the sample complexity of clustering depends on the sparsity of\nthe relevant feature set, while only scaling logarithmically with the ambient\ndimension. Additionally, we require much milder assumptions than existing work\non clustering in high dimensions. In particular, we do not require spherical\nclusters nor necessitate mean separation along relevant dimensions.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jun 2014 14:57:16 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Azizyan", "Martin", ""], ["Singh", "Aarti", ""], ["Wasserman", "Larry", ""]]}, {"id": "1406.2235", "submitter": "Michael Smith", "authors": "Michael R. Smith, Tony Martinez, Michael Gashler", "title": "A Hybrid Latent Variable Neural Network Model for Item Recommendation", "comments": "10 pages, 3 tables. arXiv admin note: text overlap with\n  arXiv:1312.5394", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering is used to recommend items to a user without\nrequiring a knowledge of the item itself and tends to outperform other\ntechniques. However, collaborative filtering suffers from the cold-start\nproblem, which occurs when an item has not yet been rated or a user has not\nrated any items. Incorporating additional information, such as item or user\ndescriptions, into collaborative filtering can address the cold-start problem.\nIn this paper, we present a neural network model with latent input variables\n(latent neural network or LNN) as a hybrid collaborative filtering technique\nthat addresses the cold-start problem. LNN outperforms a broad selection of\ncontent-based filters (which make recommendations based on item descriptions)\nand other hybrid approaches while maintaining the accuracy of state-of-the-art\ncollaborative filtering techniques.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jun 2014 16:21:11 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Smith", "Michael R.", ""], ["Martinez", "Tony", ""], ["Gashler", "Michael", ""]]}, {"id": "1406.2237", "submitter": "Michael Smith", "authors": "Michael R. Smith, Tony Martinez", "title": "Reducing the Effects of Detrimental Instances", "comments": "6 pages, 5 tables, 2 figures. arXiv admin note: substantial text\n  overlap with arXiv:1403.1893", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Not all instances in a data set are equally beneficial for inducing a model\nof the data. Some instances (such as outliers or noise) can be detrimental.\nHowever, at least initially, the instances in a data set are generally\nconsidered equally in machine learning algorithms. Many current approaches for\nhandling noisy and detrimental instances make a binary decision about whether\nan instance is detrimental or not. In this paper, we 1) extend this paradigm by\nweighting the instances on a continuous scale and 2) present a methodology for\nmeasuring how detrimental an instance may be for inducing a model of the data.\nWe call our method of identifying and weighting detrimental instances reduced\ndetrimental instance learning (RDIL). We examine RIDL on a set of 54 data sets\nand 5 learning algorithms and compare RIDL with other weighting and filtering\napproaches. RDIL is especially useful for learning algorithms where every\ninstance can affect the classification boundary and the training instances are\nconsidered individually, such as multilayer perceptrons trained with\nbackpropagation (MLPs). Our results also suggest that a more accurate estimate\nof which instances are detrimental can have a significant positive impact for\nhandling them.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jun 2014 16:34:51 GMT"}, {"version": "v2", "created": "Tue, 14 Oct 2014 22:31:36 GMT"}], "update_date": "2014-10-16", "authors_parsed": [["Smith", "Michael R.", ""], ["Martinez", "Tony", ""]]}, {"id": "1406.2240", "submitter": "Martin Azizyan", "authors": "Larry Wasserman and Martin Azizyan and Aarti Singh", "title": "Feature Selection For High-Dimensional Clustering", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a nonparametric method for selecting informative features in\nhigh-dimensional clustering problems. We start with a screening step that uses\na test for multimodality. Then we apply kernel density estimation and mode\nclustering to the selected features. The output of the method consists of a\nlist of relevant features, and cluster assignments. We provide explicit bounds\non the error rate of the resulting clustering. In addition, we provide the\nfirst error bounds on mode based clustering.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jun 2014 16:57:51 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Wasserman", "Larry", ""], ["Azizyan", "Martin", ""], ["Singh", "Aarti", ""]]}, {"id": "1406.2395", "submitter": "Ines Dutra", "authors": "Ezilda Almeida, Pedro Ferreira, Tiago Vinhoza, In\\^es Dutra, Jingwei\n  Li, Yirong Wu, Elizabeth Burnside", "title": "ExpertBayes: Automatically refining manually built Bayesian networks", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian network structures are usually built using only the data and\nstarting from an empty network or from a naive Bayes structure. Very often, in\nsome domains, like medicine, a prior structure knowledge is already known. This\nstructure can be automatically or manually refined in search for better\nperformance models. In this work, we take Bayesian networks built by\nspecialists and show that minor perturbations to this original network can\nyield better classifiers with a very small computational cost, while\nmaintaining most of the intended meaning of the original model.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 00:50:05 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Almeida", "Ezilda", ""], ["Ferreira", "Pedro", ""], ["Vinhoza", "Tiago", ""], ["Dutra", "In\u00eas", ""], ["Li", "Jingwei", ""], ["Wu", "Yirong", ""], ["Burnside", "Elizabeth", ""]]}, {"id": "1406.2504", "submitter": "Bo Xin", "authors": "Bo Xin and David Wipf", "title": "Exploring Algorithmic Limits of Matrix Rank Minimization under Affine\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications require recovering a matrix of minimal rank within an\naffine constraint set, with matrix completion a notable special case. Because\nthe problem is NP-hard in general, it is common to replace the matrix rank with\nthe nuclear norm, which acts as a convenient convex surrogate. While elegant\ntheoretical conditions elucidate when this replacement is likely to be\nsuccessful, they are highly restrictive and convex algorithms fail when the\nambient rank is too high or when the constraint set is poorly structured.\nNon-convex alternatives fare somewhat better when carefully tuned; however,\nconvergence to locally optimal solutions remains a continuing source of\nfailure. Against this backdrop we derive a deceptively simple and\nparameter-free probabilistic PCA-like algorithm that is capable, over a wide\nbattery of empirical tests, of successful recovery even at the theoretical\nlimit where the number of measurements equal the degrees of freedom in the\nunknown low-rank matrix. Somewhat surprisingly, this is possible even when the\naffine constraint set is highly ill-conditioned. While proving general recovery\nguarantees remains evasive for non-convex algorithms, Bayesian-inspired or\notherwise, we nonetheless show conditions whereby the underlying cost function\nhas a unique stationary point located at the global optimum; no existing cost\nfunction we are aware of satisfies this same property. We conclude with a\nsimple computer vision application involving image rectification and a standard\ncollaborative filtering benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 10:53:20 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2015 07:24:02 GMT"}, {"version": "v3", "created": "Wed, 1 Jul 2015 19:05:56 GMT"}], "update_date": "2015-07-08", "authors_parsed": [["Xin", "Bo", ""], ["Wipf", "David", ""]]}, {"id": "1406.2541", "submitter": "Matthew W. Hoffman", "authors": "Jos\\'e Miguel Hern\\'andez-Lobato, Matthew W. Hoffman, Zoubin\n  Ghahramani", "title": "Predictive Entropy Search for Efficient Global Optimization of Black-box\n  Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel information-theoretic approach for Bayesian optimization\ncalled Predictive Entropy Search (PES). At each iteration, PES selects the next\nevaluation point that maximizes the expected information gained with respect to\nthe global maximum. PES codifies this intractable acquisition function in terms\nof the expected reduction in the differential entropy of the predictive\ndistribution. This reformulation allows PES to obtain approximations that are\nboth more accurate and efficient than other alternatives such as Entropy Search\n(ES). Furthermore, PES can easily perform a fully Bayesian treatment of the\nmodel hyperparameters while ES cannot. We evaluate PES in both synthetic and\nreal-world applications, including optimization problems in machine learning,\nfinance, biotechnology, and robotics. We show that the increased accuracy of\nPES leads to significant gains in optimization performance.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 13:29:09 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", ""], ["Hoffman", "Matthew W.", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1406.2572", "submitter": "KyungHyun Cho", "authors": "Yann Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya\n  Ganguli and Yoshua Bengio", "title": "Identifying and attacking the saddle point problem in high-dimensional\n  non-convex optimization", "comments": "The theoretical review and analysis in this article draw heavily from\n  arXiv:1405.4604 [cs.LG]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central challenge to many fields of science and engineering involves\nminimizing non-convex error functions over continuous, high dimensional spaces.\nGradient descent or quasi-Newton methods are almost ubiquitously used to\nperform such minimizations, and it is often thought that a main source of\ndifficulty for these local methods to find the global minimum is the\nproliferation of local minima with much higher error than the global minimum.\nHere we argue, based on results from statistical physics, random matrix theory,\nneural network theory, and empirical evidence, that a deeper and more profound\ndifficulty originates from the proliferation of saddle points, not local\nminima, especially in high dimensional problems of practical interest. Such\nsaddle points are surrounded by high error plateaus that can dramatically slow\ndown learning, and give the illusory impression of the existence of a local\nminimum. Motivated by these arguments, we propose a new approach to\nsecond-order optimization, the saddle-free Newton method, that can rapidly\nescape high dimensional saddle points, unlike gradient descent and quasi-Newton\nmethods. We apply this algorithm to deep or recurrent neural network training,\nand provide numerical evidence for its superior optimization performance.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 14:52:14 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Dauphin", "Yann", ""], ["Pascanu", "Razvan", ""], ["Gulcehre", "Caglar", ""], ["Cho", "Kyunghyun", ""], ["Ganguli", "Surya", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1406.2582", "submitter": "Michael Schober", "authors": "Michael Schober, David Duvenaud, Philipp Hennig", "title": "Probabilistic ODE Solvers with Runge-Kutta Means", "comments": "18 pages (9 page conference paper, plus supplements); appears in\n  Advances in Neural Information Processing Systems (NIPS), 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Runge-Kutta methods are the classic family of solvers for ordinary\ndifferential equations (ODEs), and the basis for the state of the art. Like\nmost numerical methods, they return point estimates. We construct a family of\nprobabilistic numerical methods that instead return a Gauss-Markov process\ndefining a probability distribution over the ODE solution. In contrast to prior\nwork, we construct this family such that posterior means match the outputs of\nthe Runge-Kutta family exactly, thus inheriting their proven good properties.\nRemaining degrees of freedom not identified by the match to Runge-Kutta are\nchosen such that the posterior probability measure fits the observed structure\nof the ODE. Our results shed light on the structure of Runge-Kutta solvers from\na new direction, provide a richer, probabilistic output, have low computational\ncost, and raise new research questions.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 15:13:24 GMT"}, {"version": "v2", "created": "Fri, 24 Oct 2014 11:45:49 GMT"}], "update_date": "2014-10-27", "authors_parsed": [["Schober", "Michael", ""], ["Duvenaud", "David", ""], ["Hennig", "Philipp", ""]]}, {"id": "1406.2602", "submitter": "Ethan Fetaya", "authors": "Ethan Fetaya, Ohad Shamir and Shimon Ullman", "title": "Graph Approximation and Clustering on a Budget", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning from a similarity matrix (such as\nspectral clustering and lowd imensional embedding), when computing pairwise\nsimilarities are costly, and only a limited number of entries can be observed.\nWe provide a theoretical analysis using standard notions of graph\napproximation, significantly generalizing previous results (which focused on\nspectral clustering with two clusters). We also propose a new algorithmic\napproach based on adaptive sampling, which experimentally matches or improves\non previous methods, while being considerably more general and computationally\ncheaper.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 15:49:05 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Fetaya", "Ethan", ""], ["Shamir", "Ohad", ""], ["Ullman", "Shimon", ""]]}, {"id": "1406.2622", "submitter": "Julien Audiffren", "authors": "Julien Audiffren (CMLA), Hachem Kadri (LIF)", "title": "Equivalence of Learning Algorithms", "comments": "arXiv admin note: substantial text overlap with arXiv:1310.2451", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is to introduce a concept of equivalence between\nmachine learning algorithms. We define two notions of algorithmic equivalence,\nnamely, weak and strong equivalence. These notions are of paramount importance\nfor identifying when learning prop erties from one learning algorithm can be\ntransferred to another. Using regularized kernel machines as a case study, we\nillustrate the importance of the introduced equivalence concept by analyzing\nthe relation between kernel ridge regression (KRR) and m-power regularized\nleast squares regression (M-RLSR) algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 16:40:56 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Audiffren", "Julien", "", "CMLA"], ["Kadri", "Hachem", "", "LIF"]]}, {"id": "1406.2646", "submitter": "Franz J. Kir\\'aly", "authors": "Franz J Kir\\'aly, Martin Kreuzer, Louis Theran", "title": "Learning with Cross-Kernels and Ideal PCA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.AC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe how cross-kernel matrices, that is, kernel matrices between the\ndata and a custom chosen set of `feature spanning points' can be used for\nlearning. The main potential of cross-kernels lies in the fact that (a) only\none side of the matrix scales with the number of data points, and (b)\ncross-kernels, as opposed to the usual kernel matrices, can be used to certify\nfor the data manifold. Our theoretical framework, which is based on a duality\ninvolving the feature space and vanishing ideals, indicates that cross-kernels\nhave the potential to be used for any kind of kernel learning. We present a\nnovel algorithm, Ideal PCA (IPCA), which cross-kernelizes PCA. We demonstrate\non real and synthetic data that IPCA allows to (a) obtain PCA-like features\nfaster and (b) to extract novel and empirically validated features certifying\nfor the data manifold.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 17:48:58 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Kir\u00e1ly", "Franz J", ""], ["Kreuzer", "Martin", ""], ["Theran", "Louis", ""]]}, {"id": "1406.2661", "submitter": "Ian Goodfellow", "authors": "Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David\n  Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio", "title": "Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework for estimating generative models via an\nadversarial process, in which we simultaneously train two models: a generative\nmodel G that captures the data distribution, and a discriminative model D that\nestimates the probability that a sample came from the training data rather than\nG. The training procedure for G is to maximize the probability of D making a\nmistake. This framework corresponds to a minimax two-player game. In the space\nof arbitrary functions G and D, a unique solution exists, with G recovering the\ntraining data distribution and D equal to 1/2 everywhere. In the case where G\nand D are defined by multilayer perceptrons, the entire system can be trained\nwith backpropagation. There is no need for any Markov chains or unrolled\napproximate inference networks during either training or generation of samples.\nExperiments demonstrate the potential of the framework through qualitative and\nquantitative evaluation of the generated samples.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 18:58:17 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Goodfellow", "Ian J.", ""], ["Pouget-Abadie", "Jean", ""], ["Mirza", "Mehdi", ""], ["Xu", "Bing", ""], ["Warde-Farley", "David", ""], ["Ozair", "Sherjil", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1406.2673", "submitter": "Balaji Lakshminarayanan", "authors": "Balaji Lakshminarayanan, Daniel M. Roy and Yee Whye Teh", "title": "Mondrian Forests: Efficient Online Random Forests", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems 27 (NIPS), pages\n  3140-3148, 2014", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensembles of randomized decision trees, usually referred to as random\nforests, are widely used for classification and regression tasks in machine\nlearning and statistics. Random forests achieve competitive predictive\nperformance and are computationally efficient to train and test, making them\nexcellent candidates for real-world prediction tasks. The most popular random\nforest variants (such as Breiman's random forest and extremely randomized\ntrees) operate on batches of training data. Online methods are now in greater\ndemand. Existing online random forests, however, require more training data\nthan their batch counterpart to achieve comparable predictive performance. In\nthis work, we use Mondrian processes (Roy and Teh, 2009) to construct ensembles\nof random decision trees we call Mondrian forests. Mondrian forests can be\ngrown in an incremental/online fashion and remarkably, the distribution of\nonline Mondrian forests is the same as that of batch Mondrian forests. Mondrian\nforests achieve competitive predictive performance comparable with existing\nonline random forests and periodically re-trained batch random forests, while\nbeing more than an order of magnitude faster, thus representing a better\ncomputation vs accuracy tradeoff.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 19:34:51 GMT"}, {"version": "v2", "created": "Mon, 16 Feb 2015 14:57:52 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Lakshminarayanan", "Balaji", ""], ["Roy", "Daniel M.", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1406.2721", "submitter": "Zhaoshi Meng", "authors": "Zhaoshi Meng, Brian Eriksson, Alfred O. Hero III", "title": "Learning Latent Variable Gaussian Graphical Models", "comments": "To appear in The 31st International Conference on Machine Learning\n  (ICML 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian graphical models (GGM) have been widely used in many\nhigh-dimensional applications ranging from biological and financial data to\nrecommender systems. Sparsity in GGM plays a central role both statistically\nand computationally. Unfortunately, real-world data often does not fit well to\nsparse graphical models. In this paper, we focus on a family of latent variable\nGaussian graphical models (LVGGM), where the model is conditionally sparse\ngiven latent variables, but marginally non-sparse. In LVGGM, the inverse\ncovariance matrix has a low-rank plus sparse structure, and can be learned in a\nregularized maximum likelihood framework. We derive novel parameter estimation\nerror bounds for LVGGM under mild conditions in the high-dimensional setting.\nThese results complement the existing theory on the structural learning, and\nopen up new possibilities of using LVGGM for statistical inference.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 21:03:22 GMT"}], "update_date": "2014-06-12", "authors_parsed": [["Meng", "Zhaoshi", ""], ["Eriksson", "Brian", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1406.2784", "submitter": "Sewoong Oh", "authors": "Prateek Jain and Sewoong Oh", "title": "Provable Tensor Factorization with Missing Data", "comments": "26 pages 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of low-rank tensor factorization in the presence of\nmissing data. We ask the following question: how many sampled entries do we\nneed, to efficiently and exactly reconstruct a tensor with a low-rank\northogonal decomposition? We propose a novel alternating minimization based\nmethod which iteratively refines estimates of the singular vectors. We show\nthat under certain standard assumptions, our method can recover a three-mode\n$n\\times n\\times n$ dimensional rank-$r$ tensor exactly from $O(n^{3/2} r^5\n\\log^4 n)$ randomly sampled entries. In the process of proving this result, we\nsolve two challenging sub-problems for tensors with missing data. First, in the\nprocess of analyzing the initialization step, we prove a generalization of a\ncelebrated result by Szemer\\'edie et al. on the spectrum of random graphs.\nNext, we prove global convergence of alternating minimization with a good\ninitialization. Simulations suggest that the dependence of the sample size on\ndimensionality $n$ is indeed tight.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jun 2014 05:51:54 GMT"}], "update_date": "2014-06-12", "authors_parsed": [["Jain", "Prateek", ""], ["Oh", "Sewoong", ""]]}, {"id": "1406.2839", "submitter": "Nicolas Chopin", "authors": "Simon Barthelm\\'e and Nicolas Chopin", "title": "The Poisson transform for unnormalised statistical models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrary to standard statistical models, unnormalised statistical models only\nspecify the likelihood function up to a constant. While such models are natural\nand popular, the lack of normalisation makes inference much more difficult.\nHere we show that inferring the parameters of a unnormalised model on a space\n$\\Omega$ can be mapped onto an equivalent problem of estimating the intensity\nof a Poisson point process on $\\Omega$. The unnormalised statistical model now\nspecifies an intensity function that does not need to be normalised.\nEffectively, the normalisation constant may now be inferred as just another\nparameter, at no loss of information. The result can be extended to cover\nnon-IID models, which includes for example unnormalised models for sequences of\ngraphs (dynamical graphs), or for sequences of binary vectors. As a\nconsequence, we prove that unnormalised parameteric inference in non-IID models\ncan be turned into a semi-parametric estimation problem. Moreover, we show that\nthe noise-contrastive divergence of Gutmann & Hyv\\\"arinen (2012) can be\nunderstood as an approximation of the Poisson transform, and extended to\nnon-IID settings. We use our results to fit spatial Markov chain models of eye\nmovements, where the Poisson transform allows us to turn a highly non-standard\nmodel into vanilla semi-parametric logistic regression.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jun 2014 09:23:21 GMT"}, {"version": "v2", "created": "Thu, 27 Nov 2014 15:05:57 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Barthelm\u00e9", "Simon", ""], ["Chopin", "Nicolas", ""]]}, {"id": "1406.2864", "submitter": "Duncan Blythe", "authors": "Duncan A.J. Blythe, Louis Theran, Franz Kiraly", "title": "Algebraic-Combinatorial Methods for Low-Rank Matrix Completion with\n  Application to Athletic Performance Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents novel algorithms which exploit the intrinsic algebraic\nand combinatorial structure of the matrix completion task for estimating\nmissing en- tries in the general low rank setting. For positive data, we\nachieve results out- performing the state of the art nuclear norm, both in\naccuracy and computational efficiency, in simulations and in the task of\npredicting athletic performance from partially observed data.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jun 2014 11:38:34 GMT"}], "update_date": "2014-06-12", "authors_parsed": [["Blythe", "Duncan A. J.", ""], ["Theran", "Louis", ""], ["Kiraly", "Franz", ""]]}, {"id": "1406.2969", "submitter": "Yilun Wang", "authors": "Yilun Wang and Xinhua Su", "title": "Truncated Nuclear Norm Minimization for Image Restoration Based On\n  Iterative Support Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering a large matrix from limited measurements is a challenging task\narising in many real applications, such as image inpainting, compressive\nsensing and medical imaging, and this kind of problems are mostly formulated as\nlow-rank matrix approximation problems. Due to the rank operator being\nnon-convex and discontinuous, most of the recent theoretical studies use the\nnuclear norm as a convex relaxation and the low-rank matrix recovery problem is\nsolved through minimization of the nuclear norm regularized problem. However, a\nmajor limitation of nuclear norm minimization is that all the singular values\nare simultaneously minimized and the rank may not be well approximated\n\\cite{hu2012fast}. Correspondingly, in this paper, we propose a new multi-stage\nalgorithm, which makes use of the concept of Truncated Nuclear Norm\nRegularization (TNNR) proposed in \\citep{hu2012fast} and Iterative Support\nDetection (ISD) proposed in \\citep{wang2010sparse} to overcome the above\nlimitation. Besides matrix completion problems considered in\n\\citep{hu2012fast}, the proposed method can be also extended to the general\nlow-rank matrix recovery problems. Extensive experiments well validate the\nsuperiority of our new algorithms over other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jun 2014 17:18:25 GMT"}], "update_date": "2014-06-12", "authors_parsed": [["Wang", "Yilun", ""], ["Su", "Xinhua", ""]]}, {"id": "1406.2989", "submitter": "Mathias Berglund", "authors": "Tapani Raiko, Mathias Berglund, Guillaume Alain, Laurent Dinh", "title": "Techniques for Learning Binary Stochastic Feedforward Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic binary hidden units in a multi-layer perceptron (MLP) network give\nat least three potential benefits when compared to deterministic MLP networks.\n(1) They allow to learn one-to-many type of mappings. (2) They can be used in\nstructured prediction problems, where modeling the internal structure of the\noutput is important. (3) Stochasticity has been shown to be an excellent\nregularizer, which makes generalization performance potentially better in\ngeneral. However, training stochastic networks is considerably more difficult.\nWe study training using M samples of hidden activations per input. We show that\nthe case M=1 leads to a fundamentally different behavior where the network\ntries to avoid stochasticity. We propose two new estimators for the training\ngradient and propose benchmark tests for comparing training algorithms. Our\nexperiments confirm that training stochastic networks is difficult and show\nthat the proposed two estimators perform favorably among all the five known\nestimators.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jun 2014 18:29:27 GMT"}, {"version": "v2", "created": "Fri, 19 Dec 2014 13:37:05 GMT"}, {"version": "v3", "created": "Thu, 9 Apr 2015 12:58:06 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["Raiko", "Tapani", ""], ["Berglund", "Mathias", ""], ["Alain", "Guillaume", ""], ["Dinh", "Laurent", ""]]}, {"id": "1406.3070", "submitter": "Misha Denil", "authors": "Yariv Dror Mizrahi and Misha Denil and Nando de Freitas", "title": "Distributed Parameter Estimation in Probabilistic Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents foundational theoretical results on distributed parameter\nestimation for undirected probabilistic graphical models. It introduces a\ngeneral condition on composite likelihood decompositions of these models which\nguarantees the global consistency of distributed estimators, provided the local\nestimators are consistent.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jun 2014 21:35:41 GMT"}], "update_date": "2014-06-13", "authors_parsed": [["Mizrahi", "Yariv Dror", ""], ["Denil", "Misha", ""], ["de Freitas", "Nando", ""]]}, {"id": "1406.3100", "submitter": "Jonathan Tapson", "authors": "Philip de Chazal, Jonathan Tapson and Andr\\'e van Schaik", "title": "Learning ELM network weights using linear discriminant analysis", "comments": "In submission to the ELM 2014 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an alternative to the pseudo-inverse method for determining the\nhidden to output weight values for Extreme Learning Machines performing\nclassification tasks. The method is based on linear discriminant analysis and\nprovides Bayes optimal single point estimates for the weight values.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jun 2014 02:08:31 GMT"}], "update_date": "2014-06-13", "authors_parsed": [["de Chazal", "Philip", ""], ["Tapson", "Jonathan", ""], ["van Schaik", "Andr\u00e9", ""]]}, {"id": "1406.3140", "submitter": "Guido F.  Montufar", "authors": "Guido Montufar, Johannes Rauh, Nihat Ay", "title": "Expressive Power and Approximation Errors of Restricted Boltzmann\n  Machines", "comments": "9 pages, 3 figures, plus 1 page, 1 figure appendix, minor corrections\n  of the first publication", "journal-ref": "Advances in Neural Information Processing Systems 24, pages\n  415-423, 2011", "doi": null, "report-no": null, "categories": "stat.ML math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present explicit classes of probability distributions that can be learned\nby Restricted Boltzmann Machines (RBMs) depending on the number of units that\nthey contain, and which are representative for the expressive power of the\nmodel. We use this to show that the maximal Kullback-Leibler divergence to the\nRBM model with $n$ visible and $m$ hidden units is bounded from above by $n -\n\\left\\lfloor \\log(m+1) \\right\\rfloor -\n\\frac{m+1}{2^{\\left\\lfloor\\log(m+1)\\right\\rfloor}} \\approx (n -1) - \\log(m+1)$.\nIn this way we can specify the number of hidden units that guarantees a\nsufficiently rich model containing different classes of distributions and\nrespecting a given error tolerance.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jun 2014 08:11:19 GMT"}], "update_date": "2014-06-13", "authors_parsed": [["Montufar", "Guido", ""], ["Rauh", "Johannes", ""], ["Ay", "Nihat", ""]]}, {"id": "1406.3166", "submitter": "Nguyen Viet Cuong", "authors": "Nguyen Viet Cuong, Lam Si Tung Ho, Vu Dinh", "title": "Generalization and Robustness of Batched Weighted Average Algorithm with\n  V-geometrically Ergodic Markov Data", "comments": "This article was published in Proceedings of the 24th International\n  Conference on Algorithmic Learning Theory (ALT 2013). This is the accepted\n  version. The final publication is available at link.springer.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the generalization and robustness of the batched weighted average\nalgorithm for V-geometrically ergodic Markov data. This algorithm is a good\nalternative to the empirical risk minimization algorithm when the latter\nsuffers from overfitting or when optimizing the empirical risk is hard. For the\ngeneralization of the algorithm, we prove a PAC-style bound on the training\nsample size for the expected $L_1$-loss to converge to the optimal loss when\ntraining data are V-geometrically ergodic Markov chains. For the robustness, we\nshow that if the training target variable's values contain bounded noise, then\nthe generalization bound of the algorithm deviates at most by the range of the\nnoise. Our results can be applied to the regression problem, the classification\nproblem, and the case where there exists an unknown deterministic target\nhypothesis.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jun 2014 09:37:25 GMT"}, {"version": "v2", "created": "Tue, 12 Aug 2014 14:11:13 GMT"}], "update_date": "2014-08-13", "authors_parsed": [["Cuong", "Nguyen Viet", ""], ["Ho", "Lam Si Tung", ""], ["Dinh", "Vu", ""]]}, {"id": "1406.3175", "submitter": "Gabriel Krummenacher", "authors": "Brian McWilliams, Gabriel Krummenacher, Mario Lucic, Joachim M.\n  Buhmann", "title": "Fast and Robust Least Squares Estimation in Corrupted Linear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subsampling methods have been recently proposed to speed up least squares\nestimation in large scale settings. However, these algorithms are typically not\nrobust to outliers or corruptions in the observed covariates.\n  The concept of influence that was developed for regression diagnostics can be\nused to detect such corrupted observations as shown in this paper. This\nproperty of influence -- for which we also develop a randomized approximation\n-- motivates our proposed subsampling algorithm for large scale corrupted\nlinear regression which limits the influence of data points since highly\ninfluential points contribute most to the residual error. Under a general model\nof corrupted observations, we show theoretically and empirically on a variety\nof simulated and real datasets that our algorithm improves over the current\nstate-of-the-art approximation schemes for ordinary least squares.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jun 2014 09:55:19 GMT"}, {"version": "v2", "created": "Thu, 19 Jun 2014 12:58:52 GMT"}], "update_date": "2014-06-20", "authors_parsed": [["McWilliams", "Brian", ""], ["Krummenacher", "Gabriel", ""], ["Lucic", "Mario", ""], ["Buhmann", "Joachim M.", ""]]}, {"id": "1406.3190", "submitter": "Jie Shen", "authors": "Jie Shen and Huan Xu and Ping Li", "title": "Online Optimization for Large-Scale Max-Norm Regularization", "comments": "A conference version appears in NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Max-norm regularizer has been extensively studied in the last decade as it\npromotes an effective low-rank estimation for the underlying data. However,\nsuch max-norm regularized problems are typically formulated and solved in a\nbatch manner, which prevents it from processing big data due to possible memory\nbudget. In this paper, hence, we propose an online algorithm that is scalable\nto large-scale setting. Particularly, we consider the matrix decomposition\nproblem as an example, although a simple variant of the algorithm and analysis\ncan be adapted to other important problems such as matrix completion. The\ncrucial technique in our implementation is to reformulating the max-norm to an\nequivalent matrix factorization form, where the factors consist of a (possibly\novercomplete) basis component and a coefficients one. In this way, we may\nmaintain the basis component in the memory and optimize over it and the\ncoefficients for each sample alternatively. Since the memory footprint of the\nbasis component is independent of the sample size, our algorithm is appealing\nwhen manipulating a large collection of samples. We prove that the sequence of\nthe solutions (i.e., the basis component) produced by our algorithm converges\nto a stationary point of the expected loss function asymptotically. Numerical\nstudy demonstrates encouraging results for the efficacy and robustness of our\nalgorithm compared to the widely used nuclear norm solvers.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jun 2014 10:49:50 GMT"}, {"version": "v2", "created": "Sun, 14 Dec 2014 02:29:08 GMT"}, {"version": "v3", "created": "Thu, 7 May 2015 03:59:34 GMT"}, {"version": "v4", "created": "Sat, 14 May 2016 03:53:59 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Shen", "Jie", ""], ["Xu", "Huan", ""], ["Li", "Ping", ""]]}, {"id": "1406.3269", "submitter": "Krzysztof Geras", "authors": "Krzysztof J. Geras and Charles Sutton", "title": "Scheduled denoising autoencoders", "comments": "Published as a conference paper at ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a representation learning method that learns features at multiple\ndifferent levels of scale. Working within the unsupervised framework of\ndenoising autoencoders, we observe that when the input is heavily corrupted\nduring training, the network tends to learn coarse-grained features, whereas\nwhen the input is only slightly corrupted, the network tends to learn\nfine-grained features. This motivates the scheduled denoising autoencoder,\nwhich starts with a high level of noise that lowers as training progresses. We\nfind that the resulting representation yields a significant boost on a later\nsupervised task compared to the original input, or to a standard denoising\nautoencoder trained at a single noise level. After supervised fine-tuning our\nbest model achieves the lowest ever reported error on the CIFAR-10 data set\namong permutation-invariant methods.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jun 2014 15:40:18 GMT"}, {"version": "v2", "created": "Fri, 19 Dec 2014 20:42:11 GMT"}, {"version": "v3", "created": "Fri, 10 Apr 2015 21:05:32 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Geras", "Krzysztof J.", ""], ["Sutton", "Charles", ""]]}, {"id": "1406.3332", "submitter": "Julien Mairal", "authors": "Julien Mairal (INRIA Grenoble Rh\\^one-Alpes / LJK Laboratoire Jean\n  Kuntzmann), Piotr Koniusz (INRIA Grenoble Rh\\^one-Alpes / LJK Laboratoire\n  Jean Kuntzmann), Zaid Harchaoui (INRIA Grenoble Rh\\^one-Alpes / LJK\n  Laboratoire Jean Kuntzmann), Cordelia Schmid (INRIA Grenoble Rh\\^one-Alpes /\n  LJK Laboratoire Jean Kuntzmann)", "title": "Convolutional Kernel Networks", "comments": "appears in Advances in Neural Information Processing Systems (NIPS),\n  Dec 2014, Montreal, Canada, http://nips.cc", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important goal in visual recognition is to devise image representations\nthat are invariant to particular transformations. In this paper, we address\nthis goal with a new type of convolutional neural network (CNN) whose\ninvariance is encoded by a reproducing kernel. Unlike traditional approaches\nwhere neural networks are learned either to represent data or for solving a\nclassification task, our network learns to approximate the kernel feature map\non training data. Such an approach enjoys several benefits over classical ones.\nFirst, by teaching CNNs to be invariant, we obtain simple network architectures\nthat achieve a similar accuracy to more complex ones, while being easy to train\nand robust to overfitting. Second, we bridge a gap between the neural network\nliterature and kernels, which are natural tools to model invariance. We\nevaluate our methodology on visual recognition tasks where CNNs have proven to\nperform well, e.g., digit recognition with the MNIST dataset, and the more\nchallenging CIFAR-10 and STL-10 datasets, where our accuracy is competitive\nwith the state of the art.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jun 2014 19:41:03 GMT"}, {"version": "v2", "created": "Fri, 14 Nov 2014 16:58:48 GMT"}], "update_date": "2015-01-08", "authors_parsed": [["Mairal", "Julien", "", "INRIA Grenoble Rh\u00f4ne-Alpes / LJK Laboratoire Jean\n  Kuntzmann"], ["Koniusz", "Piotr", "", "INRIA Grenoble Rh\u00f4ne-Alpes / LJK Laboratoire\n  Jean Kuntzmann"], ["Harchaoui", "Zaid", "", "INRIA Grenoble Rh\u00f4ne-Alpes / LJK\n  Laboratoire Jean Kuntzmann"], ["Schmid", "Cordelia", "", "INRIA Grenoble Rh\u00f4ne-Alpes /\n  LJK Laboratoire Jean Kuntzmann"]]}, {"id": "1406.3469", "submitter": "Brian McWilliams", "authors": "Christina Heinze, Brian McWilliams, Nicolai Meinshausen, Gabriel\n  Krummenacher", "title": "LOCO: Distributing Ridge Regression with Random Projections", "comments": "37 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose LOCO, an algorithm for large-scale ridge regression which\ndistributes the features across workers on a cluster. Important dependencies\nbetween variables are preserved using structured random projections which are\ncheap to compute and must only be communicated once. We show that LOCO obtains\na solution which is close to the exact ridge regression solution in the fixed\ndesign setting. We verify this experimentally in a simulation study as well as\nan application to climate prediction. Furthermore, we show that LOCO achieves\nsignificant speedups compared with a state-of-the-art distributed algorithm on\na large-scale regression problem.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jun 2014 09:59:21 GMT"}, {"version": "v2", "created": "Thu, 26 Jun 2014 08:19:55 GMT"}, {"version": "v3", "created": "Fri, 5 Jun 2015 19:41:32 GMT"}, {"version": "v4", "created": "Mon, 8 Jun 2015 07:17:46 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Heinze", "Christina", ""], ["McWilliams", "Brian", ""], ["Meinshausen", "Nicolai", ""], ["Krummenacher", "Gabriel", ""]]}, {"id": "1406.3650", "submitter": "Stephan Mandt", "authors": "Stephan Mandt and David Blei", "title": "Smoothed Gradients for Stochastic Variational Inference", "comments": "Appears in Neural Information Processing Systems, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic variational inference (SVI) lets us scale up Bayesian computation\nto massive data. It uses stochastic optimization to fit a variational\ndistribution, following easy-to-compute noisy natural gradients. As with most\ntraditional stochastic optimization methods, SVI takes precautions to use\nunbiased stochastic gradients whose expectations are equal to the true\ngradients. In this paper, we explore the idea of following biased stochastic\ngradients in SVI. Our method replaces the natural gradient with a similarly\nconstructed vector that uses a fixed-window moving average of some of its\nprevious terms. We will demonstrate the many advantages of this technique.\nFirst, its computational cost is the same as for SVI and storage requirements\nonly multiply by a constant factor. Second, it enjoys significant variance\nreduction over the unbiased estimates, smaller bias than averaged gradients,\nand leads to smaller mean-squared error against the full gradient. We test our\nmethod on latent Dirichlet allocation with three large corpora.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jun 2014 21:19:09 GMT"}, {"version": "v2", "created": "Tue, 18 Nov 2014 03:12:37 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Mandt", "Stephan", ""], ["Blei", "David", ""]]}, {"id": "1406.3711", "submitter": "Diego Vidaurre", "authors": "Diego Vidaurre, Iead Rezek, Samuel L. Harrison, Stephen S. Smith and\n  Mark Woolrich", "title": "Dimensionality reduction for time series data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the fact that they do not consider the temporal nature of data,\nclassic dimensionality reduction techniques, such as PCA, are widely applied to\ntime series data. In this paper, we introduce a factor decomposition specific\nfor time series that builds upon the Bayesian multivariate autoregressive model\nand hence evades the assumption that data points are mutually independent. The\nkey is to find a low-rank estimation of the autoregressive matrices. As in the\nprobabilistic version of other factor models, this induces a latent\nlow-dimensional representation of the original data. We discuss some possible\ngeneralisations and alternatives, with the most relevant being a technique for\nsimultaneous smoothing and dimensionality reduction. To illustrate the\npotential applications, we apply the model on a synthetic data set and\ndifferent types of neuroimaging data (EEG and ECoG).\n", "versions": [{"version": "v1", "created": "Sat, 14 Jun 2014 10:21:03 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Vidaurre", "Diego", ""], ["Rezek", "Iead", ""], ["Harrison", "Samuel L.", ""], ["Smith", "Stephen S.", ""], ["Woolrich", "Mark", ""]]}, {"id": "1406.3781", "submitter": "Nishant Mehta", "authors": "Nishant A. Mehta and Robert C. Williamson", "title": "From Stochastic Mixability to Fast Rates", "comments": "21 pages, accepted to NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical risk minimization (ERM) is a fundamental learning rule for\nstatistical learning problems where the data is generated according to some\nunknown distribution $\\mathsf{P}$ and returns a hypothesis $f$ chosen from a\nfixed class $\\mathcal{F}$ with small loss $\\ell$. In the parametric setting,\ndepending upon $(\\ell, \\mathcal{F},\\mathsf{P})$ ERM can have slow\n$(1/\\sqrt{n})$ or fast $(1/n)$ rates of convergence of the excess risk as a\nfunction of the sample size $n$. There exist several results that give\nsufficient conditions for fast rates in terms of joint properties of $\\ell$,\n$\\mathcal{F}$, and $\\mathsf{P}$, such as the margin condition and the Bernstein\ncondition. In the non-statistical prediction with expert advice setting, there\nis an analogous slow and fast rate phenomenon, and it is entirely characterized\nin terms of the mixability of the loss $\\ell$ (there being no role there for\n$\\mathcal{F}$ or $\\mathsf{P}$). The notion of stochastic mixability builds a\nbridge between these two models of learning, reducing to classical mixability\nin a special case. The present paper presents a direct proof of fast rates for\nERM in terms of stochastic mixability of $(\\ell,\\mathcal{F}, \\mathsf{P})$, and\nin so doing provides new insight into the fast-rates phenomenon. The proof\nexploits an old result of Kemperman on the solution to the general moment\nproblem. We also show a partial converse that suggests a characterization of\nfast rates for ERM in terms of stochastic mixability is possible.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jun 2014 23:25:05 GMT"}, {"version": "v2", "created": "Sat, 22 Nov 2014 11:16:28 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Mehta", "Nishant A.", ""], ["Williamson", "Robert C.", ""]]}, {"id": "1406.3816", "submitter": "Francesco Orabona", "authors": "Francesco Orabona", "title": "Simultaneous Model Selection and Optimization through Parameter-free\n  Stochastic Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent algorithms for training linear and kernel\npredictors are gaining more and more importance, thanks to their scalability.\nWhile various methods have been proposed to speed up their convergence, the\nmodel selection phase is often ignored. In fact, in theoretical works most of\nthe time assumptions are made, for example, on the prior knowledge of the norm\nof the optimal solution, while in the practical world validation methods remain\nthe only viable approach. In this paper, we propose a new kernel-based\nstochastic gradient descent algorithm that performs model selection while\ntraining, with no parameters to tune, nor any form of cross-validation. The\nalgorithm builds on recent advancement in online learning theory for\nunconstrained settings, to estimate over time the right regularization in a\ndata-dependent way. Optimal rates of convergence are proved under standard\nsmoothness assumptions on the target function, using the range space of the\nfractional integral operator associated with the kernel.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jun 2014 13:34:27 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Orabona", "Francesco", ""]]}, {"id": "1406.3824", "submitter": "Yuchen Zhang", "authors": "Yuchen Zhang, Xi Chen, Dengyong Zhou, Michael I. Jordan", "title": "Spectral Methods meet EM: A Provably Optimal Algorithm for Crowdsourcing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing is a popular paradigm for effectively collecting labels at low\ncost. The Dawid-Skene estimator has been widely used for inferring the true\nlabels from the noisy labels provided by non-expert crowdsourcing workers.\nHowever, since the estimator maximizes a non-convex log-likelihood function, it\nis hard to theoretically justify its performance. In this paper, we propose a\ntwo-stage efficient algorithm for multi-class crowd labeling problems. The\nfirst stage uses the spectral method to obtain an initial estimate of\nparameters. Then the second stage refines the estimation by optimizing the\nobjective function of the Dawid-Skene estimator via the EM algorithm. We show\nthat our algorithm achieves the optimal convergence rate up to a logarithmic\nfactor. We conduct extensive experiments on synthetic and real datasets.\nExperimental results demonstrate that the proposed algorithm is comparable to\nthe most accurate empirical approach, while outperforming several other\nrecently proposed methods.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jun 2014 15:00:17 GMT"}, {"version": "v2", "created": "Wed, 18 Jun 2014 15:49:56 GMT"}, {"version": "v3", "created": "Sat, 1 Nov 2014 22:43:51 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Zhang", "Yuchen", ""], ["Chen", "Xi", ""], ["Zhou", "Dengyong", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1406.3830", "submitter": "Misha Denil", "authors": "Misha Denil and Alban Demiraj and Nal Kalchbrenner and Phil Blunsom\n  and Nando de Freitas", "title": "Modelling, Visualising and Summarising Documents with a Single\n  Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capturing the compositional process which maps the meaning of words to that\nof documents is a central challenge for researchers in Natural Language\nProcessing and Information Retrieval. We introduce a model that is able to\nrepresent the meaning of documents by embedding them in a low dimensional\nvector space, while preserving distinctions of word and sentence order crucial\nfor capturing nuanced semantics. Our model is based on an extended Dynamic\nConvolution Neural Network, which learns convolution filters at both the\nsentence and document level, hierarchically learning to capture and compose low\nlevel lexical features into high level semantic concepts. We demonstrate the\neffectiveness of this model on a range of document modelling tasks, achieving\nstrong results with no feature engineering and with a more compact model.\nInspired by recent advances in visualising deep convolution networks for\ncomputer vision, we present a novel visualisation technique for our document\nnetworks which not only provides insight into their learning process, but also\ncan be interpreted to produce a compelling automatic summarisation system for\ntexts.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jun 2014 17:15:32 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Denil", "Misha", ""], ["Demiraj", "Alban", ""], ["Kalchbrenner", "Nal", ""], ["Blunsom", "Phil", ""], ["de Freitas", "Nando", ""]]}, {"id": "1406.3837", "submitter": "Thomas Laurent", "authors": "Xavier Bresson, Huiyi Hu, Thomas Laurent, Arthur Szlam, and James von\n  Brecht", "title": "An Incremental Reseeding Strategy for Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a simple and easily parallelizable algorithm for\nmultiway graph partitioning. The algorithm alternates between three basic\ncomponents: diffusing seed vertices over the graph, thresholding the diffused\nseeds, and then randomly reseeding the thresholded clusters. We demonstrate\nexperimentally that the proper combination of these ingredients leads to an\nalgorithm that achieves state-of-the-art performance in terms of cluster purity\non standard benchmarks datasets. Moreover, the algorithm runs an order of\nmagnitude faster than the other algorithms that achieve comparable results in\nterms of accuracy. We also describe a coarsen, cluster and refine approach\nsimilar to GRACLUS and METIS that removes an additional order of magnitude from\nthe runtime of our algorithm while still maintaining competitive accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jun 2014 18:30:51 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Bresson", "Xavier", ""], ["Hu", "Huiyi", ""], ["Laurent", "Thomas", ""], ["Szlam", "Arthur", ""], ["von Brecht", "James", ""]]}, {"id": "1406.3852", "submitter": "Matthew Blaschko", "authors": "Wacha Bounliphone, Arthur Gretton, Arthur Tenenhaus (E3S), Matthew\n  Blaschko (INRIA Saclay - Ile de France, CVN)", "title": "A low variance consistent test of relative dependency", "comments": "International Conference on Machine Learning, Jul 2015, Lille, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a novel non-parametric statistical hypothesis test of relative\ndependence between a source variable and two candidate target variables. Such a\ntest enables us to determine whether one source variable is significantly more\ndependent on a first target variable or a second. Dependence is measured via\nthe Hilbert-Schmidt Independence Criterion (HSIC), resulting in a pair of\nempirical dependence measures (source-target 1, source-target 2). We test\nwhether the first dependence measure is significantly larger than the second.\nModeling the covariance between these HSIC statistics leads to a provably more\npowerful test than the construction of independent HSIC statistics by\nsub-sampling. The resulting test is consistent and unbiased, and (being based\non U-statistics) has favorable convergence properties. The test can be computed\nin quadratic time, matching the computational complexity of standard empirical\nHSIC estimators. The effectiveness of the test is demonstrated on several\nreal-world problems: we identify language groups from a multilingual corpus,\nand we prove that tumor location is more dependent on gene expression than\nchromosomal imbalances. Source code is available for download at\nhttps://github.com/wbounliphone/reldep.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jun 2014 19:23:11 GMT"}, {"version": "v2", "created": "Fri, 11 Jul 2014 17:12:58 GMT"}, {"version": "v3", "created": "Wed, 27 May 2015 08:25:19 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Bounliphone", "Wacha", "", "E3S"], ["Gretton", "Arthur", "", "E3S"], ["Tenenhaus", "Arthur", "", "E3S"], ["Blaschko", "Matthew", "", "INRIA Saclay - Ile de France, CVN"]]}, {"id": "1406.3895", "submitter": "Miguel \\'A. Carreira-Perpi\\~n\\'an", "authors": "Weiran Wang and Miguel \\'A. Carreira-Perpi\\~n\\'an", "title": "The Laplacian K-modes algorithm for clustering", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In addition to finding meaningful clusters, centroid-based clustering\nalgorithms such as K-means or mean-shift should ideally find centroids that are\nvalid patterns in the input space, representative of data in their cluster.\nThis is challenging with data having a nonconvex or manifold structure, as with\nimages or text. We introduce a new algorithm, Laplacian K-modes, which\nnaturally combines three powerful ideas in clustering: the explicit use of\nassignment variables (as in K-means); the estimation of cluster centroids which\nare modes of each cluster's density estimate (as in mean-shift); and the\nregularizing effect of the graph Laplacian, which encourages similar\nassignments for nearby points (as in spectral clustering). The optimization\nalgorithm alternates an assignment step, which is a convex quadratic program,\nand a mean-shift step, which separates for each cluster centroid. The algorithm\nfinds meaningful density estimates for each cluster, even with challenging\nproblems where the clusters have manifold structure, are highly nonconvex or in\nhigh dimension. It also provides centroids that are valid patterns, truly\nrepresentative of their cluster (unlike K-means), and an out-of-sample mapping\nthat predicts soft assignments for a new point.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jun 2014 03:29:48 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Wang", "Weiran", ""], ["Carreira-Perpi\u00f1\u00e1n", "Miguel \u00c1.", ""]]}, {"id": "1406.3896", "submitter": "Jasper Snoek", "authors": "Kevin Swersky and Jasper Snoek and Ryan Prescott Adams", "title": "Freeze-Thaw Bayesian Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop a dynamic form of Bayesian optimization for machine\nlearning models with the goal of rapidly finding good hyperparameter settings.\nOur method uses the partial information gained during the training of a machine\nlearning model in order to decide whether to pause training and start a new\nmodel, or resume the training of a previously-considered model. We specifically\ntailor our method to machine learning problems by developing a novel\npositive-definite covariance kernel to capture a variety of training curves.\nFurthermore, we develop a Gaussian process prior that scales gracefully with\nadditional temporal observations. Finally, we provide an information-theoretic\nframework to automate the decision process. Experiments on several common\nmachine learning models show that our approach is extremely effective in\npractice.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jun 2014 03:43:20 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Swersky", "Kevin", ""], ["Snoek", "Jasper", ""], ["Adams", "Ryan Prescott", ""]]}, {"id": "1406.3922", "submitter": "Amr Gerard", "authors": "Yousuf M. Soliman", "title": "Personalized Medical Treatments Using Novel Reinforcement Learning\n  Algorithms", "comments": "This paper has been withdrawn by the author. Some of the work was\n  taken from the work of Dr. Yair Goldberg and Dr. Michael R. Kosorok and they\n  have requested for the paper to be withdrawn. arXiv admin note: v1 had\n  substantial text overlap with arXiv:1202.5130, arXiv:1205.6659; and text\n  overlap with arXiv:1301.2158 by other authors without attribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In both the fields of computer science and medicine there is very strong\ninterest in developing personalized treatment policies for patients who have\nvariable responses to treatments. In particular, I aim to find an optimal\npersonalized treatment policy which is a non-deterministic function of the\npatient specific covariate data that maximizes the expected survival time or\nclinical outcome. I developed an algorithmic framework to solve multistage\ndecision problem with a varying number of stages that are subject to censoring\nin which the \"rewards\" are expected survival times. In specific, I developed a\nnovel Q-learning algorithm that dynamically adjusts for these parameters.\nFurthermore, I found finite upper bounds on the generalized error of the\ntreatment paths constructed by this algorithm. I have also shown that when the\noptimal Q-function is an element of the approximation space, the anticipated\nsurvival times for the treatment regime constructed by the algorithm will\nconverge to the optimal treatment path. I demonstrated the performance of the\nproposed algorithmic framework via simulation studies and through the analysis\nof chronic depression data and a hypothetical clinical trial. The censored\nQ-learning algorithm I developed is more effective than the state of the art\nclinical decision support systems and is able to operate in environments when\nmany covariate parameters may be unobtainable or censored.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jun 2014 07:14:26 GMT"}, {"version": "v2", "created": "Mon, 30 Jun 2014 08:29:19 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Soliman", "Yousuf M.", ""]]}, {"id": "1406.3926", "submitter": "Yasin Abbasi-Yadkori", "authors": "Yasin Abbasi-Yadkori and Csaba Szepesvari", "title": "Bayesian Optimal Control of Smoothly Parameterized Systems: The Lazy\n  Posterior Sampling Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study Bayesian optimal control of a general class of smoothly\nparameterized Markov decision problems. Since computing the optimal control is\ncomputationally expensive, we design an algorithm that trades off performance\nfor computational efficiency. The algorithm is a lazy posterior sampling method\nthat maintains a distribution over the unknown parameter. The algorithm changes\nits policy only when the variance of the distribution is reduced sufficiently.\nImportantly, we analyze the algorithm and show the precise nature of the\nperformance vs. computation tradeoff. Finally, we show the effectiveness of the\nmethod on a web server control application.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jun 2014 08:04:42 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Abbasi-Yadkori", "Yasin", ""], ["Szepesvari", "Csaba", ""]]}, {"id": "1406.4175", "submitter": "Christopher Metzler", "authors": "Christopher A. Metzler, Arian Maleki, and Richard G. Baraniuk", "title": "From Denoising to Compressed Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A denoising algorithm seeks to remove noise, errors, or perturbations from a\nsignal. Extensive research has been devoted to this arena over the last several\ndecades, and as a result, today's denoisers can effectively remove large\namounts of additive white Gaussian noise. A compressed sensing (CS)\nreconstruction algorithm seeks to recover a structured signal acquired using a\nsmall number of randomized measurements. Typical CS reconstruction algorithms\ncan be cast as iteratively estimating a signal from a perturbed observation.\nThis paper answers a natural question: How can one effectively employ a generic\ndenoiser in a CS reconstruction algorithm? In response, we develop an extension\nof the approximate message passing (AMP) framework, called Denoising-based AMP\n(D-AMP), that can integrate a wide class of denoisers within its iterations. We\ndemonstrate that, when used with a high performance denoiser for natural\nimages, D-AMP offers state-of-the-art CS recovery performance while operating\ntens of times faster than competing methods. We explain the exceptional\nperformance of D-AMP by analyzing some of its theoretical features. A key\nelement in D-AMP is the use of an appropriate Onsager correction term in its\niterations, which coerces the signal perturbation at each iteration to be very\nclose to the white Gaussian noise that denoisers are typically designed to\nremove.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jun 2014 21:20:41 GMT"}, {"version": "v2", "created": "Fri, 20 Jun 2014 02:59:58 GMT"}, {"version": "v3", "created": "Fri, 4 Jul 2014 06:30:47 GMT"}, {"version": "v4", "created": "Mon, 21 Jul 2014 16:50:49 GMT"}, {"version": "v5", "created": "Sun, 17 Apr 2016 17:30:46 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Metzler", "Christopher A.", ""], ["Maleki", "Arian", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1406.4200", "submitter": "Hung Bui", "authors": "Hung Hai Bui, Tuyen N. Huynh, David Sontag", "title": "Lifted Tree-Reweighted Variational Inference", "comments": "In: UAI (Uncertainty in Artificial Intelligence) 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze variational inference for highly symmetric graphical models such\nas those arising from first-order probabilistic models. We first show that for\nthese graphical models, the tree-reweighted variational objective lends itself\nto a compact lifted formulation which can be solved much more efficiently than\nthe standard TRW formulation for the ground graphical model. Compared to\nearlier work on lifted belief propagation, our formulation leads to a convex\noptimization problem for lifted marginal inference and provides an upper bound\non the partition function. We provide two approaches for improving the lifted\nTRW upper bound. The first is a method for efficiently computing maximum\nspanning trees in highly symmetric graphs, which can be used to optimize the\nTRW edge appearance probabilities. The second is a method for tightening the\nrelaxation of the marginal polytope using lifted cycle inequalities and novel\nexchangeable cluster consistency constraints.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jun 2014 00:43:16 GMT"}, {"version": "v2", "created": "Thu, 19 Jun 2014 23:05:37 GMT"}], "update_date": "2014-06-23", "authors_parsed": [["Bui", "Hung Hai", ""], ["Huynh", "Tuyen N.", ""], ["Sontag", "David", ""]]}, {"id": "1406.4311", "submitter": "Florent Krzakala", "authors": "Andre Manoel, Florent Krzakala, Eric W. Tramel and Lenka Zdeborov\\'a", "title": "Sparse Estimation with the Swept Approximated Message-Passing Algorithm", "comments": "11 pages, 3 figures, implementation available at\n  https://github.com/eric-tramel/SwAMP-Demo", "journal-ref": "Proceedings of the 32nd International Conference on Machine\n  Learning (ICML), 2015, 1123-1132", "doi": null, "report-no": null, "categories": "cs.IT cond-mat.dis-nn math.IT physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Message Passing (AMP) has been shown to be a superior method for\ninference problems, such as the recovery of signals from sets of noisy,\nlower-dimensionality measurements, both in terms of reconstruction accuracy and\nin computational efficiency. However, AMP suffers from serious convergence\nissues in contexts that do not exactly match its assumptions. We propose a new\napproach to stabilizing AMP in these contexts by applying AMP updates to\nindividual coefficients rather than in parallel. Our results show that this\nchange to the AMP iteration can provide theoretically expected, but hitherto\nunobtainable, performance for problems on which the standard AMP iteration\ndiverges. Additionally, we find that the computational costs of this swept\ncoefficient update scheme is not unduly burdensome, allowing it to be applied\nefficiently to signals of large dimensionality.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jun 2014 10:49:42 GMT"}], "update_date": "2015-06-10", "authors_parsed": [["Manoel", "Andre", ""], ["Krzakala", "Florent", ""], ["Tramel", "Eric W.", ""], ["Zdeborov\u00e1", "Lenka", ""]]}, {"id": "1406.4363", "submitter": "Shin Matsushima", "authors": "Shin Matsushima, Hyokun Yun, Xinhua Zhang, S.V.N. Vishwanathan", "title": "Distributed Stochastic Optimization of the Regularized Risk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning algorithms minimize a regularized risk, and stochastic\noptimization is widely used for this task. When working with massive data, it\nis desirable to perform stochastic optimization in parallel. Unfortunately,\nmany existing stochastic optimization algorithms cannot be parallelized\nefficiently. In this paper we show that one can rewrite the regularized risk\nminimization problem as an equivalent saddle-point problem, and propose an\nefficient distributed stochastic optimization (DSO) algorithm. We prove the\nalgorithm's rate of convergence; remarkably, our analysis shows that the\nalgorithm scales almost linearly with the number of processors. We also verify\nwith empirical evaluations that the proposed algorithm is competitive with\nother parallel, general purpose stochastic and batch optimization algorithms\nfor regularized risk minimization.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jun 2014 13:38:49 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2015 09:15:47 GMT"}], "update_date": "2015-06-10", "authors_parsed": [["Matsushima", "Shin", ""], ["Yun", "Hyokun", ""], ["Zhang", "Xinhua", ""], ["Vishwanathan", "S. V. N.", ""]]}, {"id": "1406.4444", "submitter": "Ziming Zhang", "authors": "Ziming Zhang and Venkatesh Saligrama", "title": "PRISM: Person Re-Identification via Structured Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (re-id), an emerging problem in visual surveillance,\ndeals with maintaining entities of individuals whilst they traverse various\nlocations surveilled by a camera network. From a visual perspective re-id is\nchallenging due to significant changes in visual appearance of individuals in\ncameras with different pose, illumination and calibration. Globally the\nchallenge arises from the need to maintain structurally consistent matches\namong all the individual entities across different camera views. We propose\nPRISM, a structured matching method to jointly account for these challenges. We\nview the global problem as a weighted graph matching problem and estimate edge\nweights by learning to predict them based on the co-occurrences of visual\npatterns in the training examples. These co-occurrence based scores in turn\naccount for appearance changes by inferring likely and unlikely visual\nco-occurrences appearing in training instances. We implement PRISM on single\nshot and multi-shot scenarios. PRISM uniformly outperforms state-of-the-art in\nterms of matching rate while being computationally efficient.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jun 2014 20:07:27 GMT"}, {"version": "v2", "created": "Wed, 18 Jun 2014 10:02:26 GMT"}, {"version": "v3", "created": "Tue, 22 Jul 2014 15:04:40 GMT"}, {"version": "v4", "created": "Fri, 8 May 2015 01:55:13 GMT"}], "update_date": "2015-05-11", "authors_parsed": [["Zhang", "Ziming", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1406.4445", "submitter": "Venkatesh Saligrama", "authors": "Ziming Zhang and Venkatesh Saligrama", "title": "RAPID: Rapidly Accelerated Proximal Gradient Algorithms for Convex\n  Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new algorithm to speed-up the convergence of\naccelerated proximal gradient (APG) methods. In order to minimize a convex\nfunction $f(\\mathbf{x})$, our algorithm introduces a simple line search step\nafter each proximal gradient step in APG so that a biconvex function\n$f(\\theta\\mathbf{x})$ is minimized over scalar variable $\\theta>0$ while fixing\nvariable $\\mathbf{x}$. We propose two new ways of constructing the auxiliary\nvariables in APG based on the intermediate solutions of the proximal gradient\nand the line search steps. We prove that at arbitrary iteration step $t\n(t\\geq1)$, our algorithm can achieve a smaller upper-bound for the gap between\nthe current and optimal objective values than those in the traditional APG\nmethods such as FISTA, making it converge faster in practice. In fact, our\nalgorithm can be potentially applied to many important convex optimization\nproblems, such as sparse linear regression and kernel SVMs. Our experimental\nresults clearly demonstrate that our algorithm converges faster than APG in all\nof the applications above, even comparable to some sophisticated solvers.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jun 2014 20:08:58 GMT"}, {"version": "v2", "created": "Wed, 18 Jun 2014 10:05:43 GMT"}], "update_date": "2014-06-19", "authors_parsed": [["Zhang", "Ziming", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1406.4465", "submitter": "Yilun Wang", "authors": "Yaru Fan and Yilun Wang", "title": "Multi-stage Multi-task feature learning via adaptive threshold", "comments": "13 pages,12 figures. arXiv admin note: text overlap with\n  arXiv:1210.5806 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task feature learning aims to identity the shared features among tasks\nto improve generalization. It has been shown that by minimizing non-convex\nlearning models, a better solution than the convex alternatives can be\nobtained. Therefore, a non-convex model based on the capped-$\\ell_{1},\\ell_{1}$\nregularization was proposed in \\cite{Gong2013}, and a corresponding efficient\nmulti-stage multi-task feature learning algorithm (MSMTFL) was presented.\nHowever, this algorithm harnesses a prescribed fixed threshold in the\ndefinition of the capped-$\\ell_{1},\\ell_{1}$ regularization and the lack of\nadaptivity might result in suboptimal performance. In this paper we propose to\nemploy an adaptive threshold in the capped-$\\ell_{1},\\ell_{1}$ regularized\nformulation, where the corresponding variant of MSMTFL will incorporate an\nadditional component to adaptively determine the threshold value. This variant\nis expected to achieve a better feature selection performance over the original\nMSMTFL algorithm. In particular, the embedded adaptive threshold component\ncomes from our previously proposed iterative support detection (ISD) method\n\\cite{Wang2010}. Empirical studies on both synthetic and real-world data sets\ndemonstrate the effectiveness of this new variant over the original MSMTFL.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jun 2014 12:47:37 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2015 19:47:37 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Fan", "Yaru", ""], ["Wang", "Yilun", ""]]}, {"id": "1406.4469", "submitter": "Santiago Segarra", "authors": "Santiago Segarra, Mark Eisen, Alejandro Ribeiro", "title": "Authorship Attribution through Function Word Adjacency Networks", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2015.2451111", "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method for authorship attribution based on function word adjacency networks\n(WANs) is introduced. Function words are parts of speech that express\ngrammatical relationships between other words but do not carry lexical meaning\non their own. In the WANs in this paper, nodes are function words and directed\nedges stand in for the likelihood of finding the sink word in the ordered\nvicinity of the source word. WANs of different authors can be interpreted as\ntransition probabilities of a Markov chain and are therefore compared in terms\nof their relative entropies. Optimal selection of WAN parameters is studied and\nattribution accuracy is benchmarked across a diverse pool of authors and\nvarying text lengths. This analysis shows that, since function words are\nindependent of content, their use tends to be specific to an author and that\nthe relational data captured by function WANs is a good summary of stylometric\nfingerprints. Attribution accuracy is observed to exceed the one achieved by\nmethods that rely on word frequencies alone. Further combining WANs with\nmethods that rely on word frequencies alone, results in larger attribution\naccuracy, indicating that both sources of information encode different aspects\nof authorial styles.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jun 2014 18:32:18 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Segarra", "Santiago", ""], ["Eisen", "Mark", ""], ["Ribeiro", "Alejandro", ""]]}, {"id": "1406.4472", "submitter": "Giorgio Valentini", "authors": "Giorgio Valentini", "title": "Notes on hierarchical ensemble methods for DAG-structured taxonomies", "comments": "12 pages, 3 figures. Typos corrected. Modified title and abstract.\n  Added references and some changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several real problems ranging from text classification to computational\nbiology are characterized by hierarchical multi-label classification tasks.\nMost of the methods presented in literature focused on tree-structured\ntaxonomies, but only few on taxonomies structured according to a Directed\nAcyclic Graph (DAG). In this contribution novel classification ensemble\nalgorithms for DAG-structured taxonomies are introduced. In particular\nHierarchical Top-Down (HTD-DAG) and True Path Rule (TPR-DAG) for DAGs are\npresented and discussed.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jun 2014 18:41:19 GMT"}, {"version": "v2", "created": "Wed, 18 Jun 2014 10:38:35 GMT"}], "update_date": "2014-06-19", "authors_parsed": [["Valentini", "Giorgio", ""]]}, {"id": "1406.4519", "submitter": "Joon Hee Choi", "authors": "Joon Hee Choi, S. V. N. Vishwanathan", "title": "DFacTo: Distributed Factorization of Tensors", "comments": "Under review for NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a technique for significantly speeding up Alternating Least\nSquares (ALS) and Gradient Descent (GD), two widely used algorithms for tensor\nfactorization. By exploiting properties of the Khatri-Rao product, we show how\nto efficiently address a computationally challenging sub-step of both\nalgorithms. Our algorithm, DFacTo, only requires two sparse matrix-vector\nproducts and is easy to parallelize. DFacTo is not only scalable but also on\naverage 4 to 10 times faster than competing algorithms on a variety of\ndatasets. For instance, DFacTo only takes 480 seconds on 4 machines to perform\none iteration of the ALS algorithm and 1,143 seconds to perform one iteration\nof the GD algorithm on a 6.5 million x 2.5 million x 1.5 million dimensional\ntensor with 1.2 billion non-zero entries.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jun 2014 20:08:04 GMT"}], "update_date": "2014-06-19", "authors_parsed": [["Choi", "Joon Hee", ""], ["Vishwanathan", "S. V. N.", ""]]}, {"id": "1406.4566", "submitter": "Furong Huang", "authors": "Furong Huang, Niranjan U.N., Ioakeim Perros, Robert Chen, Jimeng Sun,\n  Anima Anandkumar", "title": "Guaranteed Scalable Learning of Latent Tree Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an integrated approach for structure and parameter estimation in\nlatent tree graphical models. Our overall approach follows a\n\"divide-and-conquer\" strategy that learns models over small groups of variables\nand iteratively merges onto a global solution. The structure learning involves\ncombinatorial operations such as minimum spanning tree construction and local\nrecursive grouping; the parameter learning is based on the method of moments\nand on tensor decompositions. Our method is guaranteed to correctly recover the\nunknown tree structure and the model parameters with low sample complexity for\nthe class of linear multivariate latent tree models which includes discrete and\nGaussian distributions, and Gaussian mixtures. Our bulk asynchronous parallel\nalgorithm is implemented in parallel and the parallel computation complexity\nincreases only logarithmically with the number of variables and linearly with\ndimensionality of each variable.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jun 2014 01:17:27 GMT"}, {"version": "v2", "created": "Fri, 27 Feb 2015 00:22:05 GMT"}, {"version": "v3", "created": "Tue, 17 Mar 2015 01:07:13 GMT"}, {"version": "v4", "created": "Tue, 17 Dec 2019 19:49:48 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Huang", "Furong", ""], ["N.", "Niranjan U.", ""], ["Perros", "Ioakeim", ""], ["Chen", "Robert", ""], ["Sun", "Jimeng", ""], ["Anandkumar", "Anima", ""]]}, {"id": "1406.4580", "submitter": "Seunghak Lee", "authors": "Seunghak Lee, Jin Kyu Kim, Xun Zheng, Qirong Ho, Garth A. Gibson, Eric\n  P. Xing", "title": "Primitives for Dynamic Big Model Parallelism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When training large machine learning models with many variables or\nparameters, a single machine is often inadequate since the model may be too\nlarge to fit in memory, while training can take a long time even with\nstochastic updates. A natural recourse is to turn to distributed cluster\ncomputing, in order to harness additional memory and processors. However,\nnaive, unstructured parallelization of ML algorithms can make inefficient use\nof distributed memory, while failing to obtain proportional convergence\nspeedups - or can even result in divergence. We develop a framework of\nprimitives for dynamic model-parallelism, STRADS, in order to explore\npartitioning and update scheduling of model variables in distributed ML\nalgorithms - thus improving their memory efficiency while presenting new\nopportunities to speed up convergence without compromising inference\ncorrectness. We demonstrate the efficacy of model-parallel algorithms\nimplemented in STRADS versus popular implementations for Topic Modeling, Matrix\nFactorization and Lasso.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jun 2014 03:06:52 GMT"}], "update_date": "2014-06-19", "authors_parsed": [["Lee", "Seunghak", ""], ["Kim", "Jin Kyu", ""], ["Zheng", "Xun", ""], ["Ho", "Qirong", ""], ["Gibson", "Garth A.", ""], ["Xing", "Eric P.", ""]]}, {"id": "1406.4625", "submitter": "Bobak Shahriari", "authors": "Bobak Shahriari and Ziyu Wang and Matthew W. Hoffman and Alexandre\n  Bouchard-C\\^ot\\'e and Nando de Freitas", "title": "An Entropy Search Portfolio for Bayesian Optimization", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization is a sample-efficient method for black-box global\noptimization. How- ever, the performance of a Bayesian optimization method very\nmuch depends on its exploration strategy, i.e. the choice of acquisition\nfunction, and it is not clear a priori which choice will result in superior\nperformance. While portfolio methods provide an effective, principled way of\ncombining a collection of acquisition functions, they are often based on\nmeasures of past performance which can be misleading. To address this issue, we\nintroduce the Entropy Search Portfolio (ESP): a novel approach to portfolio\nconstruction which is motivated by information theoretic considerations. We\nshow that ESP outperforms existing portfolio methods on several real and\nsynthetic problems, including geostatistical datasets and simulated control\ntasks. We not only show that ESP is able to offer performance as good as the\nbest, but unknown, acquisition function, but surprisingly it often gives better\nperformance. Finally, over a wide range of conditions we find that ESP is\nrobust to the inclusion of poor acquisition functions.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jun 2014 07:26:08 GMT"}, {"version": "v2", "created": "Mon, 27 Oct 2014 23:58:14 GMT"}, {"version": "v3", "created": "Thu, 30 Oct 2014 15:54:56 GMT"}, {"version": "v4", "created": "Wed, 4 Mar 2015 21:25:31 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["Shahriari", "Bobak", ""], ["Wang", "Ziyu", ""], ["Hoffman", "Matthew W.", ""], ["Bouchard-C\u00f4t\u00e9", "Alexandre", ""], ["de Freitas", "Nando", ""]]}, {"id": "1406.4905", "submitter": "Roger Frigola", "authors": "Roger Frigola and Yutian Chen and Carl E. Rasmussen", "title": "Variational Gaussian Process State-Space Models", "comments": null, "journal-ref": "R. Frigola, Y. Chen and C. E. Rasmussen. Variational Gaussian\n  Process State-Space Models, in Advances in Neural Information Processing\n  Systems (NIPS), 2014", "doi": null, "report-no": null, "categories": "cs.LG cs.RO cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-space models have been successfully used for more than fifty years in\ndifferent areas of science and engineering. We present a procedure for\nefficient variational Bayesian learning of nonlinear state-space models based\non sparse Gaussian processes. The result of learning is a tractable posterior\nover nonlinear dynamical systems. In comparison to conventional parametric\nmodels, we offer the possibility to straightforwardly trade off model capacity\nand computational cost whilst avoiding overfitting. Our main algorithm uses a\nhybrid inference approach combining variational Bayes and sequential Monte\nCarlo. We also present stochastic variational inference and online learning\napproaches for fast learning with long time series.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jun 2014 22:16:27 GMT"}, {"version": "v2", "created": "Mon, 3 Nov 2014 08:17:59 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Frigola", "Roger", ""], ["Chen", "Yutian", ""], ["Rasmussen", "Carl E.", ""]]}, {"id": "1406.4966", "submitter": "Jingdong Wang", "authors": "Chao Du, Jingdong Wang", "title": "Inner Product Similarity Search using Compositional Codes", "comments": "The approach presented in this paper (ECCV14 submission) is closely\n  related to multi-stage vector quantization and residual quantization. Thanks\n  the reviewers (CVPR14 and ECCV14) for pointing out the relationship to the\n  two algorithms. Related paper:\n  http://sites.skoltech.ru/app/data/uploads/sites/2/2013/09/CVPR14.pdf, which\n  also adopts the summation of vectors for vector approximation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the nearest neighbor search problem under inner product\nsimilarity and introduces a compact code-based approach. The idea is to\napproximate a vector using the composition of several elements selected from a\nsource dictionary and to represent this vector by a short code composed of the\nindices of the selected elements. The inner product between a query vector and\na database vector is efficiently estimated from the query vector and the short\ncode of the database vector. We show the superior performance of the proposed\ngroup $M$-selection algorithm that selects $M$ elements from $M$ source\ndictionaries for vector approximation in terms of search accuracy and\nefficiency for compact codes of the same length via theoretical and empirical\nanalysis. Experimental results on large-scale datasets ($1M$ and $1B$ SIFT\nfeatures, $1M$ linear models and Netflix) demonstrate the superiority of the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jun 2014 07:42:05 GMT"}, {"version": "v2", "created": "Fri, 20 Jun 2014 02:13:56 GMT"}], "update_date": "2014-06-23", "authors_parsed": [["Du", "Chao", ""], ["Wang", "Jingdong", ""]]}, {"id": "1406.4993", "submitter": "Fredrik Lindsten", "authors": "Fredrik Lindsten, Adam M. Johansen, Christian A. Naesseth, Bonnie\n  Kirkpatrick, Thomas B. Sch\\\"on, John Aston, Alexandre Bouchard-C\\^ot\\'e", "title": "Divide-and-Conquer with Sequential Monte Carlo", "comments": null, "journal-ref": "Journal of Computational and Graphical Statistics, 26(2):445-458,\n  2017", "doi": "10.1080/10618600.2016.1237363", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel class of Sequential Monte Carlo (SMC) algorithms,\nappropriate for inference in probabilistic graphical models. This class of\nalgorithms adopts a divide-and-conquer approach based upon an auxiliary\ntree-structured decomposition of the model of interest, turning the overall\ninferential task into a collection of recursively solved sub-problems. The\nproposed method is applicable to a broad class of probabilistic graphical\nmodels, including models with loops. Unlike a standard SMC sampler, the\nproposed Divide-and-Conquer SMC employs multiple independent populations of\nweighted particles, which are resampled, merged, and propagated as the method\nprogresses. We illustrate empirically that this approach can outperform\nstandard methods in terms of the accuracy of the posterior expectation and\nmarginal likelihood approximations. Divide-and-Conquer SMC also opens up novel\nparallel implementation options and the possibility of concentrating the\ncomputational effort on the most challenging sub-problems. We demonstrate its\nperformance on a Markov random field and on a hierarchical logistic regression\nproblem.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jun 2014 10:01:15 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2015 10:59:00 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Lindsten", "Fredrik", ""], ["Johansen", "Adam M.", ""], ["Naesseth", "Christian A.", ""], ["Kirkpatrick", "Bonnie", ""], ["Sch\u00f6n", "Thomas B.", ""], ["Aston", "John", ""], ["Bouchard-C\u00f4t\u00e9", "Alexandre", ""]]}, {"id": "1406.5036", "submitter": "Katja Ried", "authors": "Katja Ried, Megan Agnew, Lydia Vermeyden, Dominik Janzing, Robert W.\n  Spekkens and Kevin J. Resch", "title": "Inferring causal structure: a quantum advantage", "comments": "17 pages, 6 figures. Comments welcome", "journal-ref": "Nat Phys 11, 414-420 (2015)", "doi": "10.1038/nphys3266", "report-no": null, "categories": "quant-ph cs.LG gr-qc stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of using observed correlations to infer causal relations is\nrelevant to a wide variety of scientific disciplines. Yet given correlations\nbetween just two classical variables, it is impossible to determine whether\nthey arose from a causal influence of one on the other or a common cause\ninfluencing both, unless one can implement a randomized intervention. We here\nconsider the problem of causal inference for quantum variables. We introduce\ncausal tomography, which unifies and generalizes conventional quantum\ntomography schemes to provide a complete solution to the causal inference\nproblem using a quantum analogue of a randomized trial. We furthermore show\nthat, in contrast to the classical case, observed quantum correlations alone\ncan sometimes provide a solution. We implement a quantum-optical experiment\nthat allows us to control the causal relation between two optical modes, and\ntwo measurement schemes -- one with and one without randomization -- that\nextract this relation from the observed correlations. Our results show that\nentanglement and coherence, known to be central to quantum information\nprocessing, also provide a quantum advantage for causal inference.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jun 2014 13:30:12 GMT"}], "update_date": "2015-06-08", "authors_parsed": [["Ried", "Katja", ""], ["Agnew", "Megan", ""], ["Vermeyden", "Lydia", ""], ["Janzing", "Dominik", ""], ["Spekkens", "Robert W.", ""], ["Resch", "Kevin J.", ""]]}, {"id": "1406.5143", "submitter": "Ohad Shamir", "authors": "Ohad Shamir", "title": "The Sample Complexity of Learning Linear Predictors with the Squared\n  Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this short note, we provide tight sample complexity bounds for learning\nlinear predictors with respect to the squared loss. Our focus is on an agnostic\nsetting, where no assumptions are made on the data distribution. This contrasts\nwith standard results in the literature, which either make distributional\nassumptions, refer to specific parameter settings, or use other performance\nmeasures.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jun 2014 18:42:12 GMT"}, {"version": "v2", "created": "Sat, 21 Jun 2014 08:27:42 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Shamir", "Ohad", ""]]}, {"id": "1406.5273", "submitter": "Wing-Kin Ma", "authors": "Chia-Hsiang Lin, Wing-Kin Ma, Wei-Chiang Li, Chong-Yung Chi, and\n  ArulMurugan Ambikapathi", "title": "Identifiability of the Simplex Volume Minimization Criterion for Blind\n  Hyperspectral Unmixing: The No Pure-Pixel Case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In blind hyperspectral unmixing (HU), the pure-pixel assumption is well-known\nto be powerful in enabling simple and effective blind HU solutions. However,\nthe pure-pixel assumption is not always satisfied in an exact sense, especially\nfor scenarios where pixels are heavily mixed. In the no pure-pixel case, a good\nblind HU approach to consider is the minimum volume enclosing simplex (MVES).\nEmpirical experience has suggested that MVES algorithms can perform well\nwithout pure pixels, although it was not totally clear why this is true from a\ntheoretical viewpoint. This paper aims to address the latter issue. We develop\nan analysis framework wherein the perfect endmember identifiability of MVES is\nstudied under the noiseless case. We prove that MVES is indeed robust against\nlack of pure pixels, as long as the pixels do not get too heavily mixed and too\nasymmetrically spread. The theoretical results are verified by numerical\nsimulations.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jun 2014 03:54:19 GMT"}, {"version": "v2", "created": "Thu, 26 Feb 2015 07:22:12 GMT"}], "update_date": "2015-02-27", "authors_parsed": [["Lin", "Chia-Hsiang", ""], ["Ma", "Wing-Kin", ""], ["Li", "Wei-Chiang", ""], ["Chi", "Chong-Yung", ""], ["Ambikapathi", "ArulMurugan", ""]]}, {"id": "1406.5286", "submitter": "Nicolas Gillis", "authors": "Nicolas Gillis, Wing-Kin Ma", "title": "Enhancing Pure-Pixel Identification Performance via Preconditioning", "comments": "25 pages, 3 figures", "journal-ref": "SIAM J. on Imaging Sciences 8 (2), pp. 1161-1186, 2015", "doi": "10.1137/140994915", "report-no": null, "categories": "stat.ML cs.LG math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze different preconditionings designed to enhance\nrobustness of pure-pixel search algorithms, which are used for blind\nhyperspectral unmixing and which are equivalent to near-separable nonnegative\nmatrix factorization algorithms. Our analysis focuses on the successive\nprojection algorithm (SPA), a simple, efficient and provably robust algorithm\nin the pure-pixel algorithm class. Recently, a provably robust preconditioning\nwas proposed by Gillis and Vavasis (arXiv:1310.2273) which requires the\nresolution of a semidefinite program (SDP) to find a data points-enclosing\nminimum volume ellipsoid. Since solving the SDP in high precisions can be time\nconsuming, we generalize the robustness analysis to approximate solutions of\nthe SDP, that is, solutions whose objective function values are some\nmultiplicative factors away from the optimal value. It is shown that a high\naccuracy solution is not crucial for robustness, which paves the way for faster\npreconditionings (e.g., based on first-order optimization methods). This first\ncontribution also allows us to provide a robustness analysis for two other\npreconditionings. The first one is pre-whitening, which can be interpreted as\nan optimal solution of the same SDP with additional constraints. We analyze\nrobustness of pre-whitening which allows us to characterize situations in which\nit performs competitively with the SDP-based preconditioning. The second one is\nbased on SPA itself and can be interpreted as an optimal solution of a\nrelaxation of the SDP. It is extremely fast while competing with the SDP-based\npreconditioning on several synthetic data sets.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jun 2014 06:45:24 GMT"}], "update_date": "2015-05-29", "authors_parsed": [["Gillis", "Nicolas", ""], ["Ma", "Wing-Kin", ""]]}, {"id": "1406.5291", "submitter": "Soumyadeep Chatterjee", "authors": "Soumyadeep Chatterjee and Sheng Chen and Arindam Banerjee", "title": "Generalized Dantzig Selector: Application to the k-support norm", "comments": "Updates to bound", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Generalized Dantzig Selector (GDS) for linear models, in which\nany norm encoding the parameter structure can be leveraged for estimation. We\ninvestigate both computational and statistical aspects of the GDS. Based on\nconjugate proximal operator, a flexible inexact ADMM framework is designed for\nsolving GDS, and non-asymptotic high-probability bounds are established on the\nestimation error, which rely on Gaussian width of unit norm ball and suitable\nset encompassing estimation error. Further, we consider a non-trivial example\nof the GDS using $k$-support norm. We derive an efficient method to compute the\nproximal operator for $k$-support norm since existing methods are inapplicable\nin this setting. For statistical analysis, we provide upper bounds for the\nGaussian widths needed in the GDS analysis, yielding the first statistical\nrecovery guarantee for estimation with the $k$-support norm. The experimental\nresults confirm our theoretical analysis.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jun 2014 07:11:44 GMT"}, {"version": "v2", "created": "Tue, 8 Jul 2014 14:53:00 GMT"}, {"version": "v3", "created": "Mon, 2 Feb 2015 17:54:14 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Chatterjee", "Soumyadeep", ""], ["Chen", "Sheng", ""], ["Banerjee", "Arindam", ""]]}, {"id": "1406.5295", "submitter": "Aaditya Ramdas", "authors": "Aaditya Ramdas", "title": "Rows vs Columns for Linear Systems of Equations - Randomized Kaczmarz or\n  Coordinate Descent?", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is about randomized iterative algorithms for solving a linear\nsystem of equations $X \\beta = y$ in different settings. Recent interest in the\ntopic was reignited when Strohmer and Vershynin (2009) proved the linear\nconvergence rate of a Randomized Kaczmarz (RK) algorithm that works on the rows\nof $X$ (data points). Following that, Leventhal and Lewis (2010) proved the\nlinear convergence of a Randomized Coordinate Descent (RCD) algorithm that\nworks on the columns of $X$ (features). The aim of this paper is to simplify\nour understanding of these two algorithms, establish the direct relationships\nbetween them (though RK is often compared to Stochastic Gradient Descent), and\nexamine the algorithmic commonalities or tradeoffs involved with working on\nrows or columns. We also discuss Kernel Ridge Regression and present a\nKaczmarz-style algorithm that works on data points and having the advantage of\nsolving the problem without ever storing or forming the Gram matrix, one of the\nrecognized problems encountered when scaling kernelized methods.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jun 2014 07:21:31 GMT"}], "update_date": "2014-06-23", "authors_parsed": [["Ramdas", "Aaditya", ""]]}, {"id": "1406.5298", "submitter": "Diederik P Kingma M.Sc.", "authors": "Diederik P. Kingma, Danilo J. Rezende, Shakir Mohamed, Max Welling", "title": "Semi-Supervised Learning with Deep Generative Models", "comments": "To appear in the proceedings of Neural Information Processing Systems\n  (NIPS) 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ever-increasing size of modern data sets combined with the difficulty of\nobtaining label information has made semi-supervised learning one of the\nproblems of significant practical importance in modern data analysis. We\nrevisit the approach to semi-supervised learning with generative models and\ndevelop new models that allow for effective generalisation from small labelled\ndata sets to large unlabelled ones. Generative approaches have thus far been\neither inflexible, inefficient or non-scalable. We show that deep generative\nmodels and approximate Bayesian inference exploiting recent advances in\nvariational methods can be used to provide significant improvements, making\ngenerative approaches highly competitive for semi-supervised learning.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jun 2014 07:52:18 GMT"}, {"version": "v2", "created": "Fri, 31 Oct 2014 22:43:31 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Kingma", "Diederik P.", ""], ["Rezende", "Danilo J.", ""], ["Mohamed", "Shakir", ""], ["Welling", "Max", ""]]}, {"id": "1406.5311", "submitter": "Aaditya Ramdas", "authors": "Aaditya Ramdas and Javier Pe\\~na", "title": "Towards A Deeper Geometric, Analytic and Algorithmic Understanding of\n  Margins", "comments": "18 pages, 3 figures", "journal-ref": "Optimization Methods and Software, Volume 31, Issue 2, Pages\n  377-391, 2016", "doi": "10.1080/10556788.2015.1099652", "report-no": null, "categories": "math.OC cs.AI cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a matrix $A$, a linear feasibility problem (of which linear\nclassification is a special case) aims to find a solution to a primal problem\n$w: A^Tw > \\textbf{0}$ or a certificate for the dual problem which is a\nprobability distribution $p: Ap = \\textbf{0}$. Inspired by the continued\nimportance of \"large-margin classifiers\" in machine learning, this paper\nstudies a condition measure of $A$ called its \\textit{margin} that determines\nthe difficulty of both the above problems. To aid geometrical intuition, we\nfirst establish new characterizations of the margin in terms of relevant balls,\ncones and hulls. Our second contribution is analytical, where we present\ngeneralizations of Gordan's theorem, and variants of Hoffman's theorems, both\nusing margins. We end by proving some new results on a classical iterative\nscheme, the Perceptron, whose convergence rates famously depends on the margin.\nOur results are relevant for a deeper understanding of margin-based learning\nand proving convergence rates of iterative schemes, apart from providing a\nunifying perspective on this vast topic.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jun 2014 08:35:15 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2016 05:53:45 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["Ramdas", "Aaditya", ""], ["Pe\u00f1a", "Javier", ""]]}, {"id": "1406.5362", "submitter": "Christoph H. Lampert", "authors": "Christoph H. Lampert", "title": "Predicting the Future Behavior of a Time-Varying Probability\n  Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of predicting the future, though only in the\nprobabilistic sense of estimating a future state of a time-varying probability\ndistribution. This is not only an interesting academic problem, but solving\nthis extrapolation problem also has many practical application, e.g. for\ntraining classifiers that have to operate under time-varying conditions. Our\nmain contribution is a method for predicting the next step of the time-varying\ndistribution from a given sequence of sample sets from earlier time steps. For\nthis we rely on two recent machine learning techniques: embedding probability\ndistributions into a reproducing kernel Hilbert space, and learning operators\nby vector-valued regression. We illustrate the working principles and the\npractical usefulness of our method by experiments on synthetic and real data.\nWe also highlight an exemplary application: training a classifier in a domain\nadaptation setting without having access to examples from the test time\ndistribution at training time.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jun 2014 12:14:45 GMT"}, {"version": "v2", "created": "Thu, 20 Nov 2014 17:21:19 GMT"}], "update_date": "2014-11-21", "authors_parsed": [["Lampert", "Christoph H.", ""]]}, {"id": "1406.5370", "submitter": "Alexandre d'Aspremont", "authors": "Fajwel Fogel, Alexandre d'Aspremont, Milan Vojnovic", "title": "Spectral Ranking using Seriation", "comments": "Substantially revised. Accepted by JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a seriation algorithm for ranking a set of items given pairwise\ncomparisons between these items. Intuitively, the algorithm assigns similar\nrankings to items that compare similarly with all others. It does so by\nconstructing a similarity matrix from pairwise comparisons, using seriation\nmethods to reorder this matrix and construct a ranking. We first show that this\nspectral seriation algorithm recovers the true ranking when all pairwise\ncomparisons are observed and consistent with a total order. We then show that\nranking reconstruction is still exact when some pairwise comparisons are\ncorrupted or missing, and that seriation based spectral ranking is more robust\nto noise than classical scoring methods. Finally, we bound the ranking error\nwhen only a random subset of the comparions are observed. An additional benefit\nof the seriation formulation is that it allows us to solve semi-supervised\nranking problems. Experiments on both synthetic and real datasets demonstrate\nthat seriation based spectral ranking achieves competitive and in some cases\nsuperior performance compared to classical ranking methods.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jun 2014 12:58:46 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2015 10:21:50 GMT"}, {"version": "v3", "created": "Mon, 18 Jan 2016 18:09:07 GMT"}, {"version": "v4", "created": "Thu, 10 Mar 2016 18:15:19 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["Fogel", "Fajwel", ""], ["d'Aspremont", "Alexandre", ""], ["Vojnovic", "Milan", ""]]}, {"id": "1406.5383", "submitter": "Yining Wang", "authors": "Yining Wang, Aarti Singh", "title": "Noise-adaptive Margin-based Active Learning and Lower Bounds under\n  Tsybakov Noise Condition", "comments": "16 pages, 2 figures. An abridged version to appear in Thirtieth AAAI\n  Conference on Artificial Intelligence (AAAI), which is held in Phoenix, AZ\n  USA in 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple noise-robust margin-based active learning algorithm to\nfind homogeneous (passing the origin) linear separators and analyze its error\nconvergence when labels are corrupted by noise. We show that when the imposed\nnoise satisfies the Tsybakov low noise condition (Mammen, Tsybakov, and others\n1999; Tsybakov 2004) the algorithm is able to adapt to unknown level of noise\nand achieves optimal statistical rate up to poly-logarithmic factors. We also\nderive lower bounds for margin based active learning algorithms under Tsybakov\nnoise conditions (TNC) for the membership query synthesis scenario (Angluin\n1988). Our result implies lower bounds for the stream based selective sampling\nscenario (Cohn 1990) under TNC for some fairly simple data distributions. Quite\nsurprisingly, we show that the sample complexity cannot be improved even if the\nunderlying data distribution is as simple as the uniform distribution on the\nunit ball. Our proof involves the construction of a well separated hypothesis\nset on the d-dimensional unit ball along with carefully designed label\ndistributions for the Tsybakov noise condition. Our analysis might provide\ninsights for other forms of lower bounds as well.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jun 2014 13:42:30 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2015 20:14:19 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2015 23:09:47 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Wang", "Yining", ""], ["Singh", "Aarti", ""]]}, {"id": "1406.5403", "submitter": "Quoc Tran-Dinh", "authors": "Quoc Tran-Dinh and Volkan Cevher", "title": "A Primal-Dual Algorithmic Framework for Constrained Convex Minimization", "comments": "This paper consists of 54 pages with 7 tables and 12 figures", "journal-ref": null, "doi": null, "report-no": "Technical Report LIONS-EPFL 2014", "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a primal-dual algorithmic framework to obtain approximate\nsolutions to a prototypical constrained convex optimization problem, and\nrigorously characterize how common structural assumptions affect the numerical\nefficiency. Our main analysis technique provides a fresh perspective on\nNesterov's excessive gap technique in a structured fashion and unifies it with\nsmoothing and primal-dual methods. For instance, through the choices of a dual\nsmoothing strategy and a center point, our framework subsumes decomposition\nalgorithms, augmented Lagrangian as well as the alternating direction\nmethod-of-multipliers methods as its special cases, and provides optimal\nconvergence rates on the primal objective residual as well as the primal\nfeasibility gap of the iterates for all.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jun 2014 14:32:24 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2015 10:33:18 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["Tran-Dinh", "Quoc", ""], ["Cevher", "Volkan", ""]]}, {"id": "1406.5565", "submitter": "Sam  Keene", "authors": "Kenneth D. Morton Jr., Peter Torrione, Leslie Collins, Sam Keene", "title": "An Open Source Pattern Recognition Toolbox for MATLAB", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pattern recognition and machine learning are becoming integral parts of\nalgorithms in a wide range of applications. Different algorithms and approaches\nfor machine learning include different tradeoffs between performance and\ncomputation, so during algorithm development it is often necessary to explore a\nvariety of different approaches to a given task. A toolbox with a unified\nframework across multiple pattern recognition techniques enables algorithm\ndevelopers the ability to rapidly evaluate different choices prior to\ndeployment. MATLAB is a widely used environment for algorithm development and\nprototyping, and although several MATLAB toolboxes for pattern recognition are\ncurrently available these are either incomplete, expensive, or restrictively\nlicensed. In this work we describe a MATLAB toolbox for pattern recognition and\nmachine learning known as the PRT (Pattern Recognition Toolbox), licensed under\nthe permissive MIT license. The PRT includes many popular techniques for data\npreprocessing, supervised learning, clustering, regression and feature\nselection, as well as a methodology for combining these components using a\nsimple, uniform syntax. The resulting algorithms can be evaluated using\ncross-validation and a variety of scoring metrics to ensure robust performance\nwhen the algorithm is deployed. This paper presents an overview of the PRT as\nwell as an example of usage on Fisher's Iris dataset.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jun 2014 01:50:54 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Morton", "Kenneth D.", "Jr."], ["Torrione", "Peter", ""], ["Collins", "Leslie", ""], ["Keene", "Sam", ""]]}, {"id": "1406.5577", "submitter": "Tvrtko Tadi\\'c", "authors": "Tvrtko Tadi\\'c", "title": "Graphical structure of conditional independencies in determinantal point\n  processes", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determinantal point process have recently been used as models in machine\nlearning and this has raised questions regarding the characterizations of\nconditional independence. In this paper we investigate characterizations of\nconditional independence. We describe some conditional independencies through\nthe conditions on the kernel of a determinantal point process, and show many\ncan be obtained using the graph induced by a kernel of the $L$-ensemble.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jun 2014 04:38:47 GMT"}, {"version": "v2", "created": "Sun, 29 Jun 2014 02:43:26 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Tadi\u0107", "Tvrtko", ""]]}, {"id": "1406.5614", "submitter": "Shiliang Sun", "authors": "Shiliang Sun, John Shawe-Taylor, Liang Mao", "title": "PAC-Bayes Analysis of Multi-view Learning", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents eight PAC-Bayes bounds to analyze the generalization\nperformance of multi-view classifiers. These bounds adopt data dependent\nGaussian priors which emphasize classifiers with high view agreements. The\ncenter of the prior for the first two bounds is the origin, while the center of\nthe prior for the third and fourth bounds is given by a data dependent vector.\nAn important technique to obtain these bounds is two derived logarithmic\ndeterminant inequalities whose difference lies in whether the dimensionality of\ndata is involved. The centers of the fifth and sixth bounds are calculated on a\nseparate subset of the training set. The last two bounds use unlabeled data to\nrepresent view agreements and are thus applicable to semi-supervised multi-view\nlearning. We evaluate all the presented multi-view PAC-Bayes bounds on\nbenchmark data and compare them with previous single-view PAC-Bayes bounds. The\nusefulness and performance of the multi-view bounds are discussed.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jun 2014 14:25:35 GMT"}, {"version": "v2", "created": "Sat, 4 Jun 2016 06:55:57 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Sun", "Shiliang", ""], ["Shawe-Taylor", "John", ""], ["Mao", "Liang", ""]]}, {"id": "1406.5638", "submitter": "Jiaming Xu", "authors": "Bruce Hajek and Sewoong Oh and Jiaming Xu", "title": "Minimax-optimal Inference from Partial Rankings", "comments": "16 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of inferring a global preference based on the\npartial rankings provided by many users over different subsets of items\naccording to the Plackett-Luce model. A question of particular interest is how\nto optimally assign items to users for ranking and how many item assignments\nare needed to achieve a target estimation error. For a given assignment of\nitems to users, we first derive an oracle lower bound of the estimation error\nthat holds even for the more general Thurstone models. Then we show that the\nCram\\'er-Rao lower bound and our upper bounds inversely depend on the spectral\ngap of the Laplacian of an appropriately defined comparison graph. When the\nsystem is allowed to choose the item assignment, we propose a random assignment\nscheme. Our oracle lower bound and upper bounds imply that it is\nminimax-optimal up to a logarithmic factor among all assignment schemes and the\nlower bound can be achieved by the maximum likelihood estimator as well as\npopular rank-breaking schemes that decompose partial rankings into pairwise\ncomparisons. The numerical experiments corroborate our theoretical findings.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jun 2014 17:55:54 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Hajek", "Bruce", ""], ["Oh", "Sewoong", ""], ["Xu", "Jiaming", ""]]}, {"id": "1406.5647", "submitter": "Arash Ali Amini", "authors": "Arash A. Amini, Elizaveta Levina", "title": "On semidefinite relaxations for the block model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stochastic block model (SBM) is a popular tool for community detection in\nnetworks, but fitting it by maximum likelihood (MLE) involves a computationally\ninfeasible optimization problem. We propose a new semidefinite programming\n(SDP) solution to the problem of fitting the SBM, derived as a relaxation of\nthe MLE. We put ours and previously proposed SDPs in a unified framework, as\nrelaxations of the MLE over various sub-classes of the SBM, revealing a\nconnection to sparse PCA. Our main relaxation, which we call SDP-1, is tighter\nthan other recently proposed SDP relaxations, and thus previously established\ntheoretical guarantees carry over. However, we show that SDP-1 exactly recovers\ntrue communities over a wider class of SBMs than those covered by current\nresults. In particular, the assumption of strong assortativity of the SBM,\nimplicit in consistency conditions for previously proposed SDPs, can be relaxed\nto weak assortativity for our approach, thus significantly broadening the class\nof SBMs covered by the consistency results. We also show that strong\nassortativity is indeed a necessary condition for exact recovery for previously\nproposed SDP approaches and not an artifact of the proofs. Our analysis of SDPs\nis based on primal-dual witness constructions, which provides some insight into\nthe nature of the solutions of various SDPs. We show how to combine features\nfrom SDP-1 and already available SDPs to achieve the most flexibility in terms\nof both assortativity and block-size constraints, as our relaxation has the\ntendency to produce communities of similar sizes. This tendency makes it the\nideal tool for fitting network histograms, a method gaining popularity in the\ngraphon estimation literature, as we illustrate on an example of a social\nnetworks of dolphins. We also provide empirical evidence that SDPs outperform\nspectral methods for fitting SBMs with a large number of blocks.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jun 2014 20:08:38 GMT"}, {"version": "v2", "created": "Thu, 11 Dec 2014 17:36:13 GMT"}, {"version": "v3", "created": "Wed, 16 Mar 2016 14:24:05 GMT"}], "update_date": "2016-03-17", "authors_parsed": [["Amini", "Arash A.", ""], ["Levina", "Elizaveta", ""]]}, {"id": "1406.5706", "submitter": "Francesca Paola Carli", "authors": "Francesca Paola Carli", "title": "On the Maximum Entropy Property of the First-Order Stable Spline Kernel\n  and its Implications", "comments": "12 pages. In 2014 IEEE Multi-conference on Systems and Control. IEEE,\n  2014", "journal-ref": null, "doi": "10.1109/CCA.2014.6981380", "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new nonparametric approach for system identification has been recently\nproposed where the impulse response is seen as the realization of a zero--mean\nGaussian process whose covariance, the so--called stable spline kernel,\nguarantees that the impulse response is almost surely stable. Maximum entropy\nproperties of the stable spline kernel have been pointed out in the literature.\nIn this paper we provide an independent proof that relies on the theory of\nmatrix extension problems in the graphical model literature and leads to a\nclosed form expression for the inverse of the first order stable spline kernel\nas well as to a new factorization in the form $UWU^\\top$ with $U$ upper\ntriangular and $W$ diagonal. Interestingly, all first--order stable spline\nkernels share the same factor $U$ and $W$ admits a closed form representation\nin terms of the kernel hyperparameter, making the factorization computationally\ninexpensive. Maximum likelihood properties of the stable spline kernel are also\nhighlighted. These results can be applied both to improve the stability and to\nreduce the computational complexity associated with the computation of stable\nspline estimators.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jun 2014 11:38:59 GMT"}, {"version": "v2", "created": "Sun, 21 Sep 2014 22:03:06 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Carli", "Francesca Paola", ""]]}, {"id": "1406.5736", "submitter": "Chao Ding", "authors": "Chao Ding and Hou-Duo Qi", "title": "Convex Optimization Learning of Faithful Euclidean Distance\n  Representations in Nonlinear Dimensionality Reduction", "comments": "44 pages, 10 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical multidimensional scaling only works well when the noisy distances\nobserved in a high dimensional space can be faithfully represented by Euclidean\ndistances in a low dimensional space. Advanced models such as Maximum Variance\nUnfolding (MVU) and Minimum Volume Embedding (MVE) use Semi-Definite\nProgramming (SDP) to reconstruct such faithful representations. While those SDP\nmodels are capable of producing high quality configuration numerically, they\nsuffer two major drawbacks. One is that there exist no theoretically guaranteed\nbounds on the quality of the configuration. The other is that they are slow in\ncomputation when the data points are beyond moderate size. In this paper, we\npropose a convex optimization model of Euclidean distance matrices. We\nestablish a non-asymptotic error bound for the random graph model with\nsub-Gaussian noise, and prove that our model produces a matrix estimator of\nhigh accuracy when the order of the uniform sample size is roughly the degree\nof freedom of a low-rank matrix up to a logarithmic factor. Our results\npartially explain why MVU and MVE often work well. Moreover, we develop a fast\ninexact accelerated proximal gradient method. Numerical experiments show that\nthe model can produce configurations of high quality on large data points that\nthe SDP approach would struggle to cope with.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jun 2014 15:34:15 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Ding", "Chao", ""], ["Qi", "Hou-Duo", ""]]}, {"id": "1406.5752", "submitter": "Tianyi Zhou", "authors": "Tianyi Zhou and Jeff Bilmes and Carlos Guestrin", "title": "Divide-and-Conquer Learning by Anchoring a Conical Hull", "comments": "26 pages, long version, in updating", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We reduce a broad class of machine learning problems, usually addressed by EM\nor sampling, to the problem of finding the $k$ extremal rays spanning the\nconical hull of a data point set. These $k$ \"anchors\" lead to a global solution\nand a more interpretable model that can even outperform EM and sampling on\ngeneralization error. To find the $k$ anchors, we propose a novel\ndivide-and-conquer learning scheme \"DCA\" that distributes the problem to\n$\\mathcal O(k\\log k)$ same-type sub-problems on different low-D random\nhyperplanes, each can be solved by any solver. For the 2D sub-problem, we\npresent a non-iterative solver that only needs to compute an array of cosine\nvalues and its max/min entries. DCA also provides a faster subroutine for other\nmethods to check whether a point is covered in a conical hull, which improves\nalgorithm design in multiple dimensions and brings significant speedup to\nlearning. We apply our method to GMM, HMM, LDA, NMF and subspace clustering,\nthen show its competitive performance and scalability over other methods on\nrich datasets.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jun 2014 19:16:20 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Zhou", "Tianyi", ""], ["Bilmes", "Jeff", ""], ["Guestrin", "Carlos", ""]]}, {"id": "1406.5765", "submitter": "Ming Jin", "authors": "Ming Jin, Han Zou, Kevin Weekly, Ruoxi Jia, Alexandre M. Bayen, and\n  Costas J. Spanos", "title": "Environmental Sensing by Wearable Device for Indoor Activity and\n  Location Estimation", "comments": "submitted to the 40th Annual Conference of the IEEE Industrial\n  Electronics Society (IECON)", "journal-ref": null, "doi": "10.1109/IECON.2014.7049320", "report-no": null, "categories": "cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present results from a set of experiments in this pilot study to\ninvestigate the causal influence of user activity on various environmental\nparameters monitored by occupant carried multi-purpose sensors. Hypotheses with\nrespect to each type of measurements are verified, including temperature,\nhumidity, and light level collected during eight typical activities: sitting in\nlab / cubicle, indoor walking / running, resting after physical activity,\nclimbing stairs, taking elevators, and outdoor walking. Our main contribution\nis the development of features for activity and location recognition based on\nenvironmental measurements, which exploit location- and activity-specific\ncharacteristics and capture the trends resulted from the underlying\nphysiological process. The features are statistically shown to have good\nseparability and are also information-rich. Fusing environmental sensing\ntogether with acceleration is shown to achieve classification accuracy as high\nas 99.13%. For building applications, this study motivates a sensor fusion\nparadigm for learning individualized activity, location, and environmental\npreferences for energy management and user comfort.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jun 2014 21:13:58 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Jin", "Ming", ""], ["Zou", "Han", ""], ["Weekly", "Kevin", ""], ["Jia", "Ruoxi", ""], ["Bayen", "Alexandre M.", ""], ["Spanos", "Costas J.", ""]]}, {"id": "1406.5979", "submitter": "Stephane Ross", "authors": "Stephane Ross, J. Andrew Bagnell", "title": "Reinforcement and Imitation Learning via Interactive No-Regret Learning", "comments": "14 pages. Under review for NIPS 2014 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has demonstrated that problems-- particularly imitation learning\nand structured prediction-- where a learner's predictions influence the\ninput-distribution it is tested on can be naturally addressed by an interactive\napproach and analyzed using no-regret online learning. These approaches to\nimitation learning, however, neither require nor benefit from information about\nthe cost of actions. We extend existing results in two directions: first, we\ndevelop an interactive imitation learning approach that leverages cost\ninformation; second, we extend the technique to address reinforcement learning.\nThe results provide theoretical support to the commonly observed successes of\nonline approximate policy iteration. Our approach suggests a broad new family\nof algorithms and provides a unifying view of existing techniques for imitation\nand reinforcement learning.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jun 2014 17:00:28 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Ross", "Stephane", ""], ["Bagnell", "J. Andrew", ""]]}, {"id": "1406.5986", "submitter": "Garvesh Raskutti", "authors": "Garvesh Raskutti and Michael Mahoney", "title": "A Statistical Perspective on Randomized Sketching for Ordinary\n  Least-Squares", "comments": "27 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider statistical as well as algorithmic aspects of solving large-scale\nleast-squares (LS) problems using randomized sketching algorithms. For a LS\nproblem with input data $(X, Y) \\in \\mathbb{R}^{n \\times p} \\times\n\\mathbb{R}^n$, sketching algorithms use a sketching matrix, $S\\in\\mathbb{R}^{r\n\\times n}$ with $r \\ll n$. Then, rather than solving the LS problem using the\nfull data $(X,Y)$, sketching algorithms solve the LS problem using only the\nsketched data $(SX, SY)$. Prior work has typically adopted an algorithmic\nperspective, in that it has made no statistical assumptions on the input $X$\nand $Y$, and instead it has been assumed that the data $(X,Y)$ are fixed and\nworst-case (WC). Prior results show that, when using sketching matrices such as\nrandom projections and leverage-score sampling algorithms, with $p < r \\ll n$,\nthe WC error is the same as solving the original problem, up to a small\nconstant. From a statistical perspective, we typically consider the\nmean-squared error performance of randomized sketching algorithms, when data\n$(X, Y)$ are generated according to a statistical model $Y = X \\beta +\n\\epsilon$, where $\\epsilon$ is a noise process. We provide a rigorous\ncomparison of both perspectives leading to insights on how they differ. To do\nthis, we first develop a framework for assessing algorithmic and statistical\naspects of randomized sketching methods. We then consider the statistical\nprediction efficiency (PE) and the statistical residual efficiency (RE) of the\nsketched LS estimator; and we use our framework to provide upper bounds for\nseveral types of random projection and random sampling sketching algorithms.\nAmong other results, we show that the RE can be upper bounded when $p < r \\ll\nn$ while the PE typically requires the sample size $r$ to be substantially\nlarger. Lower bounds developed in subsequent results show that our upper bounds\non PE can not be improved.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jun 2014 17:12:12 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2015 16:39:06 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Raskutti", "Garvesh", ""], ["Mahoney", "Michael", ""]]}, {"id": "1406.6038", "submitter": "Dirk Tasche", "authors": "Dirk Tasche", "title": "Exact fit of simple finite mixture models", "comments": "16 pages, 2 tables, some corrections, section on cost quantification\n  inserted", "journal-ref": "Journal of Risk and Financial Management 7(4), 150-164, 2014", "doi": "10.3390/jrfm7040150", "report-no": null, "categories": "stat.ML q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to forecast next year's portfolio-wide credit default rate based on last\nyear's default observations and the current score distribution? A classical\napproach to this problem consists of fitting a mixture of the conditional score\ndistributions observed last year to the current score distribution. This is a\nspecial (simple) case of a finite mixture model where the mixture components\nare fixed and only the weights of the components are estimated. The optimum\nweights provide a forecast of next year's portfolio-wide default rate. We point\nout that the maximum-likelihood (ML) approach to fitting the mixture\ndistribution not only gives an optimum but even an exact fit if we allow the\nmixture components to vary but keep their density ratio fix. From this\nobservation we can conclude that the standard default rate forecast based on\nlast year's conditional default rates will always be located between last\nyear's portfolio-wide default rate and the ML forecast for next year. As an\napplication example, then cost quantification is discussed. We also discuss how\nthe mixture model based estimation methods can be used to forecast total loss.\nThis involves the reinterpretation of an individual classification problem as a\ncollective quantification problem.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jun 2014 19:45:40 GMT"}, {"version": "v2", "created": "Wed, 23 Jul 2014 20:33:26 GMT"}], "update_date": "2014-11-21", "authors_parsed": [["Tasche", "Dirk", ""]]}, {"id": "1406.6145", "submitter": "Tyler Maunu", "authors": "Gilad Lerman and Tyler Maunu", "title": "Fast, Robust and Non-convex Subspace Recovery", "comments": null, "journal-ref": "Information and Inference: A Journal of the IMA 7 (2018) 277-336", "doi": "10.1093/imaiai/iax012", "report-no": null, "categories": "cs.LG cs.CV stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a fast and non-convex algorithm for robust subspace\nrecovery. The data sets considered include inliers drawn around a\nlow-dimensional subspace of a higher dimensional ambient space, and a possibly\nlarge portion of outliers that do not lie nearby this subspace. The proposed\nalgorithm, which we refer to as Fast Median Subspace (FMS), is designed to\nrobustly determine the underlying subspace of such data sets, while having\nlower computational complexity than existing methods. We prove convergence of\nthe FMS iterates to a stationary point. Further, under a special model of data,\nFMS converges to a point which is near to the global minimum with overwhelming\nprobability. Under this model, we show that the iteration complexity is\nglobally bounded and locally $r$-linear. The latter theorem holds for any fixed\nfraction of outliers (less than 1) and any fixed positive distance between the\nlimit point and the global minimum. Numerical experiments on synthetic and real\ndata demonstrate its competitive speed and accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jun 2014 06:15:07 GMT"}, {"version": "v2", "created": "Thu, 9 Jun 2016 22:58:10 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Lerman", "Gilad", ""], ["Maunu", "Tyler", ""]]}, {"id": "1406.6200", "submitter": "Thijs van Ommen", "authors": "Thijs van Ommen", "title": "Combining predictions from linear models when training and test inputs\n  differ", "comments": "12 pages, 2 figures. To appear in Proceedings of the 30th Conference\n  on Uncertainty in Artificial Intelligence (UAI2014). This version includes\n  the supplementary material (regularity assumptions, proofs)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for combining predictions from different models in a supervised\nlearning setting must somehow estimate/predict the quality of a model's\npredictions at unknown future inputs. Many of these methods (often implicitly)\nmake the assumption that the test inputs are identical to the training inputs,\nwhich is seldom reasonable. By failing to take into account that prediction\nwill generally be harder for test inputs that did not occur in the training\nset, this leads to the selection of too complex models. Based on a novel,\nunbiased expression for KL divergence, we propose XAIC and its special case\nFAIC as versions of AIC intended for prediction that use different degrees of\nknowledge of the test inputs. Both methods substantially differ from and may\noutperform all the known versions of AIC even when the training and test inputs\nare iid, and are especially useful for deterministic inputs and under covariate\nshift. Our experiments on linear models suggest that if the test and training\ninputs differ substantially, then XAIC and FAIC predictively outperform AIC,\nBIC and several other methods including Bayesian model averaging.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jun 2014 10:56:13 GMT"}], "update_date": "2014-06-25", "authors_parsed": [["van Ommen", "Thijs", ""]]}, {"id": "1406.6247", "submitter": "Volodymyr Mnih", "authors": "Volodymyr Mnih, Nicolas Heess, Alex Graves, Koray Kavukcuoglu", "title": "Recurrent Models of Visual Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying convolutional neural networks to large images is computationally\nexpensive because the amount of computation scales linearly with the number of\nimage pixels. We present a novel recurrent neural network model that is capable\nof extracting information from an image or video by adaptively selecting a\nsequence of regions or locations and only processing the selected regions at\nhigh resolution. Like convolutional neural networks, the proposed model has a\ndegree of translation invariance built-in, but the amount of computation it\nperforms can be controlled independently of the input image size. While the\nmodel is non-differentiable, it can be trained using reinforcement learning\nmethods to learn task-specific policies. We evaluate our model on several image\nclassification tasks, where it significantly outperforms a convolutional neural\nnetwork baseline on cluttered images, and on a dynamic visual control problem,\nwhere it learns to track a simple object without an explicit training signal\nfor doing so.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jun 2014 14:16:56 GMT"}], "update_date": "2014-06-25", "authors_parsed": [["Mnih", "Volodymyr", ""], ["Heess", "Nicolas", ""], ["Graves", "Alex", ""], ["Kavukcuoglu", "Koray", ""]]}, {"id": "1406.6288", "submitter": "Jean-Michel Marin", "authors": "Pierre Pudlo, Jean-Michel Marin (IMAG and IBC, Universite de\n  Montpellier), Arnaud Estoup, Jean-Marie Cornuet, Mathieu Gautier (CBGP, INRA,\n  Montpellier), Christian P. Robert (Universite Paris-Dauphine and University\n  of Warwick)", "title": "Reliable ABC model choice via random forests", "comments": "39 pages, 15 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.PE stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) methods provide an elaborate approach\nto Bayesian inference on complex models, including model choice. Both\ntheoretical arguments and simulation experiments indicate, however, that model\nposterior probabilities may be poorly evaluated by standard ABC techniques. We\npropose a novel approach based on a machine learning tool named random forests\nto conduct selection among the highly complex models covered by ABC algorithms.\nWe thus modify the way Bayesian model selection is both understood and\noperated, in that we rephrase the inferential goal as a classification problem,\nfirst predicting the model that best fits the data with random forests and\npostponing the approximation of the posterior probability of the predicted MAP\nfor a second stage also relying on random forests. Compared with earlier\nimplementations of ABC model choice, the ABC random forest approach offers\nseveral potential improvements: (i) it often has a larger discriminative power\namong the competing models, (ii) it is more robust against the number and\nchoice of statistics summarizing the data, (iii) the computing effort is\ndrastically reduced (with a gain in computation efficiency of at least fifty),\nand (iv) it includes an approximation of the posterior probability of the\nselected model. The call to random forests will undoubtedly extend the range of\nsize of datasets and complexity of models that ABC can handle. We illustrate\nthe power of this novel methodology by analyzing controlled experiments as well\nas genuine population genetics datasets. The proposed methodologies are\nimplemented in the R package abcrf available on the CRAN.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jun 2014 16:03:32 GMT"}, {"version": "v2", "created": "Mon, 29 Sep 2014 09:45:03 GMT"}, {"version": "v3", "created": "Wed, 2 Sep 2015 15:19:41 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Pudlo", "Pierre", "", "IMAG and IBC, Universite de\n  Montpellier"], ["Marin", "Jean-Michel", "", "IMAG and IBC, Universite de\n  Montpellier"], ["Estoup", "Arnaud", "", "CBGP, INRA,\n  Montpellier"], ["Cornuet", "Jean-Marie", "", "CBGP, INRA,\n  Montpellier"], ["Gautier", "Mathieu", "", "CBGP, INRA,\n  Montpellier"], ["Robert", "Christian P.", "", "Universite Paris-Dauphine and University\n  of Warwick"]]}, {"id": "1406.6314", "submitter": "Frank Nielsen", "authors": "Frank Nielsen and Richard Nock", "title": "Further heuristics for $k$-means: The merge-and-split heuristic and the\n  $(k,l)$-means", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the optimal $k$-means clustering is NP-hard in general and many\nheuristics have been designed for minimizing monotonically the $k$-means\nobjective. We first show how to extend Lloyd's batched relocation heuristic and\nHartigan's single-point relocation heuristic to take into account empty-cluster\nand single-point cluster events, respectively. Those events tend to\nincreasingly occur when $k$ or $d$ increases, or when performing several\nrestarts. First, we show that those special events are a blessing because they\nallow to partially re-seed some cluster centers while further minimizing the\n$k$-means objective function. Second, we describe a novel heuristic,\nmerge-and-split $k$-means, that consists in merging two clusters and splitting\nthis merged cluster again with two new centers provided it improves the\n$k$-means objective. This novel heuristic can improve Hartigan's $k$-means when\nit has converged to a local minimum. We show empirically that this\nmerge-and-split $k$-means improves over the Hartigan's heuristic which is the\n{\\em de facto} method of choice. Finally, we propose the $(k,l)$-means\nobjective that generalizes the $k$-means objective by associating the data\npoints to their $l$ closest cluster centers, and show how to either directly\nconvert or iteratively relax the $(k,l)$-means into a $k$-means in order to\nreach better local minima.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jun 2014 02:34:34 GMT"}], "update_date": "2014-06-25", "authors_parsed": [["Nielsen", "Frank", ""], ["Nock", "Richard", ""]]}, {"id": "1406.6315", "submitter": "Nam Lee", "authors": "Nam H. Lee and I-Jeng Wang and Youngser Park and Care E. Priebe and\n  Michael Rosen", "title": "Automatic Dimension Selection for a Non-negative Factorization Approach\n  to Clustering Multiple Random Graphs", "comments": "This paper has been withdrawn by the author due to a newer version\n  with overlapping contents", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a problem of grouping multiple graphs into several clusters using\nsingular value thesholding and non-negative factorization. We derive a model\nselection information criterion to estimate the number of clusters. We\ndemonstrate our approach using \"Swimmer data set\" as well as simulated data\nset, and compare its performance with two standard clustering algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jun 2014 17:15:11 GMT"}, {"version": "v2", "created": "Tue, 9 Sep 2014 13:53:25 GMT"}], "update_date": "2014-09-10", "authors_parsed": [["Lee", "Nam H.", ""], ["Wang", "I-Jeng", ""], ["Park", "Youngser", ""], ["Priebe", "Care E.", ""], ["Rosen", "Michael", ""]]}, {"id": "1406.6319", "submitter": "Nam Lee", "authors": "Nam H. Lee and Carey Priebe and Youngser Park and I-Jeng Wang and\n  Michael Rosen", "title": "Techniques for clustering interaction data as a collection of graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A natural approach to analyze interaction data of form\n\"what-connects-to-what-when\" is to create a time-series (or rather a sequence)\nof graphs through temporal discretization (bandwidth selection) and spatial\ndiscretization (vertex contraction). Such discretization together with\nnon-negative factorization techniques can be useful for obtaining clustering of\ngraphs. Motivating application of performing clustering of graphs (as opposed\nto vertex clustering) can be found in neuroscience and in social network\nanalysis, and it can also be used to enhance community detection (i.e., vertex\nclustering) by way of conditioning on the cluster labels. In this paper, we\nformulate a problem of clustering of graphs as a model selection problem. Our\napproach involves information criteria, non-negative matrix factorization and\nsingular value thresholding, and we illustrate our techniques using real and\nsimulated data.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jun 2014 17:26:18 GMT"}, {"version": "v2", "created": "Tue, 9 Sep 2014 13:53:10 GMT"}, {"version": "v3", "created": "Sat, 10 Jan 2015 22:06:33 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Lee", "Nam H.", ""], ["Priebe", "Carey", ""], ["Park", "Youngser", ""], ["Wang", "I-Jeng", ""], ["Rosen", "Michael", ""]]}, {"id": "1406.6603", "submitter": "Marco Prato", "authors": "Silvia Bonettini and Alessandro Chiuso and Marco Prato", "title": "A scaled gradient projection method for Bayesian learning in dynamical\n  systems", "comments": null, "journal-ref": "SIAM Journal on Scientific Computing 37 (2015), A1297-A1318", "doi": "10.1137/140973529", "report-no": null, "categories": "math.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A crucial task in system identification problems is the selection of the most\nappropriate model class, and is classically addressed resorting to\ncross-validation or using asymptotic arguments. As recently suggested in the\nliterature, this can be addressed in a Bayesian framework, where model\ncomplexity is regulated by few hyperparameters, which can be estimated via\nmarginal likelihood maximization. It is thus of primary importance to design\neffective optimization methods to solve the corresponding optimization problem.\nIf the unknown impulse response is modeled as a Gaussian process with a\nsuitable kernel, the maximization of the marginal likelihood leads to a\nchallenging nonconvex optimization problem, which requires a stable and\neffective solution strategy. In this paper we address this problem by means of\na scaled gradient projection algorithm, in which the scaling matrix and the\nsteplength parameter play a crucial role to provide a meaning solution in a\ncomputational time comparable with second order methods. In particular, we\npropose both a generalization of the split gradient approach to design the\nscaling matrix in the presence of box constraints, and an effective\nimplementation of the gradient and objective function. The extensive numerical\nexperiments carried out on several test problems show that our method is very\neffective in providing in few tenths of a second solutions of the problems with\naccuracy comparable with state-of-the-art approaches. Moreover, the flexibility\nof the proposed strategy makes it easily adaptable to a wider range of problems\narising in different areas of machine learning, signal processing and system\nidentification.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jun 2014 15:12:48 GMT"}, {"version": "v2", "created": "Fri, 28 Nov 2014 14:15:56 GMT"}, {"version": "v3", "created": "Mon, 2 Feb 2015 11:25:41 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Bonettini", "Silvia", ""], ["Chiuso", "Alessandro", ""], ["Prato", "Marco", ""]]}, {"id": "1406.6618", "submitter": "Nihar Shah", "authors": "Nihar B. Shah, Sivaraman Balakrishnan, Joseph Bradley, Abhay Parekh,\n  Kannan Ramchandran, Martin Wainwright", "title": "When is it Better to Compare than to Score?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When eliciting judgements from humans for an unknown quantity, one often has\nthe choice of making direct-scoring (cardinal) or comparative (ordinal)\nmeasurements. In this paper we study the relative merits of either choice,\nproviding empirical and theoretical guidelines for the selection of a\nmeasurement scheme. We provide empirical evidence based on experiments on\nAmazon Mechanical Turk that in a variety of tasks, (pairwise-comparative)\nordinal measurements have lower per sample noise and are typically faster to\nelicit than cardinal ones. Ordinal measurements however typically provide less\ninformation. We then consider the popular Thurstone and Bradley-Terry-Luce\n(BTL) models for ordinal measurements and characterize the minimax error rates\nfor estimating the unknown quantity. We compare these minimax error rates to\nthose under cardinal measurement models and quantify for what noise levels\nordinal measurements are better. Finally, we revisit the data collected from\nour experiments and show that fitting these models confirms this prediction:\nfor tasks where the noise in ordinal measurements is sufficiently low, the\nordinal approach results in smaller errors in the estimation.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jun 2014 15:48:41 GMT"}], "update_date": "2014-06-26", "authors_parsed": [["Shah", "Nihar B.", ""], ["Balakrishnan", "Sivaraman", ""], ["Bradley", "Joseph", ""], ["Parekh", "Abhay", ""], ["Ramchandran", "Kannan", ""], ["Wainwright", "Martin", ""]]}, {"id": "1406.6625", "submitter": "Jiaming Xu", "authors": "Bruce Hajek and Yihong Wu and Jiaming Xu", "title": "Computational Lower Bounds for Community Detection on Random Graphs", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CC stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of detecting the presence of a small dense\ncommunity planted in a large Erd\\H{o}s-R\\'enyi random graph $\\mathcal{G}(N,q)$,\nwhere the edge probability within the community exceeds $q$ by a constant\nfactor. Assuming the hardness of the planted clique detection problem, we show\nthat the computational complexity of detecting the community exhibits the\nfollowing phase transition phenomenon: As the graph size $N$ grows and the\ngraph becomes sparser according to $q=N^{-\\alpha}$, there exists a critical\nvalue of $\\alpha = \\frac{2}{3}$, below which there exists a computationally\nintensive procedure that can detect far smaller communities than any\ncomputationally efficient procedure, and above which a linear-time procedure is\nstatistically optimal. The results also lead to the average-case hardness\nresults for recovering the dense community and approximating the densest\n$K$-subgraph.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jun 2014 16:15:36 GMT"}, {"version": "v2", "created": "Sun, 6 Jul 2014 21:19:16 GMT"}, {"version": "v3", "created": "Wed, 11 Mar 2015 20:21:00 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Hajek", "Bruce", ""], ["Wu", "Yihong", ""], ["Xu", "Jiaming", ""]]}, {"id": "1406.6651", "submitter": "Ishanu Chattopadhyay", "authors": "Ishanu Chattopadhyay", "title": "Causality Networks", "comments": "22 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT q-fin.ST stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While correlation measures are used to discern statistical relationships\nbetween observed variables in almost all branches of data-driven scientific\ninquiry, what we are really interested in is the existence of causal\ndependence. Designing an efficient causality test, that may be carried out in\nthe absence of restrictive pre-suppositions on the underlying dynamical\nstructure of the data at hand, is non-trivial. Nevertheless, ability to\ncomputationally infer statistical prima facie evidence of causal dependence may\nyield a far more discriminative tool for data analysis compared to the\ncalculation of simple correlations. In the present work, we present a new\nnon-parametric test of Granger causality for quantized or symbolic data streams\ngenerated by ergodic stationary sources. In contrast to state-of-art binary\ntests, our approach makes precise and computes the degree of causal dependence\nbetween data streams, without making any restrictive assumptions, linearity or\notherwise. Additionally, without any a priori imposition of specific dynamical\nstructure, we infer explicit generative models of causal cross-dependence,\nwhich may be then used for prediction. These explicit models are represented as\ngeneralized probabilistic automata, referred to crossed automata, and are shown\nto be sufficient to capture a fairly general class of causal dependence. The\nproposed algorithms are computationally efficient in the PAC sense; $i.e.$, we\nfind good models of cross-dependence with high probability, with polynomial\nrun-times and sample complexities. The theoretical results are applied to\nweekly search-frequency data from Google Trends API for a chosen set of\nsocially \"charged\" keywords. The causality network inferred from this dataset\nreveals, quite expectedly, the causal importance of certain keywords. It is\nalso illustrated that correlation analysis fails to gather such insight.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jun 2014 17:46:32 GMT"}], "update_date": "2014-06-26", "authors_parsed": [["Chattopadhyay", "Ishanu", ""]]}, {"id": "1406.6670", "submitter": "Eran Shmaya", "authors": "Nabil Al-Najjar and Eran Shmaya", "title": "Learning the ergodic decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Bayesian agent learns about the structure of a stationary process from ob-\nserving past outcomes. We prove that his predictions about the near future\nbecome ap- proximately those he would have made if he knew the long run\nempirical frequencies of the process.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jun 2014 19:07:52 GMT"}], "update_date": "2014-06-26", "authors_parsed": [["Al-Najjar", "Nabil", ""], ["Shmaya", "Eran", ""]]}, {"id": "1406.6720", "submitter": "Seyed Mostafa Kia", "authors": "Seyed Mostafa Kia", "title": "Mass-Univariate Hypothesis Testing on MEEG Data using Cross-Validation", "comments": "Master thesis, July 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in statistical theory, together with advances in the\ncomputational power of computers, provide alternative methods to do\nmass-univariate hypothesis testing in which a large number of univariate tests,\ncan be properly used to compare MEEG data at a large number of time-frequency\npoints and scalp locations. One of the major problematic aspects of this kind\nof mass-univariate analysis is due to high number of accomplished hypothesis\ntests. Hence procedures that remove or alleviate the increased probability of\nfalse discoveries are crucial for this type of analysis. Here, I propose a new\nmethod for mass-univariate analysis of MEEG data based on cross-validation\nscheme. In this method, I suggest a hierarchical classification procedure under\nk-fold cross-validation to detect which sensors at which time-bin and which\nfrequency-bin contributes in discriminating between two different stimuli or\ntasks. To achieve this goal, a new feature extraction method based on the\ndiscrete cosine transform (DCT) employed to get maximum advantage of all three\ndata dimensions. Employing cross-validation and hierarchy architecture\nalongside the DCT feature space makes this method more reliable and at the same\ntime enough sensitive to detect the narrow effects in brain activities.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jun 2014 22:01:56 GMT"}], "update_date": "2014-06-27", "authors_parsed": [["Kia", "Seyed Mostafa", ""]]}, {"id": "1406.6812", "submitter": "Yasin Abbasi-Yadkori", "authors": "Yasin Abbasi-Yadkori and Gergely Neu", "title": "Online learning in MDPs with side information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study online learning of finite Markov decision process (MDP) problems\nwhen a side information vector is available. The problem is motivated by\napplications such as clinical trials, recommendation systems, etc. Such\napplications have an episodic structure, where each episode corresponds to a\npatient/customer. Our objective is to compete with the optimal dynamic policy\nthat can take side information into account.\n  We propose a computationally efficient algorithm and show that its regret is\nat most $O(\\sqrt{T})$, where $T$ is the number of rounds. To best of our\nknowledge, this is the first regret bound for this setting.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jun 2014 08:57:05 GMT"}], "update_date": "2014-06-27", "authors_parsed": [["Abbasi-Yadkori", "Yasin", ""], ["Neu", "Gergely", ""]]}, {"id": "1406.6832", "submitter": "Michel Plantie", "authors": "Michel Crampes and Michel Planti\\'e", "title": "Overlapping Community Detection Optimization and Nash Equilibrium", "comments": "Submitted to KDD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection using both graphs and social networks is the focus of\nmany algorithms. Recent methods aimed at optimizing the so-called modularity\nfunction proceed by maximizing relations within communities while minimizing\ninter-community relations.\n  However, given the NP-completeness of the problem, these algorithms are\nheuristics that do not guarantee an optimum. In this paper, we introduce a new\nalgorithm along with a function that takes an approximate solution and modifies\nit in order to reach an optimum. This reassignment function is considered a\n'potential function' and becomes a necessary condition to asserting that the\ncomputed optimum is indeed a Nash Equilibrium. We also use this function to\nsimultaneously show partitioning and overlapping communities, two detection and\nvisualization modes of great value in revealing interesting features of a\nsocial network. Our approach is successfully illustrated through several\nexperiments on either real unipartite, multipartite or directed graphs of\nmedium and large-sized datasets.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jun 2014 10:28:36 GMT"}], "update_date": "2014-06-27", "authors_parsed": [["Crampes", "Michel", ""], ["Planti\u00e9", "Michel", ""]]}, {"id": "1406.6897", "submitter": "Jiaming Xu", "authors": "Jiaming Xu and Laurent Massouli\\'e and Marc Lelarge", "title": "Edge Label Inference in Generalized Stochastic Block Models: from\n  Spectral Theory to Impossibility Results", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical setting of community detection consists of networks exhibiting\na clustered structure. To more accurately model real systems we consider a\nclass of networks (i) whose edges may carry labels and (ii) which may lack a\nclustered structure. Specifically we assume that nodes possess latent\nattributes drawn from a general compact space and edges between two nodes are\nrandomly generated and labeled according to some unknown distribution as a\nfunction of their latent attributes. Our goal is then to infer the edge label\ndistributions from a partially observed network. We propose a computationally\nefficient spectral algorithm and show it allows for asymptotically correct\ninference when the average node degree could be as low as logarithmic in the\ntotal number of nodes. Conversely, if the average node degree is below a\nspecific constant threshold, we show that no algorithm can achieve better\ninference than guessing without using the observations. As a byproduct of our\nanalysis, we show that our model provides a general procedure to construct\nrandom graph models with a spectrum asymptotic to a pre-specified eigenvalue\ndistribution such as a power-law distribution.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jun 2014 14:22:54 GMT"}], "update_date": "2014-06-27", "authors_parsed": [["Xu", "Jiaming", ""], ["Massouli\u00e9", "Laurent", ""], ["Lelarge", "Marc", ""]]}, {"id": "1406.7130", "submitter": "Thomas Bonis", "authors": "Thomas Bonis, Steve Oudot", "title": "A Fuzzy Clustering Algorithm for the Mode Seeking Framework", "comments": "Submitted to Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new fuzzy clustering algorithm based on the\nmode-seeking framework. Given a dataset in $\\mathbb{R}^d$, we define regions of\nhigh density that we call cluster cores. We then consider a random walk on a\nneighborhood graph built on top of our data points which is designed to be\nattracted by high density regions. The strength of this attraction is\ncontrolled by a temperature parameter $\\beta > 0$. The membership of a point to\na given cluster is then the probability for the random walk to hit the\ncorresponding cluster core before any other. While many properties of random\nwalks (such as hitting times, commute distances, etc\\dots) have been shown to\nenventually encode purely local information when the number of data points\ngrows, we show that the regularization introduced by the use of cluster cores\nsolves this issue. Empirically, we show how the choice of $\\beta$ influences\nthe behavior of our algorithm: for small values of $\\beta$ the result is close\nto hard mode-seeking whereas when $\\beta$ is close to $1$ the result is similar\nto the output of a (fuzzy) spectral clustering. Finally, we demonstrate the\nscalability of our approach by providing the fuzzy clustering of a protein\nconfiguration dataset containing a million data points in $30$ dimensions.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jun 2014 09:48:05 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2015 14:31:22 GMT"}, {"version": "v3", "created": "Mon, 20 Jun 2016 07:42:50 GMT"}, {"version": "v4", "created": "Wed, 22 Jun 2016 12:21:39 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Bonis", "Thomas", ""], ["Oudot", "Steve", ""]]}, {"id": "1406.7179", "submitter": "Alex Susemihl", "authors": "Alex Susemihl, Ron Meir, Manfred Opper", "title": "Optimal Population Codes for Control and Estimation", "comments": "9 Pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Agents acting in the natural world aim at selecting appropriate actions based\non noisy and partial sensory observations. Many behaviors leading to decision\nmak- ing and action selection in a closed loop setting are naturally phrased\nwithin a control theoretic framework. Within the framework of optimal Control\nTheory, one is usually given a cost function which is minimized by selecting a\ncontrol law based on the observations. While in standard control settings the\nsensors are assumed fixed, biological systems often gain from the extra\nflexibility of optimiz- ing the sensors themselves. However, this sensory\nadaptation is geared towards control rather than perception, as is often\nassumed. In this work we show that sen- sory adaptation for control differs\nfrom sensory adaptation for perception, even for simple control setups. This\nimplies, consistently with recent experimental results, that when studying\nsensory adaptation, it is essential to account for the task being performed.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jun 2014 13:46:05 GMT"}], "update_date": "2014-06-30", "authors_parsed": [["Susemihl", "Alex", ""], ["Meir", "Ron", ""], ["Opper", "Manfred", ""]]}, {"id": "1406.7250", "submitter": "Shankar Vembu", "authors": "Amit G. Deshwar, Shankar Vembu, Christina K. Yung, Gun Ho Jang,\n  Lincoln Stein, Quaid Morris", "title": "Reconstructing subclonal composition and evolution from whole genome\n  sequencing of tumors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tumors often contain multiple subpopulations of cancerous cells defined by\ndistinct somatic mutations. We describe a new method, PhyloWGS, that can be\napplied to WGS data from one or more tumor samples to reconstruct complete\ngenotypes of these subpopulations based on variant allele frequencies (VAFs) of\npoint mutations and population frequencies of structural variations. We\nintroduce a principled phylogenic correction for VAFs in loci affected by copy\nnumber alterations and we show that this correction greatly improves subclonal\nreconstruction compared to existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jun 2014 18:01:20 GMT"}, {"version": "v2", "created": "Mon, 27 Oct 2014 19:24:52 GMT"}, {"version": "v3", "created": "Tue, 6 Jan 2015 22:05:57 GMT"}], "update_date": "2015-01-08", "authors_parsed": [["Deshwar", "Amit G.", ""], ["Vembu", "Shankar", ""], ["Yung", "Christina K.", ""], ["Jang", "Gun Ho", ""], ["Stein", "Lincoln", ""], ["Morris", "Quaid", ""]]}, {"id": "1406.7321", "submitter": "Kai Zhong", "authors": "Kai Zhong, Ian E.H. Yen, Inderjit S. Dhillon, Pradeep Ravikumar", "title": "Proximal Quasi-Newton for Computationally Intensive L1-regularized\n  M-estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the class of optimization problems arising from computationally\nintensive L1-regularized M-estimators, where the function or gradient values\nare very expensive to compute. A particular instance of interest is the\nL1-regularized MLE for learning Conditional Random Fields (CRFs), which are a\npopular class of statistical models for varied structured prediction problems\nsuch as sequence labeling, alignment, and classification with label taxonomy.\nL1-regularized MLEs for CRFs are particularly expensive to optimize since\ncomputing the gradient values requires an expensive inference step. In this\nwork, we propose the use of a carefully constructed proximal quasi-Newton\nalgorithm for such computationally intensive M-estimation problems, where we\nemploy an aggressive active set selection technique. In a key contribution of\nthe paper, we show that the proximal quasi-Newton method is provably\nsuper-linearly convergent, even in the absence of strong convexity, by\nleveraging a restricted variant of strong convexity. In our experiments, the\nproposed algorithm converges considerably faster than current state-of-the-art\non the problems of sequence labeling and hierarchical classification.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jun 2014 21:29:17 GMT"}, {"version": "v2", "created": "Fri, 23 Jan 2015 17:45:18 GMT"}], "update_date": "2015-01-26", "authors_parsed": [["Zhong", "Kai", ""], ["Yen", "Ian E. H.", ""], ["Dhillon", "Inderjit S.", ""], ["Ravikumar", "Pradeep", ""]]}, {"id": "1406.7349", "submitter": "Yitan Zhu", "authors": "Yitan Zhu, Niya Wang, David J. Miller, and Yue Wang", "title": "Convex Analysis of Mixtures for Separating Non-negative Well-grounded\n  Sources", "comments": "15 pages, 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind Source Separation (BSS) has proven to be a powerful tool for the\nanalysis of composite patterns in engineering and science. We introduce Convex\nAnalysis of Mixtures (CAM) for separating non-negative well-grounded sources,\nwhich learns the mixing matrix by identifying the lateral edges of the convex\ndata scatter plot. We prove a sufficient and necessary condition for\nidentifying the mixing matrix through edge detection, which also serves as the\nfoundation for CAM to be applied not only to the exact-determined and\nover-determined cases, but also to the under-determined case. We show the\noptimality of the edge detection strategy, even for cases where source\nwell-groundedness is not strictly satisfied. The CAM algorithm integrates\nplug-in noise filtering using sector-based clustering, an efficient geometric\nconvex analysis scheme, and stability-based model order selection. We\ndemonstrate the principle of CAM on simulated data and numerically mixed\nnatural images. The superior performance of CAM against a panel of benchmark\nBSS techniques is demonstrated on numerically mixed gene expression data. We\nthen apply CAM to dissect dynamic contrast-enhanced magnetic resonance imaging\ndata taken from breast tumors and time-course microarray gene expression data\nderived from in-vivo muscle regeneration in mice, both producing biologically\nplausible decomposition results.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jun 2014 03:09:54 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2015 22:06:54 GMT"}, {"version": "v3", "created": "Thu, 10 Dec 2015 21:19:50 GMT"}], "update_date": "2015-12-14", "authors_parsed": [["Zhu", "Yitan", ""], ["Wang", "Niya", ""], ["Miller", "David J.", ""], ["Wang", "Yue", ""]]}, {"id": "1406.7362", "submitter": "KyungHyun Cho", "authors": "Kyunghyun Cho and Yoshua Bengio", "title": "Exponentially Increasing the Capacity-to-Computation Ratio for\n  Conditional Computation in Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many state-of-the-art results obtained with deep networks are achieved with\nthe largest models that could be trained, and if more computation power was\navailable, we might be able to exploit much larger datasets in order to improve\ngeneralization ability. Whereas in learning algorithms such as decision trees\nthe ratio of capacity (e.g., the number of parameters) to computation is very\nfavorable (up to exponentially more parameters than computation), the ratio is\nessentially 1 for deep neural networks. Conditional computation has been\nproposed as a way to increase the capacity of a deep neural network without\nincreasing the amount of computation required, by activating some parameters\nand computation \"on-demand\", on a per-example basis. In this note, we propose a\nnovel parametrization of weight matrices in neural networks which has the\npotential to increase up to exponentially the ratio of the number of parameters\nto computation. The proposed approach is based on turning on some parameters\n(weight matrices) when specific bit patterns of hidden unit activations are\nobtained. In order to better control for the overfitting that might result, we\npropose a parametrization that is tree-structured, where each node of the tree\ncorresponds to a prefix of a sequence of sign bits, or gating units, associated\nwith hidden units.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jun 2014 06:45:51 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Cho", "Kyunghyun", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1406.7443", "submitter": "Zheng Wen", "authors": "Zheng Wen, Branislav Kveton, and Azin Ashkan", "title": "Efficient Learning in Large-Scale Combinatorial Semi-Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A stochastic combinatorial semi-bandit is an online learning problem where at\neach step a learning agent chooses a subset of ground items subject to\ncombinatorial constraints, and then observes stochastic weights of these items\nand receives their sum as a payoff. In this paper, we consider efficient\nlearning in large-scale combinatorial semi-bandits with linear generalization,\nand as a solution, propose two learning algorithms called Combinatorial Linear\nThompson Sampling (CombLinTS) and Combinatorial Linear UCB (CombLinUCB). Both\nalgorithms are computationally efficient as long as the offline version of the\ncombinatorial problem can be solved efficiently. We establish that CombLinTS\nand CombLinUCB are also provably statistically efficient under reasonable\nassumptions, by developing regret bounds that are independent of the problem\nscale (number of items) and sublinear in time. We also evaluate CombLinTS on a\nvariety of problems with thousands of items. Our experiment results demonstrate\nthat CombLinTS is scalable, robust to the choice of algorithm parameters, and\nsignificantly outperforms the best of our baselines.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jun 2014 21:50:56 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2015 06:38:56 GMT"}, {"version": "v3", "created": "Mon, 8 Jun 2015 06:35:54 GMT"}, {"version": "v4", "created": "Tue, 31 Jan 2017 05:32:13 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Wen", "Zheng", ""], ["Kveton", "Branislav", ""], ["Ashkan", "Azin", ""]]}, {"id": "1406.7498", "submitter": "Aditya Gopalan", "authors": "Aditya Gopalan, Shie Mannor", "title": "Thompson Sampling for Learning Parameterized Markov Decision Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider reinforcement learning in parameterized Markov Decision Processes\n(MDPs), where the parameterization may induce correlation across transition\nprobabilities or rewards. Consequently, observing a particular state transition\nmight yield useful information about other, unobserved, parts of the MDP. We\npresent a version of Thompson sampling for parameterized reinforcement learning\nproblems, and derive a frequentist regret bound for priors over general\nparameter spaces. The result shows that the number of instants where suboptimal\nactions are chosen scales logarithmically with time, with high probability. It\nholds for prior distributions that put significant probability near the true\nmodel, without any additional, specific closed-form structure such as conjugate\nor product-form priors. The constant factor in the logarithmic scaling encodes\nthe information complexity of learning the MDP in terms of the Kullback-Leibler\ngeometry of the parameter space.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jun 2014 12:34:45 GMT"}, {"version": "v2", "created": "Fri, 20 Feb 2015 05:29:21 GMT"}, {"version": "v3", "created": "Tue, 31 Mar 2015 07:37:46 GMT"}], "update_date": "2015-04-01", "authors_parsed": [["Gopalan", "Aditya", ""], ["Mannor", "Shie", ""]]}, {"id": "1406.7536", "submitter": "Giuseppe Vinci", "authors": "Giuseppe Vinci, Peter Freeman, Jeffrey Newman, Larry Wasserman and\n  Christopher Genovese", "title": "Estimating the distribution of Galaxy Morphologies on a continuous space", "comments": "4 pages, 3 figures, Statistical Challenges in 21st Century Cosmology,\n  Proceedings IAU Symposium No. 306, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.GA astro-ph.CO stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The incredible variety of galaxy shapes cannot be summarized by human defined\ndiscrete classes of shapes without causing a possibly large loss of\ninformation. Dictionary learning and sparse coding allow us to reduce the high\ndimensional space of shapes into a manageable low dimensional continuous vector\nspace. Statistical inference can be done in the reduced space via probability\ndistribution estimation and manifold estimation.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jun 2014 18:47:18 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Vinci", "Giuseppe", ""], ["Freeman", "Peter", ""], ["Newman", "Jeffrey", ""], ["Wasserman", "Larry", ""], ["Genovese", "Christopher", ""]]}, {"id": "1406.7638", "submitter": "Hiroaki  Sasaki", "authors": "Hiroaki Sasaki, Yung-Kyun Noh, Masashi Sugiyama", "title": "Direct Density-Derivative Estimation and Its Application in\n  KL-Divergence Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of density derivatives is a versatile tool in statistical data\nanalysis. A naive approach is to first estimate the density and then compute\nits derivative. However, such a two-step approach does not work well because a\ngood density estimator does not necessarily mean a good density-derivative\nestimator. In this paper, we give a direct method to approximate the density\nderivative without estimating the density itself. Our proposed estimator allows\nanalytic and computationally efficient approximation of multi-dimensional\nhigh-order density derivatives, with the ability that all hyper-parameters can\nbe chosen objectively by cross-validation. We further show that the proposed\ndensity-derivative estimator is useful in improving the accuracy of\nnon-parametric KL-divergence estimation via metric learning. The practical\nsuperiority of the proposed method is experimentally demonstrated in change\ndetection and feature selection.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 09:06:03 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Sasaki", "Hiroaki", ""], ["Noh", "Yung-Kyun", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1406.7728", "submitter": "Yuan Yao", "authors": "Stanley Osher and Feng Ruan and Jiechao Xiong and Yuan Yao and Wotao\n  Yin", "title": "Sparse Recovery via Differential Inclusions", "comments": "In Applied and Computational Harmonic Analysis, 2016", "journal-ref": null, "doi": "10.1016/j.acha.2016.01.002", "report-no": "CAM Report 14-61", "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we recover sparse signals from their noisy linear measurements\nby solving nonlinear differential inclusions, which is based on the notion of\ninverse scale space (ISS) developed in applied mathematics. Our goal here is to\nbring this idea to address a challenging problem in statistics, \\emph{i.e.}\nfinding the oracle estimator which is unbiased and sign-consistent using\ndynamics. We call our dynamics \\emph{Bregman ISS} and \\emph{Linearized Bregman\nISS}. A well-known shortcoming of LASSO and any convex regularization\napproaches lies in the bias of estimators. However, we show that under proper\nconditions, there exists a bias-free and sign-consistent point on the solution\npaths of such dynamics, which corresponds to a signal that is the unbiased\nestimate of the true signal and whose entries have the same signs as those of\nthe true signs, \\emph{i.e.} the oracle estimator. Therefore, their solution\npaths are regularization paths better than the LASSO regularization path, since\nthe points on the latter path are biased when sign-consistency is reached. We\nalso show how to efficiently compute their solution paths in both continuous\nand discretized settings: the full solution paths can be exactly computed piece\nby piece, and a discretization leads to \\emph{Linearized Bregman iteration},\nwhich is a simple iterative thresholding rule and easy to parallelize.\nTheoretical guarantees such as sign-consistency and minimax optimal $l_2$-error\nbounds are established in both continuous and discrete settings for specific\npoints on the paths. Early-stopping rules for identifying these points are\ngiven. The key treatment relies on the development of differential inequalities\nfor differential inclusions and their discretizations, which extends the\nprevious results and leads to exponentially fast recovering of sparse signals\nbefore selecting wrong ones.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 13:30:15 GMT"}, {"version": "v2", "created": "Tue, 1 Jul 2014 01:42:32 GMT"}, {"version": "v3", "created": "Thu, 31 Jul 2014 05:11:00 GMT"}, {"version": "v4", "created": "Fri, 1 Aug 2014 02:10:11 GMT"}, {"version": "v5", "created": "Thu, 21 Jan 2016 18:27:34 GMT"}], "update_date": "2016-01-22", "authors_parsed": [["Osher", "Stanley", ""], ["Ruan", "Feng", ""], ["Xiong", "Jiechao", ""], ["Yao", "Yuan", ""], ["Yin", "Wotao", ""]]}, {"id": "1406.7758", "submitter": "Ziyu Wang", "authors": "Ziyu Wang, Nando de Freitas", "title": "Theoretical Analysis of Bayesian Optimisation with Unknown Gaussian\n  Process Hyper-Parameters", "comments": "16 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimisation has gained great popularity as a tool for optimising\nthe parameters of machine learning algorithms and models. Somewhat ironically,\nsetting up the hyper-parameters of Bayesian optimisation methods is notoriously\nhard. While reasonable practical solutions have been advanced, they can often\nfail to find the best optima. Surprisingly, there is little theoretical\nanalysis of this crucial problem in the literature. To address this, we derive\na cumulative regret bound for Bayesian optimisation with Gaussian processes and\nunknown kernel hyper-parameters in the stochastic setting. The bound, which\napplies to the expected improvement acquisition function and sub-Gaussian\nobservation noise, provides us with guidelines on how to design hyper-parameter\nestimation methods. A simple simulation demonstrates the importance of\nfollowing these guidelines.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 14:35:58 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Wang", "Ziyu", ""], ["de Freitas", "Nando", ""]]}, {"id": "1406.7806", "submitter": "Andrew Maas", "authors": "Andrew L. Maas, Peng Qi, Ziang Xie, Awni Y. Hannun, Christopher T.\n  Lengerich, Daniel Jurafsky and Andrew Y. Ng", "title": "Building DNN Acoustic Models for Large Vocabulary Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are now a central component of nearly all\nstate-of-the-art speech recognition systems. Building neural network acoustic\nmodels requires several design decisions including network architecture, size,\nand training loss function. This paper offers an empirical investigation on\nwhich aspects of DNN acoustic model design are most important for speech\nrecognition system performance. We report DNN classifier performance and final\nspeech recognizer word error rates, and compare DNNs using several metrics to\nquantify factors influencing differences in task performance. Our first set of\nexperiments use the standard Switchboard benchmark corpus, which contains\napproximately 300 hours of conversational telephone speech. We compare standard\nDNNs to convolutional networks, and present the first experiments using\nlocally-connected, untied neural networks for acoustic modeling. We\nadditionally build systems on a corpus of 2,100 hours of training data by\ncombining the Switchboard and Fisher corpora. This larger corpus allows us to\nmore thoroughly examine performance of large DNN models -- with up to ten times\nmore parameters than those typically used in speech recognition systems. Our\nresults suggest that a relatively simple DNN architecture and optimization\ntechnique produces strong results. These findings, along with previous work,\nhelp establish a set of best practices for building DNN hybrid speech\nrecognition systems with maximum likelihood training. Our experiments in DNN\noptimization additionally serve as a case study for training DNNs with\ndiscriminative loss functions for speech tasks, as well as DNN classifiers more\ngenerally.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 16:42:25 GMT"}, {"version": "v2", "created": "Tue, 20 Jan 2015 07:44:15 GMT"}], "update_date": "2015-01-21", "authors_parsed": [["Maas", "Andrew L.", ""], ["Qi", "Peng", ""], ["Xie", "Ziang", ""], ["Hannun", "Awni Y.", ""], ["Lengerich", "Christopher T.", ""], ["Jurafsky", "Daniel", ""], ["Ng", "Andrew Y.", ""]]}, {"id": "1406.7842", "submitter": "Xiaowen Dong", "authors": "Xiaowen Dong, Dorina Thanou, Pascal Frossard, Pierre Vandergheynst", "title": "Learning Laplacian Matrix in Smooth Graph Signal Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The construction of a meaningful graph plays a crucial role in the success of\nmany graph-based representations and algorithms for handling structured data,\nespecially in the emerging field of graph signal processing. However, a\nmeaningful graph is not always readily available from the data, nor easy to\ndefine depending on the application domain. In particular, it is often\ndesirable in graph signal processing applications that a graph is chosen such\nthat the data admit certain regularity or smoothness on the graph. In this\npaper, we address the problem of learning graph Laplacians, which is equivalent\nto learning graph topologies, such that the input data form graph signals with\nsmooth variations on the resulting topology. To this end, we adopt a factor\nanalysis model for the graph signals and impose a Gaussian probabilistic prior\non the latent variables that control these signals. We show that the Gaussian\nprior leads to an efficient representation that favors the smoothness property\nof the graph signals. We then propose an algorithm for learning graphs that\nenforces such property and is based on minimizing the variations of the signals\non the learned graph. Experiments on both synthetic and real world data\ndemonstrate that the proposed graph learning framework can efficiently infer\nmeaningful graph topologies from signal observations under the smoothness\nprior.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 18:33:59 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2015 14:07:03 GMT"}, {"version": "v3", "created": "Fri, 19 Feb 2016 22:12:47 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Dong", "Xiaowen", ""], ["Thanou", "Dorina", ""], ["Frossard", "Pascal", ""], ["Vandergheynst", "Pierre", ""]]}, {"id": "1406.7865", "submitter": "Antonio Sutera", "authors": "Antonio Sutera, Arnaud Joly, Vincent Fran\\c{c}ois-Lavet, Zixiao Aaron\n  Qiu, Gilles Louppe, Damien Ernst and Pierre Geurts", "title": "Simple connectome inference from partial correlation statistics in\n  calcium imaging", "comments": null, "journal-ref": "JMLR: Workshop and Conference Proceedings 46:23-35, 2015", "doi": "10.1007/978-3-319-53070-3_2", "report-no": null, "categories": "stat.ML cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a simple yet effective solution to the problem of\nconnectome inference in calcium imaging data. The proposed algorithm consists\nof two steps. First, processing the raw signals to detect neural peak\nactivities. Second, inferring the degree of association between neurons from\npartial correlation statistics. This paper summarises the methodology that led\nus to win the Connectomics Challenge, proposes a simplified version of our\nmethod, and finally compares our results with respect to other inference\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 19:34:23 GMT"}, {"version": "v2", "created": "Tue, 1 Jul 2014 07:55:27 GMT"}, {"version": "v3", "created": "Tue, 29 Jul 2014 12:36:32 GMT"}, {"version": "v4", "created": "Tue, 18 Nov 2014 14:18:42 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Sutera", "Antonio", ""], ["Joly", "Arnaud", ""], ["Fran\u00e7ois-Lavet", "Vincent", ""], ["Qiu", "Zixiao Aaron", ""], ["Louppe", "Gilles", ""], ["Ernst", "Damien", ""], ["Geurts", "Pierre", ""]]}]