[{"id": "1411.0007", "submitter": "Michael Bloodgood", "authors": "John E. Miller, Michael Bloodgood, Manabu Torii and K. Vijay-Shanker", "title": "Rapid Adaptation of POS Tagging for Domain Specific Uses", "comments": "2 pages, 2 tables; appeared in Proceedings of the HLT-NAACL BioNLP\n  Workshop on Linking Natural Language and Biology, June 2006", "journal-ref": "In Proceedings of the HLT-NAACL BioNLP Workshop on Linking Natural\n  Language and Biology, pages 118-119, New York, New York, June 2006.\n  Association for Computational Linguistics", "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Part-of-speech (POS) tagging is a fundamental component for performing\nnatural language tasks such as parsing, information extraction, and question\nanswering. When POS taggers are trained in one domain and applied in\nsignificantly different domains, their performance can degrade dramatically. We\npresent a methodology for rapid adaptation of POS taggers to new domains. Our\ntechnique is unsupervised in that a manually annotated corpus for the new\ndomain is not necessary. We use suffix information gathered from large amounts\nof raw text as well as orthographic information to increase the lexical\ncoverage. We present an experiment in the Biological domain where our POS\ntagger achieves results comparable to POS taggers specifically trained to this\ndomain.\n", "versions": [{"version": "v1", "created": "Fri, 31 Oct 2014 20:04:09 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Miller", "John E.", ""], ["Bloodgood", "Michael", ""], ["Torii", "Manabu", ""], ["Vijay-Shanker", "K.", ""]]}, {"id": "1411.0023", "submitter": "Eric Bax", "authors": "Ya Le, Eric Bax, Nicola Barbieri, David Garcia Soriano, Jitesh Mehta,\n  James Li", "title": "Validation of Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a technique to compute probably approximately correct (PAC)\nbounds on precision and recall for matching algorithms. The bounds require some\nverified matches, but those matches may be used to develop the algorithms. The\nbounds can be applied to network reconciliation or entity resolution\nalgorithms, which identify nodes in different networks or values in a data set\nthat correspond to the same entity. For network reconciliation, the bounds do\nnot require knowledge of the network generation process.\n", "versions": [{"version": "v1", "created": "Fri, 31 Oct 2014 20:46:44 GMT"}, {"version": "v2", "created": "Mon, 11 Apr 2016 04:59:59 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Le", "Ya", ""], ["Bax", "Eric", ""], ["Barbieri", "Nicola", ""], ["Soriano", "David Garcia", ""], ["Mehta", "Jitesh", ""], ["Li", "James", ""]]}, {"id": "1411.0024", "submitter": "Vu Pham", "authors": "Vu Pham, Laurent El Ghaoui, Arturo Fernandez", "title": "Robust sketching for multiple square-root LASSO problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many learning tasks, such as cross-validation, parameter search, or\nleave-one-out analysis, involve multiple instances of similar problems, each\ninstance sharing a large part of learning data with the others. We introduce a\nrobust framework for solving multiple square-root LASSO problems, based on a\nsketch of the learning data that uses low-rank approximations. Our approach\nallows a dramatic reduction in computational effort, in effect reducing the\nnumber of observations from $m$ (the number of observations to start with) to\n$k$ (the number of singular values retained in the low-rank model), while not\nsacrificing---sometimes even improving---the statistical performance.\nTheoretical analysis, as well as numerical experiments on both synthetic and\nreal data, illustrate the efficiency of the method in large scale applications.\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 05:30:42 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Pham", "Vu", ""], ["Ghaoui", "Laurent El", ""], ["Fernandez", "Arturo", ""]]}, {"id": "1411.0030", "submitter": "Chris J. Maddison", "authors": "Chris J. Maddison and Daniel Tarlow and Tom Minka", "title": "A* Sampling", "comments": "V2: - reworded the last paragraph of Section 2 to clarify that the\n  argmax is a sample from the normalized measure. - fixed notation in Algorithm\n  1. - fixed a typo in paragraph 2 of Section 5", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of drawing samples from a discrete distribution can be converted\ninto a discrete optimization problem. In this work, we show how sampling from a\ncontinuous distribution can be converted into an optimization problem over\ncontinuous space. Central to the method is a stochastic process recently\ndescribed in mathematical statistics that we call the Gumbel process. We\npresent a new construction of the Gumbel process and A* sampling, a practical\ngeneric sampling algorithm that searches for the maximum of a Gumbel process\nusing A* search. We analyze the correctness and convergence time of A* sampling\nand demonstrate empirically that it makes more efficient use of bound and\nlikelihood evaluations than the most closely related adaptive rejection\nsampling-based algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 31 Oct 2014 21:40:50 GMT"}, {"version": "v2", "created": "Mon, 26 Jan 2015 13:46:52 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Maddison", "Chris J.", ""], ["Tarlow", "Daniel", ""], ["Minka", "Tom", ""]]}, {"id": "1411.0073", "submitter": "Sewoong Oh", "authors": "Sewoong Oh, Devavrat Shah", "title": "Learning Mixed Multinomial Logit Model from Ordinal Data", "comments": "18 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by generating personalized recommendations using ordinal (or\npreference) data, we study the question of learning a mixture of MultiNomial\nLogit (MNL) model, a parameterized class of distributions over permutations,\nfrom partial ordinal or preference data (e.g. pair-wise comparisons). Despite\nits long standing importance across disciplines including social choice,\noperations research and revenue management, little is known about this\nquestion. In case of single MNL models (no mixture), computationally and\nstatistically tractable learning from pair-wise comparisons is feasible.\nHowever, even learning mixture with two MNL components is infeasible in\ngeneral.\n  Given this state of affairs, we seek conditions under which it is feasible to\nlearn the mixture model in both computationally and statistically efficient\nmanner. We present a sufficient condition as well as an efficient algorithm for\nlearning mixed MNL models from partial preferences/comparisons data. In\nparticular, a mixture of $r$ MNL components over $n$ objects can be learnt\nusing samples whose size scales polynomially in $n$ and $r$ (concretely,\n$r^{3.5}n^3(log n)^4$, with $r\\ll n^{2/7}$ when the model parameters are\nsufficiently incoherent). The algorithm has two phases: first, learn the\npair-wise marginals for each component using tensor decomposition; second,\nlearn the model parameters for each component using Rank Centrality introduced\nby Negahban et al. In the process of proving these results, we obtain a\ngeneralization of existing analysis for tensor decomposition to a more\nrealistic regime where only partial information about each sample is available.\n", "versions": [{"version": "v1", "created": "Sat, 1 Nov 2014 05:47:48 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Oh", "Sewoong", ""], ["Shah", "Devavrat", ""]]}, {"id": "1411.0161", "submitter": "Paul Honeine", "authors": "Paul Honeine", "title": "Entropy of Overcomplete Kernel Dictionaries", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV cs.LG cs.NE math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In signal analysis and synthesis, linear approximation theory considers a\nlinear decomposition of any given signal in a set of atoms, collected into a\nso-called dictionary. Relevant sparse representations are obtained by relaxing\nthe orthogonality condition of the atoms, yielding overcomplete dictionaries\nwith an extended number of atoms. More generally than the linear decomposition,\novercomplete kernel dictionaries provide an elegant nonlinear extension by\ndefining the atoms through a mapping kernel function (e.g., the gaussian\nkernel). Models based on such kernel dictionaries are used in neural networks,\ngaussian processes and online learning with kernels.\n  The quality of an overcomplete dictionary is evaluated with a diversity\nmeasure the distance, the approximation, the coherence and the Babel measures.\nIn this paper, we develop a framework to examine overcomplete kernel\ndictionaries with the entropy from information theory. Indeed, a higher value\nof the entropy is associated to a further uniform spread of the atoms over the\nspace. For each of the aforementioned diversity measures, we derive lower\nbounds on the entropy. Several definitions of the entropy are examined, with an\nextensive analysis in both the input space and the mapped feature space.\n", "versions": [{"version": "v1", "created": "Sat, 1 Nov 2014 19:41:14 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Honeine", "Paul", ""]]}, {"id": "1411.0254", "submitter": "Tom Gunter", "authors": "Chris Lloyd, Tom Gunter, Michael A. Osborne, Stephen J. Roberts", "title": "Variational Inference for Gaussian Process Modulated Poisson Processes", "comments": "in ICML 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first fully variational Bayesian inference scheme for\ncontinuous Gaussian-process-modulated Poisson processes. Such point processes\nare used in a variety of domains, including neuroscience, geo-statistics and\nastronomy, but their use is hindered by the computational cost of existing\ninference schemes. Our scheme: requires no discretisation of the domain; scales\nlinearly in the number of observed events; and is many orders of magnitude\nfaster than previous sampling based approaches. The resulting algorithm is\nshown to outperform standard methods on synthetic examples, coal mining\ndisaster data and in the prediction of Malaria incidences in Kenya.\n", "versions": [{"version": "v1", "created": "Sun, 2 Nov 2014 13:04:30 GMT"}, {"version": "v2", "created": "Tue, 24 Feb 2015 16:15:19 GMT"}, {"version": "v3", "created": "Mon, 27 Jul 2015 20:17:44 GMT"}], "update_date": "2015-07-29", "authors_parsed": [["Lloyd", "Chris", ""], ["Gunter", "Tom", ""], ["Osborne", "Michael A.", ""], ["Roberts", "Stephen J.", ""]]}, {"id": "1411.0282", "submitter": "Akshay Soni", "authors": "Akshay Soni, Swayambhoo Jain, Jarvis Haupt, and Stefano Gonella", "title": "Noisy Matrix Completion under Sparse Factor Models", "comments": "42 Pages, 7 Figures, Submitted to IEEE Transactions on Information\n  Theory", "journal-ref": null, "doi": "10.1109/TIT.2016.2549040", "report-no": null, "categories": "stat.ML cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines a general class of noisy matrix completion tasks where\nthe goal is to estimate a matrix from observations obtained at a subset of its\nentries, each of which is subject to random noise or corruption. Our specific\nfocus is on settings where the matrix to be estimated is well-approximated by a\nproduct of two (a priori unknown) matrices, one of which is sparse. Such\nstructural models - referred to here as \"sparse factor models\" - have been\nwidely used, for example, in subspace clustering applications, as well as in\ncontemporary sparse modeling and dictionary learning tasks. Our main\ntheoretical contributions are estimation error bounds for sparsity-regularized\nmaximum likelihood estimators for problems of this form, which are applicable\nto a number of different observation noise or corruption models. Several\nspecific implications are examined, including scenarios where observations are\ncorrupted by additive Gaussian noise or additive heavier-tailed (Laplace)\nnoise, Poisson-distributed observations, and highly-quantized (e.g., one-bit)\nobservations. We also propose a simple algorithmic approach based on the\nalternating direction method of multipliers for these tasks, and provide\nexperimental evidence to support our error analyses.\n", "versions": [{"version": "v1", "created": "Sun, 2 Nov 2014 17:29:48 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Soni", "Akshay", ""], ["Jain", "Swayambhoo", ""], ["Haupt", "Jarvis", ""], ["Gonella", "Stefano", ""]]}, {"id": "1411.0288", "submitter": "Eunho Yang", "authors": "Eunho Yang, Pradeep Ravikumar, Genevera I. Allen, Yulia Baker,\n  Ying-Wooi Wan, Zhandong Liu", "title": "A General Framework for Mixed Graphical Models", "comments": "40 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"Mixed Data\" comprising a large number of heterogeneous variables (e.g.\ncount, binary, continuous, skewed continuous, among other data types) are\nprevalent in varied areas such as genomics and proteomics, imaging genetics,\nnational security, social networking, and Internet advertising. There have been\nlimited efforts at statistically modeling such mixed data jointly, in part\nbecause of the lack of computationally amenable multivariate distributions that\ncan capture direct dependencies between such mixed variables of different\ntypes. In this paper, we address this by introducing a novel class of Block\nDirected Markov Random Fields (BDMRFs). Using the basic building block of\nnode-conditional univariate exponential families from Yang et al. (2012), we\nintroduce a class of mixed conditional random field distributions, that are\nthen chained according to a block-directed acyclic graph to form our class of\nBlock Directed Markov Random Fields (BDMRFs). The Markov independence graph\nstructure underlying a BDMRF thus has both directed and undirected edges. We\nintroduce conditions under which these distributions exist and are\nnormalizable, study several instances of our models, and propose scalable\npenalized conditional likelihood estimators with statistical guarantees for\nrecovering the underlying network structure. Simulations as well as an\napplication to learning mixed genomic networks from next generation sequencing\nexpression data and mutation data demonstrate the versatility of our methods.\n", "versions": [{"version": "v1", "created": "Sun, 2 Nov 2014 18:12:12 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Yang", "Eunho", ""], ["Ravikumar", "Pradeep", ""], ["Allen", "Genevera I.", ""], ["Baker", "Yulia", ""], ["Wan", "Ying-Wooi", ""], ["Liu", "Zhandong", ""]]}, {"id": "1411.0292", "submitter": "Alp Kucukelbir", "authors": "Alp Kucukelbir, David M. Blei", "title": "Population Empirical Bayes", "comments": "UAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian predictive inference analyzes a dataset to make predictions about\nnew observations. When a model does not match the data, predictive accuracy\nsuffers. We develop population empirical Bayes (POP-EB), a hierarchical\nframework that explicitly models the empirical population distribution as part\nof Bayesian analysis. We introduce a new concept, the latent dataset, as a\nhierarchical variable and set the empirical population as its prior. This leads\nto a new predictive density that mitigates model mismatch. We efficiently apply\nthis method to complex models by proposing a stochastic variational inference\nalgorithm, called bumping variational inference (BUMP-VI). We demonstrate\nimproved predictive accuracy over classical Bayesian inference in three models:\na linear regression model of health data, a Bayesian mixture model of natural\nimages, and a latent Dirichlet allocation topic model of scientific documents.\n", "versions": [{"version": "v1", "created": "Sun, 2 Nov 2014 18:50:14 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2015 21:36:22 GMT"}], "update_date": "2015-06-10", "authors_parsed": [["Kucukelbir", "Alp", ""], ["Blei", "David M.", ""]]}, {"id": "1411.0306", "submitter": "Ahmed El Alaoui", "authors": "Ahmed El Alaoui, Michael W. Mahoney", "title": "Fast Randomized Kernel Methods With Statistical Guarantees", "comments": "Improved presentation. Technical details fixed. A conference version\n  of this paper appears in NIPS15 under the modified title \"Fast Randomized\n  Kernel Ridge Regression with Statistical Guarantees\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One approach to improving the running time of kernel-based machine learning\nmethods is to build a small sketch of the input and use it in lieu of the full\nkernel matrix in the machine learning task of interest. Here, we describe a\nversion of this approach that comes with running time guarantees as well as\nimproved guarantees on its statistical performance. By extending the notion of\n\\emph{statistical leverage scores} to the setting of kernel ridge regression,\nour main statistical result is to identify an importance sampling distribution\nthat reduces the size of the sketch (i.e., the required number of columns to be\nsampled) to the \\emph{effective dimensionality} of the problem. This quantity\nis often much smaller than previous bounds that depend on the \\emph{maximal\ndegrees of freedom}. Our main algorithmic result is to present a fast algorithm\nto compute approximations to these scores. This algorithm runs in time that is\nlinear in the number of samples---more precisely, the running time is\n$O(np^2)$, where the parameter $p$ depends only on the trace of the kernel\nmatrix and the regularization parameter---and it can be applied to the matrix\nof feature vectors, without having to form the full kernel matrix. This is\nobtained via a variant of length-squared sampling that we adapt to the kernel\nsetting in a way that is of independent interest. Lastly, we provide empirical\nresults illustrating our theory, and we discuss how this new notion of the\nstatistical leverage of a data point captures in a fine way the difficulty of\nthe original statistical learning problem.\n", "versions": [{"version": "v1", "created": "Sun, 2 Nov 2014 19:57:31 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2015 02:20:23 GMT"}, {"version": "v3", "created": "Sun, 8 Nov 2015 20:33:45 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Alaoui", "Ahmed El", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1411.0347", "submitter": "Mert Pilanci", "authors": "Mert Pilanci and Martin J. Wainwright", "title": "Iterative Hessian sketch: Fast and accurate solution approximation for\n  constrained least-squares", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study randomized sketching methods for approximately solving least-squares\nproblem with a general convex constraint. The quality of a least-squares\napproximation can be assessed in different ways: either in terms of the value\nof the quadratic objective function (cost approximation), or in terms of some\ndistance measure between the approximate minimizer and the true minimizer\n(solution approximation). Focusing on the latter criterion, our first main\nresult provides a general lower bound on any randomized method that sketches\nboth the data matrix and vector in a least-squares problem; as a surprising\nconsequence, the most widely used least-squares sketch is sub-optimal for\nsolution approximation. We then present a new method known as the iterative\nHessian sketch, and show that it can be used to obtain approximations to the\noriginal least-squares problem using a projection dimension proportional to the\nstatistical complexity of the least-squares minimizer, and a logarithmic number\nof iterations. We illustrate our general theory with simulations for both\nunconstrained and constrained versions of least-squares, including\n$\\ell_1$-regularization and nuclear norm constraints. We also numerically\ndemonstrate the practicality of our approach in a real face expression\nclassification experiment.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 02:59:39 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Pilanci", "Mert", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1411.0439", "submitter": "Tom Gunter", "authors": "Tom Gunter, Michael A. Osborne, Roman Garnett, Philipp Hennig, Stephen\n  J. Roberts", "title": "Sampling for Inference in Probabilistic Models with Fast Bayesian\n  Quadrature", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems (NIPS) 2014", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel sampling framework for inference in probabilistic models:\nan active learning approach that converges more quickly (in wall-clock time)\nthan Markov chain Monte Carlo (MCMC) benchmarks. The central challenge in\nprobabilistic inference is numerical integration, to average over ensembles of\nmodels or unknown (hyper-)parameters (for example to compute the marginal\nlikelihood or a partition function). MCMC has provided approaches to numerical\nintegration that deliver state-of-the-art inference, but can suffer from sample\ninefficiency and poor convergence diagnostics. Bayesian quadrature techniques\noffer a model-based solution to such problems, but their uptake has been\nhindered by prohibitive computation costs. We introduce a warped model for\nprobabilistic integrands (likelihoods) that are known to be non-negative,\npermitting a cheap active learning scheme to optimally select sample locations.\nOur algorithm is demonstrated to offer faster convergence (in seconds) relative\nto simple Monte Carlo and annealed importance sampling on both synthetic and\nreal-world examples.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 11:48:07 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Gunter", "Tom", ""], ["Osborne", "Michael A.", ""], ["Garnett", "Roman", ""], ["Hennig", "Philipp", ""], ["Roberts", "Stephen J.", ""]]}, {"id": "1411.0560", "submitter": "Utkarsh Dang", "authors": "Utkarsh J. Dang, Antonio Punzo, Paul D. McNicholas, Salvatore\n  Ingrassia, and Ryan P. Browne", "title": "Multivariate response and parsimony for Gaussian cluster-weighted models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A family of parsimonious Gaussian cluster-weighted models is presented. This\nfamily concerns a multivariate extension to cluster-weighted modelling that can\naccount for correlations between multivariate responses. Parsimony is attained\nby constraining parts of an eigen-decomposition imposed on the component\ncovariance matrices. A sufficient condition for identifiability is provided and\nan expectation-maximization algorithm is presented for parameter estimation.\nModel performance is investigated on both synthetic and classical real data\nsets and compared with some popular approaches. Finally, accounting for linear\ndependencies in the presence of a linear regression structure is shown to offer\nbetter performance, vis-\\`{a}-vis clustering, over existing methodologies.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 16:57:39 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2016 15:05:55 GMT"}], "update_date": "2016-02-29", "authors_parsed": [["Dang", "Utkarsh J.", ""], ["Punzo", "Antonio", ""], ["McNicholas", "Paul D.", ""], ["Ingrassia", "Salvatore", ""], ["Browne", "Ryan P.", ""]]}, {"id": "1411.0589", "submitter": "Suvrit Sra", "authors": "\\'Alvaro Barbero, Suvrit Sra", "title": "Modular proximal optimization for multidimensional total-variation\n  regularization", "comments": "67 pages, 32 figures, new non-iterative fast TV algorithm, extensive\n  new experiments, corresponds to the github proxtv repository now", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study \\emph{TV regularization}, a widely used technique for eliciting\nstructured sparsity. In particular, we propose efficient algorithms for\ncomputing prox-operators for $\\ell_p$-norm TV. The most important among these\nis $\\ell_1$-norm TV, for whose prox-operator we present a new geometric\nanalysis which unveils a hitherto unknown connection to taut-string methods.\nThis connection turns out to be remarkably useful as it shows how our geometry\nguided implementation results in efficient weighted and unweighted 1D-TV\nsolvers, surpassing state-of-the-art methods. Our 1D-TV solvers provide the\nbackbone for building more complex (two or higher-dimensional) TV solvers\nwithin a modular proximal optimization approach. We review the literature for\nan array of methods exploiting this strategy, and illustrate the benefits of\nour modular design through extensive suite of experiments on (i) image\ndenoising, (ii) image deconvolution, (iii) four variants of fused-lasso, and\n(iv) video denoising. To underscore our claims and permit easy reproducibility,\nwe provide all the reviewed and our new TV solvers in an easy to use\nmulti-threaded C++, Matlab and Python library.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 18:08:44 GMT"}, {"version": "v2", "created": "Tue, 30 Dec 2014 11:22:36 GMT"}, {"version": "v3", "created": "Sat, 30 Dec 2017 20:00:21 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Barbero", "\u00c1lvaro", ""], ["Sra", "Suvrit", ""]]}, {"id": "1411.0591", "submitter": "Charles Fisher", "authors": "Charles K. Fisher and Pankaj Mehta", "title": "Bayesian feature selection with strongly-regularizing priors maps to the\n  Ising Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.stat-mech cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying small subsets of features that are relevant for prediction and/or\nclassification tasks is a central problem in machine learning and statistics.\nThe feature selection task is especially important, and computationally\ndifficult, for modern datasets where the number of features can be comparable\nto, or even exceed, the number of samples. Here, we show that feature selection\nwith Bayesian inference takes a universal form and reduces to calculating the\nmagnetizations of an Ising model, under some mild conditions. Our results\nexploit the observation that the evidence takes a universal form for\nstrongly-regularizing priors --- priors that have a large effect on the\nposterior probability even in the infinite data limit. We derive explicit\nexpressions for feature selection for generalized linear models, a large class\nof statistical techniques that include linear and logistic regression. We\nillustrate the power of our approach by analyzing feature selection in a\nlogistic regression-based classifier trained to distinguish between the letters\nB and D in the notMNIST dataset.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 18:15:29 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Fisher", "Charles K.", ""], ["Mehta", "Pankaj", ""]]}, {"id": "1411.0630", "submitter": "Aram Galstyan", "authors": "Armen E. Allahverdyan and Aram Galstyan", "title": "Active Inference for Binary Symmetric Hidden Markov Models", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": "10.1007/s10955-015-1321-y", "report-no": null, "categories": "stat.ML cond-mat.dis-nn cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider active maximum a posteriori (MAP) inference problem for Hidden\nMarkov Models (HMM), where, given an initial MAP estimate of the hidden\nsequence, we select to label certain states in the sequence to improve the\nestimation accuracy of the remaining states. We develop an analytical approach\nto this problem for the case of binary symmetric HMMs, and obtain a closed form\nsolution that relates the expected error reduction to model parameters under\nthe specified active inference scheme. We then use this solution to determine\nmost optimal active inference scheme in terms of error reduction, and examine\nthe relation of those schemes to heuristic principles of uncertainty reduction\nand solution unicity.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 19:46:07 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["Allahverdyan", "Armen E.", ""], ["Galstyan", "Aram", ""]]}, {"id": "1411.0707", "submitter": "Michael Busch Jr", "authors": "Michael Busch and Jeff Moehlis", "title": "A Nonparametric Adaptive Nonlinear Statistical Filter", "comments": "Accepted at the 2014 IEEE Conference on Decision and Control", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use statistical learning methods to construct an adaptive state estimator\nfor nonlinear stochastic systems. Optimal state estimation, in the form of a\nKalman filter, requires knowledge of the system's process and measurement\nuncertainty. We propose that these uncertainties can be estimated from\n(conditioned on) past observed data, and without making any assumptions of the\nsystem's prior distribution. The system's prior distribution at each time step\nis constructed from an ensemble of least-squares estimates on sub-sampled sets\nof the data via jackknife sampling. As new data is acquired, the state\nestimates, process uncertainty, and measurement uncertainty are updated\naccordingly, as described in this manuscript.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 21:34:24 GMT"}], "update_date": "2014-11-05", "authors_parsed": [["Busch", "Michael", ""], ["Moehlis", "Jeff", ""]]}, {"id": "1411.0877", "submitter": "Laurent Callot", "authors": "Laurent Callot and Johannes Tang Kristensen", "title": "Vector Autoregressions with Parsimoniously Time Varying Parameters and\n  an Application to Monetary Policy", "comments": "This paper has been withdrawn by the author due to an error in\n  assumption 4,ii", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a parsimoniously time varying parameter vector\nautoregressive model (with exogenous variables, VARX) and studies the\nproperties of the Lasso and adaptive Lasso as estimators of this model. The\nparameters of the model are assumed to follow parsimonious random walks, where\nparsimony stems from the assumption that increments to the parameters have a\nnon-zero probability of being exactly equal to zero. By varying the degree of\nparsimony our model can accommodate constant parameters, an unknown number of\nstructural breaks, or parameters with a high degree of variation.\n  We characterize the finite sample properties of the Lasso by deriving upper\nbounds on the estimation and prediction errors that are valid with high\nprobability; and asymptotically we show that these bounds tend to zero with\nprobability tending to one if the number of non zero increments grows slower\nthan $\\sqrt{T}$.\n  By simulation experiments we investigate the properties of the Lasso and the\nadaptive Lasso in settings where the parameters are stable, experience\nstructural breaks, or follow a parsimonious random walk. We use our model to\ninvestigate the monetary policy response to inflation and business cycle\nfluctuations in the US by estimating a parsimoniously time varying parameter\nTaylor rule. We document substantial changes in the policy response of the Fed\nin the 1980s and since 2008.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 12:22:47 GMT"}, {"version": "v2", "created": "Thu, 20 Nov 2014 14:47:44 GMT"}], "update_date": "2014-11-21", "authors_parsed": [["Callot", "Laurent", ""], ["Kristensen", "Johannes Tang", ""]]}, {"id": "1411.0894", "submitter": "Sebastien Gadat", "authors": "S\\'ebastien Gadat, Thierry Klein, Cl\\'ement Marteau", "title": "Classification with the nearest neighbor rule in general finite\n  dimensional spaces: necessary and sufficient conditions", "comments": "53 Pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Given an $n$-sample of random vectors $(X_i,Y_i)_{1 \\leq i \\leq n}$ whose\njoint law is unknown, the long-standing problem of supervised classification\naims to \\textit{optimally} predict the label $Y$ of a given a new observation\n$X$. In this context, the nearest neighbor rule is a popular flexible and\nintuitive method in non-parametric situations.\n  Even if this algorithm is commonly used in the machine learning and\nstatistics communities, less is known about its prediction ability in general\nfinite dimensional spaces, especially when the support of the density of the\nobservations is $\\mathbb{R}^d$. This paper is devoted to the study of the\nstatistical properties of the nearest neighbor rule in various situations. In\nparticular, attention is paid to the marginal law of $X$, as well as the\nsmoothness and margin properties of the \\textit{regression function} $\\eta(X) =\n\\mathbb{E}[Y | X]$. We identify two necessary and sufficient conditions to\nobtain uniform consistency rates of classification and to derive sharp\nestimates in the case of the nearest neighbor rule. Some numerical experiments\nare proposed at the end of the paper to help illustrate the discussion.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 13:05:08 GMT"}, {"version": "v2", "created": "Wed, 5 Nov 2014 06:17:03 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Gadat", "S\u00e9bastien", ""], ["Klein", "Thierry", ""], ["Marteau", "Cl\u00e9ment", ""]]}, {"id": "1411.0900", "submitter": "Krikamol Muandet", "authors": "Krikamol Muandet, Bharath Sriperumbudur, Bernhard Sch\\\"olkopf", "title": "Kernel Mean Estimation via Spectral Filtering", "comments": "To appear at the 28th Annual Conference on Neural Information\n  Processing Systems (NIPS 2014). 16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of estimating the kernel mean in a reproducing kernel Hilbert\nspace (RKHS) is central to kernel methods in that it is used by classical\napproaches (e.g., when centering a kernel PCA matrix), and it also forms the\ncore inference step of modern kernel methods (e.g., kernel-based non-parametric\ntests) that rely on embedding probability distributions in RKHSs. Muandet et\nal. (2014) has shown that shrinkage can help in constructing \"better\"\nestimators of the kernel mean than the empirical estimator. The present paper\nstudies the consistency and admissibility of the estimators in Muandet et al.\n(2014), and proposes a wider class of shrinkage estimators that improve upon\nthe empirical estimator by considering appropriate basis functions. Using the\nkernel PCA basis, we show that some of these estimators can be constructed\nusing spectral filtering algorithms which are shown to be consistent under some\ntechnical assumptions. Our theoretical analysis also reveals a fundamental\nconnection to the kernel-based supervised learning framework. The proposed\nestimators are simple to implement and perform well in practice.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 13:27:57 GMT"}], "update_date": "2014-11-05", "authors_parsed": [["Muandet", "Krikamol", ""], ["Sriperumbudur", "Bharath", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1411.0939", "submitter": "Alexis Boukouvalas", "authors": "Yordan P. Raykov, Alexis Boukouvalas, Max A. Little", "title": "Simple approximate MAP Inference for Dirichlet processes", "comments": "11 pages, 4 Figures, 5 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Dirichlet process mixture (DPM) is a ubiquitous, flexible Bayesian\nnonparametric statistical model. However, full probabilistic inference in this\nmodel is analytically intractable, so that computationally intensive techniques\nsuch as Gibb's sampling are required. As a result, DPM-based methods, which\nhave considerable potential, are restricted to applications in which\ncomputational resources and time for inference is plentiful. For example, they\nwould not be practical for digital signal processing on embedded hardware,\nwhere computational resources are at a serious premium. Here, we develop\nsimplified yet statistically rigorous approximate maximum a-posteriori (MAP)\ninference algorithms for DPMs. This algorithm is as simple as K-means\nclustering, performs in experiments as well as Gibb's sampling, while requiring\nonly a fraction of the computational effort. Unlike related small variance\nasymptotics, our algorithm is non-degenerate and so inherits the \"rich get\nricher\" property of the Dirichlet process. It also retains a non-degenerate\nclosed-form likelihood which enables standard tools such as cross-validation to\nbe used. This is a well-posed approximation to the MAP solution of the\nprobabilistic DPM model.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 15:30:25 GMT"}], "update_date": "2014-11-05", "authors_parsed": [["Raykov", "Yordan P.", ""], ["Boukouvalas", "Alexis", ""], ["Little", "Max A.", ""]]}, {"id": "1411.0972", "submitter": "Stephen Becker", "authors": "Volkan Cevher and Stephen Becker and Mark Schmidt", "title": "Convex Optimization for Big Data", "comments": "23 pages, 4 figurs, 8 algorithms", "journal-ref": "IEEE Signal Processing Magazine, Vol. 31(5), pages 32--43, 2014", "doi": "10.1109/MSP.2014.2329397", "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article reviews recent advances in convex optimization algorithms for\nBig Data, which aim to reduce the computational, storage, and communications\nbottlenecks. We provide an overview of this emerging field, describe\ncontemporary approximation techniques like first-order methods and\nrandomization for scalability, and survey the important role of parallel and\ndistributed computation. The new Big Data algorithms are based on surprisingly\nsimple principles and attain staggering accelerations even on classical\nproblems.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 17:14:27 GMT"}], "update_date": "2014-11-05", "authors_parsed": [["Cevher", "Volkan", ""], ["Becker", "Stephen", ""], ["Schmidt", "Mark", ""]]}, {"id": "1411.0997", "submitter": "Erin Pearse", "authors": "Chad Eckman, Jonathan A. Lindgren, Erin P. J. Pearse, David J. Sacco,\n  Zachariah Zhang", "title": "Iterated geometric harmonics for data imputation and reconstruction of\n  missing data", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method of geometric harmonics is adapted to the situation of incomplete\ndata by means of the iterated geometric harmonics (IGH) scheme. The method is\ntested on natural and synthetic data sets with 50--500 data points and\ndimensionality of 400--10,000. Experiments suggest that the algorithm converges\nto a near optimal solution within 4--6 iterations, at runtimes of less than 30\nminutes on a medium-grade desktop computer. The imputation of missing data\nvalues is applied to collections of damaged images (suffering from data\nannihilation rates of up to 70\\%) which are reconstructed with a surprising\ndegree of accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 18:46:34 GMT"}], "update_date": "2014-11-05", "authors_parsed": [["Eckman", "Chad", ""], ["Lindgren", "Jonathan A.", ""], ["Pearse", "Erin P. J.", ""], ["Sacco", "David J.", ""], ["Zhang", "Zachariah", ""]]}, {"id": "1411.1076", "submitter": "Andrea Montanari", "authors": "Andrea Montanari and Emile Richard", "title": "A statistical model for tensor PCA", "comments": "Neural Information Processing Systems (NIPS) 2014 (slightly expanded:\n  30 pages, 6 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Principal Component Analysis problem for large tensors of\narbitrary order $k$ under a single-spike (or rank-one plus noise) model. On the\none hand, we use information theory, and recent results in probability theory,\nto establish necessary and sufficient conditions under which the principal\ncomponent can be estimated using unbounded computational resources. It turns\nout that this is possible as soon as the signal-to-noise ratio $\\beta$ becomes\nlarger than $C\\sqrt{k\\log k}$ (and in particular $\\beta$ can remain bounded as\nthe problem dimensions increase).\n  On the other hand, we analyze several polynomial-time estimation algorithms,\nbased on tensor unfolding, power iteration and message passing ideas from\ngraphical models. We show that, unless the signal-to-noise ratio diverges in\nthe system dimensions, none of these approaches succeeds. This is possibly\nrelated to a fundamental limitation of computationally tractable estimators for\nthis problem.\n  We discuss various initializations for tensor power iteration, and show that\na tractable initialization based on the spectrum of the matricized tensor\noutperforms significantly baseline methods, statistically and computationally.\nFinally, we consider the case in which additional side information is available\nabout the unknown signal. We characterize the amount of side information that\nallows the iterative algorithms to converge to a good estimate.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 21:01:56 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Montanari", "Andrea", ""], ["Richard", "Emile", ""]]}, {"id": "1411.1087", "submitter": "Praneeth Netrapalli", "authors": "Prateek Jain and Praneeth Netrapalli", "title": "Fast Exact Matrix Completion with Finite Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.DS cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix completion is the problem of recovering a low rank matrix by observing\na small fraction of its entries. A series of recent works [KOM12,JNS13,HW14]\nhave proposed fast non-convex optimization based iterative algorithms to solve\nthis problem. However, the sample complexity in all these results is\nsub-optimal in its dependence on the rank, condition number and the desired\naccuracy.\n  In this paper, we present a fast iterative algorithm that solves the matrix\ncompletion problem by observing $O(nr^5 \\log^3 n)$ entries, which is\nindependent of the condition number and the desired accuracy. The run time of\nour algorithm is $O(nr^7\\log^3 n\\log 1/\\epsilon)$ which is near linear in the\ndimension of the matrix. To the best of our knowledge, this is the first near\nlinear time algorithm for exact matrix completion with finite sample complexity\n(i.e. independent of $\\epsilon$).\n  Our algorithm is based on a well known projected gradient descent method,\nwhere the projection is onto the (non-convex) set of low rank matrices. There\nare two key ideas in our result: 1) our argument is based on a $\\ell_{\\infty}$\nnorm potential function (as opposed to the spectral norm) and provides a novel\nway to obtain perturbation bounds for it. 2) we prove and use a natural\nextension of the Davis-Kahan theorem to obtain perturbation bounds on the best\nlow rank approximation of matrices with good eigen-gap. Both of these ideas may\nbe of independent interest.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 21:16:23 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Jain", "Prateek", ""], ["Netrapalli", "Praneeth", ""]]}, {"id": "1411.1088", "submitter": "Jennifer Gillenwater", "authors": "Jennifer Gillenwater, Alex Kulesza, Emily Fox, Ben Taskar", "title": "Expectation-Maximization for Learning Determinantal Point Processes", "comments": null, "journal-ref": "Neural Information Processing Systems (NIPS), 2014", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A determinantal point process (DPP) is a probabilistic model of set diversity\ncompactly parameterized by a positive semi-definite kernel matrix. To fit a DPP\nto a given task, we would like to learn the entries of its kernel matrix by\nmaximizing the log-likelihood of the available data. However, log-likelihood is\nnon-convex in the entries of the kernel matrix, and this learning problem is\nconjectured to be NP-hard. Thus, previous work has instead focused on more\nrestricted convex learning settings: learning only a single weight for each row\nof the kernel matrix, or learning weights for a linear combination of DPPs with\nfixed kernel matrices. In this work we propose a novel algorithm for learning\nthe full kernel matrix. By changing the kernel parameterization from matrix\nentries to eigenvalues and eigenvectors, and then lower-bounding the likelihood\nin the manner of expectation-maximization algorithms, we obtain an effective\noptimization procedure. We test our method on a real-world product\nrecommendation task, and achieve relative gains of up to 16.5% in test\nlog-likelihood compared to the naive approach of maximizing likelihood by\nprojected gradient ascent on the entries of the kernel matrix.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 21:23:35 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Gillenwater", "Jennifer", ""], ["Kulesza", "Alex", ""], ["Fox", "Emily", ""], ["Taskar", "Ben", ""]]}, {"id": "1411.1119", "submitter": "Justin Domke", "authors": "Xianghang Liu and Justin Domke", "title": "Projecting Markov Random Field Parameters for Fast Mixing", "comments": "Neural Information Processing Systems 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) algorithms are simple and extremely powerful\ntechniques to sample from almost arbitrary distributions. The flaw in practice\nis that it can take a large and/or unknown amount of time to converge to the\nstationary distribution. This paper gives sufficient conditions to guarantee\nthat univariate Gibbs sampling on Markov Random Fields (MRFs) will be fast\nmixing, in a precise sense. Further, an algorithm is given to project onto this\nset of fast-mixing parameters in the Euclidean norm. Following recent work, we\ngive an example use of this to project in various divergence measures,\ncomparing univariate marginals obtained by sampling after projection to common\nvariational methods and Gibbs sampling on the original parameters.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 00:43:08 GMT"}, {"version": "v2", "created": "Fri, 7 Nov 2014 05:38:17 GMT"}, {"version": "v3", "created": "Wed, 12 Nov 2014 00:05:12 GMT"}], "update_date": "2014-11-13", "authors_parsed": [["Liu", "Xianghang", ""], ["Domke", "Justin", ""]]}, {"id": "1411.1134", "submitter": "Christopher De Sa", "authors": "Christopher De Sa, Kunle Olukotun, and Christopher R\\'e", "title": "Global Convergence of Stochastic Gradient Descent for Some Non-convex\n  Matrix Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent (SGD) on a low-rank factorization is commonly\nemployed to speed up matrix problems including matrix completion, subspace\ntracking, and SDP relaxation. In this paper, we exhibit a step size scheme for\nSGD on a low-rank least-squares problem, and we prove that, under broad\nsampling conditions, our method converges globally from a random starting point\nwithin $O(\\epsilon^{-1} n \\log n)$ steps with constant probability for\nconstant-rank problems. Our modification of SGD relates it to stochastic power\niteration. We also show experiments to illustrate the runtime and convergence\nof the algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 03:05:43 GMT"}, {"version": "v2", "created": "Thu, 6 Nov 2014 03:10:29 GMT"}, {"version": "v3", "created": "Tue, 10 Feb 2015 20:19:28 GMT"}], "update_date": "2015-02-11", "authors_parsed": [["De Sa", "Christopher", ""], ["Olukotun", "Kunle", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1411.1158", "submitter": "Ohad Shamir", "authors": "Nicol\\`o Cesa-Bianchi, Yishay Mansour and Ohad Shamir", "title": "On the Complexity of Learning with Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A well-recognized limitation of kernel learning is the requirement to handle\na kernel matrix, whose size is quadratic in the number of training examples.\nMany methods have been proposed to reduce this computational cost, mostly by\nusing a subset of the kernel matrix entries, or some form of low-rank matrix\napproximation, or a random projection method. In this paper, we study lower\nbounds on the error attainable by such methods as a function of the number of\nentries observed in the kernel matrix or the rank of an approximate kernel\nmatrix. We show that there are kernel learning problems where no such method\nwill lead to non-trivial computational savings. Our results also quantify how\nthe problem difficulty depends on parameters such as the nature of the loss\nfunction, the regularization parameter, the norm of the desired predictor, and\nthe kernel matrix rank. Our results also suggest cases where more efficient\nkernel learning might be possible.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 06:18:14 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Cesa-Bianchi", "Nicol\u00f2", ""], ["Mansour", "Yishay", ""], ["Shamir", "Ohad", ""]]}, {"id": "1411.1243", "submitter": "Stylianos Kampakis", "authors": "Stylianos Kampakis, Andreas Adamides", "title": "Using Twitter to predict football outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Twitter has been proven to be a notable source for predictive modelling on\nvarious domains such as the stock market, the dissemination of diseases or\nsports outcomes. However, such a study has not been conducted in football\n(soccer) so far. The purpose of this research was to study whether data mined\nfrom Twitter can be used for this purpose. We built a set of predictive models\nfor the outcome of football games of the English Premier League for a 3 month\nperiod based on tweets and we studied whether these models can overcome\npredictive models which use only historical data and simple football\nstatistics. Moreover, combined models are constructed using both Twitter and\nhistorical data. The final results indicate that data mined from Twitter can\nindeed be a useful source for predicting games in the Premier League. The final\nTwitter-based model performs significantly better than chance when measured by\nCohen's kappa and is comparable to the model that uses simple statistics and\nhistorical data. Combining both models raises the performance higher than it\nwas achieved by each individual model. Thereby, this study provides evidence\nthat Twitter derived features can indeed provide useful information for the\nprediction of football (soccer) outcomes.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 11:50:15 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Kampakis", "Stylianos", ""], ["Adamides", "Andreas", ""]]}, {"id": "1411.1285", "submitter": "Benjamin Hofner", "authors": "Benjamin Hofner, Luigi Boccuto, Markus G\\\"oker", "title": "Controlling false discoveries in high-dimensional situations: Boosting\n  with stability selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern biotechnologies often result in high-dimensional data sets with much\nmore variables than observations (n $\\ll$ p). These data sets pose new\nchallenges to statistical analysis: Variable selection becomes one of the most\nimportant tasks in this setting. We assess the recently proposed flexible\nframework for variable selection called stability selection. By the use of\nresampling procedures, stability selection adds a finite sample error control\nto high-dimensional variable selection procedures such as Lasso or boosting. We\nconsider the combination of boosting and stability selection and present\nresults from a detailed simulation study that provides insights into the\nusefulness of this combination. Limitations are discussed and guidance on the\nspecification and tuning of stability selection is given. The interpretation of\nthe used error bounds is elaborated and insights for practical data analysis\nare given. The results will be used to detect differentially expressed\nphenotype measurements in patients with autism spectrum disorders. All methods\nare implemented in the freely available R package stabs.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 14:47:56 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Hofner", "Benjamin", ""], ["Boccuto", "Luigi", ""], ["G\u00f6ker", "Markus", ""]]}, {"id": "1411.1375", "submitter": "Yordan Karadzhov", "authors": "Yordan Karadzhov", "title": "Heuristic algorithm for 1D and 2D unfolding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A very simple heuristic approach to the unfolding problem will be described.\nAn iterative algorithm starts with an empty histogram and every iteration aims\nto add one entry to this histogram. The entry to be added is selected according\nto a criteria which includes a $\\chi^2$ test and a regularization. After a\nrelatively small number of iterations (500 - 1000) the growing reconstructed\ndistribution converges to the true distribution.\n", "versions": [{"version": "v1", "created": "Fri, 17 Oct 2014 13:50:14 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Karadzhov", "Yordan", ""]]}, {"id": "1411.1469", "submitter": "Jing Lei", "authors": "Jing Lei and Lingxue Zhu", "title": "A Generic Sample Splitting Approach for Refined Community Recovery in\n  Stochastic Block Models", "comments": "19 pages", "journal-ref": "Statistica Sinica 27 (2017), 1639-1659", "doi": "10.5705/ss.202015.0279", "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and analyze a generic method for community recovery in stochastic\nblock models and degree corrected block models. This approach can exactly\nrecover the hidden communities with high probability when the expected node\ndegrees are of order $\\log n$ or higher. Starting from a roughly correct\ncommunity partition given by some conventional community recovery algorithm,\nthis method refines the partition in a cross clustering step. Our results\nsimplify and extend some of the previous work on exact community recovery,\ndiscovering the key role played by sample splitting. The proposed method is\nsimple and can be implemented with many practical community recovery\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 01:34:26 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Lei", "Jing", ""], ["Zhu", "Lingxue", ""]]}, {"id": "1411.1488", "submitter": "Majid Janzamin", "authors": "Anima Anandkumar, Rong Ge, Majid Janzamin", "title": "Analyzing Tensor Power Method Dynamics in Overcomplete Regime", "comments": "38 pages; analysis of noise added to the previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel analysis of the dynamics of tensor power iterations in the\novercomplete regime where the tensor CP rank is larger than the input\ndimension. Finding the CP decomposition of an overcomplete tensor is NP-hard in\ngeneral. We consider the case where the tensor components are randomly drawn,\nand show that the simple power iteration recovers the components with bounded\nerror under mild initialization conditions. We apply our analysis to\nunsupervised learning of latent variable models, such as multi-view mixture\nmodels and spherical Gaussian mixtures. Given the third order moment tensor, we\nlearn the parameters using tensor power iterations. We prove it can correctly\nlearn the model parameters when the number of hidden components $k$ is much\nlarger than the data dimension $d$, up to $k = o(d^{1.5})$. We initialize the\npower iterations with data samples and prove its success under mild conditions\non the signal-to-noise ratio of the samples. Our analysis significantly expands\nthe class of latent variable models where spectral methods are applicable. Our\nanalysis also deals with noise in the input tensor leading to sample complexity\nresult in the application to learning latent variable models.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 03:25:54 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2015 20:56:57 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Anandkumar", "Anima", ""], ["Ge", "Rong", ""], ["Janzamin", "Majid", ""]]}, {"id": "1411.1537", "submitter": "Fei Sha", "authors": "Boqing Gong, Wei-lun Chao, Kristen Grauman and Fei Sha", "title": "Large-Margin Determinantal Point Processes", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determinantal point processes (DPPs) offer a powerful approach to modeling\ndiversity in many applications where the goal is to select a diverse subset. We\nstudy the problem of learning the parameters (the kernel matrix) of a DPP from\nlabeled training data. We make two contributions. First, we show how to\nreparameterize a DPP's kernel matrix with multiple kernel functions, thus\nenhancing modeling flexibility. Second, we propose a novel parameter estimation\ntechnique based on the principle of large margin separation. In contrast to the\nstate-of-the-art method of maximum likelihood estimation, our large-margin loss\nfunction explicitly models errors in selecting the target subsets, and it can\nbe customized to trade off different types of errors (precision vs. recall).\nExtensive empirical studies validate our contributions, including applications\non challenging document and video summarization, where flexibility in modeling\nthe kernel matrix and balancing different errors is indispensable.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 09:14:02 GMT"}, {"version": "v2", "created": "Fri, 7 Nov 2014 05:21:03 GMT"}], "update_date": "2014-11-10", "authors_parsed": [["Gong", "Boqing", ""], ["Chao", "Wei-lun", ""], ["Grauman", "Kristen", ""], ["Sha", "Fei", ""]]}, {"id": "1411.1557", "submitter": "Tom Claassen", "authors": "Tom Claassen, Joris M. Mooij, Tom Heskes", "title": "Proof Supplement - Learning Sparse Causal Models is not NP-hard\n  (UAI2013)", "comments": "11 pages, supplement to `Learning Sparse Causal Models is not\n  NP-hard' (UAI2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article contains detailed proofs and additional examples related to the\nUAI-2013 submission `Learning Sparse Causal Models is not NP-hard'. It\ndescribes the FCI+ algorithm: a method for sound and complete causal model\ndiscovery in the presence of latent confounders and/or selection bias, that has\nworst case polynomial complexity of order $N^{2(k+1)}$ in the number of\nindependence tests, for sparse graphs over $N$ nodes, bounded by node degree\n$k$. The algorithm is an adaptation of the well-known FCI algorithm by (Spirtes\net al., 2000) that is also sound and complete, but has worst case complexity\nexponential in $N$.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 10:41:32 GMT"}], "update_date": "2014-11-07", "authors_parsed": [["Claassen", "Tom", ""], ["Mooij", "Joris M.", ""], ["Heskes", "Tom", ""]]}, {"id": "1411.1670", "submitter": "Nicholas Foti", "authors": "Nicholas J. Foti, Jason Xu, Dillon Laird and Emily B. Fox", "title": "Stochastic Variational Inference for Hidden Markov Models", "comments": "Appears in Advances in Neural Information Processing Systems (NIPS),\n  2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference algorithms have proven successful for Bayesian analysis\nin large data settings, with recent advances using stochastic variational\ninference (SVI). However, such methods have largely been studied in independent\nor exchangeable data settings. We develop an SVI algorithm to learn the\nparameters of hidden Markov models (HMMs) in a time-dependent data setting. The\nchallenge in applying stochastic optimization in this setting arises from\ndependencies in the chain, which must be broken to consider minibatches of\nobservations. We propose an algorithm that harnesses the memory decay of the\nchain to adaptively bound errors arising from edge effects. We demonstrate the\neffectiveness of our algorithm on synthetic experiments and a large genomics\ndataset where a batch algorithm is computationally infeasible.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 17:56:21 GMT"}], "update_date": "2014-11-07", "authors_parsed": [["Foti", "Nicholas J.", ""], ["Xu", "Jason", ""], ["Laird", "Dillon", ""], ["Fox", "Emily B.", ""]]}, {"id": "1411.1690", "submitter": "Yutian Chen", "authors": "Yutian Chen, Vikash Mansinghka, Zoubin Ghahramani", "title": "Sublinear-Time Approximate MCMC Transitions for Probabilistic Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic programming languages can simplify the development of machine\nlearning techniques, but only if inference is sufficiently scalable.\nUnfortunately, Bayesian parameter estimation for highly coupled models such as\nregressions and state-space models still scales poorly; each MCMC transition\ntakes linear time in the number of observations. This paper describes a\nsublinear-time algorithm for making Metropolis-Hastings (MH) updates to latent\nvariables in probabilistic programs. The approach generalizes recently\nintroduced approximate MH techniques: instead of subsampling data items assumed\nto be independent, it subsamples edges in a dynamically constructed graphical\nmodel. It thus applies to a broader class of problems and interoperates with\nother general-purpose inference techniques. Empirical results, including\nconfirmation of sublinear per-transition scaling, are presented for Bayesian\nlogistic regression, nonlinear classification via joint Dirichlet process\nmixtures, and parameter estimation for stochastic volatility models (with state\nestimation via particle MCMC). All three applications use the same\nimplementation, and each requires under 20 lines of probabilistic code.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 18:32:24 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2015 17:09:54 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Chen", "Yutian", ""], ["Mansinghka", "Vikash", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1411.1752", "submitter": "Adarsh Prasad", "authors": "Adarsh Prasad, Stefanie Jegelka and Dhruv Batra", "title": "Submodular meets Structured: Finding Diverse Subsets in\n  Exponentially-Large Structured Item Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To cope with the high level of ambiguity faced in domains such as Computer\nVision or Natural Language processing, robust prediction methods often search\nfor a diverse set of high-quality candidate solutions or proposals. In\nstructured prediction problems, this becomes a daunting task, as the solution\nspace (image labelings, sentence parses, etc.) is exponentially large. We study\ngreedy algorithms for finding a diverse subset of solutions in\nstructured-output spaces by drawing new connections between submodular\nfunctions over combinatorial item sets and High-Order Potentials (HOPs) studied\nfor graphical models. Specifically, we show via examples that when marginal\ngains of submodular diversity functions allow structured representations, this\nenables efficient (sub-linear time) approximate maximization by reducing the\ngreedy augmentation step to inference in a factor graph with appropriately\nconstructed HOPs. We discuss benefits, tradeoffs, and show that our\nconstructions lead to significantly better proposals.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 20:07:37 GMT"}], "update_date": "2014-11-10", "authors_parsed": [["Prasad", "Adarsh", ""], ["Jegelka", "Stefanie", ""], ["Batra", "Dhruv", ""]]}, {"id": "1411.1784", "submitter": "Mehdi Mirza", "authors": "Mehdi Mirza, Simon Osindero", "title": "Conditional Generative Adversarial Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Nets [8] were recently introduced as a novel way to\ntrain generative models. In this work we introduce the conditional version of\ngenerative adversarial nets, which can be constructed by simply feeding the\ndata, y, we wish to condition on to both the generator and discriminator. We\nshow that this model can generate MNIST digits conditioned on class labels. We\nalso illustrate how this model could be used to learn a multi-modal model, and\nprovide preliminary examples of an application to image tagging in which we\ndemonstrate how this approach can generate descriptive tags which are not part\nof training labels.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 22:33:22 GMT"}], "update_date": "2014-11-10", "authors_parsed": [["Mirza", "Mehdi", ""], ["Osindero", "Simon", ""]]}, {"id": "1411.1804", "submitter": "Dawen Liang", "authors": "Dawen Liang, Matthew D. Hoffman", "title": "Beta Process Non-negative Matrix Factorization with Stochastic\n  Structured Mean-Field Variational Inference", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Beta process is the standard nonparametric Bayesian prior for latent factor\nmodel. In this paper, we derive a structured mean-field variational inference\nalgorithm for a beta process non-negative matrix factorization (NMF) model with\nPoisson likelihood. Unlike the linear Gaussian model, which is well-studied in\nthe nonparametric Bayesian literature, NMF model with beta process prior does\nnot enjoy the conjugacy. We leverage the recently developed stochastic\nstructured mean-field variational inference to relax the conjugacy constraint\nand restore the dependencies among the latent variables in the approximating\nvariational distribution. Preliminary results on both synthetic and real\nexamples demonstrate that the proposed inference algorithm can reasonably\nrecover the hidden structure of the data.\n", "versions": [{"version": "v1", "created": "Fri, 7 Nov 2014 00:51:03 GMT"}, {"version": "v2", "created": "Tue, 2 Dec 2014 05:23:23 GMT"}], "update_date": "2014-12-03", "authors_parsed": [["Liang", "Dawen", ""], ["Hoffman", "Matthew D.", ""]]}, {"id": "1411.1805", "submitter": "John Lafferty", "authors": "Min Xu, Minhua Chen and John Lafferty", "title": "Faithful Variable Screening for High-Dimensional Convex Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of variable selection in convex nonparametric\nregression. Under the assumption that the true regression function is convex\nand sparse, we develop a screening procedure to select a subset of variables\nthat contains the relevant variables. Our approach is a two-stage quadratic\nprogramming method that estimates a sum of one-dimensional convex functions,\nfollowed by one-dimensional concave regression fits on the residuals. In\ncontrast to previous methods for sparse additive models, the optimization is\nfinite dimensional and requires no tuning parameters for smoothness. Under\nappropriate assumptions, we prove that the procedure is faithful in the\npopulation setting, yielding no false negatives. We give a finite sample\nstatistical analysis, and introduce algorithms for efficiently carrying out the\nrequired quadratic programs. The approach leads to computational and\nstatistical advantages over fitting a full model, and provides an effective,\npractical approach to variable screening in convex regression.\n", "versions": [{"version": "v1", "created": "Fri, 7 Nov 2014 01:09:40 GMT"}, {"version": "v2", "created": "Tue, 18 Nov 2014 01:13:30 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Xu", "Min", ""], ["Chen", "Minhua", ""], ["Lafferty", "John", ""]]}, {"id": "1411.1810", "submitter": "Stephan Mandt", "authors": "Stephan Mandt, James McInerney, Farhan Abrol, Rajesh Ranganath, and\n  David Blei", "title": "Variational Tempering", "comments": "published version, 8 pages, 4 figures", "journal-ref": "Proceedings of the 19th International Conference on Artificial\n  Intelligence and Statistics (AISTATS 2016), pages 704-712", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference (VI) combined with data subsampling enables approximate\nposterior inference over large data sets, but suffers from poor local optima.\nWe first formulate a deterministic annealing approach for the generic class of\nconditionally conjugate exponential family models. This approach uses a\ndecreasing temperature parameter which deterministically deforms the objective\nduring the course of the optimization. A well-known drawback to this annealing\napproach is the choice of the cooling schedule. We therefore introduce\nvariational tempering, a variational algorithm that introduces a temperature\nlatent variable to the model. In contrast to related work in the Markov chain\nMonte Carlo literature, this algorithm results in adaptive annealing schedules.\nLastly, we develop local variational tempering, which assigns a latent\ntemperature to each data point; this allows for dynamic annealing that varies\nacross data. Compared to the traditional VI, all proposed approaches find\nimproved predictive likelihoods on held-out data.\n", "versions": [{"version": "v1", "created": "Fri, 7 Nov 2014 01:28:41 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2015 10:45:39 GMT"}, {"version": "v3", "created": "Fri, 30 Oct 2015 21:14:58 GMT"}, {"version": "v4", "created": "Sat, 28 May 2016 19:58:17 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Mandt", "Stephan", ""], ["McInerney", "James", ""], ["Abrol", "Farhan", ""], ["Ranganath", "Rajesh", ""], ["Blei", "David", ""]]}, {"id": "1411.1971", "submitter": "Jiaxin Zhang", "authors": "Xiangyang Zhou, Jiaxin Zhang, Brian Kulis", "title": "Power-Law Graph Cuts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms based on spectral graph cut objectives such as normalized cuts,\nratio cuts and ratio association have become popular in recent years because\nthey are widely applicable and simple to implement via standard eigenvector\ncomputations. Despite strong performance for a number of clustering tasks,\nspectral graph cut algorithms still suffer from several limitations: first,\nthey require the number of clusters to be known in advance, but this\ninformation is often unknown a priori; second, they tend to produce clusters\nwith uniform sizes. In some cases, the true clusters exhibit a known size\ndistribution; in image segmentation, for instance, human-segmented images tend\nto yield segment sizes that follow a power-law distribution. In this paper, we\npropose a general framework of power-law graph cut algorithms that produce\nclusters whose sizes are power-law distributed, and also does not fix the\nnumber of clusters upfront. To achieve our goals, we treat the Pitman-Yor\nexchangeable partition probability function (EPPF) as a regularizer to graph\ncut objectives. Because the resulting objectives cannot be solved by relaxing\nvia eigenvectors, we derive a simple iterative algorithm to locally optimize\nthe objectives. Moreover, we show that our proposed algorithm can be viewed as\nperforming MAP inference on a particular Pitman-Yor mixture model. Our\nexperiments on various data sets show the effectiveness of our algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 29 Oct 2014 20:46:20 GMT"}, {"version": "v2", "created": "Tue, 25 Nov 2014 21:41:07 GMT"}], "update_date": "2014-11-27", "authors_parsed": [["Zhou", "Xiangyang", ""], ["Zhang", "Jiaxin", ""], ["Kulis", "Brian", ""]]}, {"id": "1411.1990", "submitter": "Marwa El Halabi", "authors": "Marwa El Halabi and Volkan Cevher", "title": "A totally unimodular view of structured sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a simple framework for structured sparse recovery based\non convex optimization. We show that many structured sparsity models can be\nnaturally represented by linear matrix inequalities on the support of the\nunknown parameters, where the constraint matrix has a totally unimodular (TU)\nstructure. For such structured models, tight convex relaxations can be obtained\nin polynomial time via linear programming. Our modeling framework unifies the\nprevalent structured sparsity norms in the literature, introduces new\ninteresting ones, and renders their tightness and tractability arguments\ntransparent.\n", "versions": [{"version": "v1", "created": "Fri, 7 Nov 2014 17:24:30 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2015 14:23:47 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["Halabi", "Marwa El", ""], ["Cevher", "Volkan", ""]]}, {"id": "1411.1997", "submitter": "Barbara Engelhardt", "authors": "Chuan Gao, Shiwen Zhao, Ian C. McDowell, Christopher D. Brown, Barbara\n  E. Engelhardt", "title": "Differential gene co-expression networks via Bayesian biclustering\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.GN q-bio.MN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying latent structure in large data matrices is essential for\nexploring biological processes. Here, we consider recovering gene co-expression\nnetworks from gene expression data, where each network encodes relationships\nbetween genes that are locally co-regulated by shared biological mechanisms. To\ndo this, we develop a Bayesian statistical model for biclustering to infer\nsubsets of co-regulated genes whose covariation may be observed in only a\nsubset of the samples. Our biclustering method, BicMix, has desirable\nproperties, including allowing overcomplete representations of the data,\ncomputational tractability, and jointly modeling unknown confounders and\nbiological signals. Compared with related biclustering methods, BicMix recovers\nlatent structure with higher precision across diverse simulation scenarios.\nFurther, we develop a method to recover gene co-expression networks from the\nestimated sparse biclustering matrices. We apply BicMix to breast cancer gene\nexpression data and recover a gene co-expression network that is differential\nacross ER+ and ER- samples.\n", "versions": [{"version": "v1", "created": "Fri, 7 Nov 2014 17:50:48 GMT"}], "update_date": "2014-11-10", "authors_parsed": [["Gao", "Chuan", ""], ["Zhao", "Shiwen", ""], ["McDowell", "Ian C.", ""], ["Brown", "Christopher D.", ""], ["Engelhardt", "Barbara E.", ""]]}, {"id": "1411.2003", "submitter": "Shuyang Gao", "authors": "Shuyang Gao, Greg Ver Steeg and Aram Galstyan", "title": "Efficient Estimation of Mutual Information for Strongly Dependent\n  Variables", "comments": "13 pages, to appear in International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate that a popular class of nonparametric mutual information (MI)\nestimators based on k-nearest-neighbor graphs requires number of samples that\nscales exponentially with the true MI. Consequently, accurate estimation of MI\nbetween two strongly dependent variables is possible only for prohibitively\nlarge sample size. This important yet overlooked shortcoming of the existing\nestimators is due to their implicit reliance on local uniformity of the\nunderlying joint distribution. We introduce a new estimator that is robust to\nlocal non-uniformity, works well with limited data, and is able to capture\nrelationship strengths over many orders of magnitude. We demonstrate the\nsuperior performance of the proposed estimator on both synthetic and real-world\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 7 Nov 2014 19:00:57 GMT"}, {"version": "v2", "created": "Mon, 2 Feb 2015 21:58:17 GMT"}, {"version": "v3", "created": "Thu, 5 Mar 2015 22:10:18 GMT"}], "update_date": "2015-03-09", "authors_parsed": [["Gao", "Shuyang", ""], ["Steeg", "Greg Ver", ""], ["Galstyan", "Aram", ""]]}, {"id": "1411.2005", "submitter": "Alexander Matthews BA MSci MA (Cantab)", "authors": "James Hensman, Alex Matthews, Zoubin Ghahramani", "title": "Scalable Variational Gaussian Process Classification", "comments": "16 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian process classification is a popular method with a number of\nappealing properties. We show how to scale the model within a variational\ninducing point framework, outperforming the state of the art on benchmark\ndatasets. Importantly, the variational formulation can be exploited to allow\nclassification in problems with millions of data points, as we demonstrate in\nexperiments.\n", "versions": [{"version": "v1", "created": "Fri, 7 Nov 2014 19:01:19 GMT"}], "update_date": "2014-11-10", "authors_parsed": [["Hensman", "James", ""], ["Matthews", "Alex", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1411.2045", "submitter": "Kevin Moon", "authors": "Kevin R. Moon and Alfred O. Hero III", "title": "Multivariate f-Divergence Estimation With Confidence", "comments": "20 pages, 1 figure. Accepted to NIPS 2014 (supplementary material is\n  included in the appendices)", "journal-ref": "K.R. Moon and A.O. Hero III, \"Multivariate f-Divergence Estimation\n  With Confidence,\" In Advances in Neural Information Processing Systems, pp.\n  2420-2428, 2014", "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of f-divergence estimation is important in the fields of machine\nlearning, information theory, and statistics. While several nonparametric\ndivergence estimators exist, relatively few have known convergence properties.\nIn particular, even for those estimators whose MSE convergence rates are known,\nthe asymptotic distributions are unknown. We establish the asymptotic normality\nof a recently proposed ensemble estimator of f-divergence between two\ndistributions from a finite number of samples. This estimator has MSE\nconvergence rate of O(1/T), is simple to implement, and performs well in high\ndimensions. This theory enables us to perform divergence-based inference tasks\nsuch as testing equality of pairs of distributions based on empirical samples.\nWe experimentally validate our theoretical results and, as an illustration, use\nthem to empirically bound the best achievable classification error.\n", "versions": [{"version": "v1", "created": "Fri, 7 Nov 2014 22:01:43 GMT"}], "update_date": "2015-03-16", "authors_parsed": [["Moon", "Kevin R.", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1411.2066", "submitter": "Zoltan Szabo", "authors": "Zoltan Szabo, Bharath Sriperumbudur, Barnabas Poczos, Arthur Gretton", "title": "Learning Theory for Distribution Regression", "comments": "Final version appeared at JMLR, with supplement. Code:\n  https://bitbucket.org/szzoli/ite/. arXiv admin note: text overlap with\n  arXiv:1402.1754", "journal-ref": "Journal of Machine Learning Research, 17(152):1-40, 2016", "doi": null, "report-no": null, "categories": "math.ST cs.LG math.FA stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the distribution regression problem: regressing to vector-valued\noutputs from probability measures. Many important machine learning and\nstatistical tasks fit into this framework, including multi-instance learning\nand point estimation problems without analytical solution (such as\nhyperparameter or entropy estimation). Despite the large number of available\nheuristics in the literature, the inherent two-stage sampled nature of the\nproblem makes the theoretical analysis quite challenging, since in practice\nonly samples from sampled distributions are observable, and the estimates have\nto rely on similarities computed between sets of points. To the best of our\nknowledge, the only existing technique with consistency guarantees for\ndistribution regression requires kernel density estimation as an intermediate\nstep (which often performs poorly in practice), and the domain of the\ndistributions to be compact Euclidean. In this paper, we study a simple,\nanalytically computable, ridge regression-based alternative to distribution\nregression, where we embed the distributions to a reproducing kernel Hilbert\nspace, and learn the regressor from the embeddings to the outputs. Our main\ncontribution is to prove that this scheme is consistent in the two-stage\nsampled setup under mild conditions (on separable topological domains enriched\nwith kernels): we present an exact computational-statistical efficiency\ntrade-off analysis showing that our estimator is able to match the one-stage\nsampled minimax optimal rate [Caponnetto and De Vito, 2007; Steinwart et al.,\n2009]. This result answers a 17-year-old open question, establishing the\nconsistency of the classical set kernel [Haussler, 1999; Gaertner et. al, 2002]\nin regression. We also cover consistency for more recent kernels on\ndistributions, including those due to [Christmann and Steinwart, 2010].\n", "versions": [{"version": "v1", "created": "Sat, 8 Nov 2014 01:16:44 GMT"}, {"version": "v2", "created": "Sat, 6 Dec 2014 23:49:00 GMT"}, {"version": "v3", "created": "Tue, 19 Jan 2016 22:03:20 GMT"}, {"version": "v4", "created": "Fri, 21 Oct 2016 15:46:35 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Szabo", "Zoltan", ""], ["Sriperumbudur", "Bharath", ""], ["Poczos", "Barnabas", ""], ["Gretton", "Arthur", ""]]}, {"id": "1411.2158", "submitter": "Norbert Binkiewicz", "authors": "Norbert Binkiewicz, Joshua T. Vogelstein, and Karl Rohe", "title": "Covariate-assisted spectral clustering", "comments": "28 pages, 4 figures, includes substantial changes to theoretical\n  results", "journal-ref": "Biometrika, Volume 104, Issue 2, 1 June 2017, Pages 361-377", "doi": "10.1093/biomet/asx008", "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biological and social systems consist of myriad interacting units. The\ninteractions can be represented in the form of a graph or network. Measurements\nof these graphs can reveal the underlying structure of these interactions,\nwhich provides insight into the systems that generated the graphs. Moreover, in\napplications such as connectomics, social networks, and genomics, graph data\nare accompanied by contextualizing measures on each node. We utilize these node\ncovariates to help uncover latent communities in a graph, using a modification\nof spectral clustering. Statistical guarantees are provided under a joint\nmixture model that we call the node-contextualized stochastic blockmodel,\nincluding a bound on the mis-clustering rate. The bound is used to derive\nconditions for achieving perfect clustering. For most simulated cases,\ncovariate-assisted spectral clustering yields results superior to regularized\nspectral clustering without node covariates and to an adaptation of canonical\ncorrelation analysis. We apply our clustering method to large brain graphs\nderived from diffusion MRI data, using the node locations or neurological\nregion membership as covariates. In both cases, covariate-assisted spectral\nclustering yields clusters that are easier to interpret neurologically.\n", "versions": [{"version": "v1", "created": "Sat, 8 Nov 2014 20:14:59 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2015 19:14:01 GMT"}, {"version": "v3", "created": "Tue, 22 Dec 2015 04:51:04 GMT"}, {"version": "v4", "created": "Thu, 3 Mar 2016 04:07:22 GMT"}, {"version": "v5", "created": "Sun, 30 Oct 2016 04:22:47 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Binkiewicz", "Norbert", ""], ["Vogelstein", "Joshua T.", ""], ["Rohe", "Karl", ""]]}, {"id": "1411.2305", "submitter": "Xun Zheng", "authors": "Xun Zheng, Jin Kyu Kim, Qirong Ho, Eric P. Xing", "title": "Model-Parallel Inference for Big Topic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real world industrial applications of topic modeling, the ability to\ncapture gigantic conceptual space by learning an ultra-high dimensional topical\nrepresentation, i.e., the so-called \"big model\", is becoming the next\ndesideratum after enthusiasms on \"big data\", especially for fine-grained\ndownstream tasks such as online advertising, where good performances are\nusually achieved by regression-based predictors built on millions if not\nbillions of input features. The conventional data-parallel approach for\ntraining gigantic topic models turns out to be rather inefficient in utilizing\nthe power of parallelism, due to the heavy dependency on a centralized image of\n\"model\". Big model size also poses another challenge on the storage, where\navailable model size is bounded by the smallest RAM of nodes. To address these\nissues, we explore another type of parallelism, namely model-parallelism, which\nenables training of disjoint blocks of a big topic model in parallel. By\nintegrating data-parallelism with model-parallelism, we show that dependencies\nbetween distributed elements can be handled seamlessly, achieving not only\nfaster convergence but also an ability to tackle significantly bigger model\nsize. We describe an architecture for model-parallel inference of LDA, and\npresent a variant of collapsed Gibbs sampling algorithm tailored for it.\nExperimental results demonstrate the ability of this system to handle topic\nmodeling with unprecedented amount of 200 billion model variables only on a\nlow-end cluster with very limited computational resources and bandwidth.\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2014 01:25:30 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Zheng", "Xun", ""], ["Kim", "Jin Kyu", ""], ["Ho", "Qirong", ""], ["Xing", "Eric P.", ""]]}, {"id": "1411.2316", "submitter": "Vishnu Naresh Boddeti", "authors": "Joseph A. Fernandez, Vishnu Naresh Boddeti, Andres Rodriguez, B. V. K.\n  Vijaya Kumar", "title": "Zero-Aliasing Correlation Filters for Object Recognition", "comments": "14 pages, to appear in IEEE Transactions on Pattern Analysis and\n  Machine Intelligence (PAMI)", "journal-ref": null, "doi": "10.1109/TPAMI.2014.2375215", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlation filters (CFs) are a class of classifiers that are attractive for\nobject localization and tracking applications. Traditionally, CFs have been\ndesigned in the frequency domain using the discrete Fourier transform (DFT),\nwhere correlation is efficiently implemented. However, existing CF designs do\nnot account for the fact that the multiplication of two DFTs in the frequency\ndomain corresponds to a circular correlation in the time/spatial domain.\nBecause this was previously unaccounted for, prior CF designs are not truly\noptimal, as their optimization criteria do not accurately quantify their\noptimization intention. In this paper, we introduce new zero-aliasing\nconstraints that completely eliminate this aliasing problem by ensuring that\nthe optimization criterion for a given CF corresponds to a linear correlation\nrather than a circular correlation. This means that previous CF designs can be\nsignificantly improved by this reformulation. We demonstrate the benefits of\nthis new CF design approach with several important CFs. We present experimental\nresults on diverse data sets and present solutions to the computational\nchallenges associated with computing these CFs. Code for the CFs described in\nthis paper and their respective zero-aliasing versions is available at\nhttp://vishnu.boddeti.net/projects/correlation-filters.html\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2014 03:48:21 GMT"}, {"version": "v2", "created": "Wed, 19 Nov 2014 15:10:22 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Fernandez", "Joseph A.", ""], ["Boddeti", "Vishnu Naresh", ""], ["Rodriguez", "Andres", ""], ["Kumar", "B. V. K. Vijaya", ""]]}, {"id": "1411.2331", "submitter": "Makoto Yamada", "authors": "Makoto Yamada, Avishek Saha, Hua Ouyang, Dawei Yin, Yi Chang", "title": "N$^3$LARS: Minimum Redundancy Maximum Relevance Feature Selection for\n  Large and High-dimensional Data", "comments": "arXiv admin note: text overlap with arXiv:1202.0515", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a feature selection method that finds non-redundant features from\na large and high-dimensional data in nonlinear way. Specifically, we propose a\nnonlinear extension of the non-negative least-angle regression (LARS) called\nN${}^3$LARS, where the similarity between input and output is measured through\nthe normalized version of the Hilbert-Schmidt Independence Criterion (HSIC). An\nadvantage of N${}^3$LARS is that it can easily incorporate with map-reduce\nframeworks such as Hadoop and Spark. Thus, with the help of distributed\ncomputing, a set of features can be efficiently selected from a large and\nhigh-dimensional data. Moreover, N${}^3$LARS is a convex method and can find a\nglobal optimum solution. The effectiveness of the proposed method is first\ndemonstrated through feature selection experiments for classification and\nregression with small and high-dimensional datasets. Finally, we evaluate our\nproposed method over a large and high-dimensional biology dataset.\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2014 05:43:28 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Yamada", "Makoto", ""], ["Saha", "Avishek", ""], ["Ouyang", "Hua", ""], ["Yin", "Dawei", ""], ["Chang", "Yi", ""]]}, {"id": "1411.2337", "submitter": "Chen Fang", "authors": "Chen Fang and Daniel N. Rockmore", "title": "Multi-Task Metric Learning on Network Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning (MTL) improves prediction performance in different\ncontexts by learning models jointly on multiple different, but related tasks.\nNetwork data, which are a priori data with a rich relational structure, provide\nan important context for applying MTL. In particular, the explicit relational\nstructure implies that network data is not i.i.d. data. Network data also often\ncomes with significant metadata (i.e., attributes) associated with each entity\n(node). Moreover, due to the diversity and variation in network data (e.g.,\nmulti-relational links or multi-category entities), various tasks can be\nperformed and often a rich correlation exists between them. Learning algorithms\nshould exploit all of these additional sources of information for better\nperformance. In this work we take a metric-learning point of view for the MTL\nproblem in the network context. Our approach builds on structure preserving\nmetric learning (SPML). In particular SPML learns a Mahalanobis distance metric\nfor node attributes using network structure as supervision, so that the learned\ndistance function encodes the structure and can be used to predict link\npatterns from attributes. SPML is described for single-task learning on single\nnetwork. Herein, we propose a multi-task version of SPML, abbreviated as\nMT-SPML, which is able to learn across multiple related tasks on multiple\nnetworks via shared intermediate parametrization. MT-SPML learns a specific\nmetric for each task and a common metric for all tasks. The task correlation is\ncarried through the common metric and the individual metrics encode task\nspecific information. When combined together, they are structure-preserving\nwith respect to individual tasks. MT-SPML works on general networks, thus is\nsuitable for a wide variety of problems. In experiments, we challenge MT-SPML\non two real-word problems, where MT-SPML achieves significant improvement.\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2014 06:41:20 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Fang", "Chen", ""], ["Rockmore", "Daniel N.", ""]]}, {"id": "1411.2374", "submitter": "Aur\\'elien Bellet", "authors": "Kuan Liu and Aur\\'elien Bellet and Fei Sha", "title": "Similarity Learning for High-Dimensional Sparse Data", "comments": "14 pages. Proceedings of the 18th International Conference on\n  Artificial Intelligence and Statistics (AISTATS 2015). Matlab code:\n  https://github.com/bellet/HDSL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A good measure of similarity between data points is crucial to many tasks in\nmachine learning. Similarity and metric learning methods learn such measures\nautomatically from data, but they do not scale well respect to the\ndimensionality of the data. In this paper, we propose a method that can learn\nefficiently similarity measure from high-dimensional sparse data. The core idea\nis to parameterize the similarity measure as a convex combination of rank-one\nmatrices with specific sparsity structures. The parameters are then optimized\nwith an approximate Frank-Wolfe procedure to maximally satisfy relative\nsimilarity constraints on the training data. Our algorithm greedily\nincorporates one pair of features at a time into the similarity measure,\nproviding an efficient way to control the number of active features and thus\nreduce overfitting. It enjoys very appealing convergence guarantees and its\ntime and memory complexity depends on the sparsity of the data instead of the\ndimension of the feature space. Our experiments on real-world high-dimensional\ndatasets demonstrate its potential for classification, dimensionality reduction\nand data exploration.\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2014 10:40:47 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2015 13:45:00 GMT"}, {"version": "v3", "created": "Mon, 9 Sep 2019 16:53:40 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Liu", "Kuan", ""], ["Bellet", "Aur\u00e9lien", ""], ["Sha", "Fei", ""]]}, {"id": "1411.2405", "submitter": "Zahra Sabet Sarvestani", "authors": "Zahra Sabetsarvestani, Hamidreza Amindavar", "title": "Sparse Estimation with Generalized Beta Mixture and the Horseshoe Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the use of the Generalized Beta Mixture (GBM) and Horseshoe\ndistributions as priors in the Bayesian Compressive Sensing framework is\nproposed. The distributions are considered in a two-layer hierarchical model,\nmaking the corresponding inference problem amenable to Expectation Maximization\n(EM). We present an explicit, algebraic EM-update rule for the models, yielding\ntwo fast and experimentally validated algorithms for signal recovery.\nExperimental results show that our algorithms outperform state-of-the-art\nmethods on a wide range of sparsity levels and amplitudes in terms of\nreconstruction accuracy, convergence rate and sparsity. The largest improvement\ncan be observed for sparse signals with high amplitudes.\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2014 13:01:10 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Sabetsarvestani", "Zahra", ""], ["Amindavar", "Hamidreza", ""]]}, {"id": "1411.2540", "submitter": "Yu-Hui Chen", "authors": "Yu-Hui Chen, Dennis Wei, Gregory Newstadt, Marc DeGraef, Jeffrey\n  Simmons, Alfred Hero", "title": "Parameter estimation in spherical symmetry groups", "comments": "4 pages, 1 page with only references and 1 page for appendices.\n  Accepted to be published in Signal Processing Letters", "journal-ref": null, "doi": "10.1109/LSP.2014.2387206", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers statistical estimation problems where the probability\ndistribution of the observed random variable is invariant with respect to\nactions of a finite topological group. It is shown that any such distribution\nmust satisfy a restricted finite mixture representation. When specialized to\nthe case of distributions over the sphere that are invariant to the actions of\na finite spherical symmetry group $\\mathcal G$, a group-invariant extension of\nthe Von Mises Fisher (VMF) distribution is obtained. The $\\mathcal G$-invariant\nVMF is parameterized by location and scale parameters that specify the\ndistribution's mean orientation and its concentration about the mean,\nrespectively. Using the restricted finite mixture representation these\nparameters can be estimated using an Expectation Maximization (EM) maximum\nlikelihood (ML) estimation algorithm. This is illustrated for the problem of\nmean crystal orientation estimation under the spherically symmetric group\nassociated with the crystal form, e.g., cubic or octahedral or hexahedral.\nSimulations and experiments establish the advantages of the extended VMF EM-ML\nestimator for data acquired by Electron Backscatter Diffraction (EBSD)\nmicroscopy of a polycrystalline Nickel alloy sample.\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2014 19:11:41 GMT"}, {"version": "v2", "created": "Sun, 21 Dec 2014 18:16:12 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Chen", "Yu-Hui", ""], ["Wei", "Dennis", ""], ["Newstadt", "Gregory", ""], ["DeGraef", "Marc", ""], ["Simmons", "Jeffrey", ""], ["Hero", "Alfred", ""]]}, {"id": "1411.2581", "submitter": "Rajesh Ranganath", "authors": "Rajesh Ranganath, Linpeng Tang, Laurent Charlin, David M. Blei", "title": "Deep Exponential Families", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe \\textit{deep exponential families} (DEFs), a class of latent\nvariable models that are inspired by the hidden structures used in deep neural\nnetworks. DEFs capture a hierarchy of dependencies between latent variables,\nand are easily generalized to many settings through exponential families. We\nperform inference using recent \"black box\" variational inference techniques. We\nthen evaluate various DEFs on text and combine multiple DEFs into a model for\npairwise recommendation data. In an extensive study, we show that going beyond\none layer improves predictions for DEFs. We demonstrate that DEFs find\ninteresting exploratory structure in large data sets, and give better\npredictive performance than state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2014 20:57:30 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Ranganath", "Rajesh", ""], ["Tang", "Linpeng", ""], ["Charlin", "Laurent", ""], ["Blei", "David M.", ""]]}, {"id": "1411.2674", "submitter": "Fangjian Guo", "authors": "Fangjian Guo, Charles Blundell, Hanna Wallach and Katherine Heller", "title": "The Bayesian Echo Chamber: Modeling Social Influence via Linguistic\n  Accommodation", "comments": "14 pages, 7 figures, to appear in AISTATS 2015. Fixed minor\n  formatting issues", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Bayesian Echo Chamber, a new Bayesian generative model for\nsocial interaction data. By modeling the evolution of people's language usage\nover time, this model discovers latent influence relationships between them.\nUnlike previous work on inferring influence, which has primarily focused on\nsimple temporal dynamics evidenced via turn-taking behavior, our model captures\nmore nuanced influence relationships, evidenced via linguistic accommodation\npatterns in interaction content. The model, which is based on a discrete analog\nof the multivariate Hawkes process, permits a fully Bayesian inference\nalgorithm. We validate our model's ability to discover latent influence\npatterns using transcripts of arguments heard by the US Supreme Court and the\nmovie \"12 Angry Men.\" We showcase our model's capabilities by using it to infer\nlatent influence patterns from Federal Open Market Committee meeting\ntranscripts, demonstrating state-of-the-art performance at uncovering social\ndynamics in group discussions.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2014 01:02:20 GMT"}, {"version": "v2", "created": "Sun, 25 Jan 2015 18:39:13 GMT"}, {"version": "v3", "created": "Tue, 27 Jan 2015 19:37:09 GMT"}], "update_date": "2015-01-28", "authors_parsed": [["Guo", "Fangjian", ""], ["Blundell", "Charles", ""], ["Wallach", "Hanna", ""], ["Heller", "Katherine", ""]]}, {"id": "1411.2698", "submitter": "Barbara Engelhardt", "authors": "Shiwen Zhao and Chuan Gao and Sayan Mukherjee and Barbara E Engelhardt", "title": "Bayesian group latent factor analysis with structured sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent factor models are the canonical statistical tool for exploratory\nanalyses of low-dimensional linear structure for an observation matrix with p\nfeatures across n samples. We develop a structured Bayesian group factor\nanalysis model that extends the factor model to multiple coupled observation\nmatrices; in the case of two observations, this reduces to a Bayesian model of\ncanonical correlation analysis. The main contribution of this work is to\ncarefully define a structured Bayesian prior that encourages both element-wise\nand column-wise shrinkage and leads to desirable behavior on high-dimensional\ndata. In particular, our model puts a structured prior on the joint factor\nloading matrix, regularizing at three levels, which enables element-wise\nsparsity and unsupervised recovery of latent factors corresponding to\nstructured variance across arbitrary subsets of the observations. In addition,\nour structured prior allows for both dense and sparse latent factors so that\ncovariation among either all features or only a subset of features can both be\nrecovered. We use fast parameter-expanded expectation-maximization for\nparameter estimation in this model. We validate our method on both simulated\ndata with substantial structure and real data, comparing against a number of\nstate-of-the-art approaches. These results illustrate useful properties of our\nmodel, including i) recovering sparse signal in the presence of dense effects;\nii) the ability to scale naturally to large numbers of observations; iii)\nflexible observation- and factor-specific regularization to recover factors\nwith a wide variety of sparsity levels and percentage of variance explained;\nand iv) tractable inference that scales to modern genomic and document data\nsizes.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2014 04:50:32 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2015 17:24:03 GMT"}], "update_date": "2015-11-12", "authors_parsed": [["Zhao", "Shiwen", ""], ["Gao", "Chuan", ""], ["Mukherjee", "Sayan", ""], ["Engelhardt", "Barbara E", ""]]}, {"id": "1411.2820", "submitter": "Sharon Lee", "authors": "Sharon X. Lee, Geoffrey J. McLachlan, Saumyadipta Pyne", "title": "Supervised Classification of Flow Cytometric Samples via the Joint\n  Clustering and Matching (JCM) Procedure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the use of the Joint Clustering and Matching (JCM) procedure for\nthe supervised classification of a flow cytometric sample with respect to a\nnumber of predefined classes of such samples. The JCM procedure has been\nproposed as a method for the unsupervised classification of cells within a\nsample into a number of clusters and in the case of multiple samples, the\nmatching of these clusters across the samples. The two tasks of clustering and\nmatching of the clusters are performed simultaneously within the JCM framework.\nIn this paper, we consider the case where there is a number of distinct classes\nof samples whose class of origin is known, and the problem is to classify a new\nsample of unknown class of origin to one of these predefined classes. For\nexample, the different classes might correspond to the types of a particular\ndisease or to the various health outcomes of a patient subsequent to a course\nof treatment. We show and demonstrate on some real datasets how the JCM\nprocedure can be used to carry out this supervised classification task. A\nmixture distribution is used to model the distribution of the expressions of a\nfixed set of markers for each cell in a sample with the components in the\nmixture model corresponding to the various populations of cells in the\ncomposition of the sample. For each class of samples, a class template is\nformed by the adoption of random-effects terms to model the inter-sample\nvariation within a class. The classification of a new unclassified sample is\nundertaken by assigning the unclassified sample to the class that minimizes the\nKullback-Leibler distance between its fitted mixture density and each class\ndensity provided by the class templates.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2014 14:22:32 GMT"}], "update_date": "2014-11-12", "authors_parsed": [["Lee", "Sharon X.", ""], ["McLachlan", "Geoffrey J.", ""], ["Pyne", "Saumyadipta", ""]]}, {"id": "1411.3013", "submitter": "Kevin H. Knuth", "authors": "Kevin H. Knuth, Michael Habeck, Nabin K. Malakar, Asim M. Mubeen, Ben\n  Placek", "title": "Bayesian Evidence and Model Selection", "comments": "Arxiv version consists of 58 pages and 9 figures. Features theory,\n  numerical methods and four applications", "journal-ref": "Digital Signal Processing, 47:50-67 (2015)", "doi": "10.1016/j.dsp.2015.06.012", "report-no": null, "categories": "stat.ME astro-ph.IM stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we review the concepts of Bayesian evidence and Bayes factors,\nalso known as log odds ratios, and their application to model selection. The\ntheory is presented along with a discussion of analytic, approximate and\nnumerical techniques. Specific attention is paid to the Laplace approximation,\nvariational Bayes, importance sampling, thermodynamic integration, and nested\nsampling and its recent variants. Analogies to statistical physics, from which\nmany of these techniques originate, are discussed in order to provide readers\nwith deeper insights that may lead to new techniques. The utility of Bayesian\nmodel testing in the domain sciences is demonstrated by presenting four\nspecific practical examples considered within the context of signal processing\nin the areas of signal detection, sensor characterization, scientific model\nselection and molecular force characterization.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2014 23:08:54 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2015 07:13:30 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Knuth", "Kevin H.", ""], ["Habeck", "Michael", ""], ["Malakar", "Nabin K.", ""], ["Mubeen", "Asim M.", ""], ["Placek", "Ben", ""]]}, {"id": "1411.3128", "submitter": "Dimitrios Kotzias", "authors": "Dimitrios Kotzias, Misha Denil, Phil Blunsom, Nando de Freitas", "title": "Deep Multi-Instance Transfer Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach for transferring knowledge from groups to\nindividuals that comprise them. We evaluate our method in text, by inferring\nthe ratings of individual sentences using full-review ratings. This approach,\nwhich combines ideas from transfer learning, deep learning and multi-instance\nlearning, reduces the need for laborious human labelling of fine-grained data\nwhen abundant labels are available at the group level.\n", "versions": [{"version": "v1", "created": "Wed, 12 Nov 2014 10:40:52 GMT"}, {"version": "v2", "created": "Wed, 10 Dec 2014 15:55:12 GMT"}], "update_date": "2014-12-11", "authors_parsed": [["Kotzias", "Dimitrios", ""], ["Denil", "Misha", ""], ["Blunsom", "Phil", ""], ["de Freitas", "Nando", ""]]}, {"id": "1411.3169", "submitter": "Ali Ghaderi Ph.D", "authors": "Ali Ghaderi", "title": "On Coarse Graining of Information and Its Application to Pattern\n  Recognition", "comments": null, "journal-ref": null, "doi": "10.1063/1.4906011", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method based on finite mixture models for classifying a set of\nobservations into number of different categories. In order to demonstrate the\nmethod, we show how the component densities for the mixture model can be\nderived by using the maximum entropy method in conjunction with conservation of\nPythagorean means. Several examples of distributions belonging to the\nPythagorean family are derived. A discussion on estimation of model parameters\nand the number of categories is also given.\n", "versions": [{"version": "v1", "created": "Wed, 12 Nov 2014 13:21:48 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Ghaderi", "Ali", ""]]}, {"id": "1411.3224", "submitter": "L.A. Prashanth", "authors": "Nathaniel Korda and L.A. Prashanth", "title": "On TD(0) with function approximation: Concentration bounds and a\n  centered variant with exponential convergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide non-asymptotic bounds for the well-known temporal difference\nlearning algorithm TD(0) with linear function approximators. These include\nhigh-probability bounds as well as bounds in expectation. Our analysis suggests\nthat a step-size inversely proportional to the number of iterations cannot\nguarantee optimal rate of convergence unless we assume (partial) knowledge of\nthe stationary distribution for the Markov chain underlying the policy\nconsidered. We also provide bounds for the iterate averaged TD(0) variant,\nwhich gets rid of the step-size dependency while exhibiting the optimal rate of\nconvergence. Furthermore, we propose a variant of TD(0) with linear\napproximators that incorporates a centering sequence, and establish that it\nexhibits an exponential rate of convergence in expectation. We demonstrate the\nusefulness of our bounds on two synthetic experimental settings.\n", "versions": [{"version": "v1", "created": "Wed, 12 Nov 2014 16:22:28 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2015 18:20:52 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["Korda", "Nathaniel", ""], ["Prashanth", "L. A.", ""]]}, {"id": "1411.3409", "submitter": "Paul Mineiro", "authors": "Paul Mineiro, Nikos Karampatziakis", "title": "A Randomized Algorithm for CCA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present RandomizedCCA, a randomized algorithm for computing canonical\nanalysis, suitable for large datasets stored either out of core or on a\ndistributed file system. Accurate results can be obtained in as few as two data\npasses, which is relevant for distributed processing frameworks in which\niteration is expensive (e.g., Hadoop). The strategy also provides an excellent\ninitializer for standard iterative solutions.\n", "versions": [{"version": "v1", "created": "Thu, 13 Nov 2014 00:51:19 GMT"}], "update_date": "2014-11-14", "authors_parsed": [["Mineiro", "Paul", ""], ["Karampatziakis", "Nikos", ""]]}, {"id": "1411.3413", "submitter": "Tomoharu Iwata", "authors": "Tomoharu Iwata, Makoto Yamada", "title": "Multi-view Anomaly Detection via Probabilistic Latent Variable Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a nonparametric Bayesian probabilistic latent variable model for\nmulti-view anomaly detection, which is the task of finding instances that have\ninconsistent views. With the proposed model, all views of a non-anomalous\ninstance are assumed to be generated from a single latent vector. On the other\nhand, an anomalous instance is assumed to have multiple latent vectors, and its\ndifferent views are generated from different latent vectors. By inferring the\nnumber of latent vectors used for each instance with Dirichlet process priors,\nwe obtain multi-view anomaly scores. The proposed model can be seen as a robust\nextension of probabilistic canonical correlation analysis for noisy multi-view\ndata. We present Bayesian inference procedures for the proposed model based on\na stochastic EM algorithm. The effectiveness of the proposed model is\ndemonstrated in terms of performance when detecting multi-view anomalies and\nimputing missing values in multi-view data with anomalies.\n", "versions": [{"version": "v1", "created": "Thu, 13 Nov 2014 01:01:01 GMT"}], "update_date": "2014-11-14", "authors_parsed": [["Iwata", "Tomoharu", ""], ["Yamada", "Makoto", ""]]}, {"id": "1411.3436", "submitter": "Shai Shalev-Shwartz", "authors": "Shai Shalev-Shwartz", "title": "SelfieBoost: A Boosting Algorithm for Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe and analyze a new boosting algorithm for deep learning called\nSelfieBoost. Unlike other boosting algorithms, like AdaBoost, which construct\nensembles of classifiers, SelfieBoost boosts the accuracy of a single network.\nWe prove a $\\log(1/\\epsilon)$ convergence rate for SelfieBoost under some \"SGD\nsuccess\" assumption which seems to hold in practice.\n", "versions": [{"version": "v1", "created": "Thu, 13 Nov 2014 03:34:32 GMT"}, {"version": "v2", "created": "Sat, 8 Apr 2017 06:06:38 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Shalev-Shwartz", "Shai", ""]]}, {"id": "1411.3650", "submitter": "Azin Ashkan", "authors": "Azin Ashkan, Branislav Kveton, Shlomo Berkovsky, Zheng Wen", "title": "DUM: Diversity-Weighted Utility Maximization for Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for diversification of recommendation lists manifests in a number of\nrecommender systems use cases. However, an increase in diversity may undermine\nthe utility of the recommendations, as relevant items in the list may be\nreplaced by more diverse ones. In this work we propose a novel method for\nmaximizing the utility of the recommended items subject to the diversity of\nuser's tastes, and show that an optimal solution to this problem can be found\ngreedily. We evaluate the proposed method in two online user studies as well as\nin an offline analysis incorporating a number of evaluation metrics. The\nresults of evaluations show the superiority of our method over a number of\nbaselines.\n", "versions": [{"version": "v1", "created": "Thu, 13 Nov 2014 18:27:10 GMT"}], "update_date": "2014-11-14", "authors_parsed": [["Ashkan", "Azin", ""], ["Kveton", "Branislav", ""], ["Berkovsky", "Shlomo", ""], ["Wen", "Zheng", ""]]}, {"id": "1411.3685", "submitter": "Mickael Binois", "authors": "Micka\\\"el Binois (DEMO-ENSMSE), David Ginsbourger ((M\\'ethodes\n  d'Analyse Stochastique des Codes et Traitements Num\\'eriques), IMSV), Olivier\n  Roustant ((M\\'ethodes d'Analyse Stochastique des Codes et Traitements\n  Num\\'eriques), DEMO-ENSMSE)", "title": "A warped kernel improving robustness in Bayesian optimization via random\n  embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This works extends the Random Embedding Bayesian Optimization approach by\nintegrating a warping of the high dimensional subspace within the covariance\nkernel. The proposed warping, that relies on elementary geometric\nconsiderations, allows mitigating the drawbacks of the high extrinsic\ndimensionality while avoiding the algorithm to evaluate points giving redundant\ninformation. It also alleviates constraints on bound selection for the embedded\ndomain, thus improving the robustness, as illustrated with a test case with 25\nvariables and intrinsic dimension 6.\n", "versions": [{"version": "v1", "created": "Thu, 13 Nov 2014 19:57:54 GMT"}, {"version": "v2", "created": "Tue, 24 Feb 2015 10:33:40 GMT"}, {"version": "v3", "created": "Wed, 18 Mar 2015 07:47:20 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Binois", "Micka\u00ebl", "", "DEMO-ENSMSE"], ["Ginsbourger", "David", ""], ["Roustant", "Olivier", ""]]}, {"id": "1411.3784", "submitter": "Guido F.  Montufar", "authors": "Guido Montufar", "title": "Deep Narrow Boltzmann Machines are Universal Approximators", "comments": "Published as a conference paper at ICLR 2015", "journal-ref": "http://www.iclr.cc/doku.php?id=iclr2015:main", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that deep narrow Boltzmann machines are universal approximators of\nprobability distributions on the activities of their visible units, provided\nthey have sufficiently many hidden layers, each containing the same number of\nunits as the visible layer. We show that, within certain parameter domains,\ndeep Boltzmann machines can be studied as feedforward networks. We provide\nupper and lower bounds on the sufficient depth and width of universal\napproximators. These results settle various intuitions regarding undirected\nnetworks and, in particular, they show that deep narrow Boltzmann machines are\nat least as compact universal approximators as narrow sigmoid belief networks\nand restricted Boltzmann machines, with respect to the currently available\nbounds for those models.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 03:50:30 GMT"}, {"version": "v2", "created": "Thu, 26 Feb 2015 18:59:27 GMT"}, {"version": "v3", "created": "Fri, 10 Apr 2015 12:22:14 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Montufar", "Guido", ""]]}, {"id": "1411.3787", "submitter": "Ping Li", "authors": "Anshumali Shrivastava, Ping Li", "title": "Asymmetric Minwise Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DB cs.DS cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minwise hashing (Minhash) is a widely popular indexing scheme in practice.\nMinhash is designed for estimating set resemblance and is known to be\nsuboptimal in many applications where the desired measure is set overlap (i.e.,\ninner product between binary vectors) or set containment. Minhash has inherent\nbias towards smaller sets, which adversely affects its performance in\napplications where such a penalization is not desirable. In this paper, we\npropose asymmetric minwise hashing (MH-ALSH), to provide a solution to this\nproblem. The new scheme utilizes asymmetric transformations to cancel the bias\nof traditional minhash towards smaller sets, making the final \"collision\nprobability\" monotonic in the inner product. Our theoretical comparisons show\nthat for the task of retrieving with binary inner products asymmetric minhash\nis provably better than traditional minhash and other recently proposed hashing\nalgorithms for general inner products. Thus, we obtain an algorithmic\nimprovement over existing approaches in the literature. Experimental\nevaluations on four publicly available high-dimensional datasets validate our\nclaims and the proposed scheme outperforms, often significantly, other hashing\nalgorithms on the task of near neighbor retrieval with set containment. Our\nproposal is simple and easy to implement in practice.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 04:18:33 GMT"}], "update_date": "2014-11-17", "authors_parsed": [["Shrivastava", "Anshumali", ""], ["Li", "Ping", ""]]}, {"id": "1411.3803", "submitter": "Han Liu", "authors": "Mengdi Wang, Ethan X. Fang, Han Liu", "title": "Stochastic Compositional Gradient Descent: Algorithms for Minimizing\n  Compositions of Expected-Value Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical stochastic gradient methods are well suited for minimizing\nexpected-value objective functions. However, they do not apply to the\nminimization of a nonlinear function involving expected values or a composition\nof two expected-value functions, i.e., problems of the form $\\min_x\n\\mathbf{E}_v [f_v\\big(\\mathbf{E}_w [g_w(x)]\\big)]$. In order to solve this\nstochastic composition problem, we propose a class of stochastic compositional\ngradient descent (SCGD) algorithms that can be viewed as stochastic versions of\nquasi-gradient method. SCGD update the solutions based on noisy sample\ngradients of $f_v,g_{w}$ and use an auxiliary variable to track the unknown\nquantity $\\mathbf{E}_w[g_w(x)]$. We prove that the SCGD converge almost surely\nto an optimal solution for convex optimization problems, as long as such a\nsolution exists. The convergence involves the interplay of two iterations with\ndifferent time scales. For nonsmooth convex problems, the SCGD achieve a\nconvergence rate of $O(k^{-1/4})$ in the general case and $O(k^{-2/3})$ in the\nstrongly convex case, after taking $k$ samples. For smooth convex problems, the\nSCGD can be accelerated to converge at a rate of $O(k^{-2/7})$ in the general\ncase and $O(k^{-4/5})$ in the strongly convex case. For nonconvex problems, we\nprove that any limit point generated by SCGD is a stationary point, for which\nwe also provide the convergence rate analysis. Indeed, the stochastic setting\nwhere one wants to optimize compositions of expected-value functions is very\ncommon in practice. The proposed SCGD methods find wide applications in\nlearning, estimation, dynamic programming, etc.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 05:49:01 GMT"}], "update_date": "2014-11-17", "authors_parsed": [["Wang", "Mengdi", ""], ["Fang", "Ethan X.", ""], ["Liu", "Han", ""]]}, {"id": "1411.3825", "submitter": "Kayvan Sadeghi", "authors": "Kayvan Sadeghi and Alessandro Rinaldo", "title": "Statistical Models for Degree Distributions of Networks", "comments": "13 pages. 4 figures, a shorter version to be presented at NIPS\n  workshop 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define and study the statistical models in exponential family form whose\nsufficient statistics are the degree distributions and the bi-degree\ndistributions of undirected labelled simple graphs. Graphs that are constrained\nby the joint degree distributions are called $dK$-graphs in the computer\nscience literature and this paper attempts to provide the first statistically\ngrounded analysis of this type of models. In addition to formalizing these\nmodels, we provide some preliminary results for the parameter estimation and\nthe asymptotic behaviour of the model for degree distribution, and discuss the\nparameter estimation for the model for bi-degree distribution.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 08:30:53 GMT"}], "update_date": "2014-11-17", "authors_parsed": [["Sadeghi", "Kayvan", ""], ["Rinaldo", "Alessandro", ""]]}, {"id": "1411.3972", "submitter": "Philipp Geiger", "authors": "Philipp Geiger, Kun Zhang, Mingming Gong, Dominik Janzing, Bernhard\n  Sch\\\"olkopf", "title": "Causal Inference by Identification of Vector Autoregressive Processes\n  with Hidden Components", "comments": null, "journal-ref": "Extended version of a paper in the Proceedings of the 32nd\n  International Conference on Machine Learning, Lille, France, 2015", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A widely applied approach to causal inference from a non-experimental time\nseries $X$, often referred to as \"(linear) Granger causal analysis\", is to\nregress present on past and interpret the regression matrix $\\hat{B}$ causally.\nHowever, if there is an unmeasured time series $Z$ that influences $X$, then\nthis approach can lead to wrong causal conclusions, i.e., distinct from those\none would draw if one had additional information such as $Z$. In this paper we\ntake a different approach: We assume that $X$ together with some hidden $Z$\nforms a first order vector autoregressive (VAR) process with transition matrix\n$A$, and argue why it is more valid to interpret $A$ causally instead of\n$\\hat{B}$. Then we examine under which conditions the most important parts of\n$A$ are identifiable or almost identifiable from only $X$. Essentially,\nsufficient conditions are (1) non-Gaussian, independent noise or (2) no\ninfluence from $X$ to $Z$. We present two estimation algorithms that are\ntailored towards conditions (1) and (2), respectively, and evaluate them on\nsynthetic and real-world data. We discuss how to check the model using $X$.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 16:54:33 GMT"}, {"version": "v2", "created": "Mon, 18 May 2015 15:21:33 GMT"}, {"version": "v3", "created": "Fri, 22 May 2015 09:20:53 GMT"}, {"version": "v4", "created": "Tue, 22 Dec 2015 18:31:00 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["Geiger", "Philipp", ""], ["Zhang", "Kun", ""], ["Gong", "Mingming", ""], ["Janzing", "Dominik", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1411.4000", "submitter": "Fei Sha", "authors": "Zhiyun Lu and Avner May and Kuan Liu and Alireza Bagheri Garakani and\n  Dong Guo and Aur\\'elien Bellet and Linxi Fan and Michael Collins and Brian\n  Kingsbury and Michael Picheny and Fei Sha", "title": "How to Scale Up Kernel Methods to Be As Good As Deep Neural Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational complexity of kernel methods has often been a major barrier\nfor applying them to large-scale learning problems. We argue that this barrier\ncan be effectively overcome. In particular, we develop methods to scale up\nkernel models to successfully tackle large-scale learning problems that are so\nfar only approachable by deep learning architectures. Based on the seminal work\nby Rahimi and Recht on approximating kernel functions with features derived\nfrom random projections, we advance the state-of-the-art by proposing methods\nthat can efficiently train models with hundreds of millions of parameters, and\nlearn optimal representations from multiple kernels. We conduct extensive\nempirical studies on problems from image recognition and automatic speech\nrecognition, and show that the performance of our kernel models matches that of\nwell-engineered deep neural nets (DNNs). To the best of our knowledge, this is\nthe first time that a direct comparison between these two methods on\nlarge-scale problems is reported. Our kernel methods have several appealing\nproperties: training with convex optimization, cost for training a single model\ncomparable to DNNs, and significantly reduced total cost due to fewer\nhyperparameters to tune for model selection. Our contrastive study between\nthese two very different but equally competitive models sheds light on\nfundamental questions such as how to learn good representations.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 18:24:20 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2015 10:31:35 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Lu", "Zhiyun", ""], ["May", "Avner", ""], ["Liu", "Kuan", ""], ["Garakani", "Alireza Bagheri", ""], ["Guo", "Dong", ""], ["Bellet", "Aur\u00e9lien", ""], ["Fan", "Linxi", ""], ["Collins", "Michael", ""], ["Kingsbury", "Brian", ""], ["Picheny", "Michael", ""], ["Sha", "Fei", ""]]}, {"id": "1411.4005", "submitter": "Miguel Sim\\~oes", "authors": "Miguel Sim\\~oes, Jos\\'e Bioucas-Dias, Luis B. Almeida, Jocelyn\n  Chanussot", "title": "A convex formulation for hyperspectral image superresolution via\n  subspace-based regularization", "comments": "IEEE Trans. Geosci. Remote Sens., to be published", "journal-ref": null, "doi": "10.1109/TGRS.2014.2375320", "report-no": null, "categories": "cs.CV physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral remote sensing images (HSIs) usually have high spectral\nresolution and low spatial resolution. Conversely, multispectral images (MSIs)\nusually have low spectral and high spatial resolutions. The problem of\ninferring images which combine the high spectral and high spatial resolutions\nof HSIs and MSIs, respectively, is a data fusion problem that has been the\nfocus of recent active research due to the increasing availability of HSIs and\nMSIs retrieved from the same geographical area.\n  We formulate this problem as the minimization of a convex objective function\ncontaining two quadratic data-fitting terms and an edge-preserving regularizer.\nThe data-fitting terms account for blur, different resolutions, and additive\nnoise. The regularizer, a form of vector Total Variation, promotes\npiecewise-smooth solutions with discontinuities aligned across the\nhyperspectral bands.\n  The downsampling operator accounting for the different spatial resolutions,\nthe non-quadratic and non-smooth nature of the regularizer, and the very large\nsize of the HSI to be estimated lead to a hard optimization problem. We deal\nwith these difficulties by exploiting the fact that HSIs generally \"live\" in a\nlow-dimensional subspace and by tailoring the Split Augmented Lagrangian\nShrinkage Algorithm (SALSA), which is an instance of the Alternating Direction\nMethod of Multipliers (ADMM), to this optimization problem, by means of a\nconvenient variable splitting. The spatial blur and the spectral linear\noperators linked, respectively, with the HSI and MSI acquisition processes are\nalso estimated, and we obtain an effective algorithm that outperforms the\nstate-of-the-art, as illustrated in a series of experiments with simulated and\nreal-life data.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 18:36:31 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Sim\u00f5es", "Miguel", ""], ["Bioucas-Dias", "Jos\u00e9", ""], ["Almeida", "Luis B.", ""], ["Chanussot", "Jocelyn", ""]]}, {"id": "1411.4068", "submitter": "Anh Pham The", "authors": "Anh T. Pham, Raviv Raich, and Xiaoli Z. Fern", "title": "Dynamic Programming for Instance Annotation in Multi-instance\n  Multi-label Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Labeling data for classification requires significant human effort. To reduce\nlabeling cost, instead of labeling every instance, a group of instances (bag)\nis labeled by a single bag label. Computer algorithms are then used to infer\nthe label for each instance in a bag, a process referred to as instance\nannotation. This task is challenging due to the ambiguity regarding the\ninstance labels. We propose a discriminative probabilistic model for the\ninstance annotation problem and introduce an expectation maximization framework\nfor inference, based on the maximum likelihood approach. For many probabilistic\napproaches, brute-force computation of the instance label posterior probability\ngiven its bag label is exponential in the number of instances in the bag. Our\nkey contribution is a dynamic programming method for computing the posterior\nthat is linear in the number of instances. We evaluate our methods using both\nbenchmark and real world data sets, in the domain of bird song, image\nannotation, and activity recognition. In many cases, the proposed framework\noutperforms, sometimes significantly, the current state-of-the-art MIML\nlearning methods, both in instance label prediction and bag label prediction.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 21:59:38 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Pham", "Anh T.", ""], ["Raich", "Raviv", ""], ["Fern", "Xiaoli Z.", ""]]}, {"id": "1411.4070", "submitter": "Abigail Jacobs", "authors": "Abigail Z. Jacobs and Aaron Clauset", "title": "A unified view of generative models for networks: models, methods,\n  opportunities, and challenges", "comments": "10 pages. To appear at the NIPS 2014 Workshop on Networks: From\n  Graphs to Rich Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on probabilistic models of networks now spans a wide variety of\nfields, including physics, sociology, biology, statistics, and machine\nlearning. These efforts have produced a diverse ecology of models and methods.\nDespite this diversity, many of these models share a common underlying\nstructure: pairwise interactions (edges) are generated with probability\nconditional on latent vertex attributes. Differences between models generally\nstem from different philosophical choices about how to learn from data or\ndifferent empirically-motivated goals. The highly interdisciplinary nature of\nwork on these generative models, however, has inhibited the development of a\nunified view of their similarities and differences. For instance, novel\ntheoretical models and optimization techniques developed in machine learning\nare largely unknown within the social and biological sciences, which have\ninstead emphasized model interpretability. Here, we describe a unified view of\ngenerative models for networks that draws together many of these disparate\nthreads and highlights the fundamental similarities and differences that span\nthese fields. We then describe a number of opportunities and challenges for\nfuture work that are revealed by this view.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 22:00:41 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Jacobs", "Abigail Z.", ""], ["Clauset", "Aaron", ""]]}, {"id": "1411.4072", "submitter": "Bishan Yang", "authors": "Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, Li Deng", "title": "Learning Multi-Relational Semantics Using Neural-Embedding Models", "comments": "7 pages, 2 figures, NIPS 2014 workshop on Learning Semantics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a unified framework for modeling multi-relational\nrepresentations, scoring, and learning, and conduct an empirical study of\nseveral recent multi-relational embedding models under the framework. We\ninvestigate the different choices of relation operators based on linear and\nbilinear transformations, and also the effects of entity representations by\nincorporating unsupervised vectors pre-trained on extra textual resources. Our\nresults show several interesting findings, enabling the design of a simple\nembedding model that achieves the new state-of-the-art performance on a popular\nknowledge base completion task evaluated on Freebase.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 22:08:01 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Yang", "Bishan", ""], ["Yih", "Wen-tau", ""], ["He", "Xiaodong", ""], ["Gao", "Jianfeng", ""], ["Deng", "Li", ""]]}, {"id": "1411.4077", "submitter": "Scott Linderman", "authors": "Scott W. Linderman, Christopher H. Stock, and Ryan P. Adams", "title": "A framework for studying synaptic plasticity with neural spike train\n  data", "comments": "Presented at Neural Information Processing Systems (NIPS) 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning and memory in the brain are implemented by complex, time-varying\nchanges in neural circuitry. The computational rules according to which\nsynaptic weights change over time are the subject of much research, and are not\nprecisely understood. Until recently, limitations in experimental methods have\nmade it challenging to test hypotheses about synaptic plasticity on a large\nscale. However, as such data become available and these barriers are lifted, it\nbecomes necessary to develop analysis techniques to validate plasticity models.\nHere, we present a highly extensible framework for modeling arbitrary synaptic\nplasticity rules on spike train data in populations of interconnected neurons.\nWe treat synaptic weights as a (potentially nonlinear) dynamical system\nembedded in a fully-Bayesian generalized linear model (GLM). In addition, we\nprovide an algorithm for inferring synaptic weight trajectories alongside the\nparameters of the GLM and of the learning rules. Using this method, we perform\nmodel comparison of two proposed variants of the well-known\nspike-timing-dependent plasticity (STDP) rule, where nonlinear effects play a\nsubstantial role. On synthetic data generated from the biophysical simulator\nNEURON, we show that we can recover the weight trajectories, the pattern of\nconnectivity, and the underlying learning rules.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 23:01:38 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Linderman", "Scott W.", ""], ["Stock", "Christopher H.", ""], ["Adams", "Ryan P.", ""]]}, {"id": "1411.4086", "submitter": "Hongwei Li", "authors": "Hongwei Li and Bin Yu", "title": "Error Rate Bounds and Iterative Weighted Majority Voting for\n  Crowdsourcing", "comments": "Journal Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.HC cs.LG math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing has become an effective and popular tool for human-powered\ncomputation to label large datasets. Since the workers can be unreliable, it is\ncommon in crowdsourcing to assign multiple workers to one task, and to\naggregate the labels in order to obtain results of high quality. In this paper,\nwe provide finite-sample exponential bounds on the error rate (in probability\nand in expectation) of general aggregation rules under the Dawid-Skene\ncrowdsourcing model. The bounds are derived for multi-class labeling, and can\nbe used to analyze many aggregation methods, including majority voting,\nweighted majority voting and the oracle Maximum A Posteriori (MAP) rule. We\nshow that the oracle MAP rule approximately optimizes our upper bound on the\nmean error rate of weighted majority voting in certain setting. We propose an\niterative weighted majority voting (IWMV) method that optimizes the error rate\nbound and approximates the oracle MAP rule. Its one step version has a provable\ntheoretical guarantee on the error rate. The IWMV method is intuitive and\ncomputationally simple. Experimental results on simulated and real data show\nthat IWMV performs at least on par with the state-of-the-art methods, and it\nhas a much lower computational cost (around one hundred times faster) than the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 15 Nov 2014 00:02:34 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Li", "Hongwei", ""], ["Yu", "Bin", ""]]}, {"id": "1411.4101", "submitter": "Rahul Mohan Mr.", "authors": "Rahul Mohan", "title": "Deep Deconvolutional Networks for Scene Parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Scene parsing is an important and challenging prob- lem in computer vision.\nIt requires labeling each pixel in an image with the category it belongs to.\nTradition- ally, it has been approached with hand-engineered features from\ncolor information in images. Recently convolutional neural networks (CNNs),\nwhich automatically learn hierar- chies of features, have achieved record\nperformance on the task. These approaches typically include a post-processing\ntechnique, such as superpixels, to produce the final label- ing. In this paper,\nwe propose a novel network architecture that combines deep deconvolutional\nneural networks with CNNs. Our experiments show that deconvolutional neu- ral\nnetworks are capable of learning higher order image structure beyond edge\nprimitives in comparison to CNNs. The new network architecture is employed for\nmulti-patch training, introduced as part of this work. Multi-patch train- ing\nmakes it possible to effectively learn spatial priors from scenes. The proposed\napproach yields state-of-the-art per- formance on four scene parsing datasets,\nnamely Stanford Background, SIFT Flow, CamVid, and KITTI. In addition, our\nsystem has the added advantage of having a training system that can be\ncompletely automated end-to-end with- out requiring any post-processing.\n", "versions": [{"version": "v1", "created": "Sat, 15 Nov 2014 02:03:14 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Mohan", "Rahul", ""]]}, {"id": "1411.4199", "submitter": "Ke Jiang", "authors": "Ke Jiang, Qichao Que, Brian Kulis", "title": "Revisiting Kernelized Locality-Sensitive Hashing for Improved\n  Large-Scale Image Retrieval", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple but powerful reinterpretation of kernelized\nlocality-sensitive hashing (KLSH), a general and popular method developed in\nthe vision community for performing approximate nearest-neighbor searches in an\narbitrary reproducing kernel Hilbert space (RKHS). Our new perspective is based\non viewing the steps of the KLSH algorithm in an appropriately projected space,\nand has several key theoretical and practical benefits. First, it eliminates\nthe problematic conceptual difficulties that are present in the existing\nmotivation of KLSH. Second, it yields the first formal retrieval performance\nbounds for KLSH. Third, our analysis reveals two techniques for boosting the\nempirical performance of KLSH. We evaluate these extensions on several\nlarge-scale benchmark image retrieval data sets, and show that our analysis\nleads to improved recall performance of at least 12%, and sometimes much\nhigher, over the standard KLSH method.\n", "versions": [{"version": "v1", "created": "Sun, 16 Nov 2014 00:08:24 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Jiang", "Ke", ""], ["Que", "Qichao", ""], ["Kulis", "Brian", ""]]}, {"id": "1411.4286", "submitter": "Zhiwei Qin", "authors": "Zhiwei Qin, Xiaocheng Tang, Ioannis Akrotirianakis, Amit Chakraborty", "title": "HIPAD - A Hybrid Interior-Point Alternating Direction algorithm for\n  knowledge-based SVM and feature selection", "comments": "Proceedings of 8th Learning and Intelligent OptimizatioN (LION8)\n  Conference, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider classification tasks in the regime of scarce labeled training\ndata in high dimensional feature space, where specific expert knowledge is also\navailable. We propose a new hybrid optimization algorithm that solves the\nelastic-net support vector machine (SVM) through an alternating direction\nmethod of multipliers in the first phase, followed by an interior-point method\nfor the classical SVM in the second phase. Both SVM formulations are adapted to\nknowledge incorporation. Our proposed algorithm addresses the challenges of\nautomatic feature selection, high optimization accuracy, and algorithmic\nflexibility for taking advantage of prior knowledge. We demonstrate the\neffectiveness and efficiency of our algorithm and compare it with existing\nmethods on a collection of synthetic and real-world data.\n", "versions": [{"version": "v1", "created": "Sun, 16 Nov 2014 17:58:18 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Qin", "Zhiwei", ""], ["Tang", "Xiaocheng", ""], ["Akrotirianakis", "Ioannis", ""], ["Chakraborty", "Amit", ""]]}, {"id": "1411.4342", "submitter": "Kirthevasan Kandasamy", "authors": "Kirthevasan Kandasamy, Akshay Krishnamurthy, Barnabas Poczos, Larry\n  Wasserman, James M. Robins", "title": "Influence Functions for Machine Learning: Nonparametric Estimators for\n  Entropies, Divergences and Mutual Informations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and analyze estimators for statistical functionals of one or more\ndistributions under nonparametric assumptions. Our estimators are based on the\ntheory of influence functions, which appear in the semiparametric statistics\nliterature. We show that estimators based either on data-splitting or a\nleave-one-out technique enjoy fast rates of convergence and other favorable\ntheoretical properties. We apply this framework to derive estimators for\nseveral popular information theoretic quantities, and via empirical evaluation,\nshow the advantage of this approach over existing estimators.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 02:04:57 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2015 18:47:41 GMT"}, {"version": "v3", "created": "Fri, 19 Jun 2015 23:29:07 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Kandasamy", "Kirthevasan", ""], ["Krishnamurthy", "Akshay", ""], ["Poczos", "Barnabas", ""], ["Wasserman", "Larry", ""], ["Robins", "James M.", ""]]}, {"id": "1411.4378", "submitter": "Robert Vandermeulen", "authors": "Robert A. Vandermeulen and Clayton D. Scott", "title": "Robust Kernel Density Estimation by Scaling and Projection in Hilbert\n  Space", "comments": "Extended version of NIPS 2014 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While robust parameter estimation has been well studied in parametric density\nestimation, there has been little investigation into robust density estimation\nin the nonparametric setting. We present a robust version of the popular kernel\ndensity estimator (KDE). As with other estimators, a robust version of the KDE\nis useful since sample contamination is a common issue with datasets. What\n\"robustness\" means for a nonparametric density estimate is not straightforward\nand is a topic we explore in this paper. To construct a robust KDE we scale the\ntraditional KDE and project it to its nearest weighted KDE in the $L^2$ norm.\nThis yields a scaled and projected KDE (SPKDE). Because the squared $L^2$ norm\npenalizes point-wise errors superlinearly this causes the weighted KDE to\nallocate more weight to high density regions. We demonstrate the robustness of\nthe SPKDE with numerical experiments and a consistency result which shows that\nasymptotically the SPKDE recovers the uncontaminated density under sufficient\nconditions on the contamination.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 06:35:55 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Vandermeulen", "Robert A.", ""], ["Scott", "Clayton D.", ""]]}, {"id": "1411.4503", "submitter": "Itamar Katz", "authors": "Itamar Katz and Koby Crammer", "title": "Outlier-Robust Convex Segmentation", "comments": "* Accepted to AAAI-15, this version includes the\n  appendix/supplementary material referenced in the AAAI-15 submission, as well\n  as color figures * This version include some minor typos correction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive a convex optimization problem for the task of segmenting sequential\ndata, which explicitly treats presence of outliers. We describe two algorithms\nfor solving this problem, one exact and one a top-down novel approach, and we\nderive a consistency results for the case of two segments and no outliers.\nRobustness to outliers is evaluated on two real-world tasks related to speech\nsegmentation. Our algorithms outperform baseline segmentation algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 14:59:25 GMT"}, {"version": "v2", "created": "Tue, 18 Nov 2014 07:59:33 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Katz", "Itamar", ""], ["Crammer", "Koby", ""]]}, {"id": "1411.4510", "submitter": "Kian Hsiang Low", "authors": "Kian Hsiang Low, Jiangbo Yu, Jie Chen, Patrick Jaillet", "title": "Parallel Gaussian Process Regression for Big Data: Low-Rank\n  Representation Meets Markov Approximation", "comments": "29th AAAI Conference on Artificial Intelligence (AAAI 2015), Extended\n  version with proofs, 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The expressive power of a Gaussian process (GP) model comes at a cost of poor\nscalability in the data size. To improve its scalability, this paper presents a\nlow-rank-cum-Markov approximation (LMA) of the GP model that is novel in\nleveraging the dual computational advantages stemming from complementing a\nlow-rank approximate representation of the full-rank GP based on a support set\nof inputs with a Markov approximation of the resulting residual process; the\nlatter approximation is guaranteed to be closest in the Kullback-Leibler\ndistance criterion subject to some constraint and is considerably more refined\nthan that of existing sparse GP models utilizing low-rank representations due\nto its more relaxed conditional independence assumption (especially with larger\ndata). As a result, our LMA method can trade off between the size of the\nsupport set and the order of the Markov property to (a) incur lower\ncomputational cost than such sparse GP models while achieving predictive\nperformance comparable to them and (b) accurately represent features/patterns\nof any scale. Interestingly, varying the Markov order produces a spectrum of\nLMAs with PIC approximation and full-rank GP at the two extremes. An advantage\nof our LMA method is that it is amenable to parallelization on multiple\nmachines/cores, thereby gaining greater scalability. Empirical evaluation on\nthree real-world datasets in clusters of up to 32 computing nodes shows that\nour centralized and parallel LMA methods are significantly more time-efficient\nand scalable than state-of-the-art sparse and full-rank GP regression methods\nwhile achieving comparable predictive performances.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 15:31:04 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Low", "Kian Hsiang", ""], ["Yu", "Jiangbo", ""], ["Chen", "Jie", ""], ["Jaillet", "Patrick", ""]]}, {"id": "1411.4521", "submitter": "Jesse Krijthe", "authors": "Jesse H. Krijthe and Marco Loog", "title": "Implicitly Constrained Semi-Supervised Linear Discriminant Analysis", "comments": "6 pages, 3 figures and 3 tables. International Conference on Pattern\n  Recognition (ICPR) 2014, Stockholm, Sweden", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning is an important and active topic of research in\npattern recognition. For classification using linear discriminant analysis\nspecifically, several semi-supervised variants have been proposed. Using any\none of these methods is not guaranteed to outperform the supervised classifier\nwhich does not take the additional unlabeled data into account. In this work we\ncompare traditional Expectation Maximization type approaches for\nsemi-supervised linear discriminant analysis with approaches based on intrinsic\nconstraints and propose a new principled approach for semi-supervised linear\ndiscriminant analysis, using so-called implicit constraints. We explore the\nrelationships between these methods and consider the question if and in what\nsense we can expect improvement in performance over the supervised procedure.\nThe constraint based approaches are more robust to misspecification of the\nmodel, and may outperform alternatives that make more assumptions on the data,\nin terms of the log-likelihood of unseen objects.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 15:57:11 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Krijthe", "Jesse H.", ""], ["Loog", "Marco", ""]]}, {"id": "1411.4598", "submitter": "Yiyuan She", "authors": "Yiyuan She, Yuejia He, Shijie Li, Dapeng Wu", "title": "Joint Association Graph Screening and Decomposition for Large-scale\n  Linear Dynamical Systems", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2014.2373315", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies large-scale dynamical networks where the current state of\nthe system is a linear transformation of the previous state, contaminated by a\nmultivariate Gaussian noise. Examples include stock markets, human brains and\ngene regulatory networks. We introduce a transition matrix to describe the\nevolution, which can be translated to a directed Granger transition graph, and\nuse the concentration matrix of the Gaussian noise to capture the second-order\nrelations between nodes, which can be translated to an undirected conditional\ndependence graph. We propose regularizing the two graphs jointly in topology\nidentification and dynamics estimation. Based on the notion of joint\nassociation graph (JAG), we develop a joint graphical screening and estimation\n(JGSE) framework for efficient network learning in big data. In particular, our\nmethod can pre-determine and remove unnecessary edges based on the joint\ngraphical structure, referred to as JAG screening, and can decompose a large\nnetwork into smaller subnetworks in a robust manner, referred to as JAG\ndecomposition. JAG screening and decomposition can reduce the problem size and\nsearch space for fine estimation at a later stage. Experiments on both\nsynthetic data and real-world applications show the effectiveness of the\nproposed framework in large-scale network topology identification and dynamics\nestimation.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 19:21:27 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["She", "Yiyuan", ""], ["He", "Yuejia", ""], ["Li", "Shijie", ""], ["Wu", "Dapeng", ""]]}, {"id": "1411.4648", "submitter": "Rafael Chaves", "authors": "Rafael Chaves, Richard Kueng, Jonatan Bohr Brask, David Gross", "title": "A unifying framework for relaxations of the causal assumptions in Bell's\n  theorem", "comments": "6 pages + appendix, 5 figures", "journal-ref": "Phys. Rev. Lett. 114, 140403 (2015)", "doi": "10.1103/PhysRevLett.114.140403", "report-no": null, "categories": "quant-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bell's Theorem shows that quantum mechanical correlations can violate the\nconstraints that the causal structure of certain experiments impose on any\nclassical explanation. It is thus natural to ask to which degree the causal\nassumptions -- e.g. locality or measurement independence -- have to be relaxed\nin order to allow for a classical description of such experiments. Here, we\ndevelop a conceptual and computational framework for treating this problem. We\nemploy the language of Bayesian networks to systematically construct\nalternative causal structures and bound the degree of relaxation using\nquantitative measures that originate from the mathematical theory of causality.\nThe main technical insight is that the resulting problems can often be\nexpressed as computationally tractable linear programs. We demonstrate the\nversatility of the framework by applying it to a variety of scenarios, ranging\nfrom relaxations of the measurement independence, locality and bilocality\nassumptions, to a novel causal interpretation of CHSH inequality violations.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 21:00:10 GMT"}], "update_date": "2015-04-16", "authors_parsed": [["Chaves", "Rafael", ""], ["Kueng", "Richard", ""], ["Brask", "Jonatan Bohr", ""], ["Gross", "David", ""]]}, {"id": "1411.4691", "submitter": "Yiyuan She", "authors": "Yiyuan She, Zhifeng Wang and He Jiang", "title": "Group Regularized Estimation under Structural Hierarchy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection for models including interactions between explanatory\nvariables often needs to obey certain hierarchical constraints. The weak or\nstrong structural hierarchy requires that the existence of an interaction term\nimplies at least one or both associated main effects to be present in the\nmodel. Lately, this problem has attracted a lot of attention, but existing\ncomputational algorithms converge slow even with a moderate number of\npredictors. Moreover, in contrast to the rich literature on ordinary variable\nselection, there is a lack of statistical theory to show reasonably low error\nrates of hierarchical variable selection.\n  This work investigates a new class of estimators that make use of multiple\ngroup penalties to capture structural parsimony. We give the minimax lower\nbounds for strong and weak hierarchical variable selection and show that the\nproposed estimators enjoy sharp rate oracle inequalities. A general-purpose\nalgorithm is developed with guaranteed convergence and global optimality.\nSimulations and real data experiments demonstrate the efficiency and efficacy\nof the proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 23:03:11 GMT"}, {"version": "v2", "created": "Thu, 20 Oct 2016 03:51:40 GMT"}, {"version": "v3", "created": "Tue, 8 Nov 2016 21:00:25 GMT"}], "update_date": "2016-11-10", "authors_parsed": [["She", "Yiyuan", ""], ["Wang", "Zhifeng", ""], ["Jiang", "He", ""]]}, {"id": "1411.4695", "submitter": "Ali Heydari", "authors": "Ali Heydari", "title": "Feedback Solution to Optimal Switching Problems with Switching Cost", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of optimal switching between nonlinear autonomous subsystems is\ninvestigated in this study where the objective is not only bringing the states\nto close to the desired point, but also adjusting the switching pattern, in the\nsense of penalizing switching occurrences and assigning different preferences\nto utilization of different modes. The mode sequence is unspecified and a\nswitching cost term is used in the cost function for penalizing each switching.\nIt is shown that once a switching cost is incorporated, the optimal cost-to-go\nfunction depends on the already active subsystem, i.e., the subsystem which was\nengaged in the previous time step. Afterwards, an approximate dynamic\nprogramming based method is developed which provides an approximation of the\noptimal solution to the problem in a feedback form and for different initial\nconditions. Finally, the performance of the method is analyzed through\nnumerical examples.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 23:35:08 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Heydari", "Ali", ""]]}, {"id": "1411.4834", "submitter": "Christian Huemmer M.Sc.", "authors": "Christian Huemmer, Roland Maas and Walter Kellermann", "title": "The NLMS algorithm with time-variant optimum stepsize derived from a\n  Bayesian network perspective", "comments": "4 pages, 1 page of references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we derive a new stepsize adaptation for the normalized least\nmean square algorithm (NLMS) by describing the task of linear acoustic echo\ncancellation from a Bayesian network perspective. Similar to the well-known\nKalman filter equations, we model the acoustic wave propagation from the\nloudspeaker to the microphone by a latent state vector and define a linear\nobservation equation (to model the relation between the state vector and the\nobservation) as well as a linear process equation (to model the temporal\nprogress of the state vector). Based on additional assumptions on the\nstatistics of the random variables in observation and process equation, we\napply the expectation-maximization (EM) algorithm to derive an NLMS-like filter\nadaptation. By exploiting the conditional independence rules for Bayesian\nnetworks, we reveal that the resulting EM-NLMS algorithm has a stepsize update\nequivalent to the optimal-stepsize calculation proposed by Yamamoto and\nKitayama in 1982, which has been adopted in many textbooks. As main difference,\nthe instantaneous stepsize value is estimated in the M step of the EM algorithm\n(instead of being approximated by artificially extending the acoustic echo\npath). The EM-NLMS algorithm is experimentally verified for synthesized\nscenarios with both, white noise and male speech as input signal.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 13:23:11 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Huemmer", "Christian", ""], ["Maas", "Roland", ""], ["Kellermann", "Walter", ""]]}, {"id": "1411.5010", "submitter": "Noah Stein", "authors": "Noah D. Stein", "title": "Nonnegative Tensor Factorization for Directional Blind Audio Source\n  Separation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We augment the nonnegative matrix factorization method for audio source\nseparation with cues about directionality of sound propagation. This improves\nseparation quality greatly and removes the need for training data, with only a\ntwofold increase in run time. This is the first method which can exploit\ndirectional information from microphone arrays much smaller than the wavelength\nof sound, working both in simulation and in practice on millimeter-scale\nmicrophone arrays.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 20:52:52 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 18:43:59 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Stein", "Noah D.", ""]]}, {"id": "1411.5086", "submitter": "Natalia Arzeno", "authors": "Natalia M. Arzeno, Karla A. Lawson, Sarah V. Duzinski, Haris Vikalo", "title": "Designing Optimal Mortality Risk Prediction Scores that Preserve\n  Clinical Knowledge", "comments": "30 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many in-hospital mortality risk prediction scores dichotomize predictive\nvariables to simplify the score calculation. However, hard thresholding in\nthese additive stepwise scores of the form \"add x points if variable v is\nabove/below threshold t\" may lead to critical failures. In this paper, we seek\nto develop risk prediction scores that preserve clinical knowledge embedded in\nfeatures and structure of the existing additive stepwise scores while\naddressing limitations caused by variable dichotomization. To this end, we\npropose a novel score structure that relies on a transformation of predictive\nvariables by means of nonlinear logistic functions facilitating smooth\ndifferentiation between critical and normal values of the variables. We develop\nan optimization framework for inferring parameters of the logistic functions\nfor a given patient population via cyclic block coordinate descent. The\nparameters may readily be updated as the patient population and standards of\ncare evolve. We tested the proposed methodology on two populations: (1) brain\ntrauma patients admitted to the intensive care unit of the Dell Children's\nMedical Center of Central Texas between 2007 and 2012, and (2) adult ICU\npatient data from the MIMIC II database. The results are compared with those\nobtained by the widely used PRISM III and SOFA scores. The prediction power of\na score is evaluated using area under ROC curve, Youden's index, and\nprecision-recall balance in a cross-validation study. The results demonstrate\nthat the new framework enables significant performance improvements over PRISM\nIII and SOFA in terms of all three criteria.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2014 01:15:49 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2015 18:20:46 GMT"}], "update_date": "2015-04-30", "authors_parsed": [["Arzeno", "Natalia M.", ""], ["Lawson", "Karla A.", ""], ["Duzinski", "Sarah V.", ""], ["Vikalo", "Haris", ""]]}, {"id": "1411.5172", "submitter": "Markus Heinonen", "authors": "Markus Heinonen, Florence d'Alch\\'e-Buc", "title": "Learning nonparametric differential equations with operator-valued\n  kernels and gradient matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling dynamical systems with ordinary differential equations implies a\nmechanistic view of the process underlying the dynamics. However in many cases,\nthis knowledge is not available. To overcome this issue, we introduce a general\nframework for nonparametric ODE models using penalized regression in\nReproducing Kernel Hilbert Spaces (RKHS) based on operator-valued kernels.\nMoreover, we extend the scope of gradient matching approaches to nonparametric\nODE. A smooth estimate of the solution ODE is built to provide an approximation\nof the derivative of the ODE solution which is in turn used to learn the\nnonparametric ODE model. This approach benefits from the flexibility of\npenalized regression in RKHS allowing for ridge or (structured) sparse\nregression as well. Very good results are shown on 3 different ODE systems.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2014 10:48:50 GMT"}], "update_date": "2014-11-20", "authors_parsed": [["Heinonen", "Markus", ""], ["d'Alch\u00e9-Buc", "Florence", ""]]}, {"id": "1411.5260", "submitter": "Patrick Kimes", "authors": "Patrick K. Kimes, D. Neil Hayes, J. S. Marron and Yufeng Liu", "title": "Large-Margin Classification with Multiple Decision Rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary classification is a common statistical learning problem in which a\nmodel is estimated on a set of covariates for some outcome indicating the\nmembership of one of two classes. In the literature, there exists a distinction\nbetween hard and soft classification. In soft classification, the conditional\nclass probability is modeled as a function of the covariates. In contrast, hard\nclassification methods only target the optimal prediction boundary. While hard\nand soft classification methods have been studied extensively, not much work\nhas been done to compare the actual tasks of hard and soft classification. In\nthis paper we propose a spectrum of statistical learning problems which span\nthe hard and soft classification tasks based on fitting multiple decision rules\nto the data. By doing so, we reveal a novel collection of learning tasks of\nincreasing complexity. We study the problems using the framework of\nlarge-margin classifiers and a class of piecewise linear convex surrogates, for\nwhich we derive statistical properties and a corresponding sub-gradient descent\nalgorithm. We conclude by applying our approach to simulation settings and a\nmagnetic resonance imaging (MRI) dataset from the Alzheimer's Disease\nNeuroimaging Initiative (ADNI) study.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2014 15:45:54 GMT"}], "update_date": "2014-11-20", "authors_parsed": [["Kimes", "Patrick K.", ""], ["Hayes", "D. Neil", ""], ["Marron", "J. S.", ""], ["Liu", "Yufeng", ""]]}, {"id": "1411.5271", "submitter": "Charles Zheng", "authors": "Charles Zheng, Franco Pestilli, and Ariel Rokem", "title": "Quantifying error in estimates of human brain fiber directions using\n  Earth Mover's Distance", "comments": "Accepted for oral presentation at NIPS OTML'14 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion-weighted MR imaging (DWI) is the only method we currently have to\nmeasure connections between different parts of the human brain in vivo. To\nelucidate the structure of these connections, algorithms for tracking bundles\nof axonal fibers through the subcortical white matter rely on local estimates\nof the fiber orientation distribution function (fODF) in different parts of the\nbrain. These functions describe the relative abundance of populations of axonal\nfibers crossing each other in each location. Multiple models exist for\nestimating fODFs. The quality of the resulting estimates can be quantified by\nmeans of a suitable measure of distance on the space of fODFs. However, there\nare multiple distance metrics that can be applied for this purpose, including\nsmoothed $L_p$ distances and the Wasserstein metrics. Here, we give four\nreasons for the use of the Earth Mover's Distance (EMD) equipped with the\narc-length, as a distance metric. (continued)\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2014 16:02:21 GMT"}, {"version": "v2", "created": "Thu, 4 Dec 2014 17:45:52 GMT"}], "update_date": "2014-12-05", "authors_parsed": [["Zheng", "Charles", ""], ["Pestilli", "Franco", ""], ["Rokem", "Ariel", ""]]}, {"id": "1411.5371", "submitter": "Justin Kinney", "authors": "Justin B. Kinney", "title": "Unification of field theory and maximum entropy methods for learning\n  probability densities", "comments": "16 pages, 4 figures. Minor clarifying changes have been made\n  throughout. Software is available at https://github.com/jbkinney/14_maxent", "journal-ref": "Phys. Rev. E 92, 032107 (2015)", "doi": "10.1103/PhysRevE.92.032107", "report-no": null, "categories": "physics.data-an cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need to estimate smooth probability distributions (a.k.a. probability\ndensities) from finite sampled data is ubiquitous in science. Many approaches\nto this problem have been described, but none is yet regarded as providing a\ndefinitive solution. Maximum entropy estimation and Bayesian field theory are\ntwo such approaches. Both have origins in statistical physics, but the\nrelationship between them has remained unclear. Here I unify these two methods\nby showing that every maximum entropy density estimate can be recovered in the\ninfinite smoothness limit of an appropriate Bayesian field theory. I also show\nthat Bayesian field theory estimation can be performed without imposing any\nboundary conditions on candidate densities, and that the infinite smoothness\nlimit of these theories recovers the most common types of maximum entropy\nestimates. Bayesian field theory is thus seen to provide a natural test of the\nvalidity of the maximum entropy null hypothesis. Bayesian field theory also\nreturns a lower entropy density estimate when the maximum entropy hypothesis is\nfalsified. The computations necessary for this approach can be performed\nrapidly for one-dimensional data, and software for doing this is provided.\nBased on these results, I argue that Bayesian field theory is poised to provide\na definitive solution to the density estimation problem in one dimension.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2014 21:00:58 GMT"}, {"version": "v2", "created": "Fri, 21 Nov 2014 16:58:05 GMT"}, {"version": "v3", "created": "Wed, 3 Dec 2014 14:57:30 GMT"}, {"version": "v4", "created": "Sat, 21 Mar 2015 22:07:18 GMT"}, {"version": "v5", "created": "Wed, 29 Jul 2015 02:29:54 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Kinney", "Justin B.", ""]]}, {"id": "1411.5417", "submitter": "Abhradeep Guha Thakurta", "authors": "Kunal Talwar, Abhradeep Thakurta, Li Zhang", "title": "Private Empirical Risk Minimization Beyond the Worst Case: The Effect of\n  the Constraint Set Geometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical Risk Minimization (ERM) is a standard technique in machine\nlearning, where a model is selected by minimizing a loss function over\nconstraint set. When the training dataset consists of private information, it\nis natural to use a differentially private ERM algorithm, and this problem has\nbeen the subject of a long line of work started with Chaudhuri and Monteleoni\n2008. A private ERM algorithm outputs an approximate minimizer of the loss\nfunction and its error can be measured as the difference from the optimal value\nof the loss function. When the constraint set is arbitrary, the required error\nbounds are fairly well understood \\cite{BassilyST14}. In this work, we show\nthat the geometric properties of the constraint set can be used to derive\nsignificantly better results. Specifically, we show that a differentially\nprivate version of Mirror Descent leads to error bounds of the form\n$\\tilde{O}(G_{\\mathcal{C}}/n)$ for a lipschitz loss function, improving on the\n$\\tilde{O}(\\sqrt{p}/n)$ bounds in Bassily, Smith and Thakurta 2014. Here $p$ is\nthe dimensionality of the problem, $n$ is the number of data points in the\ntraining set, and $G_{\\mathcal{C}}$ denotes the Gaussian width of the\nconstraint set that we optimize over. We show similar improvements for strongly\nconvex functions, and for smooth functions. In addition, we show that when the\nloss function is Lipschitz with respect to the $\\ell_1$ norm and $\\mathcal{C}$\nis $\\ell_1$-bounded, a differentially private version of the Frank-Wolfe\nalgorithm gives error bounds of the form $\\tilde{O}(n^{-2/3})$. This captures\nthe important and common case of sparse linear regression (LASSO), when the\ndata $x_i$ satisfies $|x_i|_{\\infty} \\leq 1$ and we optimize over the $\\ell_1$\nball. We show new lower bounds for this setting, that together with known\nbounds, imply that all our upper bounds are tight.\n", "versions": [{"version": "v1", "created": "Thu, 20 Nov 2014 01:33:53 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2015 02:38:52 GMT"}, {"version": "v3", "created": "Sun, 20 Nov 2016 22:40:46 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Talwar", "Kunal", ""], ["Thakurta", "Abhradeep", ""], ["Zhang", "Li", ""]]}, {"id": "1411.5595", "submitter": "Tianze Shi", "authors": "Tianze Shi, Zhiyuan Liu", "title": "Linking GloVe with word2vec", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Global Vectors for word representation (GloVe), introduced by Jeffrey\nPennington et al. is reported to be an efficient and effective method for\nlearning vector representations of words. State-of-the-art performance is also\nprovided by skip-gram with negative-sampling (SGNS) implemented in the word2vec\ntool. In this note, we explain the similarities between the training objectives\nof the two models, and show that the objective of SGNS is similar to the\nobjective of a specialized form of GloVe, though their cost functions are\ndefined differently.\n", "versions": [{"version": "v1", "created": "Thu, 20 Nov 2014 16:39:28 GMT"}, {"version": "v2", "created": "Wed, 26 Nov 2014 06:46:18 GMT"}], "update_date": "2014-11-27", "authors_parsed": [["Shi", "Tianze", ""], ["Liu", "Zhiyuan", ""]]}, {"id": "1411.5620", "submitter": "Francesca Paola Carli", "authors": "Francesca Paola Carli, Tianshi Chen, Lennart Ljung", "title": "Maximum Entropy Kernels for System Identification", "comments": "Extends results of 2014 IEEE MSC Conference Proceedings\n  (arXiv:1406.5706)", "journal-ref": null, "doi": "10.1109/TAC.2016.2582642", "report-no": null, "categories": "math.OC cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new nonparametric approach for system identification has been recently\nproposed where the impulse response is modeled as the realization of a\nzero-mean Gaussian process whose covariance (kernel) has to be estimated from\ndata. In this scheme, quality of the estimates crucially depends on the\nparametrization of the covariance of the Gaussian process. A family of kernels\nthat have been shown to be particularly effective in the system identification\nframework is the family of Diagonal/Correlated (DC) kernels. Maximum entropy\nproperties of a related family of kernels, the Tuned/Correlated (TC) kernels,\nhave been recently pointed out in the literature. In this paper we show that\nmaximum entropy properties indeed extend to the whole family of DC kernels. The\nmaximum entropy interpretation can be exploited in conjunction with results on\nmatrix completion problems in the graphical models literature to shed light on\nthe structure of the DC kernel. In particular, we prove that the DC kernel\nadmits a closed-form factorization, inverse and determinant. These results can\nbe exploited both to improve the numerical stability and to reduce the\ncomputational complexity associated with the computation of the DC estimator.\n", "versions": [{"version": "v1", "created": "Thu, 20 Nov 2014 17:38:53 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2016 20:22:32 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Carli", "Francesca Paola", ""], ["Chen", "Tianshi", ""], ["Ljung", "Lennart", ""]]}, {"id": "1411.5639", "submitter": "Panagiotis Sidiropoulos", "authors": "Panagiotis Sidiropoulos", "title": "N-sphere chord length distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies the chord length distribution, in the case where both ends\nlie on a $N$-dimensional hypersphere ($N \\geq 2$). Actually, after connecting\nthis distribution to the recently estimated surface of a hyperspherical cap\n\\cite{SLi11}, closed-form expressions of both the probability density function\nand the cumulative distribution function are straightforwardly extracted, which\nare followed by a discussion on its basic properties, among which its\ndependence from the hypersphere dimension. Additionally, the distribution of\nthe dot product of unitary vectors is estimated, a problem that is related to\nthe chord length.\n", "versions": [{"version": "v1", "created": "Thu, 20 Nov 2014 18:55:23 GMT"}], "update_date": "2014-11-21", "authors_parsed": [["Sidiropoulos", "Panagiotis", ""]]}, {"id": "1411.5720", "submitter": "Tatsunori Hashimoto", "authors": "Tatsunori B. Hashimoto, Yi Sun, Tommi S. Jaakkola", "title": "Metric recovery from directed unweighted graphs", "comments": "Poster at NIPS workshop on networks. Submitted to AISTATS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SI math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze directed, unweighted graphs obtained from $x_i\\in \\mathbb{R}^d$ by\nconnecting vertex $i$ to $j$ iff $|x_i - x_j| < \\epsilon(x_i)$. Examples of\nsuch graphs include $k$-nearest neighbor graphs, where $\\epsilon(x_i)$ varies\nfrom point to point, and, arguably, many real world graphs such as\nco-purchasing graphs. We ask whether we can recover the underlying Euclidean\nmetric $\\epsilon(x_i)$ and the associated density $p(x_i)$ given only the\ndirected graph and $d$.\n  We show that consistent recovery is possible up to isometric scaling when the\nvertex degree is at least $\\omega(n^{2/(2+d)}\\log(n)^{d/(d+2)})$. Our estimator\nis based on a careful characterization of a random walk over the directed graph\nand the associated continuum limit. As an algorithm, it resembles the PageRank\ncentrality metric. We demonstrate empirically that the estimator performs well\non simulated examples as well as on real-world co-purchasing graphs even with a\nsmall number of points and degree scaling as low as $\\log(n)$.\n", "versions": [{"version": "v1", "created": "Thu, 20 Nov 2014 23:16:09 GMT"}], "update_date": "2014-11-24", "authors_parsed": [["Hashimoto", "Tatsunori B.", ""], ["Sun", "Yi", ""], ["Jaakkola", "Tommi S.", ""]]}, {"id": "1411.5731", "submitter": "Suleyman Cetintas", "authors": "Can Xu, Suleyman Cetintas, Kuang-Chih Lee, Li-Jia Li", "title": "Visual Sentiment Prediction with Deep Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images have become one of the most popular types of media through which users\nconvey their emotions within online social networks. Although vast amount of\nresearch is devoted to sentiment analysis of textual data, there has been very\nlimited work that focuses on analyzing sentiment of image data. In this work,\nwe propose a novel visual sentiment prediction framework that performs image\nunderstanding with Deep Convolutional Neural Networks (CNN). Specifically, the\nproposed sentiment prediction framework performs transfer learning from a CNN\nwith millions of parameters, which is pre-trained on large-scale data for\nobject recognition. Experiments conducted on two real-world datasets from\nTwitter and Tumblr demonstrate the effectiveness of the proposed visual\nsentiment analysis framework.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2014 00:39:43 GMT"}], "update_date": "2014-11-24", "authors_parsed": [["Xu", "Can", ""], ["Cetintas", "Suleyman", ""], ["Lee", "Kuang-Chih", ""], ["Li", "Li-Jia", ""]]}, {"id": "1411.5732", "submitter": "Suleyman Cetintas", "authors": "Suleyman Cetintas, Luo Si, Yan Ping Xin, Dake Zhang, Joo Young Park,\n  Ron Tzur", "title": "A Joint Probabilistic Classification Model of Relevant and Irrelevant\n  Sentences in Mathematical Word Problems", "comments": "appears in Journal of Educational Data Mining (JEDM, 2010)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the difficulty level of math word problems is an important task\nfor many educational applications. Identification of relevant and irrelevant\nsentences in math word problems is an important step for calculating the\ndifficulty levels of such problems. This paper addresses a novel application of\ntext categorization to identify two types of sentences in mathematical word\nproblems, namely relevant and irrelevant sentences. A novel joint probabilistic\nclassification model is proposed to estimate the joint probability of\nclassification decisions for all sentences of a math word problem by utilizing\nthe correlation among all sentences along with the correlation between the\nquestion sentence and other sentences, and sentence text. The proposed model is\ncompared with i) a SVM classifier which makes independent classification\ndecisions for individual sentences by only using the sentence text and ii) a\nnovel SVM classifier that considers the correlation between the question\nsentence and other sentences along with the sentence text. An extensive set of\nexperiments demonstrates the effectiveness of the joint probabilistic\nclassification model for identifying relevant and irrelevant sentences as well\nas the novel SVM classifier that utilizes the correlation between the question\nsentence and other sentences. Furthermore, empirical results and analysis show\nthat i) it is highly beneficial not to remove stopwords and ii) utilizing part\nof speech tagging does not make a significant improvement although it has been\nshown to be effective for the related task of math word problem type\nclassification.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2014 00:53:02 GMT"}], "update_date": "2014-11-24", "authors_parsed": [["Cetintas", "Suleyman", ""], ["Si", "Luo", ""], ["Xin", "Yan Ping", ""], ["Zhang", "Dake", ""], ["Park", "Joo Young", ""], ["Tzur", "Ron", ""]]}, {"id": "1411.5799", "submitter": "Eemeli Lepp\\\"aaho", "authors": "Arto Klami, Seppo Virtanen, Eemeli Lepp\\\"aaho, Samuel Kaski", "title": "Group Factor Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factor analysis provides linear factors that describe relationships between\nindividual variables of a data set. We extend this classical formulation into\nlinear factors that describe relationships between groups of variables, where\neach group represents either a set of related variables or a data set. The\nmodel also naturally extends canonical correlation analysis to more than two\nsets, in a way that is more flexible than previous extensions. Our solution is\nformulated as variational inference of a latent variable model with structural\nsparsity, and it consists of two hierarchical levels: The higher level models\nthe relationships between the groups, whereas the lower models the observed\nvariables given the higher level. We show that the resulting solution solves\nthe group factor analysis problem accurately, outperforming alternative factor\nanalysis based solutions as well as more straightforward implementations of\ngroup factor analysis. The method is demonstrated on two life science data\nsets, one on brain activation and the other on systems biology, illustrating\nits applicability to the analysis of different types of high-dimensional data\nsources.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2014 08:55:35 GMT"}, {"version": "v2", "created": "Tue, 2 Dec 2014 15:58:25 GMT"}], "update_date": "2014-12-03", "authors_parsed": [["Klami", "Arto", ""], ["Virtanen", "Seppo", ""], ["Lepp\u00e4aho", "Eemeli", ""], ["Kaski", "Samuel", ""]]}, {"id": "1411.5915", "submitter": "Giulio Bottegal", "authors": "Giulio Bottegal, Aleksandr Y. Aravkin, H{\\aa}kan Hjalmarsson,\n  Gianluigi Pillonetto", "title": "Robust EM kernel-based methods for linear system identification", "comments": "Accepted for publication in Automatica", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in system identification have brought attention to\nregularized kernel-based methods. This type of approach has been proven to\ncompare favorably with classic parametric methods. However, current\nformulations are not robust with respect to outliers. In this paper, we\nintroduce a novel method to robustify kernel-based system identification\nmethods. To this end, we model the output measurement noise using random\nvariables with heavy-tailed probability density functions (pdfs), focusing on\nthe Laplacian and the Student's t distributions. Exploiting the representation\nof these pdfs as scale mixtures of Gaussians, we cast our system identification\nproblem into a Gaussian process regression framework, which requires estimating\na number of hyperparameters of the data size order. To overcome this\ndifficulty, we design a new maximum a posteriori (MAP) estimator of the\nhyperparameters, and solve the related optimization problem with a novel\niterative scheme based on the Expectation-Maximization (EM) method. In presence\nof outliers, tests on simulated data and on a real system show a substantial\nperformance improvement compared to currently used kernel-based methods for\nlinear system identification.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2014 15:35:04 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2015 18:17:11 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2016 14:45:02 GMT"}], "update_date": "2016-11-25", "authors_parsed": [["Bottegal", "Giulio", ""], ["Aravkin", "Aleksandr Y.", ""], ["Hjalmarsson", "H\u00e5kan", ""], ["Pillonetto", "Gianluigi", ""]]}, {"id": "1411.5977", "submitter": "Nihar Shah", "authors": "Nihar B. Shah and Dengyong Zhou", "title": "On the Impossibility of Convex Inference in Human Computation", "comments": "AAAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human computation or crowdsourcing involves joint inference of the\nground-truth-answers and the worker-abilities by optimizing an objective\nfunction, for instance, by maximizing the data likelihood based on an assumed\nunderlying model. A variety of methods have been proposed in the literature to\naddress this inference problem. As far as we know, none of the objective\nfunctions in existing methods is convex. In machine learning and applied\nstatistics, a convex function such as the objective function of support vector\nmachines (SVMs) is generally preferred, since it can leverage the\nhigh-performance algorithms and rigorous guarantees established in the\nextensive literature on convex optimization. One may thus wonder if there\nexists a meaningful convex objective function for the inference problem in\nhuman computation. In this paper, we investigate this convexity issue for human\ncomputation. We take an axiomatic approach by formulating a set of axioms that\nimpose two mild and natural assumptions on the objective function for the\ninference. Under these axioms, we show that it is unfortunately impossible to\nensure convexity of the inference problem. On the other hand, we show that\ninterestingly, in the absence of a requirement to model \"spammers\", one can\nconstruct reasonable objective functions for crowdsourcing that guarantee\nconvex inference.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2014 18:51:10 GMT"}], "update_date": "2014-11-24", "authors_parsed": [["Shah", "Nihar B.", ""], ["Zhou", "Dengyong", ""]]}, {"id": "1411.5988", "submitter": "Rocco Langone", "authors": "Rocco Langone", "title": "Clustering evolving data using kernel-based methods", "comments": "PhD thesis, Faculty of Engineering, KU Leuven (Leuven, Belgium), July\n  2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this thesis, we propose several modelling strategies to tackle evolving\ndata in different contexts. In the framework of static clustering, we start by\nintroducing a soft kernel spectral clustering (SKSC) algorithm, which can\nbetter deal with overlapping clusters with respect to kernel spectral\nclustering (KSC) and provides more interpretable outcomes. Afterwards, a whole\nstrategy based upon KSC for community detection of static networks is proposed,\nwhere the extraction of a high quality training sub-graph, the choice of the\nkernel function, the model selection and the applicability to large-scale data\nare key aspects. This paves the way for the development of a novel clustering\nalgorithm for the analysis of evolving networks called kernel spectral\nclustering with memory effect (MKSC), where the temporal smoothness between\nclustering results in successive time steps is incorporated at the level of the\nprimal optimization problem, by properly modifying the KSC formulation. Later\non, an application of KSC to fault detection of an industrial machine is\npresented. Here, a smart pre-processing of the data by means of a proper\nwindowing operation is necessary to catch the ongoing degradation process\naffecting the machine. In this way, in a genuinely unsupervised manner, it is\npossible to raise an early warning when necessary, in an online fashion.\nFinally, we propose a new algorithm called incremental kernel spectral\nclustering (IKSC) for online learning of non-stationary data. This ambitious\nchallenge is faced by taking advantage of the out-of-sample property of kernel\nspectral clustering (KSC) to adapt the initial model, in order to tackle\nmerging, splitting or drifting of clusters across time. Real-world applications\nconsidered in this thesis include image segmentation, time-series clustering,\ncommunity detection of static and evolving networks.\n", "versions": [{"version": "v1", "created": "Thu, 20 Nov 2014 19:51:30 GMT"}], "update_date": "2014-11-24", "authors_parsed": [["Langone", "Rocco", ""]]}, {"id": "1411.6081", "submitter": "Cho-Jui Hsieh Cho-Jui Hsieh", "authors": "Cho-Jui Hsieh and Nagarajan Natarajan and Inderjit S. Dhillon", "title": "PU Learning for Matrix Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the matrix completion problem when the\nobservations are one-bit measurements of some underlying matrix M, and in\nparticular the observed samples consist only of ones and no zeros. This problem\nis motivated by modern applications such as recommender systems and social\nnetworks where only \"likes\" or \"friendships\" are observed. The problem of\nlearning from only positive and unlabeled examples, called PU\n(positive-unlabeled) learning, has been studied in the context of binary\nclassification. We consider the PU matrix completion problem, where an\nunderlying real-valued matrix M is first quantized to generate one-bit\nobservations and then a subset of positive entries is revealed. Under the\nassumption that M has bounded nuclear norm, we provide recovery guarantees for\ntwo different observation models: 1) M parameterizes a distribution that\ngenerates a binary matrix, 2) M is thresholded to obtain a binary matrix. For\nthe first case, we propose a \"shifted matrix completion\" method that recovers M\nusing only a subset of indices corresponding to ones, while for the second\ncase, we propose a \"biased matrix completion\" method that recovers the\n(thresholded) binary matrix. Both methods yield strong error bounds --- if M is\nn by n, the Frobenius error is bounded as O(1/((1-rho)n), where 1-rho denotes\nthe fraction of ones observed. This implies a sample complexity of O(n\\log n)\nones to achieve a small error, when M is dense and n is large. We extend our\nmethods and guarantees to the inductive matrix completion problem, where rows\nand columns of M have associated features. We provide efficient and scalable\noptimization procedures for both the methods and demonstrate the effectiveness\nof the proposed methods for link prediction (on real-world networks consisting\nof over 2 million nodes and 90 million links) and semi-supervised clustering\ntasks.\n", "versions": [{"version": "v1", "created": "Sat, 22 Nov 2014 04:37:15 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Hsieh", "Cho-Jui", ""], ["Natarajan", "Nagarajan", ""], ["Dhillon", "Inderjit S.", ""]]}, {"id": "1411.6156", "submitter": "Guy Bresler", "authors": "Guy Bresler", "title": "Efficiently learning Ising models on arbitrary graphs", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of reconstructing the graph underlying an Ising model\nfrom i.i.d. samples. Over the last fifteen years this problem has been of\nsignificant interest in the statistics, machine learning, and statistical\nphysics communities, and much of the effort has been directed towards finding\nalgorithms with low computational cost for various restricted classes of\nmodels. Nevertheless, for learning Ising models on general graphs with $p$\nnodes of degree at most $d$, it is not known whether or not it is possible to\nimprove upon the $p^{d}$ computation needed to exhaustively search over all\npossible neighborhoods for each node.\n  In this paper we show that a simple greedy procedure allows to learn the\nstructure of an Ising model on an arbitrary bounded-degree graph in time on the\norder of $p^2$. We make no assumptions on the parameters except what is\nnecessary for identifiability of the model, and in particular the results hold\nat low-temperatures as well as for highly non-uniform models. The proof rests\non a new structural property of Ising models: we show that for any node there\nexists at least one neighbor with which it has a high mutual information. This\nstructural property may be of independent interest.\n", "versions": [{"version": "v1", "created": "Sat, 22 Nov 2014 19:05:11 GMT"}, {"version": "v2", "created": "Mon, 1 Dec 2014 01:24:41 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Bresler", "Guy", ""]]}, {"id": "1411.6160", "submitter": "Martin Copenhaver", "authors": "Dimitris Bertsimas and Martin S. Copenhaver", "title": "Characterization of the equivalence of robustification and\n  regularization in linear and matrix regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.OC stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of developing statistical methods in machine learning which are\nrobust to adversarial perturbations in the underlying data has been the subject\nof increasing interest in recent years. A common feature of this work is that\nthe adversarial robustification often corresponds exactly to regularization\nmethods which appear as a loss function plus a penalty. In this paper we deepen\nand extend the understanding of the connection between robustification and\nregularization (as achieved by penalization) in regression problems.\nSpecifically, (a) in the context of linear regression, we characterize\nprecisely under which conditions on the model of uncertainty used and on the\nloss function penalties robustification and regularization are equivalent, and\n(b) we extend the characterization of robustification and regularization to\nmatrix regression problems (matrix completion and Principal Component\nAnalysis).\n", "versions": [{"version": "v1", "created": "Sat, 22 Nov 2014 19:59:32 GMT"}, {"version": "v2", "created": "Sat, 25 Feb 2017 17:58:26 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Bertsimas", "Dimitris", ""], ["Copenhaver", "Martin S.", ""]]}, {"id": "1411.6203", "submitter": "Jing Qian", "authors": "Jing Qian, Venkatesh Saligrama", "title": "Efficient Minimax Signal Detection on Graphs", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several problems such as network intrusion, community detection, and disease\noutbreak can be described by observations attributed to nodes or edges of a\ngraph. In these applications presence of intrusion, community or disease\noutbreak is characterized by novel observations on some unknown connected\nsubgraph. These problems can be formulated in terms of optimization of suitable\nobjectives on connected subgraphs, a problem which is generally computationally\ndifficult. We overcome the combinatorics of connectivity by embedding connected\nsubgraphs into linear matrix inequalities (LMI). Computationally efficient\ntests are then realized by optimizing convex objective functions subject to\nthese LMI constraints. We prove, by means of a novel Euclidean embedding\nargument, that our tests are minimax optimal for exponential family of\ndistributions on 1-D and 2-D lattices. We show that internal conductance of the\nconnected subgraph family plays a fundamental role in characterizing\ndetectability.\n", "versions": [{"version": "v1", "created": "Sun, 23 Nov 2014 07:40:09 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Qian", "Jing", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1411.6285", "submitter": "Avid Afzal", "authors": "Avid M. Afzal, Hamse Y. Mussa, Richard E. Turner, Andreas Bender,\n  Robert C. Glen", "title": "Target Fishing: A Single-Label or Multi-Label Problem?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  According to Cobanoglu et al and Murphy, it is now widely acknowledged that\nthe single target paradigm (one protein or target, one disease, one drug) that\nhas been the dominant premise in drug development in the recent past is\nuntenable. More often than not, a drug-like compound (ligand) can be\npromiscuous - that is, it can interact with more than one target protein. In\nrecent years, in in silico target prediction methods the promiscuity issue has\nbeen approached computationally in different ways. In this study we confine\nattention to the so-called ligand-based target prediction machine learning\napproaches, commonly referred to as target-fishing. With a few exceptions, the\ntarget-fishing approaches that are currently ubiquitous in cheminformatics\nliterature can be essentially viewed as single-label multi-classification\nschemes; these approaches inherently bank on the single target paradigm\nassumption that a ligand can home in on one specific target. In order to\naddress the ligand promiscuity issue, one might be able to cast target-fishing\nas a multi-label multi-class classification problem. For illustrative and\ncomparison purposes, single-label and multi-label Naive Bayes classification\nmodels (denoted here by SMM and MMM, respectively) for target-fishing were\nimplemented. The models were constructed and tested on 65,587 compounds and 308\ntargets retrieved from the ChEMBL17 database. SMM and MMM performed\ndifferently: for 16,344 test compounds, the MMM model returned recall and\nprecision values of 0.8058 and 0.6622, respectively; the corresponding recall\nand precision values yielded by the SMM model were 0.7805 and 0.7596,\nrespectively. However, at a significance level of 0.05 and one degree of\nfreedom McNemar test performed on the target prediction results returned by SMM\nand MMM for the 16,344 test ligands gave a chi-squared value of 15.656, in\nfavour of the MMM approach.\n", "versions": [{"version": "v1", "created": "Sun, 23 Nov 2014 18:50:42 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Afzal", "Avid M.", ""], ["Mussa", "Hamse Y.", ""], ["Turner", "Richard E.", ""], ["Bender", "Andreas", ""], ["Glen", "Robert C.", ""]]}, {"id": "1411.6307", "submitter": "Nematollah Kayhan Batmanghelich", "authors": "Nematollah Kayhan Batmanghelich, Gerald Quon, Alex Kulesza, Manolis\n  Kellis, Polina Golland, Luke Bornn", "title": "Diversifying Sparsity Using Variational Determinantal Point Processes", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel diverse feature selection method based on determinantal\npoint processes (DPPs). Our model enables one to flexibly define diversity\nbased on the covariance of features (similar to orthogonal matching pursuit) or\nalternatively based on side information. We introduce our approach in the\ncontext of Bayesian sparse regression, employing a DPP as a variational\napproximation to the true spike and slab posterior distribution. We\nsubsequently show how this variational DPP approximation generalizes and\nextends mean-field approximation, and can be learned efficiently by exploiting\nthe fast sampling properties of DPPs. Our motivating application comes from\nbioinformatics, where we aim to identify a diverse set of genes whose\nexpression profiles predict a tumor type where the diversity is defined with\nrespect to a gene-gene interaction network. We also explore an application in\nspatial statistics. In both cases, we demonstrate that the proposed method\nyields significantly more diverse feature sets than classic sparse methods,\nwithout compromising accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 23 Nov 2014 22:11:34 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Batmanghelich", "Nematollah Kayhan", ""], ["Quon", "Gerald", ""], ["Kulesza", "Alex", ""], ["Kellis", "Manolis", ""], ["Golland", "Polina", ""], ["Bornn", "Luke", ""]]}, {"id": "1411.6311", "submitter": "Irina Gaynanova", "authors": "Irina Gaynanova and Mladen Kolar", "title": "Optimal variable selection in multi-group sparse discriminant analysis", "comments": "22 pages, 2 figures", "journal-ref": "Electronic Journal of Statistics, Vol. 9, No. 2, 2007-2034", "doi": "10.1214/15-EJS1064", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article considers the problem of multi-group classification in the\nsetting where the number of variables $p$ is larger than the number of\nobservations $n$. Several methods have been proposed in the literature that\naddress this problem, however their variable selection performance is either\nunknown or suboptimal to the results known in the two-group case. In this work\nwe provide sharp conditions for the consistent recovery of relevant variables\nin the multi-group case using the discriminant analysis proposal of Gaynanova\net al., 2014. We achieve the rates of convergence that attain the optimal\nscaling of the sample size $n$, number of variables $p$ and the sparsity level\n$s$. These rates are significantly faster than the best known results in the\nmulti-group case. Moreover, they coincide with the optimal minimax rates for\nthe two-group case. We validate our theoretical results with numerical\nanalysis.\n", "versions": [{"version": "v1", "created": "Sun, 23 Nov 2014 22:42:21 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Gaynanova", "Irina", ""], ["Kolar", "Mladen", ""]]}, {"id": "1411.6314", "submitter": "Aaditya Ramdas", "authors": "Aaditya Ramdas, Sashank J. Reddi, Barnabas Poczos, Aarti Singh, Larry\n  Wasserman", "title": "On the High-dimensional Power of Linear-time Kernel Two-Sample Testing\n  under Mean-difference Alternatives", "comments": "25 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.AI cs.IT cs.LG math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric two sample testing deals with the question of consistently\ndeciding if two distributions are different, given samples from both, without\nmaking any parametric assumptions about the form of the distributions. The\ncurrent literature is split into two kinds of tests - those which are\nconsistent without any assumptions about how the distributions may differ\n(\\textit{general} alternatives), and those which are designed to specifically\ntest easier alternatives, like a difference in means (\\textit{mean-shift}\nalternatives).\n  The main contribution of this paper is to explicitly characterize the power\nof a popular nonparametric two sample test, designed for general alternatives,\nunder a mean-shift alternative in the high-dimensional setting. Specifically,\nwe explicitly derive the power of the linear-time Maximum Mean Discrepancy\nstatistic using the Gaussian kernel, where the dimension and sample size can\nboth tend to infinity at any rate, and the two distributions differ in their\nmeans. As a corollary, we find that if the signal-to-noise ratio is held\nconstant, then the test's power goes to one if the number of samples increases\nfaster than the dimension increases. This is the first explicit power\nderivation for a general nonparametric test in the high-dimensional setting,\nand also the first analysis of how tests designed for general alternatives\nperform when faced with easier ones.\n", "versions": [{"version": "v1", "created": "Sun, 23 Nov 2014 23:32:02 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Ramdas", "Aaditya", ""], ["Reddi", "Sashank J.", ""], ["Poczos", "Barnabas", ""], ["Singh", "Aarti", ""], ["Wasserman", "Larry", ""]]}, {"id": "1411.6370", "submitter": "Jun Zhu", "authors": "Jun Zhu, Jianfei Chen, Wenbo Hu, Bo Zhang", "title": "Big Learning with Bayesian Methods", "comments": "21 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explosive growth in data and availability of cheap computing resources have\nsparked increasing interest in Big learning, an emerging subfield that studies\nscalable machine learning algorithms, systems, and applications with Big Data.\nBayesian methods represent one important class of statistic methods for machine\nlearning, with substantial recent developments on adaptive, flexible and\nscalable Bayesian learning. This article provides a survey of the recent\nadvances in Big learning with Bayesian methods, termed Big Bayesian Learning,\nincluding nonparametric Bayesian methods for adaptively inferring model\ncomplexity, regularized Bayesian inference for improving the flexibility via\nposterior regularization, and scalable algorithms and systems based on\nstochastic subsampling and distributed computing for dealing with large-scale\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 07:28:51 GMT"}, {"version": "v2", "created": "Wed, 1 Mar 2017 14:07:26 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Zhu", "Jun", ""], ["Chen", "Jianfei", ""], ["Hu", "Wenbo", ""], ["Zhang", "Bo", ""]]}, {"id": "1411.6400", "submitter": "Min Wei", "authors": "Min Wei, Tommy W. S. Chow, Rosa H. M. Chan", "title": "Mutual Information-Based Unsupervised Feature Transformation for\n  Heterogeneous Feature Subset Selection", "comments": "This paper has been withdrawn by the author due to the number of\n  datasets and classifiers are not sufficient to support the claim. Need more\n  simulation work", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional mutual information (MI) based feature selection (FS) methods are\nunable to handle heterogeneous feature subset selection properly because of\ndata format differences or estimation methods of MI between feature subset and\nclass label. A way to solve this problem is feature transformation (FT). In\nthis study, a novel unsupervised feature transformation (UFT) which can\ntransform non-numerical features into numerical features is developed and\ntested. The UFT process is MI-based and independent of class label. MI-based FS\nalgorithms, such as Parzen window feature selector (PWFS), minimum redundancy\nmaximum relevance feature selection (mRMR), and normalized MI feature selection\n(NMIFS), can all adopt UFT for pre-processing of non-numerical features. Unlike\ntraditional FT methods, the proposed UFT is unbiased while PWFS is utilized to\nits full advantage. Simulations and analyses of large-scale datasets showed\nthat feature subset selected by the integrated method, UFT-PWFS, outperformed\nother FT-FS integrated methods in classification accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 10:15:17 GMT"}, {"version": "v2", "created": "Sun, 29 Mar 2015 05:32:50 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Wei", "Min", ""], ["Chow", "Tommy W. S.", ""], ["Chan", "Rosa H. M.", ""]]}, {"id": "1411.6520", "submitter": "Ilya Trofimov", "authors": "Ilya Trofimov, Alexander Genkin", "title": "Distributed Coordinate Descent for L1-regularized Logistic Regression", "comments": null, "journal-ref": "Analysis of Images, Social Networks and Texts. Fourth\n  International Conference, AIST 2015, Yekaterinburg, Russia, April 9-11, 2015,\n  Revised Selected Papers. Communications in Computer and Information Science,\n  Vol. 542, 243-254, Springer", "doi": "10.1007/978-3-319-26123-2_24", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving logistic regression with L1-regularization in distributed settings is\nan important problem. This problem arises when training dataset is very large\nand cannot fit the memory of a single machine. We present d-GLMNET, a new\nalgorithm solving logistic regression with L1-regularization in the distributed\nsettings. We empirically show that it is superior over distributed online\nlearning via truncated gradient.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 16:40:33 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Trofimov", "Ilya", ""], ["Genkin", "Alexander", ""]]}, {"id": "1411.6590", "submitter": "Dejan Slep\\v{c}ev", "authors": "Nicolas Garcia Trillos, Dejan Slepcev, James von Brecht, Thomas\n  Laurent and Xavier Bresson", "title": "Consistency of Cheeger and Ratio Graph Cuts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper establishes the consistency of a family of graph-cut-based\nalgorithms for clustering of data clouds. We consider point clouds obtained as\nsamples of a ground-truth measure. We investigate approaches to clustering\nbased on minimizing objective functionals defined on proximity graphs of the\ngiven sample. Our focus is on functionals based on graph cuts like the Cheeger\nand ratio cuts. We show that minimizers of the these cuts converge as the\nsample size increases to a minimizer of a corresponding continuum cut (which\npartitions the ground truth measure). Moreover, we obtain sharp conditions on\nhow the connectivity radius can be scaled with respect to the number of sample\npoints for the consistency to hold. We provide results for two-way and for\nmultiway cuts. Furthermore we provide numerical experiments that illustrate the\nresults and explore the optimality of scaling in dimension two.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 19:55:09 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Trillos", "Nicolas Garcia", ""], ["Slepcev", "Dejan", ""], ["von Brecht", "James", ""], ["Laurent", "Thomas", ""], ["Bresson", "Xavier", ""]]}, {"id": "1411.6591", "submitter": "George Chen", "authors": "Guy Bresler, George H. Chen, Devavrat Shah", "title": "A Latent Source Model for Online Collaborative Filtering", "comments": "Advances in Neural Information Processing Systems (NIPS 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the prevalence of collaborative filtering in recommendation systems,\nthere has been little theoretical development on why and how well it works,\nespecially in the \"online\" setting, where items are recommended to users over\ntime. We address this theoretical gap by introducing a model for online\nrecommendation systems, cast item recommendation under the model as a learning\nproblem, and analyze the performance of a cosine-similarity collaborative\nfiltering method. In our model, each of $n$ users either likes or dislikes each\nof $m$ items. We assume there to be $k$ types of users, and all the users of a\ngiven type share a common string of probabilities determining the chance of\nliking each item. At each time step, we recommend an item to each user, where a\nkey distinction from related bandit literature is that once a user consumes an\nitem (e.g., watches a movie), then that item cannot be recommended to the same\nuser again. The goal is to maximize the number of likable items recommended to\nusers over time. Our main result establishes that after nearly $\\log(km)$\ninitial learning time steps, a simple collaborative filtering algorithm\nachieves essentially optimal performance without knowing $k$. The algorithm has\nan exploitation step that uses cosine similarity and two types of exploration\nsteps, one to explore the space of items (standard in the literature) and the\nother to explore similarity between users (novel to this work).\n", "versions": [{"version": "v1", "created": "Fri, 31 Oct 2014 19:59:59 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Bresler", "Guy", ""], ["Chen", "George H.", ""], ["Shah", "Devavrat", ""]]}, {"id": "1411.6622", "submitter": "Osonde Osoba Ph.D.", "authors": "Osonde Adekorede Osoba", "title": "Noise Benefits in Expectation-Maximization Algorithms", "comments": "A Dissertation Presented to The Faculty of The USC Graduate School\n  University of Southern California In Partial Fulfillment of the Requirements\n  for the Degree Doctor of Philosophy (Electrical Engineering) August 2013.\n  (252 pages, 45 figures), Online:\n  http://digitallibrary.usc.edu/cdm/ref/collection/p15799coll3/id/294341", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This dissertation shows that careful injection of noise into sample data can\nsubstantially speed up Expectation-Maximization algorithms.\nExpectation-Maximization algorithms are a class of iterative algorithms for\nextracting maximum likelihood estimates from corrupted or incomplete data. The\nconvergence speed-up is an example of a noise benefit or \"stochastic resonance\"\nin statistical signal processing. The dissertation presents derivations of\nsufficient conditions for such noise-benefits and demonstrates the speed-up in\nsome ubiquitous signal-processing algorithms. These algorithms include\nparameter estimation for mixture models, the $k$-means clustering algorithm,\nthe Baum-Welch algorithm for training hidden Markov models, and backpropagation\nfor training feedforward artificial neural networks. This dissertation also\nanalyses the effects of data and model corruption on the more general Bayesian\ninference estimation framework. The main finding is a theorem guaranteeing that\nuniform approximators for Bayesian model functions produce uniform\napproximators for the posterior pdf via Bayes theorem. This result also applies\nto hierarchical and multidimensional Bayesian models.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 17:30:57 GMT"}], "update_date": "2014-11-26", "authors_parsed": [["Osoba", "Osonde Adekorede", ""]]}, {"id": "1411.6651", "submitter": "Amir Arsalan Soltani", "authors": "Amir Arsalan Soltani", "title": "A Greedy, Flexible Algorithm to Learn an Optimal Bayesian Network\n  Structure", "comments": "This was my Advanced Machine Learning's course project in Spring 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report paper we first present a report of the Advanced Machine\nLearning Course Project on the provided data set and then present a novel\nheuristic algorithm for exact Bayesian network (BN) structure discovery that\nuses decomposable scoring functions. Our algorithm follows a different approach\nto solve the problem of BN structure discovery than the previously used methods\nsuch as Dynamic Programming (DP) and Branch and Bound to reduce the search\nspace and find the global optima space for the problem. The algorithm we\npropose has some degree of flexibility that can make it more or less greedy.\nThe more the algorithm is set to be greedy, the more the speed of the algorithm\nwill be, and the less optimal the final structure. Our algorithm runs in a much\nless time than the previously known methods and guarantees to have an\noptimality of close to 99%. Therefore, it sacrifices less than one percent of\nscore of an optimal structure in order to gain a much lower running time and\nmake the algorithm feasible for large data sets (we may note that we never used\nany toolbox except for result validation)\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 21:27:37 GMT"}], "update_date": "2014-11-26", "authors_parsed": [["Soltani", "Amir Arsalan", ""]]}, {"id": "1411.6948", "submitter": "Wenwen Zhang", "authors": "Wenwen Zhang and Wei-Yin Loh", "title": "PLUTO: Penalized Unbiased Logistic Regression Trees", "comments": "59 pages, 25 figures, 14 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm called PLUTO for building logistic regression\ntrees to binary response data. PLUTO can capture the nonlinear and interaction\npatterns in messy data by recursively partitioning the sample space. It fits a\nsimple or a multiple linear logistic regression model in each partition. PLUTO\nemploys the cyclical coordinate descent method for estimation of multiple\nlinear logistic regression models with elastic net penalties, which allows it\nto deal with high-dimensional data efficiently. The tree structure comprises a\ngraphical description of the data. Together with the logistic regression\nmodels, it provides an accurate classifier as well as a piecewise smooth\nestimate of the probability of \"success\". PLUTO controls selection bias by: (1)\nseparating split variable selection from split point selection; (2) applying an\nadjusted chi-squared test to find the split variable instead of exhaustive\nsearch. A bootstrap calibration technique is employed to further correct\nselection bias. Comparison on real datasets shows that on average, the multiple\nlinear PLUTO models predict more accurately than other algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 18:09:58 GMT"}], "update_date": "2014-11-26", "authors_parsed": [["Zhang", "Wenwen", ""], ["Loh", "Wei-Yin", ""]]}, {"id": "1411.7200", "submitter": "Ilya Tolstikhin", "authors": "Ilya Tolstikhin and Gilles Blanchard and Marius Kloft", "title": "Localized Complexities for Transductive Learning", "comments": "Appeared in Conference on Learning Theory 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show two novel concentration inequalities for suprema of empirical\nprocesses when sampling without replacement, which both take the variance of\nthe functions into account. While these inequalities may potentially have broad\napplications in learning theory in general, we exemplify their significance by\nstudying the transductive setting of learning theory. For which we provide the\nfirst excess risk bounds based on the localized complexity of the hypothesis\nclass, which can yield fast rates of convergence also in the transductive\nlearning setting. We give a preliminary analysis of the localized complexities\nfor the prominent case of kernel classes.\n", "versions": [{"version": "v1", "created": "Wed, 26 Nov 2014 12:14:22 GMT"}], "update_date": "2014-11-27", "authors_parsed": [["Tolstikhin", "Ilya", ""], ["Blanchard", "Gilles", ""], ["Kloft", "Marius", ""]]}, {"id": "1411.7245", "submitter": "Nicolas Gillis", "authors": "Arnaud Vandaele and Nicolas Gillis and Fran\\c{c}ois Glineur and Daniel\n  Tuyttens", "title": "Heuristics for Exact Nonnegative Matrix Factorization", "comments": "32 pages, 2 figures, 16 tables", "journal-ref": "Journal of Global Optimization 65 (2), pp 369-400, 2016", "doi": "10.1007/s10898-015-0350-z", "report-no": null, "categories": "math.OC cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exact nonnegative matrix factorization (exact NMF) problem is the\nfollowing: given an $m$-by-$n$ nonnegative matrix $X$ and a factorization rank\n$r$, find, if possible, an $m$-by-$r$ nonnegative matrix $W$ and an $r$-by-$n$\nnonnegative matrix $H$ such that $X = WH$. In this paper, we propose two\nheuristics for exact NMF, one inspired from simulated annealing and the other\nfrom the greedy randomized adaptive search procedure. We show that these two\nheuristics are able to compute exact nonnegative factorizations for several\nclasses of nonnegative matrices (namely, linear Euclidean distance matrices,\nslack matrices, unique-disjointness matrices, and randomly generated matrices)\nand as such demonstrate their superiority over standard multi-start strategies.\nWe also consider a hybridization between these two heuristics that allows us to\ncombine the advantages of both methods. Finally, we discuss the use of these\nheuristics to gain insight on the behavior of the nonnegative rank, i.e., the\nminimum factorization rank such that an exact NMF exists. In particular, we\ndisprove a conjecture on the nonnegative rank of a Kronecker product, propose a\nnew upper bound on the extension complexity of generic $n$-gons and conjecture\nthe exact value of (i) the extension complexity of regular $n$-gons and (ii)\nthe nonnegative rank of a submatrix of the slack matrix of the correlation\npolytope.\n", "versions": [{"version": "v1", "created": "Wed, 26 Nov 2014 14:33:59 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Vandaele", "Arnaud", ""], ["Gillis", "Nicolas", ""], ["Glineur", "Fran\u00e7ois", ""], ["Tuyttens", "Daniel", ""]]}, {"id": "1411.7405", "submitter": "Karl Rohe", "authors": "Karl Rohe", "title": "A note relating ridge regression and OLS p-values to preconditioned\n  sparse penalized regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When the design matrix has orthonormal columns, \"soft thresholding\" the\nordinary least squares (OLS) solution produces the Lasso solution [Tibshirani,\n1996]. If one uses the Puffer preconditioned Lasso [Jia and Rohe, 2012], then\nthis result generalizes from orthonormal designs to full rank designs (Theorem\n1). Theorem 2 refines the Puffer preconditioner to make the Lasso select the\nsame model as removing the elements of the OLS solution with the largest\np-values. Using a generalized Puffer preconditioner, Theorem 3 relates ridge\nregression to the preconditioned Lasso; this result is for the high dimensional\nsetting, p > n. Where the standard Lasso is akin to forward selection [Efron et\nal., 2004], Theorems 1, 2, and 3 suggest that the preconditioned Lasso is more\nakin to backward elimination. These results hold for sparse penalties beyond\nl1; for a broad class of sparse and non-convex techniques (e.g. SCAD and MC+),\nthe results hold for all local minima.\n", "versions": [{"version": "v1", "created": "Wed, 26 Nov 2014 21:47:00 GMT"}, {"version": "v2", "created": "Wed, 3 Dec 2014 16:12:59 GMT"}], "update_date": "2014-12-04", "authors_parsed": [["Rohe", "Karl", ""]]}, {"id": "1411.7414", "submitter": "Siheng Chen", "authors": "Siheng Chen and Aliaksei Sandryhaila and Jos\\'e M. F. Moura and Jelena\n  Kova\\v{c}evi\\'c", "title": "Signal Recovery on Graphs: Variation Minimization", "comments": "To appear on IEEE Transactions on Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2015.2441042", "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of signal recovery on graphs as graphs model data\nwith complex structure as signals on a graph. Graph signal recovery implies\nrecovery of one or multiple smooth graph signals from noisy, corrupted, or\nincomplete measurements. We propose a graph signal model and formulate signal\nrecovery as a corresponding optimization problem. We provide a general solution\nby using the alternating direction methods of multipliers. We next show how\nsignal inpainting, matrix completion, robust principal component analysis, and\nanomaly detection all relate to graph signal recovery, and provide\ncorresponding specific solutions and theoretical analysis. Finally, we validate\nthe proposed methods on real-world recovery problems, including online blog\nclassification, bridge condition identification, temperature estimation,\nrecommender system, and expert opinion combination of online blog\nclassification.\n", "versions": [{"version": "v1", "created": "Wed, 26 Nov 2014 22:38:08 GMT"}, {"version": "v2", "created": "Sat, 6 Dec 2014 14:36:33 GMT"}, {"version": "v3", "created": "Fri, 29 May 2015 14:55:38 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Chen", "Siheng", ""], ["Sandryhaila", "Aliaksei", ""], ["Moura", "Jos\u00e9 M. F.", ""], ["Kova\u010devi\u0107", "Jelena", ""]]}, {"id": "1411.7432", "submitter": "Alessandra Tosi", "authors": "Alessandra Tosi, S{\\o}ren Hauberg, Alfredo Vellido, Neil D. Lawrence", "title": "Metrics for Probabilistic Geometries", "comments": "UAI 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the geometrical structure of probabilistic generative\ndimensionality reduction models using the tools of Riemannian geometry. We\nexplicitly define a distribution over the natural metric given by the models.\nWe provide the necessary algorithms to compute expected metric tensors where\nthe distribution over mappings is given by a Gaussian process. We treat the\ncorresponding latent variable model as a Riemannian manifold and we use the\nexpectation of the metric under the Gaussian process prior to define\ninterpolating paths and measure distance between latent points. We show how\ndistances that respect the expected metric lead to more appropriate generation\nof new data.\n", "versions": [{"version": "v1", "created": "Thu, 27 Nov 2014 00:27:05 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Tosi", "Alessandra", ""], ["Hauberg", "S\u00f8ren", ""], ["Vellido", "Alfredo", ""], ["Lawrence", "Neil D.", ""]]}, {"id": "1411.7441", "submitter": "Stefano Ermon", "authors": "Stefano Ermon, Ronan Le Bras, Santosh K. Suram, John M. Gregoire,\n  Carla Gomes, Bart Selman, Robert B. van Dover", "title": "Pattern Decomposition with Complex Combinatorial Constraints:\n  Application to Materials Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying important components or factors in large amounts of noisy data is\na key problem in machine learning and data mining. Motivated by a pattern\ndecomposition problem in materials discovery, aimed at discovering new\nmaterials for renewable energy, e.g. for fuel and solar cells, we introduce\nCombiFD, a framework for factor based pattern decomposition that allows the\nincorporation of a-priori knowledge as constraints, including complex\ncombinatorial constraints. In addition, we propose a new pattern decomposition\nalgorithm, called AMIQO, based on solving a sequence of (mixed-integer)\nquadratic programs. Our approach considerably outperforms the state of the art\non the materials discovery problem, scaling to larger datasets and recovering\nmore precise and physically meaningful decompositions. We also show the\neffectiveness of our approach for enforcing background knowledge on other\napplication domains.\n", "versions": [{"version": "v1", "created": "Thu, 27 Nov 2014 02:31:41 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Ermon", "Stefano", ""], ["Bras", "Ronan Le", ""], ["Suram", "Santosh K.", ""], ["Gregoire", "John M.", ""], ["Gomes", "Carla", ""], ["Selman", "Bart", ""], ["van Dover", "Robert B.", ""]]}, {"id": "1411.7508", "submitter": "Amirhossein Mehrkesh", "authors": "Amirhossein Mehrkesh, Maryam Ahmadi", "title": "Forecasting the Colorado River Discharge Using an Artificial Neural\n  Network (ANN) Approach", "comments": "11 pages, 4 figures and 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Neural Network (ANN) based model is a computational approach\ncommonly used for modeling the complex relationships between input and output\nparameters. Prediction of the flow rate of a river is a requisite for any\nsuccessful water resource management and river basin planning. In the current\nsurvey, the effectiveness of an Artificial Neural Network was examined to\npredict the Colorado River discharge. In this modeling process, an ANN model\nwas used to relate the discharge of the Colorado River to such parameters as\nthe amount of precipitation, ambient temperature and snowpack level at a\nspecific time of the year. The model was able to precisely study the impact of\nclimatic parameters on the flow rate of the Colorado River.\n", "versions": [{"version": "v1", "created": "Thu, 27 Nov 2014 09:26:09 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Mehrkesh", "Amirhossein", ""], ["Ahmadi", "Maryam", ""]]}, {"id": "1411.7596", "submitter": "Dustin Tran", "authors": "Dustin Tran", "title": "Convex Techniques for Model Selection", "comments": "Originally written on May 16, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a robust convex algorithm to select the regularization parameter\nin model selection. In practice this would be automated in order to save\npractitioners time from having to tune it manually. In particular, we implement\nand test the convex method for $K$-fold cross validation on ridge regression,\nalthough the same concept extends to more complex models. We then compare its\nperformance with standard methods.\n", "versions": [{"version": "v1", "created": "Thu, 27 Nov 2014 13:43:58 GMT"}, {"version": "v2", "created": "Tue, 2 Dec 2014 12:17:55 GMT"}], "update_date": "2014-12-03", "authors_parsed": [["Tran", "Dustin", ""]]}, {"id": "1411.7610", "submitter": "Justin Bayer", "authors": "Justin Bayer and Christian Osendorfer", "title": "Learning Stochastic Recurrent Networks", "comments": "Submitted to conference track of ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leveraging advances in variational inference, we propose to enhance recurrent\nneural networks with latent variables, resulting in Stochastic Recurrent\nNetworks (STORNs). The model i) can be trained with stochastic gradient\nmethods, ii) allows structured and multi-modal conditionals at each time step,\niii) features a reliable estimator of the marginal likelihood and iv) is a\ngeneralisation of deterministic recurrent neural networks. We evaluate the\nmethod on four polyphonic musical data sets and motion capture data.\n", "versions": [{"version": "v1", "created": "Thu, 27 Nov 2014 14:22:36 GMT"}, {"version": "v2", "created": "Fri, 2 Jan 2015 09:28:54 GMT"}, {"version": "v3", "created": "Thu, 5 Mar 2015 21:55:38 GMT"}], "update_date": "2015-03-09", "authors_parsed": [["Bayer", "Justin", ""], ["Osendorfer", "Christian", ""]]}, {"id": "1411.7706", "submitter": "Scott Linderman", "authors": "Scott W. Linderman, Matthew J. Johnson, Matthew A. Wilson, and Zhe\n  Chen", "title": "A Nonparametric Bayesian Approach to Uncovering Rat Hippocampal\n  Population Codes During Spatial Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rodent hippocampal population codes represent important spatial information\nabout the environment during navigation. Several computational methods have\nbeen developed to uncover the neural representation of spatial topology\nembedded in rodent hippocampal ensemble spike activity. Here we extend our\nprevious work and propose a nonparametric Bayesian approach to infer rat\nhippocampal population codes during spatial navigation. To tackle the model\nselection problem, we leverage a nonparametric Bayesian model. Specifically, to\nanalyze rat hippocampal ensemble spiking activity, we apply a hierarchical\nDirichlet process-hidden Markov model (HDP-HMM) using two Bayesian inference\nmethods, one based on Markov chain Monte Carlo (MCMC) and the other based on\nvariational Bayes (VB). We demonstrate the effectiveness of our Bayesian\napproaches on recordings from a freely-behaving rat navigating in an open field\nenvironment. We find that MCMC-based inference with Hamiltonian Monte Carlo\n(HMC) hyperparameter sampling is flexible and efficient, and outperforms VB and\nMCMC approaches with hyperparameters set by empirical Bayes.\n", "versions": [{"version": "v1", "created": "Thu, 27 Nov 2014 21:17:47 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Linderman", "Scott W.", ""], ["Johnson", "Matthew J.", ""], ["Wilson", "Matthew A.", ""], ["Chen", "Zhe", ""]]}, {"id": "1411.7717", "submitter": "James Martens", "authors": "James Martens, Venkatesh Medabalimi", "title": "On the Expressive Efficiency of Sum Product Networks", "comments": "Various minor revisions and corrections throughout", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sum Product Networks (SPNs) are a recently developed class of deep generative\nmodels which compute their associated unnormalized density functions using a\nspecial type of arithmetic circuit. When certain sufficient conditions, called\nthe decomposability and completeness conditions (or \"D&C\" conditions), are\nimposed on the structure of these circuits, marginal densities and other useful\nquantities, which are typically intractable for other deep generative models,\ncan be computed by what amounts to a single evaluation of the network (which is\na property known as \"validity\"). However, the effect that the D&C conditions\nhave on the capabilities of D&C SPNs is not well understood.\n  In this work we analyze the D&C conditions, expose the various connections\nthat D&C SPNs have with multilinear arithmetic circuits, and consider the\nquestion of how well they can capture various distributions as a function of\ntheir size and depth. Among our various contributions is a result which\nestablishes the existence of a relatively simple distribution with fully\ntractable marginal densities which cannot be efficiently captured by D&C SPNs\nof any depth, but which can be efficiently captured by various other deep\ngenerative models. We also show that with each additional layer of depth\npermitted, the set of distributions which can be efficiently captured by D&C\nSPNs grows in size. This kind of \"depth hierarchy\" property has been widely\nconjectured to hold for various deep models, but has never been proven for any\nof them. Some of our other contributions include a new characterization of the\nD&C conditions as sufficient and necessary ones for a slightly strengthened\nnotion of validity, and various state-machine characterizations of the types of\ncomputations that can be performed efficiently by D&C SPNs.\n", "versions": [{"version": "v1", "created": "Thu, 27 Nov 2014 23:02:41 GMT"}, {"version": "v2", "created": "Tue, 9 Dec 2014 02:00:39 GMT"}, {"version": "v3", "created": "Fri, 23 Jan 2015 03:28:47 GMT"}], "update_date": "2015-01-26", "authors_parsed": [["Martens", "James", ""], ["Medabalimi", "Venkatesh", ""]]}, {"id": "1411.7718", "submitter": "Dacheng Tao", "authors": "Tongliang Liu and Dacheng Tao", "title": "Classification with Noisy Labels by Importance Reweighting", "comments": null, "journal-ref": null, "doi": "10.1109/TPAMI.2015.2456899", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a classification problem in which sample labels are\nrandomly corrupted. In this scenario, there is an unobservable sample with\nnoise-free labels. However, before being observed, the true labels are\nindependently flipped with a probability $\\rho\\in[0,0.5)$, and the random label\nnoise can be class-conditional. Here, we address two fundamental problems\nraised by this scenario. The first is how to best use the abundant surrogate\nloss functions designed for the traditional classification problem when there\nis label noise. We prove that any surrogate loss function can be used for\nclassification with noisy labels by using importance reweighting, with\nconsistency assurance that the label noise does not ultimately hinder the\nsearch for the optimal classifier of the noise-free sample. The other is the\nopen problem of how to obtain the noise rate $\\rho$. We show that the rate is\nupper bounded by the conditional probability $P(y|x)$ of the noisy sample.\nConsequently, the rate can be estimated, because the upper bound can be easily\nreached in classification problems. Experimental results on synthetic and real\ndatasets confirm the efficiency of our methods.\n", "versions": [{"version": "v1", "created": "Thu, 27 Nov 2014 23:18:51 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2015 04:03:44 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Liu", "Tongliang", ""], ["Tao", "Dacheng", ""]]}, {"id": "1411.7783", "submitter": "Harri Valpola", "authors": "Harri Valpola", "title": "From neural PCA to deep unsupervised learning", "comments": "A revised version of an article that has been accepted for\n  publication in Advances in Independent Component Analysis and Learning\n  Machines (2015), edited by Ella Bingham, Samuel Kaski, Jorma Laaksonen and\n  Jouko Lampinen", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A network supporting deep unsupervised learning is presented. The network is\nan autoencoder with lateral shortcut connections from the encoder to decoder at\neach level of the hierarchy. The lateral shortcut connections allow the higher\nlevels of the hierarchy to focus on abstract invariant features. While standard\nautoencoders are analogous to latent variable models with a single layer of\nstochastic variables, the proposed network is analogous to hierarchical latent\nvariables models. Learning combines denoising autoencoder and denoising sources\nseparation frameworks. Each layer of the network contributes to the cost\nfunction a term which measures the distance of the representations produced by\nthe encoder and the decoder. Since training signals originate from all levels\nof the network, all layers can learn efficiently even in deep networks. The\nspeedup offered by cost terms from higher levels of the hierarchy and the\nability to learn invariant features are demonstrated in experiments.\n", "versions": [{"version": "v1", "created": "Fri, 28 Nov 2014 09:03:24 GMT"}, {"version": "v2", "created": "Mon, 2 Feb 2015 12:58:05 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Valpola", "Harri", ""]]}, {"id": "1411.7817", "submitter": "Franz J. Kir\\'aly", "authors": "Franz J. Kir\\'aly, Andreas Ziehe, Klaus-Robert M\\\"uller", "title": "Learning with Algebraic Invariances, and the Invariant Kernel Trick", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When solving data analysis problems it is important to integrate prior\nknowledge and/or structural invariances. This paper contributes by a novel\nframework for incorporating algebraic invariance structure into kernels. In\nparticular, we show that algebraic properties such as sign symmetries in data,\nphase independence, scaling etc. can be included easily by essentially\nperforming the kernel trick twice. We demonstrate the usefulness of our theory\nin simulations on selected applications such as sign-invariant spectral\nclustering and underdetermined ICA.\n", "versions": [{"version": "v1", "created": "Fri, 28 Nov 2014 11:20:48 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Kir\u00e1ly", "Franz J.", ""], ["Ziehe", "Andreas", ""], ["M\u00fcller", "Klaus-Robert", ""]]}, {"id": "1411.7864", "submitter": "Bjarne {\\O}rum Fruergaard", "authors": "Bjarne {\\O}rum Fruergaard, Tue Herlau", "title": "Efficient inference of overlapping communities in complex networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss two views on extending existing methods for complex network\nmodeling which we dub the communities first and the networks first view,\nrespectively. Inspired by the networks first view that we attribute to White,\nBoorman, and Breiger (1976)[1], we formulate the multiple-networks stochastic\nblockmodel (MNSBM), which seeks to separate the observed network into\nsubnetworks of different types and where the problem of inferring structure in\neach subnetwork becomes easier. We show how this model is specified in a\ngenerative Bayesian framework where parameters can be inferred efficiently\nusing Gibbs sampling. The result is an effective multiple-membership model\nwithout the drawbacks of introducing complex definitions of \"groups\" and how\nthey interact. We demonstrate results on the recovery of planted structure in\nsynthetic networks and show very encouraging results on link prediction\nperformances using multiple-networks models on a number of real-world network\ndata sets.\n", "versions": [{"version": "v1", "created": "Fri, 28 Nov 2014 13:38:40 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Fruergaard", "Bjarne \u00d8rum", ""], ["Herlau", "Tue", ""]]}, {"id": "1411.7924", "submitter": "Bjarne {\\O}rum Fruergaard", "authors": "Bjarne {\\O}rum Fruergaard", "title": "Predicting clicks in online display advertising with latent features and\n  side-information", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review a method for click-through rate prediction based on the work of\nMenon et al. [11], which combines collaborative filtering and matrix\nfactorization with a side-information model and fuses the outputs to proper\nprobabilities in [0,1]. In addition we provide details, both for the modeling\nas well as the experimental part, that are not found elsewhere. We rigorously\ntest the performance on several test data sets from consecutive days in a\nclick-through rate prediction setup, in a manner which reflects a real-world\npipeline. Our results confirm that performance can be increased using latent\nfeatures, albeit the differences in the measures are small but significant.\n", "versions": [{"version": "v1", "created": "Fri, 28 Nov 2014 16:06:52 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Fruergaard", "Bjarne \u00d8rum", ""]]}]